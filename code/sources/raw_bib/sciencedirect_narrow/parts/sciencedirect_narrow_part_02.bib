@article{SIEBRA2022109152,
title = {Engineering uncertain time for its practical integration in ontologies},
journal = {Knowledge-Based Systems},
volume = {251},
pages = {109152},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109152},
url = {https://www.sciencedirect.com/science/article/pii/S095070512200572X},
author = {Clauirton A. Siebra and Katarzyna Wac},
keywords = {Ontology design, Rule-based processing, Uncertainty, Temporal logic},
abstract = {Ontologies are commonly used as a strategy for knowledge representation. However, they are still presenting limitations to model domains that require broad forms of temporal reasoning. This study is part of the Onto-mQoL project and was motivated by the real need to extend static ontologies with diverse time concepts, relations and properties, which go beyond the commonly used Allen’s Interval Algebra. Therefore, we use the n-ary relations as the basis for temporal structures, which minimally modify the original ontology, and extend these structures with a generic set of time concepts (moments and intervals), time concept properties (precise and uncertain), time relations (interval–interval, interval–moment, and moment–moment), and time relation properties (qualitative and quantitative). We divided the scientific contribution of this study into three parts. Firstly, we present the ontological temporal model (classes and properties) and how it is integrated into static ontologies. Secondly, we discuss the creation of axioms that give the semantics for precise temporal elements. Finally, as our main contribution, these ideas are extended with axioms for uncertain time. All these elements follow the Ontology Web Language (OWL) standards, so this proposal is still compatible with the main ontology editors and reasoners currently available. A case example demonstrates the use of this approach in the nutrition assessment domain.}
}
@article{CHAVESFRAGA2020100596,
title = {GTFS-Madrid-Bench: A benchmark for virtual knowledge graph access in the transport domain},
journal = {Journal of Web Semantics},
volume = {65},
pages = {100596},
year = {2020},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100596},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300354},
author = {David Chaves-Fraga and Freddy Priyatna and Andrea Cimmino and Jhon Toledo and Edna Ruckhaus and Oscar Corcho},
keywords = {Virtual knowledge graph, Benchmark, Query translation, Data integration, GTFS},
abstract = {A large number of datasets are being made available on the Web using a variety of formats and according to diverse data models. Ontology Based Data Integration (OBDI) has been traditionally proposed as a mechanism to facilitate access to such heterogeneous datasets, providing a unified view over their data by means of ontologies. Recently, the term “Virtual Knowledge Graph Access” has begun to be used to refer to the mechanisms that provide query-based access to knowledge graphs virtually generated from heterogeneous data sources. Several OBDI engines exist in the state of the art, with overlapping capabilities but also clear differences among them (in terms of the data formats that they can deal with, mapping languages that they support, query expressivity that they allow, etc.). These engines have been evaluated with different testbeds and benchmarks. However, their heterogeneity has made it difficult to come up with a common comprehensive benchmark that allows for comparisons among them to facilitate their selection by practitioners, and more importantly, for their continuous improvement by the teams that maintain them. In this paper we present GTFS-Madrid-Bench, a benchmark to evaluate OBDI engines that can be used for the provision of access mechanisms to virtual knowledge graphs. Our proposal introduces several scenarios that aim at measuring the query capabilities, performance and scalability of all these engines, considering their heterogeneity. The data sources used in our benchmark are derived from the GTFS data files of the subway network of Madrid. They have been transformed into several formats (CSV, JSON, SQL and XML) and scaled up. The query set aims at addressing a representative number of SPARQL 1.1 features while covering usual queries that data consumers may be interested in.}
}
@article{GONZALEZSENDINO2024384,
title = {Mitigating bias in artificial intelligence: Fair data generation via causal models for transparent and explainable decision-making},
journal = {Future Generation Computer Systems},
volume = {155},
pages = {384-401},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.02.023},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24000694},
author = {Rubén González-Sendino and Emilio Serrano and Javier Bajo},
keywords = {Causal model, Bias mitigation, Fairness, Responsible artificial intelligence, Bayes},
abstract = {In the evolving field of Artificial Intelligence, concerns have arisen about the opacity of certain models and their potential biases. This study aims to improve fairness and explainability in AI decision making. Existing bias mitigation strategies are classified as pre-training, training, and post-training approaches. This paper proposes a novel technique to create a mitigated bias dataset. This is achieved using a mitigated causal model that adjusts cause-and-effect relationships and probabilities within a Bayesian network. Contributions of this work include (1) the introduction of a novel mitigation training algorithm for causal model; (2) a pioneering pretraining methodology for producing a fair dataset for Artificial Intelligence model training; (3) the diligent maintenance of sensitive features in the dataset, ensuring that these vital attributes are not overlooked during analysis and model training; (4) the enhancement of explainability and transparency around biases; and finally (5) the development of an interactive demonstration that vividly displays experimental results and provides the code for facilitating replication of the work.}
}
@article{PERNISCH2022100715,
title = {Visualising the effects of ontology changes and studying their understanding with ChImp},
journal = {Journal of Web Semantics},
volume = {74},
pages = {100715},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100715},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000117},
author = {Romana Pernisch and Daniele Dell’Aglio and Mirko Serbak and Rafael S. Gonçalves and Abraham Bernstein},
keywords = {Ontology editing, Materialisation, User study, Ontology evolution impact},
abstract = {Due to the Semantic Web’s decentralised nature, ontology engineers rarely know all applications that leverage their ontology. Consequently, they are unaware of the full extent of possible consequences that changes might cause to the ontology. Our goal is to lessen the gap between ontology engineers and users by investigating ontology engineers’ understanding of ontology changes’ impact at editing time. Hence, this paper introduces the Protégé plugin ChImp which we use to reach our goal. We elicited requirements for ChImp through a questionnaire with ontology engineers. We then developed ChImp according to these requirements and it displays all changes of a given session and provides selected information on said changes and their effects. For each change, it computes a number of metrics on both the ontology and its materialisation. It displays those metrics on both the originally loaded ontology at the beginning of the editing session and the current state to help ontology engineers understand the impact of their changes. We investigated the informativeness of materialisation impact measures, the meaning of severe impact, and also the usefulness of ChImp in an online user study with 36 ontology engineers. We asked the participants to solve two ontology engineering tasks – with and without ChImp (assigned in random order) – and answer in-depth questions about the applied changes as well as the materialisation impact measures. We found that ChImp increased the participants’ understanding of change effects and that they felt better informed. Answers also suggest that the proposed measures were useful and informative. We also learned that the participants consider different outcomes of changes severe, but most would define severity based on the amount of changes to the materialisation compared to its size. The participants also acknowledged the importance of quantifying the impact of changes and that the study will affect their approach of editing ontologies.}
}
@article{DAVID2023101223,
title = {Model consistency as a heuristic for eventual correctness},
journal = {Journal of Computer Languages},
volume = {76},
pages = {101223},
year = {2023},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2023.101223},
url = {https://www.sciencedirect.com/science/article/pii/S2590118423000333},
author = {Istvan David and Hans Vangheluwe and Eugene Syriani},
keywords = {Consistency, Correctness, Heuristics, Model consistency, Model-based systems engineering, Multi-view modeling},
abstract = {Inconsistencies between stakeholders’ views pose a severe challenge in the engineering of complex systems. The past decades have seen a vast number of sophisticated inconsistency management techniques being developed. These techniques build on the common idea of “managing consistency instead of removing inconsistency”, as put forward by Finkelstein. While it is clear what and how to do about inconsistencies, it is less clear why inconsistency is particularly useful. After all, it is the correctness of the system that should matter, as correctness is the end-user-facing quality of the product. In this paper, we analyze this question by investigating the relationship between (in)consistency and (in)correctness. We formally prove that, contrary to intuition, consistency does not imply correctness. However, consistency is still a good heuristic for eventual correctness. We elaborate on the consequences of this assertion and provide pointers as to how to make use of it in the next generation of inconsistency management techniques.}
}
@article{GHORBANI2023102452,
title = {Using type-2 fuzzy ontology to improve semantic interoperability for healthcare and diagnosis of depression},
journal = {Artificial Intelligence in Medicine},
volume = {135},
pages = {102452},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2022.102452},
url = {https://www.sciencedirect.com/science/article/pii/S0933365722002044},
author = {Abolfazl Ghorbani and Fatemeh Davoodi and Kamran Zamanifar},
keywords = {Health information interoperability, Type-2 fuzzy ontology, Semantic interoperability, Uncertainty, Major Depression Disorder (MDD)},
abstract = {Ontology enhances semantic interoperability through integrating health data from heterogeneous sources and sharing information in a meaningful way. In the field of smart health services, semantic interoperability means the exchange and interpretation of data without ambiguity and uncertainty. However, existing classical ontologies are not able to represent vague and uncertain knowledge, especially in contexts of mental health disorders which are associated with varying degrees of uncertainty and inaccuracy of diagnosis, and in this case, the treatment is a complex and common mental process necessitating to share information accurately and unambiguously. Type-2 fuzzy set theory can offer a fruitful solution in order to control uncertainty or express ambiguous concepts in a dynamic and complex environment such as healthcare systems. Herein, a semantic framework for healthcare, and also monitoring mental health disorders using type-2 fuzzy set theory based on the Internet of Thing (IoT) is suggested, in which all depression-related concepts are semantically annotated to share detailed information with the treatment staff. This framework not only paved the way to increasing the accuracy of medical diagnosis and decision-making but also provides the possibility of inference and semantic reasoning using the languages of SPARQL query and DL query.}
}
@article{BOTOEVA20191,
title = {Query inseparability for ALC ontologies},
journal = {Artificial Intelligence},
volume = {272},
pages = {1-51},
year = {2019},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370219300189},
author = {Elena Botoeva and Carsten Lutz and Vladislav Ryzhikov and Frank Wolter and Michael Zakharyaschev},
keywords = {Description logic, Knowledge base, Conjunctive query, Query inseparability, Computational complexity, Tree automaton},
abstract = {We investigate the problem whether two ALC ontologies are indistinguishable (or inseparable) by means of queries in a given signature, which is fundamental for ontology engineering tasks such as ontology versioning, modularisation, update, and forgetting. We consider both knowledge base (KB) and TBox inseparability. For KBs, we give model-theoretic criteria in terms of (finite partial) homomorphisms and products and prove that this problem is undecidable for conjunctive queries (CQs), but 2ExpTime-complete for unions of CQs (UCQs). The same results hold if (U)CQs are replaced by rooted (U)CQs, where every variable is connected to an answer variable. We also show that inseparability by CQs is still undecidable if one KB is given in the lightweight DL EL and if no restrictions are imposed on the signature of the CQs. We also consider the problem whether two ALC TBoxes give the same answers to any query over any ABox in a given signature and show that, for CQs, this problem is undecidable, too. We then develop model-theoretic criteria for HornALC TBoxes and show using tree automata that, in contrast, inseparability becomes decidable and 2ExpTime-complete, even ExpTime-complete when restricted to (unions of) rooted CQs.}
}
@article{PEREZPEREZ2021102131,
title = {A framework to extract biomedical knowledge from gluten-related tweets: The case of dietary concerns in digital era},
journal = {Artificial Intelligence in Medicine},
volume = {118},
pages = {102131},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102131},
url = {https://www.sciencedirect.com/science/article/pii/S093336572100124X},
author = {Martín Pérez-Pérez and Gilberto Igrejas and Florentino Fdez-Riverola and Anália Lourenço},
keywords = {Social media, Sociome profiling, Text mining, Graph mining, Machine learning, Health for informatics},
abstract = {Big data importance and potential are becoming more and more relevant nowadays, enhanced by the explosive growth of information volume that is being generated on the Internet in the last years. In this sense, many experts agree that social media networks are one of the internet areas with higher growth in recent years and one of the fields that are expected to have a more significant increment in the coming years. Similarly, social media sites are quickly becoming one of the most popular platforms to discuss health issues and exchange social support with others. In this context, this work presents a new methodology to process, classify, visualise and analyse the big data knowledge produced by the sociome on social media platforms. This work proposes a methodology that combines natural language processing techniques, ontology-based named entity recognition methods, machine learning algorithms and graph mining techniques to: (i) reduce the irrelevant messages by identifying and focusing the analysis only on individuals and patient experiences from the public discussion; (ii) reduce the lexical noise produced by the different ways in how users express themselves through the use of domain ontologies; (iii) infer the demographic data of the individuals through the combined analysis of textual, geographical and visual profile information; (iv) perform a community detection and evaluate the health topic study combining the semantic processing of the public discourse with knowledge graph representation techniques; and (v) gain information about the shared resources combining the social media statistics with the semantical analysis of the web contents. The practical relevance of the proposed methodology has been proven in the study of 1.1 million unique messages from >400,000 distinct users related to one of the most popular dietary fads that evolve into a multibillion-dollar industry, i.e., gluten-free food. Besides, this work analysed one of the least research fields studied on Twitter concerning public health (i.e., the allergies or immunology diseases as celiac disease), discovering a wide range of health-related conclusions.}
}
@article{SHIMIZU2024100823,
title = {Ontology design facilitating Wikibase integration — and a worked example for historical data},
journal = {Journal of Web Semantics},
volume = {82},
pages = {100823},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100823},
url = {https://www.sciencedirect.com/science/article/pii/S157082682400009X},
author = {Cogan Shimizu and Andrew Eells and Seila Gonzalez and Lu Zhou and Pascal Hitzler and Alicia Sheill and Catherine Foley and Dean Rehberger},
keywords = {Wikibase, Modular ontology modeling, Ontology design pattern},
abstract = {Wikibase – which is the software underlying Wikidata – is a powerful platform for knowledge graph creation and management. However, it has been developed with a crowd-sourced knowledge graph creation scenario in mind, which in particular means that it has not been designed for use case scenarios in which a tightly controlled high-quality schema, in the form of an ontology, is to be imposed, and indeed, independently developed ontologies do not necessarily map seamlessly to the Wikibase approach. In this paper, we provide the key ingredients needed in order to combine traditional ontology modeling with use of the Wikibase platform, namely a set of axiom patterns that bridge the paradigm gap, together with usage instructions and a worked example for historical data.}
}
@article{PEREIRA2023405,
title = {The Adoption of 4Step-Rule-Set Method for Ontological Design: Application in a Real Industrial Project},
journal = {Procedia Computer Science},
volume = {219},
pages = {405-415},
year = {2023},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN – International Conference on Project MANagement / HCist – International Conference on Health and Social Care Information Systems and Technologies 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.306},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923003150},
author = {Tiago F. Pereira and Francisco Morais and Carlos E. Salgado and Ana Lima and António Silva and Manuel Pereira and João Oliveira and Ricardo J. Machado},
keywords = {Ontology Building, Development Method, Agile Method, Semantic Interoperability, Graph Database},
abstract = {Ontology building can greatly influence the development cycle of an information system and enhance interoperability among its constituent elements. Throughout the projects we have been developing we have detected, by studying the current literature, a need to develop an agile method to conceive and mapping ontologies, which allows a quick and effective response to R&D projects. Designing a method for building an ontology, which is integrated and aligned with a systematic development approach, represents a crucial challenge in new approaches to system design and exploitation. Extant proposed methods for building an ontology, especially following agile approaches, have achieved interesting results but lack integration and alignment with a wider-view development framework. Thus, we have defined the first version of a semantic model allowing the alignment with the previously defined information model. Following the best practices for ontology building and based on our previous work on software system development, we now propose a method for designing an ontology, the 4SRS Method for Ontological Design based on the V-Model 4SRS, aligning it with a proven development method. We further demonstrate this approach by applying the proposed method in a real case, to develop an ontology for a choen restricted scope within the domain problem.}
}
@article{OS2021102751,
title = {Detection of malicious Android applications using Ontology-based intelligent model in mobile cloud environment},
journal = {Journal of Information Security and Applications},
volume = {58},
pages = {102751},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102751},
url = {https://www.sciencedirect.com/science/article/pii/S2214212621000041},
author = {Jannath Nisha O.S and Mary Saira Bhanu S},
keywords = {Mobile Cloud Computing, Malware and Benign apps, Ontology, Optimization algorithms, Machine Learning classifiers},
abstract = {Mobile Cloud Computing (MCC) is a computing model that makes mobile devices resourceful by executing mobile applications (apps) in the cloud and storing data in cloud servers. MCC faces several security threats in both the Cloud and Mobile environments. Among several threats, malicious apps are the most threatening ones, because they can perform various malicious activities in both environments. The traditional malware detection methods may not detect new types of malware or rapidly changing malware behavior. So, there is a need to develop an accurate model for detecting malicious apps in the MCC environment. Scalability and Knowledge Reusability are challenging issues in existing detection methods. To overcome these issues, the proposed model uses an effective Ontology-based intelligent model based on app permissions to detect malware apps. This model extracts the relationship between the static features from the apps and builds an Apps Feature Ontology (AFO). A concept vector set for apps is created using the items obtained from the AFO. The most discriminant features are selected using optimization algorithms like Particle Swarm Optimization, Social Spider Algorithm (SSA), and Gravitational Search Algorithm to reduce the dimension of the concept vector set. Various classifiers are applied to the reduced set. The efficiency of the proposed approach was evaluated on datasets obtained from the AndroZoo repository and VirusShare. The experimental results reveal that the proposed model can correctly detect malware using the Random Forest (RF) classifier with SSA and achieve higher detection accuracy with the lesser fall-out and less detection speed than existing Android malware detection techniques. Specifically, RF with SSA obtained higher accuracy, F1-score, and reduction in the fall-out of 94.11%, 93%, and 3%, respectively.}
}
@article{YUAN2024570,
title = {Analysis of international publication trends in artificial intelligence in skin cancer},
journal = {Clinics in Dermatology},
volume = {42},
number = {6},
pages = {570-584},
year = {2024},
issn = {0738-081X},
doi = {https://doi.org/10.1016/j.clindermatol.2024.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0738081X24001810},
author = {Lu Yuan and Kai Jin and An Shao and Jia Feng and Caiping Shi and Juan Ye and Andrzej Grzybowski},
abstract = {Bibliometric methods were used to analyze publications on the use of artificial intelligence (AI) in skin cancer from 2010 to 2022, aiming to explore current publication trends and future directions. A comprehensive search using four terms, “artificial intelligence,” “machine learning,” “deep learning,” and “skin cancer,” was performed in the Web of Science database for original English language publications on AI in skin cancer from 2010 to 2022. We visually analyzed publication, citation, and coupling information, focusing on authors, countries and regions, publishing journals, institutions, and core keywords. The analysis of 989 publications revealed a consistent year-on-year increase in publications from 2010 to 2022 (0.51% versus 33.57%). The United States, India, and China emerged as the leading contributors. IEEE Access was identified as the most prolific journal in this area. Key journals and influential authors were highlighted. Examination of the top 10 most cited publications highlights the significant potential of AI in oncology. Co-citation network analysis identified four primary categories of classical literature on AI in skin tumors. Keyword analysis indicated that "melanoma," "classification," and "deep learning" were the most prevalent keywords, suggesting that deep learning for melanoma diagnosis and grading is the current research focus. The term “pigmented skin lesions” showed the strongest burst and longest duration, whereas “texture” was the latest emerging keyword. AI represents a rapidly growing area of research in skin cancer with the potential to significantly improve skin cancer management. Future research will likely focus on machine learning and deep learning technologies for screening and diagnostic purposes.}
}
@article{BURGGRAF2024254,
title = {Paving the way for automated factory planning – applying rule-based expert systems to capacity planning},
journal = {Procedia CIRP},
volume = {126},
pages = {254-259},
year = {2024},
note = {17th CIRP Conference on Intelligent Computation in Manufacturing Engineering (CIRP ICME ‘23)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.08.335},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124009065},
author = {Peter Burggräf and Tobias Adlon and Niklas Schäfer},
keywords = {Factory Planning, Digital Factory, Planning Automation, Rule-Based Systems, Semantic Web},
abstract = {Today, numerous different software systems aid factory planners in their tasks. Nevertheless, due to their lacking interoperability and project-specific approaches, generic support for automated decision-making is still missing. Investigating the state of the art, we conclude that knowledge-based information modeling is needed for decision-making support. However, as the identified approaches of previous works propose no general automation concepts. Therefore, we define the guiding research question as how to model processual domain knowledge for automating factory planning processes. In this paper, we propose a planning assistance on rule-based expert systems. The planning assistance is composed of an ontology-based information model, a planning model consisting of individual planning functions, and a domain-specific inference engine. We implement the planning assistance with Semantic Web technologies and validate the solution using an application example from capacity planning. Thereby, we demonstrate the applicability of rule-based expert systems for automated factory planning. Finally, implications for future research are drawn for exploring further application areas and developing anticipated hybrid solution concepts.}
}
@article{SANFILIPPO2018174,
title = {Ontological foundations for feature-based modeling},
journal = {Procedia CIRP},
volume = {70},
pages = {174-179},
year = {2018},
note = {28th CIRP Design Conference 2018, 23-25 May 2018, Nantes, France},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118300994},
author = {Emilio M. Sanfilippo},
keywords = {Ontology, Feature-based modeling, Design, Manufacturing},
abstract = {Feature-based modeling is amongst the leading approaches for Computer Aided (CAx) product modeling. Its core benefit is the use of features to embed design intents into pure geometric product models in order to convey information based on experts’ knowledge and applications requirements. Despite various attempts, the very notion of feature remains ambiguous and no promising approach has been proposed to disambiguate and possibly unify its various meanings under a common framework. As a consequence, feature-based models are tuned on specific applications, are hardly reusable across systems, and are scarcely transparent for human comprehension. The purpose of this paper is to present an ontological characterization of features that can act as backbone conceptual and computational structure to represent the meaning of feature classes in a clear manner. For this goal, the ontology formalizes the most general and fundamental properties that all features are required to satisfy. The ontology is built on previous works and integrates the notion of feature within a broader framework for product knowledge representation.}
}
@article{BOGDANOVIC2023118958,
title = {Cross-portal metadata alignment – Connecting open data portals through means of formal concept analysis},
journal = {Information Sciences},
volume = {637},
pages = {118958},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.118958},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523005273},
author = {Miloš Bogdanović and Milena Frtunić Gligorijević and Nataša Veljković and Darko Puflović and Leonid Stoimenov},
keywords = {Open data, Metadata, Formal concept analysis, Semantic similarity, Natural language processing},
abstract = {Due to openness and transparency initiatives, a vast amount of data is being made publicly available. This data has great significance for business and society. However, it also led to challenges that need to be overcome for this data to reach its full potential. In this paper, we are focusing on the problem of connecting open data portals (ODPs) through metadata alignment. We investigate the available metadata accompanying datasets, especially the part related to categories datasets belong to and tags that closely describe datasets. The methodology we propose is relying on Formal Concept Analysis for the creation of the hierarchical structure used for determining the similarity of tags' usage in different ODPs. We propose such a structure to be used for open data portal metadata alignment. Further, we apply semantic similarity measures to reduce the complexity of the cross-portal data structure while preserving all its characteristics. We demonstrate how our approach can be used for determining dataset category across multiple ODPs aligned using the data structure our approach generates. We envision our approach to improve cross-portal search and metadata enrichment through open data categorization. Lastly, the quality of our approach was tested using datasets obtained from Canada’s and New Zealand’s ODPs.}
}
@article{MOKOS2020100030,
title = {A survey on the formalisation of system requirements and their validation},
journal = {Array},
volume = {7},
pages = {100030},
year = {2020},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2020.100030},
url = {https://www.sciencedirect.com/science/article/pii/S2590005620300151},
author = {Konstantinos Mokos and Panagiotis Katsaros},
keywords = {Requirement specification, Requirement formalisation, Semantic analysis, Model-based design, Component-based design, Formal verification},
abstract = {System requirements define conditions and capabilities to be met by a system under design. They are a partial definition in natural language, with inevitable ambiguities. Formalisation concerns with the transformation of requirements into a specification with unique interpretation, for resolving ambiguities, underspecified references and for assessing whether requirements are consistent, correct (i.e. valid for an acceptable solution) and attainable. Formalisation and validation of system requirements provides early evidence of adequate specification, for reducing the validation tests and high-cost corrective measures in the later system development phases. This article has the following contributions. First, we characterise the specification problem based on an ontology for some domain. Thus, requirements represent a particular system among many possible ones, and their specification takes the form of mapping their concepts to a semantic model of the system. Second, we analyse the state-of-the-art of pattern-based specification languages, which are used to avoid ambiguity. We then discuss the semantic analyses (missing requirements, inconsistencies etc.) supported in such a framework. Third, we survey related research on the derivation of formal properties from requirements, i.e. verifiable specifications that constrain the system’s structure and behaviour. Possible flaws in requirements may render the derived properties unsatisfiable or not realizable. Finally, this article discusses the important challenges for the current requirements analysis tools, towards being adopted in industrial-scale projects.}
}
@article{FALDUTI2024105999,
title = {Ontological models for representing image-based sexual abuses},
journal = {Computer Law & Security Review},
volume = {54},
pages = {105999},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.105999},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000669},
author = {Mattia Falduti and Cristine Griffo},
keywords = {Legal ontology, UFO-L, Image-based sexual abuse, Sextortion},
abstract = {In recent years, there has been extensive discourse on the moderation of abusive content online. Image-based Sexual Abuses (IBSAs) represent a type of abusive content that involves sexual images or videos. Platforms must moderate user-generated online content to tackle this issue effectively. One way to achieve this is by allowing users to report content, which can be flagged as abusive. In such instances, platforms may enforce their terms of service and prohibit certain types of content or users. Alongside these efforts, numerous countries have been making progress in defining and regulating this subject by implementing dedicated regulations. However, national solutions alone are insufficient for addressing a constantly increasing global emergency. Consequently, digital platforms create their own definitions of abusive conduct to overcome obstacles arising from conflicting national laws. In this paper, we use an ontological approach to model two types of abusive behavior. To do this, we applied the UFO-L patterns to build ontological models and based them on a top-level ontology, the Unified Foundational Ontology (UFO). The outcome is a set of ontological models that digital platforms can use to monitor and manage user compliance with the service provider’s code of conduct.}
}
@article{SIKOS201829,
title = {Representing network knowledge using provenance-aware formalisms for cyber-situational awareness},
journal = {Procedia Computer Science},
volume = {126},
pages = {29-38},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.206},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918311803},
author = {Leslie F. Sikos and Markus Stumptner and Wolfgang Mayer and Catherine Howard and Shaun Voigt and Dean Philp},
keywords = {Knowledge representation, ontology engineering, cybersecurity},
abstract = {Due to the volume, variety, and veracity of network data available, information fusion and reasoning techniques are needed to support network analysts’ cyber-situational awareness. These techniques rely on formal knowledge representation to define the network semantics with data provenance at various levels of granularity. To this end, this paper proposes the Communication Network Topology and Forwarding Ontology, a state-of-the-art ontology that enables the formal, unified representation of complex network concepts regardless of the type of the data source. The implementation of this ontology allows network analysts to represent expert knowledge and query network data fused from disparate data sources.}
}
@article{WICKETT20181175,
title = {A logic-based framework for collection/item metadata relationships},
journal = {Journal of Documentation},
volume = {74},
number = {6},
pages = {1175-1189},
year = {2018},
issn = {0022-0418},
doi = {https://doi.org/10.1108/JD-01-2018-0017},
url = {https://www.sciencedirect.com/science/article/pii/S0022041818000103},
author = {Karen Wickett},
keywords = {Semantics, Cataloguing, Metadata, Digital libraries, Linked data, Collections, Information modelling},
abstract = {Purpose
The purpose of this paper is to present a framework for the articulation of relationships between collection-level and item-level metadata as logical inference rules. The framework is intended to allow the systematic generation of relevant propagation rules and to enable the assessment of those rules for particular contexts and the translation of rules into algorithmic processes.
Design/methodology/approach
The framework was developed using first order predicate logic. Relationships between collection-level and item-level description are expressed as propagation rules – inference rules where the properties of one entity entail conclusions about another entity in virtue of a particular relationship those individuals bear to each other. Propagation rules for reasoning between the collection and item level are grouped together in the framework according to their logical form as determined by the nature of the propagation action and the attributes involved in the rule.
Findings
The primary findings are the analysis of relationships between collection-level and item-level metadata, and the framework of categories of propagation rules. In order to fully develop the framework, the paper includes an analysis of colloquial metadata records and the collection membership relation that provides a general method for the translation of metadata records into formal knowledge representation languages.
Originality/value
The method for formalizing metadata records described in the paper represents significant progress in the application of knowledge representation techniques to problems of metadata creation and management, providing a flexible technique for encoding colloquial metadata as a set of statements in first-order logic. The framework of rules for collection/item metadata relationships has a range of potential applications for the enhancement or metadata systems and vocabularies.}
}
@article{VANDAMME2022100731,
title = {The International Society for the Study of Vascular Anomalies (ISSVA) ontology},
journal = {Journal of Web Semantics},
volume = {74},
pages = {100731},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100731},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000221},
author = {Philip {van Damme} and Martijn G. Kersloot and Bruna {dos Santos Vieira} and Leo {Schultze Kool} and Ronald Cornet},
keywords = {ISSVA classification, Vascular anomalies, Vascular tumors, Vascular malformations, Ontology engineering},
abstract = {The International Society for the Study of Vascular Anomalies (ISSVA) provides a classification for vascular anomalies that enables specialists to unambiguously classify diagnoses. This classification is only available in PDF format and is not machine-readable, nor does it provide unique identifiers that allow for structured registration. In this paper, we describe the process of transforming the ISSVA classification into an ontology. We also describe the structure of this ontology, as well as two applications of the ontology using examples from the domain of rare disease research. We used the expertise of an ontology expert and clinician during the development process. We semi-automatically added mappings to relevant external ontologies using automated ontology matching systems and manual assessment by experts. The ISSVA ontology should contribute to making data for vascular anomaly research more Findable, Accessible, Interoperable, and Reusable (FAIR). The ontology is available at https://bioportal.bioontology.org/ontologies/ISSVA.}
}
@article{ZHENG2020309,
title = {A Quality-Oriented Digital Twin Modelling Method for Manufacturing Processes Based on A Multi-Agent Architecture},
journal = {Procedia Manufacturing},
volume = {51},
pages = {309-315},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.044},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920318990},
author = {Xiaochen Zheng and Foivos Psarommatis and Pierluigi Petrali and Claudio Turrin and Jinzhi Lu and Dimitris Kiritsis},
keywords = {Digital Twin, multi-agent system, quality control, manufacturing processes},
abstract = {The quality of a product is highly dependent on manufacturing processes. The recent development of industrial information technologies, such as Cyber-Physical Production Systems, Industrial Internet of Things, and Big Manufacturing Data Analytics has empowered the digitalization of manufacturing processes and promoted the concept of Digital Twin (DT). As one of the fundamental enabling technologies for Industry 4.0, DT enables the convergence between a physical system and its digital representation. DT modelling is the basis of implementing DT in practice. In this paper, we propose a DT modelling method based on a multi-agent architecture. It focuses on quality control during manufacturing processes and provides solutions to gather relevant information and analyze the corresponding influences on product quality. The MPFQ-model (Material, Production Process, Product Function/Future, Product Quality) is adopted to support the analysis of main influential factors related to the final product quality during the manufacturing phase. The five-dimension architecture is used as the basis for the DT models, including (i) physical entities, (ii) virtual models, (iii) DT data, (iv) services and (v) connections. Based on this architecture a Multi-Agent System (MAS) component and a semantic engineering component are integrated to create a quality-oriented DT framework.}
}
@article{HOCKER2020671,
title = {Participatory design for ontologies: a case study of an open science ontology for qualitative coding schemas},
journal = {Aslib Journal of Information Management},
volume = {72},
number = {4},
pages = {671-685},
year = {2020},
issn = {2050-3806},
doi = {https://doi.org/10.1108/AJIM-11-2019-0320},
url = {https://www.sciencedirect.com/science/article/pii/S2050380620000344},
author = {Julian Hocker and Christoph Schindler and Marc Rittberger},
keywords = {Ontology engineering, Participatory design, Digital humanities, Semantic web, Open science, Qualitative research, Coding schemas},
abstract = {Purpose
The open science movement calls for transparent and retraceable research processes. While infrastructures to support these practices in qualitative research are lacking, the design needs to consider different approaches and workflows. The paper bases on the definition of ontologies as shared conceptualizations of knowledge (Borst, 1999). The authors argue that participatory design is a good way to create these shared conceptualizations by giving domain experts and future users a voice in the design process via interviews, workshops and observations.
Design/methodology/approach
This paper presents a novel approach for creating ontologies in the field of open science using participatory design. As a case study the creation of an ontology for qualitative coding schemas is presented. Coding schemas are an important result of qualitative research, and reuse can yield great potential for open science making qualitative research more transparent, enhance sharing of coding schemas and teaching of qualitative methods. The participatory design process consisted of three parts: a requirement analysis using interviews and an observation, a design phase accompanied by interviews and an evaluation phase based on user tests as well as interviews.
Findings
The research showed several positive outcomes due to participatory design: higher commitment of users, mutual learning, high quality feedback and better quality of the ontology. However, there are two obstacles in this approach: First, contradictive answers by the interviewees, which needs to be balanced; second, this approach takes more time due to interview planning and analysis.
Practical implications
The implication of the paper is in the long run to decentralize the design of open science infrastructures and to involve parties affected on several levels.
Originality/value
In ontology design, several methods exist by using user-centered design or participatory design doing workshops. In this paper, the authors outline the potentials for participatory design using mainly interviews in creating an ontology for open science. The authors focus on close contact to researchers in order to build the ontology upon the expert's knowledge.}
}
@article{DRAGONI2020101840,
title = {Explainable AI meets persuasiveness: Translating reasoning results into behavioral change advice},
journal = {Artificial Intelligence in Medicine},
volume = {105},
pages = {101840},
year = {2020},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2020.101840},
url = {https://www.sciencedirect.com/science/article/pii/S0933365719310140},
author = {Mauro Dragoni and Ivan Donadello and Claudio Eccher},
keywords = {Explainable AI, Explainable reasoning, Natural Language Generation, MHealth, Ontologies},
abstract = {Explainable AI aims at building intelligent systems that are able to provide a clear, and human understandable, justification of their decisions. This holds for both rule-based and data-driven methods. In management of chronic diseases, the users of such systems are patients that follow strict dietary rules to manage such diseases. After receiving the input of the intake food, the system performs reasoning to understand whether the users follow an unhealthy behavior. Successively, the system has to communicate the results in a clear and effective way, that is, the output message has to persuade users to follow the right dietary rules. In this paper, we address the main challenges to build such systems: (i) the Natural Language Generation of messages that explain the reasoner inconsistency; and, (ii) the effectiveness of such messages at persuading the users. Results prove that the persuasive explanations are able to reduce the unhealthy users’ behaviors.}
}
@article{PETRUCCI201866,
title = {Expressive ontology learning as neural machine translation},
journal = {Journal of Web Semantics},
volume = {52-53},
pages = {66-82},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300507},
author = {Giulio Petrucci and Marco Rospocher and Chiara Ghidini},
keywords = {Ontology learning, Neural networks, Natural language processing},
abstract = {Automated ontology learning from unstructured textual sources has been proposed in literature as a way to support the difficult and time-consuming task of knowledge modeling for semantic applications. In this paper we propose a system, based on a neural network in the encoder–decoder configuration, to translate natural language definitions into Description Logics formulæ through syntactic transformation. The model has been evaluated to assess its capacity to generalize over different syntactic structures, tolerate unknown words, and improve its performance by enriching the training set with new annotated examples. The results obtained in our evaluation show how approaching the ontology learning problem as a neural machine translation task can be a valid way to tackle long term expressive ontology learning challenges such as language variability, domain independence, and high engineering costs.}
}
@article{DAUSCH20241364,
title = {Semantic Integration and Interdisciplinary Collaboration in Production Planning: A Graph-Based Approach for Enhanced Data Consistency},
journal = {Procedia CIRP},
volume = {130},
pages = {1364-1371},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.253},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124014100},
author = {Valesko Dausch and Joachim Lentes and Oliver Riedel and Matthias Kreimeyer},
keywords = {Data driven development, knowledge management, data continuity, method},
abstract = {Based on increasing individualization, shorter innovation cycles and integrating services into products complexity in the development of products and production processes increases. In combination with digitalization, this results in an increased need for interdisciplinary work. A high level of data consistency is required to cope with the resulting diverse and complex data structures, leading to the need for a consistent data model that orchestrates information flows. PLM-systems fall short in their ambition of reaching an overarching data management across the product lifecycle. This results in a lot of administrative work in planning instead of value-adding as well as in inconsistencies and incomplete change processes. To improve interdisciplinary collaboration in production planning, goal of this work is to develop a method leading to a comprehensive data model and to show it’s applicability by means of a case study. The steps of the research performed follow the typical circle of action research: diagnosis, planning, action, evaluation, and reflection. The data model resulting of the application of the method uses semantics to make relationships understandable for both humans and machines. This is necessary for downstream automation and the use of artificial intelligence. The three essential steps of the method are, firstly, an as-is modeling of the processes of interest. Then, the process models are used to evaluate the consistency and efficiency of the planning processes. The method is completed by a guideline for setting up the data model. The proposed approach aims to improve efficiency and quality in assembly planning processes, resulting in its importance for industrial companies.}
}
@article{OYEDEJI2024233,
title = {Leveraging Ontology Development to Enhance Corrosion Visualisation in Engineering Design},
journal = {Procedia CIRP},
volume = {128},
pages = {233-238},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S221282712400667X},
author = {Oluseyi Ayodeji Oyedeji and Samir Khan and John Ahmet Erkoyuncu},
keywords = {Engineering Design, Ontology, Corrosion Detection, Automation, Visualisation},
abstract = {Designing automated solutions for understanding, visualising, and detecting damage has become a key area in the era of digitalisation and advanced engineering. The use of deep learning and other artificial intelligence approaches has proven very innovative and rendered manual methods of damage inspection primitive. However, there remains a critical gap in creating a unified knowledge base that helps in understanding, conceptualising, and promoting collaborative engineering design, particularly in the context of automated corrosion detection through images. This research addresses this gap by presenting a conceptualised model for understanding corrosion detection through ontology development. This is implemented in an ontology development environment using Protege 5.5.0 and ELK 0.5.0 reasoner. Evaluation is done using expert competency questions and hypothetical scenarios thereby establishing a robust framework that is beneficial to Engineering design in terms of terminology standardisation, design process facilitation, and building corrosion inspection systems that are interoperable. The Ontology also enables sharing and reusing knowledge between automated corrosion detection systems as well as integration with existing industrial standards such as Industrial Ontology Foundry (IOF) and semantic web standards. Hence, offering a significant contribution to the digitalisation of engineering design. Besides the enhancement of damage detection, this work also advances the engineering field to utilise visual data more effectively in design, maintenance, and product lifecycle management.}
}
@article{IQBAL201873,
title = {A mathematical evaluation for measuring correctness of domain ontologies using concept maps},
journal = {Measurement},
volume = {118},
pages = {73-82},
year = {2018},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2018.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0263224118300083},
author = {Rizwan Iqbal and Masrah Azrifah {Azmi Murad} and Layth Sliman and Clay Palmeira {da Silva}},
keywords = {Ontology engineering, Concept mapping, Ontology evaluation, Closeness index, Similarity index},
abstract = {There is a need for further research in the area of ontology evaluation specifically dealing with ontology development exploiting concept maps. The existing literature on ontology evaluation primarily emphasis on ontology formalisation as well as on performing logical inferences, which is usually not directly relevant for concept maps as they are commonly exploited as communication instruments for learning purposes. Commonly used techniques for evaluating concept maps for knowledge assessment may be adopted for a kind of criteria-based evaluation of a domain concept map with respect to a particular aspect. However, this makes its validity limited to a particular aspect or criteria. This paper presents a mathematical ontology evaluation technique to measure the correctness of domain ontologies engineered using concept maps. It is based on the notion of merging two different mathematical measures, namely closeness index and similarity index to come up with a combined index that takes different criteria or aspects into account while performing ontology evaluation. Therefore, the proposed technique makes the evaluation process more reliable and robust. Two case studies were conducted employing the proposed technique for evaluating two different domain ontologies that were engineered using concept maps. Calculations and results from the case studies showed that depending on the correctness of individual ontology, different values of combined Index was calculated manifesting the measure of correctness of each individual ontology in a quantifiable form. Moreover, the results depict that the technique provides in-depth evaluation, it is easy to adopt, requires no special skills, and is conveniently replicable.}
}
@article{ALOBAIDI2018117,
title = {Automated ontology generation framework powered by linked biomedical ontologies for disease-drug domain},
journal = {Computer Methods and Programs in Biomedicine},
volume = {165},
pages = {117-128},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169260717315791},
author = {Mazen Alobaidi and Khalid Mahmood Malik and Maqbool Hussain},
keywords = {Semantic web, Ontology generation, Linked biomedical ontologies},
abstract = {Objective and background: The exponential growth of the unstructured data available in biomedical literature, and Electronic Health Record (EHR), requires powerful novel technologies and architectures to unlock the information hidden in the unstructured data. The success of smart healthcare applications such as clinical decision support systems, disease diagnosis systems, and healthcare management systems depends on knowledge that is understandable by machines to interpret and infer new knowledge from it. In this regard, ontological data models are expected to play a vital role to organize, integrate, and make informative inferences with the knowledge implicit in that unstructured data and represent the resultant knowledge in a form that machines can understand. However, constructing such models is challenging because they demand intensive labor, domain experts, and ontology engineers. Such requirements impose a limit on the scale or scope of ontological data models. We present a framework that will allow mitigating the time-intensity to build ontologies and achieve machine interoperability. Methods: Empowered by linked biomedical ontologies, our proposed novel Automated Ontology Generation Framework consists of five major modules: a) Text Processing using compute on demand approach. b) Medical Semantic Annotation using N-Gram, ontology linking and classification algorithms, c) Relation Extraction using graph method and Syntactic Patterns, d), Semantic Enrichment using RDF mining, e) Domain Inference Engine to build the formal ontology. Results: Quantitative evaluations show 84.78% recall, 53.35% precision, and 67.70% F-measure in terms of disease-drug concepts identification; 85.51% recall, 69.61% precision, and F-measure 76.74% with respect to taxonomic relation extraction; and 77.20% recall, 40.10% precision, and F-measure 52.78% with respect to biomedical non-taxonomic relation extraction. Conclusion: We present an automated ontology generation framework that is empowered by Linked Biomedical Ontologies. This framework integrates various natural language processing, semantic enrichment, syntactic pattern, and graph algorithm based techniques. Moreover, it shows that using Linked Biomedical Ontologies enables a promising solution to the problem of automating the process of disease-drug ontology generation.}
}
@article{BENITEZANDRADES2020390,
title = {An ontology-based multi-domain model in social network analysis: Experimental validation and case study},
journal = {Information Sciences},
volume = {540},
pages = {390-413},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520305909},
author = {José Alberto Benítez-Andrades and Isaías García-Rodríguez and Carmen Benavides and Héctor Alaiz-Moretón and José Emilio {Labra Gayo}},
keywords = {Ontology-based systems, Semantic web, Semantic technologies, Social network analysis, Ontology multi-domain, Knowledge-based systems},
abstract = {The use of social network theory and methods of analysis have been applied to different domains in recent years, including public health. The complete procedure for carrying out a social network analysis (SNA) is a time-consuming task that entails a series of steps in which the expert in social network analysis could make mistakes. This research presents a multi-domain knowledge model capable of automatically gathering data and carrying out different social network analyses in different domains, without errors and obtaining the same conclusions that an expert in SNA would obtain. The model is represented in an ontology called OntoSNAQA, which is made up of classes, properties and rules representing the domains of People, Questionnaires and Social Network Analysis. Besides the ontology itself, different rules are represented by SWRL and SPARQL queries. A Knowledge Based System was created using OntoSNAQA and applied to a real case study in order to show the advantages of the approach. Finally, the results of an SNA analysis obtained through the model were compared to those obtained from some of the most widely used SNA applications: UCINET, Pajek, Cytoscape and Gephi, to test and confirm the validity of the model.}
}
@article{TIMAKUM2025102462,
title = {Four decades of data & knowledge engineering: A bibliometric analysis and topic evolution study (1985–2024)},
journal = {Data & Knowledge Engineering},
volume = {159},
pages = {102462},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102462},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000576},
author = {Tatsawan Timakum and Soobin Lee and Dongha Kim and Min Song and Il-Yeol Song},
keywords = {Data and knowledge engineering journal, Bibliometric analysis, Citation analysis, Topic evolution analysis, Authorship network analysis},
abstract = {The Data and Knowledge Engineering (DKE) journal has established a significant global research presence over four decades, substantially contributing to the advancement of data and knowledge engineering disciplines. This comprehensive bibliometric study analyzes the journal’s publications over the past 40 years (1985–2024), employing bibliographic records and citation data from Scopus, Web of Science (WoS), and ScienceDirect. By utilizing CiteSpace for citation and co-citation mapping and Dirichlet Multinomial Regression (DMR) topic modeling for trend analysis, the research provides a multifaceted examination of the journal’s scholarly landscape. Over its 40-year history, DKE has published 1951 articles, accumulating 53,594 citations. The study comprehensively explores key bibliometric dimensions, including influential authors, author networks, citation patterns, topic clusters, institutional contributions, and research funding sponsors, as well as evolution of topics, showing increasing, decreasing, or constant trends. Comprehensive analysis offers a meta-analytical perspective on DKE’s scholarly contributions, positioning the journal as a pioneering publication platform that advances critical knowledge and methodological innovations in data and knowledge engineering research domains. Through an in-depth examination of the journal’s publication trajectory, the study provides insights into the field’s scholarly evolution, highlighting DKE’s pivotal role in shaping academic discourse and technological understanding.}
}
@article{JAZIRI20211152,
title = {ORVIPO: An Ontological Prototype for Modeling 3D Scenes in Operating Rooms},
journal = {Procedia Computer Science},
volume = {192},
pages = {1152-1161},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.118},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921016070},
author = {Faouzi Jaziri and Rim Messaoudi and Achraf Mtibaa and Jonathan Courbon and Mahdi Kilani and Mohamed Mhiri and Antoine Vacavant},
keywords = {Virtual reality, Ontology, Semantic interoperability, Reasoning rules, Interaction},
abstract = {Virtual Operating room characterizes a large artificial environment of interaction in the medical context. It enables a better simulation for different surgical scenarios through 3D (three‐dimensional) objects. Virtual Reality (VR) uses computer technology to develop these virtual simulated applications. It brings valuable innovations in different fields. It is integrated also to enhance healthcare and therapies. To improve and assist on medical VR applications, ontologies would be useful. They can unify the content of these applications and facilitate their implementation via semantic rules. Also, ontologies can be applied to realize an effective VR knowledge modeling. This paper presents a virtual simulation of an operating room taking advantage of the semantic representation. The goal of this study is to propose a novel ontological VR system that models an operating room and its components. The execution of the proposed method was proved by the developed ontology OROnto (Operating Room Ontology). This ontology was useful for modeling hospital scenarios and the construction of a valuable VR system. To demonstrate the proposed approach feasibility and performance, we have implemented the Operating Room Virtual Integration Process Using Ontology (ORVIPO) prototype. Compared to different other ontological methods and related works, our approach have shown interesting findings such as recall (71%), precision (83%), and F-measure (76%).}
}
@article{YUAN2025103905,
title = {Research on the construction and mapping model of knowledge organization system driven by standards},
journal = {Computer Standards & Interfaces},
volume = {92},
pages = {103905},
year = {2025},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2024.103905},
url = {https://www.sciencedirect.com/science/article/pii/S0920548924000746},
author = {Jingshu Yuan and Kexin Zhai and Hongxin Li and Man Yuan},
keywords = {Knowledge organization system, Multidisciplinary theory, Concept, Metadata, semantic, Standardization, Ontology},
abstract = {With the rapid development of artificial intelligence and enterprise digital transformation, the standardization organization, storage and management of semantic knowledge in computers have become the current research focus. As the core theory of knowledge system construction, knowledge organization (KO) provides theoretical support for the study of semantic knowledge organization and representation, among which knowledge organization system (KOS) is the important tool of semantic organization. At present, many scholars have carried out research from different perspectives of KOS based on theory, which provides the direction for the sustainable development of KOS. However, most of these studies focus on some aspects of KOS, which are in a "scattered" state, lacking systematic analysis of the basic principles of KOS construction and semantic organization based on theories and international standards. Therefore, this paper firstly constructs KOS theoretical models in the conceptual world and computer world respectively through a comprehensive study of multi-disciplinary basic theories such as semantics, logic, system theory, and international standards such as ISO 1087:2019, ISO 25964:2013, and ISO 11179:2023, and traces the iterative construction, organization and mapping process from "concept" in the conceptual world to "metadata" knowledge and semantics in the computer world. The semantic organization based on metadata is realized in computer. Secondly, on this basis, in order to realize ontology representation of domain knowledge, the ontology construction method based on MDR metadata is proposed. Finally, taking the semantic organization and ontology construction of Epicentre model in petroleum field as an example, the feasibility of the ideas and methods proposed in this paper is verified. The model and method proposed in this paper is independent of the specific type of KOS, so it is innovative and universal. The methodology is also applicable to other fields of conceptual system modeling, metadata standard construction, and data model modeling.}
}
@article{GAWICH2019341,
title = {Ontology Maintenance System for Rheumatoid Disease},
journal = {Procedia Computer Science},
volume = {154},
pages = {341-346},
year = {2019},
note = {Proceedings of the 9th International Conference of Information and Communication Technology [ICICT-2019] Nanning, Guangxi, China January 11-13, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.06.049},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930818X},
author = {Mariam Gawich and Marco Alfonse and Mostafa Aref and Abdel-Badeeh M. Salem},
keywords = {Ontology engineering, Ontology maintenance, Ontology evolution, Ontology pruning, Medical Ontology},
abstract = {The constructed medical ontologies need to be updated in order to reflect the changes occurred on the medicine such as the clinical findings, treatments and their side effects. Various researchers defined the ontology maintenance as the process of updating the ontology or the evolution of the ontology. Other researches consider the ontology maintenance as a composed process that involves the ontology evolution and pruning. This paper presents a Rheumatoid ontology maintenance system that incorporates both of the ontology evolution and the ontology pruning. The evolution is executed in a way that ensures the ontology consistency and its relevance to the domain of interest.}
}
@article{SHI2023102114,
title = {An ontology-based methodology to establish city information model of digital twin city by merging BIM, GIS and IoT},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102114},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102114},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002422},
author = {Jianyong Shi and Zeyu Pan and Liu Jiang and Xiaohui Zhai},
keywords = {CIM, Ontology, Linked data, Semantic integration, BIM-GIS-IoT integration},
abstract = {With the development of digital city and smart city construction, the City Information Model (CIM) has played a critical role as a container of spatial–temporal data to establish the Digital Twin City. For a digital twin city, a virtual high-fidelity CIM model that corresponds closely to the real physical world is the premise and cornerstone of its construction. Therefore, the integration of BIM, GIS and IoT has become the preferred topic for researchers and has received much more attention from a wide academic circle. However, traditional integration mainly focuses on the conversion of both IFC and CityGML, and IoT data are also often used as visualizations. More importantly, the underlying data formats of GIS, BIM and IoT are still independent of each other without a unified data structure expression, so real data-driven analysis and decision-making cannot be implemented. This study aims to establish a general CIM ontology to integrate heterogeneous BIM, GIS and IoT data. First, the related work of BIM, GIS and IoT integration is studied and analyzed. A comparison of three mainstream approaches, data conversion, standard extension and data linking, is conducted, and it illustrates the advantages of ontology techniques in solving data interoperability problems. Second, a technical framework of BIM, GIS and IoT data integration based on ontology technology is proposed. The approach is mainly divided into five steps: geometry processing, data instantiation, ontology construction, ontology mapping and querying application. On the basis of the CIM ontology, an application ontology is built for a specific application domain to illustrate rule-based mapping, querying and inferring. Finally, the case study shows that the Ontology-based methodology in this paper has contributed to establish a general pattern for CIM data integration by mapping and linking concepts from the semantic level. It avoids changes in the original data sources and the missing data problem.}
}
@article{VALIENTE2024100830,
title = {Web3-DAO: An ontology for decentralized autonomous organizations},
journal = {Journal of Web Semantics},
volume = {82},
pages = {100830},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100830},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000167},
author = {María-Cruz Valiente and Juan Pavón},
keywords = {Blockchain, Web3, DAO, Decentralized autonomous organization, Digital governance, E-government, Online entity, Ontology, Voting system},
abstract = {Decentralized autonomous organizations (DAOs) are relatively a newly emerging type of online entity related to governance or business models where all their members work together and participate in the decision-making processes affecting the DAO in a decentralized, collective, fair, and democratic manner. In a DAO, members interaction is mediated by software agents running on a blockchain that encode the governance of the specific entity in terms of rules that optimize their business and goals. In this context, most popular DAO software frameworks provide decision-making models aiming to facilitate digital governance and the collaboration among their members intertwining social and economic concerns. However, these models are complex, not interoperable among them and lack a common understanding and shared knowledge concerning DAOs, as well as the computational semantics needed to enable automated validation, simulation or execution. Thus, this paper presents an ontology (Web3-DAO), which can support machine-readable digital governance of DAOs adding semantics to their decision-making models. The proposed ontology captures the domain logic that allows the sharing of updated information and decisions for all the members that interact with a DAO by the interoperability of their own assessment and decision tools. Furthermore, the ontology detects semantic ambiguities, uncertainties and contradictions. The Web3-DAO ontology is available in open access at https://github.com/Grasia/semantic-web3-dao.}
}
@article{HASHEMI201828,
title = {Developing a domain ontology for knowledge management technologies},
journal = {Online Information Review},
volume = {42},
number = {1},
pages = {28-44},
year = {2018},
issn = {1468-4527},
doi = {https://doi.org/10.1108/OIR-07-2016-0177},
url = {https://www.sciencedirect.com/science/article/pii/S1468452718000033},
author = {Parvin Hashemi and Ameneh Khadivar and Mehdi Shamizanjani},
keywords = {Ontology, Knowledge management processes, Knowledge management strategies, Growth stage for knowledge management technology, Knowledge management technologies},
abstract = {Purpose
The purpose of this paper is to develop a new ontology for knowledge management (KM) technologies, determining the relationships between these technologies and classification of them.
Design/methodology/approach
The study applies NOY methodology – named after Natalya F. Noy who initiated this methodology. Protégé software and ontology web language are used for building the ontology. The presented ontology is evaluated with abbreviation and consistency criteria and knowledge retrieval of KM technologies by experts.
Findings
All the main concepts in the scope of KM technologies are extracted from existing literature. There are 241 words, 49 out of them are domain concepts, eight terms are about taxonomic and non-taxonomic relations, one term relates to data property and 183 terms are instances. These terms are used to develop KM technologies’ ontology based on three factors: facilitating KM processes, supporting KM strategies and the position of technology in the KM technology stage model. The presented ontology is created a common understanding in the field of KM technologies.
Research limitations/implications
Lack of specific documentary about logic behind decision making and prioritizing criteria in choosing KM technologies.
Practical implications
Uploading the presented ontology in the web environment provides a platform for knowledge sharing between experts from around the world. In addition, it helps to decide on the choice of KM technologies based on KM processes and KM strategy.
Originality/value
Among the many categories of KM technologies in literature, there is no classifying according to several criteria simultaneously. This paper contributes to filling this gap and considers KM processes, KM strategy and stages of growth for KM technologies simultaneously to choice the KM technologies and also there exists no formal ontology regarding KM technologies. This study has tried to propose a formal KM technologies’ ontology.}
}
@article{RODLER2022108987,
title = {One step at a time: An efficient approach to query-based ontology debugging},
journal = {Knowledge-Based Systems},
volume = {251},
pages = {108987},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108987},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122004786},
author = {Patrick Rodler},
keywords = {Ontology debugging, Query-based ontology debugging, Interactive debugging, Fault localization, Sequential diagnosis, Expert questions, Ontology quality assurance, Ontology repair, Test-driven debugging, Singleton query, Model-based diagnosis, Semantic Web, User interaction, Effort minimization, Performance optimization, Entailment, Test cases, Query computation, Query selection, One-step lookahead, Debugging cost criteria, Query quality assessment, Expert types, Heuristics, Knowledge-base debugging, OntoDebug, Protégé},
abstract = {When ontologies reach a certain size and complexity, faults such as inconsistencies, unsatisfiable classes or wrong entailments are hardly avoidable. Locating the incorrect axioms that cause these faults is a hard and time-consuming task. Addressing this issue, several techniques for a semi-automatic fault localization in ontologies have been proposed and extensively studied. One class of these approaches involve a human expert who provides answers to system-generated queries about the intended (correct) ontology in order to reduce the possible fault locations. To suggest as informative questions as possible, existing methods draw on various algorithmic optimizations as well as heuristics. However, these computations are often based on certain assumptions about the interacting expert. In this work, we demonstrate that these assumptions might not always be adequate and discuss consequences of their violations. In particular, we characterize a range of expert types with different query answering behavior and show that existing approaches are far from achieving optimal efficiency for all of them. In addition, we find that the cost metric adopted by state-of-the-art techniques might not always be realistic and that a change of metric has a decisive impact on the best choice of query answering strategy. As a remedy, we suggest a new – and simpler – type of expert question that leads to a stable fault localization performance for all analyzed expert types and effort metrics, and has numerous further advantages over existing techniques. Moreover, we present an algorithm which computes and optimizes this new query type in worst-case polynomial time and which is fully compatible with existing concepts (e.g., query selection heuristics) and infrastructure (e.g., debugging user interfaces) in the field. Comprehensive experiments on faulty real-world ontologies attest that the new querying method is substantially and statistically significantly superior to existing techniques both in terms of the number of necessary expert interactions and in terms of the query computation time. We find that relying on the new querying method can save an interacting expert more than 80% of their work, and can reduce the expert’s waiting time for the next query by more than three orders of magnitude. Beside these findings, we demonstrate that the efficiency of existing query-based tools can be significantly boosted by suggesting an appropriate query answering strategy to an expert; we also make recommendations in this regard. Further, we suggest optimal configurations of a debugger for situations where the new type of query is used. Remarkably, the proposed approach is not only applicable to ontologies, but to any monotonic knowledge representation language, and can even be adopted to solve general model-based diagnosis problems expressible using Reiter’s theory.}
}
@article{VOLPERT20242994,
title = {Compatibility Assessment for Interfaces in Drivetrains of Robot-Like Systems},
journal = {Procedia Computer Science},
volume = {232},
pages = {2994-3002},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.115},
url = {https://www.sciencedirect.com/science/article/pii/S187705092400293X},
author = {Marcus Volpert and Birgit Vogel-Heuser and Dominik Hujo and Karsten Stahl and Markus Zimmermann},
keywords = {robot-like systems, interface modeling, ontology, model-based systems engineering, drivetrains},
abstract = {Due to the complexity of robot-like systems (RLS), new developments are often avoided, and most variants of the systems are designed. The primary determinant of an RLS is its drivetrain, which comprises purchased components, namely the controller, motor driver, motor, and gearbox. Each of these purchased parts has different interfaces that are not standardized. If the interfaces are sufficiently complex, the compatibility of the parts can no longer be guaranteed manually. Therefore, this paper presents a draft ontology that verifies the compatibility of purchased parts in RLS drivetrains. Classes and properties are obtained from expert knowledge, norms, data standards, and data sheets to build an ontology. The ontology design is evaluated using an application example for a motor-gearbox interface.}
}
@article{KUDRYAVTSEV2020500,
title = {Modelling Consumer Knowledge: the Role of Ontology},
journal = {Procedia Computer Science},
volume = {176},
pages = {500-507},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.08.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920318767},
author = {D. Kudryavtsev and T. Gavrilova and M. Smirnova and K. Golovacheva},
keywords = {consumer knowledge, consumer behaviour, knowledge management, innovative products, services, knowledge economy, ontology},
abstract = {Knowledge economy and further development of the information society made knowledge of all market interaction participants a key factor of consumption, adding value, including joint generation of value and innovation. Alongside with general increase in information volumes and decrease in consumer trust to it, the very products and services, as well as consumption technology and culture have become more complex and, thus, demonstrate relevance of managing consumer knowledge. Such complexity requires to teach consumers and to exchange knowledge with them. Consumer knowledge is of paramount importance for innovative products and services, as it is a key factor of innovation-decision process. Consumer knowledge practice needs clear understanding of this concept ("consumer knowledge"), its kinds and features, processes of acquiring and changing this knowledge, its influence on consumer behaviour, as well as company’s capabilities to establish consumer knowledge. Such understanding will be provided by creating ontology of innovative products and services’ consumer knowledge. Such ontology will help to resolve a whole range of enterprise engineering tasks: design of innovative products and services, as well as ecosystem surrounding them; design of an interaction system between a company and a consumer during a whole customer journey. This paper describes main requirements on ontology, discusses some existing ontologies, as well as contains primary results of ontology conceptualisation.}
}
@article{YANG2020104437,
title = {Construction of logistics financial security risk ontology model based on risk association and machine learning},
journal = {Safety Science},
volume = {123},
pages = {104437},
year = {2020},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2019.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0925753519313050},
author = {Bo Yang},
keywords = {Logistics finance risk, Ontology, Semantic parsing, Apriori, Risk association},
abstract = {Previous research on logistics financial risk pre-warning and pre-control focuses on the linear causal relationship between risk and risk events. In fact, risk events in logistics financial field are often caused by multiple risk factors, which are directly or indirectly related to these risk factors. Therefore, it is helpful for the healthy development of logistics finance to find out the related risks of each logistics financial risk event and screen and control them one by one. This paper proposes OntoLFR (Logistics Financial Risk Ontology), and constructs the logistics financial risk ontology model to adapt to the variability, complexity and relevance of risk in early warning and pre-control. Then, based on the risk source association inference rules obtained by knowledge association analysis, Apriori algorithm is adopted to conduct association analysis on the risk hidden danger database, and the acquired association rules are reintroduced into the knowledge ontology database of risk event source to realize self-learning and self-correction of the knowledge ontology database. Taking the risk event (RW_risk) of the financing enterprise to escape, the feasibility of using the logistics financial risk ontology model for risk-related reasoning and analysis is verified.}
}
@article{FENG2022474,
title = {Computing Sufficient and Necessary Conditions in CTL: A Forgetting Approach},
journal = {Information Sciences},
volume = {616},
pages = {474-504},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.10.124},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522012518},
author = {Renyan Feng and Erman Acar and Yisong Wang and Wanwei Liu and Stefan Schlobach and Weiping Ding},
keywords = {Computation tree logic, Forgetting, Weakest sufficient condition, Model checking},
abstract = {Computation tree logic (CTL) is an essential specification language in the field of formal verification. In systems design and verification, it is often important to update existing knowledge with new attributes and subtract the irrelevant content while preserving the given properties on a known set of atoms. Under the scenario, given a specification, the weakest sufficient condition (WSC) and the strongest necessary condition (SNC) are dual concepts and very informative in formal verification. In this article, we generalize our previous results (i.e., the decomposition, homogeneity properties, and the representation theorem) on forgetting in bounded CTLto the unbounded one. The cost we pay is that, unlike the bounded case, the result of forgetting in CTLmay no longer exist. However, SNC and WSC can be obtained by the new forgetting machinery we are presenting. Furthermore, we complement our model-theoretic approach with a resolution-based method to compute forgetting results in CTL. This method is currently the only way to compute forgetting results for CTLand temporal logic. The method always terminates and is sound. That way, we set up the resolution-based approach for computing WSC and SNC in CTL.}
}
@article{MCGLINN2021103534,
title = {Publishing authoritative geospatial data to support interlinking of building information models},
journal = {Automation in Construction},
volume = {124},
pages = {103534},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103534},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520311146},
author = {Kris McGlinn and Rob Brennan and Christophe Debruyne and Alan Meehan and Lorraine McNerney and Eamonn Clinton and Philip Kelly and Declan O'Sullivan},
keywords = {Building Information Modelling, Geographic Information Systems, Ontology Engineering, Resource Description Framework (RDF), Linked Data},
abstract = {Building Information Modelling (BIM) is a key enabler to support integration of building data within the buildings life cycle (BLC) and is an important aspect to support a wide range of use cases, related to intelligent automation, navigation, energy efficiency, sustainability and so forth. Open building data faces several challenges related to standardization, data interdependency, data access, and security. In addition to these technical challenges, there remains the barrier among BIM developers who wish to protect their intellectual property, as full 3D BIM development requires expertise and effort. This means that there is often limited availability of building data. However, a Linked Data approach to BIM, combined with a supporting national geospatial identifier infrastructure makes interlinking and controlled sharing of BIM models possible. In Ireland, the Ordnance Survey Ireland (OSi) maintains a substantial data set, called Prime2, which includes not only building GIS data (polygon footprint, geodetic coordinate), but also additional building specific data (e.g. form, function and status). The data set also includes change information, recording when changes took place and who captured and validated those changes. This paper presents the development of a national geospatial identifier infrastructure based on an OSi building ontology that supports capturing OSi building data using Resource Description Framework (RDF). The paper details the different steps required to generate the ontology and publish the data. First, an initial analysis of the data set to generate the ontology is discussed. This includes identification of mappings to existing standards, e.g. GeoSPARQL to handle geometries and PROV-O to handle provenance, to the development of R2RML mappings to generate the RDF and the method for deploying the ontology and the building graphs. This data is then made available dependent on different licensing agreements handled by an access control approach. Methods are then presented to support the interlinking of the authoritative data with other building data standards and data sets using geolocation, followed finally by discussion and future work.}
}
@article{DIAMANTINI2023224,
title = {Process-aware IIoT Knowledge Graph: A semantic model for Industrial IoT integration and analytics},
journal = {Future Generation Computer Systems},
volume = {139},
pages = {224-238},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2200320X},
author = {Claudia Diamantini and Alex Mircoli and Domenico Potena and Emanuele Storti},
keywords = {Industrial Internet of Things, Data integration, Business process, Semantics, Ontology, Knowledge Graph},
abstract = {The integration of the huge data streams produced by the Industrial Internet of Things (IIoT) can provide invaluable knowledge in the context of Industry 4.0, and is also an open research issue. The present paper proposes a semantic approach to this issue, centred around the notion of process as the backbone. We build an ontology describing the fundamental elements involved in IIoT and their relations, and discuss the construction of the Process-aware IIoT Knowledge Graph, where raw sensor data are enriched with information about process activities and the physical production environment. We also propose a framework for querying the Knowledge Graph, and we demonstrate its capabilities by considering the production of metal accessories as case study.}
}
@article{SCHAFFHAUSER2023105695,
title = {A framework for the broad dissemination of hydrological models for non-expert users},
journal = {Environmental Modelling & Software},
volume = {164},
pages = {105695},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105695},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223000816},
author = {Timo Schaffhauser and Daniel Garijo and Maximiliano Osorio and Daniel Bittner and Suzanne Pierce and Hernán Vargas and Markus Disse and Yolanda Gil},
keywords = {Software metadata, Model metadata, Model encapsulation, Model catalogs, MINT, Hydrological models},
abstract = {Hydrological models are essential in water resources management, but the expertise required to operate them often exceeds that of potential stakeholders. We present an approach that facilitates the dissemination of hydrological models, and its implementation in the Model INTegration (MINT) framework. Our approach follows principles from software engineering to create software components that reveal only selected functionality of models which is of interest to users while abstracting from implementation complexity, and to generate metadata for the model components. This methodology makes the models more findable, accessible, interoperable, and reusable in support of FAIR principles. We showcase our methodology and its implementation in MINT using two case studies. We illustrate how the models SWAT and MODFLOW are turned into software components by hydrology experts, and how users without hydrology expertise can find, adapt, and execute them. The two models differ in terms of represented processes and in model design and structure. Our approach also benefits expert modelers, by simplifying model sharing and the execution of model ensembles. MINT is a general modeling framework that uses artificial intelligence techniques to assist users, and is released as open-source software.}
}
@article{AMER2022102670,
title = {Robust deep learning early alarm prediction model based on the behavioural smell for android malware},
journal = {Computers & Security},
volume = {116},
pages = {102670},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102670},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822000694},
author = {Eslam Amer and Shaker El-Sappagh},
keywords = {Android malware prediction, API Calls, Contextual behaviour, System calls, Permissions, Behavioral analysis, Process mining, Sequence reformulation},
abstract = {Due to the widespread expansion of the Android malware industry, malicious Android processes mining became a necessity to understand their behavior. Nevertheless, due to the complexities of size, length, and associations of some essential and distinguishing Android applications’ features such as API calls and system calls, mining malicious Android processes become a prominent obstacle. The malicious process mining obstacle is also coupled with the increasing rate of zero-day attacks, with no prior knowledge about those kinds of behaviors. Hence, malware detection alone is no longer enough; instead, we need new methodologies to predict malicious behaviors early. In this paper, we propose a behavioral Android malware smell predictor model. Our model relies on various static and dynamic features. We overcame the problem of massive feature size and complex associations by encapsulating related features in a few cluster classes. Accordingly, the cluster classes are exchangeably used to represent the features in the original calling sequences. Regarding substantially long sequences, experimental results showed that our model could predict whether a process is behaving maliciously or not based on rapid-sequence-snapshot analysis. The proposed model counted on the LSTM model to classify the reformed API and system call sequences snapshots. Moreover, we used ensemble machine learning classifiers to classify Android permissions. We trained the LSTM model using random snapshots of the newly formed API and system call cluster sequences. We tested our model against common ransomware attacks. We found that our trained LSTM model showed stable performance at a particular snapshot size. The model showed competitive accuracy in predicting new sequences. Accordingly, we proposed an early alarm solution for blocking malicious payloads instead of identifying them after their fulfillment. Hence, we can avoid the cost of future damage.}
}
@article{GARCIA2020104387,
title = {The GeoCore ontology: A core ontology for general use in Geology},
journal = {Computers & Geosciences},
volume = {135},
pages = {104387},
year = {2020},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2019.104387},
url = {https://www.sciencedirect.com/science/article/pii/S0098300419306284},
author = {Luan Fonseca Garcia and Mara Abel and Michel Perrin and Renata {dos Santos Alvarenga}},
keywords = {GeoCore ontology, Core ontology, Geological knowledge, Ontology engineering, BFO top-level ontology, Knowledge modeling},
abstract = {Domain ontologies assume the role of representing, in a formal way, a consensual knowledge of a community over a domain. This task is especially difficult in a wide domain like Geology, which is composed of diversified science resting on a large variety of conceptual models that were developed over time. The meaning of the concepts used by the various professionals often depends on the particular vision that they have of a domain according to their background and working habits. Ontology development in Geology thus necessitates a drastic elucidation of the concepts and vocabulary used by geologists. This article intends to contribute to solving these difficulties by proposing a core ontology named GeoCore Ontology resting on the BFO top ontology, specially designed for describing scientific fields. GeoCore Ontology contains well-founded definitions of a limited set of general concepts within the Geology field that are currently considered by all geologists whatever their skill. It allows modelers to separately consider a geological object, the substance that constitutes it, the boundaries that limit it and the internal arrangement of the matter inside it. The core ontology also allows the description of the existentially dependent qualities attached to a geological object and the geological process that generated it in a particular geological age. This small set of formally defined and described concepts combined with concepts from BFO provides a backbone for deriving by subsumption more specialized geological concepts and also constitutes a baseline for integrating different existent domain ontologies within the Geology domain. The GeoCore ontology and the methodology that we used for building it, provide solutions for unveiling major misunderstanding regarding the concepts that are commonly used for formulating geological interpretations. This will facilitate the communication of this information to external Geology users and its integration in domain applications.}
}
@article{JUNG2022103785,
title = {Logical separability of labeled data examples under ontologies},
journal = {Artificial Intelligence},
volume = {313},
pages = {103785},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103785},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001254},
author = {Jean Christoph Jung and Carsten Lutz and Hadrien Pulcini and Frank Wolter},
keywords = {Logical separability, Decidable fragments of first-order logic, Description logic, Learning from examples, Complexity, Ontologies},
abstract = {Finding a logical formula that separates positive and negative examples given in the form of labeled data items is fundamental in applications such as concept learning, reverse engineering of database queries, generating referring expressions, and entity comparison in knowledge graphs. In this paper, we investigate the existence of a separating formula for data in the presence of an ontology. Both for the ontology language and the separation language, we concentrate on first-order logic and the following important fragments thereof: the description logic ALCI, the guarded fragment, the two-variable fragment, and the guarded negation fragment. For separation, we also consider (unions of) conjunctive queries. We consider several forms of separability that differ in the treatment of negative examples and in whether or not they admit the use of additional helper symbols to achieve separation. Our main results are model-theoretic characterizations of (all variants of) separability, the comparison of the separating power of different languages, and the investigation of the computational complexity of deciding separability.}
}
@article{MAKSIMOV2021540,
title = {Knowledge ontology system},
journal = {Procedia Computer Science},
volume = {190},
pages = {540-545},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.063},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013107},
author = {Nikolay Maksimov and Alexander Lebedev},
keywords = {Ontology, knoweledge, ontology development.},
abstract = {The paper considers a purpose and features of ontologies use in describing of subject area, both from a theoretical and an applied point of view. Criteria for identifying ontologies types are indicated. Based on cognition schematism principles, a knowledge representation ontologies system has been developed, that combines language, forms of knowledge representation and process schemes. It is shown that it is exactly such a system of ontologies makes it possible for the practical use of ontologies in computing environments.}
}
@article{PASKALEVA2021103689,
title = {Leveraging integration facades for model-based tool interoperability},
journal = {Automation in Construction},
volume = {128},
pages = {103689},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103689},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521001400},
author = {Galina Paskaleva and Alexandra Mazak-Huemer and Manuel Wimmer and Thomas Bednar},
keywords = {MDE, Data exchange, Big Open BIM, Semantic integration, Pragmatic integration, Heterogeneity},
abstract = {Data exchange and management methods are of paramount importance in areas as complex as the Architecture, Engineering and Construction industries and Facility Management. For example, Big Open BIM requires seamless information flow among an arbitrary number of applications. The backbone of such information flow is a robust integration, whose tasks include overcoming technological as well as semantic and pragmatic gaps and conflicts both within and between data models. In this work, we introduce a method for integrating the pragmatics at design-time and the semantics of independent applications at run-time into so-called “integration facades”. We utilize Model-driven Engineering for the automatic discovery of functionalities and data models, and for finding a user-guided consensus. We present a case study involving the domains of architecture, building physics and structural engineering for evaluating our approach in object-oriented as well as data-oriented programming environments. The results produce, for each scenario, a single integration facade that acts as a single source of truth in the data exchange process.}
}
@article{STEVENS2019100469,
title = {Measuring expert performance at manually classifying domain entities under upper ontology classes},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100469},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S157082681830043X},
author = {Robert Stevens and Phillip Lord and James Malone and Nicolas Matentzoglu},
keywords = {OWL, Ontologies, Upper ontologies, Ontology engineering, Empirical study},
abstract = {Background:
Classifying entities in domain ontologies under upper ontology classes is a recommended task in ontology engineering to facilitate semantic interoperability and modelling consistency. Integrating upper ontologies this way is difficult and, despite emerging automated methods, remains a largely manual task.
Problem:
Little is known about how well experts perform at upper ontology integration. To develop methodological and tool support, we first need to understand how well experts do this task. We designed a study to measure the performance of human experts at manually classifying classes in a general knowledge domain ontology with entities in the Basic Formal Ontology (BFO), an upper ontology used widely in the biomedical domain.
Method:
We recruited 8 BFO experts and asked them to classify 46 commonly known entities from the domain of travel with BFO entities. The tasks were delivered as part of a web survey.
Results:
We find that, even for a well understood general knowledge domain such as travel, the results of the manual classification tasks are highly inconsistent: the mean agreement of the participants with the classification decisions of an expert panel was only 51%, and the inter-rater agreement using Fleiss’ Kappa was merely moderate (0.52). We further follow up on the conjecture that the degree of classification consistency is correlated with the frequency the respective BFO classes are used in practice and find that this is only true to a moderate degree (0.52, Pearson).
Conclusions:
We conclude that manually classifying domain entities under upper ontology classes is indeed very difficult to do correctly. Given the importance of the task and the high degree of inconsistent classifications we encountered, we further conclude that it is necessary to improve the methodological framework surrounding the manual integration of domain and upper ontologies.}
}
@article{BELLINI2025111281,
title = {Certifying entity models, entities and data messages on IoT/WoT platforms via blockchain},
journal = {Computer Networks},
volume = {265},
pages = {111281},
year = {2025},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2025.111281},
url = {https://www.sciencedirect.com/science/article/pii/S138912862500249X},
author = {Pierfrancesco Bellini and Edoardo Branchi and Enrico Collini and Luciano Alessandro Ipsaro Palesi and Paolo Nesi and Gianni Pantaleo},
keywords = {Internet of things, Web of things, Certification of data models, Blockchain, Scalable architecture, Data certification, Time series},
abstract = {The growing complexity and the increasing diffusion of applications exploiting both the Internet of Things (IoT) and the Web of Things (WoT) paradigms have led to the necessity of ensuring robust security and reciprocal trust in the management of huge amount of data from heterogeneous sources. The blockchain technology can satisfy these requirements, ensuring data integrity, univocity, and immutability of the data shared and saved in distributed IoT/IoW systems. In this paper, a deep analysis of requirements for Blockchain based IoT/WoT frameworks has been conducted. The main contributions include (i) formal implications of certifying integrity for data models and for the usage/certification of corresponding instances of entities/ devices and their time series messages in a platform that may present both non-certified and certified models, entities and data messages; (ii) automation of certification and verification processes, which implies mechanisms and formal rules; (iii) flexible certification of data messages enforcing efficiency and sustainability; (iv) support for multi-organization distributed solutions; (v) performance assessment. The solution has been validated with the interconnection to a private/permissioned blockchain developed using Hyperledger Fabric, while enforcing the solution on Snap4City federated network of platforms for applications in the domain of mobility. The research has been performed in the context of CN MOST, which is the national center on Sustainable Mobility in Italy, and within its flagship action OPTIFaaS.}
}
@article{JARVENPAA201887,
title = {Formal Resource and Capability Models supporting Re-use of Manufacturing Resources},
journal = {Procedia Manufacturing},
volume = {19},
pages = {87-94},
year = {2018},
note = {Proceedings of the 6th International Conference in Through-life Engineering Services, University of Bremen, 7th and 8th November 2017},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918300131},
author = {Eeva Järvenpää and Minna Lanz and Niko Siltala},
keywords = {Capability model, Production system representation, Adaptive manufacturing, Ontology},
abstract = {In the field of manufacturing the responsiveness has become a new strategic goal for the enterprises alongside with quality and costs. Efficient responsiveness requires production reconfiguration ranging from layout to equipment. The production system capabilities originate from the tool and equipment level. While a resource is being used, its condition and capability may change. It is crucial to consider the resources’ individual lifecycle, their actual capabilities and condition during the system design and reconfiguration. Thus, the lifecycle perspective in the capability management is of utmost importance. This paper presents the development of the Manufacturing Resource Capability Ontology (MaRCO), focusing on describing the functional capabilities of manufacturing resources. Special emphasis is placed on the lifecycle management aspect of the resource descriptions.}
}
@article{CHOUCHANI2022111082,
title = {Model-based safety engineering for autonomous train map},
journal = {Journal of Systems and Software},
volume = {183},
pages = {111082},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111082},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001795},
author = {Nadia Chouchani and Sana Debbech and Matthieu Perin},
keywords = {Model-based safety engineering, Safety ontology, Model-driven engineering, Safety/assurance case, Railway infrastructure model, Autonomous train},
abstract = {As a part of the digital revolution of railway systems, an autonomous driving train will use a complete and precise map of railway infrastructure to conduct operational actions. Nevertheless, the full autonomy of trains depends on the safety decisions management capacity both on-board and track-side. These decisions must be refined into safety requirements in order to continuously check the consistency between the perceived infrastructure and safety related properties. However, traditionally, the integration of safety analysis requires the intervention of human agent skills. This may be error-prone and in interference with the embedded aspect of the train map. In this paper, we propose a model-based approach to match between safety concepts expressed as an ontology, a derived safety model and a safety-extended railway infrastructure map model for autonomous trains. This approach is validated by railway safety case studies for autonomous train map. The integration of this model-based safety solution from the early stages of the map system design improves the safety decisions management process.}
}
@article{GHORBEL2020101864,
title = {Handling data imperfection—False data inputs in applications for Alzheimer’s patients},
journal = {Data & Knowledge Engineering},
volume = {130},
pages = {101864},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2020.101864},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2030094X},
author = {Fatma Ghorbel and Fayçal Hamdi and Nassira Achich and Elisabeth Metais},
keywords = {Applications for Alzheimer’s patients, Imperfection types, False data inputs, Believability},
abstract = {Handling data imperfection is a crucial issue in many application domains. This is particularly true when handling imperfect data inputs in applications for Alzheimer’s patients. In this paper we first propose a typology of imperfection for data entered by Alzheimer’s patients or their caregivers in the context of these applications (mainly due to the memory discordance caused by the disease). This topology includes nine direct and three indirect imperfection types. The direct ones are deduced from the data inputs e.g. uncertainty and uselessness. The indirect imperfection types are deduced from the direct ones, e.g. the redundancy. We then propose an approach, called DBE_ALZ, that handles false data entry by estimating the believability of each data input. Based on the proposed typology, the falsity of these data is related to five imperfection types: uncertainty, confusion, typing error, wrong knowledge and inconsistency. DBE_ALZ includes a believability model that defines a set of dimensions and sub-dimensions allowing a qualitative estimation of the believability of a given data input. It is estimated based on its reasonableness and the reliability of its author. Compared to related work, the data input reasonableness is measured not only based on common-sense standard, but also based on a set of personalized assertions. The reliability of the patient is estimated based on the progression of the disease and the state of his memory at the moment of entry. However, the reliability of the caregiver is estimated based on his age and his knowledge about the data input’s field. Based on the believability model, we estimate quantitatively the believability of the data input by defining a set of metrics associated to the proposed dimensions and sub-dimensions. The measurement methods rely on probability and fuzzy set theories to reason about uncertain and imprecise knowledge (Bayesian networks and Mamdani fuzzy inference systems). Three languages are supported: English, French and Arabic. Based on the generated believability degrees, a set of decisive actions are proposed to guarantee the quality of the data inputs e.g., inferring or not based on a given data. We illustrate the usefulness of our approach in the context of the Captain Memo memory prosthesis. Finally, we discuss the encouraging results derived from the evaluation step.}
}
@article{ZHU2019479,
title = {Bibliometric analysis of patent infringement retrieval model based on self-organizing map neural network algorithm},
journal = {Library Hi Tech},
volume = {38},
number = {2},
pages = {479-491},
year = {2019},
issn = {0737-8831},
doi = {https://doi.org/10.1108/LHT-12-2018-0201},
url = {https://www.sciencedirect.com/science/article/pii/S0737883119000368},
author = {Dimin Zhu},
keywords = {Databases, Data analysis, Data mining, Social sciences, Data collection techniques, Hypertext},
abstract = {Purpose
The purpose of this paper is to quickly retrieve the same or similar patents in a large patent database.
Design/methodology/approach
The research is carried out through the analysis of the issue of patent examination, the type of patent infringement search and theories related to patent infringement determination and text mining.
Findings
The results show that the model improves the speed of patent search. It can quickly, accurately and comprehensively retrieve the same or equivalent patents as the imported patent claims.
Research limitations/implications
The patent infringement detection mainly focuses on the measurement of patent similarity in the implementation method. It is not mature, and there is still much room for improvement in research.
Practical implications
The model improves the efficiency of patent infringement detection, increases the accuracy of detection and protects the interests of patent stakeholders.
Originality/value
This study has great significance for improving the efficiency of patent examiners.}
}
@article{WATROBSKI20203356,
title = {Ontology learning methods from text - an extensive knowledge-based approach},
journal = {Procedia Computer Science},
volume = {176},
pages = {3356-3368},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319566},
author = {Jarosław Wątróbski},
keywords = {Ontology learning from text, Methods for ontology leaninf from text, Domain ontology learning, Ontology integration},
abstract = {Ontologies are a key element of the Semantic Web. They aim to capture basic knowledge by providing appropriate terms and formal relationships between them, so that they can be used in a machine-processable manner. Accordingly they enable automatic aggregation and practical use as well as unexpected reuse of distributed data sources. Ontologies may come from many different sources, pursuing different goals and quality criteria. However, performed manually ontology construction is a very complex and tedious task, thus many methods proposed offer automatic or semi-automatic way for ontology construction. Many of the methods have their own, specific features. Therefore, this paper proposes an extensive knowledge-based approach covering the domain of ontology learning methods from text. This work aims to collect the knowledge of available approaches for ontology learning and the prominent differences between them, drawing on best practices in ontology engineering. The proposed approach refers to methods and aims to enrich knowledge in the field of ontology learning (OL). In this paper, the author’s ontology contains a set of various types of methods with main techniques used, and the necessary features in the miscellaneous approaches. The proposed an extensive knowledge-based approach uses a reasoning mechanism based on competency questions for individual approaches to determine their ontology learning method profiles. The validation stage has also been carried out. At the same time, it is an extension of the previous study in the form of a repository of knowledge about OL tools. In addition, the combination of both ontologies: tools and methods aim to provide a more efficient OL solution from text.}
}
@article{AZIZ201987,
title = {An ontology-based methodology for hazard identification and causation analysis},
journal = {Process Safety and Environmental Protection},
volume = {123},
pages = {87-98},
year = {2019},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2018.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S095758201831365X},
author = {Abdul Aziz and Salim Ahmed and Faisal I. Khan},
keywords = {Hazard identification, Probabilistic ontology, Web ontology language, Multi-entity Bayesian network, Expert system},
abstract = {This article presents a dynamic hazard identification methodology founded on an ontology-based knowledge modeling framework coupled with probabilistic assessment. The objective is to develop an efficient and effective knowledge-based tool for process industries to screen hazards and conduct rapid risk estimation. The proposed generic model can translate an undesired process event (state of the process) into a graphical model, demonstrating potential pathways to the process event, linking causation to the transition of states. The Semantic web-based Web Ontology Language (OWL) is used to capture knowledge about unwanted process events. The resulting knowledge model is then transformed into Probabilistic-OWL (PR-OWL) based Multi-Entity Bayesian Network (MEBN). Upon queries, the MEBNs produce Situation Specific Bayesian Networks (SSBN) to identify hazards and their pathways along with probabilities. Two open-source software programs, Protégé and UnBBayes, are used. The developed model is validated against 45 industrial accidental events extracted from the U.S. Chemical Safety Board's (CSB) database. The model is further extended to conduct causality analysis.}
}
@article{STORK2019100462,
title = {Semantic annotation of natural history collections},
journal = {Journal of Web Semantics},
volume = {59},
pages = {100462},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300283},
author = {Lise Stork and Andreas Weber and Eulàlia {Gassó Miracle} and Fons Verbeek and Aske Plaat and Jaap {van den Herik} and Katherine Wolstencroft},
keywords = {Linked data, Biodiversity, Natural history collections, Ontologies, Semantic annotation, History of science},
abstract = {Large collections of historical biodiversity expeditions are housed in natural history museums throughout the world. Potentially they can serve as rich sources of data for cultural historical and biodiversity research. However, they exist as only partially catalogued specimen repositories and images of unstructured, non-standardised, hand-written text and drawings. Although many archival collections have been digitised, disclosing their content is challenging. They refer to historical place names and outdated taxonomic classifications and are written in multiple languages. Efforts to transcribe the hand-written text can make the content accessible, but semantically describing and interlinking the content would further facilitate research. We propose a semantic model that serves to structure the named entities in natural history archival collections. In addition, we present an approach for the semantic annotation of these collections whilst documenting their provenance. This approach serves as an initial step for an adaptive learning approach for semi-automated extraction of named entities from natural history archival collections. The applicability of the semantic model and the annotation approach is demonstrated using image scans from a collection of 8, 000 field book pages gathered by the Committee for Natural History of the Netherlands Indies between 1820 and 1850, and evaluated together with domain experts from the field of natural and cultural history.}
}
@article{TUZUN20252575,
title = {Granular and Relational SWOT Analysis: An Ontological Approach},
journal = {Procedia Computer Science},
volume = {253},
pages = {2575-2585},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.317},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925003254},
author = {Alican Tüzün and Shailesh Tripathi and Nadine Bachmann and Ann-Kristin Thienemann and Manuel Brunner and Herbert Jodlbauer},
keywords = {Protege, Ontology, SWOT, SOFT, SWOT Analysis, TOWS, TOWS Matrix},
abstract = {The traditional Strength, Weakness, Opportunity, and Threat (SWOT) analysis, despite its popularity [37], faces a significant challenge in interpreting information through the ”SWOT Matrix (SM)” [20]. The conventional matrix fails to capture the complexity, semantics, and detailed characteristics necessary for comprehensive decision-making. Therefore, this paper introduces an innovative approach to address these limitations by an ontology SWOTONT [45] representing the domain of SWOT analysis. SWOTONT is developed through a literature review and a bottom-up ontology development approach [23], providing context to the SWOT attributes by introducing finer subcategories and mapping their interrelationships. By developing a structured semantic approach, authors aimed to enable more precise knowledge extraction and support advanced strategic analysis. Future work will focus on empirical validation through case studies and domain expert feedback, integrating an upper ontology and exploring additional applications and integrations of the model.}
}
@article{BERGES2025100863,
title = {Two ontology design patterns in the domain of collections},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100863},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100863},
url = {https://www.sciencedirect.com/science/article/pii/S1570826825000034},
author = {Idoia Berges and Arantza Illarramendi},
keywords = {Ontology design pattern, Collection},
abstract = {Collections are objects used to arrange, into a single unit, multiple data items that form a natural group. Different types of collections exist, due to different constraints based on whether or not they impose an order on their elements and whether or not they allow repetition of elements. Any of them are easily found in several domains of our everyday life. For instance, a deck of cards, the prime divisors of a number or the teams that compete in a championship can be seen as a collection. Thus, an effective modeling of collections is a recurring issue in information management. In the ontology design field, recurring modeling problems can be addressed by the use of Ontology Design Patterns (ODPs). In the case of collections, ODPs have been proposed for representing sequences, lists, sets and bags. However, none of these patterns are completely adequate for representing collections of ordered elements without repetition. In this paper we present an ODP for representing that notion, which we have named Permutation. Moreover, another ODP named ListOfPermutations is also introduced, which allows to represent how the order of a Permutation varies along time. Because not all constraints required by these ODPs can be represented in OWL 2, SHACL shapes have been used in their definitions.}
}
@article{TOME202081,
title = {FlexRQC: Model for a Flexible Robot-Driven Quality Control Station},
journal = {Procedia Manufacturing},
volume = {51},
pages = {81-87},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920318680},
author = {A. González Tomé and I Irigoien Ceberio and U. Ayala and J.A. Agirre and N. Arana-Arexolaleiba},
keywords = {Robotics, Flexible Quality Inspection, Programming},
abstract = {As the investment on a dedicated quality control stations is not desirable for limited production batches. In general, those systems result in very optimised systems and the lack of flexibility since they are designed for an ad-hoc production. To provide a solution for those cases, a new model to design a flexible quality inspection system is proposed. This paper introduces FlexRQC (Flexible Robotic Quality Control) a model for characterising flexible robot-driven quality control stations. FlexRQC is divided into two domains: The Quality Control Station Domain (QCSD) and the Model Under Inspection Domains (MUID). FlexRQC takes advantage of 3D CAD systems to get spacial information on the quality control station and the quality requirement. The flexibility of the model has been successfully tested in two quality control station setups and various solid rigid objects.}
}
@article{WANG2022641,
title = {An ontology-based product usage context modeling method for smart customization},
journal = {Procedia CIRP},
volume = {109},
pages = {641-646},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.307},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122007569},
author = {Xingzhi Wang and Ang Liu and Sami Kara},
keywords = {Design ontology, Product usage context, Smart customization},
abstract = {Product usage context (PUC) identification is an effective approach to approximate the complex driver behind heterogeneous customer preference. With the sweeping trend of data-driven smart customization, a large volume of product usage data has allowed designers to understand contextual customer needs (CNs) and enable them to offer highly customized products and services in time. However, as the PUC ontology is not clearly defined, most of the existing PUC models are incomplete, ambiguous and imprecise. Inappropriate use of those models will result in failing to extract knowledge from data. In this paper, an ontology-based context modeling method is proposed, with the aim to help designers understand PUC in a comprehensive manner. A case study of robot vacuum cleaner (RVC) is used as an illustrative example. It is concluded that the proposed method can enable designers quickly establish a well-defined PUC model to support smart customization.}
}
@article{AMATO2018754,
title = {Improving security in cloud by formal modeling of IaaS resources},
journal = {Future Generation Computer Systems},
volume = {87},
pages = {754-764},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17305964},
author = {Flora Amato and Francesco Moscato and Vincenzo Moscato and Francesco Colace},
keywords = {Cloud services, Verification, Big-Data, Security},
abstract = {Nowadays, it is a matter of fact that Cloud is a “must” for all complex services requiring great amount of resources. Big-Data Services are a striking example: they actually perform many kind of analysis (like analytics) on very big repositories. Many File Systems and middleware exist for efficient distribution and management of data and they usually use Cloud Resources. Anyway Several problems arose about Security of data: Virtualization is the base of Cloud resources and, even if we consider data storage as virtually separated elements, security issues exist if privilege escalation allows for gaining control on any data on physical hosts. In this paper we show how it is possible to cope Model Driven Engineering techniques to security analysis and monitoring of Cloud infrastructures. For reducing overhead, we provide a formal profile of hosts thermal behaviors. Depending on services input workloads, we detect and forecast malicious actions by comparisons with real thermal data.}
}
@article{GARCIASILVA2019550,
title = {Enabling FAIR research in Earth Science through research objects},
journal = {Future Generation Computer Systems},
volume = {98},
pages = {550-564},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.03.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18314638},
author = {Andres Garcia-Silva and Jose Manuel Gomez-Perez and Raul Palma and Marcin Krystek and Simone Mantovani and Federica Foglini and Valentina Grande and Francesco {De Leo} and Stefano Salvi and Elisa Trasatti and Vito Romaniello and Mirko Albani and Cristiano Silvagni and Rosemarie Leone and Fulvio Marelli and Sergio Albani and Michele Lazzarini and Hazel J. Napier and Helen M. Glaves and Timothy Aldridge and Charles Meertens and Fran Boler and Henry W. Loescher and Christine Laney and Melissa A. Genazzio and Daniel Crawl and Ilkay Altintas},
keywords = {FAIR principles, Research objects, Research infrastructure, Semantic technologies, Earth Science},
abstract = {Data-intensive science communities are progressively adopting FAIR practices that enhance the visibility of scientific breakthroughs and enable reuse. At the core of this movement, research objects contain and describe scientific information and resources in a way compliant with the FAIR principles and sustain the development of key infrastructure and tools. This paper provides an account of the challenges, experiences and solutions involved in the adoption of FAIR around research objects over several Earth Science disciplines. During this journey, our work has been comprehensive, with outcomes including: an extended research object model adapted to the needs of earth scientists; the provisioning of digital object identifiers (DOI) to enable persistent identification and to give due credit to authors; the generation of content-based, semantically rich, research object metadata through natural language processing, enhancing visibility and reuse through recommendation systems and third-party search engines; and various types of checklists that provide a compact representation of research object quality as a key enabler of scientific reuse. All these results have been integrated in ROHub, a platform that provides research object management functionality to a wealth of applications and interfaces across different scientific communities. To monitor and quantify the community uptake of research objects, we have defined indicators and obtained measures via ROHub that are also discussed herein.}
}
@article{ZALAMEAPATINO2018162,
title = {Merging and expanding existing ontologies to cover the Built Cultural Heritage domain},
journal = {Journal of Cultural Heritage Management and Sustainable Development},
volume = {8},
number = {2},
pages = {162-178},
year = {2018},
issn = {2044-1266},
doi = {https://doi.org/10.1108/JCHMSD-05-2017-0028},
url = {https://www.sciencedirect.com/science/article/pii/S2044126618000172},
author = {Olga Piedad {Zalamea Patino} and Jos {Van Orshoven} and Thérèse Steenberghen},
keywords = {CIDOC-CRM, BCH-ontology, CityGML, Mondis},
abstract = {Purpose
The purpose of this paper is to present the development of an ontological model consisting of terms and relationships between these terms, creating a conceptual information model for the Built Cultural Heritage (BCH) domain, more specifically for preventive conservation.
Design/methodology/approach
The On-To-Knowledge methodology was applied in the ontology development process. Terms related to preventive conservation were identified by means of a taxonomy which was used later to identify related existing ontologies. Three ontologies were identified and merged, i.e. Geneva City Geographic Markup Language (Geneva CityGML), Monument Damage ontology (Mondis) and CIDOC Conceptual Reference Model (CIDOC-CRM). Additional classes and properties were defined as to provide a complete semantic framework for management of BCH.
Findings
A BCH-ontology for preventive conservation was created. It consists of 143 classes from which 38 originate from the Mondis ontology, 38 from Geneva CityGML, 37 from CIDOC-CRM and 30 were newly created. The ontology was applied in a use case related to the New cathedral in the city of Cuenca, Ecuador. Advantages over other type of systems and for the BCH-domain were discussed based on this example.
Research limitations/implications
The proposed ontology is in a testing stage through which a number of its aspects are being verified.
Originality/value
This ontological model is the first one to focus on the preventive conservation of BCH.}
}
@article{POURJAFARIAN2025930,
title = {An ODP-based Ontology for the Digital Product Passport},
journal = {Procedia CIRP},
volume = {135},
pages = {930-935},
year = {2025},
note = {32nd CIRP Conference on Life Cycle Engineering (LCE2025)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.12.125},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125003750},
author = {Monireh Pourjafarian and Christiane Plociennik and Simon Bergweiler and Nastaran Moarefvand and Jonas Brozeit and Mahdi Rezapour and Martin Ruskowski},
keywords = {Circular Economy, Digital Product Passport, Ontology, Ontology Design Patterns, Asset Administration Shell},
abstract = {The challenges posed by climate change can only be met by changing the economic mindset to one that focuses on the idea of a circular economy (CE). Digitalization, data collection and data storage play a crucial role here: Product-related data should be collected in a consistent manner throughout the entire life cycle of the product and stored in a Digital Product Passport (DPP). The DPP should give all stakeholders of the CE access to the necessary data. Overarching modelling is required to ensure that a DPP can be used as a structure across application domains in an interoperable way. Ontologies can act as an interlingua between domains, incorporating the required domain knowledge and the full range of requirements for the DPP. Following a modular approach based on Ontology Design Patterns, this paper develops a DPP ontology with a focus on the R-strategies within the CE. Furthermore, the paper builds a bridge to the standardized approach of Industry 4.0, the modelling and storage of structured domain knowledge in Asset Administration Shells (AAS). Data can be seamlessly integrated and used for decision-making in each product life cycle phase. In addition, the reuse of existing concepts defined by others is demonstrated. The developed ontology is then evaluated on a CE use case.}
}
@article{AYADI2019572,
title = {Ontology population with deep learning-based NLP: a case study on the Biomolecular Network Ontology},
journal = {Procedia Computer Science},
volume = {159},
pages = {572-581},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.212},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919313961},
author = {Ali Ayadi and Ahmed Samet and François de Bertrand {de Beuvron} and Cecilia Zanni-Merk},
keywords = {Ontology population, Knowledge acquisition, Natural language processing, Deep learning, Biomolecular Network Ontology},
abstract = {As a scientific discipline, systems biology aims to build models of biological systems and processes through the computer analysis of a large amount of experimental data describing the behaviour of whole cells. It is within this context that we already developed the Biomolecular Network Ontology especially for the semantic understanding of the behaviour of complex biomolecular networks and their transittability. However, the challenge now is how to automatically populate it from a variety of biological documents. To this end, the target of this paper is to propose a new approach to automatically populate the Biomolecular Network Ontology and take advantage of the vast amount of biological knowledge expressed in heterogeneous unstructured data about complex biomolecular networks. Indeed, we have recently observed the emergence of deep learning techniques that provide significant and rapid progress in several domains, particularly in the process of deriving high-quality information from text. Despite its significant progress in recent years, deep learning is still not commonly used to populate ontologies. In this paper, we present a deep learning-based NLP ontology population system to populate the Biomolecular Network Ontology. Its originality is to jointly exploit deep learning and natural language processing techniques to identify, extract and classify new instances referring to the BNO ontology’s concepts from textual data. The preliminary results highlight the efficiency of our proposal for ontology population.}
}
@article{PEREZMUNOZ2025103513,
title = {Feasibility of Deep Reinforcement Learning for the real-time attitude control of a satellite system},
journal = {Journal of Systems Architecture},
volume = {167},
pages = {103513},
year = {2025},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2025.103513},
url = {https://www.sciencedirect.com/science/article/pii/S1383762125001857},
author = {Ángel-Grover Pérez-Muñoz and Guillermo López-García and Irene García-Villoria and Alejandro Alonso and Angel Porras-Hermoso and María S. Pérez},
keywords = {Artificial intelligence, Neural network controllers, Safety-critical systems, Embedded systems},
abstract = {Although Machine Learning (ML) is widely used in a variety of interdisciplinary applications, its implementation in safety-critical systems, such as the Attitude Control System (ACS) of a satellite, poses numerous challenges. While previous studies have shown promising results, there is a lack of information on the design and development process for the application of ML in real-time control systems. This paper presents the implementation of a Deep Reinforcement Learning (DRL) model for a magnetic-based ACS of the UPMSat-2 satellite. The primary objective is not only to design, implement, and validate an RL agent, but also to provide some insights and criteria of the decision-making process to achieve an adequate model. The system was trained and validated on a simulation model with positive results. To further validate non-functional requirements, the resulting trained agent was tested on a real-time embedded system according to safety standards. The obtained quantitative metrics and performance results show the ability of the agent to maintain the satellite’s attitude across various operational phases, leveraging its adaptability to dynamic conditions.}
}
@article{CHO2020257,
title = {Ontology for Strategies and Predictive Maintenance models},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {257-264},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301889},
author = {Sangje Cho and Marlène Hildebrand-Ehrhardt and Gokan May and Dimitris Kiritsis},
keywords = {Ontology, Semantic technology, Semantic interoperability, Maintenance, Predictive maintenance, Industry 4.0},
abstract = {As of today, to cope with traditional maintenance policies such as reactive and preventive maintenance, the manufacturing companies need the deployment of adaptive and responsive maintenance strategies. Meanwhile, the advent of Industry 4.0 leads the maintenance paradigm shift facilitated by the efficient monitoring of physical assets and forecasting of the potential risks. As the advanced maintenance policies benefit in terms of cost-efficiency, inventory management and reliability management, most of the manufacturing companies are trying to make their own advanced maintenance strategies and to elaborate on the development of an innovative platform for it. However, since advanced enabling technologies collect a huge amount of data from different data sources such as machine, component, document, process and so on, data federation should necessarily be achieved for further discussion, but manufacturing companies are immature to address this issue. H2020 EU project Z-BRE4K, i.e., Strategies and predictive maintenance models wrapped around physical systems for zero-unexpected-breakdowns and increased operating life of factories, deploys semantic technologies to address this issue. This paper deals with the debate on how to efficiently federate various data formats with the support of the semantic technologies in the context of maintenance. In addition, it proposes a maintenance ontology validated and implemented with an actor from European industry.}
}
@article{KANTARELIS2023100754,
title = {Functional harmony ontology: Musical harmony analysis with Description Logics},
journal = {Journal of Web Semantics},
volume = {75},
pages = {100754},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100754},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000385},
author = {Spyridon Kantarelis and Edmund Dervakos and Natalia Kotsani and Giorgos Stamou},
keywords = {Description Logics, Ontology engineering, OWL, Music harmony, Modal harmony, Tonal harmony},
abstract = {Symbolic representations of music are emerging as an important data domain both for the music industry and for computer science research, aiding in the organization of large collections of music and facilitating the development of creative and interactive AI. An aspect of symbolic representations of music, which differentiates them from audio representations, is their suitability to be linked with notions from music theory that have been developed over the centuries. One core such notion is that of functional harmony, which involves analyzing progressions of chords. This paper proposes a description of the theory of functional harmony within the OWL 2 RL profile and experimentally demonstrates its practical use.}
}
@article{POMP2018249,
title = {A Web-based UI to Enable Semantic Modeling for Everyone},
journal = {Procedia Computer Science},
volume = {137},
pages = {249-254},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.024},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316296},
author = {André Pomp and Alexander Paulus and Daniel Klischies and Christian Schwier and Tobias Meisen},
keywords = {Semantic Modeling, Knowledge Graph, User Interface},
abstract = {Since companies generate and store large amounts of data daily in centralized systems such as data lakes, understanding data sets from different sources is becoming an increasingly complex task in dealing with data heterogeneity across domains. One solution for describing semantics of data sources is the use of semantic models based on an available vocabulary. However, creating detailed semantic models can be a challenging task for users who are not familiar with semantic modeling and today’s available tools. To overcome this challenge, we developed an intuitive and user-friendly interface, allowing data owners to define detailed semantic models for their data sources. The design of the user interface is based on an intensive requirement analysis gathered among several peers. It provides an intuitive mapping of semantic concepts to data attributes and the definition of relations between those concepts using drag and drop interaction. The user is given full modeling freedom as the insertion of semantic concepts and relations that are missing in the underlying vocabulary can be done on-demand and does not delay or impair the modeling process. Additionally, the refinement of the original detected data schema is supported with several operations. We built the interface into the semantic data platform ESKAPE, which already uses a flexible knowledge graph as underlying vocabulary and provides a detailed analysis of the data schema.}
}
@article{JARVENPAA20181094,
title = {Product Model ontology and its use in capability-based matchmaking},
journal = {Procedia CIRP},
volume = {72},
pages = {1094-1099},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.211},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118303718},
author = {Eeva Järvenpää and Niko Siltala and Otto Hylli and Minna Lanz},
keywords = {Product Model, Ontology, Information Model, Capability-based matchmaking, Production system design, Reconfiguration},
abstract = {Capability-based matchmaking aims to support rapid design and reconfiguration of modular plug-and-produce type production systems. It relies on formal ontological descriptions of product requirements and resource capabilities. This paper introduces the structure and content of the developed Product Model ontology, and explains its role as a part of the capability matchmaking procedure. A case product is modelled in order to visualize a matchmaking scenario. We expect that such matchmaking will reduce the workload of system designers and reconfiguration planners as it can automatically suggest potential resources for a certain need from large resource catalogues.}
}
@article{ZAVARELLA2024e32479,
title = {Triplétoile: Extraction of knowledge from microblogging text},
journal = {Heliyon},
volume = {10},
number = {12},
pages = {e32479},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e32479},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024085104},
author = {Vanni Zavarella and Sergio Consoli and Diego {Reforgiato Recupero} and Gianni Fenu and Simone Angioni and Davide Buscaldi and Danilo Dessí and Francesco Osborne},
keywords = {Information extraction, Knowledge graphs, Social media analysis, Named entity recognition, Hierarchical clustering, Word embeddings},
abstract = {Numerous methods and pipelines have recently emerged for the automatic extraction of knowledge graphs from documents such as scientific publications and patents. However, adapting these methods to incorporate alternative text sources like micro-blogging posts and news has proven challenging as they struggle to model open-domain entities and relations, typically found in these sources. In this paper, we propose an enhanced information extraction pipeline tailored to the extraction of a knowledge graph comprising open-domain entities from micro-blogging posts on social media platforms. Our pipeline leverages dependency parsing and classifies entity relations in an unsupervised manner through hierarchical clustering over word embeddings. We provide a use case on extracting semantic triples from a corpus of 100 thousand tweets about digital transformation and publicly release the generated knowledge graph. On the same dataset, we conduct two experimental evaluations, showing that the system produces triples with precision over 95% and outperforms similar pipelines of around 5% in terms of precision, while generating a comparatively higher number of triples.}
}
@article{RIBONI2019709,
title = {Sensor-based activity recognition: One picture is worth a thousand words},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {709-722},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19303863},
author = {Daniele Riboni and Marta Murtas},
keywords = {Activity recognition, Intelligent systems, Pervasive computing, Activity models, Unsupervised reasoning},
abstract = {In several domains, including healthcare and home automation, it is important to unobtrusively monitor the activities of daily living (ADLs) carried out by people at home. A popular approach consists in the use of sensors attached to everyday objects to capture user interaction, and ADL models to recognize the current activity based on the temporal sequence of used objects. Often, ADL models are automatically extracted from labeled datasets of activities and sensor events, using supervised learning techniques. Unfortunately, acquiring such datasets in smart homes is expensive and violates users’ privacy. Hence, an alternative solution consists in manually defining ADL models based on common sense, exploiting logic languages such as description logics. However, manual specification of ADL ontologies is cumbersome, and rigid ontological definitions fail to capture the variability of activity execution. In this paper, we introduce a radically new approach enabled by the recent proliferation of tagged visual contents available on the Web. Indeed, thanks to the popularity of social network applications, people increasingly share pictures and videos taken during the execution of every kind of activity. Often, shared contents are tagged with metadata, manually specified by their owners, that concisely describe the depicted activity. Those metadata represent an implicit activity label of the picture or video. Moreover, today’s computer vision tools support accurate extraction of tags describing the situation and the objects that appear in the visual content. By reasoning with those tags and their corresponding activity labels, we can reconstruct accurate models of a comprehensive set of human activities executed in the most disparate situations. This approach overcomes the main shortcomings of existing techniques. Compared to supervised learning methods, it does not require the acquisition of training sets of sensor events and activities. Compared to knowledge-based methods, it does not involve any manual modeling effort, and it captures a comprehensive array of execution modalities. Through extensive experiments with large datasets of real-world ADLs, we show that this approach is practical and effective.}
}
@article{CAO2022103574,
title = {A core reference ontology for steelmaking process knowledge modelling and information management},
journal = {Computers in Industry},
volume = {135},
pages = {103574},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103574},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001810},
author = {Qiushi Cao and Sadeer Beden and Arnold Beckmann},
keywords = {Industry 4.0, Steelmaking, Knowledge graph, Ontology, Ontology-based data access, Condition-based maintenance},
abstract = {Following the trend of Industry 4.0, the business model of steel manufacturing is transforming from a historical inwardly focused supplier/customer relationship to one that embraces the wider end-to-end supply chain and improves productivity more holistically. However, the data and information required for supply chain planning and steelmaking process modelling are normally distributed over scattered sources across organisation boundaries and research communities. This leads to a major problem concerning semantic interoperability. To address this issue, this paper introduces a Common Reference Ontology for Steelmaking (CROS). CROS serves as a shared steelmaking resource and capability model that aims to facilitate knowledge modelling, knowledge sharing and information management. In contrast to most of the existing steelmaking ontologies which merely focus on conceptual modelling, our work pays special attention to the real-world implementation and utilisation aspects of CROS. The functionality and usefulness of CROS is evaluated and tested on a real-world condition-based monitoring and maintenance task for cold rolling mills at Tata Steel in the United Kingdom.}
}
@article{GUTIERREZ2024107854,
title = {KD SENSO-MERGER: An architecture for semantic integration of heterogeneous data},
journal = {Engineering Applications of Artificial Intelligence},
volume = {132},
pages = {107854},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.107854},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624000125},
author = {Yoan Gutiérrez and José I. Abreu Salas and Andrés Montoyo and Rafael Muñoz and Suilan Estévez-Velarde},
keywords = {Heterogeneous data, Knowledge discovery, NERC, Natural language processing, Ontology and knowledge representation, Semantic data integration},
abstract = {This paper presents KD SENSO-MERGER, a novel Knowledge Discovery (KD) architecture that is capable of semantically integrating heterogeneous data from various sources of structured and unstructured data (i.e. geolocations, demographic, socio-economic, user reviews, and comments). This goal drives the main design approach of the architecture. It works by building internal representations that adapt and merge knowledge across multiple domains, ensuring that the knowledge base is continuously updated. To deal with the challenge of integrating heterogeneous data, this proposal puts forward the corresponding solutions: (i) knowledge extraction, addressed via a plugin-based architecture of knowledge sensors; (ii) data integrity, tackled by an architecture designed to deal with uncertain or noisy information; (iii) scalability, this is also supported by the plugin-based architecture as only relevant knowledge to the scenario is integrated by switching-off non-relevant sensors. Also, we minimize the expert knowledge required, which may pose a bottleneck when integrating a fast-paced stream of new sources. As proof of concept, we developed a case study that deploys the architecture to integrate population census and economic data, municipal cartography, and Google Reviews to analyze the socio-economic contexts of educational institutions. The knowledge discovered enables us to answer questions that are not possible through individual sources. Thus, companies or public entities can discover patterns of behavior or relationships that would otherwise not be visible and this would allow extracting valuable information for the decision-making process.}
}
@article{KUKKONEN2022104067,
title = {An ontology to support flow system descriptions from design to operation of buildings},
journal = {Automation in Construction},
volume = {134},
pages = {104067},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104067},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005185},
author = {Ville Kukkonen and Ali Kücükavci and Mikki Seidenschnur and Mads Holten Rasmussen and Kevin Michael Smith and Christian Anker Hviid},
keywords = {Building information modeling, HVAC, Semantic web, Ontology, Linked data},
abstract = {The interoperability of information from design to operations is an acknowledged challenge in the fields of architecture, engineering and construction (AEC). As a potential solution to the interoperability issues, there has been increasing interest in how linked data and semantic web technologies can be used to establish an extendable data model. Semantic web ontologies have been developed for the AEC domain, but an ontology for describing the energy and mass flow between systems and components is missing. This study proposes the Flow Systems Ontology (FSO) for describing the composition of flow systems, and their mass and energy flows. Two example models are expressed using FSO vocabulary. SPARQL Protocol and RDF Query Language (SPARQL) queries are performed to further demonstrate and validate the ontology. The main contribution consists of developing FSO as an ontology complementary to the existing ontologies. Finally, the paper introduces a roadmap for future developments building on FSO.}
}
@article{ARISTA2023270,
title = {An Ontology-based Engineering system to support aircraft manufacturing system design},
journal = {Journal of Manufacturing Systems},
volume = {68},
pages = {270-288},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523000377},
author = {Rebeca Arista and Xiaochen Zheng and Jinzhi Lu and Fernando Mas},
keywords = {Ontology-based Engineering, Decision support, Ontology, Knowledge graph, Aircraft manufacturing system, Knowledge management, Cognitive Digital Twin},
abstract = {During the conceptual design phase of an aircraft manufacturing system, different industrial scenarios need to be evaluated against performance indicators in a collaborative engineering process. Domain experts’ knowledge and the motivations for decision-making is a crucial asset for enterprises which is challenging to be captured and capitalised. Ontology-based Engineering (OBE) systems emerge as a new generation of Knowledge-based Engineering techniques with advancements of ontology engineering methods and computer science technologies. Ontologies enable to capture both explicit and implicit domain knowledge from historical records and domain experts. These Ontology-based Engineering systems can stand highly complex collaborative design processes involving multidisciplinary stakeholders and various digital tools. This paper proposes a tradespace framework with Ontology-based Engineering features included on top of existing Model-Based System Engineering and interoperability capabilities. These additional Ontology-based Engineering features reuse formalised knowledge via knowledge graph technologies and generative algorithms, changing the cognitive process from the designer, to an automatic process which generates design alternatives for the designer. The tradespace framework is demonstrated in a case study to design the aircraft fuselage orbital joint process, helping the designer to take better strategic decisions at conceptual phase and proving to be an advantageous paradigm for the design process.}
}
@article{IQBAL20224767,
title = {Mobile Devices Interface Adaptivity Using Ontologies},
journal = {Computers, Materials and Continua},
volume = {71},
number = {3},
pages = {4767-4784},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.023239},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822006919},
author = {Muhammad Waseem Iqbal and Muhammad Raza Naqvi and Muhammad Adnan Khan and Faheem Khan and T. Whangbo},
keywords = {User context, adaptive interfaces, human computer interaction},
abstract = {Currently, many mobile devices provide various interaction styles and modes which create complexity in the usage of interfaces. The context offers the information base for the development of Adaptive user interface (AUI) frameworks to overcome the heterogeneity. For this purpose, the ontological modeling has been made for specific context and environment. This type of philosophy states to the relationship among elements (e.g., classes, relations, or capacities etc.) with understandable satisfied representation. The context mechanisms can be examined and understood by any machine or computational framework with these formal definitions expressed in Web ontology language (WOL)/Resource description frame work (RDF). The Protégé is used to create taxonomy in which system is framed based on four contexts such as user, device, task and environment. Some competency questions and use-cases are utilized for knowledge obtaining while the information is refined through the instances of concerned parts of context tree. The consistency of the model has been verified through the reasoning software while SPARQL querying ensured the data availability in the models for defined use-cases. The semantic context model is focused to bring in the usage of adaptive environment. This exploration has finished up with a versatile, scalable and semantically verified context learning system. This model can be mapped to individual User interface (UI) display through smart calculations for versatile UIs.}
}
@article{BOUFRIDA20221150,
title = {Rule extraction from scientific texts: Evaluation in the specialty of gynecology},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {4},
pages = {1150-1160},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820303736},
author = {Amina Boufrida and Zizette Boufaida},
keywords = {Text mining, Natural language processing, Knowledge extraction, Rule acquisition, Ontology Web Language (OWL) ontology, Semantic Web Rule Language (SWRL) rules},
abstract = {Due to the considerable increase in freely available data (especially on the Web), extracting relevant information from textual content is a critical challenge. Most of the available data is embedded in unstructured texts and is not linked to formalized knowledge structures such as ontologies or rules. A potential solution to this problem is to acquire such knowledge through natural language processing (NLP) tools and text mining techniques. Prior work has focused on the automatic extraction of ontologies from texts, but the acquired knowledge is generally limited to simple hierarchies of terms. This paper presents a polyvalent framework for acquiring complex relationships from texts and coding these in the form of rules. Our approach begins with existing domain knowledge represented as an OWL ontology, and applies NLP tools and text matching techniques to deduce different atoms, such as classes, properties and literals, to capture deductive knowledge in the form of new rules. For the reason, to enrich the existing domain ontology by these rules, in order to obtain higher relational expressiveness, make reasoning and produce new facts. The approach was tested using medical reports, specifically, in the specialty of gynecology. It reports an F-measure of 95.83% on test our corpus.}
}
@article{RABOUDI2022104007,
title = {The BMS-LM ontology for biomedical data reporting throughout the lifecycle of a research study: From data model to ontology},
journal = {Journal of Biomedical Informatics},
volume = {127},
pages = {104007},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104007},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000235},
author = {Amel Raboudi and Marianne Allanic and Daniel Balvay and Pierre-Yves Hervé and Thomas Viel and Thulaciga Yoganathan and Anais Certain and Jacques Hilbey and Jean Charlet and Alexandre Durupt and Philippe Boutinaud and Benoît Eynard and Bertrand Tavitian},
keywords = {Provenance, Local terminologies, Data sharing, Research Data Management, Data annotation, Heterogeneous data},
abstract = {Biomedical research data reuse and sharing is essential for fostering research progress. To this aim, data producers need to master data management and reporting through standard and rich metadata, as encouraged by open data initiatives such as the FAIR (Findable, Accessible, Interoperable, Reusable) guidelines. This helps data re-users to understand and reuse the shared data with confidence. Therefore, dedicated frameworks are required. The provenance reporting throughout a biomedical study lifecycle has been proposed as a way to increase confidence in data while reusing it. The Biomedical Study - Lifecycle Management (BMS-LM) data model has implemented provenance and lifecycle traceability for several multimodal-imaging techniques but this is not enough for data understanding while reusing it. Actually, in the large scope of biomedical research, a multitude of metadata sources, also called Knowledge Organization Systems (KOSs), are available for data annotation. In addition, data producers uses local terminologies or KOSs, containing vernacular terms for data reporting. The result is a set of heterogeneous KOSs (local and published) with different formats and levels of granularity. To manage the inherent heterogeneity, semantic interoperability is encouraged by the Research Data Management (RDM) community. Ontologies, and more specifically top ontologies such as BFO and DOLCE, make explicit the metadata semantics and enhance semantic interoperability. Based on the BMS-LM data model and the BFO top ontology, the BioMedical Study - Lifecycle Management (BMS-LM) core ontology is proposed together with an associated framework for semantic interoperability between heterogeneous KOSs. It is made of four ontological levels: top/core/domain/local and aims to build bridges between local and published KOSs. In this paper, the conversion of the BMS-LM data model to a core ontology is detailed. The implementation of its semantic interoperability in a specific domain context is explained and illustrated with examples from small animal preclinical research.}
}
@article{OTMANI2018359,
title = {Ontology-based approach to enhance medical web information extraction},
journal = {International Journal of Web Information Systems},
volume = {15},
number = {3},
pages = {359-382},
year = {2018},
issn = {1744-0084},
doi = {https://doi.org/10.1108/IJWIS-03-2018-0017},
url = {https://www.sciencedirect.com/science/article/pii/S1744008418000162},
author = {Nassim Abdeldjallal Otmani and Malik Si-Mohammed and Catherine Comparot and Pierre-Jean Charrel},
keywords = {Web search and information extraction, Metadata and ontologies, Knowledge engineering, Online patient-doctor conversation},
abstract = {Purpose
The purpose of this study is to propose a framework for extracting medical information from the Web using domain ontologies. Patient–Doctor conversations have become prevalent on the Web. For instance, solutions like HealthTap or AskTheDoctors allow patients to ask doctors health-related questions. However, most online health-care consumers still struggle to express their questions efficiently due mainly to the expert/layman language and knowledge discrepancy. Extracting information from these layman descriptions, which typically lack expert terminology, is challenging. This hinders the efficiency of the underlying applications such as information retrieval. Herein, an ontology-driven approach is proposed, which aims at extracting information from such sparse descriptions using a meta-model.
Design/methodology/approach
A meta-model is designed to bridge the gap between the vocabulary of the medical experts and the consumers of the health services. The meta-model is mapped with SNOMED-CT to access the comprehensive medical vocabulary, as well as with WordNet to improve the coverage of layman terms during information extraction. To assess the potential of the approach, an information extraction prototype based on syntactical patterns is implemented.
Findings
The evaluation of the approach on the gold standard corpus defined in Task1 of ShARe CLEF 2013 showed promising results, an F-score of 0.79 for recognizing medical concepts in real-life medical documents.
Originality/value
The originality of the proposed approach lies in the way information is extracted. The context defined through a meta-model proved to be efficient for the task of information extraction, especially from layman descriptions.}
}
@article{MCGRANAGHAN2023100142,
title = {The cultural-social nucleus of an open community: A multi-level community knowledge graph and NASA application},
journal = {Applied Computing and Geosciences},
volume = {20},
pages = {100142},
year = {2023},
issn = {2590-1974},
doi = {https://doi.org/10.1016/j.acags.2023.100142},
url = {https://www.sciencedirect.com/science/article/pii/S2590197423000319},
author = {Ryan M. McGranaghan and Ellie Young and Cameron Powers and Swapnali Yadav and Edlira Vakaj},
keywords = {Knowledge representation, Knowledge graph, Collaboration, Community, Data science, Open science, Inclusivity, Accessibility, Information organization, Heliophysics, Space physics, Collective intelligence},
abstract = {The challenges faced by science, engineering, and society are increasingly complex, requiring broad, cross-disciplinary teams to contribute to collective knowledge, cooperation, and sensemaking efforts. However, existing approaches to collaboration and knowledge sharing are largely manual, inadequate to meet the needs of teams that are not closely connected through personal ties or which lack the time to respond to dynamic requests for contextual information sharing. Nonetheless, in the current remote-first, complexity-driven, time-constrained workplace, such teams are both more common and more necessary. For example, the NASA Center for HelioAnalytics (CfHA) is a growing and cross-disciplinary community that is dedicated to aiding the application of emerging data science techniques and technologies, including AI/ML, to increase the speed, rigor, and depth of space physics scientific discovery. The members of that community possess innumerable skills and competencies and are involved in hundreds of projects, including proposals, committees, papers, presentations, conferences, groups, and missions. Traditional structures for information and knowledge representation do not permit the community to search and discover activities that are ongoing across the Center, nor to understand where skills and knowledge exist. The approaches that do exist are burdensome and result in inefficient use of resources, reinvention of solutions, and missed important connections. The challenge faced by the CfHA is a common one across modern groups and one that must be solved if we are to respond to the grand challenges that face our society, such as complex scientific phenomena, global pandemics and climate change. We present a solution to the problem: a community knowledge graph (KG) that aids an organization to better understand the resources (people, capabilities, affiliations, assets, content, data, models) available across its membership base, and thus supports a more cohesive community and more capable teams, enables robust and responsible application of new technologies, and provides the foundation for all members of the community to co-evolve the shared information space. We call this the Community Action and Understanding via Semantic Enrichment (CAUSE) ontology. We demonstrate the efficacy of KGs that can be instantiated from the ontology together with data from a given community (shown here for the CfHA). Finally, we discuss the implications, including the importance of the community KG for open science.}
}
@article{NUNDLOLL2021100064,
title = {A semantic approach to enable data integration for the domain of flood risk management},
journal = {Environmental Challenges},
volume = {3},
pages = {100064},
year = {2021},
issn = {2667-0100},
doi = {https://doi.org/10.1016/j.envc.2021.100064},
url = {https://www.sciencedirect.com/science/article/pii/S2667010021000433},
author = {Vatsala Nundloll and Rob Lamb and Barry Hankin and Gordon Blair},
keywords = {Ontologies, Structured data, Unstructured data, Semantic integration, Natural language processing, Flood risk management},
abstract = {With so many things around us continuously producing and processing data, be it mobile phones, or sensors attached to devices, or satellites sitting thousands of kilometres above our heads, data is becoming increasingly heterogeneous. Scientists are inevitably faced with data challenges, coined as the 4 V’s of data - volume, variety, velocity and veracity. In this paper, we address the issue of data variety. The task of integrating and querying such heterogeneous data is further compounded if the data is in unstructured form. We hence propose an approach using Semantic Web and Natural Language Processing techniques to resolve the heterogeneity arising in data formats, bring together structured and unstructured data and provide a unified data model to query from disparate data sets.}
}
@article{BARZEGAR2018319,
title = {Classification of composite semantic relations by a distributional-relational model},
journal = {Data & Knowledge Engineering},
volume = {117},
pages = {319-335},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X1730561X},
author = {Siamak Barzegar and Brian Davis and Siegfried Handschuh and Andre Freitas},
keywords = {Semantic relation, Distributional semantic, Deep learning, Classification},
abstract = {Different semantic interpretation tasks such as text entailment and question answering require the classification of semantic relations between terms or entities within text. However, in most cases, it is not possible to assign a direct semantic relation between entities/terms. This paper proposes an approach for composite semantic relation classification using one or more relations between entities/term mentions, extending the traditional semantic relation classification task. The proposed model is different from existing approaches which typically use machine learning models built over lexical and distributional word vector features in that is uses a combination of a large commonsense knowledge base of binary relations, a distributional navigational algorithm and sequence classification to provide a solution for the composite semantic relation classification problem. The proposed approach outperformed existing baselines with regard to F1-score, Accuracy, Precision and Recall.}
}
@article{BELKADI2018428,
title = {Towards an Unified Additive Manufacturing Product-Process Model for Digital Chain Management Purpose},
journal = {Procedia CIRP},
volume = {70},
pages = {428-433},
year = {2018},
note = {28th CIRP Design Conference 2018, 23-25 May 2018, Nantes, France},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.146},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118303032},
author = {Farouk Belkadi and Laura Martinez Vidal and Alain Bernard and Eujin Pei and Emilio M. Sanfilippo},
keywords = {Type your keywords here, separated by semicolons},
abstract = {Additive Manufacturing (AM) is a real example of emerging technologies that can influence the whole product development process. During the AM process, large amounts of data are created, modified stored and retrieved. The management of large amounts of data, as well as the complexity of the relationships between stakeholders are amongst the major challenges. Design activities should be well-integrated with the production process to ensure the consistency of the whole AM value chain, which begins from the conception to the production and post-treatment of the product. This paper discusses the main characteristics of the AM process. The Business Process Modelling Notation (BMPN) is used to describe the entire AM value chain and the connection between design and manufacturing processes. This is a preliminary step towards the definition of complete model dedicated to the representation of AM related knowledge. Semantic interoperability and the monitoring of the whole digital chain involved in the AM value chain are the most important applications of the proposed modeling framework.}
}
@article{SHAKED2025103954,
title = {BridgeSec: Facilitating effective communication between security engineering and systems engineering},
journal = {Journal of Information Security and Applications},
volume = {89},
pages = {103954},
year = {2025},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2024.103954},
url = {https://www.sciencedirect.com/science/article/pii/S2214212624002564},
author = {Avi Shaked and Nan Messe},
keywords = {Security by design, Model-driven engineering, System development, Vulnerability management, Threat modelling, Security engineering, Systems engineering},
abstract = {We increasingly rely on systems to perform reliably and securely. Therefore, it is imperative that security aspects are properly considered when designing and maintaining systems. However, achieving the security by design ideal is challenging. Security information is typically unstructured, dispersed, hard to communicate, and its assessment is somewhat subjective and tacit. Additionally, the inclusion of security information within design requires integrating the efforts of two knowledge-intensive disciplines: security engineering and systems engineering. In this paper, we introduce BridgeSec, a novel conceptual information-exchange interface to systemise the communication of security information between these two disciplines. The main contribution of BridgeSec lies in its explicit identification of concepts related to vulnerability management, which allows systems engineering and security engineering teams to codify pertinent information. The disciplines involved in the system design can thus coordinate policies, implementations and, ultimately, the security posture. Furthermore, based on the newly unveiled interface, an automated reasoning mechanism is specified. This mechanism allows to reason about the vulnerability posture of systems in a scalable and systematic way. First, we describe and formalise the information-exchange interface BridgeSecand how it can be used to reason about the security of systems designs. Next, we present an open-source prototype – integrated into a threat modelling tool – which rigorously implements the interface and the reasoning mechanism. Finally, we detail two diverse and prominent applications of the interface for communicating security aspects of systems designs. These applications show how BridgeSec can rigorously support the design of systems’ security in two representative scenarios: in coordinating security features and policy during design, and in coordinating mitigation to disclosed implementation vulnerabilities.}
}
@article{KATTI2020197,
title = {Bidirectional Transformation of MES Source Code and Ontologies},
journal = {Procedia Manufacturing},
volume = {42},
pages = {197-204},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.070},
url = {https://www.sciencedirect.com/science/article/pii/S235197892030634X},
author = {Badarinath Katti and Christiane Plociennik and Martin Ruskowski and Michael Schweitzer},
keywords = {Cloud based MES, Production Automation, Knowledge Representation, OPC-UA, Ontology, OWL, SWRL, SQWRL, SPARQL, Edge Computing},
abstract = {Future production environments must be flexible and reconfigurable. To achieve this, the devices and services to fulfill the different steps of a production order (PO) should not be selected in the manufacturing execution system (MES), but in an edge component close to the shop floor. To enable this, abstract services in the PO and concrete services provided by the field devices on the shop floor need to refer to a production ontology. The creation of this ontology is a challenge of its own. This research proposes a pragmatic automation of an encoding of a primary and light weight production ontology based on the source code of MES. The transformation procedure of source code to resource, product and generic concepts of the manufacturing plant ontology is described. To this end, the knowledge of OPC UA collaborations are also exploited during the creation of resource ontologies. Due to a fundamental difference between source code implementation (imperative paradigm) and ontology representation (declarative paradigm), the problem of information loss is inevitable. This problem is overcome by formulation of production and business rules that encapsulate the logic of the MES. The foundation of ontology is exploited to formulate these rulesets using OWL based constructs and OWL based rule languages such as Semantic Web Rule Language (SWRL), Semantic Query-Enhanced Web Rule Language (SQWRL) and SPARQL Protocol and RDF Query Language (SPARQL) based on feasibility and requirements of specific rules. Further, these rulesets are either run on the automatically generated ontology at design time with an intention to enrich the knowledge base, or production runtime to validate the pre-defined business rules between the production steps. The generated ontology also acts as basis for automatically generating the OWL-S ontologies for the OPC UA application methods for the purpose of dynamic manufacturing service discovery and orchestration. The generated ontology and an abstract PO hooked with formulated rules are cached to the shop-floor network for consequent production control to enable smart edge production. An implementation is conducted on an industrial use-case demonstrator to evaluate the applicability of the proposed approach.}
}
@article{SCHAEFFER20232106,
title = {OLAF: An Ontology Learning Applied Framework},
journal = {Procedia Computer Science},
volume = {225},
pages = {2106-2115},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.201},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923013595},
author = {Marion Schaeffer and Matthias Sesboüé and Jean-Philippe Kotowicz and Nicolas Delestre and Cecilia Zanni-Merk},
keywords = {ontology learning, ontology, knowledge acquisition, ontology-based system, framework, automation, NLP},
abstract = {Since the beginning of the century, research on ontology learning has gained popularity. Automatically extracting and structuring knowledge relevant to a domain of interest from unstructured textual data is a major scientific challenge. After studying the main existing methods, such as Text2Onto, we propose a new approach with a modular ontology learning framework focusing on automatically extracting knowledge from raw text sources. We consider tasks from data pre-processing to axiom extraction. Whereas previous contributions considered ontology learning systems as tools to help the domain expert craft a reusable ontology, we developed the proposed framework with full automation in mind to build a minimum viable ontology targeted at an application. Ontology Learning Applied Framework (OLAF) has been generically designed to build specific ontologies whatever the application domain, use case and text data. We implement an initial version and test the framework on an ontology-based system, a search engine for technical products.}
}
@article{GARCIA2019100487,
title = {Grounding knowledge acquisition with ontology explanation:A case study},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100487},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300672},
author = {Ana Cristina B. Garcia and Adriana S. Vivacqua},
keywords = {Explanation, Knowledge acquisition, Ontology validity, Ontology engineering},
abstract = {Knowledge validation is still a challenge when constructing knowledge-based systems. It is one of the major reasons for user rejection and disagreement between project participants. Systematic and periodic reviews of the domain ontology, with a formal agreement of the whole development team (including the experts) are a recommended good practice. Nevertheless, these reviews do not guarantee system success. This paper presents a case study of the construction process of a knowledge-based system. The process involved a group of experts with varied work experience. A great deal of negotiation happened during knowledge acquisition meetings, which took place during a 6-month period. After each meeting, changes in the ontology were verified through a web-based questionnaire, from which either consensual agreement was reached (and changes implemented) or the need for a new meeting was ascertained. An explanatory review at the beginning of each meeting further solidified the understanding of all participants. This cyclic process led to a final version of the ontology, ratified by all participants. This model supports diagnosis and prediction of failures in mechanical drilling rigs in oil exploration sites. Unexpectedly, during system trials, experts disagreed with results, which raised questions about the validity of the domain ontology. The system’s explanation module provided a cornerstone for a reflective process that helped identify inconsistencies and corrections needed. These reflections led to adjustments to the ontology, and a reflection about previous decisions and element definitions. Explanations, derived from the ontology and instantiated using real scenarios, shed light on knowledge gaps and semantic inconsistencies of the domain model. In this paper we have three main goals: (1) to present our ontology construction process; (2) to highlight a particular situation where results were inadequate; and (3) to show how the explanation system helped experts and knowledge engineers identify gaps. We also present lessons learned from the whole process, that may apply in other situations.}
}
@article{LI2024102747,
title = {Comprehensive digital twin for infrastructure: A novel ontology and graph-based modelling paradigm},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102747},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102747},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003951},
author = {Tao Li and Yi Rui and Hehua Zhu and Linhai Lu and Xiaojun Li},
keywords = {Digital twin, Infrastructure, Modelling, Ontology, Property graph model, Knowledge graph, BIM},
abstract = {The inherent uncertainty and complexity of infrastructure systems present significant challenges for precise management. By providing real-time status tracking and decision support capabilities, Digital Twin (DT) has shown significant promise in enhancing decision-making and improving efficiency and quality of management. To create a knowledge-rich and scalable DT capable of tracing the distinctive features and performance of infrastructure, this article develops a comprehensive IDT (Infrastructure DT) modelling paradigm. The ontology-based paradigm comprises five elements: scenario, virtual model, physical entity, relation, and component, supporting interoperability and multidisciplinary collaboration. By employing a graph-based modelling approach, the paradigm integrates various types of discrete data to link infrastructure entities into a cohesive system, thereby effectively capturing their dynamic characteristics. A case study comprising supported foundation excavation, subway tunnels, and pipe networks demonstrates the IDT’s capability to facilitate coordination across multidisciplinary, multi-scale, and multi-stage contexts, thereby establishing a transparent, effective, and secure pattern of infrastructure management.}
}
@article{CASTIGLIONE2025104150,
title = {SecOnto: Ontological Representation of Security Directives},
journal = {Computers & Security},
volume = {148},
pages = {104150},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104150},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004553},
author = {Gianpietro Castiglione and Giampaolo Bella and Daniele Francesco Santamaria},
keywords = {Semantic web, Reasoning, NIS 2},
abstract = {The current digital landscape demands robust security requirements and, for doing so, the institutions enact complex security directives to protect the citizens and the infrastructures, particularly in the European Union. These directives aim to safeguard data and harmonise security across the European region, and institutions must navigate this evolving legal landscape in order to implement and keep up-to-date the prescribed security measures. However, understanding and implementing these directives towards full compliance can be difficult and expensive. Ontological representation can be employed to represent and operationalise such security directives, ultimately contributing to the effectiveness and efficiency of the compliance process. Ontologies in fact promote a structured approach to represent knowledge, making the applicable directives more simply understandable by humans and more readily processable by machines. This article introduces SecOnto, a novel methodology for representing security directives as ontologies. SecOnto breaks down the process of transforming the juridical language of modern security directives into full-fledged ontologies by means of five semi-automated steps: Preprocessing, Interpretation, Structuring, Representation and Verification. Each step is described and validated by means of operational examples based upon Directive 2022/2555 of the European Parliament and of the Council of the European Union on security of network and information systems, better known as NIS 2.}
}
@article{DAPICA20211208,
title = {Towards a Semantic Knowledge Base for Competency-Based Training of Airline Pilots},
journal = {Procedia Computer Science},
volume = {192},
pages = {1208-1217},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.124},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921016136},
author = {Rubén Dapica and Federico Peinado},
keywords = {Pilot Training, Aviation Data Management, Data Interoperability, Ontology Engineering, Semantic Web},
abstract = {The acquisition and maintenance of non-technical skills by the pilots are fundamental factors for the prevention of aviation accidents. The aviation authorities are promoting that air crew training be carried out through simulator sessions using scenarios specifically designed to develop and assess the global performance of pilots in such skills. When designing custom flight training scenarios, choosing the correct events and conditions from the myriad of possible combinations with respect to their potential utility in training specific competencies is a costly task that depends entirely on highly specialized expert knowledge. In this paper, we present EBTOnto, an OWL DL ontology that allows to formalize this knowledge and other useful data from real cases, laying the foundations for a semantic knowledge base of scenarios for airline pilots training. Previous advances in this matter and possible applications of this system are reviewed. EBTOnto is built on top of a source validated by experts, the Evidence-Based Training Implementation Guide by the International Air Transport Association, and then checked using an automatic reasoner and a database of 37,568 aviation safety incidents, extracted from the widely regarded Aviation Safety Reporting System by the U.S. National Aeronautics and Space Administration. The results suggest that it is possible to classify real aviation scenarios in terms of non-technical competencies and filter useful incident reports for design and enrichment of these training scenarios. EBTOnto opens up new possibilities for interoperability between incident databases and training organizations, and smoothes the path to represent, share and generate custom simulation training scenarios for pilots based on real data.}
}
@article{OSTBERG202219,
title = {Domain Models and Data Modeling as Drivers for Data Management: The ASSISTANT Data Fabric Approach},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {19-24},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.362},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322016287},
author = {Per-Olov Östberg and Eduardo Vyhmeister and Gabriel G. Castañé and Bart Meyers and Johan {Van Noten}},
keywords = {Domain Models, Knowledge Graph, Data Modeling, Data Fabric, Data Base, Data Lake, AI, adaptive manufacturing},
abstract = {To develop AI-based models capable of governing or providing decision support to complex manufacturing environments, abstractions and mechanisms for unified management of data storage and processing capabilities are needed. Specifically, as such models tend to include and rely on detailed representations of systems, components, and tools with complex interactions, mechanisms for simplifying, integrating, and scaling management capabilities in the presence of complex data requirements (e.g., high volume, velocity, and diversity of data) are of particular interest. A data fabric is a system that provides a unified architecture for management and provisioning of data. In this work we present the background, design requirements, and high-level outline of the ASSISTANT data fabric - a flexible data management tool designed for use in adaptive manufacturing contexts. The paper outlines the implementation of the system with specific focus on the use of domain models and the data modeling approach used, as well as provides a generic use case structure reusable in many industrial contexts.}
}
@article{GUO2024104124,
title = {An ontology-based method for knowledge reuse in the design for maintenance of complex products},
journal = {Computers in Industry},
volume = {161},
pages = {104124},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104124},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524000526},
author = {Ziyue Guo and Dong Zhou and Dequan Yu and Qidi Zhou and Hongduo Wu and Aimin Hao},
keywords = {Knowledge management, Ontology development, Product design, Industrial maintenance},
abstract = {In the context of the Fourth Industrial Revolution, a large amount of heterogeneous data and information is generated during the lifecycle of complex products, which poses a considerable challenge for manufacturers and effective knowledge integration. It has been challenging for traditional experience-based design methods to meet the diverse needs of customers and maintain competitiveness in fierce global markets. Capturing, formalizing and reusing multidisciplinary knowledge that is scattered among different departments and stages to help make effective decisions has been a crucial way for digital enterprises to improve manufacturing efficiency. Design for maintenance is typical work requiring cross-domain knowledge and involving stakeholder collaboration. This paper presents a structured domain-specific ontology and its development method, namely, the Maintainability Design Ontology for Complex prOducts (MDOCO), to formalize heterogeneous knowledge and improve semantic interoperability in the maintainability design area. The MDOCO has a rigorous semantic structure and complies with well-designed top-level and middle ontologies such as the Basic Formal Ontology and the Industrial Ontology Foundry (IOF) Core Ontology to ensure semantic interoperability. A set of reasoning rules is carefully designed to enable the MDOCO to perform knowledge reasoning. In a practical case, the effectiveness of the MDOCO is validated at both the semantic and application levels. The MDOCO combines recent methodology and best practices, enabling the well-structured modeling of heterogeneous knowledge and good semantic interoperability.}
}
@article{VARGA2018240,
title = {Analytical metadata modeling for next generation BI systems},
journal = {Journal of Systems and Software},
volume = {144},
pages = {240-254},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.06.039},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218301274},
author = {Jovan Varga and Oscar Romero and Torben Bach Pedersen and Christian Thomsen},
keywords = {Business intelligence, Metadata, Ontological metamodeling},
abstract = {Business Intelligence (BI) systems are extensively used as in-house solutions to support decision-making in organizations. Next generation BI 2.0 systems claim for expanding the use of BI solutions to external data sources and assisting the user in conducting data analysis. In this context, the Analytical Metadata (AM) framework defines the metadata artifacts (e.g., schema and queries) that are exploited for user assistance purposes. As such artifacts are typically handled in ad-hoc and system specific manners, BI 2.0 argues for a flexible solution supporting metadata exploration across different systems. In this paper, we focus on the AM modeling. We propose SM4AM, an RDF-based Semantic Metamodel for AM. On the one hand, we claim for ontological metamodeling as the proper solution, instead of a fixed universal model, due to (meta)data models heterogeneity in BI 2.0. On the other hand, RDF provides means for facilitating defining and sharing flexible metadata representations. Furthermore, we provide a method to instantiate our metamodel. Finally, we present a real-world case study and discuss how SM4AM, specially the schema and query artifacts, can help traversing different models instantiating our metamodel and enabling innovative means to explore external repositories in what we call metamodel-driven (meta)data exploration.}
}
@article{BITSCH2021582,
title = {Open semantic modeling for smart production systems},
journal = {Procedia CIRP},
volume = {104},
pages = {582-587},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.098},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121009963},
author = {Günter Bitsch and Pascal Senjic},
keywords = {Ontology, Semantics, Modelling},
abstract = {Conventional production systems are evolving through cyber-physical systems and application-oriented approaches of AI, more and more into "smart" production systems, which are characterized among other things by a high level of communication and integration of the individual components. The exchange of information between the systems is usually only oriented towards the data content, where semantics is usually only implicitly considered. The adaptability required by external and internal influences requires the integration of new or the redesign of existing components. Through an open application-oriented ontology the information and communication exchange are extended by explicit semantic information. This enables a better integration of new and an easier reconfiguration of existing components. The developed ontology, the derived application and use of the semantic information will be evaluated by means of a practical use case.}
}
@article{BUNNELL2021113843,
title = {Development of a consumer financial goals ontology for use with FinTech applications for improving financial capability},
journal = {Expert Systems with Applications},
volume = {165},
pages = {113843},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113843},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420306527},
author = {Lawrence Bunnell and Kweku-Muata Osei-Bryson and Victoria Y. Yoon},
keywords = {Financial goals ontology, Ontology engineering, Ontology acquisition, Ontology evaluation, Consumer financial goals, Financial capability},
abstract = {In this research, we communicate the design and evaluation of a consumer financial goals ontology to be utilized as a knowledgebase within recommender systems applications designed to provide decision support in the domain of financial planning. The goal of this research is to provide a domain conceptualization and knowledge classification of a comprehensive set of financial capability enhancing objectives contextually appropriate for a wide range of socio-economically situated consumers. Currently, to the best of our knowledge, within the domain of consumer financial planning no formal conceptual model or knowledge classification of financial goals exists which might be utilized as a knowledgebase for applications such as a FinTech recommender system. Achieving financial goals is a key behavior associated with consumer financial capability, a topic of national economic importance. A holistic representation of domain concepts is critical for advancement of solutions to problems pertinent to a domain. One primary reason for the dearth of applications designed to assist consumers with financial goal setting is the absence of a common domain ontology of financial goals. This study addresses a gap in the literature by contributing to the research knowledgebase an ontology for a domain of consumer financial goals. In doing so, it advances scholarly research through novel domain knowledge classification while providing researchers and practitioners with an ontological knowledgebase for indexing and retrieval within applications designed to improve consumer financial capability through identification and recommendation of specific, context-aware financial goals. The ontology could be used, for example, as a knowledgebase for a Personal Financial Recommender System (PFRS), or other financial technology (FinTech) application, designed to assist users with identification, setting, and tracking of financial goals.}
}
@article{WEBER2023529,
title = {Methodology for agile and iterative ontology development for toolmaking},
journal = {Procedia CIRP},
volume = {120},
pages = {529-534},
year = {2023},
note = {56th CIRP International Conference on Manufacturing Systems 2023},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123007643},
author = {Sebastian Weber and Tammo Dannen and Lars Stauder and Sebastian Barth and Thomas Bergs},
keywords = {Semantic technology, ontology, toolmaking, digital twin, knowledge retrieval methodology},
abstract = {Complex manufacturing processes and the absence of repeat effects characterize the toolmaking industry. German-speaking toolmaking companies are increasingly faced with the challenge of having to reach the limits of what is technically feasible and are confronted with an erosion of know-how, induced by demographic change. This applies in particular to know-how-intensive areas such as design, CAM-programming and work preparation. There is currently no comprehensive system support in these areas and the knowledge required for planning activities is often only available in the form of implicit technical and empirical knowledge. However, the use of heterogeneous manufacturing technologies requires a profound understanding of technology along the entire value chain. As a result of the very high semantic expressiveness of ontologies, they enable the representation of the most complex data models with logical relationships that go beyond hierarchical subdivision of content. This paper presents a novel agile methodology for the development of domain-specific ontologies in the environment of toolmaking. The methodology makes it possible to integrate the implicitly existing technical and experiential knowledge of employees at an early stage in the modelling process. In particular, the developed methodology extends conventional methods by the identified deficits in terms of knowledge acquisition, iteration and agility, as well as a separate consideration of the life cycle along the ontology of the use phase, taking into account agile methods from requirements engineering. Compliance with various guidelines and requirements is mandatory, such as the formulation of competence questions and the use of a standardised specification document. The iterative approach also ensures the needs-based integration of the characteristics of toolmaking. Furthermore, the methodology enables an early integration of IT structure and user interface for the needs-based design and use of the domain-specific ontology. The validation is based on an example in the mechanical production of a toolmaking company.}
}
@article{REGAL20181511,
title = {Ontology for Conceptual Modelling of Intelligent Maintenance Systems and Spare Parts Supply Chain Integration},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {1511-1516},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.285},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318314095},
author = {Thiago Regal and Carlos Eduardo Pereira},
abstract = {With an increasing demand for more efficiency and less costs in industry, the integration between Intelligent Maintenance Systems (IMS) and Spare Parts Supply Chain (SPSC) results in better availability of parts and services, avoiding breakdowns and unplanned production interruptions. Better demand planning results in better cost-effectivity as well, as parts and services can be better planned and always available when needed. The proper integration of IMS and SPSC has challenges related to semantic differences between areas with diverse concepts and vocabularies. This work intends to explore these challenges and propose a conceptual model for such integration, using existing ontologies in domains such as supply chain, maintenance and manufacturing to build on top of them an ontology aimed at allowing conceptual integration between IMS and SPSC and becoming a foundation to build future information systems to integrate these two areas. Its purpose is also further explored by using the characteristics of an ontology as tool for semantic description. Artificial intelligence, reasoning and context-aware systems are some of the areas that can benefit from the existence of an ontology to model IMS and SPSC integration. In this paper, we explore such characteristics along with integration and interoperability obtained by using an ontological model in IMS and SPSC integration.}
}
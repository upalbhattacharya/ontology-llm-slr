@article{LIU2022100009,
title = {Representation and association of Chinese financial equity knowledge driven by multilayer ontology},
journal = {Data and Information Management},
volume = {6},
number = {3},
pages = {100009},
year = {2022},
issn = {2543-9251},
doi = {https://doi.org/10.1016/j.dim.2022.100009},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122001073},
author = {Zhenghao Liu and Zhijian Zhang and Xi Zeng and Huakui Lv},
keywords = {Multilayer domain ontology, Concept cube, Financial equity, Knowledge association, Knowledge representation, Triple extraction},
abstract = {Aiming at the current situation of complex financial ownership structure and isolated data organization, this study referring to the methods for multi-layer hierarchical construct domain ontology modeling. At the same time, the three dimensions of industry, company and internal environment were integrated, and the concept cube was designed and constructed based on knowledge extraction and text classification technology, so as to provide a multi-level and fine-grained knowledge representation and association method for financial equity knowledge. The experimental results show that conceptual cube structure represents semantic information as a dense low-dimensional representation vector, which greatly enhances semantic relevance and interpretability. The multi-layer ontology-driven ownership structure reflects a variety of knowledge association patterns, and in the “Intelligent Financial Big Data System” developed by the research team, the association query of three categories of association relationships in the field of industry, enterprise and internal environment is realized, as well as the dynamic analysis and supervision of typical financial management problems.}
}
@article{SCHWANCK2025107626,
title = {A Framework for testing Federated Learning algorithms using an edge-like environment},
journal = {Future Generation Computer Systems},
volume = {166},
pages = {107626},
year = {2025},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.107626},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24005909},
author = {Felipe Machado Schwanck and Marcos Tomazzoli Leipnitz and Joel Luís Carbonera and Juliano Araujo Wickboldt},
keywords = {Federated learning, Edge computing, Kubernetes, Microservices, Development framework},
abstract = {Federated Learning (FL) is a machine learning paradigm in which many clients cooperatively train a single centralized model while keeping their data private and decentralized. FL is commonly used in edge computing, which involves placing computer workloads (both hardware and software) as close as possible to the edge, where data are created and where actions are occurring, enabling faster response times, greater data privacy, and reduced data transfer costs. However, due to the heterogeneous data distributions/contents of clients, it is non-trivial to accurately evaluate the contributions of local models in global centralized model aggregation. This is an example of a major challenge in FL, commonly known as data imbalance or class imbalance. In general, testing and evaluating FL algorithms can be a very difficult and complex task due to the distributed nature of the systems. In this work, a framework is proposed and implemented to evaluate FL algorithms in a more easy and scalable way. This framework is evaluated over a distributed edge-like environment managed by a container orchestration platform (i.e. Kubernetes).}
}
@article{TRAPPEY2023102216,
title = {A comprehensive analysis of global patent landscape for recent R&D in agricultural drone technologies},
journal = {World Patent Information},
volume = {74},
pages = {102216},
year = {2023},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2023.102216},
url = {https://www.sciencedirect.com/science/article/pii/S0172219023000467},
author = {Amy J.C. Trappey and Ging-Bin Lin and Hong-Kai Chen and Ming-Chi Chen},
keywords = {Agricultural drone, Unmanned aerial vehicle (UAV), Knowledge ontology, Literature review, Patent landscape, Patent mining},
abstract = {The rapid development of information technology, along with advanced wireless communication technologies, has revolutionized the use of unmanned aerial vehicles (UAVs or drones) in many industries. In agriculture, due to the climate change and the growing global population, causing unprecedented demands and risks on food supplies, intelligent and automatic agricultural technologies are critical needs. UAVs offer a wide range of applications to transform traditional agricultural practices into Agriculture 4.0 that integrates advanced technology to optimize agricultural productivity, sustainability, and efficiency. To gain comprehensive insights into the current and future development of agricultural UAVs technologies, this research conducts extensive review and analysis of patents and non-patent literature in the field. By thoroughly examining the literature, the knowledge ontology of ag-UAV technologies is presented. Additionally, comprehensive macro- and micro-patent analyses identify the patenting trends and top tech-leaders for the key technologies related to agricultural drones. Moreover, utilizing regression modeling, technology maturity analysis, and technology-function matrix (TFM), the current and future R&D trends and the cold and hot spots of the technical innovations are identified. Through these detailed patent analyses, the state-of-the-art and potential advancements in agricultural UAV technologies are depicted, serving as crucial intelligence for R&D initiatives and IP strategies.}
}
@article{BENITEZMARTINEZ2021703,
title = {A neural blockchain for a tokenizable e-Participation model},
journal = {Neurocomputing},
volume = {423},
pages = {703-712},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.03.116},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220307335},
author = {Francisco Luis Benítez-Martínez and María Visitación Hurtado-Torres and Esteban Romero-Frías},
keywords = {Governance, Neural blockchain, e-Participation, Tokenization, Smart citizen, Open government},
abstract = {Currently, Distributed Ledger Technologies (DLTs) and, especially, Blockchain technology represent a great opportunity for public institutions to improve citizen participation and foster democratic innovation. These technologies facilitate the simplification of processes and provide secure management of recorded data, guaranteeing the transmission and public transparency of information. Based on the combination of a Blockchain as a Service (BaaS) platform and G-Cloud solutions, our proposal consists of the design of an e-Participation model that uses a tokenizable system of the actions and processes undertaken by citizens in participatory processes providing incentives to promote greater participation in public affairs. In order to develop a sustainable, scalable and resilient e-Participation system, a new blockchain concept, which organizes the blocks as a neural system, is combined with the implementation of a virtual token to reward participants. Furthermore, this virtual token is deployed through a smart contract that the block itself produces, containing information about the transaction and all the documents involved in the process. Finally, our Neural Distributed Ledger (NDL) framework facilitates the interconnection of blockchain networks in a transparent, certified, secure, auditable, scalable and traceable way.}
}
@article{KHOURI2023217,
title = {Knowledge base construction for the semantic management of environment-enriched built heritage: The case of Algerian traditional houses architecture},
journal = {Journal of Cultural Heritage},
volume = {63},
pages = {217-229},
year = {2023},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2023.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S1296207423001589},
author = {Selma Khouri and Houda Oufaida and Racha Amrani and Sabrina Kacher and Safia Ouahab and Mouna Cherrad},
keywords = {Knowledge base, Built heritage, Traditional house architecture, Environmental devices, Algeria},
abstract = {Traditional domestic architecture in Algeria reflects ancestral know-how which could serve as a model for new architectural achievements concerned with respecting the environment. This multidisciplinary knowledge covering different contexts (architectural, physical and environmental), is available but scattered through various sources of different formats (texts, tables, illustrations, etc.). Capturing such knowledge through an efficient knowledge repository is required. Knowledge base (KB) repositories have shown many significant contributions for managing the complexity, the variety of contexts and the querying of Built Heritage (BH) data. Creating a KB is a complex process that requires identifying and extracting the relevant knowledge from raw sources of information, then structuring, cleaning and integrating the extracted information into the KB repository. The management of these issues and their inter-dependencies are not always detailed in related BH literature. We propose in this paper a complete process for creating a KB dedicated to the built heritage consisting of Algerian traditional houses. Two main contributions are emphasized in this study: (i) the construction process of the KB is detailed at each step using two main suites: the ontology suite for identifying and structuring the relevant concepts, and the data suite that allows the flow of knowledge from the sources to the KB. (ii) the semantic variety of contexts of the input sources, their linguistic complexity and their format heterogeneity is handled all along the process, and is reflected at two levels: at the schema level (ontology model), and at the data level using automatic information extraction techniques. The definition of the KB includes fine-grained provenance metadata. Different results are proposed to evaluate the content and the quality of the KB.}
}
@article{PANZARELLA2023100059,
title = {Using ontologies for life science text-based resource organization},
journal = {Artificial Intelligence in the Life Sciences},
volume = {3},
pages = {100059},
year = {2023},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2023.100059},
url = {https://www.sciencedirect.com/science/article/pii/S266731852300003X},
author = {Giulia Panzarella and Pierangelo Veltri and Stefano Alcaro},
keywords = {Information overload, Ontology, Semantic web, Life science terms},
abstract = {Ontologies are used to support access to a multitude of databases that cover domains relevant information. Heterogeneity and different semantics can be accessed by using structured texts and descriptions in a hierarchical concept definition. We are interested in Life Sciences (LS) related ontologies including components taken from molecular biology, bioinformatics, physics, chemistry, medicine and other related areas. An Ontology comprises: (i) term connections, (ii) the identification of core concepts, (iii) data management, (iv) knowledge classification and integration to collect key information. An ontology may be very useful in navigating through LS terms. This paper explores some available biomedical ontologies and frameworks. It describes the most common ontology development environments (ODE): Protégé, Topbraid Composer, Ontostudio, Fluent Editor, VocBench, Swoop and Obo-edit, to create ontologies from textual scientific resources for LS plans. It also compares ontology methodologies in terms of Usability, Scalability, Stability, Integration, Documentation and Originality.}
}
@article{FITKAU2024102314,
title = {An ontology-based approach of automatic compliance checking for structural fire safety requirements},
journal = {Advanced Engineering Informatics},
volume = {59},
pages = {102314},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102314},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623004421},
author = {Isabelle Fitkau and Timo Hartmann},
keywords = {Preventive Fire Safety, Ontology, Automatic Code Compliance Checking},
abstract = {Achieving a mapping and integration of fire safety requirements into the digital planning process necessitates merging diverse and scattered regulatory knowledge related to fire safety. The nature of how requirements are phrased and described in regulations often lacks a comprehensive and detailed description of one another. Interpreting complex interrelations from these sources requires the expertise of specialists. An automatic code compliance checking is hindered without interpretation and through the absence of a formalization of preventive fire safety domain knowledge. Moreover, the non-involvement of fire safety planners in earlier project phases and the absence of fire safety properties in commonly used building modelling software further complicate the matter. To address these issues, we developed an ontological knowledge formalization, the Fire Safety Ontology (FiSa), enabling the classification of buildings and their components based on fire safety considerations. Combining building codes, technical regulations, guidelines, and semi-structured interviews with fire safety planners, we put emphasis on a fire safety perspective for buildings. Subsequently, we related fire safety domain insights from experts to knowledge about building design from other domains, such as architecture. Through this collaborative effort and the instantiation of building data within the ontology, structural fire safety requirements from regulations were automatically inferred by reasoning with common inference engines, simulating an automatic compliance checking. Our results demonstrate the contribution of ontology knowledge formalization to the integration process.}
}
@article{KOUTSIANA2023100799,
title = {An analysis of discussions in collaborative knowledge engineering through the lens of Wikidata},
journal = {Journal of Web Semantics},
volume = {78},
pages = {100799},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100799},
url = {https://www.sciencedirect.com/science/article/pii/S1570826823000288},
author = {Elisavet Koutsiana and Gabriel Maia Rocha Amaral and Neal Reeves and Albert Meroño-Peñuela and Elena Simperl},
keywords = {Collaborative knowledge engineering, Knowledge graph, Discussion analysis, Wikidata},
abstract = {We study discussions in Wikidata, the world’s largest open-source collaborative knowledge graph (KG). This is important because it helps KG community managers understand how discussions are used and inform the design of collaborative practices and support tools. We follow a mixed-methods approach with descriptive statistics, thematic analysis, and statistical tests to investigate how much discussions in Wikidata are used, what they are used for, and how they support knowledge engineering (KE) activities. The study covers three core sources of discussion, the talk pages that accompany Wikidata items and properties, and a general-purpose communication page. Our findings show low use of discussion capabilities and a power-law distribution similar to other KE projects such as Schema.org. When discussions are used, they are mostly about KE activities, including activities that span across the entire KE lifecycle from conceptualisation and implementation to maintenance and taxonomy building. We hope that the findings will help Wikidata devise improved practices and capabilities to encourage the use of discussions as a tool to collaborate, improve editor engagement, and engineer better KGs.}
}
@article{ALZAMIL2020100469,
title = {An ontological artifact for classifying social media: Text mining analysis for financial data},
journal = {International Journal of Accounting Information Systems},
volume = {38},
pages = {100469},
year = {2020},
note = {2019 UW CISA Symposium},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2020.100469},
url = {https://www.sciencedirect.com/science/article/pii/S1467089520300373},
author = {Zamil Alzamil and Deniz Appelbaum and Robert Nehmer},
keywords = {FIBO, Ontology, Social media, Frames and slots, Municipal bonds},
abstract = {In this paper we utilize a structured natural language processing implementation of the Financial Industry Business Ontology (FIBO) to extract financial information from the unstructured textual data of the social media platform Twitter regarding financial and budget information in the public sector, namely the two public-private agencies of the Port Authority of NY and NJ (PANYNJ), and the NY Metropolitan Transportation Agency (MTA). This research initiative uses the Design Science Research (DSR) perspective to develop an artifact to classify tweets as being either relevant to financial bonds or not. We apply a frame and slot approach from the artificial intelligence and natural language processing literature to operationalize this artifact. FIBO provides standards for defining the facts, terms, and relationships associated with financial concepts. We show that FIBO grammar can be used to mine semantic meaning from unstructured textual data and that it provides a nuanced representation of structured financial data. With this artifact, social media such as Twitter may be accessed for the knowledge that its text contains about financial concepts using the FIBO ontology. This process is anticipated to be of interest to bond issuers, regulators, analysts, investors, and academics. It may also be extended towards other financial domains such as securities, derivatives, commodities, and banking that relate to FIBO ontologies, as well as more generally to develop a structured knowledge representation of unstructured data through the application of an ontology.}
}
@article{BASSILIADES201881,
title = {PaaSport semantic model: An ontology for a platform-as-a-service semantically interoperable marketplace},
journal = {Data & Knowledge Engineering},
volume = {113},
pages = {81-115},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300551},
author = {Nick Bassiliades and Moisis Symeonidis and Panagiotis Gouvas and Efstratios Kontopoulos and Georgios Meditskos and Ioannis Vlahavas},
keywords = {Cloud computing, Platform-as-a-Service, Cloud Marketplace, Semantic interoperability, Ontologies, Quality and metrics},
abstract = {PaaS is a Cloud computing service that provides a computing platform to develop, run, and manage applications without the complexity of infrastructure maintenance. SMEs are reluctant to enter the growing PaaS market due to the possibility of being locked in to a certain platform, mostly provided by the market's giants. The PaaSport Marketplace aims to avoid the provider lock-in problem by allowing Platform provider SMEs to roll out semantically interoperable PaaS offerings and Software SMEs to deploy or migrate their applications on the best-matching offering, through a thin, non-intrusive Cloud broker. In this paper, we present the PaaSport semantic model, namely an OWL ontology, extension of the DUL ontology. The ontology is used for semantically representing (a) PaaS offering capabilities and (b) requirements of applications to be deployed. The ontology has been designed to optimally support a semantic matchmaking and ranking algorithm that recommends the best-matching PaaS offering to the application developer. The DUL ontology offers seamless extensibility, since both PaaS Characteristics and parameters are defined as classes; therefore, extending the ontology with new characteristics and parameters requires the addition of new specialized subclasses of the already existing classes, which is less complicated than adding ontology properties. The PaaSport ontology is evaluated through verification tools, competency questions, human experts, application tasks and query performance tests.}
}
@article{SELLAMI2022453,
title = {Keyword-based faceted search interface for knowledge graph construction and exploration},
journal = {International Journal of Web Information Systems},
volume = {18},
number = {56},
pages = {453-486},
year = {2022},
issn = {1744-0084},
doi = {https://doi.org/10.1108/IJWIS-02-2022-0037},
url = {https://www.sciencedirect.com/science/article/pii/S1744008422000118},
author = {Samir Sellami and Nacer Eddine Zarour},
keywords = {Knowledge exploration, Faceted browsing, Keyword search, Responsive interface, Virtual data exploration, Knowledge graphs, Linked data},
abstract = {Purpose
Massive amounts of data, manifesting in various forms, are being produced on the Web every minute and becoming the new standard. Exploring these information sources distributed in different Web segments in a unified way is becoming a core task for a variety of users’ and companies’ scenarios. However, knowledge creation and exploration from distributed Web data sources is a challenging task. Several data integration conflicts need to be resolved and the knowledge needs to be visualized in an intuitive manner. The purpose of this paper is to extend the authors’ previous integration works to address semantic knowledge exploration of enterprise data combined with heterogeneous social and linked Web data sources.
Design/methodology/approach
The authors synthesize information in the form of a knowledge graph to resolve interoperability conflicts at integration time. They begin by describing KGMap, a mapping model for leveraging knowledge graphs to bridge heterogeneous relational, social and linked web data sources. The mapping model relies on semantic similarity measures to connect the knowledge graph schema with the sources' metadata elements. Then, based on KGMap, this paper proposes KeyFSI, a keyword-based semantic search engine. KeyFSI provides a responsive faceted navigating Web user interface designed to facilitate the exploration and visualization of embedded data behind the knowledge graph. The authors implemented their approach for a business enterprise data exploration scenario where inputs are retrieved on the fly from a local customer relationship management database combined with the DBpedia endpoint and the Facebook Web application programming interface (API).
Findings
The authors conducted an empirical study to test the effectiveness of their approach using different similarity measures. The observed results showed better efficiency when using a semantic similarity measure. In addition, a usability evaluation was conducted to compare KeyFSI features with recent knowledge exploration systems. The obtained results demonstrate the added value and usability of the contributed approach.
Originality/value
Most state-of-the-art interfaces allow users to browse one Web segment at a time. The originality of this paper lies in proposing a cost-effective virtual on-demand knowledge creation approach, a method that enables organizations to explore valuable knowledge across multiple Web segments simultaneously. In addition, the responsive components implemented in KeyFSI allow the interface to adequately handle the uncertainty imposed by the nature of Web information, thereby providing a better user experience.}
}
@article{SANFILIPPO2019182,
title = {Ontology-based knowledge representation for additive manufacturing},
journal = {Computers in Industry},
volume = {109},
pages = {182-194},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S016636151830808X},
author = {Emilio M. Sanfilippo and Farouk Belkadi and Alain Bernard},
keywords = {Manufacturing, Additive manufacturing, Knowledge representation, Reasoning, Ontology, DOLCE},
abstract = {The flourishing development of additive manufacturing (AM) technologies calls for robust IT methodologies and solutions to manage the plethora of data that is generated in the AM value chain. The purpose of this paper is to propose a principled knowledge-based model for AM in the form of a computational ontology. As corpus of formally represented knowledge, the ontology constitutes the backbone structure to organize AM data and automatically reason over experts’ knowledge for data validation, ultimately supporting the development of algorithms and applications for decision making. By the end of the paper we show some modeling and reasoning examples based on the use of the proposed ontology in a prototype Web application.}
}
@article{SHAHZAD2021106146,
title = {Ontology Driven Smart Health Service Integration},
journal = {Computer Methods and Programs in Biomedicine},
volume = {207},
pages = {106146},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106146},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721002200},
author = {Syed Khuram Shahzad and Daniyal Ahmed and Muhammad Raza Naqvi and Muhammad Tahir Mushtaq and Muhammad Waseem Iqbal and Farrukh Munir},
keywords = {Internet of things, Integrated platform, Smart healthcare, Health service facilitation, Ontological framework},
abstract = {Background and objective
The massive increase, in the Internet of Things applications, has greatly evolved technological aspects of human life. The drastic development of IoT based smart healthcare services have layout the smart process models to facilitate all stakeholders (e.g. patients, doctors, hospitals etc.) and made it an important social-economic concern. There are variety of smart healthcare services like remote patient monitoring, diagnostic, disease specific remote treatments and telemedicine. Many trending Internet of Health Things research and development are done in a very disjoint and independent fashion providing solutions and guidelines for variant diseases, medical resources and remote services management. These expositions work over many shared resources such as health facilities for patient and human in healthcare system.
Methods
This research discusses the ontology for merging methods to form an integrated platform with shared knowledge of smart healthcare services. The proposed process model creates an ontological framework of integrated healthcare services, which are firstly defined using ontologies and lately integrated over similarities, differences, dependencies and other semantic relations. The data and process requirements for service integration facility is derived from various smart healthcare services.
Results
The proposed model is evaluated using two-step ontological modeling testing method, applied at the ontological framework of integrated smart health services. First evaluation step has targeted the model consistency validation using reasoning tool while querying tools are used to validate the retrieved data entities and relations among them for predefined use-cases.
Conclusions
The research concluded with a novel approach for smart health service integration using ontological modeling and merging techniques. The model efficiency enhancement and query optimization methods are listed in future tasks of the research.}
}
@article{MURTAZINA2021595,
title = {The constructing of cognitive functions ontology},
journal = {Procedia Computer Science},
volume = {186},
pages = {595-602},
year = {2021},
note = {14th International Symposium "Intelligent Systems},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921010188},
author = {M.Sh. Murtazina and T.V. Avdeenko},
keywords = {Ontology, OWL, cognitive functions, assessing cognitive functions},
abstract = {In present paper we consider ontology-driven approach applied to neurocognitive science. At the beginning, a brief overview of the works revealing the prospects and possibilities of constructing ontological models in this field of knowledge is given. Based on the conducted analysis we have concluded that the scientific community states the urgency of creating an ontology intended to accumulate interdisciplinary knowledge necessary for assessing cognitive functions being the most complex functions of the brain, through which the process of rational cognition of the world is carried out. So we have analyzed the key features of the knowledge representation about cognitive functions in the field of psychology, cognitive science and neurobiology. As a result we propose conceptual structure, basic classes and relations for the cognitive functions ontology intended to the clearer understanding of the relationships between the brain activity patterns and the human cognitive abilities. The ontology was implemented using OWL in the Protégé 5.2 ontology editor environment. It accumulates knowledge about cognitive functions and methods for assessing them with use of neuropsychological tests and EEG methods. A certain set of axioms also have been implemented and approved.}
}
@article{SESBOUE20221667,
title = {An Operational Architecture for Knowledge Graph-Based Systems},
journal = {Procedia Computer Science},
volume = {207},
pages = {1667-1676},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.224},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922011085},
author = {Matthias Sesboüé and Nicolas Delestre and Jean-Philippe Kotowicz and Ali Khudiyev and Cecilia Zanni-Merk},
keywords = {Knowledge Graph, Ontology, Knowledge-Based System, Semantic},
abstract = {Knowledge Graphs (KG) are gaining in popularity recently, notably since big tech giants announced they are using the technology. While the term is becoming popular, it is not new, and its ideas are even older. The research community has extensively studied knowledge Graphs in their various forms. Furthermore, the approach has been applied and proved valuable in many different applications. However, we found a lack of papers presenting the integration of KGs in a system regardless of the downstream application. We explore how KGs can fit in an overall information system independently from any specific use case, i.e., what we will consider knowledge consumption. We propose an architecture to understand better the KG roles within a system and how they can be integrated and implemented in a business context. We introduce each element of the latter architecture and discuss some candidate technology to implement them. Our work implements Knowledge Graph-Based Systems considering the constraints of a small to medium-sized enterprise.}
}
@article{BENABDALLAH2020953,
title = {Personalized cloud service review analysis based on modularized ontology},
journal = {Online Information Review},
volume = {44},
number = {5},
pages = {953-975},
year = {2020},
issn = {1468-4527},
doi = {https://doi.org/10.1108/OIR-06-2019-0207},
url = {https://www.sciencedirect.com/science/article/pii/S1468452720000414},
author = {Emna Ben-Abdallah and Khouloud Boukadi and Mohamed Hammami and Mohamed Hedi Karray},
keywords = {Cloud service, Context, Opinion analysis, Modular ontologies},
abstract = {Purpose
The purpose of this paper is to analyze cloud reviews according to the end-user context and requirements.
Design/methodology/approach
propose a comprehensive knowledge base composed of interconnected Web Ontology Language, namely, modular ontology for cloud service opinion analysis (SOPA). The SOPA knowledge base will be the basis of context-aware cloud service analysis using consumers' reviews. Moreover, the authors provide a framework to evaluate cloud services based on consumers' reviews opinions.
Findings
The findings show that there is a positive impact of personalizing the cloud service analysis by considering the reviewers' contexts in the performance of the framework. The authors also proved that the SOPA-based framework outperforms the available cloud review sites in term of precision, recall and F-measure.
Research limitations/implications
Limited information has been provided in the semantic web literature about the relationships between the different domains and the details on how that can be used to evaluate cloud service through consumer reviews and latent opinions. Furthermore, existing approaches are lacking lightweight and modular mechanisms which can be utilized to effectively exploit information existing in social media.
Practical implications
The SOPA-based framework facilitates the opinion based service evaluation through a large number of consumer's reviews and assists the end-users in analyzing services as per their requirements and their own context.
Originality/value
The SOPA ontology is capable of representing the content of a product/service as well as its related opinions, which are extracted from the customer's reviews written in a specific context. Furthermore, the SOPA-based framework facilitates the opinion based service evaluation through a large number of consumer's reviews and assists the end-users in analyzing services as per their requirements and their own context.}
}
@article{BAYOUDHI2018138,
title = {How to Repair Inconsistency in OWL 2 DL Ontology Versions?},
journal = {Data & Knowledge Engineering},
volume = {116},
pages = {138-158},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X16303172},
author = {Leila Bayoudhi and Najla Sassi and Wassim Jaziri},
keywords = {OWL 2 DL ontology, Evolution, Inconsistency, A priori approach},
abstract = {Semantic modeling knowledge formalisms, such as ontologies, have to follow the continuous evolution and changes of knowledge. However, ontology changes should never affect its consistency. Ontology needs to remain in a consistent state along its whole engineering process. In the literature, most of approaches check/repair ontology inconsistencies in an a posteriori way. In this paper, an a priori inconsistency approach was proposed to generate consistent OWL 2 DL ontology versions. It relies on the OWL 2 DL change kits, which anticipate inconsistencies upon each change request on an ontology version. The proposed approach predicts potential inconsistencies, provides an a priori repair action and applies the required changes. Consistency rules were defined and used to check logical inconsistencies, but also syntactical invalidities and style issues. A protégé plugin was implemented to validate our approach.}
}
@article{WANG2018359,
title = {Citrus ontology development based on the eight-point charter of agriculture},
journal = {Computers and Electronics in Agriculture},
volume = {155},
pages = {359-370},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917312012},
author = {Yi Wang and Ying Wang},
keywords = {Ontology, Knowledge modeling, Semantic web, Agriculture},
abstract = {Developing large-scale agricultural ontologies is a challenging and error-prone task that requires substantial effort and collaboration between domain experts and ontology developers due to the complexity of agricultural knowledge. Inspired by the Chinese Eight-Point Charter of Agriculture, i.e., Soil, Fertilization, Water, Variety, Density, Protection, Management and Tool, this paper presents an approach to modeling and integrating citrus production knowledge. Citrus domain knowledge is classified into eight categories based on the Eight-Point Charter of Agriculture, and the relationships in each category and among categories are established. The eight categories and the relationships are defined as the citrus production knowledge framework. Then, we propose mechanisms to develop citrus ontology based on the citrus production knowledge framework. The Fertilization ontology is created as an illustration of our approach, which contains 866 ontology entities and 12,583 Resource Description Framework triples. The structural evaluation results of the eight metrics for the Fertilization ontology are considerably better than the average and median values of 1413 Web ontologies. In addition, four antipatterns were used to evaluate the ontology, and no occurrence of the antipatterns was detected for the 866 ontology entities. The accuracy of the ontology is ensured by the competency evaluation of the 110 questions with 88% accuracy. Our approach provides an effective solution for modeling complex agricultural knowledge and transforming the agriculture domain knowledge into computable resources.}
}
@article{GALADIMA2025103970,
title = {Evaluating Incident Response in CSIRTs using Cube Socio-technical Systems Analysis},
journal = {Computer Standards & Interfaces},
volume = {93},
pages = {103970},
year = {2025},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2024.103970},
url = {https://www.sciencedirect.com/science/article/pii/S0920548924001399},
author = {Haula Sani Galadima and Cormac Doherty and Nick McDonald and Junli Liang and Rob Brennan},
keywords = {Incident response, Computer Security Incident Response Team (CSIRT), Cube Socio-technical Systems Analysis (STSA), Access Risk Knowledge (ARK) platform},
abstract = {This paper provides a novel method for evaluating Incident Response (IR) teams through the application of the Cube Socio-technical Systems Analysis (STSA) methodology. Cube is a form of structured Human Factors enquiry and has previously been successfully applied in both aviation and healthcare. By utilising STSA, this study aims to understand and evaluate incident knowledge across the IR socio-technical domain. Traditional approaches to IR improvement often focus solely on technical aspects, neglecting social factors that may significantly influence IR effectiveness. This research presents the results of extending the ARK platform for a cybersecurity IR Cube STSA of IR activities in a case study involving a large, accredited Computer Security Incident Response Team (CSIRT). It evaluates the IR system and team needs before the development of a technological intervention to improve IR learning and preparation capabilities. We present an extended Cube questionnaire, that defines specialised IR questions, an ontology, and terminology for the cybersecurity domain based on the ISO27000 series of standards. The case study demonstrates the ARK platform's capability to capture and analyse IR systems using a Multi-stage Cube STSA analysis shared in a reusable knowledge graph based on W3C standards. This provides a shared knowledge base based on FAIR (Findable, Accessible, Interoperable, Reusable) linked data, that may support generation of training materials, playbooks, and best practices to enhance IR capabilities and CSIRT operations. We show how this approach provides new insights and reusable artefacts for CSIRTs to enhance organisational cyber resilience and learning.}
}
@article{WATROBSKI20203345,
title = {Towards standardization in frameworks, tools and approaches dedicated to ontology building and management},
journal = {Procedia Computer Science},
volume = {176},
pages = {3345-3355},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.063},
url = {https://www.sciencedirect.com/science/article/pii/S187705092031958X},
author = {Jarosław Wątróbski},
keywords = {Ontology integration, Ontology merging, alignement, Knowledge repository, Ontology},
abstract = {The widespread popularity and spread of ontologies in many areas increasingly requires the successful integration of multiple such ontologies. The ontology integration problem has been investigated during last years and it is still a challenging task. Aligning and merging existing ontologies, which are usually handled manually, is often a big and tedious part of the sharing process. Due to the fact that there are different approaches, tools and strategies that help to avoid mistakes during modeling especially large domains and creating a new single coherent ontology, this article attempts to build a common knowledge repository in the form of an open and publicly available tool. The overall goal of the proposed open knowledge repository is to provide free access to basic data and to compare the features offered. It is worth mentioning that this form of knowledge representation is publicly available and focuses on the possibility of updating and reusing. It is an attempt to structure knowledge that can be successfully extended with additional groups of solutions and parameters. The proposed approach can be easily implemented as a light library and used in any type of OWL/RDF or OWL/XML management application or adapted in a given management information system.}
}
@article{KUO2023105462,
title = {An ontology-based framework for semantic geographic information systems development and understanding},
journal = {Computers & Geosciences},
volume = {181},
pages = {105462},
year = {2023},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105462},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001668},
author = {Chiao-Ling Kuo and Han-Chuan Chou},
keywords = {Ontology, GIS components, Semantic GIS development, System comprehensibility, WebGIS, Online thematic mapping},
abstract = {Geographic information systems (GIS) are a widely used approach for geodata manipulation, analysis, and geoinformation visualization. Although the issue of semantic heterogeneity for GIS applications has been widely addressed, the development of flexible semantic GIS systems can still be improved, and GIS interfaces can be further unified to enhance system understanding among users of various GIS systems. This study proposes an ontology-based semantic GIS conceptual framework that can seamlessly combine ontology models, GIS elements and user actions for semantic GIS application development. In addition, a GIS components ontology that encompasses the five fundamental components of GIS is proposed to integrate GIS elements that can perceive GIS operations and processes semantically and to support customized GIS system designs. Moreover, a semantic WebGIS implementation framework is designed on the basis of the proposed conceptual framework to facilitate semantic WebGIS development. The implementation uses two common application scenarios regarding urban change analysis and user interaction. The former performs semantic retrieval for urban change analysis with system understanding, whereas the latter conducts online thematic mapping with semantic interpretation. The study contributes in terms of innovation in the design of a unified GIS components ontology and successful implementation of a semantic GIS system using the ontology-driven approach, thereby providing a convincing demonstration of the latter and showing the potential for semantic GIS application paradigms.}
}
@article{MCGLINN2022105313,
title = {FAIRVASC: A semantic web approach to rare disease registry integration},
journal = {Computers in Biology and Medicine},
volume = {145},
pages = {105313},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105313},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522001056},
author = {Kris McGlinn and Matthew A. Rutherford and Karl Gisslander and Lucy Hederman and Mark A. Little and Declan O'Sullivan},
keywords = {Knowledge engineering, Linked data, Ontologies, Federated queries, Rare diseases},
abstract = {Rare disease data is often fragmented within multiple heterogeneous siloed regional disease registries, each containing a small number of cases. These data are particularly sensitive, as low subject counts make the identification of patients more likely, meaning registries are not inclined to share subject level data outside their registries. At the same time access to multiple rare disease datasets is important as it will lead to new research opportunities and analysis over larger cohorts. To enable this, two major challenges must therefore be overcome. The first is to integrate data at a semantic level, so that it is possible to query over registries and return results which are comparable. The second is to enable queries which do not take subject level data from the registries. To meet the first challenge, this paper presents the FAIRVASC ontology to manage data related to the rare disease anti-neutrophil cytoplasmic antibody (ANCA) associated vasculitis (AAV), which is based on the harmonisation of terms in seven European data registries. It has been built upon a set of key clinical questions developed by a team of experts in vasculitis selected from the registry sites and makes use of several standard classifications, such as Systematized Nomenclature of Medicine - Clinical Terms (SNOMED-CT) and Orphacode. It also presents the method for adding semantic meaning to AAV data across the registries using the declarative Relational to Resource Description Framework Mapping Language (R2RML). To meet the second challenge a federated querying approach is presented for accessing aggregated and pseudonymized data, and which supports analysis of AAV data in a manner which protects patient privacy. For additional security the federated querying approach is augmented with a method for auditing queries (and the uplift process) using the provenance ontology (PROV-O) to track when queries and changes occur and by whom. The main contribution of this work is the successful application of semantic web technologies and federated queries to provide a novel infrastructure that can readily incorporate additional registries, thus providing access to harmonised data relating to unprecedented numbers of patients with rare disease, while also meeting data privacy and security concerns.}
}
@article{DENICOLA2023100489,
title = {Development and measurement of a resilience indicator for cyber-socio-technical systems: The allostatic load},
journal = {Journal of Industrial Information Integration},
volume = {35},
pages = {100489},
year = {2023},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2023.100489},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X23000626},
author = {Antonio {De Nicola} and Maria Luisa Villani and Mark Sujan and John Watt and Francesco Costantino and Andrea Falegnami and Riccardo Patriarca},
keywords = {Resilience, Cyber-socio-technical system, Ontology, Semantic similarity, Business process, Leading indicator},
abstract = {Management of cyber-socio-technical processes often suffers from misalignments of process descriptions according to formal organization documents or manager views (Work-As-Imagined) with actual work practices as performed by sharp-end operators (Work-As-Done). Even if sometimes the accomplishment of a process requires workers to diverge from the Work-As-Imagined, the corresponding changes can potentially cause organizational tensions in the overall system and lead to safety incidents. This consideration led us to define a new resilience indicator, named allostatic load, to capture such misalignments, and the corresponding level of organizational tensions, a cyber-socio-technical system is exposed to. Then, we propose a method to measure it by leveraging semantic technologies, the Functional Resonance Analysis Method (FRAM) to model industrial processes, the WAx conceptual framework to keep track of the variety of the different process perspectives, and a crowd-based approach to elicit industrial knowledge. Finally, we discuss the feasibility of the approach in two real case studies related to a pharmaceutical manufacturing plant and an enterprise in the aluminium sector.}
}
@article{JARVENPAA2021435,
title = {Capability matchmaking software for rapid production system design and reconfiguration planning},
journal = {Procedia CIRP},
volume = {97},
pages = {435-440},
year = {2021},
note = {8th CIRP Conference of Assembly Technology and Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.264},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120314864},
author = {Eeva Järvenpää and Niko Siltala and Otto Hylli and Minna Lanz},
keywords = {Production system design, Production system reconfiguration, Capability matchmaking, Matchmaking software, Resource modelling, Ontology},
abstract = {Traditionally, the production system design and reconfiguration planning are manual processes, which rely heavily on the designers’ expertise and tacit knowledge to find feasible system configuration solutions. Rapid responsiveness of future production systems calls for new computer-aided intelligent design and planning solutions, that would reduce the time and effort put into system design, both in brownfield and greenfield scenarios. This paper describes the implementation of a capability matchmaking software, which automatizes the matchmaking between product requirements and resource capabilities. The interaction of the matchmaking system with external design and planning tools is explained and illustrated with a case example. The matchmaking approach supports production system design and reconfiguration planning by providing automatic means for checking if the existing system already fulfills the new product requirements, and for finding alternative resources and resource combinations to specific product requirements from large search spaces, e.g. from global resource catalogues.}
}
@article{VELPULA2022108037,
title = {CEECP: CT-based enhanced e-clinical pathways in terms of processing time to enable big data analytics in healthcare along with cloud computing},
journal = {Computers & Industrial Engineering},
volume = {168},
pages = {108037},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108037},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222001073},
author = {Prasad Velpula and Rajendra Pamula},
keywords = {Artificial Intelligence, Big Data, Clinical Path (CP), Electronic health (e-Health), Electronic Medical Record (EMR), Electronic Health Record (EHR), Information Technology (IT), LoS (Length of Stay), Systematized Nomenclature of Medicine-Clinical Terms (SNOMED CT)},
abstract = {Clinical pathway is generally used as a tool for the implementation of evidence-based medicine and clinical guidelines. The e-clinical pathway is helpful in clinical cases covering both diagnostic and therapeutic fields. Healthcare facility automation is a challenging task to streamline a high informatics-intensive sector. With the help of datasets which were collected on LOS, CP, and EHR for hearth patient is taken from Kaggle data sets were collected, a better solution to this existing problem is proposed in this paper in the form of automation of CP, with the help of SNOMED-CT which not only helps to study the correct health situation of the patient and also helpful to estimate the correct length of stay in hospitals and to minimize the expenditures. This paper has carried out an extensive highlighted review for the current trending mechanism and approaches for smartly tackling privacy and security issues along with suggesting short-term stay in hospitals, thereby a cost reduction. In our proposed method, a new cost dynamic approach is suggested, which is a unique method for the exact analysis of data. The proposed CEECP method achieved less average processing time of 1116.6 µs and achieved accuracy of 99.01%, which is higher than existing methods. The proposed method achieved higher values for other factors such as accuracy, precision, sensitivity, and specificity.}
}
@article{JIANG2023104728,
title = {Intelligent control of building fire protection system using digital twins and semantic web technologies},
journal = {Automation in Construction},
volume = {147},
pages = {104728},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104728},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522005982},
author = {Liu Jiang and Jianyong Shi and Chaoyu Wang and Zeyu Pan},
keywords = {Digital twins (DTs), Sematic web, Ontology, Building automated system, Fire protection system, Intelligent control, Building information modelling (BIM)},
abstract = {A fire protection system takes on a critical significance to building operation. This paper describes the use of digital twins (DTs) and semantic web technologies for the intelligent control of building fire protection (BFP) systems in fire accidents. Specifically, a data fusion stage and several information-based control mechanisms are involved in the use of the two technologies. A designed BFP ontology is considered as the semantic model and basis of data fusion between the static building geometric information and the dynamic sensing data. The above information is incorporated into a DT data model, which is considered as the mapping of physical space. Moreover, rule models and process models are developed to achieve intelligent control mechanisms and keep the DT data model synced with the physical space. A case study based on a fire accident simulation was conducted to verify the feasibility of the use of DTs and semantic web technologies.}
}
@article{SONG2025105706,
title = {Knowledge graph-based alarm management in petrochemical enterprises: A study on fusion and analysis of multi-source heterogeneous information},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {97},
pages = {105706},
year = {2025},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2025.105706},
url = {https://www.sciencedirect.com/science/article/pii/S0950423025001640},
author = {Xiaomiao Song and Fabo Yin and Dongfeng Zhao},
keywords = {Petrochemical enterprises, Alarm management, Knowledge graph, Ontology},
abstract = {In response to the increasing emphasis on alarm management in the petrochemical industry, there has been an explosive growth in relevant information. However, this information is often scattered across different systems and databases, stored in various forms such as documents, tables, and images, making it challenging to uniformly store, share, and utilize multi-source heterogeneous information. This commonly leads to the problem of “Information Islands.” In order to effectively leverage knowledge in the field of alarm management in the petrochemical industry and overcome the challenge of non-interoperable information, a method for fusing multi-source heterogeneous information in petrochemical enterprise alarm management based on knowledge graph is proposed. This method aims to standardize the management of alarm-related information and achieve information fusion. Initially, the approach utilizes data from petrochemical enterprises and publicly available data in the field of alarm management to establish both local and global ontologies. Subsequently, mapping algorithms are designed to achieve a more accurate construction of the hybrid ontology. Based on this foundation, a knowledge graph for alarm management in the petrochemical industry is established. Additionally, corresponding modules for information storage and retrieval are developed. Through the application demonstration using real alarm management information from a petrochemical enterprise, the results indicate that the proposed method for fusing multi-source heterogeneous information in petrochemical enterprise alarm management can effectively achieve information fusion.}
}
@article{EIVAZZADEH2018,
title = {Most Influential Qualities in Creating Satisfaction Among the Users of Health Information Systems: Study in Seven European Union Countries},
journal = {JMIR Medical Informatics},
volume = {6},
number = {4},
year = {2018},
issn = {2291-9694},
doi = {https://doi.org/10.2196/11252},
url = {https://www.sciencedirect.com/science/article/pii/S2291969418000613},
author = {Shahryar Eivazzadeh and Johan S Berglund and Tobias C Larsson and Markus Fiedler and Peter Anderberg},
keywords = {health information systems, telemedicine, evaluation studies as topic, consumer behavior, treatment outcome, safety, efficiency, health care costs, ontology engineering, equation models},
abstract = {Background
Several models suggest how the qualities of a product or service influence user satisfaction. Models such as the Customer Satisfaction Index (CSI), Technology Acceptance Model (TAM), and Delone and McLean Information Systems Success demonstrate those relations and have been used in the context of health information systems.
Objective
This study aimed to investigate which qualities foster greater satisfaction among patient and professional users. In addition, we are interested in knowing to what extent improvement in those qualities can explain user satisfaction and whether this makes user satisfaction a proxy indicator of those qualities.
Methods
The Unified eValuation using ONtology (UVON) method was used to construct an ontology of the required qualities for 7 electronic health (eHealth) apps being developed in the Future Internet Social and Technological Alignment Research (FI-STAR) project, a European Union (EU) project in electronic health (eHealth). The eHealth apps were deployed across 7 EU countries. The ontology included and unified the required qualities of those systems together with the aspects suggested by the Model for ASsessment of Telemedicine apps (MAST) evaluation framework. Moreover, 2 similar questionnaires for 87 patient users and 31 health professional users were elicited from the ontology. In the questionnaires, the user was asked if the system has improved the specified qualities and if the user was satisfied with the system. The results were analyzed using Kendall correlation coefficients matrices, incorporating the quality and satisfaction aspects. For the next step, 2 partial least squares structural equation modeling (PLS-SEM) path models were developed using the quality and satisfaction measure variables and the latent construct variables that were suggested by the UVON method.
Results
Most of the quality aspects grouped by the UVON method are highly correlated. Strong correlations in each group suggest that the grouped qualities can be measures that reflect a latent quality construct. The PLS-SEM path analysis for the patients reveals that the effectiveness, safety, and efficiency of treatment provided by the system are the most influential qualities in achieving and predicting user satisfaction. For the professional users, effectiveness and affordability are the most influential. The parameters of the PLS-SEM that are calculated allow for the measurement of a user satisfaction index similar to CSI for similar health information systems.
Conclusions
For both patients and professionals, the effectiveness of systems highly contributes to their satisfaction. Patients care about improvements in safety and efficiency, whereas professionals care about improvements in the affordability of treatments with health information systems. User satisfaction is reflected more in the users’ evaluation of system output and fulfillment of expectations but slightly less in how far the system is from ideal. Investigating satisfaction scores can be a simple and fast way to infer if the system has improved the abovementioned qualities in treatment and care.}
}
@article{CARMODY2023913,
title = {The Medical Action Ontology: A tool for annotating and analyzing treatments and clinical management of human disease},
journal = {Med},
volume = {4},
number = {12},
pages = {913-927.e3},
year = {2023},
issn = {2666-6340},
doi = {https://doi.org/10.1016/j.medj.2023.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2666634023003343},
author = {Leigh C. Carmody and Michael A. Gargano and Sabrina Toro and Nicole A. Vasilevsky and Margaret P. Adam and Hannah Blau and Lauren E. Chan and David Gomez-Andres and Rita Horvath and Megan L. Kraus and Markus S. Ladewig and David Lewis-Smith and Hanns Lochmüller and Nicolas A. Matentzoglu and Monica C. Munoz-Torres and Catharina Schuetz and Berthold Seitz and Morgan N. Similuk and Teresa N. Sparks and Timmy Strauss and Emilia M. Swietlik and Rachel Thompson and Xingmin Aaron Zhang and Christopher J. Mungall and Melissa A. Haendel and Peter N. Robinson},
keywords = {medical action ontology, MAxO, ontology, treatment, surgical procedure, clinical management, computational decision support},
abstract = {Summary
Background
Navigating the clinical literature to determine the optimal clinical management for rare diseases presents significant challenges. We introduce the Medical Action Ontology (MAxO), an ontology specifically designed to organize medical procedures, therapies, and interventions.
Methods
MAxO incorporates logical structures that link MAxO terms to numerous other ontologies within the OBO Foundry. Term development involves a blend of manual and semi-automated processes. Additionally, we have generated annotations detailing diagnostic modalities for specific phenotypic abnormalities defined by the Human Phenotype Ontology (HPO). We introduce a web application, POET, that facilitates MAxO annotations for specific medical actions for diseases using the Mondo Disease Ontology.
Findings
MAxO encompasses 1,757 terms spanning a wide range of biomedical domains, from human anatomy and investigations to the chemical and protein entities involved in biological processes. These terms annotate phenotypic features associated with specific disease (using HPO and Mondo). Presently, there are over 16,000 MAxO diagnostic annotations that target HPO terms. Through POET, we have created 413 MAxO annotations specifying treatments for 189 rare diseases.
Conclusions
MAxO offers a computational representation of treatments and other actions taken for the clinical management of patients. Its development is closely coupled to Mondo and HPO, broadening the scope of our computational modeling of diseases and phenotypic features. We invite the community to contribute disease annotations using POET (https://poet.jax.org/). MAxO is available under the open-source CC-BY 4.0 license (https://github.com/monarch-initiative/MAxO).
Funding
NHGRI 1U24HG011449-01A1 and NHGRI 5RM1HG010860-04.}
}
@article{ALKHARIJI2023280,
title = {Semantics-based privacy by design for Internet of Things applications},
journal = {Future Generation Computer Systems},
volume = {138},
pages = {280-295},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22002746},
author = {Lamya Alkhariji and Suparna De and Omer Rana and Charith Perera},
keywords = {Privacy, Privacy by Design, Internet of Things, Semantic web, Ontology, Context awareness},
abstract = {As Internet of Things (IoT) technologies become more widespread in everyday life, privacy issues are becoming more prominent. The aim of this research is to develop a personal assistant that can answer software engineers’ questions about Privacy by Design (PbD) practices during the design phase of IoT system development. Semantic web technologies are used to model the knowledge underlying PbD measurements, their intersections with privacy patterns, IoT system requirements and the privacy patterns that should be applied across IoT systems. This is achieved through the development of the PARROT ontology, developed through a set of representative IoT use cases relevant for software developers. This was supported by gathering Competency Questions (CQs) through a series of workshops, resulting in 81 curated CQs. These CQs were then recorded as SPARQL queries, and the developed ontology was evaluated using the Common Pitfalls model with the help of the Protégé HermiT Reasoner and the Ontology Pitfall Scanner (OOPS!), as well as evaluation by external experts. The ontology was assessed within a user study that identified that the PARROT ontology can answer up to 58% of privacy-related questions from software engineers.}
}
@article{QUEK2024109507,
title = {Dynamic knowledge graph applications for augmented built environments through “The World Avatar”},
journal = {Journal of Building Engineering},
volume = {91},
pages = {109507},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.109507},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224010751},
author = {Hou Yee Quek and Markus Hofmeister and Simon D. Rihm and Jingya Yan and Jiawei Lai and George Brownbridge and Michael Hillman and Sebastian Mosbach and Wilson Ang and Yi-Kai Tsai and Dan N. Tran and Soon Kang, William Tan and Markus Kraft},
keywords = {BIM, GIS, Interoperability, Built environment, Knowledge graph},
abstract = {The proliferation of digital building models in recent years has led to a corresponding rise in specialised, non-interoperable models. These models impede sustainable developments by forming data silos that hinder cross-application data exchange and knowledge discovery processes. Although Semantic Web solutions hold promise in addressing these silos, current approaches primarily focus on developing novel ontologies, yielding similar outcomes. But it is unclear how these methodologies could support broader knowledge discovery processes and application requirements. This paper addresses these research challenges by introducing a dynamic knowledge graph as implemented within The World Avatar for interoperable building models. We demonstrate its value through two distinct applications in urban energy management and laboratory automation. The dynamic knowledge graph revolves around a comprehensive structured knowledge model constructed from ontologies and agents. Ontologies semantically annotate data and represent domain knowledge and their relationships with standardised definitions. When augmented with an agent architecture, the resulting knowledge model can align stakeholder perspectives and accommodate the dynamic and scalable nature of urban data. Moreover, the dynamic knowledge graph fosters innovative human-machine interactions through visualisation interfaces to augment knowledge discovery processes in the built environment for greater efficiencies and innovation. As the knowledge model expands, users gain access to a broader spectrum of private and public data sources and technologies, while reducing integration barriers. This is especially pertinent for smaller and less influential entities like municipal and local governments with limited resources, who can realise substantial benefits at reduced costs.}
}
@article{HIPPOLYTE2018210,
title = {Ontology-driven development of web services to support district energy applications},
journal = {Automation in Construction},
volume = {86},
pages = {210-225},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517308907},
author = {J.-L. Hippolyte and Y. Rezgui and H. Li and B. Jayan and S. Howell},
keywords = {Ontology design, Domain engineering, Software development, Intelligent web services, Semantic web},
abstract = {Current urban and district energy management systems lack a common semantic referential for effectively interrelating intelligent sensing, data models and energy models with visualization, analysis and decision support tools. This paper describes the structure, as well as the rationale that led to this structure, of an ontology that captures the real-world concepts of a district energy system, such as a district heating and cooling system. This ontology (called ee-district ontology) is intended to support knowledge provision that can play the role of an intermediate layer between high-level energy management software applications and local monitoring and control software components. In order to achieve that goal, the authors propose to encapsulate queries to the ontology in a scalable web service, which will facilitate the development of interfaces for third-party applications. Considering the size of the ee-district ontology once populated with data from a specific district case study, this could prove to be a repetitive and time-consuming task for the software developer. This paper therefore assesses the feasibility of ontology-driven automation of web service development that is to be a core element in the deployment of heterogeneous district-wide energy management software.}
}
@article{TURCHET2020100548,
title = {The Internet of Musical Things Ontology},
journal = {Journal of Web Semantics},
volume = {60},
pages = {100548},
year = {2020},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100548},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300019},
author = {Luca Turchet and Francesco Antoniazzi and Fabio Viola and Fausto Giunchiglia and György Fazekas},
keywords = {Internet of Musical Things, Smart musical instruments, Semantic audio},
abstract = {The Internet of Musical Things (IoMusT) is an emerging research area consisting of the extension of the Internet of Things paradigm to the music domain. Interoperability represents a central issue within this domain, where heterogeneous objects dedicated to the production and/or reception of musical content (Musical Things) are envisioned to communicate between each other. This paper proposes an ontology for the representation of the knowledge related to IoMusT ecosystems to facilitate interoperability between Musical Things. There was no previous comprehensive data model for the IoMusT domain, however the new ontology relates to existing ontologies, including the SOSA Ontology for the representation of sensors and actuators and the Music Ontology focusing on the production and consumption of music. This paper documents the design of the ontology and its evaluation with respect to specific requirements gathered from an extensive literature review, which was based on scenarios involving IoMusT stakeholders, such as performers and audience members. The IoMusT Ontology can be accessed at: https://w3id.org/iomust#.}
}
@article{BUREK2019784,
title = {A pattern-based approach to a cell tracking ontology.},
journal = {Procedia Computer Science},
volume = {159},
pages = {784-793},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.237},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314243},
author = {Patryk Burek and Nico Scherf and Heinrich Herre},
keywords = {Semantic-Based Systems, Knowledge Representation, Management, Semantic annotation of images, videos, Ontologies},
abstract = {Time-lapse microscopy has thoroughly transformed our understanding of biological motion and developmental dynamics from single cells to entire organisms. The increasing amount of cell tracking data demands the creation of tools to make extracted data searchable and interoperable between experiment and data types. In order to address that problem, the current paper reports on the progress in building the Cell Tracking Ontology (CTO): An ontology framework for describing, querying and integrating data from complementary experimental techniques in the domain of cell tracking experiments. CTO is based on a basic knowledge structure: the cellular genealogy serving as a backbone model to integrate specific biological ontologies into tracking data. As a first step we integrate the Phenotype and Trait Ontology (PATO) as one of the most relevant ontologies to annotate cell tracking experiments. The CTO requires both the integration of data on various levels of generality as well as the proper structuring of collected information. Therefore, in order to provide a sound foundation of the ontology, we have built on the rich body of work on top-level ontologies and established three generic ontology design patterns addressing three modeling challenges for properly representing cellular genealogies, i.e. representing entities existing in time, undergoing changes over time and their organization into more complex structures such as situations.}
}
@article{GONZALEZSENDINO2025109979,
title = {Quantifying algorithmic discrimination: A two-dimensional approach to fairness in artificial intelligence},
journal = {Engineering Applications of Artificial Intelligence},
volume = {144},
pages = {109979},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109979},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624021389},
author = {Rubén González-Sendino and Emilio Serrano and Javier Bajo},
keywords = {Fairness, Equity, Equality, Bias detection},
abstract = {Fairness is a foundational pillar in the development of ethical and responsible artificial intelligence. One of the most pressing issues in this context is discrimination, which occurs when an algorithm displays unequal treatment towards data from different groups without an objective justification for such disparity. Artificial intelligence is increasingly taking on decision-making roles in society at large and in engineering fields in particular. In domains such as autonomous vehicle control systems, where unbiased decision-making can impact safety and trust, and in smart grid management, where equitable energy distribution is crucial, fairness must be a primary consideration. This study introduces a novel metric to measure fairness, consisting of a two-dimensional vector: Equality and Equity. When applied to benchmark datasets, this metric demonstrated superior informativeness by effectively distinguishing equality-related issues from equity-related challenges, surpassing traditional methods like Disparate Impact. Contributions of this work include (1) a pioneering metric for measuring equity; (2) a pure measure of fairness definition that takes into account equity and equality; (3) a vector to guide the mitigation algorithms; and, (4) a Fairness curve where the disparities between groups can be interpreted and explained.}
}
@article{ROCHA2018373,
title = {DKDOnto: An Ontology to Support Software Development with Distributed Teams},
journal = {Procedia Computer Science},
volume = {126},
pages = {373-382},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.271},
url = {https://www.sciencedirect.com/science/article/pii/S187705091831247X},
author = {Rodrigo Rocha and Arthur Araújo and Diogo Cordeiro and Assuero Ximenes and Jean Teixeira and Gabriel Silva and Daliton da Silva and Diogo Espinhara and Renan Fernandes and João Ambrosio and Marcos Duarte and Ryan Azevedo},
keywords = {Distributed Software Development, Ontology, Software Engineering},
abstract = {The Distributed Software Development has become an option for software companies to expand their perspective and work with dispersed teams, exploiting the advantages brought by this approach. However, this way of developing software enables new challenges to arise, such as the inexistence of a formal, normalized model of a project’s data and artifacts accessible to all the individuals involved, which makes it harder for them to communicate, understand each other and what is specified on the project’s artifacts. This paper proposes a knowledge base called DKDOnto, a domain-specific ontology for distributed development, aiming to help projects with a common vocabulary, allowing to assist better the distributed software development process.}
}
@article{RASHID201948,
title = {Completeness and consistency analysis for evolving knowledge bases},
journal = {Journal of Web Semantics},
volume = {54},
pages = {48-71},
year = {2019},
note = {Managing the Evolution and Preservation of the Data Web},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300623},
author = {Mohammad Rifat Ahmmad Rashid and Giuseppe Rizzo and Marco Torchiano and Nandana Mihindukulasooriya and Oscar Corcho and Raúl García-Castro},
keywords = {Quality assessment, Evolution analysis, Validation, Knowledge base, RDF shape, Machine learning},
abstract = {Assessing the quality of an evolving knowledge base is a challenging task as it often requires to identify correct quality assessment procedures. Since data is often derived from autonomous, and increasingly large data sources, it is impractical to manually curate the data, and challenging to continuously and automatically assess their quality. In this paper, we explore two main areas of quality assessment related to evolving knowledge bases: (i) identification of completeness issues using knowledge base evolution analysis, and (ii) identification of consistency issues based on integrity constraints, such as minimum and maximum cardinality, and range constraints. For the completeness analysis, we use data profiling information from consecutive knowledge base releases to estimate completeness measures that allow predicting quality issues. Then, we perform consistency checks to validate the results of the completeness analysis using integrity constraints and learning models. The approach has been tested both quantitatively and qualitatively by using a subset of datasets from both DBpedia and 3cixty knowledge bases. The performance of the approach is evaluated using precision, recall, and F1 score. From completeness analysis, we observe a 94% precision for the English DBpedia KB and 95% precision for the 3cixty Nice KB. We also assessed the performance of our consistency analysis by using five learning models over three sub-tasks, namely minimum cardinality, maximum cardinality, and range constraint. We observed that the best performing model in our experimental setup is Random Forest, reaching an F1 score greater than 90% for minimum and maximum cardinality and 84% for range constraints.}
}
@article{POUR20231415,
title = {Phrase2Onto: A Tool to Support Ontology Extension},
journal = {Procedia Computer Science},
volume = {225},
pages = {1415-1424},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.130},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923012887},
author = {Mina Abd Nikooie Pour and Huanyu Li and Rickard Armiento and Patrick Lambrix},
keywords = {Ontology extension, Concept discovery, phrase-based topic model},
abstract = {Due to importance of data FAIRness (Findable, Accessible, Interoperable, Reusable), ontologies as a means to make data FAIR have attracted more and more attention in different communities and are being used in semantically-enabled applications. However, to obtain good results while using ontologies in these applications, high quality ontologies are needed of which completeness is one of the important aspects. An ontology lacking information can lead to missing results. In this paper we present a tool, Phrase2Onto, that supports users in extending ontologies to make the ontologies more complete. It is particularly suited for ontology extension using a phrase-based topic model approach, but the tool can support any extension approach where a user needs to make decisions regarding the appropriateness of using phrases to define new concepts. We describe the functionality of the tool and a user study using Pizza Ontology. The user study showed a good usability of the system and high task completion. Further, we report on a real application where we extend the Materials Design Ontology.}
}
@article{QIN202396,
title = {A Knowledge Graph-based knowledge representation for adaptive manufacturing control under mass personalization},
journal = {Manufacturing Letters},
volume = {35},
pages = {96-104},
year = {2023},
note = {51st SME North American Manufacturing Research Conference (NAMRC 51)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2023.08.086},
url = {https://www.sciencedirect.com/science/article/pii/S2213846323001438},
author = {Zhaojun Qin and Yuqian Lu},
keywords = {Mass personalization, Smart manufacturing, Self-organizing manufacturing network, Knowledge Graph, Adaptive production scheduling},
abstract = {Mass personalization is an achievable manufacturing paradigm, which requires flexible and responsible manufacturing operations in response to dynamic batch sizes of personalized products. A Self-Organizing Manufacturing Network (SOMN) has been proposed to achieve mass personalization. A crucial aspect of SOMN is adaptive manufacturing control, and the Knowledge Graph, a powerful tool, has been recognized as a promising solution to enhance manufacturing intelligence. However, the current Knowledge Graph research mainly focuses on the modeling and ontology definition of the manufacturing environment, but neglects the interaction between manufacturing resources, the dynamic features of the manufacturing environment, and the application of the Knowledge Graph towards adaptive manufacturing control. Therefore, this paper proposes a Knowledge Graph-based semantic representation for adaptive manufacturing control under dynamic manufacturing environments. The proposed approach develops the Knowledge Graph based on historical and real-time scheduling data. Based on the established Knowledge Graph, Multi-Agent Reinforcement Learning has been introduced as an illustrative example of achieving adaptive scheduling control.}
}
@article{JCARLOS2025734,
title = {Semantic Mediation: a literature review on semantic interoperability through ontologies},
journal = {Procedia Computer Science},
volume = {263},
pages = {734-743},
year = {2025},
note = {International Conference on Industry Sciences and Computer Science Innovation (iSCSi’24)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.07.088},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925021386},
author = {Martins {J. Carlos} and Baptista {Ana Alice} and Sousa {Rui M.} and Martins {Paulo J.}},
keywords = {Semantic Mediation, Semantic Interoperability, Ontology, Information Systems, Integration},
abstract = {Organisations operate in increasingly complex and dynamic environments where different Information Systems (IS) must interact efficiently and quickly. The diversity of meanings or interpretations of data in different contexts present in these IS raises problems of semantic heterogeneity, an obstacle to their integration and synchronisation. Through Semantic Mediation, it is possible to achieve the goal of Semantic Interoperability, where different informatic systems can interact and understand each other’s data in an efficient and meaningful way. This can help solve the problem of semantic heterogeneity, using a common ontology for the syntactic and semantic representation of information, and creating a semantic mediator. This study presents a systematic review of the literature on the use of ontologies to achieve semantic interoperability between informatic systems in order to determine the relevance of the Semantic Mediation Model. Databases containing articles published between 2013 and September 2024 were searched. The use of ontologies to facilitate semantic interoperability was present in 66% of the articles in this review. The most important research contexts and relevant case studies were then identified. These case study articles will be included in future research.}
}
@article{ALI202123,
title = {An intelligent healthcare monitoring framework using wearable sensors and social networking data},
journal = {Future Generation Computer Systems},
volume = {114},
pages = {23-43},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.07.047},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1931605X},
author = {Farman Ali and Shaker El-Sappagh and S.M. Riazul Islam and Amjad Ali and Muhammad Attique and Muhammad Imran and Kyung-Sup Kwak},
keywords = {Machine learning, Semantic knowledge, Big data analysis, Healthcare monitoring system, Wearable sensors, Social network analysis},
abstract = {Wearable sensors and social networking platforms play a key role in providing a new method to collect patient data for efficient healthcare monitoring. However, continuous patient monitoring using wearable sensors generates a large amount of healthcare data. In addition, the user-generated healthcare data on social networking sites come in large volumes and are unstructured. The existing healthcare monitoring systems are not efficient at extracting valuable information from sensors and social networking data, and they have difficulty analyzing it effectively. On top of that, the traditional machine learning approaches are not enough to process healthcare big data for abnormality prediction. Therefore, a novel healthcare monitoring framework based on the cloud environment and a big data analytics engine is proposed to precisely store and analyze healthcare data, and to improve the classification accuracy. The proposed big data analytics engine is based on data mining techniques, ontologies, and bidirectional long short-term memory (Bi-LSTM). Data mining techniques efficiently preprocess the healthcare data and reduce the dimensionality of the data. The proposed ontologies provide semantic knowledge about entities and aspects, and their relations in the domains of diabetes and blood pressure (BP). Bi-LSTM correctly classifies the healthcare data to predict drug side effects and abnormal conditions in patients. Also, the proposed system classifies the patients’ health condition using their healthcare data related to diabetes, BP, mental health, and drug reviews. This framework is developed employing the Protégé Web Ontology Language tool with Java. The results show that the proposed model precisely handles heterogeneous data and improves the accuracy of health condition classification and drug side effect predictions.}
}
@article{BUREK20211021,
title = {Overview of GFO 2.0 Functions: An ontology module for representing teleological knowledge},
journal = {Procedia Computer Science},
volume = {192},
pages = {1021-1030},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.105},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015933},
author = {Patryk Burek and Frank Loebe and Heinrich Herre},
keywords = {Ontology, Knowledge Representation, Functional Modeling},
abstract = {Teleological knowledge and functional representation are present in numerous scientific and engineering disciplines and are key aspects addressed by a broad spectrum of design and modeling frameworks. The current paper summarizes the results obtained in the area of functional modeling achieved under the umbrella of the second release of the General Formal Ontology (GFO 2.0), which is a modular top-level ontological framework actively developed by the Onto-Med Research Group. This paper presents and discusses the Function Module of GFO 2.0, which serves three objectives. (1) It supports the representation of teleological components, let it be a function or a goal-oriented process, by a blueprint of structured specifications. (2) It provides a classification of relationships enabling the construction of functional decomposition models. (3) The module enables the description of the elements of a domain in functional terms by means of a family of function ascription constructs, which handle the modeling of not only function-to-object assignments, but also of malfunctions.}
}
@article{COLLINGE2022104391,
title = {BIM-based construction safety risk library},
journal = {Automation in Construction},
volume = {141},
pages = {104391},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104391},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002643},
author = {William H. Collinge and Karim Farghaly and Mojgan Hadi Mosleh and Patrick Manu and Clara Man Cheung and Carlos A. Osorio-Sandoval},
keywords = {Building information modelling, BIM, Design for Safety, Prevention through design, Construction safety, Health and safety, Safety in design, Ontology, Risk scenarios},
abstract = {This paper presents a digital tool and Safety Risk library to assist designers in their health and safety work in BIM digital environments. Addressing an industry need for improved knowledge sharing and collaboration, the BIM Safety Risk library tool aligns with a Prevention through Design (PtD) approach that links safety risks to treatments via different risk scenarios. Motivated by continuing sub-optimal health and safety management processes, the research employs a conceptual framework rooted in construction guidance: structuring data via a 7-stage ontology to improve designer knowledge of issues and give access to an expanding safety knowledge base (the BIM Safety Risk Library). The tool facilitates tacit and explicit knowledge sharing in visual environments, enabling the construction industry to benefit from their health and safety data while providing an interactive learning tool for designers. The structuring of data also opens up possibilities for other digital advances (e.g. via automatic rule checking).}
}
@article{ZAPPATORE20231,
title = {Semantic models for IoT sensing to infer environment–wellness relationships},
journal = {Future Generation Computer Systems},
volume = {140},
pages = {1-17},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003211},
author = {Marco Zappatore and Antonella Longo and Angelo Martella and Beniamino {Di Martino} and Antonio Esposito and Serena Angela Gracco},
keywords = {IoT interoperability, Semantic API, Environmental sensing, Mobile Crowd Sensing, Ontology patterns},
abstract = {Every time an Internet of Things (IoT) solution is deployed, every time a smartphone owner connects her/his wireless device to a wearable activity-tracker, every time groups of citizens use geo-mapping applications to move around the city, choosing the least crowded path, data are produced and information have to be exchanged appropriately via APIs. Even if novel added-value IoT-based applications appear on the market with increasing speed, true semantic interoperability is far from being achieved, thus limiting the large-scale exploitation, the scalability and the time-to-market of novel apps. Currently, connecting different data prosumers with multiple data sources is still hampered by the lack of standardized and sustainable solutions, especially due to the significant heterogeneity of IoT platforms. In such a landscape, ontologies come to the rescue, thanks to their formal semantics, knowledge representation formats, and shared vocabularies. In this paper we examine, from an ontological perspective, how to describe environmental sensing and wellness monitoring, two of the most popular application cases of Mobile Crowd Sensing (MCS) and IoT, respectively. To this purpose, an ontology of sensor-agnostic APIs is proposed, along with a set of MCS-dedicated ontology modules (and the supporting platform), leveraging on standard and reusable domain ontologies. Moreover, it will be shown how to properly combine the proposed ontologies in order to support complex functionalities based on inference rules addressing the environment–wellness relationships. Finally, specific semantic modeling patterns suitable for typical IoT and MCS scenarios will be discussed.}
}
@article{TURKI2024e38448,
title = {A framework for integrating biomedical knowledge in Wikidata with open biological and biomedical ontologies and MeSH keywords},
journal = {Heliyon},
volume = {10},
number = {19},
pages = {e38448},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e38448},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024144799},
author = {Houcemeddine Turki and Khalil Chebil and Bonaventure F.P. Dossou and Chris Chinenye Emezue and Abraham Toluwase Owodunni and Mohamed Ali {Hadj Taieb} and Mohamed {Ben Aouicha}},
keywords = {Wikidata, Open biological and biomedical ontologies, MeSH keywords, Biomedical relation identification, Crowdsourcing, PubMed},
abstract = {This study presents a comprehensive framework to enhance Wikidata as an open and collaborative knowledge graph by integrating Open Biological and Biomedical Ontologies (OBO) and Medical Subject Headings (MeSH) keywords from PubMed publications. The primary data sources include OBO ontologies and MeSH keywords, which were collected and classified using SPARQL queries for RDF knowledge graphs. The semantic alignment between OBO ontologies and Wikidata was evaluated, revealing significant gaps and distorted representations that necessitate both automated and manual interventions for improvement. We employed pointwise mutual information to extract biomedical relations among the 5000 most common MeSH keywords in PubMed, achieving an accuracy of 89.40 % for superclass-based classification and 75.32 % for relation type-based classification. Additionally, Integrated Gradients were utilized to refine the classification by removing irrelevant MeSH qualifiers, enhancing overall efficiency. The framework also explored the use of MeSH keywords to identify PubMed reviews supporting unsupported Wikidata relations, finding that 45.8 % of these relations were not present in PubMed, indicating potential inconsistencies in Wikidata. The contributions of this study include improved methodologies for enriching Wikidata with biomedical information, validated semantic alignments, and efficient classification processes. This work enhances the interoperability and multilingual capabilities of biomedical ontologies and demonstrates the critical role of MeSH keywords in verifying semantic relations, thereby contributing to the robustness and accuracy of collaborative biomedical knowledge graphs.}
}
@article{ARVOR2021112615,
title = {Towards user-adaptive remote sensing: Knowledge-driven automatic classification of Sentinel-2 time series},
journal = {Remote Sensing of Environment},
volume = {264},
pages = {112615},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112615},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721003357},
author = {Damien Arvor and Julie Betbeder and Felipe R.G. Daher and Tim Blossier and Renan {Le Roux} and Samuel Corgne and Thomas Corpetti and Vinicius {de Freitas Silgueiro} and Carlos Antonio da {Silva Junior}},
keywords = {Land cover, Sentinel-2, Time series, Knowledge-driven, Ontologies, Amazon},
abstract = {Land cover mapping over large areas is essential to address a wide spectrum of socio-environmental challenges. For this reason, many global or regional land cover products are regularly released to the scientific community. Yet, the remote sensing community has not fully addressed the challenge to extract useful information from vast volumes of satellite data. Especially, major limitations concern the use of inadequate classification schemes and “black box” methods that may not match with end-users conceptualization of geographic features. In this paper, we introduce a knowledge-driven methodological approach to automatically process Sentinel-2 time series in order to produce pre-classifications that can be adapted by end-users to match their requirements. The approach relies on a conceptual framework inspired from ontologies of scientific observation and geographic information to describe the representation of geographic entities in remote sensing images. The implementation consists in a three-stage classification system including an initial stage, a dichotomous stage and a modular stage. At each stage, the system firstly relies on natural language semantic descriptions of time series of spectral signatures before assigning labels of land cover classes. The implementation was tested on 75 time series of Sentinel-2 images (i.e. 2069 images) in the Southern Brazilian Amazon to map natural vegetation and water bodies as required by a local end-user, i.e. a non-governmental organization. The results confirmed the potential of the method to accurately detect water bodies (F-score = 0.874 for bodies larger than 10 m) and map natural vegetation (max F-score = 0.875), yet emphasizing the spatial heterogeneity of accuracy results. In addition, it proved to be efficient to provide rapid estimates of degraded riparian forests at watershed level (R2 = 0.871). Finally, we discuss potential improvements both in the system's implementation, e.g. considering additional characteristics, and in the conceptual framework, e.g. moving from pixel- to object-based image analysis and evolving towards a hybrid system combining data- and knowledge-driven approaches.}
}
@article{SPOLADORE2024109001,
title = {A knowledge-based decision support system to support family doctors in personalizing type-2 diabetes mellitus medical nutrition therapy},
journal = {Computers in Biology and Medicine},
volume = {180},
pages = {109001},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109001},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524010862},
author = {Daniele Spoladore and Francesco Stella and Martina Tosi and Erna Cecilia Lorenzini and Claudio Bettini},
keywords = {Decision support system, Ontology-based system, Clinical decision support, Type-2 diabetes mellitus, Medical nutrition therapy},
abstract = {Background
Type-2 Diabetes Mellitus (T2D) is a growing concern worldwide, and family doctors are called to help diabetic patients manage this chronic disease, also with Medical Nutrition Therapy (MNT). However, MNT for Diabetes is usually standardized, while it would be much more effective if tailored to the patient. There is a gap in patient-tailored MNT which, if addressed, could support family doctors in delivering effective recommendations. In this context, decision support systems (DSSs) are valuable tools for physicians to support MNT for T2D patients – as long as DSSs are transparent to humans in their decision-making process. Indeed, the lack of transparency in data-driven DSS might hinder their adoption in clinical practice, thus leaving family physicians to adopt general nutrition guidelines provided by the national healthcare systems.
Method
This work presents a prototypical ontology-based clinical Decision Support System (OnT2D- DSS) aimed at assisting general practice doctors in managing T2D patients, specifically in creating a tailored dietary plan, leveraging clinical expert knowledge. OnT2D-DSS exploits clinical expert knowledge formalized as a domain ontology to identify a patient's phenotype and potential comorbidities, providing personalized MNT recommendations for macro- and micro-nutrient intake. The system can be accessed via a prototypical interface.
Results
Two preliminary experiments are conducted to assess both the quality and correctness of the inferences provided by the system and the usability and acceptance of the OnT2D-DSS (conducted with nutrition experts and family doctors, respectively).
Conclusions
Overall, the system is deemed accurate by the nutrition experts and valuable by the family doctors, with minor suggestions for future improvements collected during the experiments.}
}
@article{YAN2023106798,
title = {Intelligent predictive maintenance of hydraulic systems based on virtual knowledge graph},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106798},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106798},
url = {https://www.sciencedirect.com/science/article/pii/S095219762300982X},
author = {Wei Yan and Yu Shi and Zengyan Ji and Yuan Sui and Zhenzhen Tian and Wanjing Wang and Qiushi Cao},
keywords = {Industry 4.0, Predictive maintenance, Virtual knowledge graph, Ontology, Ontology-based data access, Hydraulic systems},
abstract = {In the manufacturing industry, a hydraulic system harnesses liquid fluid power to create powerful machines. Under the trend of Industry 4.0, the predictive maintenance of hydraulic systems is transforming to more intelligent and automated approaches that leverage the strong power of artificial intelligence and data science technologies. However, due to the knowledge-intensive and heterogeneous nature of the manufacturing domain, the data and information required for predictive maintenance are normally collected from ubiquitous sensing networks. This leads to the gap between massive heterogeneous data/information resources in hydraulic system components and the limited cognitive ability of system users. Moreover, how to capture and structure useful domain knowledge (in a machine-readable way) for solving domain-specific tasks remains an open challenge for the predictive maintenance of hydraulic systems. To address these challenges, in this paper we propose a virtual knowledge graph-based approach for the digital modeling and intelligent predictive analytics of hydraulic systems. We evaluate the functionalities and effectiveness of the proposed approach on a predictive maintenance task under real-world industrial contexts. Results show that our proposed approach is capable and feasible to be implemented for digital modeling, data access, data integration, and predictive analytics.}
}
@article{ROLDANMOLINA2021101889,
title = {An ontology knowledge inspection methodology for quality assessment and continuous improvement},
journal = {Data & Knowledge Engineering},
volume = {133},
pages = {101889},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101889},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000161},
author = {Gabriela R. Roldán-Molina and David Ruano-Ordás and Vitor Basto-Fernandes and José R. Méndez},
keywords = {Ontology, Ontology fixing, Ontology quality measures, Ontology improvement methodology, Deming cycle},
abstract = {Ontology-learning methods were introduced in the knowledge engineering area to automatically build ontologies from natural language texts related to a domain. Despite the initial appeal of these methods, automatically generated ontologies may have errors, inconsistencies, and a poor design quality, all of which must be manually fixed, in order to maintain the validity and usefulness of automated output. In this work, we propose a methodology to assess ontologies quality (quantitatively and graphically) and to fix ontology inconsistencies minimizing design defects. The proposed methodology is based on the Deming cycle and is grounded on quality standards that proved effective in the software engineering domain and present high potential to be extended to knowledge engineering quality management. This paper demonstrates that software engineering quality assessment approaches and techniques can be successfully extended and applied to the ontology-fixing and quality improvement problem. The proposed methodology was validated in a testing ontology, by ontology design quality comparison between a manually created and automatically generated ontology.}
}
@article{IATRELLIS2018,
title = {A Review on Software Project Management Ontologies},
journal = {International Journal of Information Technology Project Management},
volume = {9},
number = {4},
year = {2018},
issn = {1938-0232},
doi = {https://doi.org/10.4018/IJITPM.2018100104},
url = {https://www.sciencedirect.com/science/article/pii/S1938023218000044},
author = {Omiros Iatrellis and Panos Fitsilis},
keywords = {Ontologies, Project Management, Software Project Management},
abstract = {This article aims to provide the reader with a comprehensive background for understanding current knowledge and research works on ontologies for software project management (SPM). It constitutes a systematic literature review behind key objectives of the potential adoption of ontologies in PM. Ontology development and engineering could facilitate substantially the software development process and improve knowledge management, software and artifacts reusability, internal consistency within project management processes of various phases of software life cycle. The authors examined the literature focusing on software project management ontologies and analyzed the findings of these published papers and categorized them accordingly. They used qualitative methods to evaluate and interpret findings of the collected studies. The literature review, among others, has highlighted lack of standardization in terminology and concepts, lack of systematic domain modeling and use of ontologies mainly in prototype ontology systems that address rather limited aspects of software project management processes.}
}
@article{ZHOU201849,
title = {An ontology framework towards decentralized information management for eco-industrial parks},
journal = {Computers & Chemical Engineering},
volume = {118},
pages = {49-63},
year = {2018},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2018.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0098135418300929},
author = {Li Zhou and Chuan Zhang and Iftekhar A. Karimi and Markus Kraft},
keywords = {Eco-industrial park, Knowledge base, Ontology, Process modelling},
abstract = {In this paper, we develop a skeletal ontology for eco-industrial parks. A top-down conceptual framework including five operating levels (unit operations, processes, plants, industrial resource networks and eco-industrial parks) is employed to guide the design of the ontology structure. The detailed ontological representation of each level is realized through adapting and extending OntoCAPE, an ontology of the chemical engineering domain. Based on the proposed ontology, a framework for distributed information management is proposed for eco-industrial parks. As an example, this ontology is used to create a knowledge base for Jurong Island, an industrial park in Singapore. Its potential uses in supporting process modeling and optimization and facilitating industrial symbiosis are also discussed in the paper.}
}
@article{IAKSCH2019100942,
title = {Method for digital evaluation of existing production systems adequacy to changes in product engineering in the context of the automotive industry},
journal = {Advanced Engineering Informatics},
volume = {42},
pages = {100942},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100942},
url = {https://www.sciencedirect.com/science/article/pii/S1474034618305305},
author = {Jaqueline Sebastiany Iaksch and Milton Borsato},
keywords = {Model-based engineering, Ontology, Product development process, Production systems},
abstract = {Current industry practices during the Product Development Process (PDP) still points to the isolation of knowledge domains even with the increase of digitalization. Considering manufacturing process constrains from the beginning of the PDP avoids problems in later stages during the whole product life cycle. Through the application of concepts of the Digital Thread approach, the opportunity to intelligently integrate knowledge into product development is presented, creating a “digital fabric” capable of directing and supporting all stages of the product life cycle. Through these concepts, this research proposes the elaboration of an ontological model and application method capable of evaluating, in real time, the adequacy of the existing production systems, integrating the project and process information. The methodological framework used for the development of this method was Design Science Research. In this way, six steps were performed: (i) problem identification and motivation; (ii) definition of the objectives of the solution; (iii) artifact design and development; (iv) demonstration; (v) evaluation; and, (vi) results report. Through the description of manufacturing systems, the solution contributes to the digital evaluation and facilitates the decision making regarding productive systems, as well as data recovery, reutilization and management. In order to do an initial framework validation, it was performed the application of adequacy principles of a specific production line in automotive sector. However, this choice of a complex industry sector translates a clear possibility of framework adaptation to another industrial segment.}
}
@article{RODRIGUES201912,
title = {Legal ontologies over time: A systematic mapping study},
journal = {Expert Systems with Applications},
volume = {130},
pages = {12-30},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419302398},
author = {Cleyton Mário de Oliveira Rodrigues and Frederico Luiz Gonçalves de Freitas and Emanoel Francisco Spósito Barreiros and Ryan Ribeiro de Azevedo and Adauto Trigueiro de Almeida Filho},
keywords = {Legal ontology, Systematic mapping study, Legal expert system, Legal theory, Semantic web},
abstract = {Over the last 30 years, AI & Law has provided breakthroughs in studies involving case-based reasoning, rule-based reasoning, information retrieval and, most recently, conceptual models for knowledge representation and reasoning, known as Legal Ontologies. Ontologies have been widely used by legal practitioners, scholars, and lay people in a variety of situations, such as simulating legal actions, semantic search and indexing, and to keep up-to-date with the continual change of laws and regulations. Given the high number of legal ontologies produced, the need to summarize this research realm through a well-defined methodological procedure is urgent need. This study presents the results of a systematic mapping of the literature, aiming at categorizing legal ontologies along certain dimensions, such as purpose, level of generality, underlying legal theories, among other aspects. The reasons to carry out a systematic mapping are twofold: in addition to explaining the maturation of the area over recent decades, it helps to avoid the old problem of reinventing the wheel. Through organizing and classifying what has already been produced, it is possible to realize that the development of legal ontologies can rise to the level of reusability where prefabricated models might be coupled with new and more complex ontologies for practical law.}
}
@article{HOA2025150,
title = {Integration of BIM and GIS for managing infrastructure project: a case study of the 3/2 street in Kien Giang, Vietnam},
journal = {Transportation Research Procedia},
volume = {85},
pages = {150-157},
year = {2025},
note = {TRPRO_SDCAT 2023},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2025.03.145},
url = {https://www.sciencedirect.com/science/article/pii/S2352146525002042},
author = {Trinh Van Hoa and Tri N.M. Nguyen and Le Van Phuc and Ngo Chau Phuong},
keywords = {BIM, GIS, integration, infrastructure, transportation},
abstract = {In the construction field, Building information modeling (BIM) and geographic information systems (GIS) integration have garnered a lot of interest because they can increase efficiency, accuracy, and collaboration among stakeholders. This article explores the application of BIM-GIS integration in transportation infrastructure projects. It examines the benefits, challenges, and potential solutions to implementing this integrated approach. A case study of the 3/2 street in Kien Giang, Vietnam was conducted. The findings show that BIM-GIS integration can improve project planning, design coordination, asset management, and decision-making in the management of infrastructure projects in Kien Giang province in particular and Vietnam in general, from design to operation.}
}
@article{WU2025125981,
title = {A semantic-driven approach for maintenance digitalization in the pharmaceutical industry},
journal = {International Journal of Pharmaceutics},
volume = {683},
pages = {125981},
year = {2025},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2025.125981},
url = {https://www.sciencedirect.com/science/article/pii/S037851732500818X},
author = {Ju Wu and Xiaochen Zheng and Marco Madlena and Dimitris Kiritsis},
keywords = {Semantic technology, Pharma 4.0, Quality 4.0, Zero Defect Manufacturing, Digitalization, Pharmaceutical industry, Maintenance automation},
abstract = {The digital transformation of pharmaceutical industry is a challenging task due to the high complexity of involved elements and the strict regulatory compliance. Maintenance activities in the pharmaceutical industry play an essential role in ensuring product quality and integral functioning of equipment and premises. This paper first identifies the key challenges of digitalization in pharmaceutical industry and creates the corresponding problem space for key involved elements. A semantic-driven digitalization framework is proposed aiming to improve the digital continuity of digital resources and technologies for maintenance activities. This framework aligns with Quality 4.0 principles and supports the industry’s pursuit of zero manufacturing defects. A case study is conducted to verify the feasibility of the proposed framework based on the water sampling activities in Merck Serono facility in Switzerland. A tool-chain is presented to enable the functional modules of the framework. Some of the key functional modules within the framework are implemented and have demonstrated satisfactory performance. As one of the outcomes, a digital sampling assistant with web-based services is created to support the automated workflow of water sampling activities. The implementation result proves the potential of the proposed framework to solve the identified problems of maintenance digitalization in the pharmaceutical industry.}
}
@article{PERKUSICH2020106241,
title = {Intelligent software engineering in the context of agile software development: A systematic literature review},
journal = {Information and Software Technology},
volume = {119},
pages = {106241},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.106241},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919302587},
author = {Mirko Perkusich and Lenardo {Chaves e Silva} and Alexandre Costa and Felipe Ramos and Renata Saraiva and Arthur Freire and Ednaldo Dilorenzo and Emanuel Dantas and Danilo Santos and Kyller Gorgônio and Hyggo Almeida and Angelo Perkusich},
keywords = {Intelligent software engineering, Agile software development, Search-based software engineering, Machine learning, Bayesian networks, Artificial intelligence},
abstract = {CONTEXT: Intelligent Software Engineering (ISE) refers to the application of intelligent techniques to software engineering. We define an “intelligent technique” as a technique that explores data (from digital artifacts or domain experts) for knowledge discovery, reasoning, learning, planning, natural language processing, perception or supporting decision-making. OBJECTIVE: The purpose of this study is to synthesize and analyze the state of the art of the field of applying intelligent techniques to Agile Software Development (ASD). Furthermore, we assess its maturity and identify adoption risks. METHOD: Using a systematic literature review, we identified 104 primary studies, resulting in 93 unique studies. RESULTS: We identified that there is a positive trend in the number of studies applying intelligent techniques to ASD. Also, we determined that reasoning under uncertainty (mainly, Bayesian network), search-based solutions, and machine learning are the most popular intelligent techniques in the context of ASD. In terms of purposes, the most popular ones are effort estimation, requirements prioritization, resource allocation, requirements selection, and requirements management. Furthermore, we discovered that the primary goal of applying intelligent techniques is to support decision making. As a consequence, the adoption risks in terms of the safety of the current solutions are low. Finally, we highlight the trend of using explainable intelligent techniques. CONCLUSION: Overall, although the topic area is up-and-coming, for many areas of application, it is still in its infancy. So, this means that there is a need for more empirical studies, and there are a plethora of new opportunities for researchers.}
}
@article{SZEJKA2024100661,
title = {Knowledge-based expert system to drive an informationally interoperable manufacturing system: An experimental application in the Aerospace Industry},
journal = {Journal of Industrial Information Integration},
volume = {41},
pages = {100661},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100661},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24001055},
author = {Anderson Luis Szejka and Osiris {Canciglieri Junior} and Fernando Mas},
keywords = {Manufacturing systems, Digital transformation, Semantic interoperability, Ontology, Information and knowledge formalization, Semantic rules},
abstract = {The industrial revolutions have challenged organisations to rethink their product design and manufacturing processes, making them faster and more connected to market demands and changes. Digital technologies have emerged with solutions to virtual represent physical objects, processes, systems, or assets to simulate and analyse the impact of manufacturing changes before actual implementation. However, the challenge is to deal with thousands of heterogeneous information sets which must be shared simultaneously by different groups within and across institutional boundaries. Each manufacturing industry has its format and model to represent the product in the development, manufacturing process, material features, etc. In this context, this paper explores a knowledge-based expert system to support the information exchange and inconsistency detection across the manufacturing process, specifically in an experimental application in the Aerospace Industry. The proposed framework was based on knowledge formalisation and semantic rules through ontologies, semantic reconciliation strategies and connectivity interfaces to manage information and knowledge and identify inconsistencies across the manufacturing system. It was mainly evaluated across the product and manufacturing design of sheet metal forming aluminium thin wall parts for the aerospace industry. Results demonstrate the capability of the approach to enhance data accuracy, coherence, and efficiency throughout the manufacturing of complex products. However, the solution presents challenges such as interdisciplinary collaboration in product design, specific information requirements for manufacturing planning, and the impact of production planning on manufacturing capacities.}
}
@article{WU2024e35963,
title = {Eliminating ontology contradictions based on the Myerson value},
journal = {Heliyon},
volume = {10},
number = {16},
pages = {e35963},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e35963},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024119949},
author = {Juanyong Wu and Wei Peng},
keywords = {Ontology contradictions, Cooperative game theory, Lexicographic approach, Myerson value, Shapley value},
abstract = {Ontologies play a pivotal role in knowledge representation across various artificial intelligence domains, serving as foundational frameworks for organizing data and concepts. However, the construction and evolution of ontologies frequently lead to logical contradictions that undermine their utility and accuracy. Typically, these contradictions are addressed using an Integer Linear Programming (ILP) model, which traditionally treats all formulas with equal importance, thereby neglecting the distinct impacts of individual formulas within minimal conflict sets. To advance this method, we integrate cooperative game theory to compute the Shapley value for each formula, reflecting its marginal contribution towards resolving logical contradictions. We further construct a graph-based representation of the ontology, enabling the extension of Shapley values to Myerson values. Subsequently, we introduce a Myerson-weighted ILP model that employs a lexicographic approach to eliminate logical contradictions in ontologies. The model ensures the minimum number of formula deletions, subsequently applying Myerson values to guide the prioritization of deletions. Our comparative analysis across 18 ontologies confirms that our approach not only preserves more graph edges than traditional ILP models but also quantifies formula contributions and establishes deletion priorities, presenting a novel approach to ILP-based contradiction resolution.}
}
@article{RODLER201992,
title = {Are query-based ontology debuggers really helping knowledge engineers?},
journal = {Knowledge-Based Systems},
volume = {179},
pages = {92-107},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119302126},
author = {Patrick Rodler and Dietmar Jannach and Konstantin Schekotihin and Philipp Fleiss},
keywords = {Knowledge base debugging, Interactive debugging, User study, Ontologies, Model-based diagnosis, Protégé, Ontology debugging tool},
abstract = {Real-world semantic or knowledge-based systems can become large and complex, e.g., in the biomedical domain. Tool support for the localization and repair of faults within knowledge bases of such systems can therefore be essential for their practical success. Correspondingly, a number of knowledge base debugging approaches, in particular for ontology-based systems, were proposed in recent years. Query-based debugging is a comparably recent interactive approach that localizes the true cause of an observed problem by asking knowledge engineers a series of questions. Concrete implementations of this approach exist, such as the OntoDebug plug-in for the ontology editor Protégé. To validate that a newly proposed method is favorable over an existing one, researchers often rely on simulation-based comparisons. Such an evaluation approach however has certain limitations and often cannot fully inform us about a method’s true usefulness. We therefore conducted a range of user studies to assess the practical value of query-based ontology debugging. One main insight from the studies is that the considered interactive approach is indeed more efficient than an alternative algorithmic debugging based on test cases. We also observed that users frequently made errors in the process, which highlights the importance of a careful design of the queries that users need to answer.}
}
@article{JIANG2019104967,
title = {Semantifying formal concept analysis using description logics},
journal = {Knowledge-Based Systems},
volume = {186},
pages = {104967},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.104967},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119303971},
author = {Yuncheng Jiang},
keywords = {Formal concept analysis, Ontologies, Description logics, Semantic operations, Attribute reduction},
abstract = {Formal Concept Analysis (FCA) is a field of applied mathematics with its roots in order theory, in particular the theory of complete lattices. Over the past 20 years, FCA has been widely studied. Description Logics (DLs) are a family of knowledge representation languages which can be used to represent the terminological knowledge of an application domain in a structured and formally well-understood way. Nowadays, properties and semantics of ontology constructs mainly are determined by DLs. The current research progress and the existing problems of FCA are analyzed. In this paper, we semantify FCA with DLs, in other words, we present an extended FCA (i.e., semantic FCA) by using the concepts of DLs to act as the attributes of formal contexts. Furthermore, we semantify the three components (i.e., formal concepts, attribute implications, and concept lattices) of traditional FCA. In addition, we also study the attribute reduction of formal contexts, formal concepts, and concept lattices from a semantics point of view.}
}
@article{DUARTE2021101892,
title = {An ontological analysis of software system anomalies and their associated risks},
journal = {Data & Knowledge Engineering},
volume = {134},
pages = {101892},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101892},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000197},
author = {Bruno Borlini Duarte and Ricardo {de Almeida Falbo} and Giancarlo Guizzardi and Renata Guizzardi and Vítor E. Silva Souza},
keywords = {Software defects, Errors and failures, Ontological foundations of software systems, Conceptual modeling, Methods and methodologies, Software system risk, Unified Foundational Ontology (UFO)},
abstract = {Software systems have an increasing value in our lives, as our society relies on them for the numerous services they provide. However, as our need for larger and more complex software systems grows, the risks involved in their operation also grows, with possible consequences in terms of significant material and social losses. The rational management of software defects and possible failures is a fundamental requirement for a mature software industry. Standards, professional guides and capability models directly emphasize how important it is for an organization to know and to have a well-established history of failures, errors and defects as they occur in software activities. The problem is that each of these reference models employs its own vocabulary to deal with these phenomena, which can lead to a deficiency in the understanding of these notions by software engineers, causing potential interoperability problems between supporting tools, and, consequently, a poorer adoption of these standards and tools in practice. In this paper, we address this problem of the lack of a consensual conceptualization in this area by proposing two reference conceptual models: an Ontology of Software Defects, Errors and Failures (OSDEF), which takes into account an ecosystem of software artifacts, and a Reference Ontology of Software Systems (ROSS), which characterizes software systems and related artifacts at different levels of abstraction. Moreover, we use OSDEF and ROSS to perform an ontological analysis of the impact of defects, errors and failures of software systems from a risk analysis perspective. To do that, we employee an existing core ontology, namely, the Common Ontology of Value and Risk (COVR). The ontologies presented here are grounded on the Unified Foundational Ontology (UFO) and based on well-known and widely-accepted standards, professional and scientific guides and capability models. We demonstrate how this approach can suitably promote conceptual clarification and terminological harmonization in this area.}
}
@article{BAVARESCO2024110310,
title = {An ontology-based framework for worker’s health reasoning enabled by machine learning},
journal = {Computers & Industrial Engineering},
volume = {193},
pages = {110310},
year = {2024},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110310},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224004315},
author = {Rodrigo Bavaresco and Yutian Ren and Jorge Barbosa and G.P. Li},
keywords = {Knowledge representation, Ontology, Deep learning, Reasoning, Occupational health and safety},
abstract = {Reports of fatal and nonfatal workplace injuries depict severe circumstances involving health and safety in industries. This leads to workers staying away from work in U.S. for an average of 12 days in 2020, implicating in managerial, financial, and organizational losses. In this context, Vision-Based Deep Learning (VBDL) and Knowledge Representation and Reasoning (KRR) allow real-time data retrieval of situations along with the semantic modeling and expressivity of the real world to mitigate injuries. This article presents a framework that interoperates vision-based deep learning and ontology reasoning to identify adverse working situations, introducing a novel ontology composed of a holistic perspective of workers’ health and safety. Moreover, the article provides multi-agent framework modeling to orchestrate the components’ interoperability, describing the framework’s architecture and deployment in workrooms. As a result, a practical evaluation over five days produced 8395 axioms constituted by 1210 individuals in the ontology, which allowed a temporal analysis of harmful conditions and their multiple overlapping using SPARQL and reasoning rules, particularly relevant to understanding explanations of overexertion, physical and environmental injuries. Therefore, the proposed ontology-based framework corroborates the long-term support in identifying, assessing, and controlling risks in the industries due to a well-defined knowledge model.}
}
@article{TRAPPEY2023101354,
title = {Patent landscape and key technology interaction roadmap using graph convolutional network – Case of mobile communication technologies beyond 5G},
journal = {Journal of Informetrics},
volume = {17},
number = {1},
pages = {101354},
year = {2023},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2022.101354},
url = {https://www.sciencedirect.com/science/article/pii/S1751157722001079},
author = {Amy J.C. Trappey and Ann Y.E. Wei and Neil K.T. Chen and Kuo-An Li and L.P. Hung and Charles V. Trappey},
keywords = {Tech-mining analysis, Patent analysis, Keyword extraction, Graph convolution network (GCN)},
abstract = {Beyond 5G (B5G) in mobile network technologies is the latest communication technology currently under development. B5G is expected to achieve superior capabilities in ultra-high network transmission speed, low latency, low energy consumption, and high coverage, comparing to current 5G network performance. Although B5G is still in the development and implementation stage, there are many patents and non-patent literature depicting B5G innovative technologies and applications. The landscapes of B5G technologies are great references for governments and industries to understand the advances in mobile communication for R&D strategies. Thus, this research focuses on developing a formal tech-mining workflow integrating semantic-based patent and non-patent literature analysis for ontology building, patent technological topic clustering, and graph convolutional network (GCN) modeling for depicting key technology interactions among clusters of sub-domain topics. This research emphasizes the study of B5G patent landscape and key technology interaction roadmap in comprehensive steps as a valuable reference for B5G mobile network R&D, as well as for conducting tech-mining of other technology domains of interests.}
}
@article{VIKTOROVIC201931,
title = {Semantic web technologies as enablers for truly connected mobility within smart cities},
journal = {Procedia Computer Science},
volume = {151},
pages = {31-36},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919304697},
author = {Miloš Viktorović and Dujuan Yang and Bauke de Vries and Nico Baken},
keywords = {Linked data, Authonomous vehicles, Semantic web, Mobility, Smart Cities, Ontologies},
abstract = {Most car manufacturers predict that in the first half of the next decade there will be fully autonomous vehicles on our roads. Such vehicles would have to communicate in order to mitigate problems caused by single-viewpoint approach. So there are a lot of researches and developments when it comes to communication layer of V2X (Vehicle-to-Everything), but there is still a lot to be done when it comes to data layer of this communication. This is why we propose using Semantic Web Technologies (SWT) to fill in gaps within data layer of V2X communication. By using SWT (Semantic Web Technologies) and Linked data, we plan to interconnect various data sources, in order to provide homogeneous way for connected autonomous vehicles (CAV) to access relevant information. Such information is currently contained in three distinctive type of sources. These are: Geo-stationary Static data sources (Maps, City models), Geostationary Dynamic data sources (IoT devices) and Non-geostationary Dynamic sources (Vehicles). Using SWT, our goal is to develop ontology(s), in such a way that in-vehicle algorithms can extract and process information about environment they are in, while taking into account available network bandwidth.}
}
@article{PEREZ2018239,
title = {A case study on the use of machine learning techniques for supporting technology watch},
journal = {Data & Knowledge Engineering},
volume = {117},
pages = {239-251},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18302106},
author = {Alain Perez and Rosa Basagoiti and Ronny Adalberto Cortez and Felix Larrinaga and Ekaitz Barrasa and Ainara Urrutia},
keywords = {Text mining, Knowledge management applications, Multi-classification, Technology watch automation, Semantic annotations},
abstract = {Technology Watch human agents have to read many documents in order to manually categorize and dispatch them to the correct expert, that will later add valued information to each document. In this two step process, the first one, the categorization of documents, is time consuming and relies on the knowledge of a human categorizer agent. It does not add direct valued information to the process that will be provided in the second step, when the document is revised by the correct expert. This paper proposes Machine Learning tools and techniques to learn from the manually pre-categorized data to automatically classify new content. For this work a real industrial context was considered. Text from original documents, text from added value information and Semantic Annotations of those texts were used to generate different models, considering manually pre-established categories. Moreover, three algorithms from different approaches were used to generate the models. Finally, the results obtained were compared to select the best model in terms of accuracy and also on the reduction of the amount of document readings (human workload).}
}
@article{FAILLA2025100820,
title = {Managing lifecycle of product information with an ontology-based knowledge framework},
journal = {Journal of Industrial Information Integration},
volume = {45},
pages = {100820},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100820},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25000445},
author = {Lorenzo Failla and Marco Rossoni and Marco Quirini and Giorgio Colombo},
keywords = {Product Lifecycle Management (PLM), Ontologies, Linked data, Product information management, Product knowledge formalization},
abstract = {The effective management of product information within a formalized, digital and interoperable infrastructure remains a significant gap in realizing the full potential of modern Product Lifecycle Management (PLM) implementations in industrial contexts. While the academic paradigm of PLM has been extensively emphasized in the scientific literature for over two decades as a sustainable company strategy, contemporary PLM implementations prove inadequate in handling the extensive volume and variety of information generated throughout a product’s lifecycle. Starting from a comprehensive overview of the evolution of the PLM paradigm and of its inherent implications, the analysis of the PLM implementation of a big player in engineering and manufacturing of turbomachinery products for Oil & Gas and Energy markets is analyzed, allowing to identify existing major general contradictions from an industrial perspective. While it is reaffirmed that the attainment of a neutral, harmonized and universally agreed standardization is nowadays missing and is crucial in the enabling of the PLM paradigm through digital technologies, the present study attempts to demonstrate how a general and agnostic ontology-based framework may straightforwardly fulfill all the identified demands of the PLM paradigm and, therefore, how ontologies play a central role in this field of research by bridging different domains to enable a holistic product conceptualization, lifecycle management, and data interoperability among different digital agents.}
}
@article{MOYANO2023107965,
title = {Semantic interoperability for cultural heritage conservation: Workflow from ontologies to a tool for managing and sharing data},
journal = {Journal of Building Engineering},
volume = {80},
pages = {107965},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.107965},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223021459},
author = {Juan Moyano and Alessandra Pili and Juan E. Nieto-Julián and Stefano {Della Torre} and Silvana Bruno},
keywords = {Cultural heritage, Preventive and planned conservation, Cultural value, BIM, Ontology, IFC, Interoperability, Data interchange, Sustainable process, Free and open-source software},
abstract = {The benefits of using BIM in the construction sector are now widely recognised. From this awareness comes the aspiration to have the same advantages for the Cultural Heritage (CH) sector, to obtain more sustainability in the process. Indeed, research in recent years has been orientated in this direction. Attempts to use BIM tools to CH have shown the limits of the ability to correctly represent and transmit information, especially on cultural value and conservation activities, and on the presence of available objects, due to the lack of specific content in IFC, which means a lack of interoperability. The provision of ontologies is necessary to allow interoperability. Ontologies permit the conceptualization of a representative model of reality by defining classes, attributes, and relationships that describe a domain. The research experiments and proposes a flowchart to connect a specific ontological model for Cultural Heritage in a BIM environment. Using Free and Open-Source Software (FOSS), it is possible to modify, improve, and adapt the functions according to the specific needs of users. In addition, this paper analyses the interconnectivity among three software in the BIM environment: FreeCAD, ArchiCAD and Revit, and the steps for an exchange of information between the geometric model and its semantic properties are established. The decorated umbrella vault of Masegra Castle in Sondrio was selected as a case study to show the application of the method and your tool.}
}
@article{BLUMS2023102148,
title = {Consolidating economic exchange ontologies for financial reporting standard setting},
journal = {Data & Knowledge Engineering},
volume = {145},
pages = {102148},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102148},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23000083},
author = {Ivars Blums and Hans Weigand},
keywords = {Unified Foundational Ontology, Economic exchange, Financial reporting, Conceptual modeling},
abstract = {Several UFO grounded economic exchange ontologies have been developed in the last decade, notably COFRIS, OntoREA, REA2, and ATE. An important question is whether they have reached a level to support financial reporting standard setters. This article first describes the foundational assumptions for exchange conceptualization and consolidates the latest developments in COFRIS - a core ontology for financial reporting information systems, within the most recent versions of the UFO theories and the OntoUML tool. The ontology is evaluated based on competency questions. Furthermore, it is confronted with the conceptual frameworks and standards for accounting and financial reporting and compared with other UFO grounded exchange ontologies. The results show both the maturity level of current exchange ontologies and their applicability to standard setting.}
}
@article{CHAOUCH2021890,
title = {Model for the classification of scheduling problems based on ontology},
journal = {Procedia Computer Science},
volume = {181},
pages = {890-896},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.244},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002878},
author = {Rihab Chaouch and Hanen Ghorbel and Soulef Khalfallah},
keywords = {Scheduling, classification of scheduling problems, ontology, knowledge-base;},
abstract = {In this paper, we propose an ontology-based architecture for the classification of a scheduling problem. In fact, planning and scheduling is a major concern for the production enterprise as it contributes to its profit. Most of the time, researchers propose algorithms that relax some reel situations’ constraints; given that reel shop description are not taken directly from the floor supervisor, or given the complexity of the problem. The objective of ontology for the classification of scheduling problem is to build a bridge between practitioners and researchers in order to identify the reel classification of the problem. Thus, in this paper, we start by presenting a general model for the classification of scheduling problem. On this model, we build specific domain ontology. The specific domain ontology functions as a knowledge base and a data access point for the scheduling problem classification system.}
}
@article{MOHDALI2019191,
title = {A product life cycle ontology for additive manufacturing},
journal = {Computers in Industry},
volume = {105},
pages = {191-203},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518301647},
author = {Munira {Mohd Ali} and Rahul Rai and J. Neil Otte and Barry Smith},
keywords = {, Manufacturing process ontology, Ontology engineering, Product life cycle, Dentistry product manufacturing},
abstract = {The manufacturing industry is evolving rapidly, becoming more complex, more interconnected, and more geographically distributed. Competitive pressure and diversity of consumer demand are driving manufacturing companies to rely more and more on improved knowledge management practices. As a result, multiple software systems are being created to support the integration of data across the product life cycle. Unfortunately, these systems manifest a low degree of interoperability, and this creates problems, for instance when different enterprises or different branches of an enterprise interact. Common ontologies (consensus-based controlled vocabularies) have proved themselves in various domains as a valuable tool for solving such problems. In this paper, we present a consensus-based Additive Manufacturing Ontology (AMO) and illustrate its application in promoting re-usability in the field of dentistry product manufacturing.}
}
@article{ZANGENEH2020101164,
title = {Ontology-based knowledge representation for industrial megaprojects analytics using linked data and the semantic web},
journal = {Advanced Engineering Informatics},
volume = {46},
pages = {101164},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101164},
url = {https://www.sciencedirect.com/science/article/pii/S147403462030135X},
author = {Pouya Zangeneh and Brenda McCabe},
keywords = {Industrial megaprojects, Knowledge representation, Project analytics, Risk analysis, Ontology, Semantic web},
abstract = {The fourth industrial revolution has affected most industries, including construction and those within the delivery chain of megaprojects. These major paradigm shifts, however, did not considerably improve the track record in predicting project outcomes and estimating required resources. One reason is the lack of unified data definitions and expandable knowledge representation across project lifecycle to represent megaprojects for analytics. This paper proposes and evaluates a unified ontology for project knowledge representation that facilitates data collection, processing, and utilization for industrial megaprojects through their lifecycle. The proposed Uniform Project Ontology, or UPonto, provides a data infrastructure for project analytics by enabling logical deductions and inferences, and flexible expansion and partitioning of the data utilizing linked data and the semantic web. The ontology facilitates cost normalization processes, temporal queries, and graph queries using SPARQL, while defining universal semantics for a wide range of project risk factors and characteristics based on comprehensive research of the empirical project risk and success literature augmented by practical considerations gained through expert consultations. UPonto forms the basis for a project knowledge graph to utilize unstructured data; it as well provides semantic definitions for smart IoT agents to consume project risk data and knowledge.}
}
@article{DASILVA20191041,
title = {A conceptual model for quality of experience management to provide context-aware eHealth services},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {1041-1061},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.033},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18314079},
author = {Madalena Pereira {da Silva} and Alexandre Leopoldo Gonçalves and Mário Antônio Ribeiro Dantas},
keywords = {AAL, ICT, QoC, QoE, SDN, UX},
abstract = {The emergence of new Information and Communication Technologies (ICT), such as the Internet of Things (IoT) and the Software-Defined Networking (SDN) approach, together with Autonomic Network Management (ANM), have improved the provision of eHealth services. However, proposals must evolve in order to ensure that eHealth data be transported with quality assurance and delivered with quality information. This perspective substantiates our proposal and the respective validation of a Quality of Experience (QoE) Management model in a Future Internet Architecture. A knowledge representation model of QoE was incorporated into a service delivery platform oriented to user’s needs, thus measuring UX (User Experience). An experimental environment was configured to provide eHealth services from an AAL environment to a healthcare facility. Experiments were performed to verify whether the components of the proposed approach had better quality performance in service provision when compared to the components of the native approach of the SDN controller. Experimental results pointed out that, with a 95% confidence interval, all eHealth services utilizing components of the proposed model showed superior quality when compared with those from the native approach.}
}
@article{SANTOS2024122104,
title = {O3PO: A domain ontology for offshore petroleum production plants},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122104},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122104},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423026064},
author = {Nicolau O. Santos and Fabrício H. Rodrigues and Daniela Schmidt and Régis K. Romeu and Givanildo Nascimento and Mara Abel},
keywords = {Petroleum, Ontology, Production plants, Offshore, Digital twins},
abstract = {The challenge of integrating data from many data sources has persisted as an issue in several industry areas. With the evolution of technology in the upstream petroleum sector (i.e., exploration and production), the petroleum business must contend with technological silos from diverse service providers and suffers from the associated waste of time to locate data and information throughout siloed databases. Based on a thorough compilation of industry-oriented requirements in the form of use cases and competency questions, this document defines a domain ontology for defining entities in offshore petroleum production plants. The objective is to develop a uniform and clearly defined reference vocabulary to aid engineers and information technology professionals in labeling and relating production plant monitoring, simulation measures, and facilities. BFO is the top-level ontology, while GeoCore and a continuing version of the core ontology developed by the Industry Ontology Foundry (IOF) configure the middle-level ontologies. We have studied and combined several other resources to build the ontology, such as glossaries from the industry and related ontologies. The research resulted in a well-founded domain ontology that provides universals, defined classes, and relations that can be useful in several types of applications in the domain. We have demonstrated the utility of the ontology within an actual scenario in an offshore petroleum field in Brazil, where we conceived and applied the domain ontology. This study is a component of the PeTWIN project,22Petwin is an academic cooperation between UFRGS, UiO, and Libra, Equinor, and Shell companies (www.petwin.org). which looks at the best approaches for creating digital twins of offshore petroleum plants.}
}
@article{AMADORDOMINGUEZ2023199,
title = {GEnI: A framework for the generation of explanations and insights of knowledge graph embedding predictions},
journal = {Neurocomputing},
volume = {521},
pages = {199-212},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222015053},
author = {Elvira Amador-Domínguez and Emilio Serrano and Daniel Manrique},
keywords = {Knowledge graph embeddings, Rule-learning models, Explainable AI, Influence function},
abstract = {Knowledge Graphs (KGs) are among the most commonly used knowledge representation paradigms, being at the core of tasks such as question answering or recommendation systems. Knowledge Graph Completion (KGC) is one of the key tasks concerning KGs, where the goal is to extract new elements from the existing information. Different approaches have been proposed through the years to tackle this challenge. Among them, two analogous categories can be distinguished: rule-learning and Knowledge Graph Embeddings (KGE). Different methods have been subsequently proposed to unify both types under a single framework, such that the benefits of both proposals can be exploited. However, most of these methods consider using rule-learning models as a boosting agent for KGE models, but not as an explainability tool. This work presents GEnI11https://github.com/oeg-upm/GEnI, a framework capable of generating insights and explanations for KGE models. GEnI follows a three-phase sequential process, generating a feasible explanation for a given prediction. Possible outcomes are rules, correlations, and influence detection. Moreover, the output is expressed in natural language to further extend the explainability of the proposal. GEnI has been successfully evaluated under three criteria: coherence, the meaningfulness of the output, and reliability. Moreover, it can be used by both translational and bilinear KGE models, offering broad coverage. Furthermore, this work also presents an in-depth review of existing integrative approaches between rule-learning and embedding models, providing a comparative framework between them.}
}
@article{DERAVE2024102293,
title = {A taxonomy and ontology for digital platforms},
journal = {Information Systems},
volume = {120},
pages = {102293},
year = {2024},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102293},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923001291},
author = {Thomas Derave and Frederik Gailly and Tiago Prince Sales and Geert Poels},
keywords = {Digital platform, Ontology, UFO, Taxonomy},
abstract = {In academic literature and in business communication digital platforms are categorized into different types including, but not limited to, multi-sided platform, digital marketplace, on-demand platform and sharing economy platform. Observing the substantial literature on these platform types, both in academia and professional contexts, there is lack of consensus on the definition of these digital platform types. On top of that, there is a lack of knowledge in the literature on the requirements and design of these digital platform types. We address the observed lack of shared conceptualizations by creating a taxonomy for digital platforms, by classifying digital platforms according to their distinctive features. Further, we address the lack of design knowledge with a digital platform reference ontology that describes the construction and operation of the different types of digital platform that are distinguished in the taxonomy. This reference ontology, developed using the Unified Foundational Ontology (UFO) and several of its core ontologies, is organized into different ontology modules, allowing to describe functionalities that are common to any digital platform and more specific functionalities that have their origin in the distinctive features that were used to construct the taxonomy. The taxonomy that we contribute helps to clearly define the different types of digital platform that can be observed today. It also helps analysing further evolutions in the platform domain. The contribution of a well-founded digital platform reference ontology is a step towards a better understanding of digital platform functionality, better communication between stakeholders, and eventually may facilitate future research and development of new types of digital platform.}
}
@article{BURGER2018142,
title = {A framework for semi-automated co-evolution of security knowledge and system models},
journal = {Journal of Systems and Software},
volume = {139},
pages = {142-160},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S016412121830027X},
author = {Jens Bürger and Daniel Strüber and Stefan Gärtner and Thomas Ruhroth and Jan Jürjens and Kurt Schneider},
keywords = {Security requirements, Software evolution, Co-evolution, Software design, Security impact analysis},
abstract = {Security is an important and challenging quality aspect of software-intensive systems, becoming even more demanding regarding long-living systems. Novel attacks and changing laws lead to security issues that did not necessarily rise from a flawed initial design, but also when the system fails to keep up with a changing environment. Thus, security requires maintenance throughout the operation phase. Ongoing adaptations in response to changed security knowledge are inevitable. A necessary prerequisite for such adaptations is a good understanding of the security-relevant parts of the system and the security knowledge. We present a model-based framework for supporting the maintenance of security during the long-term evolution of a software system. It uses ontologies to manage the system-specific and the security knowledge. With model queries, graph transformation and differencing techniques, knowledge changes are analyzed and the system model is adapted. We introduce the novel concept of Security Maintenance Rules to couple the evolution of security knowledge with co-evolutions of the system model. As evaluation, community knowledge about vulnerabilities is used (Common Weakness Enumeration database). We show the applicability of the framework to the iTrust system from the medical care domain and hence show the benefits of supporting co-evolution for maintaining secure systems.}
}
@article{AMER2021102449,
title = {A Multi-Perspective malware detection approach through behavioral fusion of API call sequence},
journal = {Computers & Security},
volume = {110},
pages = {102449},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102449},
url = {https://www.sciencedirect.com/science/article/pii/S016740482100273X},
author = {Eslam Amer and Ivan Zelinka and Shaker El-Sappagh},
keywords = {Malware detection, API call Sequence, Perspective models, Behavioral analysis, Features’ fusion, Sequence reformulation},
abstract = {The widespread development of the malware industry is considered the main threat to our e-society. Therefore, malware analysis should also be enriched with smart heuristic tools that recognize malicious behaviors effectively. Although the generated API calling graph representation for malicious processes encodes worthwhile information about their malicious behavior, it is pragmatically inconvenient to generate a behavior graph for each process. Therefore, we experimented with creating generic behavioral graph models that describe malicious and non-malicious processes. These behavioral models relied on the fusion of statistical, contextual, and graph mining features that capture explicit and implicit relationships between API functions in the calling sequence. Our generated behavioral models proved the behavioral contrast between malicious and non-malicious calling sequences. According to that distinction, we built different relational perspective models that characterize processes’ behaviors. To prove our approach novelty, we experimented with our approach over Windows and Android platforms. Our experimentations demonstrated that our proposed system identified unseen malicious samples with high accuracy with low false-positive. In terms of detection accuracy, our model returns an average accuracy of 0.997 and 0.977 to the unseen Windows and Android malware testing samples, respectively. Moreover, we proposed a new indexing method for APIs based on their contextual similarities. We also suggested a new expressive, a visualized form that renders the API calling sequence. Consequently, we introduced a confidence metric to our model classification decision. Furthermore, we developed a behavioral heuristic that effectively identified malicious API call sequences that were deceptive or mimicry.}
}
@article{SPOLADORE2024108193,
title = {A Knowledge-based Decision Support System for recommending safe recipes to individuals with dysphagia},
journal = {Computers in Biology and Medicine},
volume = {171},
pages = {108193},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108193},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524002774},
author = {Daniele Spoladore and Vera Colombo and Vania Campanella and Christian Lunetta and Marta Mondellini and Atieh Mahroo and Federica Cerri and Marco Sacco},
keywords = {Dysphagia, Ontology-based decision support system, Clinical decision support system, Neuromuscular diseases, Nutrition, Dysphagic patient support},
abstract = {Background
Dysphagia is a disorder that can be associated to several pathological conditions, including neuromuscular diseases, with significant impact on quality of life. Dysphagia often leads to malnutrition, as a consequence of the dietary changes made by patients or their caregivers, who may deliberately decide to reduce or avoid specific food consistencies (because they are not perceived as safe), and the lack of knowledge in how to process foods are critics. Such dietary changes often result in unbalanced nutrients intake, which can have significant consequences for frail patients. This paper presents the development of a prototypical novel ontology-based Decision Support System (DSS) to support neuromuscular patients with dysphagia (following a per-oral nutrition) and their caregivers in preparing nutritionally balanced and safe meals.
Method
After reviewing scientific literature, we developed in collaboration with Ear-Nose-Throat (ENT) specialists, neurologists, and dieticians the DSS formalizes expert knowledge to suggest recipes that are considered safe according to patient's consistency limitations and dysphagia severity and also nutritionally well-balanced.
Results
The prototype can be accessed via digital applications both by physicians to generate and verify the recommendations, and by the patients and their caregivers to follow the step-by-step procedures to autonomously prepare and process one or more recipe. The system is evaluated with 9 clinicians to assess the quality of the DSS's suggested recipes and its acceptance in clinical practice.
Conclusions
Preliminary results suggest a global positive outcome for the recipes inferred by the DSS and a good usability of the system.}
}
@article{DAWOOD2022,
title = {An Ontology Towards Predicting Terrorism Events},
journal = {International Journal of Cyber Warfare and Terrorism},
volume = {12},
number = {1},
year = {2022},
issn = {1947-3435},
doi = {https://doi.org/10.4018/IJCWT.311421},
url = {https://www.sciencedirect.com/science/article/pii/S1947343522000143},
author = {Zubeida Dawood and Carien {Van 't Wout}},
keywords = {Analysis, Database, Ontology, Ontology Development, Ontology Methodology, Ontology-Based Data Access, Semantic Interoperability, Semantic Web, Terror Attacks, Terror Events, Terrorism, Web-Scraper},
abstract = {ABSTRACT
Although there is an increasing amount of information for counter-terrorism operations freely available online, it is a complex process to extract relevant information and to detect useful patterns in the data in order for intelligence functionaries to identify threats and to predict possible terror attacks. Automation is required for intelligent decision-making. To assist with this, in this paper, the researchers propose an ontology-based data access system for counter-terrorism. The system will enable intelligence analysts to perform specialised semantic searches about terrorist events or groups for analysis using an ontology. In this paper, the researchers present the ontology that was created by following an existing methodology for ontology development, and an ontology-based data access system together with all the components used in development (i.e., databases, web-scraper tools, ontology-based data access software, and data sources). Lastly, the ontology is demonstrated by means of use cases with example queries for generating actionable intelligence for operations.}
}
@article{BUGHIO20233471,
title = {Knowledge Organization System for Partial Automation to Improve the Security Posture of IoMT Networks},
journal = {Procedia Computer Science},
volume = {225},
pages = {3471-3478},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.342},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923015004},
author = {Kulsoom Saima Bughio},
keywords = {Internet of Medical Things, Remote Patient Monitoring, Medical Devices, Knowledge Organization System, SWRL Rules, Automated Reasoning, SPARQL},
abstract = {Remote patient monitoring is a healthcare delivery model that uses technology to collect and transmit patient data from a remote location to healthcare providers for analysis and treatment. Remote patient monitoring systems rely on a network infrastructure to gather and transmit data from patients to healthcare providers through a network. While these systems become more prevalent, they may also become targets for cyberattacks. This paper deals with the development of a domain ontology to facilitate partial automation to improve the security posture of IoT networks used in remote patient monitoring. For this purpose, it captures the semantics of the concepts and properties of the main security aspects of IoT medical devices. This is complemented by a comprehensive ruleset, evaluated by using SPARQL queries, and automated reasoning over the aggregated knowledge.}
}
@article{TIELMAN2024102685,
title = {Explainable AI for all - A roadmap for inclusive XAI for people with cognitive disabilities},
journal = {Technology in Society},
volume = {79},
pages = {102685},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102685},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X24002331},
author = {Myrthe L. Tielman and Mari Carmen Suárez-Figueroa and Arne Jönsson and Mark A. Neerincx and Luciano {Cavalcante Siebert}},
keywords = {Explainable AI (XAI), Cognitive disability, Responsible AI},
abstract = {Artificial intelligence (AI) is increasingly prevalent in our daily lives, setting specific requirements for responsible development and deployment: The AI should be explainable and inclusive. Despite substantial research and development investment in explainable AI, there is a lack of effort into making AI explainable and inclusive to people with cognitive disabilities as well. In this paper, we present the first steps towards this research topic. We argue that three main questions guide this research, namely: 1) How explainable should a system be?; 2) What level of understanding can the user reach, and what is the right type of explanation to help them reach this level?; and 3) How can we implement an AI system that can generate the necessary explanations? We present the current state of the art in research on these three topics, the current open questions and the next steps. Finally, we present the challenges specific to bringing these three research topics together, in order to eventually be able to answer the question of how to make AI systems explainable also to people with cognitive disabilities.}
}
@article{KIM2019,
title = {Developing a Physical Activity Ontology to Support the Interoperability of Physical Activity Data},
journal = {Journal of Medical Internet Research},
volume = {21},
number = {4},
year = {2019},
issn = {1438-8871},
doi = {https://doi.org/10.2196/12776},
url = {https://www.sciencedirect.com/science/article/pii/S1438887119002012},
author = {Hyeoneui Kim and Jessica Mentzer and Ricky Taira},
keywords = {exercise, leisure activities, health information interoperability, terminology as topic},
abstract = {Background
Physical activity data provides important information on disease onset, progression, and treatment outcomes. Although analyzing physical activity data in conjunction with other clinical and microbiological data will lead to new insights crucial for improving human health, it has been hampered partly because of the large variations in the way the data are collected and presented.
Objective
The aim of this study was to develop a Physical Activity Ontology (PACO) to support structuring and standardizing heterogeneous descriptions of physical activities.
Methods
We prepared a corpus of 1140 unique sentences collected from various physical activity questionnaires and scales as well as existing standardized terminologies and ontologies. We extracted concepts relevant to physical activity from the corpus using a natural language processing toolkit called Multipurpose Text Processing Tool. The target concepts were formalized into an ontology using Protégé (version 4). Evaluation of PACO was performed to ensure logical and structural consistency as well as adherence to the best practice principles of building an ontology. A use case application of PACO was demonstrated by structuring and standardizing 36 exercise habit statements and then automatically classifying them to a defined class of either sufficiently active or insufficiently active using FaCT++, an ontology reasoner available in Protégé.
Results
PACO was constructed using 268 unique concepts extracted from the questionnaires and assessment scales. PACO contains 225 classes including 9 defined classes, 20 object properties, 1 data property, and 23 instances (excluding 36 exercise statements). The maximum depth of classes is 4, and the maximum number of siblings is 38. The evaluations with ontology auditing tools confirmed that PACO is structurally and logically consistent and satisfies the majority of the best practice rules of ontology authoring. We showed in a small sample of 36 exercise habit statements that we could formally represent them using PACO concepts and object properties. The formal representation was used to infer a patient activity status category of sufficiently active or insufficiently active using the FaCT++ reasoner.
Conclusions
As a first step toward standardizing and structuring heterogeneous descriptions of physical activities for integrative data analyses, PACO was constructed based on the concepts collected from physical activity questionnaires and assessment scales. PACO was evaluated to be structurally consistent and compliant to ontology authoring principles. PACO was also demonstrated to be potentially useful in standardizing heterogeneous physical activity descriptions and classifying them into clinically meaningful categories that reflect adequacy of exercise.}
}
@article{KAMRAN2023100797,
title = {SemanticHadith: An ontology-driven knowledge graph for the hadith corpus},
journal = {Journal of Web Semantics},
volume = {78},
pages = {100797},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100797},
url = {https://www.sciencedirect.com/science/article/pii/S1570826823000264},
author = {Amna Binte Kamran and Bushra Abro and Amna Basharat},
keywords = {Knowledge graph, Linked data, Semantic web, Hadith, Quran, Ontology},
abstract = {Hadith is an essential and much-celebrated resource for the Islamic domain. It is one of the two primary sources of Islamic legislation. The hadith corpus is quite large, consisting of the collection of sayings, actions and silent approval of the Prophet Muhammad. Minimal efforts have been made to date, towards unified semantic modelling, and knowledge representation of the hadith structure for enhanced interlinking and knowledge discovery. This paper presents the design, development and publishing of the hadith corpus as a knowledge graph. First, we design the SemanticHadith ontology to describe and relate core structural concepts from the hadith. We then publish the six prominent hadith collections as an RDF-Based hadith knowledge graph, which is an effort towards making the available hadith both human and machine-readable. This is the first step in the annotation and linking process of the hadith corpus aimed at enabling semantic search capabilities to support scholars, students, and researchers in the creation, evolution, and consultation of a digital representation of Islamic knowledge. The SemanticHadith knowledge graph is freely accessible at http://www.semantichadith.com.}
}
@article{ROMERO2022100409,
title = {A hybrid deep learning and ontology-driven approach to perform business process capability assessment},
journal = {Journal of Industrial Information Integration},
volume = {30},
pages = {100409},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100409},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000760},
author = {Marcelo Romero and Wided Guédria and Hervé Panetto and Béatrix Barafort},
keywords = {Deep learning, Long Short-Term Memory Network, Ontology, Process capability assessment},
abstract = {Enterprises are constantly transforming to adapt to an ever-changing and competitive environment. In this context, assessments allow to understand the state of different organisational aspects before performing transformation activities. One of these aspects is the capability of business processes. Evaluating the quality of business processes is relevant to guide improvement initiatives, considering that the way that processes are designed and executed in organisations has direct impact on the quality of products and services. However, assessments are expensive in terms of resources if they are performed by humans. In this sense, recent trends in Artificial Intelligence provide means to improve process capability assessment through the automation of some of its tasks. Following this line, this work presents a method to perform process capability assessment using raw text as input data with the aid of a smart system, able to reduce the need of human intervention to provide reliable assessment results. For this purpose, we introduce a hybrid approach to perform assessments in enterprises using text data as assessment evidence. The method combines the Long Short-Term Memory Network (LSTM) approach and the use of an Ontology named Process Capability Assessment Ontology (PCAO), which also contains a set of rules to calculate process attribute ratings, capability levels, among other aspects. The approach is grounded on the Smart Assessment Framework, a conceptual model devised to guide the development of intelligent assessments in enterprises. We introduce a demonstration of the assessment of a process based on the management of chemical samples from a research institute.}
}
@article{ELSAPPAGH2021680,
title = {Alzheimer’s disease progression detection model based on an early fusion of cost-effective multimodal data},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {680-699},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20329824},
author = {Shaker El-Sappagh and Hager Saleh and Radhya Sahal and Tamer Abuhmed and S.M. Riazul Islam and Farman Ali and Eslam Amer},
keywords = {Alzheimer disease, Machine learning, Multimodal data analysis, Disease progression detection},
abstract = {Alzheimer’s disease (AD) is a severe neurodegenerative disease. The identification of patients at high risk of conversion from mild cognitive impairment to AD via earlier close monitoring, targeted investigations, and appropriate management is crucial. Recently, several machine learning (ML) algorithms have been used for AD progression detection. Most of these studies only utilized neuroimaging data from baseline visits. However, AD is a complex chronic disease, and usually, a medical expert will analyze the patient’s whole history when making a progression diagnosis. Furthermore, neuroimaging data are always either limited or not available, especially in developing countries, due to their cost. In this paper, we compare the performance of five widely used ML algorithms, namely, the support vector machine, random forest, k-nearest neighbor, logistic regression, and decision tree to predict AD progression with a prediction horizon of 2.5 years. We use 1029 subjects from the Alzheimer’s disease neuroimaging initiative (ADNI) database. In contrast to previous literature, our models are optimized using a collection of cost-effective time-series features including patient’s comorbidities, cognitive scores, medication history, and demographics. Medication and comorbidity text data are semantically prepared. Drug terms are collected and cleaned before encoding using the therapeutic chemical classification (ATC) ontology, and then semantically aggregated to the appropriate level of granularity using ATC to ensure a less sparse dataset. Our experiments assert that the early fusion of comorbidity and medication features with other features reveals significant predictive power with all models. The random forest model achieves the most accurate performance compared to other models. This study is the first of its kind to investigate the role of such multimodal time-series data on AD prediction.}
}
@article{BOUYERBOU2019232,
title = {Geographic ontology for major disasters: Methodology and implementation},
journal = {International Journal of Disaster Risk Reduction},
volume = {34},
pages = {232-242},
year = {2019},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2018.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S221242091830476X},
author = {Hafidha Bouyerbou and Kamal Bechkoum and Richard Lepage},
keywords = {Information retrieval, Major disasters, Ontology, Ontology web language (OWL), Reasoning, Semantics},
abstract = {During a catastrophic event, the International Charter11http://www.disasterscharter.org/. "Space and Major Disasters" is regularly activated and provides the rescue teams damage maps prepared by a photo-interpreter team basing on pre and post-disaster satellite images. A satellite image manual processing must be accomplished in most cases to build these maps, a complex and demanding process. Given the importance of time in such critical situations, automatic or semiautomatic tools are highly recommended. Despite the quick treatment presented by automatic processing, it usually presents a semantic gap issue. Our aim is to express expert knowledge using a well-defined knowledge representation method: ontologies and make semantics explicit in geographic and remote sensing applications by taking the ontology advantages in knowledge representation, expression, and knowledge discovery. This research focuses on the design and implementation of a comprehensive geographic ontology in the case of major disasters, that we named GEO-MD, and illustrates its application in the case of Haiti 2010 earthquake. Results show how the ontology integration reduces the semantic gap and improves the automatic classification accuracy.}
}
@article{MUSIARI2024111776,
title = {Towards computer-aided hygienic design: Definition of a knowledge-based system for food processing equipment},
journal = {Journal of Food Engineering},
volume = {363},
pages = {111776},
year = {2024},
issn = {0260-8774},
doi = {https://doi.org/10.1016/j.jfoodeng.2023.111776},
url = {https://www.sciencedirect.com/science/article/pii/S0260877423003746},
author = {Francesco Musiari and Fabrizio Moroni and Alessandro Pirondi and Claudio Favi},
keywords = {Hygienic design, Knowledge-based system, Design for manufacturing, Food equipment, Food industry, Engineering design},
abstract = {Hygienic design requires the definition of rules allowing the correct development of food processing systems. The knowledge collection in this field would certainly help designers and engineers in developing hygienic-compliant systems. This paper aims to provide a knowledge-based (KB) system for gathering hygienic design guidelines for the design of food processing machinery and equipment. The KB system is based on a specific ontology that has been used to collect 78 hygienic design rules from different sources. The rules repository can be considered a backbone for the subsequent development of a CAD-based tool for an automatic search and detection of non-compliant design features. Starting with a CAD model, the KB system was used to check the compliance of a fish stick production machinery. Results highlight how the adoption of the KB system in the early design phase would anticipate hygienic design issues avoiding several design reviews.}
}
@article{LORVAOANTUNES2024200366,
title = {Ontology-based BIM-AMS integration in European Highways},
journal = {Intelligent Systems with Applications},
volume = {22},
pages = {200366},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200366},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324000425},
author = {António {Lorvão Antunes} and José Barateiro and Vânia Marecos and Jelena Petrović and Elsa Cardoso},
keywords = {Building Information Modeling (BIM), Decision support, Risk and condition data, Ontology development, Ontology validation},
abstract = {BIM tools enable decision-making during the lifecycle of engineering structures, such as bridges, tunnels, and roads. National Road Authorities use Asset Management Systems (AMS) to manage and monitor operational information of assets from European Highways, including access to sensor and inspection data. Interoperability between BIM and AMS systems is vital for a timely and effective decision-making process during the operational phase of these assets. The European project Connected Data for Effective Collaboration (CoDEC) designed a framework to support the connections between AMS and BIM platforms, using linked data principles. The CoDEC Data Dictionary was developed to provide standard data formats for AMS used by European NRA. This paper presents the design and development of an Engineering Structures ontology used to encode the shared conceptualization provided by the CoDEC Data Dictionary. The ontology is evaluated, validated, and demonstrated as a base for data exchange between BIM and AMS.}
}
@article{SUNG2020103504,
title = {A knowledge-based system to find over-the-counter medicines for self-medication},
journal = {Journal of Biomedical Informatics},
volume = {108},
pages = {103504},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103504},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420301325},
author = {Han-Yu Sung and Yu-Liang Chi},
keywords = {Self-medication, Over-the-counter medicine, Semantic Web, Open data, SPARQL},
abstract = {This study developed a medicine query system based on Semantic Web and open data especially for self-medication users to search over-the-counter (OTC) medicines. Most existing medicine query systems are based on keyword searches. If users are uncertain about the exact search words, these query systems do not offer effective help. Furthermore, most systems provide inadequate explanations of symptoms and ailments for users to use with confidence. To remedy these issues, this study builds a knowledge base to enable inference-based searches and data mashup for integrating information from across the Web. Three components were identified: (1) building an ontology model to describe the relationships between ailments and symptoms; (2) upgrading medicinal product datasets to link them with the ontology model on a semantic level; and (3) developing a data mashup to integrate web resources to help users to find references. Furthermore, the aim was to develop a web-based application that utilizes inference mechanisms to provide users with tools for interactive manipulation. A pilot experiment for skin ailments was implemented to learn the problem-solving skills of the system. Finally, two experts utilized a content validity index to rate a four-dimension 15-item scale. The evaluation results show that experts found the proposed system excellent for content validity.}
}
@article{CIMMINO202354,
title = {A scalable, secure, and semantically interoperable client for cloud-enabled Demand Response},
journal = {Future Generation Computer Systems},
volume = {141},
pages = {54-66},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003648},
author = {Andrea Cimmino and Juan Cano-Benito and Alba Fernández-Izquierdo and Christos Patsonakis and Apostolos C. Tsolakis and Raúl García-Castro and Dimosthenis Ioannidis and Dimitrios Tzovaras},
keywords = {Internet of energy, Demand response, Semantic interoperability},
abstract = {Demand Response (DR) is becoming a cornerstone element in the current energy sector, particularly for the EU energy markets. For this reason, considerable effort has been spent on standardising demand response data models. As a result, there is an ever-growing number of demand response proposals based on these standards. However, these proposals are usually centralised, and those that rely on cloud solutions use the cloud as a centralised data store assuming that the data is already homogenised when stored, i.e. all the data has the same format and model. Nevertheless, in practise, DR proposals rely on several components that provide data in heterogeneous formats and models. Furthermore, the different DR standards define models for different data formats that hinder data exchange between different DR systems. In this article, a generic tool called CIM is presented, which allows existing DR systems to distribute their components in the cloud, providing a solid security and privacy framework for data exchange. In addition, the CIM implements a semantic interoperability layer that is capable of translating data into a normalised form when exchanged so that it can be transparently consumed by DR components. Experiments advocate the CIM as a solution for DR systems to decentralise their architectures and exchange heterogeneous data even with other DR systems that follow different DR standards.}
}
@article{TURCHET2022100687,
title = {The Smart Musical Instruments Ontology},
journal = {Journal of Web Semantics},
volume = {72},
pages = {100687},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100687},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000573},
author = {Luca Turchet and Paolo Bouquet and Andrea Molinari and György Fazekas},
keywords = {Smart Musical Instruments, Internet of Musical Things, Semantic audio},
abstract = {The Smart Musical Instruments (SMIs) are an emerging category of musical instruments that belongs to the wider class of Musical Things within the Internet of Musical Things paradigm. SMIs encompass sensors, actuators, embedded intelligence, and wireless connectivity to local networks and to the Internet. Interoperability represents a key issue within this domain, where heterogeneous SMIs are envisioned to exchange information between each other and a plethora of Musical Things. This paper proposes an ontology for the representation of the knowledge related to SMIs, with the aim of facilitating interoperability between SMIs as well as with other Musical Things interacting with them. There was no previous comprehensive data model for the SMIs domain, however the new ontology relates to existing ontologies, including the SOSA Ontology for the representation of sensors and actuators, the Audio Effects Ontology dealing with the description of digital audio effects, and the IoMusT Ontology for the representation Musical Things and IoMusT ecosystems. This paper documents the design of the ontology and its evaluation with respect to specific requirements gathered from an extensive literature review, which was based on scenarios involving SMIs stakeholders, such as performers and studio producers. The SMI Ontology can be accessed at: https://w3id.org/smi#.}
}
@article{AMADORDOMINGUEZ202185,
title = {An ontology-based deep learning approach for triple classification with out-of-knowledge-base entities},
journal = {Information Sciences},
volume = {564},
pages = {85-102},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.02.018},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521001602},
author = {Elvira Amador-Domínguez and Emilio Serrano and Daniel Manrique and Patrick Hohenecker and Thomas Lukasiewicz},
keywords = {Knowledge graph embeddings, Entity initialization, Knowledge graph completion, Word embeddings, Ontological information},
abstract = {Knowledge graphs (KGs) are one of the most common frameworks for knowledge representation. However, they suffer from a severe scalability problem that hinders their usage. KG embedding aims to provide a solution to this issue. Nonetheless, general approaches are incapable of representing and reasoning about information not previously contained in the graph. This paper proposes to leverage semantic and ontological information for a significant benefit of knowledge graph completion, focusing on triple classification. The goal of this task is to determine whether a given fact holds. Furthermore, this paper also considers the classification of facts that include entities that have not been seen during training, denoted out-of-knowledge-base or OOKB entities. An incremental method is presented, composed of six stages. Although the proposal can be applied to any KG embedding model, this work focuses on its application for semantic matching models, such as ComplEx and DistMult. Compared to other approaches, our proposal is model-agnostic, computationally inexpensive, and does not require retraining. The results show that triple classification accuracy scales up to 15% with the proposed approach, as well as accelerating the convergence of the model to its optimal solution. Furthermore, facts containing OOKB entities can be classified with a reasonable accuracy.}
}
@article{DRAGOS2022593,
title = {Ontology Adaptation for Opinion Mining in French Corpora},
journal = {Procedia Computer Science},
volume = {207},
pages = {593-603},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.114},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922009954},
author = {Valentina Dragos and Adrien Legros},
keywords = {Opinion mining, Social data analysis, Appraisal theory},
abstract = {This paper presents the development of an ontology for opinion mining in French corpora. Since ontology construction from scratch is expensive and time consuming, the model is built by adapting an existing ontology modeling appraisal categories in English. The construction method consists of two main steps: first, the ontology of appraisals is translated in French by using a concept-to-concept approach; then different adaptation strategies are implemented to improve the result of this translation. Adaptation strategies are based on text mining, weakly supervised methods and the use of WordNet to refine the model. The goal of the adaptation phase is to cope with limitations of concept-to-concept translation, to integrate concepts and relations from external corpora and resources, and to build a model that captures various features of opinions. The ontology was designed and build to incorporate linguistic and extra linguistic information on the description of opinions in French. The model was validated by using both expert insights to validate the relevance of concepts and relations and formal criteria to describe the ontology qualities for practical use.}
}
@article{OCHOA2025128792,
title = {I40GO: A global ontology for industry 4.0},
journal = {Expert Systems with Applications},
volume = {294},
pages = {128792},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128792},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425024108},
author = {William Ochoa and Javier Cuenca and Felix Larrinaga and Alain Pérez},
keywords = {Semantic ontology, Ontology reuse, Industry 4.0, Context awareness, Workflow management},
abstract = {Over the last two decades, semantic ontologies have been developed to represent manufacturing data across various domains. These ontologies constitute the knowledge base of manufacturing management systems, which primarily focus on optimizing the manufacturing process and improving its resilience. The ontologies developed in the Industry 4.0 domain are heterogeneous, hindering the interoperability of machines, devices, and applications composing manufacturing systems. Consequently, a demand arises for an ontology that provides common vocabularies to represent the data domains inherent to Industry 4.0. A global Industry 4.0 ontology must be easily reusable in different application contexts. This paper presents I40GO: a global ontology tailored to the Industry 4.0 domain. I40GO structures in layers and modules the knowledge represented in the Industry 4.0 most relevant ontologies. The MODDALS methodology is followed to classify knowledge into different layers. This methodology classifies ontology knowledge into common, variant, and application-specific layers following a similar approach to that of Software Product Lines (SPL). I40GO assists ontology engineers in developing domain-specific ontologies for manufacturing systems and enhances interoperability among applications. This work provides an overview of I40GO, emphasizing its development methodology and its modular and layered structure. Furthermore, it demonstrates the reuse of the I40GO ontology within an Industry 4.0 use case—an architecture for context-aware workflow management.}
}
@article{EIDEN2021690,
title = {Supporting semantic PLM by using a lightweight engineering metadata mapping engine},
journal = {Procedia CIRP},
volume = {100},
pages = {690-695},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.146},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121006211},
author = {Andreas Eiden and Thomas Eickhoff and Jonas Gries and Jens C. Göbel and Thomas Psota},
keywords = {PLM, Data Integration, Data Modeling, Data Mapping, Interoperability, Open Web Standards},
abstract = {In order to handle a high variety of interdisciplinary processes and complex smart products, integration platforms are a useful approach to view and access data all along the product lifecycle, which is stored in different data management solutions like Product Lifecycle Management (PLM), Enterprise Resource Planning (ERP) and authoring systems. Here, the Metadata Repository for Semantic Product Lifecycle Management (SP²IDER) could serve as a supporting integration platform. An additional information layer on top of data source systems like PLM and ERP provides additional information, links data objects from different source systems, and provides access to these data. The SP²IDER platform consists of three basic parts: Connector units to fetch data from the source systems, a core unit with a Service Directory and a Mapping Engine, and a Metadata Store, where information about data objects is stored. This paper focuses on the view inside the Mapping Engine and the Metadata Store. The mapping engine has a three-way approach for the mapping of data objects and data types: The initial manual mapping at the data type level, second a rule-based, and third a machine-learning mapping at the data object level. This paper describes the manual mapping process, how the mapped data objects are stored inside the Metadata Store, and how this leads to a newly formed lightweight data model, that does not need heavyweight ontologies.}
}
@article{BAYOUDHI20214249,
title = {An Overview of Biomedical Ontologies for Pandemics and Infectious Diseases Representation},
journal = {Procedia Computer Science},
volume = {192},
pages = {4249-4258},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.201},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921019402},
author = {Leila Bayoudhi and Najla Sassi and Wassim Jaziri},
keywords = {Pandemics, Infectious diseases, Biomedical, Ontology, Representation},
abstract = {Several infectious diseases and pandemics have so far emerged. Pandemics are by nature rapidly evolving. In this context, COVID-19 cases, seen recently in a growing number of countries around the world, have been increasing exponentially. So, researchers and responsible actors should take quick decisions to mitigate the spread of such diseases. To do so, several computer science solutions, including ontologies, have been proposed to cope with these issues and save humanity. The ontology is the key formalism which allows modelling knowledge along with its semantics in a formal way. Indeed, the ontology provides unambiguous definitions of a discourse’s domain terms in a machine understandable way. Particularly, biomedical ontologies have ever been developed to capture and represent pandemics and infectious diseases. In this context, this paper aims to scrutinize and study these state-of-the-art ontologies.}
}
@article{HOSSEINIGOURABPASI2024109022,
title = {BIM-based automated fault detection and diagnostics of HVAC systems in commercial buildings},
journal = {Journal of Building Engineering},
volume = {87},
pages = {109022},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.109022},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224005904},
author = {Arash {Hosseini Gourabpasi} and Mazdak Nik-Bakht},
keywords = {Digital twin, BIM, HVAC, AFDD, Ontology},
abstract = {In order to meet the growing demand for effective Automated Fault Detection and Diagnostics (AFDD) for HVAC systems, innovative approaches are needed to address limitations in data diversity and access to contextual information. This study introduces a methodology that leverages Building Information Modeling (BIM) to enhance the development of the AFDD model. Feature engineering techniques are utilized to generate dynamic BIM features, compensating for the lack of sensory and contextual data in Building Management Systems (BMS). By integrating AFDD analytics with BIM, a comprehensive digital twin of the facility is created, which enables facility managers to compare, reuse, and develop AFDD models for HVAC systems. The proposed methodology demonstrates the potential of leveraging BIM-based knowledge models to overcome the challenges associated with the limited sensor and contextual information availability by utilizing BIM for feature generation and, conversely, updating the BIM model with AFDD analytics.}
}
@article{TESOLIN2023577,
title = {Enhancing heterogeneous mobile network management based on a well-founded reference ontology},
journal = {Future Generation Computer Systems},
volume = {149},
pages = {577-593},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003084},
author = {Julio Cesar Cardoso Tesolin and André M. Demori and David Fernandes Cruz Moura and Maria Cláudia Cavalcanti},
keywords = {Mobile networks, Seamless handover, Conceptual modeling, Ontology, Semantic decision, Knowledge graph},
abstract = {The fulfillment of the Always Best Connected & Served concept in future mobile wireless networks depends on their ability to maintain a seamless association between the user’s equipment and the network. As a result, many researchers and developers have proposed several decision-support mechanisms to cope with this challenge. One of the promising mechanisms to aid mobile wireless networks in achieving ubiquitous coverage with a seamless connection is semantic reasoning. However, once it relies on ontologies, proper representation of Link and Connection – entities that bind the communication nodes – is paramount. Unfortunately, although several network-related ontologies present these concepts, they are unclear on which entities they bind, nor do they present them simultaneously. Also, the same ambiguity can be perceived in vocabulary recommendations of several telecommunication standard bodies. Our main contribution is the ontological analysis that clarifies the definition of Link and Connection in telecommunication domain. Besides, we analyze their dependencies while refining other concepts such as Medium, Server, and Neighbor. Thus, we provide the foundations for a new network-related ontology to support mobility management in wireless networks.}
}
@article{CAMPOS2020104813,
title = {Finding reusable structured resources for the integration of environmental research data},
journal = {Environmental Modelling & Software},
volume = {133},
pages = {104813},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104813},
url = {https://www.sciencedirect.com/science/article/pii/S1364815219307078},
author = {Patricia M.C. Campos and Cassio C. Reginato and João Paulo A. Almeida and Monalessa P. Barcellos and Ricardo {de Almeida Falbo} and Vítor E. {Silva Souza} and Giancarlo Guizzardi},
keywords = {Data integration, Environmental research data, Knowledge resources, Reuse, Systematic search, Ontology},
abstract = {Successful data integration requires careful examination of data semantics, a task that has often been approached with the use of ontologies. However, there are some barriers to build ontologies for data integration in complex domains such as the environmental one. A relevant problem is the development of new ontologies disregarding previous knowledge resources such as reference models and vocabularies. This paper addresses this challenge by proposing a systematic approach (dubbed CLeAR) for the identification and selection of reusable artifacts for building ontologies with the purpose of research data integration. CLeAR follows some principles of the systematic literature reviews, supporting the search for structured resources in the scientific literature. We apply CLeAR to the environmental domain. A total of 543 publications were surveyed. The results obtained provide a set of 75 structured resources for the environmental domain, evaluated according domain coverage and some quality attributes (e.g., proper documentation, community acceptance).}
}
@article{AHMED2023102428,
title = {Recursive approach to combine expert knowledge and data-driven RSW weldability certification decision making process},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {79},
pages = {102428},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102428},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001132},
author = {Fahim Ahmed and Kyoung-Yun Kim},
keywords = {Resistance spot welding, Expert knowledge integration, Ontological knowledge, Weldability certification, Recursive approach},
abstract = {Data-driven techniques have shown promising results in the analysis and understanding of complex welding processes. Data analytics play a significant role to turn data into valuable insights to assist in the weldability certification decision-making for Resistance Spot Welding (RSW) as well. However, to successfully perform the associated data analytics, domain knowledge is essential to construct more ‘sense-making’ analytics models, as often the models cannot properly capture the nuances of the domain and do not properly indicate the relationship among the RSW concepts and parameters. Thus, machine learning models developed from rough experimental data often do not provide models meaningful and sensible to the domain expert. In this article, we employ a recursive approach between the domain experts and data-driven models so that the knowledge of the domain experts can be integrated into the weldability certification decision-making process. An ontology-based semantic knowledge framework supports this recursive communication while helping the experts to instil more confidence in the developed analytics models. The collaborative and recursive approach implemented in this study helps the domain experts to tap into their domain knowledge and form expert opinions using the formalized semantic RSW concepts and decision rules. The expert opinions are then used to learn new knowledge about the RSW domain and transform the RSW datasets by incorporating significant features that were not included in the earlier models. The transformed datasets help us to develop improved machine learning models, which in turn work as a new source of semantic knowledge, as we have discovered through our pilot implementation.}
}
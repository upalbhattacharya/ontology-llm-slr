@article{STOREY2025102480,
title = {Large language models for conceptual modeling: Assessment and application potential},
journal = {Data & Knowledge Engineering},
volume = {160},
pages = {102480},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102480},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000758},
author = {Veda C. Storey and Oscar Pastor and Giancarlo Guizzardi and Stephen W. Liddle and Wolfgang Maaß and Jeffrey Parsons and Jolita Ralyté and Maribel Yasmina Santos},
keywords = {Conceptual modeling, Large language models, Artificial intelligence, Semantics},
abstract = {Large Language Models (LLMs) are being rapidly adopted for many activities in organizations, business, and education. Included in their applications are capabilities to generate text, code, and models. This leads to questions about their potential role in the conceptual modeling part of information systems development. This paper reports on a panel presented at the 43rd International Conference on Conceptual Modeling where researchers discussed the current and potential role of LLMs in conceptual modeling. The panelists discussed applications and interest levels and expressed both optimism and caution in the adoption of LLMs. Suggested is a need for much continued research by the conceptual modeling community on LLM development and their role in research and teaching.}
}
@article{WISNIEWSKI2019100534,
title = {Analysis of Ontology Competency Questions and their formalizations in SPARQL-OWL},
journal = {Journal of Web Semantics},
volume = {59},
pages = {100534},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.100534},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300617},
author = {Dawid Wiśniewski and Jedrzej Potoniec and Agnieszka Ławrynowicz and C. Maria Keet},
keywords = {Ontology Authoring, Competency Questions, SPARQL-OWL},
abstract = {Competency Questions (CQs) are natural language questions outlining and constraining the scope of knowledge represented in an ontology. Despite that CQs are a part of several ontology engineering methodologies, the actual publication of CQs for the available ontologies is very limited and even scarcer is the publication of their respective formalizations in terms of, e.g., SPARQL queries. This paper aims to contribute to addressing the myriad of engineering hurdles to using CQs in ontology development. A prerequisite to this is to understand the relation between CQs and the queries over the ontology. We use a new dataset of 234 competency questions and their SPARQL-OWL queries for several ontologies in different domains developed by different groups, and analysed the CQs in two principal ways. The first stage focused on a linguistic analysis of the natural language text itself, i.e., a lexico-syntactic analysis without any presuppositions of ontology elements, and a subsequent step of semantic analysis in order to find patterns. This increased diversity of CQ sources resulted in a 4-5-fold increase of hitherto published patterns, to 106 distinct CQ patterns, which have a limited subset of few patterns shared across the CQ sets from the different ontologies. Next, we analysed the relation between the found CQ patterns and their respective SPARQL-OWL patterns, which revealed that one CQ pattern may be realized by more than one SPARQL-OWL query pattern, and vice versa. These insights may contribute to establishing common practices, templates, automation, and user tools that will support CQ formulation, formalization, execution, and general management.}
}
@article{LOPES2022117291,
title = {Predicting the top-level ontological concepts of domain entities using word embeddings, informal definitions, and deep learning},
journal = {Expert Systems with Applications},
volume = {203},
pages = {117291},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117291},
url = {https://www.sciencedirect.com/science/article/pii/S095741742200656X},
author = {Alcides Gonçalves Lopes and Joel Luis Carbonera and Daniela Schimidt and Mara Abel},
keywords = {Ontology learning, Deep learning, Well-founded ontology},
abstract = {Ontology development is a challenging task that encompasses many time-consuming activities. One of these activities is the classification of the domain entities (concepts and instances) according to top-level concepts. This activity is usually performed manually by an ontology engineer. However, when the set of entities increases in size, associating each entity to the proper top-level ontological concept becomes challenging and requires a high level of expertise in both the target domain and ontology engineering. This paper proposes a deep learning approach that automatically classifies domain entities into top-level concepts using their informal definitions and the word embedding of the terms that represent them. From these inputs, we feed a deep neural network consisting of two modules: a feed-forward neural network and a bi-directional recurrent neural network with long short-term units. Our architecture combines both outputs of these modules into a dense layer and provides the probabilities of each candidate class. For validating our proposal, we have developed a dataset based on the OntoWordNet ontology, which provides a classification of WordNet synsets into concepts specified by DOLCE-lite-plus top-level ontology. Our experiments show that our proposal outperforms the baseline approaches by 6% regarding the F-score. In addition, our proposal is less affected by the polysemy in the terms that represent the domain entities than the compared approaches. Consequently, our proposal can consider more instances during its training than the baseline methods.}
}
@article{ROMANENKO2024102342,
title = {Evaluating quality of ontology-driven conceptual models abstractions},
journal = {Data & Knowledge Engineering},
volume = {153},
pages = {102342},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102342},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000661},
author = {Elena Romanenko and Diego Calvanese and Giancarlo Guizzardi},
keywords = {Conceptual model abstraction, Ontology-driven conceptual models, Quality evaluation of abstractions, Unified foundational ontology (UFO), FAIR model catalog, User studies in conceptual modeling},
abstract = {The complexity of an (ontology-driven) conceptual model highly correlates with the complexity of the domain and software for which it is designed. With that in mind, an algorithm for producing ontology-driven conceptual model abstractions was previously proposed. In this paper, we empirically evaluate the quality of the abstractions produced by it. First, we have implemented and tested the last version of the algorithm over a FAIR catalog of models represented in the ontology-driven conceptual modeling language OntoUML. Second, we performed three user studies to evaluate the usefulness of the resulting abstractions as perceived by modelers. This paper reports on the findings of these experiments and reflects on how they can be exploited to improve the existing algorithm.}
}
@article{JAVED2021106558,
title = {iMER: Iterative process of entity relationship and business process model extraction from the requirements},
journal = {Information and Software Technology},
volume = {135},
pages = {106558},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106558},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000422},
author = {Muhammad Javed and Yuqing Lin},
keywords = {Entity relationship model, Business process model, General requirements, User stories, Use case specification, Natural language processing},
abstract = {Context
Extracting conceptual models, e.g., entity relationship model or Business Process model, from software requirement document is an essential task in the software development life cycle. Business process model presents a clear picture of required system's functionality. Operations in business process model together with the data entity consumed, help the software developers to understand the database design and operations to be implemented. Researchers have been aiming at automatic extraction of these artefacts from the requirement document.
Objective
In this paper, we present an automated approach to extract the entity relationship and business process models from requirements, which are possibly in different formats such as general requirements, use case specification and user stories. Our approach is based on the efficient natural language processing techniques.
Method
It is an iterative approach of Models Extraction from the Requirements (iMER). iMER has multiple iterations where each iteration is to address a sub-problem. In the first iteration, iMER extracts the data entities and attributes. Second iteration is to find the relationships between data entities, while extracting cardinalities is in the third step. Business process model is generated in the fourth iteration, containing the external (actors’) and internal (system's) operations.
Evaluation
To evaluate the performance and accuracy of iMER, experiments are conducted on various formats of the requirement documents. Additionally, we have also evaluated our approaches using the requirement documents which been modified by shuffling the sentences and by merging with other requirements. Comparative study is also performed. The preliminary results show a noticeable improvement.
Conclusion
The iMER is an efficient automated iterative approach that is able to extract the conceptual models from the various formats of requirements.}
}
@article{WU2025338,
title = {Design of Intelligent Q&A System Based on Knowledge Graph Combined with Large Language Model},
journal = {Procedia Computer Science},
volume = {262},
pages = {338-347},
year = {2025},
note = {The 5th International Conference on Multi-modal Information Analytics (MMIA)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.05.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925019088},
author = {Quanquan Wu},
keywords = {Large Language Model, LLM, Knowledge Graph, KG, Intelligent Question-Answering, Q&A, System Design},
abstract = {LLM can transform natural language questions into structured queries, and the KG-based Q&A system can provide accurate and reliable answers. The combination of LLM and KG is the key technology to support the modern Q&A model. Through the two-wheel drive of structured knowledge and semantic understanding, the processing ability of complex problems is significantly improved, and the Q&A system is jointly promoted from the primary to the advanced intelligence evolution. In this paper, based on LLM and KG, knowledge distillation based LLM fusion KG technology is studied, so that the target model can absorb the advantages of both, not only have the language processing capability of large models, but also use the structured knowledge of KG to improve performance and interpretability. On this basis, the multi-layer architecture of intelligent Q&A system is designed, which is easy for developers to work together. The ClaudeKG model constructed in this paper is compared with DeepSeek and Doubao baseline models, and the function and performance are analyzed.}
}
@article{ROVETTO2020451,
title = {Orbital debris ontology, terminology, and knowledge modeling},
journal = {Journal of Space Safety Engineering},
volume = {7},
number = {3},
pages = {451-458},
year = {2020},
note = {Space Debris: The State of Art},
issn = {2468-8967},
doi = {https://doi.org/10.1016/j.jsse.2020.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S2468896720300720},
author = {Robert J. Rovetto and T.S. Kelso and Daniel A. O'Neil},
keywords = {Orbital debris, space debris, ontology, knowledge graph, semantic technology, artificial intelligence, information fusion, space object, knowledge representation and reasoning, knowledge model, semantic model, terminology, taxonomy, classification, metadata, linked data, orbit data, two-line element set, knowledge engineering, space situational awareness, space system ontology},
abstract = {ABSTRACT
The looming threat orbital debris poses to assets in orbit demands solutions. As the orbital population grows, so does the risk of collision, as well as the volume of associated data. It is, however, an opportunity for interdisciplinary innovation and cooperation. This paper focuses on the data management and knowledge modeling aspect of developing solutions for a sustainable and safe orbital space environment. An in-progress work to develop an orbital debris reference ontology [Rovetto, 2015] is summarized in order to discuss knowledge modeling for orbital debris. The formal analysis of this effort can contribute to standards development, as well as terminological and policy challenges. Leveraging the growing volumes of orbital debris and space situational awareness (SSA) data will create a more complete picture of the orbital space environment. Part of the solution will be: consistent and correct data interpretation, sharing orbital debris and SSA data in one form or another, terminology development & harmonization, and knowledge or domain modeling. To facilitate this, [Rovetto, 2015] proposed ontology development for the orbital debris and broader space domain. This paper summarizes concepts from that paper, and subsequently developed concepts [2-9]. See also https://purl.org/space-ontology and https://purl.org/space-ontology/odo. Ontology engineering is an interdisciplinary area of research related to knowledge representation and reasoning in artificial intelligence, model-based systems engineering, semantic technologies and the so-called semantic web. An ontology is effectively a computable and semantically rich terminology that presents a knowledge or domain model for a topic area. Expressions of knowledge, beliefs, or assertions are stored using formally defined terms. This knowledge base is reasoned over to yield answers to database queries, among other things. Ontologies have been developed in knowledge-based projects across various disciplines, and used for such things as search engines, chatbots, and enterprise knowledge graphs. Ontologies aim to support: interoperability, automated reasoning, data sharing and integration, data search and retrieval, and communicating the meaning of data. The Orbital Debris Ontology (ODO) [1], and related ontologies [Rovetto & Kelso 2016] [Rovetto 2016, 2017], were proposed to help achieve this. ODO, for instance, is intended as a domain ontology that can be used across federated databases, offering an explicitly specified set of concepts describing the orbital debris domain. Its meaning-rich taxonomy will provide a sharable semantics for orbital debris data to, in part, consistently communicate the meaning of data to both humans and machines, and tag data elements in space object catalogs to help afford automated reasoning tasks, decision support, knowledge discovery, and information integration. ODO and the SSA ontology (SSAO) [2] is part of the overall Orbital Space Ontology concept, which is conceived as a unifying domain reference ontology. It aims to provide a knowledge representation structure of orbital space, a common semantic model, and develop a sharable terminology. Collectively this will provide common meaning for datasets, a high-level taxonomy or classification for orbital space objects, and a means to characterize space objects. Ongoing efforts have included using visualizations, R, JSON-LD, and contemporary semantic technologies. Potential applications include web-based platforms, web apps, visualizations, modeling and simulations. Community input and participation may yield a more widely understood domain model as well as facilitate terminological standards. For example, conceptual, terminological, and ontological analysis can make helpful contributions to such efforts as the Space Debris Mitigation Requirements by developing more precise, consistent and coherent terms and definitions. Other projects can also use ODO (and its related ontologies) as a common knowledge model, metadata set, taxonomy or vocabulary. Readers interested in supporting development are encouraged to make contact.}
}
@article{FERNANDEZIZQUIERDO202289,
title = {Ontology verification testing using lexico-syntactic patterns},
journal = {Information Sciences},
volume = {582},
pages = {89-113},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521009324},
author = {Alba Fernández-Izquierdo and Raul García-Castro},
keywords = {Ontology testing, Ontology verification, Ontology requirements},
abstract = {Ontology verification refers to the activity where an ontology is tested against its ontology requirements to ensure that it is built correctly in compliance with its ontology requirements specification. Therefore, it is an important activity that should be performed in any ontology development process. Since manual verification can be a time-consuming and repetitive task, testing processes to automatically verify an ontology facilitate this activity. Moreover, the involvement of not only ontology engineers during the ontology verification process, but also domain experts and users, can provide valuable feedback to avoid misunderstandings and lack of information. This paper proposes a method for ontology verification that defines the testing activities to be performed. The method uses a testing language based on lexico-syntactic patterns to facilitate the definition of tests and an ontology to store and publish such tests. Moreover, this verification testing method proposes an online tool to execute tests on one or more ontologies. The method was compared in terms of time and errors by user evaluation with other tools for ontology verification; the evaluation showed that the tools that use testing languages had better results in terms of reducing errors in the verification activity compared to the tools that do not.}
}
@article{VISHWAKARMA2025102468,
title = {Automatic query expansion for enhancing document retrieval system in healthcare application using GAN based embedding and hyper-tuned DAEBERT algorithm},
journal = {Data & Knowledge Engineering},
volume = {160},
pages = {102468},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102468},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000631},
author = {Deepak Vishwakarma and Suresh Kumar},
keywords = {Automatic query expansion, Information retrieval, Generative Adversial Network, Modified Page Ranking Algorithm, Proximity based Keyword Extraction},
abstract = {Query expansion is a useful technique for improving document retrieval systems' dependability and performance. Search engines frequently employ query expansion strategies to improve Information Retrieval (IR) performance and elucidate users' information requirements. Although there are several methods for automatically expanding queries, the list of documents that are returned can occasionally be lengthy and contain a lot of useless information, particularly when searching the Web. As the size of medical document grows, Automatic Query Expansion might struggle with efficiency and real-time application. Thus, Hyper-Tuned Dual Attention Enhanced Bi-directional Encoder Representation from Transformers (HT-DAEBERT) with automatic ranking based query expansion system is created for enhancing medical document retrieval system. Initially, the user's query from the medical corpus document was collected, and it was augmented using the Generative Adversarial Network (GAN) approach. Then augmented text is pre-processed to improve the original text's quality through tokenization, acronym expansion, stemming, stop word removal, hyperlink removal, and spell correction. After that, Keywords are extracted using the Proximity-based Keyword Extraction (PKE) technique from the pre-processed text. Afterwards, the words are converted into vector form by utilizing the Hyper-Tuned Dual Attention Enhanced Bi-directional Encoder Representation from Transformers (HT-DAEBERT) model. In DAEBERT, key parameters such as dropout rate and weight decay were optimally selected by using the Election Optimization Algorithm (EOA). After that, a ranking-based query expansion approach was employed to enhance the document retrieval system. The proposed method achieves an accuracy of 97.60 %, a Hit Rate of 98.30 %, a PPV of 93.40 %, an F1-Score of 95.79 %, and an NPV of 97.50 %. This approach improves the accuracy and relevance of document retrieval in healthcare, potentially leading to better patient care and enhanced clinical outcomes.}
}
@article{VALCALVO2025104042,
title = {OntoGenix: Leveraging Large Language Models for enhanced ontology engineering from datasets},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104042},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104042},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004011},
author = {Mikel Val-Calvo and Mikel {Egaña Aranguren} and Juan Mulero-Hernández and Ginés Almagro-Hernández and Prashant Deshmukh and José Antonio Bernabé-Díaz and Paola Espinoza-Arias and José Luis Sánchez-Fernández and Juergen Mueller and Jesualdo Tomás Fernández-Breis},
keywords = {Knowledge graphs, Large Language Models, Ontology engineering},
abstract = {Knowledge Graphs integrate data from multiple, heterogeneous sources, using ontologies to facilitate data interoperability. Ontology development is a resource-consuming task that requires the collaborative work of domain experts and ontology engineers. Therefore, companies invest considerable resources in order to generate and maintain Enterprise Knowledge Graphs and ontologies from large and complex datasets, most of which can be unfamiliar for ontology engineers. In this work, we study the use of Large Language Models to aid in the development of ontologies from datasets, ultimately increasing the automation of the generation of ontology-based Knowledge Graphs. As a result we have developed a structured workflow that leverages Large Language Models to enhance ontology engineering through data pre-processing, ontology planning, building, and entity improvement. Our method is also able to generate mappings and RDF data, but in this work we focus on the ontologies. The pipeline has been implemented in the OntoGenix tool. In this work we show the results of the application of OntoGenix to six datasets related to commercial activities. The findings indicate that the ontologies produced exhibit patterns of coherent modeling, and features that closely resemble those created by humans, although the most complex situations are better reflected by the ontologies developed by humans.}
}
@article{ZHANG2018102,
title = {Adapted TextRank for Term Extraction: A Generic Method of Improving Automatic Term Extraction Algorithms},
journal = {Procedia Computer Science},
volume = {137},
pages = {102-108},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316144},
author = {Ziqi Zhang and Johann Petrak and Diana Maynard},
keywords = {automatic term extraction, NLP, terminology, ontology engineering},
abstract = {Automatic Term Extraction is a fundamental Natural Language Processing task often used in many knowledge acquisition processes. It is a challenging NLP task due to its high domain dependence: no existing methods can consistently outperform others in all domains, and good ATE is very much an unsolved problem. We propose a generic method for improving the ranking of terms extracted by a potentially wide range of existing ATE methods. We re-design the well-known TextRank algorithm to work at corpus level, using easily obtainable domain resources in the form of seed words or phrases, to compute a score for a word from the target dataset. This is used to refine a candidate term’s score computed by an existing ATE method, potentially improving the ranking of real terms to be selected for tasks such as ontology engineering. Evaluation shows consistent improvement on 10 state of the art ATE methods by up to 25 percentage points in average precision measured at top-ranked K candidates.}
}
@article{THAKAR2018762,
title = {Enterprise Level Integration of Ontology Engineering and Process Mining for Management of Complex Data and Processes to improve Decision System},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {30},
pages = {762-767},
year = {2018},
note = {18th IFAC Conference on Technology, Culture and International Stability TECIS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.200},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318328659},
author = {Tejan Thakar and Tenzin Tsultrim and Larry Stapleton and Liam Doyle},
keywords = {Enterprise integration, Systems interoperability, Enterprise network design, implementation, Enterprise Systems, business process analysis, complex systems},
abstract = {Software development may involve international-scale, dynamic business processes that consume and generate data in complex ways which may not be obvious to management. This loads risks for global data management projects. This paper investigates an approach combining process mining and knowledge engineering to help manage complex data assets in an international software development process. The research engaged a data management team and other stakeholders over a critical one year period during which the company was involved in an acquisition of another similar sized company. This added significantly to the overall complexity of the enterprise context and decision process. Serious challenges existed with systemic complexity, including the silo-ed nature of IT assets which were not readily amenable to modelling dynamic networks of processes. Preliminary results presented here showed that some features of the combined process mining/ontology development framework would address process management complexity, aiding control of that complex data environment.}
}
@article{ESPINOZAARIAS2021100655,
title = {Crossing the chasm between ontology engineering and application development: A survey},
journal = {Journal of Web Semantics},
volume = {70},
pages = {100655},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100655},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000305},
author = {Paola Espinoza-Arias and Daniel Garijo and Oscar Corcho},
keywords = {Ontology, OWL, Ontology engineering, Web API, Application development, Knowledge graph},
abstract = {The adoption of Knowledge Graphs (KGs) by public and private organizations to integrate and publish data has increased in recent years. Ontologies play a crucial role in providing the structure for KGs, but are usually disregarded when designing Application Programming Interfaces (APIs) to enable browsing KGs in a developer-friendly manner. In this paper we provide a systematic review of the state of the art on existing approaches to ease access to ontology-based KG data by application developers. We propose two comparison frameworks to understand specifications, technologies and tools responsible for providing APIs for KGs. Our results reveal several limitations on existing API-based specifications, technologies and tools for KG consumption, which outline exciting research challenges including automatic API generation, API resource path prediction, ontology-based API versioning, and API validation and testing.}
}
@article{NAQVI20222578,
title = {A Context-Specific Modularization for Ontology Change Management},
journal = {Procedia Computer Science},
volume = {207},
pages = {2578-2587},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.316},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922012054},
author = {Muhammad Raza Naqvi and Linda Elmhadhbi and Arkopaul Sakar and Da Xu and Mohammed Hedi Karray},
keywords = {Ontologies, Modularization, Ontology Change, Semantic heterogeneity},
abstract = {Knowledge engineering has a vital role in advancing the semantic web, in which ontologies play a key role in data interoperability and integration. One of the key issues in ontology engineering is how to handle the subsequent updates in the ontologies. A number of concerns need to be considered while working on the ontology change, such as, tracking ontology versions and heterogeneity issues. Ontology change management has been partially addressed by different researchers in overlapping research areas. However, a concrete description of the problem and its related concerns are still not available in the literature. Our work aims to present an overview of ontology change management and its concerns. We point up the need for modularization in ontology change management based on its advantages in the context of ontology reuse from different contextual viewpoints. For this purpose, we propose a protege plugin for reusing OWL modules, and allowing a safe/clean manual integration and reuse of different ontology modules.}
}
@article{OKONTA2025106328,
title = {Semantic interoperability on IoT: Aligning IFC and Smart Application Reference (SAREF) sensor data models},
journal = {Automation in Construction},
volume = {177},
pages = {106328},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106328},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525003681},
author = {Ebere Donatus Okonta and Farzad Rahimian and Vladimir Vukovic and Sergio Rodriguez},
keywords = {Semantic web, IFC, BIM, SAREF, Ontology alignment, IoT, Data models},
abstract = {This paper proposes extending the Smart Application Reference (SAREF) ontology to enable sensor modelling based on the Industry Foundation Classes (IFC) standard, enhancing semantic interoperability between IoT (Internet of Things) and Building Information Modelling (BIM). The paper introduces the Information Assigned to Device Based Ontology Matching approach (IADOM) to align saref:Sensor and IfcSensor data models. Leveraging RDF (Resource Definition Framework) Semantic Web technology, the research modelled and visualised sensor data models in the Protégé software environment, exploring basic information that defined the sensor, including class, properties, relations, attributes, geometry, and interaction. Ontology results indicate property and interaction similarities and differences in saref:Sensor and IfcSensor. The extended ontology provides a standardised and interoperable representation of sensor data and their relationships within BIM and proves that SAREF ontology extension can enhance semantic interoperability between IoT devices and BIM systems, facilitating efficient data exchange, enabling advanced analytics and decision-making processes in smart buildings.}
}
@article{DAVID2023359,
title = {Deploying OWL ontologies for semantic mediation of mixed-reality interactions for human–robot collaborative assembly},
journal = {Journal of Manufacturing Systems},
volume = {70},
pages = {359-381},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001450},
author = {Joe David and Eric Coatanéa and Andrei Lobov},
keywords = {Human–robot collaboration, Mixed reality, Multi-agent systems, OWL ontology, SHACL, Belief–desire–intent, Digital thread},
abstract = {For effective human–robot collaborative assembly, it is paramount to view both robots and humans as autonomous entities in that they can communicate, undertake different roles, and not be bound to pre-planned routines and task sequences. However, with very few exceptions, most of recent research assumes static pre-defined roles during collaboration with centralised architectures devoid of runtime communication that can influence task responsibility and execution. Furthermore, from an information system standpoint, they lack the self-organisation needed to cope with today’s manufacturing landscape that is characterised by product variants. Therefore, this study presents collaborative agents for manufacturing ontology (CAMO), which is an information model based on description logic that maintains a self-organising team network between collaborating human–robot multi-agent system (MAS). CAMO is implemented using the Web Ontology Language (OWL). It models popular notions of net systems and represents the agent, manufacturing, and interaction contexts that accommodate generalisability to different assemblies and agent capabilities. As a novel element, a dynamic consensus-driven collaboration based on parametric validation of semantic representations of agent capabilities via runtime dynamic communication is presented. CAMO is instantiated as agent beliefs in a framework that benefits from real-time dynamic communication with the assembly design environment and incorporates a mixed-reality environment for use by the operator. The employment of web technologies to project scalable notions of intentions via mixed reality is discussed for its novelty from a technology standpoint and as an intention projection mechanism. A case study with a real diesel engine assembly provides appreciable results and demonstrates the feasibility of CAMO and the framework.}
}
@article{FERRARI2025107697,
title = {Formal requirements engineering and large language models: A two-way roadmap},
journal = {Information and Software Technology},
volume = {181},
pages = {107697},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107697},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925000369},
author = {Alessio Ferrari and Paola Spoletini},
keywords = {Requirements engineering, Formal methods, Large language models, LLMs, Natural language processing, NLP, NLP4RE, Prompt engineering, Prompt requirements engineering},
abstract = {Context:
Large Language Models (LLMs) have made remarkable advancements in emulating human linguistic capabilities, showing potential also in executing various requirements engineering (RE) tasks. However, despite their generally good performance, the adoption of LLM-generated solutions and artefacts prompts concerns about their correctness, fairness, and trustworthiness.
Objective:
This paper aims to address the concerns associated with the use of LLMs in RE activities. Specifically, it seeks to develop a roadmap that leverages formal methods (FMs) to provide guarantees of correctness, fairness, and trustworthiness when LLMs are utilised in RE. Symmetrically, it aims to explore how LLMs can be employed to make FMs more accessible.
Methods:
We use two sets of examples to show the current limits of FMs when used in software development and of LLMs when used for RE tasks. The highlighted limitations are addressed by proposing two roadmaps grounded in the current literature and technologies.
Results:
The proposed examples show the potential and limits of FMs in supporting software development and of LLMs when used for RE tasks. The initial investigation into how these limitations can be overcome has been concretised in two detailed roadmaps for the RE and, more largely, the software engineering community.
Conclusion:
The proposed roadmaps offer a promising approach to address the concerns of correctness, fairness, and trustworthiness associated with the use of LLMs in RE tasks through the use of FMs and to enhance the accessibility of FMs by utilising LLMs.}
}
@article{LOUGE2025126641,
title = {Events-based semantic services composition in Industry 4.0 using Asset Administration Shell meta-model for digital twins},
journal = {Expert Systems with Applications},
volume = {271},
pages = {126641},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126641},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425002635},
author = {Thierry Louge and Sina Namaki Araghi and Mohamed Hedi Karray and Arkopaul Sarkar},
keywords = {Ontologies, Semantic services, Asset Administration Shell, Industry 4.0, Digital twins},
abstract = {Since the emergence of the Semantic Web concept, considerable work has focused on service composition using ontology-based approaches. Meanwhile, the concept of Industry 4.0 has emerged, emphasizing the benefits of utilizing data and computing devices in close proximity to production lines, exemplified by concepts like digital twins. However, these two fields rarely intersect, and the requirements for integrating domain-specific knowledge into business processes with event feedback during processes execution differ between these contexts. With the recent advancements in the semantization of industrial standards, such as the Asset Administration Shell, this work explores the elements of a semantic model for describing equipment, enabling the semantic composition of equipment as services. We propose an ontology, COMPAAS, designed to facilitate the composition of production lines that can react to events reported by their components, allowing the system to adjust its behavior accordingly. This approach also addresses the removal and addition of hardware or software elements within the chain, and the entire concept is validated through a minimal use case that demonstrates the improved flexibility of the production line in response to potential disturbances.}
}
@article{FAN2022,
title = {Telehealth System Based on the Ontology Design of a Diabetes Management Pathway Model in China: Development and Usability Study},
journal = {JMIR Medical Informatics},
volume = {10},
number = {12},
year = {2022},
issn = {2291-9694},
doi = {https://doi.org/10.2196/42664},
url = {https://www.sciencedirect.com/science/article/pii/S2291969422000588},
author = {ZhiYuan Fan and LiYuan Cui and Ying Ye and ShouCheng Li and Ning Deng},
keywords = {diabetes, chronic disease management, Chronic Disease Management Pathway, ontology, Semantic Web Rule Language rules, SWRL rules},
abstract = {Background
Diabetes needs to be under control through management and intervention. Management of diabetes through mobile health is a practical approach; however, most diabetes mobile health management systems do not meet expectations, which may be because of the lack of standardized management processes in the systems and the lack of intervention implementation recommendations in the management knowledge base.
Objective
In this study, we aimed to construct a diabetes management care pathway suitable for the actual situation in China to express the diabetes management care pathway using ontology and develop a diabetes closed-loop system based on the construction results of the diabetes management pathway and apply it practically.
Methods
This study proposes a diabetes management care pathway model in which the management process of diabetes is divided into 9 management tasks, and the Diabetes Care Pathway Ontology (DCPO) is constructed to represent the knowledge contained in this pathway model. A telehealth system, which can support the comprehensive management of patients with diabetes while providing active intervention by physicians, was designed and developed based on the DCPO. A retrospective study was performed based on the data records extracted from the system to analyze the usability and treatment effects of the DCPO.
Results
The diabetes management pathway ontology constructed in this study contains 119 newly added classes, 28 object properties, 58 data properties, 81 individuals, 426 axioms, and 192 Semantic Web Rule Language rules. The developed mobile medical system was applied to 272 patients with diabetes. Within 3 months, the average fasting blood glucose of the patients decreased by 1.34 mmol/L (P=.003), and the average 2-hour postprandial blood glucose decreased by 2.63 mmol/L (P=.003); the average systolic and diastolic blood pressures decreased by 11.84 mmHg (P=.02) and 8.8 mmHg (P=.02), respectively. In patients who received physician interventions owing to abnormal attention or low-compliance warnings, the average fasting blood glucose decreased by 2.45 mmol/L (P=.003), and the average 2-hour postprandial blood glucose decreased by 2.89 mmol/L (P=.003) in all patients with diabetes; the average systolic and diastolic blood pressure decreased by 20.06 mmHg (P=.02) and 17.37 mmHg (P=.02), respectively, in patients with both hypertension and diabetes during the 3-month management period.
Conclusions
This study helps guide the timing and content of interactive interventions between physicians and patients and regulates physicians’ medical service behavior. Different management plans are formulated for physicians and patients according to different characteristics to comprehensively manage various cardiovascular risk factors. The application of the DCPO in the diabetes management system can provide effective and adequate management support for patients with diabetes and those with both diabetes and hypertension.}
}
@article{VEGGI2025e00409,
title = {The Brancacci Chapel from the Quattrocento to the semantic web: An ontology-assisted case study of cultural data management and site reconstruction},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {37},
pages = {e00409},
year = {2025},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2025.e00409},
url = {https://www.sciencedirect.com/science/article/pii/S2212054825000116},
author = {Manuele Veggi and Ivana Cerato},
keywords = {Knowledge representation, 3D semantic annotation, Digital art history, Ontology engineering, Cultural site reconstruction, Semantic web},
abstract = {This study proposes an ontological model for cultural heterogeneous data and cultural site reconstructions. It is based on the concept of interpretative unit, which extends the semantics of stratigraphic units also to non-archaeological contexts. The ontology is named after the case study of this research, the Brancacci Chapel in Florence. Indeed, after a state of the art overview of the development methodology and the description of the most relevant entities, a first test case is proposed. An entry of the catalogue of a recent exhibition on Masolino, a 15th century painter who worked at the decoration of the chapel, has been serialised as Turtle file and the semantics of knowledge graph has been assessed via competency questions. The positive results encourage the deepening of this line of research in the direction of connecting linked data with nodes in 3D models, as well as their visualisation and communication to non-specialist audiences.}
}
@article{LOPES2023110385,
title = {Using terms and informal definitions to classify domain entities into top-level ontology concepts: An approach based on language models},
journal = {Knowledge-Based Systems},
volume = {265},
pages = {110385},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110385},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123001351},
author = {Alcides Lopes and Joel Carbonera and Daniela Schmidt and Luan Garcia and Fabricio Rodrigues and Mara Abel},
keywords = {Ontology learning, Top-level ontology, Language model},
abstract = {The classification of domain entities into top-level ontology concepts remains an activity performed manually by an ontology engineer. Although some works focus on automating this task by applying machine-learning approaches using textual sentences as input, they require the existence of the domain entities in external knowledge resources, such as pre-trained embedding models. In this context, this work proposes an approach that combines the term representing the domain entity and its informal definition into a single text sentence without requiring external knowledge resources. Thus, we use this sentence as the input of a deep neural network that contains a language model as a layer. Also, we present a methodology used to extract two novel datasets from the OntoWordNet ontology based on Dolce-Lite and Dolce-Lite-Plus top-level ontologies. Our experiments show that by using the transformer-based language models, we achieve promising results in classifying domain entities into 82 top-level ontology concepts, with 94% regarding micro F1-score.}
}
@article{KOZIOL20203263,
title = {Dealing with Polysemy in the Polish Sign Language Using the OWL Ontology},
journal = {Procedia Computer Science},
volume = {176},
pages = {3263-3272},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.122},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920320226},
author = {Wojciech Kozioł and Krzysztof Pancerz and Kazimierz Sikora and Kamil Dudek},
keywords = {Modelling, Polysemy, Polish Sign Language, OWL Ontology},
abstract = {A common problem of natural language processing is synonymy, polysemy, and homonymy. In the paper, we propose to deal with polysemy in the Polish sign language using the knowledge included in the OWL2 ontology created for this purpose. The proposed approach aids the translation process of the Polish sign language into the Polish language by selection from the possible phrases, only those, with the reasonable meaning.}
}
@article{MOUSAVI20191254,
title = {A Survey of Model-Based System Engineering Methods to Analyse Complex Supply Chains: A Case Study in Semiconductor Supply Chain},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {1254-1259},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.370},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319313497},
author = {Behrouz Alizadeh Mousavi and Radhia Azzouz and Cathal Heavey and Hans Ehm},
keywords = {Model-Based system Engineering, Conceptual modelling, Simulation, SysML, Ontology, BPMN, Supply chain planning},
abstract = {Model-Based System Engineering (MBSE) is an increasingly important methodology to support system engineering and has attained a high level of attentiveness in business simulation practices as a conceptual modelling approach. In this paper, we present our results related to the application of MBSE approaches in complex semiconductor manufacturing supply chain planning systems. We investigate System Modeling Language (SysML), Web Ontology Language (OWL) and Business Process Modeling Notation (BPMN) as different approaches and languages for MBSE. These approaches are surveyed and used to develop conceptual models for the simulation of the order management process inside the supply chain management. This study aims to survey and offer a number of implications for MBSE practice and seeks to stimulate and guide further research in this area.}
}
@article{ALI2020103175,
title = {Ontology-based approach to extract product's design features from online customers’ reviews},
journal = {Computers in Industry},
volume = {116},
pages = {103175},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.103175},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519301836},
author = {Munira Mohd Ali and Mamadou Bilo Doumbouya and Thierry Louge and Rahul Rai and Mohamed Hedi Karray},
keywords = {, Product lifecycle management (PLM), Customers’ reviews, Sentiment analysis, Product design},
abstract = {Online customer reviews provide new potential customers with relevant information about a product or service. It has been empirically shown that the type of reviews (positive or negative) a product receives significantly impacts its future sales. In this paper, the online customers’ reviews analysis for the identification of key product attributes to be used in the conceptual design phase of a product is outlined. Our goal is to bring a value-added link between the Middle of Life phase to the Beginning of Life phase in the closed-loop product lifecycle management (PLM) by developing an ontology-based reasoning system to provide information that represents the customers’ opinions for the product's conceptual design. The main contributions of the proposed approach are the integration between the ontology and the natural language processing system in extracting the customers’ reviews data in the overall framework. The utility of the proposed approach is shown through the application on the digital camera product review dataset from Amazon.}
}
@article{CIROKU2024107997,
title = {Automated multimodal sensemaking: Ontology-based integration of linguistic frames and visual data},
journal = {Computers in Human Behavior},
volume = {150},
pages = {107997},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107997},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223003485},
author = {Fiorela Ciroku and Stefano {De Giorgis} and Aldo Gangemi and Delfina S. Martinez-Pandiani and Valentina Presutti},
keywords = {Multimodal sensemaking, Ontology engineering, Knowledge graph construction, Frame-based reasoning, Visual and linguistic frames},
abstract = {Frame evocation from visual data is an essential process for multimodal sensemaking, due to the multimodal abstraction provided by frame semantics. However, there is a scarcity of data-driven approaches and tools to automate it. We propose a novel approach for explainable automated multimodal sensemaking by linking linguistic frames to their physical visual occurrences, using ontology-based knowledge engineering techniques. We pair the evocation of linguistic frames from text to visual data as “framal visual manifestations”. We present a deep ontological analysis of the implicit data model of the Visual Genome image dataset, and its formalization in the novel Visual Sense Ontology (VSO). To enhance the multimodal data from this dataset, we introduce a framal knowledge expansion pipeline that extracts and connects linguistic frames – including values and emotions – to images, using multiple linguistic resources for disambiguation. It then introduces the Visual Sense Knowledge Graph (VSKG), a novel resource. VSKG is a queryable knowledge graph that enhances the accessibility and comprehensibility of Visual Genome’s multimodal data, based on SPARQL queries. VSKG includes frame visual evocation data, enabling more advanced forms of explicit reasoning, analysis and sensemaking. Our work represents a significant advancement in the automation of frame evocation and multimodal sense-making, performed in a fully interpretable and transparent way, with potential applications in various fields, including the fields of knowledge representation, computer vision, and natural language processing.}
}
@article{BATISTA2022102012,
title = {Ontologically correct taxonomies by construction},
journal = {Data & Knowledge Engineering},
volume = {139},
pages = {102012},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2022.102012},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X22000246},
author = {Jeferson O. Batista and João Paulo A. Almeida and Eduardo Zambon and Giancarlo Guizzardi},
keywords = {Taxonomies, Conceptual modeling, Ontologies, Graph grammars, Correctness by construction},
abstract = {Taxonomies play a central role in conceptual domain modeling, having a direct impact in areas such as knowledge representation, ontology engineering, and software engineering, as well as knowledge organization in information sciences. Despite this, there is little guidance on how to build high-quality taxonomies, with notable exceptions being the OntoClean methodology, and the ontology-driven conceptual modeling language OntoUML. These techniques take into account the ontological meta-properties of types to establish well-founded rules on the formation of taxonomic structures. In this paper, we show how to leverage the formal rules underlying these techniques in order to build taxonomies which are correct by construction. We define a set of correctness-preserving operations to systematically introduce types and subtyping relations into taxonomic structures. In addition to considering the ontological micro-theory of endurant types underlying OntoClean and OntoUML, we also employ the MLT (Multi-Level Theory) micro-theory of high-order types, which allows us to address multi-level taxonomies based on the powertype pattern. To validate our proposal, we formalize the model building operations as a graph grammar that incorporates both micro-theories. We apply automatic verification techniques over the grammar language to show that the graph grammar is sound, i.e., that all taxonomies produced by the grammar rules are correct, at least up to a certain size. We also show that the rules can generate all correct taxonomies up to a certain size (a completeness result).}
}
@article{LALIS2019290,
title = {Functional modeling in safety by means of foundational ontologies},
journal = {Transportation Research Procedia},
volume = {43},
pages = {290-299},
year = {2019},
note = {INAIR 2019 - Global Trends in Aviation},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2019.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S2352146519306118},
author = {Andrej Lališ and Riccardo Patriarca and Jana Ahmad and Giulio Di Gravio and Bogdan Kostov},
keywords = {aviation safety, socio-technical systems, ontology engineering, safety engineering, resilience engineering},
abstract = {Modern theory of safety deals with systemic approach to safety, formalized in form of several systemic prediction models or methods such as FRAM (Functional Resonance Analysis Method) or STAMP (System-Theoretic Accident Model and Processes). The theory of each approach emphasizes different viewpoints to be considered in approaching various industrial safety issues. This paper focuses on FRAM and its functional viewpoint for modern complex sociotechnical systems. The methodology in this paper is based on the utilization of foundational ontologies to conceptualize the core ideas of FRAM, with the focus on the concept of functions as used in theory. The outcomes of the case study in the aviation domain provide for what needs to be determined to properly model functions in FRAM and they allow for better utilization of the method in real-case applications. The results also confirm some previous research, suggesting that modern systemic approach to safety is theoretically grounded on common - or at least complementary - tenets, to be prospectively integrated by means of ontology engineering.}
}
@article{SONG2024114983,
title = {Ontology-assisted GPT-based building performance simulation and assessment: Implementation of multizone airflow simulation},
journal = {Energy and Buildings},
volume = {325},
pages = {114983},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.114983},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824010995},
author = {Jihwan Song and Sungmin Yoon},
keywords = {GPT, ChatGPT, Large language model (LLM), Building performance simulation (BPS), Building performance, Digital twins, Artificial intelligence, CONTAM},
abstract = {Building performance simulation (BPS) is crucial for building performance assessments across its lifecycle. However, the complexity of buildings and the iterative nature of simulation poses challenges, leading to high costs and low values. Previous studies focused on simplification, but did not fully utilize advanced simulation engines. Despite recent advancements, there is a lack of research on leveraging artificial intelligence (AI), specifically generative pre-trained transformer (GPT), for BPS. Therefore, this study proposes a GPT-based BPS system, enhancing simulation efficiency and value by integrating simulation engines and advanced data analytics in the GPT environment. The ontology for GPT-based BPS is also developed to enable comprehensive, reliable, informative BPS environments. Based on this framework, case studies were conducted for GPT-based multizone airflow network simulation in a high-rise residential building using CONTAM software. They demonstrate GPT’s capabilities in retrieving simulation data, visualizing results with data mining, answering questions based on building knowledge, checking compliance with design guidelines, and proposing design alternatives. Finally, this study emphasizes expert interventions with ontological engineering informatics to utilize strictly structured BPS engines.}
}
@article{ADIB2022417,
title = {Ontological user profile for E-orientation platforms},
journal = {Procedia Computer Science},
volume = {198},
pages = {417-422},
year = {2022},
note = {12th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 11th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.263},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921025023},
author = {Jihad Adib and Rachida Ait Abdelouahid and Abdelaziz Marzak and Hicham Moutachaouik},
keywords = {E-Orientation, User profile, Platform, Ontologies, Model},
abstract = {In recent years, E-orientation systems have played an increasingly significant role in the proposal of an academic and professional orientation to students. Research efforts have grown to provide more useful and effective E-orientation systems for research or other purposes. The implementation of E-orientation systems resulting from these efforts utilizes several techniques including Artificial Intelligence (AI) methodologies. This study proposes a personalised approach to support an E-orientation system that is tailored to the student’s characteristics. A key component of this system comprises an ontological model of the user profile. The objective of this research was to propose an ontology that is able to collect and analyze the user related information as well as customize the profiles with the most appropriate recommendation or orientation. The ontology employed in this study was developed using the OWL (Ontology Web Language), a knowledge representation language for authoring ontologies. In this paper we will present a definition for the user profile, and then we present our methodology of ontological modeling of the user profile, and finally the conceptual model of the user model for e-orientation systems.}
}
@article{MUNCH2022117406,
title = {Combining ontology and probabilistic models for the design of bio-based product transformation processes},
journal = {Expert Systems with Applications},
volume = {203},
pages = {117406},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117406},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422007485},
author = {Mélanie Munch and Patrice Buche and Stéphane Dervaux and Juliette Dibie and Liliana Ibanescu and Cristina Manfredotti and Pierre-Henri Wuillemin and Hélène Angellier-Coussy},
keywords = {Ontologies, Probabilistic relational models, Knowledge discovery, Causality},
abstract = {This paper presents a workflow for the design of transformation processes using different kinds of expert’s knowledge. It introduces POND (Process and observation ONtology Discovery), a workflow dedicated to answer expert’s questions about processes. It addresses two main issues: (1) how to represent the processes inner complexity, and (2) how to reason about processes taking into account uncertainty and causality. First, we show how to use a semantic model, an ontology, and its associated data to answer some of the expert’s questions concerning the processes, using semantic web languages and technologies. Then, we describe how to learn a predictive model, to discover new knowledge and provide explicative models by integrating the semantic model into a probabilistic relational model. The result is a complete workflow able to extensively analyze transformation processes through all their granularity levels and answer expert’s questions about their domains. An example of this workflow is given on biocomposites manufacturing for food packaging.}
}
@article{MARTINSAMORIM2025114154,
title = {Using natural language definitions and language models for relationship classification},
journal = {Knowledge-Based Systems},
volume = {327},
pages = {114154},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114154},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125011955},
author = {Marina {Martins Amorim} and Alcides {Gonçalves Lopes Junior} and  {Fabrício Henrique Rodrigues} and Joel {Luís Carbonera}},
keywords = {BERT, Machine learning, Lexical classification, Relation classification, NLP},
abstract = {Identifying relationships between concepts is a very important task for several NLP tasks, as well as for building explicit knowledge models (ontologies and knowledge graphs). In many of these tasks, experts usually manually establish these relationships by carefully analyzing each concept’s meaning and considering the domain knowledge elicited from domain practitioners or from domain literature. While some studies automate parts of the process of building knowledge models, most focus on identifying general concepts or rely on static word embeddings, which fail to address challenges like polysemy and contextual ambiguity. This research addresses the problem of classifying semantic relationships between concepts, focusing on hypernym and holonym relations. We propose an approach based on the pre-trained language model BERT to classify these relationships between concepts. We assume that we can represent the concept’s semantics using their definitions in natural language. To evaluate this approach, we developed a methodology to construct a labeled dataset of definitions of concepts using WordNet as a reference. Thus, our proposed approach classifies the relations based solely on natural language expressions representing the concept’s definition. Our experiments showed notable classification results, achieving an F1 score of 96 % in the classification of holonyms, hypernyms, and concepts that are not related by any of these relations, indicating that our approach can accurately predict semantic relations between concepts using only their natural language definitions as input.}
}
@article{RYS2024100720,
title = {Model management to support systems engineering workflows using ontology-based knowledge graphs},
journal = {Journal of Industrial Information Integration},
volume = {42},
pages = {100720},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100720},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24001638},
author = {Arkadiusz Ryś and Lucas Lima and Joeri Exelmans and Dennis Janssens and Hans Vangheluwe},
keywords = {Model management, Ontology, Process modelling, Knowledge graph},
abstract = {System engineering has been shifting from document-centric to model-based approaches, where assets are becoming more and more digital. Although digitisation conveys several benefits, it also brings several concerns (e.g., storage and access) and opportunities. In the context of Cyber-Physical Systems (CPS), we have experts from various domains executing complex workflows and manipulating models in a plethora of different formalisms, each with their own methods, techniques and tools. Storing knowledge on these workflows can reduce considerable effort during system development not only to allow their repeatability and replicability but also to access and reason on data generated by their execution. In this work, we propose a framework to manage modelling artefacts generated from workflow executions. The basic workflow concepts, related formalisms and artefacts are formally defined in an ontology specified in OML (Ontology Modelling Language). This ontology enables the construction of a knowledge graph that contains system engineering data to which we can apply reasoning. We also developed several tools to support system engineering during the design of workflows, their enactment, and artefact storage, considering versioning, querying and reasoning on the stored data. These tools also hide the complexity of manipulating the knowledge graph directly. Finally, we have applied our proposed framework in a real-world system development scenario of a drivetrain smart sensor system. Results show that our proposal not only helped the system engineer with fundamental difficulties like storage and versioning but also reduced the time needed to access relevant information and new knowledge that can be inferred from the knowledge graph.}
}
@article{SEQUEDA2025100858,
title = {Knowledge Graphs as a source of trust for LLM-powered enterprise question answering},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100858},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100858},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000441},
author = {Juan Sequeda and Dean Allemang and Bryon Jacob},
keywords = {Knowledge Graph, LLM, Large Language Model, Generative AI, Question answering, Knowledge engineering, SPARQL, SQL, OWL, R2RML},
abstract = {Generative AI provides an innovative and exciting way to manage knowledge and data at any scale; for small projects, at the enterprise level, and even at a world wide web scale. It is tempting to think that Generative AI has made other knowledge-based technologies obsolete; that anything we wanted to do with knowledge-based systems, Knowledge Graphs or even expert systems can instead be done with Generative AI. Our position is counter to that conclusion. Our practical experience on implementing enterprise question answering systems using Generative AI has shown that Knowledge Graphs support this infrastructure in multiple ways: they provide a formal framework to evaluate the validity of a query generated by an LLM, serve as a foundation for explaining results, and offer access to governed and trusted data. In this position paper, we share our experience, present industry needs, and outline the opportunities for future research contributions.}
}
@article{TAUQEER2023121049,
title = {Smell and Taste Disorders Knowledge Graph: Answering Questions Using Health Data},
journal = {Expert Systems with Applications},
volume = {234},
pages = {121049},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121049},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423015518},
author = {Amar Tauqeer and Ismaheel Hammid and Sareh Aghaei and Parvaneh Parvin and Elbrich M. Postma and Anna Fensel},
keywords = {Chemosensory dysfunction, Semantic modeling, Health, Data sharing, Knowledge graph, Ontology, Question answering user interface},
abstract = {Smell and taste disorders have become a more prominent issue due to their association with Covid-19, and their impact on quality of life and health outcomes. However, pertinent information regarding these disorders is often inaccessible and poorly organized, with the majority of data stored solely in clinical data repositories. To rectify this, a technological solution capable of digitizing, semantically modeling, and integrating health data is necessary. The knowledge graph, an emerging technology capable of organizing inconsistent and heterogeneous health data and inferring implicit knowledge, presents a viable solution to this problem. In pursuit of the aforementioned goal, an existing ontology pertaining to smell and taste disorders was enriched by introducing additional relevant concepts and relationships. Subsequently, a knowledge graph was constructed based on the defined ontology and patients’ data. The resultant knowledge graph was subjected to a rigorous evaluation, encompassing dimensions such as completeness, coherency, coverage, and succinctness. The evaluation established the effectiveness and usability of the knowledge graph, with only minor issues detected through the OOPS! pitfall scanner. Furthermore, as a proof-of-concept for clinical application, a user interface was created, enabling users to access pertinent information concerning smell and taste disorders, including causative factors, medications, and etiology, among others. The interface generates a graph-based structure based on the selected question from a drop-down menu. The end-user can modify the query by merely clicking on the generated graph to ask related questions. This study showcases the potential of knowledge graphs centered on smell and taste disorders to organize and provide accessible health data to end-users.}
}
@article{SALES2023102210,
title = {A FAIR catalog of ontology-driven conceptual models},
journal = {Data & Knowledge Engineering},
volume = {147},
pages = {102210},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102210},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23000708},
author = {Tiago Prince Sales and Pedro Paulo F. Barcelos and Claudenir M. Fonseca and Isadora Valle Souza and Elena Romanenko and César Henrique Bernabé and Luiz Olavo {Bonino da Silva Santos} and Mattia Fumagalli and Joshua Kritz and João Paulo A. Almeida and Giancarlo Guizzardi},
keywords = {Ontology-driven conceptual modeling, OntoUML, Unified Foundational Ontology, Model catalog, FAIR, Linked data},
abstract = {Multi-domain model catalogs serve as empirical sources of knowledge and insights about specific domains, about the use of a modeling language’s constructs, as well as about the patterns and anti-patterns recurrent in the models of that language crosscutting different domains. They may support domain and language learning, model reuse, knowledge discovery for humans, and reliable automated processing and analysis if built following generally accepted quality requirements for scientific data management. More specifically, not unlike scientific (meta)data, models should be shared according to the FAIR principles (Findability, Accessibility, Interoperability, and Reusability). In this paper, we report on the construction of a FAIR model catalog for Ontology-Driven Conceptual Modeling research, a trending paradigm lying at the intersection of conceptual modeling and ontology engineering in which the Unified Foundational Ontology (UFO) and OntoUML emerged among the most adopted technologies. The catalog, publicly available at https://w3id.org/ontouml-models, currently includes over one hundred and forty models, developed in a variety of contexts and domains.}
}
@article{CAO2019177,
title = {Towards a Core Ontology for Condition Monitoring},
journal = {Procedia Manufacturing},
volume = {28},
pages = {177-182},
year = {2019},
note = {7th International conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918313714},
author = {Qiushi Cao and Cecilia Zanni-Merk and Christoph Reich},
keywords = {condition monitoring, state of the system or machine, ontology engineering, core ontology, conceptual model},
abstract = {Condition monitoring is performed to identify the functioning state of a machine or a mechanical system. It is an important task by which the machine or mechanical system deterioration tendency and the location of a failure can be detected. In recent years, ontologies have shown promising results to enhance knowledge sharing in condition monitoring tasks, while offering a logically defined and controlled vocabulary of domain entities. Motivated by the growing demand for unification and formal representation of useful concepts in condition monitoring, in this paper we present CM-core, an ontology of core condition monitoring entities. It incorporates several ISO standards as sources and also extracts general concepts from a series of domain ontologies. The ontology contains taxonomies of core condition monitoring concepts such as system, function, behavior, structure, state, failure and fault, with their interrelationships. The CM-core ontology has a broader domain coverage than the existing ontologies, and its generality ensures further specification into more specific domain ontologies.}
}
@article{XU2018118,
title = {A knowledge base with modularized ontologies for eco-labeling: Application for laundry detergents},
journal = {Computers in Industry},
volume = {98},
pages = {118-133},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517306322},
author = {Da Xu and Mohamed Hedi Karray and Bernard Archimède},
keywords = {Ontology engineering, Ontology modularization, Knowledge base, OWL imports, SWRL, Eco-labeling},
abstract = {Along with the rising concern of environmental performance, eco-labeling is becoming more and more popular. However, the complex process of eco-labeling is demotivating manufacturers and service providers to be certificated. The knowledge contained in eco-labeling criteria documents is not semantically exploitable to computers. Traditional knowledge base in relational data model is not inter-operable, lacks inference support and is difficult to be reused. In our research, we propose a comprehensive knowledge base composed of interconnected OWL (Ontology Web Language) ontologies. This ontology based knowledge base allows reasoning and semantic query. In this paper, a modularization scheme about ontology development is introduced and it has been applied to EU Eco-label (European Union Eco-label) laundry detergent product criteria. This scheme separates entity knowledge and rule knowledge so that the ontology modules can be reused easily in other domains. Reasoning and inference based on SWRL (Semantic Web Rule Language) rules in favor of eco-labeling process is also presented.}
}
@article{DAGA2025100846,
title = {Process Knowledge Graphs (PKG): Towards unpacking and repacking AI applications},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100846},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100846},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000325},
author = {Enrico Daga},
keywords = {Knowledge graphs, Prompt engineering, Data science pipelines, Data pipelines documentation, Data pipelines design},
abstract = {In the past years, a new generation of systems has emerged, which apply recent advances in generative Artificial Intelligence (AI) in combination with traditional technologies. Specifically, generative AI is being delegated tasks in natural language or vision understanding within complex hybrid architectures that also include databases, procedural code, and interfaces. Process Knowledge Graphs (PKG) have a long-standing tradition within symbolic AI research. On the one hand, PKGs can play an important role in describing complex, hybrid applications, thus opening the way for addressing fundamental challenges such as explaining and documenting such systems (unpacking). On the other hand, by organising complex processes in simpler building blocks, PKGs can potentially increase accuracy and control over such systems (repacking). In this position paper, we discuss opportunities and challenges of PGRs and their potential role towards a more robust and principled design of AI applications.}
}
@article{SPOLADORE2024374,
title = {Towards a knowledge-based decision support system to foster the return to work of wheelchair users},
journal = {Computational and Structural Biotechnology Journal},
volume = {24},
pages = {374-392},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024001570},
author = {Daniele Spoladore and Luca Negri and Sara Arlati and Atieh Mahroo and Margherita Fossati and Emilia Biffi and Angelo Davalli and Alberto Trombetta and Marco Sacco},
keywords = {Knowledge-based decision support system, Ontology engineering, Return to work, Clinical decision support system, Wheelchair user},
abstract = {Accidents at work may force workers to face abrupt changes in their daily life: one of the most impactful accident cases consists of the worker remaining in a wheelchair. Return To Work (RTW) of wheelchair users in their working age is still challenging, encompassing the expertise of clinical and rehabilitation personnel and social workers to match the workers’ residual capabilities with job requirements. This work describes a novel and prototypical knowledge-based Decision Support System (DSS) that matches workers’ residual capabilities with job requirements, thus helping vocational therapists and clinical personnel in the RTW decision-making process for WUs. The DSS leverages expert knowledge in the form of ontologies to represent the International Classification of Functioning, Disability, and Health (ICF) and the Occupational Information Network (O*NET). These taxonomies enable both workers’ health conditions and job requirements formalization, which are processed to assess the suitability of a job depending on a worker’s condition. Consequently, the DSS suggests a list of jobs a wheelchair user can still perform, exploiting his/her residual abilities at their best. The manuscript describes the theoretical approach and technological foundations of such DSS, illustrating its development, its output metric, and application. The developed solution was tested with real wheelchair users’ health conditions provided by the Italian National Institute for Insurance against Accidents at Work. The feasibility of an approach based on objective data was thus demonstrated, providing a novel point of view in the critical process of decision-making during RTW.}
}
@article{GUIZZARDI2024102325,
title = {Explanation, semantics, and ontology},
journal = {Data & Knowledge Engineering},
volume = {153},
pages = {102325},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102325},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000491},
author = {Giancarlo Guizzardi and Nicola Guarino},
keywords = {Real-world semantics, Ontology, Explanation, Ontological unpacking, Semantic interoperability},
abstract = {The terms ‘semantics’ and ‘ontology’ are increasingly appearing together with ‘explanation’, not only in the scientific literature, but also in everyday social interactions, in particular, within organizations. Ontologies have been shown to play a key role in supporting the semantic interoperability of data and knowledge representation structures used by information systems. With the proliferation of applications of Artificial Intelligence (AI) in different settings and the increasing need to guarantee their explainability (but also their interoperability) in critical contexts, the term ‘explanation’ has also become part of the scientific and technical jargon of modern information systems engineering. However, all of these terms are also significantly overloaded. In this paper, we address several interpretations of these notions, with an emphasis on their strong connection. Specifically, we discuss a notion of explanation termed ontological unpacking, which aims at explaining symbolic domain descriptions (e.g., conceptual models, knowledge graphs, logical specifications) by revealing their ontological commitment in terms of their so-called truthmakers, i.e., the entities in one’s ontology that are responsible for the truth of a description. To illustrate this methodology, we employ an ontological theory of relations to explain a symbolic model encoded in the de facto standard modeling language UML. We also discuss the essential role played by ontology-driven conceptual models (resulting from this form of explanation processes) in supporting semantic interoperability tasks. Furthermore, we revisit a proposal for quality criteria for explanations from philosophy of science to assess our approach. Finally, we discuss the relation between ontological unpacking and other forms of explanation in philosophy and science, as well as in the subarea of Artificial Intelligence known as Explainable AI (XAI).}
}
@article{KIRAN2018205,
title = {Enabling intent to configure scientific networks for high performance demands},
journal = {Future Generation Computer Systems},
volume = {79},
pages = {205-214},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1730626X},
author = {Mariam Kiran and Eric Pouyoul and Anu Mercian and Brian Tierney and Chin Guok and Inder Monga},
keywords = {Intent-based networking, Natural language processing, Ontology engineering, SDN north-bound interface},
abstract = {Globally distributed scientific experiments involve movement of massive data volumes and many collaborators performing distributed data analysis. With complex workloads and heterogeneous resources, each user may desire certain behavior characteristics for their network paths. In this paper, we present the iNDIRA tool, which interacts with SDN north-bound interfaces to enable intent-based networking. It provides reliable, simple, and technology-agnostic communication between users and networks. Focusing particularly on science applications, iNDIRA uses natural language processing to construct semantic RDF graphs to understand, interact, and create the required network services. The technical challenges addressed by iNDIRA are: (1) development of a high-level descriptive language to query network-application requirements, (2) provides keyword identification and condition checking based on user profiles and topology details, (3) allows user negotiation based on the current network state, and (4) integrates network provisioning and service tools used by the application. iNDIRA is implemented on the ESnet network, where it interacts with OpenNSA (aka the NSI client) and Globus data transfer tools, to build complex cross-domain network paths for heterogeneous science applications, and perform secure data transfer. We argue that iNDIRA’s approach presents users with an alternative approach to interact and communicate their network demands, allowing seamless network service integration.}
}
@article{RIQUELMEGARCIA20252155,
title = {Annotation of biological samples data to standard ontologies with support from large language models},
journal = {Computational and Structural Biotechnology Journal},
volume = {27},
pages = {2155-2167},
year = {2025},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2025.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S2001037025001837},
author = {Andrea Riquelme-García and Juan Mulero-Hernández and Jesualdo Tomás Fernández-Breis},
keywords = {Bioinformatics, Generative AI, Large language models, Data interoperability, Biological samples},
abstract = {The semantic integration of biological data is hindered by the vast heterogeneity of data sources and their limited semantic formalization. A crucial step in this process is mapping data elements to ontological concepts, which typically involves substantial manual effort. Large Language Models (LLMs) have demonstrated potential in automating complex language-related tasks and may offer a solution to streamline biological data annotation. This study investigates the utility of LLMs—specifically various base and fine-tuned GPT models—for the automatic assignment of ontological identifiers to biological sample labels. We evaluated model performance in annotating labels to four widely used ontologies: the Cell Line Ontology (CLO), Cell Ontology (CL), Uber-anatomy Ontology (UBERON), and BRENDA Tissue Ontology (BTO). Our dataset was compiled from publicly available, high-quality databases containing biologically relevant sequence information, which suffers from inconsistent annotation practices, complicating integrative analyses. Model outputs were compared against annotations generated by text2term, a state-of-the-art annotation tool. The fine-tuned GPT model outperformed both the base models and text2term in annotating cell lines and cell types, particularly for the CL and UBERON ontologies, achieving a precision of 47–64% and a recall of 88–97%. In contrast, base models exhibited significantly lower performance. These results suggest that fine-tuned LLMs can accelerate and improve the accuracy of biological data annotation. Nonetheless, our evaluation highlights persistent challenges, including variable precision across ontology categories and the continued need for expert curation to ensure annotation validity.}
}
@article{MATENTZOGLU20181,
title = {Inference Inspector: Improving the verification of ontology authoring actions},
journal = {Journal of Web Semantics},
volume = {49},
pages = {1-15},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570826817300367},
author = {Nicolas Matentzoglu and Markel Vigo and Caroline Jay and Robert Stevens},
keywords = {OWL, Ontologies, Human computer interaction, Ontology engineering, Ontology authoring, Reasoning},
abstract = {Ontologies are complex systems of axioms in which unanticipated consequences of changes are both frequent, and difficult for ontology authors to apprehend. The effects of modelling actions range from unintended inferences to outright defects such as incoherency or even inconsistency. One of the central ontology authoring activities is verifying that a particular modelling step has had the intended consequences, often with the help of reasoners. For users of Protégé, this involves, for example, exploring the inferred class hierarchy. This paper provides evidence that making entailment set changes explicit to authors significantly improves the understanding of authoring actions regarding both correctness and speed. This is tested by means of the Inference Inspector, a Protégé plugin we created that provides authors with specific details about the effects of an authoring action. We empirically validate the effectiveness of the Inference Inspector in two studies. In a first, exploratory study we determine the feasibility of the Inference Inspector for supporting verification and isolating authoring actions. In a second, controlled study we formally evaluate the Inference Inspector and determine that making changes to key entailment sets explicit significantly improves author verification compared to the standard static hierarchy/frame-based approach. We discuss the advantages of the Inference Inspector for different types of verification questions and find that our approach is best suited for verifying added restrictions where no new signature, such as class names, is introduced, with a 42% improvement in verification correctness.}
}
@article{GIANNAKOPOULOS2025127117,
title = {NAVMAT: An AI-supported naval failures knowledge management system},
journal = {Expert Systems with Applications},
volume = {277},
pages = {127117},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127117},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425007390},
author = {George Giannakopoulos and Andreas Sideras and Konstantinos Stamatakis and Nikolaos Melanitis},
keywords = {Knowledge management system, Ontology, Information retrieval, Human-expert knowledge instillation},
abstract = {We present “NAVMAT”, an intelligent, multilingual knowledge management platform designed to record and categorize material failure incidents reported in naval operations. This paper provides an overview of the platform, identifying its key software components and highlighting the information retrieval approach used to support user workflows. The platform primarily facilitates real-time, multilingual search and intelligent indexing, streamlining the incident management process while offering valuable insights from past incidents and knowledge resources. To achieve this, it employs a customized natural language processing pipeline integrated with a carefully engineered ontology. The ontology, regularly updated by domain experts, enriches the retrieval mechanism by instilling domain specific knowledge. This approach aims to reduce the significant variability in specialized terminology by promoting convergence towards a unified vocabulary.}
}
@article{NUNEZ2018746,
title = {OntoProg: An ontology-based model for implementing Prognostics Health Management in mechanical machines},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {746-759},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617306080},
author = {David Lira Nuñez and Milton Borsato},
keywords = {Prognostics Health Management, Failure analysis, Ontology engineering},
abstract = {Trends in Prognostics Health Management (PHM) have been introduced into mechanical items of manufacturing systems to predict Remaining Useful Life (RUL). PHM as an estimate of the RUL allows Condition-based Maintenance (CBM) before a functional failure occurs, avoiding corrective maintenance that generates unnecessary costs on production lines. An important factor for the implementation of PHM is the correct data collection for monitoring a machine’s health, in order to evaluate its reliability. Data collection, besides providing information about the state of degradation of the machine, also assists in the analysis of failures for intelligent interventions. Thus, the present work proposes the construction of an ontological model for future applications such as expert system in the support in the correct decision-making, besides assisting in the implementation of the PHM in several manufacturing scenarios, to be used in the future by web semantics tools focused on intelligent manufacturing, standardizing its concepts, terms, and the form of collection and processing of data. The methodological approach Design Science Research (DSR) is used to guide the development of this study. The model construction is achieved using the ontology development 101 procedure. The main result is the creation of the ontological model called OntoProg, which presents: a generic ontology addressing by international standards, capable of being used in several types of mechanical machines, of different types of manufacturing, the possibility of storing the knowledge contained in events of real activities that allow through consultations in SPARQL for decision-making which enable timely interventions of maintenance in the equipment of a real industry. The limitation of the work is that said model can be implemented only by specialists who have knowledge in ontology.}
}
@article{VIGO2019100473,
title = {Comparing ontology authoring workflows with Protégé: In the laboratory, in the tutorial and in the ‘wild’},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100473},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300477},
author = {Markel Vigo and Nicolas Matentzoglu and Caroline Jay and Robert Stevens},
keywords = {Empirical studies, Ontologies, Usability, Semantic web, Authoring tools, Engineering},
abstract = {The development of ontology engineering tools has traditionally lacked a user-centred perspective, instead being guided by the need to address particular gaps indicated by anecdotal evidence. This has typically resulted in prototypes that do not obtain traction beyond a narrow scope. Understanding the authoring patterns of ontology engineers is crucial to informing the development of ontology engineering tools that cater for the activity workflows of the users and, consequently, boosting the adoption of these tools. We report evidence about how Protégé is used across three different authoring settings, addressing the threats to validity of relying on a single user study. These settings address the continuum of expertise (from intermediate to expert users), the type of tasks (whether they are free-form or prescriptive) and the effect of the location (laboratory, tutorial or on their own) and how the studies are administered (whether or not there is a close supervision). While there are activity workflows that are particular to settings, the results indicate a number of core workflows that are common to all of them. We discuss actionable recommendations for ontology engineering tools in light of these results.}
}
@article{SPOLADORE2022103690,
title = {An evaluation of agile Ontology Engineering Methodologies for the digital transformation of companies},
journal = {Computers in Industry},
volume = {140},
pages = {103690},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103690},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000872},
author = {Daniele Spoladore and Elena Pessot},
keywords = {Digital transformation, Industry 4.0, Ontology Engineering, Agile methodologies, Organisational learning},
abstract = {Ontologies are increasingly recognised among the key enablers of the digital transformation of knowledge management processes, but still with a low level of adoption in manufacturing companies. Because ontologies and underlying technologies are complex, Ontology Engineering Methodologies (OEMs) provide a set of guidelines to move from an informal to a formal representation of the company’s knowledge base. This study evaluates three agile OEMs, i.e. UPONLite, SAMOD and RapidOWL, in terms of their process and outcome features, i.e. the OEM steps and the expected quality of the ontological models produced. The assessment is performed from the viewpoint of developers of ontology-based technologies in real industrial use cases. Results show that the three agile OEMs reflect different features to effectively support the digital transformation of companies' knowledge management; thus, they cannot be interchangeable. UPONLite is more effective in contexts where there is a lack of skills in OE, with the need for a structured approach in involving domain experts and generating documentation. SAMOD requires a more extended development period, but with several cycles that allow to map different types of knowledge and enable a “try-and-learn” approach. Conversely, RapidOWL lacks a structured sequence of modelling activities and encourages developers to be creative, but at the same time requires higher expertise in OE. Thus, companies and personnel dedicated to OE should choose the methodology according to the main aims guiding their digitalisation process, the current development status, and the level of expertise.}
}
@article{TRAPPEY2025102332,
title = {Patent litigation mining using a large language model—Taking unmanned aerial vehicle development as the case domain},
journal = {World Patent Information},
volume = {80},
pages = {102332},
year = {2025},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2024.102332},
url = {https://www.sciencedirect.com/science/article/pii/S0172219024000723},
author = {Amy J.C. Trappey and Shao-Chien Chou and Gi-Kuen J. Li},
keywords = {Unmanned aerial vehicle (UAV), Drone, Patent analysis, Patent litigation mining, Technology function matrix, Dynamic topic modeling, Large language model},
abstract = {As unmanned aerial vehicle (UAV), also called “drone”, swiftly advances with innovative functions and applications, the surge in patent applications has profoundly reshaped the intellectual property (IP) landscape in the UAV industry, leading to a growing number of litigations. This study is structured in two phases, aiming to develop an intelligent approach to analyzing the trend and evolution of patent litigations. The first phase involves macro- and micro-patent analyses of the related technology domain. Macro patent analysis elucidates the fundamental patent information in the drone industry, while micro patent analysis leverages the technology function matrix (TFM) to identify R&D hotspots and potentials. The second phase involves litigation (judgement) mining based on large language model (LLM). Beginning with the construction of a knowledge ontology, the domain infringement landscape can be detected through TFMs. A comparative analysis of the two-phase TFMs (i.e., both TFMs of patent and infringement allocations) is then conducted to pinpoint the key legal actions and the relevant technology. To drill deeper in infringement mining, dynamic topic modeling (DTM) is applied to analyze trends and dynamics in drone controller technology over time. This study aims to strengthen IP protection by developing an intelligent litigation mining approach that adopts large language model (LLM) and uses UAV/drone litigation studies as examples to show how the approach being applied in the industry.}
}
@article{SCHONFELDER2025103761,
title = {Ontology-based reasoning in automatic floor plan analysis},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103761},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103761},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625006548},
author = {Phillip Schönfelder and Markus König},
keywords = {Building information modeling, As-built modeling, Floor plan analysis, Knowledge graphs, Semantic enrichment, SPARQL},
abstract = {The growing need for digital representations of existing buildings in the Architecture, Engineering, Construction & Operations (AECO) domain necessitates efficient methods to retrospectively create Building Information Modeling (BIM) models. One prominent approach to obtain the necessary information is Plan-to-BIM, i.e., analyzing building documentation such as floor plans. However, the storage of this information is not standardized which leads to compatibility issues in collaborative scenarios. To address this, the paper presents the Drawing Analysis Ontology (DAnO), which is designed to standardize the representation of technical drawing data extracted through computer vision techniques. Focusing on floor plans, DAnO enables the aggregation, integration, and validation of extracted elements by defining key concepts, such as DrawingElement, DisplayElement, and DescriptionElement, and their relationships. By means of real floor plans, a case study demonstrates the ontology’s effectiveness in facilitating the generation of building models from legacy drawings, highlighting its potential to streamline BIM reconstruction workflows and to enhance interoperability in the AECO industry.}
}
@article{POVEDAVILLALON2022104755,
title = {LOT: An industrial oriented ontology engineering framework},
journal = {Engineering Applications of Artificial Intelligence},
volume = {111},
pages = {104755},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.104755},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622000525},
author = {María Poveda-Villalón and Alba Fernández-Izquierdo and Mariano Fernández-López and Raúl García-Castro},
keywords = {Ontology engineering, Ontology development methodology, Ontology development software support, Collaborative ontology development, Ontology industrial development},
abstract = {Ontology Engineering has captured much attention during the last decades leading to the proliferation of numerous works regarding methodologies, guidelines, tools, resources, etc. including topics which are still being investigated. Even though, there are still many open questions when addressing a new ontology development project, regarding how to manage the overall project and articulate transitions between activities or which tasks and tools are recommended for each step. In this work we propose the Linked Open Terms (LOT) methodology, an overall and lightweight methodology for building ontologies based on existing methodologies and oriented to semantic web developments and technologies. The LOT methodology focuses on the alignment with industrial development, in addition to academic and research projects, and software development, that is making ontology development part of the software industry. This methodology includes lessons learnt from more than 20 years in ontological engineering and its application on 18 projects is reported.}
}
@article{ELGHOSH2025102419,
title = {CriMOnto: A generalized domain-specific ontology for modeling procedural norms of the Lebanese criminal law},
journal = {Data & Knowledge Engineering},
volume = {158},
pages = {102419},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102419},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2500014X},
author = {Mirna {El Ghosh} and Hala Naja and Habib Abdulrab and Mohamad Khalil},
keywords = {AI & Law, Criminal law, Procedural norms, Generalized ontology, UFO, UFO-L, Ontology-Driven Conceptual Modeling, Ontology Patterns, Formal rules},
abstract = {Criminal (or penal) law regulates offenses, offenders, and legal punishments. Modeling criminal law is gaining much attention in the ontology engineering community. However, a significant aspect is neglected: the explicit representation of procedural knowledge. Procedural norms, such as regulative norms, are addressed to agents in the normative system. They govern the different interactions among these agents. In this study, we propose a formal and faithful representation of the procedural aspect of legal norms in the context of the Lebanese Criminal Code. A modular domain-specific ontology named CriMOnto is developed for this purpose. CriMOnto is grounded in the Unified Foundational Ontology (UFO) and the legal core ontology UFO-L by applying the Ontology-Driven Conceptual Modeling (ODCM) process. Conceptual Ontology Patterns (COPs) are reused from UFO and UFO-L to build the hierarchical and procedural content of the ontology. CriMOnto is validated as a formal ontology and evaluated using a dual evaluation approach. The potential use of CriMOnto for lightweight rule-based decision support is discussed in this study.}
}
@article{PATEL2023118998,
title = {An NLP-guided ontology development and refinement approach to represent and query visual information},
journal = {Expert Systems with Applications},
volume = {213},
pages = {118998},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118998},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422020164},
author = {Ashish Singh Patel and Giovanni Merlino and Antonio Puliafito and Ranjana Vyas and O.P. Vyas and Muneendra Ojha and Vivek Tiwari},
keywords = {Semantic web, Multimedia representation, Ontology engineering, Knowledge graph, Information retrieval},
abstract = {The ubiquitous presence of surveillance systems generates massive amounts of video data. Storage and analysis of this data in real-time is a substantial challenge. There is huge potential in representing data in machine-readable and machine-interpretable format due to the presence of hidden semantics in images and videos. However, such representation requires ontology, which calls for expert domain knowledge. In this paper, a novel NLP-guided approach to generate an ontology for multimedia representation and information retrieval is proposed. A semi-automatic NLP-guided framework, which extracts all possible relations among objects is presented. This framework leverages the textual data of the domain to generate possible descriptions and actions within the domain. Relations among objects get embedded as object properties, whereas the category of an object as a class. Features and attributes of objects encode the data properties of the ontology. The proposed ontology is compared with existing multimedia ontologies and evaluated with regard to its capability to represent relations occurring in benchmark datasets, demonstrating the completeness and thorough coverage of the domain concepts. Spatial reasoning rules are established using Semantic Web Rule Language (SWRL) rules, and information retrieval is demonstrated using Description Logic (DL) and SPARQL queries. The proposed NLP-guided ontology generation approach is general enough to help in the development of ontologies for other domains as well, by providing video and textual data of the domain of interest, with limited human involvement.}
}
@article{WIDMER2023100807,
title = {Towards human-compatible XAI: Explaining data differentials with concept induction over background knowledge},
journal = {Journal of Web Semantics},
volume = {79},
pages = {100807},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100807},
url = {https://www.sciencedirect.com/science/article/pii/S1570826823000367},
author = {Cara Leigh Widmer and Md Kamruzzaman Sarker and Srikanth Nadella and Joshua Fiechter and Ion Juvina and Brandon Minnery and Pascal Hitzler and Joshua Schwartz and Michael Raymer},
keywords = {Concept induction, Explainable AI, Class hierarchy},
abstract = {Concept induction, which is based on formal logical reasoning over description logics, has been used in ontology engineering in order to create ontology (TBox) axioms from the base data (ABox) graph. In this paper, we show that it can also be used to explain data differentials, for example in the context of Explainable AI (XAI), and we show that it can in fact be done in a way that is meaningful to a human observer. Our approach utilizes a large class hierarchy, curated from the Wikipedia category hierarchy, as background knowledge. To make the explanations easily understandable for non-specialists, the complex description logic explanations generated by our concept induction system (ECII) were presented as a word list consisting of the concept names occurring in the highest rated system responses.}
}
@article{CHAMARI2025116257,
title = {Towards portable model predictive control-based applications for demand side management in buildings},
journal = {Energy and Buildings},
volume = {347},
pages = {116257},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2025.116257},
url = {https://www.sciencedirect.com/science/article/pii/S0378778825009879},
author = {Lasitha Chamari and Shalika Walker and Ekaterina Petrova and Pieter Pauwels},
keywords = {Brick ontology, Smart charging, Microservices, Resource description framework, Semantic web, Service-oriented architecture},
abstract = {Demand Side Management (DSM) applications in buildings rely on heterogeneous information systems, with data originating from different sources. Semantic Web technologies allow for connecting these disparate data sources by standardising metadata based on ontologies. Recent research focuses on designing portable control applications that can run across buildings. Although the first step towards realising portable control actions is standardising the metadata of the buildings, significant gaps still exist in the literature when it comes to creating portable Model Predictive Control (MPC)-based DSM applications. Many existing portable applications are simple rule-based programmes and the principles of semantic portability are not clear in the areas like with MPC systems for DSM. This paper proposes a combination of modular services and metadata standardisation towards making MPC systems more portable across buildings. The method consists of (1) decomposing the MPC system into modular and reusable services and exposing their data using already standardised web interfaces, (2) extending metadata schemes to formalise the information requirements of the modular services, and (3) devising a modular semantic-driven portability service to query, validate, and configure the MPC system. Although full portability remains a challenge due to heterogeneity in building systems and metadata modelling styles, our approach demonstrates the feasibility of using standardised ontologies, semantic validation, and modular services to partially automate configuration and integration, thereby a step towards portable MPC applications. The proposed workflow is implemented, tested, and validated in a MPC system as part of a DSM strategy for controlling Electric Vehicle (EV) charging behaviour in an office microgrid system.}
}
@article{FERNANDEZIZQUIERDO2021104026,
title = {Conformance testing of ontologies through ontology requirements},
journal = {Engineering Applications of Artificial Intelligence},
volume = {97},
pages = {104026},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.104026},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620303079},
author = {Alba Fernández-Izquierdo and Raúl García-Castro},
keywords = {Ontology conformance, Ontology engineering, Ontology testing, Standard},
abstract = {In recent years, several standard ontologies have been developed to maximise semantic interoperability in different domains; such standard ontologies ensure quality and integrity when describing a domain. Therefore, mechanisms to guarantee that developers build ontologies that conform to such standards are needed. However, while in fields such as Software Engineering or industry, conformance testing plays an essential role during product development, in the Ontology Engineering field there is a lack of techniques for this type of testing. This work introduces an ontology conformance testing method to analyse conformance between an ontology and a standard based on the standard requirements. Grounded on this method, the work also presents a minimum common knowledge identification method for analysing how a group of standards covers a particular domain and for identifying whether there are conflicts between them. This work has been validated by analysing the conformance between an ontology network and a set of standards on the Internet of Things domain, and by analysing the minimum common knowledge between such standards. This analysis shows that the conformance between ontologies and standards is mostly related to definition of classes. Furthermore, the analysis shows that although the analysed standards are related to the same domain, they are created to describe different areas of concern and, thus, there is a minimum overlap between them. Finally, it was concluded that the quality of the conformance analysis depends on the quality of the requirements specification: the more precise the requirements, the more precise the analysis between ontologies and standards.}
}
@article{WANG2020,
title = {Using Natural Language Processing Techniques to Provide Personalized Educational Materials for Chronic Disease Patients in China: Development and Assessment of a Knowledge-Based Health Recommender System},
journal = {JMIR Medical Informatics},
volume = {8},
number = {4},
year = {2020},
issn = {2291-9694},
doi = {https://doi.org/10.2196/17642},
url = {https://www.sciencedirect.com/science/article/pii/S2291969420001556},
author = {Zheyu Wang and Haoce Huang and Liping Cui and Juan Chen and Jiye An and Huilong Duan and Huiqing Ge and Ning Deng},
keywords = {health education, ontology, natural language processing, chronic disease, recommender system},
abstract = {Background
Health education emerged as an important intervention for improving the awareness and self-management abilities of chronic disease patients. The development of information technologies has changed the form of patient educational materials from traditional paper materials to electronic materials. To date, the amount of patient educational materials on the internet is tremendous, with variable quality, which makes it hard to identify the most valuable materials by individuals lacking medical backgrounds.
Objective
The aim of this study was to develop a health recommender system to provide appropriate educational materials for chronic disease patients in China and evaluate the effect of this system.
Methods
A knowledge-based recommender system was implemented using ontology and several natural language processing (NLP) techniques. The development process was divided into 3 stages. In stage 1, an ontology was constructed to describe patient characteristics contained in the data. In stage 2, an algorithm was designed and implemented to generate recommendations based on the ontology. Patient data and educational materials were mapped to the ontology and converted into vectors of the same length, and then recommendations were generated according to similarity between these vectors. In stage 3, the ontology and algorithm were incorporated into an mHealth system for practical use. Keyword extraction algorithms and pretrained word embeddings were used to preprocess educational materials. Three strategies were proposed to improve the performance of keyword extraction. System evaluation was based on a manually assembled test collection for 50 patients and 100 educational documents. Recommendation performance was assessed using the macro precision of top-ranked documents and the overall mean average precision (MAP).
Results
The constructed ontology contained 40 classes, 31 object properties, 67 data properties, and 32 individuals. A total of 80 SWRL rules were defined to implement the semantic logic of mapping patient original data to the ontology vector space. The recommender system was implemented as a separate Web service connected with patients' smartphones. According to the evaluation results, our system can achieve a macro precision up to 0.970 for the top 1 recommendation and an overall MAP score up to 0.628.
Conclusions
This study demonstrated that a knowledge-based health recommender system has the potential to accurately recommend educational materials to chronic disease patients. Traditional NLP techniques combined with improvement strategies for specific language and domain proved to be effective for improving system performance. One direction for future work is to explore the effect of such systems from the perspective of patients in a practical setting.}
}
@article{SPOLADORE2023103979,
title = {A novel agile ontology engineering methodology for supporting organizations in collaborative ontology development},
journal = {Computers in Industry},
volume = {151},
pages = {103979},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103979},
url = {https://www.sciencedirect.com/science/article/pii/S016636152300129X},
author = {Daniele Spoladore and Elena Pessot and Alberto Trombetta},
keywords = {Ontology Engineering, Ontology Authoring, Agile Ontology Development Methodology, Knowledge Engineering, Industry 4.0},
abstract = {Ontologies can represent technological enablers for knowledge elicitation and management in different kinds of organizations, especially with the exponential growth of sources and types of data fostered by digital transformation. However, their adoption in business applications is still limited, with existing Ontology Engineering Methodologies (OEMs) lacking adequate support during knowledge elicitation, authoring and reuse phases. This paper introduces a novel agile ontology engineering methodology (AgiSCOnt) to support ontologists (especially novice ones) in ontology development workflow, fostering collaboration with domain experts in an iterative, flexible and customizable approach. AgiSCOnt combines macro-level instructions with micro-level guidance, leveraging existing techniques and a management framework to help novice ontologists throughout the whole ontology engineering process. The methodology is compared to existing OEMs and assessed with three other agile methodologies (UPONLite, SAMOD, and RapidOWL). The evaluation is conducted with a sample of novice ontologists in a learning environment on Industry 4.0 technologies. Both the development process with a methodology from a user perspective and the quality of the developed ontologies were considered in the evaluation. Preliminary results show that AgiSCOnt effectively supports authoring and reuse, with developed ontologies of good quality. It is perceived as clear and simple, while being flexible and adaptable enough, thus supporting knowledge management and sharing in industrial organizations through the documentation of the ontologies.}
}
@article{EDDINEMEFTAH2025440,
title = {An Intelligent Arabic Legal Assistant system (IALAS) based on Ontology},
journal = {Transportation Research Procedia},
volume = {84},
pages = {440-447},
year = {2025},
note = {Smart Mobility and Logistics Ecosystems},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2025.03.094},
url = {https://www.sciencedirect.com/science/article/pii/S2352146525001425},
author = {Mohammed Charaf {Eddine Meftah} and Abdelhak Soussa and Adel Herzallah},
keywords = {Legal texts, Information search, Natural language processing (Arabic), Ontology, An intelligent system},
abstract = {Laws and regulations can be modified by experts in the legal field in response to various changes in the lives of individuals and communities. Massive changes and updates are constantly being made to laws to adapt to societal changes. This creates a huge database of legal information. Manually searching for information in this database takes a lot of time and effort and affects the efficiency and governance of all administrative and community affairs. To solve this problem, this paper proposes a solution based on one of the types of artificial intelligence. It is an ontology-based solution. This paper explains the design and development of a computer advisory system that helps in making legal decisions based on a proposed ontological structure using Protégé. A set of tools were also chosen to develop the proposed system. For operation, OwlReady2 with SPARQL query language was also used to extract content from the proposed ontology, Camel tools as a natural language processing (Arabic) tool, and SQLite for the database. This work contributes to filling a gap regarding the Arab cognitive modeling of Arab laws to keep pace in sustainable cognitive cities.}
}
@article{SHAW2025106282,
title = {Knowledge graph for policy- and practice-aligned life cycle analysis and reporting},
journal = {Automation in Construction},
volume = {176},
pages = {106282},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106282},
url = {https://www.sciencedirect.com/science/article/pii/S092658052500322X},
author = {Conor Shaw and Flávia {de Andrade Pereira} and Martijn {de Riet} and Cathal Hoare and Karim Farghaly and James O’Donnell},
keywords = {Environmental policy, Life cycle assessment, Asset management, Information management, Requirements engineering, Knowledge graph, Ontology engineering},
abstract = {The built environment is a key leverage point for policy intervention to combat climate change and the statutory reporting of financial and non-financial indicators over the asset lifecycle is increasingly required. This poses significant information management challenges in a sector characterised by complexity. Contributions to-date which address Life Cycle Asset Information Management (LCAIM) remain siloed and difficult to generalise, resulting in limited in-practice uptake, but domain literature identifies graph databases and ontologies as suitable strategies for addressing this information-intensive challenge. This paper provides a LCAIM ontology, co-developed with stakeholders, and verified technically through implementation in a case study by responding to end-user-defined storage, retrieval, and enrichment functions using a knowledge graph. The prototype is then validated qualitatively with experts who perceive it as addressing collective governance-practice requirements. Overall, the study suggests that addressing technical LCAIM challenges may be feasible using available technologies and recommends prioritising research towards socio-economic issues.}
}
@article{ALI2018127,
title = {Cross-Lingual Ontology Enrichment Based on Multi-Agent Architecture},
journal = {Procedia Computer Science},
volume = {137},
pages = {127-138},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S187705091831617X},
author = {Mohamed Ali and Said Fathalla and Shimaa Ibrahim and Mohamed Kholief and Yasser Hassan},
keywords = {cross-lingual ontology enrichment, multi-agent, knowledge management, ontology learning.},
abstract = {The proliferation of ontologies and multilingual data available on the Web has motivated many researchers to contribute to multilingual and cross-lingual ontology enrichment. Cross-lingual ontology enrichment greatly facilitates ontology learning from multilingual text/ontologies in order to support collaborative ontology engineering process. This article proposes a cross-lingual ontology enrichment (CLOE) approach based on a multi-agent architecture in order to enrich ontologies from a multilingual text or ontology. This has several advantages: 1) an ontology is used to enrich another one, written in a different natural language, and 2) several ontologies could be enriched at the same time using a single chunk of text (Simultaneous Ontology Enrichment). A prototype for the proposed approach has been implemented in order to enrich several ontologies using English, Arabic and German text. Evaluation results are promising and showing that CLOE performs well in comparison with four state-of-the-art approaches.}
}
@article{PANKOWSKA201911,
title = {Business Models in CMMN, DMN and ArchiMate language},
journal = {Procedia Computer Science},
volume = {164},
pages = {11-18},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.148},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321878},
author = {Malgorzata Pankowska},
keywords = {business modelling, ArchiMate, CMMN, DMN, BPMN, business model mapping},
abstract = {Business modelling can be considered as a practice for enabling change in enterprise by defining recommended Information Communication Technology (ICT) solutions, which provide value to business stakeholders. Business modelling and business analyses can be connected with different business models, techniques, and software tools. The goal of the paper is to present and discuss which business models are already well known for enterprise architecture (EA) business analysis. The literature review was done for this purpose. Beyond that, the paper aims to present a classification of business models and their mapping in ArchiMate language into different notations and languages diagrams for information architecture modelling. Proposed in this paper the business model mapping was applied in author’s earlier projects as well as it is used in university course teaching on system analysis and modelling.}
}
@article{MASSARI20232392,
title = {Effectiveness of applying Machine Learning techniques and Ontologies in Breast Cancer detection},
journal = {Procedia Computer Science},
volume = {218},
pages = {2392-2400},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.214},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923002144},
author = {Hakim El Massari and Noreddine Gherabi and Sajida Mhammedi and Zineb Sabouri and Hamza Ghandi and Fatima Qanouni},
keywords = {Prediction, Ontology, Machine Learning, SWRL, Breast cancer},
abstract = {Breast cancer is a disease that primarily affects women, but it can also affect men, although in a much smaller percentage. Recently, doctors have made great strides in this trend of early detection and treatment of breast cancer to reduce the number of deaths caused by this serious disease. Moreover, researchers are analyzing massive amounts of sophisticated medical data using a combination of statistical and machine learning approaches to help clinicians predict breast cancer. In the presented work, an ontological model based on the decision tree algorithm capable of reliably predicting breast cancer has been demonstrated. The method consists of extracting rules from the decision tree algorithm that distinguish between malignant and benign breast cancer patients, and then implementing these rules in the ontological reasoner via the Semantic Web Rule Language (SWRL). The results indicated that the ontological model achieved the highest prediction accuracy of 97.10%.}
}
@article{IQBAL2024102257,
title = {Blockchain-based ontology driven reference framework for security risk management},
journal = {Data & Knowledge Engineering},
volume = {149},
pages = {102257},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102257},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23001179},
author = {Mubashar Iqbal and Aleksandr Kormiltsyn and Vimal Dwivedi and Raimundas Matulevičius},
keywords = {Blockchain, Security risk management, Ontology framework, Web ontology language, Unified foundational ontology, CPNs tool},
abstract = {Security risk management (SRM) is crucial for protecting valuable assets from malicious harm. While blockchain technology has been proposed to mitigate security threats in traditional applications, it is not a perfect solution, and its security threats must be managed. This paper addresses the research problem of having no unified and formal knowledge models to support the SRM of traditional applications using blockchain and the SRM of blockchain-based applications. In accordance with this, we present a blockchain-based reference model (BbRM) and an ontology driven reference framework (OntReF) for the SRM of traditional and blockchain-based applications. The BbRM consolidates security threats of traditional and blockchain-based applications, structured following the SRM domain model and offers guidance for creating the OntReF using the domain model. OntReF is grounded on unified foundational ontology (UFO) and provides semantic interoperability and supporting the dynamic knowledge representation and instantiation of information security knowledge for the SRM. Our evaluation approaches demonstrate that OntReF is practical to use.}
}
@article{DEEPAK2022107736,
title = {An artificially intelligent approach for automatic speech processing based on triune ontology and adaptive tribonacci deep neural networks},
journal = {Computers & Electrical Engineering},
volume = {98},
pages = {107736},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107736},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622000489},
author = {Gerard Deepak and Deepak Surya and Ishdutt Trivedi and Ayush Kumar and Amrutha Lingampalli and Santhana vijayan},
keywords = {Acoustic model, Automatic speech recognition, Tribonacci deep neural network},
abstract = {Automatic Speech Recognition systems have become essential for an independent automation during the present-day era. A hybrid approach for Automatic Speech Recognition, the TriNNOnto has been proposed in this paper which, integrates different approaches like Language Model integrated with dynamic Triune Ontology generation scheme, Acoustic Model and Feature modelling are hybridised based on the Tribonacci based Deep Neural Network, which decides upon the number of layers depending on the size of the samples and their count. The dynamic generation of Ontologies based on the language models and triune ontology for automatic speech recognition is quite novel. The strategies for feature extraction as and the Tribonacci based deep neural network, based the dynamic adjustment of the number of layers using Tribonacci series contributes towards novelty as well as enhances the performance of speech recognition. The proposed strategy has been evaluated for two datasets and an accuracy of 98.15% and 95.18%, have been achieved for the CMUKids and the TIMIT datasets, respectively with low word error rates.}
}
@article{SPOLADORE2024,
title = {An Ontology-Based Decision Support System for Tailored Clinical Nutrition Recommendations for Patients With Chronic Obstructive Pulmonary Disease: Development and Acceptability Study},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/50980},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000723},
author = {Daniele Spoladore and Vera Colombo and Alessia Fumagalli and Martina Tosi and Erna Cecilia Lorenzini and Marco Sacco},
keywords = {ontology-based decision support system, nutritional recommendation, chronic obstructive pulmonary disease, clinical decision support system, pulmonary rehabilitation},
abstract = {Background
Chronic obstructive pulmonary disease (COPD) is a chronic condition among the main causes of morbidity and mortality worldwide, representing a burden on health care systems. Scientific literature highlights that nutrition is pivotal in respiratory inflammatory processes connected to COPD, including exacerbations. Patients with COPD have an increased risk of developing nutrition-related comorbidities, such as diabetes, cardiovascular diseases, and malnutrition. Moreover, these patients often manifest sarcopenia and cachexia. Therefore, an adequate nutritional assessment and therapy are essential to help individuals with COPD in managing the progress of the disease. However, the role of nutrition in pulmonary rehabilitation (PR) programs is often underestimated due to a lack of resources and dedicated services, mostly because pneumologists may lack the specialized training for such a discipline.
Objective
This work proposes a novel knowledge-based decision support system to support pneumologists in considering nutritional aspects in PR. The system provides clinicians with patient-tailored dietary recommendations leveraging expert knowledge.
Methods
The expert knowledge—acquired from experts and clinical literature—was formalized in domain ontologies and rules, which were developed leveraging the support of Italian clinicians with expertise in the rehabilitation of patients with COPD. Thus, by following an agile ontology engineering methodology, the relevant formal ontologies were developed to act as a backbone for an application targeted at pneumologists. The recommendations provided by the decision support system were validated by a group of nutrition experts, whereas the acceptability of such an application in the context of PR was evaluated by pneumologists.
Results
A total of 7 dieticians (mean age 46.60, SD 13.35 years) were interviewed to assess their level of agreement with the decision support system’s recommendations by evaluating 5 patients’ health conditions. The preliminary results indicate that the system performed more than adequately (with an overall average score of 4.23, SD 0.52 out of 5 points), providing meaningful and safe recommendations in compliance with clinical practice. With regard to the acceptability of the system by lung specialists (mean age 44.71, SD 11.94 years), the usefulness and relevance of the proposed solution were extremely positive—the scores on each of the perceived usefulness subscales of the technology acceptance model 3 were 4.86 (SD 0.38) out of 5 points, whereas the score on the intention to use subscale was 4.14 (SD 0.38) out of 5 points.
Conclusions
Although designed for the Italian clinical context, the proposed system can be adapted for any other national clinical context by modifying the domain ontologies, thus providing a multidisciplinary approach to the management of patients with COPD.}
}
@article{FH2025112762,
title = {BIM ontology for information management (BIM-OIM)},
journal = {Journal of Building Engineering},
volume = {107},
pages = {112762},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.112762},
url = {https://www.sciencedirect.com/science/article/pii/S2352710225009994},
author = {Abanda F.H and Akintola A and Tuhaise V.V and Tah J.H.M},
keywords = {BIM, BIM execution plan, Information management, ISO 19650, Ontology},
abstract = {The adoption of Building Information Modelling (BIM) in the construction industry has been hindered by numerous barriers, notably the limited understanding of its concepts, protocols, and the intricate interplay between processes, people, and technologies. To address these challenges, a range of standards and guidelines have been developed, most notably the ISO 19650 series, which offer a comprehensive framework for implementing various aspects of BIM in construction projects. However, despite the BIM's collaborative philosophy, the standards and specifications that guide its adoption and implementation seldom reveal and explain the relationships between their key elements and concepts. This lack of clarity limits understanding and undermines the very essence of collaboration that BIM seeks to promote in construction projects. The text-based nature of the standards and specifications makes it difficult to identify common concepts that cut across the different project phases, their relationships, and interdependencies. This study proposes a BIM ontology for information management (BIM-OIM) that makes BIM process data more available and easily useable, allowing other researchers and practitioners to implement, and extend its use within their domains of practice. To achieve the practice-driven goal of BIM-OIM, Yet Another Methodology for Ontology (YAMO), one of the leading ontology engineering methodologies, was used to develop BIM-OIM. BIM-OIM is a formal and structured representation of ISO 19650 knowledge that is machine-processable. This representation enhances understanding, promotes reusability, and supports practical applications throughout the information management lifecycle. Key applications include the development of BIM Execution Plans, compliance checking for information containers, and identifying the roles of various stakeholders within a project.}
}
@article{SHIMIZU2025100862,
title = {Accelerating knowledge graph and ontology engineering with large language models},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100862},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100862},
url = {https://www.sciencedirect.com/science/article/pii/S1570826825000022},
author = {Cogan Shimizu and Pascal Hitzler},
keywords = {Knowledge graph engineering, Ontology engineering, Large language models, Modular ontologies, Ontology modeling, Ontology population, Ontology alignment, Entity disambiguation},
abstract = {Large Language Models bear the promise of significant acceleration of key Knowledge Graph and Ontology Engineering tasks, including ontology modeling, extension, modification, population, alignment, as well as entity disambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering as a new and coming area of research, and argue that modular approaches to ontologies will be of central importance.}
}
@article{NAQVI20223679,
title = {Ontological Model for Cohesive Smart Health Services Management},
journal = {Computers, Materials and Continua},
volume = {74},
number = {2},
pages = {3679-3695},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.030340},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822004052},
author = {Muhammad Raza Naqvi and Muhammad Waseem Iqbal and Syed Khuram Shahzad and M. {Usman Ashraf} and Khalid Alsubhi and Hani Moaiteq Aljahdali},
keywords = {Ontology, internet of things, smart health, services integration},
abstract = {Health care has become an essential social-economic concern for all stakeholders (e.g., patients, doctors, hospitals etc.), health needs, private care and the elderly class of society. The massive increase in the usage of health care Internet of things (IoT) applications has great technological evolvement in human life. There are various smart health care services like remote patient monitoring, diagnostic, disease-specific remote treatments and telemedicine. These applications are available in a split fashion and provide solutions for variant diseases, medical resources and remote service management. The main objective of this research is to provide a management platform where all these services work as a single unit to facilitate the users. The ontological model of integrated healthcare services is proposed by getting requirements from various existing healthcare services. There were 26 smart health care services and 26 smart health care services to classify the knowledge-based ontological model. The proposed ontological model is derived from different classes, relationships, and constraints to integrate health care services. This model is developed using Protégé based on each interrelated/correlated health care service having different values. Semantic querying SPARQL protocol and RDF query language (SPARQL) were used for knowledge acquisition. The Pellet Reasoner is used to check the validity and relations coherency of the proposed ontology model. Comparative to other smart health care services integration systems, the proposed ontological model provides more cohesiveness.}
}
@article{HAGEDORN2025103369,
title = {OntoBPR: An ontology-based framework for performing building permit reviews using standardized information containers},
journal = {Advanced Engineering Informatics},
volume = {66},
pages = {103369},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103369},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625002629},
author = {Philipp Hagedorn and Judith Fauth and Sven Zentgraf and Sebastian Seiß and Markus König and Ioannis Brilakis},
keywords = {Digital building permit, Building permit review, Ontology & semantic web, Information Container for linked Document Delivery (ICDD), Compliance checking, Shapes Constraint Language (SHACL)},
abstract = {Building permitting is essential for ensuring the safety, sustainability, and societal alignment of construction projects. Despite interest from both practitioners and researchers, the process remains largely manual and fragmented. Ontologies offer a promising solution by managing complexity and enabling automation through semantic information, though current ontologies in the building permit domain are limited to specific aspects like building code checking. On the process level, the OntoBPR framework integrates multiple domain-specific ontologies for a seamless digital permitting process and provides a workflow to automate the lifecycle of the permit review. Therefore, it suggests integrating the submitted building application using standardized information containers. The paper explores how digital applications can be submitted, reviewed, verified for completeness, and forwarded to authorities, and how permit review results can be gathered to support decision-making and automate notification issuance, and it provides a demonstration in a case study. In conclusion, OntoBPR formalizes a multi-layered ontology that advances and aligns the partitioned building permit process and provides an adaptable framework to harmonize diverse legal, informatics, and procedural aspects.}
}
@article{DECKERS2022111415,
title = {Systematic literature review of domain-oriented specification techniques},
journal = {Journal of Systems and Software},
volume = {192},
pages = {111415},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111415},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001261},
author = {Robert Deckers and Patricia Lago},
keywords = {Domain-specific language, Domain model, Systematic literature review, Method comparison, Specification method, Modeling language},
abstract = {Context:
The popularity of domain-specific languages and model driven development has made the tacit use of domain knowledge in system development more tangible. Our vision is a development process where a (software) system specification is based on multiple domain models, and where the specification method is built from cognitive concepts, presumably derived from natural language.
Goal:
To realize this vision, we evaluate and reflect upon the existing literature in domain-oriented specification techniques.
Method:
We designed and conducted a systematic literature review on domain-oriented specification techniques.
Results:
We identified 53 primary studies, populated the classification framework for each study, and summarized our findings per classification aspect. We found many approaches for creating domain models or domain-specific languages. Observations include: (i) most methods are defined incompletely; (ii) none offers methodical support for the use of domain models or domain-specific languages to create other specifications; (iii) there are specification techniques to integrate models in general, but no study offers methodical support for multiple domain models.
Conclusion:
The results indicate which topics need further research and which can instead be reused to realize our vision on system development. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{FONSECA2021101894,
title = {Multi-level conceptual modeling: Theory, language and application},
journal = {Data & Knowledge Engineering},
volume = {134},
pages = {101894},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101894},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000215},
author = {Claudenir M. Fonseca and João Paulo A. Almeida and Giancarlo Guizzardi and Victorio A. Carvalho},
keywords = {Multi-level modeling, Modeling language, Conceptual modeling, Methodologies and tools},
abstract = {In many important subject domains, there are central real-world phenomena that span across multiple classification levels. In these subject domains, besides having the traditional type-level domain regularities (classes) that classify multiple concrete instances, we also have higher-order type-level regularities (metaclasses) that classify multiple instances that are themselves types. Multi-Level Modeling aims to address this technical challenge. Despite the advances in this area in the last decade, a number of requirements arising from representation needs in subject domains have not yet been addressed in current modeling approaches. In this paper, we address this issue by proposing an expressive multi-level conceptual modeling language (dubbed ML2). We follow a principled language engineering approach in the design of ML2, constructing its abstract syntax as to reflect a fully axiomatized theory for multi-level modeling (termed MLT*). We show that ML2 enables the expression of a number of multi-level modeling scenarios that cannot be currently expressed in the existing multi-level modeling languages. A textual syntax for ML2 is provided with an implementation in Xtext. We discuss how the formal theory influences the language in two aspects: (i) by providing rigorous justification for the language’s syntactic rules, which follow MLT* theorems and (ii) by forming the basis for model simulation and verification. We show that the language can reveal problems in multi-level taxonomic structures, using Wikidata fragments to demonstrate the language’s practical relevance.}
}
@article{LECU2024443,
title = {Using LLMs and ontologies to extract causal relationships from medical abstracts},
journal = {Procedia Computer Science},
volume = {244},
pages = {443-452},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.219},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030205},
author = {Alexandru Lecu and Adrian Groza and Lezan Hawizy},
keywords = {Causal Relation Extraction, Knowledge Graphs, Large Language Models, Age-Related Macular Degeneration},
abstract = {The substantiation of the causal relationships behind its development is very important in identifying possible interventions and early treatment. Knowledge Graphs (KG) play a crucial role in the medical research domain by organizing data into interconnected structures that represent relationships between entities such as disease, treatments, and progressions. This paper shows a complete workflow that demonstrates the extraction of causal relationships from medical abstracts using a fine-tuned GPT-based model and the integration of these relationships into a KG.}
}
@article{HARI2023367,
title = {WSD based Ontology Learning from Unstructured Text using Transformer},
journal = {Procedia Computer Science},
volume = {218},
pages = {367-374},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923000194},
author = {Akshay Hari and Priyanka Kumar},
keywords = {Deep Learning, Transformers, Ontology, Word Sense Disambiguation, RDF},
abstract = {Representation of knowledge and making it machine comprehensible has become a necessity in modern times but with the large amount of data being generated nowadays, this process has to be automated as much as possible. In this work, we propose a deep-learning based model to build an RDF based Ontology from Unstructured Text. We aim to evaluate the proposed model by creating a general knowledge ontology from newspaper article corpora. The proposed model is based on transformer, Natural Language Processing and contains a Relation Extraction model and novel implementation of RDF mapping algorithm. The main highlight of our model is its ability to handle the Word Sense Disambiguation problem. The model was able to perform well and achieved very high accuracy scores.}
}
@article{JUST20252567,
title = {The Independent Event Log Layer (IELL): Semantic Integration of Industrial IoT Event Logs},
journal = {Procedia Computer Science},
volume = {253},
pages = {2567-2574},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.316},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925003242},
author = {Valentin P. Just and Steindl Gernot and Wolfgang Kastner},
keywords = {IoT, IIoT, Event log, Process-Mining, Ontology, RDF},
abstract = {The Industrial Internet of Things (IIoT) has significantly transformed manufacturing by enabling the integration of physical and digital systems, resulting in extensive data generation. However, extracting actionable insights from this heterogeneous data poses significant challenges due to its distributed nature and the varied architectures and formats from multiple vendors. A unified access method for data processing is essential to overcome these obstacles. This paper introduces the concept of an Independent Event Log Layer (IELL), leveraging the Resource Description Framework (RDF) and ontology-based knowledge representation to standardise and analyse event logs from disparate formats like eXtensible Event Stream (XES) and Comma-Separated Values (CSV). Utilising the RDF Mapping Language (RML), we propose a novel approach to convert event logs into RDF files, creating a unified knowledge base that enhances process mining capabilities. This semantic abstraction facilitates advanced knowledge retrieval and analysis, linking various events and attributes to optimise IIoT processes. A proof-of-concept implementation demonstrates the feasibility of our approach using openly available event log data and RML tooling. The findings underscore the potential of IELL to streamline process mining in IIoT environments, providing unified access for knowledge retrieval and process optimisation.}
}
@article{LONGO2022594,
title = {An ontology-based, general-purpose and Industry 4.0-ready architecture for supporting the smart operator (Part I – Mixed reality case)},
journal = {Journal of Manufacturing Systems},
volume = {64},
pages = {594-612},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001303},
author = {Francesco Longo and Giovanni Mirabelli and Letizia Nicoletti and Vittorio Solina},
keywords = {Mixed reality, Ontology, Internet of things, Smart operator, Smart factory},
abstract = {The advent of novel industry 4.0-driven technologies is offering significant opportunities to manufacturing systems, but at the same time it is posing new great challenges. The growing number of connected and interconnected devices is enormously increasing the amount of data generated, which must be properly organized to give value to the business. Basically, the need for approaches that are able to guarantee compliance with FAIR data principles is significantly emerging. Recently, the KNOW4I platform has been proposed in the literature to support the smart operator through a suite of Smart Utilities and Objects (Longo et al., 2022). The main purpose of this paper is to extend such platform, in the form of an ontology-based, general-purpose and industry 4.0-ready architecture, capable of improving the capabilities of the smart operator, with a focus on mixed reality. The novel proposal is based on two fundamental aspects: (1) a new general ontology, developed through the ontology engineering methodology; (2) the adoption of FIWARE, an open-source infrastructure, capable of enabling interoperability between different systems. The proposed architecture is implemented and validated on two case studies belonging to the manufacturing sector, which respectively concern (1) scheduled maintenance and alarm management and (2) customer order management. The experimental phase shows that the architecture is able to effectively and efficiently support the smart operator.}
}
@article{SIOUGKROU2018385,
title = {Semantically-enabled repositories in multi-disciplinary domains: The case of biorefineries},
journal = {Computers & Chemical Engineering},
volume = {116},
pages = {385-400},
year = {2018},
note = {Multi-scale Systems Engineering – in memory & honor of Professor C.A. Floudas},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2018.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S009813541830348X},
author = {Eirini Siougkrou and Filopoimin Lykokanellos and Foteini Barla and Antonis C. Kokossis},
keywords = {Ontology engineering, Biorefineries, Biorenewables, Repository, Synthesis of value chain},
abstract = {There is an increased use of problem representations (i.e. superstructures in synthesis problems; networks in route problems; graphs; ordered graphs in various systems representations) following on significant advances in optimization technologies that hold capabilities to solve, robustly, large-scale problems. In an attempt to systematically tackle disparate domains and build high-throughput functions, the paper contributes with a semantically-enabled approach systematized and engineered by ontologies. The aim is to develop an intelligent environment with capabilities to build and scale-up system representations, automatically. The work is demonstrated on problems akin to biorenewables and biorefineries; an identical approach is possible to the general problem. Using relations and rules defined among entities, semantics are deployed to model and expand domains (biorefinery pathways) whereas enabling extracting and creating knowledge. The repository, already on a web-based platform and available as open-source, essentially upgrades conventional representations with capabilities to share (import/export) and integrate its content externally.}
}
@article{GONZALEZERAS2022100816,
title = {Ontological engineering for the definition of a COVID-19 pandemic ontology},
journal = {Informatics in Medicine Unlocked},
volume = {28},
pages = {100816},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2021.100816},
url = {https://www.sciencedirect.com/science/article/pii/S2352914821002811},
author = {Alexandra González-Eras and Ricardo Dos Santos and Jose Aguilar and Alberto Lopez},
keywords = {COVID ontology, COVID-19, Ontology integration, Ontological engineering},
abstract = {COVID-19 has generated a lot of information in different formats, and one of them is in the ontology format. Also, there are previous ontologies from other disciplines that can help to analyze the COVID-19 pandemic. Thus, due to the large quantity of COVID-19 information in the form of ontologies, approaches to ontology integration and interoperability could be beneficial. In this context, this research proposes a new ontology, called COVID-19 Pandemic ontology, which is the product of an ontological engineering process proposed in this research that allows the integration of several ontologies to cover all the aspects of this infectious disease. The ontological engineering process defines tasks of fusion, alignment, and linking for integrating the ontologies. The resulting pandemic ontology provides a simple repository for storing information about the COVID-19, reusing existing ontologies, to offer multiple views about the disease, including the social context. This ontology has been tested in different case studies to prove its capabilities to infer useful information about the COVID-19 pandemic.}
}
@article{AMINU2022200125,
title = {MaCOnto: A robust maize crop ontology based on soils, fertilizers and irrigation knowledge},
journal = {Intelligent Systems with Applications},
volume = {16},
pages = {200125},
year = {2022},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2022.200125},
url = {https://www.sciencedirect.com/science/article/pii/S266730532200062X},
author = {Enesi Femi Aminu and Ishaq Oyebisi Oyefolahan and Muhammad Bashir Abdullahi and Muhammadu Tajudeen Salaudeen},
keywords = {Maconto, Ontology evolution, Competency question, Maize's soils knowledge, Maize's fertilizer knowledge, Maize's irrigation knowledge},
abstract = {The demand for relevant information in a timely manner portrays the significance of knowledge management in all areas of lives; for instance, agriculture. To this end, soils, fertilizers and irrigation as agronomic concepts are essential knowledge inputs for any crops, such as maize. Conversely, there is always difficulty in timely retrieval of these relevant information owing to the unstructured nature of data in repositories, and complexity of concepts mismatch. Sequel to this development, ontology, a semantic data modeling technique is promising as it has been recently employed to deal with these challenges across different domains. However, the robustness of ontology, in terms of semantic expressivity of hidden knowledge, and autonomous growth of ontology leave some gaps to contend with. In view of this development, this research aims to design a robust OWL Rule based ontology for maize crop domain by considering primarily soils, fertilizers and irrigation agronomic concepts capable to evolve autonomously. The proposed ontology herein christened MaCOnto, is developed using the adapted six steps ontology-engineering principle. Over 1,430 entities are encoded in OWL; eighty Competency Questions (CQs) validated by domain experts are modeled in FOL, and implemented as rules via SWRL. Thus, the ontology is queried by SQWRL. Besides, the novel algorithmic design for the ontology to autonomously evolve is implemented in Java environment by employing WordNet. The results obtained from structural based evaluation show an outstanding performance across the eight metrics. Similarly, the results of the competency-based evaluation are also promising. Therefore, the proposed MaCOnto is a robust application based ontology capable to infer and responds to user's query based on its contextual information.}
}
@article{CIROKU2024100822,
title = {RevOnt: Reverse engineering of competency questions from knowledge graphs via language models},
journal = {Journal of Web Semantics},
volume = {82},
pages = {100822},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100822},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000088},
author = {Fiorela Ciroku and Jacopo {de Berardinis} and Jongmo Kim and Albert Meroño-Peñuela and Valentina Presutti and Elena Simperl},
keywords = {Knowledge engineering, Knowledge graph, Ontology development, Competency question extraction},
abstract = {The process of developing ontologies – a formal, explicit specification of a shared conceptualisation – is addressed by well-known methodologies. As for any engineering development, its fundamental basis is the collection of requirements, which includes the elicitation of competency questions. Competency questions are defined through interacting with domain and application experts or by investigating existing datasets that may be used to populate the ontology i.e. its knowledge graph. The rise in popularity and accessibility of knowledge graphs provides an opportunity to support this phase with automatic tools. In this work, we explore the possibility of extracting competency questions from a knowledge graph. This reverses the traditional workflow in which knowledge graphs are built from ontologies, which in turn are engineered from competency questions. We describe in detail RevOnt, an approach that extracts and abstracts triples from a knowledge graph, generates questions based on triple verbalisations, and filters the resulting questions to yield a meaningful set of competency questions; the WDV dataset. This approach is implemented utilising the Wikidata knowledge graph as a use case, and contributes a set of core competency questions from 20 domains present in the WDV dataset. To evaluate RevOnt, we contribute a new dataset of manually-annotated high-quality competency questions, and compare the extracted competency questions by calculating their BLEU score against the human references. The results for the abstraction and question generation components of the approach show good to high quality. Meanwhile, the accuracy of the filtering component is above 86%, which is comparable to the state-of-the-art classifications.}
}
@article{MELO20251649,
title = {Towards an ontology on project portfolio management},
journal = {Procedia Computer Science},
volume = {256},
pages = {1649-1657},
year = {2025},
note = {CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.02.302},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925006702},
author = {Héctor Melo and Oscar Avila and María del Pilar Villamil},
keywords = {Ontology, portfolio, project, management},
abstract = {Project Portfolio Management (PPM) is essential for organizations aiming to align projects with strategic goals. Different organizations adopt diverse PPM frameworks and standards to manage project portfolios each employing its own terminology. This semantic heterogeneity leads to communication barriers, knowledge silos, and difficulty in integrating information across various platforms in an interorganizational context. This article proposes a PPM ontology to establish a common language encapsulating key concept, addressing this challenge. The methodology involves systematic revision of three prominent PPM standards - ISO 21504, PMI Standard for Portfolio Management, and AXELOS Management of Portfolios, and employs a novel algorithm to identify equivalences and containment relationships between terms across all three standards, resolving semantic ambiguities and enriching the ontology’s expressiveness. This contribution benefits researchers, academics, portfolio managers, project managers, and PPM practitioners by providing a common vocabulary and framework for understanding and improving PPM practices, facilitates knowledge representation, improves communication and collaboration among stakeholders, and lays the groundwork for developing intelligent PPM systems capable of leveraging shared semantic understanding.}
}
@article{MAHI2022,
title = {A Novel Sentence Completion System for Punjabi Using Deep Neural Networks},
journal = {International Journal of Software Innovation},
volume = {10},
number = {1},
year = {2022},
issn = {2166-7160},
doi = {https://doi.org/10.4018/IJSI.293271},
url = {https://www.sciencedirect.com/science/article/pii/S2166716022000297},
author = {Gurjot Singh Mahi and Amandeep Verma},
keywords = {Embedding, GRU, LSTM, Neural Networks, NLP, RNN, Sentence Completion},
abstract = {ABSTRACT
Sentence completion systems are actively studied by many researchers, which ultimately results in the reduction of cognitive effort and enhancement in user experience. The review of the literature reveals that most of the work in the said area is in English and limited effort spent on other languages, especially vernacular languages. This work aims to develop a state-of-the-art sentence completion system for the Punjabi language, which is the 10th most spoken language in the world. The presented work is an outcome of the results of the experimentation on various neural network language model combinations. A new sentence search algorithm (SSA) and patching system are developed to search, complete, and rank the completed sub-string and give a syntactically rich sentence. The quantitative and qualitative evaluation metrics were utilized to evaluate the system. The results are quite promising, and the best performing model is capable of completing a given sub-string with more acceptability. The best performing model is utilized for developing the user interface.}
}
@article{LEGLAZ2021,
title = {Machine Learning and Natural Language Processing in Mental Health: Systematic Review},
journal = {Journal of Medical Internet Research},
volume = {23},
number = {5},
year = {2021},
issn = {1438-8871},
doi = {https://doi.org/10.2196/15708},
url = {https://www.sciencedirect.com/science/article/pii/S1438887121004295},
author = {Aziliz {Le Glaz} and Yannis Haralambous and Deok-Hee Kim-Dufor and Philippe Lenca and Romain Billot and Taylor C Ryan and Jonathan Marsh and Jordan DeVylder and Michel Walter and Sofian Berrouiguet and Christophe Lemey},
keywords = {machine learning, natural language processing, artificial intelligence, data mining, mental health, psychiatry},
abstract = {Background
Machine learning systems are part of the field of artificial intelligence that automatically learn models from data to make better decisions. Natural language processing (NLP), by using corpora and learning approaches, provides good performance in statistical tasks, such as text classification or sentiment mining.
Objective
The primary aim of this systematic review was to summarize and characterize, in methodological and technical terms, studies that used machine learning and NLP techniques for mental health. The secondary aim was to consider the potential use of these methods in mental health clinical practice
Methods
This systematic review follows the PRISMA (Preferred Reporting Items for Systematic Review and Meta-analysis) guidelines and is registered with PROSPERO (Prospective Register of Systematic Reviews; number CRD42019107376). The search was conducted using 4 medical databases (PubMed, Scopus, ScienceDirect, and PsycINFO) with the following keywords: machine learning, data mining, psychiatry, mental health, and mental disorder. The exclusion criteria were as follows: languages other than English, anonymization process, case studies, conference papers, and reviews. No limitations on publication dates were imposed.
Results
A total of 327 articles were identified, of which 269 (82.3%) were excluded and 58 (17.7%) were included in the review. The results were organized through a qualitative perspective. Although studies had heterogeneous topics and methods, some themes emerged. Population studies could be grouped into 3 categories: patients included in medical databases, patients who came to the emergency room, and social media users. The main objectives were to extract symptoms, classify severity of illness, compare therapy effectiveness, provide psychopathological clues, and challenge the current nosography. Medical records and social media were the 2 major data sources. With regard to the methods used, preprocessing used the standard methods of NLP and unique identifier extraction dedicated to medical texts. Efficient classifiers were preferred rather than transparent functioning classifiers. Python was the most frequently used platform.
Conclusions
Machine learning and NLP models have been highly topical issues in medicine in recent years and may be considered a new paradigm in medical research. However, these processes tend to confirm clinical hypotheses rather than developing entirely new information, and only one major category of the population (ie, social media users) is an imprecise cohort. Moreover, some language-specific features can improve the performance of NLP methods, and their extension to other languages should be more closely investigated. However, machine learning and NLP techniques provide useful information from unexplored data (ie, patients’ daily habits that are usually inaccessible to care providers). Before considering It as an additional tool of mental health care, ethical issues remain and should be discussed in a timely manner. Machine learning and NLP methods may offer multiple perspectives in mental health research but should also be considered as tools to support clinical practice.}
}
@article{BUREKA20201053,
title = {A Lightweight Approach to the Multi-perspective Modeling of Processes and Objects},
journal = {Procedia Computer Science},
volume = {176},
pages = {1053-1062},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.101},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920320019},
author = {Patryk Bureka and Heinrich Herre},
keywords = {Semantic Web, Knowledge Representation, Process Modeling, Conceptual Modeling, Ontology},
abstract = {Process modeling has a broad range of applications, varying from business and system engineering, via artifact design, up to natural process modeling utilized in natural sciences. Over the last decades, various sophisticated languages and frameworks have been developed to support process modeling. The current paper discusses an approach to process modeling, which is, in contrast to many existing solutions, intended for the integrated process and object modeling. Furthermore, it is designed to be a lightweight approach with only a few constructs, which, however, permit the representation of processes from various perspectives. The developed solution provides an abstract language-independent model (ontology), partial formalization in first-order logic as well as a Web Ontology Language (OWL) implementation.}
}
@article{KALTENEGGER2025112565,
title = {An ontology-driven framework for digital transformation and performance assessment of building materials},
journal = {Building and Environment},
volume = {271},
pages = {112565},
year = {2025},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2025.112565},
url = {https://www.sciencedirect.com/science/article/pii/S0360132325000472},
author = {Julia Kaltenegger and Kirstine Meyer Frandsen and Ekaterina Petrova},
keywords = {Material information modelling, Building information modelling, Material classification, Semantic web, Ontologies},
abstract = {Material Information Modelling (MIM) is a cornerstone of Building Performance Simulation (BPS). However, defining and exchanging data between building modelling and simulation tools is cumbersome due to notable deficiencies in the granularity of material information descriptions. The inadequacies in the data models and exchanges lead to faulty interpretations of material properties in building performance assessment. The material science domain strives to advance material research and expedite the market readiness of novel materials through intricate data modelling, performance computations, and interdisciplinary communication channels. In addition to the latter, adopting the Findable, Accessible, Interoperable, and Reusable principles holds significant potential in promoting accurate MIM within Architecture, Engineering and Construction. This study introduces an ontology-driven framework leveraging Semantic Web technologies and Linked Data to support MIM in the context of Building Information Modelling and BPS. The framework implementation is demonstrated in a web-based application that enables the dynamic assessment and benchmarking of building materials based on the Guggenheim, Anderson and de Boer model and thermal resistance computations. The development of the framework relies on ontology engineering principles to represent domain knowledge in a Building Material Performance ontology, as well as Systems Engineering coupled with test-driven development for requirement engineering, system design, implementation, and validation. The results include a novel MIM data model enabling material classification and property definitions in alignment with international standards. The implementation validates and assesses the logic of the proposed data model and software application by conducting hygric and thermal performance assessments applied on case studies.}
}
@article{WINDISCH2022550,
title = {Approach for model-based requirements engineering for the planning of engineering generations in the agile development of mechatronic systems},
journal = {Procedia CIRP},
volume = {109},
pages = {550-555},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.293},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122007429},
author = {Emily Windisch and Constantin Mandel and Simon Rapp and Nikola Bursac and Albert Albers},
keywords = {mbse, agility, validation, test planning},
abstract = {The crucial factor for a successful usage of modeling approaches of systems engineering is the interaction of language, method, and tool. For this, specific challenges arise for the application of MBSE in agile requirements engineering. From observations in agile development practice at a machine tool manufacturer, the challenges for model-based requirements engineering are described and each is assigned to its critical aspect of modeling: The language must formally represent the requirements data model, especially for planning engineering generations. The tool must support collaborative, interdisciplinary cooperation, and consider the dynamics of the requirements model during the development process. The method must individually support the requirements engineering activities, which are carried out several times in a sprint during the development process and must enable a target-oriented process for bundling the requirements into engineering generations. Taking these demands into account, an approach is then presented providing activity-based views in conjunction with activity steps based on a consistent ontology for the description of product requirements and verification activities. The activity steps are composed in activity patterns and support the user in making use of the views for modeling requirements for the engineering generations. The approach is implemented in the software JIRA at a machine tool manufacturer. The subsequent evaluation shows that the approach is used in development practice and offers the potential to plan engineering generation systematically and comprehensibly and to ensure a regular review of the implemented requirements.}
}
@article{YAGO201848,
title = {ON-SMMILE: Ontology Network-based Student Model for MultIple Learning Environments},
journal = {Data & Knowledge Engineering},
volume = {115},
pages = {48-67},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17301945},
author = {Hector Yago and Julia Clemente and Daniel Rodriguez and Pedro Fernandez-de-Cordoba},
keywords = {Ontological engineering, Student modeling, Ontology network, Learning supervision, Semantic web},
abstract = {Currently, many educational researchers focus on the extraction of information about the learning progress to properly assist students. We present ON-SMMILE, a student-centered and flexible student model which is represented as an ontology network combining information related to (i) students and their knowledge state, (ii) assessments that rely on rubrics and different types of objectives, (iii) units of learning and (iv) information resources previously employed as support for the student model in intelligent virtual environment for training/instruction and here extended. The aim of this work is to design and build methodologically, throughout ontological engineering, the ON-SMMILE model to be used as support of future works closely linked to supervision of student's learning as competence-based recommender system. For this purpose, our model is designed as a set of ontological resources that have been extended, standardized, interrelated and adapted to be used in multiple learning environments. In this paper, we also analyze the available approaches based on instructional design which can be added to ontology network to build the proposed model. As a case study, a chemical experiment in a virtual environment and its instantiation are described in terms of ON-SMMILE.}
}
@article{LENTES20223010,
title = {Towards an Ontology for a Lightweight Support System for Production System Rough Planning},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {3010-3015},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.190},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322022042},
author = {Joachim Lentes},
keywords = {Ontology-Based System, production system planning, support system, rough planning, assembly planning},
abstract = {To shorten times-to-markets, also the planners of production systems have to be supported appropriately. For this, open, adaptable systems are needed which support continuous flows of information, e.g. by leveraging standard data formats – and, which are easy to use for planners without specific knowledge about software development or ontology engineering. This paper introduces a support system for production system rough planning, especially for, but not limited to, assembly systems, which consists of two main components: a standard-based ontology as explicitly formulated external data model and a relatively universal software system working on the ontology. Thereby, focus of this contribution is mainly on the ontology, so on manufacturing modeling.}
}
@article{CIMMINO2025104282,
title = {Open Digital Rights Enforcement framework (ODRE): From descriptive to enforceable policies},
journal = {Computers & Security},
volume = {150},
pages = {104282},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104282},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005881},
author = {Andrea Cimmino and Juan Cano-Benito and Raúl García-Castro},
keywords = {Open digital rights language, Privacy policies, ODRL enforcement},
abstract = {From centralised platforms to decentralised ecosystems, like Data Spaces, sharing data has become a paramount challenge. For this reason, the definition of data usage policies has become crucial in these domains, highlighting the necessity of effective policy enforcement mechanisms. The Open Digital Rights Language (ODRL) is a W3C standard ontology designed to describe data usage policies, however, it lacks built-in enforcement capabilities, limiting its practical application. This paper introduces the Open Digital Rights Enforcement (ODRE) framework, whose goal is to provide ODRL with enforcement capabilities. The ODRE framework proposes a novel approach to express ODRL policies that integrates the descriptive ontology terms of ODRL with other languages that allow behaviour specification, such as dynamic data handling or function evaluation. The framework includes an enforcement algorithm for ODRL policies and two open-source implementations in Python and Java. The ODRE framework is also designed to support future extensions of ODRL to specific domain scenarios. In addition, current limitations of ODRE, ODRL, and current challenges are reported. Finally, to demonstrate the enforcement capabilities of the implementations, their performance, and their extensibility features, several experiments have been carried out with positive results.}
}
@article{LANGE2025100330,
title = {Ontologies relevant for improving data interoperability for food loss and waste: A review and research agenda},
journal = {Cleaner and Responsible Consumption},
volume = {19},
pages = {100330},
year = {2025},
issn = {2666-7843},
doi = {https://doi.org/10.1016/j.clrc.2025.100330},
url = {https://www.sciencedirect.com/science/article/pii/S2666784325000816},
author = {Matthew C. Lange and Ran Li and John W. Apolzan and Patrick R. Huber and Emily Steliotes and Kai Robertson and Norbert L.W. Wilson and Karthik Jain and Rajiv Ramnath and Brian E. Roe and Edward S. Spang},
keywords = {Ontology, Food loss and waste, Data interoperability, Food system, Large language models},
abstract = {Food loss and waste (FLW) is a global challenge. Interoperable FLW ontologies will foster more comprehensive data sharing and inform better solutions to reduce and recover excess food and to valorize wasted food and food byproducts. This review reveals that only eight ontologies currently address FLW with most emphasizing valorization. Notably, few are designed explicitly to support FLW reduction, and none facilitate food recovery, which is critical given that reduction and recovery are the preferred means of mitigating FLW. Furthermore, existing FLW ontologies show limited alignment with recognized gold-standard frameworks, for example the Open Biological and Biomedical Ontology (OBO) Foundry, and none support ongoing connectivity to external ontologies, restricting their utility across stakeholder domains. Looking ahead, there is a pressing need to create or expand ontologies that adhere to best practices from relevant foundries to ensure robust linkage and interoperability and undergird structured data ecosystems that support food systems stakeholders in FLW prevention and mitigation. Achieving this goal will require active collaboration among a diverse range of stakeholders, including builders of food systems cyberinfrastructure, scientists, innovators, regulators, public and private funders, community-based organizations, policymakers, and international NGOs as each rely on critical ontological elements to inform decision-making, measure impact, and drive improvement across the food supply chain. Finally, large language models offer promising capabilities for expediting ontology creation, broadening inclusivity in ontology creation, and enhancing the accuracy of resulting data infrastructures.}
}
@article{ROMANO2024124292,
title = {An NLP-based approach to assessing a company’s maturity level in the digital era},
journal = {Expert Systems with Applications},
volume = {252},
pages = {124292},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124292},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424011588},
author = {Simon Pietro Romano and Giancarlo Sperlì and Andrea Vignali},
keywords = {Natural language processing, Embedding representation, Maturity model, Digital transformation},
abstract = {Conducting a maturity assessment allows companies to measure their readiness in implementing novel technologies. However, this task is challenging due to the multidimensional, complex, unpredictable, and non-linear nature of innovation. In this paper, we introduce an innovative approach to maturity assessment that enables both intra- and inter-company analysis. Our approach evaluates a company’s absolute maturity score concerning a specific technology or area. By leveraging a Natural Language Processing pipeline applied to a semi-structured questionnaire we extract popular concepts from the answers and present them to a human expert for analysis. The expert can refine the analysis by adding or removing concepts as needed. Subsequently, we compute a similarity metric for each answer to determine a company’s maturity in specific concepts. The output of our analysis is presented through human-readable plots, offering clear insights into the internal maturity level of the company and allowing for a comparison with competitors across the chosen concepts. To demonstrate the capabilities of our method, we provide a running example showcasing both quantitative and qualitative results of the analysis. Our approach demonstrates efficiency, with preprocessing completed in 1.967±0.758 s, and information extraction in 0.074±0.017 s on average, excluding human intervention time, and requiring low hardware resources.}
}
@article{STADNICKI2020753,
title = {Towards a Modern Ontology Development Environment},
journal = {Procedia Computer Science},
volume = {176},
pages = {753-762},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.070},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319657},
author = {Adrian Stadnicki and Filip {Filip Pietroń} and Patryk Burek},
keywords = {Knowledge-Based Systems, Knowledge Representation, Management, Ontology Engineering, Semantic Web},
abstract = {Ontologies provide engineers and developers with an unambiguous, verifiable, and expandable knowledge base related to a certain domain. Every project that requires control over consistent knowledge, which is especially relatable when using artificial intelligence with datasets increasing in size every second, would reap benefits from adding ontologies to the equation. It is a powerful asset enabling the development of a project with integrity between platforms or teams. Unfortunately, the cost of entry for a developer into the ontology engineering area is high, as it has been proven over the last decades that developing an ontology is a complex, collaborative task, which requires the support of an adequate methodology as well as software tools. The current paper’s objective is twofold. First, it provides a survey on the methodology and software tools used for the creation of the ontology, its maintenance and collaboration. The paper investigates how the tools evolved over the years and what trends have emerged. Second, as the result of the analysis conducted, we show that current solutions have deficiencies and a technological debt; therefore, we present our plan to build a modern tool that uses state-of-the-art technology.}
}
@article{AYADI2019100495,
title = {BNO—An ontology for understanding the transittability of complex biomolecular networks},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100495},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300022},
author = {Ali Ayadi and Cecilia Zanni-Merk and François de Bertrand {de Beuvron} and Julie Thompson and Saoussen Krichen},
keywords = {Systems biology, Complex biomolecular networks, Transittability, Ontology engineering, Qualitative reasoning, SWRL rules},
abstract = {Analysis of biological systems is being progressively facilitated by computational tools. Most of these tools are based on qualitative and numerical methods. However, they are not always evident, and there is an increasing need to provide an additional semantic layer. Semantic technologies, especially ontologies, are one of the tools frequently used for this purpose. Indeed, they are indispensable for understanding the semantic knowledge about the operation of cells at a molecular level. We describe here the biomolecular network ontology (BNO) created specially to address the needs of analysing the complex biomolecular network’s behaviour. A biomolecular network consists of nodes, denoting cellular entities, and edges, representing interactions among cellular components. The BNO ontology provides a foundation for qualitative simulation of complex biomolecular networks. We test the performance of the proposed BNO ontology by using a real example of a biomolecular network, the bacteriophage T4 gene 32. We illustrate the proposed BNO ontology for reasoning and inferring new knowledge with sets of rules expressed in SWRL. Results demonstrate that the BNO ontology allows to precisely interpret the corresponding semantic context and intelligently model biomolecular networks and their state changes. The Biomolecular Network Ontology (BNO) is freely available at https://github.com/AliAyadi/BNO-ontology-version-1.0.}
}
@article{ROSNER2025,
title = {An Ontology for Digital Medicine Outcomes: Development of the Digital Medicine Outcomes Value Set (DOVeS)},
journal = {JMIR Medical Informatics},
volume = {13},
year = {2025},
issn = {2291-9694},
doi = {https://doi.org/10.2196/67589},
url = {https://www.sciencedirect.com/science/article/pii/S2291969425000250},
author = {Benjamin Rosner and Matthew Horridge and Guillen Austria and Tiffany Lee and Andrew Auerbach},
keywords = {digital health, digital medicine, digital therapeutics, ontology, medical informatics, value set, ontology, development, digital health tool, DHT, health systems, digital medicine outcomes value set, prototype, users},
abstract = {Background
Over the last 10-15 years, US health care and the practice of medicine itself have been transformed by a proliferation of digital medicine and digital therapeutic products (collectively, digital health tools [DHTs]). While a number of DHT classifications have been proposed to help organize these tools for discovery, retrieval, and comparison by health care organizations seeking to potentially implement them, none have specifically addressed that organizations considering their implementation approach the DHT discovery process with one or more specific outcomes in mind. An outcomes-based DHT ontology could therefore be valuable not only for health systems seeking to evaluate tools that influence certain outcomes, but also for regulators and vendors seeking to ascertain potential substantial equivalence to predicate devices.
Objective
This study aimed to develop, with inputs from industry, health care providers, payers, regulatory bodies, and patients through the Accelerated Digital Clinical Ecosystem (ADviCE) consortium, an ontology specific to DHT outcomes, the Digital medicine Outcomes Value Set (DOVeS), and to make this ontology publicly available and free to use.
Methods
From a starting point of a 4-generation–deep hierarchical taxonomy developed by ADviCE, we developed DOVeS using the Web Ontology Language through the open-source ontology editor Protégé, and data from 185 vendors who had submitted structured product information to ADviCE. We used a custom, decentralized, collaborative ontology engineering methodology, and were guided by Open Biological and Biomedical Ontologies (OBO) Foundry principles. We incorporated the Mondo Disease Ontology (MONDO) and the Ontology of Adverse Events. After development, DOVeS was field-tested between December 2022 and May 2023 with 40 additional independent vendors previously unfamiliar with ADviCE or DOVeS. As a proof of concept, we subsequently developed a prototype DHT Application Finder leveraging DOVeS to enable a user to query for DHT products based on specific outcomes of interest.
Results
In its current state, DOVeS contains 42,320 and 9481 native axioms and distinct classes, respectively. These numbers are enhanced when taking into account the axioms and classes contributed by MONDO and the Ontology of Adverse Events.
Conclusions
DOVeS is publicly available on BioPortal and GitHub, and has a Creative Commons license CC-BY-SA that is intended to encourage stakeholders to modify, adapt, build upon, and distribute it. While no ontology is complete, DOVeS will benefit from a strong and engaged user base to help it grow and evolve in a way that best serves DHT stakeholders and the patients they serve.}
}
@article{PONCE2023105404,
title = {Unification of tsunami-related terminology: Ontology engineering perspective},
journal = {Computers & Geosciences},
volume = {178},
pages = {105404},
year = {2023},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105404},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001085},
author = {Daniela Ponce and Martina Husáková and Tomáš Nacházel and Vladimír Bureš and Pavel Čech and Peter Mikulecký and Kamila Štekerová and Petr Tučník and Marek Zanker and Karel Mls and Ioanna Triantafyllou and František Babič},
keywords = {Ontology, Metadata, Modelling, Community resilience, Physical vulnerability, Meteorological tsunami},
abstract = {Like many research areas, tsunami research has plenty of related topics that lack unified terminology. Some particular sub-topics may not have enough attention or do not share the same terminology as different views on the phenomenon. This issue can be tackled by an ontology that puts knowledge from different related topics into a formal structure that connects concepts with relationships. This paper proposes the development process of Tsunami-Related Ontology (TRO) that would aid the research in this field. The proposed semi-automatic ontology development methodology applies to any research field and does not require specific algorithms or programming skills to achieve its goal. This paper particularly focuses on three research gaps related to tsunami that are expected to benefit significantly from an ontology: meteorological tsunami, community resilience, and physical vulnerability. For these topics, the created ontology provides a formal taxonomy that links individual concepts to equivalent or related concepts, providing an easy-to-understand overview of the area.}
}
@article{HAMMAMI2019239,
title = {Towards Agile and Gamified Flipped Learning Design models: Application to the System and Data Integration Course},
journal = {Procedia Computer Science},
volume = {164},
pages = {239-244},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.178},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919322173},
author = {Jihed Hammami and Maha Khemaja},
keywords = {IMS LD, Flipped classroom, Flipped learning, Agile, Gamification, Authoring Tool, Domain Specific Language},
abstract = {Education and learning have no limits. At all ages, people are willing to be engaged in new experiences and get additional knowledge, skills and competencies. In this paper, we propose a new learning model that is learner-centered, gives values to learners’ preferences and competencies and encourages an engaging and motivating learning experience. Therefore, we discuss some of its pedagogical and technical aspects and requirements. We present the flipped classroom/learning, Agile methodology and Gamification as the basis of our proposed approach. We additionally, examine the possible support that the existing authoring tools could provide. We attempt to validate our proposal with a scenario intended to design the System and Data Integration course.}
}
@article{KUSUMA2022108906,
title = {Automatic question generation with various difficulty levels based on knowledge ontology using a query template},
journal = {Knowledge-Based Systems},
volume = {249},
pages = {108906},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108906},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122004336},
author = {Selvia Ferdiana Kusuma and Daniel Oranova Siahaan and Chastine Fatichah},
keywords = {Knowledge ontology, Ontology, Question generation, Query template, Question classification},
abstract = {Ontology is a concepts and relationships that can be used to support the question-generation process. However, until now, the ontology models and question templates commonly used to support the question-generation process have remained domain-specific, allowing three weaknesses to persist. First, the role of experts is dominant in the process of ontology generation. Second, the process needs adjustment if it is to be used for other domains. Third, question templates are formed based on the vocabulary of ontology, so they cannot be used to generate questions in other domains. In response to these problems, this research focused on forming an ontology generation model and a template model for generating questions that are not domain-specific. We used a combination of two types of ontology — namely, taxonomy ontology and sentence ontology to form ontology models and question templates that were not domain-specific. We labeled this combination as “knowledge ontology”. We used template queries to retrieve information on the ontology and then translated the results of the query template into questions in natural language. The ratios from our experiments demonstrated that the proposed method was effective for generating questions. Moreover, the method produced good question quality, as evidenced by its high accuracy rate of 90.71%. This research can be applied to help e-learning developers represent information in the form of ontology without involving experts. Furthermore, this research can also help teachers to generate questions automatically with consistent question quality.}
}
@article{TURCHET2025100871,
title = {The Musician’s Context Ontology: Modeling the context for smart musical applications},
journal = {Journal of Web Semantics},
volume = {87},
pages = {100871},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100871},
url = {https://www.sciencedirect.com/science/article/pii/S1570826825000125},
author = {Luca Turchet and Jacopo Tomelleri and Andrea Molinari and Paolo Bouquet},
keywords = {Semantic audio, Smart musical instruments, Internet of Musical Things, Context-aware computing, Music information retrieval},
abstract = {The paradigm of context-aware computing allows storing situational and environmental information in such a way that its interpretation can be done easily and more meaningfully. In turn, this understanding is used to anticipate users’ needs, and proactively provide them with situation-aware content and experiences. Whereas context-awareness has been investigated extensively in the computer science and IoT disciplines, it has been largely overlooked by the research community dealing with musical interfaces design. Existing musical instruments are not equipped with the ability to understand the context around them, namely who is the musician playing them, what musical activity is being conducted, as well as where and when. Enhancing musical instruments with context-awareness has the concrete potential to enable novel kinds of interactions between musicians and musical content in a large variety of situations, from playing alone to playing in a group, from music learning to music composition. To accomplish such a vision of intelligence embedded in musical instruments it is necessary to model the context around their users. In this paper, we present an ontology devised to represent the knowledge related to musicians and musical activities, the “Musician’s Context Ontology” (MUSICO) to facilitate the development of context-aware musical applications. There was no previous comprehensive data model for the domain of musicians’ context, nevertheless, the new ontology relates to several existing ontologies, including the Internet of Musical Things Ontology to represent Internet of Musical Things ecosystems and the Music Ontology that deals with the description of the music value-chain from production to consumption. This paper documents the design of the ontology and its evaluation with respect to specific requirements gathered from an extensive literature review and interviews with musicians. The utility of the ontology is demonstrated by a smartphone application that enables to search for musicians based on both textual and content-based musical queries. MUSICO can be accessed at: https://w3id.org/musico#.}
}
@article{LALIS202037,
title = {Ontology-based reliability analysis of aircraft engine lubrication system},
journal = {Transportation Research Procedia},
volume = {51},
pages = {37-45},
year = {2020},
note = {INAIR 2020 - CHALLENGES OF AVIATION DEVELOPMENT},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520308565},
author = {Andrej Lališ and Simona Bolčeková and Oldřich Štumbauer},
keywords = {aircraft engine, failure mode, effects analysis, ontology, ontology engineering, reliability analysis},
abstract = {This article focuses on identifying limitations and deficiencies of reliability methods that are currently used in the aviation industry. The goal is to propose a solution to address these issues and, consequently, improve the way reliability analyses are carried in the industry. In collaboration with an aircraft engine manufacturer, Failure Mode and Effects Analysis (FMEA) of an aircraft engine lubrication system was carried the traditional way and with current tools used by the company. Reliability ontology suitable to carry the analysis in semi-automatic way was then proposed, implemented, and used with the same FMEA analysis. The results show that the ontology-based approach has significant potential for improving the consistency and overall quality of the reliability analyses in the aviation. This article details the process of development of an FMEA ontology model, case study of its application and the comparison of the traditional and the ontology-based approach.}
}
@article{ALHARBI2024100659,
title = {An ontology-based agriculture decision-support system with an evidence-based explanation model},
journal = {Smart Agricultural Technology},
volume = {9},
pages = {100659},
year = {2024},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2024.100659},
url = {https://www.sciencedirect.com/science/article/pii/S2772375524002648},
author = {Amani Falah Alharbi and Muhammad Ahtisham Aslam and Khalid Ali Asiry and Naif Radi Aljohani and Yury Glikman},
keywords = {Ontology modeling, Decision support systems, Machine reasoning, Smart agriculture, Semantic-web},
abstract = {Effective management of plant diseases and pests requires knowledge that covers multiple domains. At the same time, retrieving the relevant information in a timely manner is always challenging, due to the unstructured nature of agricultural data. Over the years, efforts have been made to develop an ontology-based Decision-Support System (DSS) to facilitate the diagnosis and control of plant diseases. Some major issues with these systems are that: (1) they do not adopt the full extent of the ontological constructs to represent domain entities, which, in turn, reduces reasoning capabilities and prevents systems from being more intelligent, (2) they do not adequately provide the desired level of knowledge to support complex decisions, which requires many factors to be considered, (3) they do not adequately explain or provide evidence to demonstrate the validity of the system's outputs. To address these limitations, we present a novel system termed Agriculture Ontology Based Decision Support System (AgrODSS), which aims to assist in plant disease and pest identification and control. AgrODSS architecture consists of two semantic-based models. First, we developed Plant Diseases and Pests Ontology (PDP-O) to capture, model, and represent diseases and pest knowledge in a machine-understandable format. Second, we designed and developed an Evidence-Based Explanation Model (EBEM) that points to related evidence from the literature to demonstrate the validity of the system outputs. We demonstrate the effectiveness of AgrODSS by executing various queries via AgrODSS SPARQL Endpoint and obtaining valuable information to support decision-making. Finally, we evaluated AgrODSS practically with domain experts (including entomologists and pathologists) and it produced similar answers to those given by the experts, with an overall accuracy of 80.66%. These results demonstrate AgrODSS's ability to assist agricultural stakeholders in making proper disease or pest diagnoses and choosing the appropriate control methods.}
}
@article{POLENGHI2022100298,
title = {Knowledge reuse for ontology modelling in Maintenance and Industrial Asset Management},
journal = {Journal of Industrial Information Integration},
volume = {27},
pages = {100298},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100298},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000947},
author = {Adalberto Polenghi and Irene Roda and Marco Macchi and Alessandro Pozzetti and Hervé Panetto},
keywords = {Ontology, Knowledge reuse, Interoperability, Maintenance, Asset management},
abstract = {Maintenance and Industrial Asset Management (AM) are fundamental business processes in guaranteeing the availability of physical assets at minimum risk and cost, while balancing the interests of several stakeholders. To reach operational excellence, intra- and inter-enterprise interoperability of systems is needed to support information management and integration between several involved parties. To this end, ontology engineering is relevant since it supports interoperability at technical and semantic levels. However, ontology modelling methodologies are varied, and several best practices exist, amongst which knowledge reuse. Nevertheless, reusing extant knowledge is not completely exploited so far, causing a heterogeneous ensemble of ontologies that are not orchestrated. The present work aims at promoting the adoption of knowledge reuse for ontology modelling in maintenance and AM. Therefore, an extensive review of existing ontologies for the two targeted business processes is performed with a twofold objective: firstly, to realise a cross-industrial ontological compendium, and secondly to understand the state of art of ontology modelling in maintenance and AM. To support the adoption of knowledge reuse, this practice is framed in AMODO (Asset Management Ontology Development methOdology). Finally, a laboratory-sized showcase is provided to prove the usefulness of relying on knowledge reuse during the ontology development. The results show that the developed ontology is realised faster and is inherently aligned with established ontologies, towards enterprise systems interoperability. Consequently, maintenance and AM business processes may rely on information management and integration to pursue operational excellence.}
}
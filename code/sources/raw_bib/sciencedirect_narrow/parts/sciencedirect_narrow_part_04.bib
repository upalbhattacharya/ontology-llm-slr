@article{MA2022375,
title = {Accelerating deep neural network filter pruning with mask-aware convolutional computations on modern CPUs},
journal = {Neurocomputing},
volume = {505},
pages = {375-387},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222008669},
author = {Xiu Ma and Guangli Li and Lei Liu and Huaxiao Liu and Xueying Wang},
keywords = {Deep learning systems, Neural network compression, Filter pruning},
abstract = {Filter pruning, a representative model compression technique, has been widely used to compress and accelerate sophisticated deep neural networks on resource-constrained platforms. Nevertheless, most studies focus on reducing the cost of model inference, whereas the heavy burden of the pruning optimization process is neglected. In this paper, we propose MaskACC, a mask-aware convolutional computation method, which accelerates the prevailing mask-based filter pruning process on modern CPU platforms. MaskACC dynamically reorganizes the tensors used in convolutions with the mask information to avoid unnecessary computations, thereby improving the computational efficiency of the pruning process. Evaluation with state-of-the-art neural network models on CPU cloud platforms demonstrates the effectiveness of our method, which achieves up to 1.61× speedup under commonly-used pruning rates, compared to conventional computations.}
}
@article{JARVENPAA2019261,
title = {Implementation of capability matchmaking software facilitating faster production system design and reconfiguration planning},
journal = {Journal of Manufacturing Systems},
volume = {53},
pages = {261-270},
year = {2019},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2019.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0278612519300883},
author = {Eeva Järvenpää and Niko Siltala and Otto Hylli and Minna Lanz},
keywords = {Production system design, Production system reconfiguration, Resource representation, Capability modelling, Capability matchmaking, Matchmaking software, Matchmaking web service},
abstract = {Smart manufacturing calls for rapidly responding production systems which help the manufacturing companies to operate efficiently in a highly dynamic environment. Currently, the system design and reconfiguration planning are manual processes which rely heavily on the designers’ expertise and tacit knowledge to find feasible system configuration solutions by comparing the characteristics of the product to the technical properties of the available resources. Rapid responsiveness requires new computer-aided intelligent design and planning solutions that would reduce the time and effort put into system design, both in brownfield and greenfield scenarios. This article describes the implementation of a capability matchmaking approach and software which automatizes the matchmaking between product requirements and resource capabilities. The interaction of the matchmaking system with external design and planning tools, through its web service interface, is explained and illustrated with a case example. The proposed matchmaking approach supports production system design and reconfiguration planning by providing automatic means for checking if the existing system already fulfils the new product requirements, and/or for finding alternative resources and resource combinations for specific product requirements from large search spaces, e.g. from global resource catalogues.}
}
@article{KANG2020101412,
title = {Understanding and improving ontology reasoning efficiency through learning and ranking},
journal = {Information Systems},
volume = {87},
pages = {101412},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917306476},
author = {Yong-Bin Kang and Shonali Krishnaswamy and Wudhichart Sawangphol and Lianli Gao and Yuan-Fang Li},
keywords = {OWL, Reasoning, Performance prediction, Ontology, Metrics, Learning, Meta-reasoning, Semantic web},
abstract = {Ontologies are the fundamental building blocks of the Semantic Web and Linked Data. Reasoning is critical to ensure the logical consistency of ontologies, and to compute inferred knowledge from an ontology. It has been shown both theoretically and empirically that, despite decades of intensive work on optimising ontology reasoning algorithms, performing core reasoning tasks on large and expressive ontologies is time-consuming and resource-intensive. In this paper, we present the meta-reasoning framework R2O2* to tackle the important problems of understanding the source of TBox reasoning hardness and predicting and optimising TBox reasoning efficiency by exploiting machine learning techniques. R2O2* combines state-of-the-art OWL 2 DL reasoners as well as an efficient OWL 2 EL reasoner as components, and predicts the most efficient one by using an ensemble of robust learning algorithms including XGBoost and Random Forests. A comprehensive evaluation on a large and carefully curated ontology corpus shows that R2O2* outperforms all six component reasoners as well as AutoFolio, a robust and strong algorithm selection system.}
}
@article{NAZ2020106695,
title = {Ontology-driven advanced drug-drug interaction},
journal = {Computers & Electrical Engineering},
volume = {86},
pages = {106695},
year = {2020},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106695},
url = {https://www.sciencedirect.com/science/article/pii/S0045790620305504},
author = {Tabbasum Naz and Muhammad Akhtar and Syed Khuram Shahzad and Maria Fasli and Muhammad Waseem Iqbal and Muhammad Raza Naqvi},
keywords = {Drug-drug interaction, Pharmacy semantics, Drug ontologies, Pharmaceutical informatics},
abstract = {The rapid growth of data in the pharmaceutical area has created new challenges for large-scale data mining like Drug-Drug Interaction (DDI) analysis. To meet these challenges, various types of data related to DDI must be integrated with true semantics. However, the existing tools do not provide automated DDI analysis. Interaction details are not machine readable and pharmacists need to do further processing for its extraction. This research paper proposed an ontology-driven Advanced Drug-Drug Interaction (ADDI) system to assists the physicians and pharmacists to identify the DDI effects. ADDI provides ontological definitions and semantic relations among diseases, drugs, ingredients, action mechanism, physiologic effect, dosage formation, administration methods, DDI mechanism, DDI types (Antagonism, Synergism, Potentiation, and Interaction with metabolism), DDI reactions, their frequency and duration. It can be used as Semantic Information Layer (SIL) to resolve the heterogeneity problem and can play a significant role to remove the barriers for semantic interoperability.}
}
@article{MATOS2022599,
title = {Use of Ontologies in Product Information Management: A Proposal for a Multinational Engineering and Technology Company},
journal = {Procedia Computer Science},
volume = {204},
pages = {599-609},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.073},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922008110},
author = {Alexandra Isabel Matos and Fernando Paulo Belfo},
keywords = {Industry 4.0, Industry 5.0, Product Information Management, PIM, ERP, CMS, e-commerce, ontology, action research},
abstract = {The PIM system importance has increased due to technical sophistication of products, their need of internal management or external publishment. ERP and CCMS systems should be integrated with a PIM system that acts as the “backbone” of product information. This project proposed an ontology-centric solution to manage product information for complex modular systems available in the catalog of one of the largest multinational organizations in engineering and technology sector. Based on action-research methodology, existing taxonomies of online product catalog at ERP and CCMS systems were analyzed, an ontology of updated taxonomies was created using Protégé tool and validated by experts. This solution was anchored in some Industry 4.0 and 5.0 pillars, by formalizing smart manufacturing knowledge in an interoperable way between multiple systems and allowing a tailored customer experience, based on interactive products and hyper customization. The organization can now be more efficient in managing individual or integrated products as complex modular systems and in communicating with customers.}
}
@article{WALOSZEK2020723,
title = {Contextual ontology for tonality assessment},
journal = {Procedia Computer Science},
volume = {176},
pages = {723-732},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.045},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319402},
author = {Wojciech Waloszek and Nina Rizun},
keywords = {knowledge bases, contexts, sentiment classification, topic modelling, hierarchical sentiment dictionary},
abstract = {In the paper we discuss the possibilities of using hierarchical contextual ontologies for supporting sentiment classification tasks. The discussion focuses on two important research hypotheses: (1) whether it is possible to construct such an ontology from a corpus of textual document, and (2) whether it is possible and beneficial to use inferencing from this ontology to support the process of sentiment classification. To support the first hypothesis we present a method of extraction of hierarchy of contexts from a set of textual documents and encoding this hierarchy into a multi-level contextual ontology. To support the second hypothesis, we present a method of reasoning from the ontology, and results of experimental verification, which show that use of this reasoning method can increase the accuracy of sentiment classification for longer text documents.}
}
@article{SANCHEZ2020879,
title = {Semantic-based privacy settings negotiation and management},
journal = {Future Generation Computer Systems},
volume = {111},
pages = {879-898},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.10.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18317035},
author = {Odnan Ref Sanchez and Ilaria Torre and Bart P. Knijnenburg},
keywords = {Privacy Preference Manager, Privacy ontology, Data sharing and permission management, IoT},
abstract = {By 2020, an individual is expected to own an average of 6.58 devices that share and integrate a wealth of personal user data. The management of privacy preferences across these devices is a complex task for which users are ill-equipped, which increases privacy risks. In this paper we propose an approach that exploits Semantic Web (SW) technology to manage the user’s IoT privacy preferences and negotiate the permissions for data sharing with third parties. SW technology comprises a web of data that can be processed by machines through a formal, universally shared representation. In our approach, SW enables a lightweight and interoperable communication between a Personal Data Manager (PDM) and the Third Parties (TPs) that request access to the user’s personal data. The PDM can handle multiple heterogeneous personal IoT devices and manages the negotiation process between the user and the TPs in a way that can relieve users from the burden of specifying their privacy requirement for each TP. The core of the approach is the definition of the Privacy Preference for IoT (PPIoT) Ontology which is based on the Privacy Preference Ontology, the W3C Semantic Sensor Network Ontology, the Fair Information Practices (FIP) principles, and state-of-the-art recommendation techniques for privacy protection in the IoT. This ontology aims to capture the complexity of privacy management in the IoT paradigm in light of the recent General Data Protection Regulation (GDPR) of the European Union. Along with presenting the ontology, in this paper we will provide an example on how to use the PPIoT ontology for the management of privacy preferences in the fitness IoT domain and we will show how the PDM handles the process of negotiation between the user and the TPs. The approach is based on an interactive PPIoT-based Privacy Preference Model (PPM) that meets the requirements of the GDPR to have transparent and simple TP privacy policies. Finally, we will report the results of an evaluation on a mockup fitness app that implements this PPM. The main contributions of this paper are: (i) to propose an ontology for privacy preference in the IoT context, which covers a knowledge gap in existing literature and can be used for IoT privacy management, (ii) to propose an interactive PPIoT-based Privacy Preference Model, which is in accordance with the GDPR objectives.}
}
@article{RICO2019100500,
title = {Evaluating the impact of semantic technologies on bibliographic systems: A user-centred and comparative approach},
journal = {Journal of Web Semantics},
volume = {59},
pages = {100500},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300174},
author = {Mariano Rico and Daniel Vila-Suero and Iuliana Botezan and Asunción Gómez-Pérez},
keywords = {User-centred evaluation, Usability, Cultural heritage, Digital humanities, Linked data, Ontologies},
abstract = {Semantic and linked-data technologies are currently used by several cultural heritage institutions to make their content available through the Web. Although these technologies are heavily oriented towards data reuse and integration, one clear benefit highlighted by recent literature is the enhancement of human cultural consumption and user experience through the development of novel cultural end-user applications like Online Public Access Catalogues (OPACs). However, to the best of our knowledge, studies into the impact of these technologies on end-user applications are scarce. In order to address this lack, we report the results of two within-group user-centred studies of two online bibliographic systems in a realistic setting — using a widely deployed OPAC and its counterpart linked-data based system, datos.bne.es. The results of our first within-group study show that users of the system based on linked data required significantly less time and visited fewer pages to complete a typical search and retrieval activity. Additionally, the results of our user satisfaction tests also provided significantly better results for this new system. These results are consistent with the hypothesis that semantic technologies applied to library catalogues provide an enhancement that helps satisfy users’ information needs.}
}
@article{SORMAZ2019183,
title = {SIMPM – Upper-level ontology for manufacturing process plan network generation},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {55},
pages = {183-198},
year = {2019},
note = {Extended Papers Selected from FAIM2016},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0736584517302119},
author = {Dušan Šormaz and Arkopaul Sarkar},
abstract = {Distributed computer integrated manufacturing is increasingly adopting cloud computing, software-as-a-service (SaaS) and multi-agent systems as steps towards “design anywhere, build anywhere” strategy. In this scenario, ontologies not only serve as common message exchange structure among distributed agents but also provide reasoning capability to extract implicit knowledge from explicit information already stored in the knowledge base. Foundation ontologies (upper-level), comprised of most general concepts of a domain, provide a common semantic structure to the domain-level ontologies, which capture details of multi-disciplinary manufacturing knowledge. In this paper, novel upper-level ontology, called SIMPM (Semantically Integrated Manufacturing Planning Model), is proposed in order to model three fundamental constraints of manufacturing process planning: variety, time, and aggregation. The philosophical underpinning of the proposed ontology – presented as OWL-DL axioms – is derived from a three dimensional planning model, developed during our past research on computer-aided process planning. As part of the evaluation of SIMPM ontology, we first expound on the interoperability issues with other upper-level manufacturing ontologies. Next, we present a case study on process planning for prismatic part design. In this way, we demonstrate how the generic set of proposed axioms may be used to address various manufacturing process planning concerns, such as alternative manufacturing resources, the temporal order among operations and granularity in the details of a process plan.}
}
@article{KUSTER2020102731,
title = {The UDSA ontology: An ontology to support real time urban sustainability assessment},
journal = {Advances in Engineering Software},
volume = {140},
pages = {102731},
year = {2020},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2019.102731},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818311773},
author = {Corentin Kuster and Jean-Laurent Hippolyte and Yacine Rezgui},
abstract = {Urban sustainability assessment frameworks have emerged during the past decade to address holistically the complexity of the urban landscape through a systems approach, factoring in environmental, social and economic requirements. However, the current assessment schemes are (a) static in nature, and as such don't reflect the dynamic and real-time nature of urban artefacts, (b) are not grounded in semantics (e.g. BIM and GIS), and (c) are at best used to assist in regulatory compliance, for instance in energy design, to meet increasingly stringent regulatory requirements. Information and communication technologies provide a new value proposition capitalizing on the Internet of Things (IoT) and semantics to provide real-time insights and inform decision making. Consequently, there is a real need in the field for data models that could facilitate data exchange and handle data heterogeneity. In this study, a semantic data model is considered to support near real-time urban sustainability assessment and enhance the semantics of sensor network data. Based on an extensive review of urban sustainability assessment frameworks and ontology development methodologies, the Urban District Sustainability Assessment (UDSA) ontology has been developed and validated using real data from the site of “The Works”, a newly refurbished neighbourhood in Ebbw Vale, Wales. This novel approach reconciles several domain-specific ontologies within one high-level ontology that can support the creation of real-time urban sustainability assessment software. In addition, this information model is aligned with 29 authoritative urban sustainability assessment frameworks, thus providing a useful resource not only in urban sustainability assessment, but also in the wider smart cities context.}
}
@article{SYKORA2022997,
title = {The power of emotions: Leveraging user generated content for customer experience management},
journal = {Journal of Business Research},
volume = {144},
pages = {997-1006},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.02.048},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322001679},
author = {Martin Sykora and Suzanne Elayan and Ian R. Hodgkinson and Thomas W. Jackson and Andrew West},
keywords = {Customer experience management, Sentiment analysis, Social media, Emotion analytics, Bot automation},
abstract = {Customer experience management (CEM) in the social media age finds itself needing to adapt to a rapidly changing digital environment and hence there is a need for innovative digital data analytical solutions. Drawing on an action case study of a large global automotive manufacturer, this study presents a digital innovation for enhanced emotion analytics on user generated content (UGC) and behaviour (UGB), to improve consumer insights for CEM. The digital innovation captures customer experience in real time, enabling measurement of a wide range of discrete emotions on the studied social media platform, which goes beyond traditional tools that capture positive or negative sentiment only. During the digital intervention, a substantial number of inauthentic and bot like behaviours was revealed, unbeknown to the case organisation. These accounts were found to be posting and amplifying highly emotional and potentially damaging content surrounding the case brand and its products. The study illustrates how emotion in the context of customer experience should go beyond typical categorisations, given the complexity of human emotion, while a distinction between bot and authentic users is imperative for CEM.}
}
@article{GHEDINI20236370,
title = {An ontology to integrate process-based approach in ZDM strategies in a Digital Twin framework},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {6370-6375},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.825},
url = {https://www.sciencedirect.com/science/article/pii/S240589632301203X},
author = {Lorenzo Ghedini and Adalberto Polenghi and Marco Macchi},
keywords = {Zero Defect Manufacturing, Ontology, Digital Twin, Quality Monitoring, Process-oriented approach, Product-oriented approach},
abstract = {The current context of Industry 4.0 is characterized by challenges that were not present in the past like greater variability, higher customization and greater complexity. To address these challenges and the increasing need from companies to focus on sustainability-related issues is necessary to adopt a quality improvement method. In this paper, the method considered is Zero Defect Manufacturing (ZDM) a “tool” which shows considerable potential, but needs some auxiliary technologies to operate. In this regard, the model proposed is an ontology based on a pre-existent ontology. This new ontology is capable of applying Detect and Repair strategies to go with the creation of a Digital Twin of the product. The realized ontology can be used to support decision-making in an industrial context: in fact, it provides to any operator in the production process, an indication about the quality of the product, also advising some corrective actions if needed, like the repair or the disassembly of the product and the subsequent recycling of the good quality components. To obtain this outcome, an analysis of the literature was performed to determine the gaps present in the literature, then an ontology editor allowed the creation of the ontology and, finally, the ontology was validated in the context of Industry 4.0 Laboratory at the Politecnico di Milano. In this environment, the proposed solution was populated with the data coming from the servers, determining the quality of the product as a function of the state of product components and the condition of one of the assets installed in the production line.}
}
@article{KESHAVARZI2023107520,
title = {An ontology-driven framework for knowledge representation of digital extortion attacks},
journal = {Computers in Human Behavior},
volume = {139},
pages = {107520},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2022.107520},
url = {https://www.sciencedirect.com/science/article/pii/S0747563222003405},
author = {Masoudeh Keshavarzi and Hamid Reza Ghaffary},
keywords = {Ransomware, Cyber-ontology, Conceptual modeling, Knowledge base, Knowledge graph, Philosophy of computer science},
abstract = {With the COVID-19 pandemic and the growing influence of the Internet in critical sectors of industry and society, cyberattacks have not only not declined, but have risen sharply. In the meantime, ransomware is at the forefront of the most devastating threats that have launched the lucrative illegal business. Due to the proliferation and variety of ransomware forays, there is a need for a new theory of categories. The intricacy and multiplicity of components involved in digital extortions entails the construction of a knowledge representation system that is able to organize large volumes of information from heterogeneous sources in a formal structured format and infer new knowledge from it. This paper suggests and develops a dedicated ontology of digital blackmails, called Rantology, with a particular focus on ransomware assaults. The logic coded in this ontology allows to assess the maliciousness of programs based on various factors, including called API functions and their behaviors. The proposed framework can be used to facilitate interoperability between cybersecurity experts and knowledge-based systems, and identify sensitive points for surveillance. The evaluation results based on several criteria confirm the adequacy of the suggested ontology in terms of clarity, modularity, consistency, coverage and inheritance richness.}
}
@article{REMY2019929,
title = {Building an integrated enhanced virtual research environment metadata catalogue},
journal = {The Electronic Library},
volume = {37},
number = {6},
pages = {929-951},
year = {2019},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-09-2018-0183},
url = {https://www.sciencedirect.com/science/article/pii/S0264047319000274},
author = {Laurent Remy and Dragan Ivanović and Maria Theodoridou and Athina Kritsotaki and Paul Martin and Daniele Bailo and Manuela Sbarra and Zhiming Zhao and Keith Jeffery},
keywords = {Online catalogs, CERIF, Virtual research environments, Vocabulary homogenization, X3ML tools, Metadata catalog},
abstract = {Purpose
The purpose of this paper is to boost multidisciplinary research by the building of an integrated catalogue or research assets metadata. Such an integrated catalogue should enable researchers to solve problems or analyse phenomena that require a view across several scientific domains.
Design/methodology/approach
There are two main approaches for integrating metadata catalogues provided by different e-science research infrastructures (e-RIs): centralised and distributed. The authors decided to implement a central metadata catalogue that describes, provides access to and records actions on the assets of a number of e-RIs participating in the system. The authors chose the CERIF data model for description of assets available via the integrated catalogue. Analysis of popular metadata formats used in e-RIs has been conducted, and mappings between popular formats and the CERIF data model have been defined using an XML-based tool for description and automatic execution of mappings.
Findings
An integrated catalogue of research assets metadata has been created. Metadata from e-RIs supporting Dublin Core, ISO 19139, DCAT-AP, EPOS-DCAT-AP, OIL-E and CKAN formats can be integrated into the catalogue. Metadata are stored in CERIF RDF in the integrated catalogue. A web portal for searching this catalogue has been implemented.
Research limitations/implications
Only five formats are supported at this moment. However, description of mappings between other source formats and the target CERIF format can be defined in the future using the 3M tool, an XML-based tool for describing X3ML mappings that can then be automatically executed on XML metadata records. The approach and best practices described in this paper can thus be applied in future mappings between other metadata formats.
Practical implications
The integrated catalogue is a part of the eVRE prototype, which is a result of the VRE4EIC H2020 project.
Social implications
The integrated catalogue should boost the performance of multi-disciplinary research; thus it has the potential to enhance the practice of data science and so contribute to an increasingly knowledge-based society.
Originality/value
A novel approach for creation of the integrated catalogue has been defined and implemented. The approach includes definition of mappings between various formats. Defined mappings are effective and shareable.}
}
@article{SAMOURKASIDIS2020105171,
title = {A semantic approach for timeseries data fusion},
journal = {Computers and Electronics in Agriculture},
volume = {169},
pages = {105171},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.105171},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919318514},
author = {Argyrios Samourkasidis and Ioannis N. Athanasiadis},
keywords = {Environmental timeseries, Internet of Things, Legacy data, Semantic heterogeneity, Templates, FAIR data, Reasoning, Interoperability, Data reuse, APSIM, AgMIP, DSSAT, WOFOST},
abstract = {The data deluge following the rise of Internet of Things contributes towards the creation of non-reusable data silos. Especially in the environmental sciences domain, syntactic and semantic heterogeneity hinders data re-usability as most times manual labour and domain expertise is required. Both the different syntaxes under which environmental timeseries are formatted and the implicit semantics which are used to describe them contribute to this end. Usually, the real meaning of data is obscured in a combination of short data labels, titles and various value codes, that require domain or institutional knowledge to decipher. The FAIR data principles for scientific data sharing are stewardship offer a framework based on community-adopted metadata. In this work, we present the Environmental Data Acquisition Module (EDAM) which focuses on data interoperability and reuse, and deals with syntactic and semantic heterogeneity using a template approach. Data curators draft templates to describe in an abstract fashion the syntax of the timeseries datasets they want to acquire or disseminate. They complement each template with a metadata file, which is used to annotate observables and their properties (including physical quantities and units of measurement) with terms from an ontology. EDAM employs a reasoner to infer compatibility among syntactically and semantically heterogeneous datasets, and enables timeseries, format and units of measurement transformation on-the-fly. Our approach utilizes a local ontology to store metadata about datasets, which enables EDAM to acquire and transform datasets which were originally stored with different semantics and syntaxes. We demonstrate EDAM in a case study where we transform meteorological input files of four agricultural models. Our approach, allows to cut across environmental data silos and facilitate timeseries reusability, as it enables users to (a) discover datasets in other formats, (b) transform them and (c) reuse them in their scientific workflows. This directly contributes to the toolshed for FAIR data management in environmental sciences. EDAM implementation has been released under an open-source license.}
}
@article{POLENGHI202155,
title = {Multi-attribute Ontology-based Criticality Analysis of manufacturing assets for maintenance strategies planning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {55-60},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.192},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321009630},
author = {A. Polenghi and I. Roda and M. Macchi and A. Pozzetti},
keywords = {criticality analysis, maintenance, maintenance strategy, ontology, OWL, SWRL},
abstract = {Planning maintenance strategies in advance with respect to the installation and running of manufacturing assets positively affects operational expenditure during their usage. However, the early stages of the asset lifecycle are poor of operational data. Thus, domain knowledge of experts, related to the asset, the process and production requirements, is the primary source to determine which maintenance strategy better fits in a specific context. Hence, ontology-based systems represent a relevant help in this direction. In this work, given the importance of the criticality analysis (CA) for maintenance planning, the CA is analyzed from an ontological perspective to automatically associate a maintenance strategy to the asset under analysis. Moreover, to unveil the power of CA, its multi-attribute nature is considered, including not only availability as guiding criterion, but also quality and energy. The developed ontology-based CA allows to (i) semantically align all involved experts, and (ii) potentiate the analysis through reasoning capabilities. Finally, preliminary results from an industrial case in a food company are shown.}
}
@article{JELIC20256233,
title = {Integrated cloud platform for energy management of self-sustainable island communities},
journal = {Energy Reports},
volume = {13},
pages = {6233-6250},
year = {2025},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2025.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S235248472500280X},
author = {Marko Jelić and Dayanne {Peretti Corrêa} and Dea Jelić and Lazar Berbakov and Daniel Werner and Md Nasimul Islam Maruf and Ignacio Lázaro and Izaskun Fernández and Marcus Keane and Nikola Tomašević},
keywords = {Geographical islands, Sustainable energy communities, Energy management platform, Energy forecasting, Energy optimization, Smart asset control},
abstract = {An integral part of contemporary initiatives striving to promote and create self-sustainable communities has been the integration and efficient management of renewable energy sources, energy storage solutions, and sustainable heating and cooling assets. This paper presents a unique combination of diverse technologies, ranging from established data management solutions like SQL and NoSQL databases to custom semantic solutions and device-specific control adapters powered by the OpenMUC gateway. Additionally, it incorporates analytical machine learning-based forecasting solutions paired with optimization algorithms, working together to enable energy self-sufficiency. The platform constituted by these solutions is subsequently utilized to provide predictive and real-time control of energy assets in various facilities in line with the selected operation strategy. Its application, particularly in terms of effective energy storage utilization and timing of asset activation scheduling, ultimately results in improvements in renewable energy integration and overall increase of energy efficiency in the considered buildings. As demonstrated in real-world use cases tested within prosumer-based energy communities in geographical islands, the application of the platform resulted in tangible modifications to the primary statistical characteristics of electrical energy consumption clearly signified, among others, by a reduction in mean consumption of between 40W and 190W. Through scrutinizing the achieved results, it can be concluded that the platform displays the capabilities to outperform stock control algorithms provided by the inverter vendors. Expanding upon the application limited to the electric assets, use cases in the thermal domain where optimization outputs are utilized for heat pump scheduling were also discussed and presented.}
}
@article{MONTICOLO2020100124,
title = {OCEAN: A multi agent system dedicated to knowledge management},
journal = {Journal of Industrial Information Integration},
volume = {17},
pages = {100124},
year = {2020},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2019.100124},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X19300809},
author = {Davy Monticolo and Inaya Lahoud and Pedro Chavez Barrios},
keywords = {Agent-based modeling, Ontology, Semantic approach, Knowledge management},
abstract = {Emphasis on knowledge and information is one of the challenges of the 21st century to differentiate the intelligent business enterprises. Enterprises have to develop their organization in order to capture, manage, and use information in a context of continually changing technology. Indeed knowledge and information are completely distributed in the information network of the company. In addition, knowledge is by nature, heterogeneous since it is provided from different information sources like the software, the technical report, the meeting statements, etc. We present in this paper the architecture of a multi-agent system, which allows the capitalization of the distributed and heterogeneous knowledge. We then present how the agents help business experts to design ontologies in detailing this problematic and how the agents extract knowledge from different users’ databases by using a semantic approach.}
}
@article{NIU2024106381,
title = {Critical review on data-driven approaches for learning from accidents: Comparative analysis and future research},
journal = {Safety Science},
volume = {171},
pages = {106381},
year = {2024},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2023.106381},
url = {https://www.sciencedirect.com/science/article/pii/S0925753523003235},
author = {Yi Niu and Yunxiao Fan and Xing Ju},
keywords = {Workplace safety, Accident prevention, Machine learning, Data source, Causality},
abstract = {Data-driven intelligent technologies are promoting a disruptive digital transformation of human society. Industrial accident prevention is also amid this change. Although many emerging technologies, such as machine learning (ML), are extensively employed in workplace safety, these approaches need to fit the intended safety purpose of accident analysis, risk assessment, adverse outcome prediction, or anomaly detection. Hence, examining the “real-world” need for accident prevention and the advantages of emerging data-driven methodologies to better integrate them is necessary. This study provides a systematic review to clarify the current research status, existing problems, and future insights into these evolving technologies in accident prevention. We present notable gaps and barriers in data-driven accident prevention by analyzing 194 published studies from four perspectives: Paradigm, Model, Data Source, and Purpose. The results demonstrate (1) lack of a systematic framework to guide the application of Big Data (BD) in the field of safety; (2) few prior studies have considered model interpretability; (3) more proactive data needs to be incorporated into accident analysis; (4) safety-related data and domain knowledge need to be further integrated; (5) some recent data-driven techniques are unexplored in safety science. Further, the future research opportunities are discussed based on these findings. Such review may help clarify the mapping of data-driven tasks to safety goals to accelerate the uptake of data-driven technologies in safety or accident analysis research.}
}
@article{BENAVIDES2018107,
title = {An ontology-based approach to knowledge representation for Computer-Aided Control System Design},
journal = {Data & Knowledge Engineering},
volume = {118},
pages = {107-125},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17305189},
author = {Carmen Benavides and Isaías García and Héctor Alaiz and Luis Quesada},
keywords = {Conceptual modeling, Data and knowledge visualization, Ontologies, Computer-Aided Control System Design},
abstract = {Different approaches have been used in order to represent and build control engineering concepts for the computer. Software applications for these fields are becoming more and more demanding each day, and new representation schemas are continuously being developed. This paper describes a study of the use of knowledge models represented in ontologies for building Computer Aided Control Systems Design (CACSD) tools. The use of this approach allows the construction of formal conceptual structures that can be stated independently of any software application and be used in many different ones. In order to show the advantages of this approach, an ontology and an application have been built for the domain of design of lead/lag controllers with the root locus method, presenting the results and benefits found.}
}
@article{SERRANORUIZ2022150,
title = {Toward smart manufacturing scheduling from an ontological approach of job-shop uncertainty sources},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {150-155},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.185},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322001860},
author = {Julio C. Serrano-Ruiz and Josefa Mula and Raúl Poler},
keywords = {Industry 4.0, smart manufacturing scheduling, job-shop, scheduling, stochastic, disturbance, disruption, zero-defect manufacturing, digital twin, ontological framework},
abstract = {An integral application of the enabling technologies of Industry 4.0 in the job-shop scheduling problem (JSSP) must contemplate the automation and autonomy of the involved decision-making processes as a goal, which is the main purpose of the smart manufacturing scheduling (SMS) paradigm. In a real production context, uncertainty acts as a barrier that hinders this goal being met and, therefore, any SMS model should integrate uncertainty generators in one way or another. This paper proposes an ontological framework that identifies and structures the entities shaping the joint domain formed by the job-shop scheduling process in its itinerary toward the SMS paradigm, the sources of uncertainty that it faces, and the interrelationship type that link these entities. This ontological framework will serve in future research as a conceptual basis to design new quantitative models that, from a holistic perspective, will address the stochasticity of manufacturing environments and incorporate the management of disturbances into the realtime resolution of automatic and autonomous job-shop scheduling.}
}
@article{MARTINEZGONZALEZ201979,
title = {The support of constructs in thesaurus tools from a Semantic Web perspective: Framework to assess standard conformance},
journal = {Computer Standards & Interfaces},
volume = {65},
pages = {79-91},
year = {2019},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0920548918302447},
author = {M. Mercedes Martínez-González and María-Luisa Alvite-Díez},
keywords = {ISO 2788, ISO 25964, Semantic Web, SKOS, Software conformance, Standard conformance, Thesaurus constructs, Thesaurus standards, Thesaurus tools},
abstract = {Thesauri are conceptual tools useful to achieve semantic interoperability and reusability, which are relevant goals in the Semantic Web. Thesaurus standards establish, among other issues, the constructs that can appear in a thesaurus. The ISO 25964 standard for thesauri, which supersedes ISO 2788, is the evolution of the ISO thesauri standard to a conceptual approach closer to the Semantic Web. However, it appeared when SKOS -the W3C Recommendation- was already consolidated as the standard for KOS (Knowledge Organization System) representation in the Semantic Web, including thesauri. The evolution from ISO 2788 to ISO 25964, and the relationships between constructs in ISO 2788/ISO 25964 and SKOS, are studied in this paper. From the analysis of this comparison, a methodological framework, that focuses on the construct support, is proposed to evaluate the conformance quality of thesaurus management tools. Target readers are professionals in charge of thesauri edition. A Semantic Web perspective is taken to characterize the effect that using SKOS to represent thesauri can have on the results of the assessment. A proof of concept for the model's feasibility was performed on two tools and the analysis of the results of this experimental validation is presented. The conclusions highlight the model's suitability for assessing conformance to the standards concerning support for thesaurus constructs.}
}
@article{IM20211578,
title = {Development of an Ontological Cost Estimating Knowledge Framework for EPC Projects},
journal = {KSCE Journal of Civil Engineering},
volume = {25},
number = {5},
pages = {1578-1591},
year = {2021},
issn = {1226-7988},
doi = {https://doi.org/10.1007/s12205-021-1582-8},
url = {https://www.sciencedirect.com/science/article/pii/S1226798824017094},
author = {Haekyung Im and Minhui Ha and Donghee Kim and Jaehyun Choi},
keywords = {Interoperability, Ontology, Estimation, Cost knowledge structure, Standardization, Cost classification},
abstract = {This research standardizes a knowledge structure for estimation in the construction field by creating a method to enhance cost management efficiency of construction projects while meeting the need for reusability of accumulated construction information. The construction knowledge structure was developed to execute the project cost estimation with an ontological concept. The knowledge framework was defined and relevant examples were addressed to explain the structure. The detailed estimation process and methodology for using standard unit price information was also developed to strengthen cost information interoperability by utilizing standard classification systems, such as MasterFormat, UniFormat, UniClass, and ISO 12006. This concept may be proposed as a method of connecting construction information based on a standard cost classification system in order to improve estimation efficiency by increasing the connectivity of cost information. Ontology can be a powerful tool when used in conjunction with interoperability to manage the massive volume of construction information for cost management. This methodology can expand to integrated management in cost and time to derive additional classes for work breakdown structure (WBS). Thus, ontology may improve the efficiency of cost estimation and systematization by reusing construction information. As a further step, the interoperability method will enhance overall construction project management.}
}
@article{SANFILIPPO2019174,
title = {Editorial: Formal Ontologies meet Industry},
journal = {Procedia Manufacturing},
volume = {28},
pages = {174-176},
year = {2019},
note = {7th International conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.12.028},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918313702},
author = {Emilio Sanfilippo and Walter Terkaj}
}
@article{FARGHALY2024105224,
title = {cSite ontology for production control of construction sites},
journal = {Automation in Construction},
volume = {158},
pages = {105224},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105224},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523004843},
author = {Karim Farghaly and Ranjith Soman and Jennifer Whyte},
keywords = {Interoperability, Ontology, Semantic web, Linked data, Production Control, Construction Project Management},
abstract = {In the realm of construction production control, effective communication across operational levels and the rapid influx of diverse data are essential. Yet, integrating this data faces challenges due to disparate systems and a lack of common terminology, resulting in data silos and hindered interoperability. An ontology-based solution emerges as promising for enhancing interoperability. This research paper introduces the development, implementation, and assessment of the cSite ontology, encompasses several crucial facets necessary for efficient production control such as location, activities, and documents. To evaluate its practicality, a real-case study was conducted, wherein the ontology was employed to answer competency questions through SPARQL queries. Furthermore, interactive dashboards, situated within the construction control rooms, were developed to present the information visually. This paper underscores the transformative potential of integrated and visualised production information in construction projects. Additionally, it illuminates how the cSite ontology can facilitate the development and implementation of construction digital twins.}
}
@article{GUTIERREZ2019381,
title = {Developing an ontology schema for enriching and linking digital media assets},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {381-397},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.06.023},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18323859},
author = {Yoan Gutiérrez and David Tomás and Isabel Moreno},
keywords = {Semantic representation, Ontology, Digital media asset, Entertainment industry},
abstract = {The abundance of digital media information coming from different sources, completely redefines approaches to media content production management and distribution for all contexts (i.e. technical, business and operational). Such content includes descriptive information (i.e. metadata) about an asset (e.g. a movie, song or game), as well as playable media (e.g. audio or video files). Metadata is organised following a variety of inconsistent structures and formats that are supplied by various content providers. Some challenges have been addressed in terms of standardising and enriching media assets metadata from a semantic perspective. Well known examples include Europeana and DBpedia. Nevertheless, due to the ongoing variability and evolution of digital contents, constant support and creation of new semantic representations are necessary. This article presents an ontology schema covering the requirements of users (content providers and content consumers) involved in the overall life cycle of a digital media asset, which has been designed and developed for a real scenario. The construction of this schema has been documented and evaluated following a methodology supported by quantitative and qualitative metrics. As part of the tangible results, the following outcomes were produced: (i) an RDF/XML schema available via Zenodo and GitHub; (ii) competence questions used for validation are published at GitHub; (iii) an exemplary ontology repository; and (iv) CRUD (Create, Read, Update and Delete) technologies for managing semantic repositories based on such schema. These results form an active part of the framework of a European project and other ongoing research initiatives.}
}
@article{WANG2021108823,
title = {Research on intelligent design method of ship multi-deck compartment layout based on improved taboo search genetic algorithm},
journal = {Ocean Engineering},
volume = {225},
pages = {108823},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.108823},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821002584},
author = {Yun-long Wang and Zhang-pan Wu and Guan Guan and Kai Li and Shu-hong Chai},
keywords = {Ship compartment layout, Multi-deck, Intelligent design, Optimization model, Taboo search algorithm, Genetic algorithm},
abstract = {This paper presents an improved taboo search genetic algorithm (ITSGA) for intelligent design of ship multi-deck compartment layout (SMCL). The optimization of ship multi-deck residential compartment layout belongs to the combinatorial optimization problem with various performance constraints which needs to consider the layout of function cabins, deck passages and stairways between decks and so on. In this paper, the optimization model for SMCL is established, which include the layout area model, the relative location model, the absolute location model and the ergonomic model. ITSGA is proposed to improve the local search ability of genetic algorithm (GA) by introducing the neighborhood transformation criterion and Taboo criterion of Taboo search algorithm into GA. Then a new coding method is given according to the characteristics of ship cabins layout problem to avoid the damage to the cabin sequence caused by crossover and mutation operations in GA. During the layout process, the energy method is firstly used to determine the deck layer of various cabins to be arranged, and then the position of deck passages, cabins, and stairways between upper and lower decks are carried out by nested ITSGA. Finally, the results of numerical simulation experiments demonstrate the feasibility and effectiveness of the established method.}
}
@article{SHAHINMOGHADDAM2018620,
title = {CA-FCM: Towards a formal representation of expert’s causal judgements over construction project changes},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {620-638},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S1474034618301642},
author = {Mehrzad Shahinmoghaddam and Ahad Nazari and Mostafa Zandieh},
keywords = {Fuzzy Cognitive Mapping (FCM), Contextual knowledge modelling, Knowledge-based decision support system, Semantic web technologies, Construction project changes, Causal knowledge representation},
abstract = {Aimed at improving the proactive benefits of Fuzzy Cognitive Mapping (FCM) for predicting construction project changes, this paper presents CA-FCM: a Context-aware Fuzzy Cognitive Mapping approach. CA-FCM’s main functionality is to imitate the intuitive causal judgements of project experts over change causation in different contextual settings. Invoking the logical inference capabilities of semantic web tools, a hybrid inference mechanism is embedded within the proposed framework which enables establishing contextual connections between prospective causal factors through a semi-automated process of generating relevant causal statements. Hence, CA-FCM can assist decision-makers with (1) a shared sense-making of the domain concepts which would significantly facilitate the manual construction of FCM scenarios, (2) providing contextualized recommendations of causal information required for developing FCM scenarios, (3) dynamic modelling of causal inferences, imitating expert reasoning on change causation and propagation. Towards providing a detailed delineation of CA-FCM’s effectiveness on providing assistance in planning for project changes, a partial implementation of the proposed framework was conducted within a real case scenario.}
}
@article{PUTNIK2022678,
title = {Engineering is Design and only Design - Part I: The value of making a distinctive sign},
journal = {Procedia CIRP},
volume = {109},
pages = {678-683},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.313},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122007636},
author = {Goran D. Putnik and Zlata Putnik and Pedro Pinheiro and Cátia Alves},
keywords = {Design, engineering, sign, semiotics},
abstract = {The paper addresses the question: what is engineering? We intuitively know engineering applications such as manufacturing, production, industry, management, business. The answer is not consensual because it is not easy. Furthermore, the ontological question brings us to a second question. What distinguishes engineering from other areas? It is the creative ability that distinguishes engineering. And this artificial faculty only exists in Design. Epistemology in science promotes the existence of herds, increasingly specialized groups of knowledge production. Nevertheless, engineers assume themselves as makers, and in the growing diversity promoted by specialization, they will certainly give different answers when asked about their work. We aggregate all of them as sign-makers. Therefore, engineering is Design and only Design. We reject other views. The argument presented on the phenomenological level considers them false. This paper demonstrates that it is mandatory to create a distinctive sign, which places engineering as relevant in organizations. Without the sign described in semiotics, engineering, which could pretend to be everything, becomes trivial.}
}
@article{WESTPHAL2022108233,
title = {Spatial concept learning and inference on geospatial polygon data},
journal = {Knowledge-Based Systems},
volume = {241},
pages = {108233},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108233},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122000673},
author = {Patrick Westphal and Tobias Grubenmann and Diego Collarana and Simon Bin and Lorenz Bühmann and Jens Lehmann},
keywords = {Spatial analytics, Concept learning, Description logics, Spatial knowledge graphs},
abstract = {Geospatial knowledge has always been an essential driver for many societal aspects. This concerns in particular urban planning and urban growth management. To gain insights from geospatial data and guide decisions usually authoritative and open data sources are used, combined with user or citizen sensing data. However, we see a great potential for improving geospatial analytics by combining geospatial data with the rich terminological knowledge, e.g., provided by the Linked Open Data Cloud. Having semantically explicit, integrated geospatial and terminological knowledge, expressed by means of established vocabularies and ontologies, cross-domain spatial analytics can be performed. One analytics technique working on terminological knowledge is inductive concept learning, an approach that learns classifiers expressed as logical concept descriptions. In this paper, we extend inductive concept learning to infer and make use of the spatial context of entities in spatio-terminological data. We propose a formalism for extracting and making spatial relations explicit such that they can be exploited to learn spatial concept descriptions, enabling ‘spatially aware’ concept learning. We further provide an implementation of this formalism and demonstrate its capabilities in different evaluation scenarios.}
}
@article{NOZZA2021102537,
title = {LearningToAdapt with word embeddings: Domain adaptation of Named Entity Recognition systems},
journal = {Information Processing & Management},
volume = {58},
number = {3},
pages = {102537},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102537},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321000455},
author = {Debora Nozza and Pikakshi Manchanda and Elisabetta Fersini and Matteo Palmonari and Enza Messina},
keywords = {Named Entity Recognition, Domain adaptation, Word embeddings},
abstract = {The task of Named Entity Recognition (NER) is aimed at identifying named entities in a given text and classifying them into pre-defined domain entity types such as persons, organizations, locations. Most of the existing NER systems make use of generic entity type classification schemas, however, the comparison and integration of (more or less) different entity types among different NER systems is a complex problem even for human experts. In this paper, we propose a supervised approach called L2AWE (Learning To Adapt with Word Embeddings) which aims at adapting a NER system trained on a source classification schema to a given target one. In particular, we validate the hypothesis that the embedding representation of named entities can improve the semantic meaning of the feature space used to perform the adaptation from a source to a target domain. The results obtained on benchmark datasets of informal text show that L2AWE not only outperforms several state of the art models, but it is also able to tackle errors and uncertainties given by NER systems.}
}
@article{HOLSAPPLE201832,
title = {Business social media analytics: Characterization and conceptual framework},
journal = {Decision Support Systems},
volume = {110},
pages = {32-45},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618300502},
author = {Clyde W. Holsapple and Shih-Hui Hsiao and Ram Pakath},
keywords = {Analytics, Business social media analytics, Conceptual framework, Social media, Social media analytics},
abstract = {A substantial portion of internet usage today involves social media applications. Aside from personal use, given the vast amount of content stored, and rapid diffusion of information, in social media, businesses have begun exploiting social media for competitive advantage. Its popularity has led to the recognition of Social Media Analytics (SMA) as a distinct, albeit formative, sub-field within the Analytics field. Against this backdrop, we examine available characterizations of SMA that collectively identify various considerations of interest. However, their diversity suggests the need for adopting a concise, unifying SMA definition. We present a definition that subsumes salient aspects of existing characterizations and incorporates novel features of interest to Business SMA. Further, we examine available conceptual frameworks for Business SMA and advance a framework that comprehensively models the Business SMA phenomenon. We also conduct a survey of recently published SMA research in the premier, academic Management Information Systems journals and use some of the surveyed papers to validate our framework.}
}
@article{LIM2024,
title = {An Ontology to Bridge the Clinical Management of Patients and Public Health Responses for Strengthening Infectious Disease Surveillance: Design Science Study},
journal = {JMIR Formative Research},
volume = {8},
year = {2024},
issn = {2561-326X},
doi = {https://doi.org/10.2196/53711},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X24005158},
author = {Sachiko Lim and Paul Johannesson},
keywords = {infectious disease, ontology, IoT, infectious disease surveillance, patient monitoring, infectious disease management, risk analysis, early warning, data integration, semantic interoperability, public health},
abstract = {Background
Novel surveillance approaches using digital technologies, including the Internet of Things (IoT), have evolved, enhancing traditional infectious disease surveillance systems by enabling real-time detection of outbreaks and reaching a wider population. However, disparate, heterogenous infectious disease surveillance systems often operate in silos due to a lack of interoperability. As a life-changing clinical use case, the COVID-19 pandemic has manifested that a lack of interoperability can severely inhibit public health responses to emerging infectious diseases. Interoperability is thus critical for building a robust ecosystem of infectious disease surveillance and enhancing preparedness for future outbreaks. The primary enabler for semantic interoperability is ontology.
Objective
This study aims to design the IoT-based management of infectious disease ontology (IoT-MIDO) to enhance data sharing and integration of data collected from IoT-driven patient health monitoring, clinical management of individual patients, and disparate heterogeneous infectious disease surveillance.
Methods
The ontology modeling approach was chosen for its semantic richness in knowledge representation, flexibility, ease of extensibility, and capability for knowledge inference and reasoning. The IoT-MIDO was developed using the basic formal ontology (BFO) as the top-level ontology. We reused the classes from existing BFO-based ontologies as much as possible to maximize the interoperability with other BFO-based ontologies and databases that rely on them. We formulated the competency questions as requirements for the ontology to achieve the intended goals.
Results
We designed an ontology to integrate data from heterogeneous sources, including IoT-driven patient monitoring, clinical management of individual patients, and infectious disease surveillance systems. This integration aims to facilitate the collaboration between clinical care and public health domains. We also demonstrate five use cases using the simplified ontological models to show the potential applications of IoT-MIDO: (1) IoT-driven patient monitoring, risk assessment, early warning, and risk management; (2) clinical management of patients with infectious diseases; (3) epidemic risk analysis for timely response at the public health level; (4) infectious disease surveillance; and (5) transforming patient information into surveillance information.
Conclusions
The development of the IoT-MIDO was driven by competency questions. Being able to answer all the formulated competency questions, we successfully demonstrated that our ontology has the potential to facilitate data sharing and integration for orchestrating IoT-driven patient health monitoring in the context of an infectious disease epidemic, clinical patient management, infectious disease surveillance, and epidemic risk analysis. The novelty and uniqueness of the ontology lie in building a bridge to link IoT-based individual patient monitoring and early warning based on patient risk assessment to infectious disease epidemic surveillance at the public health level. The ontology can also serve as a starting point to enable potential decision support systems, providing actionable insights to support public health organizations and practitioners in making informed decisions in a timely manner.}
}
@article{LIU2022116741,
title = {Applying ontology learning and multi-objective ant colony optimization method for focused crawling to meteorological disasters domain knowledge},
journal = {Expert Systems with Applications},
volume = {198},
pages = {116741},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116741},
url = {https://www.sciencedirect.com/science/article/pii/S095741742200210X},
author = {Jingfa Liu and Yi Dong and Zhaoxia Liu and Duanbing Chen},
keywords = {Focused crawler, Multi-objective ant colony optimization, Ontology, Ontology learning},
abstract = {The focused crawler based on semantic analysis is a research hotspot in the field of information retrieval. The domain ontology is generally applied to construct the topic model of the focused crawler. In order to overcome the limitations of builders' knowledge reserve and subjective consciousness in the process of constructing artificially ontology, a semi-automatic construction method of domain ontology based on ontology learning technology combining the latent Dirichlet allocation and the Apriori algorithm is proposed in this article. When evaluating the relevance between a hyperlink and a specific topic, the joint evaluation method considering both the web text and the link structure is usually used. However, the traditional weighted sum method is difficult to reasonably determine the optimal weights of these evaluating indicators. To solve this problem, a multi-objective optimization model for link evaluation and a subsequent multi-objective ant colony optimization algorithm (MOACO) are proposed. In the MOACO, a method of the nearest farthest candidate solution (NFCS) is combined with the fast non-dominated sorting to select a set of Pareto-optimal hyperlinks and guide the crawlers’ search directions. The experimental results of the focused crawling on the domain knowledge of typhoon disasters and rainstorm disasters prove that the ability of the proposed focused crawlers to retrieve topic-relevant webpages.}
}
@article{SERRANO2019122,
title = {Deep neural network architectures for social services diagnosis in smart cities},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {122-131},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.034},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19301918},
author = {Emilio Serrano and Javier Bajo},
keywords = {Social exclusion, Social services, Deep learning, Data analysis, Machine learning, Data mining},
abstract = {Social services intend to aid disadvantaged, distressed, or vulnerable persons or groups. Machine Learning (ML) and Deep Learning (DL), which are important technologies to leverage Internet of Things and Big Data, have not been considered to support intelligent social services in Smart Cities. Using technology to achieve more responsive, efficient, and proactive social services is a must in Smart Cities because it will lead to a more fair and egalitarian society. This research work contributes with the evaluation of a thousand Neural Networks architectures for the automatic diagnosis of chronic social exclusion. Some of them outperform previous models in quality metrics such as accuracy and F-score. Beyond the improvement in predicting this specific social condition, to the best of the authors’ knowledge, this paper open the research line of applying these methods for the general social services diagnosis in Smart Cities. Finally, the advantages of using the DL paradigm over other ML alternatives in this scope are discussed.}
}
@article{ILIADIS20191021,
title = {The Tower of Babel problem: making data make sense with Basic Formal Ontology},
journal = {Online Information Review},
volume = {43},
number = {6},
pages = {1021-1045},
year = {2019},
issn = {1468-4527},
doi = {https://doi.org/10.1108/OIR-07-2018-0210},
url = {https://www.sciencedirect.com/science/article/pii/S1468452719000325},
author = {Andrew Iliadis},
keywords = {Data ethics, Applied computational ontology, Semantic technology, Social ontology, Tower of Babel problem},
abstract = {Purpose
Applied computational ontologies (ACOs) are increasingly used in data science domains to produce semantic enhancement and interoperability among divergent data. The purpose of this paper is to propose and implement a methodology for researching the sociotechnical dimensions of data-driven ontology work, and to show how applied ontologies are communicatively constituted with ethical implications.
Design/methodology/approach
The underlying idea is to use a data assemblage approach for studying ACOs and the methods they use to add semantic complexity to digital data. The author uses a mixed methods approach, providing an analysis of the widely used Basic Formal Ontology (BFO) through digital methods and visualizations, and presents historical research alongside unstructured interview data with leading experts in BFO development.
Findings
The author found that ACOs are products of communal deliberation and decision making across institutions. While ACOs are beneficial for facilitating semantic data interoperability, ACOs may produce unintended effects when semantically enhancing data about social entities and relations. ACOs can have potentially negative consequences for data subjects. Further critical work is needed for understanding how ACOs are applied in contexts like the semantic web, digital platforms, and topic domains. ACOs do not merely reflect social reality through data but are active actors in the social shaping of data.
Originality/value
The paper presents a new approach for studying ACOs, the social impact of ACO work, and describes methods that may be used to produce further applied ontology studies.}
}
@article{HURTADO2024111230,
title = {e-Science workflow: A semantic approach for airborne pollen prediction},
journal = {Knowledge-Based Systems},
volume = {284},
pages = {111230},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.111230},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123009802},
author = {Sandro Hurtado and María Luisa Antequera-Gómez and Cristóbal Barba-González and Antonio Picornell and Ismael Navas-Delgado},
keywords = {Big data analytics, Semantics, e-Science, Pollen prediction},
abstract = {Allergic rhinitis has become a global health problem in recent decades because airborne pollen is a primary trigger of this respiratory disorder. Moreover, pollinosis can exacerbate the symptoms of asthma and favour respiratory infections. Seasonal pollen trends and climatic circumstances (such as temperature, precipitation, relative humidity, wind speed and direction, and other variables) can impact daily airborne pollen concentrations, influencing local pollen emission and dispersion. Because of that, pollen monitoring and prediction are becoming more relevant to the urban population and scientific interest is put into them. Due to such tasks’ high volume of data, scientists are starting to use computational tools like workflows to automate and speed up the process. Furthermore, using the expert scientific domain is critical for improving the analysis, allowing, among others, a better workflow configuration and data provenance. As semantic web technologies have been revealed as an essential means for knowledge representation, we implemented this workflow information as an ontology using formats like RDF(S) and OWL. Consequently, this paper provides a semantic-enhanced e-Science workflow based on the TITAN framework for pollen forecasting analysis using meteorological data. Furthermore, a catalogue of components is developed on the TITAN framework, which allows the creation of different workflow versions. Two case studies of pollen prediction were developed to test the implementation of the aforementioned methodologies. Both were elaborated with airborne pollen data obtained in the city of Málaga (Spain). Still, one was elaborated for Platanus pollen type (narrow annual main pollination period), while the other was done for Amaranthaceae pollen type (extensive annual main pollination period). The predictions have been conducted using machine and deep learning algorithms like SARIMA or CNN-LSTM that intend to optimise the pollen prediction procedure depending on its stational and seasonal profile.}
}
@article{SOBRAL2020113260,
title = {An Ontology-based approach to Knowledge-assisted Integration and Visualization of Urban Mobility Data},
journal = {Expert Systems with Applications},
volume = {150},
pages = {113260},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113260},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420300853},
author = {Thiago Sobral and Teresa Galvão and José Borges},
keywords = {Data integration, Data visualization, Urban mobility, Semantic web, ontology},
abstract = {This paper proposes an ontology-based framework to support integration and visualization of data from Intelligent Transportation Systems. These activities may be technically demanding for transportation stakeholders, due to technical and human factors, and may hinder the use of visualization tools in practice. The existing ontologies do not provide the necessary semantics for integration of spatio-temporal data from such systems. Moreover, a formal representation of the components of visualization techniques and expert knowledge can leverage the development of visualization tools that facilitate data analysis. The proposed Visualization-oriented Urban Mobility Ontology (VUMO) provides a semantic foundation to knowledge-assisted visualization tools (KVTs). VUMO contains three facets that interrelate the characteristics of spatio-temporal mobility data, visualization techniques and expert knowledge. A built-in rule set leverages semantic technologies standards to infer which visualization techniques are compatible with analytical tasks, and to discover implicit relationships within integrated data. The annotation of expert knowledge encodes qualitative and quantitative feedback from domain experts that can be exploited by recommendation methods to automate part of the visualization workflow. Data from the city of Porto, Portugal were used to demonstrate practical applications of the ontology for each facet. As a foundational domain ontology, VUMO can be extended to meet the distinctiveness of a KVT.}
}
@article{PRAYITNO20241070,
title = {Optimizing the Sustainability of Collaborative Logistics in Urban Area through Ontologies and Causal Artificial Intelligence: A Conceptual Framework},
journal = {Procedia CIRP},
volume = {130},
pages = {1070-1076},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.208},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124013659},
author = {Kutut Aji Prayitno and Hendro Wicaksono},
keywords = {Collaborative sustainable urban logistics, Causal AI, Ontology, Reinforcement learning},
abstract = {Efficiently managing logistics operations is crucial in elevating sustainability and tackling the challenges urbanization brings in today’s urban environment. Collaborations among the public and private sectors in urban logistics are essential to minimize environmental impacts. This study aims to create a novel conceptual framework for collaborative logistics designed explicitly for sustainable metropolitan areas. The framework aims to enable collaborative data-driven sustainability optimization in urban logistics. It comprises ontologies to facilitate interoperability among stakeholders by providing a shared understanding of the exchanged data. The framework utilizes causal artificial intelligence to enable traceability and transparency of data-driven decisions compared to conventional machine learning working based on correlations. Furthermore, the framework also employs causal reinforcement learning that enables agents to learn what actions lead to targeted outcomes and why those actions are effective. The developed framework optimizes vehicle routes and conveyance selection while considering several operational constraints such as time windows, split-load scenarios, and commodity-specific requirements. Moreover, the system integrates the distinctive features of public transport networks. The suggested strategy minimizes fuel use and overall delivery costs, promoting a more sustainable logistics environment in metropolitan areas measured using Environmental, Social, and Governance (ESG) indicators. This study contributes to the theoretical understanding of collaborative logistics. It underscores the importance of environmental stewardship and societal well-being in logistics planning and implementation by utilizing a data-driven approach.}
}
@article{DIMITROVA2020103450,
title = {An ontological approach for pathology assessment and diagnosis of tunnels},
journal = {Engineering Applications of Artificial Intelligence},
volume = {90},
pages = {103450},
year = {2020},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.103450},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619303446},
author = {Vania Dimitrova and Muhammad Owais Mehmood and Dhavalkumar Thakker and Bastien Sage-Vallier and Joaquin Valdes and Anthony G. Cohn},
keywords = {Tunnel diagnosis, Ontology, Intelligent decision support systems, Linear transport structures},
abstract = {Tunnel maintenance requires complex decision making, which involves pathology diagnosis and risk assessment, to ensure full safety while optimising maintenance and repair costs. A Decision Support System (DSS) can play a key role in this process by supporting the decision makers in identifying pathologies based on disorders present in various tunnel portions and contextual factors affecting a tunnel. Another key aspect is to identify which spatial stretches within a tunnel contain pathologies of similar kinds within neighbouring tunnel segments. This paper presents PADTUN, a novel intelligent decision support system that assists with pathology diagnosis and assessment of tunnels with respect to their disorders and diagnosis influencing factors. It utilises semantic web technologies for knowledge capture, representation, and reasoning. The core of PADTUN is a family of ontologies which represent the main concepts and relations associated with pathology assessment, and capture the decision process concerning tunnel maintenance. Tunnel inspection data is linked to these ontologies to take advantage of inference capabilities offered by semantic technologies. In addition, an intelligent mechanism is presented which exploits abstraction and inference capabilities. Thus PADTUN provides the world’s first semantically based intelligent DSS for tunnel maintenance. PADTUN was developed by an interdisciplinary team of tunnel experts and knowledge engineers in real-world settings offered by the NeTTUN EU Project. An evaluation of the PADTUN system is performed using real-world tunnel data and diagnosis tasks. We show how the use of semantic technologies allows addressing the complex issues of tunnel pathology inferencing, aiding in, and matching transportation experts’ expectations of decision support. The methodology is applicable to any linear transport structures, offering intelligent ways to aid with complex decision processes related to diagnosis and maintenance.}
}
@article{GUPTA2021101260,
title = {Feature-based ontological framework for semantic interoperability in product development},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101260},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101260},
url = {https://www.sciencedirect.com/science/article/pii/S147403462100015X},
author = {Ravi Kumar Gupta and Balan Gurumoorthy},
keywords = {Product information exchange, Semantic interoperability, Shape feature taxonomy, Feature semantics, Product informatics, Computer-aided design, Product lifecycle management, Product development},
abstract = {An essential requirement in integrating tasks in product development is to have a seamless exchange of product information through the entire product lifecycle. A key challenge in the integration is the exchange of shape semantics in terms of understandable labels and representations. A unified taxonomy is proposed to represent, classify, and extract shape features. This taxonomy is built using the Domain-Independent Form Feature (DIFF) model as the representation of features. All the shape features in a product model are classified under three main classes, namely, volumetric features, deformation features and free-form surface features. Shape feature ontology is developed using the unified taxonomy, which brings the shape features under a single reasoning framework. One-to-many reasoning framework is presented for mapping semantically equivalent information (label and representation) of the feature to be exchanged to target applications, and the reconstruction of the shape model automatically in that target application. An algorithm has been developed to extract the semantics of shape features and construct the model in the target application. The algorithm developed has been tested for shape models taken from literature and test cases are selected based on variations of topology and geometry. Results of exchanging product information are presented and discussed. Finally, the limitations of the proposed method for exchanging product information are explained.}
}
@article{PRADEEP202133,
title = {Leveraging context-awareness for Internet of Things ecosystem: Representation, organization, and management of context},
journal = {Computer Communications},
volume = {177},
pages = {33-50},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002280},
author = {Preeja Pradeep and Shivsubramani Krishnamoorthy and Rahul Krishnan Pathinarupothi and Athanasios V. Vasilakos},
keywords = {Context-aware computing, Context model, Context ontology, Internet of Things, Situational context},
abstract = {Present-day devices are becoming increasingly smarter than their predecessors. From a simple passive light switch to an intelligent wristwatch, great strides have been made in networking smart devices, creating an autonomous ecosystem, the so-called Internet of Things. In an increasingly information-driven world, context-awareness supports the intended applications as well as their constituent devices, making them conscious of and adaptive to the specific scenario in real-time. Moreover, heterogeneous devices in the Internet of Things ecosystem peruse disparate data formats and semantics, giving rise to interoperability and information sharing challenges. Context modeling is a core feature that facilitates interoperability and information sharing between applications. Although generic context models exist, they do not consider pertinent dimensions of context to provide a generic vocabulary, and therefore, they cannot be extended to generalize situations commonly encountered in the Internet of Things environment. An extensible, generic modeling and representation of context is required to manage pertinent context dimensions in various ecosystems by being dynamically aware of the situation. This paper presents Context Model for Internet of Things, an extensible and generic ontology-based context modeling approach that provides relevant information at the right time. This work encompasses Context Ontology for Internet of Things, an ontology-based context organization approach, which provides an abstract and overarching vocabulary that fosters knowledge reusability and sharing. The proposed model has been implemented and evaluated with a use case to validate its adaptability, effectiveness, and viability. Our evaluation based on generality, effectiveness, and consistency shows that the proposed model can effectively represent, organize, and manage the context in different Internet of Things ecosystems.}
}
@article{SINGH2019177,
title = {Generation of fashionable clothes using generative adversarial networks},
journal = {International Journal of Clothing Science and Technology},
volume = {32},
number = {2},
pages = {177-187},
year = {2019},
issn = {0955-6222},
doi = {https://doi.org/10.1108/IJCST-12-2018-0148},
url = {https://www.sciencedirect.com/science/article/pii/S0955622219000237},
author = {Montek Singh and Utkarsh Bajpai and Vijayarajan V. and Surya Prasath},
keywords = {Neural network, Cross-domain relations, Fashion clothes, Generative adversarial networks},
abstract = {Purpose
There are various style options available when one buys clothes on online shopping websites, however the availability the new fashion trends or choices require further user interaction in generating fashionable clothes. The paper aims to discuss this issue.
Design/methodology/approach
Based on generative adversarial networks (GANs) from the deep learning paradigm, here the authors suggest model system that will take the latest fashion trends and the clothes bought by users as input and generate new clothes. The new set of clothes will be based on trending fashion but at the same time will have attributes of clothes where were bought by the consumer earlier.
Findings
In the proposed machine learning based approach, the clothes generated by the system will personalized for different types of consumers. This will help the manufacturing companies to come up with the designs, which will directly target the customer.
Research limitations/implications
The biggest limitation of the collected data set is that the clothes in the two domains do not belong to a specific category. For instance the vintage clothes data set has coats, dresses, skirts, etc. These different types of clothes are not segregated. Also there is no restriction on the number of images of each type of cloth. There can many images of dresses and only a few for the coats. This can affect the end results. The aim of the paper was to find whether new and desirable clothes can be created from two different domains or not. Analyzing the impact of “the number of images for each class of cloth” is something which is aim to work in future.
Practical implications
The authors believe such personalized experience can increase the sales of fashion stores and here provide the feasibility of such a clothes generation system.
Originality/value
Applying GANs from the deep learning models for generating fashionable clothes.}
}
@article{HUARANGAJUNCO2024134,
title = {From cloud and fog computing to federated-fog computing: A comparative analysis of computational resources in real-time IoT applications based on semantic interoperability},
journal = {Future Generation Computer Systems},
volume = {159},
pages = {134-150},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24002103},
author = {Edgar Huaranga-Junco and Salvador González-Gerpe and Manuel Castillo-Cara and Andrea Cimmino and Raúl García-Castro},
keywords = {Fog computing, Federated-fog computing, IoT architectures, Semantic interoperability, Performance evaluation, Data federation},
abstract = {In contemporary computing paradigms, the evolution from cloud computing to fog computing and the recent emergence of federated-fog computing have introduced new challenges pertaining to semantic interoperability, particularly in the context of real-time applications. Fog computing, by shifting computational processes closer to the network edge at the local area network level, aims to mitigate latency and enhance efficiency by minimising data transfers to the cloud. Building upon this, federated-fog computing extends the paradigm by distributing computing resources across diverse organisations and locations, while maintaining centralised management and control. This research article addresses the inherent problematics in achieving semantic interoperability within the evolving architectures of cloud computing, fog computing, and federated-fog computing. Experimental investigations are conducted on a diverse node-based testbed, simulating various end-user devices, to emphasise the critical role of semantic interoperability in facilitating seamless data exchange and integration. Furthermore, the efficacy of federated-fog computing is rigorously evaluated in comparison to traditional fog and cloud computing frameworks. Specifically, the assessment focuses on critical factors such as latency time and computational resource utilisation while processing real-time data streams generated by Internet of Things (IoT) devices. The findings of this study underscore the advantages of federated-fog computing over conventional cloud and fog computing paradigms, particularly in the realm of real-time IoT applications demanding high performance (lowering CPU usage to 20%) and low latency (with picks up to 300ms). The research contributes valuable insights into the optimisation of processing architectures for contemporary computing paradigms, offering implications for the advancement of semantic interoperability in the context of emerging federated-fog computing for IoT applications.}
}
@article{KARDINATA2019826,
title = {Integration of Crowdsourcing into Ontology Relation Extraction},
journal = {Procedia Computer Science},
volume = {161},
pages = {826-833},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919319003},
author = {Eunike Andriani Kardinata and Nur Aini Rakhmawati},
keywords = {crowdsourcing, integration, online incremental, ontology learning, relation extraction},
abstract = {Ontology learning is a continuous process that is always being researched and developed. A learning method for one domain may not be applicable to another because of the different characteristics of the data involved. Researchers have been developing various methodologies to build the highest quality of ontology efficiently. As identified in the previous works, one problem which could not be solved my machine alone is the extra-logical errors. These errors can only be identified by human judges and are usually related to the domain of the ontology. In this research, we aim to catalogue available methods, specifically for relation extraction, and the online incremental algorithms which will allow integration of crowdsourcing into ontology learning—to handle said challenge. We also briefly discussed an existing ontology editor called OntoCop, which may be used as a reference for further research. Henceforth, we propose a framework based on our review to improve the current relation extraction method.}
}
@article{WALOSZEK2020733,
title = {Improving the Performance of Ontological Querying by using a Contextual Approach},
journal = {Procedia Computer Science},
volume = {176},
pages = {733-742},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.062},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319578},
author = {Wojciech Waloszek and Aleksander Waloszek},
keywords = {knowledge bases, contexts, Description Logics, reasoning},
abstract = {In the paper we present the results of experiment we performed to determine whether a contextual approach may be used to increase the performance of querying a knowledge base. For the experiments we have used a unique setting where we put much effort in developing a contextual and a non-contextual ontology which are as much close counterparts as possible. To achieve this we created a contextual version of a non-contextual ontology and reformulated the set of competency questions to reflect the contextual structure of the newly created knowledge base. The results of the experiment strongly suggest that using contexts might be advantageous for improving performance, and also show the further ways of development of the approach.}
}
@article{SELVI2025103553,
title = {Efficient data handling in smart healthcare using Quotient Hash Trees and gaussian hilbert regression},
journal = {Ain Shams Engineering Journal},
volume = {16},
number = {9},
pages = {103553},
year = {2025},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2025.103553},
url = {https://www.sciencedirect.com/science/article/pii/S2090447925002941},
author = {T. Kalai Selvi and S. Sasirekha},
keywords = {Heterogeneous Medical Data, Gaussian Replicating, Kernel Hilbert, Quotient Hash Tree, Data Management Server, Internet of Things},
abstract = {Significant amounts of heterogeneous medical data are developed daily from several medical devices, and sensor data analysis across IoT domains is becoming challenging. The conservative data warehouses possess the potential to integrate data and support interactive data exploration. However, transferring data with minimum loss and storing it in a unified format is arduous and time-consuming. The proposed Gaussian Replicating Hilbert Regression and Quotient Hash Tree (GRHR-QHT) method is introduced to convert heterogeneous data into a unified format with improved accuracy and reduced time. The proposed GRHR-QHT performs three processes: data collection, transfer, and storage. At first, linear acceleration and angular velocity-based vector data collection are transmitted to the data management server. The collected data are stored in the input matrix. The data management server constructs multiple data into a unified format without any loss using GRH-based sequencing. This data management server reduces variance among aspect spaces through minimum distance with empirical organizations of samples. Also, the Polynomial Regression function is used to determine the relationship between the independent parameters. Then, unified data gets stored in the data management server using a Quotient Hash Tree (QHT) for easy access and less space complexity. The experimental assessment of the GRHR-QHT technique and existing methods is compared with different metrics: accuracy, time, error rate, space complexity, throughput, packet delivery ratio, service availability, reliability, response time, and end-to-end delay. The outcome of the suggested method is compared with conventional techniques in terms of improved 11% accuracy and 36% throughput with 33%, 41%, and 20% minimum time, error rate, and space complexity.}
}
@article{COSTA2022101977,
title = {A core ontology on the Human–Computer Interaction phenomenon},
journal = {Data & Knowledge Engineering},
volume = {138},
pages = {101977},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101977},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000951},
author = {Simone Dornelas Costa and Monalessa Perini Barcellos and Ricardo de Almeida Falbo and Tayana Conte and Káthia M. {de Oliveira}},
keywords = {Human–Computer Interaction, User interface, Interactive computer system, Ontology, Ontology network},
abstract = {Human–Computer Interaction (HCI) is a complex communication phenomenon involving human beings and computer systems that gained large attention from industry and academia with the advent of new types of interactive systems (mobile applications, smart cities, smart homes, ubiquitous systems and so on). Despite of its importance, there is still a lack of formal and explicit representations of what the HCI phenomenon is. In this paper, we intend to clarify the main notions involved in the HCI phenomenon, by establishing an explicit conceptualization of it. To do so, we need to understand what interactive computer systems are, which types of actions users perform when interacting with an interactive computer system, and finally what human–computer interaction itself is. The conceptualization is presented as a core reference ontology, called HCIO (HCI Ontology), which is grounded in the Unified Foundational Ontology (UFO). HCIO was evaluated using ontology verification and validation techniques and has been used as core ontology of an HCI ontology network.}
}
@article{SHISHEHCHI2021100192,
title = {A rule based expert system based on ontology for diagnosis of ITP disease},
journal = {Smart Health},
volume = {21},
pages = {100192},
year = {2021},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2021.100192},
url = {https://www.sciencedirect.com/science/article/pii/S2352648321000143},
author = {Saman Shishehchi and Seyed Yashar Banihashem},
keywords = {ITP disease, Medical expert system, Ontology, Semantic rules},
abstract = {This paper is aimed to implement the semantic rule based expert system for diagnosing a kind of blood immune thrombocytopenia disease. This paper presents an ontology to depict the knowledge domain of this disease, symptoms and its related treatments. The developed system uses the real data from laboratory of Iranian hospitals. In the system, the observed symptoms are taken from the patient via Java based graphical user interface. This system supports the patients suffering from this disease to get a type of their disease and recommends suitable treatments. Some semantic rules were defined and then, Jess as a reasoner inferenced the rules to do the diagnosis process. The diagnosis process is validated by blood specialists. Since this system can be used by patients, doctors or medical students anywhere, it helps to make the disease follow up easier and tries to save cost and time for patients. The questionnaire is developed to measure the usability of the system. The results of questionnaire were satisfactory after it was tested with 154 respondents. The reliability test was done and the Cronbach's Alpha was .865 which is higher than 0.7. The mean value of questionnaire is more than 4 and the total mean is 4.49 which is an acceptable value to show the high degree of user acceptance and accuracy of system.}
}
@article{WANG2022111435,
title = {Missing standard features compared with similar apps? A feature recommendation method based on the knowledge from user interface},
journal = {Journal of Systems and Software},
volume = {193},
pages = {111435},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111435},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001364},
author = {Yihui Wang and Shanquan Gao and Xingtong Li and Lei Liu and Huaxiao Liu},
keywords = {Android apps, Feature recommendation, User interface},
abstract = {To attract and retain users, deciding what features should be added in the next release of apps becomes very crucial. Different from traditional software, there are rich data resources in app markets to perform market-wide analysis. Considering that capturing key features that apps lack compared with its similar products and making up for them can be conducive to enhance the competitiveness, we propose a method to establish the feature relationships from the level of UI pages and recommend missing key features for the pages of apps based on these relationships. Firstly, we utilize the UI testing tool to collect UI pages for apps in the repository, and give the method to gain the feature information in them. Then, we identify the products similar to the analyzed app based on topic modeling technique. Finally, we establish the relationships between features by analyzing UI pages gained for the analyzed app as well as its similar products, and identify suitable features recommended to UI pages of the analyzed app based on these relationships. The experiment based on Google Play shows that our method can recommend features for apps from the level of UI pages effectively.}
}
@article{EKELHART2018109,
title = {Taming the logs - Vocabularies for semantic security analysis},
journal = {Procedia Computer Science},
volume = {137},
pages = {109-119},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316156},
author = {Andreas Ekelhart and Elmar Kiesling and Kabul Kurniawan},
keywords = {semantic extraction, log vocabularies, log analysis, security analysis},
abstract = {Due to the growing complexity of information systems and the increasing prevalence and sophistication of threats, security management has become an enormously challenging task. To identify suspicious activities, security analysts need to monitor their systems constantly, which involves coping with high volumes of heterogeneous log data from various sources. Processes to aggregate these disparate logs and trigger alerts when particular events occur are often automated today. However, these methods are typically based on regular expressions and statistical correlations and do not involve any interpretation of the context in which an event occurred and do not allow for inference or sophisticated rules. Inspection and in-depth analysis of log information to link events from various sources (e.g., firewall, syslog, web server log, database log) and establish causal chains has therefore largely remained a tedious manual search process that scales poorly with a growing number of heterogeneous log sources, log volumes, and the increasing complexity of attacks. In this paper, we make the case for a semantic approach to tackle these challenges. By lifting raw log data and modeling their context, events can be linked to rich background knowledge, integrated based on causal relations, and interpreted in a context-specific manner. This builds a foundation for more comprehensive extraction of the meaning of events from unstructured log messages. Based on the results, we envision a platform to partly automate security monitoring and support analysts in coping with fast evolving threat landscapes, alleviate alert fatigue, improve situational awareness, and expedite incidence response.}
}
@article{SHENOY2022100679,
title = {A study of the quality of Wikidata},
journal = {Journal of Web Semantics},
volume = {72},
pages = {100679},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100679},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000536},
author = {Kartik Shenoy and Filip Ilievski and Daniel Garijo and Daniel Schwabe and Pedro Szekely},
keywords = {Wikidata, Data quality, Knowledge graphs, Constraints, Crowdsourcing},
abstract = {Wikidata has been increasingly adopted by many communities for a wide variety of applications, which demand high-quality knowledge to deliver successful results. In this paper, we develop a framework to detect and analyze low-quality statements in Wikidata by shedding light on the current practices exercised by the community. We explore three indicators of data quality in Wikidata, based on: (1) community consensus on the currently recorded knowledge, assuming that statements that have been removed and not added back are implicitly agreed to be of low quality; (2) statements that have been deprecated; and (3) constraint violations in the data. We combine these indicators to detect low-quality statements, revealing challenges with duplicate entities, missing triples, violated type rules, and taxonomic distinctions. Our findings complement ongoing efforts by the Wikidata community to improve data quality, aiming to make it easier for users and editors to find and correct mistakes.}
}
@article{BARBAGONZALEZ2021103546,
title = {Injecting domain knowledge in multi-objective optimization problems: A semantic approach},
journal = {Computer Standards & Interfaces},
volume = {78},
pages = {103546},
year = {2021},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2021.103546},
url = {https://www.sciencedirect.com/science/article/pii/S0920548921000416},
author = {Cristóbal Barba-González and Antonio J. Nebro and José García-Nieto and María {del Mar Roldán-García} and Ismael Navas-Delgado and José F. Aldana-Montes},
keywords = {Multi-Objective optimization, Decision making, Metaheuristics, Domain knowledge, Semantic web technologies, Ontology},
abstract = {In the field of complex problem optimization with metaheuristics, semantics has been used for modeling different aspects, such as: problem characterization, parameters, decision-maker’s preferences, or algorithms. However, there is a lack of approaches where ontologies are applied in a direct way into the optimization process, with the aim of enhancing it by allowing the systematic incorporation of additional domain knowledge. This is due to the high level of abstraction of ontologies, which makes them difficult to be mapped into the code implementing the problems and/or the specific operators of metaheuristics. In this paper, we present a strategy to inject domain knowledge (by reusing existing ontologies or creating a new one) into a problem implementation that will be optimized using a metaheuristic. Thus, this approach based on accepted ontologies enables building and exploiting complex computing systems in optimization problems. We describe a methodology to automatically induce user choices (taken from the ontology) into the problem implementations provided by the jMetal optimization framework. With the aim of illustrating our proposal, we focus on the urban domain. Concretely, we start from defining an ontology representing the domain semantics for a city (e.g., building, bridges, point of interest, routes, etc.) that allows defining a-priori preferences by a decision maker in a standard, reusable, and formal (logic-based) way. We validate our proposal with several instances of two use cases, consisting in bi-objective formulations of the Traveling Salesman Problem (TSP) and the Radio Network Design problem (RND), both in the context of an urban scenario. The results of the experiments conducted show how the semantic specification of domain constraints are effectively mapped into feasible solutions of the tackled TSP and RND scenarios. This proposal aims at representing a step forward towards the automatic modeling and adaptation of optimization problems guided by semantics, where the annotation of a human expert can be now considered during the optimization process.}
}
@article{KOONCE20191678,
title = {Metrics to gauge the success of a manufacturing ontology},
journal = {Procedia Manufacturing},
volume = {38},
pages = {1678-1682},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.116},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920301177},
author = {David Koonce and Dusan Sormaz},
keywords = {Ontology Evaluation, Ontology Usage, Manufacturing Ontology},
abstract = {Ontologies are structures of concepts that define a high-level representation of a system or area. They can serve as a foundational understanding for developing or integrating software representations of said system. And while the construction of can be a complex, multi-party exercise, the assessment of ontologies is often defined less on usage and more on completeness and coverage. In manufacturing, ontology development ranges from supply chain to production to design. Owing to the computer science foundations of ontology design and representation, the value or quality of an ontology can be assessed on notions of completeness and coverage. Recently, researchers have posited that usage should factor into the Ontology Lifecycle. Similar to how the market, and not technology, defines the success of a product or technology, this paper will examine how utilization of an ontology can define the value or quality of the ontology}
}
@article{CHHETRI2024164,
title = {Enabling privacy-aware interoperable and quality IoT data sharing with context},
journal = {Future Generation Computer Systems},
volume = {157},
pages = {164-179},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24001109},
author = {Tek Raj Chhetri and Chinmaya Kumar Dehury and Blesson Varghese and Anna Fensel and Satish Narayana Srirama and Rance J. DeLong},
keywords = {Data sharing, Edge intelligence, Interoperability, Internet of Things (IoT), Knowledge graphs, General Data Protection Regulation (GDPR), Smart cities},
abstract = {Sharing Internet of Things (IoT) data across different sectors, such as in smart cities, becomes complex due to heterogeneity. This poses challenges related to a lack of interoperability, data quality issues and lack of context information, and a lack of data veracity (or accuracy). In addition, there are privacy concerns as IoT data may contain personally identifiable information. To address the above challenges, this paper presents a novel semantic technology-based framework that enables data sharing in a GDPR-compliant manner while ensuring that the data shared is interoperable, contains required context information, is of acceptable quality, and is accurate and trustworthy. The proposed framework also accounts for the edge/fog, an upcoming computing paradigm for the IoT to support real-time decisions. We evaluate the performance of the proposed framework with two different edge and fog–edge scenarios using resource-constrained IoT devices, such as the Raspberry Pi. In addition, we also evaluate shared data quality, interoperability and veracity. Our key finding is that the proposed framework can be employed on IoT devices with limited resources due to its low CPU and memory utilization for analytics operations and data transformation and migration operations. The low overhead of the framework supports real-time decision making. In addition, the 100% accuracy of our evaluation of the data quality and veracity based on 180 different observations demonstrates that the proposed framework can guarantee both data quality and veracity.}
}
@article{SEDIGHIANI2021110989,
title = {BASBA: A framework for Building Adaptable Service-Based Applications},
journal = {Journal of Systems and Software},
volume = {179},
pages = {110989},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110989},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000868},
author = {Kavan Sedighiani and Saeed Shokrollahi and Fereidoon Shams},
keywords = {Service-based application, Self-adaptation, Models at runtime, Quality of service, Variability, Reusability},
abstract = {Due to the continuously changing environment of service-based applications (SBAs), the ability to adapt to environmental and contextual changes has become a crucial characteristic of such applications. Providing SBAs with this ability is a complex task, usually carried out in an unsystematic way and interwoven with application logic. As a result, developing and maintaining adaptive SBAs has become a costly and hardly repeatable process. The objective of this paper is to present a model-based approach to developing adaptive SBAs which separates development of adaptation concerns from development of SBAs behaviors. This approach aims to facilitate and improve the development of adaptive behaviors. In this paper, the process of developing an adaptive SBA is defined as specifying adaptive SBA models based on a metamodel and reusable adaptation tactics. These models are then transformed into runtime model artifacts and running system units performing runtime adaptive behaviors. The approach introduces a systematic method to derive adaptation behaviors from adaptation models, which facilitates the development of adaptive behaviors. The empirical evaluations in three studies show that our approach enhances the development of adaptive behaviors in terms of identifying more proper adaptation plans, reducing the development time, and increasing understandability, modifiability, and correctness of code.}
}
@article{REN201924,
title = {Building an ontological knowledgebase for bridge maintenance},
journal = {Advances in Engineering Software},
volume = {130},
pages = {24-40},
year = {2019},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818307634},
author = {Guoqian Ren and Rui Ding and Haijiang Li},
keywords = {Bridge maintenance, Semantic web, Ontology, Multi-criteria decision support, Suppliers selection, Event management},
abstract = {The operation stage has the biggest potential value in the bridge life cycle management, and it often critically influences the overall cost of the bridge. As such, changes in the efficiency of the project's operation stage could be of significant benefit to the overall project. However, current approaches in the operation stage often lack the effective support of computer-aided tools. This research presents a holistic method based on an ontology to achieve automatic rule checking and improve the management and communication of knowledge related to bridge maintenance. The developed ontology can also facilitate a smarter decision-making process for bridge management by informing engineers of choices with different considerations. Three approaches; semantic validation, syntactical validation, and case study validation, have been adopted to evaluate this ontology and demonstrate how the developed ontology can be used by engineers when dealing with different issues. The results showed that this approach can create a holistic knowledge base that can integrate various domain knowledge to enable bridge engineers to make more comprehensive decisions rather than a single objective-targeted delivery.}
}
@article{ALVITEDIEZ2021946,
title = {Linked open data portals: functionalities and user experience in semantic catalogues},
journal = {Online Information Review},
volume = {45},
number = {5},
pages = {946-963},
year = {2021},
issn = {1468-4527},
doi = {https://doi.org/10.1108/OIR-07-2020-0295},
url = {https://www.sciencedirect.com/science/article/pii/S1468452721000779},
author = {María-Luisa Alvite-Díez},
keywords = {Linked open data, User experience, Semantic Web, User interface, Information discovery},
abstract = {Purpose
This study seeks to understand the current state of the development of linked open data (LOD) bibliographic portals to discuss their functionalities, contributions, value-adds and user experience.
Design/methodology/approach
A set of evaluative aspects grouped into three analysis dimensions was established: collections, tools—technologies and standards used—and web user interface. As the object of the study, four projects of diverse nature and volume were selected to help provide a better understanding of the trends in the solutions provided for the end user when accessing linked data collections.
Findings
Publishing LOD through visual interfaces maximises information enrichment, contextualisation and discovery, in addition to improving user experience, because of both increased navigation capabilities and interrelationships between data. These more flexible environments have metamorphosised the visualisation of bibliographic information. However, aspects that needed improvement were observed, primarily relating to (1) a more intuitive interaction, (2) possibilities of greater personalisation, (3) enhanced communication with the user to favour user engagement and (4) experimental spaces of data reuse.
Research limitations/implications
Further quantitative and qualitative studies should be conducted to improve these portals, assess their adaptation to the behaviour of the user and their influence on the use of library collections.
Originality/value
This article investigates the potential of semantic technologies in bibliographic data portals, proposes a methodological model for their evaluation and advances conclusions about the usability and user experience that these platforms provide, compared to classic catalogues.}
}
@article{LI2019152,
title = {Enhancing energy management at district and building levels via an EM-KPI ontology},
journal = {Automation in Construction},
volume = {99},
pages = {152-167},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517309494},
author = {Yehong Li and Raúl García-Castro and Nandana Mihindukulasooriya and James O'Donnell and Sergio Vega-Sánchez},
keywords = {District, Building, Energy management, Stakeholders, Ontology, Linked data},
abstract = {The use of information and communication technologies facilitates energy management (EM) at both district and building levels, but also generates a considerable amount of data. To gain insights into such data, it is essential to resolve the cross-domain data interoperability problem and determine an approach to exchange performance information and insightful data among various stakeholders. This paper developed an EM-KPI (key performance indicator) ontology to exchange key performance information and data for districts and buildings. The ontology contains two components: namely KPIs and EM master data; these, respectively, represent the multi-level performance information for energy performance tracking and the key data for data exploitation. Through a demonstration, a sample linked dataset generated using the data correlation predefined in the ontology is presented. The linked data analysis proves the feasibility of the ontology for exchanging data among different stakeholders and for exploring insights in relation to performance improvement.}
}
@article{LONGO2022107824,
title = {New perspectives and results for Smart Operators in industry 4.0: A human-centered approach},
journal = {Computers & Industrial Engineering},
volume = {163},
pages = {107824},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107824},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221007282},
author = {Francesco Longo and Letizia Nicoletti and Antonio Padovano},
keywords = {Smart factory, Smart operators, Industry 4.0, Extended reality, Digital and intelligent assistants},
abstract = {Digital solutions (including Extended Reality as well as web, mobile and AI technologies), among others, are entering a broad variety of industries and modifying the capabilities of operators through (close to) real-time display of context-dependent information. However, little is known with respect to two major issues: (i) how these technologies can be seamlessly integrated for product/process and overall manufacturing system management providing of syntactic, semantic and functional interoperability and reusability among the different manufacturing systems departments; (ii) how they can be conveyed to operators also taking into account the cognitive load that is incurred by the operators. To this end, starting from a previous article (Longo et al., 2017), this article designs and proposes the KNOW4I approach and its practical implementation in an ICT platform (the KNOW4I platform) to further empower the Smart Operators concept. Two major objectives are pursued. The former is to set a standard referred to as the KNOW4I methodological approach for knowledge representation, knowledge management and digital contents management within the Smart Operator domain. The latter is the implementation of the aforementioned approach as part of the KNOW4I platform that includes a suite of Smart Utilities and Objects intelligently and interactively linked with a newly released version of the Sophos-MS digital and intelligent Assistant. Experimentations and results based on multiple KPIs are carried out to account for the effectiveness of the proposed framework.}
}
@article{CASTIGLIONE20181134,
title = {CHIS: A big data infrastructure to manage digital cultural items},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {1134-1145},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17305605},
author = {Aniello Castiglione and Francesco Colace and Vincenzo Moscato and Francesco Palmieri},
keywords = {Big data, Cultural heritage, Resource management, Big data analytics, SOA, NoSQL},
abstract = {In this paper, we describe CHIS (Cultural Heritage Information System), a big data infrastructure that can be used to query, browse, analyze and process digital contents related to cultural heritage from a set of heterogeneous and distributed repositories. CHIS is characterized by the following technical features: capability to gather information from distributed and heterogeneous data sources (e.g., Sensor Networks, Social Media Networks, Digital Libraries and Archives, Multimedia Collections, Web Data Services, etc.); advanced data management techniques and technologies; ability to provide useful and personalized data to users based on their preferences and context; advanced information retrieval facilities, data analytics and other utilities/services, according to the SOA paradigm. By means of a set of ad-hoc APIs, and value-added data processing and analytics services, our system can support several applications: mobile multimedia guides for cultural environments, web portals to promote the cultural heritage of a given organization, multimedia recommender and storytelling systems and so on. We discuss the main ideas that characterize the system, showing its use for several applications.}
}
@article{VEGETTI2022100254,
title = {Ontology network to support the integration of planning and scheduling activities in batch process industries},
journal = {Journal of Industrial Information Integration},
volume = {25},
pages = {100254},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100254},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000534},
author = {Marcela Vegetti and Gabriela Henning},
keywords = {Scheduling, Ontologies, Integration, Formal specifications},
abstract = {In the last decades, the integration of information systems supporting planning, scheduling, and control has been a serious concern of the industrial community. Several standards have been developed to tackle this issue by addressing the exchange of data between the scheduling function and its immediate lower and upper levels in the planning pyramid. However, a more comprehensive approach is required to solve these integration problems, since this matter entails much more than data exchange. Along these lines, this article presents a network of ontologies that provides the foundations to reach an effective semantic interoperability among the various applications linked to scheduling activities. The proposed approach reuses and formalizes non-ontological resources, like the ISA-88 and ISA-95 standards, as well as the Resource Task Network (RTN) model. In addition, the application of the ontology network to a case study is also discussed in this article.}
}
@article{GUERRA2025111831,
title = {A cases and clusters framework for recording, retrieving, and reusing response plans in structured cybersecurity incident management},
journal = {Engineering Applications of Artificial Intelligence},
volume = {160},
pages = {111831},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111831},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625018330},
author = {Patrick Andrei Caron Guerra and Raul Ceretta Nunes and Luis Alvaro {de Lima Silva}},
keywords = {Cybersecurity incident response, Case-based reasoning, Clustering, Ontology, Explainable artificial intelligence, Decision-support system, Cybersecurity},
abstract = {The dynamic and increasing sophistication of cyberattacks and vulnerability exploitation creates a need for Explainable Artificial Intelligence (XAI) approaches that help maintain cyber resilience in organizations. In structured cybersecurity incident management, effective incident response demands explainable outputs from AI-based decision-support systems. To approach this problem, this work presents a framework for reusing concrete experiences of cybersecurity incident response, capturing problem-solving data and knowledge as cases for integrated Case-Based Reasoning (CBR) and Clustering. The contribution includes cluster-based query answer analysis, where cybersecurity analysts reuse clusters of retrieved incident response cases to build answers to new problems. Clustering helps analysts identify relevant groups from ranked lists of retrieved cases, making the reuse process more structured and understandable, especially when dealing with retrieval results for broad and ambiguous queries. Different clustering methods are applied to organize retrieved incident response cases from a case base, supporting the grouping of similar cases for a more straightforward interpretation. Multiple experiments, including cross-validation and real-world incident response testing, are conducted to demonstrate the effectiveness of the proposed framework in improving the decision-support system’s precision. The results indicate that exploring cases and clusters can enhance the selection of incident response procedures for reuse, mainly when analysts identify the most relevant clusters of retrieved cases for the given problem situations. The proposed framework contributes to the organization and understanding of responses to cybersecurity incidents, besides supporting more informed decision-making, ultimately improving cybersecurity incident management.}
}
@article{SANTOS201811,
title = {Partial meet pseudo-contractions},
journal = {International Journal of Approximate Reasoning},
volume = {103},
pages = {11-27},
year = {2018},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2018.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X1830104X},
author = {Yuri David Santos and Vinícius Bitencourt Matos and Márcio Moretto Ribeiro and Renata Wassermann},
keywords = {Belief revision, Limited reasoning, Ontologies, Pseudo-contractions},
abstract = {In the AGM paradigm for belief revision, epistemic states are represented by logically closed sets of sentences, the so-called belief sets. An alternative approach uses belief bases, arbitrary sets of sentences. Both approaches have their problems when it comes to contraction operations. Belief bases are more expressive, but, at the same time, they present a serious syntax dependence. Between those two extremes lie a whole gamut of operations called pseudo-contractions, some of which may be interesting alternatives to the classical ones, providing a good balance between syntax dependence and expressivity. In this paper we explore some very natural and general constructions for pseudo-contractions, showing some of their properties and giving their axiomatic characterizations. We also illustrate possible practical scenarios where they can be employed.}
}
@article{RAMESH2024100677,
title = {An interoperable ontology for CPS-enabled Polyhouse Solar Dryer: A case study of the AgroESP project},
journal = {Journal of Industrial Information Integration},
volume = {42},
pages = {100677},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100677},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24001171},
author = {Gowtham Ramesh and P. {Dheepan Kanna} and C. {Shunmuga Velayutham} and Jancirani Ramaswamy},
keywords = {Polyhouse solar dryer, Polyhouse ontology, Interoperability, CPS, Food preservation},
abstract = {Polyhouse is a commonly used conventional method for solar drying of food products. These Polyhouse Solar Driers (PSDs) are characterized by their enclosed structure and translucent covering, providing a controlled environment conducive to efficient food drying. Smart polyhouses, equipped with a Cyber-Physical System (CPS), further enhance this process by optimizing environmental conditions and enabling real-time monitoring. In smart PSDs, the data are obtained from diverse sources with different specifications in accuracy, resolution, and range. This multifaceted nature of the information obtained from various sources significantly compounds the complexity of the system. This complexity of data from diverse sources within smart polyhouses necessitates a standardized knowledge representation. Ontologies serve this purpose by establishing a common vocabulary and structure for data integration, promoting semantic interoperability and effective communication among diverse systems. This paper proposes a novel unified ontology designed to model complex polyhouse CPSs, aiming to address semantic interoperability issues and streamline data integration across various domains. The proposed polyhouse ontology attempts to reuse the concepts defined in existing ontologies rather defining new concepts for efficient knowledge sharing and enhanced understanding of polyhouse operations. The practical applicability of the polyhouse ontology has been verified with competency questions and through field deployment in a CPS enabled smart Polyhouse Solar Dryer.}
}
@article{MOSSAKOWSKI201858,
title = {Partial pushout semantics of generics in DOL},
journal = {Theoretical Computer Science},
volume = {741},
pages = {58-70},
year = {2018},
note = {An Observant Mind : Essays Dedicated to Don Sannella on the Occasion of his 60th Birthday},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2018.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0304397518301828},
author = {Till Mossakowski and Bernd Krieg-Brückner},
keywords = {Generic specification, Single-pushout transformation, Institution, Category of partial maps},
abstract = {We combine CASL's pushout-style generic specification with DOL's filtering, the latter being a syntactic removal of parts of a specification. The challenge is that now the body of a generic specification can remove parts of the formal parameter. This cannot be handled with usual pushout semantics, but calls for a semantics of “match, delete, glue in” as used in the theory of graph grammars. We hence employ Heindel's theory of MipMap categories as a basis for the use of pushouts in categories of partial maps. We introduce a notion of MipMap institution that can serve as a semantic background for a partial pushout semantics of generics with filtering.}
}
@article{ELSAPPAGH2022203,
title = {Automatic detection of Alzheimer’s disease progression: An efficient information fusion approach with heterogeneous ensemble classifiers},
journal = {Neurocomputing},
volume = {512},
pages = {203-224},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222010955},
author = {Shaker El-Sappagh and Farman Ali and Tamer Abuhmed and Jaiteg Singh and Jose M. Alonso},
keywords = {Computational Intelligence, Data fusion, Ensemble classifiers, Stacking, Data analysis, Alzheimer disease progression detection},
abstract = {Predicting Alzheimer’s disease (AD) progression is crucial for improving the management of this chronic disease. Usually, data from AD patients are multimodal and time series in nature. This study proposes a novel ensemble learning framework for AD progression incorporating heterogeneous base learners into an integrated model using the stacking technique. This framework is used to build a 4-class ensemble classifier, which predicts AD progression 2.5 years in the future based on the multimodal time-series data. Statistical measures have been extracted from the longitudinal data to be used by the conventional machine learning models. The examined ensemble members include k-nearest neighbor, extreme gradient boosting, support vector machine, random forest, decision tree, and multilayer perceptron. We utilize three time-series modalities and one static non-time series modality of 1371 subjects from the Alzheimer’s disease neuroimaging initiative (ADNI) to validate our model. Several homogeneous and heterogeneous combinations of ensemble members were implemented, and their performance compared. The balance between accuracy and diversity when selecting ensemble members was investigated. We found that both accuracy and diversity are equally critical metrics to obtain an optimal ensemble model. Furthermore, our testing showed that the proposed model achieves outstanding progression prediction performance. The proposed model achieved a high performance without using neuroimaging data, which means that the model could be implemented in low-cost healthcare environments. The proposed model has achieved superior results compared with the state-of-the-art techniques in Alzheimer’s and ensemble classifiers domains. The proposed framework can be used to implement efficient information fusion ensembles for other medical and non-medical problems.}
}
@article{ZHENG2021103930,
title = {A shared ontology suite for digital construction workflow},
journal = {Automation in Construction},
volume = {132},
pages = {103930},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103930},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521003812},
author = {Yuan Zheng and Seppo Törmä and Olli Seppänen},
keywords = {Construction workflow, Ontology, Digital construction, Information management},
abstract = {With ongoing advancements in information and communication technologies (ICTs) in all stages of the construction lifecycle, information from entities related to construction workflow (CW) can now be automatically collected. These implementations are point solutions, which require systematic integration to combine their information to enable a holistic picture of CW. The major barrier to such integration is information heterogeneity, where the information is collected from different systems under multiple contexts. Scholars in the construction domain have explored the use of ontology to solve the information-integration problem, although an ontology that both adequately represents the CW and integrates the digitalized information of CW via various systems and multiple contexts is currently missing from the existing literature. This research thus presents an ontology set for formalizing and integrating CW information within the digital construction context. The proposed digital construction ontologies (DiCon) are shared representations of construction domain knowledge that specify the terms and relations of CWs and their related information. We developed the DiCon based on a hybrid ontology development approach. The DiCon includes six modules: Entities, Processes, Information, Agents, Variables, and Contexts. The developed DiCon was further evaluated by approaches including automatic consistency checking, criteria-based evaluation, expert workshops, and task-based evaluation and involved two use cases by answering relevant competency questions via SPARQL queries. The results of the evaluation demonstrate that the DiCon ontologies are sufficient to represent domain knowledge and can formalize and integrate CW information within the digital construction context.}
}
@article{BLANKENBERG2022314,
title = {Using a graph database for the ontology-based information integration of business objects from heterogenous Business Information Systems},
journal = {Procedia Computer Science},
volume = {196},
pages = {314-323},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022420},
author = {Carolin Blankenberg and Berit Gebel-Sauer and Petra Schubert},
keywords = {IS Integration, Enterprise Knowledge Graph, Ontology, Graph Database},
abstract = {This paper reports on findings from a project on information integration from multiple Business Information Systems with the help of a user-specific Enterprise Knowledge Graph. Most ERP systems currently in use store information objects in relational databases. Research in Web Sciences has shown that graph structures present information in a more intuitive way that is easier to interpret for humans. Following a DSR approach, we developed a concept for storing an ontology in a graph database that allows us to map ERP objects and load them at runtime. This allows the end user to navigate through the graph structure, thus providing an intuitive and quick access to essential job-related information. We evaluated the suggested concept with a prototype following the paradigm of polyglot persistence; the prototype was equipped with a graph database to store the company-specific ontology in its native form. The program code was encapsulated into a separate module following a service-oriented software design.}
}
@article{FATHALLA2018151,
title = {SemSur: A Core Ontology for the Semantic Representation of Research Findings},
journal = {Procedia Computer Science},
volume = {137},
pages = {151-162},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S187705091831620X},
author = {Said Fathalla and Sahar Vahdati and Sören Auer and Christoph Lange},
keywords = {SemSur Ontology, Semantic Metadata Enrichment, SWRL rules, Scholarly Communication, Semantic Publishing},
abstract = {The way how research is communicated using text publications has not changed much over the past decades. We have the vision that ultimately researchers will work on a common structured knowledge base comprising comprehensive semantic and machine-comprehensible descriptions of their research, thus making research contributions more transparent and comparable. We present the SemSur ontology for semantically capturing the information commonly found in survey and review articles. SemSur is able to represent scientific results and to publish them in a comprehensive knowledge graph, which provides an efficient overview of a research field, and to compare research findings with related works in a structured way, thus saving researchers a significant amount of time and effort. The new release of SemSur covers more domains, defines better alignment with external ontologies and rules for eliciting implicit knowledge. We discuss possible applications and present an evaluation of our approach with the retrospective, exemplary semantification of a survey. We demonstrate the utility of the SemSur ontology to answer queries about the different research contributions covered by the survey. SemSur is currently used and maintained at OpenResearch.org.}
}
@article{NIZAMIS2018382,
title = {A Semantic Framework for Agent-based Collaborative Manufacturing Eco-systems},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {382-387},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.323},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318314472},
author = {Alexandros G. Nizamis and Dimosthenis K. Ioannidis and Nikolaos T. Kaklanis and Dimitrios K. Tzovaras},
keywords = {Manufacturing eco-system, Semantic modeling, Ontology, Rule-based Matchmaking, Supply, demand chain},
abstract = {Manufacturing eco-systems aim to connect data and services between factories, suppliers and customers. Most of them are built as agent-based eco-systems which act as web-based operating systems for business connections between enterprises. The connection of supply and demand entities participating in an eco-system by exploiting knowledge and data from the business entities has become imperative for them, in order to adapt to the dynamically changing market requirements. This paper introduces a web-based semantic ontological framework designed for collaborative agent-based manufacturing eco-systems. The proposed framework and its core components enable the information modeling of the manufacturing services and the supply chain concepts. A Collaborative Manufacturing Services Ontology able to describe both manufacturing domain and e-commerce domain is offered alongside with an Application Programming Interface for the effortless manipulation of the ontological resources. Furthermore, a Rule-based Matchmaking engine able to match requesters with possible suppliers, and to evaluate offers from suppliers based on different requesters’ criteria and preferences is provided.}
}
@article{GAWICH20243208,
title = {Towards an Ontology-Driven System For Building and Farming Greenhouses},
journal = {Procedia Computer Science},
volume = {246},
pages = {3208-3217},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.319},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924023445},
author = {Mariam Gawich and Christine Lahoud and Hajer Baazaoui and Ihab Jomaa},
keywords = {Greenhouse Ontology, Greenhouse, Knowledge Engineering, Agriculture},
abstract = {Greenhouse systems are considered a part of sustainable agriculture, whose objective is food security and safety while taking into consideration the conservation of resources such as soil and water. To promote sustainable agriculture through greenhouses, it is important to develop an intelligent system that helps stakeholders in decision-making concerning the construction and management of greenhouses. This system must ensure farming activities and monitoring procedures. This work concentrates on the farming activities such as pest control, disease protection, crop cultivation, treatment, etc, and their representation in the system. Ontology is used as a technology to represent the structured information in terms of concepts and the establishment of semantic relations among them. While many existing ontologies focus on agriculture management, the greenhouse domain lack comprehensive coverage, particularly in the operational farming activities that are necessary to ensure the agriculture sustainability. Therefore, there is a need to develop a greenhouse ontology-based system that address the stakeholders’ inquiries related to greenhouse construction and essential farming activities for greenhouse management. This paper presents a synthesis analysis of the existing ontologies in the domain of agriculture and greenhouses as well as a novel modular ontology that covers the greenhouse farming module.}
}
@article{AMADORDOMINGUEZ2021115731,
title = {A hierarchical multi-agent architecture based on virtual identities to explain black-box personalization policies},
journal = {Expert Systems with Applications},
volume = {186},
pages = {115731},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115731},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421011118},
author = {Elvira Amador-Domínguez and Emilio Serrano and Daniel Manrique},
keywords = {Multi-agent system, Virtual identities, Personalization},
abstract = {Hyper-personalization policies entail a considerable improvement regarding previous personalization approaches. However, they present several issues that need to be addressed, such as minimal explainability and privacy invasion. A hierarchical Multi-Agent System (MAS) is presented in this work to provide a solution to these concerns. The system is formulated as a hybrid approach, where some of the agents work autonomously, while the user input triggers the remaining. At the autonomous level, a set of Virtual Identities (VIs) representing different user profiles interact with Black-Box Hyper-Personalization Online Systems (BBHOS), gathering a set of targeted responses. Associative patterns and profile aggregations can then be inferred from the analysis of these responses. In the user-triggered level, the real user is virtualized as an identity that represents their features. The virtual identity serves as an intermediary between the personalization system and the real user. This virtualization hinders the personalization service from extracting sensitive contextual information about the real user, protecting their privacy. The results obtained by the user identity on its interaction with the personalization service are then analyzed, adjusting the content of the response to fit the user’s requests instead of their features. A use case on the functioning of the analysis of search engines is presented to illustrate the complete behavior of the proposed architecture.}
}
@article{KOUTSIANA2025100868,
title = {Agreeing and disagreeing in collaborative knowledge graph construction: An analysis of Wikidata},
journal = {Journal of Web Semantics},
volume = {86},
pages = {100868},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100868},
url = {https://www.sciencedirect.com/science/article/pii/S1570826825000095},
author = {Elisavet Koutsiana and Tushita Yadav and Nitisha Jain and Albert Meroño-Peñuela and Elena Simperl},
keywords = {Collaborative knowledge graph, Collaborative knowledge base, Knowledge community, Discussion analysis, Community analysis, Controversy, Argumentation},
abstract = {In this work, we study disagreements in discussions around Wikidata, an online knowledge community that builds the data backend of Wikipedia. Discussions are essential in collaborative work as they can increase contributor performance and encourage the emergence of shared norms and practices. While disagreements can play a productive role in discussions, they can also lead to conflicts and controversies, which impact contributor’ well-being and their motivation to engage. We want to understand if and when such phenomena arise in Wikidata, using a mix of quantitative and qualitative analyses to identify the types of topics people disagree about, the most common patterns of interaction, and roles people play when arguing for or against an issue. We find that decisions to create Wikidata properties are much faster than those to delete properties and that more than half of controversial discussions do not lead to consensus. Our analysis suggests that Wikidata is an inclusive community, considering different opinions when making decisions, and that conflict and vandalism are rare in discussions. At the same time, while one-fourth of the editors participating in controversial discussions contribute legitimate and insightful opinions about Wikidata’s emerging issues, they respond with one or two posts and do not remain engaged in the discussions to reach consensus. Our work contributes to the analysis of collaborative KG construction with insights about communication and decision-making in projects, as well as with methodological directions and open datasets. We hope our findings will help managers and designers support community decision-making and improve discussion tools and practices.}
}
@article{SWEIDAN2023101720,
title = {Fuzzy ontology-based approach for liver fibrosis diagnosis},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {8},
pages = {101720},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101720},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823002744},
author = {Sara Sweidan and Nuha Zamzami and Sahar F. Sabbeh},
keywords = {Liver fibrosis, Fuzzy ontology, Rule-based system, Semantics reasoning, Fuzzy reasoning},
abstract = {The domain of the digestive system is prone to severe chronic disease in the form of liver cirrhosis, which is currently a leading cause of mortality. This article presents a new intelligent system for predicting the severity of liver fibrosis in patients with chronic viral hepatitis C. The proposed system is based on the inference capabilities of fuzzy ontology and operates on semantic rule-based techniques. A fuzzy decision tree technique was employed to generate the ontology rule base using a dataset of real fibrosis cases from the Mansoura University Hospital, Egypt. These rules were then encoded into a set of fuzzy semantic rules using the fuzzy description logic format. To evaluate the system’s effectiveness, the proposed ontology was then tested on 47 chronic HCV cases, with an attempt made to see if this correctly diagnosed the patients’ conditions. The performance of the proposed system was compared with that of the now-standard Mamdani fuzzy inference system; while the latter achieved an accuracy of 95.7/%, the proposed fuzzy ontology-based system demonstrated higher performance, with 97.8% accuracy. Furthermore, the proposed system also supports semantic interoperability between clinical decision support systems and electronic health record ecosystems. The positive impacts of this system on the correct prediction of liver fibrosis severity thus suggest that it has the potential to assist medical professionals in diagnosing and treating this dangerous disease.}
}
@article{PEREIRA2020101760,
title = {A knowledge representation of the beginning of the innovation process: The Front End of Innovation Integrative Ontology (FEI2O)},
journal = {Data & Knowledge Engineering},
volume = {125},
pages = {101760},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2019.101760},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18301162},
author = {Ariane Rodrigues Pereira and João José Pinto Ferreira and Alexandra Lopes},
keywords = {Front end of innovation, Ontology, Entrepreneurship, Concept development, Design science},
abstract = {The initial phase of the innovation process is widely accepted as an important driver of positive results for new products and for the success of businesses. The Front End of Innovation (FEI) is a multidisciplinary area that includes a variety of activities, such as ideation, opportunity identification and analysis, feasibility analysis, global trends analysis, concept definition, customer and competitor analysis, and even business model development. Due to the number and variety of FEI responsibilities, this phase entails a considerable level of complexity and decision making. This fact is reflected in the literature, where one finds a variety of FEI approaches and proposals, seldom overlapping and offering no clear consensual guidance. This work aimed at overcoming this gap by proposing an Ontology for the Front End of Innovation as a comprehensive knowledge representation of the FEI, the so-called Front End of Innovation Integrative Ontology (FEI2O). The ontology balanced the differences and addressed the shortcomings of the main FEI Reference Models and included contributions from the field. This research builds on a combination of qualitative and quantitative methodologies. It combines the qualitative methods of interviewing and focus group discussion to collect the views of domain experts, used to refine the artefact and later to evaluate the final ontology. Quantitative analysis of data was carried out using the Attribute Agreement approach. The FEI2O explicitly provides a description of a domain regarding concepts, properties and relations of concepts. The main benefit of the FEI2O is to provide a comprehensive formal reference model and a common vocabulary.}
}
@article{MENDONCA2020101045,
title = {Ontological emergence scheme in self-organized and emerging systems},
journal = {Advanced Engineering Informatics},
volume = {44},
pages = {101045},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101045},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620300148},
author = {Maribel Mendonça and Niriaska Perozo and Jose Aguilar},
keywords = {Emerging systems, Self-organized systems, Ontology mining, Web semantic, Ontological emergence, Knowledge discovery},
abstract = {This paper presents the concept of “Ontological Emergence”, a process that seeks to adapt an ontology to the changes and new components in a self-organized and emergent system, through the application of a set of rules that allows the emergence of a new conceptualization (emerging concepts). The Ontological Emergence provides the structuration of the information and knowledge that could be generated in the system, creating conceptual models that can adequately represent the new behavior that is emerging. It arises from the need to represent ontologically a conceptualization of a reality that is dynamic, which cannot be pre-defined or pre-determined, in order to generate emerging knowledge models that follows the scalability and the evolution of it. In that sense, in this paper is proposed an “Ontological Emergence Scheme” based on a set of processes of registration, monitoring, analysis and adaptation of the various conceptual models that interact in the system, as well as on some processing rules in regard to requirements and information of the context, in order to allow the ontological emergence. In this proposal scheme, the Meta-ontologies guide the ontological emergence process through the definition of general categories, to facilitate the integration of concepts from different ontologies or data sources. Finally, the paper presents some case studies, showing its utility in self-organized and emergent systems.}
}
@article{YAN2019259,
title = {A graph convolutional neural network for classification of building patterns using spatial vector data},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {150},
pages = {259-273},
year = {2019},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2019.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0924271619300437},
author = {Xiongfeng Yan and Tinghua Ai and Min Yang and Hongmei Yin},
keywords = {Building pattern classification, Graph convolutional neural network, Machine learning, Spatial vector data, Graph Fourier transform, Deep learning},
abstract = {Machine learning methods, specifically, convolutional neural networks (CNNs), have emerged as an integral part of scientific research in many disciplines. However, these powerful methods often fail to perform pattern analysis and knowledge mining with spatial vector data because in most cases, such data are not underlying grid-like or array structures but can only be modeled as graph structures. The present study introduces a novel graph convolution by converting it from the vertex domain into a point-wise product in the Fourier domain using the graph Fourier transform and convolution theorem. In addition, the graph convolutional neural network (GCNN) architecture is proposed to analyze graph-structured spatial vector data. The focus of this study is the classical task of building pattern classification, which remains limited by the use of design rules and manually extracted features for specific patterns. The spatial vector data representing grouped buildings are modeled as graphs, and indices for the characteristics of individual buildings are investigated to collect the input variables. The pattern features of these graphs are directly extracted by training labeled data. Experiments confirmed that the GCNN produces satisfactory results in terms of identifying regular and irregular patterns, and thus achieves a significant improvement over existing methods. In summary, the GCNN has considerable potential for the analysis of graph-structured spatial vector data as well as scope for further improvement.}
}
@article{POLENGHI2022100286,
title = {Ontology-augmented Prognostics and Health Management for shopfloor-synchronised joint maintenance and production management decisions},
journal = {Journal of Industrial Information Integration},
volume = {27},
pages = {100286},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100286},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000832},
author = {Adalberto Polenghi and Irene Roda and Marco Macchi and Alessandro Pozzetti},
keywords = {Ontology, Reasoning, Prognostics and health management, PHM, maintenance, production},
abstract = {In smart factories, guaranteeing shopfloor-synchronised and real-time decision-making is essential to be responsive to the ever-changing internal environment, namely the shopfloor of the production system and assets. At operational level, decisions should balance counter acting objectives of maintenance and production; therefore, their decision-making processes should be joint and coordinated, to fulfil production requirements considering the health state of the assets. The knowledge of the current state is promoted by the application of Prognostics and Health Management (PHM) as an aid to support informed decision-making. Nevertheless, PHM-purposed information is usually not complete in terms of production requirements. To support joint maintenance and production management decisions, an ontological approach is proposed. The ontology, called ORMA (Ontology for Reliability-centred MAintenance), has a modular structure, including formalisation of asset, process, and product knowledge. Via suitable relationships, rules, and axioms, ORMA can infer product feasibility based on the current health state of the assets and their functional units. ORMA is implemented in a Flexible Manufacturing Line at a laboratory scale. Therein, an integrated solution, involving a health state detection algorithm that interacts with the ontology, supports human decision-making via a web-based dashboard; joint maintenance and production management decisions can be then taken, relying on diversified information provided by the PHM algorithm as well as the augmentation via ontology reasoning. The proposed ontology-based solution represents a step towards reconfigurability of smart factories where human and automated decision-making processes work in synergy.}
}
@article{WANG2021,
title = {Pathway-Driven Coordinated Telehealth System for Management of Patients With Single or Multiple Chronic Diseases in China: System Development and Retrospective Study},
journal = {JMIR Medical Informatics},
volume = {9},
number = {5},
year = {2021},
issn = {2291-9694},
doi = {https://doi.org/10.2196/27228},
url = {https://www.sciencedirect.com/science/article/pii/S2291969421001940},
author = {Zheyu Wang and Jiye An and Hui Lin and Jiaqiang Zhou and Fang Liu and Juan Chen and Huilong Duan and Ning Deng},
keywords = {chronic disease, telehealth system, integrated care, pathway, ontology},
abstract = {Background
Integrated care enhanced with information technology has emerged as a means to transform health services to meet the long-term care needs of patients with chronic diseases. However, the feasibility of applying integrated care to the emerging “three-manager” mode in China remains to be explored. Moreover, few studies have attempted to integrate multiple types of chronic diseases into a single system.
Objective
The aim of this study was to develop a coordinated telehealth system that addresses the existing challenges of the “three-manager” mode in China while supporting the management of single or multiple chronic diseases.
Methods
The system was designed based on a tailored integrated care model. The model was constructed at the individual scale, mainly focusing on specifying the involved roles and responsibilities through a universal care pathway. A custom ontology was developed to represent the knowledge contained in the model. The system consists of a service engine for data storage and decision support, as well as different forms of clients for care providers and patients. Currently, the system supports management of three single chronic diseases (hypertension, type 2 diabetes mellitus, and chronic obstructive pulmonary disease) and one type of multiple chronic conditions (hypertension with type 2 diabetes mellitus). A retrospective study was performed based on the long-term observational data extracted from the database to evaluate system usability, treatment effect, and quality of care.
Results
The retrospective analysis involved 6964 patients with chronic diseases and 249 care providers who have registered in our system since its deployment in 2015. A total of 519,598 self-monitoring records have been submitted by the patients. The engine could generate different types of records regularly based on the specific care pathway. Results of the comparison tests and causal inference showed that a part of patient outcomes improved after receiving management through the system, especially the systolic blood pressure of patients with hypertension (P<.001 in all comparison tests and an approximately 5 mmHg decrease after intervention via causal inference). A regional case study showed that the work efficiency of care providers differed among individuals.
Conclusions
Our system has potential to provide effective management support for single or multiple chronic conditions simultaneously. The tailored closed-loop care pathway was feasible and effective under the “three-manager” mode in China. One direction for future work is to introduce advanced artificial intelligence techniques to construct a more personalized care pathway.}
}
@article{SAAD20223439,
title = {Towards Improved Visualization and Optimization of Aquaculture Production Process},
journal = {Procedia Computer Science},
volume = {207},
pages = {3439-3448},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.531},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922014296},
author = {Aya Saad and Oscar Nissen and Espen Eilertsen and Finn Olav Bjørnson and Tore Norheim Hagtun and Odd-Gunnar Aspaas and Alexia Artemis Baikas and Sveinung Johan Ohrem},
keywords = {Knowledge representation, graph database, aquaculture production process},
abstract = {Aquaculture is one of the largest, and fastest growing industries in Norway. Recently, the industry has experienced significant development in the daily operations acquiring new technologies and systems that capture data and automate the different processes. These emerging technologies enable the generation of enormous amounts of data from sensors in the fish cages, cameras, boats, and feeding control rooms. Additional information relevant to the aquaculture industry is based on e-mails, manual notes, or intrinsic experiences and knowledge exchanges. One of the critical aspects of successful fish farming operation management, which is yet not achieved, is to allow domain experts to gain insight into the interconnection between the broad spectrum of heterogeneous data currently realized. This paper describes a framework for storing and retrieving critical information connected to fish farming based on a graph database approach. The overall architecture is presented with detailed illustrations of how data is visualized and interpreted through a user-friendly interface. Accordingly, this work demonstrates how aquaculture users can benefit from the system to identify possible connections in the data and reveal previously undiscovered causalities and correlations that suggest optimal actions. Further, studies and evaluations of the querying system are conducted, evaluating the capability of the proposed design to process complex relationships. This work showcases that the system helps fish farmers and aquaculture users gain knowledge, reveal hidden links in the data, and improve aquaculture operations.}
}
@article{BOOSHEHRI2021100074,
title = {Introducing the Open Energy Ontology: Enhancing data interpretation and interfacing in energy systems analysis},
journal = {Energy and AI},
volume = {5},
pages = {100074},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100074},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000288},
author = {Meisam Booshehri and Lukas Emele and Simon Flügel and Hannah Förster and Johannes Frey and Ulrich Frey and Martin Glauer and Janna Hastings and Christian Hofmann and Carsten Hoyer-Klick and Ludwig Hülk and Anna Kleinau and Kevin Knosala and Leander Kotzur and Patrick Kuckertz and Till Mossakowski and Christoph Muschner and Fabian Neuhaus and Michaja Pehl and Martin Robinius and Vera Sehn and Mirjam Stappel},
keywords = {Collaborative ontology development, Linked open data, Metadata annotation, Energy systems analysis},
abstract = {Heterogeneous data, different definitions and incompatible models are a huge problem in many domains, with no exception for the field of energy systems analysis. Hence, it is hard to re-use results, compare model results or couple models at all. Ontologies provide a precisely defined vocabulary to build a common and shared conceptualisation of the energy domain. Here, we present the Open Energy Ontology (OEO) developed for the domain of energy systems analysis. Using the OEO provides several benefits for the community. First, it enables consistent annotation of large amounts of data from various research projects. One example is the Open Energy Platform (OEP). Adding such annotations makes data semantically searchable, exchangeable, re-usable and interoperable. Second, computational model coupling becomes much easier. The advantages of using an ontology such as the OEO are demonstrated with three use cases: data representation, data annotation and interface homogenisation. We also describe how the ontology can be used for linked open data (LOD).}
}
@article{VAS20181032,
title = {Implementing connectivism by semantic technologies for self-directed learning},
journal = {International Journal of Manpower},
volume = {39},
number = {8},
pages = {1032-1046},
year = {2018},
issn = {0143-7720},
doi = {https://doi.org/10.1108/IJM-10-2018-0330},
url = {https://www.sciencedirect.com/science/article/pii/S0143772018000033},
author = {Réka Vas and Christian Weber and Dimitris Gkoumas},
keywords = {Networks, Ontologie, Connectivism, Self-directed learning},
abstract = {Purpose
Connectivism has been proposed to explain the impact of new technologies on learning. According to this approach, learning may occur even outside the individual within an organization or a system. Learning objectives are not defined in advance and learning requires the ability to form connections and use networks to find the required knowledge. The connections by which individuals can learn are more important than what they currently know. The purpose of this paper is to investigate if a measure, rating the importance of concepts, can be derived from a network representation of the learning domain and if highly connected concepts – with high importance value – can describe whether information is explored in such ways as assumed by connectivism.
Design/methodology/approach
The authors empirically examined if the proposed measure can provide insight on the role of connections in learning and explain the reasons behind passing certain parts of a test using a linear regression model.
Findings
The results are twofold. First, an implementation of the information exploration principle of connectivism has been introduced, applying semantic technologies and the importance measure. Second, although no significant effects could be isolated, trends in performance improvement concerning highly important concepts were identified.
Originality/value
However, connectivism has been known since 2005, it is still lacking for successful implementations. The presented approach of a concept importance measure is a promising starting point by providing means of connected learning, enabling individuals to effectively improve their personal abilities to better fit job demand.}
}
@article{NASRABADI2024123551,
title = {The implication of user-generated content in new product development process: A systematic literature review and future research agenda},
journal = {Technological Forecasting and Social Change},
volume = {206},
pages = {123551},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123551},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524003470},
author = {Mohamadreza Azar Nasrabadi and Yvan Beauregard and Amir Ekhlassi},
keywords = {User-generated content, New product development, Product design, Product innovation, Systematic literature review, Social media},
abstract = {This study aims to provide a comprehensive overview of the current state of user-generated content (UGC) research within the context of new product development (NPD). A systematic literature review (SLR) was conducted across three prominent databases, namely Web of Science, Scopus, and Science Direct, using keywords to identify relevant articles. 5585 of 13,381 articles were deemed relevant following the application of inclusion and exclusion criteria. These articles were then thoroughly analyzed to create a comprehensive review of the topic. The selection process involved evaluating the titles and abstracts of all publications that were discovered, and carefully choosing 136 articles for full-text review. From these, 58 articles were ultimately selected for detailed analysis in this study. The study highlights the role of UGC in augmenting NPD process and identifies potential areas for future research based on evidence derived from an SLR of articles published between 2012 and 2023. The research methodologies adopted in this paper involve descriptive analysis and TCM framework (T-themes, C-contexts, and M-methodologies). Finally, the article concludes by shedding light on its potential applications by providing four themes and highlighting the importance of future research in the field with five propositions.}
}
@article{BITSCH2022577,
title = {Dynamic adaption in cyber-physical production systems based on ontologies},
journal = {Procedia Computer Science},
volume = {200},
pages = {577-584},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.255},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002642},
author = {Günter Bitsch and Pascal Senjic and Jeremy Askin},
keywords = {Cyber-Physical Production Systems (CPPSs), Ontology, Adaptability, Flexibility},
abstract = {The paradigmatic shift of production systems towards Cyber-Physical Production Systems (CPPSs) requires the development of flexible and decentralized approaches. In this way, such systems enable manufacturers to respond quickly and accurately to changing requirements. However, domain-specific applications require the use of suitable conceptualizations. The issue at hand, when using various conceptualizations is the interoperability of different ontologies. To achieve flexibility and adaptability in CPPSs though requires overcoming interoperability issues within CPPSs. This paper presents an approach to increase flexibility and adaptability in CPPSs while addressing the interoperability issue. In this work, OWL ontologies conceptualize domain knowledge. The Intelligent Manufacturing Knowledge Ontology Repository (IMKOR) connects the domain knowledge in different ontologies. Testing if adaptions in one ontology within the IMKOR provide knowledge to the whole IMKOR. The tests showed, positive results and the repository makes the knowledge available to the whole CPPS. Furthermore, an increase in flexibility and adaptability was noticed.}
}
@article{LI2023102978,
title = {CoAxNN: Optimizing on-device deep learning with conditional approximate neural networks},
journal = {Journal of Systems Architecture},
volume = {143},
pages = {102978},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2023.102978},
url = {https://www.sciencedirect.com/science/article/pii/S1383762123001571},
author = {Guangli Li and Xiu Ma and Qiuchu Yu and Lei Liu and Huaxiao Liu and Xueying Wang},
keywords = {On-device deep learning, Efficient neural networks, Model approximation and optimization},
abstract = {While deep neural networks have achieved superior performance in a variety of intelligent applications, the increasing computational complexity makes them difficult to be deployed on resource-constrained devices. To improve the performance of on-device inference, prior studies have explored various approximate strategies, such as neural network pruning, to optimize models based on different principles. However, when combining these approximate strategies, a large parameter space needs to be explored. Meanwhile, different configuration parameters may interfere with each other, damaging the performance optimization effect. In this paper, we propose a novel model optimization framework, CoAxNN, which effectively combines different approximate strategies, to facilitate on-device deep learning via model approximation. Based on the principles of different approximate optimizations, our approach constructs the design space and automatically finds reasonable configurations through genetic algorithm-based design space exploration. By combining the strengths of different approximation methods, CoAxNN enables efficient conditional inference for models at runtime. We evaluate our approach by leveraging state-of-the-art neural networks on a representative intelligent edge platform, Jetson AGX Orin. The experimental results demonstrate the effectiveness of CoAxNN, which achieves up to 1.53× speedup while reducing energy by up to 34.61%, with trivial accuracy loss on CIFAR-10/100 and CINIC-10 datasets.}
}
@article{PERNISCH2021100658,
title = {Beware of the hierarchy — An analysis of ontology evolution and the materialisation impact for biomedical ontologies},
journal = {Journal of Web Semantics},
volume = {70},
pages = {100658},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100658},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000330},
author = {Romana Pernisch and Daniele Dell’Aglio and Abraham Bernstein},
keywords = {Ontology evolution, Materialisation, Evolution impact, Ontology change},
abstract = {Ontologies are becoming a key component of numerous applications and research fields. But knowledge captured within ontologies is not static. Some ontology updates potentially have a wide ranging impact; others only affect very localised parts of the ontology and their applications. Investigating the impact of the evolution gives us insight into the editing behaviour but also signals ontology engineers and users how the ontology evolution is affecting other applications. However, such research is in its infancy. Hence, we need to investigate the evolution itself and its impact on the simplest of applications: the materialisation. In this work, we define impact measures that capture the effect of changes on the materialisation. In the future, the impact measures introduced in this work can be used to investigate how aware the ontology editors are about consequences of changes. By introducing five different measures, which focus either on the change in the materialisation with respect to the size or on the number of changes applied, we are able to quantify the consequences of ontology changes. To see these measures in action, we investigate the evolution and its impact on materialisation for nine open biomedical ontologies, most of which adhere to the EL++ description logic. Our results show that these ontologies evolve at varying paces but no statistically significant difference between the ontologies with respect to their evolution could be identified. We identify three types of ontologies based on the types of complex changes which are applied to them throughout their evolution. The impact on the materialisation is the same for the investigated ontologies, bringing us to the conclusion that the effect of changes on the materialisation can be generalised to other similar ontologies. Further, we found that the materialised concept inclusion axioms experience most of the impact induced by changes to the class inheritance of the ontology and other changes only marginally touch the materialisation.}
}
@article{YUAN2025111635,
title = {Efficient dehazing network based on mix structure for single image with uneven haze distribution},
journal = {Engineering Applications of Artificial Intelligence},
volume = {159},
pages = {111635},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111635},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625016379},
author = {Kangle Yuan and Jianguo Wei and Wenhuan Lu},
keywords = {Image dehazing, Mix structure, Uneven haze distribution, Parallel attention mechanism, Hierarchical vision transformer, Fusion structure},
abstract = {Although the defogging algorithm based on convolutional neural networks has made significant progress on synthetic uniform foggy datasets, it still exhibits subpar performance on real non-uniform foggy images. In recent years, the transformer network has been applied in the field of image dehazing and has achieved good results in removing haze from non-uniform hazy images. However, two main issues remain: The neglect of the multi-scale characteristics of the image; And the lack of effective strategies to better combine the convolutional structure with the transformer. In this paper, we propose an efficient image dehazing network framework based on a novel hybrid structure. Specifically, the mixed structure block consists of a convolutional component utilizing a parallel attention mechanism and a Transformer architecture. This design effectively captures extensive areas of blur while simultaneously restoring texture details. Additionally, it takes into account the uneven distribution of haze, thereby addressing the challenges associated with removing uneven fog in single images more effectively.Meanwhile, we propose a fusion structure that comprises a skip branch and a main branch, enabling dynamic adjustment of the receptive field size and selection of the appropriate convolution kernel. The experimental results demonstrate that the dehazing algorithm proposed by us outperforms existing methods in terms of dehazing performance.}
}
@article{SCHNEIDER2020103402,
title = {Design of knowledge-based systems for automated deployment of building management services},
journal = {Automation in Construction},
volume = {119},
pages = {103402},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103402},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520309821},
author = {Georg F. Schneider and Georgios D. Kontes and Haonan Qiu and Filipe J. Silva and Mircea Bucur and Jakub Malanik and Zdenek Schindler and Panos Andriopolous and Pablo {de Agustin-Camacho} and Ander Romero-Amorrortu and Gunnar Grün},
keywords = {Building management services, Knowledge-based systems, Energy efficiency, Knowledge engineering, Ontology},
abstract = {Despite its high potential, the building's sector lags behind in reducing its energy demand. Tremendous savings can be achieved by deploying building management services during operation, however, the manual deployment of these services needs to be undertaken by experts and it is a tedious, time and cost consuming task. It requires detailed expert knowledge to match the diverse requirements of services with the present constellation of envelope, equipment and automation system in a target building. To enable the widespread deployment of these services, this knowledge-intensive task needs to be automated. Knowledge-based methods solve this task, however, their widespread adoption is hampered and solutions proposed in the past do not stick to basic principles of state of the art knowledge engineering methods. To fill this gap we present a novel methodological approach for the design of knowledge-based systems for the automated deployment of building management services. The approach covers the essential steps and best practices: (1) representation of terminological knowledge of a building and its systems based on well-established knowledge engineering methods; (2) representation and capturing of assertional knowledge on a real building portfolio based on open standards; and (3) use of the acquired knowledge for the automated deployment of building management services to increase the energy efficiency of buildings during operation. We validate the methodological approach by deploying it in a real-world large-scale European pilot on a diverse portfolio of buildings and a novel set of building management services. In addition, a novel ontology, which reuses and extends existing ontologies is presented.}
}
@article{RASMUSSEN2019102956,
title = {Managing interrelated project information in AEC Knowledge Graphs},
journal = {Automation in Construction},
volume = {108},
pages = {102956},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102956},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519300378},
author = {Mads Holten Rasmussen and Maxime Lefrançois and Pieter Pauwels and Christian Anker Hviid and Jan Karlshøj},
keywords = {Linked data, Building information modelling, Complex design, Ontology, Inference, Information exchange, BIM, AEC Knowledge Graph, Linked building data},
abstract = {In the architecture, engineering and construction (AEC) industry stakeholders from different companies and backgrounds collaborate in realising a common goal being some physical structure. The exact goal is typically not known from the beginning, and throughout all design stages, new decisions are made - similarly to other design industries [1]. As a result, the design must adapt and subsequent consequences follow. With working methods being predominantly document-centric, highly interrelated and rapidly changing design data in a complex network of decisions, requirements and product specifications is primarily captured in static documents. In this paper, we consider a purely data-driven approach based on semantic web technologies and an earlier proposed Ontology for Property Management (OPM). The main contribution of this work consists of extensions for OPM to account for new competency questions including the description of property reliability and the reasoning logic behind derived properties. The secondary contribution is the specification of a homogeneous way to generate parametric queries for managing an OPM-compliant AEC Knowledge Graph (AEC-KG). A software library for operating an OPM-compliant AEC-KG is further presented in the form of an OPM Query Generator (OPM-QG). The library generates SPARQL 1.1 queries to query and manipulate construction project Knowledge Graphs represented using OPM. The OPM ontology aligns with latest developments in the W3C Community Group on Linked Building Data and suggests an approach to working with design data in a distributed environment using separate graphs for explicit facts and for materialised, deduced data. Finally, we evaluate the suggested approach using an open-source software artefact developed using OPM and OPM-QG, demonstrated online with an actual building Knowledge Graph. The particular design task evaluated is performing heat loss calculations for spaces of a future building using an AEC-KG described using domain- and project specific extensions of the Building Topology Ontology (BOT) in combination with OPM. With this work, we demonstrate how a typical engineering task can be accomplished and managed in an evolving design environment, thereby providing the engineers with insights to support decision making as changes occur. The application uses a strict division between the client viewer and the actual data model holding design logic, and can easily be extended to support other design tasks.}
}
@article{RANYA20233479,
title = {Application and evaluation of sentence embedding and clustering methods in the context of concept hierarchy construction},
journal = {Procedia Computer Science},
volume = {225},
pages = {3479-3487},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.343},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923015016},
author = {El Hadri Ranya and Cimpan Sorana and Damas Luc and Boissière Julien},
keywords = {Concept hierarchy, Evaluation, Sentence Embedding, Clustering, Fuzzy sets},
abstract = {Concept hierarchies, as part of knowledge representation methods, play an important role in supporting the exchange and sharing of information. We developed and automated a concept hierarchies construction process which includes several artificial intelligence techniques. When automating their construction from an existing, more or less structured, body of knowledge, the evaluation of the resulting concept hierarchy is an important step. We propose an approach for concept hierarchy construction (CHC) from short sentences, that makes use of methods like Sentence Embedding, Clustering, and Automatic Labeling to create a hierarchical representation consisting of three layers. Our major focus in this paper is not on the algorithms used but on their evaluation using manual clustering by experts and fuzzy sets.}
}
@article{NING2021102303,
title = {Differential privacy protection on weighted graph in wireless networks},
journal = {Ad Hoc Networks},
volume = {110},
pages = {102303},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2020.102303},
url = {https://www.sciencedirect.com/science/article/pii/S1570870520306612},
author = {Bo Ning and Yunhao Sun and Xiaoyu Tao and Guanyu Li},
keywords = {Wireless networks, Weighted graph, Privacy protection, Differential privacy},
abstract = {With the development of 5G communication technology, the Internet of Things technology has ushered in the development opportunity. In the application of Internet of Things, spatial and social relations can be used to provide users with convenience in life and work, meanwhile there is also the risk of personal privacy disclosure. The data transmitted in the wireless network contains a large number of graph structure data, and the edge weight in weighted graph increases the risk of privacy disclosure, therefore in this paper we design a privacy protection algorithm for weighted graph, and adopts the privacy protection model to realize the privacy protection of edge weight and graph structure. Firstly, the whole graph sets are disturbed and the noises are added during the process of graph generation. Secondly, the privacy budget is allocated to protect the weight values of edges. The graph is encoded to deal with the structure of graph conveniently without separating from the information of edges, and then the disturbed edge weight is integrated into the graph. After that the privacy protection of the graph structure is realized in the process of frequent graph mining combined with differential privacy. Finally, the algorithm proposed in this paper is validated by experiments.}
}
@article{LU2018128,
title = {Resource virtualization: A core technology for developing cyber-physical production systems},
journal = {Journal of Manufacturing Systems},
volume = {47},
pages = {128-140},
year = {2018},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0278612518300657},
author = {Yuqian Lu and Xun Xu},
keywords = {Smart factory, Cyber-physical production system, Resource virtualization, Digital twin, Ontology, Semantic web},
abstract = {Smart factory in the context of Industry 4.0 is the next wave of smart manufacturing solution to empower companies to rapidly configure manufacturing facilities and processes to enable the fast production of individualized products at change scales. A key enabling technology for developing a smart factory is resource virtualization or creation of digital twins. The presented research fills the gap that the industry needs a practical methodology to enable themselves to easily virtualize their manufacturing assets for developing a smart factory solution. A test-driven resource virtualization framework is proposed as the recommendation for the industry to adopt to create digital twins for a smart factory. The proposed framework draws inspiration from past resource virtualization outcomes with special attention paid to the usability of the proposed framework in a business environment. It provides a straightforward process for companies to create digital twins by specifying the digital twin hierarchy, the information to be modeled, and the modeling method. To validate the proposed framework, a case study was undertaken at an international company, to create digital twins for all their manufacturing resources. The testing result showed that the proposed resource virtualization framework and developed tools are easy to use in a practical business environment to virtualize complex factory setups in the cyberspace.}
}
@article{SALGUERO20181,
title = {Ontology-based feature generation to improve accuracy of activity recognition in smart environments},
journal = {Computers & Electrical Engineering},
volume = {68},
pages = {1-13},
year = {2018},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2018.03.048},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617315483},
author = {A.G. Salguero and M. Espinilla},
keywords = {Activity recognition, Smart environments, Ambient assisted living (AAL), Activities of Daily Living (ADL), Ontology, Data-Driven approaches, Knowledge-Driven approaches},
abstract = {In recent years, many techniques have been proposed for automatic recognition of Activities of Daily Living from smart home sensor data. However, classifiers usually use features created ad hoc. In this work, the use of ontologies is proposed for the fully automatic generation of these features. The process consists of converting the original dataset into an ontology and then combine all the concepts and relations in that ontology to obtain relevant class expressions. The high formalization of ontologies allows us to reduce the search space by discarding many meaningless expressions, such as contradictory or unsatisfiable expressions. The relevant class expressions are then used as features by the classifiers to build the classification model. To validate our proposal, we have used as reference the results obtained by four different classification algorithms that use the most commonly used features.}
}
@article{LEGUILLARME2023103497,
title = {A practical approach to constructing a knowledge graph for soil ecological research},
journal = {European Journal of Soil Biology},
volume = {117},
pages = {103497},
year = {2023},
issn = {1164-5563},
doi = {https://doi.org/10.1016/j.ejsobi.2023.103497},
url = {https://www.sciencedirect.com/science/article/pii/S116455632300033X},
author = {Nicolas {Le Guillarme} and Wilfried Thuiller},
keywords = {Data integration, Knowledge graph, Ontology, Reasoning, Soil ecology},
abstract = {With the rapid accumulation of biodiversity data, data integration has emerged as a hot topic in soil ecology. Data integration has indeed the potential to advance our knowledge of global patterns in soil biodiversity by facilitating large-scale meta-analytical studies of soil ecosystems. However, ecologists are still poorly equipped when it comes to integrating disparate datasets. In recent years, knowledge graphs have emerged as a powerful tool for integrating large amounts of distributed heterogeneous data while making these data more easily interpretable by humans and computers. This paper presents a practical approach to constructing a biodiversity knowledge graph from heterogeneous and distributed (semi-)structured data sources. To illustrate our approach, we integrate several datasets on the trophic ecology of soil organisms into a trophic knowledge graph and show how both explicit and implicit information can be retrieved from the graph to support multi-trophic studies.}
}
@article{DAMICO2020803,
title = {BIM And GIS Data Integration: A Novel Approach Of Technical/Environmental Decision-Making Process In Transport Infrastructure Design},
journal = {Transportation Research Procedia},
volume = {45},
pages = {803-810},
year = {2020},
note = {Transport Infrastructure and systems in a changing world. Towards a more sustainable, reliable and smarter mobility.TIS Roma 2019 Conference Proceedings},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.02.090},
url = {https://www.sciencedirect.com/science/article/pii/S235214652030140X},
author = {Fabrizio D’Amico and Alessandro Calvi and Eleonora Schiattarella and Mauro Di Prete and Valerio Veraldi},
keywords = {BIM, GIS, Infrastructures, Airport, Environment, Data Integration},
abstract = {The European Directive 2014/24/EU and its recent Italian transposition law DM 560/2017 encourage an extensive use of BIM-based practices in transport infrastructure design. Therefore, a shift from the traditional design approach towards a shared and highly integrated model, capable of including the various design phases along with economic, operational and environmental concerns, is observed. In such a framework, this work evaluates the benefits returning from the integration between geospatially-referenced data and the BIM models for a more aware design approach. The major aim of this study is to underline the potential of an interoperable and shared model supplemented by GIS data, in minimizing or definitely removing the possible conflicts that typically arise between the infrastructure design and environmental constraints. Particularly, thanks to both the simultaneous assessment of each environmental component and the evaluation of the different project configurations, this methodology can provide an integrated technical/environmental overview of the design. As a result, it allows for immediately verifying the project to comply with the national minimum environmental criteria, which are mandatory for contractors according to the Italian environmental law n° 221/2015 and the new Italian Public Procurement Code. The proposed approach was finally tested on an airport infrastructure. Preliminary results have shown viability of the data management model for supporting designer’s choices in the various project phases, thereby proving this methodology to be worthy for implementation in infrastructure design procedures.}
}
@article{PREVENTIS2021275,
title = {CLONE: Collaborative Ontology Editor as a Service in the Cloud},
journal = {Procedia Computer Science},
volume = {184},
pages = {275-282},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921007791},
author = {Alexandros Preventis and Euripides G.M. Petrakis},
keywords = {Ontology, Collaborative editor, Service Oriented Architecture, Cloud Computing},
abstract = {The evolution of Web and cloud services technology has facilitated collaboration on the Web, providing the means for concurrent editing, change tracking and storing files in the cloud (e.g. Google Docs, Office 365). Ontology development teams could greatly benefit from this technology, that until now have been applied mainly to document processing. We introduce CLONE, a Web-based ontology editor that runs in the cloud and provides a real-time collaborative environment for creating and editing ontologies. CLONE is designed as a service-oriented architecture taking advantages of the easy extensibility and scalability features of this approach. CLONE provides all the essential features of stand-alone ontology editors, as well as significant collaboration features, including concurrent editing, editing history, team conversations and role-based access-authorization mechanisms.}
}
@article{RODRIGUEZREVELLO2023120239,
title = {KNIT: Ontology reusability through knowledge graph exploration},
journal = {Expert Systems with Applications},
volume = {228},
pages = {120239},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120239},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423007418},
author = {Jorge Rodríguez-Revello and Cristóbal Barba-González and Maciej Rybinski and Ismael Navas-Delgado},
keywords = {Life sciences, Ontology, Ontology learning, Knowledge graphs},
abstract = {Ontologies have become a standard for knowledge representation across several domains. In Life Sciences, numerous ontologies have been introduced to represent human knowledge, often providing overlapping or conflicting perspectives. These ontologies are usually published as OWL or OBO, and are often registered in open repositories, e.g., BioPortal. However, the task of finding the concepts (classes and their properties) defined in the existing ontologies and the relationships between these concepts across different ontologies – for example, for developing a new ontology aligned with the existing ones – requires a great deal of manual effort in searching through the public repositories for candidate ontologies and their entities. In this work, we develop a new tool, KNIT, to automatically explore open repositories to help users fetch the previously designed concepts using keywords. User-specified keywords are then used to retrieve matching names of classes or properties. KNIT then creates a draft knowledge graph populated with the concepts and relationships retrieved from the existing ontologies. Furthermore, following the process of ontology learning, our tool refines this first draft of an ontology. We present three BioPortal-specific use cases for our tool. These use cases outline the development of new knowledge graphs and ontologies in the sub-domains of biology: genes and diseases, virome and drugs.}
}
@article{KUTT2023119968,
title = {Loki – the semantic wiki for collaborative knowledge engineering},
journal = {Expert Systems with Applications},
volume = {224},
pages = {119968},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119968},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423004700},
author = {Krzysztof Kutt and Grzegorz J. Nalepa},
keywords = {Knowledge engineering, Semantic wiki, Software engineering, Unit tests, Prolog},
abstract = {We present Loki, a semantic wiki designed to support the collaborative knowledge engineering process with the use of software engineering methods. Designed as a set of DokuWiki plug-ins, it provides a variety of knowledge representation methods, including semantic annotations, Prolog clauses, and business processes and rules oriented to specific tasks. Knowledge stored in Loki can be retrieved via SPARQL queries, in-line Semantic MediaWiki-like queries, or Prolog goals. Loki includes a number of useful features for a group of experts and knowledge engineers developing the wiki, such as knowledge visualization, ontology storage, or code hint and completion mechanism. Reasoning unit tests are also introduced to validate knowledge quality. The paper is complemented by the formulation of the collaborative knowledge engineering process and the description of experiments performed during Loki development to evaluate its functionality. Loki is available as free software at https://loki.re.}
}
@article{TERZIYAN20241388,
title = {Taxonomy-Informed Neural Networks for Smart Manufacturing},
journal = {Procedia Computer Science},
volume = {232},
pages = {1388-1399},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.01.137},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924001376},
author = {Vagan Terziyan and Oleksandra Vitko},
keywords = {neural networks, machine learning, informed machine learning, physics-informed neural networks, taxonomy, Industry 4.0},
abstract = {A neural network (NN) is known to be an efficient and learnable tool supporting decision-making processes particularly in Industry 4.0. The majority of NNs are data-driven and, therefore, depend on training data quantity and quality. The current trend in enhancing data-driven models with knowledge-based models promises to enable effective NNs with less data. So-called physics-informed NNs use additional knowledge from computational science to improve NN training. Quite much of the knowledge is available as logical constraints from domain ontologies, and NNs may benefit from using it. In this paper, we study the concept of Taxonomy-Informed NN (TINN), which combines data-driven training of NNs with ontological knowledge. We study different patterns of NN training with additional knowledge on class-subclass hierarchies and instance-class relationships with potential for federated learning. Our experiments show that additional knowledge, which influences TINNs’ training process through the loss function at backpropagation, improves the quality of trained models.}
}
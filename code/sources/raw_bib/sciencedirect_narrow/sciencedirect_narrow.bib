@article{STOREY2025102480,
title = {Large language models for conceptual modeling: Assessment and application potential},
journal = {Data & Knowledge Engineering},
volume = {160},
pages = {102480},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102480},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000758},
author = {Veda C. Storey and Oscar Pastor and Giancarlo Guizzardi and Stephen W. Liddle and Wolfgang Maaß and Jeffrey Parsons and Jolita Ralyté and Maribel Yasmina Santos},
keywords = {Conceptual modeling, Large language models, Artificial intelligence, Semantics},
abstract = {Large Language Models (LLMs) are being rapidly adopted for many activities in organizations, business, and education. Included in their applications are capabilities to generate text, code, and models. This leads to questions about their potential role in the conceptual modeling part of information systems development. This paper reports on a panel presented at the 43rd International Conference on Conceptual Modeling where researchers discussed the current and potential role of LLMs in conceptual modeling. The panelists discussed applications and interest levels and expressed both optimism and caution in the adoption of LLMs. Suggested is a need for much continued research by the conceptual modeling community on LLM development and their role in research and teaching.}
}
@article{WISNIEWSKI2019100534,
title = {Analysis of Ontology Competency Questions and their formalizations in SPARQL-OWL},
journal = {Journal of Web Semantics},
volume = {59},
pages = {100534},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.100534},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300617},
author = {Dawid Wiśniewski and Jedrzej Potoniec and Agnieszka Ławrynowicz and C. Maria Keet},
keywords = {Ontology Authoring, Competency Questions, SPARQL-OWL},
abstract = {Competency Questions (CQs) are natural language questions outlining and constraining the scope of knowledge represented in an ontology. Despite that CQs are a part of several ontology engineering methodologies, the actual publication of CQs for the available ontologies is very limited and even scarcer is the publication of their respective formalizations in terms of, e.g., SPARQL queries. This paper aims to contribute to addressing the myriad of engineering hurdles to using CQs in ontology development. A prerequisite to this is to understand the relation between CQs and the queries over the ontology. We use a new dataset of 234 competency questions and their SPARQL-OWL queries for several ontologies in different domains developed by different groups, and analysed the CQs in two principal ways. The first stage focused on a linguistic analysis of the natural language text itself, i.e., a lexico-syntactic analysis without any presuppositions of ontology elements, and a subsequent step of semantic analysis in order to find patterns. This increased diversity of CQ sources resulted in a 4-5-fold increase of hitherto published patterns, to 106 distinct CQ patterns, which have a limited subset of few patterns shared across the CQ sets from the different ontologies. Next, we analysed the relation between the found CQ patterns and their respective SPARQL-OWL patterns, which revealed that one CQ pattern may be realized by more than one SPARQL-OWL query pattern, and vice versa. These insights may contribute to establishing common practices, templates, automation, and user tools that will support CQ formulation, formalization, execution, and general management.}
}
@article{LOPES2022117291,
title = {Predicting the top-level ontological concepts of domain entities using word embeddings, informal definitions, and deep learning},
journal = {Expert Systems with Applications},
volume = {203},
pages = {117291},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117291},
url = {https://www.sciencedirect.com/science/article/pii/S095741742200656X},
author = {Alcides Gonçalves Lopes and Joel Luis Carbonera and Daniela Schimidt and Mara Abel},
keywords = {Ontology learning, Deep learning, Well-founded ontology},
abstract = {Ontology development is a challenging task that encompasses many time-consuming activities. One of these activities is the classification of the domain entities (concepts and instances) according to top-level concepts. This activity is usually performed manually by an ontology engineer. However, when the set of entities increases in size, associating each entity to the proper top-level ontological concept becomes challenging and requires a high level of expertise in both the target domain and ontology engineering. This paper proposes a deep learning approach that automatically classifies domain entities into top-level concepts using their informal definitions and the word embedding of the terms that represent them. From these inputs, we feed a deep neural network consisting of two modules: a feed-forward neural network and a bi-directional recurrent neural network with long short-term units. Our architecture combines both outputs of these modules into a dense layer and provides the probabilities of each candidate class. For validating our proposal, we have developed a dataset based on the OntoWordNet ontology, which provides a classification of WordNet synsets into concepts specified by DOLCE-lite-plus top-level ontology. Our experiments show that our proposal outperforms the baseline approaches by 6% regarding the F-score. In addition, our proposal is less affected by the polysemy in the terms that represent the domain entities than the compared approaches. Consequently, our proposal can consider more instances during its training than the baseline methods.}
}
@article{ROMANENKO2024102342,
title = {Evaluating quality of ontology-driven conceptual models abstractions},
journal = {Data & Knowledge Engineering},
volume = {153},
pages = {102342},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102342},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000661},
author = {Elena Romanenko and Diego Calvanese and Giancarlo Guizzardi},
keywords = {Conceptual model abstraction, Ontology-driven conceptual models, Quality evaluation of abstractions, Unified foundational ontology (UFO), FAIR model catalog, User studies in conceptual modeling},
abstract = {The complexity of an (ontology-driven) conceptual model highly correlates with the complexity of the domain and software for which it is designed. With that in mind, an algorithm for producing ontology-driven conceptual model abstractions was previously proposed. In this paper, we empirically evaluate the quality of the abstractions produced by it. First, we have implemented and tested the last version of the algorithm over a FAIR catalog of models represented in the ontology-driven conceptual modeling language OntoUML. Second, we performed three user studies to evaluate the usefulness of the resulting abstractions as perceived by modelers. This paper reports on the findings of these experiments and reflects on how they can be exploited to improve the existing algorithm.}
}
@article{JAVED2021106558,
title = {iMER: Iterative process of entity relationship and business process model extraction from the requirements},
journal = {Information and Software Technology},
volume = {135},
pages = {106558},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106558},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000422},
author = {Muhammad Javed and Yuqing Lin},
keywords = {Entity relationship model, Business process model, General requirements, User stories, Use case specification, Natural language processing},
abstract = {Context
Extracting conceptual models, e.g., entity relationship model or Business Process model, from software requirement document is an essential task in the software development life cycle. Business process model presents a clear picture of required system's functionality. Operations in business process model together with the data entity consumed, help the software developers to understand the database design and operations to be implemented. Researchers have been aiming at automatic extraction of these artefacts from the requirement document.
Objective
In this paper, we present an automated approach to extract the entity relationship and business process models from requirements, which are possibly in different formats such as general requirements, use case specification and user stories. Our approach is based on the efficient natural language processing techniques.
Method
It is an iterative approach of Models Extraction from the Requirements (iMER). iMER has multiple iterations where each iteration is to address a sub-problem. In the first iteration, iMER extracts the data entities and attributes. Second iteration is to find the relationships between data entities, while extracting cardinalities is in the third step. Business process model is generated in the fourth iteration, containing the external (actors’) and internal (system's) operations.
Evaluation
To evaluate the performance and accuracy of iMER, experiments are conducted on various formats of the requirement documents. Additionally, we have also evaluated our approaches using the requirement documents which been modified by shuffling the sentences and by merging with other requirements. Comparative study is also performed. The preliminary results show a noticeable improvement.
Conclusion
The iMER is an efficient automated iterative approach that is able to extract the conceptual models from the various formats of requirements.}
}
@article{WU2025338,
title = {Design of Intelligent Q&A System Based on Knowledge Graph Combined with Large Language Model},
journal = {Procedia Computer Science},
volume = {262},
pages = {338-347},
year = {2025},
note = {The 5th International Conference on Multi-modal Information Analytics (MMIA)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.05.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925019088},
author = {Quanquan Wu},
keywords = {Large Language Model, LLM, Knowledge Graph, KG, Intelligent Question-Answering, Q&A, System Design},
abstract = {LLM can transform natural language questions into structured queries, and the KG-based Q&A system can provide accurate and reliable answers. The combination of LLM and KG is the key technology to support the modern Q&A model. Through the two-wheel drive of structured knowledge and semantic understanding, the processing ability of complex problems is significantly improved, and the Q&A system is jointly promoted from the primary to the advanced intelligence evolution. In this paper, based on LLM and KG, knowledge distillation based LLM fusion KG technology is studied, so that the target model can absorb the advantages of both, not only have the language processing capability of large models, but also use the structured knowledge of KG to improve performance and interpretability. On this basis, the multi-layer architecture of intelligent Q&A system is designed, which is easy for developers to work together. The ClaudeKG model constructed in this paper is compared with DeepSeek and Doubao baseline models, and the function and performance are analyzed.}
}
@article{ROVETTO2020451,
title = {Orbital debris ontology, terminology, and knowledge modeling},
journal = {Journal of Space Safety Engineering},
volume = {7},
number = {3},
pages = {451-458},
year = {2020},
note = {Space Debris: The State of Art},
issn = {2468-8967},
doi = {https://doi.org/10.1016/j.jsse.2020.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S2468896720300720},
author = {Robert J. Rovetto and T.S. Kelso and Daniel A. O'Neil},
keywords = {Orbital debris, space debris, ontology, knowledge graph, semantic technology, artificial intelligence, information fusion, space object, knowledge representation and reasoning, knowledge model, semantic model, terminology, taxonomy, classification, metadata, linked data, orbit data, two-line element set, knowledge engineering, space situational awareness, space system ontology},
abstract = {ABSTRACT
The looming threat orbital debris poses to assets in orbit demands solutions. As the orbital population grows, so does the risk of collision, as well as the volume of associated data. It is, however, an opportunity for interdisciplinary innovation and cooperation. This paper focuses on the data management and knowledge modeling aspect of developing solutions for a sustainable and safe orbital space environment. An in-progress work to develop an orbital debris reference ontology [Rovetto, 2015] is summarized in order to discuss knowledge modeling for orbital debris. The formal analysis of this effort can contribute to standards development, as well as terminological and policy challenges. Leveraging the growing volumes of orbital debris and space situational awareness (SSA) data will create a more complete picture of the orbital space environment. Part of the solution will be: consistent and correct data interpretation, sharing orbital debris and SSA data in one form or another, terminology development & harmonization, and knowledge or domain modeling. To facilitate this, [Rovetto, 2015] proposed ontology development for the orbital debris and broader space domain. This paper summarizes concepts from that paper, and subsequently developed concepts [2-9]. See also https://purl.org/space-ontology and https://purl.org/space-ontology/odo. Ontology engineering is an interdisciplinary area of research related to knowledge representation and reasoning in artificial intelligence, model-based systems engineering, semantic technologies and the so-called semantic web. An ontology is effectively a computable and semantically rich terminology that presents a knowledge or domain model for a topic area. Expressions of knowledge, beliefs, or assertions are stored using formally defined terms. This knowledge base is reasoned over to yield answers to database queries, among other things. Ontologies have been developed in knowledge-based projects across various disciplines, and used for such things as search engines, chatbots, and enterprise knowledge graphs. Ontologies aim to support: interoperability, automated reasoning, data sharing and integration, data search and retrieval, and communicating the meaning of data. The Orbital Debris Ontology (ODO) [1], and related ontologies [Rovetto & Kelso 2016] [Rovetto 2016, 2017], were proposed to help achieve this. ODO, for instance, is intended as a domain ontology that can be used across federated databases, offering an explicitly specified set of concepts describing the orbital debris domain. Its meaning-rich taxonomy will provide a sharable semantics for orbital debris data to, in part, consistently communicate the meaning of data to both humans and machines, and tag data elements in space object catalogs to help afford automated reasoning tasks, decision support, knowledge discovery, and information integration. ODO and the SSA ontology (SSAO) [2] is part of the overall Orbital Space Ontology concept, which is conceived as a unifying domain reference ontology. It aims to provide a knowledge representation structure of orbital space, a common semantic model, and develop a sharable terminology. Collectively this will provide common meaning for datasets, a high-level taxonomy or classification for orbital space objects, and a means to characterize space objects. Ongoing efforts have included using visualizations, R, JSON-LD, and contemporary semantic technologies. Potential applications include web-based platforms, web apps, visualizations, modeling and simulations. Community input and participation may yield a more widely understood domain model as well as facilitate terminological standards. For example, conceptual, terminological, and ontological analysis can make helpful contributions to such efforts as the Space Debris Mitigation Requirements by developing more precise, consistent and coherent terms and definitions. Other projects can also use ODO (and its related ontologies) as a common knowledge model, metadata set, taxonomy or vocabulary. Readers interested in supporting development are encouraged to make contact.}
}
@article{FERNANDEZIZQUIERDO202289,
title = {Ontology verification testing using lexico-syntactic patterns},
journal = {Information Sciences},
volume = {582},
pages = {89-113},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521009324},
author = {Alba Fernández-Izquierdo and Raul García-Castro},
keywords = {Ontology testing, Ontology verification, Ontology requirements},
abstract = {Ontology verification refers to the activity where an ontology is tested against its ontology requirements to ensure that it is built correctly in compliance with its ontology requirements specification. Therefore, it is an important activity that should be performed in any ontology development process. Since manual verification can be a time-consuming and repetitive task, testing processes to automatically verify an ontology facilitate this activity. Moreover, the involvement of not only ontology engineers during the ontology verification process, but also domain experts and users, can provide valuable feedback to avoid misunderstandings and lack of information. This paper proposes a method for ontology verification that defines the testing activities to be performed. The method uses a testing language based on lexico-syntactic patterns to facilitate the definition of tests and an ontology to store and publish such tests. Moreover, this verification testing method proposes an online tool to execute tests on one or more ontologies. The method was compared in terms of time and errors by user evaluation with other tools for ontology verification; the evaluation showed that the tools that use testing languages had better results in terms of reducing errors in the verification activity compared to the tools that do not.}
}
@article{VISHWAKARMA2025102468,
title = {Automatic query expansion for enhancing document retrieval system in healthcare application using GAN based embedding and hyper-tuned DAEBERT algorithm},
journal = {Data & Knowledge Engineering},
volume = {160},
pages = {102468},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102468},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000631},
author = {Deepak Vishwakarma and Suresh Kumar},
keywords = {Automatic query expansion, Information retrieval, Generative Adversial Network, Modified Page Ranking Algorithm, Proximity based Keyword Extraction},
abstract = {Query expansion is a useful technique for improving document retrieval systems' dependability and performance. Search engines frequently employ query expansion strategies to improve Information Retrieval (IR) performance and elucidate users' information requirements. Although there are several methods for automatically expanding queries, the list of documents that are returned can occasionally be lengthy and contain a lot of useless information, particularly when searching the Web. As the size of medical document grows, Automatic Query Expansion might struggle with efficiency and real-time application. Thus, Hyper-Tuned Dual Attention Enhanced Bi-directional Encoder Representation from Transformers (HT-DAEBERT) with automatic ranking based query expansion system is created for enhancing medical document retrieval system. Initially, the user's query from the medical corpus document was collected, and it was augmented using the Generative Adversarial Network (GAN) approach. Then augmented text is pre-processed to improve the original text's quality through tokenization, acronym expansion, stemming, stop word removal, hyperlink removal, and spell correction. After that, Keywords are extracted using the Proximity-based Keyword Extraction (PKE) technique from the pre-processed text. Afterwards, the words are converted into vector form by utilizing the Hyper-Tuned Dual Attention Enhanced Bi-directional Encoder Representation from Transformers (HT-DAEBERT) model. In DAEBERT, key parameters such as dropout rate and weight decay were optimally selected by using the Election Optimization Algorithm (EOA). After that, a ranking-based query expansion approach was employed to enhance the document retrieval system. The proposed method achieves an accuracy of 97.60 %, a Hit Rate of 98.30 %, a PPV of 93.40 %, an F1-Score of 95.79 %, and an NPV of 97.50 %. This approach improves the accuracy and relevance of document retrieval in healthcare, potentially leading to better patient care and enhanced clinical outcomes.}
}
@article{VALCALVO2025104042,
title = {OntoGenix: Leveraging Large Language Models for enhanced ontology engineering from datasets},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104042},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104042},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004011},
author = {Mikel Val-Calvo and Mikel {Egaña Aranguren} and Juan Mulero-Hernández and Ginés Almagro-Hernández and Prashant Deshmukh and José Antonio Bernabé-Díaz and Paola Espinoza-Arias and José Luis Sánchez-Fernández and Juergen Mueller and Jesualdo Tomás Fernández-Breis},
keywords = {Knowledge graphs, Large Language Models, Ontology engineering},
abstract = {Knowledge Graphs integrate data from multiple, heterogeneous sources, using ontologies to facilitate data interoperability. Ontology development is a resource-consuming task that requires the collaborative work of domain experts and ontology engineers. Therefore, companies invest considerable resources in order to generate and maintain Enterprise Knowledge Graphs and ontologies from large and complex datasets, most of which can be unfamiliar for ontology engineers. In this work, we study the use of Large Language Models to aid in the development of ontologies from datasets, ultimately increasing the automation of the generation of ontology-based Knowledge Graphs. As a result we have developed a structured workflow that leverages Large Language Models to enhance ontology engineering through data pre-processing, ontology planning, building, and entity improvement. Our method is also able to generate mappings and RDF data, but in this work we focus on the ontologies. The pipeline has been implemented in the OntoGenix tool. In this work we show the results of the application of OntoGenix to six datasets related to commercial activities. The findings indicate that the ontologies produced exhibit patterns of coherent modeling, and features that closely resemble those created by humans, although the most complex situations are better reflected by the ontologies developed by humans.}
}
@article{ZHANG2018102,
title = {Adapted TextRank for Term Extraction: A Generic Method of Improving Automatic Term Extraction Algorithms},
journal = {Procedia Computer Science},
volume = {137},
pages = {102-108},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316144},
author = {Ziqi Zhang and Johann Petrak and Diana Maynard},
keywords = {automatic term extraction, NLP, terminology, ontology engineering},
abstract = {Automatic Term Extraction is a fundamental Natural Language Processing task often used in many knowledge acquisition processes. It is a challenging NLP task due to its high domain dependence: no existing methods can consistently outperform others in all domains, and good ATE is very much an unsolved problem. We propose a generic method for improving the ranking of terms extracted by a potentially wide range of existing ATE methods. We re-design the well-known TextRank algorithm to work at corpus level, using easily obtainable domain resources in the form of seed words or phrases, to compute a score for a word from the target dataset. This is used to refine a candidate term’s score computed by an existing ATE method, potentially improving the ranking of real terms to be selected for tasks such as ontology engineering. Evaluation shows consistent improvement on 10 state of the art ATE methods by up to 25 percentage points in average precision measured at top-ranked K candidates.}
}
@article{THAKAR2018762,
title = {Enterprise Level Integration of Ontology Engineering and Process Mining for Management of Complex Data and Processes to improve Decision System},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {30},
pages = {762-767},
year = {2018},
note = {18th IFAC Conference on Technology, Culture and International Stability TECIS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.200},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318328659},
author = {Tejan Thakar and Tenzin Tsultrim and Larry Stapleton and Liam Doyle},
keywords = {Enterprise integration, Systems interoperability, Enterprise network design, implementation, Enterprise Systems, business process analysis, complex systems},
abstract = {Software development may involve international-scale, dynamic business processes that consume and generate data in complex ways which may not be obvious to management. This loads risks for global data management projects. This paper investigates an approach combining process mining and knowledge engineering to help manage complex data assets in an international software development process. The research engaged a data management team and other stakeholders over a critical one year period during which the company was involved in an acquisition of another similar sized company. This added significantly to the overall complexity of the enterprise context and decision process. Serious challenges existed with systemic complexity, including the silo-ed nature of IT assets which were not readily amenable to modelling dynamic networks of processes. Preliminary results presented here showed that some features of the combined process mining/ontology development framework would address process management complexity, aiding control of that complex data environment.}
}
@article{ESPINOZAARIAS2021100655,
title = {Crossing the chasm between ontology engineering and application development: A survey},
journal = {Journal of Web Semantics},
volume = {70},
pages = {100655},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100655},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000305},
author = {Paola Espinoza-Arias and Daniel Garijo and Oscar Corcho},
keywords = {Ontology, OWL, Ontology engineering, Web API, Application development, Knowledge graph},
abstract = {The adoption of Knowledge Graphs (KGs) by public and private organizations to integrate and publish data has increased in recent years. Ontologies play a crucial role in providing the structure for KGs, but are usually disregarded when designing Application Programming Interfaces (APIs) to enable browsing KGs in a developer-friendly manner. In this paper we provide a systematic review of the state of the art on existing approaches to ease access to ontology-based KG data by application developers. We propose two comparison frameworks to understand specifications, technologies and tools responsible for providing APIs for KGs. Our results reveal several limitations on existing API-based specifications, technologies and tools for KG consumption, which outline exciting research challenges including automatic API generation, API resource path prediction, ontology-based API versioning, and API validation and testing.}
}
@article{NAQVI20222578,
title = {A Context-Specific Modularization for Ontology Change Management},
journal = {Procedia Computer Science},
volume = {207},
pages = {2578-2587},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.316},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922012054},
author = {Muhammad Raza Naqvi and Linda Elmhadhbi and Arkopaul Sakar and Da Xu and Mohammed Hedi Karray},
keywords = {Ontologies, Modularization, Ontology Change, Semantic heterogeneity},
abstract = {Knowledge engineering has a vital role in advancing the semantic web, in which ontologies play a key role in data interoperability and integration. One of the key issues in ontology engineering is how to handle the subsequent updates in the ontologies. A number of concerns need to be considered while working on the ontology change, such as, tracking ontology versions and heterogeneity issues. Ontology change management has been partially addressed by different researchers in overlapping research areas. However, a concrete description of the problem and its related concerns are still not available in the literature. Our work aims to present an overview of ontology change management and its concerns. We point up the need for modularization in ontology change management based on its advantages in the context of ontology reuse from different contextual viewpoints. For this purpose, we propose a protege plugin for reusing OWL modules, and allowing a safe/clean manual integration and reuse of different ontology modules.}
}
@article{OKONTA2025106328,
title = {Semantic interoperability on IoT: Aligning IFC and Smart Application Reference (SAREF) sensor data models},
journal = {Automation in Construction},
volume = {177},
pages = {106328},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106328},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525003681},
author = {Ebere Donatus Okonta and Farzad Rahimian and Vladimir Vukovic and Sergio Rodriguez},
keywords = {Semantic web, IFC, BIM, SAREF, Ontology alignment, IoT, Data models},
abstract = {This paper proposes extending the Smart Application Reference (SAREF) ontology to enable sensor modelling based on the Industry Foundation Classes (IFC) standard, enhancing semantic interoperability between IoT (Internet of Things) and Building Information Modelling (BIM). The paper introduces the Information Assigned to Device Based Ontology Matching approach (IADOM) to align saref:Sensor and IfcSensor data models. Leveraging RDF (Resource Definition Framework) Semantic Web technology, the research modelled and visualised sensor data models in the Protégé software environment, exploring basic information that defined the sensor, including class, properties, relations, attributes, geometry, and interaction. Ontology results indicate property and interaction similarities and differences in saref:Sensor and IfcSensor. The extended ontology provides a standardised and interoperable representation of sensor data and their relationships within BIM and proves that SAREF ontology extension can enhance semantic interoperability between IoT devices and BIM systems, facilitating efficient data exchange, enabling advanced analytics and decision-making processes in smart buildings.}
}
@article{DAVID2023359,
title = {Deploying OWL ontologies for semantic mediation of mixed-reality interactions for human–robot collaborative assembly},
journal = {Journal of Manufacturing Systems},
volume = {70},
pages = {359-381},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001450},
author = {Joe David and Eric Coatanéa and Andrei Lobov},
keywords = {Human–robot collaboration, Mixed reality, Multi-agent systems, OWL ontology, SHACL, Belief–desire–intent, Digital thread},
abstract = {For effective human–robot collaborative assembly, it is paramount to view both robots and humans as autonomous entities in that they can communicate, undertake different roles, and not be bound to pre-planned routines and task sequences. However, with very few exceptions, most of recent research assumes static pre-defined roles during collaboration with centralised architectures devoid of runtime communication that can influence task responsibility and execution. Furthermore, from an information system standpoint, they lack the self-organisation needed to cope with today’s manufacturing landscape that is characterised by product variants. Therefore, this study presents collaborative agents for manufacturing ontology (CAMO), which is an information model based on description logic that maintains a self-organising team network between collaborating human–robot multi-agent system (MAS). CAMO is implemented using the Web Ontology Language (OWL). It models popular notions of net systems and represents the agent, manufacturing, and interaction contexts that accommodate generalisability to different assemblies and agent capabilities. As a novel element, a dynamic consensus-driven collaboration based on parametric validation of semantic representations of agent capabilities via runtime dynamic communication is presented. CAMO is instantiated as agent beliefs in a framework that benefits from real-time dynamic communication with the assembly design environment and incorporates a mixed-reality environment for use by the operator. The employment of web technologies to project scalable notions of intentions via mixed reality is discussed for its novelty from a technology standpoint and as an intention projection mechanism. A case study with a real diesel engine assembly provides appreciable results and demonstrates the feasibility of CAMO and the framework.}
}
@article{FERRARI2025107697,
title = {Formal requirements engineering and large language models: A two-way roadmap},
journal = {Information and Software Technology},
volume = {181},
pages = {107697},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107697},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925000369},
author = {Alessio Ferrari and Paola Spoletini},
keywords = {Requirements engineering, Formal methods, Large language models, LLMs, Natural language processing, NLP, NLP4RE, Prompt engineering, Prompt requirements engineering},
abstract = {Context:
Large Language Models (LLMs) have made remarkable advancements in emulating human linguistic capabilities, showing potential also in executing various requirements engineering (RE) tasks. However, despite their generally good performance, the adoption of LLM-generated solutions and artefacts prompts concerns about their correctness, fairness, and trustworthiness.
Objective:
This paper aims to address the concerns associated with the use of LLMs in RE activities. Specifically, it seeks to develop a roadmap that leverages formal methods (FMs) to provide guarantees of correctness, fairness, and trustworthiness when LLMs are utilised in RE. Symmetrically, it aims to explore how LLMs can be employed to make FMs more accessible.
Methods:
We use two sets of examples to show the current limits of FMs when used in software development and of LLMs when used for RE tasks. The highlighted limitations are addressed by proposing two roadmaps grounded in the current literature and technologies.
Results:
The proposed examples show the potential and limits of FMs in supporting software development and of LLMs when used for RE tasks. The initial investigation into how these limitations can be overcome has been concretised in two detailed roadmaps for the RE and, more largely, the software engineering community.
Conclusion:
The proposed roadmaps offer a promising approach to address the concerns of correctness, fairness, and trustworthiness associated with the use of LLMs in RE tasks through the use of FMs and to enhance the accessibility of FMs by utilising LLMs.}
}
@article{LOUGE2025126641,
title = {Events-based semantic services composition in Industry 4.0 using Asset Administration Shell meta-model for digital twins},
journal = {Expert Systems with Applications},
volume = {271},
pages = {126641},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126641},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425002635},
author = {Thierry Louge and Sina Namaki Araghi and Mohamed Hedi Karray and Arkopaul Sarkar},
keywords = {Ontologies, Semantic services, Asset Administration Shell, Industry 4.0, Digital twins},
abstract = {Since the emergence of the Semantic Web concept, considerable work has focused on service composition using ontology-based approaches. Meanwhile, the concept of Industry 4.0 has emerged, emphasizing the benefits of utilizing data and computing devices in close proximity to production lines, exemplified by concepts like digital twins. However, these two fields rarely intersect, and the requirements for integrating domain-specific knowledge into business processes with event feedback during processes execution differ between these contexts. With the recent advancements in the semantization of industrial standards, such as the Asset Administration Shell, this work explores the elements of a semantic model for describing equipment, enabling the semantic composition of equipment as services. We propose an ontology, COMPAAS, designed to facilitate the composition of production lines that can react to events reported by their components, allowing the system to adjust its behavior accordingly. This approach also addresses the removal and addition of hardware or software elements within the chain, and the entire concept is validated through a minimal use case that demonstrates the improved flexibility of the production line in response to potential disturbances.}
}
@article{FAN2022,
title = {Telehealth System Based on the Ontology Design of a Diabetes Management Pathway Model in China: Development and Usability Study},
journal = {JMIR Medical Informatics},
volume = {10},
number = {12},
year = {2022},
issn = {2291-9694},
doi = {https://doi.org/10.2196/42664},
url = {https://www.sciencedirect.com/science/article/pii/S2291969422000588},
author = {ZhiYuan Fan and LiYuan Cui and Ying Ye and ShouCheng Li and Ning Deng},
keywords = {diabetes, chronic disease management, Chronic Disease Management Pathway, ontology, Semantic Web Rule Language rules, SWRL rules},
abstract = {Background
Diabetes needs to be under control through management and intervention. Management of diabetes through mobile health is a practical approach; however, most diabetes mobile health management systems do not meet expectations, which may be because of the lack of standardized management processes in the systems and the lack of intervention implementation recommendations in the management knowledge base.
Objective
In this study, we aimed to construct a diabetes management care pathway suitable for the actual situation in China to express the diabetes management care pathway using ontology and develop a diabetes closed-loop system based on the construction results of the diabetes management pathway and apply it practically.
Methods
This study proposes a diabetes management care pathway model in which the management process of diabetes is divided into 9 management tasks, and the Diabetes Care Pathway Ontology (DCPO) is constructed to represent the knowledge contained in this pathway model. A telehealth system, which can support the comprehensive management of patients with diabetes while providing active intervention by physicians, was designed and developed based on the DCPO. A retrospective study was performed based on the data records extracted from the system to analyze the usability and treatment effects of the DCPO.
Results
The diabetes management pathway ontology constructed in this study contains 119 newly added classes, 28 object properties, 58 data properties, 81 individuals, 426 axioms, and 192 Semantic Web Rule Language rules. The developed mobile medical system was applied to 272 patients with diabetes. Within 3 months, the average fasting blood glucose of the patients decreased by 1.34 mmol/L (P=.003), and the average 2-hour postprandial blood glucose decreased by 2.63 mmol/L (P=.003); the average systolic and diastolic blood pressures decreased by 11.84 mmHg (P=.02) and 8.8 mmHg (P=.02), respectively. In patients who received physician interventions owing to abnormal attention or low-compliance warnings, the average fasting blood glucose decreased by 2.45 mmol/L (P=.003), and the average 2-hour postprandial blood glucose decreased by 2.89 mmol/L (P=.003) in all patients with diabetes; the average systolic and diastolic blood pressure decreased by 20.06 mmHg (P=.02) and 17.37 mmHg (P=.02), respectively, in patients with both hypertension and diabetes during the 3-month management period.
Conclusions
This study helps guide the timing and content of interactive interventions between physicians and patients and regulates physicians’ medical service behavior. Different management plans are formulated for physicians and patients according to different characteristics to comprehensively manage various cardiovascular risk factors. The application of the DCPO in the diabetes management system can provide effective and adequate management support for patients with diabetes and those with both diabetes and hypertension.}
}
@article{VEGGI2025e00409,
title = {The Brancacci Chapel from the Quattrocento to the semantic web: An ontology-assisted case study of cultural data management and site reconstruction},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {37},
pages = {e00409},
year = {2025},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2025.e00409},
url = {https://www.sciencedirect.com/science/article/pii/S2212054825000116},
author = {Manuele Veggi and Ivana Cerato},
keywords = {Knowledge representation, 3D semantic annotation, Digital art history, Ontology engineering, Cultural site reconstruction, Semantic web},
abstract = {This study proposes an ontological model for cultural heterogeneous data and cultural site reconstructions. It is based on the concept of interpretative unit, which extends the semantics of stratigraphic units also to non-archaeological contexts. The ontology is named after the case study of this research, the Brancacci Chapel in Florence. Indeed, after a state of the art overview of the development methodology and the description of the most relevant entities, a first test case is proposed. An entry of the catalogue of a recent exhibition on Masolino, a 15th century painter who worked at the decoration of the chapel, has been serialised as Turtle file and the semantics of knowledge graph has been assessed via competency questions. The positive results encourage the deepening of this line of research in the direction of connecting linked data with nodes in 3D models, as well as their visualisation and communication to non-specialist audiences.}
}
@article{LOPES2023110385,
title = {Using terms and informal definitions to classify domain entities into top-level ontology concepts: An approach based on language models},
journal = {Knowledge-Based Systems},
volume = {265},
pages = {110385},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110385},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123001351},
author = {Alcides Lopes and Joel Carbonera and Daniela Schmidt and Luan Garcia and Fabricio Rodrigues and Mara Abel},
keywords = {Ontology learning, Top-level ontology, Language model},
abstract = {The classification of domain entities into top-level ontology concepts remains an activity performed manually by an ontology engineer. Although some works focus on automating this task by applying machine-learning approaches using textual sentences as input, they require the existence of the domain entities in external knowledge resources, such as pre-trained embedding models. In this context, this work proposes an approach that combines the term representing the domain entity and its informal definition into a single text sentence without requiring external knowledge resources. Thus, we use this sentence as the input of a deep neural network that contains a language model as a layer. Also, we present a methodology used to extract two novel datasets from the OntoWordNet ontology based on Dolce-Lite and Dolce-Lite-Plus top-level ontologies. Our experiments show that by using the transformer-based language models, we achieve promising results in classifying domain entities into 82 top-level ontology concepts, with 94% regarding micro F1-score.}
}
@article{KOZIOL20203263,
title = {Dealing with Polysemy in the Polish Sign Language Using the OWL Ontology},
journal = {Procedia Computer Science},
volume = {176},
pages = {3263-3272},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.122},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920320226},
author = {Wojciech Kozioł and Krzysztof Pancerz and Kazimierz Sikora and Kamil Dudek},
keywords = {Modelling, Polysemy, Polish Sign Language, OWL Ontology},
abstract = {A common problem of natural language processing is synonymy, polysemy, and homonymy. In the paper, we propose to deal with polysemy in the Polish sign language using the knowledge included in the OWL2 ontology created for this purpose. The proposed approach aids the translation process of the Polish sign language into the Polish language by selection from the possible phrases, only those, with the reasonable meaning.}
}
@article{MOUSAVI20191254,
title = {A Survey of Model-Based System Engineering Methods to Analyse Complex Supply Chains: A Case Study in Semiconductor Supply Chain},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {1254-1259},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.370},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319313497},
author = {Behrouz Alizadeh Mousavi and Radhia Azzouz and Cathal Heavey and Hans Ehm},
keywords = {Model-Based system Engineering, Conceptual modelling, Simulation, SysML, Ontology, BPMN, Supply chain planning},
abstract = {Model-Based System Engineering (MBSE) is an increasingly important methodology to support system engineering and has attained a high level of attentiveness in business simulation practices as a conceptual modelling approach. In this paper, we present our results related to the application of MBSE approaches in complex semiconductor manufacturing supply chain planning systems. We investigate System Modeling Language (SysML), Web Ontology Language (OWL) and Business Process Modeling Notation (BPMN) as different approaches and languages for MBSE. These approaches are surveyed and used to develop conceptual models for the simulation of the order management process inside the supply chain management. This study aims to survey and offer a number of implications for MBSE practice and seeks to stimulate and guide further research in this area.}
}
@article{ALI2020103175,
title = {Ontology-based approach to extract product's design features from online customers’ reviews},
journal = {Computers in Industry},
volume = {116},
pages = {103175},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.103175},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519301836},
author = {Munira Mohd Ali and Mamadou Bilo Doumbouya and Thierry Louge and Rahul Rai and Mohamed Hedi Karray},
keywords = {, Product lifecycle management (PLM), Customers’ reviews, Sentiment analysis, Product design},
abstract = {Online customer reviews provide new potential customers with relevant information about a product or service. It has been empirically shown that the type of reviews (positive or negative) a product receives significantly impacts its future sales. In this paper, the online customers’ reviews analysis for the identification of key product attributes to be used in the conceptual design phase of a product is outlined. Our goal is to bring a value-added link between the Middle of Life phase to the Beginning of Life phase in the closed-loop product lifecycle management (PLM) by developing an ontology-based reasoning system to provide information that represents the customers’ opinions for the product's conceptual design. The main contributions of the proposed approach are the integration between the ontology and the natural language processing system in extracting the customers’ reviews data in the overall framework. The utility of the proposed approach is shown through the application on the digital camera product review dataset from Amazon.}
}
@article{CIROKU2024107997,
title = {Automated multimodal sensemaking: Ontology-based integration of linguistic frames and visual data},
journal = {Computers in Human Behavior},
volume = {150},
pages = {107997},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107997},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223003485},
author = {Fiorela Ciroku and Stefano {De Giorgis} and Aldo Gangemi and Delfina S. Martinez-Pandiani and Valentina Presutti},
keywords = {Multimodal sensemaking, Ontology engineering, Knowledge graph construction, Frame-based reasoning, Visual and linguistic frames},
abstract = {Frame evocation from visual data is an essential process for multimodal sensemaking, due to the multimodal abstraction provided by frame semantics. However, there is a scarcity of data-driven approaches and tools to automate it. We propose a novel approach for explainable automated multimodal sensemaking by linking linguistic frames to their physical visual occurrences, using ontology-based knowledge engineering techniques. We pair the evocation of linguistic frames from text to visual data as “framal visual manifestations”. We present a deep ontological analysis of the implicit data model of the Visual Genome image dataset, and its formalization in the novel Visual Sense Ontology (VSO). To enhance the multimodal data from this dataset, we introduce a framal knowledge expansion pipeline that extracts and connects linguistic frames – including values and emotions – to images, using multiple linguistic resources for disambiguation. It then introduces the Visual Sense Knowledge Graph (VSKG), a novel resource. VSKG is a queryable knowledge graph that enhances the accessibility and comprehensibility of Visual Genome’s multimodal data, based on SPARQL queries. VSKG includes frame visual evocation data, enabling more advanced forms of explicit reasoning, analysis and sensemaking. Our work represents a significant advancement in the automation of frame evocation and multimodal sense-making, performed in a fully interpretable and transparent way, with potential applications in various fields, including the fields of knowledge representation, computer vision, and natural language processing.}
}
@article{BATISTA2022102012,
title = {Ontologically correct taxonomies by construction},
journal = {Data & Knowledge Engineering},
volume = {139},
pages = {102012},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2022.102012},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X22000246},
author = {Jeferson O. Batista and João Paulo A. Almeida and Eduardo Zambon and Giancarlo Guizzardi},
keywords = {Taxonomies, Conceptual modeling, Ontologies, Graph grammars, Correctness by construction},
abstract = {Taxonomies play a central role in conceptual domain modeling, having a direct impact in areas such as knowledge representation, ontology engineering, and software engineering, as well as knowledge organization in information sciences. Despite this, there is little guidance on how to build high-quality taxonomies, with notable exceptions being the OntoClean methodology, and the ontology-driven conceptual modeling language OntoUML. These techniques take into account the ontological meta-properties of types to establish well-founded rules on the formation of taxonomic structures. In this paper, we show how to leverage the formal rules underlying these techniques in order to build taxonomies which are correct by construction. We define a set of correctness-preserving operations to systematically introduce types and subtyping relations into taxonomic structures. In addition to considering the ontological micro-theory of endurant types underlying OntoClean and OntoUML, we also employ the MLT (Multi-Level Theory) micro-theory of high-order types, which allows us to address multi-level taxonomies based on the powertype pattern. To validate our proposal, we formalize the model building operations as a graph grammar that incorporates both micro-theories. We apply automatic verification techniques over the grammar language to show that the graph grammar is sound, i.e., that all taxonomies produced by the grammar rules are correct, at least up to a certain size. We also show that the rules can generate all correct taxonomies up to a certain size (a completeness result).}
}
@article{LALIS2019290,
title = {Functional modeling in safety by means of foundational ontologies},
journal = {Transportation Research Procedia},
volume = {43},
pages = {290-299},
year = {2019},
note = {INAIR 2019 - Global Trends in Aviation},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2019.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S2352146519306118},
author = {Andrej Lališ and Riccardo Patriarca and Jana Ahmad and Giulio Di Gravio and Bogdan Kostov},
keywords = {aviation safety, socio-technical systems, ontology engineering, safety engineering, resilience engineering},
abstract = {Modern theory of safety deals with systemic approach to safety, formalized in form of several systemic prediction models or methods such as FRAM (Functional Resonance Analysis Method) or STAMP (System-Theoretic Accident Model and Processes). The theory of each approach emphasizes different viewpoints to be considered in approaching various industrial safety issues. This paper focuses on FRAM and its functional viewpoint for modern complex sociotechnical systems. The methodology in this paper is based on the utilization of foundational ontologies to conceptualize the core ideas of FRAM, with the focus on the concept of functions as used in theory. The outcomes of the case study in the aviation domain provide for what needs to be determined to properly model functions in FRAM and they allow for better utilization of the method in real-case applications. The results also confirm some previous research, suggesting that modern systemic approach to safety is theoretically grounded on common - or at least complementary - tenets, to be prospectively integrated by means of ontology engineering.}
}
@article{SONG2024114983,
title = {Ontology-assisted GPT-based building performance simulation and assessment: Implementation of multizone airflow simulation},
journal = {Energy and Buildings},
volume = {325},
pages = {114983},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.114983},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824010995},
author = {Jihwan Song and Sungmin Yoon},
keywords = {GPT, ChatGPT, Large language model (LLM), Building performance simulation (BPS), Building performance, Digital twins, Artificial intelligence, CONTAM},
abstract = {Building performance simulation (BPS) is crucial for building performance assessments across its lifecycle. However, the complexity of buildings and the iterative nature of simulation poses challenges, leading to high costs and low values. Previous studies focused on simplification, but did not fully utilize advanced simulation engines. Despite recent advancements, there is a lack of research on leveraging artificial intelligence (AI), specifically generative pre-trained transformer (GPT), for BPS. Therefore, this study proposes a GPT-based BPS system, enhancing simulation efficiency and value by integrating simulation engines and advanced data analytics in the GPT environment. The ontology for GPT-based BPS is also developed to enable comprehensive, reliable, informative BPS environments. Based on this framework, case studies were conducted for GPT-based multizone airflow network simulation in a high-rise residential building using CONTAM software. They demonstrate GPT’s capabilities in retrieving simulation data, visualizing results with data mining, answering questions based on building knowledge, checking compliance with design guidelines, and proposing design alternatives. Finally, this study emphasizes expert interventions with ontological engineering informatics to utilize strictly structured BPS engines.}
}
@article{ADIB2022417,
title = {Ontological user profile for E-orientation platforms},
journal = {Procedia Computer Science},
volume = {198},
pages = {417-422},
year = {2022},
note = {12th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 11th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.263},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921025023},
author = {Jihad Adib and Rachida Ait Abdelouahid and Abdelaziz Marzak and Hicham Moutachaouik},
keywords = {E-Orientation, User profile, Platform, Ontologies, Model},
abstract = {In recent years, E-orientation systems have played an increasingly significant role in the proposal of an academic and professional orientation to students. Research efforts have grown to provide more useful and effective E-orientation systems for research or other purposes. The implementation of E-orientation systems resulting from these efforts utilizes several techniques including Artificial Intelligence (AI) methodologies. This study proposes a personalised approach to support an E-orientation system that is tailored to the student’s characteristics. A key component of this system comprises an ontological model of the user profile. The objective of this research was to propose an ontology that is able to collect and analyze the user related information as well as customize the profiles with the most appropriate recommendation or orientation. The ontology employed in this study was developed using the OWL (Ontology Web Language), a knowledge representation language for authoring ontologies. In this paper we will present a definition for the user profile, and then we present our methodology of ontological modeling of the user profile, and finally the conceptual model of the user model for e-orientation systems.}
}
@article{MUNCH2022117406,
title = {Combining ontology and probabilistic models for the design of bio-based product transformation processes},
journal = {Expert Systems with Applications},
volume = {203},
pages = {117406},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117406},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422007485},
author = {Mélanie Munch and Patrice Buche and Stéphane Dervaux and Juliette Dibie and Liliana Ibanescu and Cristina Manfredotti and Pierre-Henri Wuillemin and Hélène Angellier-Coussy},
keywords = {Ontologies, Probabilistic relational models, Knowledge discovery, Causality},
abstract = {This paper presents a workflow for the design of transformation processes using different kinds of expert’s knowledge. It introduces POND (Process and observation ONtology Discovery), a workflow dedicated to answer expert’s questions about processes. It addresses two main issues: (1) how to represent the processes inner complexity, and (2) how to reason about processes taking into account uncertainty and causality. First, we show how to use a semantic model, an ontology, and its associated data to answer some of the expert’s questions concerning the processes, using semantic web languages and technologies. Then, we describe how to learn a predictive model, to discover new knowledge and provide explicative models by integrating the semantic model into a probabilistic relational model. The result is a complete workflow able to extensively analyze transformation processes through all their granularity levels and answer expert’s questions about their domains. An example of this workflow is given on biocomposites manufacturing for food packaging.}
}
@article{MARTINSAMORIM2025114154,
title = {Using natural language definitions and language models for relationship classification},
journal = {Knowledge-Based Systems},
volume = {327},
pages = {114154},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114154},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125011955},
author = {Marina {Martins Amorim} and Alcides {Gonçalves Lopes Junior} and  {Fabrício Henrique Rodrigues} and Joel {Luís Carbonera}},
keywords = {BERT, Machine learning, Lexical classification, Relation classification, NLP},
abstract = {Identifying relationships between concepts is a very important task for several NLP tasks, as well as for building explicit knowledge models (ontologies and knowledge graphs). In many of these tasks, experts usually manually establish these relationships by carefully analyzing each concept’s meaning and considering the domain knowledge elicited from domain practitioners or from domain literature. While some studies automate parts of the process of building knowledge models, most focus on identifying general concepts or rely on static word embeddings, which fail to address challenges like polysemy and contextual ambiguity. This research addresses the problem of classifying semantic relationships between concepts, focusing on hypernym and holonym relations. We propose an approach based on the pre-trained language model BERT to classify these relationships between concepts. We assume that we can represent the concept’s semantics using their definitions in natural language. To evaluate this approach, we developed a methodology to construct a labeled dataset of definitions of concepts using WordNet as a reference. Thus, our proposed approach classifies the relations based solely on natural language expressions representing the concept’s definition. Our experiments showed notable classification results, achieving an F1 score of 96 % in the classification of holonyms, hypernyms, and concepts that are not related by any of these relations, indicating that our approach can accurately predict semantic relations between concepts using only their natural language definitions as input.}
}
@article{RYS2024100720,
title = {Model management to support systems engineering workflows using ontology-based knowledge graphs},
journal = {Journal of Industrial Information Integration},
volume = {42},
pages = {100720},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100720},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24001638},
author = {Arkadiusz Ryś and Lucas Lima and Joeri Exelmans and Dennis Janssens and Hans Vangheluwe},
keywords = {Model management, Ontology, Process modelling, Knowledge graph},
abstract = {System engineering has been shifting from document-centric to model-based approaches, where assets are becoming more and more digital. Although digitisation conveys several benefits, it also brings several concerns (e.g., storage and access) and opportunities. In the context of Cyber-Physical Systems (CPS), we have experts from various domains executing complex workflows and manipulating models in a plethora of different formalisms, each with their own methods, techniques and tools. Storing knowledge on these workflows can reduce considerable effort during system development not only to allow their repeatability and replicability but also to access and reason on data generated by their execution. In this work, we propose a framework to manage modelling artefacts generated from workflow executions. The basic workflow concepts, related formalisms and artefacts are formally defined in an ontology specified in OML (Ontology Modelling Language). This ontology enables the construction of a knowledge graph that contains system engineering data to which we can apply reasoning. We also developed several tools to support system engineering during the design of workflows, their enactment, and artefact storage, considering versioning, querying and reasoning on the stored data. These tools also hide the complexity of manipulating the knowledge graph directly. Finally, we have applied our proposed framework in a real-world system development scenario of a drivetrain smart sensor system. Results show that our proposal not only helped the system engineer with fundamental difficulties like storage and versioning but also reduced the time needed to access relevant information and new knowledge that can be inferred from the knowledge graph.}
}
@article{SEQUEDA2025100858,
title = {Knowledge Graphs as a source of trust for LLM-powered enterprise question answering},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100858},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100858},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000441},
author = {Juan Sequeda and Dean Allemang and Bryon Jacob},
keywords = {Knowledge Graph, LLM, Large Language Model, Generative AI, Question answering, Knowledge engineering, SPARQL, SQL, OWL, R2RML},
abstract = {Generative AI provides an innovative and exciting way to manage knowledge and data at any scale; for small projects, at the enterprise level, and even at a world wide web scale. It is tempting to think that Generative AI has made other knowledge-based technologies obsolete; that anything we wanted to do with knowledge-based systems, Knowledge Graphs or even expert systems can instead be done with Generative AI. Our position is counter to that conclusion. Our practical experience on implementing enterprise question answering systems using Generative AI has shown that Knowledge Graphs support this infrastructure in multiple ways: they provide a formal framework to evaluate the validity of a query generated by an LLM, serve as a foundation for explaining results, and offer access to governed and trusted data. In this position paper, we share our experience, present industry needs, and outline the opportunities for future research contributions.}
}
@article{TAUQEER2023121049,
title = {Smell and Taste Disorders Knowledge Graph: Answering Questions Using Health Data},
journal = {Expert Systems with Applications},
volume = {234},
pages = {121049},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121049},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423015518},
author = {Amar Tauqeer and Ismaheel Hammid and Sareh Aghaei and Parvaneh Parvin and Elbrich M. Postma and Anna Fensel},
keywords = {Chemosensory dysfunction, Semantic modeling, Health, Data sharing, Knowledge graph, Ontology, Question answering user interface},
abstract = {Smell and taste disorders have become a more prominent issue due to their association with Covid-19, and their impact on quality of life and health outcomes. However, pertinent information regarding these disorders is often inaccessible and poorly organized, with the majority of data stored solely in clinical data repositories. To rectify this, a technological solution capable of digitizing, semantically modeling, and integrating health data is necessary. The knowledge graph, an emerging technology capable of organizing inconsistent and heterogeneous health data and inferring implicit knowledge, presents a viable solution to this problem. In pursuit of the aforementioned goal, an existing ontology pertaining to smell and taste disorders was enriched by introducing additional relevant concepts and relationships. Subsequently, a knowledge graph was constructed based on the defined ontology and patients’ data. The resultant knowledge graph was subjected to a rigorous evaluation, encompassing dimensions such as completeness, coherency, coverage, and succinctness. The evaluation established the effectiveness and usability of the knowledge graph, with only minor issues detected through the OOPS! pitfall scanner. Furthermore, as a proof-of-concept for clinical application, a user interface was created, enabling users to access pertinent information concerning smell and taste disorders, including causative factors, medications, and etiology, among others. The interface generates a graph-based structure based on the selected question from a drop-down menu. The end-user can modify the query by merely clicking on the generated graph to ask related questions. This study showcases the potential of knowledge graphs centered on smell and taste disorders to organize and provide accessible health data to end-users.}
}
@article{SALES2023102210,
title = {A FAIR catalog of ontology-driven conceptual models},
journal = {Data & Knowledge Engineering},
volume = {147},
pages = {102210},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102210},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23000708},
author = {Tiago Prince Sales and Pedro Paulo F. Barcelos and Claudenir M. Fonseca and Isadora Valle Souza and Elena Romanenko and César Henrique Bernabé and Luiz Olavo {Bonino da Silva Santos} and Mattia Fumagalli and Joshua Kritz and João Paulo A. Almeida and Giancarlo Guizzardi},
keywords = {Ontology-driven conceptual modeling, OntoUML, Unified Foundational Ontology, Model catalog, FAIR, Linked data},
abstract = {Multi-domain model catalogs serve as empirical sources of knowledge and insights about specific domains, about the use of a modeling language’s constructs, as well as about the patterns and anti-patterns recurrent in the models of that language crosscutting different domains. They may support domain and language learning, model reuse, knowledge discovery for humans, and reliable automated processing and analysis if built following generally accepted quality requirements for scientific data management. More specifically, not unlike scientific (meta)data, models should be shared according to the FAIR principles (Findability, Accessibility, Interoperability, and Reusability). In this paper, we report on the construction of a FAIR model catalog for Ontology-Driven Conceptual Modeling research, a trending paradigm lying at the intersection of conceptual modeling and ontology engineering in which the Unified Foundational Ontology (UFO) and OntoUML emerged among the most adopted technologies. The catalog, publicly available at https://w3id.org/ontouml-models, currently includes over one hundred and forty models, developed in a variety of contexts and domains.}
}
@article{CAO2019177,
title = {Towards a Core Ontology for Condition Monitoring},
journal = {Procedia Manufacturing},
volume = {28},
pages = {177-182},
year = {2019},
note = {7th International conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918313714},
author = {Qiushi Cao and Cecilia Zanni-Merk and Christoph Reich},
keywords = {condition monitoring, state of the system or machine, ontology engineering, core ontology, conceptual model},
abstract = {Condition monitoring is performed to identify the functioning state of a machine or a mechanical system. It is an important task by which the machine or mechanical system deterioration tendency and the location of a failure can be detected. In recent years, ontologies have shown promising results to enhance knowledge sharing in condition monitoring tasks, while offering a logically defined and controlled vocabulary of domain entities. Motivated by the growing demand for unification and formal representation of useful concepts in condition monitoring, in this paper we present CM-core, an ontology of core condition monitoring entities. It incorporates several ISO standards as sources and also extracts general concepts from a series of domain ontologies. The ontology contains taxonomies of core condition monitoring concepts such as system, function, behavior, structure, state, failure and fault, with their interrelationships. The CM-core ontology has a broader domain coverage than the existing ontologies, and its generality ensures further specification into more specific domain ontologies.}
}
@article{XU2018118,
title = {A knowledge base with modularized ontologies for eco-labeling: Application for laundry detergents},
journal = {Computers in Industry},
volume = {98},
pages = {118-133},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517306322},
author = {Da Xu and Mohamed Hedi Karray and Bernard Archimède},
keywords = {Ontology engineering, Ontology modularization, Knowledge base, OWL imports, SWRL, Eco-labeling},
abstract = {Along with the rising concern of environmental performance, eco-labeling is becoming more and more popular. However, the complex process of eco-labeling is demotivating manufacturers and service providers to be certificated. The knowledge contained in eco-labeling criteria documents is not semantically exploitable to computers. Traditional knowledge base in relational data model is not inter-operable, lacks inference support and is difficult to be reused. In our research, we propose a comprehensive knowledge base composed of interconnected OWL (Ontology Web Language) ontologies. This ontology based knowledge base allows reasoning and semantic query. In this paper, a modularization scheme about ontology development is introduced and it has been applied to EU Eco-label (European Union Eco-label) laundry detergent product criteria. This scheme separates entity knowledge and rule knowledge so that the ontology modules can be reused easily in other domains. Reasoning and inference based on SWRL (Semantic Web Rule Language) rules in favor of eco-labeling process is also presented.}
}
@article{DAGA2025100846,
title = {Process Knowledge Graphs (PKG): Towards unpacking and repacking AI applications},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100846},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100846},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000325},
author = {Enrico Daga},
keywords = {Knowledge graphs, Prompt engineering, Data science pipelines, Data pipelines documentation, Data pipelines design},
abstract = {In the past years, a new generation of systems has emerged, which apply recent advances in generative Artificial Intelligence (AI) in combination with traditional technologies. Specifically, generative AI is being delegated tasks in natural language or vision understanding within complex hybrid architectures that also include databases, procedural code, and interfaces. Process Knowledge Graphs (PKG) have a long-standing tradition within symbolic AI research. On the one hand, PKGs can play an important role in describing complex, hybrid applications, thus opening the way for addressing fundamental challenges such as explaining and documenting such systems (unpacking). On the other hand, by organising complex processes in simpler building blocks, PKGs can potentially increase accuracy and control over such systems (repacking). In this position paper, we discuss opportunities and challenges of PGRs and their potential role towards a more robust and principled design of AI applications.}
}
@article{SPOLADORE2024374,
title = {Towards a knowledge-based decision support system to foster the return to work of wheelchair users},
journal = {Computational and Structural Biotechnology Journal},
volume = {24},
pages = {374-392},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024001570},
author = {Daniele Spoladore and Luca Negri and Sara Arlati and Atieh Mahroo and Margherita Fossati and Emilia Biffi and Angelo Davalli and Alberto Trombetta and Marco Sacco},
keywords = {Knowledge-based decision support system, Ontology engineering, Return to work, Clinical decision support system, Wheelchair user},
abstract = {Accidents at work may force workers to face abrupt changes in their daily life: one of the most impactful accident cases consists of the worker remaining in a wheelchair. Return To Work (RTW) of wheelchair users in their working age is still challenging, encompassing the expertise of clinical and rehabilitation personnel and social workers to match the workers’ residual capabilities with job requirements. This work describes a novel and prototypical knowledge-based Decision Support System (DSS) that matches workers’ residual capabilities with job requirements, thus helping vocational therapists and clinical personnel in the RTW decision-making process for WUs. The DSS leverages expert knowledge in the form of ontologies to represent the International Classification of Functioning, Disability, and Health (ICF) and the Occupational Information Network (O*NET). These taxonomies enable both workers’ health conditions and job requirements formalization, which are processed to assess the suitability of a job depending on a worker’s condition. Consequently, the DSS suggests a list of jobs a wheelchair user can still perform, exploiting his/her residual abilities at their best. The manuscript describes the theoretical approach and technological foundations of such DSS, illustrating its development, its output metric, and application. The developed solution was tested with real wheelchair users’ health conditions provided by the Italian National Institute for Insurance against Accidents at Work. The feasibility of an approach based on objective data was thus demonstrated, providing a novel point of view in the critical process of decision-making during RTW.}
}
@article{GUIZZARDI2024102325,
title = {Explanation, semantics, and ontology},
journal = {Data & Knowledge Engineering},
volume = {153},
pages = {102325},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102325},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000491},
author = {Giancarlo Guizzardi and Nicola Guarino},
keywords = {Real-world semantics, Ontology, Explanation, Ontological unpacking, Semantic interoperability},
abstract = {The terms ‘semantics’ and ‘ontology’ are increasingly appearing together with ‘explanation’, not only in the scientific literature, but also in everyday social interactions, in particular, within organizations. Ontologies have been shown to play a key role in supporting the semantic interoperability of data and knowledge representation structures used by information systems. With the proliferation of applications of Artificial Intelligence (AI) in different settings and the increasing need to guarantee their explainability (but also their interoperability) in critical contexts, the term ‘explanation’ has also become part of the scientific and technical jargon of modern information systems engineering. However, all of these terms are also significantly overloaded. In this paper, we address several interpretations of these notions, with an emphasis on their strong connection. Specifically, we discuss a notion of explanation termed ontological unpacking, which aims at explaining symbolic domain descriptions (e.g., conceptual models, knowledge graphs, logical specifications) by revealing their ontological commitment in terms of their so-called truthmakers, i.e., the entities in one’s ontology that are responsible for the truth of a description. To illustrate this methodology, we employ an ontological theory of relations to explain a symbolic model encoded in the de facto standard modeling language UML. We also discuss the essential role played by ontology-driven conceptual models (resulting from this form of explanation processes) in supporting semantic interoperability tasks. Furthermore, we revisit a proposal for quality criteria for explanations from philosophy of science to assess our approach. Finally, we discuss the relation between ontological unpacking and other forms of explanation in philosophy and science, as well as in the subarea of Artificial Intelligence known as Explainable AI (XAI).}
}
@article{KIRAN2018205,
title = {Enabling intent to configure scientific networks for high performance demands},
journal = {Future Generation Computer Systems},
volume = {79},
pages = {205-214},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1730626X},
author = {Mariam Kiran and Eric Pouyoul and Anu Mercian and Brian Tierney and Chin Guok and Inder Monga},
keywords = {Intent-based networking, Natural language processing, Ontology engineering, SDN north-bound interface},
abstract = {Globally distributed scientific experiments involve movement of massive data volumes and many collaborators performing distributed data analysis. With complex workloads and heterogeneous resources, each user may desire certain behavior characteristics for their network paths. In this paper, we present the iNDIRA tool, which interacts with SDN north-bound interfaces to enable intent-based networking. It provides reliable, simple, and technology-agnostic communication between users and networks. Focusing particularly on science applications, iNDIRA uses natural language processing to construct semantic RDF graphs to understand, interact, and create the required network services. The technical challenges addressed by iNDIRA are: (1) development of a high-level descriptive language to query network-application requirements, (2) provides keyword identification and condition checking based on user profiles and topology details, (3) allows user negotiation based on the current network state, and (4) integrates network provisioning and service tools used by the application. iNDIRA is implemented on the ESnet network, where it interacts with OpenNSA (aka the NSI client) and Globus data transfer tools, to build complex cross-domain network paths for heterogeneous science applications, and perform secure data transfer. We argue that iNDIRA’s approach presents users with an alternative approach to interact and communicate their network demands, allowing seamless network service integration.}
}
@article{RIQUELMEGARCIA20252155,
title = {Annotation of biological samples data to standard ontologies with support from large language models},
journal = {Computational and Structural Biotechnology Journal},
volume = {27},
pages = {2155-2167},
year = {2025},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2025.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S2001037025001837},
author = {Andrea Riquelme-García and Juan Mulero-Hernández and Jesualdo Tomás Fernández-Breis},
keywords = {Bioinformatics, Generative AI, Large language models, Data interoperability, Biological samples},
abstract = {The semantic integration of biological data is hindered by the vast heterogeneity of data sources and their limited semantic formalization. A crucial step in this process is mapping data elements to ontological concepts, which typically involves substantial manual effort. Large Language Models (LLMs) have demonstrated potential in automating complex language-related tasks and may offer a solution to streamline biological data annotation. This study investigates the utility of LLMs—specifically various base and fine-tuned GPT models—for the automatic assignment of ontological identifiers to biological sample labels. We evaluated model performance in annotating labels to four widely used ontologies: the Cell Line Ontology (CLO), Cell Ontology (CL), Uber-anatomy Ontology (UBERON), and BRENDA Tissue Ontology (BTO). Our dataset was compiled from publicly available, high-quality databases containing biologically relevant sequence information, which suffers from inconsistent annotation practices, complicating integrative analyses. Model outputs were compared against annotations generated by text2term, a state-of-the-art annotation tool. The fine-tuned GPT model outperformed both the base models and text2term in annotating cell lines and cell types, particularly for the CL and UBERON ontologies, achieving a precision of 47–64% and a recall of 88–97%. In contrast, base models exhibited significantly lower performance. These results suggest that fine-tuned LLMs can accelerate and improve the accuracy of biological data annotation. Nonetheless, our evaluation highlights persistent challenges, including variable precision across ontology categories and the continued need for expert curation to ensure annotation validity.}
}
@article{MATENTZOGLU20181,
title = {Inference Inspector: Improving the verification of ontology authoring actions},
journal = {Journal of Web Semantics},
volume = {49},
pages = {1-15},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570826817300367},
author = {Nicolas Matentzoglu and Markel Vigo and Caroline Jay and Robert Stevens},
keywords = {OWL, Ontologies, Human computer interaction, Ontology engineering, Ontology authoring, Reasoning},
abstract = {Ontologies are complex systems of axioms in which unanticipated consequences of changes are both frequent, and difficult for ontology authors to apprehend. The effects of modelling actions range from unintended inferences to outright defects such as incoherency or even inconsistency. One of the central ontology authoring activities is verifying that a particular modelling step has had the intended consequences, often with the help of reasoners. For users of Protégé, this involves, for example, exploring the inferred class hierarchy. This paper provides evidence that making entailment set changes explicit to authors significantly improves the understanding of authoring actions regarding both correctness and speed. This is tested by means of the Inference Inspector, a Protégé plugin we created that provides authors with specific details about the effects of an authoring action. We empirically validate the effectiveness of the Inference Inspector in two studies. In a first, exploratory study we determine the feasibility of the Inference Inspector for supporting verification and isolating authoring actions. In a second, controlled study we formally evaluate the Inference Inspector and determine that making changes to key entailment sets explicit significantly improves author verification compared to the standard static hierarchy/frame-based approach. We discuss the advantages of the Inference Inspector for different types of verification questions and find that our approach is best suited for verifying added restrictions where no new signature, such as class names, is introduced, with a 42% improvement in verification correctness.}
}
@article{GIANNAKOPOULOS2025127117,
title = {NAVMAT: An AI-supported naval failures knowledge management system},
journal = {Expert Systems with Applications},
volume = {277},
pages = {127117},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127117},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425007390},
author = {George Giannakopoulos and Andreas Sideras and Konstantinos Stamatakis and Nikolaos Melanitis},
keywords = {Knowledge management system, Ontology, Information retrieval, Human-expert knowledge instillation},
abstract = {We present “NAVMAT”, an intelligent, multilingual knowledge management platform designed to record and categorize material failure incidents reported in naval operations. This paper provides an overview of the platform, identifying its key software components and highlighting the information retrieval approach used to support user workflows. The platform primarily facilitates real-time, multilingual search and intelligent indexing, streamlining the incident management process while offering valuable insights from past incidents and knowledge resources. To achieve this, it employs a customized natural language processing pipeline integrated with a carefully engineered ontology. The ontology, regularly updated by domain experts, enriches the retrieval mechanism by instilling domain specific knowledge. This approach aims to reduce the significant variability in specialized terminology by promoting convergence towards a unified vocabulary.}
}
@article{NUNEZ2018746,
title = {OntoProg: An ontology-based model for implementing Prognostics Health Management in mechanical machines},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {746-759},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617306080},
author = {David Lira Nuñez and Milton Borsato},
keywords = {Prognostics Health Management, Failure analysis, Ontology engineering},
abstract = {Trends in Prognostics Health Management (PHM) have been introduced into mechanical items of manufacturing systems to predict Remaining Useful Life (RUL). PHM as an estimate of the RUL allows Condition-based Maintenance (CBM) before a functional failure occurs, avoiding corrective maintenance that generates unnecessary costs on production lines. An important factor for the implementation of PHM is the correct data collection for monitoring a machine’s health, in order to evaluate its reliability. Data collection, besides providing information about the state of degradation of the machine, also assists in the analysis of failures for intelligent interventions. Thus, the present work proposes the construction of an ontological model for future applications such as expert system in the support in the correct decision-making, besides assisting in the implementation of the PHM in several manufacturing scenarios, to be used in the future by web semantics tools focused on intelligent manufacturing, standardizing its concepts, terms, and the form of collection and processing of data. The methodological approach Design Science Research (DSR) is used to guide the development of this study. The model construction is achieved using the ontology development 101 procedure. The main result is the creation of the ontological model called OntoProg, which presents: a generic ontology addressing by international standards, capable of being used in several types of mechanical machines, of different types of manufacturing, the possibility of storing the knowledge contained in events of real activities that allow through consultations in SPARQL for decision-making which enable timely interventions of maintenance in the equipment of a real industry. The limitation of the work is that said model can be implemented only by specialists who have knowledge in ontology.}
}
@article{VIGO2019100473,
title = {Comparing ontology authoring workflows with Protégé: In the laboratory, in the tutorial and in the ‘wild’},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100473},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300477},
author = {Markel Vigo and Nicolas Matentzoglu and Caroline Jay and Robert Stevens},
keywords = {Empirical studies, Ontologies, Usability, Semantic web, Authoring tools, Engineering},
abstract = {The development of ontology engineering tools has traditionally lacked a user-centred perspective, instead being guided by the need to address particular gaps indicated by anecdotal evidence. This has typically resulted in prototypes that do not obtain traction beyond a narrow scope. Understanding the authoring patterns of ontology engineers is crucial to informing the development of ontology engineering tools that cater for the activity workflows of the users and, consequently, boosting the adoption of these tools. We report evidence about how Protégé is used across three different authoring settings, addressing the threats to validity of relying on a single user study. These settings address the continuum of expertise (from intermediate to expert users), the type of tasks (whether they are free-form or prescriptive) and the effect of the location (laboratory, tutorial or on their own) and how the studies are administered (whether or not there is a close supervision). While there are activity workflows that are particular to settings, the results indicate a number of core workflows that are common to all of them. We discuss actionable recommendations for ontology engineering tools in light of these results.}
}
@article{SPOLADORE2022103690,
title = {An evaluation of agile Ontology Engineering Methodologies for the digital transformation of companies},
journal = {Computers in Industry},
volume = {140},
pages = {103690},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103690},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000872},
author = {Daniele Spoladore and Elena Pessot},
keywords = {Digital transformation, Industry 4.0, Ontology Engineering, Agile methodologies, Organisational learning},
abstract = {Ontologies are increasingly recognised among the key enablers of the digital transformation of knowledge management processes, but still with a low level of adoption in manufacturing companies. Because ontologies and underlying technologies are complex, Ontology Engineering Methodologies (OEMs) provide a set of guidelines to move from an informal to a formal representation of the company’s knowledge base. This study evaluates three agile OEMs, i.e. UPONLite, SAMOD and RapidOWL, in terms of their process and outcome features, i.e. the OEM steps and the expected quality of the ontological models produced. The assessment is performed from the viewpoint of developers of ontology-based technologies in real industrial use cases. Results show that the three agile OEMs reflect different features to effectively support the digital transformation of companies' knowledge management; thus, they cannot be interchangeable. UPONLite is more effective in contexts where there is a lack of skills in OE, with the need for a structured approach in involving domain experts and generating documentation. SAMOD requires a more extended development period, but with several cycles that allow to map different types of knowledge and enable a “try-and-learn” approach. Conversely, RapidOWL lacks a structured sequence of modelling activities and encourages developers to be creative, but at the same time requires higher expertise in OE. Thus, companies and personnel dedicated to OE should choose the methodology according to the main aims guiding their digitalisation process, the current development status, and the level of expertise.}
}
@article{TRAPPEY2025102332,
title = {Patent litigation mining using a large language model—Taking unmanned aerial vehicle development as the case domain},
journal = {World Patent Information},
volume = {80},
pages = {102332},
year = {2025},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2024.102332},
url = {https://www.sciencedirect.com/science/article/pii/S0172219024000723},
author = {Amy J.C. Trappey and Shao-Chien Chou and Gi-Kuen J. Li},
keywords = {Unmanned aerial vehicle (UAV), Drone, Patent analysis, Patent litigation mining, Technology function matrix, Dynamic topic modeling, Large language model},
abstract = {As unmanned aerial vehicle (UAV), also called “drone”, swiftly advances with innovative functions and applications, the surge in patent applications has profoundly reshaped the intellectual property (IP) landscape in the UAV industry, leading to a growing number of litigations. This study is structured in two phases, aiming to develop an intelligent approach to analyzing the trend and evolution of patent litigations. The first phase involves macro- and micro-patent analyses of the related technology domain. Macro patent analysis elucidates the fundamental patent information in the drone industry, while micro patent analysis leverages the technology function matrix (TFM) to identify R&D hotspots and potentials. The second phase involves litigation (judgement) mining based on large language model (LLM). Beginning with the construction of a knowledge ontology, the domain infringement landscape can be detected through TFMs. A comparative analysis of the two-phase TFMs (i.e., both TFMs of patent and infringement allocations) is then conducted to pinpoint the key legal actions and the relevant technology. To drill deeper in infringement mining, dynamic topic modeling (DTM) is applied to analyze trends and dynamics in drone controller technology over time. This study aims to strengthen IP protection by developing an intelligent litigation mining approach that adopts large language model (LLM) and uses UAV/drone litigation studies as examples to show how the approach being applied in the industry.}
}
@article{SCHONFELDER2025103761,
title = {Ontology-based reasoning in automatic floor plan analysis},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103761},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103761},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625006548},
author = {Phillip Schönfelder and Markus König},
keywords = {Building information modeling, As-built modeling, Floor plan analysis, Knowledge graphs, Semantic enrichment, SPARQL},
abstract = {The growing need for digital representations of existing buildings in the Architecture, Engineering, Construction & Operations (AECO) domain necessitates efficient methods to retrospectively create Building Information Modeling (BIM) models. One prominent approach to obtain the necessary information is Plan-to-BIM, i.e., analyzing building documentation such as floor plans. However, the storage of this information is not standardized which leads to compatibility issues in collaborative scenarios. To address this, the paper presents the Drawing Analysis Ontology (DAnO), which is designed to standardize the representation of technical drawing data extracted through computer vision techniques. Focusing on floor plans, DAnO enables the aggregation, integration, and validation of extracted elements by defining key concepts, such as DrawingElement, DisplayElement, and DescriptionElement, and their relationships. By means of real floor plans, a case study demonstrates the ontology’s effectiveness in facilitating the generation of building models from legacy drawings, highlighting its potential to streamline BIM reconstruction workflows and to enhance interoperability in the AECO industry.}
}
@article{POVEDAVILLALON2022104755,
title = {LOT: An industrial oriented ontology engineering framework},
journal = {Engineering Applications of Artificial Intelligence},
volume = {111},
pages = {104755},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.104755},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622000525},
author = {María Poveda-Villalón and Alba Fernández-Izquierdo and Mariano Fernández-López and Raúl García-Castro},
keywords = {Ontology engineering, Ontology development methodology, Ontology development software support, Collaborative ontology development, Ontology industrial development},
abstract = {Ontology Engineering has captured much attention during the last decades leading to the proliferation of numerous works regarding methodologies, guidelines, tools, resources, etc. including topics which are still being investigated. Even though, there are still many open questions when addressing a new ontology development project, regarding how to manage the overall project and articulate transitions between activities or which tasks and tools are recommended for each step. In this work we propose the Linked Open Terms (LOT) methodology, an overall and lightweight methodology for building ontologies based on existing methodologies and oriented to semantic web developments and technologies. The LOT methodology focuses on the alignment with industrial development, in addition to academic and research projects, and software development, that is making ontology development part of the software industry. This methodology includes lessons learnt from more than 20 years in ontological engineering and its application on 18 projects is reported.}
}
@article{ELGHOSH2025102419,
title = {CriMOnto: A generalized domain-specific ontology for modeling procedural norms of the Lebanese criminal law},
journal = {Data & Knowledge Engineering},
volume = {158},
pages = {102419},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102419},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2500014X},
author = {Mirna {El Ghosh} and Hala Naja and Habib Abdulrab and Mohamad Khalil},
keywords = {AI & Law, Criminal law, Procedural norms, Generalized ontology, UFO, UFO-L, Ontology-Driven Conceptual Modeling, Ontology Patterns, Formal rules},
abstract = {Criminal (or penal) law regulates offenses, offenders, and legal punishments. Modeling criminal law is gaining much attention in the ontology engineering community. However, a significant aspect is neglected: the explicit representation of procedural knowledge. Procedural norms, such as regulative norms, are addressed to agents in the normative system. They govern the different interactions among these agents. In this study, we propose a formal and faithful representation of the procedural aspect of legal norms in the context of the Lebanese Criminal Code. A modular domain-specific ontology named CriMOnto is developed for this purpose. CriMOnto is grounded in the Unified Foundational Ontology (UFO) and the legal core ontology UFO-L by applying the Ontology-Driven Conceptual Modeling (ODCM) process. Conceptual Ontology Patterns (COPs) are reused from UFO and UFO-L to build the hierarchical and procedural content of the ontology. CriMOnto is validated as a formal ontology and evaluated using a dual evaluation approach. The potential use of CriMOnto for lightweight rule-based decision support is discussed in this study.}
}
@article{PATEL2023118998,
title = {An NLP-guided ontology development and refinement approach to represent and query visual information},
journal = {Expert Systems with Applications},
volume = {213},
pages = {118998},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118998},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422020164},
author = {Ashish Singh Patel and Giovanni Merlino and Antonio Puliafito and Ranjana Vyas and O.P. Vyas and Muneendra Ojha and Vivek Tiwari},
keywords = {Semantic web, Multimedia representation, Ontology engineering, Knowledge graph, Information retrieval},
abstract = {The ubiquitous presence of surveillance systems generates massive amounts of video data. Storage and analysis of this data in real-time is a substantial challenge. There is huge potential in representing data in machine-readable and machine-interpretable format due to the presence of hidden semantics in images and videos. However, such representation requires ontology, which calls for expert domain knowledge. In this paper, a novel NLP-guided approach to generate an ontology for multimedia representation and information retrieval is proposed. A semi-automatic NLP-guided framework, which extracts all possible relations among objects is presented. This framework leverages the textual data of the domain to generate possible descriptions and actions within the domain. Relations among objects get embedded as object properties, whereas the category of an object as a class. Features and attributes of objects encode the data properties of the ontology. The proposed ontology is compared with existing multimedia ontologies and evaluated with regard to its capability to represent relations occurring in benchmark datasets, demonstrating the completeness and thorough coverage of the domain concepts. Spatial reasoning rules are established using Semantic Web Rule Language (SWRL) rules, and information retrieval is demonstrated using Description Logic (DL) and SPARQL queries. The proposed NLP-guided ontology generation approach is general enough to help in the development of ontologies for other domains as well, by providing video and textual data of the domain of interest, with limited human involvement.}
}
@article{WIDMER2023100807,
title = {Towards human-compatible XAI: Explaining data differentials with concept induction over background knowledge},
journal = {Journal of Web Semantics},
volume = {79},
pages = {100807},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100807},
url = {https://www.sciencedirect.com/science/article/pii/S1570826823000367},
author = {Cara Leigh Widmer and Md Kamruzzaman Sarker and Srikanth Nadella and Joshua Fiechter and Ion Juvina and Brandon Minnery and Pascal Hitzler and Joshua Schwartz and Michael Raymer},
keywords = {Concept induction, Explainable AI, Class hierarchy},
abstract = {Concept induction, which is based on formal logical reasoning over description logics, has been used in ontology engineering in order to create ontology (TBox) axioms from the base data (ABox) graph. In this paper, we show that it can also be used to explain data differentials, for example in the context of Explainable AI (XAI), and we show that it can in fact be done in a way that is meaningful to a human observer. Our approach utilizes a large class hierarchy, curated from the Wikipedia category hierarchy, as background knowledge. To make the explanations easily understandable for non-specialists, the complex description logic explanations generated by our concept induction system (ECII) were presented as a word list consisting of the concept names occurring in the highest rated system responses.}
}
@article{CHAMARI2025116257,
title = {Towards portable model predictive control-based applications for demand side management in buildings},
journal = {Energy and Buildings},
volume = {347},
pages = {116257},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2025.116257},
url = {https://www.sciencedirect.com/science/article/pii/S0378778825009879},
author = {Lasitha Chamari and Shalika Walker and Ekaterina Petrova and Pieter Pauwels},
keywords = {Brick ontology, Smart charging, Microservices, Resource description framework, Semantic web, Service-oriented architecture},
abstract = {Demand Side Management (DSM) applications in buildings rely on heterogeneous information systems, with data originating from different sources. Semantic Web technologies allow for connecting these disparate data sources by standardising metadata based on ontologies. Recent research focuses on designing portable control applications that can run across buildings. Although the first step towards realising portable control actions is standardising the metadata of the buildings, significant gaps still exist in the literature when it comes to creating portable Model Predictive Control (MPC)-based DSM applications. Many existing portable applications are simple rule-based programmes and the principles of semantic portability are not clear in the areas like with MPC systems for DSM. This paper proposes a combination of modular services and metadata standardisation towards making MPC systems more portable across buildings. The method consists of (1) decomposing the MPC system into modular and reusable services and exposing their data using already standardised web interfaces, (2) extending metadata schemes to formalise the information requirements of the modular services, and (3) devising a modular semantic-driven portability service to query, validate, and configure the MPC system. Although full portability remains a challenge due to heterogeneity in building systems and metadata modelling styles, our approach demonstrates the feasibility of using standardised ontologies, semantic validation, and modular services to partially automate configuration and integration, thereby a step towards portable MPC applications. The proposed workflow is implemented, tested, and validated in a MPC system as part of a DSM strategy for controlling Electric Vehicle (EV) charging behaviour in an office microgrid system.}
}
@article{FERNANDEZIZQUIERDO2021104026,
title = {Conformance testing of ontologies through ontology requirements},
journal = {Engineering Applications of Artificial Intelligence},
volume = {97},
pages = {104026},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.104026},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620303079},
author = {Alba Fernández-Izquierdo and Raúl García-Castro},
keywords = {Ontology conformance, Ontology engineering, Ontology testing, Standard},
abstract = {In recent years, several standard ontologies have been developed to maximise semantic interoperability in different domains; such standard ontologies ensure quality and integrity when describing a domain. Therefore, mechanisms to guarantee that developers build ontologies that conform to such standards are needed. However, while in fields such as Software Engineering or industry, conformance testing plays an essential role during product development, in the Ontology Engineering field there is a lack of techniques for this type of testing. This work introduces an ontology conformance testing method to analyse conformance between an ontology and a standard based on the standard requirements. Grounded on this method, the work also presents a minimum common knowledge identification method for analysing how a group of standards covers a particular domain and for identifying whether there are conflicts between them. This work has been validated by analysing the conformance between an ontology network and a set of standards on the Internet of Things domain, and by analysing the minimum common knowledge between such standards. This analysis shows that the conformance between ontologies and standards is mostly related to definition of classes. Furthermore, the analysis shows that although the analysed standards are related to the same domain, they are created to describe different areas of concern and, thus, there is a minimum overlap between them. Finally, it was concluded that the quality of the conformance analysis depends on the quality of the requirements specification: the more precise the requirements, the more precise the analysis between ontologies and standards.}
}
@article{WANG2020,
title = {Using Natural Language Processing Techniques to Provide Personalized Educational Materials for Chronic Disease Patients in China: Development and Assessment of a Knowledge-Based Health Recommender System},
journal = {JMIR Medical Informatics},
volume = {8},
number = {4},
year = {2020},
issn = {2291-9694},
doi = {https://doi.org/10.2196/17642},
url = {https://www.sciencedirect.com/science/article/pii/S2291969420001556},
author = {Zheyu Wang and Haoce Huang and Liping Cui and Juan Chen and Jiye An and Huilong Duan and Huiqing Ge and Ning Deng},
keywords = {health education, ontology, natural language processing, chronic disease, recommender system},
abstract = {Background
Health education emerged as an important intervention for improving the awareness and self-management abilities of chronic disease patients. The development of information technologies has changed the form of patient educational materials from traditional paper materials to electronic materials. To date, the amount of patient educational materials on the internet is tremendous, with variable quality, which makes it hard to identify the most valuable materials by individuals lacking medical backgrounds.
Objective
The aim of this study was to develop a health recommender system to provide appropriate educational materials for chronic disease patients in China and evaluate the effect of this system.
Methods
A knowledge-based recommender system was implemented using ontology and several natural language processing (NLP) techniques. The development process was divided into 3 stages. In stage 1, an ontology was constructed to describe patient characteristics contained in the data. In stage 2, an algorithm was designed and implemented to generate recommendations based on the ontology. Patient data and educational materials were mapped to the ontology and converted into vectors of the same length, and then recommendations were generated according to similarity between these vectors. In stage 3, the ontology and algorithm were incorporated into an mHealth system for practical use. Keyword extraction algorithms and pretrained word embeddings were used to preprocess educational materials. Three strategies were proposed to improve the performance of keyword extraction. System evaluation was based on a manually assembled test collection for 50 patients and 100 educational documents. Recommendation performance was assessed using the macro precision of top-ranked documents and the overall mean average precision (MAP).
Results
The constructed ontology contained 40 classes, 31 object properties, 67 data properties, and 32 individuals. A total of 80 SWRL rules were defined to implement the semantic logic of mapping patient original data to the ontology vector space. The recommender system was implemented as a separate Web service connected with patients' smartphones. According to the evaluation results, our system can achieve a macro precision up to 0.970 for the top 1 recommendation and an overall MAP score up to 0.628.
Conclusions
This study demonstrated that a knowledge-based health recommender system has the potential to accurately recommend educational materials to chronic disease patients. Traditional NLP techniques combined with improvement strategies for specific language and domain proved to be effective for improving system performance. One direction for future work is to explore the effect of such systems from the perspective of patients in a practical setting.}
}
@article{SPOLADORE2023103979,
title = {A novel agile ontology engineering methodology for supporting organizations in collaborative ontology development},
journal = {Computers in Industry},
volume = {151},
pages = {103979},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103979},
url = {https://www.sciencedirect.com/science/article/pii/S016636152300129X},
author = {Daniele Spoladore and Elena Pessot and Alberto Trombetta},
keywords = {Ontology Engineering, Ontology Authoring, Agile Ontology Development Methodology, Knowledge Engineering, Industry 4.0},
abstract = {Ontologies can represent technological enablers for knowledge elicitation and management in different kinds of organizations, especially with the exponential growth of sources and types of data fostered by digital transformation. However, their adoption in business applications is still limited, with existing Ontology Engineering Methodologies (OEMs) lacking adequate support during knowledge elicitation, authoring and reuse phases. This paper introduces a novel agile ontology engineering methodology (AgiSCOnt) to support ontologists (especially novice ones) in ontology development workflow, fostering collaboration with domain experts in an iterative, flexible and customizable approach. AgiSCOnt combines macro-level instructions with micro-level guidance, leveraging existing techniques and a management framework to help novice ontologists throughout the whole ontology engineering process. The methodology is compared to existing OEMs and assessed with three other agile methodologies (UPONLite, SAMOD, and RapidOWL). The evaluation is conducted with a sample of novice ontologists in a learning environment on Industry 4.0 technologies. Both the development process with a methodology from a user perspective and the quality of the developed ontologies were considered in the evaluation. Preliminary results show that AgiSCOnt effectively supports authoring and reuse, with developed ontologies of good quality. It is perceived as clear and simple, while being flexible and adaptable enough, thus supporting knowledge management and sharing in industrial organizations through the documentation of the ontologies.}
}
@article{EDDINEMEFTAH2025440,
title = {An Intelligent Arabic Legal Assistant system (IALAS) based on Ontology},
journal = {Transportation Research Procedia},
volume = {84},
pages = {440-447},
year = {2025},
note = {Smart Mobility and Logistics Ecosystems},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2025.03.094},
url = {https://www.sciencedirect.com/science/article/pii/S2352146525001425},
author = {Mohammed Charaf {Eddine Meftah} and Abdelhak Soussa and Adel Herzallah},
keywords = {Legal texts, Information search, Natural language processing (Arabic), Ontology, An intelligent system},
abstract = {Laws and regulations can be modified by experts in the legal field in response to various changes in the lives of individuals and communities. Massive changes and updates are constantly being made to laws to adapt to societal changes. This creates a huge database of legal information. Manually searching for information in this database takes a lot of time and effort and affects the efficiency and governance of all administrative and community affairs. To solve this problem, this paper proposes a solution based on one of the types of artificial intelligence. It is an ontology-based solution. This paper explains the design and development of a computer advisory system that helps in making legal decisions based on a proposed ontological structure using Protégé. A set of tools were also chosen to develop the proposed system. For operation, OwlReady2 with SPARQL query language was also used to extract content from the proposed ontology, Camel tools as a natural language processing (Arabic) tool, and SQLite for the database. This work contributes to filling a gap regarding the Arab cognitive modeling of Arab laws to keep pace in sustainable cognitive cities.}
}
@article{SHAW2025106282,
title = {Knowledge graph for policy- and practice-aligned life cycle analysis and reporting},
journal = {Automation in Construction},
volume = {176},
pages = {106282},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106282},
url = {https://www.sciencedirect.com/science/article/pii/S092658052500322X},
author = {Conor Shaw and Flávia {de Andrade Pereira} and Martijn {de Riet} and Cathal Hoare and Karim Farghaly and James O’Donnell},
keywords = {Environmental policy, Life cycle assessment, Asset management, Information management, Requirements engineering, Knowledge graph, Ontology engineering},
abstract = {The built environment is a key leverage point for policy intervention to combat climate change and the statutory reporting of financial and non-financial indicators over the asset lifecycle is increasingly required. This poses significant information management challenges in a sector characterised by complexity. Contributions to-date which address Life Cycle Asset Information Management (LCAIM) remain siloed and difficult to generalise, resulting in limited in-practice uptake, but domain literature identifies graph databases and ontologies as suitable strategies for addressing this information-intensive challenge. This paper provides a LCAIM ontology, co-developed with stakeholders, and verified technically through implementation in a case study by responding to end-user-defined storage, retrieval, and enrichment functions using a knowledge graph. The prototype is then validated qualitatively with experts who perceive it as addressing collective governance-practice requirements. Overall, the study suggests that addressing technical LCAIM challenges may be feasible using available technologies and recommends prioritising research towards socio-economic issues.}
}
@article{ALI2018127,
title = {Cross-Lingual Ontology Enrichment Based on Multi-Agent Architecture},
journal = {Procedia Computer Science},
volume = {137},
pages = {127-138},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S187705091831617X},
author = {Mohamed Ali and Said Fathalla and Shimaa Ibrahim and Mohamed Kholief and Yasser Hassan},
keywords = {cross-lingual ontology enrichment, multi-agent, knowledge management, ontology learning.},
abstract = {The proliferation of ontologies and multilingual data available on the Web has motivated many researchers to contribute to multilingual and cross-lingual ontology enrichment. Cross-lingual ontology enrichment greatly facilitates ontology learning from multilingual text/ontologies in order to support collaborative ontology engineering process. This article proposes a cross-lingual ontology enrichment (CLOE) approach based on a multi-agent architecture in order to enrich ontologies from a multilingual text or ontology. This has several advantages: 1) an ontology is used to enrich another one, written in a different natural language, and 2) several ontologies could be enriched at the same time using a single chunk of text (Simultaneous Ontology Enrichment). A prototype for the proposed approach has been implemented in order to enrich several ontologies using English, Arabic and German text. Evaluation results are promising and showing that CLOE performs well in comparison with four state-of-the-art approaches.}
}
@article{PANKOWSKA201911,
title = {Business Models in CMMN, DMN and ArchiMate language},
journal = {Procedia Computer Science},
volume = {164},
pages = {11-18},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.148},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321878},
author = {Malgorzata Pankowska},
keywords = {business modelling, ArchiMate, CMMN, DMN, BPMN, business model mapping},
abstract = {Business modelling can be considered as a practice for enabling change in enterprise by defining recommended Information Communication Technology (ICT) solutions, which provide value to business stakeholders. Business modelling and business analyses can be connected with different business models, techniques, and software tools. The goal of the paper is to present and discuss which business models are already well known for enterprise architecture (EA) business analysis. The literature review was done for this purpose. Beyond that, the paper aims to present a classification of business models and their mapping in ArchiMate language into different notations and languages diagrams for information architecture modelling. Proposed in this paper the business model mapping was applied in author’s earlier projects as well as it is used in university course teaching on system analysis and modelling.}
}
@article{MASSARI20232392,
title = {Effectiveness of applying Machine Learning techniques and Ontologies in Breast Cancer detection},
journal = {Procedia Computer Science},
volume = {218},
pages = {2392-2400},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.214},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923002144},
author = {Hakim El Massari and Noreddine Gherabi and Sajida Mhammedi and Zineb Sabouri and Hamza Ghandi and Fatima Qanouni},
keywords = {Prediction, Ontology, Machine Learning, SWRL, Breast cancer},
abstract = {Breast cancer is a disease that primarily affects women, but it can also affect men, although in a much smaller percentage. Recently, doctors have made great strides in this trend of early detection and treatment of breast cancer to reduce the number of deaths caused by this serious disease. Moreover, researchers are analyzing massive amounts of sophisticated medical data using a combination of statistical and machine learning approaches to help clinicians predict breast cancer. In the presented work, an ontological model based on the decision tree algorithm capable of reliably predicting breast cancer has been demonstrated. The method consists of extracting rules from the decision tree algorithm that distinguish between malignant and benign breast cancer patients, and then implementing these rules in the ontological reasoner via the Semantic Web Rule Language (SWRL). The results indicated that the ontological model achieved the highest prediction accuracy of 97.10%.}
}
@article{IQBAL2024102257,
title = {Blockchain-based ontology driven reference framework for security risk management},
journal = {Data & Knowledge Engineering},
volume = {149},
pages = {102257},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102257},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23001179},
author = {Mubashar Iqbal and Aleksandr Kormiltsyn and Vimal Dwivedi and Raimundas Matulevičius},
keywords = {Blockchain, Security risk management, Ontology framework, Web ontology language, Unified foundational ontology, CPNs tool},
abstract = {Security risk management (SRM) is crucial for protecting valuable assets from malicious harm. While blockchain technology has been proposed to mitigate security threats in traditional applications, it is not a perfect solution, and its security threats must be managed. This paper addresses the research problem of having no unified and formal knowledge models to support the SRM of traditional applications using blockchain and the SRM of blockchain-based applications. In accordance with this, we present a blockchain-based reference model (BbRM) and an ontology driven reference framework (OntReF) for the SRM of traditional and blockchain-based applications. The BbRM consolidates security threats of traditional and blockchain-based applications, structured following the SRM domain model and offers guidance for creating the OntReF using the domain model. OntReF is grounded on unified foundational ontology (UFO) and provides semantic interoperability and supporting the dynamic knowledge representation and instantiation of information security knowledge for the SRM. Our evaluation approaches demonstrate that OntReF is practical to use.}
}
@article{DEEPAK2022107736,
title = {An artificially intelligent approach for automatic speech processing based on triune ontology and adaptive tribonacci deep neural networks},
journal = {Computers & Electrical Engineering},
volume = {98},
pages = {107736},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107736},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622000489},
author = {Gerard Deepak and Deepak Surya and Ishdutt Trivedi and Ayush Kumar and Amrutha Lingampalli and Santhana vijayan},
keywords = {Acoustic model, Automatic speech recognition, Tribonacci deep neural network},
abstract = {Automatic Speech Recognition systems have become essential for an independent automation during the present-day era. A hybrid approach for Automatic Speech Recognition, the TriNNOnto has been proposed in this paper which, integrates different approaches like Language Model integrated with dynamic Triune Ontology generation scheme, Acoustic Model and Feature modelling are hybridised based on the Tribonacci based Deep Neural Network, which decides upon the number of layers depending on the size of the samples and their count. The dynamic generation of Ontologies based on the language models and triune ontology for automatic speech recognition is quite novel. The strategies for feature extraction as and the Tribonacci based deep neural network, based the dynamic adjustment of the number of layers using Tribonacci series contributes towards novelty as well as enhances the performance of speech recognition. The proposed strategy has been evaluated for two datasets and an accuracy of 98.15% and 95.18%, have been achieved for the CMUKids and the TIMIT datasets, respectively with low word error rates.}
}
@article{SPOLADORE2024,
title = {An Ontology-Based Decision Support System for Tailored Clinical Nutrition Recommendations for Patients With Chronic Obstructive Pulmonary Disease: Development and Acceptability Study},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/50980},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000723},
author = {Daniele Spoladore and Vera Colombo and Alessia Fumagalli and Martina Tosi and Erna Cecilia Lorenzini and Marco Sacco},
keywords = {ontology-based decision support system, nutritional recommendation, chronic obstructive pulmonary disease, clinical decision support system, pulmonary rehabilitation},
abstract = {Background
Chronic obstructive pulmonary disease (COPD) is a chronic condition among the main causes of morbidity and mortality worldwide, representing a burden on health care systems. Scientific literature highlights that nutrition is pivotal in respiratory inflammatory processes connected to COPD, including exacerbations. Patients with COPD have an increased risk of developing nutrition-related comorbidities, such as diabetes, cardiovascular diseases, and malnutrition. Moreover, these patients often manifest sarcopenia and cachexia. Therefore, an adequate nutritional assessment and therapy are essential to help individuals with COPD in managing the progress of the disease. However, the role of nutrition in pulmonary rehabilitation (PR) programs is often underestimated due to a lack of resources and dedicated services, mostly because pneumologists may lack the specialized training for such a discipline.
Objective
This work proposes a novel knowledge-based decision support system to support pneumologists in considering nutritional aspects in PR. The system provides clinicians with patient-tailored dietary recommendations leveraging expert knowledge.
Methods
The expert knowledge—acquired from experts and clinical literature—was formalized in domain ontologies and rules, which were developed leveraging the support of Italian clinicians with expertise in the rehabilitation of patients with COPD. Thus, by following an agile ontology engineering methodology, the relevant formal ontologies were developed to act as a backbone for an application targeted at pneumologists. The recommendations provided by the decision support system were validated by a group of nutrition experts, whereas the acceptability of such an application in the context of PR was evaluated by pneumologists.
Results
A total of 7 dieticians (mean age 46.60, SD 13.35 years) were interviewed to assess their level of agreement with the decision support system’s recommendations by evaluating 5 patients’ health conditions. The preliminary results indicate that the system performed more than adequately (with an overall average score of 4.23, SD 0.52 out of 5 points), providing meaningful and safe recommendations in compliance with clinical practice. With regard to the acceptability of the system by lung specialists (mean age 44.71, SD 11.94 years), the usefulness and relevance of the proposed solution were extremely positive—the scores on each of the perceived usefulness subscales of the technology acceptance model 3 were 4.86 (SD 0.38) out of 5 points, whereas the score on the intention to use subscale was 4.14 (SD 0.38) out of 5 points.
Conclusions
Although designed for the Italian clinical context, the proposed system can be adapted for any other national clinical context by modifying the domain ontologies, thus providing a multidisciplinary approach to the management of patients with COPD.}
}
@article{FH2025112762,
title = {BIM ontology for information management (BIM-OIM)},
journal = {Journal of Building Engineering},
volume = {107},
pages = {112762},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.112762},
url = {https://www.sciencedirect.com/science/article/pii/S2352710225009994},
author = {Abanda F.H and Akintola A and Tuhaise V.V and Tah J.H.M},
keywords = {BIM, BIM execution plan, Information management, ISO 19650, Ontology},
abstract = {The adoption of Building Information Modelling (BIM) in the construction industry has been hindered by numerous barriers, notably the limited understanding of its concepts, protocols, and the intricate interplay between processes, people, and technologies. To address these challenges, a range of standards and guidelines have been developed, most notably the ISO 19650 series, which offer a comprehensive framework for implementing various aspects of BIM in construction projects. However, despite the BIM's collaborative philosophy, the standards and specifications that guide its adoption and implementation seldom reveal and explain the relationships between their key elements and concepts. This lack of clarity limits understanding and undermines the very essence of collaboration that BIM seeks to promote in construction projects. The text-based nature of the standards and specifications makes it difficult to identify common concepts that cut across the different project phases, their relationships, and interdependencies. This study proposes a BIM ontology for information management (BIM-OIM) that makes BIM process data more available and easily useable, allowing other researchers and practitioners to implement, and extend its use within their domains of practice. To achieve the practice-driven goal of BIM-OIM, Yet Another Methodology for Ontology (YAMO), one of the leading ontology engineering methodologies, was used to develop BIM-OIM. BIM-OIM is a formal and structured representation of ISO 19650 knowledge that is machine-processable. This representation enhances understanding, promotes reusability, and supports practical applications throughout the information management lifecycle. Key applications include the development of BIM Execution Plans, compliance checking for information containers, and identifying the roles of various stakeholders within a project.}
}
@article{SHIMIZU2025100862,
title = {Accelerating knowledge graph and ontology engineering with large language models},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100862},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100862},
url = {https://www.sciencedirect.com/science/article/pii/S1570826825000022},
author = {Cogan Shimizu and Pascal Hitzler},
keywords = {Knowledge graph engineering, Ontology engineering, Large language models, Modular ontologies, Ontology modeling, Ontology population, Ontology alignment, Entity disambiguation},
abstract = {Large Language Models bear the promise of significant acceleration of key Knowledge Graph and Ontology Engineering tasks, including ontology modeling, extension, modification, population, alignment, as well as entity disambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering as a new and coming area of research, and argue that modular approaches to ontologies will be of central importance.}
}
@article{NAQVI20223679,
title = {Ontological Model for Cohesive Smart Health Services Management},
journal = {Computers, Materials and Continua},
volume = {74},
number = {2},
pages = {3679-3695},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.030340},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822004052},
author = {Muhammad Raza Naqvi and Muhammad Waseem Iqbal and Syed Khuram Shahzad and M. {Usman Ashraf} and Khalid Alsubhi and Hani Moaiteq Aljahdali},
keywords = {Ontology, internet of things, smart health, services integration},
abstract = {Health care has become an essential social-economic concern for all stakeholders (e.g., patients, doctors, hospitals etc.), health needs, private care and the elderly class of society. The massive increase in the usage of health care Internet of things (IoT) applications has great technological evolvement in human life. There are various smart health care services like remote patient monitoring, diagnostic, disease-specific remote treatments and telemedicine. These applications are available in a split fashion and provide solutions for variant diseases, medical resources and remote service management. The main objective of this research is to provide a management platform where all these services work as a single unit to facilitate the users. The ontological model of integrated healthcare services is proposed by getting requirements from various existing healthcare services. There were 26 smart health care services and 26 smart health care services to classify the knowledge-based ontological model. The proposed ontological model is derived from different classes, relationships, and constraints to integrate health care services. This model is developed using Protégé based on each interrelated/correlated health care service having different values. Semantic querying SPARQL protocol and RDF query language (SPARQL) were used for knowledge acquisition. The Pellet Reasoner is used to check the validity and relations coherency of the proposed ontology model. Comparative to other smart health care services integration systems, the proposed ontological model provides more cohesiveness.}
}
@article{HAGEDORN2025103369,
title = {OntoBPR: An ontology-based framework for performing building permit reviews using standardized information containers},
journal = {Advanced Engineering Informatics},
volume = {66},
pages = {103369},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103369},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625002629},
author = {Philipp Hagedorn and Judith Fauth and Sven Zentgraf and Sebastian Seiß and Markus König and Ioannis Brilakis},
keywords = {Digital building permit, Building permit review, Ontology & semantic web, Information Container for linked Document Delivery (ICDD), Compliance checking, Shapes Constraint Language (SHACL)},
abstract = {Building permitting is essential for ensuring the safety, sustainability, and societal alignment of construction projects. Despite interest from both practitioners and researchers, the process remains largely manual and fragmented. Ontologies offer a promising solution by managing complexity and enabling automation through semantic information, though current ontologies in the building permit domain are limited to specific aspects like building code checking. On the process level, the OntoBPR framework integrates multiple domain-specific ontologies for a seamless digital permitting process and provides a workflow to automate the lifecycle of the permit review. Therefore, it suggests integrating the submitted building application using standardized information containers. The paper explores how digital applications can be submitted, reviewed, verified for completeness, and forwarded to authorities, and how permit review results can be gathered to support decision-making and automate notification issuance, and it provides a demonstration in a case study. In conclusion, OntoBPR formalizes a multi-layered ontology that advances and aligns the partitioned building permit process and provides an adaptable framework to harmonize diverse legal, informatics, and procedural aspects.}
}
@article{DECKERS2022111415,
title = {Systematic literature review of domain-oriented specification techniques},
journal = {Journal of Systems and Software},
volume = {192},
pages = {111415},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111415},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001261},
author = {Robert Deckers and Patricia Lago},
keywords = {Domain-specific language, Domain model, Systematic literature review, Method comparison, Specification method, Modeling language},
abstract = {Context:
The popularity of domain-specific languages and model driven development has made the tacit use of domain knowledge in system development more tangible. Our vision is a development process where a (software) system specification is based on multiple domain models, and where the specification method is built from cognitive concepts, presumably derived from natural language.
Goal:
To realize this vision, we evaluate and reflect upon the existing literature in domain-oriented specification techniques.
Method:
We designed and conducted a systematic literature review on domain-oriented specification techniques.
Results:
We identified 53 primary studies, populated the classification framework for each study, and summarized our findings per classification aspect. We found many approaches for creating domain models or domain-specific languages. Observations include: (i) most methods are defined incompletely; (ii) none offers methodical support for the use of domain models or domain-specific languages to create other specifications; (iii) there are specification techniques to integrate models in general, but no study offers methodical support for multiple domain models.
Conclusion:
The results indicate which topics need further research and which can instead be reused to realize our vision on system development. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{FONSECA2021101894,
title = {Multi-level conceptual modeling: Theory, language and application},
journal = {Data & Knowledge Engineering},
volume = {134},
pages = {101894},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101894},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000215},
author = {Claudenir M. Fonseca and João Paulo A. Almeida and Giancarlo Guizzardi and Victorio A. Carvalho},
keywords = {Multi-level modeling, Modeling language, Conceptual modeling, Methodologies and tools},
abstract = {In many important subject domains, there are central real-world phenomena that span across multiple classification levels. In these subject domains, besides having the traditional type-level domain regularities (classes) that classify multiple concrete instances, we also have higher-order type-level regularities (metaclasses) that classify multiple instances that are themselves types. Multi-Level Modeling aims to address this technical challenge. Despite the advances in this area in the last decade, a number of requirements arising from representation needs in subject domains have not yet been addressed in current modeling approaches. In this paper, we address this issue by proposing an expressive multi-level conceptual modeling language (dubbed ML2). We follow a principled language engineering approach in the design of ML2, constructing its abstract syntax as to reflect a fully axiomatized theory for multi-level modeling (termed MLT*). We show that ML2 enables the expression of a number of multi-level modeling scenarios that cannot be currently expressed in the existing multi-level modeling languages. A textual syntax for ML2 is provided with an implementation in Xtext. We discuss how the formal theory influences the language in two aspects: (i) by providing rigorous justification for the language’s syntactic rules, which follow MLT* theorems and (ii) by forming the basis for model simulation and verification. We show that the language can reveal problems in multi-level taxonomic structures, using Wikidata fragments to demonstrate the language’s practical relevance.}
}
@article{LECU2024443,
title = {Using LLMs and ontologies to extract causal relationships from medical abstracts},
journal = {Procedia Computer Science},
volume = {244},
pages = {443-452},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.219},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030205},
author = {Alexandru Lecu and Adrian Groza and Lezan Hawizy},
keywords = {Causal Relation Extraction, Knowledge Graphs, Large Language Models, Age-Related Macular Degeneration},
abstract = {The substantiation of the causal relationships behind its development is very important in identifying possible interventions and early treatment. Knowledge Graphs (KG) play a crucial role in the medical research domain by organizing data into interconnected structures that represent relationships between entities such as disease, treatments, and progressions. This paper shows a complete workflow that demonstrates the extraction of causal relationships from medical abstracts using a fine-tuned GPT-based model and the integration of these relationships into a KG.}
}
@article{HARI2023367,
title = {WSD based Ontology Learning from Unstructured Text using Transformer},
journal = {Procedia Computer Science},
volume = {218},
pages = {367-374},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923000194},
author = {Akshay Hari and Priyanka Kumar},
keywords = {Deep Learning, Transformers, Ontology, Word Sense Disambiguation, RDF},
abstract = {Representation of knowledge and making it machine comprehensible has become a necessity in modern times but with the large amount of data being generated nowadays, this process has to be automated as much as possible. In this work, we propose a deep-learning based model to build an RDF based Ontology from Unstructured Text. We aim to evaluate the proposed model by creating a general knowledge ontology from newspaper article corpora. The proposed model is based on transformer, Natural Language Processing and contains a Relation Extraction model and novel implementation of RDF mapping algorithm. The main highlight of our model is its ability to handle the Word Sense Disambiguation problem. The model was able to perform well and achieved very high accuracy scores.}
}
@article{JUST20252567,
title = {The Independent Event Log Layer (IELL): Semantic Integration of Industrial IoT Event Logs},
journal = {Procedia Computer Science},
volume = {253},
pages = {2567-2574},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.316},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925003242},
author = {Valentin P. Just and Steindl Gernot and Wolfgang Kastner},
keywords = {IoT, IIoT, Event log, Process-Mining, Ontology, RDF},
abstract = {The Industrial Internet of Things (IIoT) has significantly transformed manufacturing by enabling the integration of physical and digital systems, resulting in extensive data generation. However, extracting actionable insights from this heterogeneous data poses significant challenges due to its distributed nature and the varied architectures and formats from multiple vendors. A unified access method for data processing is essential to overcome these obstacles. This paper introduces the concept of an Independent Event Log Layer (IELL), leveraging the Resource Description Framework (RDF) and ontology-based knowledge representation to standardise and analyse event logs from disparate formats like eXtensible Event Stream (XES) and Comma-Separated Values (CSV). Utilising the RDF Mapping Language (RML), we propose a novel approach to convert event logs into RDF files, creating a unified knowledge base that enhances process mining capabilities. This semantic abstraction facilitates advanced knowledge retrieval and analysis, linking various events and attributes to optimise IIoT processes. A proof-of-concept implementation demonstrates the feasibility of our approach using openly available event log data and RML tooling. The findings underscore the potential of IELL to streamline process mining in IIoT environments, providing unified access for knowledge retrieval and process optimisation.}
}
@article{LONGO2022594,
title = {An ontology-based, general-purpose and Industry 4.0-ready architecture for supporting the smart operator (Part I – Mixed reality case)},
journal = {Journal of Manufacturing Systems},
volume = {64},
pages = {594-612},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001303},
author = {Francesco Longo and Giovanni Mirabelli and Letizia Nicoletti and Vittorio Solina},
keywords = {Mixed reality, Ontology, Internet of things, Smart operator, Smart factory},
abstract = {The advent of novel industry 4.0-driven technologies is offering significant opportunities to manufacturing systems, but at the same time it is posing new great challenges. The growing number of connected and interconnected devices is enormously increasing the amount of data generated, which must be properly organized to give value to the business. Basically, the need for approaches that are able to guarantee compliance with FAIR data principles is significantly emerging. Recently, the KNOW4I platform has been proposed in the literature to support the smart operator through a suite of Smart Utilities and Objects (Longo et al., 2022). The main purpose of this paper is to extend such platform, in the form of an ontology-based, general-purpose and industry 4.0-ready architecture, capable of improving the capabilities of the smart operator, with a focus on mixed reality. The novel proposal is based on two fundamental aspects: (1) a new general ontology, developed through the ontology engineering methodology; (2) the adoption of FIWARE, an open-source infrastructure, capable of enabling interoperability between different systems. The proposed architecture is implemented and validated on two case studies belonging to the manufacturing sector, which respectively concern (1) scheduled maintenance and alarm management and (2) customer order management. The experimental phase shows that the architecture is able to effectively and efficiently support the smart operator.}
}
@article{SIOUGKROU2018385,
title = {Semantically-enabled repositories in multi-disciplinary domains: The case of biorefineries},
journal = {Computers & Chemical Engineering},
volume = {116},
pages = {385-400},
year = {2018},
note = {Multi-scale Systems Engineering – in memory & honor of Professor C.A. Floudas},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2018.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S009813541830348X},
author = {Eirini Siougkrou and Filopoimin Lykokanellos and Foteini Barla and Antonis C. Kokossis},
keywords = {Ontology engineering, Biorefineries, Biorenewables, Repository, Synthesis of value chain},
abstract = {There is an increased use of problem representations (i.e. superstructures in synthesis problems; networks in route problems; graphs; ordered graphs in various systems representations) following on significant advances in optimization technologies that hold capabilities to solve, robustly, large-scale problems. In an attempt to systematically tackle disparate domains and build high-throughput functions, the paper contributes with a semantically-enabled approach systematized and engineered by ontologies. The aim is to develop an intelligent environment with capabilities to build and scale-up system representations, automatically. The work is demonstrated on problems akin to biorenewables and biorefineries; an identical approach is possible to the general problem. Using relations and rules defined among entities, semantics are deployed to model and expand domains (biorefinery pathways) whereas enabling extracting and creating knowledge. The repository, already on a web-based platform and available as open-source, essentially upgrades conventional representations with capabilities to share (import/export) and integrate its content externally.}
}
@article{GONZALEZERAS2022100816,
title = {Ontological engineering for the definition of a COVID-19 pandemic ontology},
journal = {Informatics in Medicine Unlocked},
volume = {28},
pages = {100816},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2021.100816},
url = {https://www.sciencedirect.com/science/article/pii/S2352914821002811},
author = {Alexandra González-Eras and Ricardo Dos Santos and Jose Aguilar and Alberto Lopez},
keywords = {COVID ontology, COVID-19, Ontology integration, Ontological engineering},
abstract = {COVID-19 has generated a lot of information in different formats, and one of them is in the ontology format. Also, there are previous ontologies from other disciplines that can help to analyze the COVID-19 pandemic. Thus, due to the large quantity of COVID-19 information in the form of ontologies, approaches to ontology integration and interoperability could be beneficial. In this context, this research proposes a new ontology, called COVID-19 Pandemic ontology, which is the product of an ontological engineering process proposed in this research that allows the integration of several ontologies to cover all the aspects of this infectious disease. The ontological engineering process defines tasks of fusion, alignment, and linking for integrating the ontologies. The resulting pandemic ontology provides a simple repository for storing information about the COVID-19, reusing existing ontologies, to offer multiple views about the disease, including the social context. This ontology has been tested in different case studies to prove its capabilities to infer useful information about the COVID-19 pandemic.}
}
@article{AMINU2022200125,
title = {MaCOnto: A robust maize crop ontology based on soils, fertilizers and irrigation knowledge},
journal = {Intelligent Systems with Applications},
volume = {16},
pages = {200125},
year = {2022},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2022.200125},
url = {https://www.sciencedirect.com/science/article/pii/S266730532200062X},
author = {Enesi Femi Aminu and Ishaq Oyebisi Oyefolahan and Muhammad Bashir Abdullahi and Muhammadu Tajudeen Salaudeen},
keywords = {Maconto, Ontology evolution, Competency question, Maize's soils knowledge, Maize's fertilizer knowledge, Maize's irrigation knowledge},
abstract = {The demand for relevant information in a timely manner portrays the significance of knowledge management in all areas of lives; for instance, agriculture. To this end, soils, fertilizers and irrigation as agronomic concepts are essential knowledge inputs for any crops, such as maize. Conversely, there is always difficulty in timely retrieval of these relevant information owing to the unstructured nature of data in repositories, and complexity of concepts mismatch. Sequel to this development, ontology, a semantic data modeling technique is promising as it has been recently employed to deal with these challenges across different domains. However, the robustness of ontology, in terms of semantic expressivity of hidden knowledge, and autonomous growth of ontology leave some gaps to contend with. In view of this development, this research aims to design a robust OWL Rule based ontology for maize crop domain by considering primarily soils, fertilizers and irrigation agronomic concepts capable to evolve autonomously. The proposed ontology herein christened MaCOnto, is developed using the adapted six steps ontology-engineering principle. Over 1,430 entities are encoded in OWL; eighty Competency Questions (CQs) validated by domain experts are modeled in FOL, and implemented as rules via SWRL. Thus, the ontology is queried by SQWRL. Besides, the novel algorithmic design for the ontology to autonomously evolve is implemented in Java environment by employing WordNet. The results obtained from structural based evaluation show an outstanding performance across the eight metrics. Similarly, the results of the competency-based evaluation are also promising. Therefore, the proposed MaCOnto is a robust application based ontology capable to infer and responds to user's query based on its contextual information.}
}
@article{CIROKU2024100822,
title = {RevOnt: Reverse engineering of competency questions from knowledge graphs via language models},
journal = {Journal of Web Semantics},
volume = {82},
pages = {100822},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100822},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000088},
author = {Fiorela Ciroku and Jacopo {de Berardinis} and Jongmo Kim and Albert Meroño-Peñuela and Valentina Presutti and Elena Simperl},
keywords = {Knowledge engineering, Knowledge graph, Ontology development, Competency question extraction},
abstract = {The process of developing ontologies – a formal, explicit specification of a shared conceptualisation – is addressed by well-known methodologies. As for any engineering development, its fundamental basis is the collection of requirements, which includes the elicitation of competency questions. Competency questions are defined through interacting with domain and application experts or by investigating existing datasets that may be used to populate the ontology i.e. its knowledge graph. The rise in popularity and accessibility of knowledge graphs provides an opportunity to support this phase with automatic tools. In this work, we explore the possibility of extracting competency questions from a knowledge graph. This reverses the traditional workflow in which knowledge graphs are built from ontologies, which in turn are engineered from competency questions. We describe in detail RevOnt, an approach that extracts and abstracts triples from a knowledge graph, generates questions based on triple verbalisations, and filters the resulting questions to yield a meaningful set of competency questions; the WDV dataset. This approach is implemented utilising the Wikidata knowledge graph as a use case, and contributes a set of core competency questions from 20 domains present in the WDV dataset. To evaluate RevOnt, we contribute a new dataset of manually-annotated high-quality competency questions, and compare the extracted competency questions by calculating their BLEU score against the human references. The results for the abstraction and question generation components of the approach show good to high quality. Meanwhile, the accuracy of the filtering component is above 86%, which is comparable to the state-of-the-art classifications.}
}
@article{MELO20251649,
title = {Towards an ontology on project portfolio management},
journal = {Procedia Computer Science},
volume = {256},
pages = {1649-1657},
year = {2025},
note = {CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.02.302},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925006702},
author = {Héctor Melo and Oscar Avila and María del Pilar Villamil},
keywords = {Ontology, portfolio, project, management},
abstract = {Project Portfolio Management (PPM) is essential for organizations aiming to align projects with strategic goals. Different organizations adopt diverse PPM frameworks and standards to manage project portfolios each employing its own terminology. This semantic heterogeneity leads to communication barriers, knowledge silos, and difficulty in integrating information across various platforms in an interorganizational context. This article proposes a PPM ontology to establish a common language encapsulating key concept, addressing this challenge. The methodology involves systematic revision of three prominent PPM standards - ISO 21504, PMI Standard for Portfolio Management, and AXELOS Management of Portfolios, and employs a novel algorithm to identify equivalences and containment relationships between terms across all three standards, resolving semantic ambiguities and enriching the ontology’s expressiveness. This contribution benefits researchers, academics, portfolio managers, project managers, and PPM practitioners by providing a common vocabulary and framework for understanding and improving PPM practices, facilitates knowledge representation, improves communication and collaboration among stakeholders, and lays the groundwork for developing intelligent PPM systems capable of leveraging shared semantic understanding.}
}
@article{MAHI2022,
title = {A Novel Sentence Completion System for Punjabi Using Deep Neural Networks},
journal = {International Journal of Software Innovation},
volume = {10},
number = {1},
year = {2022},
issn = {2166-7160},
doi = {https://doi.org/10.4018/IJSI.293271},
url = {https://www.sciencedirect.com/science/article/pii/S2166716022000297},
author = {Gurjot Singh Mahi and Amandeep Verma},
keywords = {Embedding, GRU, LSTM, Neural Networks, NLP, RNN, Sentence Completion},
abstract = {ABSTRACT
Sentence completion systems are actively studied by many researchers, which ultimately results in the reduction of cognitive effort and enhancement in user experience. The review of the literature reveals that most of the work in the said area is in English and limited effort spent on other languages, especially vernacular languages. This work aims to develop a state-of-the-art sentence completion system for the Punjabi language, which is the 10th most spoken language in the world. The presented work is an outcome of the results of the experimentation on various neural network language model combinations. A new sentence search algorithm (SSA) and patching system are developed to search, complete, and rank the completed sub-string and give a syntactically rich sentence. The quantitative and qualitative evaluation metrics were utilized to evaluate the system. The results are quite promising, and the best performing model is capable of completing a given sub-string with more acceptability. The best performing model is utilized for developing the user interface.}
}
@article{LEGLAZ2021,
title = {Machine Learning and Natural Language Processing in Mental Health: Systematic Review},
journal = {Journal of Medical Internet Research},
volume = {23},
number = {5},
year = {2021},
issn = {1438-8871},
doi = {https://doi.org/10.2196/15708},
url = {https://www.sciencedirect.com/science/article/pii/S1438887121004295},
author = {Aziliz {Le Glaz} and Yannis Haralambous and Deok-Hee Kim-Dufor and Philippe Lenca and Romain Billot and Taylor C Ryan and Jonathan Marsh and Jordan DeVylder and Michel Walter and Sofian Berrouiguet and Christophe Lemey},
keywords = {machine learning, natural language processing, artificial intelligence, data mining, mental health, psychiatry},
abstract = {Background
Machine learning systems are part of the field of artificial intelligence that automatically learn models from data to make better decisions. Natural language processing (NLP), by using corpora and learning approaches, provides good performance in statistical tasks, such as text classification or sentiment mining.
Objective
The primary aim of this systematic review was to summarize and characterize, in methodological and technical terms, studies that used machine learning and NLP techniques for mental health. The secondary aim was to consider the potential use of these methods in mental health clinical practice
Methods
This systematic review follows the PRISMA (Preferred Reporting Items for Systematic Review and Meta-analysis) guidelines and is registered with PROSPERO (Prospective Register of Systematic Reviews; number CRD42019107376). The search was conducted using 4 medical databases (PubMed, Scopus, ScienceDirect, and PsycINFO) with the following keywords: machine learning, data mining, psychiatry, mental health, and mental disorder. The exclusion criteria were as follows: languages other than English, anonymization process, case studies, conference papers, and reviews. No limitations on publication dates were imposed.
Results
A total of 327 articles were identified, of which 269 (82.3%) were excluded and 58 (17.7%) were included in the review. The results were organized through a qualitative perspective. Although studies had heterogeneous topics and methods, some themes emerged. Population studies could be grouped into 3 categories: patients included in medical databases, patients who came to the emergency room, and social media users. The main objectives were to extract symptoms, classify severity of illness, compare therapy effectiveness, provide psychopathological clues, and challenge the current nosography. Medical records and social media were the 2 major data sources. With regard to the methods used, preprocessing used the standard methods of NLP and unique identifier extraction dedicated to medical texts. Efficient classifiers were preferred rather than transparent functioning classifiers. Python was the most frequently used platform.
Conclusions
Machine learning and NLP models have been highly topical issues in medicine in recent years and may be considered a new paradigm in medical research. However, these processes tend to confirm clinical hypotheses rather than developing entirely new information, and only one major category of the population (ie, social media users) is an imprecise cohort. Moreover, some language-specific features can improve the performance of NLP methods, and their extension to other languages should be more closely investigated. However, machine learning and NLP techniques provide useful information from unexplored data (ie, patients’ daily habits that are usually inaccessible to care providers). Before considering It as an additional tool of mental health care, ethical issues remain and should be discussed in a timely manner. Machine learning and NLP methods may offer multiple perspectives in mental health research but should also be considered as tools to support clinical practice.}
}
@article{BUREKA20201053,
title = {A Lightweight Approach to the Multi-perspective Modeling of Processes and Objects},
journal = {Procedia Computer Science},
volume = {176},
pages = {1053-1062},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.101},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920320019},
author = {Patryk Bureka and Heinrich Herre},
keywords = {Semantic Web, Knowledge Representation, Process Modeling, Conceptual Modeling, Ontology},
abstract = {Process modeling has a broad range of applications, varying from business and system engineering, via artifact design, up to natural process modeling utilized in natural sciences. Over the last decades, various sophisticated languages and frameworks have been developed to support process modeling. The current paper discusses an approach to process modeling, which is, in contrast to many existing solutions, intended for the integrated process and object modeling. Furthermore, it is designed to be a lightweight approach with only a few constructs, which, however, permit the representation of processes from various perspectives. The developed solution provides an abstract language-independent model (ontology), partial formalization in first-order logic as well as a Web Ontology Language (OWL) implementation.}
}
@article{KALTENEGGER2025112565,
title = {An ontology-driven framework for digital transformation and performance assessment of building materials},
journal = {Building and Environment},
volume = {271},
pages = {112565},
year = {2025},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2025.112565},
url = {https://www.sciencedirect.com/science/article/pii/S0360132325000472},
author = {Julia Kaltenegger and Kirstine Meyer Frandsen and Ekaterina Petrova},
keywords = {Material information modelling, Building information modelling, Material classification, Semantic web, Ontologies},
abstract = {Material Information Modelling (MIM) is a cornerstone of Building Performance Simulation (BPS). However, defining and exchanging data between building modelling and simulation tools is cumbersome due to notable deficiencies in the granularity of material information descriptions. The inadequacies in the data models and exchanges lead to faulty interpretations of material properties in building performance assessment. The material science domain strives to advance material research and expedite the market readiness of novel materials through intricate data modelling, performance computations, and interdisciplinary communication channels. In addition to the latter, adopting the Findable, Accessible, Interoperable, and Reusable principles holds significant potential in promoting accurate MIM within Architecture, Engineering and Construction. This study introduces an ontology-driven framework leveraging Semantic Web technologies and Linked Data to support MIM in the context of Building Information Modelling and BPS. The framework implementation is demonstrated in a web-based application that enables the dynamic assessment and benchmarking of building materials based on the Guggenheim, Anderson and de Boer model and thermal resistance computations. The development of the framework relies on ontology engineering principles to represent domain knowledge in a Building Material Performance ontology, as well as Systems Engineering coupled with test-driven development for requirement engineering, system design, implementation, and validation. The results include a novel MIM data model enabling material classification and property definitions in alignment with international standards. The implementation validates and assesses the logic of the proposed data model and software application by conducting hygric and thermal performance assessments applied on case studies.}
}
@article{WINDISCH2022550,
title = {Approach for model-based requirements engineering for the planning of engineering generations in the agile development of mechatronic systems},
journal = {Procedia CIRP},
volume = {109},
pages = {550-555},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.293},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122007429},
author = {Emily Windisch and Constantin Mandel and Simon Rapp and Nikola Bursac and Albert Albers},
keywords = {mbse, agility, validation, test planning},
abstract = {The crucial factor for a successful usage of modeling approaches of systems engineering is the interaction of language, method, and tool. For this, specific challenges arise for the application of MBSE in agile requirements engineering. From observations in agile development practice at a machine tool manufacturer, the challenges for model-based requirements engineering are described and each is assigned to its critical aspect of modeling: The language must formally represent the requirements data model, especially for planning engineering generations. The tool must support collaborative, interdisciplinary cooperation, and consider the dynamics of the requirements model during the development process. The method must individually support the requirements engineering activities, which are carried out several times in a sprint during the development process and must enable a target-oriented process for bundling the requirements into engineering generations. Taking these demands into account, an approach is then presented providing activity-based views in conjunction with activity steps based on a consistent ontology for the description of product requirements and verification activities. The activity steps are composed in activity patterns and support the user in making use of the views for modeling requirements for the engineering generations. The approach is implemented in the software JIRA at a machine tool manufacturer. The subsequent evaluation shows that the approach is used in development practice and offers the potential to plan engineering generation systematically and comprehensibly and to ensure a regular review of the implemented requirements.}
}
@article{YAGO201848,
title = {ON-SMMILE: Ontology Network-based Student Model for MultIple Learning Environments},
journal = {Data & Knowledge Engineering},
volume = {115},
pages = {48-67},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17301945},
author = {Hector Yago and Julia Clemente and Daniel Rodriguez and Pedro Fernandez-de-Cordoba},
keywords = {Ontological engineering, Student modeling, Ontology network, Learning supervision, Semantic web},
abstract = {Currently, many educational researchers focus on the extraction of information about the learning progress to properly assist students. We present ON-SMMILE, a student-centered and flexible student model which is represented as an ontology network combining information related to (i) students and their knowledge state, (ii) assessments that rely on rubrics and different types of objectives, (iii) units of learning and (iv) information resources previously employed as support for the student model in intelligent virtual environment for training/instruction and here extended. The aim of this work is to design and build methodologically, throughout ontological engineering, the ON-SMMILE model to be used as support of future works closely linked to supervision of student's learning as competence-based recommender system. For this purpose, our model is designed as a set of ontological resources that have been extended, standardized, interrelated and adapted to be used in multiple learning environments. In this paper, we also analyze the available approaches based on instructional design which can be added to ontology network to build the proposed model. As a case study, a chemical experiment in a virtual environment and its instantiation are described in terms of ON-SMMILE.}
}
@article{LENTES20223010,
title = {Towards an Ontology for a Lightweight Support System for Production System Rough Planning},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {3010-3015},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.190},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322022042},
author = {Joachim Lentes},
keywords = {Ontology-Based System, production system planning, support system, rough planning, assembly planning},
abstract = {To shorten times-to-markets, also the planners of production systems have to be supported appropriately. For this, open, adaptable systems are needed which support continuous flows of information, e.g. by leveraging standard data formats – and, which are easy to use for planners without specific knowledge about software development or ontology engineering. This paper introduces a support system for production system rough planning, especially for, but not limited to, assembly systems, which consists of two main components: a standard-based ontology as explicitly formulated external data model and a relatively universal software system working on the ontology. Thereby, focus of this contribution is mainly on the ontology, so on manufacturing modeling.}
}
@article{CIMMINO2025104282,
title = {Open Digital Rights Enforcement framework (ODRE): From descriptive to enforceable policies},
journal = {Computers & Security},
volume = {150},
pages = {104282},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104282},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005881},
author = {Andrea Cimmino and Juan Cano-Benito and Raúl García-Castro},
keywords = {Open digital rights language, Privacy policies, ODRL enforcement},
abstract = {From centralised platforms to decentralised ecosystems, like Data Spaces, sharing data has become a paramount challenge. For this reason, the definition of data usage policies has become crucial in these domains, highlighting the necessity of effective policy enforcement mechanisms. The Open Digital Rights Language (ODRL) is a W3C standard ontology designed to describe data usage policies, however, it lacks built-in enforcement capabilities, limiting its practical application. This paper introduces the Open Digital Rights Enforcement (ODRE) framework, whose goal is to provide ODRL with enforcement capabilities. The ODRE framework proposes a novel approach to express ODRL policies that integrates the descriptive ontology terms of ODRL with other languages that allow behaviour specification, such as dynamic data handling or function evaluation. The framework includes an enforcement algorithm for ODRL policies and two open-source implementations in Python and Java. The ODRE framework is also designed to support future extensions of ODRL to specific domain scenarios. In addition, current limitations of ODRE, ODRL, and current challenges are reported. Finally, to demonstrate the enforcement capabilities of the implementations, their performance, and their extensibility features, several experiments have been carried out with positive results.}
}
@article{LANGE2025100330,
title = {Ontologies relevant for improving data interoperability for food loss and waste: A review and research agenda},
journal = {Cleaner and Responsible Consumption},
volume = {19},
pages = {100330},
year = {2025},
issn = {2666-7843},
doi = {https://doi.org/10.1016/j.clrc.2025.100330},
url = {https://www.sciencedirect.com/science/article/pii/S2666784325000816},
author = {Matthew C. Lange and Ran Li and John W. Apolzan and Patrick R. Huber and Emily Steliotes and Kai Robertson and Norbert L.W. Wilson and Karthik Jain and Rajiv Ramnath and Brian E. Roe and Edward S. Spang},
keywords = {Ontology, Food loss and waste, Data interoperability, Food system, Large language models},
abstract = {Food loss and waste (FLW) is a global challenge. Interoperable FLW ontologies will foster more comprehensive data sharing and inform better solutions to reduce and recover excess food and to valorize wasted food and food byproducts. This review reveals that only eight ontologies currently address FLW with most emphasizing valorization. Notably, few are designed explicitly to support FLW reduction, and none facilitate food recovery, which is critical given that reduction and recovery are the preferred means of mitigating FLW. Furthermore, existing FLW ontologies show limited alignment with recognized gold-standard frameworks, for example the Open Biological and Biomedical Ontology (OBO) Foundry, and none support ongoing connectivity to external ontologies, restricting their utility across stakeholder domains. Looking ahead, there is a pressing need to create or expand ontologies that adhere to best practices from relevant foundries to ensure robust linkage and interoperability and undergird structured data ecosystems that support food systems stakeholders in FLW prevention and mitigation. Achieving this goal will require active collaboration among a diverse range of stakeholders, including builders of food systems cyberinfrastructure, scientists, innovators, regulators, public and private funders, community-based organizations, policymakers, and international NGOs as each rely on critical ontological elements to inform decision-making, measure impact, and drive improvement across the food supply chain. Finally, large language models offer promising capabilities for expediting ontology creation, broadening inclusivity in ontology creation, and enhancing the accuracy of resulting data infrastructures.}
}
@article{ROMANO2024124292,
title = {An NLP-based approach to assessing a company’s maturity level in the digital era},
journal = {Expert Systems with Applications},
volume = {252},
pages = {124292},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124292},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424011588},
author = {Simon Pietro Romano and Giancarlo Sperlì and Andrea Vignali},
keywords = {Natural language processing, Embedding representation, Maturity model, Digital transformation},
abstract = {Conducting a maturity assessment allows companies to measure their readiness in implementing novel technologies. However, this task is challenging due to the multidimensional, complex, unpredictable, and non-linear nature of innovation. In this paper, we introduce an innovative approach to maturity assessment that enables both intra- and inter-company analysis. Our approach evaluates a company’s absolute maturity score concerning a specific technology or area. By leveraging a Natural Language Processing pipeline applied to a semi-structured questionnaire we extract popular concepts from the answers and present them to a human expert for analysis. The expert can refine the analysis by adding or removing concepts as needed. Subsequently, we compute a similarity metric for each answer to determine a company’s maturity in specific concepts. The output of our analysis is presented through human-readable plots, offering clear insights into the internal maturity level of the company and allowing for a comparison with competitors across the chosen concepts. To demonstrate the capabilities of our method, we provide a running example showcasing both quantitative and qualitative results of the analysis. Our approach demonstrates efficiency, with preprocessing completed in 1.967±0.758 s, and information extraction in 0.074±0.017 s on average, excluding human intervention time, and requiring low hardware resources.}
}
@article{STADNICKI2020753,
title = {Towards a Modern Ontology Development Environment},
journal = {Procedia Computer Science},
volume = {176},
pages = {753-762},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.070},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319657},
author = {Adrian Stadnicki and Filip {Filip Pietroń} and Patryk Burek},
keywords = {Knowledge-Based Systems, Knowledge Representation, Management, Ontology Engineering, Semantic Web},
abstract = {Ontologies provide engineers and developers with an unambiguous, verifiable, and expandable knowledge base related to a certain domain. Every project that requires control over consistent knowledge, which is especially relatable when using artificial intelligence with datasets increasing in size every second, would reap benefits from adding ontologies to the equation. It is a powerful asset enabling the development of a project with integrity between platforms or teams. Unfortunately, the cost of entry for a developer into the ontology engineering area is high, as it has been proven over the last decades that developing an ontology is a complex, collaborative task, which requires the support of an adequate methodology as well as software tools. The current paper’s objective is twofold. First, it provides a survey on the methodology and software tools used for the creation of the ontology, its maintenance and collaboration. The paper investigates how the tools evolved over the years and what trends have emerged. Second, as the result of the analysis conducted, we show that current solutions have deficiencies and a technological debt; therefore, we present our plan to build a modern tool that uses state-of-the-art technology.}
}
@article{AYADI2019100495,
title = {BNO—An ontology for understanding the transittability of complex biomolecular networks},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100495},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300022},
author = {Ali Ayadi and Cecilia Zanni-Merk and François de Bertrand {de Beuvron} and Julie Thompson and Saoussen Krichen},
keywords = {Systems biology, Complex biomolecular networks, Transittability, Ontology engineering, Qualitative reasoning, SWRL rules},
abstract = {Analysis of biological systems is being progressively facilitated by computational tools. Most of these tools are based on qualitative and numerical methods. However, they are not always evident, and there is an increasing need to provide an additional semantic layer. Semantic technologies, especially ontologies, are one of the tools frequently used for this purpose. Indeed, they are indispensable for understanding the semantic knowledge about the operation of cells at a molecular level. We describe here the biomolecular network ontology (BNO) created specially to address the needs of analysing the complex biomolecular network’s behaviour. A biomolecular network consists of nodes, denoting cellular entities, and edges, representing interactions among cellular components. The BNO ontology provides a foundation for qualitative simulation of complex biomolecular networks. We test the performance of the proposed BNO ontology by using a real example of a biomolecular network, the bacteriophage T4 gene 32. We illustrate the proposed BNO ontology for reasoning and inferring new knowledge with sets of rules expressed in SWRL. Results demonstrate that the BNO ontology allows to precisely interpret the corresponding semantic context and intelligently model biomolecular networks and their state changes. The Biomolecular Network Ontology (BNO) is freely available at https://github.com/AliAyadi/BNO-ontology-version-1.0.}
}
@article{ROSNER2025,
title = {An Ontology for Digital Medicine Outcomes: Development of the Digital Medicine Outcomes Value Set (DOVeS)},
journal = {JMIR Medical Informatics},
volume = {13},
year = {2025},
issn = {2291-9694},
doi = {https://doi.org/10.2196/67589},
url = {https://www.sciencedirect.com/science/article/pii/S2291969425000250},
author = {Benjamin Rosner and Matthew Horridge and Guillen Austria and Tiffany Lee and Andrew Auerbach},
keywords = {digital health, digital medicine, digital therapeutics, ontology, medical informatics, value set, ontology, development, digital health tool, DHT, health systems, digital medicine outcomes value set, prototype, users},
abstract = {Background
Over the last 10-15 years, US health care and the practice of medicine itself have been transformed by a proliferation of digital medicine and digital therapeutic products (collectively, digital health tools [DHTs]). While a number of DHT classifications have been proposed to help organize these tools for discovery, retrieval, and comparison by health care organizations seeking to potentially implement them, none have specifically addressed that organizations considering their implementation approach the DHT discovery process with one or more specific outcomes in mind. An outcomes-based DHT ontology could therefore be valuable not only for health systems seeking to evaluate tools that influence certain outcomes, but also for regulators and vendors seeking to ascertain potential substantial equivalence to predicate devices.
Objective
This study aimed to develop, with inputs from industry, health care providers, payers, regulatory bodies, and patients through the Accelerated Digital Clinical Ecosystem (ADviCE) consortium, an ontology specific to DHT outcomes, the Digital medicine Outcomes Value Set (DOVeS), and to make this ontology publicly available and free to use.
Methods
From a starting point of a 4-generation–deep hierarchical taxonomy developed by ADviCE, we developed DOVeS using the Web Ontology Language through the open-source ontology editor Protégé, and data from 185 vendors who had submitted structured product information to ADviCE. We used a custom, decentralized, collaborative ontology engineering methodology, and were guided by Open Biological and Biomedical Ontologies (OBO) Foundry principles. We incorporated the Mondo Disease Ontology (MONDO) and the Ontology of Adverse Events. After development, DOVeS was field-tested between December 2022 and May 2023 with 40 additional independent vendors previously unfamiliar with ADviCE or DOVeS. As a proof of concept, we subsequently developed a prototype DHT Application Finder leveraging DOVeS to enable a user to query for DHT products based on specific outcomes of interest.
Results
In its current state, DOVeS contains 42,320 and 9481 native axioms and distinct classes, respectively. These numbers are enhanced when taking into account the axioms and classes contributed by MONDO and the Ontology of Adverse Events.
Conclusions
DOVeS is publicly available on BioPortal and GitHub, and has a Creative Commons license CC-BY-SA that is intended to encourage stakeholders to modify, adapt, build upon, and distribute it. While no ontology is complete, DOVeS will benefit from a strong and engaged user base to help it grow and evolve in a way that best serves DHT stakeholders and the patients they serve.}
}
@article{PONCE2023105404,
title = {Unification of tsunami-related terminology: Ontology engineering perspective},
journal = {Computers & Geosciences},
volume = {178},
pages = {105404},
year = {2023},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105404},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001085},
author = {Daniela Ponce and Martina Husáková and Tomáš Nacházel and Vladimír Bureš and Pavel Čech and Peter Mikulecký and Kamila Štekerová and Petr Tučník and Marek Zanker and Karel Mls and Ioanna Triantafyllou and František Babič},
keywords = {Ontology, Metadata, Modelling, Community resilience, Physical vulnerability, Meteorological tsunami},
abstract = {Like many research areas, tsunami research has plenty of related topics that lack unified terminology. Some particular sub-topics may not have enough attention or do not share the same terminology as different views on the phenomenon. This issue can be tackled by an ontology that puts knowledge from different related topics into a formal structure that connects concepts with relationships. This paper proposes the development process of Tsunami-Related Ontology (TRO) that would aid the research in this field. The proposed semi-automatic ontology development methodology applies to any research field and does not require specific algorithms or programming skills to achieve its goal. This paper particularly focuses on three research gaps related to tsunami that are expected to benefit significantly from an ontology: meteorological tsunami, community resilience, and physical vulnerability. For these topics, the created ontology provides a formal taxonomy that links individual concepts to equivalent or related concepts, providing an easy-to-understand overview of the area.}
}
@article{HAMMAMI2019239,
title = {Towards Agile and Gamified Flipped Learning Design models: Application to the System and Data Integration Course},
journal = {Procedia Computer Science},
volume = {164},
pages = {239-244},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.178},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919322173},
author = {Jihed Hammami and Maha Khemaja},
keywords = {IMS LD, Flipped classroom, Flipped learning, Agile, Gamification, Authoring Tool, Domain Specific Language},
abstract = {Education and learning have no limits. At all ages, people are willing to be engaged in new experiences and get additional knowledge, skills and competencies. In this paper, we propose a new learning model that is learner-centered, gives values to learners’ preferences and competencies and encourages an engaging and motivating learning experience. Therefore, we discuss some of its pedagogical and technical aspects and requirements. We present the flipped classroom/learning, Agile methodology and Gamification as the basis of our proposed approach. We additionally, examine the possible support that the existing authoring tools could provide. We attempt to validate our proposal with a scenario intended to design the System and Data Integration course.}
}
@article{KUSUMA2022108906,
title = {Automatic question generation with various difficulty levels based on knowledge ontology using a query template},
journal = {Knowledge-Based Systems},
volume = {249},
pages = {108906},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108906},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122004336},
author = {Selvia Ferdiana Kusuma and Daniel Oranova Siahaan and Chastine Fatichah},
keywords = {Knowledge ontology, Ontology, Question generation, Query template, Question classification},
abstract = {Ontology is a concepts and relationships that can be used to support the question-generation process. However, until now, the ontology models and question templates commonly used to support the question-generation process have remained domain-specific, allowing three weaknesses to persist. First, the role of experts is dominant in the process of ontology generation. Second, the process needs adjustment if it is to be used for other domains. Third, question templates are formed based on the vocabulary of ontology, so they cannot be used to generate questions in other domains. In response to these problems, this research focused on forming an ontology generation model and a template model for generating questions that are not domain-specific. We used a combination of two types of ontology — namely, taxonomy ontology and sentence ontology to form ontology models and question templates that were not domain-specific. We labeled this combination as “knowledge ontology”. We used template queries to retrieve information on the ontology and then translated the results of the query template into questions in natural language. The ratios from our experiments demonstrated that the proposed method was effective for generating questions. Moreover, the method produced good question quality, as evidenced by its high accuracy rate of 90.71%. This research can be applied to help e-learning developers represent information in the form of ontology without involving experts. Furthermore, this research can also help teachers to generate questions automatically with consistent question quality.}
}
@article{TURCHET2025100871,
title = {The Musician’s Context Ontology: Modeling the context for smart musical applications},
journal = {Journal of Web Semantics},
volume = {87},
pages = {100871},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100871},
url = {https://www.sciencedirect.com/science/article/pii/S1570826825000125},
author = {Luca Turchet and Jacopo Tomelleri and Andrea Molinari and Paolo Bouquet},
keywords = {Semantic audio, Smart musical instruments, Internet of Musical Things, Context-aware computing, Music information retrieval},
abstract = {The paradigm of context-aware computing allows storing situational and environmental information in such a way that its interpretation can be done easily and more meaningfully. In turn, this understanding is used to anticipate users’ needs, and proactively provide them with situation-aware content and experiences. Whereas context-awareness has been investigated extensively in the computer science and IoT disciplines, it has been largely overlooked by the research community dealing with musical interfaces design. Existing musical instruments are not equipped with the ability to understand the context around them, namely who is the musician playing them, what musical activity is being conducted, as well as where and when. Enhancing musical instruments with context-awareness has the concrete potential to enable novel kinds of interactions between musicians and musical content in a large variety of situations, from playing alone to playing in a group, from music learning to music composition. To accomplish such a vision of intelligence embedded in musical instruments it is necessary to model the context around their users. In this paper, we present an ontology devised to represent the knowledge related to musicians and musical activities, the “Musician’s Context Ontology” (MUSICO) to facilitate the development of context-aware musical applications. There was no previous comprehensive data model for the domain of musicians’ context, nevertheless, the new ontology relates to several existing ontologies, including the Internet of Musical Things Ontology to represent Internet of Musical Things ecosystems and the Music Ontology that deals with the description of the music value-chain from production to consumption. This paper documents the design of the ontology and its evaluation with respect to specific requirements gathered from an extensive literature review and interviews with musicians. The utility of the ontology is demonstrated by a smartphone application that enables to search for musicians based on both textual and content-based musical queries. MUSICO can be accessed at: https://w3id.org/musico#.}
}
@article{LALIS202037,
title = {Ontology-based reliability analysis of aircraft engine lubrication system},
journal = {Transportation Research Procedia},
volume = {51},
pages = {37-45},
year = {2020},
note = {INAIR 2020 - CHALLENGES OF AVIATION DEVELOPMENT},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520308565},
author = {Andrej Lališ and Simona Bolčeková and Oldřich Štumbauer},
keywords = {aircraft engine, failure mode, effects analysis, ontology, ontology engineering, reliability analysis},
abstract = {This article focuses on identifying limitations and deficiencies of reliability methods that are currently used in the aviation industry. The goal is to propose a solution to address these issues and, consequently, improve the way reliability analyses are carried in the industry. In collaboration with an aircraft engine manufacturer, Failure Mode and Effects Analysis (FMEA) of an aircraft engine lubrication system was carried the traditional way and with current tools used by the company. Reliability ontology suitable to carry the analysis in semi-automatic way was then proposed, implemented, and used with the same FMEA analysis. The results show that the ontology-based approach has significant potential for improving the consistency and overall quality of the reliability analyses in the aviation. This article details the process of development of an FMEA ontology model, case study of its application and the comparison of the traditional and the ontology-based approach.}
}
@article{ALHARBI2024100659,
title = {An ontology-based agriculture decision-support system with an evidence-based explanation model},
journal = {Smart Agricultural Technology},
volume = {9},
pages = {100659},
year = {2024},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2024.100659},
url = {https://www.sciencedirect.com/science/article/pii/S2772375524002648},
author = {Amani Falah Alharbi and Muhammad Ahtisham Aslam and Khalid Ali Asiry and Naif Radi Aljohani and Yury Glikman},
keywords = {Ontology modeling, Decision support systems, Machine reasoning, Smart agriculture, Semantic-web},
abstract = {Effective management of plant diseases and pests requires knowledge that covers multiple domains. At the same time, retrieving the relevant information in a timely manner is always challenging, due to the unstructured nature of agricultural data. Over the years, efforts have been made to develop an ontology-based Decision-Support System (DSS) to facilitate the diagnosis and control of plant diseases. Some major issues with these systems are that: (1) they do not adopt the full extent of the ontological constructs to represent domain entities, which, in turn, reduces reasoning capabilities and prevents systems from being more intelligent, (2) they do not adequately provide the desired level of knowledge to support complex decisions, which requires many factors to be considered, (3) they do not adequately explain or provide evidence to demonstrate the validity of the system's outputs. To address these limitations, we present a novel system termed Agriculture Ontology Based Decision Support System (AgrODSS), which aims to assist in plant disease and pest identification and control. AgrODSS architecture consists of two semantic-based models. First, we developed Plant Diseases and Pests Ontology (PDP-O) to capture, model, and represent diseases and pest knowledge in a machine-understandable format. Second, we designed and developed an Evidence-Based Explanation Model (EBEM) that points to related evidence from the literature to demonstrate the validity of the system outputs. We demonstrate the effectiveness of AgrODSS by executing various queries via AgrODSS SPARQL Endpoint and obtaining valuable information to support decision-making. Finally, we evaluated AgrODSS practically with domain experts (including entomologists and pathologists) and it produced similar answers to those given by the experts, with an overall accuracy of 80.66%. These results demonstrate AgrODSS's ability to assist agricultural stakeholders in making proper disease or pest diagnoses and choosing the appropriate control methods.}
}
@article{POLENGHI2022100298,
title = {Knowledge reuse for ontology modelling in Maintenance and Industrial Asset Management},
journal = {Journal of Industrial Information Integration},
volume = {27},
pages = {100298},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100298},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000947},
author = {Adalberto Polenghi and Irene Roda and Marco Macchi and Alessandro Pozzetti and Hervé Panetto},
keywords = {Ontology, Knowledge reuse, Interoperability, Maintenance, Asset management},
abstract = {Maintenance and Industrial Asset Management (AM) are fundamental business processes in guaranteeing the availability of physical assets at minimum risk and cost, while balancing the interests of several stakeholders. To reach operational excellence, intra- and inter-enterprise interoperability of systems is needed to support information management and integration between several involved parties. To this end, ontology engineering is relevant since it supports interoperability at technical and semantic levels. However, ontology modelling methodologies are varied, and several best practices exist, amongst which knowledge reuse. Nevertheless, reusing extant knowledge is not completely exploited so far, causing a heterogeneous ensemble of ontologies that are not orchestrated. The present work aims at promoting the adoption of knowledge reuse for ontology modelling in maintenance and AM. Therefore, an extensive review of existing ontologies for the two targeted business processes is performed with a twofold objective: firstly, to realise a cross-industrial ontological compendium, and secondly to understand the state of art of ontology modelling in maintenance and AM. To support the adoption of knowledge reuse, this practice is framed in AMODO (Asset Management Ontology Development methOdology). Finally, a laboratory-sized showcase is provided to prove the usefulness of relying on knowledge reuse during the ontology development. The results show that the developed ontology is realised faster and is inherently aligned with established ontologies, towards enterprise systems interoperability. Consequently, maintenance and AM business processes may rely on information management and integration to pursue operational excellence.}
}@article{SIEBRA2022109152,
title = {Engineering uncertain time for its practical integration in ontologies},
journal = {Knowledge-Based Systems},
volume = {251},
pages = {109152},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109152},
url = {https://www.sciencedirect.com/science/article/pii/S095070512200572X},
author = {Clauirton A. Siebra and Katarzyna Wac},
keywords = {Ontology design, Rule-based processing, Uncertainty, Temporal logic},
abstract = {Ontologies are commonly used as a strategy for knowledge representation. However, they are still presenting limitations to model domains that require broad forms of temporal reasoning. This study is part of the Onto-mQoL project and was motivated by the real need to extend static ontologies with diverse time concepts, relations and properties, which go beyond the commonly used Allen’s Interval Algebra. Therefore, we use the n-ary relations as the basis for temporal structures, which minimally modify the original ontology, and extend these structures with a generic set of time concepts (moments and intervals), time concept properties (precise and uncertain), time relations (interval–interval, interval–moment, and moment–moment), and time relation properties (qualitative and quantitative). We divided the scientific contribution of this study into three parts. Firstly, we present the ontological temporal model (classes and properties) and how it is integrated into static ontologies. Secondly, we discuss the creation of axioms that give the semantics for precise temporal elements. Finally, as our main contribution, these ideas are extended with axioms for uncertain time. All these elements follow the Ontology Web Language (OWL) standards, so this proposal is still compatible with the main ontology editors and reasoners currently available. A case example demonstrates the use of this approach in the nutrition assessment domain.}
}
@article{CHAVESFRAGA2020100596,
title = {GTFS-Madrid-Bench: A benchmark for virtual knowledge graph access in the transport domain},
journal = {Journal of Web Semantics},
volume = {65},
pages = {100596},
year = {2020},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100596},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300354},
author = {David Chaves-Fraga and Freddy Priyatna and Andrea Cimmino and Jhon Toledo and Edna Ruckhaus and Oscar Corcho},
keywords = {Virtual knowledge graph, Benchmark, Query translation, Data integration, GTFS},
abstract = {A large number of datasets are being made available on the Web using a variety of formats and according to diverse data models. Ontology Based Data Integration (OBDI) has been traditionally proposed as a mechanism to facilitate access to such heterogeneous datasets, providing a unified view over their data by means of ontologies. Recently, the term “Virtual Knowledge Graph Access” has begun to be used to refer to the mechanisms that provide query-based access to knowledge graphs virtually generated from heterogeneous data sources. Several OBDI engines exist in the state of the art, with overlapping capabilities but also clear differences among them (in terms of the data formats that they can deal with, mapping languages that they support, query expressivity that they allow, etc.). These engines have been evaluated with different testbeds and benchmarks. However, their heterogeneity has made it difficult to come up with a common comprehensive benchmark that allows for comparisons among them to facilitate their selection by practitioners, and more importantly, for their continuous improvement by the teams that maintain them. In this paper we present GTFS-Madrid-Bench, a benchmark to evaluate OBDI engines that can be used for the provision of access mechanisms to virtual knowledge graphs. Our proposal introduces several scenarios that aim at measuring the query capabilities, performance and scalability of all these engines, considering their heterogeneity. The data sources used in our benchmark are derived from the GTFS data files of the subway network of Madrid. They have been transformed into several formats (CSV, JSON, SQL and XML) and scaled up. The query set aims at addressing a representative number of SPARQL 1.1 features while covering usual queries that data consumers may be interested in.}
}
@article{GONZALEZSENDINO2024384,
title = {Mitigating bias in artificial intelligence: Fair data generation via causal models for transparent and explainable decision-making},
journal = {Future Generation Computer Systems},
volume = {155},
pages = {384-401},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.02.023},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24000694},
author = {Rubén González-Sendino and Emilio Serrano and Javier Bajo},
keywords = {Causal model, Bias mitigation, Fairness, Responsible artificial intelligence, Bayes},
abstract = {In the evolving field of Artificial Intelligence, concerns have arisen about the opacity of certain models and their potential biases. This study aims to improve fairness and explainability in AI decision making. Existing bias mitigation strategies are classified as pre-training, training, and post-training approaches. This paper proposes a novel technique to create a mitigated bias dataset. This is achieved using a mitigated causal model that adjusts cause-and-effect relationships and probabilities within a Bayesian network. Contributions of this work include (1) the introduction of a novel mitigation training algorithm for causal model; (2) a pioneering pretraining methodology for producing a fair dataset for Artificial Intelligence model training; (3) the diligent maintenance of sensitive features in the dataset, ensuring that these vital attributes are not overlooked during analysis and model training; (4) the enhancement of explainability and transparency around biases; and finally (5) the development of an interactive demonstration that vividly displays experimental results and provides the code for facilitating replication of the work.}
}
@article{PERNISCH2022100715,
title = {Visualising the effects of ontology changes and studying their understanding with ChImp},
journal = {Journal of Web Semantics},
volume = {74},
pages = {100715},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100715},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000117},
author = {Romana Pernisch and Daniele Dell’Aglio and Mirko Serbak and Rafael S. Gonçalves and Abraham Bernstein},
keywords = {Ontology editing, Materialisation, User study, Ontology evolution impact},
abstract = {Due to the Semantic Web’s decentralised nature, ontology engineers rarely know all applications that leverage their ontology. Consequently, they are unaware of the full extent of possible consequences that changes might cause to the ontology. Our goal is to lessen the gap between ontology engineers and users by investigating ontology engineers’ understanding of ontology changes’ impact at editing time. Hence, this paper introduces the Protégé plugin ChImp which we use to reach our goal. We elicited requirements for ChImp through a questionnaire with ontology engineers. We then developed ChImp according to these requirements and it displays all changes of a given session and provides selected information on said changes and their effects. For each change, it computes a number of metrics on both the ontology and its materialisation. It displays those metrics on both the originally loaded ontology at the beginning of the editing session and the current state to help ontology engineers understand the impact of their changes. We investigated the informativeness of materialisation impact measures, the meaning of severe impact, and also the usefulness of ChImp in an online user study with 36 ontology engineers. We asked the participants to solve two ontology engineering tasks – with and without ChImp (assigned in random order) – and answer in-depth questions about the applied changes as well as the materialisation impact measures. We found that ChImp increased the participants’ understanding of change effects and that they felt better informed. Answers also suggest that the proposed measures were useful and informative. We also learned that the participants consider different outcomes of changes severe, but most would define severity based on the amount of changes to the materialisation compared to its size. The participants also acknowledged the importance of quantifying the impact of changes and that the study will affect their approach of editing ontologies.}
}
@article{DAVID2023101223,
title = {Model consistency as a heuristic for eventual correctness},
journal = {Journal of Computer Languages},
volume = {76},
pages = {101223},
year = {2023},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2023.101223},
url = {https://www.sciencedirect.com/science/article/pii/S2590118423000333},
author = {Istvan David and Hans Vangheluwe and Eugene Syriani},
keywords = {Consistency, Correctness, Heuristics, Model consistency, Model-based systems engineering, Multi-view modeling},
abstract = {Inconsistencies between stakeholders’ views pose a severe challenge in the engineering of complex systems. The past decades have seen a vast number of sophisticated inconsistency management techniques being developed. These techniques build on the common idea of “managing consistency instead of removing inconsistency”, as put forward by Finkelstein. While it is clear what and how to do about inconsistencies, it is less clear why inconsistency is particularly useful. After all, it is the correctness of the system that should matter, as correctness is the end-user-facing quality of the product. In this paper, we analyze this question by investigating the relationship between (in)consistency and (in)correctness. We formally prove that, contrary to intuition, consistency does not imply correctness. However, consistency is still a good heuristic for eventual correctness. We elaborate on the consequences of this assertion and provide pointers as to how to make use of it in the next generation of inconsistency management techniques.}
}
@article{GHORBANI2023102452,
title = {Using type-2 fuzzy ontology to improve semantic interoperability for healthcare and diagnosis of depression},
journal = {Artificial Intelligence in Medicine},
volume = {135},
pages = {102452},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2022.102452},
url = {https://www.sciencedirect.com/science/article/pii/S0933365722002044},
author = {Abolfazl Ghorbani and Fatemeh Davoodi and Kamran Zamanifar},
keywords = {Health information interoperability, Type-2 fuzzy ontology, Semantic interoperability, Uncertainty, Major Depression Disorder (MDD)},
abstract = {Ontology enhances semantic interoperability through integrating health data from heterogeneous sources and sharing information in a meaningful way. In the field of smart health services, semantic interoperability means the exchange and interpretation of data without ambiguity and uncertainty. However, existing classical ontologies are not able to represent vague and uncertain knowledge, especially in contexts of mental health disorders which are associated with varying degrees of uncertainty and inaccuracy of diagnosis, and in this case, the treatment is a complex and common mental process necessitating to share information accurately and unambiguously. Type-2 fuzzy set theory can offer a fruitful solution in order to control uncertainty or express ambiguous concepts in a dynamic and complex environment such as healthcare systems. Herein, a semantic framework for healthcare, and also monitoring mental health disorders using type-2 fuzzy set theory based on the Internet of Thing (IoT) is suggested, in which all depression-related concepts are semantically annotated to share detailed information with the treatment staff. This framework not only paved the way to increasing the accuracy of medical diagnosis and decision-making but also provides the possibility of inference and semantic reasoning using the languages of SPARQL query and DL query.}
}
@article{BOTOEVA20191,
title = {Query inseparability for ALC ontologies},
journal = {Artificial Intelligence},
volume = {272},
pages = {1-51},
year = {2019},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370219300189},
author = {Elena Botoeva and Carsten Lutz and Vladislav Ryzhikov and Frank Wolter and Michael Zakharyaschev},
keywords = {Description logic, Knowledge base, Conjunctive query, Query inseparability, Computational complexity, Tree automaton},
abstract = {We investigate the problem whether two ALC ontologies are indistinguishable (or inseparable) by means of queries in a given signature, which is fundamental for ontology engineering tasks such as ontology versioning, modularisation, update, and forgetting. We consider both knowledge base (KB) and TBox inseparability. For KBs, we give model-theoretic criteria in terms of (finite partial) homomorphisms and products and prove that this problem is undecidable for conjunctive queries (CQs), but 2ExpTime-complete for unions of CQs (UCQs). The same results hold if (U)CQs are replaced by rooted (U)CQs, where every variable is connected to an answer variable. We also show that inseparability by CQs is still undecidable if one KB is given in the lightweight DL EL and if no restrictions are imposed on the signature of the CQs. We also consider the problem whether two ALC TBoxes give the same answers to any query over any ABox in a given signature and show that, for CQs, this problem is undecidable, too. We then develop model-theoretic criteria for HornALC TBoxes and show using tree automata that, in contrast, inseparability becomes decidable and 2ExpTime-complete, even ExpTime-complete when restricted to (unions of) rooted CQs.}
}
@article{PEREZPEREZ2021102131,
title = {A framework to extract biomedical knowledge from gluten-related tweets: The case of dietary concerns in digital era},
journal = {Artificial Intelligence in Medicine},
volume = {118},
pages = {102131},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102131},
url = {https://www.sciencedirect.com/science/article/pii/S093336572100124X},
author = {Martín Pérez-Pérez and Gilberto Igrejas and Florentino Fdez-Riverola and Anália Lourenço},
keywords = {Social media, Sociome profiling, Text mining, Graph mining, Machine learning, Health for informatics},
abstract = {Big data importance and potential are becoming more and more relevant nowadays, enhanced by the explosive growth of information volume that is being generated on the Internet in the last years. In this sense, many experts agree that social media networks are one of the internet areas with higher growth in recent years and one of the fields that are expected to have a more significant increment in the coming years. Similarly, social media sites are quickly becoming one of the most popular platforms to discuss health issues and exchange social support with others. In this context, this work presents a new methodology to process, classify, visualise and analyse the big data knowledge produced by the sociome on social media platforms. This work proposes a methodology that combines natural language processing techniques, ontology-based named entity recognition methods, machine learning algorithms and graph mining techniques to: (i) reduce the irrelevant messages by identifying and focusing the analysis only on individuals and patient experiences from the public discussion; (ii) reduce the lexical noise produced by the different ways in how users express themselves through the use of domain ontologies; (iii) infer the demographic data of the individuals through the combined analysis of textual, geographical and visual profile information; (iv) perform a community detection and evaluate the health topic study combining the semantic processing of the public discourse with knowledge graph representation techniques; and (v) gain information about the shared resources combining the social media statistics with the semantical analysis of the web contents. The practical relevance of the proposed methodology has been proven in the study of 1.1 million unique messages from >400,000 distinct users related to one of the most popular dietary fads that evolve into a multibillion-dollar industry, i.e., gluten-free food. Besides, this work analysed one of the least research fields studied on Twitter concerning public health (i.e., the allergies or immunology diseases as celiac disease), discovering a wide range of health-related conclusions.}
}
@article{SHIMIZU2024100823,
title = {Ontology design facilitating Wikibase integration — and a worked example for historical data},
journal = {Journal of Web Semantics},
volume = {82},
pages = {100823},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100823},
url = {https://www.sciencedirect.com/science/article/pii/S157082682400009X},
author = {Cogan Shimizu and Andrew Eells and Seila Gonzalez and Lu Zhou and Pascal Hitzler and Alicia Sheill and Catherine Foley and Dean Rehberger},
keywords = {Wikibase, Modular ontology modeling, Ontology design pattern},
abstract = {Wikibase – which is the software underlying Wikidata – is a powerful platform for knowledge graph creation and management. However, it has been developed with a crowd-sourced knowledge graph creation scenario in mind, which in particular means that it has not been designed for use case scenarios in which a tightly controlled high-quality schema, in the form of an ontology, is to be imposed, and indeed, independently developed ontologies do not necessarily map seamlessly to the Wikibase approach. In this paper, we provide the key ingredients needed in order to combine traditional ontology modeling with use of the Wikibase platform, namely a set of axiom patterns that bridge the paradigm gap, together with usage instructions and a worked example for historical data.}
}
@article{PEREIRA2023405,
title = {The Adoption of 4Step-Rule-Set Method for Ontological Design: Application in a Real Industrial Project},
journal = {Procedia Computer Science},
volume = {219},
pages = {405-415},
year = {2023},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN – International Conference on Project MANagement / HCist – International Conference on Health and Social Care Information Systems and Technologies 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.306},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923003150},
author = {Tiago F. Pereira and Francisco Morais and Carlos E. Salgado and Ana Lima and António Silva and Manuel Pereira and João Oliveira and Ricardo J. Machado},
keywords = {Ontology Building, Development Method, Agile Method, Semantic Interoperability, Graph Database},
abstract = {Ontology building can greatly influence the development cycle of an information system and enhance interoperability among its constituent elements. Throughout the projects we have been developing we have detected, by studying the current literature, a need to develop an agile method to conceive and mapping ontologies, which allows a quick and effective response to R&D projects. Designing a method for building an ontology, which is integrated and aligned with a systematic development approach, represents a crucial challenge in new approaches to system design and exploitation. Extant proposed methods for building an ontology, especially following agile approaches, have achieved interesting results but lack integration and alignment with a wider-view development framework. Thus, we have defined the first version of a semantic model allowing the alignment with the previously defined information model. Following the best practices for ontology building and based on our previous work on software system development, we now propose a method for designing an ontology, the 4SRS Method for Ontological Design based on the V-Model 4SRS, aligning it with a proven development method. We further demonstrate this approach by applying the proposed method in a real case, to develop an ontology for a choen restricted scope within the domain problem.}
}
@article{OS2021102751,
title = {Detection of malicious Android applications using Ontology-based intelligent model in mobile cloud environment},
journal = {Journal of Information Security and Applications},
volume = {58},
pages = {102751},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102751},
url = {https://www.sciencedirect.com/science/article/pii/S2214212621000041},
author = {Jannath Nisha O.S and Mary Saira Bhanu S},
keywords = {Mobile Cloud Computing, Malware and Benign apps, Ontology, Optimization algorithms, Machine Learning classifiers},
abstract = {Mobile Cloud Computing (MCC) is a computing model that makes mobile devices resourceful by executing mobile applications (apps) in the cloud and storing data in cloud servers. MCC faces several security threats in both the Cloud and Mobile environments. Among several threats, malicious apps are the most threatening ones, because they can perform various malicious activities in both environments. The traditional malware detection methods may not detect new types of malware or rapidly changing malware behavior. So, there is a need to develop an accurate model for detecting malicious apps in the MCC environment. Scalability and Knowledge Reusability are challenging issues in existing detection methods. To overcome these issues, the proposed model uses an effective Ontology-based intelligent model based on app permissions to detect malware apps. This model extracts the relationship between the static features from the apps and builds an Apps Feature Ontology (AFO). A concept vector set for apps is created using the items obtained from the AFO. The most discriminant features are selected using optimization algorithms like Particle Swarm Optimization, Social Spider Algorithm (SSA), and Gravitational Search Algorithm to reduce the dimension of the concept vector set. Various classifiers are applied to the reduced set. The efficiency of the proposed approach was evaluated on datasets obtained from the AndroZoo repository and VirusShare. The experimental results reveal that the proposed model can correctly detect malware using the Random Forest (RF) classifier with SSA and achieve higher detection accuracy with the lesser fall-out and less detection speed than existing Android malware detection techniques. Specifically, RF with SSA obtained higher accuracy, F1-score, and reduction in the fall-out of 94.11%, 93%, and 3%, respectively.}
}
@article{YUAN2024570,
title = {Analysis of international publication trends in artificial intelligence in skin cancer},
journal = {Clinics in Dermatology},
volume = {42},
number = {6},
pages = {570-584},
year = {2024},
issn = {0738-081X},
doi = {https://doi.org/10.1016/j.clindermatol.2024.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0738081X24001810},
author = {Lu Yuan and Kai Jin and An Shao and Jia Feng and Caiping Shi and Juan Ye and Andrzej Grzybowski},
abstract = {Bibliometric methods were used to analyze publications on the use of artificial intelligence (AI) in skin cancer from 2010 to 2022, aiming to explore current publication trends and future directions. A comprehensive search using four terms, “artificial intelligence,” “machine learning,” “deep learning,” and “skin cancer,” was performed in the Web of Science database for original English language publications on AI in skin cancer from 2010 to 2022. We visually analyzed publication, citation, and coupling information, focusing on authors, countries and regions, publishing journals, institutions, and core keywords. The analysis of 989 publications revealed a consistent year-on-year increase in publications from 2010 to 2022 (0.51% versus 33.57%). The United States, India, and China emerged as the leading contributors. IEEE Access was identified as the most prolific journal in this area. Key journals and influential authors were highlighted. Examination of the top 10 most cited publications highlights the significant potential of AI in oncology. Co-citation network analysis identified four primary categories of classical literature on AI in skin tumors. Keyword analysis indicated that "melanoma," "classification," and "deep learning" were the most prevalent keywords, suggesting that deep learning for melanoma diagnosis and grading is the current research focus. The term “pigmented skin lesions” showed the strongest burst and longest duration, whereas “texture” was the latest emerging keyword. AI represents a rapidly growing area of research in skin cancer with the potential to significantly improve skin cancer management. Future research will likely focus on machine learning and deep learning technologies for screening and diagnostic purposes.}
}
@article{BURGGRAF2024254,
title = {Paving the way for automated factory planning – applying rule-based expert systems to capacity planning},
journal = {Procedia CIRP},
volume = {126},
pages = {254-259},
year = {2024},
note = {17th CIRP Conference on Intelligent Computation in Manufacturing Engineering (CIRP ICME ‘23)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.08.335},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124009065},
author = {Peter Burggräf and Tobias Adlon and Niklas Schäfer},
keywords = {Factory Planning, Digital Factory, Planning Automation, Rule-Based Systems, Semantic Web},
abstract = {Today, numerous different software systems aid factory planners in their tasks. Nevertheless, due to their lacking interoperability and project-specific approaches, generic support for automated decision-making is still missing. Investigating the state of the art, we conclude that knowledge-based information modeling is needed for decision-making support. However, as the identified approaches of previous works propose no general automation concepts. Therefore, we define the guiding research question as how to model processual domain knowledge for automating factory planning processes. In this paper, we propose a planning assistance on rule-based expert systems. The planning assistance is composed of an ontology-based information model, a planning model consisting of individual planning functions, and a domain-specific inference engine. We implement the planning assistance with Semantic Web technologies and validate the solution using an application example from capacity planning. Thereby, we demonstrate the applicability of rule-based expert systems for automated factory planning. Finally, implications for future research are drawn for exploring further application areas and developing anticipated hybrid solution concepts.}
}
@article{SANFILIPPO2018174,
title = {Ontological foundations for feature-based modeling},
journal = {Procedia CIRP},
volume = {70},
pages = {174-179},
year = {2018},
note = {28th CIRP Design Conference 2018, 23-25 May 2018, Nantes, France},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118300994},
author = {Emilio M. Sanfilippo},
keywords = {Ontology, Feature-based modeling, Design, Manufacturing},
abstract = {Feature-based modeling is amongst the leading approaches for Computer Aided (CAx) product modeling. Its core benefit is the use of features to embed design intents into pure geometric product models in order to convey information based on experts’ knowledge and applications requirements. Despite various attempts, the very notion of feature remains ambiguous and no promising approach has been proposed to disambiguate and possibly unify its various meanings under a common framework. As a consequence, feature-based models are tuned on specific applications, are hardly reusable across systems, and are scarcely transparent for human comprehension. The purpose of this paper is to present an ontological characterization of features that can act as backbone conceptual and computational structure to represent the meaning of feature classes in a clear manner. For this goal, the ontology formalizes the most general and fundamental properties that all features are required to satisfy. The ontology is built on previous works and integrates the notion of feature within a broader framework for product knowledge representation.}
}
@article{BOGDANOVIC2023118958,
title = {Cross-portal metadata alignment – Connecting open data portals through means of formal concept analysis},
journal = {Information Sciences},
volume = {637},
pages = {118958},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.118958},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523005273},
author = {Miloš Bogdanović and Milena Frtunić Gligorijević and Nataša Veljković and Darko Puflović and Leonid Stoimenov},
keywords = {Open data, Metadata, Formal concept analysis, Semantic similarity, Natural language processing},
abstract = {Due to openness and transparency initiatives, a vast amount of data is being made publicly available. This data has great significance for business and society. However, it also led to challenges that need to be overcome for this data to reach its full potential. In this paper, we are focusing on the problem of connecting open data portals (ODPs) through metadata alignment. We investigate the available metadata accompanying datasets, especially the part related to categories datasets belong to and tags that closely describe datasets. The methodology we propose is relying on Formal Concept Analysis for the creation of the hierarchical structure used for determining the similarity of tags' usage in different ODPs. We propose such a structure to be used for open data portal metadata alignment. Further, we apply semantic similarity measures to reduce the complexity of the cross-portal data structure while preserving all its characteristics. We demonstrate how our approach can be used for determining dataset category across multiple ODPs aligned using the data structure our approach generates. We envision our approach to improve cross-portal search and metadata enrichment through open data categorization. Lastly, the quality of our approach was tested using datasets obtained from Canada’s and New Zealand’s ODPs.}
}
@article{MOKOS2020100030,
title = {A survey on the formalisation of system requirements and their validation},
journal = {Array},
volume = {7},
pages = {100030},
year = {2020},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2020.100030},
url = {https://www.sciencedirect.com/science/article/pii/S2590005620300151},
author = {Konstantinos Mokos and Panagiotis Katsaros},
keywords = {Requirement specification, Requirement formalisation, Semantic analysis, Model-based design, Component-based design, Formal verification},
abstract = {System requirements define conditions and capabilities to be met by a system under design. They are a partial definition in natural language, with inevitable ambiguities. Formalisation concerns with the transformation of requirements into a specification with unique interpretation, for resolving ambiguities, underspecified references and for assessing whether requirements are consistent, correct (i.e. valid for an acceptable solution) and attainable. Formalisation and validation of system requirements provides early evidence of adequate specification, for reducing the validation tests and high-cost corrective measures in the later system development phases. This article has the following contributions. First, we characterise the specification problem based on an ontology for some domain. Thus, requirements represent a particular system among many possible ones, and their specification takes the form of mapping their concepts to a semantic model of the system. Second, we analyse the state-of-the-art of pattern-based specification languages, which are used to avoid ambiguity. We then discuss the semantic analyses (missing requirements, inconsistencies etc.) supported in such a framework. Third, we survey related research on the derivation of formal properties from requirements, i.e. verifiable specifications that constrain the system’s structure and behaviour. Possible flaws in requirements may render the derived properties unsatisfiable or not realizable. Finally, this article discusses the important challenges for the current requirements analysis tools, towards being adopted in industrial-scale projects.}
}
@article{FALDUTI2024105999,
title = {Ontological models for representing image-based sexual abuses},
journal = {Computer Law & Security Review},
volume = {54},
pages = {105999},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.105999},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000669},
author = {Mattia Falduti and Cristine Griffo},
keywords = {Legal ontology, UFO-L, Image-based sexual abuse, Sextortion},
abstract = {In recent years, there has been extensive discourse on the moderation of abusive content online. Image-based Sexual Abuses (IBSAs) represent a type of abusive content that involves sexual images or videos. Platforms must moderate user-generated online content to tackle this issue effectively. One way to achieve this is by allowing users to report content, which can be flagged as abusive. In such instances, platforms may enforce their terms of service and prohibit certain types of content or users. Alongside these efforts, numerous countries have been making progress in defining and regulating this subject by implementing dedicated regulations. However, national solutions alone are insufficient for addressing a constantly increasing global emergency. Consequently, digital platforms create their own definitions of abusive conduct to overcome obstacles arising from conflicting national laws. In this paper, we use an ontological approach to model two types of abusive behavior. To do this, we applied the UFO-L patterns to build ontological models and based them on a top-level ontology, the Unified Foundational Ontology (UFO). The outcome is a set of ontological models that digital platforms can use to monitor and manage user compliance with the service provider’s code of conduct.}
}
@article{SIKOS201829,
title = {Representing network knowledge using provenance-aware formalisms for cyber-situational awareness},
journal = {Procedia Computer Science},
volume = {126},
pages = {29-38},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.206},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918311803},
author = {Leslie F. Sikos and Markus Stumptner and Wolfgang Mayer and Catherine Howard and Shaun Voigt and Dean Philp},
keywords = {Knowledge representation, ontology engineering, cybersecurity},
abstract = {Due to the volume, variety, and veracity of network data available, information fusion and reasoning techniques are needed to support network analysts’ cyber-situational awareness. These techniques rely on formal knowledge representation to define the network semantics with data provenance at various levels of granularity. To this end, this paper proposes the Communication Network Topology and Forwarding Ontology, a state-of-the-art ontology that enables the formal, unified representation of complex network concepts regardless of the type of the data source. The implementation of this ontology allows network analysts to represent expert knowledge and query network data fused from disparate data sources.}
}
@article{WICKETT20181175,
title = {A logic-based framework for collection/item metadata relationships},
journal = {Journal of Documentation},
volume = {74},
number = {6},
pages = {1175-1189},
year = {2018},
issn = {0022-0418},
doi = {https://doi.org/10.1108/JD-01-2018-0017},
url = {https://www.sciencedirect.com/science/article/pii/S0022041818000103},
author = {Karen Wickett},
keywords = {Semantics, Cataloguing, Metadata, Digital libraries, Linked data, Collections, Information modelling},
abstract = {Purpose
The purpose of this paper is to present a framework for the articulation of relationships between collection-level and item-level metadata as logical inference rules. The framework is intended to allow the systematic generation of relevant propagation rules and to enable the assessment of those rules for particular contexts and the translation of rules into algorithmic processes.
Design/methodology/approach
The framework was developed using first order predicate logic. Relationships between collection-level and item-level description are expressed as propagation rules – inference rules where the properties of one entity entail conclusions about another entity in virtue of a particular relationship those individuals bear to each other. Propagation rules for reasoning between the collection and item level are grouped together in the framework according to their logical form as determined by the nature of the propagation action and the attributes involved in the rule.
Findings
The primary findings are the analysis of relationships between collection-level and item-level metadata, and the framework of categories of propagation rules. In order to fully develop the framework, the paper includes an analysis of colloquial metadata records and the collection membership relation that provides a general method for the translation of metadata records into formal knowledge representation languages.
Originality/value
The method for formalizing metadata records described in the paper represents significant progress in the application of knowledge representation techniques to problems of metadata creation and management, providing a flexible technique for encoding colloquial metadata as a set of statements in first-order logic. The framework of rules for collection/item metadata relationships has a range of potential applications for the enhancement or metadata systems and vocabularies.}
}
@article{VANDAMME2022100731,
title = {The International Society for the Study of Vascular Anomalies (ISSVA) ontology},
journal = {Journal of Web Semantics},
volume = {74},
pages = {100731},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100731},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000221},
author = {Philip {van Damme} and Martijn G. Kersloot and Bruna {dos Santos Vieira} and Leo {Schultze Kool} and Ronald Cornet},
keywords = {ISSVA classification, Vascular anomalies, Vascular tumors, Vascular malformations, Ontology engineering},
abstract = {The International Society for the Study of Vascular Anomalies (ISSVA) provides a classification for vascular anomalies that enables specialists to unambiguously classify diagnoses. This classification is only available in PDF format and is not machine-readable, nor does it provide unique identifiers that allow for structured registration. In this paper, we describe the process of transforming the ISSVA classification into an ontology. We also describe the structure of this ontology, as well as two applications of the ontology using examples from the domain of rare disease research. We used the expertise of an ontology expert and clinician during the development process. We semi-automatically added mappings to relevant external ontologies using automated ontology matching systems and manual assessment by experts. The ISSVA ontology should contribute to making data for vascular anomaly research more Findable, Accessible, Interoperable, and Reusable (FAIR). The ontology is available at https://bioportal.bioontology.org/ontologies/ISSVA.}
}
@article{ZHENG2020309,
title = {A Quality-Oriented Digital Twin Modelling Method for Manufacturing Processes Based on A Multi-Agent Architecture},
journal = {Procedia Manufacturing},
volume = {51},
pages = {309-315},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.044},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920318990},
author = {Xiaochen Zheng and Foivos Psarommatis and Pierluigi Petrali and Claudio Turrin and Jinzhi Lu and Dimitris Kiritsis},
keywords = {Digital Twin, multi-agent system, quality control, manufacturing processes},
abstract = {The quality of a product is highly dependent on manufacturing processes. The recent development of industrial information technologies, such as Cyber-Physical Production Systems, Industrial Internet of Things, and Big Manufacturing Data Analytics has empowered the digitalization of manufacturing processes and promoted the concept of Digital Twin (DT). As one of the fundamental enabling technologies for Industry 4.0, DT enables the convergence between a physical system and its digital representation. DT modelling is the basis of implementing DT in practice. In this paper, we propose a DT modelling method based on a multi-agent architecture. It focuses on quality control during manufacturing processes and provides solutions to gather relevant information and analyze the corresponding influences on product quality. The MPFQ-model (Material, Production Process, Product Function/Future, Product Quality) is adopted to support the analysis of main influential factors related to the final product quality during the manufacturing phase. The five-dimension architecture is used as the basis for the DT models, including (i) physical entities, (ii) virtual models, (iii) DT data, (iv) services and (v) connections. Based on this architecture a Multi-Agent System (MAS) component and a semantic engineering component are integrated to create a quality-oriented DT framework.}
}
@article{HOCKER2020671,
title = {Participatory design for ontologies: a case study of an open science ontology for qualitative coding schemas},
journal = {Aslib Journal of Information Management},
volume = {72},
number = {4},
pages = {671-685},
year = {2020},
issn = {2050-3806},
doi = {https://doi.org/10.1108/AJIM-11-2019-0320},
url = {https://www.sciencedirect.com/science/article/pii/S2050380620000344},
author = {Julian Hocker and Christoph Schindler and Marc Rittberger},
keywords = {Ontology engineering, Participatory design, Digital humanities, Semantic web, Open science, Qualitative research, Coding schemas},
abstract = {Purpose
The open science movement calls for transparent and retraceable research processes. While infrastructures to support these practices in qualitative research are lacking, the design needs to consider different approaches and workflows. The paper bases on the definition of ontologies as shared conceptualizations of knowledge (Borst, 1999). The authors argue that participatory design is a good way to create these shared conceptualizations by giving domain experts and future users a voice in the design process via interviews, workshops and observations.
Design/methodology/approach
This paper presents a novel approach for creating ontologies in the field of open science using participatory design. As a case study the creation of an ontology for qualitative coding schemas is presented. Coding schemas are an important result of qualitative research, and reuse can yield great potential for open science making qualitative research more transparent, enhance sharing of coding schemas and teaching of qualitative methods. The participatory design process consisted of three parts: a requirement analysis using interviews and an observation, a design phase accompanied by interviews and an evaluation phase based on user tests as well as interviews.
Findings
The research showed several positive outcomes due to participatory design: higher commitment of users, mutual learning, high quality feedback and better quality of the ontology. However, there are two obstacles in this approach: First, contradictive answers by the interviewees, which needs to be balanced; second, this approach takes more time due to interview planning and analysis.
Practical implications
The implication of the paper is in the long run to decentralize the design of open science infrastructures and to involve parties affected on several levels.
Originality/value
In ontology design, several methods exist by using user-centered design or participatory design doing workshops. In this paper, the authors outline the potentials for participatory design using mainly interviews in creating an ontology for open science. The authors focus on close contact to researchers in order to build the ontology upon the expert's knowledge.}
}
@article{DRAGONI2020101840,
title = {Explainable AI meets persuasiveness: Translating reasoning results into behavioral change advice},
journal = {Artificial Intelligence in Medicine},
volume = {105},
pages = {101840},
year = {2020},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2020.101840},
url = {https://www.sciencedirect.com/science/article/pii/S0933365719310140},
author = {Mauro Dragoni and Ivan Donadello and Claudio Eccher},
keywords = {Explainable AI, Explainable reasoning, Natural Language Generation, MHealth, Ontologies},
abstract = {Explainable AI aims at building intelligent systems that are able to provide a clear, and human understandable, justification of their decisions. This holds for both rule-based and data-driven methods. In management of chronic diseases, the users of such systems are patients that follow strict dietary rules to manage such diseases. After receiving the input of the intake food, the system performs reasoning to understand whether the users follow an unhealthy behavior. Successively, the system has to communicate the results in a clear and effective way, that is, the output message has to persuade users to follow the right dietary rules. In this paper, we address the main challenges to build such systems: (i) the Natural Language Generation of messages that explain the reasoner inconsistency; and, (ii) the effectiveness of such messages at persuading the users. Results prove that the persuasive explanations are able to reduce the unhealthy users’ behaviors.}
}
@article{PETRUCCI201866,
title = {Expressive ontology learning as neural machine translation},
journal = {Journal of Web Semantics},
volume = {52-53},
pages = {66-82},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300507},
author = {Giulio Petrucci and Marco Rospocher and Chiara Ghidini},
keywords = {Ontology learning, Neural networks, Natural language processing},
abstract = {Automated ontology learning from unstructured textual sources has been proposed in literature as a way to support the difficult and time-consuming task of knowledge modeling for semantic applications. In this paper we propose a system, based on a neural network in the encoder–decoder configuration, to translate natural language definitions into Description Logics formulæ through syntactic transformation. The model has been evaluated to assess its capacity to generalize over different syntactic structures, tolerate unknown words, and improve its performance by enriching the training set with new annotated examples. The results obtained in our evaluation show how approaching the ontology learning problem as a neural machine translation task can be a valid way to tackle long term expressive ontology learning challenges such as language variability, domain independence, and high engineering costs.}
}
@article{DAUSCH20241364,
title = {Semantic Integration and Interdisciplinary Collaboration in Production Planning: A Graph-Based Approach for Enhanced Data Consistency},
journal = {Procedia CIRP},
volume = {130},
pages = {1364-1371},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.253},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124014100},
author = {Valesko Dausch and Joachim Lentes and Oliver Riedel and Matthias Kreimeyer},
keywords = {Data driven development, knowledge management, data continuity, method},
abstract = {Based on increasing individualization, shorter innovation cycles and integrating services into products complexity in the development of products and production processes increases. In combination with digitalization, this results in an increased need for interdisciplinary work. A high level of data consistency is required to cope with the resulting diverse and complex data structures, leading to the need for a consistent data model that orchestrates information flows. PLM-systems fall short in their ambition of reaching an overarching data management across the product lifecycle. This results in a lot of administrative work in planning instead of value-adding as well as in inconsistencies and incomplete change processes. To improve interdisciplinary collaboration in production planning, goal of this work is to develop a method leading to a comprehensive data model and to show it’s applicability by means of a case study. The steps of the research performed follow the typical circle of action research: diagnosis, planning, action, evaluation, and reflection. The data model resulting of the application of the method uses semantics to make relationships understandable for both humans and machines. This is necessary for downstream automation and the use of artificial intelligence. The three essential steps of the method are, firstly, an as-is modeling of the processes of interest. Then, the process models are used to evaluate the consistency and efficiency of the planning processes. The method is completed by a guideline for setting up the data model. The proposed approach aims to improve efficiency and quality in assembly planning processes, resulting in its importance for industrial companies.}
}
@article{OYEDEJI2024233,
title = {Leveraging Ontology Development to Enhance Corrosion Visualisation in Engineering Design},
journal = {Procedia CIRP},
volume = {128},
pages = {233-238},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S221282712400667X},
author = {Oluseyi Ayodeji Oyedeji and Samir Khan and John Ahmet Erkoyuncu},
keywords = {Engineering Design, Ontology, Corrosion Detection, Automation, Visualisation},
abstract = {Designing automated solutions for understanding, visualising, and detecting damage has become a key area in the era of digitalisation and advanced engineering. The use of deep learning and other artificial intelligence approaches has proven very innovative and rendered manual methods of damage inspection primitive. However, there remains a critical gap in creating a unified knowledge base that helps in understanding, conceptualising, and promoting collaborative engineering design, particularly in the context of automated corrosion detection through images. This research addresses this gap by presenting a conceptualised model for understanding corrosion detection through ontology development. This is implemented in an ontology development environment using Protege 5.5.0 and ELK 0.5.0 reasoner. Evaluation is done using expert competency questions and hypothetical scenarios thereby establishing a robust framework that is beneficial to Engineering design in terms of terminology standardisation, design process facilitation, and building corrosion inspection systems that are interoperable. The Ontology also enables sharing and reusing knowledge between automated corrosion detection systems as well as integration with existing industrial standards such as Industrial Ontology Foundry (IOF) and semantic web standards. Hence, offering a significant contribution to the digitalisation of engineering design. Besides the enhancement of damage detection, this work also advances the engineering field to utilise visual data more effectively in design, maintenance, and product lifecycle management.}
}
@article{IQBAL201873,
title = {A mathematical evaluation for measuring correctness of domain ontologies using concept maps},
journal = {Measurement},
volume = {118},
pages = {73-82},
year = {2018},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2018.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0263224118300083},
author = {Rizwan Iqbal and Masrah Azrifah {Azmi Murad} and Layth Sliman and Clay Palmeira {da Silva}},
keywords = {Ontology engineering, Concept mapping, Ontology evaluation, Closeness index, Similarity index},
abstract = {There is a need for further research in the area of ontology evaluation specifically dealing with ontology development exploiting concept maps. The existing literature on ontology evaluation primarily emphasis on ontology formalisation as well as on performing logical inferences, which is usually not directly relevant for concept maps as they are commonly exploited as communication instruments for learning purposes. Commonly used techniques for evaluating concept maps for knowledge assessment may be adopted for a kind of criteria-based evaluation of a domain concept map with respect to a particular aspect. However, this makes its validity limited to a particular aspect or criteria. This paper presents a mathematical ontology evaluation technique to measure the correctness of domain ontologies engineered using concept maps. It is based on the notion of merging two different mathematical measures, namely closeness index and similarity index to come up with a combined index that takes different criteria or aspects into account while performing ontology evaluation. Therefore, the proposed technique makes the evaluation process more reliable and robust. Two case studies were conducted employing the proposed technique for evaluating two different domain ontologies that were engineered using concept maps. Calculations and results from the case studies showed that depending on the correctness of individual ontology, different values of combined Index was calculated manifesting the measure of correctness of each individual ontology in a quantifiable form. Moreover, the results depict that the technique provides in-depth evaluation, it is easy to adopt, requires no special skills, and is conveniently replicable.}
}
@article{ALOBAIDI2018117,
title = {Automated ontology generation framework powered by linked biomedical ontologies for disease-drug domain},
journal = {Computer Methods and Programs in Biomedicine},
volume = {165},
pages = {117-128},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169260717315791},
author = {Mazen Alobaidi and Khalid Mahmood Malik and Maqbool Hussain},
keywords = {Semantic web, Ontology generation, Linked biomedical ontologies},
abstract = {Objective and background: The exponential growth of the unstructured data available in biomedical literature, and Electronic Health Record (EHR), requires powerful novel technologies and architectures to unlock the information hidden in the unstructured data. The success of smart healthcare applications such as clinical decision support systems, disease diagnosis systems, and healthcare management systems depends on knowledge that is understandable by machines to interpret and infer new knowledge from it. In this regard, ontological data models are expected to play a vital role to organize, integrate, and make informative inferences with the knowledge implicit in that unstructured data and represent the resultant knowledge in a form that machines can understand. However, constructing such models is challenging because they demand intensive labor, domain experts, and ontology engineers. Such requirements impose a limit on the scale or scope of ontological data models. We present a framework that will allow mitigating the time-intensity to build ontologies and achieve machine interoperability. Methods: Empowered by linked biomedical ontologies, our proposed novel Automated Ontology Generation Framework consists of five major modules: a) Text Processing using compute on demand approach. b) Medical Semantic Annotation using N-Gram, ontology linking and classification algorithms, c) Relation Extraction using graph method and Syntactic Patterns, d), Semantic Enrichment using RDF mining, e) Domain Inference Engine to build the formal ontology. Results: Quantitative evaluations show 84.78% recall, 53.35% precision, and 67.70% F-measure in terms of disease-drug concepts identification; 85.51% recall, 69.61% precision, and F-measure 76.74% with respect to taxonomic relation extraction; and 77.20% recall, 40.10% precision, and F-measure 52.78% with respect to biomedical non-taxonomic relation extraction. Conclusion: We present an automated ontology generation framework that is empowered by Linked Biomedical Ontologies. This framework integrates various natural language processing, semantic enrichment, syntactic pattern, and graph algorithm based techniques. Moreover, it shows that using Linked Biomedical Ontologies enables a promising solution to the problem of automating the process of disease-drug ontology generation.}
}
@article{BENITEZANDRADES2020390,
title = {An ontology-based multi-domain model in social network analysis: Experimental validation and case study},
journal = {Information Sciences},
volume = {540},
pages = {390-413},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520305909},
author = {José Alberto Benítez-Andrades and Isaías García-Rodríguez and Carmen Benavides and Héctor Alaiz-Moretón and José Emilio {Labra Gayo}},
keywords = {Ontology-based systems, Semantic web, Semantic technologies, Social network analysis, Ontology multi-domain, Knowledge-based systems},
abstract = {The use of social network theory and methods of analysis have been applied to different domains in recent years, including public health. The complete procedure for carrying out a social network analysis (SNA) is a time-consuming task that entails a series of steps in which the expert in social network analysis could make mistakes. This research presents a multi-domain knowledge model capable of automatically gathering data and carrying out different social network analyses in different domains, without errors and obtaining the same conclusions that an expert in SNA would obtain. The model is represented in an ontology called OntoSNAQA, which is made up of classes, properties and rules representing the domains of People, Questionnaires and Social Network Analysis. Besides the ontology itself, different rules are represented by SWRL and SPARQL queries. A Knowledge Based System was created using OntoSNAQA and applied to a real case study in order to show the advantages of the approach. Finally, the results of an SNA analysis obtained through the model were compared to those obtained from some of the most widely used SNA applications: UCINET, Pajek, Cytoscape and Gephi, to test and confirm the validity of the model.}
}
@article{TIMAKUM2025102462,
title = {Four decades of data & knowledge engineering: A bibliometric analysis and topic evolution study (1985–2024)},
journal = {Data & Knowledge Engineering},
volume = {159},
pages = {102462},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102462},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000576},
author = {Tatsawan Timakum and Soobin Lee and Dongha Kim and Min Song and Il-Yeol Song},
keywords = {Data and knowledge engineering journal, Bibliometric analysis, Citation analysis, Topic evolution analysis, Authorship network analysis},
abstract = {The Data and Knowledge Engineering (DKE) journal has established a significant global research presence over four decades, substantially contributing to the advancement of data and knowledge engineering disciplines. This comprehensive bibliometric study analyzes the journal’s publications over the past 40 years (1985–2024), employing bibliographic records and citation data from Scopus, Web of Science (WoS), and ScienceDirect. By utilizing CiteSpace for citation and co-citation mapping and Dirichlet Multinomial Regression (DMR) topic modeling for trend analysis, the research provides a multifaceted examination of the journal’s scholarly landscape. Over its 40-year history, DKE has published 1951 articles, accumulating 53,594 citations. The study comprehensively explores key bibliometric dimensions, including influential authors, author networks, citation patterns, topic clusters, institutional contributions, and research funding sponsors, as well as evolution of topics, showing increasing, decreasing, or constant trends. Comprehensive analysis offers a meta-analytical perspective on DKE’s scholarly contributions, positioning the journal as a pioneering publication platform that advances critical knowledge and methodological innovations in data and knowledge engineering research domains. Through an in-depth examination of the journal’s publication trajectory, the study provides insights into the field’s scholarly evolution, highlighting DKE’s pivotal role in shaping academic discourse and technological understanding.}
}
@article{JAZIRI20211152,
title = {ORVIPO: An Ontological Prototype for Modeling 3D Scenes in Operating Rooms},
journal = {Procedia Computer Science},
volume = {192},
pages = {1152-1161},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.118},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921016070},
author = {Faouzi Jaziri and Rim Messaoudi and Achraf Mtibaa and Jonathan Courbon and Mahdi Kilani and Mohamed Mhiri and Antoine Vacavant},
keywords = {Virtual reality, Ontology, Semantic interoperability, Reasoning rules, Interaction},
abstract = {Virtual Operating room characterizes a large artificial environment of interaction in the medical context. It enables a better simulation for different surgical scenarios through 3D (three‐dimensional) objects. Virtual Reality (VR) uses computer technology to develop these virtual simulated applications. It brings valuable innovations in different fields. It is integrated also to enhance healthcare and therapies. To improve and assist on medical VR applications, ontologies would be useful. They can unify the content of these applications and facilitate their implementation via semantic rules. Also, ontologies can be applied to realize an effective VR knowledge modeling. This paper presents a virtual simulation of an operating room taking advantage of the semantic representation. The goal of this study is to propose a novel ontological VR system that models an operating room and its components. The execution of the proposed method was proved by the developed ontology OROnto (Operating Room Ontology). This ontology was useful for modeling hospital scenarios and the construction of a valuable VR system. To demonstrate the proposed approach feasibility and performance, we have implemented the Operating Room Virtual Integration Process Using Ontology (ORVIPO) prototype. Compared to different other ontological methods and related works, our approach have shown interesting findings such as recall (71%), precision (83%), and F-measure (76%).}
}
@article{YUAN2025103905,
title = {Research on the construction and mapping model of knowledge organization system driven by standards},
journal = {Computer Standards & Interfaces},
volume = {92},
pages = {103905},
year = {2025},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2024.103905},
url = {https://www.sciencedirect.com/science/article/pii/S0920548924000746},
author = {Jingshu Yuan and Kexin Zhai and Hongxin Li and Man Yuan},
keywords = {Knowledge organization system, Multidisciplinary theory, Concept, Metadata, semantic, Standardization, Ontology},
abstract = {With the rapid development of artificial intelligence and enterprise digital transformation, the standardization organization, storage and management of semantic knowledge in computers have become the current research focus. As the core theory of knowledge system construction, knowledge organization (KO) provides theoretical support for the study of semantic knowledge organization and representation, among which knowledge organization system (KOS) is the important tool of semantic organization. At present, many scholars have carried out research from different perspectives of KOS based on theory, which provides the direction for the sustainable development of KOS. However, most of these studies focus on some aspects of KOS, which are in a "scattered" state, lacking systematic analysis of the basic principles of KOS construction and semantic organization based on theories and international standards. Therefore, this paper firstly constructs KOS theoretical models in the conceptual world and computer world respectively through a comprehensive study of multi-disciplinary basic theories such as semantics, logic, system theory, and international standards such as ISO 1087:2019, ISO 25964:2013, and ISO 11179:2023, and traces the iterative construction, organization and mapping process from "concept" in the conceptual world to "metadata" knowledge and semantics in the computer world. The semantic organization based on metadata is realized in computer. Secondly, on this basis, in order to realize ontology representation of domain knowledge, the ontology construction method based on MDR metadata is proposed. Finally, taking the semantic organization and ontology construction of Epicentre model in petroleum field as an example, the feasibility of the ideas and methods proposed in this paper is verified. The model and method proposed in this paper is independent of the specific type of KOS, so it is innovative and universal. The methodology is also applicable to other fields of conceptual system modeling, metadata standard construction, and data model modeling.}
}
@article{GAWICH2019341,
title = {Ontology Maintenance System for Rheumatoid Disease},
journal = {Procedia Computer Science},
volume = {154},
pages = {341-346},
year = {2019},
note = {Proceedings of the 9th International Conference of Information and Communication Technology [ICICT-2019] Nanning, Guangxi, China January 11-13, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.06.049},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930818X},
author = {Mariam Gawich and Marco Alfonse and Mostafa Aref and Abdel-Badeeh M. Salem},
keywords = {Ontology engineering, Ontology maintenance, Ontology evolution, Ontology pruning, Medical Ontology},
abstract = {The constructed medical ontologies need to be updated in order to reflect the changes occurred on the medicine such as the clinical findings, treatments and their side effects. Various researchers defined the ontology maintenance as the process of updating the ontology or the evolution of the ontology. Other researches consider the ontology maintenance as a composed process that involves the ontology evolution and pruning. This paper presents a Rheumatoid ontology maintenance system that incorporates both of the ontology evolution and the ontology pruning. The evolution is executed in a way that ensures the ontology consistency and its relevance to the domain of interest.}
}
@article{SHI2023102114,
title = {An ontology-based methodology to establish city information model of digital twin city by merging BIM, GIS and IoT},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102114},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102114},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002422},
author = {Jianyong Shi and Zeyu Pan and Liu Jiang and Xiaohui Zhai},
keywords = {CIM, Ontology, Linked data, Semantic integration, BIM-GIS-IoT integration},
abstract = {With the development of digital city and smart city construction, the City Information Model (CIM) has played a critical role as a container of spatial–temporal data to establish the Digital Twin City. For a digital twin city, a virtual high-fidelity CIM model that corresponds closely to the real physical world is the premise and cornerstone of its construction. Therefore, the integration of BIM, GIS and IoT has become the preferred topic for researchers and has received much more attention from a wide academic circle. However, traditional integration mainly focuses on the conversion of both IFC and CityGML, and IoT data are also often used as visualizations. More importantly, the underlying data formats of GIS, BIM and IoT are still independent of each other without a unified data structure expression, so real data-driven analysis and decision-making cannot be implemented. This study aims to establish a general CIM ontology to integrate heterogeneous BIM, GIS and IoT data. First, the related work of BIM, GIS and IoT integration is studied and analyzed. A comparison of three mainstream approaches, data conversion, standard extension and data linking, is conducted, and it illustrates the advantages of ontology techniques in solving data interoperability problems. Second, a technical framework of BIM, GIS and IoT data integration based on ontology technology is proposed. The approach is mainly divided into five steps: geometry processing, data instantiation, ontology construction, ontology mapping and querying application. On the basis of the CIM ontology, an application ontology is built for a specific application domain to illustrate rule-based mapping, querying and inferring. Finally, the case study shows that the Ontology-based methodology in this paper has contributed to establish a general pattern for CIM data integration by mapping and linking concepts from the semantic level. It avoids changes in the original data sources and the missing data problem.}
}
@article{VALIENTE2024100830,
title = {Web3-DAO: An ontology for decentralized autonomous organizations},
journal = {Journal of Web Semantics},
volume = {82},
pages = {100830},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100830},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000167},
author = {María-Cruz Valiente and Juan Pavón},
keywords = {Blockchain, Web3, DAO, Decentralized autonomous organization, Digital governance, E-government, Online entity, Ontology, Voting system},
abstract = {Decentralized autonomous organizations (DAOs) are relatively a newly emerging type of online entity related to governance or business models where all their members work together and participate in the decision-making processes affecting the DAO in a decentralized, collective, fair, and democratic manner. In a DAO, members interaction is mediated by software agents running on a blockchain that encode the governance of the specific entity in terms of rules that optimize their business and goals. In this context, most popular DAO software frameworks provide decision-making models aiming to facilitate digital governance and the collaboration among their members intertwining social and economic concerns. However, these models are complex, not interoperable among them and lack a common understanding and shared knowledge concerning DAOs, as well as the computational semantics needed to enable automated validation, simulation or execution. Thus, this paper presents an ontology (Web3-DAO), which can support machine-readable digital governance of DAOs adding semantics to their decision-making models. The proposed ontology captures the domain logic that allows the sharing of updated information and decisions for all the members that interact with a DAO by the interoperability of their own assessment and decision tools. Furthermore, the ontology detects semantic ambiguities, uncertainties and contradictions. The Web3-DAO ontology is available in open access at https://github.com/Grasia/semantic-web3-dao.}
}
@article{HASHEMI201828,
title = {Developing a domain ontology for knowledge management technologies},
journal = {Online Information Review},
volume = {42},
number = {1},
pages = {28-44},
year = {2018},
issn = {1468-4527},
doi = {https://doi.org/10.1108/OIR-07-2016-0177},
url = {https://www.sciencedirect.com/science/article/pii/S1468452718000033},
author = {Parvin Hashemi and Ameneh Khadivar and Mehdi Shamizanjani},
keywords = {Ontology, Knowledge management processes, Knowledge management strategies, Growth stage for knowledge management technology, Knowledge management technologies},
abstract = {Purpose
The purpose of this paper is to develop a new ontology for knowledge management (KM) technologies, determining the relationships between these technologies and classification of them.
Design/methodology/approach
The study applies NOY methodology – named after Natalya F. Noy who initiated this methodology. Protégé software and ontology web language are used for building the ontology. The presented ontology is evaluated with abbreviation and consistency criteria and knowledge retrieval of KM technologies by experts.
Findings
All the main concepts in the scope of KM technologies are extracted from existing literature. There are 241 words, 49 out of them are domain concepts, eight terms are about taxonomic and non-taxonomic relations, one term relates to data property and 183 terms are instances. These terms are used to develop KM technologies’ ontology based on three factors: facilitating KM processes, supporting KM strategies and the position of technology in the KM technology stage model. The presented ontology is created a common understanding in the field of KM technologies.
Research limitations/implications
Lack of specific documentary about logic behind decision making and prioritizing criteria in choosing KM technologies.
Practical implications
Uploading the presented ontology in the web environment provides a platform for knowledge sharing between experts from around the world. In addition, it helps to decide on the choice of KM technologies based on KM processes and KM strategy.
Originality/value
Among the many categories of KM technologies in literature, there is no classifying according to several criteria simultaneously. This paper contributes to filling this gap and considers KM processes, KM strategy and stages of growth for KM technologies simultaneously to choice the KM technologies and also there exists no formal ontology regarding KM technologies. This study has tried to propose a formal KM technologies’ ontology.}
}
@article{RODLER2022108987,
title = {One step at a time: An efficient approach to query-based ontology debugging},
journal = {Knowledge-Based Systems},
volume = {251},
pages = {108987},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108987},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122004786},
author = {Patrick Rodler},
keywords = {Ontology debugging, Query-based ontology debugging, Interactive debugging, Fault localization, Sequential diagnosis, Expert questions, Ontology quality assurance, Ontology repair, Test-driven debugging, Singleton query, Model-based diagnosis, Semantic Web, User interaction, Effort minimization, Performance optimization, Entailment, Test cases, Query computation, Query selection, One-step lookahead, Debugging cost criteria, Query quality assessment, Expert types, Heuristics, Knowledge-base debugging, OntoDebug, Protégé},
abstract = {When ontologies reach a certain size and complexity, faults such as inconsistencies, unsatisfiable classes or wrong entailments are hardly avoidable. Locating the incorrect axioms that cause these faults is a hard and time-consuming task. Addressing this issue, several techniques for a semi-automatic fault localization in ontologies have been proposed and extensively studied. One class of these approaches involve a human expert who provides answers to system-generated queries about the intended (correct) ontology in order to reduce the possible fault locations. To suggest as informative questions as possible, existing methods draw on various algorithmic optimizations as well as heuristics. However, these computations are often based on certain assumptions about the interacting expert. In this work, we demonstrate that these assumptions might not always be adequate and discuss consequences of their violations. In particular, we characterize a range of expert types with different query answering behavior and show that existing approaches are far from achieving optimal efficiency for all of them. In addition, we find that the cost metric adopted by state-of-the-art techniques might not always be realistic and that a change of metric has a decisive impact on the best choice of query answering strategy. As a remedy, we suggest a new – and simpler – type of expert question that leads to a stable fault localization performance for all analyzed expert types and effort metrics, and has numerous further advantages over existing techniques. Moreover, we present an algorithm which computes and optimizes this new query type in worst-case polynomial time and which is fully compatible with existing concepts (e.g., query selection heuristics) and infrastructure (e.g., debugging user interfaces) in the field. Comprehensive experiments on faulty real-world ontologies attest that the new querying method is substantially and statistically significantly superior to existing techniques both in terms of the number of necessary expert interactions and in terms of the query computation time. We find that relying on the new querying method can save an interacting expert more than 80% of their work, and can reduce the expert’s waiting time for the next query by more than three orders of magnitude. Beside these findings, we demonstrate that the efficiency of existing query-based tools can be significantly boosted by suggesting an appropriate query answering strategy to an expert; we also make recommendations in this regard. Further, we suggest optimal configurations of a debugger for situations where the new type of query is used. Remarkably, the proposed approach is not only applicable to ontologies, but to any monotonic knowledge representation language, and can even be adopted to solve general model-based diagnosis problems expressible using Reiter’s theory.}
}
@article{VOLPERT20242994,
title = {Compatibility Assessment for Interfaces in Drivetrains of Robot-Like Systems},
journal = {Procedia Computer Science},
volume = {232},
pages = {2994-3002},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.115},
url = {https://www.sciencedirect.com/science/article/pii/S187705092400293X},
author = {Marcus Volpert and Birgit Vogel-Heuser and Dominik Hujo and Karsten Stahl and Markus Zimmermann},
keywords = {robot-like systems, interface modeling, ontology, model-based systems engineering, drivetrains},
abstract = {Due to the complexity of robot-like systems (RLS), new developments are often avoided, and most variants of the systems are designed. The primary determinant of an RLS is its drivetrain, which comprises purchased components, namely the controller, motor driver, motor, and gearbox. Each of these purchased parts has different interfaces that are not standardized. If the interfaces are sufficiently complex, the compatibility of the parts can no longer be guaranteed manually. Therefore, this paper presents a draft ontology that verifies the compatibility of purchased parts in RLS drivetrains. Classes and properties are obtained from expert knowledge, norms, data standards, and data sheets to build an ontology. The ontology design is evaluated using an application example for a motor-gearbox interface.}
}
@article{KUDRYAVTSEV2020500,
title = {Modelling Consumer Knowledge: the Role of Ontology},
journal = {Procedia Computer Science},
volume = {176},
pages = {500-507},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.08.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920318767},
author = {D. Kudryavtsev and T. Gavrilova and M. Smirnova and K. Golovacheva},
keywords = {consumer knowledge, consumer behaviour, knowledge management, innovative products, services, knowledge economy, ontology},
abstract = {Knowledge economy and further development of the information society made knowledge of all market interaction participants a key factor of consumption, adding value, including joint generation of value and innovation. Alongside with general increase in information volumes and decrease in consumer trust to it, the very products and services, as well as consumption technology and culture have become more complex and, thus, demonstrate relevance of managing consumer knowledge. Such complexity requires to teach consumers and to exchange knowledge with them. Consumer knowledge is of paramount importance for innovative products and services, as it is a key factor of innovation-decision process. Consumer knowledge practice needs clear understanding of this concept ("consumer knowledge"), its kinds and features, processes of acquiring and changing this knowledge, its influence on consumer behaviour, as well as company’s capabilities to establish consumer knowledge. Such understanding will be provided by creating ontology of innovative products and services’ consumer knowledge. Such ontology will help to resolve a whole range of enterprise engineering tasks: design of innovative products and services, as well as ecosystem surrounding them; design of an interaction system between a company and a consumer during a whole customer journey. This paper describes main requirements on ontology, discusses some existing ontologies, as well as contains primary results of ontology conceptualisation.}
}
@article{YANG2020104437,
title = {Construction of logistics financial security risk ontology model based on risk association and machine learning},
journal = {Safety Science},
volume = {123},
pages = {104437},
year = {2020},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2019.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0925753519313050},
author = {Bo Yang},
keywords = {Logistics finance risk, Ontology, Semantic parsing, Apriori, Risk association},
abstract = {Previous research on logistics financial risk pre-warning and pre-control focuses on the linear causal relationship between risk and risk events. In fact, risk events in logistics financial field are often caused by multiple risk factors, which are directly or indirectly related to these risk factors. Therefore, it is helpful for the healthy development of logistics finance to find out the related risks of each logistics financial risk event and screen and control them one by one. This paper proposes OntoLFR (Logistics Financial Risk Ontology), and constructs the logistics financial risk ontology model to adapt to the variability, complexity and relevance of risk in early warning and pre-control. Then, based on the risk source association inference rules obtained by knowledge association analysis, Apriori algorithm is adopted to conduct association analysis on the risk hidden danger database, and the acquired association rules are reintroduced into the knowledge ontology database of risk event source to realize self-learning and self-correction of the knowledge ontology database. Taking the risk event (RW_risk) of the financing enterprise to escape, the feasibility of using the logistics financial risk ontology model for risk-related reasoning and analysis is verified.}
}
@article{FENG2022474,
title = {Computing Sufficient and Necessary Conditions in CTL: A Forgetting Approach},
journal = {Information Sciences},
volume = {616},
pages = {474-504},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.10.124},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522012518},
author = {Renyan Feng and Erman Acar and Yisong Wang and Wanwei Liu and Stefan Schlobach and Weiping Ding},
keywords = {Computation tree logic, Forgetting, Weakest sufficient condition, Model checking},
abstract = {Computation tree logic (CTL) is an essential specification language in the field of formal verification. In systems design and verification, it is often important to update existing knowledge with new attributes and subtract the irrelevant content while preserving the given properties on a known set of atoms. Under the scenario, given a specification, the weakest sufficient condition (WSC) and the strongest necessary condition (SNC) are dual concepts and very informative in formal verification. In this article, we generalize our previous results (i.e., the decomposition, homogeneity properties, and the representation theorem) on forgetting in bounded CTLto the unbounded one. The cost we pay is that, unlike the bounded case, the result of forgetting in CTLmay no longer exist. However, SNC and WSC can be obtained by the new forgetting machinery we are presenting. Furthermore, we complement our model-theoretic approach with a resolution-based method to compute forgetting results in CTL. This method is currently the only way to compute forgetting results for CTLand temporal logic. The method always terminates and is sound. That way, we set up the resolution-based approach for computing WSC and SNC in CTL.}
}
@article{MCGLINN2021103534,
title = {Publishing authoritative geospatial data to support interlinking of building information models},
journal = {Automation in Construction},
volume = {124},
pages = {103534},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103534},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520311146},
author = {Kris McGlinn and Rob Brennan and Christophe Debruyne and Alan Meehan and Lorraine McNerney and Eamonn Clinton and Philip Kelly and Declan O'Sullivan},
keywords = {Building Information Modelling, Geographic Information Systems, Ontology Engineering, Resource Description Framework (RDF), Linked Data},
abstract = {Building Information Modelling (BIM) is a key enabler to support integration of building data within the buildings life cycle (BLC) and is an important aspect to support a wide range of use cases, related to intelligent automation, navigation, energy efficiency, sustainability and so forth. Open building data faces several challenges related to standardization, data interdependency, data access, and security. In addition to these technical challenges, there remains the barrier among BIM developers who wish to protect their intellectual property, as full 3D BIM development requires expertise and effort. This means that there is often limited availability of building data. However, a Linked Data approach to BIM, combined with a supporting national geospatial identifier infrastructure makes interlinking and controlled sharing of BIM models possible. In Ireland, the Ordnance Survey Ireland (OSi) maintains a substantial data set, called Prime2, which includes not only building GIS data (polygon footprint, geodetic coordinate), but also additional building specific data (e.g. form, function and status). The data set also includes change information, recording when changes took place and who captured and validated those changes. This paper presents the development of a national geospatial identifier infrastructure based on an OSi building ontology that supports capturing OSi building data using Resource Description Framework (RDF). The paper details the different steps required to generate the ontology and publish the data. First, an initial analysis of the data set to generate the ontology is discussed. This includes identification of mappings to existing standards, e.g. GeoSPARQL to handle geometries and PROV-O to handle provenance, to the development of R2RML mappings to generate the RDF and the method for deploying the ontology and the building graphs. This data is then made available dependent on different licensing agreements handled by an access control approach. Methods are then presented to support the interlinking of the authoritative data with other building data standards and data sets using geolocation, followed finally by discussion and future work.}
}
@article{DIAMANTINI2023224,
title = {Process-aware IIoT Knowledge Graph: A semantic model for Industrial IoT integration and analytics},
journal = {Future Generation Computer Systems},
volume = {139},
pages = {224-238},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2200320X},
author = {Claudia Diamantini and Alex Mircoli and Domenico Potena and Emanuele Storti},
keywords = {Industrial Internet of Things, Data integration, Business process, Semantics, Ontology, Knowledge Graph},
abstract = {The integration of the huge data streams produced by the Industrial Internet of Things (IIoT) can provide invaluable knowledge in the context of Industry 4.0, and is also an open research issue. The present paper proposes a semantic approach to this issue, centred around the notion of process as the backbone. We build an ontology describing the fundamental elements involved in IIoT and their relations, and discuss the construction of the Process-aware IIoT Knowledge Graph, where raw sensor data are enriched with information about process activities and the physical production environment. We also propose a framework for querying the Knowledge Graph, and we demonstrate its capabilities by considering the production of metal accessories as case study.}
}
@article{SCHAFFHAUSER2023105695,
title = {A framework for the broad dissemination of hydrological models for non-expert users},
journal = {Environmental Modelling & Software},
volume = {164},
pages = {105695},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105695},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223000816},
author = {Timo Schaffhauser and Daniel Garijo and Maximiliano Osorio and Daniel Bittner and Suzanne Pierce and Hernán Vargas and Markus Disse and Yolanda Gil},
keywords = {Software metadata, Model metadata, Model encapsulation, Model catalogs, MINT, Hydrological models},
abstract = {Hydrological models are essential in water resources management, but the expertise required to operate them often exceeds that of potential stakeholders. We present an approach that facilitates the dissemination of hydrological models, and its implementation in the Model INTegration (MINT) framework. Our approach follows principles from software engineering to create software components that reveal only selected functionality of models which is of interest to users while abstracting from implementation complexity, and to generate metadata for the model components. This methodology makes the models more findable, accessible, interoperable, and reusable in support of FAIR principles. We showcase our methodology and its implementation in MINT using two case studies. We illustrate how the models SWAT and MODFLOW are turned into software components by hydrology experts, and how users without hydrology expertise can find, adapt, and execute them. The two models differ in terms of represented processes and in model design and structure. Our approach also benefits expert modelers, by simplifying model sharing and the execution of model ensembles. MINT is a general modeling framework that uses artificial intelligence techniques to assist users, and is released as open-source software.}
}
@article{AMER2022102670,
title = {Robust deep learning early alarm prediction model based on the behavioural smell for android malware},
journal = {Computers & Security},
volume = {116},
pages = {102670},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102670},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822000694},
author = {Eslam Amer and Shaker El-Sappagh},
keywords = {Android malware prediction, API Calls, Contextual behaviour, System calls, Permissions, Behavioral analysis, Process mining, Sequence reformulation},
abstract = {Due to the widespread expansion of the Android malware industry, malicious Android processes mining became a necessity to understand their behavior. Nevertheless, due to the complexities of size, length, and associations of some essential and distinguishing Android applications’ features such as API calls and system calls, mining malicious Android processes become a prominent obstacle. The malicious process mining obstacle is also coupled with the increasing rate of zero-day attacks, with no prior knowledge about those kinds of behaviors. Hence, malware detection alone is no longer enough; instead, we need new methodologies to predict malicious behaviors early. In this paper, we propose a behavioral Android malware smell predictor model. Our model relies on various static and dynamic features. We overcame the problem of massive feature size and complex associations by encapsulating related features in a few cluster classes. Accordingly, the cluster classes are exchangeably used to represent the features in the original calling sequences. Regarding substantially long sequences, experimental results showed that our model could predict whether a process is behaving maliciously or not based on rapid-sequence-snapshot analysis. The proposed model counted on the LSTM model to classify the reformed API and system call sequences snapshots. Moreover, we used ensemble machine learning classifiers to classify Android permissions. We trained the LSTM model using random snapshots of the newly formed API and system call cluster sequences. We tested our model against common ransomware attacks. We found that our trained LSTM model showed stable performance at a particular snapshot size. The model showed competitive accuracy in predicting new sequences. Accordingly, we proposed an early alarm solution for blocking malicious payloads instead of identifying them after their fulfillment. Hence, we can avoid the cost of future damage.}
}
@article{GARCIA2020104387,
title = {The GeoCore ontology: A core ontology for general use in Geology},
journal = {Computers & Geosciences},
volume = {135},
pages = {104387},
year = {2020},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2019.104387},
url = {https://www.sciencedirect.com/science/article/pii/S0098300419306284},
author = {Luan Fonseca Garcia and Mara Abel and Michel Perrin and Renata {dos Santos Alvarenga}},
keywords = {GeoCore ontology, Core ontology, Geological knowledge, Ontology engineering, BFO top-level ontology, Knowledge modeling},
abstract = {Domain ontologies assume the role of representing, in a formal way, a consensual knowledge of a community over a domain. This task is especially difficult in a wide domain like Geology, which is composed of diversified science resting on a large variety of conceptual models that were developed over time. The meaning of the concepts used by the various professionals often depends on the particular vision that they have of a domain according to their background and working habits. Ontology development in Geology thus necessitates a drastic elucidation of the concepts and vocabulary used by geologists. This article intends to contribute to solving these difficulties by proposing a core ontology named GeoCore Ontology resting on the BFO top ontology, specially designed for describing scientific fields. GeoCore Ontology contains well-founded definitions of a limited set of general concepts within the Geology field that are currently considered by all geologists whatever their skill. It allows modelers to separately consider a geological object, the substance that constitutes it, the boundaries that limit it and the internal arrangement of the matter inside it. The core ontology also allows the description of the existentially dependent qualities attached to a geological object and the geological process that generated it in a particular geological age. This small set of formally defined and described concepts combined with concepts from BFO provides a backbone for deriving by subsumption more specialized geological concepts and also constitutes a baseline for integrating different existent domain ontologies within the Geology domain. The GeoCore ontology and the methodology that we used for building it, provide solutions for unveiling major misunderstanding regarding the concepts that are commonly used for formulating geological interpretations. This will facilitate the communication of this information to external Geology users and its integration in domain applications.}
}
@article{JUNG2022103785,
title = {Logical separability of labeled data examples under ontologies},
journal = {Artificial Intelligence},
volume = {313},
pages = {103785},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103785},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001254},
author = {Jean Christoph Jung and Carsten Lutz and Hadrien Pulcini and Frank Wolter},
keywords = {Logical separability, Decidable fragments of first-order logic, Description logic, Learning from examples, Complexity, Ontologies},
abstract = {Finding a logical formula that separates positive and negative examples given in the form of labeled data items is fundamental in applications such as concept learning, reverse engineering of database queries, generating referring expressions, and entity comparison in knowledge graphs. In this paper, we investigate the existence of a separating formula for data in the presence of an ontology. Both for the ontology language and the separation language, we concentrate on first-order logic and the following important fragments thereof: the description logic ALCI, the guarded fragment, the two-variable fragment, and the guarded negation fragment. For separation, we also consider (unions of) conjunctive queries. We consider several forms of separability that differ in the treatment of negative examples and in whether or not they admit the use of additional helper symbols to achieve separation. Our main results are model-theoretic characterizations of (all variants of) separability, the comparison of the separating power of different languages, and the investigation of the computational complexity of deciding separability.}
}
@article{MAKSIMOV2021540,
title = {Knowledge ontology system},
journal = {Procedia Computer Science},
volume = {190},
pages = {540-545},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.063},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013107},
author = {Nikolay Maksimov and Alexander Lebedev},
keywords = {Ontology, knoweledge, ontology development.},
abstract = {The paper considers a purpose and features of ontologies use in describing of subject area, both from a theoretical and an applied point of view. Criteria for identifying ontologies types are indicated. Based on cognition schematism principles, a knowledge representation ontologies system has been developed, that combines language, forms of knowledge representation and process schemes. It is shown that it is exactly such a system of ontologies makes it possible for the practical use of ontologies in computing environments.}
}
@article{PASKALEVA2021103689,
title = {Leveraging integration facades for model-based tool interoperability},
journal = {Automation in Construction},
volume = {128},
pages = {103689},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103689},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521001400},
author = {Galina Paskaleva and Alexandra Mazak-Huemer and Manuel Wimmer and Thomas Bednar},
keywords = {MDE, Data exchange, Big Open BIM, Semantic integration, Pragmatic integration, Heterogeneity},
abstract = {Data exchange and management methods are of paramount importance in areas as complex as the Architecture, Engineering and Construction industries and Facility Management. For example, Big Open BIM requires seamless information flow among an arbitrary number of applications. The backbone of such information flow is a robust integration, whose tasks include overcoming technological as well as semantic and pragmatic gaps and conflicts both within and between data models. In this work, we introduce a method for integrating the pragmatics at design-time and the semantics of independent applications at run-time into so-called “integration facades”. We utilize Model-driven Engineering for the automatic discovery of functionalities and data models, and for finding a user-guided consensus. We present a case study involving the domains of architecture, building physics and structural engineering for evaluating our approach in object-oriented as well as data-oriented programming environments. The results produce, for each scenario, a single integration facade that acts as a single source of truth in the data exchange process.}
}
@article{STEVENS2019100469,
title = {Measuring expert performance at manually classifying domain entities under upper ontology classes},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100469},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S157082681830043X},
author = {Robert Stevens and Phillip Lord and James Malone and Nicolas Matentzoglu},
keywords = {OWL, Ontologies, Upper ontologies, Ontology engineering, Empirical study},
abstract = {Background:
Classifying entities in domain ontologies under upper ontology classes is a recommended task in ontology engineering to facilitate semantic interoperability and modelling consistency. Integrating upper ontologies this way is difficult and, despite emerging automated methods, remains a largely manual task.
Problem:
Little is known about how well experts perform at upper ontology integration. To develop methodological and tool support, we first need to understand how well experts do this task. We designed a study to measure the performance of human experts at manually classifying classes in a general knowledge domain ontology with entities in the Basic Formal Ontology (BFO), an upper ontology used widely in the biomedical domain.
Method:
We recruited 8 BFO experts and asked them to classify 46 commonly known entities from the domain of travel with BFO entities. The tasks were delivered as part of a web survey.
Results:
We find that, even for a well understood general knowledge domain such as travel, the results of the manual classification tasks are highly inconsistent: the mean agreement of the participants with the classification decisions of an expert panel was only 51%, and the inter-rater agreement using Fleiss’ Kappa was merely moderate (0.52). We further follow up on the conjecture that the degree of classification consistency is correlated with the frequency the respective BFO classes are used in practice and find that this is only true to a moderate degree (0.52, Pearson).
Conclusions:
We conclude that manually classifying domain entities under upper ontology classes is indeed very difficult to do correctly. Given the importance of the task and the high degree of inconsistent classifications we encountered, we further conclude that it is necessary to improve the methodological framework surrounding the manual integration of domain and upper ontologies.}
}
@article{BELLINI2025111281,
title = {Certifying entity models, entities and data messages on IoT/WoT platforms via blockchain},
journal = {Computer Networks},
volume = {265},
pages = {111281},
year = {2025},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2025.111281},
url = {https://www.sciencedirect.com/science/article/pii/S138912862500249X},
author = {Pierfrancesco Bellini and Edoardo Branchi and Enrico Collini and Luciano Alessandro Ipsaro Palesi and Paolo Nesi and Gianni Pantaleo},
keywords = {Internet of things, Web of things, Certification of data models, Blockchain, Scalable architecture, Data certification, Time series},
abstract = {The growing complexity and the increasing diffusion of applications exploiting both the Internet of Things (IoT) and the Web of Things (WoT) paradigms have led to the necessity of ensuring robust security and reciprocal trust in the management of huge amount of data from heterogeneous sources. The blockchain technology can satisfy these requirements, ensuring data integrity, univocity, and immutability of the data shared and saved in distributed IoT/IoW systems. In this paper, a deep analysis of requirements for Blockchain based IoT/WoT frameworks has been conducted. The main contributions include (i) formal implications of certifying integrity for data models and for the usage/certification of corresponding instances of entities/ devices and their time series messages in a platform that may present both non-certified and certified models, entities and data messages; (ii) automation of certification and verification processes, which implies mechanisms and formal rules; (iii) flexible certification of data messages enforcing efficiency and sustainability; (iv) support for multi-organization distributed solutions; (v) performance assessment. The solution has been validated with the interconnection to a private/permissioned blockchain developed using Hyperledger Fabric, while enforcing the solution on Snap4City federated network of platforms for applications in the domain of mobility. The research has been performed in the context of CN MOST, which is the national center on Sustainable Mobility in Italy, and within its flagship action OPTIFaaS.}
}
@article{JARVENPAA201887,
title = {Formal Resource and Capability Models supporting Re-use of Manufacturing Resources},
journal = {Procedia Manufacturing},
volume = {19},
pages = {87-94},
year = {2018},
note = {Proceedings of the 6th International Conference in Through-life Engineering Services, University of Bremen, 7th and 8th November 2017},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918300131},
author = {Eeva Järvenpää and Minna Lanz and Niko Siltala},
keywords = {Capability model, Production system representation, Adaptive manufacturing, Ontology},
abstract = {In the field of manufacturing the responsiveness has become a new strategic goal for the enterprises alongside with quality and costs. Efficient responsiveness requires production reconfiguration ranging from layout to equipment. The production system capabilities originate from the tool and equipment level. While a resource is being used, its condition and capability may change. It is crucial to consider the resources’ individual lifecycle, their actual capabilities and condition during the system design and reconfiguration. Thus, the lifecycle perspective in the capability management is of utmost importance. This paper presents the development of the Manufacturing Resource Capability Ontology (MaRCO), focusing on describing the functional capabilities of manufacturing resources. Special emphasis is placed on the lifecycle management aspect of the resource descriptions.}
}
@article{CHOUCHANI2022111082,
title = {Model-based safety engineering for autonomous train map},
journal = {Journal of Systems and Software},
volume = {183},
pages = {111082},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111082},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001795},
author = {Nadia Chouchani and Sana Debbech and Matthieu Perin},
keywords = {Model-based safety engineering, Safety ontology, Model-driven engineering, Safety/assurance case, Railway infrastructure model, Autonomous train},
abstract = {As a part of the digital revolution of railway systems, an autonomous driving train will use a complete and precise map of railway infrastructure to conduct operational actions. Nevertheless, the full autonomy of trains depends on the safety decisions management capacity both on-board and track-side. These decisions must be refined into safety requirements in order to continuously check the consistency between the perceived infrastructure and safety related properties. However, traditionally, the integration of safety analysis requires the intervention of human agent skills. This may be error-prone and in interference with the embedded aspect of the train map. In this paper, we propose a model-based approach to match between safety concepts expressed as an ontology, a derived safety model and a safety-extended railway infrastructure map model for autonomous trains. This approach is validated by railway safety case studies for autonomous train map. The integration of this model-based safety solution from the early stages of the map system design improves the safety decisions management process.}
}
@article{GHORBEL2020101864,
title = {Handling data imperfection—False data inputs in applications for Alzheimer’s patients},
journal = {Data & Knowledge Engineering},
volume = {130},
pages = {101864},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2020.101864},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2030094X},
author = {Fatma Ghorbel and Fayçal Hamdi and Nassira Achich and Elisabeth Metais},
keywords = {Applications for Alzheimer’s patients, Imperfection types, False data inputs, Believability},
abstract = {Handling data imperfection is a crucial issue in many application domains. This is particularly true when handling imperfect data inputs in applications for Alzheimer’s patients. In this paper we first propose a typology of imperfection for data entered by Alzheimer’s patients or their caregivers in the context of these applications (mainly due to the memory discordance caused by the disease). This topology includes nine direct and three indirect imperfection types. The direct ones are deduced from the data inputs e.g. uncertainty and uselessness. The indirect imperfection types are deduced from the direct ones, e.g. the redundancy. We then propose an approach, called DBE_ALZ, that handles false data entry by estimating the believability of each data input. Based on the proposed typology, the falsity of these data is related to five imperfection types: uncertainty, confusion, typing error, wrong knowledge and inconsistency. DBE_ALZ includes a believability model that defines a set of dimensions and sub-dimensions allowing a qualitative estimation of the believability of a given data input. It is estimated based on its reasonableness and the reliability of its author. Compared to related work, the data input reasonableness is measured not only based on common-sense standard, but also based on a set of personalized assertions. The reliability of the patient is estimated based on the progression of the disease and the state of his memory at the moment of entry. However, the reliability of the caregiver is estimated based on his age and his knowledge about the data input’s field. Based on the believability model, we estimate quantitatively the believability of the data input by defining a set of metrics associated to the proposed dimensions and sub-dimensions. The measurement methods rely on probability and fuzzy set theories to reason about uncertain and imprecise knowledge (Bayesian networks and Mamdani fuzzy inference systems). Three languages are supported: English, French and Arabic. Based on the generated believability degrees, a set of decisive actions are proposed to guarantee the quality of the data inputs e.g., inferring or not based on a given data. We illustrate the usefulness of our approach in the context of the Captain Memo memory prosthesis. Finally, we discuss the encouraging results derived from the evaluation step.}
}
@article{ZHU2019479,
title = {Bibliometric analysis of patent infringement retrieval model based on self-organizing map neural network algorithm},
journal = {Library Hi Tech},
volume = {38},
number = {2},
pages = {479-491},
year = {2019},
issn = {0737-8831},
doi = {https://doi.org/10.1108/LHT-12-2018-0201},
url = {https://www.sciencedirect.com/science/article/pii/S0737883119000368},
author = {Dimin Zhu},
keywords = {Databases, Data analysis, Data mining, Social sciences, Data collection techniques, Hypertext},
abstract = {Purpose
The purpose of this paper is to quickly retrieve the same or similar patents in a large patent database.
Design/methodology/approach
The research is carried out through the analysis of the issue of patent examination, the type of patent infringement search and theories related to patent infringement determination and text mining.
Findings
The results show that the model improves the speed of patent search. It can quickly, accurately and comprehensively retrieve the same or equivalent patents as the imported patent claims.
Research limitations/implications
The patent infringement detection mainly focuses on the measurement of patent similarity in the implementation method. It is not mature, and there is still much room for improvement in research.
Practical implications
The model improves the efficiency of patent infringement detection, increases the accuracy of detection and protects the interests of patent stakeholders.
Originality/value
This study has great significance for improving the efficiency of patent examiners.}
}
@article{WATROBSKI20203356,
title = {Ontology learning methods from text - an extensive knowledge-based approach},
journal = {Procedia Computer Science},
volume = {176},
pages = {3356-3368},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319566},
author = {Jarosław Wątróbski},
keywords = {Ontology learning from text, Methods for ontology leaninf from text, Domain ontology learning, Ontology integration},
abstract = {Ontologies are a key element of the Semantic Web. They aim to capture basic knowledge by providing appropriate terms and formal relationships between them, so that they can be used in a machine-processable manner. Accordingly they enable automatic aggregation and practical use as well as unexpected reuse of distributed data sources. Ontologies may come from many different sources, pursuing different goals and quality criteria. However, performed manually ontology construction is a very complex and tedious task, thus many methods proposed offer automatic or semi-automatic way for ontology construction. Many of the methods have their own, specific features. Therefore, this paper proposes an extensive knowledge-based approach covering the domain of ontology learning methods from text. This work aims to collect the knowledge of available approaches for ontology learning and the prominent differences between them, drawing on best practices in ontology engineering. The proposed approach refers to methods and aims to enrich knowledge in the field of ontology learning (OL). In this paper, the author’s ontology contains a set of various types of methods with main techniques used, and the necessary features in the miscellaneous approaches. The proposed an extensive knowledge-based approach uses a reasoning mechanism based on competency questions for individual approaches to determine their ontology learning method profiles. The validation stage has also been carried out. At the same time, it is an extension of the previous study in the form of a repository of knowledge about OL tools. In addition, the combination of both ontologies: tools and methods aim to provide a more efficient OL solution from text.}
}
@article{AZIZ201987,
title = {An ontology-based methodology for hazard identification and causation analysis},
journal = {Process Safety and Environmental Protection},
volume = {123},
pages = {87-98},
year = {2019},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2018.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S095758201831365X},
author = {Abdul Aziz and Salim Ahmed and Faisal I. Khan},
keywords = {Hazard identification, Probabilistic ontology, Web ontology language, Multi-entity Bayesian network, Expert system},
abstract = {This article presents a dynamic hazard identification methodology founded on an ontology-based knowledge modeling framework coupled with probabilistic assessment. The objective is to develop an efficient and effective knowledge-based tool for process industries to screen hazards and conduct rapid risk estimation. The proposed generic model can translate an undesired process event (state of the process) into a graphical model, demonstrating potential pathways to the process event, linking causation to the transition of states. The Semantic web-based Web Ontology Language (OWL) is used to capture knowledge about unwanted process events. The resulting knowledge model is then transformed into Probabilistic-OWL (PR-OWL) based Multi-Entity Bayesian Network (MEBN). Upon queries, the MEBNs produce Situation Specific Bayesian Networks (SSBN) to identify hazards and their pathways along with probabilities. Two open-source software programs, Protégé and UnBBayes, are used. The developed model is validated against 45 industrial accidental events extracted from the U.S. Chemical Safety Board's (CSB) database. The model is further extended to conduct causality analysis.}
}
@article{STORK2019100462,
title = {Semantic annotation of natural history collections},
journal = {Journal of Web Semantics},
volume = {59},
pages = {100462},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300283},
author = {Lise Stork and Andreas Weber and Eulàlia {Gassó Miracle} and Fons Verbeek and Aske Plaat and Jaap {van den Herik} and Katherine Wolstencroft},
keywords = {Linked data, Biodiversity, Natural history collections, Ontologies, Semantic annotation, History of science},
abstract = {Large collections of historical biodiversity expeditions are housed in natural history museums throughout the world. Potentially they can serve as rich sources of data for cultural historical and biodiversity research. However, they exist as only partially catalogued specimen repositories and images of unstructured, non-standardised, hand-written text and drawings. Although many archival collections have been digitised, disclosing their content is challenging. They refer to historical place names and outdated taxonomic classifications and are written in multiple languages. Efforts to transcribe the hand-written text can make the content accessible, but semantically describing and interlinking the content would further facilitate research. We propose a semantic model that serves to structure the named entities in natural history archival collections. In addition, we present an approach for the semantic annotation of these collections whilst documenting their provenance. This approach serves as an initial step for an adaptive learning approach for semi-automated extraction of named entities from natural history archival collections. The applicability of the semantic model and the annotation approach is demonstrated using image scans from a collection of 8, 000 field book pages gathered by the Committee for Natural History of the Netherlands Indies between 1820 and 1850, and evaluated together with domain experts from the field of natural and cultural history.}
}
@article{TUZUN20252575,
title = {Granular and Relational SWOT Analysis: An Ontological Approach},
journal = {Procedia Computer Science},
volume = {253},
pages = {2575-2585},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.317},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925003254},
author = {Alican Tüzün and Shailesh Tripathi and Nadine Bachmann and Ann-Kristin Thienemann and Manuel Brunner and Herbert Jodlbauer},
keywords = {Protege, Ontology, SWOT, SOFT, SWOT Analysis, TOWS, TOWS Matrix},
abstract = {The traditional Strength, Weakness, Opportunity, and Threat (SWOT) analysis, despite its popularity [37], faces a significant challenge in interpreting information through the ”SWOT Matrix (SM)” [20]. The conventional matrix fails to capture the complexity, semantics, and detailed characteristics necessary for comprehensive decision-making. Therefore, this paper introduces an innovative approach to address these limitations by an ontology SWOTONT [45] representing the domain of SWOT analysis. SWOTONT is developed through a literature review and a bottom-up ontology development approach [23], providing context to the SWOT attributes by introducing finer subcategories and mapping their interrelationships. By developing a structured semantic approach, authors aimed to enable more precise knowledge extraction and support advanced strategic analysis. Future work will focus on empirical validation through case studies and domain expert feedback, integrating an upper ontology and exploring additional applications and integrations of the model.}
}
@article{BERGES2025100863,
title = {Two ontology design patterns in the domain of collections},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100863},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100863},
url = {https://www.sciencedirect.com/science/article/pii/S1570826825000034},
author = {Idoia Berges and Arantza Illarramendi},
keywords = {Ontology design pattern, Collection},
abstract = {Collections are objects used to arrange, into a single unit, multiple data items that form a natural group. Different types of collections exist, due to different constraints based on whether or not they impose an order on their elements and whether or not they allow repetition of elements. Any of them are easily found in several domains of our everyday life. For instance, a deck of cards, the prime divisors of a number or the teams that compete in a championship can be seen as a collection. Thus, an effective modeling of collections is a recurring issue in information management. In the ontology design field, recurring modeling problems can be addressed by the use of Ontology Design Patterns (ODPs). In the case of collections, ODPs have been proposed for representing sequences, lists, sets and bags. However, none of these patterns are completely adequate for representing collections of ordered elements without repetition. In this paper we present an ODP for representing that notion, which we have named Permutation. Moreover, another ODP named ListOfPermutations is also introduced, which allows to represent how the order of a Permutation varies along time. Because not all constraints required by these ODPs can be represented in OWL 2, SHACL shapes have been used in their definitions.}
}
@article{TOME202081,
title = {FlexRQC: Model for a Flexible Robot-Driven Quality Control Station},
journal = {Procedia Manufacturing},
volume = {51},
pages = {81-87},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920318680},
author = {A. González Tomé and I Irigoien Ceberio and U. Ayala and J.A. Agirre and N. Arana-Arexolaleiba},
keywords = {Robotics, Flexible Quality Inspection, Programming},
abstract = {As the investment on a dedicated quality control stations is not desirable for limited production batches. In general, those systems result in very optimised systems and the lack of flexibility since they are designed for an ad-hoc production. To provide a solution for those cases, a new model to design a flexible quality inspection system is proposed. This paper introduces FlexRQC (Flexible Robotic Quality Control) a model for characterising flexible robot-driven quality control stations. FlexRQC is divided into two domains: The Quality Control Station Domain (QCSD) and the Model Under Inspection Domains (MUID). FlexRQC takes advantage of 3D CAD systems to get spacial information on the quality control station and the quality requirement. The flexibility of the model has been successfully tested in two quality control station setups and various solid rigid objects.}
}
@article{WANG2022641,
title = {An ontology-based product usage context modeling method for smart customization},
journal = {Procedia CIRP},
volume = {109},
pages = {641-646},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.307},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122007569},
author = {Xingzhi Wang and Ang Liu and Sami Kara},
keywords = {Design ontology, Product usage context, Smart customization},
abstract = {Product usage context (PUC) identification is an effective approach to approximate the complex driver behind heterogeneous customer preference. With the sweeping trend of data-driven smart customization, a large volume of product usage data has allowed designers to understand contextual customer needs (CNs) and enable them to offer highly customized products and services in time. However, as the PUC ontology is not clearly defined, most of the existing PUC models are incomplete, ambiguous and imprecise. Inappropriate use of those models will result in failing to extract knowledge from data. In this paper, an ontology-based context modeling method is proposed, with the aim to help designers understand PUC in a comprehensive manner. A case study of robot vacuum cleaner (RVC) is used as an illustrative example. It is concluded that the proposed method can enable designers quickly establish a well-defined PUC model to support smart customization.}
}
@article{AMATO2018754,
title = {Improving security in cloud by formal modeling of IaaS resources},
journal = {Future Generation Computer Systems},
volume = {87},
pages = {754-764},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17305964},
author = {Flora Amato and Francesco Moscato and Vincenzo Moscato and Francesco Colace},
keywords = {Cloud services, Verification, Big-Data, Security},
abstract = {Nowadays, it is a matter of fact that Cloud is a “must” for all complex services requiring great amount of resources. Big-Data Services are a striking example: they actually perform many kind of analysis (like analytics) on very big repositories. Many File Systems and middleware exist for efficient distribution and management of data and they usually use Cloud Resources. Anyway Several problems arose about Security of data: Virtualization is the base of Cloud resources and, even if we consider data storage as virtually separated elements, security issues exist if privilege escalation allows for gaining control on any data on physical hosts. In this paper we show how it is possible to cope Model Driven Engineering techniques to security analysis and monitoring of Cloud infrastructures. For reducing overhead, we provide a formal profile of hosts thermal behaviors. Depending on services input workloads, we detect and forecast malicious actions by comparisons with real thermal data.}
}
@article{GARCIASILVA2019550,
title = {Enabling FAIR research in Earth Science through research objects},
journal = {Future Generation Computer Systems},
volume = {98},
pages = {550-564},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.03.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18314638},
author = {Andres Garcia-Silva and Jose Manuel Gomez-Perez and Raul Palma and Marcin Krystek and Simone Mantovani and Federica Foglini and Valentina Grande and Francesco {De Leo} and Stefano Salvi and Elisa Trasatti and Vito Romaniello and Mirko Albani and Cristiano Silvagni and Rosemarie Leone and Fulvio Marelli and Sergio Albani and Michele Lazzarini and Hazel J. Napier and Helen M. Glaves and Timothy Aldridge and Charles Meertens and Fran Boler and Henry W. Loescher and Christine Laney and Melissa A. Genazzio and Daniel Crawl and Ilkay Altintas},
keywords = {FAIR principles, Research objects, Research infrastructure, Semantic technologies, Earth Science},
abstract = {Data-intensive science communities are progressively adopting FAIR practices that enhance the visibility of scientific breakthroughs and enable reuse. At the core of this movement, research objects contain and describe scientific information and resources in a way compliant with the FAIR principles and sustain the development of key infrastructure and tools. This paper provides an account of the challenges, experiences and solutions involved in the adoption of FAIR around research objects over several Earth Science disciplines. During this journey, our work has been comprehensive, with outcomes including: an extended research object model adapted to the needs of earth scientists; the provisioning of digital object identifiers (DOI) to enable persistent identification and to give due credit to authors; the generation of content-based, semantically rich, research object metadata through natural language processing, enhancing visibility and reuse through recommendation systems and third-party search engines; and various types of checklists that provide a compact representation of research object quality as a key enabler of scientific reuse. All these results have been integrated in ROHub, a platform that provides research object management functionality to a wealth of applications and interfaces across different scientific communities. To monitor and quantify the community uptake of research objects, we have defined indicators and obtained measures via ROHub that are also discussed herein.}
}
@article{ZALAMEAPATINO2018162,
title = {Merging and expanding existing ontologies to cover the Built Cultural Heritage domain},
journal = {Journal of Cultural Heritage Management and Sustainable Development},
volume = {8},
number = {2},
pages = {162-178},
year = {2018},
issn = {2044-1266},
doi = {https://doi.org/10.1108/JCHMSD-05-2017-0028},
url = {https://www.sciencedirect.com/science/article/pii/S2044126618000172},
author = {Olga Piedad {Zalamea Patino} and Jos {Van Orshoven} and Thérèse Steenberghen},
keywords = {CIDOC-CRM, BCH-ontology, CityGML, Mondis},
abstract = {Purpose
The purpose of this paper is to present the development of an ontological model consisting of terms and relationships between these terms, creating a conceptual information model for the Built Cultural Heritage (BCH) domain, more specifically for preventive conservation.
Design/methodology/approach
The On-To-Knowledge methodology was applied in the ontology development process. Terms related to preventive conservation were identified by means of a taxonomy which was used later to identify related existing ontologies. Three ontologies were identified and merged, i.e. Geneva City Geographic Markup Language (Geneva CityGML), Monument Damage ontology (Mondis) and CIDOC Conceptual Reference Model (CIDOC-CRM). Additional classes and properties were defined as to provide a complete semantic framework for management of BCH.
Findings
A BCH-ontology for preventive conservation was created. It consists of 143 classes from which 38 originate from the Mondis ontology, 38 from Geneva CityGML, 37 from CIDOC-CRM and 30 were newly created. The ontology was applied in a use case related to the New cathedral in the city of Cuenca, Ecuador. Advantages over other type of systems and for the BCH-domain were discussed based on this example.
Research limitations/implications
The proposed ontology is in a testing stage through which a number of its aspects are being verified.
Originality/value
This ontological model is the first one to focus on the preventive conservation of BCH.}
}
@article{POURJAFARIAN2025930,
title = {An ODP-based Ontology for the Digital Product Passport},
journal = {Procedia CIRP},
volume = {135},
pages = {930-935},
year = {2025},
note = {32nd CIRP Conference on Life Cycle Engineering (LCE2025)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.12.125},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125003750},
author = {Monireh Pourjafarian and Christiane Plociennik and Simon Bergweiler and Nastaran Moarefvand and Jonas Brozeit and Mahdi Rezapour and Martin Ruskowski},
keywords = {Circular Economy, Digital Product Passport, Ontology, Ontology Design Patterns, Asset Administration Shell},
abstract = {The challenges posed by climate change can only be met by changing the economic mindset to one that focuses on the idea of a circular economy (CE). Digitalization, data collection and data storage play a crucial role here: Product-related data should be collected in a consistent manner throughout the entire life cycle of the product and stored in a Digital Product Passport (DPP). The DPP should give all stakeholders of the CE access to the necessary data. Overarching modelling is required to ensure that a DPP can be used as a structure across application domains in an interoperable way. Ontologies can act as an interlingua between domains, incorporating the required domain knowledge and the full range of requirements for the DPP. Following a modular approach based on Ontology Design Patterns, this paper develops a DPP ontology with a focus on the R-strategies within the CE. Furthermore, the paper builds a bridge to the standardized approach of Industry 4.0, the modelling and storage of structured domain knowledge in Asset Administration Shells (AAS). Data can be seamlessly integrated and used for decision-making in each product life cycle phase. In addition, the reuse of existing concepts defined by others is demonstrated. The developed ontology is then evaluated on a CE use case.}
}
@article{AYADI2019572,
title = {Ontology population with deep learning-based NLP: a case study on the Biomolecular Network Ontology},
journal = {Procedia Computer Science},
volume = {159},
pages = {572-581},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.212},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919313961},
author = {Ali Ayadi and Ahmed Samet and François de Bertrand {de Beuvron} and Cecilia Zanni-Merk},
keywords = {Ontology population, Knowledge acquisition, Natural language processing, Deep learning, Biomolecular Network Ontology},
abstract = {As a scientific discipline, systems biology aims to build models of biological systems and processes through the computer analysis of a large amount of experimental data describing the behaviour of whole cells. It is within this context that we already developed the Biomolecular Network Ontology especially for the semantic understanding of the behaviour of complex biomolecular networks and their transittability. However, the challenge now is how to automatically populate it from a variety of biological documents. To this end, the target of this paper is to propose a new approach to automatically populate the Biomolecular Network Ontology and take advantage of the vast amount of biological knowledge expressed in heterogeneous unstructured data about complex biomolecular networks. Indeed, we have recently observed the emergence of deep learning techniques that provide significant and rapid progress in several domains, particularly in the process of deriving high-quality information from text. Despite its significant progress in recent years, deep learning is still not commonly used to populate ontologies. In this paper, we present a deep learning-based NLP ontology population system to populate the Biomolecular Network Ontology. Its originality is to jointly exploit deep learning and natural language processing techniques to identify, extract and classify new instances referring to the BNO ontology’s concepts from textual data. The preliminary results highlight the efficiency of our proposal for ontology population.}
}
@article{PEREZMUNOZ2025103513,
title = {Feasibility of Deep Reinforcement Learning for the real-time attitude control of a satellite system},
journal = {Journal of Systems Architecture},
volume = {167},
pages = {103513},
year = {2025},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2025.103513},
url = {https://www.sciencedirect.com/science/article/pii/S1383762125001857},
author = {Ángel-Grover Pérez-Muñoz and Guillermo López-García and Irene García-Villoria and Alejandro Alonso and Angel Porras-Hermoso and María S. Pérez},
keywords = {Artificial intelligence, Neural network controllers, Safety-critical systems, Embedded systems},
abstract = {Although Machine Learning (ML) is widely used in a variety of interdisciplinary applications, its implementation in safety-critical systems, such as the Attitude Control System (ACS) of a satellite, poses numerous challenges. While previous studies have shown promising results, there is a lack of information on the design and development process for the application of ML in real-time control systems. This paper presents the implementation of a Deep Reinforcement Learning (DRL) model for a magnetic-based ACS of the UPMSat-2 satellite. The primary objective is not only to design, implement, and validate an RL agent, but also to provide some insights and criteria of the decision-making process to achieve an adequate model. The system was trained and validated on a simulation model with positive results. To further validate non-functional requirements, the resulting trained agent was tested on a real-time embedded system according to safety standards. The obtained quantitative metrics and performance results show the ability of the agent to maintain the satellite’s attitude across various operational phases, leveraging its adaptability to dynamic conditions.}
}
@article{CHO2020257,
title = {Ontology for Strategies and Predictive Maintenance models},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {257-264},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301889},
author = {Sangje Cho and Marlène Hildebrand-Ehrhardt and Gokan May and Dimitris Kiritsis},
keywords = {Ontology, Semantic technology, Semantic interoperability, Maintenance, Predictive maintenance, Industry 4.0},
abstract = {As of today, to cope with traditional maintenance policies such as reactive and preventive maintenance, the manufacturing companies need the deployment of adaptive and responsive maintenance strategies. Meanwhile, the advent of Industry 4.0 leads the maintenance paradigm shift facilitated by the efficient monitoring of physical assets and forecasting of the potential risks. As the advanced maintenance policies benefit in terms of cost-efficiency, inventory management and reliability management, most of the manufacturing companies are trying to make their own advanced maintenance strategies and to elaborate on the development of an innovative platform for it. However, since advanced enabling technologies collect a huge amount of data from different data sources such as machine, component, document, process and so on, data federation should necessarily be achieved for further discussion, but manufacturing companies are immature to address this issue. H2020 EU project Z-BRE4K, i.e., Strategies and predictive maintenance models wrapped around physical systems for zero-unexpected-breakdowns and increased operating life of factories, deploys semantic technologies to address this issue. This paper deals with the debate on how to efficiently federate various data formats with the support of the semantic technologies in the context of maintenance. In addition, it proposes a maintenance ontology validated and implemented with an actor from European industry.}
}
@article{KANTARELIS2023100754,
title = {Functional harmony ontology: Musical harmony analysis with Description Logics},
journal = {Journal of Web Semantics},
volume = {75},
pages = {100754},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100754},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000385},
author = {Spyridon Kantarelis and Edmund Dervakos and Natalia Kotsani and Giorgos Stamou},
keywords = {Description Logics, Ontology engineering, OWL, Music harmony, Modal harmony, Tonal harmony},
abstract = {Symbolic representations of music are emerging as an important data domain both for the music industry and for computer science research, aiding in the organization of large collections of music and facilitating the development of creative and interactive AI. An aspect of symbolic representations of music, which differentiates them from audio representations, is their suitability to be linked with notions from music theory that have been developed over the centuries. One core such notion is that of functional harmony, which involves analyzing progressions of chords. This paper proposes a description of the theory of functional harmony within the OWL 2 RL profile and experimentally demonstrates its practical use.}
}
@article{POMP2018249,
title = {A Web-based UI to Enable Semantic Modeling for Everyone},
journal = {Procedia Computer Science},
volume = {137},
pages = {249-254},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.024},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316296},
author = {André Pomp and Alexander Paulus and Daniel Klischies and Christian Schwier and Tobias Meisen},
keywords = {Semantic Modeling, Knowledge Graph, User Interface},
abstract = {Since companies generate and store large amounts of data daily in centralized systems such as data lakes, understanding data sets from different sources is becoming an increasingly complex task in dealing with data heterogeneity across domains. One solution for describing semantics of data sources is the use of semantic models based on an available vocabulary. However, creating detailed semantic models can be a challenging task for users who are not familiar with semantic modeling and today’s available tools. To overcome this challenge, we developed an intuitive and user-friendly interface, allowing data owners to define detailed semantic models for their data sources. The design of the user interface is based on an intensive requirement analysis gathered among several peers. It provides an intuitive mapping of semantic concepts to data attributes and the definition of relations between those concepts using drag and drop interaction. The user is given full modeling freedom as the insertion of semantic concepts and relations that are missing in the underlying vocabulary can be done on-demand and does not delay or impair the modeling process. Additionally, the refinement of the original detected data schema is supported with several operations. We built the interface into the semantic data platform ESKAPE, which already uses a flexible knowledge graph as underlying vocabulary and provides a detailed analysis of the data schema.}
}
@article{JARVENPAA20181094,
title = {Product Model ontology and its use in capability-based matchmaking},
journal = {Procedia CIRP},
volume = {72},
pages = {1094-1099},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.211},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118303718},
author = {Eeva Järvenpää and Niko Siltala and Otto Hylli and Minna Lanz},
keywords = {Product Model, Ontology, Information Model, Capability-based matchmaking, Production system design, Reconfiguration},
abstract = {Capability-based matchmaking aims to support rapid design and reconfiguration of modular plug-and-produce type production systems. It relies on formal ontological descriptions of product requirements and resource capabilities. This paper introduces the structure and content of the developed Product Model ontology, and explains its role as a part of the capability matchmaking procedure. A case product is modelled in order to visualize a matchmaking scenario. We expect that such matchmaking will reduce the workload of system designers and reconfiguration planners as it can automatically suggest potential resources for a certain need from large resource catalogues.}
}
@article{ZAVARELLA2024e32479,
title = {Triplétoile: Extraction of knowledge from microblogging text},
journal = {Heliyon},
volume = {10},
number = {12},
pages = {e32479},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e32479},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024085104},
author = {Vanni Zavarella and Sergio Consoli and Diego {Reforgiato Recupero} and Gianni Fenu and Simone Angioni and Davide Buscaldi and Danilo Dessí and Francesco Osborne},
keywords = {Information extraction, Knowledge graphs, Social media analysis, Named entity recognition, Hierarchical clustering, Word embeddings},
abstract = {Numerous methods and pipelines have recently emerged for the automatic extraction of knowledge graphs from documents such as scientific publications and patents. However, adapting these methods to incorporate alternative text sources like micro-blogging posts and news has proven challenging as they struggle to model open-domain entities and relations, typically found in these sources. In this paper, we propose an enhanced information extraction pipeline tailored to the extraction of a knowledge graph comprising open-domain entities from micro-blogging posts on social media platforms. Our pipeline leverages dependency parsing and classifies entity relations in an unsupervised manner through hierarchical clustering over word embeddings. We provide a use case on extracting semantic triples from a corpus of 100 thousand tweets about digital transformation and publicly release the generated knowledge graph. On the same dataset, we conduct two experimental evaluations, showing that the system produces triples with precision over 95% and outperforms similar pipelines of around 5% in terms of precision, while generating a comparatively higher number of triples.}
}
@article{RIBONI2019709,
title = {Sensor-based activity recognition: One picture is worth a thousand words},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {709-722},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19303863},
author = {Daniele Riboni and Marta Murtas},
keywords = {Activity recognition, Intelligent systems, Pervasive computing, Activity models, Unsupervised reasoning},
abstract = {In several domains, including healthcare and home automation, it is important to unobtrusively monitor the activities of daily living (ADLs) carried out by people at home. A popular approach consists in the use of sensors attached to everyday objects to capture user interaction, and ADL models to recognize the current activity based on the temporal sequence of used objects. Often, ADL models are automatically extracted from labeled datasets of activities and sensor events, using supervised learning techniques. Unfortunately, acquiring such datasets in smart homes is expensive and violates users’ privacy. Hence, an alternative solution consists in manually defining ADL models based on common sense, exploiting logic languages such as description logics. However, manual specification of ADL ontologies is cumbersome, and rigid ontological definitions fail to capture the variability of activity execution. In this paper, we introduce a radically new approach enabled by the recent proliferation of tagged visual contents available on the Web. Indeed, thanks to the popularity of social network applications, people increasingly share pictures and videos taken during the execution of every kind of activity. Often, shared contents are tagged with metadata, manually specified by their owners, that concisely describe the depicted activity. Those metadata represent an implicit activity label of the picture or video. Moreover, today’s computer vision tools support accurate extraction of tags describing the situation and the objects that appear in the visual content. By reasoning with those tags and their corresponding activity labels, we can reconstruct accurate models of a comprehensive set of human activities executed in the most disparate situations. This approach overcomes the main shortcomings of existing techniques. Compared to supervised learning methods, it does not require the acquisition of training sets of sensor events and activities. Compared to knowledge-based methods, it does not involve any manual modeling effort, and it captures a comprehensive array of execution modalities. Through extensive experiments with large datasets of real-world ADLs, we show that this approach is practical and effective.}
}
@article{CAO2022103574,
title = {A core reference ontology for steelmaking process knowledge modelling and information management},
journal = {Computers in Industry},
volume = {135},
pages = {103574},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103574},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001810},
author = {Qiushi Cao and Sadeer Beden and Arnold Beckmann},
keywords = {Industry 4.0, Steelmaking, Knowledge graph, Ontology, Ontology-based data access, Condition-based maintenance},
abstract = {Following the trend of Industry 4.0, the business model of steel manufacturing is transforming from a historical inwardly focused supplier/customer relationship to one that embraces the wider end-to-end supply chain and improves productivity more holistically. However, the data and information required for supply chain planning and steelmaking process modelling are normally distributed over scattered sources across organisation boundaries and research communities. This leads to a major problem concerning semantic interoperability. To address this issue, this paper introduces a Common Reference Ontology for Steelmaking (CROS). CROS serves as a shared steelmaking resource and capability model that aims to facilitate knowledge modelling, knowledge sharing and information management. In contrast to most of the existing steelmaking ontologies which merely focus on conceptual modelling, our work pays special attention to the real-world implementation and utilisation aspects of CROS. The functionality and usefulness of CROS is evaluated and tested on a real-world condition-based monitoring and maintenance task for cold rolling mills at Tata Steel in the United Kingdom.}
}
@article{GUTIERREZ2024107854,
title = {KD SENSO-MERGER: An architecture for semantic integration of heterogeneous data},
journal = {Engineering Applications of Artificial Intelligence},
volume = {132},
pages = {107854},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.107854},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624000125},
author = {Yoan Gutiérrez and José I. Abreu Salas and Andrés Montoyo and Rafael Muñoz and Suilan Estévez-Velarde},
keywords = {Heterogeneous data, Knowledge discovery, NERC, Natural language processing, Ontology and knowledge representation, Semantic data integration},
abstract = {This paper presents KD SENSO-MERGER, a novel Knowledge Discovery (KD) architecture that is capable of semantically integrating heterogeneous data from various sources of structured and unstructured data (i.e. geolocations, demographic, socio-economic, user reviews, and comments). This goal drives the main design approach of the architecture. It works by building internal representations that adapt and merge knowledge across multiple domains, ensuring that the knowledge base is continuously updated. To deal with the challenge of integrating heterogeneous data, this proposal puts forward the corresponding solutions: (i) knowledge extraction, addressed via a plugin-based architecture of knowledge sensors; (ii) data integrity, tackled by an architecture designed to deal with uncertain or noisy information; (iii) scalability, this is also supported by the plugin-based architecture as only relevant knowledge to the scenario is integrated by switching-off non-relevant sensors. Also, we minimize the expert knowledge required, which may pose a bottleneck when integrating a fast-paced stream of new sources. As proof of concept, we developed a case study that deploys the architecture to integrate population census and economic data, municipal cartography, and Google Reviews to analyze the socio-economic contexts of educational institutions. The knowledge discovered enables us to answer questions that are not possible through individual sources. Thus, companies or public entities can discover patterns of behavior or relationships that would otherwise not be visible and this would allow extracting valuable information for the decision-making process.}
}
@article{KUKKONEN2022104067,
title = {An ontology to support flow system descriptions from design to operation of buildings},
journal = {Automation in Construction},
volume = {134},
pages = {104067},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104067},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005185},
author = {Ville Kukkonen and Ali Kücükavci and Mikki Seidenschnur and Mads Holten Rasmussen and Kevin Michael Smith and Christian Anker Hviid},
keywords = {Building information modeling, HVAC, Semantic web, Ontology, Linked data},
abstract = {The interoperability of information from design to operations is an acknowledged challenge in the fields of architecture, engineering and construction (AEC). As a potential solution to the interoperability issues, there has been increasing interest in how linked data and semantic web technologies can be used to establish an extendable data model. Semantic web ontologies have been developed for the AEC domain, but an ontology for describing the energy and mass flow between systems and components is missing. This study proposes the Flow Systems Ontology (FSO) for describing the composition of flow systems, and their mass and energy flows. Two example models are expressed using FSO vocabulary. SPARQL Protocol and RDF Query Language (SPARQL) queries are performed to further demonstrate and validate the ontology. The main contribution consists of developing FSO as an ontology complementary to the existing ontologies. Finally, the paper introduces a roadmap for future developments building on FSO.}
}
@article{ARISTA2023270,
title = {An Ontology-based Engineering system to support aircraft manufacturing system design},
journal = {Journal of Manufacturing Systems},
volume = {68},
pages = {270-288},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523000377},
author = {Rebeca Arista and Xiaochen Zheng and Jinzhi Lu and Fernando Mas},
keywords = {Ontology-based Engineering, Decision support, Ontology, Knowledge graph, Aircraft manufacturing system, Knowledge management, Cognitive Digital Twin},
abstract = {During the conceptual design phase of an aircraft manufacturing system, different industrial scenarios need to be evaluated against performance indicators in a collaborative engineering process. Domain experts’ knowledge and the motivations for decision-making is a crucial asset for enterprises which is challenging to be captured and capitalised. Ontology-based Engineering (OBE) systems emerge as a new generation of Knowledge-based Engineering techniques with advancements of ontology engineering methods and computer science technologies. Ontologies enable to capture both explicit and implicit domain knowledge from historical records and domain experts. These Ontology-based Engineering systems can stand highly complex collaborative design processes involving multidisciplinary stakeholders and various digital tools. This paper proposes a tradespace framework with Ontology-based Engineering features included on top of existing Model-Based System Engineering and interoperability capabilities. These additional Ontology-based Engineering features reuse formalised knowledge via knowledge graph technologies and generative algorithms, changing the cognitive process from the designer, to an automatic process which generates design alternatives for the designer. The tradespace framework is demonstrated in a case study to design the aircraft fuselage orbital joint process, helping the designer to take better strategic decisions at conceptual phase and proving to be an advantageous paradigm for the design process.}
}
@article{IQBAL20224767,
title = {Mobile Devices Interface Adaptivity Using Ontologies},
journal = {Computers, Materials and Continua},
volume = {71},
number = {3},
pages = {4767-4784},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.023239},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822006919},
author = {Muhammad Waseem Iqbal and Muhammad Raza Naqvi and Muhammad Adnan Khan and Faheem Khan and T. Whangbo},
keywords = {User context, adaptive interfaces, human computer interaction},
abstract = {Currently, many mobile devices provide various interaction styles and modes which create complexity in the usage of interfaces. The context offers the information base for the development of Adaptive user interface (AUI) frameworks to overcome the heterogeneity. For this purpose, the ontological modeling has been made for specific context and environment. This type of philosophy states to the relationship among elements (e.g., classes, relations, or capacities etc.) with understandable satisfied representation. The context mechanisms can be examined and understood by any machine or computational framework with these formal definitions expressed in Web ontology language (WOL)/Resource description frame work (RDF). The Protégé is used to create taxonomy in which system is framed based on four contexts such as user, device, task and environment. Some competency questions and use-cases are utilized for knowledge obtaining while the information is refined through the instances of concerned parts of context tree. The consistency of the model has been verified through the reasoning software while SPARQL querying ensured the data availability in the models for defined use-cases. The semantic context model is focused to bring in the usage of adaptive environment. This exploration has finished up with a versatile, scalable and semantically verified context learning system. This model can be mapped to individual User interface (UI) display through smart calculations for versatile UIs.}
}
@article{BOUFRIDA20221150,
title = {Rule extraction from scientific texts: Evaluation in the specialty of gynecology},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {4},
pages = {1150-1160},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820303736},
author = {Amina Boufrida and Zizette Boufaida},
keywords = {Text mining, Natural language processing, Knowledge extraction, Rule acquisition, Ontology Web Language (OWL) ontology, Semantic Web Rule Language (SWRL) rules},
abstract = {Due to the considerable increase in freely available data (especially on the Web), extracting relevant information from textual content is a critical challenge. Most of the available data is embedded in unstructured texts and is not linked to formalized knowledge structures such as ontologies or rules. A potential solution to this problem is to acquire such knowledge through natural language processing (NLP) tools and text mining techniques. Prior work has focused on the automatic extraction of ontologies from texts, but the acquired knowledge is generally limited to simple hierarchies of terms. This paper presents a polyvalent framework for acquiring complex relationships from texts and coding these in the form of rules. Our approach begins with existing domain knowledge represented as an OWL ontology, and applies NLP tools and text matching techniques to deduce different atoms, such as classes, properties and literals, to capture deductive knowledge in the form of new rules. For the reason, to enrich the existing domain ontology by these rules, in order to obtain higher relational expressiveness, make reasoning and produce new facts. The approach was tested using medical reports, specifically, in the specialty of gynecology. It reports an F-measure of 95.83% on test our corpus.}
}
@article{RABOUDI2022104007,
title = {The BMS-LM ontology for biomedical data reporting throughout the lifecycle of a research study: From data model to ontology},
journal = {Journal of Biomedical Informatics},
volume = {127},
pages = {104007},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104007},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000235},
author = {Amel Raboudi and Marianne Allanic and Daniel Balvay and Pierre-Yves Hervé and Thomas Viel and Thulaciga Yoganathan and Anais Certain and Jacques Hilbey and Jean Charlet and Alexandre Durupt and Philippe Boutinaud and Benoît Eynard and Bertrand Tavitian},
keywords = {Provenance, Local terminologies, Data sharing, Research Data Management, Data annotation, Heterogeneous data},
abstract = {Biomedical research data reuse and sharing is essential for fostering research progress. To this aim, data producers need to master data management and reporting through standard and rich metadata, as encouraged by open data initiatives such as the FAIR (Findable, Accessible, Interoperable, Reusable) guidelines. This helps data re-users to understand and reuse the shared data with confidence. Therefore, dedicated frameworks are required. The provenance reporting throughout a biomedical study lifecycle has been proposed as a way to increase confidence in data while reusing it. The Biomedical Study - Lifecycle Management (BMS-LM) data model has implemented provenance and lifecycle traceability for several multimodal-imaging techniques but this is not enough for data understanding while reusing it. Actually, in the large scope of biomedical research, a multitude of metadata sources, also called Knowledge Organization Systems (KOSs), are available for data annotation. In addition, data producers uses local terminologies or KOSs, containing vernacular terms for data reporting. The result is a set of heterogeneous KOSs (local and published) with different formats and levels of granularity. To manage the inherent heterogeneity, semantic interoperability is encouraged by the Research Data Management (RDM) community. Ontologies, and more specifically top ontologies such as BFO and DOLCE, make explicit the metadata semantics and enhance semantic interoperability. Based on the BMS-LM data model and the BFO top ontology, the BioMedical Study - Lifecycle Management (BMS-LM) core ontology is proposed together with an associated framework for semantic interoperability between heterogeneous KOSs. It is made of four ontological levels: top/core/domain/local and aims to build bridges between local and published KOSs. In this paper, the conversion of the BMS-LM data model to a core ontology is detailed. The implementation of its semantic interoperability in a specific domain context is explained and illustrated with examples from small animal preclinical research.}
}
@article{OTMANI2018359,
title = {Ontology-based approach to enhance medical web information extraction},
journal = {International Journal of Web Information Systems},
volume = {15},
number = {3},
pages = {359-382},
year = {2018},
issn = {1744-0084},
doi = {https://doi.org/10.1108/IJWIS-03-2018-0017},
url = {https://www.sciencedirect.com/science/article/pii/S1744008418000162},
author = {Nassim Abdeldjallal Otmani and Malik Si-Mohammed and Catherine Comparot and Pierre-Jean Charrel},
keywords = {Web search and information extraction, Metadata and ontologies, Knowledge engineering, Online patient-doctor conversation},
abstract = {Purpose
The purpose of this study is to propose a framework for extracting medical information from the Web using domain ontologies. Patient–Doctor conversations have become prevalent on the Web. For instance, solutions like HealthTap or AskTheDoctors allow patients to ask doctors health-related questions. However, most online health-care consumers still struggle to express their questions efficiently due mainly to the expert/layman language and knowledge discrepancy. Extracting information from these layman descriptions, which typically lack expert terminology, is challenging. This hinders the efficiency of the underlying applications such as information retrieval. Herein, an ontology-driven approach is proposed, which aims at extracting information from such sparse descriptions using a meta-model.
Design/methodology/approach
A meta-model is designed to bridge the gap between the vocabulary of the medical experts and the consumers of the health services. The meta-model is mapped with SNOMED-CT to access the comprehensive medical vocabulary, as well as with WordNet to improve the coverage of layman terms during information extraction. To assess the potential of the approach, an information extraction prototype based on syntactical patterns is implemented.
Findings
The evaluation of the approach on the gold standard corpus defined in Task1 of ShARe CLEF 2013 showed promising results, an F-score of 0.79 for recognizing medical concepts in real-life medical documents.
Originality/value
The originality of the proposed approach lies in the way information is extracted. The context defined through a meta-model proved to be efficient for the task of information extraction, especially from layman descriptions.}
}
@article{MCGRANAGHAN2023100142,
title = {The cultural-social nucleus of an open community: A multi-level community knowledge graph and NASA application},
journal = {Applied Computing and Geosciences},
volume = {20},
pages = {100142},
year = {2023},
issn = {2590-1974},
doi = {https://doi.org/10.1016/j.acags.2023.100142},
url = {https://www.sciencedirect.com/science/article/pii/S2590197423000319},
author = {Ryan M. McGranaghan and Ellie Young and Cameron Powers and Swapnali Yadav and Edlira Vakaj},
keywords = {Knowledge representation, Knowledge graph, Collaboration, Community, Data science, Open science, Inclusivity, Accessibility, Information organization, Heliophysics, Space physics, Collective intelligence},
abstract = {The challenges faced by science, engineering, and society are increasingly complex, requiring broad, cross-disciplinary teams to contribute to collective knowledge, cooperation, and sensemaking efforts. However, existing approaches to collaboration and knowledge sharing are largely manual, inadequate to meet the needs of teams that are not closely connected through personal ties or which lack the time to respond to dynamic requests for contextual information sharing. Nonetheless, in the current remote-first, complexity-driven, time-constrained workplace, such teams are both more common and more necessary. For example, the NASA Center for HelioAnalytics (CfHA) is a growing and cross-disciplinary community that is dedicated to aiding the application of emerging data science techniques and technologies, including AI/ML, to increase the speed, rigor, and depth of space physics scientific discovery. The members of that community possess innumerable skills and competencies and are involved in hundreds of projects, including proposals, committees, papers, presentations, conferences, groups, and missions. Traditional structures for information and knowledge representation do not permit the community to search and discover activities that are ongoing across the Center, nor to understand where skills and knowledge exist. The approaches that do exist are burdensome and result in inefficient use of resources, reinvention of solutions, and missed important connections. The challenge faced by the CfHA is a common one across modern groups and one that must be solved if we are to respond to the grand challenges that face our society, such as complex scientific phenomena, global pandemics and climate change. We present a solution to the problem: a community knowledge graph (KG) that aids an organization to better understand the resources (people, capabilities, affiliations, assets, content, data, models) available across its membership base, and thus supports a more cohesive community and more capable teams, enables robust and responsible application of new technologies, and provides the foundation for all members of the community to co-evolve the shared information space. We call this the Community Action and Understanding via Semantic Enrichment (CAUSE) ontology. We demonstrate the efficacy of KGs that can be instantiated from the ontology together with data from a given community (shown here for the CfHA). Finally, we discuss the implications, including the importance of the community KG for open science.}
}
@article{NUNDLOLL2021100064,
title = {A semantic approach to enable data integration for the domain of flood risk management},
journal = {Environmental Challenges},
volume = {3},
pages = {100064},
year = {2021},
issn = {2667-0100},
doi = {https://doi.org/10.1016/j.envc.2021.100064},
url = {https://www.sciencedirect.com/science/article/pii/S2667010021000433},
author = {Vatsala Nundloll and Rob Lamb and Barry Hankin and Gordon Blair},
keywords = {Ontologies, Structured data, Unstructured data, Semantic integration, Natural language processing, Flood risk management},
abstract = {With so many things around us continuously producing and processing data, be it mobile phones, or sensors attached to devices, or satellites sitting thousands of kilometres above our heads, data is becoming increasingly heterogeneous. Scientists are inevitably faced with data challenges, coined as the 4 V’s of data - volume, variety, velocity and veracity. In this paper, we address the issue of data variety. The task of integrating and querying such heterogeneous data is further compounded if the data is in unstructured form. We hence propose an approach using Semantic Web and Natural Language Processing techniques to resolve the heterogeneity arising in data formats, bring together structured and unstructured data and provide a unified data model to query from disparate data sets.}
}
@article{BARZEGAR2018319,
title = {Classification of composite semantic relations by a distributional-relational model},
journal = {Data & Knowledge Engineering},
volume = {117},
pages = {319-335},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X1730561X},
author = {Siamak Barzegar and Brian Davis and Siegfried Handschuh and Andre Freitas},
keywords = {Semantic relation, Distributional semantic, Deep learning, Classification},
abstract = {Different semantic interpretation tasks such as text entailment and question answering require the classification of semantic relations between terms or entities within text. However, in most cases, it is not possible to assign a direct semantic relation between entities/terms. This paper proposes an approach for composite semantic relation classification using one or more relations between entities/term mentions, extending the traditional semantic relation classification task. The proposed model is different from existing approaches which typically use machine learning models built over lexical and distributional word vector features in that is uses a combination of a large commonsense knowledge base of binary relations, a distributional navigational algorithm and sequence classification to provide a solution for the composite semantic relation classification problem. The proposed approach outperformed existing baselines with regard to F1-score, Accuracy, Precision and Recall.}
}
@article{BELKADI2018428,
title = {Towards an Unified Additive Manufacturing Product-Process Model for Digital Chain Management Purpose},
journal = {Procedia CIRP},
volume = {70},
pages = {428-433},
year = {2018},
note = {28th CIRP Design Conference 2018, 23-25 May 2018, Nantes, France},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.146},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118303032},
author = {Farouk Belkadi and Laura Martinez Vidal and Alain Bernard and Eujin Pei and Emilio M. Sanfilippo},
keywords = {Type your keywords here, separated by semicolons},
abstract = {Additive Manufacturing (AM) is a real example of emerging technologies that can influence the whole product development process. During the AM process, large amounts of data are created, modified stored and retrieved. The management of large amounts of data, as well as the complexity of the relationships between stakeholders are amongst the major challenges. Design activities should be well-integrated with the production process to ensure the consistency of the whole AM value chain, which begins from the conception to the production and post-treatment of the product. This paper discusses the main characteristics of the AM process. The Business Process Modelling Notation (BMPN) is used to describe the entire AM value chain and the connection between design and manufacturing processes. This is a preliminary step towards the definition of complete model dedicated to the representation of AM related knowledge. Semantic interoperability and the monitoring of the whole digital chain involved in the AM value chain are the most important applications of the proposed modeling framework.}
}
@article{SHAKED2025103954,
title = {BridgeSec: Facilitating effective communication between security engineering and systems engineering},
journal = {Journal of Information Security and Applications},
volume = {89},
pages = {103954},
year = {2025},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2024.103954},
url = {https://www.sciencedirect.com/science/article/pii/S2214212624002564},
author = {Avi Shaked and Nan Messe},
keywords = {Security by design, Model-driven engineering, System development, Vulnerability management, Threat modelling, Security engineering, Systems engineering},
abstract = {We increasingly rely on systems to perform reliably and securely. Therefore, it is imperative that security aspects are properly considered when designing and maintaining systems. However, achieving the security by design ideal is challenging. Security information is typically unstructured, dispersed, hard to communicate, and its assessment is somewhat subjective and tacit. Additionally, the inclusion of security information within design requires integrating the efforts of two knowledge-intensive disciplines: security engineering and systems engineering. In this paper, we introduce BridgeSec, a novel conceptual information-exchange interface to systemise the communication of security information between these two disciplines. The main contribution of BridgeSec lies in its explicit identification of concepts related to vulnerability management, which allows systems engineering and security engineering teams to codify pertinent information. The disciplines involved in the system design can thus coordinate policies, implementations and, ultimately, the security posture. Furthermore, based on the newly unveiled interface, an automated reasoning mechanism is specified. This mechanism allows to reason about the vulnerability posture of systems in a scalable and systematic way. First, we describe and formalise the information-exchange interface BridgeSecand how it can be used to reason about the security of systems designs. Next, we present an open-source prototype – integrated into a threat modelling tool – which rigorously implements the interface and the reasoning mechanism. Finally, we detail two diverse and prominent applications of the interface for communicating security aspects of systems designs. These applications show how BridgeSec can rigorously support the design of systems’ security in two representative scenarios: in coordinating security features and policy during design, and in coordinating mitigation to disclosed implementation vulnerabilities.}
}
@article{KATTI2020197,
title = {Bidirectional Transformation of MES Source Code and Ontologies},
journal = {Procedia Manufacturing},
volume = {42},
pages = {197-204},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.070},
url = {https://www.sciencedirect.com/science/article/pii/S235197892030634X},
author = {Badarinath Katti and Christiane Plociennik and Martin Ruskowski and Michael Schweitzer},
keywords = {Cloud based MES, Production Automation, Knowledge Representation, OPC-UA, Ontology, OWL, SWRL, SQWRL, SPARQL, Edge Computing},
abstract = {Future production environments must be flexible and reconfigurable. To achieve this, the devices and services to fulfill the different steps of a production order (PO) should not be selected in the manufacturing execution system (MES), but in an edge component close to the shop floor. To enable this, abstract services in the PO and concrete services provided by the field devices on the shop floor need to refer to a production ontology. The creation of this ontology is a challenge of its own. This research proposes a pragmatic automation of an encoding of a primary and light weight production ontology based on the source code of MES. The transformation procedure of source code to resource, product and generic concepts of the manufacturing plant ontology is described. To this end, the knowledge of OPC UA collaborations are also exploited during the creation of resource ontologies. Due to a fundamental difference between source code implementation (imperative paradigm) and ontology representation (declarative paradigm), the problem of information loss is inevitable. This problem is overcome by formulation of production and business rules that encapsulate the logic of the MES. The foundation of ontology is exploited to formulate these rulesets using OWL based constructs and OWL based rule languages such as Semantic Web Rule Language (SWRL), Semantic Query-Enhanced Web Rule Language (SQWRL) and SPARQL Protocol and RDF Query Language (SPARQL) based on feasibility and requirements of specific rules. Further, these rulesets are either run on the automatically generated ontology at design time with an intention to enrich the knowledge base, or production runtime to validate the pre-defined business rules between the production steps. The generated ontology also acts as basis for automatically generating the OWL-S ontologies for the OPC UA application methods for the purpose of dynamic manufacturing service discovery and orchestration. The generated ontology and an abstract PO hooked with formulated rules are cached to the shop-floor network for consequent production control to enable smart edge production. An implementation is conducted on an industrial use-case demonstrator to evaluate the applicability of the proposed approach.}
}
@article{SCHAEFFER20232106,
title = {OLAF: An Ontology Learning Applied Framework},
journal = {Procedia Computer Science},
volume = {225},
pages = {2106-2115},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.201},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923013595},
author = {Marion Schaeffer and Matthias Sesboüé and Jean-Philippe Kotowicz and Nicolas Delestre and Cecilia Zanni-Merk},
keywords = {ontology learning, ontology, knowledge acquisition, ontology-based system, framework, automation, NLP},
abstract = {Since the beginning of the century, research on ontology learning has gained popularity. Automatically extracting and structuring knowledge relevant to a domain of interest from unstructured textual data is a major scientific challenge. After studying the main existing methods, such as Text2Onto, we propose a new approach with a modular ontology learning framework focusing on automatically extracting knowledge from raw text sources. We consider tasks from data pre-processing to axiom extraction. Whereas previous contributions considered ontology learning systems as tools to help the domain expert craft a reusable ontology, we developed the proposed framework with full automation in mind to build a minimum viable ontology targeted at an application. Ontology Learning Applied Framework (OLAF) has been generically designed to build specific ontologies whatever the application domain, use case and text data. We implement an initial version and test the framework on an ontology-based system, a search engine for technical products.}
}
@article{GARCIA2019100487,
title = {Grounding knowledge acquisition with ontology explanation:A case study},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100487},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300672},
author = {Ana Cristina B. Garcia and Adriana S. Vivacqua},
keywords = {Explanation, Knowledge acquisition, Ontology validity, Ontology engineering},
abstract = {Knowledge validation is still a challenge when constructing knowledge-based systems. It is one of the major reasons for user rejection and disagreement between project participants. Systematic and periodic reviews of the domain ontology, with a formal agreement of the whole development team (including the experts) are a recommended good practice. Nevertheless, these reviews do not guarantee system success. This paper presents a case study of the construction process of a knowledge-based system. The process involved a group of experts with varied work experience. A great deal of negotiation happened during knowledge acquisition meetings, which took place during a 6-month period. After each meeting, changes in the ontology were verified through a web-based questionnaire, from which either consensual agreement was reached (and changes implemented) or the need for a new meeting was ascertained. An explanatory review at the beginning of each meeting further solidified the understanding of all participants. This cyclic process led to a final version of the ontology, ratified by all participants. This model supports diagnosis and prediction of failures in mechanical drilling rigs in oil exploration sites. Unexpectedly, during system trials, experts disagreed with results, which raised questions about the validity of the domain ontology. The system’s explanation module provided a cornerstone for a reflective process that helped identify inconsistencies and corrections needed. These reflections led to adjustments to the ontology, and a reflection about previous decisions and element definitions. Explanations, derived from the ontology and instantiated using real scenarios, shed light on knowledge gaps and semantic inconsistencies of the domain model. In this paper we have three main goals: (1) to present our ontology construction process; (2) to highlight a particular situation where results were inadequate; and (3) to show how the explanation system helped experts and knowledge engineers identify gaps. We also present lessons learned from the whole process, that may apply in other situations.}
}
@article{LI2024102747,
title = {Comprehensive digital twin for infrastructure: A novel ontology and graph-based modelling paradigm},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102747},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102747},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003951},
author = {Tao Li and Yi Rui and Hehua Zhu and Linhai Lu and Xiaojun Li},
keywords = {Digital twin, Infrastructure, Modelling, Ontology, Property graph model, Knowledge graph, BIM},
abstract = {The inherent uncertainty and complexity of infrastructure systems present significant challenges for precise management. By providing real-time status tracking and decision support capabilities, Digital Twin (DT) has shown significant promise in enhancing decision-making and improving efficiency and quality of management. To create a knowledge-rich and scalable DT capable of tracing the distinctive features and performance of infrastructure, this article develops a comprehensive IDT (Infrastructure DT) modelling paradigm. The ontology-based paradigm comprises five elements: scenario, virtual model, physical entity, relation, and component, supporting interoperability and multidisciplinary collaboration. By employing a graph-based modelling approach, the paradigm integrates various types of discrete data to link infrastructure entities into a cohesive system, thereby effectively capturing their dynamic characteristics. A case study comprising supported foundation excavation, subway tunnels, and pipe networks demonstrates the IDT’s capability to facilitate coordination across multidisciplinary, multi-scale, and multi-stage contexts, thereby establishing a transparent, effective, and secure pattern of infrastructure management.}
}
@article{CASTIGLIONE2025104150,
title = {SecOnto: Ontological Representation of Security Directives},
journal = {Computers & Security},
volume = {148},
pages = {104150},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104150},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004553},
author = {Gianpietro Castiglione and Giampaolo Bella and Daniele Francesco Santamaria},
keywords = {Semantic web, Reasoning, NIS 2},
abstract = {The current digital landscape demands robust security requirements and, for doing so, the institutions enact complex security directives to protect the citizens and the infrastructures, particularly in the European Union. These directives aim to safeguard data and harmonise security across the European region, and institutions must navigate this evolving legal landscape in order to implement and keep up-to-date the prescribed security measures. However, understanding and implementing these directives towards full compliance can be difficult and expensive. Ontological representation can be employed to represent and operationalise such security directives, ultimately contributing to the effectiveness and efficiency of the compliance process. Ontologies in fact promote a structured approach to represent knowledge, making the applicable directives more simply understandable by humans and more readily processable by machines. This article introduces SecOnto, a novel methodology for representing security directives as ontologies. SecOnto breaks down the process of transforming the juridical language of modern security directives into full-fledged ontologies by means of five semi-automated steps: Preprocessing, Interpretation, Structuring, Representation and Verification. Each step is described and validated by means of operational examples based upon Directive 2022/2555 of the European Parliament and of the Council of the European Union on security of network and information systems, better known as NIS 2.}
}
@article{DAPICA20211208,
title = {Towards a Semantic Knowledge Base for Competency-Based Training of Airline Pilots},
journal = {Procedia Computer Science},
volume = {192},
pages = {1208-1217},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.124},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921016136},
author = {Rubén Dapica and Federico Peinado},
keywords = {Pilot Training, Aviation Data Management, Data Interoperability, Ontology Engineering, Semantic Web},
abstract = {The acquisition and maintenance of non-technical skills by the pilots are fundamental factors for the prevention of aviation accidents. The aviation authorities are promoting that air crew training be carried out through simulator sessions using scenarios specifically designed to develop and assess the global performance of pilots in such skills. When designing custom flight training scenarios, choosing the correct events and conditions from the myriad of possible combinations with respect to their potential utility in training specific competencies is a costly task that depends entirely on highly specialized expert knowledge. In this paper, we present EBTOnto, an OWL DL ontology that allows to formalize this knowledge and other useful data from real cases, laying the foundations for a semantic knowledge base of scenarios for airline pilots training. Previous advances in this matter and possible applications of this system are reviewed. EBTOnto is built on top of a source validated by experts, the Evidence-Based Training Implementation Guide by the International Air Transport Association, and then checked using an automatic reasoner and a database of 37,568 aviation safety incidents, extracted from the widely regarded Aviation Safety Reporting System by the U.S. National Aeronautics and Space Administration. The results suggest that it is possible to classify real aviation scenarios in terms of non-technical competencies and filter useful incident reports for design and enrichment of these training scenarios. EBTOnto opens up new possibilities for interoperability between incident databases and training organizations, and smoothes the path to represent, share and generate custom simulation training scenarios for pilots based on real data.}
}
@article{OSTBERG202219,
title = {Domain Models and Data Modeling as Drivers for Data Management: The ASSISTANT Data Fabric Approach},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {19-24},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.362},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322016287},
author = {Per-Olov Östberg and Eduardo Vyhmeister and Gabriel G. Castañé and Bart Meyers and Johan {Van Noten}},
keywords = {Domain Models, Knowledge Graph, Data Modeling, Data Fabric, Data Base, Data Lake, AI, adaptive manufacturing},
abstract = {To develop AI-based models capable of governing or providing decision support to complex manufacturing environments, abstractions and mechanisms for unified management of data storage and processing capabilities are needed. Specifically, as such models tend to include and rely on detailed representations of systems, components, and tools with complex interactions, mechanisms for simplifying, integrating, and scaling management capabilities in the presence of complex data requirements (e.g., high volume, velocity, and diversity of data) are of particular interest. A data fabric is a system that provides a unified architecture for management and provisioning of data. In this work we present the background, design requirements, and high-level outline of the ASSISTANT data fabric - a flexible data management tool designed for use in adaptive manufacturing contexts. The paper outlines the implementation of the system with specific focus on the use of domain models and the data modeling approach used, as well as provides a generic use case structure reusable in many industrial contexts.}
}
@article{GUO2024104124,
title = {An ontology-based method for knowledge reuse in the design for maintenance of complex products},
journal = {Computers in Industry},
volume = {161},
pages = {104124},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104124},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524000526},
author = {Ziyue Guo and Dong Zhou and Dequan Yu and Qidi Zhou and Hongduo Wu and Aimin Hao},
keywords = {Knowledge management, Ontology development, Product design, Industrial maintenance},
abstract = {In the context of the Fourth Industrial Revolution, a large amount of heterogeneous data and information is generated during the lifecycle of complex products, which poses a considerable challenge for manufacturers and effective knowledge integration. It has been challenging for traditional experience-based design methods to meet the diverse needs of customers and maintain competitiveness in fierce global markets. Capturing, formalizing and reusing multidisciplinary knowledge that is scattered among different departments and stages to help make effective decisions has been a crucial way for digital enterprises to improve manufacturing efficiency. Design for maintenance is typical work requiring cross-domain knowledge and involving stakeholder collaboration. This paper presents a structured domain-specific ontology and its development method, namely, the Maintainability Design Ontology for Complex prOducts (MDOCO), to formalize heterogeneous knowledge and improve semantic interoperability in the maintainability design area. The MDOCO has a rigorous semantic structure and complies with well-designed top-level and middle ontologies such as the Basic Formal Ontology and the Industrial Ontology Foundry (IOF) Core Ontology to ensure semantic interoperability. A set of reasoning rules is carefully designed to enable the MDOCO to perform knowledge reasoning. In a practical case, the effectiveness of the MDOCO is validated at both the semantic and application levels. The MDOCO combines recent methodology and best practices, enabling the well-structured modeling of heterogeneous knowledge and good semantic interoperability.}
}
@article{VARGA2018240,
title = {Analytical metadata modeling for next generation BI systems},
journal = {Journal of Systems and Software},
volume = {144},
pages = {240-254},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.06.039},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218301274},
author = {Jovan Varga and Oscar Romero and Torben Bach Pedersen and Christian Thomsen},
keywords = {Business intelligence, Metadata, Ontological metamodeling},
abstract = {Business Intelligence (BI) systems are extensively used as in-house solutions to support decision-making in organizations. Next generation BI 2.0 systems claim for expanding the use of BI solutions to external data sources and assisting the user in conducting data analysis. In this context, the Analytical Metadata (AM) framework defines the metadata artifacts (e.g., schema and queries) that are exploited for user assistance purposes. As such artifacts are typically handled in ad-hoc and system specific manners, BI 2.0 argues for a flexible solution supporting metadata exploration across different systems. In this paper, we focus on the AM modeling. We propose SM4AM, an RDF-based Semantic Metamodel for AM. On the one hand, we claim for ontological metamodeling as the proper solution, instead of a fixed universal model, due to (meta)data models heterogeneity in BI 2.0. On the other hand, RDF provides means for facilitating defining and sharing flexible metadata representations. Furthermore, we provide a method to instantiate our metamodel. Finally, we present a real-world case study and discuss how SM4AM, specially the schema and query artifacts, can help traversing different models instantiating our metamodel and enabling innovative means to explore external repositories in what we call metamodel-driven (meta)data exploration.}
}
@article{BITSCH2021582,
title = {Open semantic modeling for smart production systems},
journal = {Procedia CIRP},
volume = {104},
pages = {582-587},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.098},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121009963},
author = {Günter Bitsch and Pascal Senjic},
keywords = {Ontology, Semantics, Modelling},
abstract = {Conventional production systems are evolving through cyber-physical systems and application-oriented approaches of AI, more and more into "smart" production systems, which are characterized among other things by a high level of communication and integration of the individual components. The exchange of information between the systems is usually only oriented towards the data content, where semantics is usually only implicitly considered. The adaptability required by external and internal influences requires the integration of new or the redesign of existing components. Through an open application-oriented ontology the information and communication exchange are extended by explicit semantic information. This enables a better integration of new and an easier reconfiguration of existing components. The developed ontology, the derived application and use of the semantic information will be evaluated by means of a practical use case.}
}
@article{BUNNELL2021113843,
title = {Development of a consumer financial goals ontology for use with FinTech applications for improving financial capability},
journal = {Expert Systems with Applications},
volume = {165},
pages = {113843},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113843},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420306527},
author = {Lawrence Bunnell and Kweku-Muata Osei-Bryson and Victoria Y. Yoon},
keywords = {Financial goals ontology, Ontology engineering, Ontology acquisition, Ontology evaluation, Consumer financial goals, Financial capability},
abstract = {In this research, we communicate the design and evaluation of a consumer financial goals ontology to be utilized as a knowledgebase within recommender systems applications designed to provide decision support in the domain of financial planning. The goal of this research is to provide a domain conceptualization and knowledge classification of a comprehensive set of financial capability enhancing objectives contextually appropriate for a wide range of socio-economically situated consumers. Currently, to the best of our knowledge, within the domain of consumer financial planning no formal conceptual model or knowledge classification of financial goals exists which might be utilized as a knowledgebase for applications such as a FinTech recommender system. Achieving financial goals is a key behavior associated with consumer financial capability, a topic of national economic importance. A holistic representation of domain concepts is critical for advancement of solutions to problems pertinent to a domain. One primary reason for the dearth of applications designed to assist consumers with financial goal setting is the absence of a common domain ontology of financial goals. This study addresses a gap in the literature by contributing to the research knowledgebase an ontology for a domain of consumer financial goals. In doing so, it advances scholarly research through novel domain knowledge classification while providing researchers and practitioners with an ontological knowledgebase for indexing and retrieval within applications designed to improve consumer financial capability through identification and recommendation of specific, context-aware financial goals. The ontology could be used, for example, as a knowledgebase for a Personal Financial Recommender System (PFRS), or other financial technology (FinTech) application, designed to assist users with identification, setting, and tracking of financial goals.}
}
@article{WEBER2023529,
title = {Methodology for agile and iterative ontology development for toolmaking},
journal = {Procedia CIRP},
volume = {120},
pages = {529-534},
year = {2023},
note = {56th CIRP International Conference on Manufacturing Systems 2023},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123007643},
author = {Sebastian Weber and Tammo Dannen and Lars Stauder and Sebastian Barth and Thomas Bergs},
keywords = {Semantic technology, ontology, toolmaking, digital twin, knowledge retrieval methodology},
abstract = {Complex manufacturing processes and the absence of repeat effects characterize the toolmaking industry. German-speaking toolmaking companies are increasingly faced with the challenge of having to reach the limits of what is technically feasible and are confronted with an erosion of know-how, induced by demographic change. This applies in particular to know-how-intensive areas such as design, CAM-programming and work preparation. There is currently no comprehensive system support in these areas and the knowledge required for planning activities is often only available in the form of implicit technical and empirical knowledge. However, the use of heterogeneous manufacturing technologies requires a profound understanding of technology along the entire value chain. As a result of the very high semantic expressiveness of ontologies, they enable the representation of the most complex data models with logical relationships that go beyond hierarchical subdivision of content. This paper presents a novel agile methodology for the development of domain-specific ontologies in the environment of toolmaking. The methodology makes it possible to integrate the implicitly existing technical and experiential knowledge of employees at an early stage in the modelling process. In particular, the developed methodology extends conventional methods by the identified deficits in terms of knowledge acquisition, iteration and agility, as well as a separate consideration of the life cycle along the ontology of the use phase, taking into account agile methods from requirements engineering. Compliance with various guidelines and requirements is mandatory, such as the formulation of competence questions and the use of a standardised specification document. The iterative approach also ensures the needs-based integration of the characteristics of toolmaking. Furthermore, the methodology enables an early integration of IT structure and user interface for the needs-based design and use of the domain-specific ontology. The validation is based on an example in the mechanical production of a toolmaking company.}
}
@article{REGAL20181511,
title = {Ontology for Conceptual Modelling of Intelligent Maintenance Systems and Spare Parts Supply Chain Integration},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {1511-1516},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.285},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318314095},
author = {Thiago Regal and Carlos Eduardo Pereira},
abstract = {With an increasing demand for more efficiency and less costs in industry, the integration between Intelligent Maintenance Systems (IMS) and Spare Parts Supply Chain (SPSC) results in better availability of parts and services, avoiding breakdowns and unplanned production interruptions. Better demand planning results in better cost-effectivity as well, as parts and services can be better planned and always available when needed. The proper integration of IMS and SPSC has challenges related to semantic differences between areas with diverse concepts and vocabularies. This work intends to explore these challenges and propose a conceptual model for such integration, using existing ontologies in domains such as supply chain, maintenance and manufacturing to build on top of them an ontology aimed at allowing conceptual integration between IMS and SPSC and becoming a foundation to build future information systems to integrate these two areas. Its purpose is also further explored by using the characteristics of an ontology as tool for semantic description. Artificial intelligence, reasoning and context-aware systems are some of the areas that can benefit from the existence of an ontology to model IMS and SPSC integration. In this paper, we explore such characteristics along with integration and interoperability obtained by using an ontological model in IMS and SPSC integration.}
}@article{LIU2022100009,
title = {Representation and association of Chinese financial equity knowledge driven by multilayer ontology},
journal = {Data and Information Management},
volume = {6},
number = {3},
pages = {100009},
year = {2022},
issn = {2543-9251},
doi = {https://doi.org/10.1016/j.dim.2022.100009},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122001073},
author = {Zhenghao Liu and Zhijian Zhang and Xi Zeng and Huakui Lv},
keywords = {Multilayer domain ontology, Concept cube, Financial equity, Knowledge association, Knowledge representation, Triple extraction},
abstract = {Aiming at the current situation of complex financial ownership structure and isolated data organization, this study referring to the methods for multi-layer hierarchical construct domain ontology modeling. At the same time, the three dimensions of industry, company and internal environment were integrated, and the concept cube was designed and constructed based on knowledge extraction and text classification technology, so as to provide a multi-level and fine-grained knowledge representation and association method for financial equity knowledge. The experimental results show that conceptual cube structure represents semantic information as a dense low-dimensional representation vector, which greatly enhances semantic relevance and interpretability. The multi-layer ontology-driven ownership structure reflects a variety of knowledge association patterns, and in the “Intelligent Financial Big Data System” developed by the research team, the association query of three categories of association relationships in the field of industry, enterprise and internal environment is realized, as well as the dynamic analysis and supervision of typical financial management problems.}
}
@article{SCHWANCK2025107626,
title = {A Framework for testing Federated Learning algorithms using an edge-like environment},
journal = {Future Generation Computer Systems},
volume = {166},
pages = {107626},
year = {2025},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.107626},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24005909},
author = {Felipe Machado Schwanck and Marcos Tomazzoli Leipnitz and Joel Luís Carbonera and Juliano Araujo Wickboldt},
keywords = {Federated learning, Edge computing, Kubernetes, Microservices, Development framework},
abstract = {Federated Learning (FL) is a machine learning paradigm in which many clients cooperatively train a single centralized model while keeping their data private and decentralized. FL is commonly used in edge computing, which involves placing computer workloads (both hardware and software) as close as possible to the edge, where data are created and where actions are occurring, enabling faster response times, greater data privacy, and reduced data transfer costs. However, due to the heterogeneous data distributions/contents of clients, it is non-trivial to accurately evaluate the contributions of local models in global centralized model aggregation. This is an example of a major challenge in FL, commonly known as data imbalance or class imbalance. In general, testing and evaluating FL algorithms can be a very difficult and complex task due to the distributed nature of the systems. In this work, a framework is proposed and implemented to evaluate FL algorithms in a more easy and scalable way. This framework is evaluated over a distributed edge-like environment managed by a container orchestration platform (i.e. Kubernetes).}
}
@article{TRAPPEY2023102216,
title = {A comprehensive analysis of global patent landscape for recent R&D in agricultural drone technologies},
journal = {World Patent Information},
volume = {74},
pages = {102216},
year = {2023},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2023.102216},
url = {https://www.sciencedirect.com/science/article/pii/S0172219023000467},
author = {Amy J.C. Trappey and Ging-Bin Lin and Hong-Kai Chen and Ming-Chi Chen},
keywords = {Agricultural drone, Unmanned aerial vehicle (UAV), Knowledge ontology, Literature review, Patent landscape, Patent mining},
abstract = {The rapid development of information technology, along with advanced wireless communication technologies, has revolutionized the use of unmanned aerial vehicles (UAVs or drones) in many industries. In agriculture, due to the climate change and the growing global population, causing unprecedented demands and risks on food supplies, intelligent and automatic agricultural technologies are critical needs. UAVs offer a wide range of applications to transform traditional agricultural practices into Agriculture 4.0 that integrates advanced technology to optimize agricultural productivity, sustainability, and efficiency. To gain comprehensive insights into the current and future development of agricultural UAVs technologies, this research conducts extensive review and analysis of patents and non-patent literature in the field. By thoroughly examining the literature, the knowledge ontology of ag-UAV technologies is presented. Additionally, comprehensive macro- and micro-patent analyses identify the patenting trends and top tech-leaders for the key technologies related to agricultural drones. Moreover, utilizing regression modeling, technology maturity analysis, and technology-function matrix (TFM), the current and future R&D trends and the cold and hot spots of the technical innovations are identified. Through these detailed patent analyses, the state-of-the-art and potential advancements in agricultural UAV technologies are depicted, serving as crucial intelligence for R&D initiatives and IP strategies.}
}
@article{BENITEZMARTINEZ2021703,
title = {A neural blockchain for a tokenizable e-Participation model},
journal = {Neurocomputing},
volume = {423},
pages = {703-712},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.03.116},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220307335},
author = {Francisco Luis Benítez-Martínez and María Visitación Hurtado-Torres and Esteban Romero-Frías},
keywords = {Governance, Neural blockchain, e-Participation, Tokenization, Smart citizen, Open government},
abstract = {Currently, Distributed Ledger Technologies (DLTs) and, especially, Blockchain technology represent a great opportunity for public institutions to improve citizen participation and foster democratic innovation. These technologies facilitate the simplification of processes and provide secure management of recorded data, guaranteeing the transmission and public transparency of information. Based on the combination of a Blockchain as a Service (BaaS) platform and G-Cloud solutions, our proposal consists of the design of an e-Participation model that uses a tokenizable system of the actions and processes undertaken by citizens in participatory processes providing incentives to promote greater participation in public affairs. In order to develop a sustainable, scalable and resilient e-Participation system, a new blockchain concept, which organizes the blocks as a neural system, is combined with the implementation of a virtual token to reward participants. Furthermore, this virtual token is deployed through a smart contract that the block itself produces, containing information about the transaction and all the documents involved in the process. Finally, our Neural Distributed Ledger (NDL) framework facilitates the interconnection of blockchain networks in a transparent, certified, secure, auditable, scalable and traceable way.}
}
@article{KHOURI2023217,
title = {Knowledge base construction for the semantic management of environment-enriched built heritage: The case of Algerian traditional houses architecture},
journal = {Journal of Cultural Heritage},
volume = {63},
pages = {217-229},
year = {2023},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2023.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S1296207423001589},
author = {Selma Khouri and Houda Oufaida and Racha Amrani and Sabrina Kacher and Safia Ouahab and Mouna Cherrad},
keywords = {Knowledge base, Built heritage, Traditional house architecture, Environmental devices, Algeria},
abstract = {Traditional domestic architecture in Algeria reflects ancestral know-how which could serve as a model for new architectural achievements concerned with respecting the environment. This multidisciplinary knowledge covering different contexts (architectural, physical and environmental), is available but scattered through various sources of different formats (texts, tables, illustrations, etc.). Capturing such knowledge through an efficient knowledge repository is required. Knowledge base (KB) repositories have shown many significant contributions for managing the complexity, the variety of contexts and the querying of Built Heritage (BH) data. Creating a KB is a complex process that requires identifying and extracting the relevant knowledge from raw sources of information, then structuring, cleaning and integrating the extracted information into the KB repository. The management of these issues and their inter-dependencies are not always detailed in related BH literature. We propose in this paper a complete process for creating a KB dedicated to the built heritage consisting of Algerian traditional houses. Two main contributions are emphasized in this study: (i) the construction process of the KB is detailed at each step using two main suites: the ontology suite for identifying and structuring the relevant concepts, and the data suite that allows the flow of knowledge from the sources to the KB. (ii) the semantic variety of contexts of the input sources, their linguistic complexity and their format heterogeneity is handled all along the process, and is reflected at two levels: at the schema level (ontology model), and at the data level using automatic information extraction techniques. The definition of the KB includes fine-grained provenance metadata. Different results are proposed to evaluate the content and the quality of the KB.}
}
@article{PANZARELLA2023100059,
title = {Using ontologies for life science text-based resource organization},
journal = {Artificial Intelligence in the Life Sciences},
volume = {3},
pages = {100059},
year = {2023},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2023.100059},
url = {https://www.sciencedirect.com/science/article/pii/S266731852300003X},
author = {Giulia Panzarella and Pierangelo Veltri and Stefano Alcaro},
keywords = {Information overload, Ontology, Semantic web, Life science terms},
abstract = {Ontologies are used to support access to a multitude of databases that cover domains relevant information. Heterogeneity and different semantics can be accessed by using structured texts and descriptions in a hierarchical concept definition. We are interested in Life Sciences (LS) related ontologies including components taken from molecular biology, bioinformatics, physics, chemistry, medicine and other related areas. An Ontology comprises: (i) term connections, (ii) the identification of core concepts, (iii) data management, (iv) knowledge classification and integration to collect key information. An ontology may be very useful in navigating through LS terms. This paper explores some available biomedical ontologies and frameworks. It describes the most common ontology development environments (ODE): Protégé, Topbraid Composer, Ontostudio, Fluent Editor, VocBench, Swoop and Obo-edit, to create ontologies from textual scientific resources for LS plans. It also compares ontology methodologies in terms of Usability, Scalability, Stability, Integration, Documentation and Originality.}
}
@article{FITKAU2024102314,
title = {An ontology-based approach of automatic compliance checking for structural fire safety requirements},
journal = {Advanced Engineering Informatics},
volume = {59},
pages = {102314},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102314},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623004421},
author = {Isabelle Fitkau and Timo Hartmann},
keywords = {Preventive Fire Safety, Ontology, Automatic Code Compliance Checking},
abstract = {Achieving a mapping and integration of fire safety requirements into the digital planning process necessitates merging diverse and scattered regulatory knowledge related to fire safety. The nature of how requirements are phrased and described in regulations often lacks a comprehensive and detailed description of one another. Interpreting complex interrelations from these sources requires the expertise of specialists. An automatic code compliance checking is hindered without interpretation and through the absence of a formalization of preventive fire safety domain knowledge. Moreover, the non-involvement of fire safety planners in earlier project phases and the absence of fire safety properties in commonly used building modelling software further complicate the matter. To address these issues, we developed an ontological knowledge formalization, the Fire Safety Ontology (FiSa), enabling the classification of buildings and their components based on fire safety considerations. Combining building codes, technical regulations, guidelines, and semi-structured interviews with fire safety planners, we put emphasis on a fire safety perspective for buildings. Subsequently, we related fire safety domain insights from experts to knowledge about building design from other domains, such as architecture. Through this collaborative effort and the instantiation of building data within the ontology, structural fire safety requirements from regulations were automatically inferred by reasoning with common inference engines, simulating an automatic compliance checking. Our results demonstrate the contribution of ontology knowledge formalization to the integration process.}
}
@article{KOUTSIANA2023100799,
title = {An analysis of discussions in collaborative knowledge engineering through the lens of Wikidata},
journal = {Journal of Web Semantics},
volume = {78},
pages = {100799},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100799},
url = {https://www.sciencedirect.com/science/article/pii/S1570826823000288},
author = {Elisavet Koutsiana and Gabriel Maia Rocha Amaral and Neal Reeves and Albert Meroño-Peñuela and Elena Simperl},
keywords = {Collaborative knowledge engineering, Knowledge graph, Discussion analysis, Wikidata},
abstract = {We study discussions in Wikidata, the world’s largest open-source collaborative knowledge graph (KG). This is important because it helps KG community managers understand how discussions are used and inform the design of collaborative practices and support tools. We follow a mixed-methods approach with descriptive statistics, thematic analysis, and statistical tests to investigate how much discussions in Wikidata are used, what they are used for, and how they support knowledge engineering (KE) activities. The study covers three core sources of discussion, the talk pages that accompany Wikidata items and properties, and a general-purpose communication page. Our findings show low use of discussion capabilities and a power-law distribution similar to other KE projects such as Schema.org. When discussions are used, they are mostly about KE activities, including activities that span across the entire KE lifecycle from conceptualisation and implementation to maintenance and taxonomy building. We hope that the findings will help Wikidata devise improved practices and capabilities to encourage the use of discussions as a tool to collaborate, improve editor engagement, and engineer better KGs.}
}
@article{ALZAMIL2020100469,
title = {An ontological artifact for classifying social media: Text mining analysis for financial data},
journal = {International Journal of Accounting Information Systems},
volume = {38},
pages = {100469},
year = {2020},
note = {2019 UW CISA Symposium},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2020.100469},
url = {https://www.sciencedirect.com/science/article/pii/S1467089520300373},
author = {Zamil Alzamil and Deniz Appelbaum and Robert Nehmer},
keywords = {FIBO, Ontology, Social media, Frames and slots, Municipal bonds},
abstract = {In this paper we utilize a structured natural language processing implementation of the Financial Industry Business Ontology (FIBO) to extract financial information from the unstructured textual data of the social media platform Twitter regarding financial and budget information in the public sector, namely the two public-private agencies of the Port Authority of NY and NJ (PANYNJ), and the NY Metropolitan Transportation Agency (MTA). This research initiative uses the Design Science Research (DSR) perspective to develop an artifact to classify tweets as being either relevant to financial bonds or not. We apply a frame and slot approach from the artificial intelligence and natural language processing literature to operationalize this artifact. FIBO provides standards for defining the facts, terms, and relationships associated with financial concepts. We show that FIBO grammar can be used to mine semantic meaning from unstructured textual data and that it provides a nuanced representation of structured financial data. With this artifact, social media such as Twitter may be accessed for the knowledge that its text contains about financial concepts using the FIBO ontology. This process is anticipated to be of interest to bond issuers, regulators, analysts, investors, and academics. It may also be extended towards other financial domains such as securities, derivatives, commodities, and banking that relate to FIBO ontologies, as well as more generally to develop a structured knowledge representation of unstructured data through the application of an ontology.}
}
@article{BASSILIADES201881,
title = {PaaSport semantic model: An ontology for a platform-as-a-service semantically interoperable marketplace},
journal = {Data & Knowledge Engineering},
volume = {113},
pages = {81-115},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300551},
author = {Nick Bassiliades and Moisis Symeonidis and Panagiotis Gouvas and Efstratios Kontopoulos and Georgios Meditskos and Ioannis Vlahavas},
keywords = {Cloud computing, Platform-as-a-Service, Cloud Marketplace, Semantic interoperability, Ontologies, Quality and metrics},
abstract = {PaaS is a Cloud computing service that provides a computing platform to develop, run, and manage applications without the complexity of infrastructure maintenance. SMEs are reluctant to enter the growing PaaS market due to the possibility of being locked in to a certain platform, mostly provided by the market's giants. The PaaSport Marketplace aims to avoid the provider lock-in problem by allowing Platform provider SMEs to roll out semantically interoperable PaaS offerings and Software SMEs to deploy or migrate their applications on the best-matching offering, through a thin, non-intrusive Cloud broker. In this paper, we present the PaaSport semantic model, namely an OWL ontology, extension of the DUL ontology. The ontology is used for semantically representing (a) PaaS offering capabilities and (b) requirements of applications to be deployed. The ontology has been designed to optimally support a semantic matchmaking and ranking algorithm that recommends the best-matching PaaS offering to the application developer. The DUL ontology offers seamless extensibility, since both PaaS Characteristics and parameters are defined as classes; therefore, extending the ontology with new characteristics and parameters requires the addition of new specialized subclasses of the already existing classes, which is less complicated than adding ontology properties. The PaaSport ontology is evaluated through verification tools, competency questions, human experts, application tasks and query performance tests.}
}
@article{SELLAMI2022453,
title = {Keyword-based faceted search interface for knowledge graph construction and exploration},
journal = {International Journal of Web Information Systems},
volume = {18},
number = {56},
pages = {453-486},
year = {2022},
issn = {1744-0084},
doi = {https://doi.org/10.1108/IJWIS-02-2022-0037},
url = {https://www.sciencedirect.com/science/article/pii/S1744008422000118},
author = {Samir Sellami and Nacer Eddine Zarour},
keywords = {Knowledge exploration, Faceted browsing, Keyword search, Responsive interface, Virtual data exploration, Knowledge graphs, Linked data},
abstract = {Purpose
Massive amounts of data, manifesting in various forms, are being produced on the Web every minute and becoming the new standard. Exploring these information sources distributed in different Web segments in a unified way is becoming a core task for a variety of users’ and companies’ scenarios. However, knowledge creation and exploration from distributed Web data sources is a challenging task. Several data integration conflicts need to be resolved and the knowledge needs to be visualized in an intuitive manner. The purpose of this paper is to extend the authors’ previous integration works to address semantic knowledge exploration of enterprise data combined with heterogeneous social and linked Web data sources.
Design/methodology/approach
The authors synthesize information in the form of a knowledge graph to resolve interoperability conflicts at integration time. They begin by describing KGMap, a mapping model for leveraging knowledge graphs to bridge heterogeneous relational, social and linked web data sources. The mapping model relies on semantic similarity measures to connect the knowledge graph schema with the sources' metadata elements. Then, based on KGMap, this paper proposes KeyFSI, a keyword-based semantic search engine. KeyFSI provides a responsive faceted navigating Web user interface designed to facilitate the exploration and visualization of embedded data behind the knowledge graph. The authors implemented their approach for a business enterprise data exploration scenario where inputs are retrieved on the fly from a local customer relationship management database combined with the DBpedia endpoint and the Facebook Web application programming interface (API).
Findings
The authors conducted an empirical study to test the effectiveness of their approach using different similarity measures. The observed results showed better efficiency when using a semantic similarity measure. In addition, a usability evaluation was conducted to compare KeyFSI features with recent knowledge exploration systems. The obtained results demonstrate the added value and usability of the contributed approach.
Originality/value
Most state-of-the-art interfaces allow users to browse one Web segment at a time. The originality of this paper lies in proposing a cost-effective virtual on-demand knowledge creation approach, a method that enables organizations to explore valuable knowledge across multiple Web segments simultaneously. In addition, the responsive components implemented in KeyFSI allow the interface to adequately handle the uncertainty imposed by the nature of Web information, thereby providing a better user experience.}
}
@article{SANFILIPPO2019182,
title = {Ontology-based knowledge representation for additive manufacturing},
journal = {Computers in Industry},
volume = {109},
pages = {182-194},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S016636151830808X},
author = {Emilio M. Sanfilippo and Farouk Belkadi and Alain Bernard},
keywords = {Manufacturing, Additive manufacturing, Knowledge representation, Reasoning, Ontology, DOLCE},
abstract = {The flourishing development of additive manufacturing (AM) technologies calls for robust IT methodologies and solutions to manage the plethora of data that is generated in the AM value chain. The purpose of this paper is to propose a principled knowledge-based model for AM in the form of a computational ontology. As corpus of formally represented knowledge, the ontology constitutes the backbone structure to organize AM data and automatically reason over experts’ knowledge for data validation, ultimately supporting the development of algorithms and applications for decision making. By the end of the paper we show some modeling and reasoning examples based on the use of the proposed ontology in a prototype Web application.}
}
@article{SHAHZAD2021106146,
title = {Ontology Driven Smart Health Service Integration},
journal = {Computer Methods and Programs in Biomedicine},
volume = {207},
pages = {106146},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106146},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721002200},
author = {Syed Khuram Shahzad and Daniyal Ahmed and Muhammad Raza Naqvi and Muhammad Tahir Mushtaq and Muhammad Waseem Iqbal and Farrukh Munir},
keywords = {Internet of things, Integrated platform, Smart healthcare, Health service facilitation, Ontological framework},
abstract = {Background and objective
The massive increase, in the Internet of Things applications, has greatly evolved technological aspects of human life. The drastic development of IoT based smart healthcare services have layout the smart process models to facilitate all stakeholders (e.g. patients, doctors, hospitals etc.) and made it an important social-economic concern. There are variety of smart healthcare services like remote patient monitoring, diagnostic, disease specific remote treatments and telemedicine. Many trending Internet of Health Things research and development are done in a very disjoint and independent fashion providing solutions and guidelines for variant diseases, medical resources and remote services management. These expositions work over many shared resources such as health facilities for patient and human in healthcare system.
Methods
This research discusses the ontology for merging methods to form an integrated platform with shared knowledge of smart healthcare services. The proposed process model creates an ontological framework of integrated healthcare services, which are firstly defined using ontologies and lately integrated over similarities, differences, dependencies and other semantic relations. The data and process requirements for service integration facility is derived from various smart healthcare services.
Results
The proposed model is evaluated using two-step ontological modeling testing method, applied at the ontological framework of integrated smart health services. First evaluation step has targeted the model consistency validation using reasoning tool while querying tools are used to validate the retrieved data entities and relations among them for predefined use-cases.
Conclusions
The research concluded with a novel approach for smart health service integration using ontological modeling and merging techniques. The model efficiency enhancement and query optimization methods are listed in future tasks of the research.}
}
@article{MURTAZINA2021595,
title = {The constructing of cognitive functions ontology},
journal = {Procedia Computer Science},
volume = {186},
pages = {595-602},
year = {2021},
note = {14th International Symposium "Intelligent Systems},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921010188},
author = {M.Sh. Murtazina and T.V. Avdeenko},
keywords = {Ontology, OWL, cognitive functions, assessing cognitive functions},
abstract = {In present paper we consider ontology-driven approach applied to neurocognitive science. At the beginning, a brief overview of the works revealing the prospects and possibilities of constructing ontological models in this field of knowledge is given. Based on the conducted analysis we have concluded that the scientific community states the urgency of creating an ontology intended to accumulate interdisciplinary knowledge necessary for assessing cognitive functions being the most complex functions of the brain, through which the process of rational cognition of the world is carried out. So we have analyzed the key features of the knowledge representation about cognitive functions in the field of psychology, cognitive science and neurobiology. As a result we propose conceptual structure, basic classes and relations for the cognitive functions ontology intended to the clearer understanding of the relationships between the brain activity patterns and the human cognitive abilities. The ontology was implemented using OWL in the Protégé 5.2 ontology editor environment. It accumulates knowledge about cognitive functions and methods for assessing them with use of neuropsychological tests and EEG methods. A certain set of axioms also have been implemented and approved.}
}
@article{SESBOUE20221667,
title = {An Operational Architecture for Knowledge Graph-Based Systems},
journal = {Procedia Computer Science},
volume = {207},
pages = {1667-1676},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.224},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922011085},
author = {Matthias Sesboüé and Nicolas Delestre and Jean-Philippe Kotowicz and Ali Khudiyev and Cecilia Zanni-Merk},
keywords = {Knowledge Graph, Ontology, Knowledge-Based System, Semantic},
abstract = {Knowledge Graphs (KG) are gaining in popularity recently, notably since big tech giants announced they are using the technology. While the term is becoming popular, it is not new, and its ideas are even older. The research community has extensively studied knowledge Graphs in their various forms. Furthermore, the approach has been applied and proved valuable in many different applications. However, we found a lack of papers presenting the integration of KGs in a system regardless of the downstream application. We explore how KGs can fit in an overall information system independently from any specific use case, i.e., what we will consider knowledge consumption. We propose an architecture to understand better the KG roles within a system and how they can be integrated and implemented in a business context. We introduce each element of the latter architecture and discuss some candidate technology to implement them. Our work implements Knowledge Graph-Based Systems considering the constraints of a small to medium-sized enterprise.}
}
@article{BENABDALLAH2020953,
title = {Personalized cloud service review analysis based on modularized ontology},
journal = {Online Information Review},
volume = {44},
number = {5},
pages = {953-975},
year = {2020},
issn = {1468-4527},
doi = {https://doi.org/10.1108/OIR-06-2019-0207},
url = {https://www.sciencedirect.com/science/article/pii/S1468452720000414},
author = {Emna Ben-Abdallah and Khouloud Boukadi and Mohamed Hammami and Mohamed Hedi Karray},
keywords = {Cloud service, Context, Opinion analysis, Modular ontologies},
abstract = {Purpose
The purpose of this paper is to analyze cloud reviews according to the end-user context and requirements.
Design/methodology/approach
propose a comprehensive knowledge base composed of interconnected Web Ontology Language, namely, modular ontology for cloud service opinion analysis (SOPA). The SOPA knowledge base will be the basis of context-aware cloud service analysis using consumers' reviews. Moreover, the authors provide a framework to evaluate cloud services based on consumers' reviews opinions.
Findings
The findings show that there is a positive impact of personalizing the cloud service analysis by considering the reviewers' contexts in the performance of the framework. The authors also proved that the SOPA-based framework outperforms the available cloud review sites in term of precision, recall and F-measure.
Research limitations/implications
Limited information has been provided in the semantic web literature about the relationships between the different domains and the details on how that can be used to evaluate cloud service through consumer reviews and latent opinions. Furthermore, existing approaches are lacking lightweight and modular mechanisms which can be utilized to effectively exploit information existing in social media.
Practical implications
The SOPA-based framework facilitates the opinion based service evaluation through a large number of consumer's reviews and assists the end-users in analyzing services as per their requirements and their own context.
Originality/value
The SOPA ontology is capable of representing the content of a product/service as well as its related opinions, which are extracted from the customer's reviews written in a specific context. Furthermore, the SOPA-based framework facilitates the opinion based service evaluation through a large number of consumer's reviews and assists the end-users in analyzing services as per their requirements and their own context.}
}
@article{BAYOUDHI2018138,
title = {How to Repair Inconsistency in OWL 2 DL Ontology Versions?},
journal = {Data & Knowledge Engineering},
volume = {116},
pages = {138-158},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X16303172},
author = {Leila Bayoudhi and Najla Sassi and Wassim Jaziri},
keywords = {OWL 2 DL ontology, Evolution, Inconsistency, A priori approach},
abstract = {Semantic modeling knowledge formalisms, such as ontologies, have to follow the continuous evolution and changes of knowledge. However, ontology changes should never affect its consistency. Ontology needs to remain in a consistent state along its whole engineering process. In the literature, most of approaches check/repair ontology inconsistencies in an a posteriori way. In this paper, an a priori inconsistency approach was proposed to generate consistent OWL 2 DL ontology versions. It relies on the OWL 2 DL change kits, which anticipate inconsistencies upon each change request on an ontology version. The proposed approach predicts potential inconsistencies, provides an a priori repair action and applies the required changes. Consistency rules were defined and used to check logical inconsistencies, but also syntactical invalidities and style issues. A protégé plugin was implemented to validate our approach.}
}
@article{WANG2018359,
title = {Citrus ontology development based on the eight-point charter of agriculture},
journal = {Computers and Electronics in Agriculture},
volume = {155},
pages = {359-370},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917312012},
author = {Yi Wang and Ying Wang},
keywords = {Ontology, Knowledge modeling, Semantic web, Agriculture},
abstract = {Developing large-scale agricultural ontologies is a challenging and error-prone task that requires substantial effort and collaboration between domain experts and ontology developers due to the complexity of agricultural knowledge. Inspired by the Chinese Eight-Point Charter of Agriculture, i.e., Soil, Fertilization, Water, Variety, Density, Protection, Management and Tool, this paper presents an approach to modeling and integrating citrus production knowledge. Citrus domain knowledge is classified into eight categories based on the Eight-Point Charter of Agriculture, and the relationships in each category and among categories are established. The eight categories and the relationships are defined as the citrus production knowledge framework. Then, we propose mechanisms to develop citrus ontology based on the citrus production knowledge framework. The Fertilization ontology is created as an illustration of our approach, which contains 866 ontology entities and 12,583 Resource Description Framework triples. The structural evaluation results of the eight metrics for the Fertilization ontology are considerably better than the average and median values of 1413 Web ontologies. In addition, four antipatterns were used to evaluate the ontology, and no occurrence of the antipatterns was detected for the 866 ontology entities. The accuracy of the ontology is ensured by the competency evaluation of the 110 questions with 88% accuracy. Our approach provides an effective solution for modeling complex agricultural knowledge and transforming the agriculture domain knowledge into computable resources.}
}
@article{GALADIMA2025103970,
title = {Evaluating Incident Response in CSIRTs using Cube Socio-technical Systems Analysis},
journal = {Computer Standards & Interfaces},
volume = {93},
pages = {103970},
year = {2025},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2024.103970},
url = {https://www.sciencedirect.com/science/article/pii/S0920548924001399},
author = {Haula Sani Galadima and Cormac Doherty and Nick McDonald and Junli Liang and Rob Brennan},
keywords = {Incident response, Computer Security Incident Response Team (CSIRT), Cube Socio-technical Systems Analysis (STSA), Access Risk Knowledge (ARK) platform},
abstract = {This paper provides a novel method for evaluating Incident Response (IR) teams through the application of the Cube Socio-technical Systems Analysis (STSA) methodology. Cube is a form of structured Human Factors enquiry and has previously been successfully applied in both aviation and healthcare. By utilising STSA, this study aims to understand and evaluate incident knowledge across the IR socio-technical domain. Traditional approaches to IR improvement often focus solely on technical aspects, neglecting social factors that may significantly influence IR effectiveness. This research presents the results of extending the ARK platform for a cybersecurity IR Cube STSA of IR activities in a case study involving a large, accredited Computer Security Incident Response Team (CSIRT). It evaluates the IR system and team needs before the development of a technological intervention to improve IR learning and preparation capabilities. We present an extended Cube questionnaire, that defines specialised IR questions, an ontology, and terminology for the cybersecurity domain based on the ISO27000 series of standards. The case study demonstrates the ARK platform's capability to capture and analyse IR systems using a Multi-stage Cube STSA analysis shared in a reusable knowledge graph based on W3C standards. This provides a shared knowledge base based on FAIR (Findable, Accessible, Interoperable, Reusable) linked data, that may support generation of training materials, playbooks, and best practices to enhance IR capabilities and CSIRT operations. We show how this approach provides new insights and reusable artefacts for CSIRTs to enhance organisational cyber resilience and learning.}
}
@article{WATROBSKI20203345,
title = {Towards standardization in frameworks, tools and approaches dedicated to ontology building and management},
journal = {Procedia Computer Science},
volume = {176},
pages = {3345-3355},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.063},
url = {https://www.sciencedirect.com/science/article/pii/S187705092031958X},
author = {Jarosław Wątróbski},
keywords = {Ontology integration, Ontology merging, alignement, Knowledge repository, Ontology},
abstract = {The widespread popularity and spread of ontologies in many areas increasingly requires the successful integration of multiple such ontologies. The ontology integration problem has been investigated during last years and it is still a challenging task. Aligning and merging existing ontologies, which are usually handled manually, is often a big and tedious part of the sharing process. Due to the fact that there are different approaches, tools and strategies that help to avoid mistakes during modeling especially large domains and creating a new single coherent ontology, this article attempts to build a common knowledge repository in the form of an open and publicly available tool. The overall goal of the proposed open knowledge repository is to provide free access to basic data and to compare the features offered. It is worth mentioning that this form of knowledge representation is publicly available and focuses on the possibility of updating and reusing. It is an attempt to structure knowledge that can be successfully extended with additional groups of solutions and parameters. The proposed approach can be easily implemented as a light library and used in any type of OWL/RDF or OWL/XML management application or adapted in a given management information system.}
}
@article{KUO2023105462,
title = {An ontology-based framework for semantic geographic information systems development and understanding},
journal = {Computers & Geosciences},
volume = {181},
pages = {105462},
year = {2023},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105462},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001668},
author = {Chiao-Ling Kuo and Han-Chuan Chou},
keywords = {Ontology, GIS components, Semantic GIS development, System comprehensibility, WebGIS, Online thematic mapping},
abstract = {Geographic information systems (GIS) are a widely used approach for geodata manipulation, analysis, and geoinformation visualization. Although the issue of semantic heterogeneity for GIS applications has been widely addressed, the development of flexible semantic GIS systems can still be improved, and GIS interfaces can be further unified to enhance system understanding among users of various GIS systems. This study proposes an ontology-based semantic GIS conceptual framework that can seamlessly combine ontology models, GIS elements and user actions for semantic GIS application development. In addition, a GIS components ontology that encompasses the five fundamental components of GIS is proposed to integrate GIS elements that can perceive GIS operations and processes semantically and to support customized GIS system designs. Moreover, a semantic WebGIS implementation framework is designed on the basis of the proposed conceptual framework to facilitate semantic WebGIS development. The implementation uses two common application scenarios regarding urban change analysis and user interaction. The former performs semantic retrieval for urban change analysis with system understanding, whereas the latter conducts online thematic mapping with semantic interpretation. The study contributes in terms of innovation in the design of a unified GIS components ontology and successful implementation of a semantic GIS system using the ontology-driven approach, thereby providing a convincing demonstration of the latter and showing the potential for semantic GIS application paradigms.}
}
@article{MCGLINN2022105313,
title = {FAIRVASC: A semantic web approach to rare disease registry integration},
journal = {Computers in Biology and Medicine},
volume = {145},
pages = {105313},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105313},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522001056},
author = {Kris McGlinn and Matthew A. Rutherford and Karl Gisslander and Lucy Hederman and Mark A. Little and Declan O'Sullivan},
keywords = {Knowledge engineering, Linked data, Ontologies, Federated queries, Rare diseases},
abstract = {Rare disease data is often fragmented within multiple heterogeneous siloed regional disease registries, each containing a small number of cases. These data are particularly sensitive, as low subject counts make the identification of patients more likely, meaning registries are not inclined to share subject level data outside their registries. At the same time access to multiple rare disease datasets is important as it will lead to new research opportunities and analysis over larger cohorts. To enable this, two major challenges must therefore be overcome. The first is to integrate data at a semantic level, so that it is possible to query over registries and return results which are comparable. The second is to enable queries which do not take subject level data from the registries. To meet the first challenge, this paper presents the FAIRVASC ontology to manage data related to the rare disease anti-neutrophil cytoplasmic antibody (ANCA) associated vasculitis (AAV), which is based on the harmonisation of terms in seven European data registries. It has been built upon a set of key clinical questions developed by a team of experts in vasculitis selected from the registry sites and makes use of several standard classifications, such as Systematized Nomenclature of Medicine - Clinical Terms (SNOMED-CT) and Orphacode. It also presents the method for adding semantic meaning to AAV data across the registries using the declarative Relational to Resource Description Framework Mapping Language (R2RML). To meet the second challenge a federated querying approach is presented for accessing aggregated and pseudonymized data, and which supports analysis of AAV data in a manner which protects patient privacy. For additional security the federated querying approach is augmented with a method for auditing queries (and the uplift process) using the provenance ontology (PROV-O) to track when queries and changes occur and by whom. The main contribution of this work is the successful application of semantic web technologies and federated queries to provide a novel infrastructure that can readily incorporate additional registries, thus providing access to harmonised data relating to unprecedented numbers of patients with rare disease, while also meeting data privacy and security concerns.}
}
@article{DENICOLA2023100489,
title = {Development and measurement of a resilience indicator for cyber-socio-technical systems: The allostatic load},
journal = {Journal of Industrial Information Integration},
volume = {35},
pages = {100489},
year = {2023},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2023.100489},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X23000626},
author = {Antonio {De Nicola} and Maria Luisa Villani and Mark Sujan and John Watt and Francesco Costantino and Andrea Falegnami and Riccardo Patriarca},
keywords = {Resilience, Cyber-socio-technical system, Ontology, Semantic similarity, Business process, Leading indicator},
abstract = {Management of cyber-socio-technical processes often suffers from misalignments of process descriptions according to formal organization documents or manager views (Work-As-Imagined) with actual work practices as performed by sharp-end operators (Work-As-Done). Even if sometimes the accomplishment of a process requires workers to diverge from the Work-As-Imagined, the corresponding changes can potentially cause organizational tensions in the overall system and lead to safety incidents. This consideration led us to define a new resilience indicator, named allostatic load, to capture such misalignments, and the corresponding level of organizational tensions, a cyber-socio-technical system is exposed to. Then, we propose a method to measure it by leveraging semantic technologies, the Functional Resonance Analysis Method (FRAM) to model industrial processes, the WAx conceptual framework to keep track of the variety of the different process perspectives, and a crowd-based approach to elicit industrial knowledge. Finally, we discuss the feasibility of the approach in two real case studies related to a pharmaceutical manufacturing plant and an enterprise in the aluminium sector.}
}
@article{JARVENPAA2021435,
title = {Capability matchmaking software for rapid production system design and reconfiguration planning},
journal = {Procedia CIRP},
volume = {97},
pages = {435-440},
year = {2021},
note = {8th CIRP Conference of Assembly Technology and Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.264},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120314864},
author = {Eeva Järvenpää and Niko Siltala and Otto Hylli and Minna Lanz},
keywords = {Production system design, Production system reconfiguration, Capability matchmaking, Matchmaking software, Resource modelling, Ontology},
abstract = {Traditionally, the production system design and reconfiguration planning are manual processes, which rely heavily on the designers’ expertise and tacit knowledge to find feasible system configuration solutions. Rapid responsiveness of future production systems calls for new computer-aided intelligent design and planning solutions, that would reduce the time and effort put into system design, both in brownfield and greenfield scenarios. This paper describes the implementation of a capability matchmaking software, which automatizes the matchmaking between product requirements and resource capabilities. The interaction of the matchmaking system with external design and planning tools is explained and illustrated with a case example. The matchmaking approach supports production system design and reconfiguration planning by providing automatic means for checking if the existing system already fulfills the new product requirements, and for finding alternative resources and resource combinations to specific product requirements from large search spaces, e.g. from global resource catalogues.}
}
@article{VELPULA2022108037,
title = {CEECP: CT-based enhanced e-clinical pathways in terms of processing time to enable big data analytics in healthcare along with cloud computing},
journal = {Computers & Industrial Engineering},
volume = {168},
pages = {108037},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108037},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222001073},
author = {Prasad Velpula and Rajendra Pamula},
keywords = {Artificial Intelligence, Big Data, Clinical Path (CP), Electronic health (e-Health), Electronic Medical Record (EMR), Electronic Health Record (EHR), Information Technology (IT), LoS (Length of Stay), Systematized Nomenclature of Medicine-Clinical Terms (SNOMED CT)},
abstract = {Clinical pathway is generally used as a tool for the implementation of evidence-based medicine and clinical guidelines. The e-clinical pathway is helpful in clinical cases covering both diagnostic and therapeutic fields. Healthcare facility automation is a challenging task to streamline a high informatics-intensive sector. With the help of datasets which were collected on LOS, CP, and EHR for hearth patient is taken from Kaggle data sets were collected, a better solution to this existing problem is proposed in this paper in the form of automation of CP, with the help of SNOMED-CT which not only helps to study the correct health situation of the patient and also helpful to estimate the correct length of stay in hospitals and to minimize the expenditures. This paper has carried out an extensive highlighted review for the current trending mechanism and approaches for smartly tackling privacy and security issues along with suggesting short-term stay in hospitals, thereby a cost reduction. In our proposed method, a new cost dynamic approach is suggested, which is a unique method for the exact analysis of data. The proposed CEECP method achieved less average processing time of 1116.6 µs and achieved accuracy of 99.01%, which is higher than existing methods. The proposed method achieved higher values for other factors such as accuracy, precision, sensitivity, and specificity.}
}
@article{JIANG2023104728,
title = {Intelligent control of building fire protection system using digital twins and semantic web technologies},
journal = {Automation in Construction},
volume = {147},
pages = {104728},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104728},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522005982},
author = {Liu Jiang and Jianyong Shi and Chaoyu Wang and Zeyu Pan},
keywords = {Digital twins (DTs), Sematic web, Ontology, Building automated system, Fire protection system, Intelligent control, Building information modelling (BIM)},
abstract = {A fire protection system takes on a critical significance to building operation. This paper describes the use of digital twins (DTs) and semantic web technologies for the intelligent control of building fire protection (BFP) systems in fire accidents. Specifically, a data fusion stage and several information-based control mechanisms are involved in the use of the two technologies. A designed BFP ontology is considered as the semantic model and basis of data fusion between the static building geometric information and the dynamic sensing data. The above information is incorporated into a DT data model, which is considered as the mapping of physical space. Moreover, rule models and process models are developed to achieve intelligent control mechanisms and keep the DT data model synced with the physical space. A case study based on a fire accident simulation was conducted to verify the feasibility of the use of DTs and semantic web technologies.}
}
@article{SONG2025105706,
title = {Knowledge graph-based alarm management in petrochemical enterprises: A study on fusion and analysis of multi-source heterogeneous information},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {97},
pages = {105706},
year = {2025},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2025.105706},
url = {https://www.sciencedirect.com/science/article/pii/S0950423025001640},
author = {Xiaomiao Song and Fabo Yin and Dongfeng Zhao},
keywords = {Petrochemical enterprises, Alarm management, Knowledge graph, Ontology},
abstract = {In response to the increasing emphasis on alarm management in the petrochemical industry, there has been an explosive growth in relevant information. However, this information is often scattered across different systems and databases, stored in various forms such as documents, tables, and images, making it challenging to uniformly store, share, and utilize multi-source heterogeneous information. This commonly leads to the problem of “Information Islands.” In order to effectively leverage knowledge in the field of alarm management in the petrochemical industry and overcome the challenge of non-interoperable information, a method for fusing multi-source heterogeneous information in petrochemical enterprise alarm management based on knowledge graph is proposed. This method aims to standardize the management of alarm-related information and achieve information fusion. Initially, the approach utilizes data from petrochemical enterprises and publicly available data in the field of alarm management to establish both local and global ontologies. Subsequently, mapping algorithms are designed to achieve a more accurate construction of the hybrid ontology. Based on this foundation, a knowledge graph for alarm management in the petrochemical industry is established. Additionally, corresponding modules for information storage and retrieval are developed. Through the application demonstration using real alarm management information from a petrochemical enterprise, the results indicate that the proposed method for fusing multi-source heterogeneous information in petrochemical enterprise alarm management can effectively achieve information fusion.}
}
@article{EIVAZZADEH2018,
title = {Most Influential Qualities in Creating Satisfaction Among the Users of Health Information Systems: Study in Seven European Union Countries},
journal = {JMIR Medical Informatics},
volume = {6},
number = {4},
year = {2018},
issn = {2291-9694},
doi = {https://doi.org/10.2196/11252},
url = {https://www.sciencedirect.com/science/article/pii/S2291969418000613},
author = {Shahryar Eivazzadeh and Johan S Berglund and Tobias C Larsson and Markus Fiedler and Peter Anderberg},
keywords = {health information systems, telemedicine, evaluation studies as topic, consumer behavior, treatment outcome, safety, efficiency, health care costs, ontology engineering, equation models},
abstract = {Background
Several models suggest how the qualities of a product or service influence user satisfaction. Models such as the Customer Satisfaction Index (CSI), Technology Acceptance Model (TAM), and Delone and McLean Information Systems Success demonstrate those relations and have been used in the context of health information systems.
Objective
This study aimed to investigate which qualities foster greater satisfaction among patient and professional users. In addition, we are interested in knowing to what extent improvement in those qualities can explain user satisfaction and whether this makes user satisfaction a proxy indicator of those qualities.
Methods
The Unified eValuation using ONtology (UVON) method was used to construct an ontology of the required qualities for 7 electronic health (eHealth) apps being developed in the Future Internet Social and Technological Alignment Research (FI-STAR) project, a European Union (EU) project in electronic health (eHealth). The eHealth apps were deployed across 7 EU countries. The ontology included and unified the required qualities of those systems together with the aspects suggested by the Model for ASsessment of Telemedicine apps (MAST) evaluation framework. Moreover, 2 similar questionnaires for 87 patient users and 31 health professional users were elicited from the ontology. In the questionnaires, the user was asked if the system has improved the specified qualities and if the user was satisfied with the system. The results were analyzed using Kendall correlation coefficients matrices, incorporating the quality and satisfaction aspects. For the next step, 2 partial least squares structural equation modeling (PLS-SEM) path models were developed using the quality and satisfaction measure variables and the latent construct variables that were suggested by the UVON method.
Results
Most of the quality aspects grouped by the UVON method are highly correlated. Strong correlations in each group suggest that the grouped qualities can be measures that reflect a latent quality construct. The PLS-SEM path analysis for the patients reveals that the effectiveness, safety, and efficiency of treatment provided by the system are the most influential qualities in achieving and predicting user satisfaction. For the professional users, effectiveness and affordability are the most influential. The parameters of the PLS-SEM that are calculated allow for the measurement of a user satisfaction index similar to CSI for similar health information systems.
Conclusions
For both patients and professionals, the effectiveness of systems highly contributes to their satisfaction. Patients care about improvements in safety and efficiency, whereas professionals care about improvements in the affordability of treatments with health information systems. User satisfaction is reflected more in the users’ evaluation of system output and fulfillment of expectations but slightly less in how far the system is from ideal. Investigating satisfaction scores can be a simple and fast way to infer if the system has improved the abovementioned qualities in treatment and care.}
}
@article{CARMODY2023913,
title = {The Medical Action Ontology: A tool for annotating and analyzing treatments and clinical management of human disease},
journal = {Med},
volume = {4},
number = {12},
pages = {913-927.e3},
year = {2023},
issn = {2666-6340},
doi = {https://doi.org/10.1016/j.medj.2023.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2666634023003343},
author = {Leigh C. Carmody and Michael A. Gargano and Sabrina Toro and Nicole A. Vasilevsky and Margaret P. Adam and Hannah Blau and Lauren E. Chan and David Gomez-Andres and Rita Horvath and Megan L. Kraus and Markus S. Ladewig and David Lewis-Smith and Hanns Lochmüller and Nicolas A. Matentzoglu and Monica C. Munoz-Torres and Catharina Schuetz and Berthold Seitz and Morgan N. Similuk and Teresa N. Sparks and Timmy Strauss and Emilia M. Swietlik and Rachel Thompson and Xingmin Aaron Zhang and Christopher J. Mungall and Melissa A. Haendel and Peter N. Robinson},
keywords = {medical action ontology, MAxO, ontology, treatment, surgical procedure, clinical management, computational decision support},
abstract = {Summary
Background
Navigating the clinical literature to determine the optimal clinical management for rare diseases presents significant challenges. We introduce the Medical Action Ontology (MAxO), an ontology specifically designed to organize medical procedures, therapies, and interventions.
Methods
MAxO incorporates logical structures that link MAxO terms to numerous other ontologies within the OBO Foundry. Term development involves a blend of manual and semi-automated processes. Additionally, we have generated annotations detailing diagnostic modalities for specific phenotypic abnormalities defined by the Human Phenotype Ontology (HPO). We introduce a web application, POET, that facilitates MAxO annotations for specific medical actions for diseases using the Mondo Disease Ontology.
Findings
MAxO encompasses 1,757 terms spanning a wide range of biomedical domains, from human anatomy and investigations to the chemical and protein entities involved in biological processes. These terms annotate phenotypic features associated with specific disease (using HPO and Mondo). Presently, there are over 16,000 MAxO diagnostic annotations that target HPO terms. Through POET, we have created 413 MAxO annotations specifying treatments for 189 rare diseases.
Conclusions
MAxO offers a computational representation of treatments and other actions taken for the clinical management of patients. Its development is closely coupled to Mondo and HPO, broadening the scope of our computational modeling of diseases and phenotypic features. We invite the community to contribute disease annotations using POET (https://poet.jax.org/). MAxO is available under the open-source CC-BY 4.0 license (https://github.com/monarch-initiative/MAxO).
Funding
NHGRI 1U24HG011449-01A1 and NHGRI 5RM1HG010860-04.}
}
@article{ALKHARIJI2023280,
title = {Semantics-based privacy by design for Internet of Things applications},
journal = {Future Generation Computer Systems},
volume = {138},
pages = {280-295},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22002746},
author = {Lamya Alkhariji and Suparna De and Omer Rana and Charith Perera},
keywords = {Privacy, Privacy by Design, Internet of Things, Semantic web, Ontology, Context awareness},
abstract = {As Internet of Things (IoT) technologies become more widespread in everyday life, privacy issues are becoming more prominent. The aim of this research is to develop a personal assistant that can answer software engineers’ questions about Privacy by Design (PbD) practices during the design phase of IoT system development. Semantic web technologies are used to model the knowledge underlying PbD measurements, their intersections with privacy patterns, IoT system requirements and the privacy patterns that should be applied across IoT systems. This is achieved through the development of the PARROT ontology, developed through a set of representative IoT use cases relevant for software developers. This was supported by gathering Competency Questions (CQs) through a series of workshops, resulting in 81 curated CQs. These CQs were then recorded as SPARQL queries, and the developed ontology was evaluated using the Common Pitfalls model with the help of the Protégé HermiT Reasoner and the Ontology Pitfall Scanner (OOPS!), as well as evaluation by external experts. The ontology was assessed within a user study that identified that the PARROT ontology can answer up to 58% of privacy-related questions from software engineers.}
}
@article{QUEK2024109507,
title = {Dynamic knowledge graph applications for augmented built environments through “The World Avatar”},
journal = {Journal of Building Engineering},
volume = {91},
pages = {109507},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.109507},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224010751},
author = {Hou Yee Quek and Markus Hofmeister and Simon D. Rihm and Jingya Yan and Jiawei Lai and George Brownbridge and Michael Hillman and Sebastian Mosbach and Wilson Ang and Yi-Kai Tsai and Dan N. Tran and Soon Kang, William Tan and Markus Kraft},
keywords = {BIM, GIS, Interoperability, Built environment, Knowledge graph},
abstract = {The proliferation of digital building models in recent years has led to a corresponding rise in specialised, non-interoperable models. These models impede sustainable developments by forming data silos that hinder cross-application data exchange and knowledge discovery processes. Although Semantic Web solutions hold promise in addressing these silos, current approaches primarily focus on developing novel ontologies, yielding similar outcomes. But it is unclear how these methodologies could support broader knowledge discovery processes and application requirements. This paper addresses these research challenges by introducing a dynamic knowledge graph as implemented within The World Avatar for interoperable building models. We demonstrate its value through two distinct applications in urban energy management and laboratory automation. The dynamic knowledge graph revolves around a comprehensive structured knowledge model constructed from ontologies and agents. Ontologies semantically annotate data and represent domain knowledge and their relationships with standardised definitions. When augmented with an agent architecture, the resulting knowledge model can align stakeholder perspectives and accommodate the dynamic and scalable nature of urban data. Moreover, the dynamic knowledge graph fosters innovative human-machine interactions through visualisation interfaces to augment knowledge discovery processes in the built environment for greater efficiencies and innovation. As the knowledge model expands, users gain access to a broader spectrum of private and public data sources and technologies, while reducing integration barriers. This is especially pertinent for smaller and less influential entities like municipal and local governments with limited resources, who can realise substantial benefits at reduced costs.}
}
@article{HIPPOLYTE2018210,
title = {Ontology-driven development of web services to support district energy applications},
journal = {Automation in Construction},
volume = {86},
pages = {210-225},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517308907},
author = {J.-L. Hippolyte and Y. Rezgui and H. Li and B. Jayan and S. Howell},
keywords = {Ontology design, Domain engineering, Software development, Intelligent web services, Semantic web},
abstract = {Current urban and district energy management systems lack a common semantic referential for effectively interrelating intelligent sensing, data models and energy models with visualization, analysis and decision support tools. This paper describes the structure, as well as the rationale that led to this structure, of an ontology that captures the real-world concepts of a district energy system, such as a district heating and cooling system. This ontology (called ee-district ontology) is intended to support knowledge provision that can play the role of an intermediate layer between high-level energy management software applications and local monitoring and control software components. In order to achieve that goal, the authors propose to encapsulate queries to the ontology in a scalable web service, which will facilitate the development of interfaces for third-party applications. Considering the size of the ee-district ontology once populated with data from a specific district case study, this could prove to be a repetitive and time-consuming task for the software developer. This paper therefore assesses the feasibility of ontology-driven automation of web service development that is to be a core element in the deployment of heterogeneous district-wide energy management software.}
}
@article{TURCHET2020100548,
title = {The Internet of Musical Things Ontology},
journal = {Journal of Web Semantics},
volume = {60},
pages = {100548},
year = {2020},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100548},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300019},
author = {Luca Turchet and Francesco Antoniazzi and Fabio Viola and Fausto Giunchiglia and György Fazekas},
keywords = {Internet of Musical Things, Smart musical instruments, Semantic audio},
abstract = {The Internet of Musical Things (IoMusT) is an emerging research area consisting of the extension of the Internet of Things paradigm to the music domain. Interoperability represents a central issue within this domain, where heterogeneous objects dedicated to the production and/or reception of musical content (Musical Things) are envisioned to communicate between each other. This paper proposes an ontology for the representation of the knowledge related to IoMusT ecosystems to facilitate interoperability between Musical Things. There was no previous comprehensive data model for the IoMusT domain, however the new ontology relates to existing ontologies, including the SOSA Ontology for the representation of sensors and actuators and the Music Ontology focusing on the production and consumption of music. This paper documents the design of the ontology and its evaluation with respect to specific requirements gathered from an extensive literature review, which was based on scenarios involving IoMusT stakeholders, such as performers and audience members. The IoMusT Ontology can be accessed at: https://w3id.org/iomust#.}
}
@article{BUREK2019784,
title = {A pattern-based approach to a cell tracking ontology.},
journal = {Procedia Computer Science},
volume = {159},
pages = {784-793},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.237},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314243},
author = {Patryk Burek and Nico Scherf and Heinrich Herre},
keywords = {Semantic-Based Systems, Knowledge Representation, Management, Semantic annotation of images, videos, Ontologies},
abstract = {Time-lapse microscopy has thoroughly transformed our understanding of biological motion and developmental dynamics from single cells to entire organisms. The increasing amount of cell tracking data demands the creation of tools to make extracted data searchable and interoperable between experiment and data types. In order to address that problem, the current paper reports on the progress in building the Cell Tracking Ontology (CTO): An ontology framework for describing, querying and integrating data from complementary experimental techniques in the domain of cell tracking experiments. CTO is based on a basic knowledge structure: the cellular genealogy serving as a backbone model to integrate specific biological ontologies into tracking data. As a first step we integrate the Phenotype and Trait Ontology (PATO) as one of the most relevant ontologies to annotate cell tracking experiments. The CTO requires both the integration of data on various levels of generality as well as the proper structuring of collected information. Therefore, in order to provide a sound foundation of the ontology, we have built on the rich body of work on top-level ontologies and established three generic ontology design patterns addressing three modeling challenges for properly representing cellular genealogies, i.e. representing entities existing in time, undergoing changes over time and their organization into more complex structures such as situations.}
}
@article{GONZALEZSENDINO2025109979,
title = {Quantifying algorithmic discrimination: A two-dimensional approach to fairness in artificial intelligence},
journal = {Engineering Applications of Artificial Intelligence},
volume = {144},
pages = {109979},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109979},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624021389},
author = {Rubén González-Sendino and Emilio Serrano and Javier Bajo},
keywords = {Fairness, Equity, Equality, Bias detection},
abstract = {Fairness is a foundational pillar in the development of ethical and responsible artificial intelligence. One of the most pressing issues in this context is discrimination, which occurs when an algorithm displays unequal treatment towards data from different groups without an objective justification for such disparity. Artificial intelligence is increasingly taking on decision-making roles in society at large and in engineering fields in particular. In domains such as autonomous vehicle control systems, where unbiased decision-making can impact safety and trust, and in smart grid management, where equitable energy distribution is crucial, fairness must be a primary consideration. This study introduces a novel metric to measure fairness, consisting of a two-dimensional vector: Equality and Equity. When applied to benchmark datasets, this metric demonstrated superior informativeness by effectively distinguishing equality-related issues from equity-related challenges, surpassing traditional methods like Disparate Impact. Contributions of this work include (1) a pioneering metric for measuring equity; (2) a pure measure of fairness definition that takes into account equity and equality; (3) a vector to guide the mitigation algorithms; and, (4) a Fairness curve where the disparities between groups can be interpreted and explained.}
}
@article{ROCHA2018373,
title = {DKDOnto: An Ontology to Support Software Development with Distributed Teams},
journal = {Procedia Computer Science},
volume = {126},
pages = {373-382},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.271},
url = {https://www.sciencedirect.com/science/article/pii/S187705091831247X},
author = {Rodrigo Rocha and Arthur Araújo and Diogo Cordeiro and Assuero Ximenes and Jean Teixeira and Gabriel Silva and Daliton da Silva and Diogo Espinhara and Renan Fernandes and João Ambrosio and Marcos Duarte and Ryan Azevedo},
keywords = {Distributed Software Development, Ontology, Software Engineering},
abstract = {The Distributed Software Development has become an option for software companies to expand their perspective and work with dispersed teams, exploiting the advantages brought by this approach. However, this way of developing software enables new challenges to arise, such as the inexistence of a formal, normalized model of a project’s data and artifacts accessible to all the individuals involved, which makes it harder for them to communicate, understand each other and what is specified on the project’s artifacts. This paper proposes a knowledge base called DKDOnto, a domain-specific ontology for distributed development, aiming to help projects with a common vocabulary, allowing to assist better the distributed software development process.}
}
@article{RASHID201948,
title = {Completeness and consistency analysis for evolving knowledge bases},
journal = {Journal of Web Semantics},
volume = {54},
pages = {48-71},
year = {2019},
note = {Managing the Evolution and Preservation of the Data Web},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300623},
author = {Mohammad Rifat Ahmmad Rashid and Giuseppe Rizzo and Marco Torchiano and Nandana Mihindukulasooriya and Oscar Corcho and Raúl García-Castro},
keywords = {Quality assessment, Evolution analysis, Validation, Knowledge base, RDF shape, Machine learning},
abstract = {Assessing the quality of an evolving knowledge base is a challenging task as it often requires to identify correct quality assessment procedures. Since data is often derived from autonomous, and increasingly large data sources, it is impractical to manually curate the data, and challenging to continuously and automatically assess their quality. In this paper, we explore two main areas of quality assessment related to evolving knowledge bases: (i) identification of completeness issues using knowledge base evolution analysis, and (ii) identification of consistency issues based on integrity constraints, such as minimum and maximum cardinality, and range constraints. For the completeness analysis, we use data profiling information from consecutive knowledge base releases to estimate completeness measures that allow predicting quality issues. Then, we perform consistency checks to validate the results of the completeness analysis using integrity constraints and learning models. The approach has been tested both quantitatively and qualitatively by using a subset of datasets from both DBpedia and 3cixty knowledge bases. The performance of the approach is evaluated using precision, recall, and F1 score. From completeness analysis, we observe a 94% precision for the English DBpedia KB and 95% precision for the 3cixty Nice KB. We also assessed the performance of our consistency analysis by using five learning models over three sub-tasks, namely minimum cardinality, maximum cardinality, and range constraint. We observed that the best performing model in our experimental setup is Random Forest, reaching an F1 score greater than 90% for minimum and maximum cardinality and 84% for range constraints.}
}
@article{POUR20231415,
title = {Phrase2Onto: A Tool to Support Ontology Extension},
journal = {Procedia Computer Science},
volume = {225},
pages = {1415-1424},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.130},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923012887},
author = {Mina Abd Nikooie Pour and Huanyu Li and Rickard Armiento and Patrick Lambrix},
keywords = {Ontology extension, Concept discovery, phrase-based topic model},
abstract = {Due to importance of data FAIRness (Findable, Accessible, Interoperable, Reusable), ontologies as a means to make data FAIR have attracted more and more attention in different communities and are being used in semantically-enabled applications. However, to obtain good results while using ontologies in these applications, high quality ontologies are needed of which completeness is one of the important aspects. An ontology lacking information can lead to missing results. In this paper we present a tool, Phrase2Onto, that supports users in extending ontologies to make the ontologies more complete. It is particularly suited for ontology extension using a phrase-based topic model approach, but the tool can support any extension approach where a user needs to make decisions regarding the appropriateness of using phrases to define new concepts. We describe the functionality of the tool and a user study using Pizza Ontology. The user study showed a good usability of the system and high task completion. Further, we report on a real application where we extend the Materials Design Ontology.}
}
@article{QIN202396,
title = {A Knowledge Graph-based knowledge representation for adaptive manufacturing control under mass personalization},
journal = {Manufacturing Letters},
volume = {35},
pages = {96-104},
year = {2023},
note = {51st SME North American Manufacturing Research Conference (NAMRC 51)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2023.08.086},
url = {https://www.sciencedirect.com/science/article/pii/S2213846323001438},
author = {Zhaojun Qin and Yuqian Lu},
keywords = {Mass personalization, Smart manufacturing, Self-organizing manufacturing network, Knowledge Graph, Adaptive production scheduling},
abstract = {Mass personalization is an achievable manufacturing paradigm, which requires flexible and responsible manufacturing operations in response to dynamic batch sizes of personalized products. A Self-Organizing Manufacturing Network (SOMN) has been proposed to achieve mass personalization. A crucial aspect of SOMN is adaptive manufacturing control, and the Knowledge Graph, a powerful tool, has been recognized as a promising solution to enhance manufacturing intelligence. However, the current Knowledge Graph research mainly focuses on the modeling and ontology definition of the manufacturing environment, but neglects the interaction between manufacturing resources, the dynamic features of the manufacturing environment, and the application of the Knowledge Graph towards adaptive manufacturing control. Therefore, this paper proposes a Knowledge Graph-based semantic representation for adaptive manufacturing control under dynamic manufacturing environments. The proposed approach develops the Knowledge Graph based on historical and real-time scheduling data. Based on the established Knowledge Graph, Multi-Agent Reinforcement Learning has been introduced as an illustrative example of achieving adaptive scheduling control.}
}
@article{JCARLOS2025734,
title = {Semantic Mediation: a literature review on semantic interoperability through ontologies},
journal = {Procedia Computer Science},
volume = {263},
pages = {734-743},
year = {2025},
note = {International Conference on Industry Sciences and Computer Science Innovation (iSCSi’24)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.07.088},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925021386},
author = {Martins {J. Carlos} and Baptista {Ana Alice} and Sousa {Rui M.} and Martins {Paulo J.}},
keywords = {Semantic Mediation, Semantic Interoperability, Ontology, Information Systems, Integration},
abstract = {Organisations operate in increasingly complex and dynamic environments where different Information Systems (IS) must interact efficiently and quickly. The diversity of meanings or interpretations of data in different contexts present in these IS raises problems of semantic heterogeneity, an obstacle to their integration and synchronisation. Through Semantic Mediation, it is possible to achieve the goal of Semantic Interoperability, where different informatic systems can interact and understand each other’s data in an efficient and meaningful way. This can help solve the problem of semantic heterogeneity, using a common ontology for the syntactic and semantic representation of information, and creating a semantic mediator. This study presents a systematic review of the literature on the use of ontologies to achieve semantic interoperability between informatic systems in order to determine the relevance of the Semantic Mediation Model. Databases containing articles published between 2013 and September 2024 were searched. The use of ontologies to facilitate semantic interoperability was present in 66% of the articles in this review. The most important research contexts and relevant case studies were then identified. These case study articles will be included in future research.}
}
@article{ALI202123,
title = {An intelligent healthcare monitoring framework using wearable sensors and social networking data},
journal = {Future Generation Computer Systems},
volume = {114},
pages = {23-43},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.07.047},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1931605X},
author = {Farman Ali and Shaker El-Sappagh and S.M. Riazul Islam and Amjad Ali and Muhammad Attique and Muhammad Imran and Kyung-Sup Kwak},
keywords = {Machine learning, Semantic knowledge, Big data analysis, Healthcare monitoring system, Wearable sensors, Social network analysis},
abstract = {Wearable sensors and social networking platforms play a key role in providing a new method to collect patient data for efficient healthcare monitoring. However, continuous patient monitoring using wearable sensors generates a large amount of healthcare data. In addition, the user-generated healthcare data on social networking sites come in large volumes and are unstructured. The existing healthcare monitoring systems are not efficient at extracting valuable information from sensors and social networking data, and they have difficulty analyzing it effectively. On top of that, the traditional machine learning approaches are not enough to process healthcare big data for abnormality prediction. Therefore, a novel healthcare monitoring framework based on the cloud environment and a big data analytics engine is proposed to precisely store and analyze healthcare data, and to improve the classification accuracy. The proposed big data analytics engine is based on data mining techniques, ontologies, and bidirectional long short-term memory (Bi-LSTM). Data mining techniques efficiently preprocess the healthcare data and reduce the dimensionality of the data. The proposed ontologies provide semantic knowledge about entities and aspects, and their relations in the domains of diabetes and blood pressure (BP). Bi-LSTM correctly classifies the healthcare data to predict drug side effects and abnormal conditions in patients. Also, the proposed system classifies the patients’ health condition using their healthcare data related to diabetes, BP, mental health, and drug reviews. This framework is developed employing the Protégé Web Ontology Language tool with Java. The results show that the proposed model precisely handles heterogeneous data and improves the accuracy of health condition classification and drug side effect predictions.}
}
@article{BUREK20211021,
title = {Overview of GFO 2.0 Functions: An ontology module for representing teleological knowledge},
journal = {Procedia Computer Science},
volume = {192},
pages = {1021-1030},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.105},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015933},
author = {Patryk Burek and Frank Loebe and Heinrich Herre},
keywords = {Ontology, Knowledge Representation, Functional Modeling},
abstract = {Teleological knowledge and functional representation are present in numerous scientific and engineering disciplines and are key aspects addressed by a broad spectrum of design and modeling frameworks. The current paper summarizes the results obtained in the area of functional modeling achieved under the umbrella of the second release of the General Formal Ontology (GFO 2.0), which is a modular top-level ontological framework actively developed by the Onto-Med Research Group. This paper presents and discusses the Function Module of GFO 2.0, which serves three objectives. (1) It supports the representation of teleological components, let it be a function or a goal-oriented process, by a blueprint of structured specifications. (2) It provides a classification of relationships enabling the construction of functional decomposition models. (3) The module enables the description of the elements of a domain in functional terms by means of a family of function ascription constructs, which handle the modeling of not only function-to-object assignments, but also of malfunctions.}
}
@article{COLLINGE2022104391,
title = {BIM-based construction safety risk library},
journal = {Automation in Construction},
volume = {141},
pages = {104391},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104391},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002643},
author = {William H. Collinge and Karim Farghaly and Mojgan Hadi Mosleh and Patrick Manu and Clara Man Cheung and Carlos A. Osorio-Sandoval},
keywords = {Building information modelling, BIM, Design for Safety, Prevention through design, Construction safety, Health and safety, Safety in design, Ontology, Risk scenarios},
abstract = {This paper presents a digital tool and Safety Risk library to assist designers in their health and safety work in BIM digital environments. Addressing an industry need for improved knowledge sharing and collaboration, the BIM Safety Risk library tool aligns with a Prevention through Design (PtD) approach that links safety risks to treatments via different risk scenarios. Motivated by continuing sub-optimal health and safety management processes, the research employs a conceptual framework rooted in construction guidance: structuring data via a 7-stage ontology to improve designer knowledge of issues and give access to an expanding safety knowledge base (the BIM Safety Risk Library). The tool facilitates tacit and explicit knowledge sharing in visual environments, enabling the construction industry to benefit from their health and safety data while providing an interactive learning tool for designers. The structuring of data also opens up possibilities for other digital advances (e.g. via automatic rule checking).}
}
@article{ZAPPATORE20231,
title = {Semantic models for IoT sensing to infer environment–wellness relationships},
journal = {Future Generation Computer Systems},
volume = {140},
pages = {1-17},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003211},
author = {Marco Zappatore and Antonella Longo and Angelo Martella and Beniamino {Di Martino} and Antonio Esposito and Serena Angela Gracco},
keywords = {IoT interoperability, Semantic API, Environmental sensing, Mobile Crowd Sensing, Ontology patterns},
abstract = {Every time an Internet of Things (IoT) solution is deployed, every time a smartphone owner connects her/his wireless device to a wearable activity-tracker, every time groups of citizens use geo-mapping applications to move around the city, choosing the least crowded path, data are produced and information have to be exchanged appropriately via APIs. Even if novel added-value IoT-based applications appear on the market with increasing speed, true semantic interoperability is far from being achieved, thus limiting the large-scale exploitation, the scalability and the time-to-market of novel apps. Currently, connecting different data prosumers with multiple data sources is still hampered by the lack of standardized and sustainable solutions, especially due to the significant heterogeneity of IoT platforms. In such a landscape, ontologies come to the rescue, thanks to their formal semantics, knowledge representation formats, and shared vocabularies. In this paper we examine, from an ontological perspective, how to describe environmental sensing and wellness monitoring, two of the most popular application cases of Mobile Crowd Sensing (MCS) and IoT, respectively. To this purpose, an ontology of sensor-agnostic APIs is proposed, along with a set of MCS-dedicated ontology modules (and the supporting platform), leveraging on standard and reusable domain ontologies. Moreover, it will be shown how to properly combine the proposed ontologies in order to support complex functionalities based on inference rules addressing the environment–wellness relationships. Finally, specific semantic modeling patterns suitable for typical IoT and MCS scenarios will be discussed.}
}
@article{TURKI2024e38448,
title = {A framework for integrating biomedical knowledge in Wikidata with open biological and biomedical ontologies and MeSH keywords},
journal = {Heliyon},
volume = {10},
number = {19},
pages = {e38448},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e38448},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024144799},
author = {Houcemeddine Turki and Khalil Chebil and Bonaventure F.P. Dossou and Chris Chinenye Emezue and Abraham Toluwase Owodunni and Mohamed Ali {Hadj Taieb} and Mohamed {Ben Aouicha}},
keywords = {Wikidata, Open biological and biomedical ontologies, MeSH keywords, Biomedical relation identification, Crowdsourcing, PubMed},
abstract = {This study presents a comprehensive framework to enhance Wikidata as an open and collaborative knowledge graph by integrating Open Biological and Biomedical Ontologies (OBO) and Medical Subject Headings (MeSH) keywords from PubMed publications. The primary data sources include OBO ontologies and MeSH keywords, which were collected and classified using SPARQL queries for RDF knowledge graphs. The semantic alignment between OBO ontologies and Wikidata was evaluated, revealing significant gaps and distorted representations that necessitate both automated and manual interventions for improvement. We employed pointwise mutual information to extract biomedical relations among the 5000 most common MeSH keywords in PubMed, achieving an accuracy of 89.40 % for superclass-based classification and 75.32 % for relation type-based classification. Additionally, Integrated Gradients were utilized to refine the classification by removing irrelevant MeSH qualifiers, enhancing overall efficiency. The framework also explored the use of MeSH keywords to identify PubMed reviews supporting unsupported Wikidata relations, finding that 45.8 % of these relations were not present in PubMed, indicating potential inconsistencies in Wikidata. The contributions of this study include improved methodologies for enriching Wikidata with biomedical information, validated semantic alignments, and efficient classification processes. This work enhances the interoperability and multilingual capabilities of biomedical ontologies and demonstrates the critical role of MeSH keywords in verifying semantic relations, thereby contributing to the robustness and accuracy of collaborative biomedical knowledge graphs.}
}
@article{ARVOR2021112615,
title = {Towards user-adaptive remote sensing: Knowledge-driven automatic classification of Sentinel-2 time series},
journal = {Remote Sensing of Environment},
volume = {264},
pages = {112615},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112615},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721003357},
author = {Damien Arvor and Julie Betbeder and Felipe R.G. Daher and Tim Blossier and Renan {Le Roux} and Samuel Corgne and Thomas Corpetti and Vinicius {de Freitas Silgueiro} and Carlos Antonio da {Silva Junior}},
keywords = {Land cover, Sentinel-2, Time series, Knowledge-driven, Ontologies, Amazon},
abstract = {Land cover mapping over large areas is essential to address a wide spectrum of socio-environmental challenges. For this reason, many global or regional land cover products are regularly released to the scientific community. Yet, the remote sensing community has not fully addressed the challenge to extract useful information from vast volumes of satellite data. Especially, major limitations concern the use of inadequate classification schemes and “black box” methods that may not match with end-users conceptualization of geographic features. In this paper, we introduce a knowledge-driven methodological approach to automatically process Sentinel-2 time series in order to produce pre-classifications that can be adapted by end-users to match their requirements. The approach relies on a conceptual framework inspired from ontologies of scientific observation and geographic information to describe the representation of geographic entities in remote sensing images. The implementation consists in a three-stage classification system including an initial stage, a dichotomous stage and a modular stage. At each stage, the system firstly relies on natural language semantic descriptions of time series of spectral signatures before assigning labels of land cover classes. The implementation was tested on 75 time series of Sentinel-2 images (i.e. 2069 images) in the Southern Brazilian Amazon to map natural vegetation and water bodies as required by a local end-user, i.e. a non-governmental organization. The results confirmed the potential of the method to accurately detect water bodies (F-score = 0.874 for bodies larger than 10 m) and map natural vegetation (max F-score = 0.875), yet emphasizing the spatial heterogeneity of accuracy results. In addition, it proved to be efficient to provide rapid estimates of degraded riparian forests at watershed level (R2 = 0.871). Finally, we discuss potential improvements both in the system's implementation, e.g. considering additional characteristics, and in the conceptual framework, e.g. moving from pixel- to object-based image analysis and evolving towards a hybrid system combining data- and knowledge-driven approaches.}
}
@article{SPOLADORE2024109001,
title = {A knowledge-based decision support system to support family doctors in personalizing type-2 diabetes mellitus medical nutrition therapy},
journal = {Computers in Biology and Medicine},
volume = {180},
pages = {109001},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109001},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524010862},
author = {Daniele Spoladore and Francesco Stella and Martina Tosi and Erna Cecilia Lorenzini and Claudio Bettini},
keywords = {Decision support system, Ontology-based system, Clinical decision support, Type-2 diabetes mellitus, Medical nutrition therapy},
abstract = {Background
Type-2 Diabetes Mellitus (T2D) is a growing concern worldwide, and family doctors are called to help diabetic patients manage this chronic disease, also with Medical Nutrition Therapy (MNT). However, MNT for Diabetes is usually standardized, while it would be much more effective if tailored to the patient. There is a gap in patient-tailored MNT which, if addressed, could support family doctors in delivering effective recommendations. In this context, decision support systems (DSSs) are valuable tools for physicians to support MNT for T2D patients – as long as DSSs are transparent to humans in their decision-making process. Indeed, the lack of transparency in data-driven DSS might hinder their adoption in clinical practice, thus leaving family physicians to adopt general nutrition guidelines provided by the national healthcare systems.
Method
This work presents a prototypical ontology-based clinical Decision Support System (OnT2D- DSS) aimed at assisting general practice doctors in managing T2D patients, specifically in creating a tailored dietary plan, leveraging clinical expert knowledge. OnT2D-DSS exploits clinical expert knowledge formalized as a domain ontology to identify a patient's phenotype and potential comorbidities, providing personalized MNT recommendations for macro- and micro-nutrient intake. The system can be accessed via a prototypical interface.
Results
Two preliminary experiments are conducted to assess both the quality and correctness of the inferences provided by the system and the usability and acceptance of the OnT2D-DSS (conducted with nutrition experts and family doctors, respectively).
Conclusions
Overall, the system is deemed accurate by the nutrition experts and valuable by the family doctors, with minor suggestions for future improvements collected during the experiments.}
}
@article{YAN2023106798,
title = {Intelligent predictive maintenance of hydraulic systems based on virtual knowledge graph},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106798},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106798},
url = {https://www.sciencedirect.com/science/article/pii/S095219762300982X},
author = {Wei Yan and Yu Shi and Zengyan Ji and Yuan Sui and Zhenzhen Tian and Wanjing Wang and Qiushi Cao},
keywords = {Industry 4.0, Predictive maintenance, Virtual knowledge graph, Ontology, Ontology-based data access, Hydraulic systems},
abstract = {In the manufacturing industry, a hydraulic system harnesses liquid fluid power to create powerful machines. Under the trend of Industry 4.0, the predictive maintenance of hydraulic systems is transforming to more intelligent and automated approaches that leverage the strong power of artificial intelligence and data science technologies. However, due to the knowledge-intensive and heterogeneous nature of the manufacturing domain, the data and information required for predictive maintenance are normally collected from ubiquitous sensing networks. This leads to the gap between massive heterogeneous data/information resources in hydraulic system components and the limited cognitive ability of system users. Moreover, how to capture and structure useful domain knowledge (in a machine-readable way) for solving domain-specific tasks remains an open challenge for the predictive maintenance of hydraulic systems. To address these challenges, in this paper we propose a virtual knowledge graph-based approach for the digital modeling and intelligent predictive analytics of hydraulic systems. We evaluate the functionalities and effectiveness of the proposed approach on a predictive maintenance task under real-world industrial contexts. Results show that our proposed approach is capable and feasible to be implemented for digital modeling, data access, data integration, and predictive analytics.}
}
@article{ROLDANMOLINA2021101889,
title = {An ontology knowledge inspection methodology for quality assessment and continuous improvement},
journal = {Data & Knowledge Engineering},
volume = {133},
pages = {101889},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101889},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000161},
author = {Gabriela R. Roldán-Molina and David Ruano-Ordás and Vitor Basto-Fernandes and José R. Méndez},
keywords = {Ontology, Ontology fixing, Ontology quality measures, Ontology improvement methodology, Deming cycle},
abstract = {Ontology-learning methods were introduced in the knowledge engineering area to automatically build ontologies from natural language texts related to a domain. Despite the initial appeal of these methods, automatically generated ontologies may have errors, inconsistencies, and a poor design quality, all of which must be manually fixed, in order to maintain the validity and usefulness of automated output. In this work, we propose a methodology to assess ontologies quality (quantitatively and graphically) and to fix ontology inconsistencies minimizing design defects. The proposed methodology is based on the Deming cycle and is grounded on quality standards that proved effective in the software engineering domain and present high potential to be extended to knowledge engineering quality management. This paper demonstrates that software engineering quality assessment approaches and techniques can be successfully extended and applied to the ontology-fixing and quality improvement problem. The proposed methodology was validated in a testing ontology, by ontology design quality comparison between a manually created and automatically generated ontology.}
}
@article{IATRELLIS2018,
title = {A Review on Software Project Management Ontologies},
journal = {International Journal of Information Technology Project Management},
volume = {9},
number = {4},
year = {2018},
issn = {1938-0232},
doi = {https://doi.org/10.4018/IJITPM.2018100104},
url = {https://www.sciencedirect.com/science/article/pii/S1938023218000044},
author = {Omiros Iatrellis and Panos Fitsilis},
keywords = {Ontologies, Project Management, Software Project Management},
abstract = {This article aims to provide the reader with a comprehensive background for understanding current knowledge and research works on ontologies for software project management (SPM). It constitutes a systematic literature review behind key objectives of the potential adoption of ontologies in PM. Ontology development and engineering could facilitate substantially the software development process and improve knowledge management, software and artifacts reusability, internal consistency within project management processes of various phases of software life cycle. The authors examined the literature focusing on software project management ontologies and analyzed the findings of these published papers and categorized them accordingly. They used qualitative methods to evaluate and interpret findings of the collected studies. The literature review, among others, has highlighted lack of standardization in terminology and concepts, lack of systematic domain modeling and use of ontologies mainly in prototype ontology systems that address rather limited aspects of software project management processes.}
}
@article{ZHOU201849,
title = {An ontology framework towards decentralized information management for eco-industrial parks},
journal = {Computers & Chemical Engineering},
volume = {118},
pages = {49-63},
year = {2018},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2018.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0098135418300929},
author = {Li Zhou and Chuan Zhang and Iftekhar A. Karimi and Markus Kraft},
keywords = {Eco-industrial park, Knowledge base, Ontology, Process modelling},
abstract = {In this paper, we develop a skeletal ontology for eco-industrial parks. A top-down conceptual framework including five operating levels (unit operations, processes, plants, industrial resource networks and eco-industrial parks) is employed to guide the design of the ontology structure. The detailed ontological representation of each level is realized through adapting and extending OntoCAPE, an ontology of the chemical engineering domain. Based on the proposed ontology, a framework for distributed information management is proposed for eco-industrial parks. As an example, this ontology is used to create a knowledge base for Jurong Island, an industrial park in Singapore. Its potential uses in supporting process modeling and optimization and facilitating industrial symbiosis are also discussed in the paper.}
}
@article{IAKSCH2019100942,
title = {Method for digital evaluation of existing production systems adequacy to changes in product engineering in the context of the automotive industry},
journal = {Advanced Engineering Informatics},
volume = {42},
pages = {100942},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100942},
url = {https://www.sciencedirect.com/science/article/pii/S1474034618305305},
author = {Jaqueline Sebastiany Iaksch and Milton Borsato},
keywords = {Model-based engineering, Ontology, Product development process, Production systems},
abstract = {Current industry practices during the Product Development Process (PDP) still points to the isolation of knowledge domains even with the increase of digitalization. Considering manufacturing process constrains from the beginning of the PDP avoids problems in later stages during the whole product life cycle. Through the application of concepts of the Digital Thread approach, the opportunity to intelligently integrate knowledge into product development is presented, creating a “digital fabric” capable of directing and supporting all stages of the product life cycle. Through these concepts, this research proposes the elaboration of an ontological model and application method capable of evaluating, in real time, the adequacy of the existing production systems, integrating the project and process information. The methodological framework used for the development of this method was Design Science Research. In this way, six steps were performed: (i) problem identification and motivation; (ii) definition of the objectives of the solution; (iii) artifact design and development; (iv) demonstration; (v) evaluation; and, (vi) results report. Through the description of manufacturing systems, the solution contributes to the digital evaluation and facilitates the decision making regarding productive systems, as well as data recovery, reutilization and management. In order to do an initial framework validation, it was performed the application of adequacy principles of a specific production line in automotive sector. However, this choice of a complex industry sector translates a clear possibility of framework adaptation to another industrial segment.}
}
@article{RODRIGUES201912,
title = {Legal ontologies over time: A systematic mapping study},
journal = {Expert Systems with Applications},
volume = {130},
pages = {12-30},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419302398},
author = {Cleyton Mário de Oliveira Rodrigues and Frederico Luiz Gonçalves de Freitas and Emanoel Francisco Spósito Barreiros and Ryan Ribeiro de Azevedo and Adauto Trigueiro de Almeida Filho},
keywords = {Legal ontology, Systematic mapping study, Legal expert system, Legal theory, Semantic web},
abstract = {Over the last 30 years, AI & Law has provided breakthroughs in studies involving case-based reasoning, rule-based reasoning, information retrieval and, most recently, conceptual models for knowledge representation and reasoning, known as Legal Ontologies. Ontologies have been widely used by legal practitioners, scholars, and lay people in a variety of situations, such as simulating legal actions, semantic search and indexing, and to keep up-to-date with the continual change of laws and regulations. Given the high number of legal ontologies produced, the need to summarize this research realm through a well-defined methodological procedure is urgent need. This study presents the results of a systematic mapping of the literature, aiming at categorizing legal ontologies along certain dimensions, such as purpose, level of generality, underlying legal theories, among other aspects. The reasons to carry out a systematic mapping are twofold: in addition to explaining the maturation of the area over recent decades, it helps to avoid the old problem of reinventing the wheel. Through organizing and classifying what has already been produced, it is possible to realize that the development of legal ontologies can rise to the level of reusability where prefabricated models might be coupled with new and more complex ontologies for practical law.}
}
@article{HOA2025150,
title = {Integration of BIM and GIS for managing infrastructure project: a case study of the 3/2 street in Kien Giang, Vietnam},
journal = {Transportation Research Procedia},
volume = {85},
pages = {150-157},
year = {2025},
note = {TRPRO_SDCAT 2023},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2025.03.145},
url = {https://www.sciencedirect.com/science/article/pii/S2352146525002042},
author = {Trinh Van Hoa and Tri N.M. Nguyen and Le Van Phuc and Ngo Chau Phuong},
keywords = {BIM, GIS, integration, infrastructure, transportation},
abstract = {In the construction field, Building information modeling (BIM) and geographic information systems (GIS) integration have garnered a lot of interest because they can increase efficiency, accuracy, and collaboration among stakeholders. This article explores the application of BIM-GIS integration in transportation infrastructure projects. It examines the benefits, challenges, and potential solutions to implementing this integrated approach. A case study of the 3/2 street in Kien Giang, Vietnam was conducted. The findings show that BIM-GIS integration can improve project planning, design coordination, asset management, and decision-making in the management of infrastructure projects in Kien Giang province in particular and Vietnam in general, from design to operation.}
}
@article{WU2025125981,
title = {A semantic-driven approach for maintenance digitalization in the pharmaceutical industry},
journal = {International Journal of Pharmaceutics},
volume = {683},
pages = {125981},
year = {2025},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2025.125981},
url = {https://www.sciencedirect.com/science/article/pii/S037851732500818X},
author = {Ju Wu and Xiaochen Zheng and Marco Madlena and Dimitris Kiritsis},
keywords = {Semantic technology, Pharma 4.0, Quality 4.0, Zero Defect Manufacturing, Digitalization, Pharmaceutical industry, Maintenance automation},
abstract = {The digital transformation of pharmaceutical industry is a challenging task due to the high complexity of involved elements and the strict regulatory compliance. Maintenance activities in the pharmaceutical industry play an essential role in ensuring product quality and integral functioning of equipment and premises. This paper first identifies the key challenges of digitalization in pharmaceutical industry and creates the corresponding problem space for key involved elements. A semantic-driven digitalization framework is proposed aiming to improve the digital continuity of digital resources and technologies for maintenance activities. This framework aligns with Quality 4.0 principles and supports the industry’s pursuit of zero manufacturing defects. A case study is conducted to verify the feasibility of the proposed framework based on the water sampling activities in Merck Serono facility in Switzerland. A tool-chain is presented to enable the functional modules of the framework. Some of the key functional modules within the framework are implemented and have demonstrated satisfactory performance. As one of the outcomes, a digital sampling assistant with web-based services is created to support the automated workflow of water sampling activities. The implementation result proves the potential of the proposed framework to solve the identified problems of maintenance digitalization in the pharmaceutical industry.}
}
@article{PERKUSICH2020106241,
title = {Intelligent software engineering in the context of agile software development: A systematic literature review},
journal = {Information and Software Technology},
volume = {119},
pages = {106241},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.106241},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919302587},
author = {Mirko Perkusich and Lenardo {Chaves e Silva} and Alexandre Costa and Felipe Ramos and Renata Saraiva and Arthur Freire and Ednaldo Dilorenzo and Emanuel Dantas and Danilo Santos and Kyller Gorgônio and Hyggo Almeida and Angelo Perkusich},
keywords = {Intelligent software engineering, Agile software development, Search-based software engineering, Machine learning, Bayesian networks, Artificial intelligence},
abstract = {CONTEXT: Intelligent Software Engineering (ISE) refers to the application of intelligent techniques to software engineering. We define an “intelligent technique” as a technique that explores data (from digital artifacts or domain experts) for knowledge discovery, reasoning, learning, planning, natural language processing, perception or supporting decision-making. OBJECTIVE: The purpose of this study is to synthesize and analyze the state of the art of the field of applying intelligent techniques to Agile Software Development (ASD). Furthermore, we assess its maturity and identify adoption risks. METHOD: Using a systematic literature review, we identified 104 primary studies, resulting in 93 unique studies. RESULTS: We identified that there is a positive trend in the number of studies applying intelligent techniques to ASD. Also, we determined that reasoning under uncertainty (mainly, Bayesian network), search-based solutions, and machine learning are the most popular intelligent techniques in the context of ASD. In terms of purposes, the most popular ones are effort estimation, requirements prioritization, resource allocation, requirements selection, and requirements management. Furthermore, we discovered that the primary goal of applying intelligent techniques is to support decision making. As a consequence, the adoption risks in terms of the safety of the current solutions are low. Finally, we highlight the trend of using explainable intelligent techniques. CONCLUSION: Overall, although the topic area is up-and-coming, for many areas of application, it is still in its infancy. So, this means that there is a need for more empirical studies, and there are a plethora of new opportunities for researchers.}
}
@article{SZEJKA2024100661,
title = {Knowledge-based expert system to drive an informationally interoperable manufacturing system: An experimental application in the Aerospace Industry},
journal = {Journal of Industrial Information Integration},
volume = {41},
pages = {100661},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100661},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24001055},
author = {Anderson Luis Szejka and Osiris {Canciglieri Junior} and Fernando Mas},
keywords = {Manufacturing systems, Digital transformation, Semantic interoperability, Ontology, Information and knowledge formalization, Semantic rules},
abstract = {The industrial revolutions have challenged organisations to rethink their product design and manufacturing processes, making them faster and more connected to market demands and changes. Digital technologies have emerged with solutions to virtual represent physical objects, processes, systems, or assets to simulate and analyse the impact of manufacturing changes before actual implementation. However, the challenge is to deal with thousands of heterogeneous information sets which must be shared simultaneously by different groups within and across institutional boundaries. Each manufacturing industry has its format and model to represent the product in the development, manufacturing process, material features, etc. In this context, this paper explores a knowledge-based expert system to support the information exchange and inconsistency detection across the manufacturing process, specifically in an experimental application in the Aerospace Industry. The proposed framework was based on knowledge formalisation and semantic rules through ontologies, semantic reconciliation strategies and connectivity interfaces to manage information and knowledge and identify inconsistencies across the manufacturing system. It was mainly evaluated across the product and manufacturing design of sheet metal forming aluminium thin wall parts for the aerospace industry. Results demonstrate the capability of the approach to enhance data accuracy, coherence, and efficiency throughout the manufacturing of complex products. However, the solution presents challenges such as interdisciplinary collaboration in product design, specific information requirements for manufacturing planning, and the impact of production planning on manufacturing capacities.}
}
@article{WU2024e35963,
title = {Eliminating ontology contradictions based on the Myerson value},
journal = {Heliyon},
volume = {10},
number = {16},
pages = {e35963},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e35963},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024119949},
author = {Juanyong Wu and Wei Peng},
keywords = {Ontology contradictions, Cooperative game theory, Lexicographic approach, Myerson value, Shapley value},
abstract = {Ontologies play a pivotal role in knowledge representation across various artificial intelligence domains, serving as foundational frameworks for organizing data and concepts. However, the construction and evolution of ontologies frequently lead to logical contradictions that undermine their utility and accuracy. Typically, these contradictions are addressed using an Integer Linear Programming (ILP) model, which traditionally treats all formulas with equal importance, thereby neglecting the distinct impacts of individual formulas within minimal conflict sets. To advance this method, we integrate cooperative game theory to compute the Shapley value for each formula, reflecting its marginal contribution towards resolving logical contradictions. We further construct a graph-based representation of the ontology, enabling the extension of Shapley values to Myerson values. Subsequently, we introduce a Myerson-weighted ILP model that employs a lexicographic approach to eliminate logical contradictions in ontologies. The model ensures the minimum number of formula deletions, subsequently applying Myerson values to guide the prioritization of deletions. Our comparative analysis across 18 ontologies confirms that our approach not only preserves more graph edges than traditional ILP models but also quantifies formula contributions and establishes deletion priorities, presenting a novel approach to ILP-based contradiction resolution.}
}
@article{RODLER201992,
title = {Are query-based ontology debuggers really helping knowledge engineers?},
journal = {Knowledge-Based Systems},
volume = {179},
pages = {92-107},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119302126},
author = {Patrick Rodler and Dietmar Jannach and Konstantin Schekotihin and Philipp Fleiss},
keywords = {Knowledge base debugging, Interactive debugging, User study, Ontologies, Model-based diagnosis, Protégé, Ontology debugging tool},
abstract = {Real-world semantic or knowledge-based systems can become large and complex, e.g., in the biomedical domain. Tool support for the localization and repair of faults within knowledge bases of such systems can therefore be essential for their practical success. Correspondingly, a number of knowledge base debugging approaches, in particular for ontology-based systems, were proposed in recent years. Query-based debugging is a comparably recent interactive approach that localizes the true cause of an observed problem by asking knowledge engineers a series of questions. Concrete implementations of this approach exist, such as the OntoDebug plug-in for the ontology editor Protégé. To validate that a newly proposed method is favorable over an existing one, researchers often rely on simulation-based comparisons. Such an evaluation approach however has certain limitations and often cannot fully inform us about a method’s true usefulness. We therefore conducted a range of user studies to assess the practical value of query-based ontology debugging. One main insight from the studies is that the considered interactive approach is indeed more efficient than an alternative algorithmic debugging based on test cases. We also observed that users frequently made errors in the process, which highlights the importance of a careful design of the queries that users need to answer.}
}
@article{JIANG2019104967,
title = {Semantifying formal concept analysis using description logics},
journal = {Knowledge-Based Systems},
volume = {186},
pages = {104967},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.104967},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119303971},
author = {Yuncheng Jiang},
keywords = {Formal concept analysis, Ontologies, Description logics, Semantic operations, Attribute reduction},
abstract = {Formal Concept Analysis (FCA) is a field of applied mathematics with its roots in order theory, in particular the theory of complete lattices. Over the past 20 years, FCA has been widely studied. Description Logics (DLs) are a family of knowledge representation languages which can be used to represent the terminological knowledge of an application domain in a structured and formally well-understood way. Nowadays, properties and semantics of ontology constructs mainly are determined by DLs. The current research progress and the existing problems of FCA are analyzed. In this paper, we semantify FCA with DLs, in other words, we present an extended FCA (i.e., semantic FCA) by using the concepts of DLs to act as the attributes of formal contexts. Furthermore, we semantify the three components (i.e., formal concepts, attribute implications, and concept lattices) of traditional FCA. In addition, we also study the attribute reduction of formal contexts, formal concepts, and concept lattices from a semantics point of view.}
}
@article{DUARTE2021101892,
title = {An ontological analysis of software system anomalies and their associated risks},
journal = {Data & Knowledge Engineering},
volume = {134},
pages = {101892},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101892},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000197},
author = {Bruno Borlini Duarte and Ricardo {de Almeida Falbo} and Giancarlo Guizzardi and Renata Guizzardi and Vítor E. Silva Souza},
keywords = {Software defects, Errors and failures, Ontological foundations of software systems, Conceptual modeling, Methods and methodologies, Software system risk, Unified Foundational Ontology (UFO)},
abstract = {Software systems have an increasing value in our lives, as our society relies on them for the numerous services they provide. However, as our need for larger and more complex software systems grows, the risks involved in their operation also grows, with possible consequences in terms of significant material and social losses. The rational management of software defects and possible failures is a fundamental requirement for a mature software industry. Standards, professional guides and capability models directly emphasize how important it is for an organization to know and to have a well-established history of failures, errors and defects as they occur in software activities. The problem is that each of these reference models employs its own vocabulary to deal with these phenomena, which can lead to a deficiency in the understanding of these notions by software engineers, causing potential interoperability problems between supporting tools, and, consequently, a poorer adoption of these standards and tools in practice. In this paper, we address this problem of the lack of a consensual conceptualization in this area by proposing two reference conceptual models: an Ontology of Software Defects, Errors and Failures (OSDEF), which takes into account an ecosystem of software artifacts, and a Reference Ontology of Software Systems (ROSS), which characterizes software systems and related artifacts at different levels of abstraction. Moreover, we use OSDEF and ROSS to perform an ontological analysis of the impact of defects, errors and failures of software systems from a risk analysis perspective. To do that, we employee an existing core ontology, namely, the Common Ontology of Value and Risk (COVR). The ontologies presented here are grounded on the Unified Foundational Ontology (UFO) and based on well-known and widely-accepted standards, professional and scientific guides and capability models. We demonstrate how this approach can suitably promote conceptual clarification and terminological harmonization in this area.}
}
@article{BAVARESCO2024110310,
title = {An ontology-based framework for worker’s health reasoning enabled by machine learning},
journal = {Computers & Industrial Engineering},
volume = {193},
pages = {110310},
year = {2024},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110310},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224004315},
author = {Rodrigo Bavaresco and Yutian Ren and Jorge Barbosa and G.P. Li},
keywords = {Knowledge representation, Ontology, Deep learning, Reasoning, Occupational health and safety},
abstract = {Reports of fatal and nonfatal workplace injuries depict severe circumstances involving health and safety in industries. This leads to workers staying away from work in U.S. for an average of 12 days in 2020, implicating in managerial, financial, and organizational losses. In this context, Vision-Based Deep Learning (VBDL) and Knowledge Representation and Reasoning (KRR) allow real-time data retrieval of situations along with the semantic modeling and expressivity of the real world to mitigate injuries. This article presents a framework that interoperates vision-based deep learning and ontology reasoning to identify adverse working situations, introducing a novel ontology composed of a holistic perspective of workers’ health and safety. Moreover, the article provides multi-agent framework modeling to orchestrate the components’ interoperability, describing the framework’s architecture and deployment in workrooms. As a result, a practical evaluation over five days produced 8395 axioms constituted by 1210 individuals in the ontology, which allowed a temporal analysis of harmful conditions and their multiple overlapping using SPARQL and reasoning rules, particularly relevant to understanding explanations of overexertion, physical and environmental injuries. Therefore, the proposed ontology-based framework corroborates the long-term support in identifying, assessing, and controlling risks in the industries due to a well-defined knowledge model.}
}
@article{TRAPPEY2023101354,
title = {Patent landscape and key technology interaction roadmap using graph convolutional network – Case of mobile communication technologies beyond 5G},
journal = {Journal of Informetrics},
volume = {17},
number = {1},
pages = {101354},
year = {2023},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2022.101354},
url = {https://www.sciencedirect.com/science/article/pii/S1751157722001079},
author = {Amy J.C. Trappey and Ann Y.E. Wei and Neil K.T. Chen and Kuo-An Li and L.P. Hung and Charles V. Trappey},
keywords = {Tech-mining analysis, Patent analysis, Keyword extraction, Graph convolution network (GCN)},
abstract = {Beyond 5G (B5G) in mobile network technologies is the latest communication technology currently under development. B5G is expected to achieve superior capabilities in ultra-high network transmission speed, low latency, low energy consumption, and high coverage, comparing to current 5G network performance. Although B5G is still in the development and implementation stage, there are many patents and non-patent literature depicting B5G innovative technologies and applications. The landscapes of B5G technologies are great references for governments and industries to understand the advances in mobile communication for R&D strategies. Thus, this research focuses on developing a formal tech-mining workflow integrating semantic-based patent and non-patent literature analysis for ontology building, patent technological topic clustering, and graph convolutional network (GCN) modeling for depicting key technology interactions among clusters of sub-domain topics. This research emphasizes the study of B5G patent landscape and key technology interaction roadmap in comprehensive steps as a valuable reference for B5G mobile network R&D, as well as for conducting tech-mining of other technology domains of interests.}
}
@article{VIKTOROVIC201931,
title = {Semantic web technologies as enablers for truly connected mobility within smart cities},
journal = {Procedia Computer Science},
volume = {151},
pages = {31-36},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919304697},
author = {Miloš Viktorović and Dujuan Yang and Bauke de Vries and Nico Baken},
keywords = {Linked data, Authonomous vehicles, Semantic web, Mobility, Smart Cities, Ontologies},
abstract = {Most car manufacturers predict that in the first half of the next decade there will be fully autonomous vehicles on our roads. Such vehicles would have to communicate in order to mitigate problems caused by single-viewpoint approach. So there are a lot of researches and developments when it comes to communication layer of V2X (Vehicle-to-Everything), but there is still a lot to be done when it comes to data layer of this communication. This is why we propose using Semantic Web Technologies (SWT) to fill in gaps within data layer of V2X communication. By using SWT (Semantic Web Technologies) and Linked data, we plan to interconnect various data sources, in order to provide homogeneous way for connected autonomous vehicles (CAV) to access relevant information. Such information is currently contained in three distinctive type of sources. These are: Geo-stationary Static data sources (Maps, City models), Geostationary Dynamic data sources (IoT devices) and Non-geostationary Dynamic sources (Vehicles). Using SWT, our goal is to develop ontology(s), in such a way that in-vehicle algorithms can extract and process information about environment they are in, while taking into account available network bandwidth.}
}
@article{PEREZ2018239,
title = {A case study on the use of machine learning techniques for supporting technology watch},
journal = {Data & Knowledge Engineering},
volume = {117},
pages = {239-251},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18302106},
author = {Alain Perez and Rosa Basagoiti and Ronny Adalberto Cortez and Felix Larrinaga and Ekaitz Barrasa and Ainara Urrutia},
keywords = {Text mining, Knowledge management applications, Multi-classification, Technology watch automation, Semantic annotations},
abstract = {Technology Watch human agents have to read many documents in order to manually categorize and dispatch them to the correct expert, that will later add valued information to each document. In this two step process, the first one, the categorization of documents, is time consuming and relies on the knowledge of a human categorizer agent. It does not add direct valued information to the process that will be provided in the second step, when the document is revised by the correct expert. This paper proposes Machine Learning tools and techniques to learn from the manually pre-categorized data to automatically classify new content. For this work a real industrial context was considered. Text from original documents, text from added value information and Semantic Annotations of those texts were used to generate different models, considering manually pre-established categories. Moreover, three algorithms from different approaches were used to generate the models. Finally, the results obtained were compared to select the best model in terms of accuracy and also on the reduction of the amount of document readings (human workload).}
}
@article{FAILLA2025100820,
title = {Managing lifecycle of product information with an ontology-based knowledge framework},
journal = {Journal of Industrial Information Integration},
volume = {45},
pages = {100820},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100820},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25000445},
author = {Lorenzo Failla and Marco Rossoni and Marco Quirini and Giorgio Colombo},
keywords = {Product Lifecycle Management (PLM), Ontologies, Linked data, Product information management, Product knowledge formalization},
abstract = {The effective management of product information within a formalized, digital and interoperable infrastructure remains a significant gap in realizing the full potential of modern Product Lifecycle Management (PLM) implementations in industrial contexts. While the academic paradigm of PLM has been extensively emphasized in the scientific literature for over two decades as a sustainable company strategy, contemporary PLM implementations prove inadequate in handling the extensive volume and variety of information generated throughout a product’s lifecycle. Starting from a comprehensive overview of the evolution of the PLM paradigm and of its inherent implications, the analysis of the PLM implementation of a big player in engineering and manufacturing of turbomachinery products for Oil & Gas and Energy markets is analyzed, allowing to identify existing major general contradictions from an industrial perspective. While it is reaffirmed that the attainment of a neutral, harmonized and universally agreed standardization is nowadays missing and is crucial in the enabling of the PLM paradigm through digital technologies, the present study attempts to demonstrate how a general and agnostic ontology-based framework may straightforwardly fulfill all the identified demands of the PLM paradigm and, therefore, how ontologies play a central role in this field of research by bridging different domains to enable a holistic product conceptualization, lifecycle management, and data interoperability among different digital agents.}
}
@article{MOYANO2023107965,
title = {Semantic interoperability for cultural heritage conservation: Workflow from ontologies to a tool for managing and sharing data},
journal = {Journal of Building Engineering},
volume = {80},
pages = {107965},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.107965},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223021459},
author = {Juan Moyano and Alessandra Pili and Juan E. Nieto-Julián and Stefano {Della Torre} and Silvana Bruno},
keywords = {Cultural heritage, Preventive and planned conservation, Cultural value, BIM, Ontology, IFC, Interoperability, Data interchange, Sustainable process, Free and open-source software},
abstract = {The benefits of using BIM in the construction sector are now widely recognised. From this awareness comes the aspiration to have the same advantages for the Cultural Heritage (CH) sector, to obtain more sustainability in the process. Indeed, research in recent years has been orientated in this direction. Attempts to use BIM tools to CH have shown the limits of the ability to correctly represent and transmit information, especially on cultural value and conservation activities, and on the presence of available objects, due to the lack of specific content in IFC, which means a lack of interoperability. The provision of ontologies is necessary to allow interoperability. Ontologies permit the conceptualization of a representative model of reality by defining classes, attributes, and relationships that describe a domain. The research experiments and proposes a flowchart to connect a specific ontological model for Cultural Heritage in a BIM environment. Using Free and Open-Source Software (FOSS), it is possible to modify, improve, and adapt the functions according to the specific needs of users. In addition, this paper analyses the interconnectivity among three software in the BIM environment: FreeCAD, ArchiCAD and Revit, and the steps for an exchange of information between the geometric model and its semantic properties are established. The decorated umbrella vault of Masegra Castle in Sondrio was selected as a case study to show the application of the method and your tool.}
}
@article{BLUMS2023102148,
title = {Consolidating economic exchange ontologies for financial reporting standard setting},
journal = {Data & Knowledge Engineering},
volume = {145},
pages = {102148},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102148},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23000083},
author = {Ivars Blums and Hans Weigand},
keywords = {Unified Foundational Ontology, Economic exchange, Financial reporting, Conceptual modeling},
abstract = {Several UFO grounded economic exchange ontologies have been developed in the last decade, notably COFRIS, OntoREA, REA2, and ATE. An important question is whether they have reached a level to support financial reporting standard setters. This article first describes the foundational assumptions for exchange conceptualization and consolidates the latest developments in COFRIS - a core ontology for financial reporting information systems, within the most recent versions of the UFO theories and the OntoUML tool. The ontology is evaluated based on competency questions. Furthermore, it is confronted with the conceptual frameworks and standards for accounting and financial reporting and compared with other UFO grounded exchange ontologies. The results show both the maturity level of current exchange ontologies and their applicability to standard setting.}
}
@article{CHAOUCH2021890,
title = {Model for the classification of scheduling problems based on ontology},
journal = {Procedia Computer Science},
volume = {181},
pages = {890-896},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.244},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002878},
author = {Rihab Chaouch and Hanen Ghorbel and Soulef Khalfallah},
keywords = {Scheduling, classification of scheduling problems, ontology, knowledge-base;},
abstract = {In this paper, we propose an ontology-based architecture for the classification of a scheduling problem. In fact, planning and scheduling is a major concern for the production enterprise as it contributes to its profit. Most of the time, researchers propose algorithms that relax some reel situations’ constraints; given that reel shop description are not taken directly from the floor supervisor, or given the complexity of the problem. The objective of ontology for the classification of scheduling problem is to build a bridge between practitioners and researchers in order to identify the reel classification of the problem. Thus, in this paper, we start by presenting a general model for the classification of scheduling problem. On this model, we build specific domain ontology. The specific domain ontology functions as a knowledge base and a data access point for the scheduling problem classification system.}
}
@article{MOHDALI2019191,
title = {A product life cycle ontology for additive manufacturing},
journal = {Computers in Industry},
volume = {105},
pages = {191-203},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518301647},
author = {Munira {Mohd Ali} and Rahul Rai and J. Neil Otte and Barry Smith},
keywords = {, Manufacturing process ontology, Ontology engineering, Product life cycle, Dentistry product manufacturing},
abstract = {The manufacturing industry is evolving rapidly, becoming more complex, more interconnected, and more geographically distributed. Competitive pressure and diversity of consumer demand are driving manufacturing companies to rely more and more on improved knowledge management practices. As a result, multiple software systems are being created to support the integration of data across the product life cycle. Unfortunately, these systems manifest a low degree of interoperability, and this creates problems, for instance when different enterprises or different branches of an enterprise interact. Common ontologies (consensus-based controlled vocabularies) have proved themselves in various domains as a valuable tool for solving such problems. In this paper, we present a consensus-based Additive Manufacturing Ontology (AMO) and illustrate its application in promoting re-usability in the field of dentistry product manufacturing.}
}
@article{ZANGENEH2020101164,
title = {Ontology-based knowledge representation for industrial megaprojects analytics using linked data and the semantic web},
journal = {Advanced Engineering Informatics},
volume = {46},
pages = {101164},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101164},
url = {https://www.sciencedirect.com/science/article/pii/S147403462030135X},
author = {Pouya Zangeneh and Brenda McCabe},
keywords = {Industrial megaprojects, Knowledge representation, Project analytics, Risk analysis, Ontology, Semantic web},
abstract = {The fourth industrial revolution has affected most industries, including construction and those within the delivery chain of megaprojects. These major paradigm shifts, however, did not considerably improve the track record in predicting project outcomes and estimating required resources. One reason is the lack of unified data definitions and expandable knowledge representation across project lifecycle to represent megaprojects for analytics. This paper proposes and evaluates a unified ontology for project knowledge representation that facilitates data collection, processing, and utilization for industrial megaprojects through their lifecycle. The proposed Uniform Project Ontology, or UPonto, provides a data infrastructure for project analytics by enabling logical deductions and inferences, and flexible expansion and partitioning of the data utilizing linked data and the semantic web. The ontology facilitates cost normalization processes, temporal queries, and graph queries using SPARQL, while defining universal semantics for a wide range of project risk factors and characteristics based on comprehensive research of the empirical project risk and success literature augmented by practical considerations gained through expert consultations. UPonto forms the basis for a project knowledge graph to utilize unstructured data; it as well provides semantic definitions for smart IoT agents to consume project risk data and knowledge.}
}
@article{DASILVA20191041,
title = {A conceptual model for quality of experience management to provide context-aware eHealth services},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {1041-1061},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.033},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18314079},
author = {Madalena Pereira {da Silva} and Alexandre Leopoldo Gonçalves and Mário Antônio Ribeiro Dantas},
keywords = {AAL, ICT, QoC, QoE, SDN, UX},
abstract = {The emergence of new Information and Communication Technologies (ICT), such as the Internet of Things (IoT) and the Software-Defined Networking (SDN) approach, together with Autonomic Network Management (ANM), have improved the provision of eHealth services. However, proposals must evolve in order to ensure that eHealth data be transported with quality assurance and delivered with quality information. This perspective substantiates our proposal and the respective validation of a Quality of Experience (QoE) Management model in a Future Internet Architecture. A knowledge representation model of QoE was incorporated into a service delivery platform oriented to user’s needs, thus measuring UX (User Experience). An experimental environment was configured to provide eHealth services from an AAL environment to a healthcare facility. Experiments were performed to verify whether the components of the proposed approach had better quality performance in service provision when compared to the components of the native approach of the SDN controller. Experimental results pointed out that, with a 95% confidence interval, all eHealth services utilizing components of the proposed model showed superior quality when compared with those from the native approach.}
}
@article{SANTOS2024122104,
title = {O3PO: A domain ontology for offshore petroleum production plants},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122104},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122104},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423026064},
author = {Nicolau O. Santos and Fabrício H. Rodrigues and Daniela Schmidt and Régis K. Romeu and Givanildo Nascimento and Mara Abel},
keywords = {Petroleum, Ontology, Production plants, Offshore, Digital twins},
abstract = {The challenge of integrating data from many data sources has persisted as an issue in several industry areas. With the evolution of technology in the upstream petroleum sector (i.e., exploration and production), the petroleum business must contend with technological silos from diverse service providers and suffers from the associated waste of time to locate data and information throughout siloed databases. Based on a thorough compilation of industry-oriented requirements in the form of use cases and competency questions, this document defines a domain ontology for defining entities in offshore petroleum production plants. The objective is to develop a uniform and clearly defined reference vocabulary to aid engineers and information technology professionals in labeling and relating production plant monitoring, simulation measures, and facilities. BFO is the top-level ontology, while GeoCore and a continuing version of the core ontology developed by the Industry Ontology Foundry (IOF) configure the middle-level ontologies. We have studied and combined several other resources to build the ontology, such as glossaries from the industry and related ontologies. The research resulted in a well-founded domain ontology that provides universals, defined classes, and relations that can be useful in several types of applications in the domain. We have demonstrated the utility of the ontology within an actual scenario in an offshore petroleum field in Brazil, where we conceived and applied the domain ontology. This study is a component of the PeTWIN project,22Petwin is an academic cooperation between UFRGS, UiO, and Libra, Equinor, and Shell companies (www.petwin.org). which looks at the best approaches for creating digital twins of offshore petroleum plants.}
}
@article{AMADORDOMINGUEZ2023199,
title = {GEnI: A framework for the generation of explanations and insights of knowledge graph embedding predictions},
journal = {Neurocomputing},
volume = {521},
pages = {199-212},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222015053},
author = {Elvira Amador-Domínguez and Emilio Serrano and Daniel Manrique},
keywords = {Knowledge graph embeddings, Rule-learning models, Explainable AI, Influence function},
abstract = {Knowledge Graphs (KGs) are among the most commonly used knowledge representation paradigms, being at the core of tasks such as question answering or recommendation systems. Knowledge Graph Completion (KGC) is one of the key tasks concerning KGs, where the goal is to extract new elements from the existing information. Different approaches have been proposed through the years to tackle this challenge. Among them, two analogous categories can be distinguished: rule-learning and Knowledge Graph Embeddings (KGE). Different methods have been subsequently proposed to unify both types under a single framework, such that the benefits of both proposals can be exploited. However, most of these methods consider using rule-learning models as a boosting agent for KGE models, but not as an explainability tool. This work presents GEnI11https://github.com/oeg-upm/GEnI, a framework capable of generating insights and explanations for KGE models. GEnI follows a three-phase sequential process, generating a feasible explanation for a given prediction. Possible outcomes are rules, correlations, and influence detection. Moreover, the output is expressed in natural language to further extend the explainability of the proposal. GEnI has been successfully evaluated under three criteria: coherence, the meaningfulness of the output, and reliability. Moreover, it can be used by both translational and bilinear KGE models, offering broad coverage. Furthermore, this work also presents an in-depth review of existing integrative approaches between rule-learning and embedding models, providing a comparative framework between them.}
}
@article{DERAVE2024102293,
title = {A taxonomy and ontology for digital platforms},
journal = {Information Systems},
volume = {120},
pages = {102293},
year = {2024},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102293},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923001291},
author = {Thomas Derave and Frederik Gailly and Tiago Prince Sales and Geert Poels},
keywords = {Digital platform, Ontology, UFO, Taxonomy},
abstract = {In academic literature and in business communication digital platforms are categorized into different types including, but not limited to, multi-sided platform, digital marketplace, on-demand platform and sharing economy platform. Observing the substantial literature on these platform types, both in academia and professional contexts, there is lack of consensus on the definition of these digital platform types. On top of that, there is a lack of knowledge in the literature on the requirements and design of these digital platform types. We address the observed lack of shared conceptualizations by creating a taxonomy for digital platforms, by classifying digital platforms according to their distinctive features. Further, we address the lack of design knowledge with a digital platform reference ontology that describes the construction and operation of the different types of digital platform that are distinguished in the taxonomy. This reference ontology, developed using the Unified Foundational Ontology (UFO) and several of its core ontologies, is organized into different ontology modules, allowing to describe functionalities that are common to any digital platform and more specific functionalities that have their origin in the distinctive features that were used to construct the taxonomy. The taxonomy that we contribute helps to clearly define the different types of digital platform that can be observed today. It also helps analysing further evolutions in the platform domain. The contribution of a well-founded digital platform reference ontology is a step towards a better understanding of digital platform functionality, better communication between stakeholders, and eventually may facilitate future research and development of new types of digital platform.}
}
@article{BURGER2018142,
title = {A framework for semi-automated co-evolution of security knowledge and system models},
journal = {Journal of Systems and Software},
volume = {139},
pages = {142-160},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S016412121830027X},
author = {Jens Bürger and Daniel Strüber and Stefan Gärtner and Thomas Ruhroth and Jan Jürjens and Kurt Schneider},
keywords = {Security requirements, Software evolution, Co-evolution, Software design, Security impact analysis},
abstract = {Security is an important and challenging quality aspect of software-intensive systems, becoming even more demanding regarding long-living systems. Novel attacks and changing laws lead to security issues that did not necessarily rise from a flawed initial design, but also when the system fails to keep up with a changing environment. Thus, security requires maintenance throughout the operation phase. Ongoing adaptations in response to changed security knowledge are inevitable. A necessary prerequisite for such adaptations is a good understanding of the security-relevant parts of the system and the security knowledge. We present a model-based framework for supporting the maintenance of security during the long-term evolution of a software system. It uses ontologies to manage the system-specific and the security knowledge. With model queries, graph transformation and differencing techniques, knowledge changes are analyzed and the system model is adapted. We introduce the novel concept of Security Maintenance Rules to couple the evolution of security knowledge with co-evolutions of the system model. As evaluation, community knowledge about vulnerabilities is used (Common Weakness Enumeration database). We show the applicability of the framework to the iTrust system from the medical care domain and hence show the benefits of supporting co-evolution for maintaining secure systems.}
}
@article{AMER2021102449,
title = {A Multi-Perspective malware detection approach through behavioral fusion of API call sequence},
journal = {Computers & Security},
volume = {110},
pages = {102449},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102449},
url = {https://www.sciencedirect.com/science/article/pii/S016740482100273X},
author = {Eslam Amer and Ivan Zelinka and Shaker El-Sappagh},
keywords = {Malware detection, API call Sequence, Perspective models, Behavioral analysis, Features’ fusion, Sequence reformulation},
abstract = {The widespread development of the malware industry is considered the main threat to our e-society. Therefore, malware analysis should also be enriched with smart heuristic tools that recognize malicious behaviors effectively. Although the generated API calling graph representation for malicious processes encodes worthwhile information about their malicious behavior, it is pragmatically inconvenient to generate a behavior graph for each process. Therefore, we experimented with creating generic behavioral graph models that describe malicious and non-malicious processes. These behavioral models relied on the fusion of statistical, contextual, and graph mining features that capture explicit and implicit relationships between API functions in the calling sequence. Our generated behavioral models proved the behavioral contrast between malicious and non-malicious calling sequences. According to that distinction, we built different relational perspective models that characterize processes’ behaviors. To prove our approach novelty, we experimented with our approach over Windows and Android platforms. Our experimentations demonstrated that our proposed system identified unseen malicious samples with high accuracy with low false-positive. In terms of detection accuracy, our model returns an average accuracy of 0.997 and 0.977 to the unseen Windows and Android malware testing samples, respectively. Moreover, we proposed a new indexing method for APIs based on their contextual similarities. We also suggested a new expressive, a visualized form that renders the API calling sequence. Consequently, we introduced a confidence metric to our model classification decision. Furthermore, we developed a behavioral heuristic that effectively identified malicious API call sequences that were deceptive or mimicry.}
}
@article{SPOLADORE2024108193,
title = {A Knowledge-based Decision Support System for recommending safe recipes to individuals with dysphagia},
journal = {Computers in Biology and Medicine},
volume = {171},
pages = {108193},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108193},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524002774},
author = {Daniele Spoladore and Vera Colombo and Vania Campanella and Christian Lunetta and Marta Mondellini and Atieh Mahroo and Federica Cerri and Marco Sacco},
keywords = {Dysphagia, Ontology-based decision support system, Clinical decision support system, Neuromuscular diseases, Nutrition, Dysphagic patient support},
abstract = {Background
Dysphagia is a disorder that can be associated to several pathological conditions, including neuromuscular diseases, with significant impact on quality of life. Dysphagia often leads to malnutrition, as a consequence of the dietary changes made by patients or their caregivers, who may deliberately decide to reduce or avoid specific food consistencies (because they are not perceived as safe), and the lack of knowledge in how to process foods are critics. Such dietary changes often result in unbalanced nutrients intake, which can have significant consequences for frail patients. This paper presents the development of a prototypical novel ontology-based Decision Support System (DSS) to support neuromuscular patients with dysphagia (following a per-oral nutrition) and their caregivers in preparing nutritionally balanced and safe meals.
Method
After reviewing scientific literature, we developed in collaboration with Ear-Nose-Throat (ENT) specialists, neurologists, and dieticians the DSS formalizes expert knowledge to suggest recipes that are considered safe according to patient's consistency limitations and dysphagia severity and also nutritionally well-balanced.
Results
The prototype can be accessed via digital applications both by physicians to generate and verify the recommendations, and by the patients and their caregivers to follow the step-by-step procedures to autonomously prepare and process one or more recipe. The system is evaluated with 9 clinicians to assess the quality of the DSS's suggested recipes and its acceptance in clinical practice.
Conclusions
Preliminary results suggest a global positive outcome for the recipes inferred by the DSS and a good usability of the system.}
}
@article{DAWOOD2022,
title = {An Ontology Towards Predicting Terrorism Events},
journal = {International Journal of Cyber Warfare and Terrorism},
volume = {12},
number = {1},
year = {2022},
issn = {1947-3435},
doi = {https://doi.org/10.4018/IJCWT.311421},
url = {https://www.sciencedirect.com/science/article/pii/S1947343522000143},
author = {Zubeida Dawood and Carien {Van 't Wout}},
keywords = {Analysis, Database, Ontology, Ontology Development, Ontology Methodology, Ontology-Based Data Access, Semantic Interoperability, Semantic Web, Terror Attacks, Terror Events, Terrorism, Web-Scraper},
abstract = {ABSTRACT
Although there is an increasing amount of information for counter-terrorism operations freely available online, it is a complex process to extract relevant information and to detect useful patterns in the data in order for intelligence functionaries to identify threats and to predict possible terror attacks. Automation is required for intelligent decision-making. To assist with this, in this paper, the researchers propose an ontology-based data access system for counter-terrorism. The system will enable intelligence analysts to perform specialised semantic searches about terrorist events or groups for analysis using an ontology. In this paper, the researchers present the ontology that was created by following an existing methodology for ontology development, and an ontology-based data access system together with all the components used in development (i.e., databases, web-scraper tools, ontology-based data access software, and data sources). Lastly, the ontology is demonstrated by means of use cases with example queries for generating actionable intelligence for operations.}
}
@article{BUGHIO20233471,
title = {Knowledge Organization System for Partial Automation to Improve the Security Posture of IoMT Networks},
journal = {Procedia Computer Science},
volume = {225},
pages = {3471-3478},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.342},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923015004},
author = {Kulsoom Saima Bughio},
keywords = {Internet of Medical Things, Remote Patient Monitoring, Medical Devices, Knowledge Organization System, SWRL Rules, Automated Reasoning, SPARQL},
abstract = {Remote patient monitoring is a healthcare delivery model that uses technology to collect and transmit patient data from a remote location to healthcare providers for analysis and treatment. Remote patient monitoring systems rely on a network infrastructure to gather and transmit data from patients to healthcare providers through a network. While these systems become more prevalent, they may also become targets for cyberattacks. This paper deals with the development of a domain ontology to facilitate partial automation to improve the security posture of IoT networks used in remote patient monitoring. For this purpose, it captures the semantics of the concepts and properties of the main security aspects of IoT medical devices. This is complemented by a comprehensive ruleset, evaluated by using SPARQL queries, and automated reasoning over the aggregated knowledge.}
}
@article{TIELMAN2024102685,
title = {Explainable AI for all - A roadmap for inclusive XAI for people with cognitive disabilities},
journal = {Technology in Society},
volume = {79},
pages = {102685},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102685},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X24002331},
author = {Myrthe L. Tielman and Mari Carmen Suárez-Figueroa and Arne Jönsson and Mark A. Neerincx and Luciano {Cavalcante Siebert}},
keywords = {Explainable AI (XAI), Cognitive disability, Responsible AI},
abstract = {Artificial intelligence (AI) is increasingly prevalent in our daily lives, setting specific requirements for responsible development and deployment: The AI should be explainable and inclusive. Despite substantial research and development investment in explainable AI, there is a lack of effort into making AI explainable and inclusive to people with cognitive disabilities as well. In this paper, we present the first steps towards this research topic. We argue that three main questions guide this research, namely: 1) How explainable should a system be?; 2) What level of understanding can the user reach, and what is the right type of explanation to help them reach this level?; and 3) How can we implement an AI system that can generate the necessary explanations? We present the current state of the art in research on these three topics, the current open questions and the next steps. Finally, we present the challenges specific to bringing these three research topics together, in order to eventually be able to answer the question of how to make AI systems explainable also to people with cognitive disabilities.}
}
@article{KIM2019,
title = {Developing a Physical Activity Ontology to Support the Interoperability of Physical Activity Data},
journal = {Journal of Medical Internet Research},
volume = {21},
number = {4},
year = {2019},
issn = {1438-8871},
doi = {https://doi.org/10.2196/12776},
url = {https://www.sciencedirect.com/science/article/pii/S1438887119002012},
author = {Hyeoneui Kim and Jessica Mentzer and Ricky Taira},
keywords = {exercise, leisure activities, health information interoperability, terminology as topic},
abstract = {Background
Physical activity data provides important information on disease onset, progression, and treatment outcomes. Although analyzing physical activity data in conjunction with other clinical and microbiological data will lead to new insights crucial for improving human health, it has been hampered partly because of the large variations in the way the data are collected and presented.
Objective
The aim of this study was to develop a Physical Activity Ontology (PACO) to support structuring and standardizing heterogeneous descriptions of physical activities.
Methods
We prepared a corpus of 1140 unique sentences collected from various physical activity questionnaires and scales as well as existing standardized terminologies and ontologies. We extracted concepts relevant to physical activity from the corpus using a natural language processing toolkit called Multipurpose Text Processing Tool. The target concepts were formalized into an ontology using Protégé (version 4). Evaluation of PACO was performed to ensure logical and structural consistency as well as adherence to the best practice principles of building an ontology. A use case application of PACO was demonstrated by structuring and standardizing 36 exercise habit statements and then automatically classifying them to a defined class of either sufficiently active or insufficiently active using FaCT++, an ontology reasoner available in Protégé.
Results
PACO was constructed using 268 unique concepts extracted from the questionnaires and assessment scales. PACO contains 225 classes including 9 defined classes, 20 object properties, 1 data property, and 23 instances (excluding 36 exercise statements). The maximum depth of classes is 4, and the maximum number of siblings is 38. The evaluations with ontology auditing tools confirmed that PACO is structurally and logically consistent and satisfies the majority of the best practice rules of ontology authoring. We showed in a small sample of 36 exercise habit statements that we could formally represent them using PACO concepts and object properties. The formal representation was used to infer a patient activity status category of sufficiently active or insufficiently active using the FaCT++ reasoner.
Conclusions
As a first step toward standardizing and structuring heterogeneous descriptions of physical activities for integrative data analyses, PACO was constructed based on the concepts collected from physical activity questionnaires and assessment scales. PACO was evaluated to be structurally consistent and compliant to ontology authoring principles. PACO was also demonstrated to be potentially useful in standardizing heterogeneous physical activity descriptions and classifying them into clinically meaningful categories that reflect adequacy of exercise.}
}
@article{KAMRAN2023100797,
title = {SemanticHadith: An ontology-driven knowledge graph for the hadith corpus},
journal = {Journal of Web Semantics},
volume = {78},
pages = {100797},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100797},
url = {https://www.sciencedirect.com/science/article/pii/S1570826823000264},
author = {Amna Binte Kamran and Bushra Abro and Amna Basharat},
keywords = {Knowledge graph, Linked data, Semantic web, Hadith, Quran, Ontology},
abstract = {Hadith is an essential and much-celebrated resource for the Islamic domain. It is one of the two primary sources of Islamic legislation. The hadith corpus is quite large, consisting of the collection of sayings, actions and silent approval of the Prophet Muhammad. Minimal efforts have been made to date, towards unified semantic modelling, and knowledge representation of the hadith structure for enhanced interlinking and knowledge discovery. This paper presents the design, development and publishing of the hadith corpus as a knowledge graph. First, we design the SemanticHadith ontology to describe and relate core structural concepts from the hadith. We then publish the six prominent hadith collections as an RDF-Based hadith knowledge graph, which is an effort towards making the available hadith both human and machine-readable. This is the first step in the annotation and linking process of the hadith corpus aimed at enabling semantic search capabilities to support scholars, students, and researchers in the creation, evolution, and consultation of a digital representation of Islamic knowledge. The SemanticHadith knowledge graph is freely accessible at http://www.semantichadith.com.}
}
@article{ROMERO2022100409,
title = {A hybrid deep learning and ontology-driven approach to perform business process capability assessment},
journal = {Journal of Industrial Information Integration},
volume = {30},
pages = {100409},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100409},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000760},
author = {Marcelo Romero and Wided Guédria and Hervé Panetto and Béatrix Barafort},
keywords = {Deep learning, Long Short-Term Memory Network, Ontology, Process capability assessment},
abstract = {Enterprises are constantly transforming to adapt to an ever-changing and competitive environment. In this context, assessments allow to understand the state of different organisational aspects before performing transformation activities. One of these aspects is the capability of business processes. Evaluating the quality of business processes is relevant to guide improvement initiatives, considering that the way that processes are designed and executed in organisations has direct impact on the quality of products and services. However, assessments are expensive in terms of resources if they are performed by humans. In this sense, recent trends in Artificial Intelligence provide means to improve process capability assessment through the automation of some of its tasks. Following this line, this work presents a method to perform process capability assessment using raw text as input data with the aid of a smart system, able to reduce the need of human intervention to provide reliable assessment results. For this purpose, we introduce a hybrid approach to perform assessments in enterprises using text data as assessment evidence. The method combines the Long Short-Term Memory Network (LSTM) approach and the use of an Ontology named Process Capability Assessment Ontology (PCAO), which also contains a set of rules to calculate process attribute ratings, capability levels, among other aspects. The approach is grounded on the Smart Assessment Framework, a conceptual model devised to guide the development of intelligent assessments in enterprises. We introduce a demonstration of the assessment of a process based on the management of chemical samples from a research institute.}
}
@article{ELSAPPAGH2021680,
title = {Alzheimer’s disease progression detection model based on an early fusion of cost-effective multimodal data},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {680-699},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20329824},
author = {Shaker El-Sappagh and Hager Saleh and Radhya Sahal and Tamer Abuhmed and S.M. Riazul Islam and Farman Ali and Eslam Amer},
keywords = {Alzheimer disease, Machine learning, Multimodal data analysis, Disease progression detection},
abstract = {Alzheimer’s disease (AD) is a severe neurodegenerative disease. The identification of patients at high risk of conversion from mild cognitive impairment to AD via earlier close monitoring, targeted investigations, and appropriate management is crucial. Recently, several machine learning (ML) algorithms have been used for AD progression detection. Most of these studies only utilized neuroimaging data from baseline visits. However, AD is a complex chronic disease, and usually, a medical expert will analyze the patient’s whole history when making a progression diagnosis. Furthermore, neuroimaging data are always either limited or not available, especially in developing countries, due to their cost. In this paper, we compare the performance of five widely used ML algorithms, namely, the support vector machine, random forest, k-nearest neighbor, logistic regression, and decision tree to predict AD progression with a prediction horizon of 2.5 years. We use 1029 subjects from the Alzheimer’s disease neuroimaging initiative (ADNI) database. In contrast to previous literature, our models are optimized using a collection of cost-effective time-series features including patient’s comorbidities, cognitive scores, medication history, and demographics. Medication and comorbidity text data are semantically prepared. Drug terms are collected and cleaned before encoding using the therapeutic chemical classification (ATC) ontology, and then semantically aggregated to the appropriate level of granularity using ATC to ensure a less sparse dataset. Our experiments assert that the early fusion of comorbidity and medication features with other features reveals significant predictive power with all models. The random forest model achieves the most accurate performance compared to other models. This study is the first of its kind to investigate the role of such multimodal time-series data on AD prediction.}
}
@article{BOUYERBOU2019232,
title = {Geographic ontology for major disasters: Methodology and implementation},
journal = {International Journal of Disaster Risk Reduction},
volume = {34},
pages = {232-242},
year = {2019},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2018.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S221242091830476X},
author = {Hafidha Bouyerbou and Kamal Bechkoum and Richard Lepage},
keywords = {Information retrieval, Major disasters, Ontology, Ontology web language (OWL), Reasoning, Semantics},
abstract = {During a catastrophic event, the International Charter11http://www.disasterscharter.org/. "Space and Major Disasters" is regularly activated and provides the rescue teams damage maps prepared by a photo-interpreter team basing on pre and post-disaster satellite images. A satellite image manual processing must be accomplished in most cases to build these maps, a complex and demanding process. Given the importance of time in such critical situations, automatic or semiautomatic tools are highly recommended. Despite the quick treatment presented by automatic processing, it usually presents a semantic gap issue. Our aim is to express expert knowledge using a well-defined knowledge representation method: ontologies and make semantics explicit in geographic and remote sensing applications by taking the ontology advantages in knowledge representation, expression, and knowledge discovery. This research focuses on the design and implementation of a comprehensive geographic ontology in the case of major disasters, that we named GEO-MD, and illustrates its application in the case of Haiti 2010 earthquake. Results show how the ontology integration reduces the semantic gap and improves the automatic classification accuracy.}
}
@article{MUSIARI2024111776,
title = {Towards computer-aided hygienic design: Definition of a knowledge-based system for food processing equipment},
journal = {Journal of Food Engineering},
volume = {363},
pages = {111776},
year = {2024},
issn = {0260-8774},
doi = {https://doi.org/10.1016/j.jfoodeng.2023.111776},
url = {https://www.sciencedirect.com/science/article/pii/S0260877423003746},
author = {Francesco Musiari and Fabrizio Moroni and Alessandro Pirondi and Claudio Favi},
keywords = {Hygienic design, Knowledge-based system, Design for manufacturing, Food equipment, Food industry, Engineering design},
abstract = {Hygienic design requires the definition of rules allowing the correct development of food processing systems. The knowledge collection in this field would certainly help designers and engineers in developing hygienic-compliant systems. This paper aims to provide a knowledge-based (KB) system for gathering hygienic design guidelines for the design of food processing machinery and equipment. The KB system is based on a specific ontology that has been used to collect 78 hygienic design rules from different sources. The rules repository can be considered a backbone for the subsequent development of a CAD-based tool for an automatic search and detection of non-compliant design features. Starting with a CAD model, the KB system was used to check the compliance of a fish stick production machinery. Results highlight how the adoption of the KB system in the early design phase would anticipate hygienic design issues avoiding several design reviews.}
}
@article{LORVAOANTUNES2024200366,
title = {Ontology-based BIM-AMS integration in European Highways},
journal = {Intelligent Systems with Applications},
volume = {22},
pages = {200366},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200366},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324000425},
author = {António {Lorvão Antunes} and José Barateiro and Vânia Marecos and Jelena Petrović and Elsa Cardoso},
keywords = {Building Information Modeling (BIM), Decision support, Risk and condition data, Ontology development, Ontology validation},
abstract = {BIM tools enable decision-making during the lifecycle of engineering structures, such as bridges, tunnels, and roads. National Road Authorities use Asset Management Systems (AMS) to manage and monitor operational information of assets from European Highways, including access to sensor and inspection data. Interoperability between BIM and AMS systems is vital for a timely and effective decision-making process during the operational phase of these assets. The European project Connected Data for Effective Collaboration (CoDEC) designed a framework to support the connections between AMS and BIM platforms, using linked data principles. The CoDEC Data Dictionary was developed to provide standard data formats for AMS used by European NRA. This paper presents the design and development of an Engineering Structures ontology used to encode the shared conceptualization provided by the CoDEC Data Dictionary. The ontology is evaluated, validated, and demonstrated as a base for data exchange between BIM and AMS.}
}
@article{SUNG2020103504,
title = {A knowledge-based system to find over-the-counter medicines for self-medication},
journal = {Journal of Biomedical Informatics},
volume = {108},
pages = {103504},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103504},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420301325},
author = {Han-Yu Sung and Yu-Liang Chi},
keywords = {Self-medication, Over-the-counter medicine, Semantic Web, Open data, SPARQL},
abstract = {This study developed a medicine query system based on Semantic Web and open data especially for self-medication users to search over-the-counter (OTC) medicines. Most existing medicine query systems are based on keyword searches. If users are uncertain about the exact search words, these query systems do not offer effective help. Furthermore, most systems provide inadequate explanations of symptoms and ailments for users to use with confidence. To remedy these issues, this study builds a knowledge base to enable inference-based searches and data mashup for integrating information from across the Web. Three components were identified: (1) building an ontology model to describe the relationships between ailments and symptoms; (2) upgrading medicinal product datasets to link them with the ontology model on a semantic level; and (3) developing a data mashup to integrate web resources to help users to find references. Furthermore, the aim was to develop a web-based application that utilizes inference mechanisms to provide users with tools for interactive manipulation. A pilot experiment for skin ailments was implemented to learn the problem-solving skills of the system. Finally, two experts utilized a content validity index to rate a four-dimension 15-item scale. The evaluation results show that experts found the proposed system excellent for content validity.}
}
@article{CIMMINO202354,
title = {A scalable, secure, and semantically interoperable client for cloud-enabled Demand Response},
journal = {Future Generation Computer Systems},
volume = {141},
pages = {54-66},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003648},
author = {Andrea Cimmino and Juan Cano-Benito and Alba Fernández-Izquierdo and Christos Patsonakis and Apostolos C. Tsolakis and Raúl García-Castro and Dimosthenis Ioannidis and Dimitrios Tzovaras},
keywords = {Internet of energy, Demand response, Semantic interoperability},
abstract = {Demand Response (DR) is becoming a cornerstone element in the current energy sector, particularly for the EU energy markets. For this reason, considerable effort has been spent on standardising demand response data models. As a result, there is an ever-growing number of demand response proposals based on these standards. However, these proposals are usually centralised, and those that rely on cloud solutions use the cloud as a centralised data store assuming that the data is already homogenised when stored, i.e. all the data has the same format and model. Nevertheless, in practise, DR proposals rely on several components that provide data in heterogeneous formats and models. Furthermore, the different DR standards define models for different data formats that hinder data exchange between different DR systems. In this article, a generic tool called CIM is presented, which allows existing DR systems to distribute their components in the cloud, providing a solid security and privacy framework for data exchange. In addition, the CIM implements a semantic interoperability layer that is capable of translating data into a normalised form when exchanged so that it can be transparently consumed by DR components. Experiments advocate the CIM as a solution for DR systems to decentralise their architectures and exchange heterogeneous data even with other DR systems that follow different DR standards.}
}
@article{TURCHET2022100687,
title = {The Smart Musical Instruments Ontology},
journal = {Journal of Web Semantics},
volume = {72},
pages = {100687},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100687},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000573},
author = {Luca Turchet and Paolo Bouquet and Andrea Molinari and György Fazekas},
keywords = {Smart Musical Instruments, Internet of Musical Things, Semantic audio},
abstract = {The Smart Musical Instruments (SMIs) are an emerging category of musical instruments that belongs to the wider class of Musical Things within the Internet of Musical Things paradigm. SMIs encompass sensors, actuators, embedded intelligence, and wireless connectivity to local networks and to the Internet. Interoperability represents a key issue within this domain, where heterogeneous SMIs are envisioned to exchange information between each other and a plethora of Musical Things. This paper proposes an ontology for the representation of the knowledge related to SMIs, with the aim of facilitating interoperability between SMIs as well as with other Musical Things interacting with them. There was no previous comprehensive data model for the SMIs domain, however the new ontology relates to existing ontologies, including the SOSA Ontology for the representation of sensors and actuators, the Audio Effects Ontology dealing with the description of digital audio effects, and the IoMusT Ontology for the representation Musical Things and IoMusT ecosystems. This paper documents the design of the ontology and its evaluation with respect to specific requirements gathered from an extensive literature review, which was based on scenarios involving SMIs stakeholders, such as performers and studio producers. The SMI Ontology can be accessed at: https://w3id.org/smi#.}
}
@article{AMADORDOMINGUEZ202185,
title = {An ontology-based deep learning approach for triple classification with out-of-knowledge-base entities},
journal = {Information Sciences},
volume = {564},
pages = {85-102},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.02.018},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521001602},
author = {Elvira Amador-Domínguez and Emilio Serrano and Daniel Manrique and Patrick Hohenecker and Thomas Lukasiewicz},
keywords = {Knowledge graph embeddings, Entity initialization, Knowledge graph completion, Word embeddings, Ontological information},
abstract = {Knowledge graphs (KGs) are one of the most common frameworks for knowledge representation. However, they suffer from a severe scalability problem that hinders their usage. KG embedding aims to provide a solution to this issue. Nonetheless, general approaches are incapable of representing and reasoning about information not previously contained in the graph. This paper proposes to leverage semantic and ontological information for a significant benefit of knowledge graph completion, focusing on triple classification. The goal of this task is to determine whether a given fact holds. Furthermore, this paper also considers the classification of facts that include entities that have not been seen during training, denoted out-of-knowledge-base or OOKB entities. An incremental method is presented, composed of six stages. Although the proposal can be applied to any KG embedding model, this work focuses on its application for semantic matching models, such as ComplEx and DistMult. Compared to other approaches, our proposal is model-agnostic, computationally inexpensive, and does not require retraining. The results show that triple classification accuracy scales up to 15% with the proposed approach, as well as accelerating the convergence of the model to its optimal solution. Furthermore, facts containing OOKB entities can be classified with a reasonable accuracy.}
}
@article{DRAGOS2022593,
title = {Ontology Adaptation for Opinion Mining in French Corpora},
journal = {Procedia Computer Science},
volume = {207},
pages = {593-603},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.114},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922009954},
author = {Valentina Dragos and Adrien Legros},
keywords = {Opinion mining, Social data analysis, Appraisal theory},
abstract = {This paper presents the development of an ontology for opinion mining in French corpora. Since ontology construction from scratch is expensive and time consuming, the model is built by adapting an existing ontology modeling appraisal categories in English. The construction method consists of two main steps: first, the ontology of appraisals is translated in French by using a concept-to-concept approach; then different adaptation strategies are implemented to improve the result of this translation. Adaptation strategies are based on text mining, weakly supervised methods and the use of WordNet to refine the model. The goal of the adaptation phase is to cope with limitations of concept-to-concept translation, to integrate concepts and relations from external corpora and resources, and to build a model that captures various features of opinions. The ontology was designed and build to incorporate linguistic and extra linguistic information on the description of opinions in French. The model was validated by using both expert insights to validate the relevance of concepts and relations and formal criteria to describe the ontology qualities for practical use.}
}
@article{OCHOA2025128792,
title = {I40GO: A global ontology for industry 4.0},
journal = {Expert Systems with Applications},
volume = {294},
pages = {128792},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128792},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425024108},
author = {William Ochoa and Javier Cuenca and Felix Larrinaga and Alain Pérez},
keywords = {Semantic ontology, Ontology reuse, Industry 4.0, Context awareness, Workflow management},
abstract = {Over the last two decades, semantic ontologies have been developed to represent manufacturing data across various domains. These ontologies constitute the knowledge base of manufacturing management systems, which primarily focus on optimizing the manufacturing process and improving its resilience. The ontologies developed in the Industry 4.0 domain are heterogeneous, hindering the interoperability of machines, devices, and applications composing manufacturing systems. Consequently, a demand arises for an ontology that provides common vocabularies to represent the data domains inherent to Industry 4.0. A global Industry 4.0 ontology must be easily reusable in different application contexts. This paper presents I40GO: a global ontology tailored to the Industry 4.0 domain. I40GO structures in layers and modules the knowledge represented in the Industry 4.0 most relevant ontologies. The MODDALS methodology is followed to classify knowledge into different layers. This methodology classifies ontology knowledge into common, variant, and application-specific layers following a similar approach to that of Software Product Lines (SPL). I40GO assists ontology engineers in developing domain-specific ontologies for manufacturing systems and enhances interoperability among applications. This work provides an overview of I40GO, emphasizing its development methodology and its modular and layered structure. Furthermore, it demonstrates the reuse of the I40GO ontology within an Industry 4.0 use case—an architecture for context-aware workflow management.}
}
@article{EIDEN2021690,
title = {Supporting semantic PLM by using a lightweight engineering metadata mapping engine},
journal = {Procedia CIRP},
volume = {100},
pages = {690-695},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.146},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121006211},
author = {Andreas Eiden and Thomas Eickhoff and Jonas Gries and Jens C. Göbel and Thomas Psota},
keywords = {PLM, Data Integration, Data Modeling, Data Mapping, Interoperability, Open Web Standards},
abstract = {In order to handle a high variety of interdisciplinary processes and complex smart products, integration platforms are a useful approach to view and access data all along the product lifecycle, which is stored in different data management solutions like Product Lifecycle Management (PLM), Enterprise Resource Planning (ERP) and authoring systems. Here, the Metadata Repository for Semantic Product Lifecycle Management (SP²IDER) could serve as a supporting integration platform. An additional information layer on top of data source systems like PLM and ERP provides additional information, links data objects from different source systems, and provides access to these data. The SP²IDER platform consists of three basic parts: Connector units to fetch data from the source systems, a core unit with a Service Directory and a Mapping Engine, and a Metadata Store, where information about data objects is stored. This paper focuses on the view inside the Mapping Engine and the Metadata Store. The mapping engine has a three-way approach for the mapping of data objects and data types: The initial manual mapping at the data type level, second a rule-based, and third a machine-learning mapping at the data object level. This paper describes the manual mapping process, how the mapped data objects are stored inside the Metadata Store, and how this leads to a newly formed lightweight data model, that does not need heavyweight ontologies.}
}
@article{BAYOUDHI20214249,
title = {An Overview of Biomedical Ontologies for Pandemics and Infectious Diseases Representation},
journal = {Procedia Computer Science},
volume = {192},
pages = {4249-4258},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.201},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921019402},
author = {Leila Bayoudhi and Najla Sassi and Wassim Jaziri},
keywords = {Pandemics, Infectious diseases, Biomedical, Ontology, Representation},
abstract = {Several infectious diseases and pandemics have so far emerged. Pandemics are by nature rapidly evolving. In this context, COVID-19 cases, seen recently in a growing number of countries around the world, have been increasing exponentially. So, researchers and responsible actors should take quick decisions to mitigate the spread of such diseases. To do so, several computer science solutions, including ontologies, have been proposed to cope with these issues and save humanity. The ontology is the key formalism which allows modelling knowledge along with its semantics in a formal way. Indeed, the ontology provides unambiguous definitions of a discourse’s domain terms in a machine understandable way. Particularly, biomedical ontologies have ever been developed to capture and represent pandemics and infectious diseases. In this context, this paper aims to scrutinize and study these state-of-the-art ontologies.}
}
@article{HOSSEINIGOURABPASI2024109022,
title = {BIM-based automated fault detection and diagnostics of HVAC systems in commercial buildings},
journal = {Journal of Building Engineering},
volume = {87},
pages = {109022},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.109022},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224005904},
author = {Arash {Hosseini Gourabpasi} and Mazdak Nik-Bakht},
keywords = {Digital twin, BIM, HVAC, AFDD, Ontology},
abstract = {In order to meet the growing demand for effective Automated Fault Detection and Diagnostics (AFDD) for HVAC systems, innovative approaches are needed to address limitations in data diversity and access to contextual information. This study introduces a methodology that leverages Building Information Modeling (BIM) to enhance the development of the AFDD model. Feature engineering techniques are utilized to generate dynamic BIM features, compensating for the lack of sensory and contextual data in Building Management Systems (BMS). By integrating AFDD analytics with BIM, a comprehensive digital twin of the facility is created, which enables facility managers to compare, reuse, and develop AFDD models for HVAC systems. The proposed methodology demonstrates the potential of leveraging BIM-based knowledge models to overcome the challenges associated with the limited sensor and contextual information availability by utilizing BIM for feature generation and, conversely, updating the BIM model with AFDD analytics.}
}
@article{TESOLIN2023577,
title = {Enhancing heterogeneous mobile network management based on a well-founded reference ontology},
journal = {Future Generation Computer Systems},
volume = {149},
pages = {577-593},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003084},
author = {Julio Cesar Cardoso Tesolin and André M. Demori and David Fernandes Cruz Moura and Maria Cláudia Cavalcanti},
keywords = {Mobile networks, Seamless handover, Conceptual modeling, Ontology, Semantic decision, Knowledge graph},
abstract = {The fulfillment of the Always Best Connected & Served concept in future mobile wireless networks depends on their ability to maintain a seamless association between the user’s equipment and the network. As a result, many researchers and developers have proposed several decision-support mechanisms to cope with this challenge. One of the promising mechanisms to aid mobile wireless networks in achieving ubiquitous coverage with a seamless connection is semantic reasoning. However, once it relies on ontologies, proper representation of Link and Connection – entities that bind the communication nodes – is paramount. Unfortunately, although several network-related ontologies present these concepts, they are unclear on which entities they bind, nor do they present them simultaneously. Also, the same ambiguity can be perceived in vocabulary recommendations of several telecommunication standard bodies. Our main contribution is the ontological analysis that clarifies the definition of Link and Connection in telecommunication domain. Besides, we analyze their dependencies while refining other concepts such as Medium, Server, and Neighbor. Thus, we provide the foundations for a new network-related ontology to support mobility management in wireless networks.}
}
@article{CAMPOS2020104813,
title = {Finding reusable structured resources for the integration of environmental research data},
journal = {Environmental Modelling & Software},
volume = {133},
pages = {104813},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104813},
url = {https://www.sciencedirect.com/science/article/pii/S1364815219307078},
author = {Patricia M.C. Campos and Cassio C. Reginato and João Paulo A. Almeida and Monalessa P. Barcellos and Ricardo {de Almeida Falbo} and Vítor E. {Silva Souza} and Giancarlo Guizzardi},
keywords = {Data integration, Environmental research data, Knowledge resources, Reuse, Systematic search, Ontology},
abstract = {Successful data integration requires careful examination of data semantics, a task that has often been approached with the use of ontologies. However, there are some barriers to build ontologies for data integration in complex domains such as the environmental one. A relevant problem is the development of new ontologies disregarding previous knowledge resources such as reference models and vocabularies. This paper addresses this challenge by proposing a systematic approach (dubbed CLeAR) for the identification and selection of reusable artifacts for building ontologies with the purpose of research data integration. CLeAR follows some principles of the systematic literature reviews, supporting the search for structured resources in the scientific literature. We apply CLeAR to the environmental domain. A total of 543 publications were surveyed. The results obtained provide a set of 75 structured resources for the environmental domain, evaluated according domain coverage and some quality attributes (e.g., proper documentation, community acceptance).}
}
@article{AHMED2023102428,
title = {Recursive approach to combine expert knowledge and data-driven RSW weldability certification decision making process},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {79},
pages = {102428},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102428},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001132},
author = {Fahim Ahmed and Kyoung-Yun Kim},
keywords = {Resistance spot welding, Expert knowledge integration, Ontological knowledge, Weldability certification, Recursive approach},
abstract = {Data-driven techniques have shown promising results in the analysis and understanding of complex welding processes. Data analytics play a significant role to turn data into valuable insights to assist in the weldability certification decision-making for Resistance Spot Welding (RSW) as well. However, to successfully perform the associated data analytics, domain knowledge is essential to construct more ‘sense-making’ analytics models, as often the models cannot properly capture the nuances of the domain and do not properly indicate the relationship among the RSW concepts and parameters. Thus, machine learning models developed from rough experimental data often do not provide models meaningful and sensible to the domain expert. In this article, we employ a recursive approach between the domain experts and data-driven models so that the knowledge of the domain experts can be integrated into the weldability certification decision-making process. An ontology-based semantic knowledge framework supports this recursive communication while helping the experts to instil more confidence in the developed analytics models. The collaborative and recursive approach implemented in this study helps the domain experts to tap into their domain knowledge and form expert opinions using the formalized semantic RSW concepts and decision rules. The expert opinions are then used to learn new knowledge about the RSW domain and transform the RSW datasets by incorporating significant features that were not included in the earlier models. The transformed datasets help us to develop improved machine learning models, which in turn work as a new source of semantic knowledge, as we have discovered through our pilot implementation.}
}@article{MA2022375,
title = {Accelerating deep neural network filter pruning with mask-aware convolutional computations on modern CPUs},
journal = {Neurocomputing},
volume = {505},
pages = {375-387},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222008669},
author = {Xiu Ma and Guangli Li and Lei Liu and Huaxiao Liu and Xueying Wang},
keywords = {Deep learning systems, Neural network compression, Filter pruning},
abstract = {Filter pruning, a representative model compression technique, has been widely used to compress and accelerate sophisticated deep neural networks on resource-constrained platforms. Nevertheless, most studies focus on reducing the cost of model inference, whereas the heavy burden of the pruning optimization process is neglected. In this paper, we propose MaskACC, a mask-aware convolutional computation method, which accelerates the prevailing mask-based filter pruning process on modern CPU platforms. MaskACC dynamically reorganizes the tensors used in convolutions with the mask information to avoid unnecessary computations, thereby improving the computational efficiency of the pruning process. Evaluation with state-of-the-art neural network models on CPU cloud platforms demonstrates the effectiveness of our method, which achieves up to 1.61× speedup under commonly-used pruning rates, compared to conventional computations.}
}
@article{JARVENPAA2019261,
title = {Implementation of capability matchmaking software facilitating faster production system design and reconfiguration planning},
journal = {Journal of Manufacturing Systems},
volume = {53},
pages = {261-270},
year = {2019},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2019.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0278612519300883},
author = {Eeva Järvenpää and Niko Siltala and Otto Hylli and Minna Lanz},
keywords = {Production system design, Production system reconfiguration, Resource representation, Capability modelling, Capability matchmaking, Matchmaking software, Matchmaking web service},
abstract = {Smart manufacturing calls for rapidly responding production systems which help the manufacturing companies to operate efficiently in a highly dynamic environment. Currently, the system design and reconfiguration planning are manual processes which rely heavily on the designers’ expertise and tacit knowledge to find feasible system configuration solutions by comparing the characteristics of the product to the technical properties of the available resources. Rapid responsiveness requires new computer-aided intelligent design and planning solutions that would reduce the time and effort put into system design, both in brownfield and greenfield scenarios. This article describes the implementation of a capability matchmaking approach and software which automatizes the matchmaking between product requirements and resource capabilities. The interaction of the matchmaking system with external design and planning tools, through its web service interface, is explained and illustrated with a case example. The proposed matchmaking approach supports production system design and reconfiguration planning by providing automatic means for checking if the existing system already fulfils the new product requirements, and/or for finding alternative resources and resource combinations for specific product requirements from large search spaces, e.g. from global resource catalogues.}
}
@article{KANG2020101412,
title = {Understanding and improving ontology reasoning efficiency through learning and ranking},
journal = {Information Systems},
volume = {87},
pages = {101412},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917306476},
author = {Yong-Bin Kang and Shonali Krishnaswamy and Wudhichart Sawangphol and Lianli Gao and Yuan-Fang Li},
keywords = {OWL, Reasoning, Performance prediction, Ontology, Metrics, Learning, Meta-reasoning, Semantic web},
abstract = {Ontologies are the fundamental building blocks of the Semantic Web and Linked Data. Reasoning is critical to ensure the logical consistency of ontologies, and to compute inferred knowledge from an ontology. It has been shown both theoretically and empirically that, despite decades of intensive work on optimising ontology reasoning algorithms, performing core reasoning tasks on large and expressive ontologies is time-consuming and resource-intensive. In this paper, we present the meta-reasoning framework R2O2* to tackle the important problems of understanding the source of TBox reasoning hardness and predicting and optimising TBox reasoning efficiency by exploiting machine learning techniques. R2O2* combines state-of-the-art OWL 2 DL reasoners as well as an efficient OWL 2 EL reasoner as components, and predicts the most efficient one by using an ensemble of robust learning algorithms including XGBoost and Random Forests. A comprehensive evaluation on a large and carefully curated ontology corpus shows that R2O2* outperforms all six component reasoners as well as AutoFolio, a robust and strong algorithm selection system.}
}
@article{NAZ2020106695,
title = {Ontology-driven advanced drug-drug interaction},
journal = {Computers & Electrical Engineering},
volume = {86},
pages = {106695},
year = {2020},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106695},
url = {https://www.sciencedirect.com/science/article/pii/S0045790620305504},
author = {Tabbasum Naz and Muhammad Akhtar and Syed Khuram Shahzad and Maria Fasli and Muhammad Waseem Iqbal and Muhammad Raza Naqvi},
keywords = {Drug-drug interaction, Pharmacy semantics, Drug ontologies, Pharmaceutical informatics},
abstract = {The rapid growth of data in the pharmaceutical area has created new challenges for large-scale data mining like Drug-Drug Interaction (DDI) analysis. To meet these challenges, various types of data related to DDI must be integrated with true semantics. However, the existing tools do not provide automated DDI analysis. Interaction details are not machine readable and pharmacists need to do further processing for its extraction. This research paper proposed an ontology-driven Advanced Drug-Drug Interaction (ADDI) system to assists the physicians and pharmacists to identify the DDI effects. ADDI provides ontological definitions and semantic relations among diseases, drugs, ingredients, action mechanism, physiologic effect, dosage formation, administration methods, DDI mechanism, DDI types (Antagonism, Synergism, Potentiation, and Interaction with metabolism), DDI reactions, their frequency and duration. It can be used as Semantic Information Layer (SIL) to resolve the heterogeneity problem and can play a significant role to remove the barriers for semantic interoperability.}
}
@article{MATOS2022599,
title = {Use of Ontologies in Product Information Management: A Proposal for a Multinational Engineering and Technology Company},
journal = {Procedia Computer Science},
volume = {204},
pages = {599-609},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.073},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922008110},
author = {Alexandra Isabel Matos and Fernando Paulo Belfo},
keywords = {Industry 4.0, Industry 5.0, Product Information Management, PIM, ERP, CMS, e-commerce, ontology, action research},
abstract = {The PIM system importance has increased due to technical sophistication of products, their need of internal management or external publishment. ERP and CCMS systems should be integrated with a PIM system that acts as the “backbone” of product information. This project proposed an ontology-centric solution to manage product information for complex modular systems available in the catalog of one of the largest multinational organizations in engineering and technology sector. Based on action-research methodology, existing taxonomies of online product catalog at ERP and CCMS systems were analyzed, an ontology of updated taxonomies was created using Protégé tool and validated by experts. This solution was anchored in some Industry 4.0 and 5.0 pillars, by formalizing smart manufacturing knowledge in an interoperable way between multiple systems and allowing a tailored customer experience, based on interactive products and hyper customization. The organization can now be more efficient in managing individual or integrated products as complex modular systems and in communicating with customers.}
}
@article{WALOSZEK2020723,
title = {Contextual ontology for tonality assessment},
journal = {Procedia Computer Science},
volume = {176},
pages = {723-732},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.045},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319402},
author = {Wojciech Waloszek and Nina Rizun},
keywords = {knowledge bases, contexts, sentiment classification, topic modelling, hierarchical sentiment dictionary},
abstract = {In the paper we discuss the possibilities of using hierarchical contextual ontologies for supporting sentiment classification tasks. The discussion focuses on two important research hypotheses: (1) whether it is possible to construct such an ontology from a corpus of textual document, and (2) whether it is possible and beneficial to use inferencing from this ontology to support the process of sentiment classification. To support the first hypothesis we present a method of extraction of hierarchy of contexts from a set of textual documents and encoding this hierarchy into a multi-level contextual ontology. To support the second hypothesis, we present a method of reasoning from the ontology, and results of experimental verification, which show that use of this reasoning method can increase the accuracy of sentiment classification for longer text documents.}
}
@article{SANCHEZ2020879,
title = {Semantic-based privacy settings negotiation and management},
journal = {Future Generation Computer Systems},
volume = {111},
pages = {879-898},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.10.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18317035},
author = {Odnan Ref Sanchez and Ilaria Torre and Bart P. Knijnenburg},
keywords = {Privacy Preference Manager, Privacy ontology, Data sharing and permission management, IoT},
abstract = {By 2020, an individual is expected to own an average of 6.58 devices that share and integrate a wealth of personal user data. The management of privacy preferences across these devices is a complex task for which users are ill-equipped, which increases privacy risks. In this paper we propose an approach that exploits Semantic Web (SW) technology to manage the user’s IoT privacy preferences and negotiate the permissions for data sharing with third parties. SW technology comprises a web of data that can be processed by machines through a formal, universally shared representation. In our approach, SW enables a lightweight and interoperable communication between a Personal Data Manager (PDM) and the Third Parties (TPs) that request access to the user’s personal data. The PDM can handle multiple heterogeneous personal IoT devices and manages the negotiation process between the user and the TPs in a way that can relieve users from the burden of specifying their privacy requirement for each TP. The core of the approach is the definition of the Privacy Preference for IoT (PPIoT) Ontology which is based on the Privacy Preference Ontology, the W3C Semantic Sensor Network Ontology, the Fair Information Practices (FIP) principles, and state-of-the-art recommendation techniques for privacy protection in the IoT. This ontology aims to capture the complexity of privacy management in the IoT paradigm in light of the recent General Data Protection Regulation (GDPR) of the European Union. Along with presenting the ontology, in this paper we will provide an example on how to use the PPIoT ontology for the management of privacy preferences in the fitness IoT domain and we will show how the PDM handles the process of negotiation between the user and the TPs. The approach is based on an interactive PPIoT-based Privacy Preference Model (PPM) that meets the requirements of the GDPR to have transparent and simple TP privacy policies. Finally, we will report the results of an evaluation on a mockup fitness app that implements this PPM. The main contributions of this paper are: (i) to propose an ontology for privacy preference in the IoT context, which covers a knowledge gap in existing literature and can be used for IoT privacy management, (ii) to propose an interactive PPIoT-based Privacy Preference Model, which is in accordance with the GDPR objectives.}
}
@article{RICO2019100500,
title = {Evaluating the impact of semantic technologies on bibliographic systems: A user-centred and comparative approach},
journal = {Journal of Web Semantics},
volume = {59},
pages = {100500},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300174},
author = {Mariano Rico and Daniel Vila-Suero and Iuliana Botezan and Asunción Gómez-Pérez},
keywords = {User-centred evaluation, Usability, Cultural heritage, Digital humanities, Linked data, Ontologies},
abstract = {Semantic and linked-data technologies are currently used by several cultural heritage institutions to make their content available through the Web. Although these technologies are heavily oriented towards data reuse and integration, one clear benefit highlighted by recent literature is the enhancement of human cultural consumption and user experience through the development of novel cultural end-user applications like Online Public Access Catalogues (OPACs). However, to the best of our knowledge, studies into the impact of these technologies on end-user applications are scarce. In order to address this lack, we report the results of two within-group user-centred studies of two online bibliographic systems in a realistic setting — using a widely deployed OPAC and its counterpart linked-data based system, datos.bne.es. The results of our first within-group study show that users of the system based on linked data required significantly less time and visited fewer pages to complete a typical search and retrieval activity. Additionally, the results of our user satisfaction tests also provided significantly better results for this new system. These results are consistent with the hypothesis that semantic technologies applied to library catalogues provide an enhancement that helps satisfy users’ information needs.}
}
@article{SORMAZ2019183,
title = {SIMPM – Upper-level ontology for manufacturing process plan network generation},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {55},
pages = {183-198},
year = {2019},
note = {Extended Papers Selected from FAIM2016},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0736584517302119},
author = {Dušan Šormaz and Arkopaul Sarkar},
abstract = {Distributed computer integrated manufacturing is increasingly adopting cloud computing, software-as-a-service (SaaS) and multi-agent systems as steps towards “design anywhere, build anywhere” strategy. In this scenario, ontologies not only serve as common message exchange structure among distributed agents but also provide reasoning capability to extract implicit knowledge from explicit information already stored in the knowledge base. Foundation ontologies (upper-level), comprised of most general concepts of a domain, provide a common semantic structure to the domain-level ontologies, which capture details of multi-disciplinary manufacturing knowledge. In this paper, novel upper-level ontology, called SIMPM (Semantically Integrated Manufacturing Planning Model), is proposed in order to model three fundamental constraints of manufacturing process planning: variety, time, and aggregation. The philosophical underpinning of the proposed ontology – presented as OWL-DL axioms – is derived from a three dimensional planning model, developed during our past research on computer-aided process planning. As part of the evaluation of SIMPM ontology, we first expound on the interoperability issues with other upper-level manufacturing ontologies. Next, we present a case study on process planning for prismatic part design. In this way, we demonstrate how the generic set of proposed axioms may be used to address various manufacturing process planning concerns, such as alternative manufacturing resources, the temporal order among operations and granularity in the details of a process plan.}
}
@article{KUSTER2020102731,
title = {The UDSA ontology: An ontology to support real time urban sustainability assessment},
journal = {Advances in Engineering Software},
volume = {140},
pages = {102731},
year = {2020},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2019.102731},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818311773},
author = {Corentin Kuster and Jean-Laurent Hippolyte and Yacine Rezgui},
abstract = {Urban sustainability assessment frameworks have emerged during the past decade to address holistically the complexity of the urban landscape through a systems approach, factoring in environmental, social and economic requirements. However, the current assessment schemes are (a) static in nature, and as such don't reflect the dynamic and real-time nature of urban artefacts, (b) are not grounded in semantics (e.g. BIM and GIS), and (c) are at best used to assist in regulatory compliance, for instance in energy design, to meet increasingly stringent regulatory requirements. Information and communication technologies provide a new value proposition capitalizing on the Internet of Things (IoT) and semantics to provide real-time insights and inform decision making. Consequently, there is a real need in the field for data models that could facilitate data exchange and handle data heterogeneity. In this study, a semantic data model is considered to support near real-time urban sustainability assessment and enhance the semantics of sensor network data. Based on an extensive review of urban sustainability assessment frameworks and ontology development methodologies, the Urban District Sustainability Assessment (UDSA) ontology has been developed and validated using real data from the site of “The Works”, a newly refurbished neighbourhood in Ebbw Vale, Wales. This novel approach reconciles several domain-specific ontologies within one high-level ontology that can support the creation of real-time urban sustainability assessment software. In addition, this information model is aligned with 29 authoritative urban sustainability assessment frameworks, thus providing a useful resource not only in urban sustainability assessment, but also in the wider smart cities context.}
}
@article{SYKORA2022997,
title = {The power of emotions: Leveraging user generated content for customer experience management},
journal = {Journal of Business Research},
volume = {144},
pages = {997-1006},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.02.048},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322001679},
author = {Martin Sykora and Suzanne Elayan and Ian R. Hodgkinson and Thomas W. Jackson and Andrew West},
keywords = {Customer experience management, Sentiment analysis, Social media, Emotion analytics, Bot automation},
abstract = {Customer experience management (CEM) in the social media age finds itself needing to adapt to a rapidly changing digital environment and hence there is a need for innovative digital data analytical solutions. Drawing on an action case study of a large global automotive manufacturer, this study presents a digital innovation for enhanced emotion analytics on user generated content (UGC) and behaviour (UGB), to improve consumer insights for CEM. The digital innovation captures customer experience in real time, enabling measurement of a wide range of discrete emotions on the studied social media platform, which goes beyond traditional tools that capture positive or negative sentiment only. During the digital intervention, a substantial number of inauthentic and bot like behaviours was revealed, unbeknown to the case organisation. These accounts were found to be posting and amplifying highly emotional and potentially damaging content surrounding the case brand and its products. The study illustrates how emotion in the context of customer experience should go beyond typical categorisations, given the complexity of human emotion, while a distinction between bot and authentic users is imperative for CEM.}
}
@article{GHEDINI20236370,
title = {An ontology to integrate process-based approach in ZDM strategies in a Digital Twin framework},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {6370-6375},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.825},
url = {https://www.sciencedirect.com/science/article/pii/S240589632301203X},
author = {Lorenzo Ghedini and Adalberto Polenghi and Marco Macchi},
keywords = {Zero Defect Manufacturing, Ontology, Digital Twin, Quality Monitoring, Process-oriented approach, Product-oriented approach},
abstract = {The current context of Industry 4.0 is characterized by challenges that were not present in the past like greater variability, higher customization and greater complexity. To address these challenges and the increasing need from companies to focus on sustainability-related issues is necessary to adopt a quality improvement method. In this paper, the method considered is Zero Defect Manufacturing (ZDM) a “tool” which shows considerable potential, but needs some auxiliary technologies to operate. In this regard, the model proposed is an ontology based on a pre-existent ontology. This new ontology is capable of applying Detect and Repair strategies to go with the creation of a Digital Twin of the product. The realized ontology can be used to support decision-making in an industrial context: in fact, it provides to any operator in the production process, an indication about the quality of the product, also advising some corrective actions if needed, like the repair or the disassembly of the product and the subsequent recycling of the good quality components. To obtain this outcome, an analysis of the literature was performed to determine the gaps present in the literature, then an ontology editor allowed the creation of the ontology and, finally, the ontology was validated in the context of Industry 4.0 Laboratory at the Politecnico di Milano. In this environment, the proposed solution was populated with the data coming from the servers, determining the quality of the product as a function of the state of product components and the condition of one of the assets installed in the production line.}
}
@article{KESHAVARZI2023107520,
title = {An ontology-driven framework for knowledge representation of digital extortion attacks},
journal = {Computers in Human Behavior},
volume = {139},
pages = {107520},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2022.107520},
url = {https://www.sciencedirect.com/science/article/pii/S0747563222003405},
author = {Masoudeh Keshavarzi and Hamid Reza Ghaffary},
keywords = {Ransomware, Cyber-ontology, Conceptual modeling, Knowledge base, Knowledge graph, Philosophy of computer science},
abstract = {With the COVID-19 pandemic and the growing influence of the Internet in critical sectors of industry and society, cyberattacks have not only not declined, but have risen sharply. In the meantime, ransomware is at the forefront of the most devastating threats that have launched the lucrative illegal business. Due to the proliferation and variety of ransomware forays, there is a need for a new theory of categories. The intricacy and multiplicity of components involved in digital extortions entails the construction of a knowledge representation system that is able to organize large volumes of information from heterogeneous sources in a formal structured format and infer new knowledge from it. This paper suggests and develops a dedicated ontology of digital blackmails, called Rantology, with a particular focus on ransomware assaults. The logic coded in this ontology allows to assess the maliciousness of programs based on various factors, including called API functions and their behaviors. The proposed framework can be used to facilitate interoperability between cybersecurity experts and knowledge-based systems, and identify sensitive points for surveillance. The evaluation results based on several criteria confirm the adequacy of the suggested ontology in terms of clarity, modularity, consistency, coverage and inheritance richness.}
}
@article{REMY2019929,
title = {Building an integrated enhanced virtual research environment metadata catalogue},
journal = {The Electronic Library},
volume = {37},
number = {6},
pages = {929-951},
year = {2019},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-09-2018-0183},
url = {https://www.sciencedirect.com/science/article/pii/S0264047319000274},
author = {Laurent Remy and Dragan Ivanović and Maria Theodoridou and Athina Kritsotaki and Paul Martin and Daniele Bailo and Manuela Sbarra and Zhiming Zhao and Keith Jeffery},
keywords = {Online catalogs, CERIF, Virtual research environments, Vocabulary homogenization, X3ML tools, Metadata catalog},
abstract = {Purpose
The purpose of this paper is to boost multidisciplinary research by the building of an integrated catalogue or research assets metadata. Such an integrated catalogue should enable researchers to solve problems or analyse phenomena that require a view across several scientific domains.
Design/methodology/approach
There are two main approaches for integrating metadata catalogues provided by different e-science research infrastructures (e-RIs): centralised and distributed. The authors decided to implement a central metadata catalogue that describes, provides access to and records actions on the assets of a number of e-RIs participating in the system. The authors chose the CERIF data model for description of assets available via the integrated catalogue. Analysis of popular metadata formats used in e-RIs has been conducted, and mappings between popular formats and the CERIF data model have been defined using an XML-based tool for description and automatic execution of mappings.
Findings
An integrated catalogue of research assets metadata has been created. Metadata from e-RIs supporting Dublin Core, ISO 19139, DCAT-AP, EPOS-DCAT-AP, OIL-E and CKAN formats can be integrated into the catalogue. Metadata are stored in CERIF RDF in the integrated catalogue. A web portal for searching this catalogue has been implemented.
Research limitations/implications
Only five formats are supported at this moment. However, description of mappings between other source formats and the target CERIF format can be defined in the future using the 3M tool, an XML-based tool for describing X3ML mappings that can then be automatically executed on XML metadata records. The approach and best practices described in this paper can thus be applied in future mappings between other metadata formats.
Practical implications
The integrated catalogue is a part of the eVRE prototype, which is a result of the VRE4EIC H2020 project.
Social implications
The integrated catalogue should boost the performance of multi-disciplinary research; thus it has the potential to enhance the practice of data science and so contribute to an increasingly knowledge-based society.
Originality/value
A novel approach for creation of the integrated catalogue has been defined and implemented. The approach includes definition of mappings between various formats. Defined mappings are effective and shareable.}
}
@article{SAMOURKASIDIS2020105171,
title = {A semantic approach for timeseries data fusion},
journal = {Computers and Electronics in Agriculture},
volume = {169},
pages = {105171},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.105171},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919318514},
author = {Argyrios Samourkasidis and Ioannis N. Athanasiadis},
keywords = {Environmental timeseries, Internet of Things, Legacy data, Semantic heterogeneity, Templates, FAIR data, Reasoning, Interoperability, Data reuse, APSIM, AgMIP, DSSAT, WOFOST},
abstract = {The data deluge following the rise of Internet of Things contributes towards the creation of non-reusable data silos. Especially in the environmental sciences domain, syntactic and semantic heterogeneity hinders data re-usability as most times manual labour and domain expertise is required. Both the different syntaxes under which environmental timeseries are formatted and the implicit semantics which are used to describe them contribute to this end. Usually, the real meaning of data is obscured in a combination of short data labels, titles and various value codes, that require domain or institutional knowledge to decipher. The FAIR data principles for scientific data sharing are stewardship offer a framework based on community-adopted metadata. In this work, we present the Environmental Data Acquisition Module (EDAM) which focuses on data interoperability and reuse, and deals with syntactic and semantic heterogeneity using a template approach. Data curators draft templates to describe in an abstract fashion the syntax of the timeseries datasets they want to acquire or disseminate. They complement each template with a metadata file, which is used to annotate observables and their properties (including physical quantities and units of measurement) with terms from an ontology. EDAM employs a reasoner to infer compatibility among syntactically and semantically heterogeneous datasets, and enables timeseries, format and units of measurement transformation on-the-fly. Our approach utilizes a local ontology to store metadata about datasets, which enables EDAM to acquire and transform datasets which were originally stored with different semantics and syntaxes. We demonstrate EDAM in a case study where we transform meteorological input files of four agricultural models. Our approach, allows to cut across environmental data silos and facilitate timeseries reusability, as it enables users to (a) discover datasets in other formats, (b) transform them and (c) reuse them in their scientific workflows. This directly contributes to the toolshed for FAIR data management in environmental sciences. EDAM implementation has been released under an open-source license.}
}
@article{POLENGHI202155,
title = {Multi-attribute Ontology-based Criticality Analysis of manufacturing assets for maintenance strategies planning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {55-60},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.192},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321009630},
author = {A. Polenghi and I. Roda and M. Macchi and A. Pozzetti},
keywords = {criticality analysis, maintenance, maintenance strategy, ontology, OWL, SWRL},
abstract = {Planning maintenance strategies in advance with respect to the installation and running of manufacturing assets positively affects operational expenditure during their usage. However, the early stages of the asset lifecycle are poor of operational data. Thus, domain knowledge of experts, related to the asset, the process and production requirements, is the primary source to determine which maintenance strategy better fits in a specific context. Hence, ontology-based systems represent a relevant help in this direction. In this work, given the importance of the criticality analysis (CA) for maintenance planning, the CA is analyzed from an ontological perspective to automatically associate a maintenance strategy to the asset under analysis. Moreover, to unveil the power of CA, its multi-attribute nature is considered, including not only availability as guiding criterion, but also quality and energy. The developed ontology-based CA allows to (i) semantically align all involved experts, and (ii) potentiate the analysis through reasoning capabilities. Finally, preliminary results from an industrial case in a food company are shown.}
}
@article{JELIC20256233,
title = {Integrated cloud platform for energy management of self-sustainable island communities},
journal = {Energy Reports},
volume = {13},
pages = {6233-6250},
year = {2025},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2025.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S235248472500280X},
author = {Marko Jelić and Dayanne {Peretti Corrêa} and Dea Jelić and Lazar Berbakov and Daniel Werner and Md Nasimul Islam Maruf and Ignacio Lázaro and Izaskun Fernández and Marcus Keane and Nikola Tomašević},
keywords = {Geographical islands, Sustainable energy communities, Energy management platform, Energy forecasting, Energy optimization, Smart asset control},
abstract = {An integral part of contemporary initiatives striving to promote and create self-sustainable communities has been the integration and efficient management of renewable energy sources, energy storage solutions, and sustainable heating and cooling assets. This paper presents a unique combination of diverse technologies, ranging from established data management solutions like SQL and NoSQL databases to custom semantic solutions and device-specific control adapters powered by the OpenMUC gateway. Additionally, it incorporates analytical machine learning-based forecasting solutions paired with optimization algorithms, working together to enable energy self-sufficiency. The platform constituted by these solutions is subsequently utilized to provide predictive and real-time control of energy assets in various facilities in line with the selected operation strategy. Its application, particularly in terms of effective energy storage utilization and timing of asset activation scheduling, ultimately results in improvements in renewable energy integration and overall increase of energy efficiency in the considered buildings. As demonstrated in real-world use cases tested within prosumer-based energy communities in geographical islands, the application of the platform resulted in tangible modifications to the primary statistical characteristics of electrical energy consumption clearly signified, among others, by a reduction in mean consumption of between 40W and 190W. Through scrutinizing the achieved results, it can be concluded that the platform displays the capabilities to outperform stock control algorithms provided by the inverter vendors. Expanding upon the application limited to the electric assets, use cases in the thermal domain where optimization outputs are utilized for heat pump scheduling were also discussed and presented.}
}
@article{MONTICOLO2020100124,
title = {OCEAN: A multi agent system dedicated to knowledge management},
journal = {Journal of Industrial Information Integration},
volume = {17},
pages = {100124},
year = {2020},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2019.100124},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X19300809},
author = {Davy Monticolo and Inaya Lahoud and Pedro Chavez Barrios},
keywords = {Agent-based modeling, Ontology, Semantic approach, Knowledge management},
abstract = {Emphasis on knowledge and information is one of the challenges of the 21st century to differentiate the intelligent business enterprises. Enterprises have to develop their organization in order to capture, manage, and use information in a context of continually changing technology. Indeed knowledge and information are completely distributed in the information network of the company. In addition, knowledge is by nature, heterogeneous since it is provided from different information sources like the software, the technical report, the meeting statements, etc. We present in this paper the architecture of a multi-agent system, which allows the capitalization of the distributed and heterogeneous knowledge. We then present how the agents help business experts to design ontologies in detailing this problematic and how the agents extract knowledge from different users’ databases by using a semantic approach.}
}
@article{NIU2024106381,
title = {Critical review on data-driven approaches for learning from accidents: Comparative analysis and future research},
journal = {Safety Science},
volume = {171},
pages = {106381},
year = {2024},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2023.106381},
url = {https://www.sciencedirect.com/science/article/pii/S0925753523003235},
author = {Yi Niu and Yunxiao Fan and Xing Ju},
keywords = {Workplace safety, Accident prevention, Machine learning, Data source, Causality},
abstract = {Data-driven intelligent technologies are promoting a disruptive digital transformation of human society. Industrial accident prevention is also amid this change. Although many emerging technologies, such as machine learning (ML), are extensively employed in workplace safety, these approaches need to fit the intended safety purpose of accident analysis, risk assessment, adverse outcome prediction, or anomaly detection. Hence, examining the “real-world” need for accident prevention and the advantages of emerging data-driven methodologies to better integrate them is necessary. This study provides a systematic review to clarify the current research status, existing problems, and future insights into these evolving technologies in accident prevention. We present notable gaps and barriers in data-driven accident prevention by analyzing 194 published studies from four perspectives: Paradigm, Model, Data Source, and Purpose. The results demonstrate (1) lack of a systematic framework to guide the application of Big Data (BD) in the field of safety; (2) few prior studies have considered model interpretability; (3) more proactive data needs to be incorporated into accident analysis; (4) safety-related data and domain knowledge need to be further integrated; (5) some recent data-driven techniques are unexplored in safety science. Further, the future research opportunities are discussed based on these findings. Such review may help clarify the mapping of data-driven tasks to safety goals to accelerate the uptake of data-driven technologies in safety or accident analysis research.}
}
@article{BENAVIDES2018107,
title = {An ontology-based approach to knowledge representation for Computer-Aided Control System Design},
journal = {Data & Knowledge Engineering},
volume = {118},
pages = {107-125},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17305189},
author = {Carmen Benavides and Isaías García and Héctor Alaiz and Luis Quesada},
keywords = {Conceptual modeling, Data and knowledge visualization, Ontologies, Computer-Aided Control System Design},
abstract = {Different approaches have been used in order to represent and build control engineering concepts for the computer. Software applications for these fields are becoming more and more demanding each day, and new representation schemas are continuously being developed. This paper describes a study of the use of knowledge models represented in ontologies for building Computer Aided Control Systems Design (CACSD) tools. The use of this approach allows the construction of formal conceptual structures that can be stated independently of any software application and be used in many different ones. In order to show the advantages of this approach, an ontology and an application have been built for the domain of design of lead/lag controllers with the root locus method, presenting the results and benefits found.}
}
@article{SERRANORUIZ2022150,
title = {Toward smart manufacturing scheduling from an ontological approach of job-shop uncertainty sources},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {150-155},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.185},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322001860},
author = {Julio C. Serrano-Ruiz and Josefa Mula and Raúl Poler},
keywords = {Industry 4.0, smart manufacturing scheduling, job-shop, scheduling, stochastic, disturbance, disruption, zero-defect manufacturing, digital twin, ontological framework},
abstract = {An integral application of the enabling technologies of Industry 4.0 in the job-shop scheduling problem (JSSP) must contemplate the automation and autonomy of the involved decision-making processes as a goal, which is the main purpose of the smart manufacturing scheduling (SMS) paradigm. In a real production context, uncertainty acts as a barrier that hinders this goal being met and, therefore, any SMS model should integrate uncertainty generators in one way or another. This paper proposes an ontological framework that identifies and structures the entities shaping the joint domain formed by the job-shop scheduling process in its itinerary toward the SMS paradigm, the sources of uncertainty that it faces, and the interrelationship type that link these entities. This ontological framework will serve in future research as a conceptual basis to design new quantitative models that, from a holistic perspective, will address the stochasticity of manufacturing environments and incorporate the management of disturbances into the realtime resolution of automatic and autonomous job-shop scheduling.}
}
@article{MARTINEZGONZALEZ201979,
title = {The support of constructs in thesaurus tools from a Semantic Web perspective: Framework to assess standard conformance},
journal = {Computer Standards & Interfaces},
volume = {65},
pages = {79-91},
year = {2019},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0920548918302447},
author = {M. Mercedes Martínez-González and María-Luisa Alvite-Díez},
keywords = {ISO 2788, ISO 25964, Semantic Web, SKOS, Software conformance, Standard conformance, Thesaurus constructs, Thesaurus standards, Thesaurus tools},
abstract = {Thesauri are conceptual tools useful to achieve semantic interoperability and reusability, which are relevant goals in the Semantic Web. Thesaurus standards establish, among other issues, the constructs that can appear in a thesaurus. The ISO 25964 standard for thesauri, which supersedes ISO 2788, is the evolution of the ISO thesauri standard to a conceptual approach closer to the Semantic Web. However, it appeared when SKOS -the W3C Recommendation- was already consolidated as the standard for KOS (Knowledge Organization System) representation in the Semantic Web, including thesauri. The evolution from ISO 2788 to ISO 25964, and the relationships between constructs in ISO 2788/ISO 25964 and SKOS, are studied in this paper. From the analysis of this comparison, a methodological framework, that focuses on the construct support, is proposed to evaluate the conformance quality of thesaurus management tools. Target readers are professionals in charge of thesauri edition. A Semantic Web perspective is taken to characterize the effect that using SKOS to represent thesauri can have on the results of the assessment. A proof of concept for the model's feasibility was performed on two tools and the analysis of the results of this experimental validation is presented. The conclusions highlight the model's suitability for assessing conformance to the standards concerning support for thesaurus constructs.}
}
@article{IM20211578,
title = {Development of an Ontological Cost Estimating Knowledge Framework for EPC Projects},
journal = {KSCE Journal of Civil Engineering},
volume = {25},
number = {5},
pages = {1578-1591},
year = {2021},
issn = {1226-7988},
doi = {https://doi.org/10.1007/s12205-021-1582-8},
url = {https://www.sciencedirect.com/science/article/pii/S1226798824017094},
author = {Haekyung Im and Minhui Ha and Donghee Kim and Jaehyun Choi},
keywords = {Interoperability, Ontology, Estimation, Cost knowledge structure, Standardization, Cost classification},
abstract = {This research standardizes a knowledge structure for estimation in the construction field by creating a method to enhance cost management efficiency of construction projects while meeting the need for reusability of accumulated construction information. The construction knowledge structure was developed to execute the project cost estimation with an ontological concept. The knowledge framework was defined and relevant examples were addressed to explain the structure. The detailed estimation process and methodology for using standard unit price information was also developed to strengthen cost information interoperability by utilizing standard classification systems, such as MasterFormat, UniFormat, UniClass, and ISO 12006. This concept may be proposed as a method of connecting construction information based on a standard cost classification system in order to improve estimation efficiency by increasing the connectivity of cost information. Ontology can be a powerful tool when used in conjunction with interoperability to manage the massive volume of construction information for cost management. This methodology can expand to integrated management in cost and time to derive additional classes for work breakdown structure (WBS). Thus, ontology may improve the efficiency of cost estimation and systematization by reusing construction information. As a further step, the interoperability method will enhance overall construction project management.}
}
@article{SANFILIPPO2019174,
title = {Editorial: Formal Ontologies meet Industry},
journal = {Procedia Manufacturing},
volume = {28},
pages = {174-176},
year = {2019},
note = {7th International conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.12.028},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918313702},
author = {Emilio Sanfilippo and Walter Terkaj}
}
@article{FARGHALY2024105224,
title = {cSite ontology for production control of construction sites},
journal = {Automation in Construction},
volume = {158},
pages = {105224},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105224},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523004843},
author = {Karim Farghaly and Ranjith Soman and Jennifer Whyte},
keywords = {Interoperability, Ontology, Semantic web, Linked data, Production Control, Construction Project Management},
abstract = {In the realm of construction production control, effective communication across operational levels and the rapid influx of diverse data are essential. Yet, integrating this data faces challenges due to disparate systems and a lack of common terminology, resulting in data silos and hindered interoperability. An ontology-based solution emerges as promising for enhancing interoperability. This research paper introduces the development, implementation, and assessment of the cSite ontology, encompasses several crucial facets necessary for efficient production control such as location, activities, and documents. To evaluate its practicality, a real-case study was conducted, wherein the ontology was employed to answer competency questions through SPARQL queries. Furthermore, interactive dashboards, situated within the construction control rooms, were developed to present the information visually. This paper underscores the transformative potential of integrated and visualised production information in construction projects. Additionally, it illuminates how the cSite ontology can facilitate the development and implementation of construction digital twins.}
}
@article{GUTIERREZ2019381,
title = {Developing an ontology schema for enriching and linking digital media assets},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {381-397},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.06.023},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18323859},
author = {Yoan Gutiérrez and David Tomás and Isabel Moreno},
keywords = {Semantic representation, Ontology, Digital media asset, Entertainment industry},
abstract = {The abundance of digital media information coming from different sources, completely redefines approaches to media content production management and distribution for all contexts (i.e. technical, business and operational). Such content includes descriptive information (i.e. metadata) about an asset (e.g. a movie, song or game), as well as playable media (e.g. audio or video files). Metadata is organised following a variety of inconsistent structures and formats that are supplied by various content providers. Some challenges have been addressed in terms of standardising and enriching media assets metadata from a semantic perspective. Well known examples include Europeana and DBpedia. Nevertheless, due to the ongoing variability and evolution of digital contents, constant support and creation of new semantic representations are necessary. This article presents an ontology schema covering the requirements of users (content providers and content consumers) involved in the overall life cycle of a digital media asset, which has been designed and developed for a real scenario. The construction of this schema has been documented and evaluated following a methodology supported by quantitative and qualitative metrics. As part of the tangible results, the following outcomes were produced: (i) an RDF/XML schema available via Zenodo and GitHub; (ii) competence questions used for validation are published at GitHub; (iii) an exemplary ontology repository; and (iv) CRUD (Create, Read, Update and Delete) technologies for managing semantic repositories based on such schema. These results form an active part of the framework of a European project and other ongoing research initiatives.}
}
@article{WANG2021108823,
title = {Research on intelligent design method of ship multi-deck compartment layout based on improved taboo search genetic algorithm},
journal = {Ocean Engineering},
volume = {225},
pages = {108823},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.108823},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821002584},
author = {Yun-long Wang and Zhang-pan Wu and Guan Guan and Kai Li and Shu-hong Chai},
keywords = {Ship compartment layout, Multi-deck, Intelligent design, Optimization model, Taboo search algorithm, Genetic algorithm},
abstract = {This paper presents an improved taboo search genetic algorithm (ITSGA) for intelligent design of ship multi-deck compartment layout (SMCL). The optimization of ship multi-deck residential compartment layout belongs to the combinatorial optimization problem with various performance constraints which needs to consider the layout of function cabins, deck passages and stairways between decks and so on. In this paper, the optimization model for SMCL is established, which include the layout area model, the relative location model, the absolute location model and the ergonomic model. ITSGA is proposed to improve the local search ability of genetic algorithm (GA) by introducing the neighborhood transformation criterion and Taboo criterion of Taboo search algorithm into GA. Then a new coding method is given according to the characteristics of ship cabins layout problem to avoid the damage to the cabin sequence caused by crossover and mutation operations in GA. During the layout process, the energy method is firstly used to determine the deck layer of various cabins to be arranged, and then the position of deck passages, cabins, and stairways between upper and lower decks are carried out by nested ITSGA. Finally, the results of numerical simulation experiments demonstrate the feasibility and effectiveness of the established method.}
}
@article{SHAHINMOGHADDAM2018620,
title = {CA-FCM: Towards a formal representation of expert’s causal judgements over construction project changes},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {620-638},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S1474034618301642},
author = {Mehrzad Shahinmoghaddam and Ahad Nazari and Mostafa Zandieh},
keywords = {Fuzzy Cognitive Mapping (FCM), Contextual knowledge modelling, Knowledge-based decision support system, Semantic web technologies, Construction project changes, Causal knowledge representation},
abstract = {Aimed at improving the proactive benefits of Fuzzy Cognitive Mapping (FCM) for predicting construction project changes, this paper presents CA-FCM: a Context-aware Fuzzy Cognitive Mapping approach. CA-FCM’s main functionality is to imitate the intuitive causal judgements of project experts over change causation in different contextual settings. Invoking the logical inference capabilities of semantic web tools, a hybrid inference mechanism is embedded within the proposed framework which enables establishing contextual connections between prospective causal factors through a semi-automated process of generating relevant causal statements. Hence, CA-FCM can assist decision-makers with (1) a shared sense-making of the domain concepts which would significantly facilitate the manual construction of FCM scenarios, (2) providing contextualized recommendations of causal information required for developing FCM scenarios, (3) dynamic modelling of causal inferences, imitating expert reasoning on change causation and propagation. Towards providing a detailed delineation of CA-FCM’s effectiveness on providing assistance in planning for project changes, a partial implementation of the proposed framework was conducted within a real case scenario.}
}
@article{PUTNIK2022678,
title = {Engineering is Design and only Design - Part I: The value of making a distinctive sign},
journal = {Procedia CIRP},
volume = {109},
pages = {678-683},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.313},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122007636},
author = {Goran D. Putnik and Zlata Putnik and Pedro Pinheiro and Cátia Alves},
keywords = {Design, engineering, sign, semiotics},
abstract = {The paper addresses the question: what is engineering? We intuitively know engineering applications such as manufacturing, production, industry, management, business. The answer is not consensual because it is not easy. Furthermore, the ontological question brings us to a second question. What distinguishes engineering from other areas? It is the creative ability that distinguishes engineering. And this artificial faculty only exists in Design. Epistemology in science promotes the existence of herds, increasingly specialized groups of knowledge production. Nevertheless, engineers assume themselves as makers, and in the growing diversity promoted by specialization, they will certainly give different answers when asked about their work. We aggregate all of them as sign-makers. Therefore, engineering is Design and only Design. We reject other views. The argument presented on the phenomenological level considers them false. This paper demonstrates that it is mandatory to create a distinctive sign, which places engineering as relevant in organizations. Without the sign described in semiotics, engineering, which could pretend to be everything, becomes trivial.}
}
@article{WESTPHAL2022108233,
title = {Spatial concept learning and inference on geospatial polygon data},
journal = {Knowledge-Based Systems},
volume = {241},
pages = {108233},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108233},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122000673},
author = {Patrick Westphal and Tobias Grubenmann and Diego Collarana and Simon Bin and Lorenz Bühmann and Jens Lehmann},
keywords = {Spatial analytics, Concept learning, Description logics, Spatial knowledge graphs},
abstract = {Geospatial knowledge has always been an essential driver for many societal aspects. This concerns in particular urban planning and urban growth management. To gain insights from geospatial data and guide decisions usually authoritative and open data sources are used, combined with user or citizen sensing data. However, we see a great potential for improving geospatial analytics by combining geospatial data with the rich terminological knowledge, e.g., provided by the Linked Open Data Cloud. Having semantically explicit, integrated geospatial and terminological knowledge, expressed by means of established vocabularies and ontologies, cross-domain spatial analytics can be performed. One analytics technique working on terminological knowledge is inductive concept learning, an approach that learns classifiers expressed as logical concept descriptions. In this paper, we extend inductive concept learning to infer and make use of the spatial context of entities in spatio-terminological data. We propose a formalism for extracting and making spatial relations explicit such that they can be exploited to learn spatial concept descriptions, enabling ‘spatially aware’ concept learning. We further provide an implementation of this formalism and demonstrate its capabilities in different evaluation scenarios.}
}
@article{NOZZA2021102537,
title = {LearningToAdapt with word embeddings: Domain adaptation of Named Entity Recognition systems},
journal = {Information Processing & Management},
volume = {58},
number = {3},
pages = {102537},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102537},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321000455},
author = {Debora Nozza and Pikakshi Manchanda and Elisabetta Fersini and Matteo Palmonari and Enza Messina},
keywords = {Named Entity Recognition, Domain adaptation, Word embeddings},
abstract = {The task of Named Entity Recognition (NER) is aimed at identifying named entities in a given text and classifying them into pre-defined domain entity types such as persons, organizations, locations. Most of the existing NER systems make use of generic entity type classification schemas, however, the comparison and integration of (more or less) different entity types among different NER systems is a complex problem even for human experts. In this paper, we propose a supervised approach called L2AWE (Learning To Adapt with Word Embeddings) which aims at adapting a NER system trained on a source classification schema to a given target one. In particular, we validate the hypothesis that the embedding representation of named entities can improve the semantic meaning of the feature space used to perform the adaptation from a source to a target domain. The results obtained on benchmark datasets of informal text show that L2AWE not only outperforms several state of the art models, but it is also able to tackle errors and uncertainties given by NER systems.}
}
@article{HOLSAPPLE201832,
title = {Business social media analytics: Characterization and conceptual framework},
journal = {Decision Support Systems},
volume = {110},
pages = {32-45},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618300502},
author = {Clyde W. Holsapple and Shih-Hui Hsiao and Ram Pakath},
keywords = {Analytics, Business social media analytics, Conceptual framework, Social media, Social media analytics},
abstract = {A substantial portion of internet usage today involves social media applications. Aside from personal use, given the vast amount of content stored, and rapid diffusion of information, in social media, businesses have begun exploiting social media for competitive advantage. Its popularity has led to the recognition of Social Media Analytics (SMA) as a distinct, albeit formative, sub-field within the Analytics field. Against this backdrop, we examine available characterizations of SMA that collectively identify various considerations of interest. However, their diversity suggests the need for adopting a concise, unifying SMA definition. We present a definition that subsumes salient aspects of existing characterizations and incorporates novel features of interest to Business SMA. Further, we examine available conceptual frameworks for Business SMA and advance a framework that comprehensively models the Business SMA phenomenon. We also conduct a survey of recently published SMA research in the premier, academic Management Information Systems journals and use some of the surveyed papers to validate our framework.}
}
@article{LIM2024,
title = {An Ontology to Bridge the Clinical Management of Patients and Public Health Responses for Strengthening Infectious Disease Surveillance: Design Science Study},
journal = {JMIR Formative Research},
volume = {8},
year = {2024},
issn = {2561-326X},
doi = {https://doi.org/10.2196/53711},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X24005158},
author = {Sachiko Lim and Paul Johannesson},
keywords = {infectious disease, ontology, IoT, infectious disease surveillance, patient monitoring, infectious disease management, risk analysis, early warning, data integration, semantic interoperability, public health},
abstract = {Background
Novel surveillance approaches using digital technologies, including the Internet of Things (IoT), have evolved, enhancing traditional infectious disease surveillance systems by enabling real-time detection of outbreaks and reaching a wider population. However, disparate, heterogenous infectious disease surveillance systems often operate in silos due to a lack of interoperability. As a life-changing clinical use case, the COVID-19 pandemic has manifested that a lack of interoperability can severely inhibit public health responses to emerging infectious diseases. Interoperability is thus critical for building a robust ecosystem of infectious disease surveillance and enhancing preparedness for future outbreaks. The primary enabler for semantic interoperability is ontology.
Objective
This study aims to design the IoT-based management of infectious disease ontology (IoT-MIDO) to enhance data sharing and integration of data collected from IoT-driven patient health monitoring, clinical management of individual patients, and disparate heterogeneous infectious disease surveillance.
Methods
The ontology modeling approach was chosen for its semantic richness in knowledge representation, flexibility, ease of extensibility, and capability for knowledge inference and reasoning. The IoT-MIDO was developed using the basic formal ontology (BFO) as the top-level ontology. We reused the classes from existing BFO-based ontologies as much as possible to maximize the interoperability with other BFO-based ontologies and databases that rely on them. We formulated the competency questions as requirements for the ontology to achieve the intended goals.
Results
We designed an ontology to integrate data from heterogeneous sources, including IoT-driven patient monitoring, clinical management of individual patients, and infectious disease surveillance systems. This integration aims to facilitate the collaboration between clinical care and public health domains. We also demonstrate five use cases using the simplified ontological models to show the potential applications of IoT-MIDO: (1) IoT-driven patient monitoring, risk assessment, early warning, and risk management; (2) clinical management of patients with infectious diseases; (3) epidemic risk analysis for timely response at the public health level; (4) infectious disease surveillance; and (5) transforming patient information into surveillance information.
Conclusions
The development of the IoT-MIDO was driven by competency questions. Being able to answer all the formulated competency questions, we successfully demonstrated that our ontology has the potential to facilitate data sharing and integration for orchestrating IoT-driven patient health monitoring in the context of an infectious disease epidemic, clinical patient management, infectious disease surveillance, and epidemic risk analysis. The novelty and uniqueness of the ontology lie in building a bridge to link IoT-based individual patient monitoring and early warning based on patient risk assessment to infectious disease epidemic surveillance at the public health level. The ontology can also serve as a starting point to enable potential decision support systems, providing actionable insights to support public health organizations and practitioners in making informed decisions in a timely manner.}
}
@article{LIU2022116741,
title = {Applying ontology learning and multi-objective ant colony optimization method for focused crawling to meteorological disasters domain knowledge},
journal = {Expert Systems with Applications},
volume = {198},
pages = {116741},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116741},
url = {https://www.sciencedirect.com/science/article/pii/S095741742200210X},
author = {Jingfa Liu and Yi Dong and Zhaoxia Liu and Duanbing Chen},
keywords = {Focused crawler, Multi-objective ant colony optimization, Ontology, Ontology learning},
abstract = {The focused crawler based on semantic analysis is a research hotspot in the field of information retrieval. The domain ontology is generally applied to construct the topic model of the focused crawler. In order to overcome the limitations of builders' knowledge reserve and subjective consciousness in the process of constructing artificially ontology, a semi-automatic construction method of domain ontology based on ontology learning technology combining the latent Dirichlet allocation and the Apriori algorithm is proposed in this article. When evaluating the relevance between a hyperlink and a specific topic, the joint evaluation method considering both the web text and the link structure is usually used. However, the traditional weighted sum method is difficult to reasonably determine the optimal weights of these evaluating indicators. To solve this problem, a multi-objective optimization model for link evaluation and a subsequent multi-objective ant colony optimization algorithm (MOACO) are proposed. In the MOACO, a method of the nearest farthest candidate solution (NFCS) is combined with the fast non-dominated sorting to select a set of Pareto-optimal hyperlinks and guide the crawlers’ search directions. The experimental results of the focused crawling on the domain knowledge of typhoon disasters and rainstorm disasters prove that the ability of the proposed focused crawlers to retrieve topic-relevant webpages.}
}
@article{SERRANO2019122,
title = {Deep neural network architectures for social services diagnosis in smart cities},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {122-131},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.034},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19301918},
author = {Emilio Serrano and Javier Bajo},
keywords = {Social exclusion, Social services, Deep learning, Data analysis, Machine learning, Data mining},
abstract = {Social services intend to aid disadvantaged, distressed, or vulnerable persons or groups. Machine Learning (ML) and Deep Learning (DL), which are important technologies to leverage Internet of Things and Big Data, have not been considered to support intelligent social services in Smart Cities. Using technology to achieve more responsive, efficient, and proactive social services is a must in Smart Cities because it will lead to a more fair and egalitarian society. This research work contributes with the evaluation of a thousand Neural Networks architectures for the automatic diagnosis of chronic social exclusion. Some of them outperform previous models in quality metrics such as accuracy and F-score. Beyond the improvement in predicting this specific social condition, to the best of the authors’ knowledge, this paper open the research line of applying these methods for the general social services diagnosis in Smart Cities. Finally, the advantages of using the DL paradigm over other ML alternatives in this scope are discussed.}
}
@article{ILIADIS20191021,
title = {The Tower of Babel problem: making data make sense with Basic Formal Ontology},
journal = {Online Information Review},
volume = {43},
number = {6},
pages = {1021-1045},
year = {2019},
issn = {1468-4527},
doi = {https://doi.org/10.1108/OIR-07-2018-0210},
url = {https://www.sciencedirect.com/science/article/pii/S1468452719000325},
author = {Andrew Iliadis},
keywords = {Data ethics, Applied computational ontology, Semantic technology, Social ontology, Tower of Babel problem},
abstract = {Purpose
Applied computational ontologies (ACOs) are increasingly used in data science domains to produce semantic enhancement and interoperability among divergent data. The purpose of this paper is to propose and implement a methodology for researching the sociotechnical dimensions of data-driven ontology work, and to show how applied ontologies are communicatively constituted with ethical implications.
Design/methodology/approach
The underlying idea is to use a data assemblage approach for studying ACOs and the methods they use to add semantic complexity to digital data. The author uses a mixed methods approach, providing an analysis of the widely used Basic Formal Ontology (BFO) through digital methods and visualizations, and presents historical research alongside unstructured interview data with leading experts in BFO development.
Findings
The author found that ACOs are products of communal deliberation and decision making across institutions. While ACOs are beneficial for facilitating semantic data interoperability, ACOs may produce unintended effects when semantically enhancing data about social entities and relations. ACOs can have potentially negative consequences for data subjects. Further critical work is needed for understanding how ACOs are applied in contexts like the semantic web, digital platforms, and topic domains. ACOs do not merely reflect social reality through data but are active actors in the social shaping of data.
Originality/value
The paper presents a new approach for studying ACOs, the social impact of ACO work, and describes methods that may be used to produce further applied ontology studies.}
}
@article{HURTADO2024111230,
title = {e-Science workflow: A semantic approach for airborne pollen prediction},
journal = {Knowledge-Based Systems},
volume = {284},
pages = {111230},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.111230},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123009802},
author = {Sandro Hurtado and María Luisa Antequera-Gómez and Cristóbal Barba-González and Antonio Picornell and Ismael Navas-Delgado},
keywords = {Big data analytics, Semantics, e-Science, Pollen prediction},
abstract = {Allergic rhinitis has become a global health problem in recent decades because airborne pollen is a primary trigger of this respiratory disorder. Moreover, pollinosis can exacerbate the symptoms of asthma and favour respiratory infections. Seasonal pollen trends and climatic circumstances (such as temperature, precipitation, relative humidity, wind speed and direction, and other variables) can impact daily airborne pollen concentrations, influencing local pollen emission and dispersion. Because of that, pollen monitoring and prediction are becoming more relevant to the urban population and scientific interest is put into them. Due to such tasks’ high volume of data, scientists are starting to use computational tools like workflows to automate and speed up the process. Furthermore, using the expert scientific domain is critical for improving the analysis, allowing, among others, a better workflow configuration and data provenance. As semantic web technologies have been revealed as an essential means for knowledge representation, we implemented this workflow information as an ontology using formats like RDF(S) and OWL. Consequently, this paper provides a semantic-enhanced e-Science workflow based on the TITAN framework for pollen forecasting analysis using meteorological data. Furthermore, a catalogue of components is developed on the TITAN framework, which allows the creation of different workflow versions. Two case studies of pollen prediction were developed to test the implementation of the aforementioned methodologies. Both were elaborated with airborne pollen data obtained in the city of Málaga (Spain). Still, one was elaborated for Platanus pollen type (narrow annual main pollination period), while the other was done for Amaranthaceae pollen type (extensive annual main pollination period). The predictions have been conducted using machine and deep learning algorithms like SARIMA or CNN-LSTM that intend to optimise the pollen prediction procedure depending on its stational and seasonal profile.}
}
@article{SOBRAL2020113260,
title = {An Ontology-based approach to Knowledge-assisted Integration and Visualization of Urban Mobility Data},
journal = {Expert Systems with Applications},
volume = {150},
pages = {113260},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113260},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420300853},
author = {Thiago Sobral and Teresa Galvão and José Borges},
keywords = {Data integration, Data visualization, Urban mobility, Semantic web, ontology},
abstract = {This paper proposes an ontology-based framework to support integration and visualization of data from Intelligent Transportation Systems. These activities may be technically demanding for transportation stakeholders, due to technical and human factors, and may hinder the use of visualization tools in practice. The existing ontologies do not provide the necessary semantics for integration of spatio-temporal data from such systems. Moreover, a formal representation of the components of visualization techniques and expert knowledge can leverage the development of visualization tools that facilitate data analysis. The proposed Visualization-oriented Urban Mobility Ontology (VUMO) provides a semantic foundation to knowledge-assisted visualization tools (KVTs). VUMO contains three facets that interrelate the characteristics of spatio-temporal mobility data, visualization techniques and expert knowledge. A built-in rule set leverages semantic technologies standards to infer which visualization techniques are compatible with analytical tasks, and to discover implicit relationships within integrated data. The annotation of expert knowledge encodes qualitative and quantitative feedback from domain experts that can be exploited by recommendation methods to automate part of the visualization workflow. Data from the city of Porto, Portugal were used to demonstrate practical applications of the ontology for each facet. As a foundational domain ontology, VUMO can be extended to meet the distinctiveness of a KVT.}
}
@article{PRAYITNO20241070,
title = {Optimizing the Sustainability of Collaborative Logistics in Urban Area through Ontologies and Causal Artificial Intelligence: A Conceptual Framework},
journal = {Procedia CIRP},
volume = {130},
pages = {1070-1076},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.208},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124013659},
author = {Kutut Aji Prayitno and Hendro Wicaksono},
keywords = {Collaborative sustainable urban logistics, Causal AI, Ontology, Reinforcement learning},
abstract = {Efficiently managing logistics operations is crucial in elevating sustainability and tackling the challenges urbanization brings in today’s urban environment. Collaborations among the public and private sectors in urban logistics are essential to minimize environmental impacts. This study aims to create a novel conceptual framework for collaborative logistics designed explicitly for sustainable metropolitan areas. The framework aims to enable collaborative data-driven sustainability optimization in urban logistics. It comprises ontologies to facilitate interoperability among stakeholders by providing a shared understanding of the exchanged data. The framework utilizes causal artificial intelligence to enable traceability and transparency of data-driven decisions compared to conventional machine learning working based on correlations. Furthermore, the framework also employs causal reinforcement learning that enables agents to learn what actions lead to targeted outcomes and why those actions are effective. The developed framework optimizes vehicle routes and conveyance selection while considering several operational constraints such as time windows, split-load scenarios, and commodity-specific requirements. Moreover, the system integrates the distinctive features of public transport networks. The suggested strategy minimizes fuel use and overall delivery costs, promoting a more sustainable logistics environment in metropolitan areas measured using Environmental, Social, and Governance (ESG) indicators. This study contributes to the theoretical understanding of collaborative logistics. It underscores the importance of environmental stewardship and societal well-being in logistics planning and implementation by utilizing a data-driven approach.}
}
@article{DIMITROVA2020103450,
title = {An ontological approach for pathology assessment and diagnosis of tunnels},
journal = {Engineering Applications of Artificial Intelligence},
volume = {90},
pages = {103450},
year = {2020},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.103450},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619303446},
author = {Vania Dimitrova and Muhammad Owais Mehmood and Dhavalkumar Thakker and Bastien Sage-Vallier and Joaquin Valdes and Anthony G. Cohn},
keywords = {Tunnel diagnosis, Ontology, Intelligent decision support systems, Linear transport structures},
abstract = {Tunnel maintenance requires complex decision making, which involves pathology diagnosis and risk assessment, to ensure full safety while optimising maintenance and repair costs. A Decision Support System (DSS) can play a key role in this process by supporting the decision makers in identifying pathologies based on disorders present in various tunnel portions and contextual factors affecting a tunnel. Another key aspect is to identify which spatial stretches within a tunnel contain pathologies of similar kinds within neighbouring tunnel segments. This paper presents PADTUN, a novel intelligent decision support system that assists with pathology diagnosis and assessment of tunnels with respect to their disorders and diagnosis influencing factors. It utilises semantic web technologies for knowledge capture, representation, and reasoning. The core of PADTUN is a family of ontologies which represent the main concepts and relations associated with pathology assessment, and capture the decision process concerning tunnel maintenance. Tunnel inspection data is linked to these ontologies to take advantage of inference capabilities offered by semantic technologies. In addition, an intelligent mechanism is presented which exploits abstraction and inference capabilities. Thus PADTUN provides the world’s first semantically based intelligent DSS for tunnel maintenance. PADTUN was developed by an interdisciplinary team of tunnel experts and knowledge engineers in real-world settings offered by the NeTTUN EU Project. An evaluation of the PADTUN system is performed using real-world tunnel data and diagnosis tasks. We show how the use of semantic technologies allows addressing the complex issues of tunnel pathology inferencing, aiding in, and matching transportation experts’ expectations of decision support. The methodology is applicable to any linear transport structures, offering intelligent ways to aid with complex decision processes related to diagnosis and maintenance.}
}
@article{GUPTA2021101260,
title = {Feature-based ontological framework for semantic interoperability in product development},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101260},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101260},
url = {https://www.sciencedirect.com/science/article/pii/S147403462100015X},
author = {Ravi Kumar Gupta and Balan Gurumoorthy},
keywords = {Product information exchange, Semantic interoperability, Shape feature taxonomy, Feature semantics, Product informatics, Computer-aided design, Product lifecycle management, Product development},
abstract = {An essential requirement in integrating tasks in product development is to have a seamless exchange of product information through the entire product lifecycle. A key challenge in the integration is the exchange of shape semantics in terms of understandable labels and representations. A unified taxonomy is proposed to represent, classify, and extract shape features. This taxonomy is built using the Domain-Independent Form Feature (DIFF) model as the representation of features. All the shape features in a product model are classified under three main classes, namely, volumetric features, deformation features and free-form surface features. Shape feature ontology is developed using the unified taxonomy, which brings the shape features under a single reasoning framework. One-to-many reasoning framework is presented for mapping semantically equivalent information (label and representation) of the feature to be exchanged to target applications, and the reconstruction of the shape model automatically in that target application. An algorithm has been developed to extract the semantics of shape features and construct the model in the target application. The algorithm developed has been tested for shape models taken from literature and test cases are selected based on variations of topology and geometry. Results of exchanging product information are presented and discussed. Finally, the limitations of the proposed method for exchanging product information are explained.}
}
@article{PRADEEP202133,
title = {Leveraging context-awareness for Internet of Things ecosystem: Representation, organization, and management of context},
journal = {Computer Communications},
volume = {177},
pages = {33-50},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002280},
author = {Preeja Pradeep and Shivsubramani Krishnamoorthy and Rahul Krishnan Pathinarupothi and Athanasios V. Vasilakos},
keywords = {Context-aware computing, Context model, Context ontology, Internet of Things, Situational context},
abstract = {Present-day devices are becoming increasingly smarter than their predecessors. From a simple passive light switch to an intelligent wristwatch, great strides have been made in networking smart devices, creating an autonomous ecosystem, the so-called Internet of Things. In an increasingly information-driven world, context-awareness supports the intended applications as well as their constituent devices, making them conscious of and adaptive to the specific scenario in real-time. Moreover, heterogeneous devices in the Internet of Things ecosystem peruse disparate data formats and semantics, giving rise to interoperability and information sharing challenges. Context modeling is a core feature that facilitates interoperability and information sharing between applications. Although generic context models exist, they do not consider pertinent dimensions of context to provide a generic vocabulary, and therefore, they cannot be extended to generalize situations commonly encountered in the Internet of Things environment. An extensible, generic modeling and representation of context is required to manage pertinent context dimensions in various ecosystems by being dynamically aware of the situation. This paper presents Context Model for Internet of Things, an extensible and generic ontology-based context modeling approach that provides relevant information at the right time. This work encompasses Context Ontology for Internet of Things, an ontology-based context organization approach, which provides an abstract and overarching vocabulary that fosters knowledge reusability and sharing. The proposed model has been implemented and evaluated with a use case to validate its adaptability, effectiveness, and viability. Our evaluation based on generality, effectiveness, and consistency shows that the proposed model can effectively represent, organize, and manage the context in different Internet of Things ecosystems.}
}
@article{SINGH2019177,
title = {Generation of fashionable clothes using generative adversarial networks},
journal = {International Journal of Clothing Science and Technology},
volume = {32},
number = {2},
pages = {177-187},
year = {2019},
issn = {0955-6222},
doi = {https://doi.org/10.1108/IJCST-12-2018-0148},
url = {https://www.sciencedirect.com/science/article/pii/S0955622219000237},
author = {Montek Singh and Utkarsh Bajpai and Vijayarajan V. and Surya Prasath},
keywords = {Neural network, Cross-domain relations, Fashion clothes, Generative adversarial networks},
abstract = {Purpose
There are various style options available when one buys clothes on online shopping websites, however the availability the new fashion trends or choices require further user interaction in generating fashionable clothes. The paper aims to discuss this issue.
Design/methodology/approach
Based on generative adversarial networks (GANs) from the deep learning paradigm, here the authors suggest model system that will take the latest fashion trends and the clothes bought by users as input and generate new clothes. The new set of clothes will be based on trending fashion but at the same time will have attributes of clothes where were bought by the consumer earlier.
Findings
In the proposed machine learning based approach, the clothes generated by the system will personalized for different types of consumers. This will help the manufacturing companies to come up with the designs, which will directly target the customer.
Research limitations/implications
The biggest limitation of the collected data set is that the clothes in the two domains do not belong to a specific category. For instance the vintage clothes data set has coats, dresses, skirts, etc. These different types of clothes are not segregated. Also there is no restriction on the number of images of each type of cloth. There can many images of dresses and only a few for the coats. This can affect the end results. The aim of the paper was to find whether new and desirable clothes can be created from two different domains or not. Analyzing the impact of “the number of images for each class of cloth” is something which is aim to work in future.
Practical implications
The authors believe such personalized experience can increase the sales of fashion stores and here provide the feasibility of such a clothes generation system.
Originality/value
Applying GANs from the deep learning models for generating fashionable clothes.}
}
@article{HUARANGAJUNCO2024134,
title = {From cloud and fog computing to federated-fog computing: A comparative analysis of computational resources in real-time IoT applications based on semantic interoperability},
journal = {Future Generation Computer Systems},
volume = {159},
pages = {134-150},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24002103},
author = {Edgar Huaranga-Junco and Salvador González-Gerpe and Manuel Castillo-Cara and Andrea Cimmino and Raúl García-Castro},
keywords = {Fog computing, Federated-fog computing, IoT architectures, Semantic interoperability, Performance evaluation, Data federation},
abstract = {In contemporary computing paradigms, the evolution from cloud computing to fog computing and the recent emergence of federated-fog computing have introduced new challenges pertaining to semantic interoperability, particularly in the context of real-time applications. Fog computing, by shifting computational processes closer to the network edge at the local area network level, aims to mitigate latency and enhance efficiency by minimising data transfers to the cloud. Building upon this, federated-fog computing extends the paradigm by distributing computing resources across diverse organisations and locations, while maintaining centralised management and control. This research article addresses the inherent problematics in achieving semantic interoperability within the evolving architectures of cloud computing, fog computing, and federated-fog computing. Experimental investigations are conducted on a diverse node-based testbed, simulating various end-user devices, to emphasise the critical role of semantic interoperability in facilitating seamless data exchange and integration. Furthermore, the efficacy of federated-fog computing is rigorously evaluated in comparison to traditional fog and cloud computing frameworks. Specifically, the assessment focuses on critical factors such as latency time and computational resource utilisation while processing real-time data streams generated by Internet of Things (IoT) devices. The findings of this study underscore the advantages of federated-fog computing over conventional cloud and fog computing paradigms, particularly in the realm of real-time IoT applications demanding high performance (lowering CPU usage to 20%) and low latency (with picks up to 300ms). The research contributes valuable insights into the optimisation of processing architectures for contemporary computing paradigms, offering implications for the advancement of semantic interoperability in the context of emerging federated-fog computing for IoT applications.}
}
@article{KARDINATA2019826,
title = {Integration of Crowdsourcing into Ontology Relation Extraction},
journal = {Procedia Computer Science},
volume = {161},
pages = {826-833},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919319003},
author = {Eunike Andriani Kardinata and Nur Aini Rakhmawati},
keywords = {crowdsourcing, integration, online incremental, ontology learning, relation extraction},
abstract = {Ontology learning is a continuous process that is always being researched and developed. A learning method for one domain may not be applicable to another because of the different characteristics of the data involved. Researchers have been developing various methodologies to build the highest quality of ontology efficiently. As identified in the previous works, one problem which could not be solved my machine alone is the extra-logical errors. These errors can only be identified by human judges and are usually related to the domain of the ontology. In this research, we aim to catalogue available methods, specifically for relation extraction, and the online incremental algorithms which will allow integration of crowdsourcing into ontology learning—to handle said challenge. We also briefly discussed an existing ontology editor called OntoCop, which may be used as a reference for further research. Henceforth, we propose a framework based on our review to improve the current relation extraction method.}
}
@article{WALOSZEK2020733,
title = {Improving the Performance of Ontological Querying by using a Contextual Approach},
journal = {Procedia Computer Science},
volume = {176},
pages = {733-742},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.062},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319578},
author = {Wojciech Waloszek and Aleksander Waloszek},
keywords = {knowledge bases, contexts, Description Logics, reasoning},
abstract = {In the paper we present the results of experiment we performed to determine whether a contextual approach may be used to increase the performance of querying a knowledge base. For the experiments we have used a unique setting where we put much effort in developing a contextual and a non-contextual ontology which are as much close counterparts as possible. To achieve this we created a contextual version of a non-contextual ontology and reformulated the set of competency questions to reflect the contextual structure of the newly created knowledge base. The results of the experiment strongly suggest that using contexts might be advantageous for improving performance, and also show the further ways of development of the approach.}
}
@article{SELVI2025103553,
title = {Efficient data handling in smart healthcare using Quotient Hash Trees and gaussian hilbert regression},
journal = {Ain Shams Engineering Journal},
volume = {16},
number = {9},
pages = {103553},
year = {2025},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2025.103553},
url = {https://www.sciencedirect.com/science/article/pii/S2090447925002941},
author = {T. Kalai Selvi and S. Sasirekha},
keywords = {Heterogeneous Medical Data, Gaussian Replicating, Kernel Hilbert, Quotient Hash Tree, Data Management Server, Internet of Things},
abstract = {Significant amounts of heterogeneous medical data are developed daily from several medical devices, and sensor data analysis across IoT domains is becoming challenging. The conservative data warehouses possess the potential to integrate data and support interactive data exploration. However, transferring data with minimum loss and storing it in a unified format is arduous and time-consuming. The proposed Gaussian Replicating Hilbert Regression and Quotient Hash Tree (GRHR-QHT) method is introduced to convert heterogeneous data into a unified format with improved accuracy and reduced time. The proposed GRHR-QHT performs three processes: data collection, transfer, and storage. At first, linear acceleration and angular velocity-based vector data collection are transmitted to the data management server. The collected data are stored in the input matrix. The data management server constructs multiple data into a unified format without any loss using GRH-based sequencing. This data management server reduces variance among aspect spaces through minimum distance with empirical organizations of samples. Also, the Polynomial Regression function is used to determine the relationship between the independent parameters. Then, unified data gets stored in the data management server using a Quotient Hash Tree (QHT) for easy access and less space complexity. The experimental assessment of the GRHR-QHT technique and existing methods is compared with different metrics: accuracy, time, error rate, space complexity, throughput, packet delivery ratio, service availability, reliability, response time, and end-to-end delay. The outcome of the suggested method is compared with conventional techniques in terms of improved 11% accuracy and 36% throughput with 33%, 41%, and 20% minimum time, error rate, and space complexity.}
}
@article{COSTA2022101977,
title = {A core ontology on the Human–Computer Interaction phenomenon},
journal = {Data & Knowledge Engineering},
volume = {138},
pages = {101977},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101977},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000951},
author = {Simone Dornelas Costa and Monalessa Perini Barcellos and Ricardo de Almeida Falbo and Tayana Conte and Káthia M. {de Oliveira}},
keywords = {Human–Computer Interaction, User interface, Interactive computer system, Ontology, Ontology network},
abstract = {Human–Computer Interaction (HCI) is a complex communication phenomenon involving human beings and computer systems that gained large attention from industry and academia with the advent of new types of interactive systems (mobile applications, smart cities, smart homes, ubiquitous systems and so on). Despite of its importance, there is still a lack of formal and explicit representations of what the HCI phenomenon is. In this paper, we intend to clarify the main notions involved in the HCI phenomenon, by establishing an explicit conceptualization of it. To do so, we need to understand what interactive computer systems are, which types of actions users perform when interacting with an interactive computer system, and finally what human–computer interaction itself is. The conceptualization is presented as a core reference ontology, called HCIO (HCI Ontology), which is grounded in the Unified Foundational Ontology (UFO). HCIO was evaluated using ontology verification and validation techniques and has been used as core ontology of an HCI ontology network.}
}
@article{SHISHEHCHI2021100192,
title = {A rule based expert system based on ontology for diagnosis of ITP disease},
journal = {Smart Health},
volume = {21},
pages = {100192},
year = {2021},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2021.100192},
url = {https://www.sciencedirect.com/science/article/pii/S2352648321000143},
author = {Saman Shishehchi and Seyed Yashar Banihashem},
keywords = {ITP disease, Medical expert system, Ontology, Semantic rules},
abstract = {This paper is aimed to implement the semantic rule based expert system for diagnosing a kind of blood immune thrombocytopenia disease. This paper presents an ontology to depict the knowledge domain of this disease, symptoms and its related treatments. The developed system uses the real data from laboratory of Iranian hospitals. In the system, the observed symptoms are taken from the patient via Java based graphical user interface. This system supports the patients suffering from this disease to get a type of their disease and recommends suitable treatments. Some semantic rules were defined and then, Jess as a reasoner inferenced the rules to do the diagnosis process. The diagnosis process is validated by blood specialists. Since this system can be used by patients, doctors or medical students anywhere, it helps to make the disease follow up easier and tries to save cost and time for patients. The questionnaire is developed to measure the usability of the system. The results of questionnaire were satisfactory after it was tested with 154 respondents. The reliability test was done and the Cronbach's Alpha was .865 which is higher than 0.7. The mean value of questionnaire is more than 4 and the total mean is 4.49 which is an acceptable value to show the high degree of user acceptance and accuracy of system.}
}
@article{WANG2022111435,
title = {Missing standard features compared with similar apps? A feature recommendation method based on the knowledge from user interface},
journal = {Journal of Systems and Software},
volume = {193},
pages = {111435},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111435},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001364},
author = {Yihui Wang and Shanquan Gao and Xingtong Li and Lei Liu and Huaxiao Liu},
keywords = {Android apps, Feature recommendation, User interface},
abstract = {To attract and retain users, deciding what features should be added in the next release of apps becomes very crucial. Different from traditional software, there are rich data resources in app markets to perform market-wide analysis. Considering that capturing key features that apps lack compared with its similar products and making up for them can be conducive to enhance the competitiveness, we propose a method to establish the feature relationships from the level of UI pages and recommend missing key features for the pages of apps based on these relationships. Firstly, we utilize the UI testing tool to collect UI pages for apps in the repository, and give the method to gain the feature information in them. Then, we identify the products similar to the analyzed app based on topic modeling technique. Finally, we establish the relationships between features by analyzing UI pages gained for the analyzed app as well as its similar products, and identify suitable features recommended to UI pages of the analyzed app based on these relationships. The experiment based on Google Play shows that our method can recommend features for apps from the level of UI pages effectively.}
}
@article{EKELHART2018109,
title = {Taming the logs - Vocabularies for semantic security analysis},
journal = {Procedia Computer Science},
volume = {137},
pages = {109-119},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316156},
author = {Andreas Ekelhart and Elmar Kiesling and Kabul Kurniawan},
keywords = {semantic extraction, log vocabularies, log analysis, security analysis},
abstract = {Due to the growing complexity of information systems and the increasing prevalence and sophistication of threats, security management has become an enormously challenging task. To identify suspicious activities, security analysts need to monitor their systems constantly, which involves coping with high volumes of heterogeneous log data from various sources. Processes to aggregate these disparate logs and trigger alerts when particular events occur are often automated today. However, these methods are typically based on regular expressions and statistical correlations and do not involve any interpretation of the context in which an event occurred and do not allow for inference or sophisticated rules. Inspection and in-depth analysis of log information to link events from various sources (e.g., firewall, syslog, web server log, database log) and establish causal chains has therefore largely remained a tedious manual search process that scales poorly with a growing number of heterogeneous log sources, log volumes, and the increasing complexity of attacks. In this paper, we make the case for a semantic approach to tackle these challenges. By lifting raw log data and modeling their context, events can be linked to rich background knowledge, integrated based on causal relations, and interpreted in a context-specific manner. This builds a foundation for more comprehensive extraction of the meaning of events from unstructured log messages. Based on the results, we envision a platform to partly automate security monitoring and support analysts in coping with fast evolving threat landscapes, alleviate alert fatigue, improve situational awareness, and expedite incidence response.}
}
@article{SHENOY2022100679,
title = {A study of the quality of Wikidata},
journal = {Journal of Web Semantics},
volume = {72},
pages = {100679},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100679},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000536},
author = {Kartik Shenoy and Filip Ilievski and Daniel Garijo and Daniel Schwabe and Pedro Szekely},
keywords = {Wikidata, Data quality, Knowledge graphs, Constraints, Crowdsourcing},
abstract = {Wikidata has been increasingly adopted by many communities for a wide variety of applications, which demand high-quality knowledge to deliver successful results. In this paper, we develop a framework to detect and analyze low-quality statements in Wikidata by shedding light on the current practices exercised by the community. We explore three indicators of data quality in Wikidata, based on: (1) community consensus on the currently recorded knowledge, assuming that statements that have been removed and not added back are implicitly agreed to be of low quality; (2) statements that have been deprecated; and (3) constraint violations in the data. We combine these indicators to detect low-quality statements, revealing challenges with duplicate entities, missing triples, violated type rules, and taxonomic distinctions. Our findings complement ongoing efforts by the Wikidata community to improve data quality, aiming to make it easier for users and editors to find and correct mistakes.}
}
@article{BARBAGONZALEZ2021103546,
title = {Injecting domain knowledge in multi-objective optimization problems: A semantic approach},
journal = {Computer Standards & Interfaces},
volume = {78},
pages = {103546},
year = {2021},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2021.103546},
url = {https://www.sciencedirect.com/science/article/pii/S0920548921000416},
author = {Cristóbal Barba-González and Antonio J. Nebro and José García-Nieto and María {del Mar Roldán-García} and Ismael Navas-Delgado and José F. Aldana-Montes},
keywords = {Multi-Objective optimization, Decision making, Metaheuristics, Domain knowledge, Semantic web technologies, Ontology},
abstract = {In the field of complex problem optimization with metaheuristics, semantics has been used for modeling different aspects, such as: problem characterization, parameters, decision-maker’s preferences, or algorithms. However, there is a lack of approaches where ontologies are applied in a direct way into the optimization process, with the aim of enhancing it by allowing the systematic incorporation of additional domain knowledge. This is due to the high level of abstraction of ontologies, which makes them difficult to be mapped into the code implementing the problems and/or the specific operators of metaheuristics. In this paper, we present a strategy to inject domain knowledge (by reusing existing ontologies or creating a new one) into a problem implementation that will be optimized using a metaheuristic. Thus, this approach based on accepted ontologies enables building and exploiting complex computing systems in optimization problems. We describe a methodology to automatically induce user choices (taken from the ontology) into the problem implementations provided by the jMetal optimization framework. With the aim of illustrating our proposal, we focus on the urban domain. Concretely, we start from defining an ontology representing the domain semantics for a city (e.g., building, bridges, point of interest, routes, etc.) that allows defining a-priori preferences by a decision maker in a standard, reusable, and formal (logic-based) way. We validate our proposal with several instances of two use cases, consisting in bi-objective formulations of the Traveling Salesman Problem (TSP) and the Radio Network Design problem (RND), both in the context of an urban scenario. The results of the experiments conducted show how the semantic specification of domain constraints are effectively mapped into feasible solutions of the tackled TSP and RND scenarios. This proposal aims at representing a step forward towards the automatic modeling and adaptation of optimization problems guided by semantics, where the annotation of a human expert can be now considered during the optimization process.}
}
@article{KOONCE20191678,
title = {Metrics to gauge the success of a manufacturing ontology},
journal = {Procedia Manufacturing},
volume = {38},
pages = {1678-1682},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.116},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920301177},
author = {David Koonce and Dusan Sormaz},
keywords = {Ontology Evaluation, Ontology Usage, Manufacturing Ontology},
abstract = {Ontologies are structures of concepts that define a high-level representation of a system or area. They can serve as a foundational understanding for developing or integrating software representations of said system. And while the construction of can be a complex, multi-party exercise, the assessment of ontologies is often defined less on usage and more on completeness and coverage. In manufacturing, ontology development ranges from supply chain to production to design. Owing to the computer science foundations of ontology design and representation, the value or quality of an ontology can be assessed on notions of completeness and coverage. Recently, researchers have posited that usage should factor into the Ontology Lifecycle. Similar to how the market, and not technology, defines the success of a product or technology, this paper will examine how utilization of an ontology can define the value or quality of the ontology}
}
@article{CHHETRI2024164,
title = {Enabling privacy-aware interoperable and quality IoT data sharing with context},
journal = {Future Generation Computer Systems},
volume = {157},
pages = {164-179},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24001109},
author = {Tek Raj Chhetri and Chinmaya Kumar Dehury and Blesson Varghese and Anna Fensel and Satish Narayana Srirama and Rance J. DeLong},
keywords = {Data sharing, Edge intelligence, Interoperability, Internet of Things (IoT), Knowledge graphs, General Data Protection Regulation (GDPR), Smart cities},
abstract = {Sharing Internet of Things (IoT) data across different sectors, such as in smart cities, becomes complex due to heterogeneity. This poses challenges related to a lack of interoperability, data quality issues and lack of context information, and a lack of data veracity (or accuracy). In addition, there are privacy concerns as IoT data may contain personally identifiable information. To address the above challenges, this paper presents a novel semantic technology-based framework that enables data sharing in a GDPR-compliant manner while ensuring that the data shared is interoperable, contains required context information, is of acceptable quality, and is accurate and trustworthy. The proposed framework also accounts for the edge/fog, an upcoming computing paradigm for the IoT to support real-time decisions. We evaluate the performance of the proposed framework with two different edge and fog–edge scenarios using resource-constrained IoT devices, such as the Raspberry Pi. In addition, we also evaluate shared data quality, interoperability and veracity. Our key finding is that the proposed framework can be employed on IoT devices with limited resources due to its low CPU and memory utilization for analytics operations and data transformation and migration operations. The low overhead of the framework supports real-time decision making. In addition, the 100% accuracy of our evaluation of the data quality and veracity based on 180 different observations demonstrates that the proposed framework can guarantee both data quality and veracity.}
}
@article{SEDIGHIANI2021110989,
title = {BASBA: A framework for Building Adaptable Service-Based Applications},
journal = {Journal of Systems and Software},
volume = {179},
pages = {110989},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110989},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000868},
author = {Kavan Sedighiani and Saeed Shokrollahi and Fereidoon Shams},
keywords = {Service-based application, Self-adaptation, Models at runtime, Quality of service, Variability, Reusability},
abstract = {Due to the continuously changing environment of service-based applications (SBAs), the ability to adapt to environmental and contextual changes has become a crucial characteristic of such applications. Providing SBAs with this ability is a complex task, usually carried out in an unsystematic way and interwoven with application logic. As a result, developing and maintaining adaptive SBAs has become a costly and hardly repeatable process. The objective of this paper is to present a model-based approach to developing adaptive SBAs which separates development of adaptation concerns from development of SBAs behaviors. This approach aims to facilitate and improve the development of adaptive behaviors. In this paper, the process of developing an adaptive SBA is defined as specifying adaptive SBA models based on a metamodel and reusable adaptation tactics. These models are then transformed into runtime model artifacts and running system units performing runtime adaptive behaviors. The approach introduces a systematic method to derive adaptation behaviors from adaptation models, which facilitates the development of adaptive behaviors. The empirical evaluations in three studies show that our approach enhances the development of adaptive behaviors in terms of identifying more proper adaptation plans, reducing the development time, and increasing understandability, modifiability, and correctness of code.}
}
@article{REN201924,
title = {Building an ontological knowledgebase for bridge maintenance},
journal = {Advances in Engineering Software},
volume = {130},
pages = {24-40},
year = {2019},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818307634},
author = {Guoqian Ren and Rui Ding and Haijiang Li},
keywords = {Bridge maintenance, Semantic web, Ontology, Multi-criteria decision support, Suppliers selection, Event management},
abstract = {The operation stage has the biggest potential value in the bridge life cycle management, and it often critically influences the overall cost of the bridge. As such, changes in the efficiency of the project's operation stage could be of significant benefit to the overall project. However, current approaches in the operation stage often lack the effective support of computer-aided tools. This research presents a holistic method based on an ontology to achieve automatic rule checking and improve the management and communication of knowledge related to bridge maintenance. The developed ontology can also facilitate a smarter decision-making process for bridge management by informing engineers of choices with different considerations. Three approaches; semantic validation, syntactical validation, and case study validation, have been adopted to evaluate this ontology and demonstrate how the developed ontology can be used by engineers when dealing with different issues. The results showed that this approach can create a holistic knowledge base that can integrate various domain knowledge to enable bridge engineers to make more comprehensive decisions rather than a single objective-targeted delivery.}
}
@article{ALVITEDIEZ2021946,
title = {Linked open data portals: functionalities and user experience in semantic catalogues},
journal = {Online Information Review},
volume = {45},
number = {5},
pages = {946-963},
year = {2021},
issn = {1468-4527},
doi = {https://doi.org/10.1108/OIR-07-2020-0295},
url = {https://www.sciencedirect.com/science/article/pii/S1468452721000779},
author = {María-Luisa Alvite-Díez},
keywords = {Linked open data, User experience, Semantic Web, User interface, Information discovery},
abstract = {Purpose
This study seeks to understand the current state of the development of linked open data (LOD) bibliographic portals to discuss their functionalities, contributions, value-adds and user experience.
Design/methodology/approach
A set of evaluative aspects grouped into three analysis dimensions was established: collections, tools—technologies and standards used—and web user interface. As the object of the study, four projects of diverse nature and volume were selected to help provide a better understanding of the trends in the solutions provided for the end user when accessing linked data collections.
Findings
Publishing LOD through visual interfaces maximises information enrichment, contextualisation and discovery, in addition to improving user experience, because of both increased navigation capabilities and interrelationships between data. These more flexible environments have metamorphosised the visualisation of bibliographic information. However, aspects that needed improvement were observed, primarily relating to (1) a more intuitive interaction, (2) possibilities of greater personalisation, (3) enhanced communication with the user to favour user engagement and (4) experimental spaces of data reuse.
Research limitations/implications
Further quantitative and qualitative studies should be conducted to improve these portals, assess their adaptation to the behaviour of the user and their influence on the use of library collections.
Originality/value
This article investigates the potential of semantic technologies in bibliographic data portals, proposes a methodological model for their evaluation and advances conclusions about the usability and user experience that these platforms provide, compared to classic catalogues.}
}
@article{LI2019152,
title = {Enhancing energy management at district and building levels via an EM-KPI ontology},
journal = {Automation in Construction},
volume = {99},
pages = {152-167},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517309494},
author = {Yehong Li and Raúl García-Castro and Nandana Mihindukulasooriya and James O'Donnell and Sergio Vega-Sánchez},
keywords = {District, Building, Energy management, Stakeholders, Ontology, Linked data},
abstract = {The use of information and communication technologies facilitates energy management (EM) at both district and building levels, but also generates a considerable amount of data. To gain insights into such data, it is essential to resolve the cross-domain data interoperability problem and determine an approach to exchange performance information and insightful data among various stakeholders. This paper developed an EM-KPI (key performance indicator) ontology to exchange key performance information and data for districts and buildings. The ontology contains two components: namely KPIs and EM master data; these, respectively, represent the multi-level performance information for energy performance tracking and the key data for data exploitation. Through a demonstration, a sample linked dataset generated using the data correlation predefined in the ontology is presented. The linked data analysis proves the feasibility of the ontology for exchanging data among different stakeholders and for exploring insights in relation to performance improvement.}
}
@article{LONGO2022107824,
title = {New perspectives and results for Smart Operators in industry 4.0: A human-centered approach},
journal = {Computers & Industrial Engineering},
volume = {163},
pages = {107824},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107824},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221007282},
author = {Francesco Longo and Letizia Nicoletti and Antonio Padovano},
keywords = {Smart factory, Smart operators, Industry 4.0, Extended reality, Digital and intelligent assistants},
abstract = {Digital solutions (including Extended Reality as well as web, mobile and AI technologies), among others, are entering a broad variety of industries and modifying the capabilities of operators through (close to) real-time display of context-dependent information. However, little is known with respect to two major issues: (i) how these technologies can be seamlessly integrated for product/process and overall manufacturing system management providing of syntactic, semantic and functional interoperability and reusability among the different manufacturing systems departments; (ii) how they can be conveyed to operators also taking into account the cognitive load that is incurred by the operators. To this end, starting from a previous article (Longo et al., 2017), this article designs and proposes the KNOW4I approach and its practical implementation in an ICT platform (the KNOW4I platform) to further empower the Smart Operators concept. Two major objectives are pursued. The former is to set a standard referred to as the KNOW4I methodological approach for knowledge representation, knowledge management and digital contents management within the Smart Operator domain. The latter is the implementation of the aforementioned approach as part of the KNOW4I platform that includes a suite of Smart Utilities and Objects intelligently and interactively linked with a newly released version of the Sophos-MS digital and intelligent Assistant. Experimentations and results based on multiple KPIs are carried out to account for the effectiveness of the proposed framework.}
}
@article{CASTIGLIONE20181134,
title = {CHIS: A big data infrastructure to manage digital cultural items},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {1134-1145},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17305605},
author = {Aniello Castiglione and Francesco Colace and Vincenzo Moscato and Francesco Palmieri},
keywords = {Big data, Cultural heritage, Resource management, Big data analytics, SOA, NoSQL},
abstract = {In this paper, we describe CHIS (Cultural Heritage Information System), a big data infrastructure that can be used to query, browse, analyze and process digital contents related to cultural heritage from a set of heterogeneous and distributed repositories. CHIS is characterized by the following technical features: capability to gather information from distributed and heterogeneous data sources (e.g., Sensor Networks, Social Media Networks, Digital Libraries and Archives, Multimedia Collections, Web Data Services, etc.); advanced data management techniques and technologies; ability to provide useful and personalized data to users based on their preferences and context; advanced information retrieval facilities, data analytics and other utilities/services, according to the SOA paradigm. By means of a set of ad-hoc APIs, and value-added data processing and analytics services, our system can support several applications: mobile multimedia guides for cultural environments, web portals to promote the cultural heritage of a given organization, multimedia recommender and storytelling systems and so on. We discuss the main ideas that characterize the system, showing its use for several applications.}
}
@article{VEGETTI2022100254,
title = {Ontology network to support the integration of planning and scheduling activities in batch process industries},
journal = {Journal of Industrial Information Integration},
volume = {25},
pages = {100254},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100254},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000534},
author = {Marcela Vegetti and Gabriela Henning},
keywords = {Scheduling, Ontologies, Integration, Formal specifications},
abstract = {In the last decades, the integration of information systems supporting planning, scheduling, and control has been a serious concern of the industrial community. Several standards have been developed to tackle this issue by addressing the exchange of data between the scheduling function and its immediate lower and upper levels in the planning pyramid. However, a more comprehensive approach is required to solve these integration problems, since this matter entails much more than data exchange. Along these lines, this article presents a network of ontologies that provides the foundations to reach an effective semantic interoperability among the various applications linked to scheduling activities. The proposed approach reuses and formalizes non-ontological resources, like the ISA-88 and ISA-95 standards, as well as the Resource Task Network (RTN) model. In addition, the application of the ontology network to a case study is also discussed in this article.}
}
@article{GUERRA2025111831,
title = {A cases and clusters framework for recording, retrieving, and reusing response plans in structured cybersecurity incident management},
journal = {Engineering Applications of Artificial Intelligence},
volume = {160},
pages = {111831},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111831},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625018330},
author = {Patrick Andrei Caron Guerra and Raul Ceretta Nunes and Luis Alvaro {de Lima Silva}},
keywords = {Cybersecurity incident response, Case-based reasoning, Clustering, Ontology, Explainable artificial intelligence, Decision-support system, Cybersecurity},
abstract = {The dynamic and increasing sophistication of cyberattacks and vulnerability exploitation creates a need for Explainable Artificial Intelligence (XAI) approaches that help maintain cyber resilience in organizations. In structured cybersecurity incident management, effective incident response demands explainable outputs from AI-based decision-support systems. To approach this problem, this work presents a framework for reusing concrete experiences of cybersecurity incident response, capturing problem-solving data and knowledge as cases for integrated Case-Based Reasoning (CBR) and Clustering. The contribution includes cluster-based query answer analysis, where cybersecurity analysts reuse clusters of retrieved incident response cases to build answers to new problems. Clustering helps analysts identify relevant groups from ranked lists of retrieved cases, making the reuse process more structured and understandable, especially when dealing with retrieval results for broad and ambiguous queries. Different clustering methods are applied to organize retrieved incident response cases from a case base, supporting the grouping of similar cases for a more straightforward interpretation. Multiple experiments, including cross-validation and real-world incident response testing, are conducted to demonstrate the effectiveness of the proposed framework in improving the decision-support system’s precision. The results indicate that exploring cases and clusters can enhance the selection of incident response procedures for reuse, mainly when analysts identify the most relevant clusters of retrieved cases for the given problem situations. The proposed framework contributes to the organization and understanding of responses to cybersecurity incidents, besides supporting more informed decision-making, ultimately improving cybersecurity incident management.}
}
@article{SANTOS201811,
title = {Partial meet pseudo-contractions},
journal = {International Journal of Approximate Reasoning},
volume = {103},
pages = {11-27},
year = {2018},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2018.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X1830104X},
author = {Yuri David Santos and Vinícius Bitencourt Matos and Márcio Moretto Ribeiro and Renata Wassermann},
keywords = {Belief revision, Limited reasoning, Ontologies, Pseudo-contractions},
abstract = {In the AGM paradigm for belief revision, epistemic states are represented by logically closed sets of sentences, the so-called belief sets. An alternative approach uses belief bases, arbitrary sets of sentences. Both approaches have their problems when it comes to contraction operations. Belief bases are more expressive, but, at the same time, they present a serious syntax dependence. Between those two extremes lie a whole gamut of operations called pseudo-contractions, some of which may be interesting alternatives to the classical ones, providing a good balance between syntax dependence and expressivity. In this paper we explore some very natural and general constructions for pseudo-contractions, showing some of their properties and giving their axiomatic characterizations. We also illustrate possible practical scenarios where they can be employed.}
}
@article{RAMESH2024100677,
title = {An interoperable ontology for CPS-enabled Polyhouse Solar Dryer: A case study of the AgroESP project},
journal = {Journal of Industrial Information Integration},
volume = {42},
pages = {100677},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100677},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24001171},
author = {Gowtham Ramesh and P. {Dheepan Kanna} and C. {Shunmuga Velayutham} and Jancirani Ramaswamy},
keywords = {Polyhouse solar dryer, Polyhouse ontology, Interoperability, CPS, Food preservation},
abstract = {Polyhouse is a commonly used conventional method for solar drying of food products. These Polyhouse Solar Driers (PSDs) are characterized by their enclosed structure and translucent covering, providing a controlled environment conducive to efficient food drying. Smart polyhouses, equipped with a Cyber-Physical System (CPS), further enhance this process by optimizing environmental conditions and enabling real-time monitoring. In smart PSDs, the data are obtained from diverse sources with different specifications in accuracy, resolution, and range. This multifaceted nature of the information obtained from various sources significantly compounds the complexity of the system. This complexity of data from diverse sources within smart polyhouses necessitates a standardized knowledge representation. Ontologies serve this purpose by establishing a common vocabulary and structure for data integration, promoting semantic interoperability and effective communication among diverse systems. This paper proposes a novel unified ontology designed to model complex polyhouse CPSs, aiming to address semantic interoperability issues and streamline data integration across various domains. The proposed polyhouse ontology attempts to reuse the concepts defined in existing ontologies rather defining new concepts for efficient knowledge sharing and enhanced understanding of polyhouse operations. The practical applicability of the polyhouse ontology has been verified with competency questions and through field deployment in a CPS enabled smart Polyhouse Solar Dryer.}
}
@article{MOSSAKOWSKI201858,
title = {Partial pushout semantics of generics in DOL},
journal = {Theoretical Computer Science},
volume = {741},
pages = {58-70},
year = {2018},
note = {An Observant Mind : Essays Dedicated to Don Sannella on the Occasion of his 60th Birthday},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2018.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0304397518301828},
author = {Till Mossakowski and Bernd Krieg-Brückner},
keywords = {Generic specification, Single-pushout transformation, Institution, Category of partial maps},
abstract = {We combine CASL's pushout-style generic specification with DOL's filtering, the latter being a syntactic removal of parts of a specification. The challenge is that now the body of a generic specification can remove parts of the formal parameter. This cannot be handled with usual pushout semantics, but calls for a semantics of “match, delete, glue in” as used in the theory of graph grammars. We hence employ Heindel's theory of MipMap categories as a basis for the use of pushouts in categories of partial maps. We introduce a notion of MipMap institution that can serve as a semantic background for a partial pushout semantics of generics with filtering.}
}
@article{ELSAPPAGH2022203,
title = {Automatic detection of Alzheimer’s disease progression: An efficient information fusion approach with heterogeneous ensemble classifiers},
journal = {Neurocomputing},
volume = {512},
pages = {203-224},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222010955},
author = {Shaker El-Sappagh and Farman Ali and Tamer Abuhmed and Jaiteg Singh and Jose M. Alonso},
keywords = {Computational Intelligence, Data fusion, Ensemble classifiers, Stacking, Data analysis, Alzheimer disease progression detection},
abstract = {Predicting Alzheimer’s disease (AD) progression is crucial for improving the management of this chronic disease. Usually, data from AD patients are multimodal and time series in nature. This study proposes a novel ensemble learning framework for AD progression incorporating heterogeneous base learners into an integrated model using the stacking technique. This framework is used to build a 4-class ensemble classifier, which predicts AD progression 2.5 years in the future based on the multimodal time-series data. Statistical measures have been extracted from the longitudinal data to be used by the conventional machine learning models. The examined ensemble members include k-nearest neighbor, extreme gradient boosting, support vector machine, random forest, decision tree, and multilayer perceptron. We utilize three time-series modalities and one static non-time series modality of 1371 subjects from the Alzheimer’s disease neuroimaging initiative (ADNI) to validate our model. Several homogeneous and heterogeneous combinations of ensemble members were implemented, and their performance compared. The balance between accuracy and diversity when selecting ensemble members was investigated. We found that both accuracy and diversity are equally critical metrics to obtain an optimal ensemble model. Furthermore, our testing showed that the proposed model achieves outstanding progression prediction performance. The proposed model achieved a high performance without using neuroimaging data, which means that the model could be implemented in low-cost healthcare environments. The proposed model has achieved superior results compared with the state-of-the-art techniques in Alzheimer’s and ensemble classifiers domains. The proposed framework can be used to implement efficient information fusion ensembles for other medical and non-medical problems.}
}
@article{ZHENG2021103930,
title = {A shared ontology suite for digital construction workflow},
journal = {Automation in Construction},
volume = {132},
pages = {103930},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103930},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521003812},
author = {Yuan Zheng and Seppo Törmä and Olli Seppänen},
keywords = {Construction workflow, Ontology, Digital construction, Information management},
abstract = {With ongoing advancements in information and communication technologies (ICTs) in all stages of the construction lifecycle, information from entities related to construction workflow (CW) can now be automatically collected. These implementations are point solutions, which require systematic integration to combine their information to enable a holistic picture of CW. The major barrier to such integration is information heterogeneity, where the information is collected from different systems under multiple contexts. Scholars in the construction domain have explored the use of ontology to solve the information-integration problem, although an ontology that both adequately represents the CW and integrates the digitalized information of CW via various systems and multiple contexts is currently missing from the existing literature. This research thus presents an ontology set for formalizing and integrating CW information within the digital construction context. The proposed digital construction ontologies (DiCon) are shared representations of construction domain knowledge that specify the terms and relations of CWs and their related information. We developed the DiCon based on a hybrid ontology development approach. The DiCon includes six modules: Entities, Processes, Information, Agents, Variables, and Contexts. The developed DiCon was further evaluated by approaches including automatic consistency checking, criteria-based evaluation, expert workshops, and task-based evaluation and involved two use cases by answering relevant competency questions via SPARQL queries. The results of the evaluation demonstrate that the DiCon ontologies are sufficient to represent domain knowledge and can formalize and integrate CW information within the digital construction context.}
}
@article{BLANKENBERG2022314,
title = {Using a graph database for the ontology-based information integration of business objects from heterogenous Business Information Systems},
journal = {Procedia Computer Science},
volume = {196},
pages = {314-323},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022420},
author = {Carolin Blankenberg and Berit Gebel-Sauer and Petra Schubert},
keywords = {IS Integration, Enterprise Knowledge Graph, Ontology, Graph Database},
abstract = {This paper reports on findings from a project on information integration from multiple Business Information Systems with the help of a user-specific Enterprise Knowledge Graph. Most ERP systems currently in use store information objects in relational databases. Research in Web Sciences has shown that graph structures present information in a more intuitive way that is easier to interpret for humans. Following a DSR approach, we developed a concept for storing an ontology in a graph database that allows us to map ERP objects and load them at runtime. This allows the end user to navigate through the graph structure, thus providing an intuitive and quick access to essential job-related information. We evaluated the suggested concept with a prototype following the paradigm of polyglot persistence; the prototype was equipped with a graph database to store the company-specific ontology in its native form. The program code was encapsulated into a separate module following a service-oriented software design.}
}
@article{FATHALLA2018151,
title = {SemSur: A Core Ontology for the Semantic Representation of Research Findings},
journal = {Procedia Computer Science},
volume = {137},
pages = {151-162},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S187705091831620X},
author = {Said Fathalla and Sahar Vahdati and Sören Auer and Christoph Lange},
keywords = {SemSur Ontology, Semantic Metadata Enrichment, SWRL rules, Scholarly Communication, Semantic Publishing},
abstract = {The way how research is communicated using text publications has not changed much over the past decades. We have the vision that ultimately researchers will work on a common structured knowledge base comprising comprehensive semantic and machine-comprehensible descriptions of their research, thus making research contributions more transparent and comparable. We present the SemSur ontology for semantically capturing the information commonly found in survey and review articles. SemSur is able to represent scientific results and to publish them in a comprehensive knowledge graph, which provides an efficient overview of a research field, and to compare research findings with related works in a structured way, thus saving researchers a significant amount of time and effort. The new release of SemSur covers more domains, defines better alignment with external ontologies and rules for eliciting implicit knowledge. We discuss possible applications and present an evaluation of our approach with the retrospective, exemplary semantification of a survey. We demonstrate the utility of the SemSur ontology to answer queries about the different research contributions covered by the survey. SemSur is currently used and maintained at OpenResearch.org.}
}
@article{NIZAMIS2018382,
title = {A Semantic Framework for Agent-based Collaborative Manufacturing Eco-systems},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {382-387},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.323},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318314472},
author = {Alexandros G. Nizamis and Dimosthenis K. Ioannidis and Nikolaos T. Kaklanis and Dimitrios K. Tzovaras},
keywords = {Manufacturing eco-system, Semantic modeling, Ontology, Rule-based Matchmaking, Supply, demand chain},
abstract = {Manufacturing eco-systems aim to connect data and services between factories, suppliers and customers. Most of them are built as agent-based eco-systems which act as web-based operating systems for business connections between enterprises. The connection of supply and demand entities participating in an eco-system by exploiting knowledge and data from the business entities has become imperative for them, in order to adapt to the dynamically changing market requirements. This paper introduces a web-based semantic ontological framework designed for collaborative agent-based manufacturing eco-systems. The proposed framework and its core components enable the information modeling of the manufacturing services and the supply chain concepts. A Collaborative Manufacturing Services Ontology able to describe both manufacturing domain and e-commerce domain is offered alongside with an Application Programming Interface for the effortless manipulation of the ontological resources. Furthermore, a Rule-based Matchmaking engine able to match requesters with possible suppliers, and to evaluate offers from suppliers based on different requesters’ criteria and preferences is provided.}
}
@article{GAWICH20243208,
title = {Towards an Ontology-Driven System For Building and Farming Greenhouses},
journal = {Procedia Computer Science},
volume = {246},
pages = {3208-3217},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.319},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924023445},
author = {Mariam Gawich and Christine Lahoud and Hajer Baazaoui and Ihab Jomaa},
keywords = {Greenhouse Ontology, Greenhouse, Knowledge Engineering, Agriculture},
abstract = {Greenhouse systems are considered a part of sustainable agriculture, whose objective is food security and safety while taking into consideration the conservation of resources such as soil and water. To promote sustainable agriculture through greenhouses, it is important to develop an intelligent system that helps stakeholders in decision-making concerning the construction and management of greenhouses. This system must ensure farming activities and monitoring procedures. This work concentrates on the farming activities such as pest control, disease protection, crop cultivation, treatment, etc, and their representation in the system. Ontology is used as a technology to represent the structured information in terms of concepts and the establishment of semantic relations among them. While many existing ontologies focus on agriculture management, the greenhouse domain lack comprehensive coverage, particularly in the operational farming activities that are necessary to ensure the agriculture sustainability. Therefore, there is a need to develop a greenhouse ontology-based system that address the stakeholders’ inquiries related to greenhouse construction and essential farming activities for greenhouse management. This paper presents a synthesis analysis of the existing ontologies in the domain of agriculture and greenhouses as well as a novel modular ontology that covers the greenhouse farming module.}
}
@article{AMADORDOMINGUEZ2021115731,
title = {A hierarchical multi-agent architecture based on virtual identities to explain black-box personalization policies},
journal = {Expert Systems with Applications},
volume = {186},
pages = {115731},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115731},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421011118},
author = {Elvira Amador-Domínguez and Emilio Serrano and Daniel Manrique},
keywords = {Multi-agent system, Virtual identities, Personalization},
abstract = {Hyper-personalization policies entail a considerable improvement regarding previous personalization approaches. However, they present several issues that need to be addressed, such as minimal explainability and privacy invasion. A hierarchical Multi-Agent System (MAS) is presented in this work to provide a solution to these concerns. The system is formulated as a hybrid approach, where some of the agents work autonomously, while the user input triggers the remaining. At the autonomous level, a set of Virtual Identities (VIs) representing different user profiles interact with Black-Box Hyper-Personalization Online Systems (BBHOS), gathering a set of targeted responses. Associative patterns and profile aggregations can then be inferred from the analysis of these responses. In the user-triggered level, the real user is virtualized as an identity that represents their features. The virtual identity serves as an intermediary between the personalization system and the real user. This virtualization hinders the personalization service from extracting sensitive contextual information about the real user, protecting their privacy. The results obtained by the user identity on its interaction with the personalization service are then analyzed, adjusting the content of the response to fit the user’s requests instead of their features. A use case on the functioning of the analysis of search engines is presented to illustrate the complete behavior of the proposed architecture.}
}
@article{KOUTSIANA2025100868,
title = {Agreeing and disagreeing in collaborative knowledge graph construction: An analysis of Wikidata},
journal = {Journal of Web Semantics},
volume = {86},
pages = {100868},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100868},
url = {https://www.sciencedirect.com/science/article/pii/S1570826825000095},
author = {Elisavet Koutsiana and Tushita Yadav and Nitisha Jain and Albert Meroño-Peñuela and Elena Simperl},
keywords = {Collaborative knowledge graph, Collaborative knowledge base, Knowledge community, Discussion analysis, Community analysis, Controversy, Argumentation},
abstract = {In this work, we study disagreements in discussions around Wikidata, an online knowledge community that builds the data backend of Wikipedia. Discussions are essential in collaborative work as they can increase contributor performance and encourage the emergence of shared norms and practices. While disagreements can play a productive role in discussions, they can also lead to conflicts and controversies, which impact contributor’ well-being and their motivation to engage. We want to understand if and when such phenomena arise in Wikidata, using a mix of quantitative and qualitative analyses to identify the types of topics people disagree about, the most common patterns of interaction, and roles people play when arguing for or against an issue. We find that decisions to create Wikidata properties are much faster than those to delete properties and that more than half of controversial discussions do not lead to consensus. Our analysis suggests that Wikidata is an inclusive community, considering different opinions when making decisions, and that conflict and vandalism are rare in discussions. At the same time, while one-fourth of the editors participating in controversial discussions contribute legitimate and insightful opinions about Wikidata’s emerging issues, they respond with one or two posts and do not remain engaged in the discussions to reach consensus. Our work contributes to the analysis of collaborative KG construction with insights about communication and decision-making in projects, as well as with methodological directions and open datasets. We hope our findings will help managers and designers support community decision-making and improve discussion tools and practices.}
}
@article{SWEIDAN2023101720,
title = {Fuzzy ontology-based approach for liver fibrosis diagnosis},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {8},
pages = {101720},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101720},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823002744},
author = {Sara Sweidan and Nuha Zamzami and Sahar F. Sabbeh},
keywords = {Liver fibrosis, Fuzzy ontology, Rule-based system, Semantics reasoning, Fuzzy reasoning},
abstract = {The domain of the digestive system is prone to severe chronic disease in the form of liver cirrhosis, which is currently a leading cause of mortality. This article presents a new intelligent system for predicting the severity of liver fibrosis in patients with chronic viral hepatitis C. The proposed system is based on the inference capabilities of fuzzy ontology and operates on semantic rule-based techniques. A fuzzy decision tree technique was employed to generate the ontology rule base using a dataset of real fibrosis cases from the Mansoura University Hospital, Egypt. These rules were then encoded into a set of fuzzy semantic rules using the fuzzy description logic format. To evaluate the system’s effectiveness, the proposed ontology was then tested on 47 chronic HCV cases, with an attempt made to see if this correctly diagnosed the patients’ conditions. The performance of the proposed system was compared with that of the now-standard Mamdani fuzzy inference system; while the latter achieved an accuracy of 95.7/%, the proposed fuzzy ontology-based system demonstrated higher performance, with 97.8% accuracy. Furthermore, the proposed system also supports semantic interoperability between clinical decision support systems and electronic health record ecosystems. The positive impacts of this system on the correct prediction of liver fibrosis severity thus suggest that it has the potential to assist medical professionals in diagnosing and treating this dangerous disease.}
}
@article{PEREIRA2020101760,
title = {A knowledge representation of the beginning of the innovation process: The Front End of Innovation Integrative Ontology (FEI2O)},
journal = {Data & Knowledge Engineering},
volume = {125},
pages = {101760},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2019.101760},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18301162},
author = {Ariane Rodrigues Pereira and João José Pinto Ferreira and Alexandra Lopes},
keywords = {Front end of innovation, Ontology, Entrepreneurship, Concept development, Design science},
abstract = {The initial phase of the innovation process is widely accepted as an important driver of positive results for new products and for the success of businesses. The Front End of Innovation (FEI) is a multidisciplinary area that includes a variety of activities, such as ideation, opportunity identification and analysis, feasibility analysis, global trends analysis, concept definition, customer and competitor analysis, and even business model development. Due to the number and variety of FEI responsibilities, this phase entails a considerable level of complexity and decision making. This fact is reflected in the literature, where one finds a variety of FEI approaches and proposals, seldom overlapping and offering no clear consensual guidance. This work aimed at overcoming this gap by proposing an Ontology for the Front End of Innovation as a comprehensive knowledge representation of the FEI, the so-called Front End of Innovation Integrative Ontology (FEI2O). The ontology balanced the differences and addressed the shortcomings of the main FEI Reference Models and included contributions from the field. This research builds on a combination of qualitative and quantitative methodologies. It combines the qualitative methods of interviewing and focus group discussion to collect the views of domain experts, used to refine the artefact and later to evaluate the final ontology. Quantitative analysis of data was carried out using the Attribute Agreement approach. The FEI2O explicitly provides a description of a domain regarding concepts, properties and relations of concepts. The main benefit of the FEI2O is to provide a comprehensive formal reference model and a common vocabulary.}
}
@article{MENDONCA2020101045,
title = {Ontological emergence scheme in self-organized and emerging systems},
journal = {Advanced Engineering Informatics},
volume = {44},
pages = {101045},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101045},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620300148},
author = {Maribel Mendonça and Niriaska Perozo and Jose Aguilar},
keywords = {Emerging systems, Self-organized systems, Ontology mining, Web semantic, Ontological emergence, Knowledge discovery},
abstract = {This paper presents the concept of “Ontological Emergence”, a process that seeks to adapt an ontology to the changes and new components in a self-organized and emergent system, through the application of a set of rules that allows the emergence of a new conceptualization (emerging concepts). The Ontological Emergence provides the structuration of the information and knowledge that could be generated in the system, creating conceptual models that can adequately represent the new behavior that is emerging. It arises from the need to represent ontologically a conceptualization of a reality that is dynamic, which cannot be pre-defined or pre-determined, in order to generate emerging knowledge models that follows the scalability and the evolution of it. In that sense, in this paper is proposed an “Ontological Emergence Scheme” based on a set of processes of registration, monitoring, analysis and adaptation of the various conceptual models that interact in the system, as well as on some processing rules in regard to requirements and information of the context, in order to allow the ontological emergence. In this proposal scheme, the Meta-ontologies guide the ontological emergence process through the definition of general categories, to facilitate the integration of concepts from different ontologies or data sources. Finally, the paper presents some case studies, showing its utility in self-organized and emergent systems.}
}
@article{YAN2019259,
title = {A graph convolutional neural network for classification of building patterns using spatial vector data},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {150},
pages = {259-273},
year = {2019},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2019.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0924271619300437},
author = {Xiongfeng Yan and Tinghua Ai and Min Yang and Hongmei Yin},
keywords = {Building pattern classification, Graph convolutional neural network, Machine learning, Spatial vector data, Graph Fourier transform, Deep learning},
abstract = {Machine learning methods, specifically, convolutional neural networks (CNNs), have emerged as an integral part of scientific research in many disciplines. However, these powerful methods often fail to perform pattern analysis and knowledge mining with spatial vector data because in most cases, such data are not underlying grid-like or array structures but can only be modeled as graph structures. The present study introduces a novel graph convolution by converting it from the vertex domain into a point-wise product in the Fourier domain using the graph Fourier transform and convolution theorem. In addition, the graph convolutional neural network (GCNN) architecture is proposed to analyze graph-structured spatial vector data. The focus of this study is the classical task of building pattern classification, which remains limited by the use of design rules and manually extracted features for specific patterns. The spatial vector data representing grouped buildings are modeled as graphs, and indices for the characteristics of individual buildings are investigated to collect the input variables. The pattern features of these graphs are directly extracted by training labeled data. Experiments confirmed that the GCNN produces satisfactory results in terms of identifying regular and irregular patterns, and thus achieves a significant improvement over existing methods. In summary, the GCNN has considerable potential for the analysis of graph-structured spatial vector data as well as scope for further improvement.}
}
@article{POLENGHI2022100286,
title = {Ontology-augmented Prognostics and Health Management for shopfloor-synchronised joint maintenance and production management decisions},
journal = {Journal of Industrial Information Integration},
volume = {27},
pages = {100286},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100286},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000832},
author = {Adalberto Polenghi and Irene Roda and Marco Macchi and Alessandro Pozzetti},
keywords = {Ontology, Reasoning, Prognostics and health management, PHM, maintenance, production},
abstract = {In smart factories, guaranteeing shopfloor-synchronised and real-time decision-making is essential to be responsive to the ever-changing internal environment, namely the shopfloor of the production system and assets. At operational level, decisions should balance counter acting objectives of maintenance and production; therefore, their decision-making processes should be joint and coordinated, to fulfil production requirements considering the health state of the assets. The knowledge of the current state is promoted by the application of Prognostics and Health Management (PHM) as an aid to support informed decision-making. Nevertheless, PHM-purposed information is usually not complete in terms of production requirements. To support joint maintenance and production management decisions, an ontological approach is proposed. The ontology, called ORMA (Ontology for Reliability-centred MAintenance), has a modular structure, including formalisation of asset, process, and product knowledge. Via suitable relationships, rules, and axioms, ORMA can infer product feasibility based on the current health state of the assets and their functional units. ORMA is implemented in a Flexible Manufacturing Line at a laboratory scale. Therein, an integrated solution, involving a health state detection algorithm that interacts with the ontology, supports human decision-making via a web-based dashboard; joint maintenance and production management decisions can be then taken, relying on diversified information provided by the PHM algorithm as well as the augmentation via ontology reasoning. The proposed ontology-based solution represents a step towards reconfigurability of smart factories where human and automated decision-making processes work in synergy.}
}
@article{WANG2021,
title = {Pathway-Driven Coordinated Telehealth System for Management of Patients With Single or Multiple Chronic Diseases in China: System Development and Retrospective Study},
journal = {JMIR Medical Informatics},
volume = {9},
number = {5},
year = {2021},
issn = {2291-9694},
doi = {https://doi.org/10.2196/27228},
url = {https://www.sciencedirect.com/science/article/pii/S2291969421001940},
author = {Zheyu Wang and Jiye An and Hui Lin and Jiaqiang Zhou and Fang Liu and Juan Chen and Huilong Duan and Ning Deng},
keywords = {chronic disease, telehealth system, integrated care, pathway, ontology},
abstract = {Background
Integrated care enhanced with information technology has emerged as a means to transform health services to meet the long-term care needs of patients with chronic diseases. However, the feasibility of applying integrated care to the emerging “three-manager” mode in China remains to be explored. Moreover, few studies have attempted to integrate multiple types of chronic diseases into a single system.
Objective
The aim of this study was to develop a coordinated telehealth system that addresses the existing challenges of the “three-manager” mode in China while supporting the management of single or multiple chronic diseases.
Methods
The system was designed based on a tailored integrated care model. The model was constructed at the individual scale, mainly focusing on specifying the involved roles and responsibilities through a universal care pathway. A custom ontology was developed to represent the knowledge contained in the model. The system consists of a service engine for data storage and decision support, as well as different forms of clients for care providers and patients. Currently, the system supports management of three single chronic diseases (hypertension, type 2 diabetes mellitus, and chronic obstructive pulmonary disease) and one type of multiple chronic conditions (hypertension with type 2 diabetes mellitus). A retrospective study was performed based on the long-term observational data extracted from the database to evaluate system usability, treatment effect, and quality of care.
Results
The retrospective analysis involved 6964 patients with chronic diseases and 249 care providers who have registered in our system since its deployment in 2015. A total of 519,598 self-monitoring records have been submitted by the patients. The engine could generate different types of records regularly based on the specific care pathway. Results of the comparison tests and causal inference showed that a part of patient outcomes improved after receiving management through the system, especially the systolic blood pressure of patients with hypertension (P<.001 in all comparison tests and an approximately 5 mmHg decrease after intervention via causal inference). A regional case study showed that the work efficiency of care providers differed among individuals.
Conclusions
Our system has potential to provide effective management support for single or multiple chronic conditions simultaneously. The tailored closed-loop care pathway was feasible and effective under the “three-manager” mode in China. One direction for future work is to introduce advanced artificial intelligence techniques to construct a more personalized care pathway.}
}
@article{SAAD20223439,
title = {Towards Improved Visualization and Optimization of Aquaculture Production Process},
journal = {Procedia Computer Science},
volume = {207},
pages = {3439-3448},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.531},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922014296},
author = {Aya Saad and Oscar Nissen and Espen Eilertsen and Finn Olav Bjørnson and Tore Norheim Hagtun and Odd-Gunnar Aspaas and Alexia Artemis Baikas and Sveinung Johan Ohrem},
keywords = {Knowledge representation, graph database, aquaculture production process},
abstract = {Aquaculture is one of the largest, and fastest growing industries in Norway. Recently, the industry has experienced significant development in the daily operations acquiring new technologies and systems that capture data and automate the different processes. These emerging technologies enable the generation of enormous amounts of data from sensors in the fish cages, cameras, boats, and feeding control rooms. Additional information relevant to the aquaculture industry is based on e-mails, manual notes, or intrinsic experiences and knowledge exchanges. One of the critical aspects of successful fish farming operation management, which is yet not achieved, is to allow domain experts to gain insight into the interconnection between the broad spectrum of heterogeneous data currently realized. This paper describes a framework for storing and retrieving critical information connected to fish farming based on a graph database approach. The overall architecture is presented with detailed illustrations of how data is visualized and interpreted through a user-friendly interface. Accordingly, this work demonstrates how aquaculture users can benefit from the system to identify possible connections in the data and reveal previously undiscovered causalities and correlations that suggest optimal actions. Further, studies and evaluations of the querying system are conducted, evaluating the capability of the proposed design to process complex relationships. This work showcases that the system helps fish farmers and aquaculture users gain knowledge, reveal hidden links in the data, and improve aquaculture operations.}
}
@article{BOOSHEHRI2021100074,
title = {Introducing the Open Energy Ontology: Enhancing data interpretation and interfacing in energy systems analysis},
journal = {Energy and AI},
volume = {5},
pages = {100074},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100074},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000288},
author = {Meisam Booshehri and Lukas Emele and Simon Flügel and Hannah Förster and Johannes Frey and Ulrich Frey and Martin Glauer and Janna Hastings and Christian Hofmann and Carsten Hoyer-Klick and Ludwig Hülk and Anna Kleinau and Kevin Knosala and Leander Kotzur and Patrick Kuckertz and Till Mossakowski and Christoph Muschner and Fabian Neuhaus and Michaja Pehl and Martin Robinius and Vera Sehn and Mirjam Stappel},
keywords = {Collaborative ontology development, Linked open data, Metadata annotation, Energy systems analysis},
abstract = {Heterogeneous data, different definitions and incompatible models are a huge problem in many domains, with no exception for the field of energy systems analysis. Hence, it is hard to re-use results, compare model results or couple models at all. Ontologies provide a precisely defined vocabulary to build a common and shared conceptualisation of the energy domain. Here, we present the Open Energy Ontology (OEO) developed for the domain of energy systems analysis. Using the OEO provides several benefits for the community. First, it enables consistent annotation of large amounts of data from various research projects. One example is the Open Energy Platform (OEP). Adding such annotations makes data semantically searchable, exchangeable, re-usable and interoperable. Second, computational model coupling becomes much easier. The advantages of using an ontology such as the OEO are demonstrated with three use cases: data representation, data annotation and interface homogenisation. We also describe how the ontology can be used for linked open data (LOD).}
}
@article{VAS20181032,
title = {Implementing connectivism by semantic technologies for self-directed learning},
journal = {International Journal of Manpower},
volume = {39},
number = {8},
pages = {1032-1046},
year = {2018},
issn = {0143-7720},
doi = {https://doi.org/10.1108/IJM-10-2018-0330},
url = {https://www.sciencedirect.com/science/article/pii/S0143772018000033},
author = {Réka Vas and Christian Weber and Dimitris Gkoumas},
keywords = {Networks, Ontologie, Connectivism, Self-directed learning},
abstract = {Purpose
Connectivism has been proposed to explain the impact of new technologies on learning. According to this approach, learning may occur even outside the individual within an organization or a system. Learning objectives are not defined in advance and learning requires the ability to form connections and use networks to find the required knowledge. The connections by which individuals can learn are more important than what they currently know. The purpose of this paper is to investigate if a measure, rating the importance of concepts, can be derived from a network representation of the learning domain and if highly connected concepts – with high importance value – can describe whether information is explored in such ways as assumed by connectivism.
Design/methodology/approach
The authors empirically examined if the proposed measure can provide insight on the role of connections in learning and explain the reasons behind passing certain parts of a test using a linear regression model.
Findings
The results are twofold. First, an implementation of the information exploration principle of connectivism has been introduced, applying semantic technologies and the importance measure. Second, although no significant effects could be isolated, trends in performance improvement concerning highly important concepts were identified.
Originality/value
However, connectivism has been known since 2005, it is still lacking for successful implementations. The presented approach of a concept importance measure is a promising starting point by providing means of connected learning, enabling individuals to effectively improve their personal abilities to better fit job demand.}
}
@article{NASRABADI2024123551,
title = {The implication of user-generated content in new product development process: A systematic literature review and future research agenda},
journal = {Technological Forecasting and Social Change},
volume = {206},
pages = {123551},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123551},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524003470},
author = {Mohamadreza Azar Nasrabadi and Yvan Beauregard and Amir Ekhlassi},
keywords = {User-generated content, New product development, Product design, Product innovation, Systematic literature review, Social media},
abstract = {This study aims to provide a comprehensive overview of the current state of user-generated content (UGC) research within the context of new product development (NPD). A systematic literature review (SLR) was conducted across three prominent databases, namely Web of Science, Scopus, and Science Direct, using keywords to identify relevant articles. 5585 of 13,381 articles were deemed relevant following the application of inclusion and exclusion criteria. These articles were then thoroughly analyzed to create a comprehensive review of the topic. The selection process involved evaluating the titles and abstracts of all publications that were discovered, and carefully choosing 136 articles for full-text review. From these, 58 articles were ultimately selected for detailed analysis in this study. The study highlights the role of UGC in augmenting NPD process and identifies potential areas for future research based on evidence derived from an SLR of articles published between 2012 and 2023. The research methodologies adopted in this paper involve descriptive analysis and TCM framework (T-themes, C-contexts, and M-methodologies). Finally, the article concludes by shedding light on its potential applications by providing four themes and highlighting the importance of future research in the field with five propositions.}
}
@article{BITSCH2022577,
title = {Dynamic adaption in cyber-physical production systems based on ontologies},
journal = {Procedia Computer Science},
volume = {200},
pages = {577-584},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.255},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002642},
author = {Günter Bitsch and Pascal Senjic and Jeremy Askin},
keywords = {Cyber-Physical Production Systems (CPPSs), Ontology, Adaptability, Flexibility},
abstract = {The paradigmatic shift of production systems towards Cyber-Physical Production Systems (CPPSs) requires the development of flexible and decentralized approaches. In this way, such systems enable manufacturers to respond quickly and accurately to changing requirements. However, domain-specific applications require the use of suitable conceptualizations. The issue at hand, when using various conceptualizations is the interoperability of different ontologies. To achieve flexibility and adaptability in CPPSs though requires overcoming interoperability issues within CPPSs. This paper presents an approach to increase flexibility and adaptability in CPPSs while addressing the interoperability issue. In this work, OWL ontologies conceptualize domain knowledge. The Intelligent Manufacturing Knowledge Ontology Repository (IMKOR) connects the domain knowledge in different ontologies. Testing if adaptions in one ontology within the IMKOR provide knowledge to the whole IMKOR. The tests showed, positive results and the repository makes the knowledge available to the whole CPPS. Furthermore, an increase in flexibility and adaptability was noticed.}
}
@article{LI2023102978,
title = {CoAxNN: Optimizing on-device deep learning with conditional approximate neural networks},
journal = {Journal of Systems Architecture},
volume = {143},
pages = {102978},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2023.102978},
url = {https://www.sciencedirect.com/science/article/pii/S1383762123001571},
author = {Guangli Li and Xiu Ma and Qiuchu Yu and Lei Liu and Huaxiao Liu and Xueying Wang},
keywords = {On-device deep learning, Efficient neural networks, Model approximation and optimization},
abstract = {While deep neural networks have achieved superior performance in a variety of intelligent applications, the increasing computational complexity makes them difficult to be deployed on resource-constrained devices. To improve the performance of on-device inference, prior studies have explored various approximate strategies, such as neural network pruning, to optimize models based on different principles. However, when combining these approximate strategies, a large parameter space needs to be explored. Meanwhile, different configuration parameters may interfere with each other, damaging the performance optimization effect. In this paper, we propose a novel model optimization framework, CoAxNN, which effectively combines different approximate strategies, to facilitate on-device deep learning via model approximation. Based on the principles of different approximate optimizations, our approach constructs the design space and automatically finds reasonable configurations through genetic algorithm-based design space exploration. By combining the strengths of different approximation methods, CoAxNN enables efficient conditional inference for models at runtime. We evaluate our approach by leveraging state-of-the-art neural networks on a representative intelligent edge platform, Jetson AGX Orin. The experimental results demonstrate the effectiveness of CoAxNN, which achieves up to 1.53× speedup while reducing energy by up to 34.61%, with trivial accuracy loss on CIFAR-10/100 and CINIC-10 datasets.}
}
@article{PERNISCH2021100658,
title = {Beware of the hierarchy — An analysis of ontology evolution and the materialisation impact for biomedical ontologies},
journal = {Journal of Web Semantics},
volume = {70},
pages = {100658},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100658},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000330},
author = {Romana Pernisch and Daniele Dell’Aglio and Abraham Bernstein},
keywords = {Ontology evolution, Materialisation, Evolution impact, Ontology change},
abstract = {Ontologies are becoming a key component of numerous applications and research fields. But knowledge captured within ontologies is not static. Some ontology updates potentially have a wide ranging impact; others only affect very localised parts of the ontology and their applications. Investigating the impact of the evolution gives us insight into the editing behaviour but also signals ontology engineers and users how the ontology evolution is affecting other applications. However, such research is in its infancy. Hence, we need to investigate the evolution itself and its impact on the simplest of applications: the materialisation. In this work, we define impact measures that capture the effect of changes on the materialisation. In the future, the impact measures introduced in this work can be used to investigate how aware the ontology editors are about consequences of changes. By introducing five different measures, which focus either on the change in the materialisation with respect to the size or on the number of changes applied, we are able to quantify the consequences of ontology changes. To see these measures in action, we investigate the evolution and its impact on materialisation for nine open biomedical ontologies, most of which adhere to the EL++ description logic. Our results show that these ontologies evolve at varying paces but no statistically significant difference between the ontologies with respect to their evolution could be identified. We identify three types of ontologies based on the types of complex changes which are applied to them throughout their evolution. The impact on the materialisation is the same for the investigated ontologies, bringing us to the conclusion that the effect of changes on the materialisation can be generalised to other similar ontologies. Further, we found that the materialised concept inclusion axioms experience most of the impact induced by changes to the class inheritance of the ontology and other changes only marginally touch the materialisation.}
}
@article{YUAN2025111635,
title = {Efficient dehazing network based on mix structure for single image with uneven haze distribution},
journal = {Engineering Applications of Artificial Intelligence},
volume = {159},
pages = {111635},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111635},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625016379},
author = {Kangle Yuan and Jianguo Wei and Wenhuan Lu},
keywords = {Image dehazing, Mix structure, Uneven haze distribution, Parallel attention mechanism, Hierarchical vision transformer, Fusion structure},
abstract = {Although the defogging algorithm based on convolutional neural networks has made significant progress on synthetic uniform foggy datasets, it still exhibits subpar performance on real non-uniform foggy images. In recent years, the transformer network has been applied in the field of image dehazing and has achieved good results in removing haze from non-uniform hazy images. However, two main issues remain: The neglect of the multi-scale characteristics of the image; And the lack of effective strategies to better combine the convolutional structure with the transformer. In this paper, we propose an efficient image dehazing network framework based on a novel hybrid structure. Specifically, the mixed structure block consists of a convolutional component utilizing a parallel attention mechanism and a Transformer architecture. This design effectively captures extensive areas of blur while simultaneously restoring texture details. Additionally, it takes into account the uneven distribution of haze, thereby addressing the challenges associated with removing uneven fog in single images more effectively.Meanwhile, we propose a fusion structure that comprises a skip branch and a main branch, enabling dynamic adjustment of the receptive field size and selection of the appropriate convolution kernel. The experimental results demonstrate that the dehazing algorithm proposed by us outperforms existing methods in terms of dehazing performance.}
}
@article{SCHNEIDER2020103402,
title = {Design of knowledge-based systems for automated deployment of building management services},
journal = {Automation in Construction},
volume = {119},
pages = {103402},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103402},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520309821},
author = {Georg F. Schneider and Georgios D. Kontes and Haonan Qiu and Filipe J. Silva and Mircea Bucur and Jakub Malanik and Zdenek Schindler and Panos Andriopolous and Pablo {de Agustin-Camacho} and Ander Romero-Amorrortu and Gunnar Grün},
keywords = {Building management services, Knowledge-based systems, Energy efficiency, Knowledge engineering, Ontology},
abstract = {Despite its high potential, the building's sector lags behind in reducing its energy demand. Tremendous savings can be achieved by deploying building management services during operation, however, the manual deployment of these services needs to be undertaken by experts and it is a tedious, time and cost consuming task. It requires detailed expert knowledge to match the diverse requirements of services with the present constellation of envelope, equipment and automation system in a target building. To enable the widespread deployment of these services, this knowledge-intensive task needs to be automated. Knowledge-based methods solve this task, however, their widespread adoption is hampered and solutions proposed in the past do not stick to basic principles of state of the art knowledge engineering methods. To fill this gap we present a novel methodological approach for the design of knowledge-based systems for the automated deployment of building management services. The approach covers the essential steps and best practices: (1) representation of terminological knowledge of a building and its systems based on well-established knowledge engineering methods; (2) representation and capturing of assertional knowledge on a real building portfolio based on open standards; and (3) use of the acquired knowledge for the automated deployment of building management services to increase the energy efficiency of buildings during operation. We validate the methodological approach by deploying it in a real-world large-scale European pilot on a diverse portfolio of buildings and a novel set of building management services. In addition, a novel ontology, which reuses and extends existing ontologies is presented.}
}
@article{RASMUSSEN2019102956,
title = {Managing interrelated project information in AEC Knowledge Graphs},
journal = {Automation in Construction},
volume = {108},
pages = {102956},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102956},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519300378},
author = {Mads Holten Rasmussen and Maxime Lefrançois and Pieter Pauwels and Christian Anker Hviid and Jan Karlshøj},
keywords = {Linked data, Building information modelling, Complex design, Ontology, Inference, Information exchange, BIM, AEC Knowledge Graph, Linked building data},
abstract = {In the architecture, engineering and construction (AEC) industry stakeholders from different companies and backgrounds collaborate in realising a common goal being some physical structure. The exact goal is typically not known from the beginning, and throughout all design stages, new decisions are made - similarly to other design industries [1]. As a result, the design must adapt and subsequent consequences follow. With working methods being predominantly document-centric, highly interrelated and rapidly changing design data in a complex network of decisions, requirements and product specifications is primarily captured in static documents. In this paper, we consider a purely data-driven approach based on semantic web technologies and an earlier proposed Ontology for Property Management (OPM). The main contribution of this work consists of extensions for OPM to account for new competency questions including the description of property reliability and the reasoning logic behind derived properties. The secondary contribution is the specification of a homogeneous way to generate parametric queries for managing an OPM-compliant AEC Knowledge Graph (AEC-KG). A software library for operating an OPM-compliant AEC-KG is further presented in the form of an OPM Query Generator (OPM-QG). The library generates SPARQL 1.1 queries to query and manipulate construction project Knowledge Graphs represented using OPM. The OPM ontology aligns with latest developments in the W3C Community Group on Linked Building Data and suggests an approach to working with design data in a distributed environment using separate graphs for explicit facts and for materialised, deduced data. Finally, we evaluate the suggested approach using an open-source software artefact developed using OPM and OPM-QG, demonstrated online with an actual building Knowledge Graph. The particular design task evaluated is performing heat loss calculations for spaces of a future building using an AEC-KG described using domain- and project specific extensions of the Building Topology Ontology (BOT) in combination with OPM. With this work, we demonstrate how a typical engineering task can be accomplished and managed in an evolving design environment, thereby providing the engineers with insights to support decision making as changes occur. The application uses a strict division between the client viewer and the actual data model holding design logic, and can easily be extended to support other design tasks.}
}
@article{RANYA20233479,
title = {Application and evaluation of sentence embedding and clustering methods in the context of concept hierarchy construction},
journal = {Procedia Computer Science},
volume = {225},
pages = {3479-3487},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.343},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923015016},
author = {El Hadri Ranya and Cimpan Sorana and Damas Luc and Boissière Julien},
keywords = {Concept hierarchy, Evaluation, Sentence Embedding, Clustering, Fuzzy sets},
abstract = {Concept hierarchies, as part of knowledge representation methods, play an important role in supporting the exchange and sharing of information. We developed and automated a concept hierarchies construction process which includes several artificial intelligence techniques. When automating their construction from an existing, more or less structured, body of knowledge, the evaluation of the resulting concept hierarchy is an important step. We propose an approach for concept hierarchy construction (CHC) from short sentences, that makes use of methods like Sentence Embedding, Clustering, and Automatic Labeling to create a hierarchical representation consisting of three layers. Our major focus in this paper is not on the algorithms used but on their evaluation using manual clustering by experts and fuzzy sets.}
}
@article{NING2021102303,
title = {Differential privacy protection on weighted graph in wireless networks},
journal = {Ad Hoc Networks},
volume = {110},
pages = {102303},
year = {2021},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2020.102303},
url = {https://www.sciencedirect.com/science/article/pii/S1570870520306612},
author = {Bo Ning and Yunhao Sun and Xiaoyu Tao and Guanyu Li},
keywords = {Wireless networks, Weighted graph, Privacy protection, Differential privacy},
abstract = {With the development of 5G communication technology, the Internet of Things technology has ushered in the development opportunity. In the application of Internet of Things, spatial and social relations can be used to provide users with convenience in life and work, meanwhile there is also the risk of personal privacy disclosure. The data transmitted in the wireless network contains a large number of graph structure data, and the edge weight in weighted graph increases the risk of privacy disclosure, therefore in this paper we design a privacy protection algorithm for weighted graph, and adopts the privacy protection model to realize the privacy protection of edge weight and graph structure. Firstly, the whole graph sets are disturbed and the noises are added during the process of graph generation. Secondly, the privacy budget is allocated to protect the weight values of edges. The graph is encoded to deal with the structure of graph conveniently without separating from the information of edges, and then the disturbed edge weight is integrated into the graph. After that the privacy protection of the graph structure is realized in the process of frequent graph mining combined with differential privacy. Finally, the algorithm proposed in this paper is validated by experiments.}
}
@article{LU2018128,
title = {Resource virtualization: A core technology for developing cyber-physical production systems},
journal = {Journal of Manufacturing Systems},
volume = {47},
pages = {128-140},
year = {2018},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0278612518300657},
author = {Yuqian Lu and Xun Xu},
keywords = {Smart factory, Cyber-physical production system, Resource virtualization, Digital twin, Ontology, Semantic web},
abstract = {Smart factory in the context of Industry 4.0 is the next wave of smart manufacturing solution to empower companies to rapidly configure manufacturing facilities and processes to enable the fast production of individualized products at change scales. A key enabling technology for developing a smart factory is resource virtualization or creation of digital twins. The presented research fills the gap that the industry needs a practical methodology to enable themselves to easily virtualize their manufacturing assets for developing a smart factory solution. A test-driven resource virtualization framework is proposed as the recommendation for the industry to adopt to create digital twins for a smart factory. The proposed framework draws inspiration from past resource virtualization outcomes with special attention paid to the usability of the proposed framework in a business environment. It provides a straightforward process for companies to create digital twins by specifying the digital twin hierarchy, the information to be modeled, and the modeling method. To validate the proposed framework, a case study was undertaken at an international company, to create digital twins for all their manufacturing resources. The testing result showed that the proposed resource virtualization framework and developed tools are easy to use in a practical business environment to virtualize complex factory setups in the cyberspace.}
}
@article{SALGUERO20181,
title = {Ontology-based feature generation to improve accuracy of activity recognition in smart environments},
journal = {Computers & Electrical Engineering},
volume = {68},
pages = {1-13},
year = {2018},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2018.03.048},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617315483},
author = {A.G. Salguero and M. Espinilla},
keywords = {Activity recognition, Smart environments, Ambient assisted living (AAL), Activities of Daily Living (ADL), Ontology, Data-Driven approaches, Knowledge-Driven approaches},
abstract = {In recent years, many techniques have been proposed for automatic recognition of Activities of Daily Living from smart home sensor data. However, classifiers usually use features created ad hoc. In this work, the use of ontologies is proposed for the fully automatic generation of these features. The process consists of converting the original dataset into an ontology and then combine all the concepts and relations in that ontology to obtain relevant class expressions. The high formalization of ontologies allows us to reduce the search space by discarding many meaningless expressions, such as contradictory or unsatisfiable expressions. The relevant class expressions are then used as features by the classifiers to build the classification model. To validate our proposal, we have used as reference the results obtained by four different classification algorithms that use the most commonly used features.}
}
@article{LEGUILLARME2023103497,
title = {A practical approach to constructing a knowledge graph for soil ecological research},
journal = {European Journal of Soil Biology},
volume = {117},
pages = {103497},
year = {2023},
issn = {1164-5563},
doi = {https://doi.org/10.1016/j.ejsobi.2023.103497},
url = {https://www.sciencedirect.com/science/article/pii/S116455632300033X},
author = {Nicolas {Le Guillarme} and Wilfried Thuiller},
keywords = {Data integration, Knowledge graph, Ontology, Reasoning, Soil ecology},
abstract = {With the rapid accumulation of biodiversity data, data integration has emerged as a hot topic in soil ecology. Data integration has indeed the potential to advance our knowledge of global patterns in soil biodiversity by facilitating large-scale meta-analytical studies of soil ecosystems. However, ecologists are still poorly equipped when it comes to integrating disparate datasets. In recent years, knowledge graphs have emerged as a powerful tool for integrating large amounts of distributed heterogeneous data while making these data more easily interpretable by humans and computers. This paper presents a practical approach to constructing a biodiversity knowledge graph from heterogeneous and distributed (semi-)structured data sources. To illustrate our approach, we integrate several datasets on the trophic ecology of soil organisms into a trophic knowledge graph and show how both explicit and implicit information can be retrieved from the graph to support multi-trophic studies.}
}
@article{DAMICO2020803,
title = {BIM And GIS Data Integration: A Novel Approach Of Technical/Environmental Decision-Making Process In Transport Infrastructure Design},
journal = {Transportation Research Procedia},
volume = {45},
pages = {803-810},
year = {2020},
note = {Transport Infrastructure and systems in a changing world. Towards a more sustainable, reliable and smarter mobility.TIS Roma 2019 Conference Proceedings},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.02.090},
url = {https://www.sciencedirect.com/science/article/pii/S235214652030140X},
author = {Fabrizio D’Amico and Alessandro Calvi and Eleonora Schiattarella and Mauro Di Prete and Valerio Veraldi},
keywords = {BIM, GIS, Infrastructures, Airport, Environment, Data Integration},
abstract = {The European Directive 2014/24/EU and its recent Italian transposition law DM 560/2017 encourage an extensive use of BIM-based practices in transport infrastructure design. Therefore, a shift from the traditional design approach towards a shared and highly integrated model, capable of including the various design phases along with economic, operational and environmental concerns, is observed. In such a framework, this work evaluates the benefits returning from the integration between geospatially-referenced data and the BIM models for a more aware design approach. The major aim of this study is to underline the potential of an interoperable and shared model supplemented by GIS data, in minimizing or definitely removing the possible conflicts that typically arise between the infrastructure design and environmental constraints. Particularly, thanks to both the simultaneous assessment of each environmental component and the evaluation of the different project configurations, this methodology can provide an integrated technical/environmental overview of the design. As a result, it allows for immediately verifying the project to comply with the national minimum environmental criteria, which are mandatory for contractors according to the Italian environmental law n° 221/2015 and the new Italian Public Procurement Code. The proposed approach was finally tested on an airport infrastructure. Preliminary results have shown viability of the data management model for supporting designer’s choices in the various project phases, thereby proving this methodology to be worthy for implementation in infrastructure design procedures.}
}
@article{PREVENTIS2021275,
title = {CLONE: Collaborative Ontology Editor as a Service in the Cloud},
journal = {Procedia Computer Science},
volume = {184},
pages = {275-282},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921007791},
author = {Alexandros Preventis and Euripides G.M. Petrakis},
keywords = {Ontology, Collaborative editor, Service Oriented Architecture, Cloud Computing},
abstract = {The evolution of Web and cloud services technology has facilitated collaboration on the Web, providing the means for concurrent editing, change tracking and storing files in the cloud (e.g. Google Docs, Office 365). Ontology development teams could greatly benefit from this technology, that until now have been applied mainly to document processing. We introduce CLONE, a Web-based ontology editor that runs in the cloud and provides a real-time collaborative environment for creating and editing ontologies. CLONE is designed as a service-oriented architecture taking advantages of the easy extensibility and scalability features of this approach. CLONE provides all the essential features of stand-alone ontology editors, as well as significant collaboration features, including concurrent editing, editing history, team conversations and role-based access-authorization mechanisms.}
}
@article{RODRIGUEZREVELLO2023120239,
title = {KNIT: Ontology reusability through knowledge graph exploration},
journal = {Expert Systems with Applications},
volume = {228},
pages = {120239},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120239},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423007418},
author = {Jorge Rodríguez-Revello and Cristóbal Barba-González and Maciej Rybinski and Ismael Navas-Delgado},
keywords = {Life sciences, Ontology, Ontology learning, Knowledge graphs},
abstract = {Ontologies have become a standard for knowledge representation across several domains. In Life Sciences, numerous ontologies have been introduced to represent human knowledge, often providing overlapping or conflicting perspectives. These ontologies are usually published as OWL or OBO, and are often registered in open repositories, e.g., BioPortal. However, the task of finding the concepts (classes and their properties) defined in the existing ontologies and the relationships between these concepts across different ontologies – for example, for developing a new ontology aligned with the existing ones – requires a great deal of manual effort in searching through the public repositories for candidate ontologies and their entities. In this work, we develop a new tool, KNIT, to automatically explore open repositories to help users fetch the previously designed concepts using keywords. User-specified keywords are then used to retrieve matching names of classes or properties. KNIT then creates a draft knowledge graph populated with the concepts and relationships retrieved from the existing ontologies. Furthermore, following the process of ontology learning, our tool refines this first draft of an ontology. We present three BioPortal-specific use cases for our tool. These use cases outline the development of new knowledge graphs and ontologies in the sub-domains of biology: genes and diseases, virome and drugs.}
}
@article{KUTT2023119968,
title = {Loki – the semantic wiki for collaborative knowledge engineering},
journal = {Expert Systems with Applications},
volume = {224},
pages = {119968},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119968},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423004700},
author = {Krzysztof Kutt and Grzegorz J. Nalepa},
keywords = {Knowledge engineering, Semantic wiki, Software engineering, Unit tests, Prolog},
abstract = {We present Loki, a semantic wiki designed to support the collaborative knowledge engineering process with the use of software engineering methods. Designed as a set of DokuWiki plug-ins, it provides a variety of knowledge representation methods, including semantic annotations, Prolog clauses, and business processes and rules oriented to specific tasks. Knowledge stored in Loki can be retrieved via SPARQL queries, in-line Semantic MediaWiki-like queries, or Prolog goals. Loki includes a number of useful features for a group of experts and knowledge engineers developing the wiki, such as knowledge visualization, ontology storage, or code hint and completion mechanism. Reasoning unit tests are also introduced to validate knowledge quality. The paper is complemented by the formulation of the collaborative knowledge engineering process and the description of experiments performed during Loki development to evaluate its functionality. Loki is available as free software at https://loki.re.}
}
@article{TERZIYAN20241388,
title = {Taxonomy-Informed Neural Networks for Smart Manufacturing},
journal = {Procedia Computer Science},
volume = {232},
pages = {1388-1399},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.01.137},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924001376},
author = {Vagan Terziyan and Oleksandra Vitko},
keywords = {neural networks, machine learning, informed machine learning, physics-informed neural networks, taxonomy, Industry 4.0},
abstract = {A neural network (NN) is known to be an efficient and learnable tool supporting decision-making processes particularly in Industry 4.0. The majority of NNs are data-driven and, therefore, depend on training data quantity and quality. The current trend in enhancing data-driven models with knowledge-based models promises to enable effective NNs with less data. So-called physics-informed NNs use additional knowledge from computational science to improve NN training. Quite much of the knowledge is available as logical constraints from domain ontologies, and NNs may benefit from using it. In this paper, we study the concept of Taxonomy-Informed NN (TINN), which combines data-driven training of NNs with ontological knowledge. We study different patterns of NN training with additional knowledge on class-subclass hierarchies and instance-class relationships with potential for federated learning. Our experiments show that additional knowledge, which influences TINNs’ training process through the loss function at backpropagation, improves the quality of trained models.}
}@article{CHENG20221,
title = {6G service-oriented space-air-ground integrated network: A survey},
journal = {Chinese Journal of Aeronautics},
volume = {35},
number = {9},
pages = {1-18},
year = {2022},
issn = {1000-9361},
doi = {https://doi.org/10.1016/j.cja.2021.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1000936121004738},
author = {Nan CHENG and Jingchao HE and Zhisheng YIN and Conghao ZHOU and Huaqing WU and Feng LYU and Haibo ZHOU and Xuemin SHEN},
keywords = {Mobile Edge Computing (MEC), Network Function Virtualization (NFV), Network slicing, Service-oriented network, Software Defined Networking (SDN), Space-Air-Ground Integrated Networks (SAGINs)},
abstract = {As an indispensable component of the emerging 6G networks, Space-Air-Ground Integrated Networks (SAGINs) are envisioned to provide ubiquitous network connectivity and services by integrating satellite networks, aerial networks, and terrestrial networks. In 6G SAGINs, a wide variety of network services with the features of diverse requirements, complex mobility, and multi-dimensional resources will pose great challenges to service provisioning, which urges the development of service-oriented SAGINs. In this paper, we conduct a comprehensive review of 6G SAGINs from a new perspective of service-oriented network. First, we present the requirements of service-oriented networks, and then propose a service-oriented SAGINs management architecture. Two categories of critical technologies are presented and discussed, i.e., heterogeneous resource orchestration technologies and the cloud-edge synergy technologies, which facilitate the interoperability of different network segments and cooperatively orchestrate heterogeneous resources across different domains, according to the service features and requirements. In addition, the potential future research directions are also presented and discussed.}
}
@article{XUE2018475,
title = {Development of an urban FEW nexus online analyzer to support urban circular economy strategy planning},
journal = {Energy},
volume = {164},
pages = {475-495},
year = {2018},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2018.08.198},
url = {https://www.sciencedirect.com/science/article/pii/S0360544218317341},
author = {Jingyan Xue and Gengyuan Liu and Marco Casazza and Sergio Ulgiati},
keywords = {Urban, Energy-water-food, Nexus, Policy, Circular economy},
abstract = {Growth of urban population around the world and, particularly, within urban areas, has placed various pressing challenges on Food, Energy, and Water (FEW) such as food security, water safety as well as energy scarcity and so on. Current studies on urban FEW nexus are mainly focused on the correlation analysis of elements by pairs, while these works are developed separately. With respect to the methods, the existing researches mostly adopt the bottom-up approach, accounting for the direct relationship between the individual production sectors. While the associations between the internal elements of the system still lack of simulation. In this study, we aim at developing an online open access tool for cities, the Urban Circular Economy Calculator (UCEC), which enables to develop different circular economy scenarios associated to FEW management. UCEC v1.0 uses Beijing data as test case. In particular, more than 20 circular economy policies related on food, energy and water are selected and divided into 6 categories. Long-term simulations on the social, economic and environmental impacts are provided to test the trajectories of policy effects. Being an open access tool, UCEC can be used also for supporting participatory processes as an urban management instrument. The solution is economically and financially feasible, due to the low level of technical requirements. The necessity of such a tool is proved by the societal need of transition toward a low-carbon and sustainable framework, which can be effectively supported by the introduction of circular economy. This transition, such as the idea behind UCEC, should preserve (or even improve) the societal wellbeing, while increasing basic resources (i.e.: FEW) accessibility, security and preservation.}
}
@article{FERNANDEZLOPEZ2019100492,
title = {Why are ontologies not reused across the same domain?},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100492},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300726},
author = {Mariano Fernández-López and María Poveda-Villalón and Mari Carmen Suárez-Figueroa and Asunción Gómez-Pérez},
keywords = {Ontology reuse, Ontology reliability, Ontology heterogeneity, Percentage of reuse, Reuse patterns, Drawbacks for reuse},
abstract = {Even though one of the main characteristics of ontologies has always been claimed to be their reusability, throughout this paper it will be shown that ontology reuse across a given domain is not a consolidated practice. We have carried out a statistical study on ontology reuse in the ontologies collected in Linked Open Vocabularies (LOV), in addition to a particular analysis of a use case. The results of the present work show that, when building an ontology, the heterogeneity between the needed conceptualization and that of available ontologies, as well as the deficiencies in some of such ontologies (concerning documentation, licensing, etc.) are important obstacles for reusing ontologies of the same domain of the ontology under development. A possible approach to lessen these problems could be the creation of communities similar to open software ones in charge of developing and maintaining ontologies.}
}
@article{LI2021103592,
title = {An information sharing strategy based on linked data for net zero energy buildings and clusters},
journal = {Automation in Construction},
volume = {124},
pages = {103592},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103592},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521000431},
author = {Yehong Li and Shushan Hu and Cathal Hoare and James O'Donnell and Raúl García-Castro and Sergio Vega-Sánchez and Xiangyang Jiang},
keywords = {Linked data, Information sharing, Building prosumer, Net zero energy cluster, Net zero energy building},
abstract = {Buildings now incorporate increasing levels of renewable energy to the point where net zero energy buildings (NetZEBs) and net zero energy clusters (NetZECs) have the potential to become widely used. Information exchange among stakeholders is essential to enable energy sharing among buildings that form a NetZECs. However, traditional, centralised energy management usually places emphasis on the supply side and does not address data exchange among individual stakeholders. This paper proposes an information sharing strategy based on linked data for improved management of NetZEBs and NetZECs. This strategy systematically integrates stakeholders' engagement by using energy performance indicators as information carriers and linked data as engagement channels. A case study proves the advantages of the developed strategy for improving the energy balance at individual building level, building cluster level, and community level. It is found that the strategy could help achieve NetZEBs and NetZECs through empowering stakeholders' interaction and information sharing.}
}
@article{HINZE2019100516,
title = {Manual semantic annotations: User evaluation of interface and interaction designs},
journal = {Journal of Web Semantics},
volume = {58},
pages = {100516},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.100516},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300459},
author = {Annika Hinze and Ralf Heese and Alexa Schlegel and Adrian Paschke},
keywords = {Manual semantic annotation, Semantic identity, Digital curation, Linked data, Named entity recognition, Semantic analysis, Semantic web, User evaluation},
abstract = {Semantic annotation is the process by which existing texts receive a mark-up that allows automatic identification of named entities (e.g., to distinguish between a turkey bird and the country Turkey). Manual annotation is useful both as stand-alone process in a domain-specific setting or as post-processing for automatic annotation algorithms. Development of most annotation tools strongly focuses on providing novel functionality — end-user evaluations of interface and user interaction are rare. This article reports on the results of a series of user studies executed to explore how non-expert users understand the process of enriching texts with semantic annotations. We find that these users can easily create simple semantic annotations (e.g., assigning concepts to text passages) but have difficulties understanding complex semantic annotations (e.g., assigning semantic identifiers to text passages). We present the results of three user studies on manual semantic annotation and discusses the lessons learned about both the semantic enrichment process and our methodology of exposing non-experts to semantic enrichment.}
}
@article{NICOLAS20237318,
title = {Multi-relational and Concept Analysis based Knowledge extraction in the Industry 4.0: A systematic mapping},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {7318-7329},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.345},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323007127},
author = {Leutwyler Nicolás and Lezoche Mario and Torres Diego and Panetto Hervé},
keywords = {Enterprise interoperability, AI-based enterprise systems, Systems interoperability, Cyber physical system, Smart factory},
abstract = {Smart Enterprises, Smart Manufacturing, and Cyber-Physical Systems are gaining traction in many industry areas. On top of that, the amounts of available data grow rapidly, and organizations are eager to exploit their advantages. To accomplish that, it is mandatory to have a wide variety of methods and algorithms for knowledge extraction in order to fit the different needs and problems of the industry. In this study, we review and dissect the current state of the art in knowledge extraction applied to smart enterprises, smart manufacturing, and cyber-physical systems. More specifically, we provide a classification of the characteristics of the available methods in the literature according to their applications, and point out areas of improvement.}
}
@article{BAI2024102258,
title = {S_IDS: An efficient skyline query algorithm over incomplete data streams},
journal = {Data & Knowledge Engineering},
volume = {149},
pages = {102258},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102258},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23001180},
author = {Mei Bai and Yuxue Han and Peng Yin and Xite Wang and Guanyu Li and Bo Ning and Qian Ma},
keywords = {Skyline query, Incomplete data stream, Hierarchical grid index, Dependency rule},
abstract = {The efficient processing of mass stream data has attracted wide attention in the database field. The skyline query on the sensor data stream can monitor multiple targets in real time, to avoid abnormal events such as fire and explosion, which is very useful in the practical application of sensor data monitoring. However, real-world stream data may often contain incomplete data attributes due to faulty sensing devices or imperfect data collection techniques. Skyline queries over incomplete data streams may lead to a lack of transitivity and loop domination issues. To solve the problem of the skyline query over incomplete data streams, firstly, this paper uses differential dependency rule (DD) to fill the missing attribute values of data in the incomplete data stream. Then, the hierarchical grid index (HGrid) is introduced into the field of skyline query to improve pruning efficiency. In the process of skyline calculation, this paper only keeps as few calculation results as possible for the data that may affect the result to avoid a large number of repeated calculations. Thus, S_IDS (Skyline query algorithm over Incomplete Data Stream) is proposed to query skyline results with high confidence from the incomplete data stream. Finally, by comparing with the most advanced skyline query algorithms over incomplete data streams, the correctness and efficiency of the proposed S_IDS algorithm are proved.}
}
@article{SILVA201928,
title = {Visualization and analysis of schema and instances of ontologies for improving user tasks and knowledge discovery},
journal = {Journal of Computer Languages},
volume = {51},
pages = {28-47},
year = {2019},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X17302458},
author = {Isabel Cristina Siqueira Silva and Giuseppe Santucci and Carla Maria Dal Sasso Freitas},
keywords = {Ontology, Information visualization, Data visualization, Visual analytics, Interaction},
abstract = {Ontologies are an important resource for knowledge representation. Their structure can be complex due to role relations between several concepts, distinct attributes, and different instances. In this paper, we discuss a Visual Analytics solution, relying on the use of multiple coordinated views for exploring different ontology aspects and a novel use of the degree of interest (DoI) suppression technique to reduce the complexity of the ontology visual representation. Visual Analytics facilitates the understanding of the domain and tasks represented by ontologies, thus allowing to carry out exploratory analysis to optimize the comprehension of data semantics including non-explicit relationships between data. Through the DoI technique, we place the main concept in focus, distinguishing it from the unnecessary information and facilitating the analysis and understanding of correlated data. We evaluated all the devised solutions, and the results reinforce the importance of providing visualization and analysis techniques dedicated to the schema and instances levels of ontologies for the discovery of non-explicit information.}
}
@article{PAKKALA2024100644,
title = {Improving efficiency and quality of operational industrial production assets information management in customer–vendor interaction},
journal = {Journal of Industrial Information Integration},
volume = {41},
pages = {100644},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100644},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24000888},
author = {Daniel Pakkala and Jukka Kääriäinen and Teemu Mätäsniemi},
keywords = {Information management, Industrial assets management, Research-industry collaboration, Digital marketplace concept, Design science research},
abstract = {Industrial assets management has a key role in ensuring continuity of industrial production via purchasing, warehousing and maintenance of devices and spare parts used in manufacturing, but it suffers from data and information accessibility, completeness, consistency, interoperability, and timeliness challenges in the customer - equipment vendors interaction and information sharing. Current practices in devices and spare parts purchasing, warehousing and maintenance include a lot of manual information search, validation and update work done at the manufacturing companies to maintain high quality master data records for the production assets management. This article addresses the challenge of improving efficiency and quality of operational production assets information management in industry customer company interaction with their production equipment vendors, with a design science research approach. The research was executed in collaboration between research organizations, forest industry companies, industrial production equipment vendors and information technology (IT) companies on two parallel and complementary research tracks, which are described with their key research results. The contributions are an ex-ante evaluated digital marketplace concept with preliminary requirements, and an ex-post evaluated semi-automated master data harmonization tool, as artefacts for improving efficiency and quality of production assets information management in industrial customer–vendors interaction. As theoretical contribution, the article provides a synthesis of the research results in the framework of information systems (IS) design theory as a nascent and novel design theory on designing IS artifacts for improving efficiency and quality of production assets information management, both in single industrial customer company and ecosystem settings. The contributions provide a starting point for further research and development of a digital marketplace for improving production assets information management in industrial customers – vendors interaction, potentially also applicable for similar ecosystems in other continuous production process -based manufacturing industries.}
}
@article{STEENWINCKEL202130,
title = {FLAGS: A methodology for adaptive anomaly detection and root cause analysis on sensor data streams by fusing expert knowledge with machine learning},
journal = {Future Generation Computer Systems},
volume = {116},
pages = {30-48},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20329927},
author = {Bram Steenwinckel and Dieter {De Paepe} and Sander {Vanden Hautte} and Pieter Heyvaert and Mohamed Bentefrit and Pieter Moens and Anastasia Dimou and Bruno {Van Den Bossche} and Filip {De Turck} and Sofie {Van Hoecke} and Femke Ongenae},
keywords = {Anomaly detection, Root cause analysis, Machine learning, Semantic web, Internet of Things, Fused AI, User feedback},
abstract = {Anomalies and faults can be detected, and their causes verified, using both data-driven and knowledge-driven techniques. Data-driven techniques can adapt their internal functioning based on the raw input data but fail to explain the manifestation of any detection. Knowledge-driven techniques inherently deliver the cause of the faults that were detected but require too much human effort to set up. In this paper, we introduce FLAGS, the Fused-AI interpretabLe Anomaly Generation System, and combine both techniques in one methodology to overcome their limitations and optimize them based on limited user feedback. Semantic knowledge is incorporated in a machine learning technique to enhance expressivity. At the same time, feedback about the faults and anomalies that occurred is provided as input to increase adaptiveness using semantic rule mining methods. This new methodology is evaluated on a predictive maintenance case for trains. We show that our method reduces their downtime and provides more insight into frequently occurring problems.}
}
@article{WATROBSKI20191591,
title = {Towards Knowledge Handling in Sustainable Management Domain},
journal = {Procedia Computer Science},
volume = {159},
pages = {1591-1601},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.330},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919315315},
author = {Jarosław Wątróbski},
keywords = {sustainable management, knowledge management, ontology, knowledge engineering, sustainable decision problem},
abstract = {Recent years the term of sustainability has received increased meaning in successful management. Consequently, a lot of attention is paid for developing effective means to support this process. The concept of sustainable management is an important focal point for decision-makers in business. To advance sustainable management activities, obviously an adequate and modern approach is needed. Actual research trends confirm that knowledge management gives a way to handle domain knowledge practically and effectively. Thus, the practical application of knowledge-based mechanisms may provide a relevant instrument. This paper addresses the specific kind of problem of domain knowledge handling in sustainable management. In scientific terms, the paper presents an attempt of knowledge handling in the form of domain ontology for sustainable management. Presented case studies show the practical capabilities of proposed approach.}
}
@article{KLIEGR2018174,
title = {Antonyms are similar: Towards paradigmatic association approach to rating similarity in SimLex-999 and WordSim-353},
journal = {Data & Knowledge Engineering},
volume = {115},
pages = {174-193},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17301325},
author = {Tomáš Kliegr and Ondřej Zamazal},
keywords = {Word similarity, Word relatedness, WordSim353, SimLex-999},
abstract = {SimLex-999 is a widely used lexical resource for tracking progress in word similarity computation. It anchors similarity in synonymy, while other researchers such as Agirre et al. (2009) adopt broader similarity definition, involving also hyponymy and antonymy relations. Paradigmatic association covers synonymy, antonymy and co-hyponymy relations (Lapesa et al., 2014) largely overlapping with this broader similarity definition. Two words are paradigmatically associated if they can replace one another without affecting the grammaticality or acceptability of the sentence. Paradigmatic association can be elicited by asking for word interchangeability, which we hypothesize might be more natural than instructing raters with a list of relations to consider. To validate the proposed approach, we reannotated WordSim353 and SimLex-999 using two new guidelines: one explicitly qualifying antonymy as a similarity relation, the second one eliciting word interchangeability. As additional datasets we present a crowdsourced version of WordSim353 and a Czech version of SimLex-999. The paper also includes detailed analysis of lexical content of SimLex-999 and benchmark of thesaurus-based and distributional algorithms on multiple word similarity and relatedness datasets.}
}
@article{JONQUET2018126,
title = {AgroPortal: A vocabulary and ontology repository for agronomy},
journal = {Computers and Electronics in Agriculture},
volume = {144},
pages = {126-143},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2017.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0168169916309541},
author = {Clément Jonquet and Anne Toulet and Elizabeth Arnaud and Sophie Aubin and Esther {Dzalé Yeumo} and Vincent Emonet and John Graybeal and Marie-Angélique Laporte and Mark A. Musen and Valeria Pesce and Pierre Larmande},
keywords = {Ontologies, Controlled vocabularies, Knowledge organization systems or artifacts, Ontology repository, Metadata, Mapping, Recommendation, Semantic annotation, Agronomy, Food, Plant sciences, Biodiversity},
abstract = {Many vocabularies and ontologies are produced to represent and annotate agronomic data. However, those ontologies are spread out, in different formats, of different size, with different structures and from overlapping domains. Therefore, there is need for a common platform to receive and host them, align them, and enabling their use in agro-informatics applications. By reusing the National Center for Biomedical Ontologies (NCBO) BioPortal technology, we have designed AgroPortal, an ontology repository for the agronomy domain. The AgroPortal project re-uses the biomedical domain’s semantic tools and insights to serve agronomy, but also food, plant, and biodiversity sciences. We offer a portal that features ontology hosting, search, versioning, visualization, comment, and recommendation; enables semantic annotation; stores and exploits ontology alignments; and enables interoperation with the semantic web. The AgroPortal specifically satisfies requirements of the agronomy community in terms of ontology formats (e.g., SKOS vocabularies and trait dictionaries) and supported features (offering detailed metadata and advanced annotation capabilities). In this paper, we present our platform’s content and features, including the additions to the original technology, as well as preliminary outputs of five driving agronomic use cases that participated in the design and orientation of the project to anchor it in the community. By building on the experience and existing technology acquired from the biomedical domain, we can present in AgroPortal a robust and feature-rich repository of great value for the agronomic domain.}
}
@article{MALIK20181202,
title = {A Novel Approach to Web-Based Review Analysis Using Opinion Mining},
journal = {Procedia Computer Science},
volume = {132},
pages = {1202-1209},
year = {2018},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.05.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918307671},
author = {Monica Malik and Sharib Habib and Parul Agarwal},
keywords = {Priority, Weights, Web-based reviews, Opinion Mining, Ontology, Modified OGC},
abstract = {With the advent of E-Commerce, a huge amount of data is being generated these days. This data can be useful if knowledge can be extracted from the business perspective. People often use websites reviews for decision making about whether the products should be bought or not. Opinion Mining enables the process of selection and decision making easier. Though several techniques exist for opinion mining based on decision making in this paper the approach is novel. In this paper the approach used is in addition to the opinions generated from the reviews collected from E-Commerce websites and calculating the overall sentiment for decision making, this paper also incorporates a priority for particular features of a product entered by the buyer for making the final decision. This has been incorporated in the form of additional weights which can be entered by the user and adjusted according to the priority. The reason to do this is that priority for a particular feature of a product may vary from person to person. Moreover, the final decision lay in the buyer’s hand in addition to the opinions collected and analysed from reviews.}
}
@article{LEUSIN2020101988,
title = {Patenting patterns in Artificial Intelligence: Identifying national and international breeding grounds},
journal = {World Patent Information},
volume = {62},
pages = {101988},
year = {2020},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2020.101988},
url = {https://www.sciencedirect.com/science/article/pii/S0172219020300806},
author = {Matheus Eduardo Leusin and Jutta Günther and Björn Jindra and Martin G. Moehrle},
keywords = {Artificial intelligence, Digitalisation, Patent analysis, International comparison, Specialisation},
abstract = {This paper identifies countries at the forefront of Artificial Intelligence (AI) development and proposes two novel patent-based indicators to differentiate structural differences in the patterns of intellectual property (IP) protection observed for AI across countries. In particular, we consider (i) the extent to which countries specialise in AI and are relevant markets for corresponding IP protection (‘National Breeding Ground’); and (ii) the extent to which countries attract AI from abroad for IP protection and extend the protection of their AI-related IP to foreign markets (‘International Breeding Ground’). Our investigation confirms prior findings regarding substantial changes in the technological leadership in AI, besides drastic changes in the relevance of AI techniques over time. Particularly, we find that National and International Breeding Grounds overlap only partially. China and the US can be characterised as dominant National Breeding Grounds. Australia and selected European countries, but primarily the US, are major International Breeding Grounds. We conclude that China promotes AI development with a major focus on IP protection in its domestic market, whereas the US sustains its AI progress in the international context as well. This might indicate a considerable bifurcation in the structural patterns of IP protection in global AI development.}
}
@article{ADAMS2019105337,
title = {Guaranteeing privacy policies using lightweight type systems},
journal = {Computer Law & Security Review},
volume = {35},
number = {6},
pages = {105337},
year = {2019},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0267364918303364},
author = {Robin Adams and Wolfgang Schulz and Sibylle Schupp and Florian Wittner},
keywords = {GDPR, Type theory, Data protection by design, Accountability, State of the art}
}
@article{QUINN2021101233,
title = {A case study comparing the completeness and expressiveness of two industry recognized ontologies},
journal = {Advanced Engineering Informatics},
volume = {47},
pages = {101233},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101233},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620302020},
author = {Caroline Quinn and J.J. McArthur},
keywords = {Brick, Haystack, Smart building, Ontology},
abstract = {Enabling Smart Building applications will help to achieve the ongoing efficient commissioning of buildings, ultimately attaining peak performance in energyuse and improved occupant health and comfort, at minimum cost. For these technologies to be scalable, ontology must be adopted to semantically represent data generated by building mechanical systems, acting as conduit for connection to Smart Building applications. As the Building Automation System (BAS) industry considers Brick and Project Haystack ontologies for such applications, this paper provides a quantitative comparison of their completeness and expressiveness using a case study. This is contextualized within the broader set of ontological approaches developed for Smart Buildings, and critically evaluated using key ontology qualities outlined in literature. Brick achieved higher assessment values in completeness and expressiveness achieving 59% and 100% respectively, as compared to Haystacks 43% and 96%. Additionally, Brick exhibited five of six desirable qualities, where Haystack exhibited only three. Overall, this critical analysis has found Brick to be preferable to Haystack but still lacking in completeness; to overcome this, it should be integrated with other existing ontologies to serve as a holistic ontology for the longer- term development of Smart Building applications. These will support innovative approachesto sustainability in building operations across scales and as next- generation building controls and automation strategies.}
}
@article{SKOBELEV2019154,
title = {Development of a Knowledge Base in the “Smart Farming” System for Agricultural Enterprise Management},
journal = {Procedia Computer Science},
volume = {150},
pages = {154-161},
year = {2019},
note = {Proceedings of the 13th International Symposium “Intelligent Systems 2018” (INTELS’18), 22-24 October, 2018, St. Petersburg, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919303734},
author = {P.O. Skobelev and E.V. Simonova and S.V. Smirnov and D.S. Budaev and G.Yu. Voshchuk and A.L. Morokov},
keywords = {knowledge base, Aristotle metaontology, ontology of plant growing, semantic network, multi-agent technology, smart decision support system},
abstract = {Increasing the efficiency of agricultural production is a very important task. To solve this problem, it is proposed to use the “Smart Farming” cloud system for precision farming management. The novelty of the proposed approach lies in using the knowledge base and multi-agent technology to develop coordinated decisions on management of agricultural enterprises. The paper focuses on development of the knowledge base in the "Smart Farming" system on precision agriculture. Information storage is organized in the form of a semantic network of concepts and relations on the "All about the concept" principle in a single repository, which facilitates the work of farmers with this resource. The paper covers storage, editing, verification, and visualization of knowledge representation about the domain of crop production, production resources, agricultural machinery, equipment and other material resources, as well as peculiarities of the tasks of precision farming. The knowledge base of plant production, built on ontological principles, will be useful to enterprise managers, agronomists, machine operators, planning services and other specialists of large, medium and small farms, as well as to individual farmers.}
}
@article{MA2020104620,
title = {A new structure for representing and tracking version information in a deep time knowledge graph},
journal = {Computers & Geosciences},
volume = {145},
pages = {104620},
year = {2020},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2020.104620},
url = {https://www.sciencedirect.com/science/article/pii/S0098300420305987},
author = {Xiaogang Ma and Chao Ma and Chengbin Wang},
keywords = {Knowledge graph, Geologic time scale, Ontology, Version control, Open data},
abstract = {Ontologies and vocabularies are an effective way to promote data interoperability in open data and open science. The deep time knowledge graph is one of the most discussed and studied topics in geoscience ontologies and vocabularies. The continuous evolution of deep time concepts calls for a mechanism of version control and organization to reduce the semantic ambiguity. In this paper we propose a new structure for version control and tracking of concepts, attributes and topological relationships in the deep time knowledge graph. In our work we have reused the existing ontologies for geologic time scale and vocabularies for the International (Chrono)stratigraphic Chart (ISC). Through the new structure, we are able to represent the whole version history of the ISC charts (from 2004 to 2018) in a single knowledge graph. Moreover, the resulting knowledge graph is consistent with the existing ontologies and vocabularies. Experiments of SPARQL queries prove the efficiency of this structure for version tracking of concepts and attributes. We are now extending the knowledge graph with concepts from regional and local geologic time standards, such as North America, Europe, Britain, China, and Australia, and building a graphic user interface for the services. In a future work, we will implement the knowledge graph in data integration workflows. We hope this research will spur more discussion and development of methods for version control of knowledge graphs in geoscience and other disciplines.}
}
@article{POZZATO201981,
title = {Typicalities and probabilities of exceptions in nonmotonic Description Logics},
journal = {International Journal of Approximate Reasoning},
volume = {107},
pages = {81-100},
year = {2019},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X17305741},
author = {Gian Luca Pozzato},
keywords = {Description Logics, Typicality, Nonmonotonic Reasoning, Probabilities of exceptions},
abstract = {We introduce a nonmonotonic procedure for preferential Description Logics in order to reason about typicality by taking probabilities of exceptions into account. We consider an extension, called ALC+TRP, of the logic of typicality ALC+TR by inclusions of the form T(C)⊑pD with probability p, whose intuitive meaning is that “all the typical Cs are Ds, and the probability that a C is not a D is 1−p”. We consider a notion of extension of an ABox containing only some typicality assertions, then we equip each extension with a probability. We then restrict entailment of a query F to those extensions whose probabilities belong to a given and fixed range. We propose a decision procedure for reasoning in ALC+TRP and we exploit it to show that entailment is ExpTime-complete as for the underlying ALC.}
}
@article{ABIDI2018,
title = {Diabetes-Related Behavior Change Knowledge Transfer to Primary Care Practitioners and Patients: Implementation and Evaluation of a Digital Health Platform},
journal = {JMIR Medical Informatics},
volume = {6},
number = {2},
year = {2018},
issn = {2291-9694},
doi = {https://doi.org/10.2196/medinform.9629},
url = {https://www.sciencedirect.com/science/article/pii/S2291969418000212},
author = {Samina Abidi and Michael Vallis and Helena Piccinini-Vallis and Syed Ali Imran and Syed Sibte Raza Abidi},
keywords = {type 2 diabetes mellitus, self-management, health behavior, knowledge management, clinical decision support system},
abstract = {Background
Behavioral science is now being integrated into diabetes self-management interventions. However, the challenge that presents itself is how to translate these knowledge resources during care so that primary care practitioners can use them to offer evidence-informed behavior change support and diabetes management recommendations to patients with diabetes.
Objective
The aim of this study was to develop and evaluate a computerized decision support platform called “Diabetes Web-Centric Information and Support Environment” (DWISE) that assists primary care practitioners in applying standardized behavior change strategies and clinical practice guidelines–based recommendations to an individual patient and empower the patient with the skills and knowledge required to self-manage their diabetes through planned, personalized, and pervasive behavior change strategies.
Methods
A health care knowledge management approach is used to implement DWISE so that it features the following functionalities: (1) assessment of primary care practitioners’ readiness to administer validated behavior change interventions to patients with diabetes; (2) educational support for primary care practitioners to help them offer behavior change interventions to patients; (3) access to evidence-based material, such as the Canadian Diabetes Association’s (CDA) clinical practice guidelines, to primary care practitioners; (4) development of personalized patient self-management programs to help patients with diabetes achieve healthy behaviors to meet CDA targets for managing type 2 diabetes; (5) educational support for patients to help them achieve behavior change; and (6) monitoring of the patients’ progress to assess their adherence to the behavior change program and motivating them to ensure compliance with their program. DWISE offers these functionalities through an interactive Web-based interface to primary care practitioners, whereas the patient’s self-management program and associated behavior interventions are delivered through a mobile patient diary via mobile phones and tablets. DWISE has been tested for its usability, functionality, usefulness, and acceptance through a series of qualitative studies.
Results
For the primary care practitioner tool, most usability problems were associated with the navigation of the tool and the presentation, formatting, understandability, and suitability of the content. For the patient tool, most issues were related to the tool’s screen layout, design features, understandability of the content, clarity of the labels used, and navigation across the tool. Facilitators and barriers to DWISE use in a shared decision-making environment have also been identified.
Conclusions
This work has provided a unique electronic health solution to translate complex health care knowledge in terms of easy-to-use, evidence-informed, point-of-care decision aids for primary care practitioners. Patients’ feedback is now being used to make necessary modification to DWISE.}
}
@article{XU2020103006,
title = {Semantic approach to compliance checking of underground utilities},
journal = {Automation in Construction},
volume = {109},
pages = {103006},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.103006},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519305606},
author = {Xin Xu and Hubo Cai},
keywords = {Ontology, Data integration, Semantic reasoning, Utility compliance checking},
abstract = {Utility regulations stipulate the spatial configurations between underground utilities and their surroundings to avoid interferences and disruptions of utility services. Utility compliance checking aims to detect spatial non-compliances in underground utilities by examining geospatial data of utilities and their surroundings against textual data of utility regulations. However, the integration of heterogeneous utility geospatial and textual data for compliance checking remains a big challenge. This paper presents a semantic approach to integrate heterogeneous data and enable automated compliance checking of underground utilities through logic and spatial reasoning. The approach consists of the following key components: (1) four interlinked ontologies that provide the semantic schema for heterogeneous data relevant to utility compliance checking, (2) two data convertors for the conversion of heterogeneous data from proprietary formats into a common and interoperable format following the semantic schema, and (3) a query mechanism with spatial extensions for the detection of non-compliant utility instances. The approach was tested on a sample utility database, and the results demonstrate the success of the proposed approach in the integration of heterogeneous data from multiple sources and automated detection of spatial non-compliances in underground utilities. In addition to utility compliance checking, the approach can be extended to other application cases where both data integration from multiple sources and spatial reasoning are required.}
}
@article{DIVAN2022298,
title = {Measurement project interoperability for real-time data gathering systems},
journal = {Future Generation Computer Systems},
volume = {129},
pages = {298-314},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.11.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004738},
author = {Mario José Diván and María Laura Sánchez-Reynoso and Silvio Miguel Gonnet},
keywords = {Measurement, Interoperability, Internet-of-Things, Merkle tree, Metadata-guided processing},
abstract = {The Internet-of-Things devices have allowed the increment of the Spatiotemporal resolution in the Real-time Data Gathering Systems. However, this has increased the complexity due to the heterogeneity of sensors. Thus, it is important to know how data can be understood aware of its context at the edge. Also, a transversal perspective is expected for aligning and reusing as many sensors as possible (based on an experimental design). Because of the hardware limitations, the experimental design should be communicated as simple, integrated, and consistent as possible under single content fostering the projects’ complementarity. This work describes a strategy for defining and communicating a set of measurement projects, following a conceptual hierarchy derived from a measurement ontology. Every project definition is obtained from the user requirements following a well-established strategy. BriefPD is introduced as a data interchange format that is self-contained, partially verifiable (through a Merkle tree insight), consistent, and does not require the use of tags to determine the content meaning. BriefPD generates a project definition in 6.27 ms (1.9 times quicker than JSON-equivalent), consuming 20.39% of JSON’s size. A Github library is provided with the reference implementation jointly with a ‘Code Ocean’ capsule for reproducing the simulation results.}
}
@article{ASKI2022108687,
title = {Advances on networked ehealth information access and sharing: Status, challenges and prospects},
journal = {Computer Networks},
volume = {204},
pages = {108687},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108687},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621005545},
author = {Vidyadhar Jinnappa Aski and Vijaypal Singh Dhaka and Sunil Kumar and Sahil Verma and Danda B. Rawat},
keywords = {Internet of medical things, Access controlling techniques, Interoperability},
abstract = {Abstarct
Internet of Medical Things (IoMT) is no longer a futuristic technology and has become a day-to-day reality with the increasing ease of availing modern internet services. IoMT services interconnect numerous healthcare ecosystem stakeholders, such as doctors, patients, pharmacists, etc., seamlessly under the light of Advanced Internet Communication Tools (AICT). Wireless Body Area Network (WBAN) is a primary constituent of any IoMT device, offering a data acquisition environment through various bio-sensors deployed in edge devices. The data generated through such edge devices should be stored securely in Electronic Health Server (EHS) for further analysis and knowledge inference by medical professionals. Thus designing access control techniques that prevent unauthorised access at the cloud and device level is crucial. Interoperability concern in the IoMT development cycle is becoming a focus of interest for many researchers because most devices and underlying protocols are highly heterogeneous. Lack of worldwide acceptable protocol standards is another major issue in dealing with platform interoperability. In this study, the authors describe the various existing device and data access control strategies such as RBAC (Role Based Access Controlling), CaPBAC (Capability-Based Access Controlling), and ABAC (Attribute-Based Access Controlling), etc. This article majorly surveys the literature from 2000 to 2020 on various access controlling and Interoperability aspects as they are potential players of modern IoT applications. The study also comprehensively discusses the state-of-the-art strategies that provide platform interoperability followed by crucial implementation challenges. CCS CONCEPTS • General and reference→Surveys and overview;• Access Control→Access control strategies; • Interoperability→ patterns}
}
@article{ZAOUGA2019417,
title = {Towards an Ontology Based-Approach for Human Resource Management},
journal = {Procedia Computer Science},
volume = {151},
pages = {417-424},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.057},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305198},
author = {Wiem Zaouga and Latifa Ben {Arfa Rabai} and Wafa Rashid Alalyani},
keywords = {Human Resource Ontology, Domain Ontology, Human Resource Management Processes, PMBOK},
abstract = {Human Resources (HRs) as one of the most valuable asset of any organizations play a crucial role in their success. Within the context of project management, HR Management (HRM) is perceived as an entire knowledge area with defined processes, Tools and Techniques (T&T) in PMBOK 5th Guide. Although this guide shows a strong focus on HRM, it does not illustrate, in a clear way, how to perform each process with the required competencies and the related T&T. Hence, by using only PMBOK processes, the project manager cannot be assisted to select the suitable team according to their skills, also the project team members are not able to interchange their competencies. To address these issues, we propose to use an ontological approach in order to build a common shared representation in the HRM domain. This approach fosters the interoperability among HRs as well as their efficient use of T&T; further it can provide whom using PMBOK a better understanding and guidance for managing HRs with evidence.}
}
@article{ARDIZZONE2018326,
title = {A knowledge based architecture for the virtual restoration of ancient photos},
journal = {Pattern Recognition},
volume = {74},
pages = {326-339},
year = {2018},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2017.09.031},
url = {https://www.sciencedirect.com/science/article/pii/S0031320317303837},
author = {E. Ardizzone and H. Dindo and G. Mazzola},
keywords = {Image restoration, Historical photos, Digitization, Ontology, Knowledge base},
abstract = {Historical images are essential documents of the recent past. Nevertheless, time and bad preservation corrupt their physical supports. Digitization can be the solution to extend their “lives”, and digital techniques can be used to recover lost information. This task is often difficult and time-consuming, if commercial restoration tools are used for the purpose. A new solution is proposed to help non-expert users in restoring their damaged photos. First, we defined a dual taxonomy for the defects in printed and digitized photos. We represented our restoration domain with an ontology and we created some rules to suggest actions to perform in case of some specific events. Classes and properties of the ontology are included into a knowledge base, that grows dynamically with its use. A prototypal tool and a web application version have been implemented as an interface to the database, and to support non-expert users in the restoration process.}
}
@article{GOVIL2022103209,
title = {Estimation of cost and development effort in Scrum-based software projects considering dimensional success factors},
journal = {Advances in Engineering Software},
volume = {172},
pages = {103209},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103209},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822001144},
author = {Nikhil Govil and Ashish Sharma},
keywords = {Agile methodologies, Effort estimation, Cost estimation, Scrum, Success factors},
abstract = {Effort and cost estimation is essential processes during software development. For agile methodologies as well, several approaches have been proposed so far to estimate the required effort and cost to develop any software system. Genuine assessment of software products is not an easy task as it requires the constant attention of the product owner. For effective estimation, we need to consider various success factors associated with all possible dimensions. In this article, we look at Scrum-based Agile projects that are developed over multiple sprints. We proposed an extension to the existing algorithm based on a total of 36 success factors; Which estimates the development effort and cost required to complete the project. For assessment and calculation, we have taken a data-set of a total of 30 projects categorized into three groups as low, medium, and high-level projects. This dataset is further validated by experienced professionals. We also compared our results with the existing approach and found that our results are cost-effective and require less effort even after considering more success factors.}
}
@article{PALOMINO2023100596,
title = {Predicting user types with symbolic images: An empirical validation based on two card-sorting studies},
journal = {Entertainment Computing},
volume = {47},
pages = {100596},
year = {2023},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100596},
url = {https://www.sciencedirect.com/science/article/pii/S1875952123000514},
author = {Paula T. Palomino and Luiz Rodrigues and Alessandra Luz and Armando M. Toda and Lennart Nacke and Seiji Isotani},
keywords = {Gamification, Card-sorting, User types, Semiotics},
abstract = {In gamification, personalization is considered an important field of research since it can improve user engagement and motivation. Particularly, effective gamified learning needs to strike a balance between gameful design that promotes engagement and does not negatively affect content comprehension and absorption. The existing traditional user-type questionnaires approaches are time-consuming and intrusive, hampering the user’s focus. Previous research in gamification demonstrated how to automate player profile creation through log file analysis of gameful systems instead of traditional approaches, however, psychological archetypes and motivational groups can simplify the process of collecting data to achieve a balanced level of personalization for gamified systems, which are grounded on the user’s profile and preferences which can help with their immersion. To attend to the needs of a wide range of fields that introduce gamification to systems that are not classified as games, a new ludic approach was created to define user types based on user interaction using symbolic images. In this exploratory research, we tested this approach in two card-sorting studies (N=35 and N=19). Our results show that the images can be used to predict the users’ archetypes and motivational groups accurately. Thus, we contribute with validating an image-based user type classification method that is easier to deploy in systems that do not aim to behave as a game but are looking to reap the benefits of personalizing a user’s content and gameful experience where relevant. Our second contribution is delivering guidelines to replicate this validation method to other existing approaches. We expect that with this guideline, we facilitate personalization with other user typologies without disrupting the gameful experience.}
}
@article{ZAKARIA2022101191,
title = {Data Analytics Skill Development for Design Education: A Case Study in Optimal Product-Service Bundle Design},
journal = {Thinking Skills and Creativity},
volume = {46},
pages = {101191},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101191},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122001924},
author = {A. F Zakaria and S. C {Johnson Lim}},
keywords = {Design education, Skill training, Decision making, Data-driven design, Product-service bundle},
abstract = {Design education (DE) is a growing field that focuses on the educational perspective of domains such as technology and engineering to support the design study. It is aimed to equip engineers with the essential knowledge and skills for future needs. Previously, there exist a number of related studies on skills development that had applied information and communication technology (ICT) tools to improve teaching and learning experience. Nevertheless, data analytics skills development in the DE field, particularly on how data is organized and readily accessible during analysis and decision-making process, is less emphasized. In this paper, we have presented an implementation of design skill module using a data mining software and an optimization software package to train design engineers in data-driven design decision-making. A case study in optimal product service bundle (PSB) design that illustrates the usefulness of our proposed skill training modules is presented to showcase the level of trainees’ achievement, usability and user experience. In overall, our case study has produced promising outcomes which indicate the merits of our approach in data analytics skill development.}
}
@article{YIN2022108022,
title = {Computational discovery of Metal–Organic Frameworks for sustainable energy systems: Open challenges},
journal = {Computers & Chemical Engineering},
volume = {167},
pages = {108022},
year = {2022},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.108022},
url = {https://www.sciencedirect.com/science/article/pii/S0098135422003568},
author = {Xiangyu Yin and Chrysanthos E. Gounaris},
keywords = {Metal–organic frameworks, Microporous materials, Gas separations, Structure–function relationships},
abstract = {Metal–Organic Frameworks (MOFs) are promising functional microporous materials for a variety of next-generation sustainable energy systems. Their large design space makes it impossible to synthesize, test, and screen them all to identify best candidates. The computational discovery of MOFs has thus become a popular research topic, with methodological advances in computational chemistry and data science heavily contributing to this. Structure databases, materials representation, property evaluation methodologies, performance metrics, and search algorithms all pose open challenges for the community to solve. These challenges are summarized and briefly discussed in this study, with a focus on the engineering aspects required for computational MOF discovery to become a reliable tool for industry. As computational discovery workflows are complicated and necessitate skills from a variety of disciplines, bridging the knowledge gap and enhancing collaboration are critical. Despite the challenges, we remain optimistic about the great potential of computational MOF discovery technology.}
}
@article{OPPERMANN2021100070,
title = {Finding and analysing energy research funding data: The EnArgus system},
journal = {Energy and AI},
volume = {5},
pages = {100070},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100070},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000240},
author = {Leif Oppermann and Simon Hirzel and Alexander Güldner and Karoline Heiwolt and Joachim Krassowski and Ulrich Schade and Christoph Lange and Wolfgang Prinz},
keywords = {Energiewende, Information System, Wiki, Ontology, Databases, Evaluation},
abstract = {This paper presents the concept, a system-overview, and the evaluation of EnArgus, the central information system for energy research funding in Germany. Initiated by the German Federal Ministry for Economic Affairs and Energy (BMWi), EnArgus establishes a one-stop information system about all recent and ongoing energy research funding projects in Germany. Participants ranging from laypersons to experts were surveyed in three workshops to evaluate both the public and expert interfaces of the EnArgus system in comparison to peer systems. The results showed that the EnArgus system was predominantly evaluated positively by the various participants. It contributes to making the energy sector more transparent and offers clear advantages for professional use compared to similar systems. The system’s semantic processing enables more precise hits and better coverage by including semantically related terms in search results; its intelligence makes it fail-safe, rendering it suitable for areas where poor results can have dire consequences. Reporting on an actual real-world system, the paper also provides a roadmap-view of how electronic filing of administrative project data can be semantically enhanced and opened-up to provide the basis for new ways into the data that are key for future breakthrough AI interfaces.}
}
@article{LAADHAR2022997,
title = {Web of Things Semantic Interoperability in Smart Buildings},
journal = {Procedia Computer Science},
volume = {207},
pages = {997-1006},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.155},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922010377},
author = {Amir Laadhar and Junior Dongo and Søren Enevoldsen and Frédéric Revaz and Dominique Gabioud and Torben Bach Pedersen and Martin Meyer and Brian Nielsen and Christian Thomsen},
keywords = {Web of Things, Internet of Things, Web of Things Discovery, Semantic Interoperability, Semantic Web, Ontology},
abstract = {Buildings are the largest energy consumers in Europe and are responsible for approximately 40% of EU energy consumption and 36% of the greenhouse gas emissions in Europe. Two-thirds of the building consumption is for residential buildings. To achieve energy efficiency, buildings are being integrated with IoT devices through the use of smart IoT services. For instance, a smart space heating service reduces energy consumption by dynamically heating apartments based on indoor and outdoor temperatures. The W3C recommends the use of the Web of Things (WoT) standard to enable IoT interoperability on the Web. However, in the context of a smart building, the ability to search and discover building metadata and IoT devices available in the WoT ecosystems remains a challenge due to the limitation of the current WoT Discovery, which only includes a directory containing only IoT devices metadata without including building metadata. Integrating the IoT device's metadata with building metadata in the same directory can provide better discovery capabilities to the IoT services providers. In this paper, we integrate building metadata into the W3C WoT Discovery through the construction of a Building Description JSON-LD file. This Building Description is integrated into the W3C WoT Discovery and based on the domOS Common Ontology (dCO) to achieve semantic interoperability in smart residential buildings for the WoT IoT ecosystem within the Horizon 2020 domOS project. This integration results in a Thing and Building Description Directory. dCO integrates the SAREF core ontology with the Thing Description ontology, devices, and building metadata. We have implemented and validated the WoT discovery on top of a WoT Thing and Building Description Directory. The WoT Discovery implementation is also made available for the WoT community.}
}
@article{BILGIN2018384,
title = {An ontology-based approach for delay analysis in construction},
journal = {KSCE Journal of Civil Engineering},
volume = {22},
number = {2},
pages = {384-398},
year = {2018},
issn = {1226-7988},
doi = {https://doi.org/10.1007/s12205-017-0651-5},
url = {https://www.sciencedirect.com/science/article/pii/S1226798824023481},
author = {Gozde Bilgin and Irem Dikmen and M. Talat Birgonul},
keywords = {construction sector, delay, delay analysis, ontology, ontology evaluation, taxonomy},
abstract = {Delay is a common problem of the construction sector and it is one of the major reasons of claims between project participants. Systematic and reliable delay analysis is critical for successful management of claims. In this study, a delay analysis ontology is proposed that may facilitate development of databases, information sharing as well as retrieval for delay analysis within construction companies. A detailed literature review on construction delays has been carried out during the development of the ontology and it is evaluated by using five case studies. The delay analysis ontology may be used for different purposes especially to support decision-making during risk and claim management processes. It may enable companies to create their own databases, corporate memories and develop decision support systems for better analysis of delays.}
}
@article{KONYS20202297,
title = {How to support digital sustainability assessment? An attempt to knowledge systematization},
journal = {Procedia Computer Science},
volume = {176},
pages = {2297-2311},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.288},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920321943},
author = {Agnieszka Konys},
keywords = {Digital sustainability, sustainable development, sustainability, knowledge base, digital development, ontology},
abstract = {Digital sustainability has great potential for use in many areas. It refers to technological ecosystems, including various mobile payment platforms, crowdfunding, peer-to-peer loans, large financial-related datasets, artificial intelligence, machine learning, blockchain, and digital tokens internet of things. This creates relatively new opportunities for sustainable development. As a consequence, the digital sustainability gains the more attention of the research community. Therefore, a knowledge base containing selected documents in the field of digital sustainability is the main aim of this article. An analysis of available resources is supported by theoretical foundations including both comparative analysis and bibliometric analysis of selected approaches. Based on the retrieved 22 documents and their in-depth analysis by manual screening and by using bibliometric analysis, the set of criteria encompassed 12 main criteria and 276 sub-criteria covering various aspects of digital sustainability and providing relevant knowledge about analyzed documents and their content. To test the functionality of the proposed knowledge base, a number of sample competency queries were provided. Hopefully, this approach will be a starting point to complete missing knowledge to complete the puzzle of digital sustainability.}
}
@article{KONYS20182194,
title = {Knowledge systematization for ontology learning methods},
journal = {Procedia Computer Science},
volume = {126},
pages = {2194-2207},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.229},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918312043},
author = {Agnieszka Konys},
keywords = {ontology learning, knowledge systematization, ontology learning approaches, ontology},
abstract = {The need for interoperable semantics in modern information systems forces to develop more and more intelligent solutions. The increasing demand for these solutions, the explosion of various types of information and the technological development pose new challenges and requirements. Ontologies are often viewed as the answer to this need. The connections between ontologies and Semantic Web become a very promising area. The Semantic Web’s success is dependent on the quality of its underline ontologies, whereas ontologies provide a shared and a common understanding of a domain enabling communication between people and heterogeneous and distributed systems. However, key issue helps ontologies to power the Semantic Web have made ontology learning from various data sources a very auspicious field of research. It aims at semi-automatically or automatically building ontologies from given data sources with a limited human exert. A huge number of available approaches for ontology learning and the prominent differences between them cause the necessity of knowledge systematization for this domain. The paper yields the author’s proposal of ontological elaboration for methods for ontology learning and their features, providing formal, practical and technological guidance to knowledge management based approach to methods supporting ontology learning.}
}
@article{WOODHEAD201835,
title = {Digital construction: From point solutions to IoT ecosystem},
journal = {Automation in Construction},
volume = {93},
pages = {35-46},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517309512},
author = {Roy Woodhead and Paul Stephenson and Denise Morrey},
keywords = {Internet of Things, IoT, Digital construction, Digital innovation, Digital transformation, Industry 4.0, Industrial Internet},
abstract = {This paper takes a longitudinal view of literature to explain the current period as disruptive technology drives an evolutionary adaptation of the construction industry in a historical socio-technological process. The authors argue the way Internet of Things (IoT) solutions are conceived as singularly focused “point solutions” undermine future opportunities. An evolutionary view is overlooked because extant literature describes technology in a particular epoch. An ecosystem perspective needs to influence IT strategy as an emerging “digital layer” transcends a smart city and continues to function long after a traditional construction project completes. We describe innovation as a succession of transformational waves in an evolutionary process that is currently manifesting as “Industry 4.0” and changing expectations for the construction industry. The paper concludes by listing emerging trends and warns existing UK construction companies must understand the transformational process they are in and learn how to adapt with a stronger drive for R&D.}
}
@article{ZHANG201826,
title = {An ontology-based approach supporting holistic structural design with the consideration of safety, environmental impact and cost},
journal = {Advances in Engineering Software},
volume = {115},
pages = {26-39},
year = {2018},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2017.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0965997817303319},
author = {Jisong Zhang and Haijiang Li and Yinghua Zhao and Guoqian Ren},
keywords = {Holistic structural design, Ontology, Environmental impact, Lifecycle cost, Multi-criteria decision support},
abstract = {Early stage decision-making for structural design critically influences the overall cost and environmental performance of buildings and infrastructure. However, the current approach often fails to consider the multi-perspectives of structural design, such as safety, environmental issues and cost in a comprehensive way. This paper presents a holistic approach based on knowledge processing (ontology) to facilitate a smarter decision-making process for early design stage by informing designers of the environmental impact and cost along with safety considerations. The approach can give a reasoning based quantitative understanding of how the design alternatives using different concrete materials can affect the ultimate overall performance. Embodied CO2 and cost are both considered along with safety criteria as indicative multi-perspectives to demonstrate the novelty of the approach. A case study of a concrete structural frame is used to explain how the proposed method can be used by structural designers when taking multi performance criteria into account. The major contribution of the paper lies on the creation of a holistic knowledge base which links through different knowledge across sectors to enable the structural engineer to come up with much more comprehensive decisions instead of individual single objective targeted delivery.}
}
@article{GARCIANUNES20191,
title = {Using a conceptual system for weak signals classification to detect threats and opportunities from web},
journal = {Futures},
volume = {107},
pages = {1-16},
year = {2019},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0016328717303270},
author = {Pedro Ivo Garcia-Nunes and Ana Estela Antunes {da Silva}},
keywords = {Competitive intelligence, Knowledge representation, Conceptual system, Strategic discontinuities, Weak signal},
abstract = {The concept of weak signal (WS) was proposed by Igor Ansoff in the 1970s to describe early information about an emerging phenomenon in organizational environments. Surprising events can be avoided by methods for monitoring and analyzing these signals, continually. These methods enable the detection process of potential threats and opportunities that may arise from those surprising events. While this strategic process is very useful, conventional methods for WS monitoring and analysis do not avoid cognitive and motivational problems that can harm the competitive intelligence capacity of organizations. The solution of these problems can be related to methods that allow conducting the detection process collectively, quickly and with the least effort, through the eventual support of automatic tools. Ontologies, for example, are computational artifacts that represent organizational knowledge whose features can be used in a conceptual system for classification of WS. The purpose of this work is to present such system, which also uses Web crawling, competitive forces and PESTLE analysis in the detection of threats and opportunities. A case example also is discussed showing that this approach can be used as a viable alternative to support surveillance of strategic discontinuities that permeate the organizational environment.}
}
@article{BOOMCARCAMO2022132607,
title = {Opportunities and challenges for the waste management in emerging and frontier countries through industrial symbiosis},
journal = {Journal of Cleaner Production},
volume = {363},
pages = {132607},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.132607},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622022065},
author = {Efrain A. {Boom Cárcamo} and Rita Peñabaena-Niebles},
keywords = {Industrial symbiosis, Waste management, Recycling, Industrial residuals, Emerging and frontiers countries},
abstract = {The development of industrial and commercial activities in developing countries has increased the amount of waste generated by economic activities, which has generated environmental problems due to the complexity of waste management. Due to the economic, environmental, social, and sustainability implications, proper waste management has become a worldwide concern. In response to this problem, strategies have been implemented to develop clean energy, improve the efficiency of industrial resources, and recycle and reuse materials. As a result, the concept of industrial symbiosis (IS) has emerged as a strategy for waste utilization in a productive chain, based on physical exchanges of waste and materials, which finds ways to use the waste from one industry as inputs or raw materials for the other, all supported by business collaboration. This work aims to identify the opportunities and challenges that emerging and frontier countries have for waste utilization through the IS approach through a literature review. The needs that triggered the IS and how researchers materialized this approach to integrate the harnessing resources and materials in economic activities were identified. This paper identifies how emerging and frontier countries realize solid waste management through IS, and highlights opportunities for governments, researchers, and stakeholders to make effective decisions to achieve sustainable development, reduce environmental concerns and address current waste management issues. The literature review identified IS as an excellent way to dispose of and use the solid waste but also showed that emerging and frontier countries face many challenges in solid waste management due to the lack of policies focused on sustainability, lack of communication and trust among stakeholders, low use of technology in the industrial sector and technical barriers.}
}
@article{CIVITARESE201988,
title = {newNECTAR: Collaborative active learning for knowledge-based probabilistic activity recognition},
journal = {Pervasive and Mobile Computing},
volume = {56},
pages = {88-105},
year = {2019},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1574119218303572},
author = {Gabriele Civitarese and Claudio Bettini and Timo Sztyler and Daniele Riboni and Heiner Stuckenschmidt},
keywords = {Pervasive computing, Activity recognition, Active learning, Temporal pattern mining},
abstract = {The increasing popularity of ambient assisted living solutions is claiming adaptive and scalable tools to monitor activities of daily living. Currently, most sensor-based activity recognition techniques rely on supervised learning algorithms. However, the acquisition of comprehensive training sets of activities in smart homes is expensive and violates the individual’s privacy. In this work, we address this problem by proposing a novel hybrid approach that couples collaborative active learning with probabilistic and knowledge-based reasoning. The rationale of our approach is that a generic, and possibly incomplete, knowledge-based model of activities can be refined to target specific individuals and environments by collaboratively acquiring feedback from inhabitants. Specifically, we propose a collaborative active learning method exploiting users’ feedback to (i) refine correlations among sensor events and activity types that are initially extracted from a high-level ontology, and (ii) mine temporal patterns of sensor events that are frequently generated by the execution of specific activities. A Markov Logic Network is used to recognize activities with probabilistic rules that capture both the ontological knowledge and the information obtained by active learning. We experimented our solution with a real-world dataset of activities carried out by several individuals in an interleaved fashion. Experimental results show that our collaborative and personalized active learning solution significantly improves recognition rates, while triggering a small number of feedback requests. Moreover, the overall recognition rates compare favorably with existing supervised and unsupervised activity recognition methods.}
}
@article{DELUSIGNAN2020,
title = {COVID-19 Surveillance in a Primary Care Sentinel Network: In-Pandemic Development of an Application Ontology},
journal = {JMIR Public Health and Surveillance},
volume = {6},
number = {4},
year = {2020},
issn = {2369-2960},
doi = {https://doi.org/10.2196/21434},
url = {https://www.sciencedirect.com/science/article/pii/S2369296020001611},
author = {Simon {de Lusignan} and Harshana Liyanage and Dylan McGagh and Bhautesh Dinesh Jani and Jorgen Bauwens and Rachel Byford and Dai Evans and Tom Fahey and Trisha Greenhalgh and Nicholas Jones and Frances S Mair and Cecilia Okusi and Vaishnavi Parimalanathan and Jill P Pell and Julian Sherlock and Oscar Tamburis and Manasa Tripathy and Filipa Ferreira and John Williams and F D Richard Hobbs},
keywords = {COVID-19, medical informatics, sentinel surveillance},
abstract = {Background
Creating an ontology for COVID-19 surveillance should help ensure transparency and consistency. Ontologies formalize conceptualizations at either the domain or application level. Application ontologies cross domains and are specified through testable use cases. Our use case was an extension of the role of the Oxford Royal College of General Practitioners (RCGP) Research and Surveillance Centre (RSC) to monitor the current pandemic and become an in-pandemic research platform.
Objective
This study aimed to develop an application ontology for COVID-19 that can be deployed across the various use-case domains of the RCGP RSC research and surveillance activities.
Methods
We described our domain-specific use case. The actor was the RCGP RSC sentinel network, the system was the course of the COVID-19 pandemic, and the outcomes were the spread and effect of mitigation measures. We used our established 3-step method to develop the ontology, separating ontological concept development from code mapping and data extract validation. We developed a coding system–independent COVID-19 case identification algorithm. As there were no gold-standard pandemic surveillance ontologies, we conducted a rapid Delphi consensus exercise through the International Medical Informatics Association Primary Health Care Informatics working group and extended networks.
Results
Our use-case domains included primary care, public health, virology, clinical research, and clinical informatics. Our ontology supported (1) case identification, microbiological sampling, and health outcomes at an individual practice and at the national level; (2) feedback through a dashboard; (3) a national observatory; (4) regular updates for Public Health England; and (5) transformation of a sentinel network into a trial platform. We have identified a total of 19,115 people with a definite COVID-19 status, 5226 probable cases, and 74,293 people with possible COVID-19, within the RCGP RSC network (N=5,370,225).
Conclusions
The underpinning structure of our ontological approach has coped with multiple clinical coding challenges. At a time when there is uncertainty about international comparisons, clarity about the basis on which case definitions and outcomes are made from routine data is essential.}
}
@article{CUENCA2020100550,
title = {DABGEO: A reusable and usable global energy ontology for the energy domain},
journal = {Journal of Web Semantics},
volume = {61-62},
pages = {100550},
year = {2020},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100550},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300020},
author = {Javier Cuenca and Felix Larrinaga and Edward Curry},
keywords = {Ontology, Energy domain, Ontology reusability, Ontology usability},
abstract = {The heterogeneity of energy ontologies hinders the interoperability between ontology-based energy management applications to perform a large-scale energy management. Thus, there is the need for a global ontology that provides common vocabularies to represent the energy subdomains. A global energy ontology must provide a balance of reusability–usability to moderate the effort required to reuse it in different applications. This paper presents DABGEO: a reusable and usable global ontology for the energy domain that provides a common representation of energy domains represented by existing energy ontologies. DABGEO can be reused by ontology engineers to develop ontologies for specific energy management applications. In contrast to previous global energy ontologies, it follows a layered structure to provide a balance of reusability–usability. In this work, we provide an overview of the structure of DABGEO and we explain how to reuse it in a particular application case. In addition, the paper includes an evaluation of DABGEO to demonstrate that it provides a balance of reusability–usability.}
}
@article{UMBRICO20201097,
title = {An Ontology for Human-Robot Collaboration},
journal = {Procedia CIRP},
volume = {93},
pages = {1097-1102},
year = {2020},
note = {53rd CIRP Conference on Manufacturing Systems 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.04.045},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120306107},
author = {Alessandro Umbrico and Andrea Orlandini and Amedeo Cesta},
keywords = {Human-Robot Collaboration, Artificial Intelligence, Formal Ontology},
abstract = {The diffusion of Human-Robot Collaborative cells is prevented by some barriers. Classical control approaches seem not yet fully suitable for facing variability conveyed by the presence of human operators beside robots. Heterogeneous knowledge representation capabilities and abstract reasoning are crucial to enhance flexibility of control solutions. This work presents SOHO (Sharework Ontology for Human Robot Collaboration), a novel ontology specifically designed for Human-Robot Collaboration. The paper describes the pursued context-based approach, the novelty of the designed ontology with respect to the state of the art and shows its validity in a realistic Human-Robot Collaboration scenario.}
}
@article{ANTON2018511,
title = {The method of personalized corporate e-learning based on personal traits of employees},
journal = {Procedia Computer Science},
volume = {136},
pages = {511-521},
year = {2018},
note = {7th International Young Scientists Conference on Computational Science, YSC2018, 02-06 July2018, Heraklion, Greece},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.253},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918315588},
author = {Chunaev Anton and Alexey Shikov},
keywords = {personal traits of employees, e-learning, individual learning paths, personalized corporate training},
abstract = {The article considers a method of personalized corporate training based on personal traits of employees with the aim of improving the motivation and quality of training of specialists on various enterprises. Enterprises receive well-trained, promising and motivated employees who are ready to solve the most sophisticated corporate tasks. The proposed approach is promising and relevant for those companies that cannot provide high productivity of their personnel and do not know ways to achieve this. These studies have made it possible to formulate a clear necessity for the process of identifying the desires of employees in a change of position, both in vertical and horizontal versions, and also to plan these transitions in the interests of the company. For this purpose it is proposed to introduce a mechanism of individual educational paths into the company. As part of the construction of an individual path for an employee, a personal competency profile must be created that has already been earned, after which they are compared with the necessary competencies for a particular position. The ways of further development of the presented methodology and creation of a special information system are analyzed.}
}
@article{LU2020348,
title = {Semantic communications between distributed cyber-physical systems towards collaborative automation for smart manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {55},
pages = {348-359},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520300650},
author = {Yuqian Lu and Muhammad Rizwan Asghar},
keywords = {Machine-to-machine communication, Cyber-physical system, Smart manufacturing, Semantic web, Industrial Internet of Things (IIoT), Semantic-aware cyber-physical system, Knowledge graph},
abstract = {Machine-to-machine (M2M) communication is a crucial technology for collaborative manufacturing automation in the Industrial Internet of Things (IIoT)-empowered industrial networks. The new decentralized manufacturing automation paradigm features ubiquitous communication and interoperable interactions between machines. However, peer-to-peer (P2P) interoperable communications at the semantic level between industrial machines is a challenge. To address this challenge, we introduce a concept of Semantic-aware Cyber-Physical Systems (SCPSs) based on which manufacturing devices can establish semantic M2M communications. In this work, we propose a generic system architecture of SCPS and its enabling technologies. Our proposed system architecture adds a semantic layer and a communication layer to the conventional cyber-physical system (CPS) in order to maximize compatibility with the diverse CPS implementation architecture. With Semantic Web technologies as the backbone of the semantic layer, SCPSs can exchange semantic messages with maximum interoperability following the same understanding of the manufacturing context. A pilot implementation of the presented work is illustrated with a proof-of-concept case study between two semantic-aware cyber-physical machine tools. The semantic communication provided by the SCPS architecture makes ubiquitous M2M communication in a network of manufacturing devices environment possible, laying the foundation for collaborative manufacturing automation for achieving smart manufacturing. Another case study focusing on decentralized production control between machines in a workshop also proved the merits of semantic-aware M2M communication technologies.}
}
@article{FOTOHI2021108331,
title = {Securing communication between things using blockchain technology based on authentication and SHA-256 to improving scalability in large-scale IoT},
journal = {Computer Networks},
volume = {197},
pages = {108331},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108331},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621003303},
author = {Reza Fotohi and Fereidoon {Shams Aliee}},
keywords = {Large-Scale IoT, Blockchain, Authentication, Signature-based, IoT Security},
abstract = {The IoT ecosystem allows communication between billions of devices worldwide that are collecting data autonomously. The vast amount of data generated by these devices must be controlled totally securely. The centralized solutions are not capable of responding to these concerns due to security challenges and scalability problems. Thus, blockchain technology is an effective solution, and the “distributed” method has been employed to overcome these concerns to allow for entirely secure communication between devices. The present paper proposes a technique consisting of two steps. The first step deals with the authentication of each node using the identity-based signature to secure communication between devices on a blockchain platform. In the second step, blocks are sent by hashing. For this purpose, the identities of the devices have been used as public keys. The theory analysis and simulation results indicated the superiority of the proposed method in terms of average detection rate, throughput, scalability, time spent on block authentication, and consumption energy compared to S-LoRaWAN and DLBA-IoT. Also, the simulation results show how the proposed approach can significantly increase the security of each thing and network security.}
}
@article{LEE2021101228,
title = {Sequential routing framework: Fully capsule network-based speech recognition},
journal = {Computer Speech & Language},
volume = {70},
pages = {101228},
year = {2021},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2021.101228},
url = {https://www.sciencedirect.com/science/article/pii/S0885230821000358},
author = {Kyungmin Lee and Hyunwhan Joe and Hyeontaek Lim and Kwangyoun Kim and Sungsoo Kim and Chang Woo Han and Hong-Gee Kim},
keywords = {Capsule network, Automatic speech recognition, Sequence-to-sequence, Connectionist temporal classification},
abstract = {Capsule networks (CapsNets) have recently gotten attention as a novel neural architecture. This paper presents the sequential routing framework which we believe is the first method to adapt a CapsNet-only structure to sequence-to-sequence recognition. Input sequences are capsulized then sliced by a window size. Each slice is classified to a label at the corresponding time through iterative routing mechanisms. Afterwards, losses are computed by connectionist temporal classification (CTC). During routing, the required number of parameters can be controlled by the window size regardless of the length of sequences by sharing learnable weights across the slices. We additionally propose a sequential dynamic routing algorithm to replace traditional dynamic routing. The proposed technique can minimize decoding speed degradation caused by the routing iterations since it can operate in a non-iterative manner without dropping accuracy. The method achieves a 1.1% lower word error rate at 16.9% on the Wall Street Journal corpus compared to bidirectional long short-term memory-based CTC networks. On the TIMIT corpus, it attains a 0.7% lower phone error rate at 17.5% compared to convolutional neural network-based CTC networks (Zhang et al., 2016).}
}
@article{HOWELL201894,
title = {Water utility decision support through the semantic web of things},
journal = {Environmental Modelling & Software},
volume = {102},
pages = {94-114},
year = {2018},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364815216307551},
author = {Shaun Howell and Yacine Rezgui and Thomas Beach},
keywords = {Water management, Decision support tool, Interoperability, Big data, Ontology, Semantic web, Internet of things, Smart water networks},
abstract = {Urban environments are urgently required to become smarter. However, building advanced applications on the Internet of Things requires seamless interoperability. This paper proposes a water knowledge management platform which extends the Internet of Things towards a Semantic Web of Things, by leveraging the semantic web to address the heterogeneity of web resources. Proof of concept is demonstrated through a decision support tool which leverages both the data-driven and knowledge-based programming interfaces of the platform. The solution is grounded in a comprehensive ontology and rule base developed with industry experts. This is instantiated from GIS, sensor, and EPANET data for a Welsh pilot. The web service provides discoverability, context, and meaning for the sensor readings stored in a scalable database. An interface displays sensor data and fault inference notifications, leveraging the complementary nature of serving coherent lower and higher-order knowledge.}
}
@article{OCHOA2018511,
title = {A systematic literature review on the semi-automatic configuration of extended product lines},
journal = {Journal of Systems and Software},
volume = {144},
pages = {511-532},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.07.054},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218301511},
author = {Lina Ochoa and Oscar González-Rojas and Alves Pereira Juliana and Harold Castro and Gunter Saake},
keywords = {Extended product line, Product configuration, Systematic literature review},
abstract = {Product line engineering has become essential in mass customisation given its ability to reduce production costs and time to market, and to improve product quality and customer satisfaction. In product line literature, mass customisation is known as product configuration. Currently, there are multiple heterogeneous contributions in the product line configuration domain. However, a secondary study that shows an overview of the progress, trends, and gaps faced by researchers in this domain is still missing. In this context, we provide a comprehensive systematic literature review to discover which approaches exist to support the configuration process of extended product lines and how these approaches perform in practice. Extend product lines consider non-functional properties in the product line modelling. We compare and classify a total of 66 primary studies from 2000 to 2016. Mainly, we give an in-depth view of techniques used by each work, how these techniques are evaluated and their main shortcomings. As main results, our review identified (i) the need to improve the quality of the evaluation of existing approaches, (ii) a lack of hybrid solutions to support multiple configuration constraints, and (iii) a need to improve scalability and performance conditions.}
}
@ARTICLE{11151739,
  author={Garimella, Ritvik and Yip, Hong Yung and Venkataramanan, Revathy and Sheth, Amit P.},
  journal={IEEE Internet Computing}, 
  title={Building Multimodal Knowledge Graphs: Automation for Enterprise Integration}, 
  year={2025},
  volume={29},
  number={3},
  pages={76-84},
  abstract={As enterprises increasingly aim to incorporate artificial intelligence into their workflows to tackle complex, multimodal tasks, the demand for intelligent, robust, and trustworthy systems is paramount. While multimodal large language models offer initial capabilities for processing diverse data streams, their dependence on embedding-based representations limit their effectiveness in delivering semantically grounded explanations and reasoning, as well as qualities essential for enterprise-grade applications. Neurosymbolic approaches provide a promising alternative by enabling traceable, context-aware decision making. However, constructing enterprise-level multimodal knowledge graphs (MMKGs) that enable neurosymbolic approaches remains largely impractical. Although prior efforts have explored MMKG construction, they fall short in addressing the scalability, modularity, and integration that are necessary for any enterprise-grade application. We present a fully automated MMKG construction framework tailored to real-world enterprise environments. Our system features a modular, self-refining lifecycle with a support for human-in-the-loop feedback, enabling scalable, cost-effective, and task-aligned MMKG generation. We demonstrate the practical value of our framework through a real-world case study, showcasing its ability to transform unstructured multimodal data into actionable, semantically grounded knowledge assets for enterprise use.},
  keywords={Scalability;Large language models;Decision making;Semantics;Knowledge graphs;Transforms;Human in the loop;Cognition;Internet;Iterative methods;Artificial intelligence;Multimodal sensors},
  doi={10.1109/MIC.2025.3588546},
  ISSN={1941-0131},
  month={May},}@INPROCEEDINGS{11131908,
  author={Đurić, B. Okreša and Carrascosa, C.},
  booktitle={2025 MIPRO 48th ICT and Electronics Convention}, 
  title={A Framework for Ontology-Driven Multiagent System Generation}, 
  year={2025},
  volume={},
  number={},
  pages={198-203},
  abstract={This paper presents a novel framework for developing multiagent systems using ontology-based modelling consisting of an ontology and an executable module to translate ontology to implementation templates. It systematically maps concepts and relationships to agent roles, behaviours, and interactions by capturing domain knowledge in an ontology. Furthermore, it automatically generates implementation templates for the modelled multiagent system. This way, the need for extensive manual coding may be somewhat reduced, ensuring that conceptual integrity is maintained throughout development. A case study showcases how the proposed framework may speed up the design cycle, reduce coding errors, and enhance system extensibility. Using ontology-driven generation ensures consistency and interoperability, as each agent artefact follows the established domain semantics. This approach may be especially beneficial in complex, knowledge-intensive environments that require collaborative and distributed solutions. Ultimately, it may lead to a flexible and scalable process from domain modelling to multiagent implementation. Our framework provides a way that might help bridge the gap between conceptual design and agent-based execution, streamlining multiagent system development and ensuring robust alignment with domainspecific ontologies.},
  keywords={Semantics;Collaboration;Manuals;Ontologies;Encoding;Software;Extensibility;Artificial intelligence;Interoperability;Multi-agent systems;multiagent system;ontology engineering;software frameworks;knowledge representation;artificial intelligence},
  doi={10.1109/MIPRO65660.2025.11131908},
  ISSN={1847-3938},
  month={June},}@INPROCEEDINGS{11137761,
  author={Paik, Incheon},
  booktitle={2025 International Technical Conference on Circuits/Systems, Computers, and Communications (ITC-CSCC)}, 
  title={Integrating Ontology Rules with Large Language Models for Enhanced Reasoning}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Ontology-based reasoning enables structured inference and logical consistency in specialized domains. However, traditional ontology reasoning frameworks rely heavily on predefined inference engines and XML-based structured data, limiting their integration with Large Language Models (LLMs). Despite their strong general reasoning capabilities, LLMs struggle with structured ontological knowledge due to their reliance on unstructured text.This study proposes a novel approach to integrating ontology-based reasoning into LLMs by transforming ontological concepts and rules into structured natural language. By systematically defining ontology components—such as classes, properties, and rules—using a natural language-friendly format, we enable LLMs to process and infer knowledge from domain-specific ontologies without dedicated inference engines. The feasibility of this transformation is demonstrated through experiments on a domain-specific cocktail ontology. Our findings highlight the potential of natural language-driven ontology representation in bridging the gap between rule-based inference and flexible LLM reasoning.},
  keywords={Computers;Limiting;Large language models;Scalability;Natural languages;Computer architecture;Ontologies;Cognition;Engines;Optimization;Ontology;Large Language Model;Ontology Reasoning;Ontology in Natural Language;RAG},
  doi={10.1109/ITC-CSCC66376.2025.11137761},
  ISSN={2997-741X},
  month={July},}@INPROCEEDINGS{11127920,
  author={Saucedo, Mario A.V. and Viswanathan, Vignesh Kottayam and Kanellakis, Christoforos and Nikolakopoulos, George},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Estimating Commonsense Scene Composition on Belief Scene Graphs}, 
  year={2025},
  volume={},
  number={},
  pages={2861-2867},
  abstract={This work establishes the concept of commonsense scene composition, with a focus on extending Belief Scene Graphs by estimating the spatial distribution of unseen objects. Specifically, the commonsense scene composition capability refers to the understanding of the spatial relationships among related objects in the scene, which in this article is modeled as a joint probability distribution for all possible locations of the semantic object class. The proposed framework includes two variants of a Correlation Information (CECI) model for learning probability distributions: (i) a baseline approach based on a Graph Convolutional Network, and (ii) a neuro-symbolic extension that integrates a spatial ontology based on Large Language Models (LLMs). Furthermore, this article provides a detailed description of the dataset generation process for such tasks. Finally, the framework has been validated through multiple runs on simulated data, as well as in a real-world indoor environment, demonstrating its ability to spatially interpret scenes across different room types. For a video of the article, showcasing the experimental demonstration, please refer to the following link: https://youtu.be/f0tqtPVFZ2A},
  keywords={Graphical models;Graph convolutional networks;Large language models;Semantics;Ontologies;Probability distribution;Indoor environment;Robotics and automation;Videos;Distribution functions},
  doi={10.1109/ICRA55743.2025.11127920},
  ISSN={},
  month={May},}@INPROCEEDINGS{11139975,
  author={Han, Qichang and Zhang, Juan and Bi, Lie and Lu, Xi},
  booktitle={2025 4th International Symposium on Robotics, Artificial Intelligence and Information Engineering (RAIIE)}, 
  title={A Reasoning Method for Micro Assembly Sequence Based on Knowledge Graph and Large Language Model}, 
  year={2025},
  volume={},
  number={},
  pages={216-224},
  abstract={For the assembly of multi-species laser inertial confinement fusion microtargets, micro-assembly robots cannot adaptively respond to changes in the structure of the microtargets and generate assembly sequences autonomously, and manual intervention is needed to reset the assembly process, so there are the problems of autonomous configuration capability and low rapid response capability of the robot system for the assembly of new microtargets and the problem of low response capability, which can not satisfy the needs of small batch and multi-species assembly and rapid response. For this reason, this paper introduces Large-scale Language Model (LLM) and Knowledge Graph (KG) in the assembly sequence generation of micro-assembly robots, and puts forward the method of "Thinking with Knowledge Graph" (TwKG). By integrating external knowledge such as manual assembly experience, a priori information of parts, and robot information, the method enables the micro-assembly robot to generate assembly sequences autonomously, optimize the sequences based on assembly experience, and update the sequences autonomously in case of environmental changes. Since the sequences directly generated by this method cannot be directly used for device control, a Reactive Behavioral Trees method (RBT) based on dynamic behavioral trees is designed based on the analysis of rationality relationships among assembly sequences for automatically creating and updating assembly sequences to cope with abnormal situations. Finally, the effectiveness of the proposed method in improving assembly efficiency and sequence rationality is verified by comprehensive tests in the equipment platform.},
  keywords={Knowledge engineering;Training;Visualization;Large language models;Knowledge graphs;Manuals;Cognition;Planning;Assembly;Robots;assembly sequence planning;information fusion;knowledge graph;large language modeling;micro-target},
  doi={10.1109/RAIIE65740.2025.11139975},
  ISSN={},
  month={June},}@INPROCEEDINGS{11129570,
  author={Sridharan, Hayagriv and Sarkar, Saurabh and Paul, Parag and Cheng, Eugene},
  booktitle={2025 IEEE 11th International Conference on Big Data Computing Service and Machine Learning Applications (BigDataService)}, 
  title={Toward a Hybrid Ontology Framework for Semantic Data Understanding in AI-Augmented Data Warehousing}, 
  year={2025},
  volume={},
  number={},
  pages={174-175},
  abstract={The growing complexity of hybrid data infrastructures, combining structured, semi-structured, and unstructured sources, poses significant challenges for achieving semantic data understanding in enterprise environments. While Large Language Models (LLMs) offer promising capabilities for data exploration, their effectiveness is limited without a unified semantic layer that bridges business meaning and technical metadata. We present a hybrid ontology framework that integrates deterministic schema metadata (such as tables, keys, and ER diagrams) with flexible semantic enrichment derived from documentation, ETL scripts, and business glossaries. This framework enables the construction of a layered knowledge graph aligned with metadata catalogs like OpenMetadata and DataHub, supporting both metadata ingestion and semantic feedback loops. Although named an ontology, our prototype is a Neo4j like property graph; conversion to RDF/OWL is deferred because SHACL (Shapes Constraint Language) normalization is nontrivial, our graph uses rich property types, while OWL/RDF requires explicit classes/relations. Our implementation demonstrates improvements in entity disambiguation, catalog enrichment, and cross-system understanding.},
  keywords={Shape;Semantics;Warehousing;Prototypes;Knowledge graphs;Machine learning;Ontologies;Metadata;Big Data;Business;Ontology;Knowledge Graph;Hybrid Infrastructure;Big Data;Data Warehouse;Semantic Retrieval Techniques;Semantic Data Understanding;Metadata Catalog},
  doi={10.1109/BigDataService65758.2025.00033},
  ISSN={2690-828X},
  month={July},}@INPROCEEDINGS{11139242,
  author={Ospan, Assel and Mussa, Aman and Mansurova, Madina and Sarsembayeva, Talshyn},
  booktitle={2025 IEEE 5th International Conference on Smart Information Systems and Technologies (SIST)}, 
  title={LLM Agents for Enhanced Tabular Data Interpretation: A Perspective}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The task of interpreting tabular data semantically is central to domains ranging from biomedical research to finance, where structured tables must be linked to rich domain knowledge. Ontology-driven methods, such as the iterative approaches could provide a structured, interpretable framework for entity recognition and semantic enrichment. However, these methods often struggle with handling ambiguous terms, evolving taxonomies, and rapid domain shifts. Simultaneously, recent advances in Large Language Models (LLMs) have demonstrated remarkable flexibility and context-aware reasoning capabilities, making them attractive complementary tools. In this perspective paper, we review the ontology-driven semantic analysis landscape, highlight its limitations, and discuss how LLM-based agents can address these challenges. We then outline a conceptual framework for integrating LLM reasoning with ontology-driven pipelines, enabling dynamic ontology extension, robust entity disambiguation, and scalable cross-domain adaptation. By bridging symbolic knowledge structures with the adaptive intelligence of LLMs, we propose a pathway to more flexible, accurate, and future-proof semantic interpretation of tabular data.},
  keywords={Accuracy;Terminology;Large language models;Semantics;Taxonomy;Ontologies;Cognition;Stability analysis;Tuning;Thermal stability;semantic analysis;OWL ontology;table interpretation;large language model;deep learning},
  doi={10.1109/SIST61657.2025.11139242},
  ISSN={},
  month={May},}@INPROCEEDINGS{11126792,
  author={Yu, Muran and Wang, Jie and Chen, Yirong and Lepech, Micheal D. and Liu, Ying and Law, Kincho H.},
  booktitle={2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Ontology-based Adaptive Knowledge System (OAKS): Adaptive and Consistent Knowledge Acquisition through LLMs for Diverse User Backgrounds}, 
  year={2025},
  volume={},
  number={},
  pages={593-599},
  abstract={Although most of the research on large language models (LLMs) focuses on their development and validation against datasets, significant gaps remain in their application to real-world knowledge-intensive tasks. This research addresses key challenges in using LLMs for extracting and synthesizing knowledge from unstructured sources, focusing on applications where the validity and consistency of the results are critical. We propose Ontology-based Adaptive Knowledge System (OAKS) as a holistic approach to manage the complexities of acquiring unstructured knowledge, varying user expertise, and dynamic query formulation. This research provides practical value for enabling the domain user community to leverage their technical documentation and expertise and accelerate ongoing working projects through improved literature review and cross-disciplinary insight discovery. Validated through empirical studies, our findings offer insight into best practices for the deployment of OAKS, bridging the gaps between AI capabilities and real-world needs in knowledge acquisition and research.},
  keywords={Adaptive systems;Knowledge acquisition;Knowledge based systems;Semantics;Ontologies;Software;Real-time systems;Reliability;Problem-solving;Systematic literature review;Human-LLM interaction;knowledge acquisition;nonmonotonic reasoning;ontology-based dynamic query},
  doi={10.1109/COMPSAC65507.2025.00081},
  ISSN={2836-3795},
  month={July},}@INPROCEEDINGS{11126748,
  author={Chowdhury, Suparno Roy and Desai, Ayush and Kapure, Mrunal and Shikalgar, Mustakim and Venugopal, Ramchander and Bansal, Srividya},
  booktitle={2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Enhanced tracking and reporting of missing persons using Knowledge Graph and Ontology Engineering}, 
  year={2025},
  volume={},
  number={},
  pages={619-627},
  abstract={The issue of missing persons remains a significant concern in the United States, with thousands of cases reported annually. Addressing this challenge requires innovative methods to integrate and analyze disparate data sources to uncover patterns in disappearances. This research explores the potential of Ontology Engineering and Knowledge Graphs to provide a structured and interconnected perspective on missing person data. Ontologies enable the representation and linking of real-world entities within a unified framework, facilitating more meaningful data relationships.In this study, a knowledge graph is constructed to capture key details about missing persons, including the circumstances of their disappearance, personal characteristics, and last known locations. Additionally, a Large Language Model (LLM) is integrated to summarize query results, enhancing data interpretation. This knowledge graph serves as a foundation for advanced data analysis, enabling real-time insights, supporting proactive interventions, and aiding in case resolution.},
  keywords={Semantic Web;Data analysis;Databases;Large language models;Soft sensors;Data integration;Knowledge graphs;Ontologies;Real-time systems;Open data;Semantic web;Ontologies;Knowledge Graphs;Data Integration;Missing people;Risk factors;NamUS;LLM},
  doi={10.1109/COMPSAC65507.2025.00084},
  ISSN={2836-3795},
  month={July},}@INPROCEEDINGS{11126672,
  author={Busany, Nimrod and Hadar, Ethan and Hadad, Hananel and Rosenblum, Gil and Maszlanka, Zofia and Akhigbe, Okhaide and Amyot, Daniel},
  booktitle={2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Automating Business Intelligence Requirements with Generative AI and Semantic Search}, 
  year={2025},
  volume={},
  number={},
  pages={1891-1898},
  abstract={Eliciting Business Intelligence (BI) requirements is challenging, especially in dynamic business environments. This paper introduces AutoBIR, an AI-driven system that uses semantic search and Large Language Models (LLMs) to automate BI specification and prototyping. Through a conversational interface, it translates user inputs into analytic code, descriptions, and data dependencies while generating test-case reports with optional visuals. AutoBIR refines BI reporting via feedback, accelerating data-driven decision-making. We also explore the broader potential of generative AI in transforming BI development, illustrating its role in enhancing data engineering practice for large-scale, evolving systems.},
  keywords={Visualization;Translation;Codes;Semantic search;Generative AI;Large language models;Decision making;Data engineering;Software;Business intelligence;Business Intelligence;Generative AI;Large Language Models;AI-Driven Data Engineering;Requirements;Semantic Search;Ontology-Based Query Generation;Prototyping;Text-to-SQL},
  doi={10.1109/COMPSAC65507.2025.00260},
  ISSN={2836-3795},
  month={July},}@INPROCEEDINGS{11123952,
  author={de Villiers, J.P. and de Freitas, A. and Jousselme, A-L. and Kaplan, L. and Blasch, E. and Laudy, C. and Costa, P. C.},
  booktitle={2025 28th International Conference on Information Fusion (FUSION)}, 
  title={Evaluation of LLM Reasoning Under Uncertainty: An Atomic Comparison to Normative Approaches}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Evaluating uncertainty in large language model (LLM) reasoning is challenging due to their vast parameter space, abstract knowledge representation, and limited transparency regarding training data. While normative formalisms, such as deductive logic, clearly define sound reasoning in the absence of uncertainty, reasoning under uncertainty admits multiple approaches, including probabilistic reasoning (e.g. Bayesian), belief function reasoning (e.g. Dempster-Shafer), or fuzzy logic, to name a few. This paper examines how LLMs handle uncertainty by analyzing outcomes based on an atomic fusion and reasoning problem. We establish a point of reference using the simplest of fusion topologies to facilitate transparency and understanding of how LLMs align with established theories. The reasoning approaches of different LLMs with varying complexities are compared to established normative frameworks, providing insights into which formalism best aligns with LLM reasoning and assessing its soundness and consistency. A deviation function for assessment is developed, and the results indicate that the tested LLMs' reasoning under uncertainty does not consistently align with established theories, even for the simplest information fusion topologies. These preliminary results form the basis for further investigations and LLM refinements.},
  keywords={Measurement;Uncertainty;Large language models;Training data;Reliability theory;Ontologies;Probabilistic logic;Cognition;Topology;Standards;Large language models;reasoning under uncertainty;URREF;evaluation;evaluation criteria;reasoning models},
  doi={10.23919/FUSION65864.2025.11123952},
  ISSN={},
  month={July},}@INPROCEEDINGS{11129447,
  author={Cheng, Yutong and Bajaber, Osama and Tsegai, Saimon Amanuel and Song, Dawn and Gao, Peng},
  booktitle={2025 IEEE 10th European Symposium on Security and Privacy (EuroS&P)}, 
  title={CTINexus: Automatic Cyber Threat Intelligence Knowledge Graph Construction Using Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={923-938},
  abstract={Textual descriptions in cyber threat intelligence (CTI) reports, such as security articles and news, are rich sources of knowledge about cyber threats, crucial for organizations to stay informed about the rapidly evolving threat landscape. However, current CTI knowledge extraction methods lack flexibility and generalizability, often resulting in inaccurate and incomplete knowledge extraction. Syntax parsing relies on fixed rules and dictionaries, while model fine-tuning requires large annotated datasets, making both paradigms challenging to adapt to new threats and ontologies. To bridge the gap, we propose CTINexus, a novel framework leveraging optimized in-context learning (ICL) of large language models (LLMs) for data-efficient CTI knowledge extraction and high-quality cybersecurity knowledge graph (CSKG) construction. Unlike existing methods, CTINexus requires neither extensive data nor parameter tuning and can adapt to various ontologies with minimal annotated examples. This is achieved through: (1) a carefully designed automatic prompt construction strategy with optimal demonstration retrieval for extracting a wide range of cybersecurity entities and relations; (2) a hierarchical entity alignment technique that canonicalizes the extracted knowledge and removes redundancy; (3) an long-distance relation prediction technique to further complete the CSKG with missing links. Our extensive evaluations using 150 real-world CTI reports collected from 10 platforms demonstrate that CTINexus significantly outperforms existing methods in constructing accurate and complete CSKG, highlighting its potential to transform CTI analysis with an efficient and adaptable solution for the dynamic threat landscape.},
  keywords={Large language models;Redundancy;Knowledge graphs;Transforms;Organizations;Ontologies;Syntactics;Cyber threat intelligence;Computer security;Tuning;Cyber Threat Intelligence;Large Language Model;In-Context Learning;Cybersecurity Knowledge Graph},
  doi={10.1109/EuroSP63326.2025.00057},
  ISSN={2995-1356},
  month={June},}@INPROCEEDINGS{11120386,
  author={AlShomar, Ahmad and Supakkul, Sam and Bao Pham, To Kim and Hill, Tom and Chung, Lawrence},
  booktitle={2025 IEEE International Conference on Software Services Engineering (SSE)}, 
  title={Teaching LLMs Non-Functional Requirements Modeling: A Grammar and RAG Approach}, 
  year={2025},
  volume={},
  number={},
  pages={57-66},
  abstract={A picture is worth a thousand words. Non-Functional Requirements (NFRs), such as security and usability, are modeled using Softgoal Interdependency Graphs (SIGs) to capture potential conflicts and synergies. However, the practice of NFR modeling remains limited, partly due to unfamiliarity with modeling languages like SIG and insufficient understanding of relevant NFRs. Large Language Models (LLMs) show some knowledge of NFRs and SIG concepts, such as goal decomposition and operationalization, but often lack precise knowledge of formal SIG syntax. We introduce SIG-GPT, a GPT-4-based LLM augmented with SIG knowledge using text-based grammar supplied and Retrieval Augmented Generation (RAG). RAG enhancs LLM responses by retrieving relevant external knowledge, while the grammar enforces correct syntax, guiding the LLM to generate SIGs align with formal notation. To help practitioners better understand SIG modeling, reduce time and effort, and enhance NFR proficiency, we apply textual grammar to SIG-GPT, ensuring it is ready for seamless integration with visual modeling tools like RE- Tool, enabling the LLM to generate correct SIG structures without requiring a large dataset of SIG examples. Results show that SIG-GPT with grammar and RAG achieves 100% syntactic accuracy, 95% semantic accuracy, and 98% cohesion (CCR) while aligning with Bloom's Taxonomy to enhance structured reasoning in SIG modeling.},
  keywords={Visualization;Accuracy;Large language models;Retrieval augmented generation;Taxonomy;Semantics;Syntactics;Software;Grammar;Usability;Non-Functional Requirements (NFRs);Softgoal Interdependency Graph (SIG);Retrieval Augmented Generation (RAG);Large Language Models (LLMs);Visual Modeling Languages;Goal Modeling and Goal-oriented Requirement Language},
  doi={10.1109/SSE67621.2025.00016},
  ISSN={},
  month={July},}@INPROCEEDINGS{11107463,
  author={Goel, Mansi and Andres, Frederic},
  booktitle={2025 IEEE 41st International Conference on Data Engineering Workshops (ICDEW)}, 
  title={A Multilingual Ontology and Knowledge Graph for Recipes}, 
  year={2025},
  volume={},
  number={},
  pages={282-288},
  abstract={In recent years, the fusion of artificial intelligence and semantic web technologies has paved the way for innovative approaches to managing and utilizing information. With the growing demand for structured gastronomical data, there is a need for well-defined ontologies that facilitate recipe organization, ingredient classification, nutritional insights, and personalized diet recommendations. This article presents a multilingual recipe ontology and knowledge graph, capturing critical relationships between ingredients, nutrition, cooking actions, and recipe planning. Our ontology supports interoperability across different languages (English, Hindi, and Japanese), facilitating AI applications such as ingredient substitution and personalized recommendations. The proposed knowledge graph leverages semantic web technologies to enhance structured data accessibility and machine-readable representations, ultimately contributing to computational gastronomy and food informatics.},
  keywords={Semantic Web;Redundancy;Knowledge graphs;Ontologies;Cognition;Real-time systems;Multilingual;Planning;Informatics;Interoperability;Ontology Construction;Recipe Ontology;Knowledge Graph;Multilingual Data;Food Informatics},
  doi={10.1109/ICDEW67478.2025.00040},
  ISSN={2473-3490},
  month={May},}@INPROCEEDINGS{11118904,
  author={Sriharsha, A V and Ganesh, C Sai and Teja, Gudi and Sumanth, K and Sindhuja, G and Ajay, C},
  booktitle={2025 International Conference on Computing Technologies (ICOCT)}, 
  title={Image-to-Label-to-Answer: A Simplified LLM Framework Medical Visual Question Answering}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Medical Visual Question Answering (MVQA) has emerged as a promising approach for assisting clinicians in interpreting complex medical images by providing accurate and context-aware answers to specific visual queries. In this project we introduce an innovative framework, "Image to Label to Answer" (ILA) designed to enhance the efficiency and accuracy of clinical applications in MVQA. The proposed project will involve a setup of integrated image recognition techniques along with advanced natural language processing models that systematically convert the medical images into interpretable labels, which in turn generate the correct answers to clinical questions. The ILA framework will operate in three stages: image labelling, context generation, and answer synthesis. First, the system uses deep learning-based image segmentation and classification models to extract relevant features and assign diagnostic labels to medical images. These labels provide the basis for generating contextual understanding, which is crucial for the subsequent stage. In the context generation phase, the framework uses ontology-based reasoning in combination with language models to interpret the labelled data within the clinical context. As seen, the synthesis module can build full and accurate replies after the contextual understanding created over the input queries.},
  keywords={Visualization;Image segmentation;Accuracy;Image recognition;Optical character recognition;Feature extraction;Question answering (information retrieval);Labeling;Medical diagnostic imaging;Context modeling;Medical Visual Question Answering;Convolutional Neural Networks;Optical Character Recognition;Medical Images;Image Segmentation;Context Generation},
  doi={10.1109/ICOCT64433.2025.11118904},
  ISSN={},
  month={June},}@ARTICLE{11123856,
  author={Gaddafi, Ibtisam El and Zakaria Rashad, Magdi and Abou Eleneen, Amal},
  journal={IEEE Access}, 
  title={A Survey on Modeling of Blockchain Oriented Software Systems and Smart Contracts}, 
  year={2025},
  volume={13},
  number={},
  pages={142397-142415},
  abstract={Modeling is the process of developing models representing software systems from different perspectives using a modeling language that includes graphical notation. This process assists software developers in understanding the system functionality, evaluating design proposals, and documenting the software to be implemented. Blockchain is a popular, decentralized, efficient, and secure technology that enables transparent and tamper-resistant transactions across distributed networks. However, blockchain systems lack a dedicated modeling language to represent Blockchain-Based Applications (BBAs) and related Smart Contracts (SCs). Recent research has introduced adaptations of well-known graphical notations to meet the unique modeling needs of blockchain systems. This paper proposes a structured review of graphical models, techniques, and languages to enhance Blockchain-Oriented Software Engineering (BOSE) modeling. A detailed analysis of 36 studies published between 2018 and 2023 highlights the trends and developments in blockchain modeling, and a classification of modified modeling techniques and languages for BBAs and SCs is presented. A modeling framework that guides blockchain developers in describing, designing, and documenting BBAs and related SCs is developed, and an example is provided to demonstrate how this framework is used. These insights can help software engineers select suitable modeling strategies to improve the reliability and quality of BBAs.},
  keywords={Unified modeling language;Blockchains;Adaptation models;Surveys;Smart contracts;Decentralized applications;Software systems;Ontologies;Computational modeling;Supply chain management;Blockchain;modeling languages;modeling;dApp design},
  doi={10.1109/ACCESS.2025.3598627},
  ISSN={2169-3536},
  month={},}@ARTICLE{11124350,
  author={Triantafyllopoulos, Andreas and Tsangko, Iosif and Gebhard, Alexander and Mesaros, Annamaria and Virtanen, Tuomas and Schuller, Björn W.},
  journal={Proceedings of the IEEE}, 
  title={Computer Audition: From Task-Specific Machine Learning to Foundation Models}, 
  year={2025},
  volume={113},
  number={4},
  pages={317-343},
  abstract={Foundation models (FMs) are increasingly spearheading recent advances on a variety of tasks that fall under the purview of computer audition—i.e., the use of machines to understand sounds. They feature several advantages over traditional pipelines: among others, the ability to consolidate multiple tasks in a single model, the option to leverage knowledge from other modalities, and the readily available interaction with human users. Naturally, these promises have created substantial excitement in the audio community and have led to a wave of early attempts to build new, generalpurpose FMs for audio. In the present contribution, we give an overview of computational audio analysis as it transitions from traditional pipelines toward auditory FMs. Our work highlights the key operating principles that underpin those models and showcases how they can accommodate multiple tasks that the audio community previously tackled separately.},
  keywords={Frequency modulation;Acoustics;Machine learning;Automobiles;Foundation models;Computational modeling;Training;Tagging;Ontologies;Market research;Acoustic scene classification;artificial intelligence (AI);audio captioning (AC);computational audio analysis;computer audition;foundation models (FMs);large audio models;machine listening;sound event detection (SED)},
  doi={10.1109/JPROC.2025.3593952},
  ISSN={1558-2256},
  month={April},}@INPROCEEDINGS{11106622,
  author={Alex, Gabriel},
  booktitle={2025 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)}, 
  title={Leveraging Large Language Models for Automated XR Instructional Content Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={This paper presents a study in which authors examine the potential of leveraging large language models to generate instructional content for eXtended Reality environments. Considering the IEEE ARLEM standard as a framework for structuring data, it could be integrated and interpreted by existing authoring tools. In terms of methods, authors have adopted an exploratory approach in testing various strategies. A case study focusing on the use of an eXtended Reality authoring tool for teaching operating procedures is presented. Finally, this exploratory work shows that while simple prompts can produce scenarios with satisfactory quality, imposing a structured schema through more complex prompts leads to less reliable outcomes.},
  keywords={Technological innovation;Authoring systems;Extended reality;Large language models;Focusing;Ontologies;Reliability;Engineering education;Standards;Testing;extended reality;Artificial Intelligence;Large Language Model;ontology;authoring tool;engineering education},
  doi={10.1109/ICE/ITMC65658.2025.11106622},
  ISSN={2693-8855},
  month={June},}@INPROCEEDINGS{11106597,
  author={Wan, Yuwei and Liu, Ying and Zammit, Joseph Paul and Chen, Zheyuan and Li, Li and Francalanza, Emmanuel},
  booktitle={2025 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)}, 
  title={Facilitating Design for Additive Manufacturing with KG-based Retrieval-Augmented Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Additive manufacturing (AM), or 3D printing, enables the production of complex geometries and highly customized components. However, design for AM (DfAM) requires specialised and comprehensive knowledge of process constraints, material behaviours, and performance parameters. While knowledge graphs (KGs) have been utilized to organize and integrate DfAM knowledge, they do not understand natural language and have limited reasoning capabilities, which make them less accessible to non-experts and ineffective in handling complex and context-dependent design queries. Large language models (LLMs) offer powerful language processing and generalizability but suffer from hallucinations when handling specialized domains. To address these limitations, this study proposes a KG-based retrieval-augmented generation (RAG) approach to develop a domain-specific question-answering (Q&A) in DfAM. By integrating structured knowledge from a DfAM KG with the strengths of LLMs, the proposed approach improves response accuracy and relevance. Comparative experiments evaluated LLMs with non-RAG and KG-based RAG using generic and domain-specific metrics. Results demonstrated that KG-based RAG enhances information retrieval and response quality, reduces hallucinations, and ensures alignment with domain knowledge.},
  keywords={Technological innovation;Accuracy;Large language models;Retrieval augmented generation;Pipelines;Knowledge graphs;Production;Three-dimensional printing;Stakeholders;Prompt engineering;Design for additive manufacturing;Additive manufacturing;Retrieval-augmented generation;Large language model;Knowledge graph},
  doi={10.1109/ICE/ITMC65658.2025.11106597},
  ISSN={2693-8855},
  month={June},}@INPROCEEDINGS{11106608,
  author={Büschelberger, Matthias and Tsitseklis, Konstantinos and Morand, Lukas and Zafeiropoulos, Anastasios and Nahshon, Yoav and Papavassiliou, Symeon and Helm, Dirk},
  booktitle={2025 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)}, 
  title={Digital Products Based on Large Language Models for the Exploration of Graph-Databases in Materials Science and Manufacturing}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={Semantic technologies are gaining traction in materials science and manufacturing. Specifically, the integration of graph databases with ontologies facilitates the harmonization of typically heterogeneous materials and process data, as well as the representation of complex workflows in the field (e.g., processing experimental and simulation data or transferring and tracking data along process chains). This approach enables previously inaccessible data for scientists and engineers to be made available in a FAIR (findable, accessible, interoperable, reusable) manner. On this basis, both science and industry, anticipate a significant boost in materials and process innovation, leading to a more resilient and sustainable production. Nevertheless, one of the main challenges in making semantic technologies usable for engineers is enabling navigation and exploration of the typically complex and flexible graph-based data structures. This work presents two approaches for data exploration in graph-databases using large language models (LLMs), namely LLM-CypherGen and SPARQL-Agent, and their application in two digital products developed within the EU research project DiMAT demonstrated across different use cases in materials science and manufacturing.},
  keywords={Materials science and technology;Ciphers;Technological innovation;Large language models;Knowledge graphs;Production;Ontologies;Syntactics;Manufacturing;Semantic technology;AI;LLM;Knowledge Graph;Material Science;SPARQL;Cypher;Ontology},
  doi={10.1109/ICE/ITMC65658.2025.11106608},
  ISSN={2693-8855},
  month={June},}@INPROCEEDINGS{11094726,
  author={Li, Bozheng and Wu, Yongliang and Lu, Yi and Yu, Jiashuo and Tang, Licheng and Cao, Jiawang and Zhu, Wenqing and Sun, Yuyang and Wu, Jay and Zhu, Wenbo},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={VEU-Bench: Towards Comprehensive Understanding of Video Editing}, 
  year={2025},
  volume={},
  number={},
  pages={13671-13680},
  abstract={Widely shared videos on the internet are often edited. Recently, although Video Large Language Models (Vid-LLMs) have made great progress in general video understanding tasks, their capabilities in video editing understanding (VEU) tasks remain unexplored. To address this gap, in this paper, we introduce VEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark that categorizes video editing components across various dimensions, from intra-frame features like shot size to inter-shot attributes such as cut types and transitions. Unlike previous video editing understanding benchmarks that focus mainly on editing element classification, VEU-Bench encompasses 19 fine-grained tasks across three stages: recognition, reasoning, and judging. To enhance the annotation of VEU automatically, we built an annotation pipeline integrated with an ontology-based knowledge base. Through extensive experiments with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs face significant challenges in VEU tasks, with some performing worse than random choice. To alleviate this issue, we develop Oscars1, a VEU expert model fine-tuned on the curated VEU-Bench dataset. It outperforms existing open-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves performance comparable to commercial models like GPT-4o. We also demonstrate that incorporating VEU data significantly enhances the performance of Vid-LLMs on general video understanding benchmarks, with an average improvement of 8.3% across nine reasoning tasks. The code and data are available at project page},
  keywords={Annotations;Soft sensors;Large language models;Face recognition;Pipelines;Knowledge based systems;Benchmark testing;Performance gain;Cognition;Videos;video editing;video understanding;videollm;video benchmark},
  doi={10.1109/CVPR52734.2025.01276},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{11096639,
  author={Teslya, Nikolay},
  booktitle={2025 IEEE 26th International Conference of Young Professionals in Electron Devices and Materials (EDM)}, 
  title={Using NLP Tools for Linking Materials Within the “Pushkin Digital” Resource}, 
  year={2025},
  volume={},
  number={},
  pages={1540-1543},
  abstract={The development of the scientific and educational resource “Pushkin Digital” requires processing a substantial volume of documents to create an ontology of A. S. Pushkin's literary heritage. The works of A. S. Pushkin and related texts contain mentions of entities, such as historical figure names, geographical locations, dates, and references biographical sources. All these entities are the source of links in the ontology. The paper presents a description of the natural language processing techniques used in processing the materials featured on the Pushkin Digital resource in order to create links between them. The proposed system system utilizes state-of-the-art NLP techniques including BERT for robust named entity recognition, identifying and classifying key entities within the text, SBERT for enabling the system to discern relationships and connections between entities even when expressed with different wording, and LLMs for complex text analysis. Regular expressions are employed for identifying and processing structured text elements, such as dates and bibliographic references, ensuring data consistency and accuracy. This combination of techniques allows for the automated construction of a rich and interconnected ontology, facilitating indepth exploration of Pushkin's literary heritage and its broader cultural significance.},
  keywords={Training;Text mining;Adaptation models;Text analysis;Large language models;Semantics;Named entity recognition;Ontologies;Cultural differences;Information systems;natural language processing;ontology;document analysis;text mining;information extraction},
  doi={10.1109/EDM65517.2025.11096639},
  ISSN={2325-419X},
  month={June},}@INPROCEEDINGS{11096818,
  author={Kotenko, Igor and Abramenko, Georgii},
  booktitle={2025 IEEE 26th International Conference of Young Professionals in Electron Devices and Materials (EDM)}, 
  title={Detecting and Analysing Cyber Attacks Based on Graph Neural Networks, Ontologies and Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={1460-1464},
  abstract={This paper presents an intelligent system to automate the process of detecting and analysing cyber-attacks using Suricata logs, graph neural networks (GNNs), and large language models (LLMs). The proposed approach is based on several key components: collecting and preprocessing network events from Suricata, building an ontological model of attacks using MITRE ATT&CK, applying graph neural networks to identify relationships between events, and finally integrating a language model for dialogue interaction with the operator and generating attack hypotheses. Experimental results demonstrate high accuracy in detecting anomalous network patterns and operator friendliness and indicate the potential for further development of the system for use in high-load and distributed infrastructures.},
  keywords={Accuracy;Large language models;Buildings;Ontologies;Graph neural networks;Computational efficiency;Intelligent systems;Electron devices;Cyberattack;Optimization;LLM;GNN;RAG;process automation;anomaly detection;ontologies;Suricata},
  doi={10.1109/EDM65517.2025.11096818},
  ISSN={2325-419X},
  month={June},}@INPROCEEDINGS{11091706,
  author={Tomasović, Željka and Tomić, Marijana},
  booktitle={2025 10th International Conference on Smart and Sustainable Technologies (SpliTech)}, 
  title={Semantic Usability of Digital Images -Ontology-Based Readability Assessment}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper introduces an ontology-driven method for assessing the semantic usability of digitized images, with a focus on readability and layout readiness, emphasizing practical metrics relevant for text extraction and annotation workflows: Optical Character Recognition (OCR) performance, line segmentation quality, script and annotation compatibility. While previous work has addressed blind Image Quality Assessment (IQA) using perceptual and statistical methods, such approaches often overlook a critical dimension - the semantic usability of images. The method assesses digitized images using a combination of objective metrics (e.g., resolution, noise, compression) and semantic tests (e.g. OCR accuracy, layout analysis) with future goal of implementing metadata completeness, Linked Open Data and International Image Interoperability Framework (LOD/IIIF) compatibility in order to provide a composite usability score. The proposed model is grounded in the Image Authenticity and Quality Ontology (IAQO), a lightweight ontology designed to represent assessment outputs in a standardized, machine-readable format. A dataset of 800 public-domain manuscript images was analyzed, yielding average semantic usability scores ranging from 73.8 to 85.4 (out of 100) across repositories. Results demonstrate that high visual fidelity does not always imply high semantic usability but also the framework’s potential to improve digitizing practices. This approach supports FAIR (Findable, Accessible, Interoperable, Reusable)-compliant image assessment.},
  keywords={Image quality;Image resolution;Annotations;Digital images;Semantics;Optical character recognition;Layout;Ontologies;Cultural differences;Usability;Semantic Usability;Digital Image Quality Assessment;Ontology Modeling;Cultural Heritage Repositories;Digital Humanities;JSON-LD},
  doi={10.23919/SpliTech65624.2025.11091706},
  ISSN={},
  month={June},}@INPROCEEDINGS{11087870,
  author={Heppner, Sebastian and Miny, Torben and Kleinert, Tobias and Zholdybayev, Ayan and Ristin, Marko and van de Venn, Hans Wernher},
  booktitle={2025 IEEE 8th International Conference on Industrial Cyber-Physical Systems (ICPS)}, 
  title={Refining NLP Semantic Matches Through Dialogue with Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Advancements in digitization are fostering innovation within Industrie 4.0 (I4.0) ecosystems. The 2030 vision for I4.0 prioritizes sovereignty, sustainability, and interoperability to transform value chains into dynamic networks. While many physical and syntactical interoperability challenges have been addressed, there are still open questions regarding semantic interoperability. Natural Language Processing (NLP)-based ranking by semantic similarity offers a straightforward approach for matching semantically heterogeneous data but often lacks informative rationale and usability, especially when relevant matches are buried deep in the rankings. In this paper, we explore how we can further improve the ranking to obtain more precise semantical matches by leveraging Large Language Model (LLM), in particular Retrieval-Augmented Generation (RAG). Since comprehensibility is crucial for the end user when selecting the matches, we employ the LLM to additionally explain the rankings and enable follow-up queries. In a series of experiments, we show that this not only improves the usability, but also ensures that users can navigate complex semantic matches effectively.},
  keywords={Industries;Databases;Large language models;Biological system modeling;Semantics;Retrieval augmented generation;Transforms;Natural language processing;Usability;Interoperability;Industrie 4.0;Semantic Matching;Large Language Model;Retrieval-Augmented Generation},
  doi={10.1109/ICPS65515.2025.11087870},
  ISSN={2769-3899},
  month={May},}@BOOK{11099035,
  author={Raieli, Salvatore and Iuculano, Gabriele},
  booktitle={Building AI Agents with LLMs, RAG, and Knowledge Graphs: A practical guide to autonomous and modern AI agents},
  year={2025},
  volume={},
  number={},
  pages={},
  abstract={Master LLM fundamentals to advanced techniques like RAG, reinforcement learning, and knowledge graphs to build, deploy, and scale intelligent AI agents that reason, retrieve, and act autonomouslyKey FeaturesImplement RAG and knowledge graphs for advanced problem-solvingLeverage innovative approaches like LangChain to create real-world intelligent systemsIntegrate large language models, graph databases, and tool use for next-gen AI solutionsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThis AI agents book addresses the challenge of building AI that not only generates text but also grounds its responses in real data and takes action. Authored by AI specialists with deep expertise in drug discovery and systems optimization, this guide empowers you to leverage retrieval-augmented generation (RAG), knowledge graphs, and agent-based architectures to engineer truly intelligent behavior. By combining large language models (LLMs) with up-to-date information retrieval and structured knowledge, you'll create AI agents capable of deeper reasoning and more reliable problem-solving. Inside, you'll find a practical roadmap from concept to implementation. You’ll discover how to connect language models with external data via RAG pipelines for increasing factual accuracy and incorporate knowledge graphs for context-rich reasoning. The chapters will help you build and orchestrate autonomous agents that combine planning, tool use, and knowledge retrieval to achieve complex goals. Concrete Python examples built on popular libraries, along with real-world case studies, reinforce each concept and show you how these techniques come together. By the end of this book, you’ll be well-equipped to build intelligent AI agents that reason, retrieve, and interact dynamically, empowering you to deploy powerful AI solutions across industries.What you will learnLearn how LLMs work, their structure, uses, and limits, and design RAG pipelines to link them to external dataBuild and query knowledge graphs for structured context and factual groundingDevelop AI agents that plan, reason, and use tools to complete tasksIntegrate LLMs with external APIs and databases to incorporate live dataApply techniques to minimize hallucinations and ensure accurate outputsOrchestrate multiple agents to solve complex, multi-step problemsOptimize prompts, memory, and context handling for long-running tasksDeploy and monitor AI agents in production environmentsWho this book is forIf you are a data scientist or researcher who wants to learn how to create and deploy an AI agent to solve limitless tasks, this book is for you. To get the most out of this book, you should have basic knowledge of Python and Gen AI. This book is also excellent for experienced data scientists who want to explore state-of-the-art developments in LLM and LLM-based applications.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835080382},
  url={https://ieeexplore.ieee.org/document/11099035},}@INPROCEEDINGS{11091995,
  author={Peng, Yuying and Zhao, Siqi and Luo, Ximeng and Ning, Yongzhi},
  booktitle={2025 7th International Conference on Computer Science and Technologies in Education (CSTE)}, 
  title={Large Language Models and Knowledge Graphs Synergistically Enhancing Personalized Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1115-1119},
  abstract={The rapid development of artificial intelligence technology has promoted the intelligence of the education field. The collaborative work of large language models (LLMs) and knowledge graphs (KGs) can effectively develop personalized learning paths. Personalized learning in programming courses is crucial for meeting students' different needs and proficiency levels. This article proposes a method for personalized teaching of programming courses using LLMs and KGs. The LLMs is used to analyze students' learning status, provide real-time feedback, and generate customized learning materials, while the KGs provides a structured knowledge base to plan essential skills and learning progress, highly integrating the advantages of the LLMs and the logical connections of knowledge from various disciplines. The application results show that designing a knowledge system ontology through disciplinary knowledge analysis, retraining and fine-tuning the LLMs, forms the final personalized knowledge system, improves the accuracy of learning path recommendations and the relevance of generated exercises, and can be easily promoted to other disciplinary fields.},
  keywords={Accuracy;Large language models;Atmospheric modeling;Education;Knowledge based systems;Semantics;Knowledge graphs;Learning (artificial intelligence);Real-time systems;Programming profession;large language models;knowledge graphs;personalized learning;artificial intelligence technology;programming courses},
  doi={10.1109/CSTE64638.2025.11091995},
  ISSN={},
  month={April},}@INPROCEEDINGS{11090152,
  author={Yang, Ye and Duan, Ran and Yi, Xinyang and Ma, Qicheng and Liu, Jie and Hu, Zhongxu and Hu, Youmin},
  booktitle={15th Prognostics and System Health Management Conference (PHM 2025)}, 
  title={Knowledge graph-based dual-modal collaborative QA framework for hydropower operation and maintenance}, 
  year={2025},
  volume={2025},
  number={},
  pages={152-157},
  abstract={Maintenance of hydropower equipment is essential for ensuring a reliable supply of renewable energy. However, in practical maintenance operations, technicians rely heavily on personal experience and extensive domain manuals, which can reduce the accuracy and efficiency of decision-making processes. This paper proposes a knowledge graph-based dual-modal collaborative QA (KGD-QA) framework for hydropower operation and maintenance. Firstly, a multi-granularity segmentation strategy is used to divide domain manuals into thematically grouped text blocks and path-labeled sentence units. Secondly, a LoRA-fine-tuned large language model (LLM), guided by domain ontologies and chain-of-thought prompts, extracts entity-relation triples. Finally, a dual-modal QA mechanism is designed to integrate knowledge graph subgraph queries and text vector retrieval, enabling information acquisition in both detailed and global modes. Experimental results show that, compared with baseline model, this approach improves the accuracy of knowledge extraction while the dual-modal collaborative QA retrieval method provides more traceable and evidence-backed responses to maintenance-related queries. This framework offers a novel paradigm for advancing intelligent operation and maintenance technologies for hydropower equipment.},
  keywords={},
  doi={10.1049/icp.2025.2348},
  ISSN={},
  month={June},}@INPROCEEDINGS{11081553,
  author={He, Bofan and Cheng, Jerry Q. and Gu, Huanying},
  booktitle={2025 IEEE 13th International Conference on Healthcare Informatics (ICHI)}, 
  title={Static and Dynamic Embedding Approaches to Identify Is-A Relations in SNOMED CT}, 
  year={2025},
  volume={},
  number={},
  pages={369-378},
  abstract={Systematized Nomenclature of Medicine - Clinical Terms (SNOMED CT) is a comprehensive clinical terminology with over 350,000 unique concepts and 1 million relationships. It serves as an essential resource for both clinical practice and medical research. Approximately 40% of these relationships are of the is-a type, which is key to classifying concepts as subtypes or subclasses within broader categories and is fundamental for decision support systems and automated reasoning in clinical environments. Identifying is-a relationships in SNOMED CT is crucial for enhancing the accuracy of patient cohort queries, which form the backbone of clinical and research applications. Our study aims to use deep learning-based neural networks to determine whether a relationship between two concepts falls under the is-a or non-is-a categories. This approach can help detect misclassified or undefined is-a concept pairs, ultimately improving the quality of SNOMED CT ontology. We construct a node-edge-node structure for concept pairs and their relationships, which we then split into two categories: is-a and non-is-a classes. In this study, we propose two classification approaches. In the first approach using embeddingBag, the data are mapped into vector representations and used as input for a fully connected neural network to learn the patterns of the is-a relationship. During model training, 80% of the data is used for training and the remaining 20% for testing.We achieved a precision of 0.903, recall of 0.894, and F1 Score of 0.898 in predicting the is-a relation among the associated medical concepts. The second approach involved transformer-based models like BERT(Bidirectional Encoder Representations from Transformers), introduced by Devlin et al. [1], which dramatically advanced various natural language processing (NLP) tasks by offering deeply contextualized word embeddings. And RoBERTa(Robustly optimized BERT approach)[2], both BERT and RoBERTa models have significantly advanced the field of NLP. This work uses them as encoders connected to a classifier head to predict is-a or non-is-a categories. We fine-tune our models using a historical SNOMED CT dataset from 2017, while evaluation is carried out on new data introduced after 2023 in the 2024 release. With a recall of 0.933, precision of 0.933, F1 score of 0.932, accuracy of 0.933, and ROC of 0.972, RoBERTa outperforms the other baseline models (Support Vector Machine(SVM), K-Nearest Neighbors(KNN), Naive Bayes, and Multilayer Perceptron(MLP)) across all metrics. This work demonstrates that our implementation can effectively identify unknown is-a relations in future datasets.},
  keywords={Training;Accuracy;Neural networks;Bidirectional control;Ontologies;Transformers;Natural language processing;Encoding;Vectors;Data models;Classification of Multi Sentence;Deep Learning;Information Retrieval;Natural Language Processing;SNOMED-CT;Large Language Model;BERT},
  doi={10.1109/ICHI64645.2025.00050},
  ISSN={2575-2634},
  month={June},}@INPROCEEDINGS{11081548,
  author={Cox, Kyle and Qu, Gang and Hsu, Chi-Yang and Xu, Jiawei and Zhou, Yingtong and Tan, Zhen and Hu, Mengzhou and Chen, Tianlong and Hu, Ziniu and Zhao, Zhongming and Ding, Ying},
  booktitle={2025 IEEE 13th International Conference on Healthcare Informatics (ICHI)}, 
  title={Thought Graph: Balancing Specificity and Uncertainty in LLM-Based Gene Set Annotation}, 
  year={2025},
  volume={},
  number={},
  pages={462-470},
  abstract={Accurate predictive reasoning is a cornerstone of biomedical decision-making, particularly in precision oncology, where elucidating the intricate relationships between disease-risk genes and biological processes is critical. This study presents a novel "Thought Graph" methodology, an advancement of the Tree of Thoughts framework, to systematically generate and refine biological process representations derived from gene sets while addressing the trade-off between specificity and uncertainty. Balancing these factors is essential for robust and interpretable gene set analyses, as it accounts for the complexity, variability, and overlapping functions of biological pathways. Furthermore, we introduce a quantitative metric that integrates specificity and uncertainty, thereby enhancing the rigor and transparency of the inference process. Using a subset of the Gene Ontology database, we evaluate the effectiveness of our system in generating biologically meaningful terms that accurately describe the underlying biological processes of gene sets. We compare its performance against a domain-specific tool (GSEA) and five LLM baselines across multiple metrics. Our system achieves the highest cosine similarity (64.00%) and specificity percentile (96.40%), highlighting its capacity to generate terms closely aligned with human annotations while maintaining a balance between specificity and accuracy. By advancing the artificial intelligence driven analyses, this work facilitates more informed decision-making in biomedical research, precision oncology, and related fields.},
  keywords={Measurement;Uncertainty;Accuracy;Large language models;Decision making;Biological processes;Oncology;Genetics;Cognition;Cancer;Gene set analysis;Large language model;Medical reasoning;Precision oncology;Thought graph},
  doi={10.1109/ICHI64645.2025.00060},
  ISSN={2575-2634},
  month={June},}@INPROCEEDINGS{11082040,
  author={Khalov, Andrey and Ataeva, Olga},
  booktitle={2025 8th International Conference on Artificial Intelligence and Big Data (ICAIBD)}, 
  title={Automatic Mapping of Upper-Level Ontology Classes (DOLCE) and Domain-Specific Ontology ITSMO}, 
  year={2025},
  volume={},
  number={},
  pages={795-802},
  abstract={This paper proposes a method for extending the top-level ontology DOLCE (DOLCE-lite version, referred to as TLO) to the domain of IT services without expert involvement. The main challenge addressed is the automatic mapping of classes in conditions of a small number of objects (<100) and the absence of annotated data. A review of existing approaches is conducted, their limitations are identified, and novel mapping methods are proposed, integrating embeddings and large language models. The suggested method achieved an 82.35% mapping accuracy when integrating DOLCE and ITSMO ontologies. As a result, the ITO-seed ontology was developed, containing linked classes from DOLCE and ITSMO, which can be utilized in further research and in building knowledge graphs for IT Service Management (ITSM) systems.},
  keywords={Reviews;Large language models;OWL;Knowledge graphs;Ontologies;Transformers;Resource description framework;Knowledge management;Graph neural networks;Prompt engineering;Ontology;RDF;OWL;mapping;rdf2vec;owl2vec;BERT;GPT;prompt engineering;clustering;ITIL;DOLCE},
  doi={10.1109/ICAIBD64986.2025.11082040},
  ISSN={2769-3554},
  month={May},}@INPROCEEDINGS{11075342,
  author={Wang, Xi and Shen, Tian and Chen, Xi and Zhang, Jundong},
  booktitle={2025 16th International Conference on E-Education, E-Business, E-Management and E-Learning (IC4e)}, 
  title={Knowledge Graphs Combined with ChatGPT in the Field of Acupuncture: A Case Study on the Treatment of Depression 1}, 
  year={2025},
  volume={},
  number={},
  pages={216-220},
  abstract={This study integrates knowledge graphs (KGs) and large language models (LLMs) to enhance acupuncture-based depression treatment through structured knowledge sharing. We developed a depression-specific KG using traditional acupuncture principles and modern IT, leveraging LLMs to optimize knowledge integration and global accessibility for researchers and clinicians.},
  keywords={Electronic learning;Large language models;Knowledge graphs;Depression;Chatbots;Acupuncture;Acupuncture;Knowledge graph;Depression},
  doi={10.1109/IC4e65071.2025.11075342},
  ISSN={},
  month={April},}@INPROCEEDINGS{11073642,
  author={Neupane, Roshan Lal and Pusapati, Vamsi and Edara, Lakshmi Srinivas and Cheng, Xiyao and Neupane, Kiran and Chintapatla, Harshavardhan and Mitra, Reshmi and Korkali, Mert and Suk Na, Hyeong and Srinivas, Sharan and Calyam, Prasad},
  booktitle={NOMS 2025-2025 IEEE Network Operations and Management Symposium}, 
  title={Securing Inverter-Based Resources via Knowledge-Driven Threat Modeling, Analysis, and Mitigation}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={Inverter-based Resources (IBRs) present unique cybersecurity challenges due to their digital control systems and connection to the electric grid. They rely on digital communications and control systems, making them vulnerable to cyberattacks. These attacks can disrupt grid operations and stability, compromise data, or cause physical damage to equipment. To address these challenges, it is essential to establish robust cybersecurity measures that meet and exceed existing industry standards. In this paper, we describe a comprehensive strategy to bolster the cybersecurity of IBRs through cutting-edge applications and technologies via a cybersecurity framework called “CIBR-Fort”, a knowledge-driven, interoperable, scalable, and manageable framework for modeling, analysis, and mitigation of cyber threats disrupting different components of IBR systems. Our knowledge-driven analysis consists of a fusion of knowledge graphs (KGs) in cybersecurity and the electric grid, achieved through link prediction leveraging Large Language Models (LLMs) and cosine similarity, attributed towards informed decision-making for threat mitigation. The evaluation results show how we can automate LLM-driven link prediction based on the fusion of two distantly separated ontologies, generating a dataset that can be used for scaling via graph learning that can be utilized for further security analyses of IBR systems. In addition, we show our knowledge-driven threat analysis can predict different attacks with 91.88% maximum accuracy. Lastly, we show how we can achieve real-time end-to-end threat mitigation with an average of 40 ms per traffic flow.},
  keywords={Threat modeling;Prevention and mitigation;Large language models;Stability criteria;Retrieval augmented generation;Knowledge graphs;Power system stability;Real-time systems;Computer security;Standards;knowledge graph;cybersecurity;inverter-based resources;large language models;retrieval augmented generation},
  doi={10.1109/NOMS57970.2025.11073642},
  ISSN={2374-9709},
  month={May},}@INPROCEEDINGS{11079470,
  author={Blasch, Erik and Insaurralde, Carlos C.},
  booktitle={2025 IEEE Conference on Cognitive and Computational Aspects of Situation Management (CogSIMA)}, 
  title={Space and Air Traffic Management Situation Awareness with Notices}, 
  year={2025},
  volume={},
  number={},
  pages={192-199},
  abstract={Air traffic management has long been associated with situation awareness, especially supporting pilots in assessment and response to challenging situations. For multi-domain air and space operations, artificial intelligence and recently large language models (LLMs) can increase semantic understanding. In this paper, we focus on LLM aerospace analysis of Notice to Airman (NOTAM) and Notice to Space Operators (NOTSO). The notices afford a communication of the situation that can be extracted as an ontology to support human operators. Results show that LLM-based clustering can facilitate cognitive situation awareness.},
  keywords={Space vehicles;Uncertainty;Large language models;Semantics;Data integration;Ontologies;Orbits;Digital twins;Air traffic control;Data Fusion;Air traffic Management;Space Traffic Management;Uncertainty Ontology},
  doi={10.1109/CogSIMA64436.2025.11079470},
  ISSN={2379-1675},
  month={June},}@INPROCEEDINGS{11063905,
  author={Li, Yi and Lu, Wenxin and Xiang, Xiuzhen},
  booktitle={2025 5th International Conference on Neural Networks, Information and Communication Engineering (NNICE)}, 
  title={OntoCons: A Method for Intelligent Construction of Domain Ontology Models for Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={706-712},
  abstract={With the development of large language models technology, its application in the field of education has been steadily expanding. However, due to limitations in understanding specialized literature in vertical fields, large language models face serious issues of “hallucination” and prior bias when responding to teachers' questions about specialized domains. To address this, this paper proposes an intelligent construction framework for ontology models based on knowledge tuple extraction, called OntoCons, which introduces entity extraction and relation extraction methods into the field of ontology model construction. OntoCons transforms the ontology model construction problem into a knowledge tuple extraction problem, enhancing the transparency and interpretability of ontology model construction methods, and introduces subgraph encoding strategies to improve the accuracy of relation extraction. This research provides a new method for the intelligent construction of ontology models in vertical fields, improving the understanding and response quality of large language models in specialized domains by combining machine learning and manual analysis.},
  keywords={Knowledge engineering;Accuracy;Large language models;Manuals;Machine learning;Transforms;Ontologies;Reliability theory;Encoding;Data mining;large language model;ontology model;knowledge extraction;entity extraction;relation extraction},
  doi={10.1109/NNICE64954.2025.11063905},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{11070747,
  author={Zhou, Liyun},
  booktitle={2025 6th International Conference on Computing, Networks and Internet of Things (CNIOT)}, 
  title={Design and Implementation of a Modern Chinese History QA System Based on Knowledge Graphs}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={To address the challenge of quickly obtaining accurate answers while exploring modern historical knowledge, this study develops an intelligent question-answering system based on knowledge graphs, focusing on modern Chinese history. The system leverages the Robustly Optimized BERT Approach (RoBERTa) model for word embedding construction and integrates an adversarial training mechanism to enhance robustness. BiLSTM is employed to capture the contextual features of input text, while Conditional Random Fields (CRF) are used to generate the optimal prediction sequence. Experimental results demonstrate that the model achieves an accuracy of 92.2%, a recall of 94.2%, and an F1-score of 93.0% on the modern history dataset, significantly outperforming other approaches.},
  keywords={Training;Accuracy;Text recognition;Knowledge graphs;Ontologies;Market research;Conditional random fields;Robustness;Question answering (information retrieval);History;question answering system;knowledge graph;domain ontology;modern Chinese history},
  doi={10.1109/CNIOT65435.2025.11070747},
  ISSN={},
  month={May},}@INPROCEEDINGS{11070706,
  author={Pokkuluri, Kiran Sree and jain, Kirti and Rajya Lakshmi, V Sravani and Pastariya, Rishab and Lathigara, Amit and Navanitha, Dubbaka},
  booktitle={2025 4th OPJU International Technology Conference (OTCON) on Smart Computing for Innovation and Advancement in Industry 5.0}, 
  title={A Unified Knowledge Base for Drug Label Analysis Using Learning Models, NLP, and IoT Tasks}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Linguistic transmission advanced humanity. Voice, text, and pictures affect knowledge transmission. Techniques for interpolating data may retain knowledge by determining semantic equivalents based on its context, the lesson, and connections without sacrificing data integrity. Knowledge has affected the security and privacy of data, showing vulnerability on every side of privacy and security violations with numerous legislation and standards. linguistic models convert voice impulses to digital data to examine phonological, prosodic, phonotactic, and lexical features. In contrast, embedding text in pictures and voice patterns modify word meaning via resolution, simplicity, and pitch adjustment. NLP uses vectors to detect word similarity, ensuring anonymity for ontology-based understanding with compatibility while deciphering control features. Using text-based semantic similarities analysis, visual text is classed by arrangement and topic. Every individual's linguistic dialect is distinct, with a tendency toward the local dialect. Due of rapid modifications to the environment, methods' acoustic qualities are difficult to transmit. Voice translating syllables might or might not communicate word feeling according to phonetical modification. A learnt variables taxonomy with prosody properties helps knowledge bases find relevant material without intermediary stages. For successful information transmission, integration, and edge manufacturing, edge learning requires context. An ontology of learnt connections from text, speech, and pictures shows information resemblance usefulness, and interest. The suggested XTI-CNN technique excels in all four metrics and has 93.2% accuracy, making it ideal for hybrid analysis of data in related to drugs data extraction applications.},
  keywords={Drugs;Data privacy;Visualization;Data integrity;Semantics;Knowledge based systems;Ontologies;Linguistics;Data models;Security;Text Extraction;Context Learning;Semantic Similarity;Privateness;Semantic Reasoning;Voice cloning;Interoperability;Speech recognition;Voice activity detection;Language models;Ontology},
  doi={10.1109/OTCON65728.2025.11070706},
  ISSN={},
  month={April},}@INPROCEEDINGS{11068551,
  author={Johnson, Hamilton and SureshKumar, Mayuranath and Thomas, L. Dale and Kannan, Hanumanthrao},
  booktitle={2025 IEEE Aerospace Conference}, 
  title={Ontological Methods of Functional Analysis for Aerospace Concepts}, 
  year={2025},
  volume={},
  number={},
  pages={1-13},
  abstract={Functional analysis and decomposition are crucial steps in the development of complex aerospace systems, involving the translation of high-level system requirements into high-level functions, their breakdown into lower-level functions, and linking these functions to system elements. Traditionally, these processes are conducted informally, relying on the engineering judgment of developers, which may limit consistency and reduce the potential for automated assistance. To formalize these processes, we propose using an ontology-a structured conceptualization of terms and relations within a domain. This paper presents a function sub-ontology, part of an ongoing ontology development effort in association with the Advanced Concepts Office at Marshall Space Flight Center, aimed at modeling early stage space system concepts. The function sub-ontology defines types of functions, their interactions, and decomposition rules using the Ontological Modeling Language (OML). This structured approach supports automated reasoning, facilitating more consistent and traceable functional decomposition. The ontology's reasoning capabilities are demonstrated through several examples, showcasing its utility in space system concept development. These examples highlight how the ontology can help ensure consistency between functions, determine equivalence of functions, suggest functional decompositions, and recommend necessary system elements. While the ontology shows promise in enhancing functional analysis in aerospace systems, it is a work in progress. Future work will focus on refining the formal theory underpinning the ontology and further expanding its capabilities.},
  keywords={Translation;Observatories;Subject matter experts;Refining;Ontologies;Propulsion;Functional analysis;Cognition;Modeling;Aerospace engineering},
  doi={10.1109/AERO63441.2025.11068551},
  ISSN={2996-2358},
  month={March},}@INPROCEEDINGS{11064138,
  author={Sharma, Devanshi and Das, Asmita},
  booktitle={2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)}, 
  title={LYNX-RNA: A Scalable Nextflow Workflow for RNA-Seq Analysis with Integrated Large Language Models for Comprehensive Result Interpretation}, 
  year={2025},
  volume={3},
  number={},
  pages={334-342},
  abstract={RNA sequencing has become an essential technique in the life sciences, providing a comprehensive methodology. RNA sequencing has become an essential method in the life sciences, providing a comprehensive means to quantitatively evaluate RNA levels in tissues and cells. From alignment to subsequent pathway analysis, the increasing usage of RNA-seq has inspired continuous creation of creative tools for all phases of study. For non-experts especially, it is difficult to use these analytical approaches in a scalable, repeatable, and easily available manner. We have established an intuitive, rapid, efficient, and thorough process for RNA-seq analysis using the workflow management system “Nextflow.” LYNX-RNA (Language-augmented Yield for Nextflow-based RNA eXpression analysis) enhances RNA-seq analysis involves the processing of raw sequencing data through alignment, quality control, and subsequent differential expression and pathway analysis. LYNX-RNA incorporates a distinctive integration with a Large Language Model (LLM) that autonomously produces concise, comprehensible summaries of results, thereby rendering RNA-seq data analysis accessible to a diverse audience, including clinicians, biomedical researchers, pharmaceutical scientists, public health professionals, educators, and students. This enhanced accessibility enables other sectors to extract relevant insights from RNA-seq data without necessitating substantial bioinformatics knowledge. The pipeline was verified with patient-derived xenografts from leukemia and lymphoma, identifying differentially expressed genes and enriched pathways pertinent to disease development and therapy response. LYNX-RNA's applications encompass oncology, virology, immunology, personalized medicine, neuroscience, and agriculture, facilitating biomarker discovery, therapeutic target identification, and sustainable agricultural innovations, thereby rendering RNA-seq analysis both accessible and significant across various fields. Looking forward, LYNX-RNA aims to incorporate multi-omics integration, single-cell and spatial transcriptomics support, and machine learning-based predictive analytics. Planned enhancements include cloud deployment, improved visualization, compatibility with long-read sequencing, and clinical application readiness. These advancements will broaden LYNX-RNA's utility in biomarker discovery, personalized medicine, and agricultural innovations, solidifying its role as a comprehensive RNA-seq analysis tool across various fields.},
  keywords={Sequential analysis;Technological innovation;RNA;Precision medicine;Large language models;Process control;Rendering (computer graphics);Life sciences;Workflow management software;Virology;RNA sequencing (RNA-seq);Peline;Nextflow;LYNX-RNA;large language models (LLM);biomarker discovery;personalized medicine},
  doi={10.1109/ICCSAI64074.2025.11064138},
  ISSN={},
  month={April},}@INPROCEEDINGS{11071598,
  author={Molina, Victor and Ruiz-Celada, Oriol and Suarez, Raul and Rosell, Jan and Zaplana, Isiah},
  booktitle={2025 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)}, 
  title={Robot Situation and Task Awareness Using Large Language Models and Ontologies}, 
  year={2025},
  volume={},
  number={},
  pages={96-103},
  abstract={Robot situation and task awareness requires a deep understanding of the environment, the domain knowledge, and task planning. We present a novel framework that integrates ontologies, Large Language Models (LLMs), and the Planning Domain Definition Language (PDDL) to enhance the comprehension capabilities of robotic systems. The framework employs an LLM to extract structured knowledge from natural language descriptions provided by a human user, populating an OWL ontology that captures relevant objects, properties, and relations. This populated ontology is then used to parse a PDDL Domain file and generate a corresponding PDDL Problem file to solve particular planning problems. This research contributes to the intersection of knowledge representation, natural language processing, and automated planning, providing a solution for intuitive human-robot interaction through LLMs.},
  keywords={Large language models;Conferences;OWL;Human-robot interaction;Ontologies;Natural language processing;Planning;Robots;robotic manipulation;task planning;ontologies;Large Language Models},
  doi={10.1109/DSN-W65791.2025.00045},
  ISSN={2325-6664},
  month={June},}@ARTICLE{11078835,
  author={Niu, Guanglin and Li, Bo and Feng, Siling},
  journal={IEEE Transactions on Big Data}, 
  title={A Pluggable Common Sense-Enhanced Framework for Knowledge Graph Completion}, 
  year={2025},
  volume={},
  number={},
  pages={1-18},
  abstract={Knowledge graph completion (KGC) tasks aim to infer missing facts in a knowledge graph (KG) for many knowledgeintensive applications. However, existing embedding-based KGC approaches primarily rely on factual triples, potentially leading to outcomes inconsistent with common sense. Besides, generating explicit common sense is often impractical or costly for a KG. To address these challenges, we propose a pluggable common sense-enhanced KGC framework that incorporates both fact and common sense for KGC. This framework is adaptable to different KGs based on their entity concept richness and has the capability to automatically generate explicit or implicit common sense from factual triples. Furthermore, we introduce common senseguided negative sampling and a coarse-to-fine inference approach for KGs with rich entity concepts. For KGs without concepts, we propose a dual scoring scheme involving a relation-aware concept embedding mechanism. Importantly, our approach can be integrated as a pluggable module for many knowledge graph embedding (KGE) models, facilitating joint common sense and fact-driven training and inference. The experiments illustrate that our framework exhibits good scalability and outperforms existing models across various KGC tasks.},
  keywords={Training;Ontologies;Knowledge graphs;Tensors;Internet;Uncertainty;Translation;Large language models;Data models;Visualization;Knowledge graph completion;pluggable framework;common sense;entity concepts;negative sampling},
  doi={10.1109/TBDATA.2025.3588081},
  ISSN={2332-7790},
  month={},}@ARTICLE{11075757,
  author={Sheth, Amit P. and Roy, Kaushik and Venkataramanan, Revathy and Nadimuthu, Venkatesan and Shyalika, Chathurangi},
  journal={IEEE Internet Computing}, 
  title={Composite AI With Custom, Compact, Neurosymbolic Models: The Emergent Enterprise Artificial Intelligence Paradigm}, 
  year={2025},
  volume={29},
  number={2},
  pages={37-49},
  abstract={Artificial-intelligence architectures are shifting from monolithic, internet-scale models toward multi-component composite systems that must perform reliably in high-stakes settings. We introduce Custom, Compact and Composite AI with a Neurosymbolic approach (C3AN), a fourth-generation framework that integrates data, domain knowledge and human expertise through 14 foundation elements spanning reliability, grounding and safety. Custom focuses on right-sized, domain-specific data and workflows; Compact emphasizes resource-conscious models that can run on edge or mobile hardware; Composite unifies neural, symbolic and feedback modules for end-to-end reasoning. Three pilot systems illustrate the paradigm: Nourich (diabetes-aware dietary guidance) outperforms nine LLM baselines on recipe suitability; MAIC (K–12 mental-health triage) matches PHQ-9 gold-standard accuracy while halving compute; and SmartPilot (edge manufacturing copilot) achieves 93% anomaly accuracy and 21% forecasting gain over LSTM baselines. C3AN demonstrates that domain-aligned, neurosymbolic design can deliver transparent, trustworthy enterprise AI without extreme scale.},
  keywords={Computational modeling;Mission critical systems;Cognition;Safety;Manufacturing;Reliability;Artificial intelligence;Long short term memory;Neural engineering},
  doi={10.1109/MIC.2025.3570554},
  ISSN={1941-0131},
  month={March},}@ARTICLE{11072468,
  author={Vereno, Dominik and Neureiter, Christian and Eschlberger, Simon and Millaku, Mergim and Kuchenbuch, René and Uslar, Mathias},
  journal={IEEE Access}, 
  title={SGAM Toolbox Revisited: A Standards-Based Domain-Specific Modeling Language and Toolset}, 
  year={2025},
  volume={13},
  number={},
  pages={119243-119261},
  abstract={The SGAM Toolbox has established itself as a valuable modeling tool in the energy sector, particularly for interdisciplinary system-of-systems use cases. Built on a domain-specific modeling language that is anchored in European smart grid standardization, the toolbox has been adopted across academia and industry. Drawing on this extensive experience, we introduce the new and improved SGAM Toolbox. First, we review published applications of the tool; it has been widely used for high-level architecture modeling, often alongside other software in areas such as security, privacy, and e-mobility integration. Based on our findings, the key updates for the new toolbox include a strong formal foundation, a clear definition on how the tool should interface with requirements engineering, comprehensive semantics, and a viewpoint structure that segregates logical from technical aspects; these improvements enhance usability and real-world applicability. Second, the updated toolbox is presented in accordance with the TILO language-engineering stack; the specification includes the underlying ontology, a MOF-conformant metamodel, a UML-based implementation, and a modeling add-in for Enterprise Architect for advanced features. Third, we offer a practical demonstration of the toolbox, showing use case–driven architecture modeling. The SGAM Toolbox aims to strengthen its role as a platform for collaborating on system-of-systems use cases, bringing together the diverse stakeholders in smart grid projects.},
  keywords={Unified modeling language;Modeling;Smart grids;DSL;Stakeholders;Ontologies;Computer architecture;Object oriented modeling;Systems engineering and theory;Syntactics;Smart grid architecture model;systems engineering;model-based systems engineering;use case;domain-specific language;smart grid;system of systems;architecture},
  doi={10.1109/ACCESS.2025.3586722},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11050458,
  author={Shyalika, Chathurangi and Prasad, Renjith and Al Ghazo, Alaa and Eswaramoorthi, Darssan and Kaur, Harleen and Muthuselvam, Sara Shree and Sheth, Amit},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={SmartPilot: A Multiagent CoPilot for Adaptive and Intelligent Manufacturing}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={In the dynamic landscape of Industry 4.0, achieving efficiency, precision, and adaptability is essential to optimize manufacturing operations. Industries suffer due to supply chain disruptions caused by anomalies, which are being detected by current AI models but leaving domain experts uncertain without deeper insights into these anomalies. Additionally, operational inefficiencies persist due to inaccurate production forecasts and the limited effectiveness of traditional AI models for processing complex sensor data. Despite these advancements, existing systems lack the seamless integration of these capabilities needed to create a truly unified solution for enhancing production and decision-making. We propose SmartPilot, a neurosymbolic, multiagent CoPilot designed for advanced reasoning and contextual decision-making to address these challenges. SmartPilot processes multimodal sensor data and is compact to deploy on edge devices. It focuses on three key tasks: anomaly prediction, production forecasting, and domain-specific question answering. By bridging the gap between AI capabilities and real-world industrial needs, SmartPilot empowers industries with intelligent decision-making and drives transformative innovation in manufacturing. The demonstration video, datasets, and supplementary materials are available at https://github.com/ChathurangiShyalika/SmartPilot.},
  keywords={Industries;Technological innovation;Multimodal sensors;Decision making;Supply chains;Production;Predictive models;Question answering (information retrieval);Artificial intelligence;Smart manufacturing;Smart Manufacturing;Multimodal data;CoPilot;Multiagent;Neurosymbolic AI},
  doi={10.1109/CAI64502.2025.00007},
  ISSN={},
  month={May},}@INPROCEEDINGS{11050553,
  author={Tong, Richard J. and Cortês, Marina and DeFalco, Jeanine A. and Underwood, Mark and Zalewski, Janusz},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={A First-Principles Based Risk Assessment Framework and the IEEE P3396 Standard}, 
  year={2025},
  volume={},
  number={},
  pages={1588-1595},
  abstract={Generative Artificial Intelligence (AI) is enabling unprecedented automation in content creation and decision support, but it also raises novel risks. This paper presents a first-principles risk assessment framework underlying the IEEE P3396 Recommended Practice for AI Risk, Safety, Trustworthiness, and Responsibility. We distinguish between process risks (risks arising from how AI systems are built or operated) and outcome risks (risks manifest in the AI system's outputs and their real-world effects), arguing that generative AI governance should prioritize outcome risks. Central to our approach is an information-centric ontology that classifies AI-generated outputs into four fundamen-tal categories: (1) Perception-level information, (2) Knowledge-level information, (3) Decision/Action plan information, and (4) Control tokens (access or resource directives). This classification allows systematic identification of harms and more precise attribution of responsibility to stakeholders (developers, deployers, users, regulators) based on the nature of the information produced. We illustrate how each information type entails distinct outcome risks (e.g, deception, misinformation, unsafe recommendations, security breaches) and requires tailored risk metrics and mitigations. By grounding the framework in the essence of information, human agency, and cognition, we align risk evaluation with how AI outputs influence human understanding and action. The result is a principled approach to AI risk that supports clear accountability and targeted safeguards, in contrast to broad application-based risk categorizations. We include example tables mapping information types to risks and responsibilities. This work aims to inform the IEEE P3396 Recommended Practice and broader AI governance with a rigorous, first-principles foundation for assessing generative AI risks while enabling responsible innovation.},
  keywords={Ethics;Technological innovation;Regulators;Generative AI;Standards organizations;Organizations;Cognition;Stakeholders;Security;Fake news;Risk;Information Categorization;AI Agency;Human Agency;GenAl},
  doi={10.1109/CAI64502.2025.00237},
  ISSN={},
  month={May},}@INPROCEEDINGS{11050462,
  author={Xu, Bin and Tong, Richard J and Li, Yanyan and Chen, Penghe and Li, Hanming and Liang, Joleen and Fan, Xing and Tong, Jessie},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={An Architectural Framework for Educational Knowledge Graphs (IEEE P2807.6): Ontology Design, Llm Integration, and Adaptive Learning Applications}, 
  year={2025},
  volume={},
  number={},
  pages={1610-1616},
  abstract={The IEEE P2807.6 Education Knowledge Graph (EduKG) standard defines a semantic infrastructure to represent educational knowledge, resources, and pedagogy in a unified graph format. This paper expands on the core EduKG architecture, detailing its ontology design and key entities-Learning Points, Resource Items, and Pedagogical Rules-that collectively model the domain, content, and instructional strategies of learning systems. We further explore how EduKG can be integrated with advanced AI technologies, including large language models (LLMs) and retrieval-augmented generation (Graph-RAG) via embedding databases, to enable intelligent behavior such as semantic search, question answering, and dynamic content generation. These integrations position EduKG as a central component in next-generation smart education systems, wherein knowledge graphs work in concert with intelligent agents and adaptive instructional systems to deliver fully automated, personalized, and interactive learning experiences. By leveraging the standardized graph-structured representation and semantic reasoning capabilities of EduKG, such systems can achieve interoperability across platforms and support complex AI-driven tutoring and training scenarios. This work provides a comprehensive overview of the EduKG framework and highlights its role in empowering adaptive, cognitive, and collaborative learning solutions for the future of digital education.},
  keywords={Learning systems;Adaptive systems;Large language models;Semantics;Retrieval augmented generation;Knowledge graphs;Ontologies;Artificial intelligence;Standards;Interoperability;Education Knowledge Graph;Ontology;Large Language Models;Retrieval-Augmented Generation;Adaptive Instructional Systems;Intelligent Tutoring;Interoperability},
  doi={10.1109/CAI64502.2025.00249},
  ISSN={},
  month={May},}@INPROCEEDINGS{11050705,
  author={Do, Thanh Son and Hier, Daniel B. and Obafemi-Ajayi, Tayo},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Mapping Biomedical Ontology Terms to Ids: Effect of Domain Prevalence on Prediction Accuracy}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This study evaluates the ability of large language models (LLMs) to map biomedical ontology terms to their corresponding ontology IDs across the Human Phenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies. Using counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate for their prevalence in the biomedical literature, we examined the relationship between ontology ID prevalence and mapping accuracy. Results indicate that ontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO IDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers. Higher prevalence of ontology IDs in the biomedical literature correlated with higher mapping accuracy. Predictive models based on receiver operating characteristic (ROC) curves confirmed this relationship. In contrast, this pattern did not apply to mapping protein names to Human Genome Organisation's (HUGO) gene symbols. GPT-4 achieved a high baseline performance (95 %) in mapping protein names to HUGO gene symbols, with mapping accuracy unaffected by prevalence. We propose that the high prevalence of HUGO gene symbols in the literature has caused these symbols to become lexicalized, enabling GPT-4 to map protein names to HUGO gene symbols with high accuracy. These findings highlight the limitations of LLMs in mapping ontology terms to low-prevalence ontology IDs and underscore the importance of incorporating ontology ID prevalence into the training and evaluation of LLMs for biomedical applications.},
  keywords={Proteins;Training;Protein engineering;Accuracy;Phenotypes;Terminology;Large language models;Symbols;Training data;Ontologies;Ontology mapping;machine codes;large language models;Zipf's Law;Gene Ontology;Human Phenotype Ontology;lexicalization;UniProt KB},
  doi={10.1109/CAI64502.2025.00101},
  ISSN={},
  month={May},}@INPROCEEDINGS{11050457,
  author={Al Khatib, Hassan S. and Mittal, Sudip and Rahimi, Shahram and Marhamati, Nina and Bozorgzad, Sean},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={From Patient Consultations to Graphs: Leveraging LLMs for Patient Journey Knowledge Graph Construction}, 
  year={2025},
  volume={},
  number={},
  pages={410-415},
  abstract={The shift toward patient-centric healthcare requires understanding comprehensive patient journeys. Current healthcare data systems often fail to provide holistic representations, hindering coordinated care. Patient Journey Knowledge Graphs (PJKGs) solve this by integrating diverse patient information into unified, structured formats. This paper presents a methodology for constructing PJKGs using Large Language Models (LLMs) to process both clinical documentation and patient-provider conversations. These graphs capture temporal and causal relationships between clinical events, enabling advanced reasoning and personalized insights. Our evaluation of four LLMs (Claude 3.5, Mistral, Llama 3.1, ChatGPT4o) shows all achieved perfect structural compliance but varied in medical entity processing, computational efficiency, and semantic accuracy. This work advances patient-centric healthcare through actionable knowledge graphs (KGs) that enhance care coordination and outcome prediction.},
  keywords={Accuracy;Large language models;Semantics;Medical services;Knowledge graphs;Oral communication;Documentation;Cognition;Data systems;Computational efficiency;Healthcare Journey Mapping;Large Language Model (LLM);Temporal and Causal Reasoning;Patient-Centric Healthcare;Knowledge Graph},
  doi={10.1109/CAI64502.2025.00075},
  ISSN={},
  month={May},}@INPROCEEDINGS{11058583,
  author={Menad, Safaa and Medeiros, Gabriel H. A. and Soualmia, Lina F.},
  booktitle={2025 IEEE 38th International Symposium on Computer-Based Medical Systems (CBMS)}, 
  title={Enhancing the Description-Detection Framework with Semantic Clustering Using Biostransformers}, 
  year={2025},
  volume={},
  number={},
  pages={654-659},
  abstract={Event-Based Surveillance Systems (EBS) are crucial for detecting emerging public health threats. However, these systems face significant challenges, including overreliance on manual expert intervention, limited handling of heterogeneous textual data, etc. The Description-Detection Framework (DDF) addresses some of these limitations by leveraging PropaPhen (Core Propagation Phenomenon Ontology), UMLS, and OpenStreetMaps to detect suspicious health-related cases using spatiotemporal and textual data. However, DDF is restricted to detection and lacks the ability to classify the detected observations into meaningful categories. To adress this limitation, we propose to enhance DDF by incorporating a clustering-based classification process. This enhancement employs BioSTransformers, a pretrained biomedical language model built on Sentence Transformers trained on PubMed data, to compute semantic similarity between observations. By capturing domain-specific semantic relationships, BioSTransformers enables clustering that integrates biological semantics with spatiotemporal context, outperforming traditional methods from the literature in observation classification. Our proposed approach reduces the dependency on manual expert effort, improves the system's ability to process heterogeneous data, and enhances the accuracy and contextual relevance of case classification. The results demonstrate the potential of this method to advance EBS systems, providing a scalable and automated solution to public health surveillance challenges.},
  keywords={Biological system modeling;Computational modeling;Surveillance;Unified modeling language;Semantics;Ontologies;Transformers;Spatiotemporal phenomena;Public healthcare;Context modeling;Public Health Surveillance;Biomedical Language Models;Spatiotemporal Reasoning;Description Detection Framework;Core Propagation Phenomenon Ontology},
  doi={10.1109/CBMS65348.2025.00135},
  ISSN={2372-9198},
  month={June},}@INPROCEEDINGS{11058739,
  author={Menad, Safaa and Abdeddaïm, Saïd and Soualmia, Lina F.},
  booktitle={2025 IEEE 38th International Symposium on Computer-Based Medical Systems (CBMS)}, 
  title={Predicting Similarities Between Biomedical Ontologies : The UMLs Use-Case}, 
  year={2025},
  volume={},
  number={},
  pages={648-653},
  abstract={Biomedical ontologies are crucial for organizing domain-specific knowledge, yet traditional alignment methods relying on lexical matching often fail to capture complex semantic relationships. To address this limitation, we propose a novel approach leveraging siamese neural networks and transformerbased models to enhance ontology alignment within the biomedical domain. Our method applies self-supervised contrastive learning to biomedical literature, optimizing the prediction of semantic similarities between concepts in the UMLS Metathesaurus. The results demonstrate that this approach surpasses lexical-based techniques by identifying contextual relationships and uncovering new interconnections among UMLS terminologies. This highlights the potential of our models in improving ontology alignment and enriching biomedical knowledge integration.},
  keywords={Vocabulary;Terminology;Biological system modeling;Unified modeling language;Semantics;Neural networks;Contrastive learning;Ontologies;Transformers;Vectors;Ontology;Alignment;Matching;UMLS Metathesaurus;Biomedical Ontology;Transformers;Siamese Neural Network;Semantic Similarity;Sentence Embeddings},
  doi={10.1109/CBMS65348.2025.00134},
  ISSN={2372-9198},
  month={June},}@INPROCEEDINGS{11049984,
  author={Zlobin, O. N. and Litvinov, V. L. and Filippov, F. V.},
  booktitle={2025 VI International Conference on Neural Networks and Neurotechnologies (NeuroNT)}, 
  title={Domain-Specific Language Models for Continuous Learning}, 
  year={2025},
  volume={},
  number={},
  pages={3-5},
  abstract={The paper examines the issue of building domain-specific (domain-oriented) language models and the possibility of their continuous learning. A concept is proposed that provides for the introduction of additional cognitive memory with domain administration, designed for the gradual accumulation of new information obtained after the termination of learning the language model. The issue of transition from the inference mode to a new model learning is considered.},
  keywords={Training;Knowledge engineering;Buildings;Ontologies;Search problems;Data mining;Tuning;Neurotechnology;Domain specific languages;Context modeling;artificial intelligence;language models;verbal memory;cognitive memory;update context;embeddings;domains;ontology concepts},
  doi={10.1109/NeuroNT66873.2025.11049984},
  ISSN={},
  month={June},}@INPROCEEDINGS{11059459,
  author={Gueddes, Abdelweheb and Fathallah, Wyssem and Mahjoub, Mohamed Ali},
  booktitle={2025 International Wireless Communications and Mobile Computing (IWCMC)}, 
  title={BERT-Based Knowledge Graph Construction from Social Media}, 
  year={2025},
  volume={},
  number={},
  pages={461-466},
  abstract={Social media platforms serve as massive repositories of textual data, reflecting diverse human interactions and preferences. However, the unstructured nature of this content poses significant challenges for extracting semantically rich insights. This paper introduces a novel methodology for the automated construction of knowledge graphs (KGs) from social media discourse, specifically focusing on Twitter tweets. Our approach synergistically integrates large language models (LLMs), specifically a fine-tuned BERT model, with an ontology-driven framework. First, we define a detailed ontology of online communication concepts. The pre-trained BERT model is then fine-tuned using a multi-task learning approach on a curated dataset of anonymized and segmented Twitter discussions, thereby aligning its semantic representations with the predefined ontology. The fine-tuned LLM is leveraged for several critical tasks including entity and relation extraction, sentiment analysis, intention classification and the inference of contextual information and discussion styles. Furthermore, a mechanism is introduced to infer inter-user relationships and shared interests using graph neural networks (GNNs), analyzing patterns in interaction and language use. This multi-faceted extracted and inferred data is subsequently employed to build a knowledge graph, stored and queried via the Neo4j graph database management system. This study presents several contributions such as the integration of a ontology with an LLM method and the innovative user relationship and shared interest extraction using graph neural networks. The proposed methodology was rigorously evaluated using a real-world dataset of Twitter discussions, showcasing its ability to capture semantic content, and elucidate inter-user relationships effectively and revealing shared interests of the users involved. Furthermore, an ablation study is included which further demonstrates each of the method component contribution and demonstrates the importance of such integrations. Our findings highlight the potential for various downstream applications such as in community structure analysis and sentiment analysis to improve information management within online social networks.},
  keywords={Sentiment analysis;Ethics;Social networking (online);Scalability;Large language models;Blogs;Semantics;Knowledge graphs;Ontologies;Graph neural networks;Knowledge Graph;Social Media Analysis;Large Language Models (LLMs);Ontology-Driven Methodology;BERT},
  doi={10.1109/IWCMC65282.2025.11059459},
  ISSN={2376-6506},
  month={May},}@INPROCEEDINGS{11052053,
  author={Shukla, Varsha and Pradhan, Rahul},
  booktitle={2025 International Conference on Intelligent and Cloud Computing (ICoICC)}, 
  title={Protein Matching for Function Prediction using Siamese Neural Network}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Protein similarity lies at the heart of many critical tasks in bio-informatics, from predicting protein function to drug discovery and evolutionary studies. The discipline has deployed traditional approaches such as sequence alignment and structural comparison to serve the field for decades, their limitations in scalability, sensitivity to remote homologs, and reliance on handcrafted features have become increasingly apparent in the era of high-throughput data. In this study, one of the concept of deep learning, specifically Siamese neural networks for similarity matching, is explored for protein matching. By leveraging pretrained protein sequence embeddings and biologically informed functional annotations, a framework is developed that can learn nuanced relationships between protein pairs.},
  keywords={Heart;Deep learning;Cloud computing;Sensitivity;Scalability;Graph neural networks;Protein sequence;Biology;Drug discovery;Convolutional neural networks;Siamese neural network (SNN);Convolutional neural network (CNN);Graph neural network (GNN)},
  doi={10.1109/ICoICC64033.2025.11052053},
  ISSN={},
  month={May},}@INPROCEEDINGS{11047951,
  author={Loevenich, Johannes F. and Adler, Erik and Hürten, Tobias and Spelter, Florian and Roncevic, Damian and Lopes, Roberto Rigolin F.},
  booktitle={2025 International Conference on Military Communication and Information Systems (ICMCIS)}, 
  title={Automating Cyber Threat Intelligence and Attack Chain Generation using Cyber Security Knowledge Graphs and Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={Modern cyberattacks are increasingly complex, using sophisticated tactics, techniques and procedures (TTPs) to evade detection and compromise systems. Effective cyber defence relies on real-time and accurate Cyber Threat Intelligence (CTI), which is often challenged by data quality, completeness and accessibility. While traditional methods and manually maintained knowledge bases provide valuable insights, they struggle to adapt to the rapidly evolving threat landscape. To address these challenges, we propose an architecture that uses Large Language Models (LLMs) for automated annotation of CTI reports and construction of Cybersecurity Knowledge Graphs (CSKG) to build sophisticated attack chains. Building on our previous research, we extend the capabilities of Autonomous Cyber Defence (ACD) agents to improve situational awareness and defence mechanisms in dynamic environments. Experimental results demonstrate the effectiveness of our approach in improving CTI accessibility, accuracy, and integration into defence strategies. Our experimental results highlight the potential of combining LLM, knowledge graphs and automated planning to improve proactive cyber defence and attack simulation methodologies.},
  keywords={Military communication;Accuracy;Annotations;Large language models;Soft sensors;Knowledge based systems;Knowledge graphs;Real-time systems;Cyber threat intelligence;Planning;Autonomous Cyber Defence;Knowledge Graphs;Cybersecurity;Large Language Model},
  doi={10.1109/ICMCIS64378.2025.11047951},
  ISSN={2993-4974},
  month={May},}@ARTICLE{11053835,
  author={Nagata, Yoshiteru and Kohama, Daiki and Watanabe, Yoshiki and Katayama, Shin and Urano, Kenta and Yonezawa, Takuro and Kawaguchi, Nobuo},
  journal={IEEE Access}, 
  title={SemantiPack: An Efficient Real-World Data Compressor Using Structural and Semantic Metadata}, 
  year={2025},
  volume={13},
  number={},
  pages={114159-114178},
  abstract={The exponential growth of Real-World Data (RWD), primarily collected from IoT sensors and spanning domains such as mobility, environment, and energy consumption, presents critical challenges due to its scale, heterogeneity, and structural variability. Traditional compression methods often fail to adapt efficiently to these complexities, leading to sub-optimal storage and analysis performance. Additionally, while several metadata schemas exist to enhance the availability of RWD, smaller organizations often lack the resources to create and manage metadata effectively. This paper introduces RWD Profile, an automatically generatable metadata schema for RWD, and SemantiPack, which applies tailored compression to data fields in the individual RWD based on RWD Profile. RWD Profile has two kinds of metadata, Structural Profile and Semantic Profile. Structural Profile is generated through rule-based systems, while Semantic Profile is generated using large language models (LLMs) to capture semantic data properties. On the other hand, SemantiPack performs compression at the data field level in RWD using RWD Profile to achieve higher compression ratios. Experimental results demonstrate up to 23.2% improved compression rate for JSON and 19.3% for CSV compared to conventional methods while maintaining a faster or equivalent processing speed compared to conventional methods. Furthermore, SemantiPack supports lossy compression for applications prioritizing storage over precision. This research not only improves compression efficiency but also establishes a scalable solution for automated analysis and sustainable data utilization, paving the way for advancements in RWD management.},
  keywords={Metadata;Semantics;Data compression;Image coding;Sensor phenomena and characterization;Resource description framework;Ontologies;Standards organizations;Wireless sensor networks;Transform coding;Data compression;real-world data;linked data;semantic web},
  doi={10.1109/ACCESS.2025.3583829},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11042856,
  author={S, Jenny Kalaiarasi. and K, Nimala.},
  booktitle={2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)}, 
  title={Advancements in Context-Aware Recommender Systems: An Exhaustive Exploration of Ontology and LLM in improving ranking accuracy}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Recommendation systems have become a ubiquitous presence across various domains such as e-commerce, entertainment (movies and music), tourism, news, advertising, stock markets and social networks. The integration of Ontology and Large Language Models (LLMs) presents a transformative approach for enhancing recommender systems by improving contextual intelligence, personalization, and explainability. Traditional recommendation algorithms often suffer from data sparsity, cold-start problems, and limited semantic understanding, which hinder their ability to deliver highly relevant and interpretable suggestions. This research provides an exhaustive exploration of how ontology-driven knowledge representation can be seamlessly combined with LLMs to refine user-item interactions, enrich embeddings, and improve reasoning in recommendation pipelines. We propose a hybrid framework where ontological structures encode domain-specific relationships while LLMs process, infer, and enhance recommendations through contextual embeddings.This synergy enables semantic-aware recommendations, explainable item suggestions, and adaptive learning mechanisms that adjust based on user behavior and external context. Furthermore, we investigate novel ontology injection techniques, including knowledge graph-based reinforcement, to improve long-tail item exposure and enhance recommendation diversity. Through rigorous evaluation on real-world datasets, we demonstrated how ontology-LLM fusion significantly outperforms baseline models in accuracy, novelty, and interpretability. This study establishes a foundation for the next generation of context-aware, knowledge-enhanced recommender systems that leverage LLMs for reasoning and ontologies for structured knowledge representation, paving the way for more intelligent and user-centric recommendations.},
  keywords={Accuracy;Heavily-tailed distribution;Social networking (online);Semantics;Diversity reception;Ontologies;Cognition;Graph neural networks;Stock markets;Recommender systems;Context-Aware Recommendation Systems;Computational intelligence;LLM;Ontology;semantic-aware},
  doi={10.1109/RMKMATE64874.2025.11042856},
  ISSN={},
  month={May},}@INPROCEEDINGS{11042435,
  author={Shi, Ruhui and Chen, Tianran and Lu, Chunyu and Shang, Duo and Luo, Jun and Hui, Xin and Li, Haoran and He, Huihong},
  booktitle={2025 8th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)}, 
  title={Dynamic Assessment and Early Warning of Cross-Border Pipeline Risks Based on Knowledge Graph}, 
  year={2025},
  volume={},
  number={},
  pages={374-377},
  abstract={This paper introduces a novel framework for the dynamic assessment and early warning of cross-border pipeline risks, leveraging the zero-shot learning capabilities of the GPT-4 large language model (LLM) to construct and utilize a domain-specific knowledge graph. The approach circumvents the need for extensive pre-training or fine-tuning, thereby enabling rapid development and deployment in a domain characterized by data scarcity. The methodology begins with the construction of a knowledge graph, which involves a comprehensive ontology design, followed by knowledge extraction from unstructured text sources, and culminates in knowledge representation and storage in a structured format. Subsequently, the framework facilitates dynamic risk understanding and analysis, encompassing risk factor identification, risk event inference, and the analysis of complex relationships between these elements, all supported by the structured knowledge base. Finally, the system provides knowledge-based risk interpretation and decision-making support, generating explanations for risk events, assessing their potential impacts, and offering actionable recommendations to stakeholders. By employing carefully crafted prompts and the inherent natural language processing capabilities of GPT-4, the framework facilitates the extraction of entities and relationships from unstructured textual data, effectively populating the knowledge graph. This knowledge graph, representing entities, attributes, and relationships pertinent to cross-border pipeline risks, serves as a foundation for dynamic risk assessment. This method presents a scalable, data-efficient approach tailored to the specialized domain of cross-border pipeline risk management, particularly advantageous in contexts with limited resources. The proposed framework demonstrates the potential of LLMs, specifically GPT-4, in conjunction with knowledge graph methodologies, to advance risk management practices in complex, data-sparse domains.},
  keywords={Large language models;Pipelines;Knowledge based systems;Zero shot learning;Knowledge graphs;Ontologies;Safety;Risk management;Stakeholders;Software engineering;LLM;China-Russia pipeline;knowledge graph},
  doi={10.1109/AEMCSE65292.2025.11042435},
  ISSN={},
  month={May},}@INPROCEEDINGS{11041826,
  author={Wang, Shenghui and Li, Mengjiao and Miao, Yuxin and Liu, Nan and Sun, Zihao},
  booktitle={2025 7th International Conference on Information Science, Electrical and Automation Engineering (ISEAE)}, 
  title={Construction and application of power transformer fault knowledge graph based on Sentence-BERT}, 
  year={2025},
  volume={},
  number={},
  pages={841-846},
  abstract={With the rapid development of smart grid, the relevance of multidimensional information in power grid is weak, and the decision-making generation efficiency in operation and maintenance process is low. At present, knowledge graph has innovative applications in various fields, which improves the efficiency of knowledge query in related professional fields. This paper proposes a power transformer fault diagnosis system driven by knowledge graph, which aims to solve the problems of weak information relevance and low decision-making efficiency in transformer operation and maintenance; The system is composed of three modules: data acquisition, data analysis and data service. By constructing the knowledge graph and model of fault diagnosis, the intelligent query and diagnosis of transformer fault are realized; The construction of knowledge graph is based on expert knowledge and fault data, and the entity extraction is carried out by using the Sentence-BERT-BiLSTM-CRF model, which improves the extraction accuracy of transformer fault information; The three tuple data is stored in the neo4j graph database, and the cypher language is used for efficient query, providing accurate fault diagnosis and maintenance strategies for operation and maintenance personnel. Experimental results show that the proposed method can effectively improve the accuracy and efficiency of fault diagnosis.},
  keywords={Fault diagnosis;Accuracy;Decision making;Knowledge graphs;Ontologies;Data models;Maintenance;Power transformers;Data mining;Visual databases;Knowledge graph;Knowledge extraction;Fault diagnosis;Power transformer;Bert model},
  doi={10.1109/ISEAE64934.2025.11041826},
  ISSN={},
  month={April},}@INPROCEEDINGS{11035002,
  author={Zhao, Wei and Bai, Wei and Hu, Zhi-Yuan and He, Chao-Xin},
  booktitle={2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={DMM-GPT: A Dual Modeling Mechanism-Based Generic Protocol Translation Framework}, 
  year={2025},
  volume={},
  number={},
  pages={1-12},
  abstract={With the increasing complexity of heterogeneous communication environments, the problem of data sharing caused by multiple heterogeneous protocols has become the core challenge restricting interoperability. Aiming at the problems of poor scalability and automation ability of existing protocol translation methods, this study proposes a Dual Model Mechanism-based Generic Protocol Translation (DMM-GPT) framework. The framework creatively combines ontology-based semantic modeling with state-machine-based behavioral modeling, enabling automated identification of protocol translation bridging points and autonomous construction of a unified protocol state transition model. Firstly, a protocol ontology model is constructed to model the semantic associations between protocol entities. Based on the ontology, semantic information is extracted. Semantic similarities from message level to field level are computed, thereby sequentially identifying message mapping and field mapping relationships. Secondly, modeling protocol behavior through state machine models. Based on the identified mapping relationships, the heterogeneous protocol bridge points are constructed, and a unified state transition model is synthesized automatically. Thus, interoperable behavior between heterogeneous protocols is implemented. Finally, the complete closed loop of protocol interoperability is implemented by applying semantic alignment results to the state transition model. To validate the effectiveness of DMM-GPT, experiments were conducted in two representative scenarios: service discovery (SLP-Bonjour) and IoT communication (HTTP-CoAP). The results demonstrate its effectiveness in enabling interoperability between heterogeneous protocols.},
  keywords={Seminars;Protocols;Translation;Computational modeling;Scalability;Semantics;Ontologies;Data mining;Information technology;Interoperability;protocol translation;semantic similarity;ontology modeling;state machine},
  doi={10.1109/AINIT65432.2025.11035002},
  ISSN={},
  month={April},}@INPROCEEDINGS{11033426,
  author={Matta, Nada and Pfundstein, Patrick and Larde, Camille and Ismedon, Baptiste},
  booktitle={2025 28th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
  title={Answering Application of Generative AI in Industry: Integration of Semantic Representation}, 
  year={2025},
  volume={},
  number={},
  pages={825-828},
  abstract={Generative AI acts as an effective guide in decision-making process. This paper is designed to outline the main challenges in applying this technique within businesses and for specific activities. Semantic representation as ontology can be one solution for these challenges. Our first work to link ontology to LLM algorithms is illustrated in financial institutions},
  keywords={Industries;Machine learning algorithms;Generative AI;Text recognition;Federated learning;Semantics;Retrieval augmented generation;Finance;Companies;Ontologies;Generative AI;ChatGPT;Bard;LLM;Ontology;finance},
  doi={10.1109/CSCWD64889.2025.11033426},
  ISSN={2768-1904},
  month={May},}@INPROCEEDINGS{11036147,
  author={Onozuka, Soichi and Ohnishi, Takaaki},
  booktitle={2025 19th International Conference on Semantic Computing (ICSC)}, 
  title={Analysis of LLMs for RDF Triple Generation: Semantic and Syntactic Evaluation Using WebNLG}, 
  year={2025},
  volume={},
  number={},
  pages={136-143},
  abstract={Using the WebNLG dataset as ground truth, we evaluate the capability of large language models (LLMs) to generate resource description framework triples from natural language input. The proposed method employs two complementary evaluation metrics: cosine similarity for assessing semantic proximity and graph edit distance for comparing structural aspects of triple sets. The analysis demonstrates that these metrics provide distinct yet complementary perspectives on semantic evaluation, enabling a comprehensive assessment of the natural language understanding capabilities of LLMs. Through this approach, we demonstrate that modern LLMs exhibit sophisticated abilities in integrated syntax and semantics processing by utilizing distributed representations where both types of information coexist within high-dimensional vector spaces. This integration suggests that understanding of language structure of LLMs transcends simple pattern recognition to achieve meaningful semantic comprehension.},
  keywords={Measurement;Large language models;Semantics;Syntactics;Ontologies;Resource description framework;Vectors;Natural language processing;Pattern recognition;Ontology;RDF;LLMs;Similarity},
  doi={10.1109/ICSC64641.2025.00025},
  ISSN={2472-9671},
  month={Feb},}@INPROCEEDINGS{11036150,
  author={Johnson, Matthew and Rashid, Sabbir M. and McGuinness, Deborah L.},
  booktitle={2025 19th International Conference on Semantic Computing (ICSC)}, 
  title={Improving Tabular Reusability Through Data Dictionary Descriptions}, 
  year={2025},
  volume={},
  number={},
  pages={209-216},
  abstract={Tables have become a ubiquitous standard for capturing, storing, and sharing data on the web. This is primarily due to the semi-structured nature of tables, where relationships between data are often ambiguously encoded using locality. While this format can be easy for humans to interpret in simple cases, as table complexity increases, so does the difficulty in interpretability. To bridge this context gap, many data publishers provide a data dictionary to capture schema elements' meaning through text descriptions. Existing work compounds the need for data dictionaries to improve tabular interoperability, but few provide detailed requirements for data dictionary descriptions. This paper identifies and defines three common types of data dictionary descriptions in the biomedical domain. We then compare the effectiveness of each description type by normalizing data dictionary descriptions to a single type using large language models and measuring their performance using a semantic tabular interpretation algorithm. Our experiments show that intensional descriptions, which describe the general properties a column member should have, are most effective for tabular alignment and improve the reusability of data dictionaries.},
  keywords={Dictionaries;Reviews;Large language models;Semantics;Information sharing;Complexity theory;Compounds;Interoperability;Standards;Lenses;tabular data;data dictionary;semantic data dictionary;semantic annotation;large language model},
  doi={10.1109/ICSC64641.2025.00035},
  ISSN={2472-9671},
  month={Feb},}@INPROCEEDINGS{11036145,
  author={Jiang, Longquan and Huang, Junbo and Möller, Cedric and Usbeck, Ricardo},
  booktitle={2025 19th International Conference on Semantic Computing (ICSC)}, 
  title={Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge Graph Question Answering}, 
  year={2025},
  volume={},
  number={},
  pages={28-35},
  abstract={Most existing Knowledge Graph Question Answering (KGQA) approaches are designed for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the heterogeneity of the underlying graph schema, topology and assertions, most KGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without resource-intensive training data. We present OntoSCPrompt, a novel Large Language Model (LLM)-based KGQA approach with a two-stage architecture that separates semantic parsing from KG-dependent interactions. OntoSCPrompt first generates a SPARQL query structure (including SPARQL keywords such as SELECT, ASK, WHERE and placeholders for missing tokens) and then fills them with KG-specific information. To enhance the understanding of the underlying KG, we present an ontology-guided, hybrid prompt learning strategy that integrates KG ontology into the learning process of hybrid prompts (e.g., discrete and continuous vectors). We also present several task-specific decoding strategies to ensure the correctness and executability of generated SPARQL queries in both stages. Experimental results demonstrate that OntoSCPrompt performs as well as SOTA approaches without retraining on a number of KGQA datasets such as CWQ, WebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well to unseen domain-specific KGs like DBLP-QuAD and CoyPu KG 11Code: https://github.com/LongquanJiang/OntoSCPrompt.},
  keywords={Large language models;Semantics;Training data;Knowledge graphs;Ontologies;Predictive models;Question answering (information retrieval);Vectors;Decoding;Topology;QA;KGQA;LLM;Generalization},
  doi={10.1109/ICSC64641.2025.00010},
  ISSN={2472-9671},
  month={Feb},}@INPROCEEDINGS{11034402,
  author={Robert, Kuo-Chung Lin and Jessica, Wen-Jie Lin},
  booktitle={2025 4th International Conference on Artificial Intelligence, Internet and Digital Economy (ICAID)}, 
  title={Case Study: Apply Ontology and AIGC for Task Appoint in Smart Building Control Centers}, 
  year={2025},
  volume={},
  number={},
  pages={22-26},
  abstract={Since the amazing effect of ChatGPT has been recognized by the industry, towards integrating with employees’ workflow to create more business applications. At the same time, many artificial intelligence scientists are actively looking for methods and platforms that are highly efficient and can quickly embed LLM and gain business value quickly. This study proposes two important innovations. First, it uses historical operating data to quickly and automatically generate LLMs in the company’s exclusive domain. Second, to quickly embed those models in ontology maps that to generate thinking and decision-making documents (as AIGC). In the paper, first step is use auto-insight engine to transfer input data from operational datasets. Secondly, to adjust parameters the training in deep learning framework to output models. Finally, integrate more API that they like conversation tools or other services. It can automatically generate and output high-precision event determination results in real time and can automatic notification and complete the processing flow that integrate daily report for frontline partners. In future, it will be able to develop more follow-up applications, such as artificial intelligence assistant for the security control center partner, which can determine the high level of daily incidents and automatically complete high-complexity daily operation procedures in a closed-loop security environment, and moderately connect high-level voice assistants to achieve important indicators such as health care, efficiency improvement and cost reduction.},
  keywords={Industries;Training;Smart buildings;Costs;Large language models;Biological system modeling;Ontologies;Predictive models;Data models;Security;Ontology;Smart Building;LLMs;AIGC},
  doi={10.1109/ICAID65275.2025.11034402},
  ISSN={},
  month={April},}@INPROCEEDINGS{11021058,
  author={Lee, Sam Yu-Te and Hung, Cheng-Wei and Yuan, Mei-Hua and Ma, Kwan-Liu},
  booktitle={2025 IEEE 18th Pacific Visualization Conference (PacificVis)}, 
  title={Visual Text Mining with Progressive Taxonomy Construction for Environmental Studies}, 
  year={2025},
  volume={},
  number={},
  pages={307-317},
  abstract={Environmental experts have developed the DPSIR (Driver, Pressure, State, Impact, Response) framework to systematically study and communicate key relationships between society and the environment. Using this framework requires experts to construct a DPSIR taxonomy from a corpus, annotate the documents, and identify DPSIR variables and relationships, which is laborious and inflexible. Automating it with conventional text mining faces technical challenges, primarily because the taxonomy often begins with abstract definitions, which experts progressively refine and contextualize as they annotate the corpus. In response, we develop GreenMine, a system that supports interactive text mining with prompt engineering. The system implements a prompting pipeline consisting of three simple and evaluable subtasks. In each subtask, the DPSIR taxonomy can be defined in natural language and iteratively refined as experts analyze the corpus. To support users evaluate the taxonomy, we introduce an uncertainty score based on response consistency. Then, we design a radial uncertainty chart that visualizes uncertainties and corpus topics, which supports interleaved evaluation and exploration. Using the system, experts can progressively construct the DPSIR taxonomy and annotate the corpus with LLMs. Using real-world interview transcripts, we present a case study to demonstrate the capability of the system in supporting interactive mining of DPSIR relationships, and an expert review in the form of collaborative discussion to understand the potential and limitations of the system. We discuss the lessons learned from developing the system and future opportunities for supporting interactive text mining in knowledge-intensive tasks for other application scenarios.},
  keywords={Text mining;Visualization;Uncertainty;Reviews;Taxonomy;Pipelines;Green products;Data visualization;Systems support;Information systems;Information systems;Information systems applications;Data Mining;Human-centered computing;Visualization;Visualization systems and tools},
  doi={10.1109/PacificVis64226.2025.00037},
  ISSN={2165-8773},
  month={April},}@INPROCEEDINGS{11028174,
  author={Guryanov, A. V. and Moshkin, V. S. and Dyrnochkin, A. A.},
  booktitle={2025 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)}, 
  title={An Approach to Automated Ontology Extraction From Technological Documentation Using NLP and LLM}, 
  year={2025},
  volume={},
  number={},
  pages={1011-1016},
  abstract={The paper presents an approach to constructing a formal description of a subject area using automated analysis of technological documentation. The first stage of the proposed approach is extracting data (terms, objects of the subject area) from technical documentation on the subject area. Then, a natural language query is formed for a large language model (LLM), which helps to extract relations between the extracted terms. At the final stage, an OWL ontology is formed by postprocessing the obtained data. Evaluation experiments were conducted on 16 documents (ISO) on the topic of “Space industry”. Experiments were also conducted to compare algorithms for extracting terms and relations using LLM and the C-value algorithm. The proposed approach has proven its effectiveness on strictly formalized texts.},
  keywords={Industries;Large language models;ISO Standards;OWL;Prototypes;Documentation;Ontologies;Industrial engineering;Natural language processing;Manufacturing;ontology;large language model;natural language processing;design documentation;technical documentation;term},
  doi={10.1109/ICIEAM65163.2025.11028174},
  ISSN={2993-4060},
  month={May},}@INPROCEEDINGS{11025595,
  author={Ahmed, Nafisa and Kwok, Hin Chi and Hamdaqa, Mohammad and Assunção, Wesley K. G.},
  booktitle={2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR)}, 
  title={SMATCH-M-LLM: Semantic Similarity in Metamodel Matching With Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={199-210},
  abstract={Metamodel matching plays a crucial role in defining transformation rules in model-driven engineering by identifying correspondences between different metamodels, forming the foundation for effective transformations. Current techniques face significant challenges due to syntactical and structural heterogeneity. To address this, matching techniques often employ semantic similarity to identify correspondences. Traditional semantic matchers, however, rely on ontology matching tools or lexical databases, which often struggle when metamodels use different terminologies or hierarchical structures. Inspired by the contextual understanding capabilities of Large Language Models (LLMs), this paper explores the capability of GPT-4 potentials as a semantic matcher and alternative to existing methods for metamodel matching. However, metamodels can be large, which can overwhelm LLMs if provided in a single prompt, leading to reduced accuracy. Therefore, we propose prompting LLMs with fragments of the source and target metamodels, identifying correspondences through an iterative process. The fragments to be provided in the prompt are identified based on an initial mapping derived from their elements’ definitions. Through experiments with 10 metamodels, our results show that our LLMbased approach improves the accuracy of metamodel matching, achieving an average F-measure of $\approx 91 \%$, outperforming both the baseline and hybrid approaches, which have a maximum average F-measure of $\approx \mathbf{2 9 \%}$ and $\approx \mathbf{7 4 \%}$, respectively. Moreover, our approach surpasses single-prompt LLM-based matching, which has an average $\mathbf{F}$-measure of $\mathbf{8 0 \%}$, by approximately $\mathbf{1 1 \%}$.},
  keywords={Accuracy;Costs;Terminology;Databases;Large language models;Semantics;Ontologies;Model driven engineering;Software;Iterative methods;Model-driven Engineering;Metamodel Matching;Domain-Specific Languages;Model Migration},
  doi={10.1109/MSR66628.2025.00040},
  ISSN={2574-3864},
  month={April},}@INPROCEEDINGS{11016516,
  author={Pruski, Cédric and Gallais, Marie and Da Silveira, Marcos},
  booktitle={2025 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Enhancing ESCO with Generative AI: A Dynamic Approach to Supporting 21st Century Education}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={In the rapidly evolving landscape of engineering education, upskilling and lifelong learning have become critical to maintaining competitiveness and fostering innovation. The use of ontologies, such as the European Skills, Competences, Qualifications, and Occupations (ESCO), plays a crucial role in organizing and managing the skills required for modern engineering roles. However, the slow pace of ontology updates and the lack of contextual adaptability present significant challenges, leading to outdated and irrelevant information for educators, learners, and industry professionals. This paper explores the potential of integrating Large Language Models (LLMs) with knowledge engineering to accelerate the process of updating ontologies like ESCO. By dynamically analyzing data and incor-porating contextual information, LLMs offer promising avenues for enhancing the evolution and precision of these ontologies. We discuss the potential impact of this approach in engineering education, particularly in aligning ups killing and reskilling efforts with the demands of emerging technologies such as AI -driven automation and digital engineering. This paper aims to highlight how LLMs can support the creation of more responsive, context-aware learning frameworks, ultimately sustaining educational ex-cellence and fostering critical thinking in engineering education.},
  keywords={Industries;Knowledge engineering;Technological innovation;Large language models;Soft sensors;Ontologies;Robustness;Multilingual;Engineering education;Qualifications;Ontology evolution;Contextualization;LLM;Upskilling;lifelong learning},
  doi={10.1109/EDUCON62633.2025.11016516},
  ISSN={2165-9567},
  month={April},}@INPROCEEDINGS{11016377,
  author={Abu-Rasheed, Hasan and Jumbo, Constance and Al Amin, Rashed and Weber, Christian and Wiese, Veit and Obermaisser, Roman and Fathi, Madjid},
  booktitle={2025 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={LLM-Assisted Knowledge Graph Completion for Curriculum and Domain Modelling in Personalized Higher Education Recommendations}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={While learning personalization offers great potential for learners, modern practices in higher education require a deeper consideration of domain models and learning contexts, to develop effective personalization algorithms. This paper introduces an innovative approach to higher education curriculum modelling that utilizes large language models (LLMs) for knowledge graph (KG) completion, with the goal of creating personalized learning-path recommendations. Our research focuses on modelling university subjects and linking their topics to corresponding domain models, enabling the integration of learning modules from different faculties and institutions in the student's learning path. Central to our approach is a collaborative process, where LLMs assist human experts in extracting high-quality, fine-grained topics from lecture materials. We develop a domain, curriculum, and user models for university modules and stakeholders. We implement this model to create the KG from two study modules: Embedded Systems and Development of Embedded Systems Using FPGA. The resulting KG structures the curriculum and links it to the domain models. We evaluate our approach through qualitative expert feedback and quantitative graph quality metrics. Domain experts validated the relevance and accuracy of the model, while the graph quality metrics measured the structural properties of our KG. Our results show that the LLM-assisted graph completion approach enhances the ability to connect related courses across disciplines to personalize the learning experience. Expert feedback also showed high acceptance of the proposed collaborative approach for concept extraction and classification.},
  keywords={Measurement;Embedded systems;Large language models;Pipelines;Collaboration;Knowledge graphs;Ontologies;Stakeholders;Field programmable gate arrays;Recommender systems;Higher education;Knowledge graph (KG);Large language models (LLM);curriculum model;domain model;Recommender systems},
  doi={10.1109/EDUCON62633.2025.11016377},
  ISSN={2165-9567},
  month={April},}@INPROCEEDINGS{11011971,
  author={Garzo, Grazia and Palumbo, Alessandro},
  booktitle={2025 13th International Symposium on Digital Forensics and Security (ISDFS)}, 
  title={Human-in-the-Loop: Legal Knowledge Formalization in Attempto Controlled English}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Automating legal knowledge through the use of computational languages presents a series of challenges, particularly in translating complex normative texts into formalized representations that are semantically faithful and computationally valid. A salient concern is reconciling the ambiguity inherent in natural legal language with the syntactic constraints of controlled languages. This work proposes a human-in-the-loop methodology for formalizing normative texts in the controlled language Attempto Controlled English (ACE), centered on the interaction between legal experts and computational constraints. Specifically, key legal provisions and fundamental case-law principles were translated into ACE. The findings reveal that even ostensibly straightforward legal provisions necessitate a substantial decomposition, abstraction, and adaptation process. The analysis also identified recurrent linguistic patterns (modal, temporal, and conditional) and reusable translation strategies.},
  keywords={Translation;Systematics;Law;Semantics;Europe;Syntactics;Ontologies;Human in the loop;Cognition;Security;Computable Law;Human-Machine Interaction;Controlled Language;Attempto Controlled English},
  doi={10.1109/ISDFS65363.2025.11011971},
  ISSN={2768-1831},
  month={April},}@INPROCEEDINGS{11014969,
  author={Friedenberger, Dirk and Pirl, Lukas and Boockmeyer, Arne and Schmid, Robert and Polze, Andreas},
  booktitle={2025 IEEE 22nd International Conference on Software Architecture Companion (ICSA-C)}, 
  title={A Train Dispatcher in the Cloud Generated from RDF Models}, 
  year={2025},
  volume={},
  number={},
  pages={111-119},
  abstract={In the context of fast systems and software development, combining model-based systems engineering with automated code generation is essential for managing complexity while enhancing efficiency and adaptability. This paper presents a model-based approach, where a composite RDF model is created, which serves as a flexible basis for the subsequent generation of various artifacts. The artifacts include not only source code but also technical configuration files (e.g. Dockerfiles, Kubernetes objects), CI/CD pipeline configurations, and documentation. The approach has been successfully applied to the Train Dispatcher in the Cloud (ZLiC), a cloud-based approach to digitalize the German Zugleitbetrieb. An iterative development process enabled continuous system expansion and adaptation to specific project requirements. The generated prototype has been validated through simulations and field tests, confirming the robustness and practical applicability of the approach.},
  keywords={Adaptation models;Codes;Source coding;Pipelines;Prototypes;Documentation;Computer architecture;Resource description framework;Iterative methods;Modeling;Model-based systems engineering;ontology-based systems engineering;code generation;railway},
  doi={10.1109/ICSA-C65153.2025.00023},
  ISSN={2768-4288},
  month={March},}@INPROCEEDINGS{11014868,
  author={Davila-Andino, Arturo J. and Huang, Edward and Zaidi, Abbas K.},
  booktitle={2025 IEEE International systems Conference (SysCon)}, 
  title={Identifying Vagueness in Model-Based Systems Engineering Theory Through a Materialist Ontology}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Model-Based Systems Engineering (MBSE) has had inconsistent implementation across industry and academia. This inconsistency is caused by different definitions of the term “system” have led to theoretical vagueness and contradictions that undermine the effectiveness of MBSE practices including metamodeling. By grounding MBSE in materialist philosophy, we aim to analyze vagueness in current definitions of “system.” We analyze the current definitions by using semantic analysis of decomposing their reference classes and intensions. Where reference classes determine the set of objects that a theory refers to, and intensions are the logical decomposition of a statement as it is written. Our findings reveal that the vagueness in current systems engineering theories is mostly due to the inability to resolve the ontological status of information. That is, current definitions do not clearly state how information interacts in the material world. Moreover, we have found internal inconsistencies and contradictions in current foundational theories, particularly with regard to emergence, interactions, and properties in both real and conceptual systems. We conclude that current systems engineering theories are not suitable for scientific practice. Moreover, we argue that adopting a materialist stance provides a precise ontological basis for defining systems, thereby reducing inconsistencies and enhancing the rigor of MBSE theories.},
  keywords={Industries;Philosophical considerations;Grounding;Semantics;Metamodeling;Ontologies;Systems engineering and theory;Mathematical models;Modeling;System analysis and design;Model-Based Systems Engineering (MBSE);Systems Engineering;Systems Theory;Ontology;Semantic Analysis},
  doi={10.1109/SysCon64521.2025.11014868},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{11014844,
  author={Xu, Tianxiao and Moalla, Néjib and Bentaha, Mohand-Lounes and Aktekin, Hazal and Agostinelli, Claudia},
  booktitle={2025 IEEE International systems Conference (SysCon)}, 
  title={A MBSE-Enhanced Semantically Integration Method for Populating MDAO Design Processes}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={With the development of the automotive market, the complexity of product portfolios is significantly increasing, while the frequency of market requirement changes is also rising. In this context, automotive OEMs face significant challenges in exploring feasible solutions for the next generation of vehicle development. Model-Based Systems Engineering (MBSE) effectively addresses these challenges by managing product complexity and establishing connections between requirements and solutions. However, an alternative solution often necessitates early verification & validation (V&V) and trade-off analysis across multiple disciplines. Multidisciplinary Design Analysis and Optimization (MDAO) has already been applied in solving such multidisciplinary challenges. MBSE is an effective approach for demonstrating multidisciplinary coupling relationships needed to meet specific requirements. By integrating MBSE with MDAO, it is possible to realize traceability from MDAO elements and results to the corresponding system elements and requirements, and to keep data consistency. This paper discusses a method of semantically linking MBSE and MDAO by establishing a MDAO domainspecific metamodel as a SysML profile. This approach positions MDAO as a viewpoint for vehicle development. MBSE serves as the overarching framework throughout different lifecycle stages, ensuring the consistency of MDAO specification. An instantiation of MDAO metamodel has been established to present a coupled multidisciplinary problem structure.},
  keywords={Hands;Couplings;Analytical models;Semantics;Data models;Complexity theory;Modeling;Optimization;Next generation networking;Automotive engineering;Model-Based Systems Engineering (MBSE);Multidisciplinary Design Analysis and Optimization (MDAO);Metamodel;Early verification & validation (V&V);SysML},
  doi={10.1109/SysCon64521.2025.11014844},
  ISSN={2472-9647},
  month={April},}@ARTICLE{11008609,
  author={Donald, Andy and Galanopoulos, Apostolos and Curry, Edward and Muñoz, Emir and Ullah, Ihsan and Waskow, M. A. and Kalra, Manan and Saxena, Sagar and Iqbal, Talha},
  journal={IEEE Access}, 
  title={A Semantic Approach for Linked Model, Data, and Dataspace Cards}, 
  year={2025},
  volume={13},
  number={},
  pages={110194-110207},
  abstract={In artificial intelligence, the significance of thorough documentation of models and datasets for publication is underestimated. However, due to the rising trend in the explainability and fairness of AI models, frameworks like Model Cards, Service Cards and Data Cards have emerged to facilitate understanding and reusing those models and datasets. Moreover, the Dataspace concept integrates these resources into Dataspace Cards, a comprehensive framework that systematically captures and organises crucial information to guide model and data selection for a specific application. This paper advocates a Semantic Web approach for transforming Model/Data Cards into Linked Data or knowledge graphs within a Dataspace, rendering them machine-readable and interoperable. A significant contribution is the development of a vocabulary that unifies Data, Model and Dataspace Card ontologies, enhancing consistent documentation and understanding of the Dataspace design. The paper further demonstrates the applicability of the proposed schema in various use cases, including bias detection in BERT-base-uncased and Large Language Models. Additionally, we propose a conceptual semantic approach, examined in-depth for sentiment and emotion analysis, to highlight how extended Dataspace Cards can improve applicability and outcomes. We found that this unified, ontology-driven approach results in more consistent metadata linking and more fine-grained bias detection in BERT-based-uncased than standalone documentation tools relying solely on Model or Data Cards. Furthermore, compared to existing frameworks, the richer interlinking capabilities of our proposed Dataspace Cards also facilitated easier traceability of performance outcomes, thereby ultimately fostering higher trustworthiness and reusability of AI resources.},
  keywords={Data models;Artificial intelligence;Documentation;Semantics;Adaptation models;Linked data;Europe;Training;Semantic Web;Vocabulary;Semantic web;AI documentation;data cards;model cards;service cards;dataspace cards},
  doi={10.1109/ACCESS.2025.3572211},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11004934,
  author={Almoqren, Nuha and Alrashoud, Mubarak},
  booktitle={2024 International Conference on IT Innovation and Knowledge Discovery (ITIKD)}, 
  title={A Smart Framework for Optimizing User Feedback Prioritization in Application Development}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={In mobile app development, user reviews are a significant source of requirements. Users frequently report bugs, request new features, or suggest enhancements. Mobile app vendors aim to maximize user satisfaction by addressing these continuous comments and requests as early as possible. Typically, they prioritize delivering the most promising features, extracted from user reviews, in early releases while deferring fewer promising ones to later stages. However, due to the massive volume of reviews, redundancy, and conflicts among them, manually extracting requirements is inefficient and often challenging, making requirement prioritization even more difficult. Therefore, automating this process is essential. This paper presents a conceptual framework for the requirements prioritization process's automation, continuity, and scalability. The proposed framework follows a hybrid approach that integrates multiple advanced techniques: generative artificial intelligence, active learning, ontologies, and optimization algorithms. Generative artificial intelligence enables the identification of important patterns and the automatic extraction of requirements and their properties, which aids in assessing properties to determine requirement priorities. The generative artificial intelligence framework integrates with an active learning system to improve annotation efficiency. Ontologies help comprehend relationships, properties, and dependencies among requirements, aligning them with domain-specific knowledge. Optimization methods playa crucial role in the requirements prioritization process by computing the weights of various properties to identify the most effective combination of requirements and determine the optimal order for implementation. Consequently, this research presents a smart theoretical framework for enhancing user-driven maintenance and development of mobile applications. Researchers are tackling several critical challenges that remain unresolved in the field, with future directions focusing on evaluating its applicability and effectiveness in real-world scenarios.},
  keywords={Technological innovation;Reviews;Generative AI;Active learning;Ontologies;Feature extraction;Software;Mobile applications;Software measurement;Software development management;Software Requirements;App Reviews;Generative AI;Optimization;Mobile Applications;Smart Framework},
  doi={10.1109/ITIKD63574.2025.11004934},
  ISSN={},
  month={April},}@ARTICLE{10981743,
  author={Ouriques, Leandro and Barbosa, Carlos Eduardo and Kritz, Joshua and Xexéo, Geraldo},
  journal={IEEE Access}, 
  title={Toward an Ontology of Wargame Design}, 
  year={2025},
  volume={13},
  number={},
  pages={78928-78958},
  abstract={Governments rely on military power to address conflicts, manage crises, and adapt to the evolving nature of modern warfare, which is often characterized by uncertainty and disorder. Wargames, traditionally military tools for simulating conflicts and decision-making, have gained prominence in civilian applications, including business, cybersecurity, disaster management, and critical infrastructure protection. Despite their utility, designing wargames is a time-intensive process with significant challenges, such as scenario creation and decision modeling, necessitating structured and systematic approaches. This research formalizes wargame design through ontology-driven conceptual modeling, structuring its key concepts, characteristics, and elements. Ontologies provide a structured representation of knowledge, facilitating communication, knowledge management, and collaborative design. As a result, we developed core ontologies for wargame design based on the Unified Foundational Ontology (UFO) and implemented them using OntoUML. Our innovation lies in analyzing wargame design processes across various countries and military organizations to develop a comprehensive reference model for wargame design. Additionally, we are the first to apply UFO for conceptual modeling of the wargame domain. These ontologies enhance wargame design by fostering standardization, adaptability, and support for intelligent systems, enabling dynamic and responsive scenarios. These contributions enhance wargame design efficiency and effectiveness, applicable to military and civilian contexts.},
  keywords={Ontologies;Games;Decision making;Adaptation models;Uncertainty;Training;Systematics;Planning;Personnel;Object recognition;Ontology;ontology-driven conceptual modeling (ODCM);OntoUML;unified foundational ontology (UFO);wargame;wargame design;wargaming},
  doi={10.1109/ACCESS.2025.3566249},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10974109,
  author={Cooper, Sara and Ros, Raquel and Lemaignan, Séverin and Gebellí, Ferran and Ferrini, Lorenzo and Juričić, Luka},
  booktitle={2025 20th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
  title={Demonstration of an Open-Source ROS 2 Framework and Simulator for Situated Interactive Social Robots}, 
  year={2025},
  volume={},
  number={},
  pages={1770-1772},
  abstract={We introduce an open-source ROS 2 architecture for situated social robots, along with a simulator that allows mixed-reality development and interactions. The architecture is a hybrid symbolic/subsymbolic system that integrates explicit ontology semantics for perception, reasoning, and execution, with LLMs. It features multimodal social perception by leveraging the open source ROS4HRI framework; LLMs (both edge- and cloud-based) to facilitate natural language interaction between the user and system; KnowledgeCore, an open-source knowledge base, to reason about facts in the world; and an intent-based controller to supervise the execution of parallel/sequential tasks and skills. We demonstrate our system architecture with a social robot running the mixed-reality system.},
  keywords={Social robots;Semantics;Natural languages;Knowledge based systems;Systems architecture;Virtual reality;Ontologies;Cognition;Situated social robots;ROS 2 framework;mixed-reality simulator},
  doi={10.1109/HRI61500.2025.10974109},
  ISSN={},
  month={March},}@ARTICLE{10971367,
  author={Kim, Seungyeon and Kim, Donghyun and Hwang, Seokju and Lee, Kyong-Ho and Lee, Kyunghwa},
  journal={IEEE Access}, 
  title={LLM-Assisted Ontology Restriction Verification With Clustering-Based Description Generation}, 
  year={2025},
  volume={13},
  number={},
  pages={73603-73618},
  abstract={An ontology is a scheme for structuring relationships between concepts in a domain, promoting data interoperability and system integration. However, poorly designed ontologies can lead to errors and performance issues. While systems engineering has standardized evaluation guidelines (e.g., ISO/IEC), ontology engineering lacks such standards, leading to various independent evaluation methods. One frequent issue among novice developers is the misuse of ontology restrictions, particularly ‘allValuesFrom’ and ‘someValuesFrom’, which can significantly impact the correctness and reliability of ontologies. However, existing studies have not adequately addressed effective methods for detecting such errors. To address this gap, we propose a context-aware verification framework utilizing large language models to detect and correct misuse in ontology restrictions. Unlike conventional methods, our framework integrates contextual descriptions derived from ontological axioms, enabling more accurate verification. Additionally, we introduce a clustering-based description generation method that systematically organizes contextual information, further enhancing verification accuracy. Experimental evaluation conducted on diverse ontology datasets suggests that contextual integration improves verification performance. Moreover, the clustering-based description generation improves restriction misuse detection and correction compared to traditional approaches. By automating ontology restriction verification, this study contributes significantly to enhancing the reliability of ontology evaluation and provides a foundation for developing more scalable and standardized verification techniques.},
  keywords={Ontologies;Accuracy;Reliability;ISO Standards;IEC Standards;Translation;Software;Semantics;Scalability;Quality assessment;Ontology evaluation;ontology restriction verification;text generation;clustering},
  doi={10.1109/ACCESS.2025.3562560},
  ISSN={2169-3536},
  month={},}@ARTICLE{10962554,
  author={Chen, Jiaoyan and Mashkova, Olga and Zhapa-Camacho, Fernando and Hoehndorf, Robert and He, Yuan and Horrocks, Ian},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Ontology Embedding: A Survey of Methods, Applications and Resources}, 
  year={2025},
  volume={37},
  number={7},
  pages={4193-4212},
  abstract={Ontologies are widely used for representing domain knowledge and meta data, playing an increasingly important role in Information Systems, the Semantic Web, Bioinformatics and many other domains. However, logical reasoning that ontologies can directly support are quite limited in learning, approximation and prediction. One straightforward solution is to integrate statistical analysis and machine learning. To this end, automatically learning vector representation for knowledge of an ontology i.e., ontology embedding has been widely investigated. Numerous papers have been published on ontology embedding, but a lack of systematic reviews hinders researchers from gaining a comprehensive understanding of this field. To bridge this gap, we write this survey paper, which first introduces different kinds of semantics of ontologies and formally defines ontology embedding as well as its property of faithfulness. Based on this, it systematically categorizes and analyses a relatively complete set of over 80 papers, according to the ontologies they aim at and their technical solutions including geometric modeling, sequence modeling and graph propagation. This survey also introduces the applications of ontology embedding in ontology engineering, machine learning augmentation and life sciences, presents a new library mOWL and discusses the challenges and future directions.},
  keywords={Ontologies;Resource description framework;OWL;Semantics;Vectors;Machine learning;Surveys;Knowledge engineering;Logic;Cognition;Ontology;ontology embedding;web ontology language;knowledge graph;representation learning},
  doi={10.1109/TKDE.2025.3559023},
  ISSN={1558-2191},
  month={July},}@ARTICLE{10962207,
  author={Mishra, Pratham and Narayanasamy, Senthil Kumar and Srinivasan, Kathiravan},
  journal={IEEE Access}, 
  title={Context-Aware Embedded Language Transformers for Evaluating Climate Change-Based Sustainable Development Goals}, 
  year={2025},
  volume={13},
  number={},
  pages={65757-65775},
  abstract={This research work addresses the pressing issue of climate change and the urgent need for a comprehensive and reliable dataset that can assist stakeholders, policymakers and researchers in making informed and data-driven decisions. We propose the process for leveraging publicly available raw data such as news articles and social media posts and help to create meaningful ontologies. The study demonstrates the use of advanced Natural Language Processing and Deep Learning techniques to transform raw data into potential insights. Additionally, we present a methodology for utilizing these generated ontologies to anticipate the impacts of decisions on climate change. The primary objective of this research is to present a Context-Aware Embedded Language Transformers model that can be easily integrated into various pipelines by generating meaningful ontologies to support more informed decision-making. These ontologies will serve as knowledge graphs and contribute to a large dataset for future research in the field and address the current lack of comprehensive resources.},
  keywords={Climate change;Meteorology;Sustainable development;Ontologies;Transformers;Social networking (online);Decision making;Data models;Databases;Surveys;Climate change;graph convolution networks (GCNs);knowledge graphs;natural language processing (NLP);sustainable development goals;transformers},
  doi={10.1109/ACCESS.2025.3559548},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10947388,
  author={Coffelt, Jeremy Paul and Kampmann, Peter and Beetz, Michael},
  booktitle={2025 IEEE Underwater Technology (UT)}, 
  title={Implementation and Application of a Knowledge Service for AUV Mission Explainability}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={This paper presents a knowledge service aimed at enhancing mission explainability in subsea robotics operations. The proposed system consists of an AI agent that chains together specialized large language models (LLMs) and a graph database to enable natural language querying and interactive visualization. The graph database models entities and relationships relevant to subsea inspection and maintenance, such as clients, industries, sites, vehicles, sensors, and underwater scene elements, including pipeline components and seafloor characteristics. The browser-based GUI aims to allow stakeholders—including field teams, robot developers, industry clients, and regulatory agencies—to intuitively interact with mission data, supporting post-mission analysis and explainability. Using pipeline inspections as a case study, we illustrate the potential of this approach and discuss future developments needed to advance this framework toward a practical solution for subsea robotics.},
  keywords={Industries;Service robots;Large language models;Pipelines;Natural languages;Sensor phenomena and characterization;Inspection;Robot sensing systems;Visual databases;Underwater technology;AI agent;graph database;subsea robotics;knowledge representation and reasoning;mission explainability},
  doi={10.1109/UT61067.2025.10947388},
  ISSN={},
  month={March},}@INPROCEEDINGS{10943742,
  author={Singh, Chandan Kumar and Kumar, Devesh and Sanap, Vipul and Sinha, Rajesh},
  booktitle={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={LLM-RSPF: Large Language Model-Based Robotic System Planning Framework for Domain Specific Use-cases}, 
  year={2025},
  volume={},
  number={},
  pages={7277-7286},
  abstract={The employment of large language models (LLMs) for task planning and reasoning has emerged as a focal point of interest within the robotics research community. However, directly applying LLMs, even with large token-sized prompts, does not achieve the task planning performance required for an industrial-grade domain-specific use-case (DSU). This work aims to overcome the obstacles of a robotic task planner for DSUs by introducing a novel planning framework, LLM-RSPF (Large Language Model-based Robotic System Planning Framework). Central to the LLM-RSPF is a novel robotic system ontology that organizes the components of the robotic system in a coherent and a systematic manner. The ontology empowers the LLM-RSP F to efficiently capture a contextual representation of the DSU using the LLMs. Subsequently, the research introduces a LLM-tuning regimen referred as chain of hierarchical thought (CoHT), specifically crafted to complement the proposed system ontology. Integrating these two components, the LLM-RSPF aims to enhance the accuracy, robustness, and throughput of a robotic system in a cost-effective manner. In addition, the research presents an empirical methodology to generate the LLM-tuning dataset size for a guaranteed performance. The LLM-RSPF is validated on a retail order-fulfillment use-case thereby, illustrating the efficacy of the framework. Through rigorous evaluation, the LLM-RSPF demonstrates exceptional performance on the generated dataset, effectively meeting the DSU objectives.},
  keywords={Solid modeling;Accuracy;Three-dimensional displays;Systematics;Service robots;Ontologies;Throughput;Robustness;Planning;Robots;coht;domain-specific use-case;large language model;robotic system ontology;task planning},
  doi={10.1109/WACV61041.2025.00707},
  ISSN={2642-9381},
  month={Feb},}@INPROCEEDINGS{10935139,
  author={Abolhasani, Mohammad Sadeq and Pan, Rong},
  booktitle={2025 Annual Reliability and Maintainability Symposium (RAMS)}, 
  title={OntoKGen: A Genuine Ontology and Knowledge Graph Generator Using Large Language Model}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Extracting relevant and structured knowledge from large, complex technical documents within the Reliability and Maintainability (RAM) domain is labor-intensive and prone to errors. Our work addresses this challenge by presenting OntoKGen, a Genuine pipeline for Ontology extraction and Knowledge Graph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through an interactive user interface guided by our adaptive iterative Chain of Thought (CoT) algorithm to ensure that the ontology extraction process and, thus, KG generation align with user-specific requirements. Although KG generation follows a clear, structured path based on the confirmed ontology, there is no universally correct ontology as it is inherently based on the user's preferences. OntoKGen recommends an ontology grounded in best practices, minimizing user effort and providing valuable insights that may have been overlooked, all while giving the user complete control over the final ontology. Having generated the KG based on the confirmed ontology, OntoKGen enables seamless integration into schemeless, non-relational databases like Neo4j. This integration allows for flexible storage and retrieval of knowledge from diverse, unstructured sources, facilitating advanced querying, analysis, and decision-making. Moreover, the generated KG serves as a robust foundation for future integration into Retrieval-Augmented Generation (RAG) systems, offering enhanced capabilities for developing domain-specific intelligent applications.},
  keywords={Large language models;Retrieval augmented generation;Pipelines;Random access memory;Knowledge graphs;Ontologies;User interfaces;Reliability engineering;Generators;Prompt engineering;Large Language Model;Ontology and Knowledge Graph Generator;Prompt Engineering;Neo4J},
  doi={10.1109/RAMS48127.2025.10935139},
  ISSN={2577-0993},
  month={Jan},}@INPROCEEDINGS{10937875,
  author={Khalil, Ibrahim and Ahmad, Israr and Rasheed, Uzair and Butt, Wasi Haider and Anwaar, Zaeem},
  booktitle={2025 6th International Conference on Advancements in Computational Sciences (ICACS)}, 
  title={Detecting Cross-Domain Ambiguity In Requirements Through Natural Language Processing, A Systematic Literature Review}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={A strong foundation for a project is the first step to measuring or estimating the success parameters of a project. Requirement engineers gather and elicit the requirements of the project in detail after the feasibility study. A project’s success can be measured/estimated if and only if the initially collected requirements are clear, unambiguous, and well-understood. Similarly, ambiguous or not understandable requirements can lead to failure or closure of the project in disastrous form. An initial step in requirement elicitation is usually gathering requirements in natural language. This study analyzes different tools, techniques, and approaches in different research articles used for detecting ambiguities in requirements in the natural language. We identified 33 research articles, published during 2012-24 through a Systematic Literature Review. For automated ambiguities detection in requirements engineering, we identified 17 tools & techniques and 12 NLP approaches. We analyzed the better approach for cross-domain ambiguity detection. SGNS (Skipgram Negative Sampling) variant of Word2Vec for the cross-domain ambiguity detection in the review of the study, resulted to be the most efficient technique implemented with the usage view and easily scalable for multiple requirements and domains.},
  keywords={Scalability;Software algorithms;Natural language processing;Software;Requirements engineering;Systematic literature review;requirement’s engineering;natural language processing (nlp);ambiguity detection in natural language requirements;cross-domain ambiguity},
  doi={10.1109/ICACS64902.2025.10937875},
  ISSN={2616-3330},
  month={Feb},}@INPROCEEDINGS{10932144,
  author={Naik, Anushka and Naik, Akanksha and Deshmukh, Pratiksha},
  booktitle={2025 1st International Conference on AIML-Applications for Engineering & Technology (ICAET)}, 
  title={ProtGAT: Refining Antimicrobial Resistance Detection with Graph-Based Deep Learning*}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The rise of antimicrobial resistance (AMR) is a critical threat to global health, necessitating urgent advancements in diagnostic capabilities. Current methods for detecting AMR are hampered by slow processing times and a lack of precision, making early and accurate detection challenging. This paper introduces ProtGAT, a novel computational framework that combines ProteinBERT and Graph Attention Networks (GAT) to enhance the detection of AMR. Our model leverages deep learning to analyze complex protein sequences and their interactions, improving the accuracy of AMR predictions. The integration of ProteinBERT provides a robust feature extraction from protein sequences, while GAT focuses on identifying key relational features within these sequences. Early testing of ProtGAT demonstrates its superior performance over traditional models, particularly in identifying novel AMR sequences that evade conventional detection methods. This study highlights the potential of advanced computational approaches to revolutionize AMR diagnostics, offering faster and more reliable tools for healthcare providers in combating this growing public health concern.},
  keywords={Deep learning;Accuracy;Computational modeling;Refining;Feature extraction;Protein sequence;Reliability;Public healthcare;Immune system;Testing;ProteinBERT;Graph Attention Networks;Antimicrobial Resistance;Protein Sequence Analysis;Computational Biology},
  doi={10.1109/ICAET63349.2025.10932144},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10932270,
  author={Desai, Arun and Kulkarni, Anagha},
  booktitle={2025 1st International Conference on AIML-Applications for Engineering & Technology (ICAET)}, 
  title={Unleashing the Potential of Ontology in Skill Extraction}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Research reveals ontology's ability to extract information about skills from online job sites by giving a structured and semantically rich representation of skills. The study talks about more accurate and thorough skill profiling by systematically building ontological models that allow for an indepth knowledge of the complex links between skills, abilities, and domains. In this research, we study papers from different methods like supervised learning, unsupervised learning, and LLMs. The paper begins by providing not only an overview what is new in ontology generation but also its application in the context of skill extraction. It then delves into the challenges and opportunities associated with ontology-based skill extraction, highlighting the ways of processing natural language in bridging the gap between unstructured text and the formal representation of skills and competencies. Furthermore, the paper presents a comprehensive framework for ontology-driven skill extraction, emphasizing the importance of contextual awareness and the identification of implicit skills that may not be explicitly stated in the source text. The potential implications of this approach are manifold, as it could significantly impact various aspects of the talent management ecosystem.},
  keywords={Manifolds;Accuracy;Biological system modeling;Supervised learning;Natural languages;Ecosystems;Machine learning;Ontologies;Data mining;Unsupervised learning;Ontology;skill extraction;skills;Jobs;Machine Learning},
  doi={10.1109/ICAET63349.2025.10932270},
  ISSN={},
  month={Jan},}@ARTICLE{10922135,
  author={Xiahou, Xiaer and Chen, Gaotong and Li, Zirui and Xu, Xin and Li, Qiming},
  journal={IEEE Transactions on Engineering Management}, 
  title={Knowledge Management in Construction Quality Management: Current State, Challenges, and Future Directions}, 
  year={2025},
  volume={72},
  number={},
  pages={1069-1088},
  abstract={Construction quality management (CQM), as one of the major activities in construction project management, relies heavily on knowledge. Unfortunately, the knowledge of CQM is diverse in format and scattered in different stakeholders within the whole construction processes. Therefore, knowledge management (KM) of CQM is underinvestigated. To offering a comprehensive view of KM in CQM, this article employed a mixed review method to critically review 87 related articles. The results indicate 1) building information modeling, ontology, and natural language processing are identified as critical technologies in KM, 2) expert system and decision support, structural health monitoring, and project management are the major application domains. This article conducts an in-depth analysis of the literature based on the three phases of quality control: pre-construction, in-construction, and post-construction. The results are discussed to critically assess the critical technologies in KM. A framework is proposed to guide the effective implementation of KM in CQM, alongside a discussion of the current challenges and opportunities. The article further identifies potential development directions for KM in CQM, including total quality management, digital twins, development of large language models, construction of “No-cost” KM platforms, uniform evaluation and standardization mechanisms, tacit knowledge capture, and confidentiality and security. A novel paradigm for knowledge-driven quality management decision-making is first introduced. This article offers a comprehensive perspective on the application of KM in CQM, which will significantly enhance the effectiveness of CQM implementation in the future.},
  keywords={Quality management;Reviews;Industries;Engineering management;Training;Data mining;Costs;Stakeholders;Complexity theory;Total quality management;Construction quality management (CQM);knowledge management (KM);knowledge-driven quality management;mixed review;quality control},
  doi={10.1109/TEM.2025.3550354},
  ISSN={1558-0040},
  month={},}@ARTICLE{10904297,
  author={Hosseini, Ali M. and Kastner, Wolfgang and Sauter, Thilo},
  journal={IEEE Open Journal of the Industrial Electronics Society}, 
  title={Leveraging LLMs and Knowledge Graphs to Design Secure Automation Systems}, 
  year={2025},
  volume={6},
  number={},
  pages={380-395},
  abstract={The digital transformation of Industrial Control Systems (ICSs) within the Industry 4.0 paradigm is essential for industrial organizations to remain competitive, while cybersecurity is an enabler. However, security measures, often implemented late in the engineering process, lead to costly and complicated implementations. Thus, this article is concerned with the “security by design” principle in ICSs and facilitates compliance with ICS security standards, which can be legally mandated for some critical systems or adopted by asset owners to protect their assets. Current methods for compliance demand manual efforts from security experts, making the compliance process time-consuming and costly. To address this, we propose a framework for leveraging large language models (LLMs) combined with knowledge graphs to automate the interpretation of security requirements and system architecture as two main elements of the design phase. Our knowledge graph-augmented LLM framework converts system architectures into human natural language, enhancing the automation of various security analyses, especially those that need to handle textual requirements. The framework enables validating applicable security requirements provided by IEC 62443-3-3 (a widely-used ICS security standard) concerning system designs through a question-and-answer interface. To evaluate the framework, various questions with reference responses from human experts were prepared in the context of a use case, and the quality of the LLMs' responses was measured across various metrics. Moreover, we compared the framework with a baseline approach based on formal queries. The results show that the proposed framework effectively automates security tasks and offers a user-friendly interface accessible to nonexperts.},
  keywords={Security;Ontologies;Knowledge graphs;Computer security;IEC Standards;Systems architecture;Cyberattack;Cognition;Natural languages;Large language models;Industrial control system (ICS);security by design;knowledge graph;large language model (LLM);ontology},
  doi={10.1109/OJIES.2025.3545811},
  ISSN={2644-1284},
  month={},}@ARTICLE{10891880,
  author={Yang, Yang and Shen, Wei and Shu, Junfeng and Liu, Yinan and Curry, Edward and Li, Guoliang},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={CMVC+: A Multi-View Clustering Framework for Open Knowledge Base Canonicalization Via Contrastive Learning}, 
  year={2025},
  volume={37},
  number={5},
  pages={2296-2310},
  abstract={Open information extraction (OIE) methods extract plenty of OIE triples $< $<noun phrase, relation phrase, noun phrase$> $> from unstructured text, which compose large open knowledge bases (OKBs). Noun phrases and relation phrases in such OKBs are not canonicalized, which leads to scattered and redundant facts. It is found that two views of knowledge (i.e., a fact view based on the fact triple and a context view based on the fact triple's source context) provide complementary information that is vital to the task of OKB canonicalization, which clusters synonymous noun phrases and relation phrases into the same group and assigns them unique identifiers. In order to leverage these two views of knowledge jointly, we propose CMVC+, a novel unsupervised framework for canonicalizing OKBs without the need for manually annotated labels. Specifically, we propose a multi-view CHF K-Means clustering algorithm to mutually reinforce the clustering of view-specific embeddings learned from each view by considering the clustering quality in a fine-grained manner. Furthermore, we propose a novel contrastive learning module to refine the learned view-specific embeddings and further enhance the canonicalization performance. We demonstrate the superiority of our framework through extensive experiments on multiple real-world OKB data sets against state-of-the-art methods.},
  keywords={Contrastive learning;Clustering algorithms;Knowledge based systems;Organizations;Electronic mail;Data mining;Ontologies;Information retrieval;Indexes;Training;Open knowledge base canonicalization;multi-view clustering;contrastive learning},
  doi={10.1109/TKDE.2025.3543423},
  ISSN={1558-2191},
  month={May},}@ARTICLE{10887205,
  author={Huang, Wenhao and Zhang, Jiahao and Li, Xin and Zhou, Xiao and Qi, Deyu and Xi, Jianqing and Liu, Wenjun},
  journal={IEEE Access}, 
  title={A Semantic and Intelligent Focused Crawler based on BERT Semantic Vector Space Model and Hybrid Algorithm (October 2024)}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={The goal of a focused crawler is to selectively fetch pages that are relevant to a given topic. Previous crawlers use text content to determine text topic relevance and manually determined weighting factors to predict the priority of unvisited URLs. However, there are still some problems in the above focused crawler methods, the calculation formula of semantic similarity between words is flawed. The weighting factor for the priority of unvisited URLs is determined arbitrarily. In order to solve the above problems, this paper proposes a semantic and intelligent focused crawler based on BERT semantic vector space model and hybrid algorithm. This method used BERT semantic vector space model to calculate the topic relevance of documents, and used a hybrid algorithm to optimize the weighting factor of unvisited URL priority. The experimental results show that the proposed BSVSM-HA crawler can obtain better evaluation indicators compared with the other three crawlers including Word2vec crawler, ELMO crawler and BSVSM crawler. In conclusion, the semantic and intelligent crawler proposed in this paper makes the semantic similarity between terms more accurate, and improves the topic relevance of the text, and the optimized weighting factor makes the priority evaluation of unvisited URLs more accurate.},
  keywords={Crawlers;Semantics;Vectors;Web pages;Uniform resource locators;Hypertext systems;Accuracy;Ontologies;Training;Search problems;Focused Crawler;Semantic Vector Space Model;Hybrid Algorithm},
  doi={10.1109/ACCESS.2025.3542064},
  ISSN={2169-3536},
  month={},}@ARTICLE{10870246,
  author={Kim, Han Kyul and Park, Yujin and Kim, Yoon Ji and Yi, Seungah and Park, Yeju and So, Sujin and Lee, Hyeon-Ji and Bae, Ye Seul},
  journal={IEEE Access}, 
  title={EILEEN: A Multi-Modal Framework for Extracting Alcohol Consumption Patterns From Bilingual Clinical Notes}, 
  year={2025},
  volume={13},
  number={},
  pages={25741-25751},
  abstract={In this work, we introduce EILEEN (Efficient Inference for Language-based Extraction of EHR Notes), a novel multi-modal natural language processing (NLP) framework designed to extract various alcohol consumption patterns from unstructured clinical notes, particularly in bilingual and non-English contexts. Recent advances in NLP have significantly improved information extraction capability across various domains. However, identifying patterns of alcohol consumption in medical documents remains underexplored, with existing approaches heavily relying on traditional NLP methods such as bag-of-words models that require extensive text preprocessing. These methods are often limited to English-language clinical settings, where robust medical ontologies and NLP toolkits are available to support preprocessing tasks. Therefore, this limitation hinders their use in multilingual healthcare settings and in environments lacking robust NLP toolkits to facilitate preprocessing. Motivated by the need for a more generalizable and accurate approach, this paper investigates the impact of large language models (LLMs) in advancing alcohol consumption pattern extraction from clinical notes. By reducing the need for manual preprocessing and improving adaptability to multilingual clinical notes, this work aims to enable broader, more practical applications of NLP models in extracting alcohol consumption patterns from clinical notes. By fine-tuning multilingual language models along with additional data sources, EILEEN effectively analyzes unstructured electronic health records (EHR) without relying on traditional concept normalization or extensive text preprocessing resources. Furthermore, the multi-modal component of EILEEN enables it to integrate and leverage diverse types of alcohol-related information, such as various types and amounts of alcohol consumed by a patient, thereby improving its pattern extraction accuracy. Our experiments, conducted in two different medical institutions in Korea, demonstrate that EILEEN significantly outperforms existing NLP methods in accurately identifying clinically relevant alcohol consumption patterns. By providing accurate, detailed, and clinically useful alcohol consumption patterns from unstructured clinical notes, EILEEN empowers healthcare practitioners with actionable insights essential for informed clinical decision-making.},
  keywords={Natural language processing;Multilingual;Accuracy;Data mining;Ontologies;Hospitals;Vectors;Unified modeling language;Transformers;Feature extraction;Clinical informatics;alcohol information extraction;natural language processing;multimodal learning;multilingual transformers},
  doi={10.1109/ACCESS.2025.3538803},
  ISSN={2169-3536},
  month={},}@ARTICLE{10858700,
  author={Yhdego, Tsegai O. and Wang, Hui},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Automated Ontology Generation for Zero-shot Defect Identification in Manufacturing}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={A lack of labeled data presents a significant challenge to automatic defect identification in manufacturing, which is a crucial step in process control and certification during process development. State-of-the-art transfer learning is incapable of handling such zero-shot learning (ZSL) when defect labels are absent in training datasets. The latest research on ZSL leverages natural language processing (NLP) based on large language models (LLM) and shows promise by supplementing information to generate labels. However, its performance is hampered by the supporting LLMs pre-trained on generic vocabulary that failed to characterize manufacturing defects accurately. This paper establishes a methodology to automatically extract multi-level attributes from literature to improve defect representation, thereby facilitating ZSL. The extracted attributes contribute to a hierarchical knowledge graph, called defect ontology, to characterize multiple aspects of manufacturing defects. The proposed algorithm takes the defect images and associated text from the literature as input and develops an unsupervised method to identify the hierarchical relationships among the tokenized information extracted from the input text-feature corpora. The hierarchical graph is refined to retain the most relevant information by a pruning algorithm based on a minimum path search. A walk algorithm, along with NLP, parsed the generated ontology to create embedding of defects to enable zero-shot attribute learning to identify defects. The proposed method advances the ZSL methodology by automatically creating a hierarchical knowledge representation from literature and images to replace generic vocabulary in LLM adopted by ZSL algorithms, thus improving defect representation. The case studies are among the earlier attempts to demonstrate the feasibility of using literature data from public sources to extract attributes automatically to identify defects in a real additive manufacturing process based on direct-ink-writing.},
  keywords={Manufacturing;Zero shot learning;Ontologies;Automation;Data mining;Vocabulary;Accuracy;Transfer learning;Certification;Process control;Zero-shot learning;defect identification;manufacturing automation;self-supervised learning},
  doi={10.1109/TASE.2025.3537463},
  ISSN={1558-3783},
  month={},}@ARTICLE{10841380,
  author={Wang, Wenkang and Shuai, Yunyan and Li, Yiming and Zeng, Min and Li, Min},
  journal={IEEE Transactions on Computational Biology and Bioinformatics}, 
  title={Enhancing Protein Function Prediction Through the Fusion of Multi-Type Biological Knowledge With Protein Language Model and Graph Neural Network}, 
  year={2025},
  volume={22},
  number={2},
  pages={581-590},
  abstract={Proteins play crucial roles in diverse biological functions. Accurately annotating their functions is essential for understanding cellular mechanisms and developing therapies for complex diseases. Computational methods have been proposed as alternatives to labor-intensive and expensive experimental approaches. Existing computational methods have demonstrated that protein evolution information and Protein-Protein Interactions (PPIs) are essential for protein function prediction. However, traditional computational approaches for generating evolution information are time-consuming. On the other hand, proteins lacking interactions are ignored in previous studies. To address these limitations, we propose a novel deep learning framework, named DeepFMB, which incorporates multi-type biological knowledge. DeepFMB leverages a pre-trained protein language model to extract evolution information. Moreover, DeepFMB generates PPI-related features and orthology-related features using graph neural networks on the constructed PPI and orthology networks. Then, these multi-type features are fused adaptively for protein function prediction. Compared to eight state-of-the-art methods, DeepFMB outperforms all of them in terms of F-max and AUPR. Additionally, with the combination of sequence similarity-based inference, our predicted model predicts protein functions more accurately. Experimental results also validate the superior performance of our methods in predicting low-frequency GO terms. Ablation studies demonstrate that the multi-type biological knowledge we use is highly relevant to protein functions.},
  keywords={Proteins;Feature extraction;Databases;Biological system modeling;Biological information theory;Predictive models;Protein sequence;Evolution (biology);Aggregates;Knowledge engineering;Protein function prediction;protein-protein interactions;orthology relations;pre-trained protein language model;graph neural network},
  doi={10.1109/TCBBIO.2025.3529301},
  ISSN={2998-4165},
  month={March},}@ARTICLE{10819318,
  author={Wu, Yanting and Sun, Yicheng and Wen, Xiaojian and Liu, Xiaoqiang and Bao, Jinsong and Wang, Sen},
  journal={IEEE Internet Computing}, 
  title={A Generative Modeling Method for Digital Twin Shop Floor}, 
  year={2025},
  volume={29},
  number={1},
  pages={24-31},
  abstract={Digital twin (DT) as a key enabling technology for achieving digitization, flexibility, and customization in shop floors has attracted significant attention. However, the shop floor involves diverse assets across multiple dimensions, scales, and interdisciplinary fields, making the modeling process complex. To address this issue, this article analyzes the construction process of ontology-based information models and proposes a generative modeling method for digital twin shop floor driven by large language models (LLMs). First, LLMs are utilized to analyze user intentions, acquiring the hierarchical object structure of DT models. Second, by combining an analysis–retrieval method to extract domain knowledge and generate dynamic prompts, LLMs are guided to realize the creation and fusion of objects and construct structured and semantically enriched DT models. Finally, the effectiveness of the proposed method is validated through examples of shop floor resource scheduling.},
  keywords={Data models;Object oriented modeling;Digital twins;Analytical models;Ontologies;Semantics;Context modeling;Computational modeling;Internet;Natural languages;Industrial facilities;Job shop scheduling},
  doi={10.1109/MIC.2024.3522301},
  ISSN={1941-0131},
  month={Jan},}@INBOOK{10766917,
  author={Cady, Field},
  booktitle={The Data Science Handbook}, 
  title={Traditional Natural Language Processing}, 
  year={2025},
  volume={},
  number={},
  pages={197-208},
  abstract={Summary <p>This chapter discusses techniques for natural language process, especially the ones that pre&#x2010;date large language models. It starts with bag&#x2010;of&#x2010;words &#x2013; the central concept for turning text into a numerical vector &#x2013; and covers increasingly sophisticated ways of using linguistic concepts (lemmatization, synsets, etc.) to improve the vectorization process. Classic problems like sentiment analysis and topic modeling, which you can do once the data is vectorized, are covered along with more advanced topics like syntax trees and ontologies.</p>},
  keywords={Natural language processing;Vectors;Training;Web pages;Training data;Tokenization;Libraries;Internet;Computer hacking;Turning},
  doi={10.1002/9781394234523.ch16},
  ISSN={},
  publisher={Wiley},
  isbn={9781394234516},
  url={https://ieeexplore.ieee.org/document/10766917},}@ARTICLE{10721368,
  author={Dalal, Sumit and Tilwani, Deepa and Gaur, Manas and Jain, Sarika and Shalin, Valerie L. and Sheth, Amit P.},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={A Cross Attention Approach to Diagnostic Explainability Using Clinical Practice Guidelines for Depression}, 
  year={2025},
  volume={29},
  number={2},
  pages={1333-1342},
  abstract={The lack of explainability in using relevant clinical knowledge hinders the adoption of artificial intelligence-powered analysis of unstructured clinical dialogue. A wealth of relevant, untapped Mental Health (MH) data is available in online communities, providing the opportunity to address the explainability problem with substantial potential impact as a screening tool for both online and offline applications. Inspired by how clinicians rely on their expertise when interacting with patients, we leverage relevant clinical knowledge to classify and explain depression-related data, reducing manual review time and engendering trust. We developed a method to enhance attention in contemporary transformer models and generate explanations for classifications that are understandable by mental health practitioners (MHPs) by incorporating external clinical knowledge. We propose a domain-general architecture called ProcesS knowledge-infused cross ATtention (PSAT) that incorporates clinical practice guidelines (CPG) when computing attention. We transform a CPG resource focused on depression, such as the Patient Health Questionnaire (e.g. PHQ-9) and related questions, into a machine-readable ontology using SNOMED-CT. With this resource, PSAT enhances the ability of models like GPT-3.5 to generate application-relevant explanations. Evaluation of four expert-curated datasets related to depression demonstrates PSAT’s application-relevant explanations. PSAT surpasses the performance of twelve baseline models and can provide explanations where other baselines fall short.},
  keywords={Depression;Ontologies;Computer architecture;Guidelines;Mental health;Unified modeling language;Medical treatment;Manuals;Knowledge based systems;Closed box;Cross attention;depression;explainable;language models;mental health;PHQ-9},
  doi={10.1109/JBHI.2024.3483577},
  ISSN={2168-2208},
  month={Feb},}@ARTICLE{10684379,
  author={Yang, Jian and Shu, Liqi and Duan, Huilong and Li, Haomin},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={RDguru: A Conversational Intelligent Agent for Rare Diseases}, 
  year={2025},
  volume={29},
  number={9},
  pages={6366-6378},
  abstract={Large language models (LLMs) hold significant promise in clinical practice, yet their real-world adoption is constrained by their propensity to produce erroneous and occasionally harmful outputs, particularly in the intricate domain of rare diseases (RDs). This study introduces RDguru, a conversational intelligent agent leveraging the LangChain framework and powered by GPT-3.5-turbo. RDguru offers a comprehensive suite of functionalities, encompassing evidence-traceable knowledge Q&A and professional medical consultations for differential diagnosis (DDX), integrating authoritative knowledge sources and reliable tools. A novel multi-source fusion diagnostic model, rooted in deep Q-network, amalgamates three diagnostic recommendation strategies (GPT-4, PheLR, and phenotype matching) to enhance diagnostic recall during medical consultations. Through tailored tools and advanced algorithms for retrieval-augmented generation, RDguru excels in knowledge Q&A, automated phenotype annotation, and RD DDX. A multi-aspect Q&A analysis demonstrates RDguru outperforms ChatGPT in generating descriptions aligned with authoritative knowledge, quantified by ROUGE scores, GPT-4-based automatic rating, and RAGAs evaluation metrics. Testing on 238 published RD cases reveals that RDguru's top 5 multi-source fusion diagnoses recapture 63.87% of actual diagnoses, marking a 5.47% improvement over the state-of-the-art diagnostic method PheLR. Furthermore, RDguru's consultation strategy proves effective in eliciting diagnostically beneficial phenotypes and refining the prioritization of genuine diagnoses through multi-round phenotype-orient questioning. Evaluations against established benchmarks and real-world patient data demonstrate RDguru's efficacy and reliability, highlighting its potential to enhance clinical decision-making in the realm of RDs.},
  keywords={Medical diagnostic imaging;Diseases;Phenotypes;Cognition;Intelligent agents;Bioinformatics;Knowledge engineering;Conversational AI;deep Q-network;knowledge Q&A;large language model;medical consultation;rare diseases},
  doi={10.1109/JBHI.2024.3464555},
  ISSN={2168-2208},
  month={Sep.},}@ARTICLE{10637955,
  author={Rezayi, Saed and Liu, Zhengliang and Wu, Zihao and Dhakal, Chandra and Ge, Bao and Dai, Haixing and Mai, Gengchen and Liu, Ninghao and Zhen, Chen and Liu, Tianming and Li, Sheng},
  journal={IEEE Transactions on Big Data}, 
  title={Exploring New Frontiers in Agricultural NLP: Investigating the Potential of Large Language Models for Food Applications}, 
  year={2025},
  volume={11},
  number={3},
  pages={1235-1246},
  abstract={This paper explores new frontiers in agricultural natural language processing (NLP) by investigating the effectiveness of food-related text corpora for pretraining transformer-based language models. Specifically, we focus on semantic matching, establishing mappings between food descriptions and nutrition data through fine-tuning AgriBERT with the FoodOn ontology. Our work introduces an expanded comparison with state-of-the-art language models such as GPT-4, Mistral-large, Claude 3 Sonnet, and Gemini 1.0 Ultra. This exploratory investigation, rather than a direct comparison, aims to understand how AgriBERT, a domain-specific, fine-tuned, open-source model, complements the broad knowledge and generative abilities of these advanced LLMs in addressing the unique challenges of the agricultural sector. We also experiment with other applications, such as cuisine prediction from ingredients, expanding our research to include various NLP tasks beyond semantic matching. Overall, this paper underscores the potential of integrating domain-specific models like AgriBERT with advanced LLMs to enhance the performance and applicability of agricultural NLP applications.},
  keywords={Data models;Task analysis;Semantics;Training;Context modeling;Biological system modeling;Natural language processing;ChatGPT;food applications;language models;natural language processing;semantic matching},
  doi={10.1109/TBDATA.2024.3442542},
  ISSN={2332-7790},
  month={June},}@ARTICLE{10632866,
  author={Gratius, Nicolas and Bergés, Mario and Akinci, Burcu},
  journal={IEEE Transactions on Aerospace and Electronic Systems}, 
  title={A Parameter Ontology for Simulation Models of Deep Space Habitats}, 
  year={2025},
  volume={61},
  number={1},
  pages={61-75},
  abstract={Deep space habitat operations will depend less on ground support than in low Earth orbit because of increasing communication delays. For the crew to be more autonomous, some of the resources dedicated to anomaly response mechanisms must be transferred from the ground to the habitat. Simulation models replicating the behavior of the system are examples of such resources as they can inform decisions to select mitigation strategies. Multiple models are typically developed for a single spacecraft as many domains of expertise and stakeholders are involved. Enabling simulation onboard a habitat will therefore require integration efforts to provide system-wide capabilities. Specifically, model parameters are model components that must be regularly calibrated to accurately represent the system state during operation. Depending on the type of simulation, these can differ greatly in their nature, thereby making difficult the tasks of creating, reading, updating, and deleting parameters. There is no standard specifying how to represent simulation parameters in a multimodel setting. Model ontologies have been shown to support integration, but their representation of model parameters is usually limited, i.e., the required type of parameter attributes and their hierarchical relationships are unknown. For example, quantifying the uncertainties in parameter values is essential in deep space operations but it is unclear how such information should be formally represented. In this article, we propose a parameter ontology extending existing standards with attributes enabling probabilistic parameter identification. This article is validated by comparing related work with the responses of the proposed ontology to competency questions and by evaluating its consistency, conciseness, completeness, and expandability.},
  keywords={Calibration;Task analysis;Unified modeling language;Space technology;Computational modeling;Buildings;Ontologies;Autonomous system;calibration;knowledge base;ontology;parameter;simulation models;space habitat;vocabulary},
  doi={10.1109/TAES.2024.3440967},
  ISSN={1557-9603},
  month={Feb},}@ARTICLE{10454094,
  author={Ji, Fan and Vogel-Heuser, Birgit and Schypula, Rafael and Wünnenberg, Maximilian and Goedicke, Michael and Fottner, Johannes},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Ontology Versioning for Managing Inconsistencies in Engineering Models Arising From Model Changes in the Design of Intralogistics Systems}, 
  year={2025},
  volume={22},
  number={},
  pages={1249-1261},
  abstract={The interdisciplinary design of intralogistics systems (ILS) involves engineers from various disciplines, resulting in the generation of discipline-specific model files with overlapping information. For instance, a conveyor system can be represented from various perspectives, such as 3D-CAD models that capture its geometric information and discrete-event simulation models that depict the system’s dynamic material flow performance. The growing demands for flexible reconfigurability and adaptability in intralogistics systems necessitate frequent updates to engineering models. However, these updates often result in potential model inconsistencies due to insufficient stakeholder communication. Detecting the impact of model changes and related inconsistencies is challenging in practice due to data heterogeneity and complex inter-model relations. To address these challenges, we propose an ontology-versioning approach that automates the identification of inconsistencies resulting from model changes. Our approach facilitates the integration of heterogeneous model data, enables database versioning, detects inconsistencies caused by model updates, and provides traceability for identified issues. The concept is evaluated utilizing models from a prototypical implementation on a lab-sized demonstrator. Note to Practitioners—In the industry, the current development of intralogistics systems often lacks automated synchronization of overlapping model information and consistent model interfaces, frequently leading to contradictions among the models. This has been identified as a significant source of errors in the design of both industrial and academic intralogistics systems, as revealed by a study involving intralogistics experts from different technical disciplines. Effectively managing model inconsistencies is crucial for project success, particularly when frequent model changes occur. A promising approach to tackle this issue is to systematically link model data from different disciplines, through which model inconsistencies caused by inadequate communication among engineers can be identified and prevented. However, in many cases, changes in different model versions and their resulting inconsistencies are not adequately considered. To address this issue, we propose a concept based on ontology versioning that allows for the generation, comparison, and analysis of different versions of an ontological model database. This concept automatically identifies model changes, assesses their impacts on other models, and provides information to assist engineers in problem-solving. The effectiveness of our approach is assessed through an evaluation of three representative change scenarios, simplified from real-world use cases. In future research, we plan to extend the approach to general production systems and incorporate industrial-scale models from the broad range of disciplines involved in the design process.},
  keywords={Ontologies;Solid modeling;Unified modeling language;Modeling;Data models;Analytical models;Adaptation models;Intralogistics;inconsistency management;model change management},
  doi={10.1109/TASE.2024.3362599},
  ISSN={1558-3783},
  month={},}
@ARTICLE{10336875,
  author={Fu, Chengcheng and Pan, Xueli and Wu, Jieyu and Cai, Junkai and Huang, Zhisheng and van Harmelen, Frank and Zhao, Weizhong and Jiang, Xingpeng and He, Tingting},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={KG4NH: A Comprehensive Knowledge Graph for Question Answering in Dietary Nutrition and Human Health}, 
  year={2025},
  volume={29},
  number={3},
  pages={1793-1804},
  abstract={It is commonly known that food nutrition is closely related to human health. The complex interactions between food nutrients and diseases, influenced by gut microbial metabolism, present challenges in systematizing and practically applying knowledge. To address this, we propose a method for extracting triples from a vast amount of literature, which is used to construct a comprehensive knowledge graph on nutrition and human health. Concurrently, we develop a query-based question answering system over our knowledge graph, proficiently addressing three types of questions. The results show that our proposed model outperforms other state-of-art methods, achieving a precision of 0.92, a recall of 0.81, and an F1 score of 0.86 in the nutrition and disease relation extraction task. Meanwhile, our question answering system achieves an accuracy of 0.68 and an F1 score of 0.61 on our benchmark dataset, showcasing competitiveness in practical scenarios. Furthermore, we design five independent experiments to assess the quality of the data structure in the knowledge graph, ensuring results characterized by high accuracy and interpretability. In conclusion, the construction of our knowledge graph shows significant promise in facilitating diet recommendations, enhancing patient care applications, and informing decision-making in clinical research.},
  keywords={Diseases;Knowledge graphs;Ontologies;Question answering (information retrieval);Semantics;Text mining;Task analysis;Knowledge graph;text mining;question answering;nutrition;human diseases},
  doi={10.1109/JBHI.2023.3338356},
  ISSN={2168-2208},
  month={March},}@ARTICLE{10056133,
  author={Gao, Su and Cao, Yue and Li, Yinqiao and Chen, Yujun and Jin, Song and Xie, Shikun and Liu, Jihong},
  journal={IEEE Access}, 
  title={Reusability, Reconfigurability and Efficiency Optimization of Satellite Network Modeling and Simulation}, 
  year={2025},
  volume={13},
  number={},
  pages={122035-122058},
  abstract={Model-based system engineering (MBSE) with reusable mechanisms can serve as an effective way for complex system architecture design. Stakeholder needs should be satisfied while product and architecture design need to be consistent with user requirements in all stages during the whole product lifecycle. In this paper, satellite network as an example of complex system is modeled in a reusable, reconfigurable and efficient manner using the system modeling language (SysML) together with pattern viewpoints and simulation constructs. Based upon abstract syntax described using metamodels and a set of profiles, concept reusability is established for the specific domain. Additionally a reusable modeling framework is developed with tailored design patterns and multiple viewpoints. Analysis metamodel, profile and interface are further presented to preserve reusability during iterations among multiple optimization rounds. A novel satellite network simulation model is formulated and multi-objective optimization is solved by transformation under practical application scenarios. A set of metrics are designed to assess and validate the models. Results show that the proposed reusable model has viewpoint coverage of more than 80 percent compared to a half for the baseline OOSEM model. The proposed model thus covers the pattern viewpoints and ontologies in a wider and more frequent way and is more efficient. Design choices made based on the model can be incorporated into this mechanism which is extensible along the system lifespan.},
  keywords={Object oriented modeling;Satellites;Unified modeling language;Modeling;Software;Optimization;Computer architecture;Domain specific languages;Reusability;satellite network;domain specific model;pattern viewpoint;simulation;multi-objective optimization},
  doi={10.1109/ACCESS.2023.3250426},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11093511,
  author={Shah-Mohammadi, Fatemeh and Finkelstein, Joseph},
  booktitle={2024 IEEE 15th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)}, 
  title={Comparative Analysis of Rule-Based and Large Language Model-Based Approaches in Addressing Variability in Clinical Outcome Reporting}, 
  year={2024},
  volume={},
  number={},
  pages={039-045},
  abstract={In clinical trials, varied terminologies and definitions often obscure the clarity and consistency needed to interpret results effectively. The ability to standardize clinical outcome reports and align semantically similar outcomes is crucial in healthcare and research, as inconsistencies can impede the comparability of trial results, complicating metaanalyses and informed decision-making. This research focuses on minimizing variability in the reporting of outcome measures through a comparative analysis of rule-based and advanced language modeling techniques. The rule-based method employs established ontologies, while the language model-based approach utilizes large language models. Findings indicate a low linkage of outcomes to traditional rule-based ontology, particularly for three-word outcomes, and underscore large language models’ efficacy in recognizing semantically similar outcomes across varying word counts. This supports the critical role of large language models in harmonizing outcome data, reducing redundancies, and improving data interoperability in clinical research contexts.},
  keywords={Analytical models;Terminology;Large language models;Semantics;Redundancy;Pipelines;Clinical trials;Ontologies;Mobile communication;Interoperability;Clinical trial;Semantic variability;Large language model;GPT;SBERT;Outcome alignment},
  doi={10.1109/IEMCON62851.2024.11093511},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11063495,
  author={Racharak, Teeradaj and Wang, Tongyu and Jearanaiwongkul, Watanee},
  booktitle={2024 16th International Conference on Knowledge and System Engineering (KSE)}, 
  title={An Automated Medical Rdf Knowledge Graph Construction From Text Using in-Context Learning}, 
  year={2024},
  volume={},
  number={},
  pages={465-471},
  abstract={The parameterized knowledge within large language models (LLMs), like ChatGPT, offers a significant opportunity for modelling domain knowledge base from text. However, LLMs' context sensitivity can hinder obtaining precise and taskaligned outcomes, thus requiring a suitable design for leveraging prompt engineering. This study explores the efficacy of different prompting methods for RDF knowledge graph construction from medical documents as our preliminary investigation, aiming to develop an efficient pipeline for a large-scale automatic knowledge graph construction according to semantic web standards and technologies. The results show that leveraging in-context learning within LLMs is capable of extracting an array of precise RDF triples from text. We perform a qualitative analysis of the extracted triples with different prompt templates, giving insights that could guide potential development in the research field.},
  keywords={Knowledge engineering;Sensitivity;Large language models;Pipelines;Knowledge graphs;Ontologies;Systems engineering and theory;Resource description framework;Prompt engineering;Standards;Generative Knowledge Graph Extraction;Resource Description Framework;OpenAI;Prompting;Few-shot},
  doi={10.1109/KSE63888.2024.11063495},
  ISSN={2694-4804},
  month={Nov},}@INPROCEEDINGS{11037900,
  author={Haw, Su-Cheng and Ng, Kok-Why and J, Jayapradha and Naveen, Palanichamy},
  booktitle={2024 7th Asia Conference on Cognitive Engineering and Intelligent lnteraction (CEII)}, 
  title={OD-SIF: An Ontology-Driven Schema Integration Framework for e-Commerce Platform}, 
  year={2024},
  volume={},
  number={},
  pages={196-201},
  abstract={Automated data mapping is crucial in modern e-commerce ensuring seamless integration of diverse and heterogeneous datasets when migration from legacy systems to advanced platforms without any data loss or corruption. OD-SIF addresses these challenges by leveraging ontologies to unify data semantics, ensuring accurate and consistent schema integration. This framework excellently enhances the e-commerce operation by harmonizing product attributes such as brand, category, and specifications with a high precision of 93% and recall of 88%, completely surpassing baseline methods by 15-20% in the Fl-score. Due to OD-SIF's semantic matching capabilities, it can easily manage noisy and incomplete data with less dependency on manual intervention. On the other hand, special ontologies may further refine the limits in handling niche domains. While improving accuracy, OD-SIF allows real-time data enrichment and ensures cross-platform interoperability that supports core functionalities like inventory management, customer data processing, and transaction across diverse systems. All these advantages make OD-SIF a key enabler for digital transformation in e-commerce bridging various platforms with payment processors, logistics providers, and marketing tools into a unified efficient ecosystem.},
  keywords={Accuracy;Program processors;Semantics;Manuals;Ontologies;Inventory management;Real-time systems;Electronic commerce;Noise measurement;Logistics;automated data mapping;schema-based;ontology;mapping;e-Commerce},
  doi={10.1109/CEII65291.2024.00046},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11025395,
  author={Wang, Xuefeng and Hu, Xiaoqing and Li, Dongsheng and Luo, Xuan},
  booktitle={2024 5th International Conference on Information Science and Education (ICISE-IE)}, 
  title={Research on the Construction of Knowledge Graph for Emergency Management of Social Security Events}, 
  year={2024},
  volume={},
  number={},
  pages={616-619},
  abstract={Constructing a knowledge graph for social security event emergency management promotes the development of social security event emergency management towards an intelligent model that relies on a vast amount of knowledge and data. Relevant content is collected from multiple information sources, and the UIE model, fine-tuned with a small amount of annotated data, is utilized to achieve joint entity-relation extraction. The results are stored in a Neo4j graph database, laying the foundation for application and analysis. The research concludes that utilizing LLM-KG fine-tuning provides a feasible means for constructing large-scale knowledge graphs in vertical domains with low resources, The knowledge graph obtained from this research can satisfy application scenarios in many situations.},
  keywords={Information science;Standards organizations;Semantics;Knowledge graphs;Organizations;Emergency services;Data models;Public security;Planning;Security;social security events;emergency management;knowledge graph},
  doi={10.1109/ICISE-IE64355.2024.11025395},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10992171,
  author={Xie, Jiangcun and Li, Ren and Yang, Jianxi and Xiao, Qiao},
  booktitle={2024 4th International Conference on Digital Society and Intelligent Systems (DSInS)}, 
  title={Ontology Embeddings for Subsumption Prediction Based on Graph Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={133-137},
  abstract={With the growing importance of knowledge graphs in artificial intelligence, accurately modeling hierarchical relationships in ontologies has become a critical issue in knowledge representation learning. To address this, this paper proposes an ontology embedding framework based on graph language models, named GLMSubs, aimed at enhancing the prediction of subclass relationships. The GLMSubs framework adopts a two-stage strategy of “multi-semantic view partitioning” and “advanced training of graph language models”. Initially, it deconstructs the ontology's concepts, attributes, and instance information into five types of semantic views, such as class hierarchy view and class-attribute relationship view, through a multi-view partitioning mechanism, comprehensively capturing information from different semantic dimensions. Subsequently, the framework employs graph language models for joint training on the multi-view data to obtain embeddings that integrate both semantic and structural information. Experiments on datasets such as FoodOn and GO validate the effectiveness of GLMSubs, demonstrating that its performance in class hierarchy relationship prediction tasks significantly surpasses existing methods.},
  keywords={Training;Proteins;Biological system modeling;Semantics;OWL;Knowledge graphs;Medical services;Ontologies;Predictive models;Resource description framework;knowledge representation learning;graph language models;OWL ontology;knowledge graph},
  doi={10.1109/DSInS64146.2024.10992171},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10990107,
  author={Zaeifi, Mehrnoosh and Mosallanezhad, Ahmadreza and Bansal, Srividya},
  booktitle={2024 International Conference on AI x Data and Knowledge Engineering (AIxDKE)}, 
  title={Deeper and Deeper: A Lightweight Semi-Supervised Deep Reinforcement Adaptive Learning-Based Ontology Alignment}, 
  year={2024},
  volume={},
  number={},
  pages={28-35},
  abstract={Ontology alignment, also known as ontology matching, is pivotal for addressing semantic heterogeneity on the Semantic Web. Essentially, it entails linking entities across different ontologies or knowledge graphs in order to resolve ambiguity and enhance the interoperability of data. While various techniques exist, many still rely on rule-based or logic-based approaches, often requiring human intervention and domain specificity. Despite these challenges, ontology alignment remains crucial for seamlessly integrating disparate knowledge sources and facilitating effective data integration. In this paper, we tackle the limitations of current ontology alignment models by introducing a novel, lightweight, semi-supervised deep reinforcement learning model called Deep Reinforcement Adaptive Learning for Ontology Alignment (DRAL-OA). The DRAL-OA method incorporates both syntactic and structural information into the training phase. In addition, this approach is semi-supervised, utilizing a portion of the training data and automatically generating the rest, which reduces the need for human intervention. Moreover, DRAL-OA uses non-domain-specific language models to ensure broad applicability and reduce the need for extensive domain expertise. We evaluate our proposed approach using two datasets from the Ontology Alignment Evaluation Initiative (OAEI). In our experiments, we have shown that the proposed model can achieve high-quality alignments with F-measures on par with other state-of-the-art systems, all while maintaining a very short runtime and a compact model size.},
  keywords={Semantic Web;Training;Knowledge engineering;Adaptation models;Runtime;Semantics;Training data;Knowledge graphs;Ontologies;Syntactics;ontology alignment;reinforcement learning;knowledge graph;semantic web},
  doi={10.1109/AIxDKE63520.2024.00012},
  ISSN={2831-7203},
  month={Dec},}@INPROCEEDINGS{10990088,
  author={Soularidis, Andreas and Kotis, Konstantinos and Lamolle, Myriam and Mejdoul, Zakaria and Lortal, Gaëlle and Vouros, George},
  booktitle={2024 International Conference on AI x Data and Knowledge Engineering (AIxDKE)}, 
  title={LLM-Assisted Generation of SWRL Rules from Natural Language}, 
  year={2024},
  volume={},
  number={},
  pages={7-12},
  abstract={Recently, Large Language Models (LLMs) have attracted great attention due to their remarkable performance in human-like text generation and reasoning skills (although their memory and hallucination problems still remain key issues to tackle more efficiently). LLMs have been applied to various application domains, including Knowledge Graph (KG) generation, question and answering over KGs and text-to-SPARQL translation. In this work, we investigate the capabilities of LLMs in text-to-SWRL translation, i.e., translation of Natural Language (NL) rules into Semantic Web Rule Language (SWRL) rules, put in the context of an industrial Ontology Engineering (OE) environment called GLUON, presenting our first experimental results. The aim of this work is to identify the level of automation that is adequate for the LLM to generate well-formed SWRL rules, towards the development of an LLM-based framework, as a plugin to the GLUON OE environment. In this direction we leverage and combine the reasoning capabilities of GPT-4o model, the Retrieval-Augmented Generation (RAG) technology, and prompt engineering. We employ quantitative and qualitative metrics to evaluate the generated SWRL rules, focusing on the correct syntax and the level of human intervention.},
  keywords={Semantic Web;Translation;Automation;Large language models;Retrieval augmented generation;Memory management;Ontologies;Syntactics;Cognition;Prompt engineering;SWRL;Large Language Models (LLM);Retrieval-Augmented Generation (RAG);Ontology Engineering},
  doi={10.1109/AIxDKE63520.2024.00008},
  ISSN={2831-7203},
  month={Dec},}@INPROCEEDINGS{10990753,
  author={N, Sushma Rani and CH, Dhawaleswar Rao and P, Srinivasa Rao},
  booktitle={2024 2nd International Conference on Signal Processing, Communication, Power and Embedded System (SCOPES)}, 
  title={Enhanced Named Entity Recognition in Medical Texts Using Transformer-Based Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={The increase of digital medical text data which includes electronic health records (EHRs), clinical notes, and medical literature, gives an invaluable resource for advancing healthcare. As the data is unstructured, it is challenging to extract valuable insights from the data. To extract these insights there are models such as Rule-based and dictionary-based. These existing models face the drawback of handling ambiguity of words, out-of-vocabulary words. The proposed approach leverages advanced Natural Language Processing techniques, specifically state-of-the-art transformer-based models like BioBERT and ClinicalBERT, to perform Named Entity Recognition in the medical domain. By integrating additional domain-specific resources, including comprehensive medical terminologies and ontologies, we enhance the performance of entity recognition. The objective is to accurately identify and classify key medical entities from diverse medical text sources. The system has been evaluated using the metric accuracy, precision, recall and F1 score. The achieved F1 score is 91.5. The resulting structured information can be utilized in numerous applications like clinical decision support, patient data management and medical research.},
  keywords={Measurement;Terminology;Biological system modeling;Face recognition;Named entity recognition;Medical services;Signal processing;Ontologies;Transformers;Object recognition;Named Entity Recognition;medical records;transformers},
  doi={10.1109/SCOPES64467.2024.10990753},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10974777,
  author={Balaadich, Youness and Jakimi, Abdeslam},
  booktitle={2024 34th International Conference on Computer Theory and Applications (ICCTA)}, 
  title={Toward an Advanced Rural Tourism Ontology for Enhancing Visitor Experiences in Morocco’s Draa-Tafilalet Region}, 
  year={2024},
  volume={},
  number={},
  pages={74-79},
  abstract={Integrating ontologies in tourism applications represents a transformative step toward achieving a structured digital transformation to enhance the competitive edge of tourism destinations. In Morocco’s tourism sector, particularly in the culturally and naturally rich Draa-Tafilalet region, implementing an ontology is essential to organize diverse tourism-related data and provide a more personalized and accessible visitor experience. Despite Morocco’s significant efforts, the tourism industry continues to face persistent challenges in delivering customized and accessible information aligned with tourists' preferences and needs. To address this issue, this study develops comprehensive ontology using a semi-automated methodology that combines natural language processing, language models, and expert validation. The ontology encapsulates attractions, services, and visitor preferences specific to the region. The main objective is to modernize and enhance the informational structure of the tourism sector, facilitating better navigation for tourists and supporting the strategic promotion of the region’s assets.},
  keywords={Navigation;Digital transformation;Large language models;Tourism industry;Machine learning;Ontologies;Chatbots;Mobile applications;Stakeholders;Faces;Digital Transformation;Rural Tourism;Ontology;Personalized Experiences},
  doi={10.1109/ICCTA64612.2024.10974777},
  ISSN={2770-6575},
  month={Dec},}@INBOOK{10952605,
  author={Vayadande, Kuldeep and Bohri, Mustansir and Chawala, Mohit and Kulkarni, Ashutosh M. and Mursal, Asif},
  booktitle={How Machine Learning is Innovating Today's World: A Concise Technical Guide}, 
  title={The Rise of AI&#x2010;Generated News Videos}, 
  year={2024},
  volume={},
  number={},
  pages={423-451},
  abstract={Summary <p>The rapid advancements in Artificial Intelligence (AI) have given rise to the possibility of automating news video creation. AI&#x2010;powered news videos will offer a fresh and dynamic perspective on the day's top stories, delivering the content people need in a way that is easy to consume. AI&#x2010;generated news videos are the next evolution in journalism, providing a fast and accurate way to consume news content that is both informative and visually stunning. In this review paper, we explore the process of converting news articles into AI&#x2010;generated videos that can be published on platforms like YouTube. The process involves web scraping of text and images, news authentication, image searching, voice&#x2010;over creation, video generation, thumbnail creation, and YouTube video upload. We have reviewed several research papers related to each of these steps and highlighted their applications in news video creation. We have identified the challenges involved in each step, such as the authenticity of news articles, relevance of images, and the need for high&#x2010;quality voice&#x2010;overs. We have discussed proposed solutions for these challenges and the potential of AI&#x2010;generated news videos in revolutionizing the news industry. Our review paper also highlights the research gaps in this field, such as the need for more advanced image and voice recognition technology and the potential ethical concerns of using AI&#x2010;generated content.</p>},
  keywords={Videos;Artificial intelligence;Media;Reviews;Face recognition;Data mining;Visualization;Text recognition;Ontologies;Industries},
  doi={10.1002/9781394214167.ch25},
  ISSN={},
  publisher={Wiley},
  isbn={9781394214150},
  url={https://ieeexplore.ieee.org/document/10952605},}@INPROCEEDINGS{10947604,
  author={Baidya, Anushuya and Do, Tuyen and Gnimpieba, Etienne Z.},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Toward Fine-Tuning Large Language Models in Ontology of Microbial Phenotypes Construction}, 
  year={2024},
  volume={},
  number={},
  pages={6913-6920},
  abstract={Ontologies are crucial for organizing domainspecific knowledge in biomedical fields, but their manual construction is time-consuming. This study explores the automation of ontology learning using large language models (LLMs) like BERT, RoBERTa, and DistilBERT, focusing on the Ontology of Microbial Phenotypes (OMP). We investigate three key tasks: (1) entity extraction, (2) relation extraction between entities, and (3) ontology verification. These tasks align with broader applications in biomedical annotation and named entity recognition (NER) by enabling the identification and structuring of key terms and relationships within microbial phenotypes. We evaluate LLMs in two scenarios: baseline performance using pre-trained models and fine-tuned performance after training on OMP-specific data. Our approach integrates spaCy for entity extraction, Llama 2 for relation identification, and LLMs for ontology verification. Experiments reveal that fine-tuned models significantly improve accuracy, precision, recall, and F1 scores, particularly for ontology verification. This research highlights the potential of LLMs to enhance ontology learning and support related biomedical applications like biofilm analysis, annotation, and NER, while emphasizing the value of expert curation.},
  keywords={Training;Phenotypes;Annotations;Large language models;Biological system modeling;Focusing;Named entity recognition;Manuals;Ontologies;Data models;Ontology Learning;Large Language Models;Fine-tuning;Microbial Phenotypes;Ontology Verification},
  doi={10.1109/BIBM62325.2024.10947604},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10936732,
  author={Ge, Xing and Liu, Yafei and Yang, Pei and Sun, Xin and Qiao, Junfeng and Qu, Luyao and Qiu, Jingyi},
  booktitle={2024 International Conference on Information Technology, Comunication Ecosystem and Management (ITCEM)}, 
  title={Research and Application of Electronic Data Retrieval in Material Supply Chain Enhanced by Large Language Models and Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={156-160},
  abstract={In response to the new goals of building green and modern smart supply chains, the electric power equipment supply chain is experiencing a shift toward digital intelligence and low-carbon, environmentally friendly development [1]. However, traditional search platforms based on relational databases face challenges in handling vast amounts of multimodal electronic data. These platforms often suffer from low search accuracy, limited cross-dimensional correlation analysis capabilities, and inefficiencies, making them inadequate for constructing comprehensive big data platforms that integrate and share information across the entire green, modern, smart supply chain. This paper introduces an innovative electronic data retrieval method designed to meet the data retrieval needs of material supply chains. By integrating large language models with knowledge graph technology, it proposes an electronic data retrieval system that leverages vector database technology for text embedding of multimodal data. Additionally, artificial intelligence is used to enable knowledge retrieval and augmented generation, significantly enhancing data retrieval capabilities within the specialized domain of material supply chains.},
  keywords={Green buildings;Large language models;Supply chains;Knowledge graphs;Relational databases;Data retrieval;Vectors;Power systems;Information technology;Faces;large language model;knowledge graph;material supply chain;data retrieval},
  doi={10.1109/ITCEM65710.2024.00037},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10928264,
  author={Ying, Zhao and Weiyu, Chen and Longlong, Liao and Jie, Liu},
  booktitle={2024 IEEE International Conference on Computing (ICOCO)}, 
  title={Masked Theme-Specific Named Entity Recognition Assisted with Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={457-462},
  abstract={Existing Named Entity Recognition (NER) methods are required to label relevant samples and train the concrete NER models. Due to the specification of theme-specific documents, these NER models are considerably hard to identify potential theme-specific entities. To address this challenge, we propose an effective two-stage approach of masked theme-specific NER associated with Large Language Models (LLMs), which uses the unsupervised mechanism rather than the supervised one. The approach involves theme-specific entity ontology construction and masked NER in heterogeneous documents. The first stage is associated with LLMs and Wikipedia category pages, and the second one is implemented with the masked NER based on the created ontology in the first stage. Extensive experimental results suggest that the proposed masked NER can precisely locate the known entities in the theme-specific entity ontology while improving the accuracy of NER in the remaining text. Compared to the mainstream NER frameworks such as spaCy 3, the masked NER can identify more valid entities in the input Markdown text and continuously use the newly detected unknown entities to update the created ontology.},
  keywords={Accuracy;Filtering;Large language models;Computational modeling;Named entity recognition;Encyclopedias;Ontologies;Libraries;Internet;Online services;Natural Language Processing;Named Entity Recognition;Large Language Model;Heterogeneous Documents;Theme-specific Entity Ontology},
  doi={10.1109/ICOCO62848.2024.10928264},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10913712,
  author={Hier, Daniel B. and Munzir, S. Ilyas and Stahlfeld, Anne and Obafemi-Ajayi, Tayo and Carrithers, Michael D.},
  booktitle={2024 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI)}, 
  title={High-Throughput Phenotyping of Clinical Text Using Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={High-throughput phenotyping automates the mapping of patient signs to standardized concepts, such as those in Human Phenotype Ontology (HPO), a process critical to precision medicine. We evaluated the automated phenotyping of clinical summaries from the Online Mendelian Inheritance in Man (OMIM) database using a large language model. Various APIs were used to automate text retrieval, sign identification, categorization, and normalization. GPT-4 outperformed GPT-3.5Turbo in identifying, categorizing, and normalizing signs, achieving concordance with manual annotators comparable to concordance between manual annotators. While GPT-4 demonstrates high accuracy in sign identification and categorization, limitations remain in sign normalization, particularly in retrieving the correct HPO ID for a normalized term. Methods such as retrieval-augmented generation, changes in pre-training, and additional fine-tuning may help address these limitations. The combination of APIs with large language models presents a promising approach for high-throughput phenotyping of free text.},
  keywords={Neurology;Phenotypes;Databases;Large language models;Precision medicine;Retrieval augmented generation;Manuals;Ontologies;Natural language processing;Bioinformatics;phenotype;large language model;natural language processing;high-throughput;OMIM;neurology;HPO;GPT-4},
  doi={10.1109/BHI62660.2024.10913712},
  ISSN={2641-3604},
  month={Nov},}@INPROCEEDINGS{10913373,
  author={Zhou, Peiyao and Hu, Zhikui and Zi, Kangli and Zhang, Dawei},
  booktitle={2024 6th International Conference on Frontier Technologies of Information and Computer (ICFTIC)}, 
  title={Leveraging Knowledge Distillation for Improved Event Extraction in QA Models}, 
  year={2024},
  volume={},
  number={},
  pages={703-708},
  abstract={Event extraction is an important task in natural language processing, and it is widely utilized in intelligence domains such as business and military for information extraction. Recently, many works have successfully transformed document-level event extraction into Question-answering (QA) tasks with remarkable results. This approach embeds event argument information within the questions, introducing prior knowledge into the process. Jin et al. highlighted the importance of high-quality QA pairs for practical QA tasks, emphasizing that constructing these pairs remains a key challenge[1]. In this study, we propose a Prompt-based question generation method to automatically generate questions containing event arguments, which converts event extraction into a QA task. We introduce an event ontology-based information retrieval module to enhance answer accuracy and select the most relevant document segments as input text. Additionally, we employ knowledge distillation to build the QA model, transferring knowledge from a large pre-trained model to a more compact and efficient one, improving the student model's performance. Experiments show that our model performs strongly in mainstream benchmarks, with 4.8 improvements on WikiEvents.},
  keywords={Accuracy;Benchmark testing;Information retrieval;Question generation;Question answering (information retrieval);Data mining;Business;component;document-level event extraction;Question Answering;knowledge distillation},
  doi={10.1109/ICFTIC64248.2024.10913373},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10912181,
  author={Sharma, Simple and Panda, Supriya P.},
  booktitle={2024 2nd International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)}, 
  title={Performance Metrics Analysis for Deep Learning Models}, 
  year={2024},
  volume={1},
  number={},
  pages={970-976},
  abstract={Evaluating an Information Retrieval (IR) model is a multi-faceted process that requires selecting the right quantitative metrics to assess performance. The selection of evaluation metrics in IR depends on specific tasks, relevant standards, and desired characteristics. To thoroughly assess an IR system’s performance, a combination of metrics is necessary. This study explores the effectiveness of deep learning (DL) models in semantic and personalized information retrieval (SIR), focusing on BERT and other large language models (LLMs) that provide context-sensitive embeddings and advanced language comprehension capabilities. Through a detailed analysis, this paper demonstrates how DL models can significantly enhance accuracy and relevance in IR, using evaluation metrics critical for assessing model performance. Metrics like recall, precision, and F1-score are key in capturing model accuracy and coverage; for example, a recall of $\mathbf{1. 0}$ indicates complete retrieval of relevant instances, while a precision of 0.6 reflects a $\mathbf{6 0 \%}$ accuracy in positive predictions, balancing these measures at an F1-score of 46.15%. Ranking and prioritization metrics, such as Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG), evaluated at 91.67% and 95.1%, respectively highlight the models’ capabilities in prioritizing relevant results effectively. In personalized and semantic search, selecting the right evaluation metrics is essential to maximize DL model potential and improve the user experience. Recent LLM advancements, like GPT-4, are instrumental in capturing nuanced meanings and understanding complex user queries, which enhances the accuracy of search engines. Continuous optimization of these models through tailored metrics can improve IR systems’ contextual relevance and accuracy, fostering greater user satisfaction.},
  keywords={Measurement;Deep learning;Analytical models;Accuracy;Semantic search;Computational modeling;Transformers;User experience;Standards;Context modeling;BERT;Deep Learning Models;Evaluation;Metrics;Personalized IR;Semantic IR;Transformer Models;Ontology;User Profiling},
  doi={10.1109/ICAICCIT64383.2024.10912181},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10910646,
  author={Kalaiarasi, S. Jenny and Nimala, K.},
  booktitle={2024 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)}, 
  title={Enhancing E-Commerce Product Recommendations Using LLMs and Transformer-Based Deep Learning Architectures}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={The integration of large language models with deep learning architectures brought an evolutionary revolution in the context of product recommendation systems because of several limitations of traditional methods, such as collaborative and content-based filtering. In this paper, a novel framework for product recommendation is proposed by integrating large language models jointly with deep learning. The contribution of large language models is that it adds semantic understanding capability to the predictive power provided through neural networks. Domain ontologies will be used in this hybrid model to enhance the accuracy and personalization of recommendations, considering complex user preferences and product attributes in e-commerce platforms. It takes a state-of-the-art pre-trained LLM, such as Llama-3, as input and generates personalized embeddings of users based on history, item descriptions, and contextual information. In this work, the Transformer architecture has been used in re-fining and ranking the products for relevance using attention mechanisms that select the most important features in each recommendation task. Besides, knowledge distillation will be used to conduct the small and efficient student model training process. The distilled model receives soft predictions that involve the teacher LLM, which greatly reduces computational overhead but preserves high recommendation accuracy. Eventually, the framework will be evaluated on a real-world e-commerce dataset to explore how it increases the click-through rate, purchase rate, and user's engagement compared to the traditional systems.},
  keywords={Training;Accuracy;Filtering;Computational modeling;Large language models;Collaboration;Computer architecture;Ontologies;Transformers;Electronic commerce;Pre-trained LLM;Llama-3;Domain ontologies;collaborative and content-based filtering},
  doi={10.1109/ICSES63760.2024.10910646},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10908257,
  author={Huynh, Trung-Tru and Nguyen, The-Bao and Ho-Dac, Hung},
  booktitle={2024 International Conference on Advanced Technologies for Communications (ATC)}, 
  title={A Practical Approach Applying Deep Learning and Ontology to Identify Aspects in Opinions}, 
  year={2024},
  volume={},
  number={},
  pages={969-973},
  abstract={Aspect-based sentiment analysis is a very interesting problem in opinion mining. Accurately determining the evaluated aspect in an opinion contributes to improving the performance of the sentiment analysis problem. This study proposes an approach to identify aspects that is not based on keywords but on the semantics of opinions. To determine the evaluated aspects in opinions, the proposed approach uses the method of embedding knowledge from the ontology into the corpus to train deep learning algorithms. The structure of the ontology used in this study is based on the relationship between aspect words and emotion words in the field of car evaluation. The corpus is labeled with aspects not only based on keywords indicating aspects but also based on the semantics of the sentence. The high accuracy test results show the prominent application of the proposed approach.},
  keywords={Deep learning;Sentiment analysis;Accuracy;Semantics;Ontologies;Data models;Automobiles;Aspect;Corpus;Deep Learning;Ontology},
  doi={10.1109/ATC63255.2024.10908257},
  ISSN={2162-1039},
  month={Oct},}@INPROCEEDINGS{10903241,
  author={Barron, Ryan C. and Grantcharov, Vesselin and Wanna, Selma and Eren, Maksim E. and Bhattarai, Manish and Solovyev, Nicholas and Tompkins, George and Nicholas, Charles and Rasmussen, Kim Ø. and Matuszek, Cynthia and Alexandrov, Boian S.},
  booktitle={2024 International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization}, 
  year={2024},
  volume={},
  number={},
  pages={1669-1676},
  abstract={Large Language Models (LLMs) are pre-trained on large-scale corpora and excel in numerous general natural language processing (NLP) tasks, such as question answering (QA). Despite their advanced language capabilities, when it comes to domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations, knowledge cut-offs, and lack of knowledge attributions. Additionally, fine tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and time consuming process. The retrieval-augmented generation (RAG) process has recently emerged as a method capable of optimization of LLM responses, by referencing them to a predetermined ontology. It was shown that using a Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into account relevant sub-graphs that preserve the information in a structured manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM framework, that integrates RAG with KG and a vector store (VS) that store factual domain specific information. Importantly, to avoid hallucinations in the KG, we build these highly domain-specific KGs and VSs without the use of LLMs, but via NLP, data mining, and nonnegative tensor factorization with automatic model selection. Pairing our RAG with a domain-specific: (i) KG (containing structured information), and (ii) VS (containing unstructured information) enables the development of domain-specific chat-bots that attribute the source of information, mitigate hallucinations, lessen the need for fine-tuning, and excel in highly domain-specific question answering tasks. We pair SMART-SLIC with chain-of-thought prompting agents. The framework is designed to be generalizable to adapt to any specific or specialized domain. In this paper, we demonstrate the question answering capabilities of our framework on a corpus of scientific publications on malware analysis and anomaly detection.},
  keywords={Tensors;Accuracy;Retrieval augmented generation;Knowledge graphs;Ontologies;Question answering (information retrieval);Vectors;Malware;Reliability;Tuning;Artificial Intelligence;Retrieval Augmented Generation;Knowledge Graph;Natural Language Processing;Non-Negative Tensor Factorization;Topic Modeling;Agents},
  doi={10.1109/ICMLA61862.2024.00258},
  ISSN={1946-0759},
  month={Dec},}@INPROCEEDINGS{10895371,
  author={R, Menaha and R, Abilaash and N, Mohanram P and Unnikrishnan, Akash and S, Sukumar},
  booktitle={2024 International Conference on Emerging Research in Computational Science (ICERCS)}, 
  title={Drug Pills Identification System using Google Gemini LLM: A Generative AI approach}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Generative AI is emerging as a disruptive force in the healthcare industry, bringing novel solutions ranging from drug development and clinical decision support to personalized patient care. This study is focused on drug discovery using the Generative AI model. In this paper, a system is proposed for providing drug descriptions from drug pill images. The system is implemented by utilizing Large Language Models (LLMs) in combination with computer vision to detect and provide detailed information about drugs from pill images. In the proposed system, the identification process begins by taking the medicinal drug pills and their cover images. Then, the image is converted into binary values using a standard built-in function. In addition, the target language for providing audio descriptions about the drugs is also used. Then, the Google Gemini LLM model is customized by using binary values of the image, target language, and ontology-based prompt engineering. As a result, the LLM model provides drug descriptions in text. Then, the textual description of the drug is converted into the target language audio format by using the Google Text to Speech Converter. The system is experimented by using 807 medicinal drug images which are collected from web resources. The performance of the system is measured by using accuracy. The system achieved an accuracy of 95.04% which is a little higher when compared with the current state-of-the-art model.},
  keywords={Drugs;Accuracy;Text analysis;Computational modeling;System performance;Medical services;Streaming media;Internet;Text to speech;Biomedical imaging;Drug Pill Identification;Generative AI;Large Language Models;Pharmaceutical Image Analysis;Multimodal Learning;Medical Text Analysis},
  doi={10.1109/ICERCS63125.2024.10895371},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10892858,
  author={Couder, Juan Ortiz and Pate, William C. and Machado, Daniel A. and Ochoa, Omar},
  booktitle={2024 IEEE Frontiers in Education Conference (FIE)}, 
  title={Incorporating AI in the Teaching of Requirements Tracing Within Software Engineering}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={During the Software Development Lifecycle (SDLC), the first stage entails the Requirement Engineering phase. In this phase, engineers gather, analyze, and specify the requirements for a software system. Requirements playa crucial role in the SDLC as they establish the foundation for the entire system by defining the expected behaviors of the software system to be built. The resulting specifications are captured in a Software Requirement Specification (SRS) document. As part of the validation process, requirement specifications are traced. Requirement tracing involves linking the requirement to the artifacts where the customer requested the high-level requirement. Teaching proper requirements tracing can be challenging in a traditional classroom setting. It is essential to educate future software engineers on the proper process of developing an SRS document and of tracing requirements back to the originating artifact, which is also challenging due to the complexity and large scope of applying the complete requirements engineering process. Understanding how changes in customer needs can impact requirements is an imperative learning opportunity. In this work, we aim to incorporate the use of AI in the teaching of requirements tracing using Large Language Models. In this experiment, both GPT -3.5 and GPT -4 are provided the transcript of an interview between the customer and the engineering team, as well as the subsequent requirements elicited from that meeting and other customer provided artifacts. The GPTs are then instructed to determine which requirements can be traced back to the interview transcript. At the same time, the students (the requirements engineering team) conduct their own effort to trace requirements back to the original interview. The experiment was taken one step further to assess students' and the GPTs abilities to address requirements modifications. After another interview with the customer, where some needs were changed, some requirements were modified, and students, and GPTs were asked to trace the modified requirements to the new interview. The results proved that students are better than both GPT versions at tracing modified requirements, yet GPTs again identified requirements that students didn't trace back. The findings, illustrate that AI can help in the teaching of requirement tracing; these results suggest that while no AI model is currently capable of replacing real requirement engineers as they don't outperform students, it can be used as a tool to test the completeness of the requirement tracing process. We posit that GPT can be a tool for students to self-assess the degree to which their own requirements tracing is exhaustive.},
  keywords={Training;Visualization;Atmospheric modeling;Prototypes;Software systems;Requirements engineering;Interviews;Artificial intelligence;Software engineering;Software development management;AI;Requirement Tracing;Education;Software Requirement Specification;Large Language Models},
  doi={10.1109/FIE61694.2024.10892858},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{10885332,
  author={Hou, Lijuan and Qin, Hanyan and Zhang, Xiankun and Zhang, Yiying},
  booktitle={2024 IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA)}, 
  title={Protein Function Prediction Based on the Pretrained Language Model ESM2 and Graph Convolutional Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1776-1781},
  abstract={Understanding protein function is crucial for comprehending life at the molecular level. Currently, less than 0.1% of proteins having experimental GO annotations. Traditional experimental methods are time-consuming and expensive. To narrow this gap, employing accurate and efficient computational methods can fill the void in automated protein function prediction (AFP). We have developed a new method for predicting protein function using sequence and predicted structural information. We use the large-scale pretrained language model ESM2 to pretrain protein sequences and an encoder to capture contextual information. Using the 3D structural data of proteins generated by AlphaFold2, combined with the sequence, as input for the Graph Convolutional Neural Network, to infer the probabilities of Gene Ontology (GO) annotations for the proteins. Compared to earlier methods, our model achieves better performance. Evaluations on the human dataset show AUPR improvements of 9%, 9.4%, and 20.7% in the BP, MF, and CC branches, respectively, demonstrating that our model is an effective tool for predicting protein function.},
  keywords={Three-dimensional displays;Accuracy;Annotations;Predictive models;Ontologies;Logic gates;Feature extraction;Protein sequence;Data models;Context modeling;Gene Ontology;protein function prediction;ESM2;AlphaFold2;graph pooling},
  doi={10.1109/ISPA63168.2024.00242},
  ISSN={2158-9208},
  month={Oct},}@INPROCEEDINGS{10882566,
  author={Singh, Navjot and Bathla, Gaurav},
  booktitle={2024 13th International Conference on System Modeling & Advancement in Research Trends (SMART)}, 
  title={A Review on Opinion Mining Approaches: Using Deep Learning for Large Scale Data}, 
  year={2024},
  volume={},
  number={},
  pages={756-762},
  abstract={Opinion mining, another name for sentiment analysis (SA), is a branch of computer science. that focuses on analyzing the opinions and feelings expressed in text, audio and video-Sentiment analysis involves determining the sentiment expressed by person. The objective of the opinion-mining field is to conduct subjectivity analysis, indicating whether a document is subjective or objective. Subjectivity implies the presence of sentiment, while objectivity signifies content devoid of sentiment. Currently, an abundance of information about a specific product is available, with a single product often garnering hundreds of reviews across various webpages. Numerous websites, such as imdb.com, amazon.com, idlebrain.com, among others, aggregate user information and expert opinions to publish reviews. Experts meticulously analyse reviews, extract opinions, and generate ratings related to the dataset provided by the requesting agencies. However, handling the vast amount of data is a labour intensive task for experts. The continuously growing volume of web data poses challenges in extracting precise opinions from content. Hence, there is a need to design a system that can efficiently perform these tasks with human-like accuracy. This paper analyses various approaches which are capable of handling and analysing large amounts of reviews and generate the aspect level opinion mining (AOM).},
  keywords={Sentiment analysis;Accuracy;Text analysis;Reviews;Social networking (online);Terrorism;Soft sensors;Media;Monitoring;Text processing;Opining Mining;Review;Text Data;Subjectivity},
  doi={10.1109/SMART63812.2024.10882566},
  ISSN={2767-7362},
  month={Dec},}@INPROCEEDINGS{10884260,
  author={Lee, Sejin and Kim, Dongha and Song, Min},
  booktitle={2024 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={Beyond Ontology in Dialogue State Tracking for Goal-Oriented Chatbot}, 
  year={2024},
  volume={},
  number={},
  pages={177-185},
  abstract={Goal-oriented chatbots are essential for automating user tasks, such as booking flights or making restaurant reservations. A key component of these systems is Dialogue State Tracking (DST), which interprets user intent and maintains the dialogue state. However, existing DST methods often rely on fixed ontologies and manually compiled slot values, limiting their adaptability to open-domain dialogues. We propose a novel approach that leverages instruction tuning and advanced prompt strategies to enhance DST performance, without relying on any predefined ontologies. Our method enables Large Language Model (LLM) to infer dialogue states through carefully designed prompts and includes an anti-hallucination mechanism to ensure accurate tracking in diverse conversation contexts. Additionally, we employ a Variational Graph Auto-Encoder (VGAE) to model and predict subsequent user intent. Our approach achieved state-of-the-art with a JGA of 42.57% outperforming existing ontologyless DST models, and performed well in open-domain real-world conversations. This work presents a significant advancement in creating more adaptive and accurate goal-oriented chatbots. 1},
  keywords={Adaptation models;Accuracy;Limiting;Large language models;Neural networks;Oral communication;Ontologies;Predictive models;Chatbots;Tuning;Goal-oriented Dialogue;Dialogue State Tracking;Chatbot;Prompt engineering;Graph Neural Network},
  doi={10.1109/ICKG63256.2024.00030},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10884217,
  author={Chen, Gui and Liu, Xianhui},
  booktitle={2024 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={A Multi-Agent Collaborative Framework for Constructing Knowledge Graphs from Text}, 
  year={2024},
  volume={},
  number={},
  pages={9-16},
  abstract={Recent advancements in large language models (LLMs) have significantly improved natural language understanding and generation, making them valuable tools for knowledge graph construction. However, a single LLM often struggles with the complexity of this task, leading to suboptimal results. To address this challenge, we propose a robust multi-agent collaborative framework for constructing knowledge graphs from text. This framework leverages dynamic interactions among specialized agents, including knowledge graph experts, knowledge extraction experts, data processing experts, and domain-specific experts, to effectively build accurate knowledge graphs from text. Additionally, we introduce a novel prompt construction method tailored for knowledge extraction and a revision mechanism to revise preliminary knowledge graphs. These innovations address common issues in knowledge extraction and enhance the quality of model-generated content. Experimental results on four datasets across two tasks (Named Entity Recognition and Relation Extraction) demonstrate that our approach achieves superior performance in the F1 score compared to baseline methods, highlighting its effectiveness and robustness.},
  keywords={Knowledge engineering;Technological innovation;Large language models;Collaboration;Knowledge graphs;Named entity recognition;Ontologies;Data processing;Robustness;Data mining;knowledge graph construction;multi-agent;large language model;knowledge extraction;prompt engineering},
  doi={10.1109/ICKG63256.2024.00010},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10884275,
  author={Zhou, Xiaofa and Shi, Jianyong and Dong, Lei and Zhang, You and Pan, Jin and Huang, Hao},
  booktitle={2024 International Conference on New Power System and Power Electronics (NPSPE)}, 
  title={Construction of a Multimodal Knowledge Graph for Power Grid Construction Safety Based on Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={21-28},
  abstract={In response to the difficulties of safety management and the complexity of information at power grid construction sites, a multimodal knowledge graph construction method based on large language models is proposed. Data from the construction site is collected and filtered, and an ontology for safety management at the construction site is constructed. The ontology is then used as retrieval augmented generation(RAG) for assistance, enabling multimodal large model image extraction, resulting in structured data in the power grid safety field. Finally, the extracted results are displayed using a graph database, completing the construction of the multimodal knowledge graph. The constructed knowledge graph includes multimodal data from the construction site, allowing for quick querying of on-site safety incidents, providing safety managers with a valuable tool for site management.},
  keywords={Accuracy;Large language models;Safety management;Knowledge graphs;Ontologies;Information filters;Power grids;Data models;Power electronics;Data mining;Ontology;Large language model;Safety management;Knowledge extraction;Knowledge graph},
  doi={10.1109/NPSPE62515.2024.00013},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10881842,
  author={Li, Yingna and Ding, Zhiguo and Yan, Zheng and Li, Zhuahua and Shao, Hang},
  booktitle={2024 7th International Conference on Data Science and Information Technology (DSIT)}, 
  title={Insider Threat Detection based on Knowledge Graph and Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={This article proposes an insider threat detection method based on a combination of knowledge graph and large language model (LLM); first, the internal systems, users, IPs, access behaviors, etc. are modeled through the knowledge graph ontology; then, a few-shot learning information extraction method based on LLMs is used to extract knowledge from the behavior logs to complete the threat detection knowledge graph. Finally, the representation learning method based on the knowledge graph and the embedding based on the LLM are used to extract feature vectors, which are used as input of the insider threat detection model training based on Deep SVDD. The experimental results show that this method can automatically detect abnormal threat behaviors from massive logs at high accuracy, and has the ability to detect deeply hidden abnormal threat behaviors.},
  keywords={Training;Representation learning;Large language models;Semantics;Knowledge graphs;Feature extraction;Threat assessment;Vectors;Data models;Cognition;insider threat detection;knowledge graph;large language model;deep-SVDD},
  doi={10.1109/DSIT61374.2024.10881842},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10880799,
  author={Mejia, Jose M. Ruiz and Rawat, Danda B.},
  booktitle={2024 IEEE International Conference on E-health Networking, Application & Services (HealthCom)}, 
  title={ClinicalGraph: An Applied Approach in Clinical EHR Knowledge Graph Generation for Optimized Clinical Decision Support System}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Electronic Health Records (EHR) are well-known for their extensive capabilities in storing pertinent patient information across various specializations. However, EHRs are part of a broader ecosystem of applications that support the medical treatment process. This ecosystem, although somewhat centralized, does not have the interoperability that many assume. The main issue is that current triage models are not personalized to each patient. Context matters in a medical emergency situation especially when resources are low. In this research, we propose a possible solution by applying state of the art techniques in order to develop a clinical knowledge graph that contains relevant data used for triage optimization. We utilize a fine-tuned named entity recognition model (NER) to extract 41 label entity categories from previous medical records. Additionally, we employed prompt engineering utilizing a large language text generation model with medical knowledge to generate relationships. This resulted in a sum of 1,429 relationship type categories and approximately 999 entity nodes were created with 2,387 relationships.},
  keywords={Decision support systems;Reviews;Biological system modeling;Large language models;Ecosystems;Retrieval augmented generation;Medical treatment;Knowledge graphs;Prompt engineering;Medical diagnostic imaging;Rapid Triage;E-triage;Generative Artificial Intelligence;Knowledge Graph Generation;Patient Centered Nodes;Knowledge Graph;Clinical Knowledge Graph},
  doi={10.1109/HealthCom60970.2024.10880799},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10858998,
  author={Mohsenzadegan, Kabeh and Tavakkoli, Vahid and Kambale, Witesyavwirwa Vianney and Kyamakya, Kyandoghere},
  booktitle={2024 19th International Workshop on Semantic and Social Media Adaptation & Personalization (SMAP)}, 
  title={Towards Seamless Data Translation Based on Data Models: A Hybrid AI Framework for Smart Transportation and Manufacturing}, 
  year={2024},
  volume={},
  number={},
  pages={68-73},
  abstract={Interoperability between different data standards is essential for advancing digital technologies in smart manufacturing and transportation. This paper presents a hybrid AI framework that integrates Ontology Learning (OL), Knowledge Graphs (KGs), and Large Language Models (LLMs) to improve data translation across these standards. Focusing on IEEE 1451, ISO 15926, and IEC 61499, we address the challenges posed by these data models' unique structures and semantics. Our comparative analysis evaluates the strengths and limitations of OL, KGs, and LLMs across key metrics like accuracy, scalability, efficiency, robustness, and flexibility. The proposed framework leverages OL for systematic structuring, KGs for relational modeling, and LLMs for linguistic processing, enhancing translation accuracy and adaptability. However, integrating these approaches introduces scalability and processing efficiency trade-offs, particularly in resource-constrained environments. This study contributes to developing more sophisticated and scalable data translation models tailored for heterogeneous data environments, with practical implications for smart manufacturing and transportation.},
  keywords={Adaptation models;Translation;Accuracy;Systematics;Scalability;Semantics;Data models;Smart transportation;Standards;Smart manufacturing;Data Model Translation;Ontology Learning (OL);Knowledge Graphs (KGs);Large Language Models (LLMs);Smart Manufacturing;Data Interoperability;AI in Transportation;Hybrid AI Framework;Semantic Mapping;Cross-Standard Data Integration},
  doi={10.1109/SMAP63474.2024.00022},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10859032,
  author={He, Lei and Zhang, Wenbo and Shi, Tao and Zhao, Yichen and Wang, Xing and Xing, Lumin and Li, Yongmeng},
  booktitle={2024 IEEE 9th International Conference on Data Science in Cyberspace (DSC)}, 
  title={Knowledge Graph Construction in the Context of Traditional Chinese Medicine: A review}, 
  year={2024},
  volume={},
  number={},
  pages={287-293},
  abstract={Under the background of traditional Chinese medicine(TCM) informatization, knowledge graph as a tool of the information construction solves the complex and difficult problem of TCM knowledge structure. With the continuous optimization and iteration of artificial intelligence such as deep learning, the construction technology of knowledge graph in the field of TCM is also constantly updated. In the text, it summarizes the different methods and steps of knowledge graph construction, and on this basis, introduces the most important knowledge extraction technologies in the construction of TCM knowledge graph. This summarizes the knowledge graph technology applied in the scene of TCM from four fields: Chinese medical books, TCM medical records, Chinese medicine formulae and TCM health maintenance. Finally, it will provide construction ideas for the knowledge graph of TCM prevention and control of plague.},
  keywords={Deep learning;Reviews;Prevention and mitigation;Cyberspace;Knowledge graphs;Data science;Maintenance;Data mining;Optimization;Engines;Knowledge graph;Traditional Chinese medicine;Identity of named entity},
  doi={10.1109/DSC63484.2024.00045},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10858988,
  author={Fu, Yibin and Ding, Zhaoyun and Xu, Xiaojie},
  booktitle={2024 IEEE 9th International Conference on Data Science in Cyberspace (DSC)}, 
  title={LLM & Bagging for 1-shot Joint IE}, 
  year={2024},
  volume={},
  number={},
  pages={204-208},
  abstract={Domain-specific few-shot information extraction (IE) has always been the difficulty of domain knowledge graph construction, and there is a new solution in this direction after the emergence of large language models (LLM). In this paper, based on previous research, we propose LLM-based 1-shot relation-entity joint IE scheme, and the bagging enhance LLM IE method is proposed to take advantage of the randomness of the LLM output. Against the background of the concept of Internet of Things (IoT) which has received wide attention globally, we selects the IoT interconnective communication as a domain-specific example, crawls the text of the device pages of L3Harris, RockwellCollins as our corpus, selects large language models that differ in the number of parameters and invocation methods to test the proposed joint IE method in relations given by the IoT interconnective communication ontology. The bagging method is tested based on the IE results of GPT-4 Turbo, and there is an improvement of 1-3%, which shows the effectiveness of traditional machine learning methods in LLM. Finally, the results and shortcomings of this study are analyzed.},
  keywords={Training;Large language models;Knowledge graphs;Ontologies;Data science;Information retrieval;Internet of Things;Bagging;Standards;Random forests;Large Language Model (LLM);Information Extraction (IE);bagging;1-shot;Communication},
  doi={10.1109/DSC63484.2024.00034},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10852450,
  author={Mou, Yongli and Chen, Hanbin and Lode, Gwendolyn Isabella and Truhn, Daniel and Sowe, Sulayman and Decker, Stefan},
  booktitle={2024 2nd International Conference on Foundation and Large Language Models (FLLM)}, 
  title={RadLink: Linking Clinical Entities from Radiology Reports}, 
  year={2024},
  volume={},
  number={},
  pages={443-449},
  abstract={Radiology reports are a critical source of information for patient diagnosis and treatment in the medical domain. However, the vast amount of data contained in these reports is often unstructured, making it challenging to extract and normalize relevant clinical entities. Named Entity Normalization (NEN) is essential for mapping these entities to a standard ontology, facilitating better data integration, retrieval, and analysis. In this paper, we introduce RadLink, a benchmark for NEN in radiology. RadLink builds upon 425 expert-annotated radiology reports from the RadGraph dataset, extending it for NEN by mapping entities to the Unified Medical Language System (UMLS) ontology. We employ a combination of morphological and semantic matching approaches to generate normalization annotations, followed by human review for validation. We aim to set a standard with our benchmark for evaluating NEN methods in the radiology domain, that facilitate interoperability across healthcare systems and accelerate medical research by providing structured, standardized data.},
  keywords={Accuracy;Reviews;Large language models;Unified modeling language;Semantics;Radiology;Ontologies;Benchmark testing;Medical diagnostic imaging;Standards;named entity normalization;large language models;radiology reports},
  doi={10.1109/FLLM63129.2024.10852450},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10852465,
  author={Sorokoletova, Olga and Antonioni, Emanuele and Colò, Giordano},
  booktitle={2024 2nd International Conference on Foundation and Large Language Models (FLLM)}, 
  title={Towards a scalable AI-driven framework for data-independent Cyber Threat Intelligence Information Extraction}, 
  year={2024},
  volume={},
  number={},
  pages={398-406},
  abstract={Cyber Threat Intelligence (CTI) is critical for mitigating threats to organizations, governments, and institutions, yet the necessary data are often dispersed across diverse formats. AI-driven solutions for CTI Information Extraction (IE) typically depend on high-quality, annotated data, which are not always available. This paper introduces 0-CTI, a scalable AI-based framework designed for efficient CTI Information Extraction. Leveraging advanced Natural Language Processing (NLP) techniques, particularly Transformer-based architectures, the proposed system processes complete text sequences of CTI reports to extract a cyber ontology of named entities and their relationships.Our contribution is the development of 0-CTI, the first modular framework for CTI Information Extraction that supports both supervised and zero-shot learning. Unlike existing state-of-the-art models that rely heavily on annotated datasets, our system enables fully dataless operation through zero-shot methods for both Entity and Relation Extraction, making it adaptable to various data availability scenarios. Additionally, our supervised Entity Extractor surpasses current state-of-the-art performance in cyber Entity Extraction, highlighting the dual strength of the framework in both low-resource and data-rich environments.By aligning the system’s outputs with the Structured Threat Information Expression (STIX) format, a standard for information exchange in the cybersecurity domain, 0-CTI standardizes extracted knowledge, enhancing communication and collaboration in cybersecurity operations.},
  keywords={Large language models;Zero shot learning;Standards organizations;Ontologies;Information retrieval;Transformers;Natural language processing;Cyber threat intelligence;Data mining;Computer security;Cyber Threat Intelligence;Natural Language Processing;Structured Threat Information Expression;Named Entity Recognition;Relation Extraction},
  doi={10.1109/FLLM63129.2024.10852465},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10849428,
  author={Adjei-Frempah, Douglas and Chen, Lisa and LePendu, Paea},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={KB2Bench: Toward a Benchmark Framework for Large Language Models on Medical Knowledge}, 
  year={2024},
  volume={},
  number={},
  pages={485-493},
  abstract={While Large Language Models (LLMs) have trans-formed question answering tasks, their propensity for hallucinations continues to drive an area of active research. Efforts toward creating benchmarks to test LLMs' performance on queries, in particular for the field of medicine, have led to a few reputable benchmarks, but these are limited in scope because of the amount of human annotation required. Our framework addresses this issue by leveraging existing, large knowledge bases for medicine to generate vast query and answer sets dynamically, which are less likely to be memorized by LLMs. The framework rests on designing a few key knowledge patterns, which can then generate millions (potentially billions) of queries. This offers a more efficient, cost-effective, and scalable alternative to human-curated annotations used in medical question-and-answer benchmarks. Applying our framework to a small sample of five drug related ontologies, we are already capable of more than 100,000 unique drug related queries, which is 10 to 1000 times larger than existing various human annotation efforts. This paper introduces the KB2Bench framework.},
  keywords={Drugs;Annotations;Large language models;Knowledge based systems;Benchmark testing;Ontologies;Question answering (information retrieval);biomedical informatics;knowledge bases;large language models;ontologies;vocabularies;knowledge representation and reasoning;benchmarking},
  doi={10.1109/ICTAI62512.2024.00075},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{10849427,
  author={Armary, Pauline and El-Vaigh, Cheikh-Brahim and Spicher, Antoine and Narsis, Ouassila Labbani and Nicolle, Christophe},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Identifying Logical Patterns in Text for Reasoning}, 
  year={2024},
  volume={},
  number={},
  pages={837-844},
  abstract={Translating unstructured text into logical format is a key challenge for building ontologies automatically and addressing deductive inference. Most of the approaches have tackled the identification of concepts and relations in text, but few of them have addressed the most complex axioms like class expression subsumption. This work proposes DeLIR, a neuro-symbolic approach to identify complex logical patterns in text by combining a grammatical translation of dependency parsing trees and a fine-tuned Large language Model (LLM). DeLIR combines the strength of the parsing accuracy provided by a grammatical approach and pattern flexibility provided by a finetuned LLM. We evaluated our approach on FOLIO dataset for both translation capacity and inference capability. Our grammatical approach has a perfect parsing accuracy and combining the grammatical approach with LLMs improves the LLMS translation capacity: tinyLlama, T5-small-text2logic, Llama-7B and Mistral-7B. We also evaluate the inference capacity of the different LLMs. Mistral-7B, while being smaller than the state-of-the-art approach using GPT-4, presents similar results to predict the correct inference labels.},
  keywords={Hands;Translation;Accuracy;Large language models;Zero shot learning;Buildings;Ontologies;Syntactics;Cognition;Ontology Learning;Translation to Logic;Natural Language Inference},
  doi={10.1109/ICTAI62512.2024.00122},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{10851643,
  author={Nedelchev, Iliya and Tabakova-Komsalova, Veneta and Stoyanov, Ivan and Stoyanov, Stanimir and Ivanova, Vanya and Kazashka, Tsvetomira},
  booktitle={2024 International Conference Automatics and Informatics (ICAI)}, 
  title={Supporting Digitization of a Cultural and Historical Heritage Platform}, 
  year={2024},
  volume={},
  number={},
  pages={490-493},
  abstract={The article introduces a platform designed for the storage and retrieval of digitized cultural and historical objects from Bulgaria. To enhance the platform’s accessibility for tourists, a dedicated tourist guide has been created and implemented as a personalized assistant. The architecture, along with its distinct components, is thoroughly elucidated. The article provides an overview of the current state of the platform, details the experiments conducted with the realized prototype, and outlines its future development, with a focus on integrating cultural and historical object ontologies. Additionally, a brief background of the platform is provided for context.},
  keywords={Electronic learning;Prototypes;Ontologies;Cultural differences;Informatics;cultural-historical heritage platform;personal assistants;tourist guide;digitalization of cultural-historical objects;ontologies},
  doi={10.1109/ICAI63388.2024.10851643},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10849391,
  author={Saini, Anmol and Ethier, Jeffrey G. and Shimizu, Cogan},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={An Ontology for Conversations with Virtual Research Assistants}, 
  year={2024},
  volume={},
  number={},
  pages={181-186},
  abstract={Conversational artificial intelligence has expanded rapidly in recent years, especially with the growth of large language models (LLMs). Its incorporation in scientific research in the form of research assistants has also become more common-place but remains limited in some capacities, such as in the realm of polymer science. The limitations of LLMs, especially in terms of domain knowledge, warrant the need for other tools, such as knowledge graphs (KGs), to better guide conversations. While such conversational models have been developed in the past, they are generally restricted to particular domains and lack the ability to integrate semantics from various kinds of conversations. Thus, we make progress toward the construction of a universal conversational model that has a focus on the materials domain by combining aspects of existing models. We aim to implement it in such a way that renders it amenable to modifications and usable in a variety of situations. We posit that this model will be adopted and extended by others seeking to accomplish a similar goal in the future.},
  keywords={Adaptation models;Limiting;Conversational artificial intelligence;Large language models;Semantics;Natural languages;Oral communication;Knowledge graphs;Ontologies;Polymers;artificial intelligence;conversational model;knowledge graph;large language model;ontology;ontology design pattern;polymer science},
  doi={10.1109/ICTAI62512.2024.00034},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{10830995,
  author={Li, Jinghong and Phan, Huy and Gu, Wen and Ota, Koichi and Hasegawa, Shinobu},
  booktitle={2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Fish-Bone Diagram of Research Issue: Gain a Bird's-Eye View on a Specific Research Topic}, 
  year={2024},
  volume={},
  number={},
  pages={4936-4941},
  abstract={Novice researchers often face difficulties in understanding a multitude of academic papers and grasping the fundamentals of a new research field. To solve such problems, the knowledge graph supporting research survey is gradually being developed. Existing keyword-based knowledge graphs make it difficult for researchers to deeply understand abstract concepts. Meanwhile, novice researchers may find it difficult to use ChatGPT effectively for research surveys due to their limited understanding of the research field. Without the ability to ask proficient questions that align with key concepts, obtaining desired and accurate answers from this large language model (LLM) could be inefficient. This study aims to help novice researchers by providing a fish-bone diagram that includes causal relationships, offering an overview of the research topic. The diagram is constructed using the issue ontology from academic papers, and it offers a broad, highly generalized perspective of the research field, based on relevance and logical factors. Furthermore, we evaluate the strengths and improvable points of the fish-bone diagram derived from this study's development pattern, emphasizing its potential as a viable tool for supporting research survey.},
  keywords={Surveys;Training;Reviews;Knowledge graphs;Machine learning;Ontologies;User interfaces;Information retrieval;Prompt engineering;Sustainable development},
  doi={10.1109/SMC54092.2024.10830995},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10838760,
  author={Dimitrakopoulos, George and Ehm, Hans and Tsaousi, Eleni},
  booktitle={2024 Winter Simulation Conference (WSC)}, 
  title={Enhanced Ontology Extraction: Integrating GPT AI with Human Knowledge on the Example of EU Standards Related to Semiconductor Supply Chains}, 
  year={2024},
  volume={},
  number={},
  pages={1955-1965},
  abstract={This paper addresses challenges in creating ontologies for the semiconductor supply chain. Ontologies are crucial for seamless data exchange within the semantic web, enabling initiatives like GAIA-X and CatenA-X. Traditionally, ontology creation is complex. Here, we propose a novel AI-assisted method using large language models (LLMs) like ChatGPT 4 Turbo to support human experts. This collaboration aims to expedite ontology generation while maintaining quality. While initial tests show promise, refining the human-AI interface for clear content generation remains a focus. By improving this collaboration, we expect to create more accurate and complete ontologies, fostering efficient information sharing and strengthening the meaningfulness of standards within the semiconductor supply chain.},
  keywords={Semantic Web;Accuracy;Supply chains;Collaboration;Information sharing;Knowledge graphs;Ontologies;Chatbots;Reliability;Standards},
  doi={10.1109/WSC63780.2024.10838760},
  ISSN={1558-4305},
  month={Dec},}@INPROCEEDINGS{10842699,
  author={Mankari, Sagar and Sanghavi, Abhishek},
  booktitle={2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)}, 
  title={Enhancing Vector based Retrieval Augmented Generation with Contextual Knowledge Graph Construction}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The proliferation of unstructured text data necessitates efficient information retrieval systems. Traditional vector-based Retrieval Augmented Generation (RAG) models often fail to capture complex relationships and contextual nuances, limiting effectiveness in knowledge-intensive tasks. We introduce Contextual Knowledge Graph Construction (CKGC), a novel approach enhancing vector-based RAG by dynamically building a knowledge graph that reflects inherent data structures and connections.CKGC leverages text chunking, large language models (LLMs), and ontology mapping. By segmenting text and using LLMs to identify key entities and relationships, CKGC constructs a contextualized knowledge graph enriching information representation. This bridges the gap between semantic similarity and deeper contextual understanding, enabling more accurate and nuanced retrieval.Experiments on 2,000 lease agreements demonstrate that CKGC significantly improves vector-based RAG in information retrieval and question answering tasks, with substantial gains in Mean Reciprocal Rank (MRR) and Top-k Accuracy. CKGC’s adaptability across domains positions it as a valuable tool for enhancing performance and understanding of complex textual data. Our findings underscore CKGC’s transformative potential in unlocking insights from vast text corpora, paving the way for more intelligent and context-aware information retrieval systems.},
  keywords={Accuracy;Large language models;Retrieval augmented generation;Semantics;Knowledge graphs;Organizations;Information retrieval;Question answering (information retrieval);Vectors;Time factors;Retrieval Augmented Generation;Knowledge Graph Construction;Information Retrieval;Ontology Mapping},
  doi={10.1109/IDICAIEI61867.2024.10842699},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10837665,
  author={Okuhara, Fumika and Egami, Shusaku and Sei, Yuichi and Tahara, Yasuyuki and Ohsuga, Akihiko},
  booktitle={2024 21st International Conference on Information Technology Based Higher Education and Training (ITHET)}, 
  title={Automatic Question Generation with Knowledge Graph for Panoramic Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={In recent years, the global social landscape has become increasingly complex, requiring the ability to think from a wide range of diverse perspectives for effective problem-solving. In the field of education, panoramic learning, which implements interdisciplinary and comprehensive education, has become essential. Also, there has been recent research on various aspects of automatic question generation (AGQ), with some studies focusing on generating panoramic questions, which provide a comprehensive understanding, across different genres using knowledge graph (KG). KG is a knowledge base that uses a graph-structured data model and consists of entities and relationships between entities. On the other hand, research on generating panoramic questions for specific subjects with educational purposes has been limited, and this study aims to address that. In this work, we specifically targeted the field of history for question generation and used complemented entities to enhance the inclusion of panoramic knowledge in the field of history. The approach involves enhancing subgraphs with link prediction, which complements missing relationships in KGs, particularly in historical contexts requiring temporal and spatial insights. Through evaluation, it was validated that the proposed method could generate questions containing more panoramic knowledge compared to existing methods.},
  keywords={Training;Hands;Accuracy;Knowledge based systems;Focusing;Knowledge graphs;Question generation;History;Problem-solving;Information technology;Panoramic Learning;Automatic Question Gen-eration;Knowledge Graph;Linked Data},
  doi={10.1109/ITHET61869.2024.10837665},
  ISSN={2473-2060},
  month={Nov},}@INPROCEEDINGS{10825070,
  author={Li, Tangrui and Zhou, Jun and Wang, Hongzheng},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Aligning Knowledge Graphs Provided by Humans and Generated by Neural Networks}, 
  year={2024},
  volume={},
  number={},
  pages={3441-3447},
  abstract={In this paper, an approach that extracts knowledge graphs (KGs) from neural networks (NNs) and aligns the generated KGs with human-provided ones is proposed for network optimization or transparency enhancement, which is achieved by leveraging Vector Symbolic Architectures (VSAs). The approach identifies entities and relations of NN’s knowledge along with the training process, which makes it a plug-and-play solution. Experiments on synthetic data showed that the matching method works on middle and small-size KGs, and tests on MNIST demonstrated that the aligned NN-generated KG could be very close to the human-provided ones. Further tests on Text2KGBench showed that the method could produce KGs from embedding generated by backbone large language models (LLM) that aligned well with human-provided labels as well.},
  keywords={Training;Codes;Large language models;Artificial neural networks;Knowledge graphs;Ontologies;Vectors;Optimization;Synthetic data;Software development management;Vector Symbolic Architecture;Knowledge Graph;Knowledge Graph Alignment},
  doi={10.1109/BigData62323.2024.10825070},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10825567,
  author={Berger, Armin and Berghaus, David and Bashir, Ali Hamza and Grigull, Lorenz and Fendrich, Lara and Lagones, Tom Anglim and Högl, Henriette and Ernst, Gundula and Schmidt, Ralf and Bascom, David and Deußer, Tobias and Bell, Thiago and Lübbering, Max and Sifa, Rafet},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Advancing Personalized Medicine: A Scalable LLM-based Recommender System for Patient Matching}, 
  year={2024},
  volume={},
  number={},
  pages={5876-5883},
  abstract={This study explores efficient algorithms to enhance user matching in Unrare.me, a novel social networking platform designed to connect individuals affected by rare diseases. Our primary objective is to develop a recommender system that identifies and suggests users with similar medical conditions, facilitating meaningful connections within these unique communities. Utilizing textual user profile data, we train sentence embedder models to generate similar embeddings for users that have rated each other high. We investigate various fine-tuning strategies, as well as a hybrid approach between a dense embedder and sparse SPLADE embeddings. Furthermore, we investigate the efficacy of various clustering algorithms, such as TopicBERT for thematic analysis, K-Means for centroid-based grouping, and Latent Dirichlet Allocation (LDA) for probabilistic topic modeling, to reduce the matching complexity and enable better scalability of the platform.},
  keywords={Measurement;Sparse approximation;Social networking (online);Biological system modeling;Computational modeling;Clustering algorithms;Performance gain;Data models;Recommender systems;Diseases;Large Language Models;Text Matching;Rare Diseases;Recommender Systems;Text Embeddings},
  doi={10.1109/BigData62323.2024.10825567},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10825608,
  author={Ho, Duy H. and Das, Udiptaman and Ho, Regina and Lee, Yugyung},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Leveraging Multi-Agent Systems and Large Language Models for Diabetes Knowledge Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={3401-3410},
  abstract={This paper presents a novel framework for constructing a diabetes-specific knowledge graph (KG) using a streamlined multi-agent system powered by Gemini-based Large Language Models (LLMs). Leveraging insights from the 2016 National Diabetes Survey (NNDS) conducted by the National Diabetes Education Program (NDEP), the framework extracts critical variables related to diagnosis, risk perception, medical advice, and self-management practices across diverse U.S. populations. By processing data from the NNDS’s extensive 94-question survey, the methodology performs adaptive ontology mapping using APIs for six major medical standards (e.g., SNOMED CT, ICD-11), ensuring semantic interoperability. Relationships between variables are identified and structured using RDF, RDFS, and OWL standards. The integration of LLMs with ontology tools like Protégé enhances automation and scalability. Results demonstrate the framework’s effectiveness in generating contextually rich and clinically relevant knowledge graphs, providing a robust foundation for advancing healthcare informatics and personalized diabetes management.},
  keywords={Surveys;Scalability;Knowledge graphs;Medical services;Ontologies;Resource description framework;Diabetes;Standards;Medical diagnostic imaging;Multi-agent systems;Knowledge Graph;Multi-Agent System;Large Language Models;Diabetes;Ontology Mapping;RDF;OWL;Healthcare Informatics;AI},
  doi={10.1109/BigData62323.2024.10825608},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10825160,
  author={Kim, Edward and Shrestha, Manil and Foty, Richard and DeLay, Tom and Seyfert-Margolis, Vicki},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Structured Extraction of Real World Medical Knowledge using LLMs for Summarization and Search}, 
  year={2024},
  volume={},
  number={},
  pages={3421-3430},
  abstract={Creation and curation of knowledge graphs at scale can be used to exponentially accelerate the discovery, matching, and analysis of diseases in real-world data. While disease ontologies are useful for annotation, integration, and analysis of biological data, codified disease and procedure categories e.g. SNOMED-CT, ICD10, CPT, etc. rarely capture all of the nuances in a patient condition or, in the case of rare disease, may not even exist. Furthermore, there are multiple disease definitions used in data sources and publications, each having its own structure and hierarchy. Mapping between ontologies, finding disease clusters, and building a representation of the chosen disease area are resource-intensive, often requiring significant human capital. We propose the creation and curation of a patient knowledge graph utilizing large language model extraction techniques. In order to expand in volume and scale, knowledge graphs with generalized language capability allow for data to be extracted using natural language rather than being constrained by the exact terminology or hierarchy of existing ontologies. We develop a method of mapping back to existing ontologies such as MeSH, SNOMED-CT, RxNORM, HPO, etc. to ground the extracted entities to known entities in the medical community.We have access to one of the largest ambulatory care EHR databases in the country. To demonstrate the effectiveness of our method, we benchmark our extraction in a test set with over 33.6M unique patients, in the area of patient search. In this case study, we perform a patient search for a rare disease: Dravet syndrome. Dravet syndrome was codified as an ICD10 recognizable disease in October 2020. In the following research, we describe our method of the construction of patient-specific knowledge graphs and subsequent searches for patients who exhibit symptoms of a particular disease. Using patients with confirmed ICD10 codes for Dravet syndrome as our ground truth, we utilize our LLM-based entity extraction techniques and formalize an algorithmic way of characterizing patients in a grounded ontology to assist in mapping patients to specific diseases. Finally, we present the results of a real-world discovery method on Beta-propeller protein-associated neurodegeneration (BPAN), identifying patients with a rare disease, where no ground truth currently exists.},
  keywords={Proteins;Translation;Annotations;Terminology;Large language models;Knowledge graphs;Ontologies;Benchmark testing;Data mining;Diseases;Large Language Models;Knowledge Graphs;Ontology Mapping;Structured Extraction;Dravet Syndrome;Beta-propeller protein-associated neurodegeneration (BPAN)},
  doi={10.1109/BigData62323.2024.10825160},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10825755,
  author={Oranekwu, Ikechukwu and Elluri, Lavanya and Batra, Gunjan},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Automated Knowledge Framework for IoT Cybersecurity Compliance}, 
  year={2024},
  volume={},
  number={},
  pages={6336-6345},
  abstract={Rapid expansion in the manufacture and use of Internet of Things (IoT) devices has introduced significant challenges in ensuring compliance with cybersecurity standards. To protect user data and privacy, all organizations providing IoT devices must adhere to complex guidelines such as the National Institute of Standards and Technology Inter agency Report (NIST IR) 8259, which defines essential cybersecurity guidelines for IoT manufacturers. However, interpreting and applying these rules from these guidelines and the privacy policies remains a significant challenge for companies. Thus, this project presents a novel approach to extract knowledge from NIST 8259 for creating semantically rich ontology mappings. Our ontology captures key compliance rules, which are stored in a knowledge graph (KG) that allows organizations to crosscheck and update privacy policy documents with ease. The KG also enables real-time querying using SPARQL and offers a transparent view of regulatory adherence for IoT manufacturers and users. By automating the process of verifying cybersecurity compliance, the framework ensures that companies remain aligned with NIST standards, eliminating manual checks and reducing the risk of non-compliance. We also demonstrate that compared to the baseline Large Language Models (LLMs), our proposed framework has more compliance accuracy, and is more efficient and scalable.},
  keywords={Privacy;Companies;Manuals;NIST;Ontologies;NIST Standards;Real-time systems;Internet of Things;Computer security;Guidelines;IoT;Cybersecurity;NIST 8259 standards;KGs;regulatory compliance;automated compliance;LLMs;privacy policies;SPARQL},
  doi={10.1109/BigData62323.2024.10825755},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10825984,
  author={Linxen, Andrea and Schmidt, Vera-Maria and Klinke, Harald and Beecks, Christian},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Ontology-driven knowledge base for digital humanities: Restructuring knowledge organization at the library of the Folkwang University of the Arts}, 
  year={2024},
  volume={},
  number={},
  pages={2449-2455},
  abstract={Academic libraries are increasingly challenged by the need to efficiently manage and analyse vast collections of data and knowledge. The divers formats and organisation methods of these collections, ranging from traditional print media to digital archives and multimedia assets, can hinder researchers’ ability to easily access and retrieve relevant information. This paper introduces an ontology-driven knowledge base to address this issue by enabling the efficient access to knowledge in the application domain and enhancing the semantic search capabilities in the field of Digital Humanities. Our approach focuses on the development of an ontology-drive knowledge base for semantic search in academic libraries by the example of the library of the Folkwang University of Arts that captures the knowledge concepts present in the library’s archival collections. The resulting ontology framework provides a structured representation of domain knowledge, facilitating the integration of diverse data sources, including structured, semi-structured, and unstructured data from the application domain into a triple store knowledge base. By leveraging SPARQL queries generated from Large Language Model (LLM) prompts, we aim to facilitate more intuitive and effective knowledge retrieval. This approach allows users to express their information needs in a more natural and flexible way, leading to more accurate and relevant search results. We evaluate the proposed ontology-driven knowledge base in terms of its integrity, consistency, flexibility, relevance, and scalability. Our evaluation methodology includes a combination of verification and validation techniques, including automated reasoners and query results based on competence questions. Our findings demonstrate the potential of ontology engineering to enhance complex information retrieval in academic libraries. However, we also identify limitations related to processing speed for complex queries and the quality of search results. This research contributes to the field of computational archival science by providing a novel approach to semantic search in academic libraries. By enabling more precise and efficient access to knowledge, our ontology-driven knowledge base has the potential to enrich the academic and Digital Humanities landscape, empowering researchers to delve deeper into the vast resources available within these institutions.},
  keywords={Knowledge engineering;Art;Semantic search;Soft sensors;Scalability;Knowledge based systems;Organizations;Ontologies;Media;Libraries;knowledge engineering;ontology framework;digital humanities;knowledge base;semantic search},
  doi={10.1109/BigData62323.2024.10825984},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10825404,
  author={Berger, Armin and Lagones, Tom Anglim and Grigull, Lorenz and Fendrich, Lara and Bell, Thiago and Högl, Henriette and Ernst, Gundula and Schmidt, Ralf and Bascom, David and Sifa, Rafet and Lübbering, Max},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Tackling Data Sparsity and Combinatorial Challenges in Rare Disease Matching with Medical Informed Machine Learning}, 
  year={2024},
  volume={},
  number={},
  pages={6430-6438},
  abstract={With over 7,000 known rare diseases and a prevalence of less than one in a thousand, rare diseases pose substantial challenges to advanced medical support networks. This study investigates the efficacy of Unrare.me, a novel social networking platform designed for individuals affected by rare diseases, including patients, their family members, and medical professionals, addressing data sparsity and combinatorial complexities in user matching. We demonstrate that simple matching heuristics already serve as a decent basis for collecting user feedback on match quality. Leveraging over 10,000 user matching feedback scores from more than 2,000 active users, we evaluate algorithms including collaborative filtering and user embedding similarity with state-of-the-art Large Language Models (LLMs). With a top-10 and top-5 hit-rate of 55% and 37%, respectively, we show that a combination of medical data augmentation and embeddings significantly enhances performance beyond the initial heuristic baseline.},
  keywords={Social networking (online);Large language models;Collaborative filtering;Machine learning;Big Data;Data augmentation;Data models;Complexity theory;Diseases;Large Language Models;Text Matching;Rare Diseases;Recommender Systems},
  doi={10.1109/BigData62323.2024.10825404},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10825705,
  author={Purohit, Sumit and Chin, George and Mackey, Patrick S and Cottam, Joseph A},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={GraphAide: Advanced Graph-Assisted Query and Reasoning System}, 
  year={2024},
  volume={},
  number={},
  pages={3485-3493},
  abstract={Curating knowledge from multiple siloed sources that contain both structured and unstructured data is a major challenge in many real-world applications. Pattern matching and querying represent fundamental tasks in modern data analytics that leverage this curated knowledge. The development of such applications necessitates overcoming several research challenges, including data extraction, named entity recognition, data modeling, and designing query interfaces. Moreover, the explainability of these functionalities is critical for their broader adoption.The emergence of Large Language Models (LLMs) has accelerated the development lifecycle of new capabilities. Nonetheless, there is an ongoing need for domain-specific tools tailored to user activities. The creation of such digital assistants has gained considerable traction in recent years, with LLMs offering a promising avenue to develop such assistants utilizing domain-specific knowledge and assumptions.In this context, we introduce an advanced query and reasoning system, GraphAide, which constructs a knowledge graph (KG) from diverse sources and allows to query and reason over the resulting KG. GraphAide harnesses both the KG and LLMs to rapidly develop domain-specific digital assistants. It integrates design patterns from retrieval augmented generation (RAG) and the semantic web to create an agentic LLM application. GraphAide underscores the potential for streamlined and efficient development of specialized digital assistants, thereby enhancing their applicability across various domains.},
  keywords={Semantic Web;Accuracy;Large language models;Scalability;Retrieval augmented generation;Semantics;Knowledge graphs;Cognition;Usability;Pattern matching},
  doi={10.1109/BigData62323.2024.10825705},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10837958,
  author={Okuhara, Fumika and Egami, Shusaku and Sei, Yuichi and Tahara, Yasuyuki and Ohsuga, Akihiko},
  booktitle={2024 IEEE 7th International Conference on Computer and Communication Engineering Technology (CCET)}, 
  title={Enhancing Panoramic Competency Through Link Prediction in Question Knowledge Graphs using a Language Representation Model}, 
  year={2024},
  volume={},
  number={},
  pages={267-272},
  abstract={Recently, panoramic knowledge has been required. On the other hand, multiple choice questions are suitable for efficient self-learning. Therefore, the purpose of this study is to create multiple choice questions that can reinforce learners' panoramic knowledge. Specifically, we proposed a method for automatically generating multiple choice questions that use Linked Data to present relevant information to give respondents an overall picture of relevant knowledge. There is some research on the methods that generated questions by extracting small subgraphs from the knowledge graphs consisting of entities(words) and relations(links) between the entities and hiding target words (correct answer words). In this study, our goal is to enhance the panoramic of the subgraphs of a specified size by using the link prediction method to complement edges and represent relationships not present in the knowledge graph when generating questions targeted at specific fields. The method of complementing edges involves first inputting two words as subject and object in Knowledge Graph to calculate the cosine similarity using a pretrained language model based on Wikipedia and Wikidata, then predicting the links as a predicate that should be complemented, and finally generating subgraphs by using the Graph Database added the complemented edges. For this study, we generated questions in the field of history, and since history requires temporal and spatial panoramic knowledge, words related to these aspects were focused on and complemented the relationships between them. As a result, 2,746 relationships were complemented by the proposed method in the subgraphs, and the subgraphs contained more words to learn (words found in textbooks that need to be learned) in a specific field compared to those generated using existing methods.},
  keywords={Hands;Databases;Linked data;Computational modeling;Knowledge graphs;Encyclopedias;Predictive models;History;Online services;Panoramic Knowledge;Linked Data;Knowledge Graph;Automatic Generate Question;Educational Application},
  doi={10.1109/CCET62233.2024.10837958},
  ISSN={2836-5992},
  month={Aug},}@INPROCEEDINGS{10822222,
  author={Yang, Hang and Xiao, Liang and Zhu, Rujun and Liu, Ziji and Chen, Jianxia},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={An LLM supported approach to ontology and knowledge graph construction}, 
  year={2024},
  volume={},
  number={},
  pages={5240-5246},
  abstract={The continuous development in the medical field faces multiple challenges in managing a large amount of literature and research results using traditional ontology and knowledge graph construction methods. These challenges include high labor costs, limited coverage, and poor dynamism of traditional ontology and knowledge graph construction methods. Large language models (LLMs) can solve various natural language processing tasks and can understand and generate human-like natural language, which makes automated construction of ontology expansion and knowledge graphs (KGs) possible. This paper proposes an ontology expansion method based on LLMs, using LLMs to formulate competency questions (CQs) to extend the initial ontology, and then constructing the knowledge graph based on the extended ontology. We demonstrated the feasibility of the method by creating a knowledge graph for breast cancer treatment. The combination of LLMs-based medical ontology and knowledge graph can achieve more efficient medical knowledge management and application, promoting the informatization and intelligent development of the medical field.},
  keywords={Semantic Web;Large language models;Refining;Knowledge graphs;Medical services;Ontologies;Natural language processing;Iterative methods;Reliability;Usability;Ontology;Knowledge graph;LLM;Breast cancer treatment},
  doi={10.1109/BIBM62325.2024.10822222},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10822279,
  author={Chen, Zhijian and Hu, Chuan and Wu, Min and Long, Qingqing and Wang, Xuezhi and Zhou, Yuanchun and Xiao, Meng},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={GeneSum: Large Language Model-based Gene Summary Extraction}, 
  year={2024},
  volume={},
  number={},
  pages={1438-1443},
  abstract={Emerging topics in biomedical research are continuously expanding, providing a wealth of information about genes and their function. This rapid proliferation of knowledge presents unprecedented opportunities for scientific discovery and formidable challenges for researchers striving to keep abreast of the latest advancements. One significant challenge is navigating the vast corpus of literature to extract vital gene-related information, a time-consuming and cumbersome task. To enhance the efficiency of this process, it is crucial to address several key challenges: (1) the overwhelming volume of literature, (2) the complexity of gene functions, and (3) the automated integration and generation. In response, we propose GeneSum, a two-stage automated gene summary extractor utilizing a large language model (LLM). Our approach retrieves and eliminates redundancy of target gene literature and then fine-tunes the LLM to refine and streamline the summarization process. We conducted extensive experiments to validate the efficacy of our proposed framework. The results demonstrate that LLM significantly enhances the integration of gene-specific information, allowing more efficient decision-making in ongoing research.},
  keywords={Navigation;Biological system modeling;Large language models;Redundancy;Decision making;Complexity theory;Data mining;Bioinformatics;Gene summary;prompt learning;large language model},
  doi={10.1109/BIBM62325.2024.10822279},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10822688,
  author={Chen, Dehua and Shen, Zijian and Wang, Mei and Dong, Na and Pan, Qiao and Su, Jianwen},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Extracting Structure Information from Narrative Medical Reports based on LLMs}, 
  year={2024},
  volume={},
  number={},
  pages={5616-5623},
  abstract={Extracting structured information and key details from medical report narratives is crucial to support healthcare data management, analysis and decision-making. However, the specialized nature of the reports, the complexity of the contents, and the high accuracy requirements of the results pose significant challenges to the structuring task. In this paper, we develop an LLM-based method to extract structure information from medical report narratives. Defining the structuring problem as mapping the narrative reports to the domain ontology, we design a framework to develop specialized LLMs that automatically learn and establish the mappings. At the core of this framework are report partitioning and interactive training data generation modules are. By separating complete reports into logically independent segments and training the LLMs on these segments independently, the trained LLMs can accurately capture the semantic relationships within each segment. Additionally, we explore different LLMs and formulate a simplistic scoring method to compare their accuracy, enabling us to select the best-performing model. Experimental evaluation on a real-world breast ultrasound report dataset demonstrates that our method achieves high accuracy with a small training dataset (400 samples). Specifically, the accuracy of structural information extraction and the attribute-value matching accuracy both exceed 96%.},
  keywords={Training;Accuracy;Ultrasonic imaging;Semantics;Training data;Medical services;Ontologies;Information retrieval;Data mining;Standards;Medical examination reports;Large language models;Report structuring},
  doi={10.1109/BIBM62325.2024.10822688},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10822556,
  author={Qi, Jiewei and Luo, Ling and Yang, Zhihao and Wang, Jian and Zhou, Huiwei and Lin, Hongfei},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={An Improved Method for Phenotype Concept Recognition Using Rich HPO Information}, 
  year={2024},
  volume={},
  number={},
  pages={1135-1140},
  abstract={Automatically identifying human phenotype ontology (HPO) concepts from text is important for disease analysis. Existing ontology-driven methods for phenotype concept recognition mainly rely on concept names and synonym information from the ontology, without fully exploiting the rich ontology information. In this paper, we present an improved phenotype concept recognition method by incorporating rich HPO information. We first design prompts with HPO information and use a cutting-edge large language model GPT-4 to generate synonym augmentation for expanding distant supervised training data. We then propose an ontology vector-enhanced phenotype concept classification model to efficiently integrate the taxonomic hierarchical structure of HPO. Additionally, we employ noisy data augmentation to improve the model’s recognition ability in noisy texts and implement a negation detection function. Experimental results on three standard corpora and two typo corpora show our method compares favorably to previous methods and achieves a significant improvement in noisy texts. The source code and data are freely available at https://github.com/DUTIR-BioNLP/PhenoTagger-Updates.},
  keywords={Phenotypes;Source coding;Large language models;Training data;Ontologies;Data augmentation;Noise measurement;Reliability;Standards;Diseases;Phenotype Concept Recognition;Human Phenotype Ontology;Ontology Information Enhancement},
  doi={10.1109/BIBM62325.2024.10822556},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10817901,
  author={Sugioka, Koki and Kamei, Sayaka and Morimoto, Yasuhiko},
  booktitle={2024 Twelfth International Symposium on Computing and Networking Workshops (CANDARW)}, 
  title={BERT Pre-training for Cooking Time Prediction from Cooking Recipes}, 
  year={2024},
  volume={},
  number={},
  pages={157-162},
  abstract={Recently, websites that allow ordinary users to share and search for recipes have become popular. Typically, each recipe contains various information such as a title, a list of ingredients, and descriptions of the cooking process through text and photos. The estimated cooking time is another valuable piece of information when selecting a recipe. However, it can be difficult to accurately describe cooking time because it depends on the cooking environment and conditions, such as heat level and quantity. Therefore, some recipes do not include cooking time information. In this study, we consider the prediction of cooking time in general cases from the list of ingredients and the textual description of the cooking process of each recipe by BERT, a natural language processing model. For this purpose, we propose an additional pre-training method that weights words related to cooking time from a cooking ontology. Our experimental results show that our methods outperform a fine-tuned BERT model with additional pre-training by a standard method.},
  keywords={Heating systems;Conferences;Computational modeling;Ontologies;Predictive models;Natural language processing;Standards;Cooking recipes;BERT;domain-adaptive pre-training},
  doi={10.1109/CANDARW64572.2024.00032},
  ISSN={2832-1324},
  month={Nov},}@INPROCEEDINGS{10818303,
  author={Anaguchi, Fumikatsu and Chakraborty, Sudesna and Morita, Takeshi and Egami, Shusaku and Ugai, Takanori and Fukuda, Ken},
  booktitle={2024 Twelfth International Symposium on Computing and Networking (CANDAR)}, 
  title={Reasoning and Justification System for Domestic Hazardous Behaviors Based on Knowledge Graph of Daily Activities and Retrieval-Augmented Generation}, 
  year={2024},
  volume={},
  number={},
  pages={11-20},
  abstract={Accidents among people over 65 years of age predominantly occur within residential settings, making the maintenance of a safe home environment a crucial social issue. To address this issue, previous research has developed systems that construct Knowledge Graphs (KG) based on simulations of daily household activities, and studies have been conducted on detecting hazardous behaviors using such KG analysis. In this current study, we propose a system capable of presenting the reason and justification for the detected domestic hazardous behaviors. Our system will first generates the reason for the detected behavior using a Large Language Model (LLM). To ensure the accuracy, reliability and reproducibility of the LLM output, the system will provides reliable sources to support the output. We employed Retrieval-Augmented Generation (RAG) to search for sentences similar to the reason generated by the LLM within reliable, authoritative documents describing domestic accident cases and their causes and these will be presented as the evidence alongside the search engine results to the users. Consequently, a knowledge graph (KG) of domestic hazardous behavior is developed based on evidence ontology. Finally, to evaluate the ability of our proposed system in appropriately generating reasons for domestic hazardous behaviors and the adequacy of the justifications provided, the output was rated using LLMs and human volunteers. The rating results showed a significant correlation between LLMs and human evaluation, indicating that the proposed system can provide sufficient reasons and justifications for domestic hazardous behaviors at residential setting.},
  keywords={Correlation;Retrieval augmented generation;Knowledge graphs;Search engines;Ontologies;Reproducibility of results;Behavioral sciences;Sensors;Optimization;Accidents;Retrieval-Augmented Generation;Large Language Model;Explainable AI;Knowledge Graph},
  doi={10.1109/CANDAR64496.2024.00010},
  ISSN={2379-1896},
  month={Nov},}@INPROCEEDINGS{10814501,
  author={Sadlek, Lukáš and Husák, Martin and Čeleda, Pavel},
  booktitle={2024 20th International Conference on Network and Service Management (CNSM)}, 
  title={Hierarchical Modeling of Cyber Assets in Kill Chain Attack Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Cyber threat modeling is a proactive method for identifying possible cyber attacks on network infrastructure that has a wide range of applications in security assessment, risk analysis, and threat exposure management. Popular modeling methods are kill chains and attack graphs. Kill chains divide attacks into phases, and attack graphs depict attack paths. A difficult issue is how to hierarchically model categories of cyber assets that should be used in threat models due to the variety of cyber systems in the current networks. This task should be addressed to provide automation of realistic threat modeling and interoperability with public knowledge bases, such as MITRE ATT&CK. In this paper, we propose a hierarchical modeling methodology for representing cyber assets in kill chain attack graphs. We illustrate its practical application on MITRE D3FEND’s Digital Artifact Ontology. Moreover, we define how cyber assets with related attack techniques should be transformed into logical facts and attack rules. We implemented proof-of-concept software modules that can process data obtained from network and host-based monitoring together with attack rules to generate attack graphs. We evaluated the approach with data from a cyber exercise captured in a network of a digital twin organization. The results show that the approach is applicable in real-world networks and can reveal ground-truth attacks.},
  keywords={Threat modeling;Automation;Large language models;Knowledge based systems;Organizations;Ontologies;Software;Security;Risk analysis;Monitoring;attack graph;kill chain;cyber threat scenario;MITRE ATT&CK;MITRE D3FEND},
  doi={10.23919/CNSM62983.2024.10814501},
  ISSN={2165-963X},
  month={Oct},}@INPROCEEDINGS{10810521,
  author={Sophaken, Chotanansub and Vongpanich, Kantapong and Intaphan, Wachirawit and Utasri, Tharathon and Deepho, Chutamas and Takhom, Akkharawoot},
  booktitle={2024 8th International Conference on Information Technology (InCIT)}, 
  title={Leveraging Graph-RAG for Enhanced Diagnostic and Treatment Strategies in Dentistry}, 
  year={2024},
  volume={},
  number={},
  pages={606-611},
  abstract={This paper presents a method for extracting and interpreting information from diverse, unstructured dental literature using advanced AI techniques. By integrating information extraction, ontologies, and knowledge graphs, the approach enhances the efficiency and accuracy of dental data analysis. Named Entity Recognition (NER) and a Large Language Model (LLM) are employed to extract relevant entities and relationships, which are then structured into triples and integrated with a dental ontology to ensure contextual relevance. This enriched ontology supports Retrieval-Augmented Generation (RAG) applications, enabling advanced querying and analysis. The methodology improves the identification and categorization of dental conditions, treatments, and anatomical terms, providing a structured representation of dental knowledge. Knowledge graphs facilitate the representation and analysis of relationships between entities, fostering insightful interpretations and supporting hypothesis generation, thereby enhancing the accessibility and usability of dental knowledge. Experimental results demonstrate the effectiveness of this approach in managing complex dental information, showcasing the benefits of combining Knowledge Representation (KR) with Machine Learning (ML). This research contributes to dental studies by offering a robust framework for extracting and utilizing knowledge from diverse and extensive datasets.},
  keywords={Large language models;Retrieval augmented generation;Knowledge graphs;Named entity recognition;Machine learning;Ontologies;Dentistry;Data mining;Usability;Information technology;Information Extraction;Dental Literature;Large Language Models;Ontologies;Knowledge Graphs;Oral Health;Dentistry},
  doi={10.1109/InCIT63192.2024.10810521},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10810102,
  author={Henry, Matthew Martianus and Heryanto, Nur Adhianti and Isnan, Mahmud and Nugroho, Kuncahyo Setyo and Pardamean, Bens},
  booktitle={2024 9th International Conference on Information Technology and Digital Applications (ICITDA)}, 
  title={Automatic Multiple Choice Question Generation: A Systematic Literature Review}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Despite their drawbacks, multiple-choice questions (MCQ) have been widely used to assess the students' understanding of lectures through examinations. The development of automatic MCQ generation is beneficial, especially for educators. As a starting point for further development, a Systematic Literature Review (SLR) is conducted to uncover current trends, future challenges, and opportunities in automatic MCQ generation. Previously, an SLR was conducted, but it lacks coverage of the utilization of transformer-based models. This SLR covers the development of automatic MCQ generation using either traditional or advanced approaches such as Transformers. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses framework was used to gather the data from Scopus, IEEE Xplore, SpringerLink, arXiv, and Semantic Scholar. The included articles must be open-access computer science conference papers or journal articles and written in English less than five years ago. Four independent reviewers analyzed the research workflow, evaluation metric, and dataset used in each study. There are 18 included studies, where 17% (n = 3) studies are from 2024, 33% (n = 6) studies are from 2023, 22% (n = 4) studies are from 2022, 11% (n = 2) studies are from 2021, and 17% (n = 3) studies are from 2020. There are 33% (n = 6) of the studies used the traditional feature-based engineering approach, 39% (n = 7) of the studies used the Transformer-based model fine-tuning approach, and the remaining used novel approaches. The study found that BERT variants are the most utilized Transformer-based model in automatic MCQ. The research notes some challenges, but also open various opportunities for further research, including Large Language Model (LLM) utilization for automatic MCQ generation, the utilization BERT-based models for standardized machine-learned evaluation metrics, and the initiative for the creation of an MCQ dataset benchmark.},
  keywords={Measurement;Semantics;Reinforcement learning;Benchmark testing;Ontologies;Transformers;Question generation;Prompt engineering;Standards;Systematic literature review;Multiple choice question;question generation;systematic literature review;Transformer model},
  doi={10.1109/ICITDA64560.2024.10810102},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10801853,
  author={Cornelio, Cristina and Diab, Mohammed},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery}, 
  year={2024},
  volume={},
  number={},
  pages={12435-12442},
  abstract={Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor’s logical rules accurately detect all failures in the analyzed tasks, and that Recover considerably outperforms, for both failure detection and recovery, a baseline method reliant solely on LLMs. Supplementary material, including the OntoThor ontology, is available at: https://recover-ontothor.github.io.},
  keywords={Costs;Large language models;Ontologies;Intelligent robots},
  doi={10.1109/IROS58592.2024.10801853},
  ISSN={2153-0866},
  month={Oct},}@INPROCEEDINGS{10802273,
  author={Nakajima, Haru and Miura, Jun},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Combining Ontological Knowledge and Large Language Model for User-Friendly Service Robots}, 
  year={2024},
  volume={},
  number={},
  pages={4755-4762},
  abstract={Lifestyle support through robotics is an increasingly promising field, with expectations for robots to take over or assist with chores like floor cleaning, table setting and clearing, and fetching items. The growth of AI, particularly foundation models, such as large language models (LLMs) and visual language models (VLMs), is significantly shaping this sector. LLMs, by facilitating natural interactions and providing vast general knowledge, are proving invaluable for robotic tasks. This paper focuses on the benefits of LLMs for "bring-me" tasks, where robots fetch specific items for users, often based on ambiguous instructions. Our previous efforts utilized an ontology extended to handle environmental data to resolve such ambiguities, but faced limitations when unresolvable ambiguities required user intervention for clarity. Here, we enhance our approach by integrating LLMs for providing additional commonsense knowledge, pairing it with ontological data to mitigate the issue of hallucinations and reduce the need for user queries, thus improving system usability. We present a system that merges these knowledge bases and assess its efficacy on "bring-me" tasks, aiming to provide a more seamless and efficient robotic assistance experience.},
  keywords={Visualization;Service robots;Foundation models;Large language models;Knowledge based systems;Ontologies;Usability;Intelligent robots;Floors;Commonsense reasoning},
  doi={10.1109/IROS58592.2024.10802273},
  ISSN={2153-0866},
  month={Oct},}@INPROCEEDINGS{10803481,
  author={Li, Haotian and Xia, Congmin and Hou, Youjuan and Hu, Sile and Quan, Jiang and Liu, Yanjun},
  booktitle={2024 IEEE International Conference on Medical Artificial Intelligence (MedAI)}, 
  title={TCMRD-KG: Design and Development of a Rheumatism Knowledge Graph Based on Ancient Chinese Literature}, 
  year={2024},
  volume={},
  number={},
  pages={588-593},
  abstract={The use of Traditional Chinese medicineTCM in rheumatic diseases dates back to thousands of years ago. Compared with standardized treatment, TCM has the advantages of low cost, low side effects, and flexible medication. Ancient books of traditional Chinese medicine play an important role in clinical and scientific research. This study takes the content related to rheumatism in ancient books of traditional Chinese medicine as the research object, integrates the ontology theory and technology in the knowledge graph, realizes the reconstruction of traditional Chinese medicine information knowledge, and provides basic data structure for data mining and knowledge discovery. This study is the first rheumatism-specific knowledge graph constructed based on ancient books of traditional Chinese medicine; it has tried the construction method of knowledge graph of ancient books of traditional Chinese medicine by combining automatic labeling of mainstream large language models with manual review; and according to the knowledge characteristics of ancient books of traditional Chinese medicine, the existing word segmentation technology is difficult to accurately reproduce the accurate meaning of the original text of ancient books, a new type of entity extraction method is given.},
  keywords={Reviews;Large language models;Knowledge graphs;Manuals;Ontologies;Knowledge discovery;Data structures;Labeling;Data mining;Diseases;Traditional Chinese Medicine;Rheumatic Diseases;Knowledge Graph},
  doi={10.1109/MedAI62885.2024.00083},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10800417,
  author={Chow, Sabrina and Guo, Lilian and Chow, Jonathan and Chia, Chelsea and Li, Sarah and Huang, Dong-Yan},
  booktitle={2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP)}, 
  title={Semantic Search Using LLM-Aided Topic Generation on Knowledge Graphs for Paper Discovery}, 
  year={2024},
  volume={},
  number={},
  pages={353-357},
  abstract={The exponential growth of academic papers presents a huge challenge for researchers, exacerbating the already tedious literature review process. Current tools like Google Scholar and Connected Papers offer solutions for text-based and citation-based searches but fail to address the need for finding semantically similar yet terminologically different papers efficiently. This paper proposes an innovative approach to paper discovery using semantic search to create a knowledge graph of topics and papers. By generating a tree of topics using ChatGPT 4o and calculating semantic similarity with SciBERT, this method aims to uncover relevant papers overlooked by traditional citation-based searches. The solution, validated through quantitative evaluation, demonstrates the potential to improve the efficiency and comprehensiveness of paper discovery.},
  keywords={Semantic search;Navigation;Bibliographies;Focusing;Knowledge graphs;Chatbots;Rendering (computer graphics);Internet;Semantic Search;Knowledge Graphs;Literature Review;Natural Language Processing (NLP);SciBERT},
  doi={10.1109/ISCSLP63861.2024.10800417},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10800180,
  author={Wang, Changlong and Sang, Xiujuan and Wang, Xijie and Gao, Yuan and Liu, Yi},
  booktitle={2024 7th International Conference on Machine Learning and Natural Language Processing (MLNLP)}, 
  title={Research on Knowledge Graph Extraction Methods for Chinese STEM Curriculum}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={STEM education, as an innovative teaching model, has gained widespread attention in recent years. However, the lack of relevant textbooks and learning resources has made its implementation challenging. Developing interdisciplinary knowledge graphs tailored for STEM education has become an urgent issue. To address this, a knowledge extraction framework named Llms4edu is proposed, which utilizes a series of effective prompts to guide large language models in knowledge extraction. Specifically, the knowledge extraction task is transformed into multiple rounds of question-and-answer interactions with the LLM, gradually identifying entity-relation triplets from subject data. Through experiments, an F1-score of 89.4% was achieved on the named entity recognition task in the chemistry subject, and an F1-score of 66.7% on the relation extraction task. Finally, a subject ontology model was built for subject text, and a subject data set was constructed using Llms4edu, which includes three subjects of junior high school mathematics, physics, and chemistry, a total of 2,511 entities, 2,010 relationship triples, and cross-disciplinary knowledge is linked to construct a cross-disciplinary knowledge graph.},
  keywords={Knowledge engineering;Training;Chemistry;Annotations;Large language models;Knowledge graphs;Named entity recognition;Ontologies;Data mining;Physics;interdisciplinary knowledge graph;large language model;prompt engineering},
  doi={10.1109/MLNLP63328.2024.10800180},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10800729,
  author={Lee, Chang-Shing and Wang, Mei-Hui and Tseng, Guan-Ying and Yue, Chao-Cyuan and Hsieh, Hao-Chun and Reformat, Marek},
  booktitle={2024 27th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)}, 
  title={Cao Robot for Taiwanese/English Knowledge Graph Application}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper proposes a Content Attention Ontology (CAO) robot for constructing Taiwanese/English Knowledge Graphs (KGs) by prompting audio or texts to Large Language Models (LLMs), including TAIDE, Zephyr, and Llama 3.1. The collected data includes lecture videos from the IEEE WCCI 2024 in Japan and the 2024 National Language Development Forum in Taiwan, along with students' learning data from the 2024 Summer School on Taiwanese/English Human and Robot Co-Learning at Rende Elementary School (RDES). In addition, the fundamental concepts of Computational Intelligence (CI) and Quantum CI (QCI) learning were incorporated into the study. The generative KGs highlight important concepts, relations, and communities within the collected teaching and learning data. Additionally, we utilized data from subjects wearing braincomputer interface (BCI) devices while speaking Taiwanese/English to generate KGs. We also compared the differences in these KGs and analyzed the similarities between the transcribed texts of lectures and learners. In the future, we plan to expand the CAO robot to more validation fields across Taiwan, aiming to engage young students in speaking Taiwanese while concurrently enhancing their English language skills through interaction with the robot.},
  keywords={Measurement;Quantum computing;Statistical analysis;Large language models;Knowledge graphs;Speech enhancement;Ontologies;Physiology;Robots;Videos;CAO Robot;Knowledge Graph;Taiwanese/English Language Co-learning;Large Language Model;Llama 3.1;TAIDE},
  doi={10.1109/O-COCOSDA64382.2024.10800729},
  ISSN={2472-7695},
  month={Oct},}@INPROCEEDINGS{10801278,
  author={Krouwel, Marien R. and Mulder, Mark A.T.},
  booktitle={2024 26th International Conference on Business Informatics (CBI)}, 
  title={Revising the DEMO method: modelling wait links}, 
  year={2024},
  volume={},
  number={},
  pages={188-197},
  abstract={The DEMO method and modelling language for enterprises has evolved over the past 30 years. Extensive work has been done to specify the modelling language to create DEMO based modelling tools and code generators. However, many issues have been identified regarding the adoption, readability, and completeness of DEMO specification.In this paper, we focus on the Process Model that has several issues regarding its relation with the Action Model and wait links. The proposed solution encompasses adjusting the visualisation of the fact kinds in the Process Model so that they can directly be linked to the event part of the Action Rules Specifications in the Action Model. Some other suggestions for improving the comprehensibility of the Process Model and the impact on DEMO are included. The proposed solution is illustrated with three cases and evaluated against the expected benefits. Preliminary results show an improved readability and easier creation of the Action Model that comply with the Process Model. More research is needed to validate the proposed solution with experts and to solve other issues regarding DEMO.},
  keywords={Visualization;Codes;Symbols;Generators;Informatics;Business;DEMO;Action Model;Process Model;Enterprise Ontology;Enterprise Implementation;Enterprise Design},
  doi={10.1109/CBI62504.2024.00030},
  ISSN={2378-1971},
  month={Sep.},}@INPROCEEDINGS{10782681,
  author={Ye, Bowei and Liang, Jie},
  booktitle={2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
  title={Predicting Functional Surface Topographies Combining Topological Data Analysis and Deep Learning Across the Human Protein Universe}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Characterizing geometric and topological properties of protein structures encompassing surface pockets, interior cavities, and cross channels is important for understanding their functions. Our knowledge of protein structures has been greatly advanced by AI-powered structure prediction tools, with AlphaFold2 (AF2) providing accurate 3D structure predictions for most protein sequences. Nonetheless, there is a substantial lack of function annotations and corresponding functional surface topographical information. We develop a method to predict functional pockets, along with their associated Gene Ontology (GO) terms and Enzyme Commission (EC) numbers, for a set of 65,013 AF2-predicted human non-singleton representative structures, which can be mapped to 186,095 "non-fragment" AF2-predicted human protein structures. The identification of functional pockets, along with their respective GO terms and EC numbers, is achieved by combining topological data analysis and the deep learning method of DeepFRI. All predicted functional pockets for these 65,013 AF2-predicted human representative structures are accessible at: https://cfold.bme.uic.edu/castpfold.},
  keywords={Proteins;Deep learning;Knowledge engineering;Enzymes;Data analysis;Three-dimensional displays;Databases;Annotations;Ontologies;Surface topography},
  doi={10.1109/EMBC53108.2024.10782681},
  ISSN={2694-0604},
  month={July},}@INPROCEEDINGS{10782119,
  author={Munzir, Syed I. and Hier, Daniel B. and Carrithers, Michael D.},
  booktitle={2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
  title={High Throughput Phenotyping of Physician Notes with Large Language and Hybrid NLP Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Deep phenotyping is the detailed description of patient signs and symptoms using concepts from an ontology. The deep phenotyping of the numerous physician notes in electronic health records requires high throughput methods. Over the past 30 years, progress toward making high-throughput phenotyping feasible. In this study, we demonstrate that a large language model and a hybrid NLP model (combining word vectors with a machine learning classifier) can perform high throughput phenotyping on physician notes with high accuracy. Large language models will likely emerge as the preferred method for high throughput deep phenotyping physician notes.Clinical relevance: Large language models will likely emerge as the dominant method for the high throughput phenotyping of signs and symptoms in physician notes},
  keywords={Accuracy;Large language models;Biological system modeling;Medical services;Machine learning;Ontologies;Assistive technologies;Throughput;Vectors;Electronic medical records},
  doi={10.1109/EMBC53108.2024.10782119},
  ISSN={2694-0604},
  month={July},}@INPROCEEDINGS{10773797,
  author={Piazza, Nancirose and Upadhayay, Bibek and Scarpa, Ronald and Behzadan, Vahid},
  booktitle={MILCOM 2024 - 2024 IEEE Military Communications Conference (MILCOM)}, 
  title={Large Language Models for Automatic Standardization of Cyber Deception Plans based on the Adversary Engagement Ontology}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Adversary Engagement Ontology (AEO) is a candidate ontology for the Unified Cyber Ontology (UCO), a community effort aimed at ontological standardization of cyber domain concepts and objects under a unifying framework. It forms a part of the Cyber Domain Ontology (CDO). In the past, community efforts and development have always been labor-intensive with regards to changes in ontology, example generation for adopters, and documentation generation. Large Language Models (LLMs), such as Claude-3.5-Sonnet and GPT4, have been proven capable of automating many tasks and aiding in human expert decision-making. Additionally, LLMs have been used in code interpretation, generation, and evaluation with efficiency and accuracy comparable to that of humans. This emergent capability of LLMs has led to the advantage of using LLMs to streamline the process of ontology development. Motivated by the aforementioned-approaches, we aim to demonstrate how these foundational LLMs can assist in ontology example generation and development, as well as be utilized to automate structured, albeit tedious tasks.},
  keywords={Military communication;Codes;Accuracy;Large language models;Decision making;Standardization;Documentation;Ontologies;Natural language processing;Adversary Engagement;Ontology},
  doi={10.1109/MILCOM61039.2024.10773797},
  ISSN={2155-7586},
  month={Oct},}@INPROCEEDINGS{10773919,
  author={Mohsenzadegan, Kabeh and Tavakkoli, Vahid and Kambale, Witesyavwirwa Vianney and Kyamakya, Kyandoghere},
  booktitle={2024 Sensor Data Fusion: Trends, Solutions, Applications (SDF)}, 
  title={A Hybrid AI Framework Integrating Ontology Learning, Knowledge Graphs, and Large Language Models for Improved Data Model Translation in Smart Manufacturing and Transportation}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Interoperability among diverse data standards is crucial for advancing digital technologies in smart manufacturing and transportation. This paper studies and presents a hybrid AI framework that integrates Ontology Learning (OL), Knowledge Graphs (KGs), and Large Language Models (LLMs) to enhance data translation across different standards. Focusing on IEEE 1451, ISO 15926, and IEC 61499, which exemplify the challenges of translating between distinct data models, we evaluate the performance of OL, KGs, and LLMs in terms of accuracy, scalability, efficiency, robustness, and flexibility. The findings indicate that the hybrid framework effectively leverages OL for semantic structuring, KGs for relational modeling, and LLMs for linguistic and contextual processing. This integration significantly improves the accuracy and adaptability of data translations, offering a comprehensive solution tailored to the complex environments of smart manufacturing and transportation, thereby advancing cross-standard data interoperability.},
  keywords={Accuracy;Large language models;Transportation;Data integration;Knowledge graphs;Ontologies;Data models;Standards;Interoperability;Smart manufacturing;Data Model Translation;Ontology Learning (OL);Knowledge Graphs (KGs);Large Language Models (LLMs);Smart Manufacturing;Data Interoperability;AI in Transportation;Hybrid AI Framework;Semantic Mapping;Cross-Standard Data Integration},
  doi={10.1109/SDF63218.2024.10773919},
  ISSN={2473-7666},
  month={Nov},}@INPROCEEDINGS{10772514,
  author={Li, Yi and Tian, Liwei and Yi, Chengyi and Li, Jingjing and Qin, Xiaodong and He, Yuxuan and Su, Huai},
  booktitle={2024 6th International Conference on System Reliability and Safety Engineering (SRSE)}, 
  title={A Large Language Model Based Knowledge Mining Method for Improving the Reliability of Fire Water Systems}, 
  year={2024},
  volume={},
  number={},
  pages={410-413},
  abstract={The fire water system plays a critical role in protecting both infrastructure and human lives. An essential aspect of enhancing the reliability of this system is fault diagnosis. However, the current fault diagnosis methods primarily rely on data-driven approaches, which often result in a high threshold for application due to their lack of interpretability. To tackle this challenge, this paper introduces a novel approach based on large language models for knowledge mining from textual data to extract fault information related to the fire water system, thereby enhancing the interpretability of data-driven fault diagnosis methods. The methodology followed in this paper consists of two main steps: firstly, analyzing the characteristics and principles of fire water system faults to develop a fault ontology, and secondly, creating a knowledge mining model using a large language model guided by the established fault ontology. Experimental findings indicate that the proposed model achieves an F1 score of 0.944, meeting the necessary criteria for effective knowledge mining in fire water system fault analysis. Furthermore, a comparative experiment was conducted to evaluate the performance of various encoder models, including GRU, BiGRU, LSTM, BiLSTM, and pre-trained large language model BERT. The results revealed a significant improvement in performance with the BERT encoder, showing increases in F1 scores of $22.12 \%$, $2.27 \%, 17.41 \%$, and $3.16 \%$ compared to the other models, respectively. This study provides valuable interpretative insights that can enhance the engineering applicability and reliability of data-driven fault diagnosis methods in fire water system.},
  keywords={Fault diagnosis;Water;Analytical models;Accuracy;Large language models;Bidirectional control;Ontologies;Reliability engineering;Encoding;Data mining;large language model;system reliability;fire water system;safety engineering;knowledge mining},
  doi={10.1109/SRSE63568.2024.10772514},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10758538,
  author={Chusova, Alina and Artemieva, Irina and Chusov, Andrey},
  booktitle={2024 IEEE International Multi-Conference on Engineering, Computer and Information Sciences (SIBIRCON)}, 
  title={A Hybrid Approach to Extraction of Knowledge From Scientific Texts Based on Large Language Models and Domain Dictionaries}, 
  year={2024},
  volume={},
  number={},
  pages={266-271},
  abstract={The vast information landscape of the Internet constitute a significant challenge for extracting valuable content. The lack of standardized data models and structures necessitates ad hoc solutions, often requiring expert knowledge that developers may lack. While Large Language Models (LLMs) hold promise for addressing this challenge, their susceptibility to AI hallucinations and inaccuracies necessitates ongoing research. This paper introduces a hybrid approach for extracting information on computational methods that combines domain-specific dictionaries with LLMs in order to improve the accuracy of method categorization. Our system incorporates explainability, allowing users to understand the reasoning behind method assignments. Furthermore, user-driven training is facilitated by allowing users to select theories and highlight relevant keywords, enhancing learning capabilities of the system. Its implementation demonstrates the effectiveness of this approach, achieving an impressive Fl-score of up to 90.3 %. This research contributes to the ongoing effort to develop robust and accurate knowledge extraction systems for navigating the ever-expanding landscape of online information.},
  keywords={Training;Accuracy;Dictionaries;Navigation;Large language models;Computational modeling;Ontologies;Acoustics;Software;Data mining;Large Language Model;Dictionary;Classilication;Acoustic models},
  doi={10.1109/SIBIRCON63777.2024.10758538},
  ISSN={2995-0996},
  month={Sep.},}@INPROCEEDINGS{10754778,
  author={Govindharajan, Hariharan and Vijayakumar, Senthilkumar},
  booktitle={2024 IEEE 15th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)}, 
  title={A Framework for automated selective Fine-Tuning of Domain-Specific Large Language Models Using Graph-Based Retrieval Augmented Generation}, 
  year={2024},
  volume={},
  number={},
  pages={431-439},
  abstract={Graph based retrieval augmented generation technique in Large Language Model (LLM) brings in major advantages by providing deep context to LLMs through relational knowledge graph for text generation, classification, question and answering use cases. However, maintaining vast data volume of domain specific data in a knowledge graph with complex relationships and querying from it every time a prompt is being posted to LLM, is a time consuming and expensive process. This paper presents a novel framework for selectively fine-tuning domain-specific large language models (LLMs) using a multi-stage Knowledge Graph (KG) based Retrieval Augmented Generation (RAG) pipeline and an Automated Incremental Fine-tuning System (AIFS). The proposed system aims to enhance the accuracy and relevance of LLM responses for text generation and Question Answering use cases by finetuning the LLM incrementally based on highly sought and highly relevant information in knowledge graph identified by leveraging page rank algorithm in KG. The framework comprises three major subsystems: Knowledge Graph Generation, Automated Incremental fine-tuning system (AIFS), and Domain Based Information Retrieval (DBIR). The effectiveness of the system is demonstrated through its ability to incrementally fine-tune LLMs based on selected highly relevant nodes within the KG, thereby improving the model’s domain-specific knowledge, response accuracy by 90% and reduce cost by 71.8%.},
  keywords={Accuracy;Costs;Large language models;Pipelines;Knowledge graphs;Information retrieval;Mobile communication;Question answering (information retrieval);Artificial Intelligence(AI);Knowledge Graph(KG);Large Language Models(LLM);LLM Fine-tuning;Contextual Information Extraction &Retrieval Systems},
  doi={10.1109/UEMCON62879.2024.10754778},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10749735,
  author={Casalicchio, Emiliano and Cotumaccio, Alberto},
  booktitle={2024 IEEE International Conference on Cloud Engineering (IC2E)}, 
  title={AI-CRAS: AI-driven Cloud Service Requirement Analysis and Specification}, 
  year={2024},
  volume={},
  number={},
  pages={11-21},
  abstract={Automated analysis and specification of software requirements expressed in natural language is a challenge addressed by the research community and is becoming a reality thanks to the advances in Artificial Intelligence (AI) and Natural Language Processing (NLP) techniques. While the research community focuses mainly on generic software requirements or specialized solutions for security requirements, we find a gap in the automation of analysis and specification for requirements in the cloud computing domain and the automatic mapping of requirements on actual products offered in the cloud service market. In this research work, we propose AI-CRAS an AI-driven cloud service requirement analysis and specification methodology. The proposed method, which leverages state-of-the-art transformer-based large language model, has been implemented and validated in a real case. Experimental results demonstrate that the model performed well in binary and multi-label classification of requirements (achieving recall/F1-score of $0.96 / 0.92$ and $0.86 / 0.76$, respectively) and mapping requirements into actual cloud services.},
  keywords={Training;Cloud computing;Accuracy;Feature extraction;Transformers;Natural language processing;Software;Vectors;Stakeholders;Testing;cloud computing;cloud migration;cloud services;requirement engineering;artificial intelligence;natural language processing;cloud service broker},
  doi={10.1109/IC2E61754.2024.00009},
  ISSN={2694-0825},
  month={Sep.},}@INPROCEEDINGS{10743881,
  author={Zhang, Kaiwen and Su, Feiyu and Huang, Yixiang and Li, Yanming and Wu, Fengqi and Mao, Yuhan},
  booktitle={2024 9th International Conference on Intelligent Computing and Signal Processing (ICSP)}, 
  title={The Application of Fine-Tuning on Pretrained Language Model in Information Extraction for Fault Knowledge Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={469-473},
  abstract={Constructing fault knowledge graphs holds significant importance for achieving intelligent maintenance and diagnosis in high-end equipment manufacturing. Effective information extraction and knowledge graph construction have proven challenging due to the lack of standardized representation of semantically complex unstructured text in the industrial domain. Therefore, in this study, we performed fine-tuning on the pre-trained language model (ChatGLM2-6B) with specific prompts to achieve information extraction from fault-related texts, ultimately leading to the construction of a fault knowledge graph. Experimental results demonstrate that the proposed method not only supports fine-tuning with limited data but also exhibits enhanced capability in understanding complex semantics related to fault symptoms and causes.},
  keywords={Computational modeling;Semantics;Knowledge graphs;Signal processing;Ontologies;Information retrieval;Stability analysis;Data models;Manufacturing;Maintenance;Pretrained language model;Parameter-efficient fine-tuning;Information extraction;Fault knowledge graph},
  doi={10.1109/ICSP62122.2024.10743881},
  ISSN={},
  month={April},}@INPROCEEDINGS{10740201,
  author={Batten, Christopher and Pinckney, Nathaniel and Liu, Mingjie and Ren, Haoxing and Khailany, Brucek},
  booktitle={2024 ACM/IEEE 6th Symposium on Machine Learning for CAD (MLCAD)}, 
  title={PyHDL-Eval: An LLM Evaluation Framework for Hardware Design Using Python-Embedded DSLs}, 
  year={2024},
  volume={},
  number={},
  pages={1-17},
  abstract={Embedding hardware design frameworks within Python is a promising technique to improve the productivity of hardware engineers. At the same time, there is significant interest in using large-language models (LLMs) to improve key chip design tasks. This paper describes PyHDL-Eval, a new framework for evaluating LLMs on specification-to-RTL tasks in the context of Python-embedded domainspecific languages (DSLs). The framework includes 168 problems, Verilog reference solutions, Verilog test benches, Python test scripts, and workflow orchestration scripts. We use the framework to conduct a detailed case study comparing five LLMs (CodeGemma 7B, Llama3 8B/70B, GPT4, and GPT4 Turbo) targeting Verilog and five Python-embedded DSLs (PyMTL3, PyRTL, MyHDL, Migen, and Amaranth). Our results demonstrate the promise of in-context learning when applied to smaller models (e.g.,pass rate for CodeGemma 7B improves from 14.9% to 32.7% on Verilog) and Python-embedded DSLs (e.g., pass rate for LLama3 70B improves from 0.6% to 33.0% on PyMTL3). We find LLMs perform better when targeting Verilog as compared to Python-embedded DSLs (e.g., pass rate for GPT4 Turbo is 72.2% on Verilog and 29.8–62.0% on the Python-embedded DSLs) despite using a popular general-purpose host language. PyHDL-Eval will serve as a useful framework for future research at the intersection of Python-embedded DSLs and LLMs. CCS Concepts • Hardware → Hardware description languages and compilation; • Computing methodologies → Machine learning.},
  keywords={Productivity;Solid modeling;Machine learning;Hardware;DSL;Hardware design languages;Chip scale packaging;hardware description languages;Python-embedded domain-specific languages;large language models},
  doi={10.1109/MLCAD62225.2024.10740201},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10731381,
  author={Grassi, Lucrezia and Recchiuto, Carmine Tommaso and Sgorbissa, Antonio},
  booktitle={2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)}, 
  title={Enhancing LLM-Based Human-Robot Interaction with Nuances for Diversity Awareness}, 
  year={2024},
  volume={},
  number={},
  pages={2287-2294},
  abstract={This paper presents a system for diversity-aware autonomous conversation leveraging the capabilities of large language models (LLMs). The system adapts to diverse populations and individuals, considering factors like background, personality, age, gender, and culture. The conversation flow is guided by the structure of the system’s pre-established knowledge base, while LLMs are tasked with various functions, including generating diversity-aware sentences. Achieving diversity-awareness involves providing carefully crafted prompts to the models, incorporating comprehensive information about users, conversation history, contextual details, and specific guidelines. To assess the system’s performance, we conducted both controlled and real-world experiments, measuring a wide range of performance indicators.},
  keywords={Large language models;Knowledge based systems;Human-robot interaction;Oral communication;Ontologies;Hybrid power systems;Time factors;Noise measurement;History;Robots},
  doi={10.1109/RO-MAN60168.2024.10731381},
  ISSN={1944-9437},
  month={Aug},}@INPROCEEDINGS{10731499,
  author={Wu, Shanglin and Yao, Xiaodong and Liu, Shu and Liang, Haitao and Liu, Zhenyuan},
  booktitle={2024 16th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)}, 
  title={Domain Knowledge Graph Construction Methods of Construction Schedule in Steel Structure Projects}, 
  year={2024},
  volume={},
  number={},
  pages={39-44},
  abstract={In the domain of engineering construction, steel structure engineering construction has accumulated a large amount of data, and the development of knowledge graph construction technology to structure this data can provide effective support for high-quality construction organization. This paper analyzes the knowledge sources of construction schedule for steel structure projects, constructs a domain ontology of construction schedule for assembled steel structure projects by using a seven-step method and points out the current problems of knowledge extraction for construction organization design for assembled steel structure projects. The semi-structured construction schedule data is subject to event extraction, and the unstructured knowledge of construction schedule is discussed from the perspective of few-shot methods and large language modeling for the knowledge extraction of construction schedule for steel structure projects.},
  keywords={Schedules;Human-machine systems;Knowledge graphs;Organizations;Ontologies;Data models;Steel;Data mining;Cybernetics;domain knowledge graphs;construction schedule;ontology construction;event extraction},
  doi={10.1109/IHMSC62065.2024.00017},
  ISSN={2157-8982},
  month={Aug},}@ARTICLE{10734210,
  author={Xu, Jun and Zhang, Hao and Zhang, Haijing and Lu, Jiawei and Xiao, Gang},
  journal={IEEE Access}, 
  title={ChatTf: A Knowledge Graph-Enhanced Intelligent Q&A System for Mitigating Factuality Hallucinations in Traditional Folklore}, 
  year={2024},
  volume={12},
  number={},
  pages={162638-162650},
  abstract={Large language models are rapidly advancing the field of artificial intelligence, with current research focusing primarily on traditional natural language understanding tasks, such as question answering and information extraction. However, in knowledge-intensive domains, such as intangible cultural heritage, hallucination problems due to insufficient domain knowledge persist. To address this, we present ChatTf, a knowledge graph-enhanced intelligent Q&A system, exemplified by Chinese traditional folklore, aimed at reducing factuality hallucinations in this domain. Specifically, we constructed the Traditional Folklore Ontology (TFOnto) and proposed the Zero-shot Traditional Folklore Triplet Extraction (ZFTE) framework. Driven by TFOnto, ZFTE builds a Traditional Folklore Knowledge Graph (TFKG). We then proposed a dual-stage Retrieval-Augmented Generation framework (TFKG-RAG) based on TFKG to provide traditional folklore knowledge to large language models, mitigating factuality hallucinations in folklore Q&A tasks. In the experimental phase, ChatTf achieved an accuracy of 96.7% on a self-built TFCQD test set, outperforming several state-of-the-art baseline methods. This demonstrates the accuracy and reliability of folklore domain question answering.},
  keywords={Ontologies;Knowledge graphs;Cultural differences;Large language models;Knowledge engineering;Training;Cognition;Accuracy;Semantics;Reliability;Knowledge graph;large language model;question answering;retrieval-augmented generation;traditional folklore},
  doi={10.1109/ACCESS.2024.3485877},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10722791,
  author={Haque, Mohd Ariful and Kamal, Marufa and George, Roy and Gupta, Kishor Datta},
  booktitle={2024 IEEE 11th International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={Utilizing Structural Metrics from Knowledge Graphs to Enhance the Robustness Quantification of Large Language Models (Extended Abstract)}, 
  year={2024},
  volume={},
  number={},
  pages={1-2},
  abstract={The goal of this study is to determine whether large language models (LLMs) like CodeLlama, Mistral, and Vicuna can be used to build knowledge graphs (KGs) from textual data. We create class descriptions for well-known KGs such as DBpedia, YAGO, and Google Knowledge Graph, from which we extract RDF triples and enhance these graphs using different preprocessing methods. Six structural quality measures are used in the study to compare the constructed and existing KGs. Our results demonstrate how important LLMs are to improving KG construction and provide insightful information for KG construction researchers. Moreover, an in-depth analysis of popular open-source LLM models enables researchers to identify the most efficient model for various tasks, ensuring optimal performance in specific applications.},
  keywords={Measurement;Analytical models;Large language models;Knowledge graphs;Ontologies;Data science;Resource description framework;Robustness;Internet},
  doi={10.1109/DSAA61799.2024.10722791},
  ISSN={2766-4112},
  month={Oct},}@INPROCEEDINGS{10711793,
  author={Pan, Xinyu and Gong, Jie and Wen, Sijie and Zhuang, Weibin and Li, Xinyu},
  booktitle={2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)}, 
  title={Mining User Requirement Scenarios and Generating Design Solutions for Rehabilitation Aids Based on Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={3155-3161},
  abstract={The development of the rehabilitation aids industry for the disabled has been pivotal in recent years, particularly in the personalized design of lower limb rehabilitation aids. Facing challenges in meeting individualized demands in design practice and the information gap between medical professionals and users, we propose a design Knowledge Graph (KG) method based on the Function-Behavior-Structure (FBS) model. This approach utilizes open-source large language models (LLMs) and fine-tunes them with instruction data generated by self-instructions to improve the accuracy of user requirements mining. The method aims to enhance the personalization and innovation of rehabilitation aids design through the integration of KG and LLM, effectively narrowing the cognitive gap between service providers and users. The anticipated results of the study are expected to promote efficient innovation in rehabilitation aids design, better meeting the needs of the disabled community.},
  keywords={Industries;Technological innovation;Computer aided software engineering;Automation;Accuracy;Large language models;Conferences;Knowledge graphs;Data mining},
  doi={10.1109/CASE59546.2024.10711793},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{10711329,
  author={Höfgen, Josua and Vogel-Heuser, Birgit and Vicaria, Alejandra and Pouzolz, François and Kurzhals, Christian},
  booktitle={2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)}, 
  title={Enhancing Model-Based System Architecting Through Formalized Decision Management}, 
  year={2024},
  volume={},
  number={},
  pages={1053-1060},
  abstract={System architecture decisions are typically informally captured in design documents. This practice leads to a loss of knowledge that impedes later activities like design changes, impact analysis, and reuse. Model-Based Systems Engineering (MBSE) frameworks support the development of increasingly complex systems but must address the problem of capitalization on architectural knowledge. To this end the "Decision Ontology for System Architectures (DOSA)" is developed to provide a formalized data model to capture system architecture decisions. DOSA is developed through a synthesis of decisions observed while developing an architecture model for a preliminary study of a novel satellite navigation system at Airbus Defence and Space. The approach is integrated into an MBSE framework enabling engineers to capture decisions that influence the architecture’s characteristics while developing the system model and imminently trace decision to artifacts of the system architecture. Subsequent visual inspection and formal querying of the decision graph facilitates the analysis of made decisions, and their interrelations.},
  keywords={Knowledge engineering;Visualization;Computer aided software engineering;Atmospheric modeling;Systems architecture;Ontologies;Inspection;Satellite navigation systems;Data models;Complex systems;MBSE;System Architecture;Decision Management;Ontology},
  doi={10.1109/CASE59546.2024.10711329},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{10708354,
  author={Kawther, Dridi and Wahiba, Ben Abdessalem Karaa},
  booktitle={2024 10th International Conference on Control, Decision and Information Technologies (CoDIT)}, 
  title={Comparative Analysis of Multilingual Text Classification Techniques: A Review of Current Approaches and Emerging}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The widespread availability of electronic documents and the exponential growth of the World Wide Web have made the automatic categorization of documents a critical method for organizing information and facilitating knowledge discovery, and information retrieval. This paper aims to examine the key techniques and methodologies utilized in multilingual document classification, while also bringing attention to some of the complex challenges that still need to be addressed. In particular, the paper presents a thorough review of the literature concerning the theory and methods of multilingual document representation and classification.},
  keywords={Dimensionality reduction;Reviews;Text categorization;Process control;Ontologies;Knowledge discovery;Vectors;Web sites;Information technology;Indexing},
  doi={10.1109/CoDIT62066.2024.10708354},
  ISSN={2576-3555},
  month={July},}@INPROCEEDINGS{10711065,
  author={Reif, Jonathan and Jeleniewski, Tom and Gill, Milapji Singh and Gehlhoff, Felix and Fay, Alexander},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Chatbot-Based Ontology Interaction Using Large Language Models and Domain-Specific Standards}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={The following contribution introduces a concept that employs Large Language Models (LLMs) and a chatbot interface to enhance SPARQL query generation for ontologies, thereby facilitating intuitive access to formalized knowledge. Utilizing natural language inputs, the system converts user inquiries into accurate SPARQL queries that strictly query the factual content of the ontology, effectively preventing misinformation or fabrication by the LLM. To enhance the quality and precision of outcomes, additional textual information from established domain-specific standards is integrated into the ontology for precise descriptions of its concepts and relationships. An experimental study assesses the accuracy of generated SPARQL queries, revealing significant benefits of using LLMs for querying ontologies and highlighting areas for future research.},
  keywords={Fabrication;Accuracy;Large language models;Semantics;Natural languages;Ontologies;Chatbots;Fake news;Standards;Manufacturing automation;Semantic Web;Ontologies;Large Language Models;Cyber-Physical Systems;Industry 4.0},
  doi={10.1109/ETFA61755.2024.10711065},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10710775,
  author={Vieira da Silva, Luis Miguel and Kocher, Aljosha and Gehlhoff, Felix and Fay, Alexander},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={On the Use of Large Language Models to Generate Capability Ontologies}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Capability ontologies are increasingly used to model functionalities of systems or machines. The creation of such onto-logical models with all properties and constraints of capabilities is very complex and can only be done by ontology experts. However, Large Language Models (LLMs) have shown that they can generate machine-interpretable models from natural language text input and thus support engineers / ontology experts. Therefore, this paper investigates how LLMs can be used to create capability ontologies. We present a study with a series of experiments in which capabilities with varying complexities are generated using different prompting techniques and with different LLMs. Errors in the generated ontologies are recorded and compared. To analyze the quality of the generated ontologies, a semi-automated approach based on RDF syntax checking, OWL reasoning, and SHACL constraints is used. The results of this study are very promising because even for complex capabilities, the generated ontologies are almost free of errors.},
  keywords={Shape;Large language models;Natural languages;OWL;Ontologies;Syntactics;Cognition;Resource description framework;Complexity theory;Testing;Large Language Models;LLMs;Capabilities;Skills;Ontologies;Semantic Web;Model-Generation},
  doi={10.1109/ETFA61755.2024.10710775},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10711054,
  author={Knollmeyer, Simon and Akmal, Muhammad Uzair and Koval, Leonid and Asif, Saara and Mathias, Selvine G. and Groβmann, Daniel},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Document Knowledge Graph to Enhance Question Answering with Retrieval Augmented Generation}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Reusing and managing existing knowledge from available documents is crucial for success in the factory planning domain. By leveraging Artificial Intelligence (AI) and Question Answering (QA) systems, users can query a document corpus through a chat-based application and receive precise answers. The recent advancements in Large Language Models (LLMs) and their linguistic capabilities present new opportunities for such applications. Utilizing the methodology of Retrieval Augmented Generation (RAG), document sections are provided to the LLM based on user queries. However, existing RAG implementations that use vector databases as document repositories face limitations when answering questions that extend beyond the text content of the documents. To address this issue, this paper proposes a concept to enhance RAG systems by integrating a Knowledge Graph (KG) constructed from the document structures.},
  keywords={Databases;Large language models;Knowledge graphs;Linguistics;Question answering (information retrieval);Vectors;Production facilities;Planning;Manufacturing automation;Faces;Information management;Retrieval Augmented Generation;Knowledge Graph;Large Language Models},
  doi={10.1109/ETFA61755.2024.10711054},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10710783,
  author={Vieira da Silva, Luis Miguel and Kocher, Aljosha and Gehlhoff, Felix and Fay, Alexander},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Toward a Method to Generate Capability Ontologies from Natural Language Descriptions}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={To achieve a flexible and adaptable system, capabil-ity ontologies are increasingly leveraged to describe functions in a machine-interpretable way. However, modeling such complex ontological descriptions is still a manual and error-prone task that requires a significant amount of effort and ontology expertise. This contribution presents an innovative method to automate capability ontology modeling using Large Language Models (LLMs), which have proven to be well suited for such tasks. Our approach requires only a natural language description of a capability, which is then automatically inserted into a predefined prompt using a few-shot prompting technique. After prompting an LLM, the resulting capability ontology is automatically verified through various steps in a loop with the LLM to check the overall correctness of the capability ontology. First, a syntax check is performed, then a check for contradictions, and finally a check for hallucinations and missing ontology elements. Our method greatly reduces manual effort, as only the initial natural language description and a final human review and possible correction are necessary, thereby streamlining the capability ontology generation process.},
  keywords={Adaptation models;Costs;Reviews;Large language models;Natural languages;Manuals;Ontologies;Syntactics;Manufacturing automation;Testing;Large Language Models;LLMs;Capabilities;Skills;Ontologies;Semantic Web;Model-Generation},
  doi={10.1109/ETFA61755.2024.10710783},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10710806,
  author={Meyer, Frederic and Freitag, Lennart and Hinrichsen, Sven and Niggemann, Oliver},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Potentials of Large Language Models for Generating Assembly Instructions}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={With the increasing complexity in manual assembly and a demographic decline in skilled workforce, the importance of well-documented processes through assembly instructions has grown. Creating these instructions is a time-consuming and knowledge-intensive task that typically relies on experienced employees. Although various automation solutions have been proposed to assist in generating assembly instructions, they often fall short in providing detailed textual guidance. With the rise of generative artificial intelligence (AI), new potentials arise in this domain. Therefore, this paper explores these potentials by employing various large language models (LLMs), prompting techniques and input data in an experimental setup for generating detailed assembly instructions, including the planning of assembly sequences as well as textual guidance on tools, assembly activities, and quality assurance measures. The findings reveal promising opportunities in leveraging LLMs but also substantial challenges, particularly in assembly sequence planning. To improve the reliability of generating assembly instructions, we propose a multi-agent concept that decomposes the complex task into simpler subtasks, each managed by specialized agents.},
  keywords={Quality assurance;Generative AI;Large language models;Decision making;Manuals;Planning;Complexity theory;Reliability;Assembly;Manufacturing automation;agent;assembly instruction;experiment;GPT;large language model;LLM;prompt},
  doi={10.1109/ETFA61755.2024.10710806},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10710789,
  author={Schoch, Nicolai and Hoernicke, Mario and Strem, Nika and Stark, Katharina},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Engineering Data Funnel (WIP) – An Ontology-Enhanced LLM-Based Agent and MoE System for Engineering Data Processing}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Automation Engineering of a process automation system is still a very manual effort due to limited support for the interpretation and processing of process design specification documents. Even though standards for digital data exchange between process and automation engineering do exist, those formats are rarely used and consequently the immense automation potential in automation engineering cannot be lifted. This contribution presents an AI -based approach and prototype - using an ontology-enhanced LLM -based agent and a mixture-of-experts system - to structure and formalize multimodal unstructured process design information as in PDF, Excel, and Word formats and make it available for state-of-the-art engineering tools for the long-known “Automation of Automation”.},
  keywords={Process design;Automation;Prototypes;Manuals;Portable document format;Data processing;Artificial intelligence;Standards;Manufacturing automation;engineering design specification;engineering data processing;LLM-based agent;mixture of experts;ontology-driven information processing;automation of automation},
  doi={10.1109/ETFA61755.2024.10710789},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10706469,
  author={Incitti, Francesca and Salfinger, Andrea and Snidaro, Lauro and Challapalli, Sri},
  booktitle={2024 27th International Conference on Information Fusion (FUSION)}, 
  title={Leveraging LLMs for Knowledge Engineering from Technical Manuals: A Case Study in the Medical Prosthesis Manufacturing Domain}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Ontologies are nowadays widely used to organize information across specific domains, being effective due to their hierarchical structure and the ability to explicitly represent relationships between concepts. Knowledge engineering, like compiling companies’ vast bodies of knowledge into these structures, however, still represents a time-consuming, largely manually performed process, esp. with significant amounts of knowledge often only recorded within unstructured text documents. Since the recently introduced Large Language Models (LLMs) excel on text summarization, this raises the question whether these could be exploited within dedicated knowledge fusion architectures to assist human knowledge engineers by automatically suggesting relevant classes, instances and relations extracted from textual corpora. We therefore propose a novel approach that leverages the taxonomic structure of a partially defined ontology to prompt LLMs for hierarchical knowledge organization. Unlike conventional methods that rely solely on static ontologies, our methodology dynamically generates prompts based on the ontology’s existing class taxonomy, prompting the LLM to generate responses that extract supplementary information from unstructured documents. It thus introduces the concept of using ontologies as scaffolds for guiding LLMs, in order to realize a mutual interplay between structured ontological knowledge and the soft fusion capabilities of LLMs. We evaluate our proposed algorithm on a real-world case study, performing a knowledge fusion task on heterogeneous technical documentation from a medical prosthesis manufacturer.},
  keywords={Knowledge engineering;Large language models;Taxonomy;Text summarization;Organizations;Manuals;Documentation;Ontologies;Manufacturing;Prosthetics;Large Language Models;Knowledge Engineering;Ontology Population;Soft Fusion;Natural Language Processing},
  doi={10.23919/FUSION59988.2024.10706469},
  ISSN={},
  month={July},}@ARTICLE{10713368,
  author={Lee, Jinkyu and Kim, Jihie},
  journal={IEEE Access}, 
  title={Improving Commonsense Bias Classification by Mitigating the Influence of Demographic Terms}, 
  year={2024},
  volume={12},
  number={},
  pages={161480-161489},
  abstract={Understanding commonsense knowledge is crucial in the field of Natural Language Processing (NLP). However, the presence of demographic terms in commonsense knowledge poses a potential risk of compromising the performance of NLP models. This study aims to investigate and propose methods for enhancing the performance and effectiveness of a commonsense polarization classifier by mitigating the influence of demographic terms. Three methods are introduced in this paper: (1) hierarchical generalization of demographic terms (2) threshold-based augmentation and (3) integration of hierarchical generalization and threshold-based augmentation methods(IHTA). The first method involves replacing demographic terms with more general ones based on a term hierarchy ontology, aiming to mitigate the influence of specific terms. To address the limited bias-related information, the second method measures the polarization of demographic terms by comparing the changes in the model’s predictions when these terms are masked versus unmasked. This method augments commonsense sentences containing terms with high polarization values by replacing their predicates with synonyms generated by ChatGPT. The third method combines the two approaches, starting with threshold-based augmentation followed by hierarchical generalization. The experiments show that the first method increases the accuracy over the baseline by 2.33%, and the second one by 0.96% over standard augmentation methods. The IHTA techniques yielded an 8.82% and 9.96% higher accuracy than threshold-based and standard augmentation methods, respectively.},
  keywords={Accuracy;Predictive models;Commonsense reasoning;Standards;Ontologies;Chatbots;Training data;Systematics;Semantics;Prevention and mitigation;Demography;Natural language processing;Classification algorithms;Commonsense bias;demographic term;bias mitigation;hierarchical generalization;threshold-based augmentation},
  doi={10.1109/ACCESS.2024.3477599},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10705189,
  author={Baghdadi, Sarra and Saleh, Majd and Paquelet, Stéphane},
  booktitle={2024 IEEE 12th International Conference on Intelligent Systems (IS)}, 
  title={TocBERT: Medical Document Structure Extraction Using Bidirectional Transformers}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Text segmentation holds paramount importance in the field of Natural Language Processing (NLP). It plays an important role in several NLP downstream tasks like information retrieval and document summarization. In this work, we propose a new solution, namely TocBERT, for segmenting texts using bidirectional transformers. TocBERT represents a supervised solution trained on the detection of titles and sub-titles from their semantic representations. This task was formulated as a named entity recognition (NER) problem. The solution has been applied on a medical text segmentation use-case where the Bio-ClinicalBERT model is fine-tuned to segment discharge summaries of the MIMIC-III dataset. The performance of TocBERT has been evaluated on a human-labeled ground truth corpus of 250 notes. It achieved an F1-score of 84.6% when evaluated on a linear text segmentation problem and 72.8 % on a hierarchical text segmentation problem. It outperformed a carefully designed rule-based solution, particularly in distinguishing titles from subtitles.},
  keywords={Biological system modeling;Semantics;MIMICs;Named entity recognition;Ontologies;Transformers;Information retrieval;Cleaning;Intelligent systems;Title detection;text segmentation;NLP;language models;transformers;BERT;information retrieval;medical text cleaning},
  doi={10.1109/IS61756.2024.10705189},
  ISSN={2767-9802},
  month={Aug},}@INPROCEEDINGS{10705235,
  author={Baddour, Moussa and Paquelet, Stéphane and Rollier, Paul and De Tayrac, Marie and Dameron, Olivier and Labbe, Thomas},
  booktitle={2024 IEEE 12th International Conference on Intelligent Systems (IS)}, 
  title={Phenotypes Extraction from Text: Analysis and Perspective in the LLM Era}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Collecting the relevant list of patient phenotypes, known as deep phenotyping, can significantly improve the final diagnosis. As textual clinical reports are the richest source of phenotypes information, their automatic extraction is a critical task. The main challenges of this Information Extraction (IE) task are to identify precisely the text spans related to a phenotype and to link them unequivocally to referenced entities from a source such as the Human Phenotype Ontology (HPO). Recently, Language Models (LMs) have been the most suc-cessful approach for extracting phenotypes from clinical reports. Solutions such as PhenoBERT, relying on BERT or GPT, have shown promising results when applied to datasets built on the hypothesis that most phenotypes are explicitly mentioned in the text. However, this assumption is not always true in medical genetics. Hence, although the LMs carry powerful semantic abilities, their contributions are not clear compared to syntactic string-matching steps that are used within the current pipelines. The goal of this study is to improve phenotype extraction from clinical notes related to genetic diseases. Our contributions are threefold: First, we provide a clear definition of the phenotype extraction task from free text, along with a high-level overview of the involved functions. Second, we conduct an in-depth analysis of PhenoBERT, one of the best existing solutions, to evaluate the proportion of phenotypes predicted with simple string-matching. Third, we demonstrate how utilizing and incorporating large language models (LLMs) for span detection step can improve performance especially with implicit phenotypes. In addition, this experiment revealed that the annotations of existing dataset are not exhaustive, and that LLM can identify relevant spans missed by human labelers.},
  keywords={Phenotypes;Annotations;Large language models;Semantics;Detectors;Syntactics;Ontologies;Data mining;Intelligent systems;Medical diagnostic imaging;phenotype;genetic;entity linking;phenoBERT;LLM;embed dings},
  doi={10.1109/IS61756.2024.10705235},
  ISSN={2767-9802},
  month={Aug},}@INPROCEEDINGS{10698993,
  author={Kumar, Amala Rashmi and Kumari, S. Meena and Rao, Tanvi and Shetty, Tavishi S},
  booktitle={2024 Second International Conference on Networks, Multimedia and Information Technology (NMITCON)}, 
  title={ReidLM: Fine-Tuning LLaMA3 using Evol-Instruct for Enhanced Contextual Accuracy in Rare Disease Research}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This study introduces ReidLM, a fine-tuned large language model (LLM) optimized for the rare disease domain. By generating a diverse and complex question-and-answer dataset using the EvolInstruct methodology, Meta’s LLaMA-3-8B-Instruct model was enhanced to deliver better performance across multiple evaluation metrics. ReidLM demonstrates significant improvements in generating contextually accurate responses for rare diseases, highlighting the potential of Evol-Instruct in specialized medical applications. Specifically, ReidLM achieved the highest ROUGE-1 (0.3281) and GEval (0.87) scores among the evaluated models, along with strong performances in METEOR (0.3662) and BERTScore (0.8782), indicating its effectiveness in producing semantically sound and relevant responses. These results offer promising advancements in medical research and patient care, with future work aimed at expanding datasets and validating clinical utility.},
  keywords={Measurement;Accuracy;Terminology;Multimedia systems;Large language models;Medical services;Meteors;Information technology;Medical diagnostic imaging;Diseases;fine-tuning;LLaMA 3;evol-instruct;rare diseases},
  doi={10.1109/NMITCON62075.2024.10698993},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10695412,
  author={Luo, Junjun and Zhu, Zhongyan and Zhu, Haijiang and Dong, Xiaohui},
  booktitle={2024 7th International Conference on Computer Information Science and Application Technology (CISAT)}, 
  title={Research on knowledge graph construction method for mine hoist fault field}, 
  year={2024},
  volume={},
  number={},
  pages={342-346},
  abstract={Mine hoists are integral to mining hoisting systems, with their safe and reliable operation being critical for ensuring the safety of mining operations. The consequences of hoist failure are severe, particularly when the root cause of the malfunction is not promptly identified and addressed, potentially compromising the overall safety of mining activities. The complexity of mine hoist systems stems from the interdependent and restrictive relationships among their components, each of which generates unique operational state information. This information, when aggregated and processed, can be distilled into various fault characteristic parameters. This paper introduces a novel approach to fault diagnosis within mine hoist systems by constructing a fault knowledge graph based on ontological principles. The proposed method harnesses the power of knowledge graphs to systematically represent and analyze the complex interplay of components within the hoist system. By doing so, it enhances the diagnostic capabilities and the preemptive identification of potential faults. The research focuses on the mine hoist as the subject of study and proposes the development of an ontologically-based fault knowledge graph. This approach is not only of significant importance to the coal mining industry but also offers innovative insights for knowledge graph construction across various domains. The implications of this study extend beyond the mining sector, providing a foundation for more robust and intelligent fault diagnosis systems in complex mechanical systems.},
  keywords={Fault diagnosis;Information science;Instruments;Knowledge graphs;Named entity recognition;Information retrieval;Safety;Complexity theory;Mechanical systems;Lifting equipment;artificial intelligence;entity recognition;fault knowledge graph;mine hoist;relation extraction},
  doi={10.1109/CISAT62382.2024.10695412},
  ISSN={},
  month={July},}@INPROCEEDINGS{10679346,
  author={Kougioumtzidou, Anna and Papoutsis, Angelos and Kavallieros, Dimitrios and Mavropoulos, Thanassis and Tsikrika, Theodora and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
  booktitle={2024 IEEE International Conference on Cyber Security and Resilience (CSR)}, 
  title={An End-to-End Framework for Cybersecurity Taxonomy and Ontology Generation and Updating}, 
  year={2024},
  volume={},
  number={},
  pages={247-254},
  abstract={Effective cyber-defense practices often require the use of structured knowledge representations, such as taxonomies and ontologies, to organise vast amounts of data and facili-tate knowledge representation and reasoning. To this end, we present an Artificial Intelligence (AI)-assisted framework for the construction and update of cybersecurity taxonomies and ontologies. The proposed framework can be divided into three main phases: Taxonomy Construction, Ontology Construction, and Taxonomy/Ontology Update, each phase consisting of both information extraction and semantic knowledge representation components. For information extraction, we employ a variety of techniques originating from Natural Language Processing (NLP), particularly Transformer Neural Networks. For constructing ontologies, we propose a conceptual ontology schema based on the STIX 2.1 standard for modeling information related to attacks, threats, and vulnerabilities, and use the Owlready2 Python library. Overall, our framework effectively builds cybersecurity taxonomies and ontologies and updates existing knowledge of both the generated and open-source taxonomies and ontologies.},
  keywords={Filtering;Semantic search;Taxonomy;Ontologies;Transformers;Natural language processing;Smart grids;ontologies;taxonomies;cybersecurity;attacks;vulnerabilities;dynamic update;large language models;natural language processing;artificial intelligence},
  doi={10.1109/CSR61664.2024.10679346},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10679399,
  author={Karalka, Christina and Meditskos, Georgios and Papoutsoglou, Maria and Bassiliades, Nick},
  booktitle={2024 IEEE International Conference on Cyber Security and Resilience (CSR)}, 
  title={Towards a Generic Knowledge Graph Construction Framework for Privacy Awareness}, 
  year={2024},
  volume={},
  number={},
  pages={700-705},
  abstract={Knowledge graphs (KGs) organize data from multi-ple sources, capturing information about entities of interest in a given domain or task, such as people, places, or events, and forge connections between them. In this paper, we introduce a generic framework for building knowledge graphs designed to enhance data privacy through semantic interpretation. We demonstrate the effectiveness of our framework by applying it to the healthcare sector, where it helps organize and analyze com-plex information, support data analysis, improve decision-making processes, and uncover hidden relationships between entities. Our approach leverages domain-specific ontologies like SNOMED CT and integrates vector databases to assess and mitigate privacy risks. By using semantic techniques, we enhance the robustness of data against reidentification attacks and suggest appropriate de-identification methods. The integration of SNOMED with vector databases allows for efficient storage, retrieval, and analysis of high-dimensional healthcare data, facilitating advanced data an-alytics and knowledge discovery while maintaining data privacy. Through this framework, we aim to provide sufficient insights for identifying privacy vulnerabilities and ensuring the security and usability of sensitive health information.},
  keywords={Data privacy;Privacy;Databases;Semantics;Knowledge graphs;Medical services;Vectors},
  doi={10.1109/CSR61664.2024.10679399},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10677287,
  author={Saini, Shashwat and Vrindavanam, Jayavrinda and Mondal, Subhash},
  booktitle={2024 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)}, 
  title={Methodological Insights Into Protein Clustering Using BERT & RoBERTa}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Proteins are present in all living organisms, and understanding their processes is vital. Protein databases such as SWISS-PROT include curated information on only 570,000 protein sequences, representing a fraction of the 250 million known evidential and predicted sequences; it becomes crucial to cluster proteins into similar groups. This research explores the application of two transformer architectures, BERT and RoBERTa in clustering proteins in the supervised prediction of Gene Ontology (GO) annotations. The detailed methodology for both the pre-training and fine-tuning processes, as well as results that showcase RoBERTa outperforming BERT in the context of protein clustering, on performance metrics of accuracy and loss. Operating under constrained computational resources, the deployed model exhibits strong performance and highlight the robustness of methodology in protein clustering within resource constraints. This study not only contributes to the understanding of protein clustering but also signifies the potential of transformer models to handle biological data.},
  keywords={Proteins;Training;Analytical models;Annotations;Biological system modeling;Computational modeling;Ontologies;Protein Clustering;BERT;RoBERTa;Natural Language Processing;Transformers;Masked Language Modelling},
  doi={10.1109/CONECCT62155.2024.10677287},
  ISSN={2766-2101},
  month={July},}
@INPROCEEDINGS{10677303,
  author={Gupta, Pranav and Sharma, Raunak and Kumari, Rashmi and Aditya, Sri Krishna and Choudhary, Shwetank and Kumar, Sumit and M, Kanchana and R, Thilagavathy},
  booktitle={2024 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)}, 
  title={ECHO: Environmental Sound Classification with Hierarchical Ontology-guided Semi-Supervised Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Environment Sound Classification has been a well-studied research problem in the field of signal processing and till now more focus has been laid on fully supervised approaches. Recently, the focus has moved towards semi-supervised methods which concentrate on utilizing unlabeled data, and self-supervised methods which learn the intermediate representation through pretext tasks or contrastive learning. However, both approaches require a vast amount of unlabelled data to improve performance. In this work, we propose a novel framework called Environmental Sound Classification with Hierarchical Ontology-guided semi-supervised Learning (ECHO) that utilizes label ontology-based hierarchy to learn semantic representation by defining a novel pretext task. The model tries to predict coarse labels represented by the Large Language Model (LLM) based on ground truth label ontology, then further fine-tuned in a supervised way to predict the actual task. ECHO achieves a 1% to 8% accuracy improvement over baseline systems across UrbanSound8K, ESC-10, and ESC-50 datasets.},
  keywords={Accuracy;Large language models;Semantics;Contrastive learning;Ontologies;Semisupervised learning;Signal processing;semi-supervised learning;Environment Sound Classification;Label ontology},
  doi={10.1109/CONECCT62155.2024.10677303},
  ISSN={2766-2101},
  month={July},}@INPROCEEDINGS{10655250,
  author={Quan, Ruijie and Wang, Wenguan and Ma, Fan and Fan, Hehe and Yang, Yi},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Clustering for Protein Representation Learning}, 
  year={2024},
  volume={},
  number={},
  pages={319-329},
  abstract={Protein representation learning is a challenging task that aims to capture the structure and function of proteins from their amino acid sequences. Previous methods largely ignored the fact that not all amino acids are equally important for protein folding and activity. In this article, we propose a neural clustering framework that can automatically discover the critical components of a protein by considering both its primary and tertiary structure information. Our framework treats a protein as a graph, where each node represents an amino acid and each edge represents a spatial or sequential connection between amino acids. We then apply an iterative clustering strategy to group the nodes into clusters based on their 1D and 3D positions and assign scores to each cluster. We select the highest-scoring clusters and use their medoid nodes for the next iteration of clustering, until we obtain a hierarchical and informative representation of the protein. We evaluate on four protein-related tasks: protein fold classification, enzyme reaction classification, gene ontology term prediction, and enzyme commission number prediction. Experimental results demonstrate that our method achieves state-of-the-art performance.},
  keywords={Proteins;Representation learning;Enzymes;Computer vision;Three-dimensional displays;Ontologies;Amino acids;Protein Representation Learning;Clustering},
  doi={10.1109/CVPR52733.2024.00038},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10655304,
  author={Ma, Jiawei and Huang, Po-Yao and Xie, Saining and Li, Shang-Wen and Zettlemoyer, Luke and Chang, Shih-Fu and Yih, Wen-Tau and Xu, Hu},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={MoDE: CLIP Data Experts via Clustering}, 
  year={2024},
  volume={},
  number={},
  pages={26344-26353},
  abstract={The success of contrastive language-image pretraining (CLIP) relies on the supervision from the pairing between images and captions, which tends to be noisy in web- crawled data. We present Mixture of Data Experts (MoDE) and learn a system of CLIP data experts via clustering. Each data expert is trained on one data cluster, being less sensitive to false negative noises in other clusters. At inference time, we ensemble their outputs by applying weights determined through the correlation between task metadata and cluster conditions. To estimate the correlation pre-cisely, the samples in one cluster should be semantically similar, but the number of data experts should still be rea-sonable for training and inference. As such, we consider the ontology in human language and propose to use fine- grained cluster centers to represent each data expert at a coarse-grained level. Experimental studies show that four CLIP data experts on ViT-B/16 outperform the ViT-L/14 by OpenAI CLIP and OpenCLIP on zero-shot image classification but with less (<35%) training cost. Meanwhile, MoDE can train all data expert asynchronously and can flexibly include new data experts. The code is available here.},
  keywords={Training;Adaptation models;Correlation;Costs;Computational modeling;Noise;Semantics;Data Expert;Multi-Modal;Data Clustering},
  doi={10.1109/CVPR52733.2024.02489},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10651359,
  author={Liu, Xu and Chen, Xinming and Zhu, Yangfu and Wu, Bin},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Prompt-Enhanced Prototype Framework for Few-shot Event Detection}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Few-shot event detection (ED) aims at identifying and typing event mentions from text with limited annotations. Most existing methods for few-shot ED use event ontology and related knowledge to construct prototypes and fail to fully leverage the rich knowledge of pre-trained language models (PLMs) which could help improve the representation of prototypes. Motivated by this, we propose an prompt-enhanced prototype framework which combines prototype and prompt for few-shot ED. Considering the scarcity of labeled data, we also introduce contrastive learning to enrich prototypes. Specifically, we use heuristic rules to align FrameNet with annotated data to get corresponding prompts for each event and convert them into prompt prototype. We then leverage contrastive learning to aggregate event mentions into prototypes and maintain these prototypes for few-shot ED. Furthermore, We explore diverse prompt formats for representing prompt prototypes and introduce a more comprehensive lexical prompt which improves the performance of few-shot ED. We conduct extensive experiments on the MAVEN corpus to reveal the effectiveness of the proposed framework compared to state-of-the-art methods.},
  keywords={Event detection;Annotations;Aggregates;Semantics;Neural networks;Prototypes;Contrastive learning},
  doi={10.1109/IJCNN60899.2024.10651359},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10650745,
  author={Jian, Zhaorui and Liu, Shengquan and Gao, Wei and Cheng, Jianming},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Distantly Supervised Relation Extraction based on Non-taxonomic Relation and Self-Optimization}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={Distantly supervised relation extraction (DS-RE) leverages existing knowledge bases to generate annotated data for relation extraction (RE), addressing the issue of scarce labeled data. However, distant supervision (DS) is often limited by coarse annotations and insufficient contextual awareness, leading to relational ambiguity and introducing noise in the labeled results. Moreover, although one can optimize the classifiers in DS-RE models through weight updates, the static nature of the guiding rules for such adjustments often falls short when addressing the challenges posed by diverse non-taxonomic relations and complex noise patterns in datasets. In this paper, we propose a DS-RE framework that capitalizes on non-taxonomic relations and a self-optimizing mechanism. We define a set of consistent DS relation candidates and combine DS with a LLM to enhance the perception of entities’ contextual states during the DS process. Then, we design a Self-Optimizing Ontology-Enhanced Non-taxonomic Relation Extraction Model (SO-NRE). The model incorporates additional entity-relation knowledge to enhance the semantic depth of Non-taxonomic relation ontologies and uses an adaptive dynamic scheduling mechanism to refine the classification strategy through iterations informed by self-perception outcomes. The experimental results show that the improved DS annotation workflow has enhanced accuracy, and SO-NRE outperforms mainstream baselines in RE performance.},
  keywords={Adaptation models;Accuracy;Annotations;Large language models;Noise;Semantics;Neural networks;Distantly Supervised Relation Extraction;Non-taxonomic Relation;LLM;Self-Optimization},
  doi={10.1109/IJCNN60899.2024.10650745},
  ISSN={2161-4407},
  month={June},}@ARTICLE{10666776,
  author={Achintalwar, Swapnaja and Baldini, Ioana and Bouneffouf, Djallel and Byamugisha, Joan and Chang, Maria and Dognin, Pierre and Farchi, Eitan and Makondo, Ndivhuwo and Mojsilović, Aleksandra and Nagireddy, Manish and Natesan Ramamurthy, Karthikeyan and Padhi, Inkit and Raz, Orna and Rios, Jesus and Sattigeri, Prasanna and Singh, Moninder and Thwala, Siphiwe A. and Uceda-Sosa, Rosario A. and Varshney, Kush R.},
  journal={IEEE Internet Computing}, 
  title={Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations}, 
  year={2024},
  volume={28},
  number={5},
  pages={28-36},
  abstract={The alignment of large language models is usually done by model providers to add or control behaviors that are common or universally understood across use cases and contexts. By contrast, in this article, we present an approach and architecture that empowers application developers to tune a model to their particular values, social norms, laws, and other regulations and orchestrate between potentially conflicting requirements in context. We lay out three main components of such an Alignment Studio architecture: Framers, Instructors, and Auditors, which work in concert to control the behavior of a language model. We illustrate this approach with a running example of aligning a company’s internal-facing enterprise chatbot to its business conduct guidelines.},
  keywords={Data models;Regulation;Internet;Guidelines;Synthetic data;Chatbots;Large language models;Context modeling;Context-aware services},
  doi={10.1109/MIC.2024.3453671},
  ISSN={1941-0131},
  month={Sep.},}@INPROCEEDINGS{10633526,
  author={Yang, Hang and Xiao, Liang and Zhu, Rujun and Liu, Ziji and Chen, Jianxia},
  booktitle={2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Interlinking Clinical Guidelines via Mining Medical Literature Knowledge for Multi-Morbidity Decision-Making}, 
  year={2024},
  volume={},
  number={},
  pages={1250-1255},
  abstract={Independently developed clinical guidelines present a systematic challenge in managing patients with multi-morbidity in a consistent and integrated manner. Existing approaches mainly focus on combining multiple guidelines and lack approaches that combine with additional medical resources. The correlations and conflicts between treatment plans in the management of multi-morbidity are well-documented in medical literature but are less explored in the Clinical Decision Support line of research. In this paper, we propose a literature-based guideline interlinking method to address these challenges through the integration of clinical guidelines and the harmonization of conflicting recommendations, thereby providing a more holistic and efficient way to manage patients with multi-morbidity conditions. This method employs an ontology model and knowledge graph technology to represent and analyze the complexity and interrelations of diseases, with the aim of transcending the limitations of traditional single disease guidelines and providing a holistic and integrated framework for multi-morbidity management. The objective is to construct a multi-morbidity knowledge graph by correlating medical literature with clinical guidelines and to provide optimal decision support for patients with multi-morbidity complications in a clinical decision support system (CDSS).},
  keywords={Accuracy;Systematics;Databases;Computational modeling;Biological system modeling;Knowledge graphs;Ontologies;multi-morbidity management;clinical guidelines;ontology model;knowledge graph},
  doi={10.1109/COMPSAC61105.2024.00165},
  ISSN={2836-3795},
  month={July},}@INPROCEEDINGS{10638283,
  author={Hendawi, Rasha and Alian, Shadi and Li, Juan},
  booktitle={2024 15th International Conference on Information and Communication Systems (ICICS)}, 
  title={Breaking Down Barriers: Empowering Diabetes Patients with User-Friendly Medical Explanations}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Effective management of diabetes is contingent upon patients' understanding of their medical conditions and treatments. However, medical documents often contain complex jargon and technical details that can be challenging for patients, especially those with limited health literacy. This paper presents DiaKnow, an innovative tool that simplifies medical documents and customizes explanations to suit individual health literacy levels. Employing a robust self-attention transformer model and a comprehensive diabetes-focused knowledge graph, DiaKnow enhances patient comprehension by providing contextually relevant, simplified medical information. This study assesses DiaKnow’s efficacy in real-world clinical settings through a structured use case evaluation method. We tested the tool’s ability to accurately identify, link, and simplify crucial medical terms using a diverse set of medical documents. Our findings confirm that DiaKnow not only improves the readability of medical documents but also ensures that explanations are medically accurate, clear, and comprehensive.},
  keywords={Accuracy;Communication systems;Transforms;Knowledge graphs;Transformers;Diabetes;Context modeling;health literacy;knowledge graph;ontology;self-attention transformers;medical entity recognition;entity linking},
  doi={10.1109/ICICS63486.2024.10638283},
  ISSN={2573-3346},
  month={Aug},}@INPROCEEDINGS{10628803,
  author={Liu, Hao and Zhou, Shuxin and Chen, Zhehuan and Perl, Yehoshua and Wang, Jiayin},
  booktitle={2024 IEEE 12th International Conference on Healthcare Informatics (ICHI)}, 
  title={Using Generative Large Language Models for Hierarchical Relationship Prediction in Medical Ontologies}, 
  year={2024},
  volume={},
  number={},
  pages={248-256},
  abstract={This study extends the exploration of ontology enrichment by evaluating the performance of various open-sourced Large Language Models (LLMs) on the task of predicting hierarchical relationships (IS-A) in medical ontologies including SNOMED CT Clinical Finding and Procedure hierarchies and the human Disease Ontology. With the previous finetuned BERT models for hierarchical relationship prediction as the baseline, we assessed eight open-source generative LLMs for the same task. We observed only three models, without finetuning, demonstrated comparable or superior performance compared to the baseline BERT -based models. The best performance model OpenChat achieved a macro average F1 score of 0.96 (0.95) on SNOMED CT Clinical Finding (Procedure) hierarchy, an increase over 7% from the baseline 0.89 (0.85). On human Disease Ontology, OpenChat excels with an F1 score of 0.91, outperforming the second-best performance model Vicuna (0.84). Notably, some LLMs prove unsuitable for hierarchical relationship prediction tasks or appliable for concept placement of medical ontologies. We also explored various prompt templates and ensemble techniques to uncover potential confounding factors in applying LLMs for IS-A relation predictions for medical ontologies.},
  keywords={Accuracy;Large language models;Medical services;Ontologies;Predictive models;Task analysis;Informatics;Hieratical Relation Prediction;Large Language Models;Medical Ontology;Prompts Design;SNOMED CT},
  doi={10.1109/ICHI61247.2024.00040},
  ISSN={2575-2634},
  month={June},}@INPROCEEDINGS{10628465,
  author={Lian, Xiaoli and Ma, Jieping and Lv, Heyang and Zhang, Li},
  booktitle={2024 IEEE 32nd International Requirements Engineering Conference (RE)}, 
  title={ReqCompletion: Domain-Enhanced Automatic Completion for Software Requirements}, 
  year={2024},
  volume={},
  number={},
  pages={142-154},
  abstract={Software requirements are the driving force behind software development. As the cornerstone of the entire software lifecycle, the efficiency of crafting requirement specifications and the quality of these requirements significantly influence the duration of software development. Despite massive research on requirements elicitation, the reality is that requirements are often painstakingly crafted manually, word by word. This manual process is not only time-consuming but also prone to issues such as the misuse of terminology. To address these challenges, we introduce ReqCompletion, an approach designed to recommend the next token in real-time for given prefix of requirements description. ReqCompletion comprises two primary components. First, we have devised and integrated a knowledge-injection module into GPT-2—which stands as the largest available GPT model that allows for fine-tuning on specialized downstream tasks. This injection imbues GPT-2 with richer domain-specific knowledge, thus improving the relevance of the suggested tokens. Additionally, we employ a pointer network to optimize the recommendation quality by utilizing completed requirements as contextual support. Empirical evaluations using two public datasets demonstrate that ReqCompletion surpasses all baselines in performance (Recall@7 gains up to 65.87% than the second-best model). Furthermore, the effectiveness of its two pivotal design elements has been substantiated through rigorous ablation studies. The utility of our work has been evaluated preliminarily through a small user study.},
  keywords={Terminology;Force;Manuals;Computer architecture;Benchmark testing;Software;Real-time systems;Software Requirements;Automatic Text Completion;Knowledge Injection},
  doi={10.1109/RE59067.2024.00023},
  ISSN={2332-6441},
  month={June},}@INPROCEEDINGS{10628558,
  author={Fieblinger, Romy and Alam, Md Tanvirul and Rastogi, Nidhi},
  booktitle={2024 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)}, 
  title={Actionable Cyber Threat Intelligence Using Knowledge Graphs and Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={100-111},
  abstract={Cyber threats are constantly evolving. Extracting actionable insights from unstructured Cyber Threat Intelligence (CTI) data is essential to guide cybersecurity decisions. Increasingly, organizations like Microsoft, Trend Micro, and CrowdS trike are using generative AI to facilitate CTI extraction. This paper addresses the challenge of automating the extraction of actionable CTI using advancements in Large Language Models (LLMs) and Knowledge Graphs (KGs). We explore the application of state-of-the-art open-source LLMs, including the Llama 2 series, Mistral 7B Instruct, and Zephyr for extracting meaningful triples from CTI texts. Our methodology evaluates techniques such as prompt engineering, the guidance framework, and fine-tuning to optimize information extraction and structuring. The extracted data is then utilized to construct a KG, offering a structured and queryable representation of threat intelligence. Experimental results demonstrate the effectiveness of our approach in extracting relevant information, with guidance and fine-tuning showing superior performance over prompt engineering. However, while our methods prove effective in small-scale tests, applying LLMs to large-scale data for KG construction and Link Prediction presents ongoing challenges.},
  keywords={Large language models;Refining;Knowledge graphs;Organizations;Predictive models;Ontologies;Cyber threat intelligence;Cyber Threat Intelligence;Large Language Models;Knowledge Graphs;Threat Prediction},
  doi={10.1109/EuroSPW61312.2024.00018},
  ISSN={2768-0657},
  month={July},}@ARTICLE{10633699,
  author={Woods, Caitlin and Hodkiewicz, Melinda and French, Tim},
  journal={IEEE Access}, 
  title={Semantic Quality Assurance of Industrial Maintenance Procedures}, 
  year={2024},
  volume={12},
  number={},
  pages={122029-122046},
  abstract={Maintenance technicians in industry follow procedures that guide them through inspection, repair, and service tasks. Organisations seek to convert procedure documentation to machine-readable formats as their digital capabilities improve and regulatory requirements tighten. In this paper, we consider the opportunity for semantic quality assurance of digital procedures. We demonstrate a configurable and repeatable workflow containing three modules. The completeness module makes implicit information in procedures explicit using OpenAI’s Generative Pre-trained Transformer (GPT) model. The consistency module creates Resource Description Framework (RDF) triples that are aligned with, and checked against, the axioms of the open-source Ontology for Maintenance Procedure Documentation (OMPD). Finally, the correctness module performs closed-world checks on the RDF triples using the Shapes Constraints Language (SHACL). Each module can be used in isolation, or together, to realise an end-to-end semi-automated quality assurance workflow. Pre-processing of the raw maintenance procedure documents to extract entities (tools, materials and activities) and relations is achieved in a novel manner using prompt engineering with OpenAI’s GPT-3.5 Turbo model and few-shot learning. This end-to-end workflow enables organisations to perform quality assurance such as assessing the correct order for task sequences, and checking that all maintenance procedures have at least one maintenance task. We demonstrate this workflow on six procedures from the iFixit repository. The outputs of this workflow support maintenance technicians, planners and engineers by realising high-quality procedure documentation and automated procedure management update processes. The code and data used in this work is publicly available at https://github.com/equonto/quokka/.},
  keywords={Maintenance;Ontologies;OWL;Resource description framework;Data models;Documentation;Task analysis;Industrial ontology;ontology templates;OpenAI GPT;OTTR;SHACL;technical language processing},
  doi={10.1109/ACCESS.2024.3441757},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10621570,
  author={Motevallian, Mahsa and Esfar-E-Alam, AM and Taherkordi, Amir and Abbasi, Golnoush},
  booktitle={2024 20th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT)}, 
  title={Semantic Modeling of Waste Dataflow for Automating Circular Economy Systems}, 
  year={2024},
  volume={},
  number={},
  pages={677-684},
  abstract={Circular Economy (CE) is a model with a concrete action plan covering the whole life cycle of a product, from production and consumption to waste management (WM). Information technologies considerably contribute to the transition towards CE, e.g., waste tracking using Internet of Things (IoT). This will cause the businesses and organizations to confront a large diversity of data (i.e. waste amount, types, locations, etc.). The generated data is often stored and processed through manual or semi-manual methods by each business or organization. However, an automated method which can also interpret and integrate the diverse data in WM fields across different organizations is still in its infancy. Often, such data is not organized and falls short of reaching its full potential in facilitating coordinated management and enabling Circular Economy initiatives. In this paper, we aim to address this need through automated interpretation and integration of municipal waste data by applying semantic data modeling. Our approach proposes to capture the semantical description of entities in the WM process and their relations, which can appear between waste producers, authorities and consumers. Then, the obtained semantic model will facilitate and automate the required interpretation and integration of waste data, both for intra- and inter-organization scenarios. We realize intelligent semantic-based searching using natural language processing and large language models.},
  keywords={Waste management;Computational modeling;Large language models;Semantics;Organizations;Production;Manuals;Circular Economy;Waste Data Modeling;Semantic Data;Neural Search;NLP;Large Language Models},
  doi={10.1109/DCOSS-IoT61029.2024.00105},
  ISSN={2325-2944},
  month={April},}@INPROCEEDINGS{10611970,
  author={Lee, Chang-Shing and Wang, Mei-Hui and Chiang, Jun-Kui and Kubota, Naoyuki and Sato-Shimokawara, Eri and Nojima, Yusuke and Acampora, Giovanni and Wu, Pei-Yu and Chiu, Szu-Chi and Yang, Sheng-Chi and Siow, Chyan-Zheng},
  booktitle={2024 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
  title={Quantum Computational Intelligence with Generative AI Image for Human-Machine Interaction}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper introduces a Quantum Computational Intelligence (QCI) agent equipped with a content attention ontology model, specifically designed to enhance human-machine interaction based on a Generative Artificial Intelligence (GAI) image generation agent for Taiwanese/English learning and experience. Its diverse primary applications include social media analysis on Facebook groups and YouTube learning videos related to the 2023 IEEE CIS Education Portal (EP) Subcommittee, as well as in the areas of Taiwanese/English language learning and dialogue experience with GAI image generation. To establish the knowledge and inference models for the QCI agent, we initially developed a Taiwanese/English learning and experience ontology, including a content attention ontology, and an image attention ontology. The QCI agent utilizes metrics such as the number of views, posts, and comments to predict the fuzzy number of reactions. In addition, the GAI image agent generates Taiwanese speech-based/English text-based images and evaluates the fuzzy similarity score between Taiwanese/English and the attention ontology together with the Sentence BERT (SBERT) agent. This Taiwanese/English fuzzy similarity score is further validated through human assessments, with these evaluations subsequently serving as an additional metric for comparative analysis of Human-Machine Interaction (HMI). Furthermore, the GAI image agent is designed to create images and Chinese/English texts from text/speech translated by the Meta AI Universal Speech Translator (UST) Taiwanese/English agent. A Particle Swarm Optimization (PSO)-based machine learning mechanism is employed to train the QCI model for assessing learners' performance and predicting the performance of others. The National University of Tainan (NUTN) Taiwan-Large Language Model (NUTN.TW-LLM) agent has been further enhanced to support interactive learning experiences for HMI. An SBERT-based assessment agent is used to calculate fuzzy similarities between questions and answers in Taiwanese/English experiences and dialogues. Experimental results demonstrate the feasibility and efficacy of the proposed QCI model, equipped with QCI&AI-FML (Artificial Intelligence-Fuzzy Markup Language) and machine learning capabilities, for social media and language learning applications on HMI. In the future, we will extend the QCI model to various HMI applications for student learning around the world.},
  keywords={Human computer interaction;Measurement;Quantum computing;Social networking (online);Image synthesis;Generative AI;Computational modeling;Quantum CI Agent;Content Attention Ontology;ChatGPT;Generative AI Image Agent;IEEE CIS Education Portal;Fuzzy Markup Language;Sentence BERT;NUTN.TW-LLM},
  doi={10.1109/FUZZ-IEEE60900.2024.10611970},
  ISSN={1558-4739},
  month={June},}@INPROCEEDINGS{10605700,
  author={Safronov, Artyom A.},
  booktitle={2024 4th International Conference on Technology Enhanced Learning in Higher Education (TELE)}, 
  title={Using Neural Networks in Building an Ontology of Educational Subjects for Solving Educational Tasks}, 
  year={2024},
  volume={},
  number={},
  pages={189-191},
  abstract={In the modern world, information technologies have become an integral part of a human life, including the professional level. The rapid development of technologies based on artificial intelligence over the past few years has opened up new opportunities for their application in solving various educational tasks. One of the relevant topics for study is the investigation of ontological constructs in texts, identifying the terminology of concepts and determining the relationships between them. This article is dedicated to studying artificial intelligence systems as a tool for solving educational process tasks: it proposes the use of chatbots based on AI systems in conjunction with various digital tools in researching ontological constructs in educational texts. As part of the research, an example is provided with the processing an educational text on mathematics. Methodological characteristics are considered and a model for researching educational text using the ChatGPT is briefly described. Conclusions are drawn about the existing possibilities and difficulties in implementing the aforementioned model.},
  keywords={Systematics;Terminology;Neural networks;Chatbots;Educational courses;Mathematical models;Thesauri;artificial intelligence systems;ontology;thesaurus;chatbot;educational texts;digital tools},
  doi={10.1109/TELE62556.2024.10605700},
  ISSN={},
  month={June},}@INPROCEEDINGS{10605389,
  author={Schoch, Nicolai and Hoernicke, Mario},
  booktitle={2024 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={NL2IBE – Ontology-controlled Transformation of Natural Language into Formalized Engineering Artefacts}, 
  year={2024},
  volume={},
  number={},
  pages={997-1004},
  abstract={Looking at Process and Automation Engineering (P&AE) today, for the technically adept engineer, there are many different tools available to support the engineering work from translation of engineering intentions into module and plant descriptions, to definition and parametrization of entire process plant setups, for export to a control system. However, still today, in the very early engineering phases, engineering intentions either need to be entered already in a structured and controlled expert language or require a human expert’s manual efforts for translation from unstructured language into formalized representations, in order for thereon-based consistent further processing in the existing tools. This process is time-consuming, fuzzy, and error-prone due to potential misconceptions and ambiguities, even for domain experts. In this work, we therefore present our NL2IBE Tool, which makes use of modern Natural Language Processing in combination with Ontology Mining, and which, based on and controlled by an underlying ontology, allows for the deterministic transformation of natural language intentions into structured and consistent engineering artefacts. We describe the overall tool architecture as well as crucial functionalities and implementation features, followed by an evaluation by the example of a hydrogen generation and CCSU use case. We conclude with a discussion of the proposed tool and give an outlook on future research. (Abstract)},
  keywords={Automation;Hydrogen;Process control;Manuals;Ontologies;Control systems;Natural language processing;process &Amp; automation engineering;intend-based engineering;natural language processing;NLP;generative AI;ontological domain representation},
  doi={10.1109/CAI59869.2024.00182},
  ISSN={},
  month={June},}@INPROCEEDINGS{10595652,
  author={Arrotta, Luca and Bettini, Claudio and Civitarese, Gabriele and Fiori, Michele},
  booktitle={2024 IEEE International Conference on Smart Computing (SMARTCOMP)}, 
  title={ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models}, 
  year={2024},
  volume={},
  number={},
  pages={55-62},
  abstract={Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human activities. In this work, we propose ContextGPT: a novel prompt engineering approach to retrieve from LLMs common-sense knowledge about the relationship between human activities and the context in which they are performed. Unlike ontologies, ContextGPT requires limited human effort and expertise, while sharing similar privacy concerns if the reasoning is performed in the cloud. An extensive evaluation using two public datasets shows how a NeSy model obtained by infusing common-sense knowledge from ContextGPT is effective in data scarcity scenarios, leading to similar (and sometimes better) recognition rates than logic-based approaches with a fraction of the effort.},
  keywords={Knowledge engineering;Deep learning;Training;Privacy;Computational modeling;Ontologies;Human activity recognition;human activity recognition;context-awareness;large language models},
  doi={10.1109/SMARTCOMP61445.2024.00029},
  ISSN={2693-8340},
  month={June},}@INPROCEEDINGS{10597753,
  author={Helali, Mossad and Monjazeb, Niki and Vashisth, Shubham and Carrier, Philippe and Helal, Ahmed and Cavalcante, Antonio and Ammar, Khaled and Hose, Katja and Mansour, Essam},
  booktitle={2024 IEEE 40th International Conference on Data Engineering (ICDE)}, 
  title={KGLiDS: A Platform for Semantic Abstraction, Linking, and Automation of Data Science}, 
  year={2024},
  volume={},
  number={},
  pages={179-192},
  abstract={In recent years, we have witnessed the growing interest from academia and industry in applying data science technologies to analyze large amounts of data. In this process, a myriad of artifacts (datasets, pipeline scripts, etc.) are created. However, there has been no systematic attempt to holistically collect and exploit all the knowledge and experiences that are implicitly contained in those artifacts. Instead, data scientists recover information and expertise from colleagues or learn via trial and error. Hence, this paper presents a scalable platform, KGLiDS, that employs machine learning and knowledge graph technologies to abstract and capture the semantics of data science artifacts and their connections. Based on this information, KGLiDS enables various downstream applications, such as data discovery and pipeline automation. Our comprehensive evaluation covers use cases in data discovery, data cleaning, transformation, and AutoML. It shows that KGLiDS is significantly faster with a lower memory footprint than the state-of-the-art systems while achieving comparable or better accuracy.},
  keywords={Automation;Accuracy;Systematics;Semantics;Pipelines;Machine learning;Knowledge graphs;Linked Data Science;Knowledge Graphs;Graph Neural Networks;Data Integration;Data Discovery},
  doi={10.1109/ICDE60146.2024.00021},
  ISSN={2375-026X},
  month={May},}@INPROCEEDINGS{10597810,
  author={Cavalleri, Emanuele and Mesiti, Marco},
  booktitle={2024 IEEE 40th International Conference on Data Engineering (ICDE)}, 
  title={Construction and Enhancement of an RNA-Based Knowledge Graph for Discovering New RNA Drugs}, 
  year={2024},
  volume={},
  number={},
  pages={5639-5643},
  abstract={Cutting-edge technologies in RNA biology are pushing the study of fundamental biological processes and human diseases and accelerate the development of new drugs tailored to the patient's biomolecular characteristics. Even if many structured and unstructured data sources report the interaction among different RNA molecules and some other biomedical entities (e.g., drugs, diseases, genes), we still lack a comprehensive and well-described RNA-centered Knowledge Graph (KG) that contains such information and sophisticated services that support the user in its creation, maintenance, and enhancement. This PhD project aims to create a biomedical KG (named RNA-KG) to represent, and eventually infer, biological, experimentally validated interactions between different RNA molecules. We also wish to enhance the KG content and develop sophisticated services designed ad-hoc to support the user in predicting uncovered relationships and identifying new RNA-based drugs. Services will rely on deep learning methods that consider the heterogeneity of the graph and the presence of an ontology that describes the possible relationships existing among the involved entities. Moreover, we will consider Large Language Models (LLMs) in combination with RNA-KG for interacting with the user with the ground truth information contained in our KG for extracting relationships from unstructured data sources.},
  keywords={Drugs;Knowledge engineering;Deep learning;RNA;Soft sensors;Large language models;Knowledge graphs;Biomedical knowledge graphs;Graph representation learning;LLMs;RNA therapeutics},
  doi={10.1109/ICDE60146.2024.00453},
  ISSN={2375-026X},
  month={May},}@INPROCEEDINGS{10588309,
  author={Peng, Tao and Rao, Taiwen and Xu, Yansong and Yang, Chao and Xie, Xiaotian and Yang, Chunhua},
  booktitle={2024 36th Chinese Control and Decision Conference (CCDC)}, 
  title={Construction of Rail Transit Network Fault Knowledge Graph Based on Pseudo-Dynamic Relationship Ontology Architecture}, 
  year={2024},
  volume={},
  number={},
  pages={4582-4587},
  abstract={In this paper, an approach to construction of rail transit network fault knowledge graph based on pseudo-dynamic relationship ontology architecture is proposed. Firstly, an ontology architecture that includes pseudo-dynamic relationships is employed to completely separate fault patterns from fault entities without losing semantics. Secondly, a named entity recognition method based on the BERT-BiGRU-CRF model is adopted, which performs well in short entity extraction tasks. Thirdly, a pseudo-dynamic relationship extraction method based on the BERT-MEA(multi entities attention) model is adopted to extract static simple relationships, followed by treating triples as head and tail entities to determine pseudo-dynamic relationships. The implementation of rail transit network fault knowledge graph demonstrates the effectiveness of the proposed approach.},
  keywords={Rails;Databases;Semantics;Knowledge graphs;Tail;Named entity recognition;Ontologies;Ontology architecture;Pseudo-dynamic relationship;Short entity;Fault knowledge graph},
  doi={10.1109/CCDC62350.2024.10588309},
  ISSN={1948-9447},
  month={May},}@INPROCEEDINGS{10582050,
  author={Vanitha, V. and Antony Rai, A. Stephan and Vinodhini, D. and Gnanaprasanambikai, L. and Kumar, L. Senthil and Renukadevi, S.},
  booktitle={2024 International Conference on Advances in Modern Age Technologies for Health and Engineering Science (AMATHE)}, 
  title={Synergy of Human Language Processing and Artificial Intelligence}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={This study explores the intriguing harmony between human language processing and artificial intelligence (AI). We delve into the intricate process of translating mental concepts into linguistic expressions, akin to the capabilities of AI language models. Through a survey of existing research, we uncover the parallelism between the ontological taxonomy guiding human cognition and AI's language understanding mechanisms. Moreover, we investigate how AI's multilingual proficiency mirrors the cognitive multilingualism found in individuals. This convergence holds implications for cross-lingual communication and AI-human collaboration. Our analysis anticipates a symbiotic future where the interplay of cognitive insights and AI advancements amplifies the potential of both realms.},
  keywords={Symbiosis;Surveys;Computational modeling;Taxonomy;Semantics;Linguistics;Mathematical models;Language Cognition;Artificial Intelligence;On tological Taxonomy;Multilingualism;Cross-Lingual Communication},
  doi={10.1109/AMATHE61652.2024.10582050},
  ISSN={},
  month={May},}@ARTICLE{10592819,
  author={Sun, Zhigang and Wang, Zixu and Halilaj, Lavdim and Luettin, Juergen},
  journal={IEEE Robotics and Automation Letters}, 
  title={SemanticFormer: Holistic and Semantic Traffic Scene Representation for Trajectory Prediction Using Knowledge Graphs}, 
  year={2024},
  volume={9},
  number={9},
  pages={7381-7388},
  abstract={Trajectory prediction in autonomous driving relies on accurate representation of all relevant contexts of the driving scene, including traffic participants, road topology, traffic signs, as well as their semantic relations to each other. Despite increased attention to this issue, most approaches in trajectory prediction do not consider all of these factors sufficiently. We present SemanticFormer, an approach for predicting multimodal trajectories by reasoning over a semantic traffic scene graph using a hybrid approach. It utilizes high-level information in the form of meta-paths, i.e. trajectories on which an agent is allowed to drive from a knowledge graph which is then processed by a novel pipeline based on multiple attention mechanisms to predict accurate trajectories. SemanticFormer comprises a hierarchical heterogeneous graph encoder to capture spatio-temporal and relational information across agents as well as between agents and road elements. Further, it includes a predictor to fuse different encodings and decode trajectories with probabilities. Finally, a refinement module assesses permitted meta-paths of trajectories and speed profiles to obtain final predicted trajectories. Evaluation of the nuScenes benchmark demonstrates improved performance compared to several SOTA methods. In addition, we demonstrate that our knowledge graph can be easily added to two graph-based existing SOTA methods, namely VectorNet and LaFormer, replacing their original homogeneous graphs. The evaluation results suggest that by adding our knowledge graph the performance of the original methods is enhanced by 5% and 4%, respectively.},
  keywords={Trajectory;Knowledge graphs;Semantics;Ontologies;Encoding;Predictive models;Transformers;Semantic scene understanding;autonomous agents;intelligent transportation systems},
  doi={10.1109/LRA.2024.3426386},
  ISSN={2377-3766},
  month={Sep.},}@INPROCEEDINGS{10578858,
  author={Lehmann, Alexander and Landes, Dieter},
  booktitle={2024 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Extracting Metadata from Learning Videos for Ontology-Based Recommender Systems Using Whisper & GPT}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={In modern education, individualized learning environments play a vital role by allowing learners to tailor their learning paths based on personal needs, interests, and abilities. Achieving effective individualization relies on dynamic adaptation of the learning path, typically facilitated by recommender systems. These systems offer personalized suggestions, commonly employing content-based or collaborative filtering approaches. However, traditional recommender systems often lack consideration of the semantics of learning elements. To address this limitation, ontology-based recommender systems integrate semantic modeling, establishing additional connections within a domain to enhance precision and context in recommendations. Notably, these systems mitigate the cold start problem and are particularly advantageous in learning environments with limited data. While videos are prevalent in learning platforms, their unstructured nature poses challenges for processing. This paper introduces an innovative approach, leveraging Large Language Models, specifically GPT, to extract metadata from learning videos. The proposed method intelligently augments videos and links them to a domain ontology, enabling the integration of videos into ontology-based recommender systems. The application of this approach is demonstrated through a case study in software engineering education, showcasing its potential to enhance individualized learning experiences in specific domains. The presented method offers an automated alternative to manual video processing, aligning with the evolving landscape of education technology.},
  keywords={Large language models;Semantics;Manuals;Metadata;Ontologies;Engineering education;Recommender systems;learning analytics;adaptive learning environments;generative AI;large language models;ontology-based recommender systems;learning videos},
  doi={10.1109/EDUCON60312.2024.10578858},
  ISSN={2165-9567},
  month={May},}@INPROCEEDINGS{10569741,
  author={Keber, M. and Grubišić, I. and Barešić, A. and Jović, A.},
  booktitle={2024 47th MIPRO ICT and Electronics Convention (MIPRO)}, 
  title={A Review on Neuro-symbolic AI Improvements to Natural Language Processing}, 
  year={2024},
  volume={},
  number={},
  pages={66-72},
  abstract={Symbolic artificial intelligence (AI) reflects the domain knowledge of experts and adheres to the logic of the subject area, rules, or any relations between entities. Connectionist (neuro) approaches based on artificial neural networks are excellent for extracting abstract features, contextualizing, and embedding interactions between features. When connectionist and symbolic approaches are properly aligned in a model, they benefit from complementary strengths; the combination is referred to as a hybrid or neuro-symbolic artificial intelligence (NSAI) model. The advantages that NSAI brings to the field of natural language processing (NLP) have received little attention from researchers in recent years. Therefore, in this review, we focus on the impact of neuro-symbolic approaches for NLP tasks, i.e. text classification, information extraction, machine translation, and language understanding. Relevant research articles from Scopus, Web of Science, and Google Scholar were carefully examined using appropriate keywords in the period from 2019 to 2024. The review aims to show the types of NSAI systems, identify the motivation for using NSAI, evaluate the use of additional annotations for content description, and briefly describe how the neuro-symbolic connection improves the methodology and enables trustworthy and explainable AI systems in current NLP research. The review also highlights areas of application and improvements achieved by NSAI approaches in benchmarks.},
  keywords={Training;Reviews;Annotations;Text categorization;Feature extraction;Natural language processing;Machine translation;neuro-symbolic artificial intelligence;natural language processing;knowledge representation;deep learning},
  doi={10.1109/MIPRO60963.2024.10569741},
  ISSN={2623-8764},
  month={May},}@INPROCEEDINGS{10553629,
  author={Melzer, Sylvia and Weilkiens, Tim and Muggeo, Christian and Berres, Axel},
  booktitle={2024 IEEE International Systems Conference (SysCon)}, 
  title={Sustainable Development of Information Systems Using SysML, FAS and DOL}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={The use of product families can improve the efficiency of product development as opposed to develop a single-product by reusing existing artefacts and optimizing variability, which leads to a saving of resources and is therefore a sustainable approach. To extend new variants of an already modelled product via the approach of a product family the challenge is to merge the product models, so that the same and varying parts in the model are semantically correct identified and mapped to each other. This paper proposes a comprehensive approach for sustainable development of product families using the Distributed Ontology Language (DOL), Functional Architectures for Systems (FAS), and Systems Modeling Language (SysML). The proposed approach integrates the idea of DOL to represent an ontology of sustainability criteria and to map them to the functional architecture of a product family. FAS is used to model the functional architecture of the product family, while the FAS ontology is used to formalize the functional architecture and provide a standardized vocabulary and set of rules for modelling the functional aspects of the product family. SysML is used to model the product family. The proposed method is demonstrated through a case study of developing a sustainable product family of vacuum cleaner robots. The results show that the proposed method can help to identify opportunities for reducing environ-mental impact and improving social responsibility in the product family design while ensuring that functional requirements and design constraints are met semantically correct.},
  keywords={Vocabulary;Biological system modeling;Ontologies;Systems Modeling Language;Product development;Product design;Sustainable development;sustainability;functional architectures for systems;Distributed Ontology Language;product family;SysML},
  doi={10.1109/SysCon61195.2024.10553629},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{10554074,
  author={Chiş, Andrei and Stoica, Oliviu Ionuţ and Ghiran, Ana-Maria and Buchmann, Robert Andrei},
  booktitle={2024 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)}, 
  title={A Knowledge Graph Approach to Cyber Threat Mitigation Derived from Data Flow Diagrams}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Data Flow Diagrams (DFD) have proven effective in designing and analyzing the flow of data in enterprise systems. They serve as indispensable tools for enterprises that are undergoing transition to cloud services. DFDs aid in understanding the current processes, identifying interfaces and integration points that require security measures. This paper reports a Design Science project to mitigate the cyber security threats at the design phase of a system and to perform auditing of an existing system through knowledge graphs. The proposal leverages knowledge gathered from various sources in a knowledge graph to identify semantic relationships and patterns, enabling automated inference, analysis and detection of vulnerability patterns. Furthermore, LLM-based (large language models) capabilities transform data management details captured as Data Flow Diagrams (DFD) into knowledge graphs for semantic querying and improved decision support.},
  keywords={Current measurement;Semantics;Knowledge graphs;Transforms;Data models;Security;Proposals;knowledge graphs;security;privacy;data flow diagrams;threat modeling;LLMs},
  doi={10.1109/AQTR61889.2024.10554074},
  ISSN={1844-7872},
  month={May},}@ARTICLE{10553231,
  author={Li, Xiaodong and Tian, Guohui and Cui, Yongcheng},
  journal={IEEE Robotics and Automation Letters}, 
  title={Fine-Grained Task Planning for Service Robots Based on Object Ontology Knowledge via Large Language Models}, 
  year={2024},
  volume={9},
  number={8},
  pages={6872-6879},
  abstract={In domestic environment, the successful execution of service tasks heavily relies on the robot's capability to identify and understand objects within its surrounding. This crucial process predominantly takes place during task planning, prior to the actual performance of service tasks. Therefore, it is vital that the robot is capable of formulating object-specific action sequences through task planning. In this letter, we propose the Fine-Grained Task Planning (FGTP) framework, an innovative method that combines object ontology knowledge with Large Language Models (LLMs) to create detailed action sequences. The FGTP framework is uniquely designed to process both text descriptions of service tasks and images of relevant objects, enabling a thorough comprehension of object attributes essential for task execution. Moreover, we have developed a set of rules based on these attributes to assist in the robot's decision-making process. In scenarios where service tasks fail because the object is in an unsuitable state, our framework deploys a logic-based reasoning method, concentrating on object attributes to identify suitable substitutes. This process leverages a pre-established semantic map to locate these alternatives, thus enabling a transition back to standard task planning. Our evaluations, conducted in both the VirtualHome simulation environment and with the TIAGo real robot, demonstrate the efficacy of our approach. This confirms our framework's capability to generate practical and implementable plans for various service tasks.},
  keywords={Task analysis;Planning;Ontologies;Service robots;Robot kinematics;Object recognition;Task planning;service robotics},
  doi={10.1109/LRA.2024.3412593},
  ISSN={2377-3766},
  month={Aug},}@ARTICLE{10552698,
  author={Larhrib, Mohamed and Escribano, Miguel and Cerrada, Carlos and Escribano, Juan Jose},
  journal={IEEE Access}, 
  title={An Ontological Behavioral Modeling Approach With SHACL, SPARQL, and RDF Applied to Smart Grids}, 
  year={2024},
  volume={12},
  number={},
  pages={82041-82056},
  abstract={Every engineering process, especially software, involves two complementary aspects: structural and behavioral. Behavior is, in essence, transforming the structure associated with the system. As a language for the object-oriented paradigm, Unified Modeling Language (UML) offers constructs for both aspects, for example, class diagrams for the structural aspect and activity diagrams for the behavioral aspect. However, without obtaining directly executable models, in glass-box terms, or reasoning support, on the other hand, when software engineering is approached with ontologies, only constructs for structural aspects are provided to develop a directly executable model, thanks to their reasoning capability. However, there are no constructs or approaches for this paradigm’s specification or definition of behavior. This lack appears mainly in the early stages of the software engineering process, where there are no constructs similar to, e.g., the activity diagram in the object-oriented domain. Object Management Group (OMG) already addressed the transformation between the two paradigms in structural terms throughout Ontology Definition Metamodel (ODM) from UML to Resource Description Framework (RDF) and Web Ontology Language (OWL). However, there is no transformation of the object-oriented behavioral constructs into ontologies because they are not defined in the ontological paradigm. This paper addresses the definition of behavior in the ontology paradigm and the transformation of behavioral constructs between the two paradigms. The foundation of behavior specification is the flow concept, and the basis of this is the transformation of the structural model in an evaluative sense. Therefore, once the behavior has been defined in the ontology domain, the artifacts obtained throughout the life cycle are directly executable, and their validation and testing are automatic. With this approach, the life cycle is reduced to a modeling process. Thus, the resulting software engineering process improves features such as agility, simplicity, productivity, and formalism. The target audience for this work is the software engineering community, especially in the Model-Driven Engineering (MDE) paradigm approached from object-oriented and ontology perspectives. The evaluation of the proposed approach has been performed in the electric utilities, solving the problem of the validation flow for the interoperability process specified by the Common Grid Model Exchange Standard (CGMES) standard.},
  keywords={Unified modeling language;Ontologies;Object oriented modeling;Resource description framework;Software engineering;OWL;Cognition;Behavioral sciences;Behavioral modeling;CIM for ENTSO-E (CGMES);directly executable;ontology RDF/RDFS/OWL/SHACL},
  doi={10.1109/ACCESS.2024.3412656},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10544941,
  author={Dequan, Gao and Pengyu, Zhu and Sheng, Wang and Ziyan, Zhao},
  booktitle={2024 6th Asia Energy and Electrical Engineering Symposium (AEEES)}, 
  title={Deep Learning-Based Fault Knowledge Graph Construction for Power Communication Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1088-1093},
  abstract={Power communication network is a crucial infrastructure in the model power system, and its maintenance capability are crucial to ensuring the stable operation of power grid business. As an organized semantic knowledge base, the knowledge graph effectively organizes power communication network fault documentation and expert experience to enhance intelligent maintenance. This paper outlines a top-down approach to systematically construct a fault knowledge graph in the domain of power communication networks. The approach utilizes a seven-step method to establish a domain ontology model and integrates deep learning algorithms, including pre-trained language models, bidirectional long short time memory networks, convolutional neural networks and attention mechanisms. These algorithms process unstructured text to extract key entities and relationships. The effectiveness of the approach is verified through experiments using a product device document as a test case. Extracted knowledge is then visualized and stored using Neo4j database. Finally, this paper proposes a knowledge service model centered on fault knowledge graph and explores its application in fault diagnosis.},
  keywords={Deep learning;Patents;Semantics;Knowledge graphs;Ontologies;Real-time systems;Power grids;power communication networks;fault knowledge graph;deep learning;fault diagnosis},
  doi={10.1109/AEEES61147.2024.10544941},
  ISSN={},
  month={March},}@INPROCEEDINGS{10540801,
  author={Libro, Mario and Gaiardelli, Sebastiano and Lora, Michele and Fummi, Franco},
  booktitle={2024 IEEE International Conference on Industrial Technology (ICIT)}, 
  title={Integrating Modeling Languages with Ontologies in the Context of Industry 4.0}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={The evolving landscape of manufacturing systems and the increasing complexity of production lines necessitate innovative approaches for efficient information management and process modeling. The System Modeling Language (SysML) provides a powerful language to express such information. However, the expressiveness comes at a cost: on the one hand, the modeling phase requires a deep understanding of the domain; on the other, SysML lacks rigorous semantics. This work introduces a novel methodology that enriches the SysML with ontology reasoning in the context of manufacturing systems. The approach uses ontologies as a comprehensive knowledge base that encapsulates essential details about the machinery, their provided functions, and the associated constraints. The approach offers a reliable and efficient way to verify the consistency and correctness of production recipes: it ensures recipes' practical applicability in the manufacturing process while reducing errors that can occur in the modeling phase. The proposed methodology has been validated through its application to a fully-fledged manufacturing line, showing its applicability in real-world scenarios.},
  keywords={Manufacturing processes;Process modeling;Knowledge based systems;Ontologies;Cognition;Systems Modeling Language;Reliability;Computer-aided manufacturing;process modeling;knowledge representation},
  doi={10.1109/ICIT58233.2024.10540801},
  ISSN={2643-2978},
  month={March},}@INPROCEEDINGS{10521188,
  author={Timperley, Louis and Berthoud, Lucy and Snider, Chris and Tryfonas, Theo},
  booktitle={2024 IEEE Aerospace Conference}, 
  title={Mapping the MBSE Environment and Complementary Design Space Exploration Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={1-20},
  abstract={Today’s MBSE tools and environments are highly varied and therefore present a challenge for organizations looking to implement MBSE. Furthermore, while MBSE environments are highly capable of supporting the description of design baselines, the current capabilities within these environments could be further refined for exploring alternative designs. As a result it is important to gain an understanding of the limitations of current MBSE tooling in performing the valuable activity of design space exploration, and identify a set of candidate techniques to combat these. This paper reviews the various options available to MBSE practitioners by comparing some of the most common MBSE languages, tools and methods. The possible issues that can be encountered when exploring different designs have been identified and assigned a severity rating. A set of design space exploration techniques are presented, and where possible these have been sourced from existing literature. A knowledge graph has been constructed to collect all this data into a structured format, containing all the MBSE languages, tools, methods, design space exploration-related issues and techniques, as well as the relationships between each of these. This knowledge graph, implemented as a Neo4j graph database, allowed deeper insights to be drawn from the collected information. By defining a selected MBSE environment, including language, tool and method, the knowledge graph could be used to identify the least troublesome sequence (with minimum number of related issues) to arrive at a desired design artifact, for example a set of optimized system parameters. Beside this, the knowledge graph could be used to display the relationships and clusters of MBSE languages, tools and methods, to assist organizations with selecting suitable MBSE environment elements. Future work will bring greater depth to the analysis available with the knowledge graph, for instance, differentiation between different types of design space exploration issues and techniques.},
  keywords={Codes;Reviews;Databases;Design methodology;Knowledge based systems;Knowledge graphs;Organizations},
  doi={10.1109/AERO58975.2024.10521188},
  ISSN={1095-323X},
  month={March},}@INPROCEEDINGS{10521022,
  author={Peer, Jordan and Mordecai, Yaniv and Reich, Yoram},
  booktitle={2024 IEEE Aerospace Conference}, 
  title={NLP4ReF: Requirements Classification and Forecasting: From Model-Based Design to Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-16},
  abstract={We introduce Natural Language Processing for Requirement Forecasting (NLP4ReF), a model-based machine learning and natural language processing solution for enhancing the Requirements Engineering (RE) process. RE continues to face significant challenges and demands innovative approaches for process efficiency. Traditional RE methods relying on natural language struggle with incomplete, hidden, forgotten, and evolving requirements during and after the critical design review, risking project failures and setbacks. NLP4ReF tackles several key challenges: a) distinguishing between functional and non-functional requirements, b) classification of requirements by their respective system classes, and c) generation of unanticipated requirements to enhance project success. NLP4ReF employs a common natural language toolkit (NLTK) package and the recently-trending Chat-GPT. We tested NLP4ReF on PROMISE_exp, a pre-existing dataset with 1000 software requirements, and PROMISE_IoT, an enhanced dataset with 2000 software and IoT requirements. We validated NLP4ReF on a genuine IoT project. NLP4ReF swiftly generated dozens of new requirements, verified by a team of systems engineers, of which over 70% were crucial for project success. We found that GPT is superior in authentic requirement generation, while NLTK excels at requirement classification. NLP4ReF offers significant time saving, effort reduction, and improved future-proofing. Our model-based design approach provides a foundation for enhanced RE practices and future research in this domain.},
  keywords={Software algorithms;Training data;Software;Natural language processing;Data models;Classification algorithms;Requirements engineering;Natural Language Processing;Requirements Engineering Requirement Forecasting;Internet of Things;Machine Learning;Model-Based Systems Engineering},
  doi={10.1109/AERO58975.2024.10521022},
  ISSN={1095-323X},
  month={March},}@ARTICLE{10508456,
  author={Yang, Puhai and Huang, Heyan and Shi, Shumin and Mao, Xian-Ling},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={STN4DST: A Scalable Dialogue State Tracking Based on Slot Tagging Navigation}, 
  year={2024},
  volume={32},
  number={},
  pages={2494-2507},
  abstract={Dialogue state tracking plays a key role in tracking user intentions in task-oriented dialogue systems. Traditional dialogue state tracking methods usually rely on selecting slot values from a fixed ontology to represent the dialogue state. In recent years, more flexible open vocabulary based approaches have become the mainstream focus which are mainly divided into two categories: generative methods and span extraction methods. Among them, the span extraction method is favored for its outstanding ability to predict unknown slot values. However, the span extraction method only focuses on the predicted slot values, but ignores other potential slot values in the utterance, which leads to insufficient semantic understanding of the utterance and difficulty in dealing with complex utterance scenarios, such as more or longer unknown slot values. To tackle the above drawbacks, in this paper, we propose a novel scalable dialogue state tracking method, which employs slot tagging to locate all potential slot values in the utterances and jointly learns slot pointers to select the predicted slot value from them. Specifically, our STN4DST (Slot Tagging Navigation for Dialogue State Tracking) model not only adopts the above joint learning strategy, which we call slot tagging navigation, to extract slot values from utterances, but also uses previous dialogue states as dialogue contexts to track the change of slot values, and introduces appendix slot values to predict special slot values that cannot be extracted. Extensive experiments show that in the open vocabulary setting, STN4DST achieves the state-of-the-art joint goal accuracy of 85.4% and 96.5% on Sim-M and Sim-R datasets with a large number of unknown slot values, and is also comparable to other state-of-the-art models in the absence of token-level slot annotations for all potential slot values.},
  keywords={Tagging;Navigation;Speech processing;Vocabulary;Semantics;Predictive models;Ontologies;Task-oriented dialogue system;dialogue state tracking;scalable DST;unknown slot value},
  doi={10.1109/TASLP.2024.3393733},
  ISSN={2329-9304},
  month={},}@INPROCEEDINGS{10507138,
  author={Anass, Bayaga},
  booktitle={2024 Conference on Information Communications Technology and Society (ICTAS)}, 
  title={Advancing STEM cognition with current AI landscape and systems}, 
  year={2024},
  volume={},
  number={},
  pages={20-25},
  abstract={Application of AI explores the potential of algorithms to ensure fairness, accuracy, and efficiency in grading students' performance, offering valuable insights into their strengths and areas for improvement. While the current AI landscape showcases remarkable progress, there are several areas ripe for exploration. One such avenue is AI steps to consider in STEM, wherein researchers aim to develop specialised steps/models to understand and generate domain-specific STEM content. The systematic literature review highlighted the importance of domain adaptation techniques for enhancing STEM comprehension by fine-tuning transformer-based language models like BERT. Integrating domain knowledge through ontology-based and context of STEM disciplines. Future research should focus on building domain-specific annotated datasets to improve the performance models in STEM comprehension. Additionally, exploring unsupervised domain adaptation techniques and leveraging domain-specific knowledge graphs can further enhance the NLP models’ adaptability to diverse STEM domains.},
  keywords={Adaptation models;Vocabulary;Systematics;Bibliographies;Laboratories;Knowledge graphs;Transformers;Data models;Artificial intelligence;STEM;Artificial Intelligence;STEM Education;Explainable AI;Natural Language Processing;AI-Augmented Laboratories},
  doi={10.1109/ICTAS59620.2024.10507138},
  ISSN={},
  month={March},}@INPROCEEDINGS{10485759,
  author={Zhu, Ruiliang and Song, Xiangshuai and Zhang, Hao and Cai, Xuli},
  booktitle={2024 IEEE 3rd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)}, 
  title={Joint Extraction of Entity Relationships in Walnut Disease and Pest Based on Chinese NLP Models}, 
  year={2024},
  volume={},
  number={},
  pages={1027-1035},
  abstract={This study addresses the limited application of deep learning techniques in the field of walnut disease and pest and the challenges posed by complex relationships and diverse entity types in this domain. We propose a deep learning-based method for constructing a knowledge graph in the walnut disease and pest domain, incorporating ontologies to establish a conceptual model for the disease and pest knowledge graph. To overcome issues such as relationship overlap (e.g., one-to-many, many-to-many) and loss of relationship chains, we introduce a novel labeling scheme called “based on ontology binding BIESO (Begin-Inside-End-Single-Other)” that directly models triplets. By employing a label matching algorithm, we obtain triplet data. We train and predict on the dataset using an end-to-end model consisting of Bidirectional Encoder Representations from Transformers (BERT), Bi-directional Gate Recurrent Unit (BiGRU), and Conditional Random Field (CRF). Experimental results show an F1 score of 75.79%, outperforming models such as BERT-BiLSTM-CRF and word2vec-BiGRU-CRF. We semiautomatically extract unstructured knowledge and store the extracted triplets in a Neo4j graph database, enabling visualization of the knowledge. The research methodology of this knowledge graph can serve as a reference for constructing knowledge graphs in walnut agriculture and developing intelligent question-answering systems for walnut disease and pest.},
  keywords={Knowledge graphs;Bidirectional control;Ontologies;Predictive models;Logic gates;Transformers;Prediction algorithms;Walnut disease and pest;ontology;BERTBiGRU-CRF;knowledge graph;deep learning},
  doi={10.1109/EEBDA60612.2024.10485759},
  ISSN={},
  month={Feb},}@ARTICLE{10487851,
  author={Strader, Jared and Hughes, Nathan and Chen, William and Speranzon, Alberto and Carlone, Luca},
  journal={IEEE Robotics and Automation Letters}, 
  title={Indoor and Outdoor 3D Scene Graph Generation Via Language-Enabled Spatial Ontologies}, 
  year={2024},
  volume={9},
  number={6},
  pages={4886-4893},
  abstract={This letter proposes an approach to build 3D scene graphs in arbitrary indoor and outdoor environments. Such extension is challenging; the hierarchy of concepts that describe an outdoor environment is more complex than for indoors, and manually defining such hierarchy is time-consuming and does not scale. Furthermore, the lack of training data prevents the straightforward application of learning-based tools used in indoor settings. To address these challenges, we propose two novel extensions. First, we develop methods to build a spatial ontology defining concepts and relations relevant for indoor and outdoor robot operation. In particular, we use a Large Language Model (LLM) to build such an ontology, thus largely reducing the amount of manual effort required. Second, we leverage the spatial ontology for 3D scene graph construction using Logic Tensor Networks (LTN) to add logical rules, or axioms (e.g., “a beach contains sand”), which provide additional supervisory signals at training time thus reducing the need for labelled data, providing better predictions, and even allowing predicting concepts unseen at training time. We test our approach in a variety of datasets, including indoor, rural, and coastal environments, and show that it leads to a significant increase in the quality of the 3D scene graph generation with sparsely annotated data.},
  keywords={Three-dimensional displays;Ontologies;Semantics;Training data;Solid modeling;Artificial intelligence;Semantics;Image analysis;Spatial resolution;Indoor environment;AI-based methods;3D scene graphs;semantic scene understanding;spatial ontologies},
  doi={10.1109/LRA.2024.3384084},
  ISSN={2377-3766},
  month={June},}@INPROCEEDINGS{10475639,
  author={Vizcarra, Julio and Haruta, Shuichiro and Kurokawa, Mori},
  booktitle={2024 IEEE 18th International Conference on Semantic Computing (ICSC)}, 
  title={Representing the Interaction between Users and Products via LLM-assisted Knowledge Graph Construction}, 
  year={2024},
  volume={},
  number={},
  pages={231-232},
  abstract={To understand user behavior, representing the semantic knowledge of user-product interaction is essential. In this paper, we represent the interaction between user and product via large language model (LLM)-assisted knowledge graph construction. We capture users’ behavioral actions and static properties of the products from raw text data of “user review” and “product catalog”. Moreover, the information needed for updating the knowledge graph is captured by raw texts of “news related to the products”. The proposed methodology integrates them as a single knowledge graph to provide causal reasoning on user-product interaction. To alleviate the situation where a small quantity of annotated text exists in these data, we use LLM as a data annotator and augmentor.},
  keywords={Text mining;Reviews;Annotations;Semantics;Knowledge graphs;Data augmentation;Cognition;Knowledge graph;text mining;ontology;causality;LLM;user-product interaction},
  doi={10.1109/ICSC59802.2024.00043},
  ISSN={2472-9671},
  month={Feb},}@INPROCEEDINGS{10446860,
  author={Luo, Zhizhao and Wang, Youchen and Ke, Wenjun and Qi, Rui and Guo, Yikai and Wang, Peng},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Boosting LLMS with Ontology-Aware Prompt for Ner Data Augmentation}, 
  year={2024},
  volume={},
  number={},
  pages={12361-12365},
  abstract={Named Entity Recognition (NER) data augmentation (DA) aims to improve the performance and generalization capabilities of NER models by generating scalable training data. The key challenge lies in ensuring the generated samples maintain contextual diversity while preserving label consistency. However, existing dominant methods fail to simultaneously satisfy both criteria. Inspired by the extensive generative capabilities of large language models (LLMs), we propose ANGEL, a frAmework integrating the oNtoloGy structure and instructivE prompting within LLMs. Specifically, the hierarchical ontology structure guides prompt ranking, while instructive prompting enhances LLMs’ mastery of domain knowledge, empowering synthetic sample generation and annotation. Experiments show ANGEL surpasses state-of-the-art (SOTA) baselines, conferring absolute F1 increases of 2.86% and 0.93% on two benchmark datasets, respectively.},
  keywords={Training data;Speech recognition;Ontologies;Syntactics;Signal processing;Data augmentation;Boosting;Named Entity Recognition;Data Augmentation;Large language Model;Knowledge Graph},
  doi={10.1109/ICASSP48485.2024.10446860},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10446928,
  author={Liu, Wuyang and Ren, Yanzhen},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Semantic Proximity Alignment: Towards Human Perception-Consistent Audio Tagging by Aligning with Label Text Description}, 
  year={2024},
  volume={},
  number={},
  pages={541-545},
  abstract={Most audio tagging models are trained with one-hot labels as supervised information. However, one-hot labels treat all sound events equally, ignoring the semantic hierarchy and proximity relationships between sound events. In contrast, the event descriptions contains richer information, describing the distance between different sound events with semantic proximity. In this paper, we explore the impact of training audio tagging models with auxiliary text descriptions of sound events. By aligning the audio features with the text features of corresponding labels, we inject the hierarchy and proximity information of sound events into audio encoders, improving the performance while making the prediction more consistent with human perception. We refer to this approach as Semantic Proximity Alignment (SPA). We use Ontology-aware mean Average Precision (OmAP) as the main evaluation metric for the models. OmAP reweights the false positives based on Audioset ontology distance and is more consistent with human perception compared to mAP. Experimental results show that the audio tagging models trained with SPA achieve higher OmAP compared to models trained with one-hot labels solely (+1.8 OmAP). Human evaluations also demonstrate that the predictions of SPA models are more consistent with human perception.},
  keywords={Training;Measurement;Semantics;Natural languages;Tagging;Signal processing;Predictive models;Audio Classification;Sound Event Detection;Multi-modality;Audio-text Pretraining;Transformer},
  doi={10.1109/ICASSP48485.2024.10446928},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10446325,
  author={Xu, Hongshen and Cao, Ruisheng and Zhu, Su and Jiang, Sheng and Zhang, Hanchong and Chen, Lu and Yu, Kai},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A Birgat Model for Multi-Intent Spoken Language Understanding with Hierarchical Semantic Frames}, 
  year={2024},
  volume={},
  number={},
  pages={12251-12255},
  abstract={Previous work on spoken language understanding (SLU) mainly focuses on single-intent settings, where each input utterance merely contains one user intent. This configuration significantly limits the surface form of user utterances and the capacity of output semantics. In this work, we firstly propose a Multi-Intent dataset which is collected from a realistic in-Vehicle dialogue System, called MIVS. The target semantic frame is organized in a 3-layer hierarchical structure to tackle the alignment and assignment problems in multi-intent cases. Accordingly, we devise a BiRGAT model to encode the hierarchy of ontology items, the backbone of which is a dual relational graph attention network. Coupled with the 3-way pointer-generator decoder, our method outperforms traditional sequence labeling and classification-based schemes by a large margin. Ablation study in transfer learning settings further uncovers the poor generalizability of current models in multi-intent cases.},
  keywords={Semantics;Transfer learning;Ontologies;Signal processing;Acoustics;Decoding;Labeling;Spoken Language Understanding;relational graph attention network;hierarchical semantic frame},
  doi={10.1109/ICASSP48485.2024.10446325},
  ISSN={2379-190X},
  month={April},}@ARTICLE{10465250,
  author={Ma, Wenjian and Bi, Xiangpeng and Jiang, Huasen and Zhang, Shugang and Wei, Zhiqiang},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={CollaPPI: A Collaborative Learning Framework for Predicting Protein-Protein Interactions}, 
  year={2024},
  volume={28},
  number={5},
  pages={3167-3177},
  abstract={Exploring protein-protein interaction (PPI) is of paramount importance for elucidating the intrinsic mechanism of various biological processes. Nevertheless, experimental determination of PPI can be both time-consuming and expensive, motivating the exploration of data-driven deep learning technologies as a viable, efficient, and accurate alternative. Nonetheless, most current deep learning-based methods regarded a pair of proteins to be predicted for possible interaction as two separate entities when extracting PPI features, thus neglecting the knowledge sharing among the collaborative protein and the target protein. Aiming at the above issue, a collaborative learning framework CollaPPI was proposed in this study, where two kinds of collaboration, i.e., protein-level collaboration and task-level collaboration, were incorporated to achieve not only the knowledge-sharing between a pair of proteins, but also the complementation of such shared knowledge between biological domains closely related to PPI (i.e., protein function, and subcellular location). Evaluation results demonstrated that CollaPPI obtained superior performance compared to state-of-the-art methods on two PPI benchmarks. Besides, evaluation results of CollaPPI on the additional PPI type prediction task further proved its excellent generalization ability.},
  keywords={Proteins;Collaboration;Task analysis;Feature extraction;Protein engineering;Deep learning;Vectors;Graph neural network;multi-task learning;protein-protein interaction;protein representation learning},
  doi={10.1109/JBHI.2024.3375621},
  ISSN={2168-2208},
  month={May},}@ARTICLE{10452779,
  author={Hu, Fan and Zhang, Weihong and Huang, Huazhen and Li, Wang and Li, Yang and Yin, Peng},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={A Transferability-Based Method for Evaluating the Protein Representation Learning}, 
  year={2024},
  volume={28},
  number={5},
  pages={3158-3166},
  abstract={Self-supervised pre-trained language models have recently risen as a powerful approach in learning protein representations, showing exceptional effectiveness in various biological tasks, such as drug discovery. Amidst the evolving trend in protein language model development, there is an observable shift towards employing large-scale multimodal and multitask models. However, the predominant reliance on empirical assessments using specific benchmark datasets for evaluating these models raises concerns about the comprehensiveness and efficiency of current evaluation methods. Addressing this gap, our study introduces a novel quantitative approach for estimating the performance of transferring multi-task pre-trained protein representations to downstream tasks. This transferability-based method is designed to quantify the similarities in latent space distributions between pre-trained features and those fine-tuned for downstream tasks. It encompasses a broad spectrum, covering multiple domains and a variety of heterogeneous tasks. To validate this method, we constructed a diverse set of protein-specific pre-training tasks. The resulting protein representations were then evaluated across several downstream biological tasks. Our experimental results demonstrate a robust correlation between the transferability scores obtained using our method and the actual transfer performance observed. This significant correlation highlights the potential of our method as a more comprehensive and efficient tool for evaluating protein representation learning.},
  keywords={Task analysis;Proteins;Protein engineering;Biological system modeling;Computational modeling;Predictive models;Biological information theory;Transferability;protein representation learning;optimal transport},
  doi={10.1109/JBHI.2024.3370680},
  ISSN={2168-2208},
  month={May},}@ARTICLE{10440197,
  author={Chen, Long and Li, Yuchen and Silamu, Wushour and Li, Qingquan and Ge, Shirong and Wang, Fei-Yue},
  journal={IEEE Transactions on Intelligent Vehicles}, 
  title={Smart Mining With Autonomous Driving in Industry 5.0: Architectures, Platforms, Operating Systems, Foundation Models, and Applications}, 
  year={2024},
  volume={9},
  number={3},
  pages={4383-4393},
  abstract={The increasing importance of mineral resources in contemporary society is becoming more prominent, playing an indispensable and crucial role in the global economy. These resources not only provide essential raw materials for the global economic system but also play an irreplaceable role in supporting the development of modern industry, technology, and infrastructure. With the rapid development of intelligent technologies such as Industry 5.0 and advanced Large Language Models (LLMs), the mining industry is facing unprecedented opportunities and challenges. The development of smart mines has become a crucial direction for industry progress. This article aims to explore the strategic requirements for the development of smart mines by combining advanced products or technologies such as Chat-GPT (one of the successful applications of LLMs), digital twins, and scenario engineering. We propose a comprehensive architecture consisting of three different levels, the mining industrial Internet of Things (IoT) platform, mining operating systems, and foundation models. The systems and models empower the mining equipment for transportation. The architecture delivers a comprehensive solution that aligns perfectly with the demands of Industry 5.0. The application and validation outcomes of this intelligent solution showcase a noteworthy enhancement in mining efficiency and a reduction in safety risks, thereby laying a sturdy groundwork for the advent of Mining 5.0.},
  keywords={Digital twins;Fifth Industrial Revolution;Ontologies;Autonomous driving;Mining industry;Network architecture;Large language models;Mining 5.0;smart mining;autonomous driving;industry 5.0;architectures;mining transportation trucks},
  doi={10.1109/TIV.2024.3365997},
  ISSN={2379-8904},
  month={March},}@ARTICLE{10423114,
  author={Zhao, Yingwen and Yang, Zhihao and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Predicting Protein Functions Based on Heterogeneous Graph Attention Technique}, 
  year={2024},
  volume={28},
  number={4},
  pages={2408-2415},
  abstract={In bioinformatics, protein function prediction stands as a fundamental area of research and plays a crucial role in addressing various biological challenges, such as the identification of potential targets for drug discovery and the elucidation of disease mechanisms. However, known functional annotation databases usually provide positive experimental annotations that proteins carry out a given function, and rarely record negative experimental annotations that proteins do not carry out a given function. Therefore, existing computational methods based on deep learning models focus on these positive annotations for prediction and ignore these scarce but informative negative annotations, leading to an underestimation of precision. To address this issue, we introduce a deep learning method that utilizes a heterogeneous graph attention technique. The method first constructs a heterogeneous graph that covers the protein-protein interaction network, ontology structure, and positive and negative annotation information. Then, it learns embedding representations of proteins and ontology terms by using the heterogeneous graph attention technique. Finally, it leverages these learned representations to reconstruct the positive protein-term associations and score unobserved functional annotations. It can enhance the predictive performance by incorporating these known limited negative annotations into the constructed heterogeneous graph. Experimental results on three species (i.e., Human, Mouse, and Arabidopsis) demonstrate that our method can achieve better performance in predicting new protein annotations than state-of-the-art methods.},
  keywords={Proteins;Protein engineering;Annotations;Feature extraction;Predictive models;Deep learning;Amino acids;Protein function prediction;positive and negative annotations;constructed heterogeneous graph;heterogeneous graph attention},
  doi={10.1109/JBHI.2024.3357834},
  ISSN={2168-2208},
  month={April},}@ARTICLE{10418085,
  author={Sewunetie, Walelign Tewabe and Kovács, László},
  journal={IEEE Access}, 
  title={Exploring Sentence Parsing: OpenAI API-Based and Hybrid Parser-Based Approaches}, 
  year={2024},
  volume={12},
  number={},
  pages={38801-38815},
  abstract={This study focuses on the fundamental process of parsing sentences to create semantic graphs from textual documents. It introduces novel techniques for parsing phrases within semantic graph-based induction, employing both ChatGPT-based and Hybrid parser-based approaches. Through a thorough analysis, the study evaluates the performance of these methods in generating semantic networks from text, particularly in capturing detailed event descriptions and relationships. Results indicate a slight advantage in accuracy for the Hybrid parser-based approach (87%) compared to ChatGPT (85%) in sentence parsing tasks. Furthermore, efficiency analysis reveals that ChatGPT’s response quality varies with prompt sizes, while the Hybrid parser-based method consistently maintains excellent response quality.},
  keywords={Semantics;Chatbots;Adaptation models;Knowledge graphs;Task analysis;Context modeling;Training;Natural language processing;Predictive models;Application of sentence parsing;adverb prediction;ChatGPT;hybrid parser;natural language processing;sentence parsing;semantic graph},
  doi={10.1109/ACCESS.2024.3360480},
  ISSN={2169-3536},
  month={},}@ARTICLE{10320368,
  author={Quevedo, Ernesto and Cerny, Tomas and Rodriguez, Alejandro and Rivas, Pablo and Yero, Jorge and Sooksatra, Korn and Zhakubayev, Alibek and Taibi, Davide},
  journal={IEEE Access}, 
  title={Legal Natural Language Processing From 2015 to 2022: A Comprehensive Systematic Mapping Study of Advances and Applications}, 
  year={2024},
  volume={12},
  number={},
  pages={145286-145317},
  abstract={The surge in legal text production has amplified the workload for legal professionals, making many tasks repetitive and time-consuming. Furthermore, the complexity and specialized language of legal documents pose challenges not just for those in the legal domain but also for the general public. This emphasizes the potential role and impact of Legal Natural Language Processing (Legal NLP). Although advancements have been made in this domain, particularly after 2015 with the advent of Deep Learning and Large Language Models (LLMs), a systematic exploration of this progress until 2022 is nonexistent. In this research, we perform a Systematic Mapping Study (SMS) to bridge this gap. We aim to provide a descriptive statistical analysis of the Legal NLP research between 2015 and 2022. Categorize and sub-categorize primary publications based on their research problems. Identify limitations and areas of improvement in current research. Using a robust search methodology across four reputable indexers, we filtered 536 papers down to 75 pivotal articles. Our findings reveal the diverse methods employed for tasks such as Multiclass Classification, Summarization, and Question Answering in the Legal NLP field. We also highlight resources, challenges, and gaps in current methodologies and emphasize the need for curated datasets, ontologies, and a focus on inherent difficulties like data accessibility. As the legal sector gradually embraces Natural Language Processing (NLP), understanding the capabilities and limitations of Legal NLP becomes vital for ensuring efficient and ethical application. The research offers insights for both Legal NLP researchers and the broader legal community, advocating for continued advancements in automation while also addressing ethical concerns.},
  keywords={Law;Natural language processing;Task analysis;Systematics;Information retrieval;Surveys;Search problems;Deep learning;Systematic-mapping-study;legal-NLP;deep learning},
  doi={10.1109/ACCESS.2023.3333946},
  ISSN={2169-3536},
  month={},}@ARTICLE{10313062,
  author={Zhang, Liyuan and Jiang, Yongquan and Yang, Yan},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={GNNGO3D: Protein Function Prediction Based on 3D Structure and Functional Hierarchy Learning}, 
  year={2024},
  volume={36},
  number={8},
  pages={3867-3878},
  abstract={Protein sequences accumulate in large quantities, and the traditional method of annotating protein function by experiment has been unable to bridge the gap between annotated proteins and unannotated proteins. Machine learning-based protein function prediction is an effective approach to solve this problem. Most of the existing methods only use the protein sequence but ignore the three-dimensional structure which is closely related to the protein function. And the hierarchy of protein functions is not adequately considered. To solve this problem, we propose a graph neural network (GNNGO3D) that combines the three-dimensional structure and functional hierarchy learning. GNNGO3D simultaneously uses three kinds of information: protein sequence, tertiary structure, and hierarchical relationship of protein function to predict protein function. The novelty of GNNGO3D lies in that it integrates the learning of functional level information into the method of predicting protein function by using tertiary structure information, fully learning the relationship between protein functions, and helping to better predict protein function. Experimental results show that our method is superior to existing methods for predicting protein function based on sequence and structure.},
  keywords={Proteins;Feature extraction;Protein sequence;Three-dimensional displays;Task analysis;Ontologies;Convolutional neural networks;Graph neural networks;gene ontology;language model;machine learning;protein function prediction},
  doi={10.1109/TKDE.2023.3331005},
  ISSN={1558-2191},
  month={Aug},}@INPROCEEDINGS{10708094,
  author={Wang, Dan and Li, Xiaofeng and Gu, Bin and Cao, Yue and Liu, Yusheng},
  booktitle={2023 6th International Conference on Mechatronics, Robotics and Automation (ICMRA)(}, 
  title={An Architecture Modeling Framework for Distributed Automation Systems Using SysML and Semantic Web Technologies}, 
  year={2023},
  volume={},
  number={},
  pages={191-200},
  abstract={The rising interdisciplinarity and complexity of the Distributed Automation Systems (DASs) require the systems to be modeled in an unambiguous and high-level abstract way for cross-discipline/stage communication and interoperability in the Model-Driven Development (MDD) lifecycle. The concept of the System Architecture Model in systems engineering has been adopted for this challenge. To support the creation and analysis of this model, a modeling framework with a modeling methodology, modeling language, knowledge base, and related toolkit is established based on SysML and Semantic Web Technologies. A modeling methodology which is the core of the framework for modeling the architecture of DASs is formally defined with domain-specificity, comprehensiveness, discipline-neutrality, and platform-independency. Based on it, the SysML-DAS modeling language is extended from SysML, and the System Architecture Ontology is built with the help of the Knowledge Extraction Tool. This ontology works as the knowledge base not only to provide a unified and unambiguous view of the system but also to support the automated accomplishment of tasks in the MDD process. As a typical task, the semantic correctness and integrity of the system architecture model can be assessed by the Knowledge Analysis Tool in this framework.},
  keywords={Semantic Web;Analytical models;Automation;Mechatronics;Knowledge based systems;Semantics;Systems architecture;Ontologies;Service-oriented architecture;Model-driven development;distributed automation system;SysML;Semantic Web technologies;System Architecture Model},
  doi={10.1109/ICMRA59796.2023.10708094},
  ISSN={2996-380X},
  month={Nov},}@INPROCEEDINGS{10568296,
  author={Tona, Claudia and Juárez-Ramírez, Reyes and Jiménez, Samantha and Murillo-Muñoz, Fernanda},
  booktitle={2023 11th International Conference in Software Engineering Research and Innovation (CONISOFT)}, 
  title={Q-Story: An Ontology-Based on Quality of User Stories in Scrum. A Quantitative Assessment}, 
  year={2023},
  volume={},
  number={},
  pages={55-64},
  abstract={Q-Story ontology was created utilizing the Methontology approach and further represented through the Meta Object Facility (MOF) and Unified Modeling Language (UML). Because aspects such as their structure, level of granularity, and comprehensibility hold considerable signifi-cance in ensuring a favorable project execution. That is, the quality of user stories significantly impacts the outcome of a software project, influencing its success or failure. Therefore, we performed a quantitative evaluation using the OntoQA method, resulting in a relationship richness value of 0.95, an attribute richness of 4.00, and an inheritance richness of 1.26. The outcome of this work will contribute to developing an ontology that can effectively create user stories with quality. Furthermore, it will serve as a valuable guide for development teams, aiding them in the creation, analysis, and development processes of user stories.},
  keywords={Technological innovation;Unified modeling language;Ontologies;Software;Software engineering;Ontology;Quantitative Assessment;Quality Met-rics;Software Engineering;User Story;Ontology Quality Evaluation},
  doi={10.1109/CONISOFT58849.2023.00017},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10522716,
  author={Mateiu, Patricia and Groza, Adrian},
  booktitle={2023 25th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)}, 
  title={Ontology engineering with Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={226-229},
  abstract={We tackle the task of enriching ontologies by automatically translating natural language (NL) into Description Logic (DL). Since Large Language Models (LLMs) are the best tools for translations, we fine-tuned a GPT-3 model to convert NL into OWL Functional Syntax. For fine-tuning, we designed pairs of sentences in NL and the corresponding translations. This training pairs cover various aspects from ontology engineering: instances, class subsumption, domain and range of relations, object properties relationships, disjoint classes, complements, or cardinality restrictions. The resulted axioms are used to enrich an ontology, in a human supervised manner. The developed tool is publicly provided as a Protégé plugin.},
  keywords={Training;Scientific computing;Description logic;OWL;Natural languages;Syntactics;Task analysis;ontology engineering;large language models;Protege plugin;fine-tuning},
  doi={10.1109/SYNASC61333.2023.00038},
  ISSN={2470-881X},
  month={Sep.},}@INPROCEEDINGS{10479303,
  author={Ferchichi, Olfa and Beltaifa, Raoudha and Labed Jilani, Lamia},
  booktitle={2023 20th ACS/IEEE International Conference on Computer Systems and Applications (AICCSA)}, 
  title={Artificial Intelligence Based SysML Block Diagram Extension and Evolution for Product Lines}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={SysML is a standard language that permits to model systems of any type such as plane, ships and software intensive systems. Software Product Line large scale reuse approach has demonstrated its success. The industry provides benefits in term of cost savings and acceleration of time to maket. The available literature indicates that there have been efforts to enhance the capability of SysML in handling product families. However, these attempts are not yet fully systematic, and there remains a significant amount of work to be undertaken in this area. In this present paper, we deal with the SysML Block Diagram in order to investigate to what extent it permits variability representation and how it can evolve during the system evolution or when agility is needed. We want to capitalize on the knowledge necessary for block diagram extention and evolution and take advantage of knowledge from Product Line domain engineering and application engineering. So, we decide to use an ontology which is an articifial intelligence artifact. An ontology is a powerful mean to represent knowledge and reason about it. Here, we use the ontology to help decision making for Block diagram evolution as well.},
  keywords={Industries;Systematics;Description logic;Systems architecture;Ontologies;Software;Mobile handsets;SysML block diagram;Product Line Engineering;Variability;Evolution;Artificial Intelligence Artifact;Ontology},
  doi={10.1109/AICCSA59173.2023.10479303},
  ISSN={2161-5330},
  month={Dec},}@INPROCEEDINGS{10466986,
  author={Du, Zhihong and Xu, Duo and Huang, Danruo and Hu, Yuren and He, Keqing and Wang, Chong and Wang, Jian and Zhang, Hong-Yu and Mayer, Wolfgang and Duan, Yucong and Wang, Ying and Feng, Zaiwen},
  booktitle={2023 IEEE International Conference on High Performance Computing & Communications, Data Science & Systems, Smart City & Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)}, 
  title={An Ontology-based Method for Heterogeneous Data Governance with MFI and MDR}, 
  year={2023},
  volume={},
  number={},
  pages={1106-1113},
  abstract={Currently, data governance and integration in the fields are hindered by semantic ambiguity and syntactic inconsistencies among different sub domain data and models, making it difficult to use these valuable data for further application and research jointly. Therefore, it is necessary to unify and integrate existing metadata and meta-models more effectively for information resource sharing and interoperability in the field. This paper proposes an ontology-based approach with a hybrid ISO/IEC 11179 (MDR) and ISO/IEC 19763 (MFI) framework for data governance and integration. This framework takes a Global Ontology Model (GOM) constructed for the global domain as a bridge and basis for integrating and aligning heterogeneous sub domains. It extends MDR by adding ontology registration items to implement the mapping between the GOM and multiple subdomains, thereby promoting the semantic sharing of metadata between subdomains. In addition, the MFI-12 and MFI-10 are used to solve the model interoperability between different subdomains. A detailed case study is provided to illustrate the concrete registration process and demonstrate the validity of our method.},
  keywords={Information resources;ISO Standards;Semantics;Standardization;Metadata;Ontologies;Syntactics;Ontology;MDR;MFI;Data Governance;Data Standardization},
  doi={10.1109/HPCC-DSS-SmartCity-DependSys60770.2023.00158},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10471711,
  author={Qin, Xiaodong and He, Yuxuan and Ma, Jie and Peng, Weiyuan and Zio, Enrico and Su, Huai},
  booktitle={2023 International Conference on Computer Science and Automation Technology (CSAT)}, 
  title={An Effective Knowledge Mining Method for Compressor Fault Text Data Based on Large Language Model}, 
  year={2023},
  volume={},
  number={},
  pages={44-48},
  abstract={The fault diagnosis method of compressors determines the reliability of the gas transmission pipeline station. Existing compressor fault diagnosis methods mostly relies on data-driven, which leads to a high application threshold from the mechanism. To address this issue, this paper introduces the knowledge graph into the compressor fault diagnosis for the first time and proposes a compressor fault text data knowledge mining method based on large language model. Firstly, the characteristics and principles of compressor faults are analyzed. Then, a text data knowledge mining model called CFRTE for compressors is constructed. Experimental results show that the Fl score of the CFRTE model can reach 0.98, meeting the requirements of compressor fault knowledge mining. Finally, combined with the results of knowledge mining and the graph database, a new system for the storage and indexing of the compressor fault knowledge graph is proposed. To further verify the role of the large language model in compressor fault knowledge mining, this paper conducts a comparative experiment of CFRTE models based on RNN encoder and BERT encoder. Experimental results show that compared with GRU, BiGRU, LSTM, and BiLSTM as the encoder layer, the Fl score of the CFRTE model with BERT as the encoder layer has increased by 26.78%, 6.18%, 21.89%, and 5.49% respectively. This work provides a systematic feasible scheme for introducing knowledge graphs into compressor fault diagnosis, which can be used for reference in the fault diagnosis of related equipment.},
  keywords={Fault diagnosis;Systematics;Computational modeling;Pipelines;Knowledge graphs;Ontologies;Compressors;compressor station;compressor;large language model;knowledge mining;knowledge graph},
  doi={10.1109/CSAT61646.2023.00024},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10471842,
  author={Do, Nhon V. and Mai, Thanh T.},
  booktitle={2023 RIVF International Conference on Computing and Communication Technologies (RIVF)}, 
  title={A Knowledge Representation Model for Designing the Knowledge Querying System in Programming Language C/C++}, 
  year={2023},
  volume={},
  number={},
  pages={366-371},
  abstract={Knowledge querying support systems need to assist users in querying the knowledge, relationships, or combination of multiple requirements. A proper knowledge representation model and the well-structured query language play important roles in the developing the knowledge querying systems. There are knowledge representation models and systems that support the querying or searching on the knowledge-based, but they have not supported well for various query requirements on the knowledge. Specially, the structured query sentences combine the multiple requirements. The paper will propose a knowledge representation model for the programming language C/C++ knowledge domain. Moreover, the paper will present structured query sentences that meet various requirements from users. Especially, the combination of multiple requirements based on the operators AND, OR, and NOT. Results of the research will be applied to design the knowledge querying system in the programming language C/C++ knowledge domain. The system is useful for first and second-year students in the field of technology information.},
  keywords={Computational modeling;Knowledge based systems;Knowledge representation;Communications technology;Database languages;ontology;structured query sentences;knowledge representation;knowledge querying system;programming language},
  doi={10.1109/RIVF60135.2023.10471842},
  ISSN={2473-0130},
  month={Dec},}@INPROCEEDINGS{10456842,
  author={Ollier, Guillaume and Adedjouma, Morayo and Gerasimou, Simos and Mraidha, Chokri},
  booktitle={2023 26th Euromicro Conference on Digital System Design (DSD)}, 
  title={An Ontological Approach for the Dependability Analysis of Automated Systems}, 
  year={2023},
  volume={},
  number={},
  pages={593-601},
  abstract={This paper presents the Ontology Language for the Dependability of Automated Systems (OLDAS), a modeling language based on Unified Modeling Language (UML) that aims to support dependability assessment for Automated Systems (ASs), i.e., systems intended to perform a function with minimal or no human intervention. OLDAS extends the Unified Foundational Ontology (UFO) and embeds validation rules to prevent constraint violations in ASs analysis. Specifically, the paper presents how OLDAS can support different activities during the design of ASs, from the definition of the Operational Design Domain to scenario-based analysis. OLDAS is available as a plugin of the open-source Papyrus for Robotics framework.},
  keywords={Analytical models;Runtime;Unified modeling language;Redundancy;Ontologies;Probabilistic logic;Hazards;Autonomous Systems;Automated Driving Systems;Artificial Intelligence;Safety Engineering;ODD;ML-based Systems},
  doi={10.1109/DSD60849.2023.00087},
  ISSN={2771-2508},
  month={Sep.},}@INPROCEEDINGS{10459922,
  author={Sazzed, Salim},
  booktitle={2023 International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Comprehending Lexical and Affective Ontologies in the Demographically Diverse Spatial Social Media Discourse}, 
  year={2023},
  volume={},
  number={},
  pages={2247-2252},
  abstract={This study aims to comprehend linguistic and sociodemographic features, encompassing English language styles, conveyed sentiments, and lexical diversity within spatial online social media review data. To this end, we undertake a case study that scrutinizes reviews composed by two distinct and demographically diverse groups. Our analysis entails the extraction and examination of various statistical, grammatical, and sentimental features from these two groups. Subsequently, we leverage these features with machine learning (ML) classifiers to discern their potential in effectively differentiating between the groups. Our investigation unveils substantial disparities in certain linguistic attributes between the two groups. When integrated into ML classifiers, these attributes exhibit a marked efficacy in distinguishing the groups, yielding a macro F1 score of approximately 0.85. Furthermore, we conduct a comparative evaluation of these linguistic features with word n-gram-based lexical features in discerning demographically diverse review data. As expected, the n-gram lexical features, coupled with finetuned transformer-based models, show superior performance, attaining accuracies surpassing 95% and macro F1 scores exceeding 0.96. Our meticulous analysis and comprehensive evaluations substantiate the efficacy of linguistic and sentimental features in effectively discerning demographically diverse review data. The findings of this study provide valuable guidelines for future research endeavors concerning the analysis of demographic patterns in textual content across various social media platforms.},
  keywords={Reviews;Social networking (online);Machine learning;Linguistics;Ontologies;Feature extraction;Transformers;n/a},
  doi={10.1109/ICMLA58977.2023.00339},
  ISSN={1946-0759},
  month={Dec},}@INPROCEEDINGS{10453813,
  author={Albokae, Nazeer and AlKhtib, Bassel and Omar, Khaled},
  booktitle={2023 24th International Arab Conference on Information Technology (ACIT)}, 
  title={Hybrid Method for ICD Prediction Using Word Embedding and Natural Language Processing}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={The international classification of diseases is a standard in medical coding, and it is contains all information and description of diseases in heroical structure, and finding the International Classification of Diseases (ICD) code for diseases is important and essential thing in medical sector, the coding process takes a lot of time and money to find the correct and the exact code of the patient disease, researchers in artificial intelligence and in natural language processing and in machine learning make a huge efforts to build and develop automatic systems and algorithms for automatic ICD encoding, in this paper we propose a hybrid method for automatic ICD encoding from patient claims, the proposed method contains two main parts first for ICD chapter, ICD group classification, and the second one for find the most relevant ICD code based on patient claim diagnosis description, the first step was implemented by using natural language processing techniques, that it include stemming (Porter Stemmer was used for stemming), stop word removing, and the implementation of the second step was done by using PubMed BERT model for embedding for the ICD codes the embedding done based on the descriptions, and also the embedding done for the patient claim diagnosis description, we have tested the developed algorithm on medical dataset The results of our tests indicate that the proposed method is highly efficient, with a precision rate of 87%.},
  keywords={Codes;Prediction algorithms;Natural language processing;Encoding;Classification algorithms;Medical diagnostic imaging;Diseases;automatic ICD coding;PubMed BERT;ICD ontology},
  doi={10.1109/ACIT58888.2023.10453813},
  ISSN={2831-4948},
  month={Dec},}@INPROCEEDINGS{10450023,
  author={Pal, Suman and Gaur, Monica and Chaudhuri, Rupanjali and Benny Anto, Oshin and R, Kalaivanan and KV, Chetan and Pradhan, Pragnya},
  booktitle={2023 International Conference on Computational Intelligence, Networks and Security (ICCINS)}, 
  title={Recommendation System for Clinical Concept Mapping}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In the past decade, the healthcare industry has shifted from paper-based document storage to Electronic Health Records (EHR), enabling quick, safe access to patient data. A key role is played by Semantic Interoperability (SI) which enables seamless data exchange between diverse care settings and clinical software. SI necessitates the linking of bio-medical data (aka. clinical events) with shared, standardized, and controlled vocabularies like SNOMED, LOINC, etc. However, the healthcare data across various client domains are filled with ambiguous textual representations of clinical events that may be present in the form of synonyms, acronyms, and abbreviations. To make interoperability work, Healthcare IT service providers must map related clinical events with the appropriate standard concepts, which requires additional time and resources. Natural Language Processing (NLP) plays a vital role in addressing the challenges of SI by learning effective representations of text words in the bio-medical domain thereby capturing their semantic meaning. Our method utilizes various pre-trained word embeddings trained on the bio-medical corpus like BioWordVec fastText and SapBERT that captures fine-grained semantic relationships. In this study, we have developed a recommendation system that provides recommendations of Top ‘N’ clinical events for mapping to a standard concept. The recommendation system showed good performance with a sensitivity of above 99 % using both the pre-trained word embedding. Further, this product can be integrated into the mapping workflow to help make accurate automated suggestions that minimize manual effort.},
  keywords={Sensitivity;Semantics;Medical services;Manuals;Natural language processing;Recommender systems;Interoperability;semantic interoperability;ontology;mapping;natural language processing;word embeddings;SapBERT},
  doi={10.1109/ICCINS58907.2023.10450023},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10449869,
  author={Omar, Mussa A.},
  booktitle={2023 IEEE 11th International Conference on Systems and Control (ICSC)}, 
  title={Measurement of ChatGPT Performance in Mapping Natural Language Speficaction into an Entity Relationship Diagram}, 
  year={2023},
  volume={},
  number={},
  pages={530-535},
  abstract={This paper explores the entity relationship diagram, a popular conceptual model used to depict entities, attributes, and relationships graphically. To help with this, we use ChatGPT, a sophisticated language model based on the GPT architecture, which can translate natural language text into an entity relationship diagram. The paper details the process of evaluating how well ChatGPT can perform compared to other state-of-the-art approaches for entity and relationship extraction. Our experimental findings demonstrate the strong ability of ChatGPT to translate natural language text into entity relationship diagrams, which has potential applications for knowledge graph building, data integration, and database schema design. Moreover, it can aid in automating the extraction and organization of information from unstructured text data, thereby simplifying the study of complex systems.},
  keywords={Adaptation models;Natural languages;Machine learning;Companies;Chatbots;Task analysis;Software engineering;entity relationship diagram;ChatGPT;natural language processing},
  doi={10.1109/ICSC58660.2023.10449869},
  ISSN={2379-0067},
  month={Dec},}@INPROCEEDINGS{10429736,
  author={Wang, Yanfeng and Zhang, Liangji and Peng, Chao and Wang, Junhui and Zhu, Yingying},
  booktitle={2023 9th International Conference on Big Data and Information Analytics (BigDIA)}, 
  title={Study on the Construction and Application of Combat Simulation Models Knowledge Graph}, 
  year={2023},
  volume={},
  number={},
  pages={442-449},
  abstract={Exploiting the knowledge graph effectively enables the integration, the management, the illustration, the retrieval, the mining, and the reasoning of the knowledge. As for the combat simulation experiments, the combat simulation knowledge graph shall enhance the efficiency and quality and reduce the complexity and cost for developing the models, as well as support the agile construction of the simulation applications. Thus, this paper focused on the requirements analysis, creation idea, construction methods and typical applications of the combat simulation models knowledge graph, which provides some guiding significance for using knowledge graphs to boost the combat simulations.},
  keywords={Deep learning;Analytical models;Costs;Semantic search;Knowledge graphs;Ontologies;Research and development;Combat simulation experiments;Combat simulation models;Knowledge graph construction;Knowledge graph application},
  doi={10.1109/BigDIA60676.2023.10429736},
  ISSN={2771-6902},
  month={Dec},}@INPROCEEDINGS{10422308,
  author={Tang, Yun and Da Costa, Antonio A. Bruto and Zhang, Xizhe and Patrick, Irvine and Khastgir, Siddartha and Jennings, Paul},
  booktitle={2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain}, 
  year={2023},
  volume={},
  number={},
  pages={3893-3900},
  abstract={Engineering knowledge-based (or expert) systems require extensive manual effort and domain knowledge. As Large Language Models (LLMs) are trained using an enormous amount of cross-domain knowledge, it becomes possible to automate such engineering processes. This paper presents an empirical automation and semi-automation framework for domain knowledge distillation using prompt engineering and the LLM ChatGPT. We assess the framework empirically in the autonomous driving domain and present our key observations. In our implementation, we construct the domain knowledge ontology by “chatting” with ChatGPT. The key finding is that while fully automated domain ontology construction is possible, human supervision and early intervention typically improve efficiency and output quality as they lessen the effects of response randomness and the butterfly effect. We, therefore, also develop a web-based distillation assistant enabling supervision and flexible intervention at runtime. We hope our findings and tools could inspire future research toward revolutionizing the engineering of knowledge-based systems across application domains.},
  keywords={Knowledge engineering;Runtime;Manuals;Ontologies;Chatbots;Autonomous vehicles;Intelligent transportation systems;large language model;domain ontology distillation;autonomous driving},
  doi={10.1109/ITSC57777.2023.10422308},
  ISSN={2153-0017},
  month={Sep.},}@INPROCEEDINGS{10412761,
  author={Vijayakumar, Senthilkumar and Louis, Filious},
  booktitle={2023 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={Revolutionizing Staffing and Recruiting with Contextual Knowledge Graphs and QNLP: An End-to-End Quantum Training Paradigm}, 
  year={2023},
  volume={},
  number={},
  pages={45-51},
  abstract={The staffing and recruiting industry is continuously evolving, and recent advancements in Knowledge Graphs (KG) and Quantum Natural Language Processing (QNLP) has garnered considerable attention. The integration of these state-of-the-art technologies is fueled by the necessity to improve language models' capacity to comprehend context and make precise decisions. This research paper presents a novel approach to revolutionize the staffing and recruiting industry by integrating Knowledge Graph (KG) and Quantum Natural Language Processing (QNLP) to formulate an end-to-end QNLP training pipeline. The proposed solution consists of three interdependent subsystems that work in unison to construct contextual KG and train language models. The Information Extraction subsystem extracts semantic relationships and connections between entities from large and complex recruitment data to construct domain specific contextual KG. The QNLP model training pipeline subsystem, which is fed with domain-rich KG data, runs on Quantum Circuits, accelerates the training process by effectively incorporating high-dimensional features to the deep layers of language models. Finally, the Information Retrieval subsystem is based on semantic data taxonomy, retrieving contextual data from the KG for the trained language models to be implemented on various distinctive use cases in the staffing and recruiting industry. This solution provides a faster and more contextual approach to analyze recruitment data, empowering recruiters to concentrate on strategic tasks such as candidate engagement and client relationship building, ultimately leading to better business decision-making capabilities.},
  keywords={Training;Industries;Knowledge graphs;Natural language processing;Data models;Integrated circuit modeling;Context modeling;Artificial Intelligence (AI);Knowledge Graph (KG);Quantum Natural Language Processing (QNLP);Large Language Models (LLM);Contextual Information Extraction & Retrieval Systems},
  doi={10.1109/ICKG59574.2023.00011},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10407599,
  author={Tu, Ming-Yu and Ehm, Hans and Ismail, Abdelgafar and Ulrich, Philipp},
  booktitle={2023 Winter Simulation Conference (WSC)}, 
  title={Reusable Ontology Generation and Matching from Simulation Models}, 
  year={2023},
  volume={},
  number={},
  pages={2298-2309},
  abstract={As simulating semiconductor manufacturing grows complex, model reuse becomes appealing since it can reduce the time incurred in developing future models. Also, considering a large network of the semiconductor supply chain, knowledge sharing can enable the efficient development of simulation models in a collaborative organization. Such necessity of reusability and interoperability of simulation models motivates this paper. We will address these challenges through ontological modeling and linking of the simulation components. The first application is generating reusable ontologies from simulation models. Another discussed application is ontology matching for knowledge sharing between simulation components and a meta-model of the semiconductor supply chain. The proposed approach succeeds in automatically transforming simulation into reusable knowledge and identifying interconnection in a semiconductor manufacturing system.},
  keywords={Semiconductor device modeling;Knowledge engineering;Supply chains;Organizations;Ontologies;Semiconductor device manufacture;Interoperability},
  doi={10.1109/WSC60868.2023.10407599},
  ISSN={1558-4305},
  month={Dec},}@INPROCEEDINGS{10402219,
  author={Liem, Truc Nguyen and Cao Hoai, Sinh Nguyen and Quoc, Hung Nguyen and Van, Tien Nguyen and Pham Trung, Hieu and Quoc, Trung Nguyen and Hoang, Vinh Truong},
  booktitle={2023 IEEE 15th International Conference on Computational Intelligence and Communication Networks (CICN)}, 
  title={GradTOD - A Unified Dialogue State Tracking Model for Task-Oriented and Open Domain Dialogues}, 
  year={2023},
  volume={},
  number={},
  pages={711-719},
  abstract={The task-oriented dialogue domain system requires classifying intent and replying to a specific goal domain. In the sub-module of Task-oriented, the Dialogue State Tracker (DST) is well-known as a variety processing tracker. However, existing DST models often specialize in only task-oriented domains (ToD), leading to limited performance when applied to scenarios. In this paper, we propose GradTOD, a unified DST model that predicts both two task types, task-oriented dialogue (TOD) and open-domain dialogue (ODD). Our model leverages the recent advances in prompt engineering and conditional generation to perform zero-shot learning. After experiments, GradTOD has achieved an 88.6% and 82.5% score on Joint Goal Accuracy metrics when evaluating the Scheme-Guided Dialogue (SGD) and FusedChat test sets correspondingly, demonstrating the adaption ability for multi-domains.},
  keywords={Measurement;Adaptation models;Zero-shot learning;Computational modeling;Predictive models;Task analysis;Computational intelligence;component;formatting;style;styling;insert},
  doi={10.1109/CICN59264.2023.10402219},
  ISSN={2472-7555},
  month={Dec},}@INPROCEEDINGS{10386182,
  author={Kosten, Catherine and Cudré-Mauroux, Philippe and Stockinger, Kurt},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph Question Answering Systems}, 
  year={2023},
  volume={},
  number={},
  pages={5272-5281},
  abstract={With the recent spike in the number and availability of Large Language Models (LLMs), it has become increasingly important to provide large and realistic benchmarks for evaluating Knowledge Graph Question Answering (KGQA) systems. So far the majority of benchmarks rely on pattern-based SPARQL query generation approaches. The subsequent natural language (NL) question generation is conducted through crowdsourcing or other automated methods, such as rule-based paraphrasing or NL question templates. Although some of these datasets are of considerable size, their pitfall lies in their pattern-based generation approaches, which do not always generalize well to the vague and linguistically diverse questions asked by humans in real-world contexts. In this paper, we introduce Spider4SPARQL -a new SPARQL benchmark dataset featuring 9,693 previously existing manually generated NL questions and 4,721 unique, novel, and complex SPARQL queries of varying complexity. In addition to the NL/SPARQL pairs, we also provide their corresponding 166 knowledge graphs and ontologies, which cover 138 different domains. Our complex benchmark enables novel ways of evaluating the strengths and weaknesses of modern KGQA systems. We evaluate the system with state-of-the-art KGQA systems as well as LLMs, which achieve only up to 45% execution accuracy, demonstrating that Spider4SPARQL is a challenging benchmark for future research.},
  keywords={Measurement;Crowdsourcing;Natural languages;Knowledge graphs;Benchmark testing;Ontologies;Question answering (information retrieval);Benchmark for Question Answering over Knowledge Graphs;Language Models;Performance Evaluation},
  doi={10.1109/BigData59044.2023.10386182},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10386611,
  author={Liu, Jiehui and Zhan, Jieyu},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={Constructing Knowledge Graph from Cyber Threat Intelligence Using Large Language Model}, 
  year={2023},
  volume={},
  number={},
  pages={516-521},
  abstract={Cyber Threat Intelligence (CTI) reports are valuable resources in various applications but manually extracting information from them is time-consuming. Existing approaches for automating extraction require specialized models trained on a substantial corpus. In this paper, we present an efficient methodology for constructing knowledge graphs from CTI by leveraging the Large Language Model (LLM), using ChatGPT for instance. Our approach automatically extracts attack-related entities and their relationships, organizing them within a CTI knowledge graph. We evaluate our approach on 13 CTIs, demonstrating better performance compared to AttacKG and REBEL while requiring less manual intervention and computational resources. This proves the feasibility and suitability of our method in low-resource scenarios, specifically within the domain of cyber threat intelligence.},
  keywords={Computational modeling;Knowledge graphs;Manuals;Ontologies;Information retrieval;Data models;Cognition;knowledge graph;threat intelligence;large language model;ChatGPT},
  doi={10.1109/BigData59044.2023.10386611},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10391425,
  author={Sharma, Hemendra Shanker and Sharma, Ashish},
  booktitle={2023 Second International Conference On Smart Technologies For Smart Nation (SmartTechCon)}, 
  title={Query Expansion Using Word Embedding, Ontology and Natural Language Processing}, 
  year={2023},
  volume={},
  number={},
  pages={410-414},
  abstract={Query Expansion (QE) is the art of reconstructing specific queries to expand validation presentation, especially in the data mining process in a requirement understanding environment. Expanding requirements is one of the techniques involved in finding information. In the search engine environment, the query extension includes the evaluation of the value of the construction and the extension of search queries to match new documents. In natural language processing (NLP), word embedding is a term used in textbook parsing, usually as a real-valued vector that encodes the meaning of adjacent words in the vector. It is assumed that the space will be analogous in meaning. Word embedding can be achieved using a set of language models and point literacy methods where vocabulary words or expressions are mapped to vectors of real numbers. For query expansion, one method used is natural language processing through word embedding. Other approaches are ontology, machine learning, and deep learning for automatic query expansion. This paper proposes a hybrid approach for query expansion by combining NLP and ontology through word embedding.},
  keywords={Deep learning;Vocabulary;Art;Ontologies;Search engines;Natural language processing;Data mining;Query expansion;word embedding;natural language processing;Data mining;information retrieval},
  doi={10.1109/SmartTechCon57526.2023.10391425},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10386048,
  author={Guo, Kuo and Li, Yifan and Chen, Hao and Shen, Hong-Bin and Yang, Yang},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Isoform Function Prediction Based on Heterogeneous Graph Attention Networks}, 
  year={2023},
  volume={},
  number={},
  pages={522-527},
  abstract={Isoforms refer to different mRNA molecules transcribed from the same gene, which can be translated into proteins with varying structures and functions. Predicting the functions of isoforms is an essential topic in bioinformatics as it can provide valuable insights into the intricate mechanisms of gene regulation and biological processes. Conventionally, gene function labels are standardized in Gene Ontology (GO) terms. However, traditional methods for predicting isoform function are largely limited by the absence of isoform-specific labels, sparse annotations, and the vast number of GO terms. To address these issues, we propose HANIso, a deep learning-based method for isoform function prediction. HANIso leverages a pretrained protein language model to extract features from protein sequences. It also integrates heterogeneous information, such as isoform sequence features, GO annotations, and isoform interaction data, using a Heterogeneous Graph Attention Network (HAN). This allows the model to learn the importance of different sources of information and their semantic relationships through the attention mechanism. Our method can predict function labels at both the gene level and isoform level. We conduct experiments on two species datasets, and the results demonstrate that our method outperforms existing methods on both AUROC and AUPRC. HANIso has the potential to overcome the limitations of traditional methods and provide a more accurate and comprehensive understanding of isoform function.},
  keywords={Proteins;Annotations;Biological system modeling;Semantics;Predictive models;Ontologies;Feature extraction;alternative splicing;isoform function prediction;protein language model;gene ontology;heterogeneous graph attention network},
  doi={10.1109/BIBM58861.2023.10386048},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10385754,
  author={Wang, Xun and Qu, Peng and Meng, Xiangyu and Yang, Qing and Qiao, Lian and Zhang, Chaogang and Xie, Xianjin},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={MulAxialGO: Multi-Modal Feature-Enhanced Deep Learning Model for Protein Function Prediction}, 
  year={2023},
  volume={},
  number={},
  pages={132-137},
  abstract={Predicting protein function from sequences through machine learning can improve the understanding of novel proteins and biological mechanisms. Existing methods mainly rely on one-dimensional convolution or natural language processing (NLP) techniques to extract features from sequences, but they suffer from limited predictive performance. To address this challenge, we propose MulAxialGO, a new method that leverages multi-modal feature fusion to improve prediction accuracy. MulAxialGO integrates the prior features of a large-scale pre-trained protein language model and the posterior features of dynamic embedding coding and sequence homology. In addition, MulAxialGO employs a comprehensive image feature encoder to extract features from sequences, providing a novel perspective for protein function prediction. MulAxialGO is tested on two benchmark datasets and achieves state-of-the-art results. On the 2016 dataset, MulAxialGO significantly outperforms DeepGOPlus, improving molecular function by 4.5 points, biological process by 2.4 points and cellular component by 1.6 points for the AUPR metric. Similarly, on the NetGO dataset, MulAxialGO outperforms the state-of-the-art NetGO2.0, improving Fmax by 1.1 points for biological process and 2.3 points for cellular component.},
  keywords={Proteins;Measurement;Deep learning;Protein engineering;Convolution;Biological processes;Predictive models;Protein function prediction;Sequence analysis;Bioinformatics;Attention mechanism;Deep learning},
  doi={10.1109/BIBM58861.2023.10385754},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10385760,
  author={Shuai, Yunyan and Wang, Wenkang and Li, Yiming and Zeng, Min and Li, Min},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Protein function prediction using graph neural network with multi-type biological knowledge}, 
  year={2023},
  volume={},
  number={},
  pages={30-35},
  abstract={Proteins play crucial roles in diverse biological functions, and accurately annotating their functions is essential for understanding cellular mechanisms and developing therapies for complex diseases. Computational methods have been proposed as alternatives to laborious experimental approaches. However, existing network-based methods focus on the protein-protein interaction (PPI) networks, while the proteins without interactions are ignored. To address this limitation, we propose a novel deep learning framework for protein function prediction, named PFP-GMB, which incorporates multi-type biological knowledge to consider the proteins not present in the PPI networks. PFP-GMB leverages a pre-trained protein language model to extract sequence representations. Moreover, PPIs and orthology relationships are used to generate functional related features via graph neural networks and attention mechanisms. Finally, these multi-type features are fused for protein function prediction. Compared to eight state-of-the-art methods, PFP-GMB outperforms all of them in terms of F-max and AUPR. The ablation studies further confirm the relevance and significance of the multi-type biological knowledge incorporated into PFP-GMB for protein function prediction.},
  keywords={Proteins;Knowledge engineering;Protein engineering;Medical treatment;Feature extraction;Graph neural networks;Diseases;protein function;orthology network;PPI network;protein sequence;graph neural network},
  doi={10.1109/BIBM58861.2023.10385760},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10385745,
  author={Fu, Chengcheng and Yao, Yanan and Wu, Jieyu and Zhao, Weizhong and He, Tingting and Jiang, Xingpeng},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Multimodal reasoning for nutrition and human health via knowledge graph embedding}, 
  year={2023},
  volume={},
  number={},
  pages={1901-1904},
  abstract={The established links between nutrition and human health are widely acknowledged. Dietary nutrients play a crucial role in regulating gut microbial communities, influencing various human diseases. With a growing number of related studies, there’s a need to systematically organize these associations for coherent knowledge reasoning. However, due to the diverse and extensive nature of the knowledge landscape, significant challenges persist. To address this, we propose an approach using multimodal data and knowledge embeddings for effective knowledge reasoning in nutrition and human health. We create a comprehensive knowledge graph, KG4NH, covering dietary nutrition, gut microbiota, and human diseases. To ensure efficient knowledge representation, we employ knowledge embedding techniques to develop modality-specific encoders for structure, category, and description. Additionally, we introduce a mul-timodal fusion method to capture shared information across modalities. Our experimental results demonstrate the superiority of our approach over other state-of-the-art methods.},
  keywords={Knowledge graphs;Ontologies;Feature extraction;Cognition;Bioinformatics;Diseases;Knowledge graph;Multimodal embedding;Knowledge reasoning;Nutrition;Human health},
  doi={10.1109/BIBM58861.2023.10385745},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10386763,
  author={Jesus, Vitor and Patel, Asma and Kumar, Deepak},
  booktitle={2023 10th International Conference on Behavioural and Social Computing (BESC)}, 
  title={Feasibility of Structured, Machine-Readable Privacy Notices}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper offers a novel approach to the long standing problem of the interface of humans and online privacy notices. As literature and practice, and even art, for more than a decade have identified, privacy notices are nearly always ignored and "accepted" with little thought, mostly because it is not practical nor user-friendly to depend on reading a long text simply to access, e.g., a news website. Nevertheless, privacy notices are a central element, often mandated by law.We approach the problem by (partially) relieving the human from the task of inspecting such documents. Because they are documents written in natural language, often legal language, we assess the feasibility of representing privacy notices in a machine-readable format. Should this be feasible, automated processing of notices that still respect individual choices could be enabled. To this end, we manually inspected privacy notices under EU/UK's GDPR from common websites, and designed a JSON schema that captures their structure.},
  keywords={Privacy;Social computing;Art;Law;Natural languages;Task analysis},
  doi={10.1109/BESC59560.2023.10386763},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10391548,
  author={Taye, Mohammad Mustafa and Abulail, Rawan and Al-Oudat, Mohammad},
  booktitle={2023 7th International Symposium on Innovative Approaches in Smart Technologies (ISAS)}, 
  title={An Ontology Learning Framework for unstructured Arabic Text}, 
  year={2023},
  volume={},
  number={},
  pages={1-12},
  abstract={Ontologies are widely regarded as valuable sources of semantics and interoperability in all artificially intelligent systems. Due to the rapid growth of unstructured data on the web, studying how to automatically get ontology from unstructured text is important. Therefore, ontology learning (OL) is an important process in the business world. It involves finding and extracting concepts from the text so that these concepts can be used for things such as information retrieval. Unfortunately, learning ontology is not easy for some reasons, and there has not been much research on how to automatically learn a domain-specific ontology from data.Ontology Studying Arabic text is not as developed as learning Latin text. There is almost no automated support for using Arabic literary knowledge in semantically enabled systems. Machine learning (ML) has proven beneficial in numerous fields, including text mining. By employing neural language models such as AraBERT, it is possible to obtain word embeddings as distributed word representations from textual input using machine learning. However, the application of machine learning to aid the development of Arabic ontology is largely unexplored. This research examines the performance of AraBERT for ontology learning tasks in Arabic. Early performance results as an application of Arabic ontology learning are promising. In this research, we provide a method for populating an existing ontology with instance information extracted from the input natural language text. This prototype has achieved an information extraction accuracy of 91%.},
  keywords={Text mining;Semantics;Natural languages;Prototypes;Machine learning;Ontologies;Information retrieval;Arabic Ontology;Natural language Processing (NLP);Ontology;Ontology Learning (OL);Semantic Web;semantic representation},
  doi={10.1109/ISAS60782.2023.10391548},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10387602,
  author={Leventi-Peetz, Anastasia-Maria and Raber, Frederic and Rüll, Annika and Weber, Kai},
  booktitle={2023 Fifth International Conference on Transdisciplinary AI (TransAI)}, 
  title={Biotechnology Machine Learning Techniques for Natural Language Processing}, 
  year={2023},
  volume={},
  number={},
  pages={118-119},
  abstract={The possibility to transfer machine learning techniques from biotechnology to natural language processing models to increase training efficiency will be generally discussed. The motivation and reasoning behind the idea will be briefly outlined.},
  keywords={Training;Biotechnology;Biological system modeling;Machine learning;Natural language processing;Cognition;Sustainable development;Machine learning;natural language processing;gene ontology;protein function;model sustainability},
  doi={10.1109/TransAI60598.2023.00029},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10387600,
  author={Procko, Tyler Thomas and Elvira, Timothy and Ochoa, Omar},
  booktitle={2023 Fifth International Conference on Transdisciplinary AI (TransAI)}, 
  title={GPT-4: A Stochastic Parrot or Ontological Craftsman? Discovering Implicit Knowledge Structures in Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={147-154},
  abstract={Ontologies are representational artifacts that purport to accurately portray the aspect of reality under the purview of the ontologists laboring upon them. Ontologies exist in a spectrum of formality, from lexical thesauri to knowledge graphs, to collections of statements of first-order logic. The recent proliferation of Large Language Models (LLMs) has brought to bear interactive “knowledge bases” with general awareness of most things. As ontologists create ontologies from their understanding of reality; and as LLMs, presumably, possess some “understanding” of reality, embedded in their vector matrices corresponding to lexical terms from massive quantities of learned texts, a question is posed: what form of ontology can an LLM create when prompted about some novel facet of reality, without explicitly asking it for an ontology? I.e., will an LLM categorize things into bins, or a subsumption hierarchy, or perhaps something else? LLMs, as they are understood, respond when prompted with the most likely response, because they are predictors of next tokens, i.e., they are stochastic parrots. In any case, it is posited that, if prompted without any explicit request for an ontology, an LLM can produce an ontology of novel form, effectively granting insight into the “understanding” an LLM has of the world, as all humans possess an understanding of the world that ontologies are based upon. This paper explores the use of the flagship LLM, GPT-4, in forming an ontology of a novel domain.},
  keywords={Visualization;Taxonomy;Natural languages;Supervised learning;OWL;Stochastic processes;Organizations;ontology;taxonomy;large language models;GPT},
  doi={10.1109/TransAI60598.2023.00043},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10387634,
  author={Procko, Tyler Thomas and Ochoa, Omar and Elvira, Timothy},
  booktitle={2023 Fifth International Conference on Transdisciplinary AI (TransAI)}, 
  title={Automatic Generation of BFO-Compliant Aristotelian Definitions in OWL Ontologies with GPT}, 
  year={2023},
  volume={},
  number={},
  pages={141-146},
  abstract={Ontologies are representational artifacts that purport to accurately describe some aspect of reality, including the entities and the relations that hold between them. In computer science, ontologies are software artifacts containing the schematic structure for machine-readable knowledge, typically formed as a graph of subject-predicate-object triples, constrained through Description Logics. These resources and their relations are self-defining, i.e., some resource may be defined by considering all its stated relations. Resources are often attended with natural language annotations, that humans may read and interpret, such as labels and definitions. Many long-standing ontologies have useless lexical definitions that define resources cyclically, e.g., a FOAF: Person is simply defined as “A person”. In Aristotelian terms, the definition of a thing should be reducible, by using terms simpler than itself, such that every definition can be unpacked up to the most general thing, which can only be defined by stating examples and use cases. This paper presents an innovative technique that leverages the Generative Pre-trained Transformer (GPT) large language model, GPT -4, for automatically generating Aristotelian definition annotations for OWL classes that engenders compliance with the Basic Formal Ontology standard.},
  keywords={Annotations;Description logic;OWL;Natural languages;Maintenance engineering;Transformers;Software;ontology;epistemology;Linked Data;BFO;GPT},
  doi={10.1109/TransAI60598.2023.00042},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10343171,
  author={Reynolds, Sarah and Pate, William C. and Ochoa, Omar},
  booktitle={2023 IEEE Frontiers in Education Conference (FIE)}, 
  title={An Ontology and Management System for Learning Outcomes and Student Mastery}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Universities, faculty, and students use Learning Outcomes (LO) to create a shared understanding of the content provided in an individual course, known as Outcome-Based Education (OBE). One area of interest in OBE is evaluating whether the instructor and individual student performance have met the LO, which is integral to ensuring all invested parties are on the same page about class content and student performance. This work proposes a system for the management and evaluation of LO. Primarily, this work defines an ontology to support the management and evaluation of LO via Knowledge Graphs (KG). The KG links individual LO with individual assessment items. Two state-of-the-art Natural Language Processing models, BERT and ChatGPT, are evaluated in respect to their effectiveness in automating this linking. This data allows the educational professional to reflect on how well their assessments match the course's LO. The second part of this system harnesses student data to measure performance in relation to LO. In this Work-in-Progress paper, the system is prototyped and tested on the midterm results of a course in the Software Engineering curriculum. Student performance is documented in relation to each assessment question on the exams to measure student mastery of course material. Through this approach, courses can be evaluated and improved to deliver better quality education to all students. This includes improvements at the course level and possibilities for early intervention to ensure student success. This paper details the development of this system and through its implementation shows how it benefits engineering educators and their students.},
  keywords={Knowledge engineering;Taxonomy;Knowledge graphs;Ontologies;Market research;Chatbots;Software measurement;Learning outcomes;BERT;ontology;knowledge graph;assessment;Bloom's taxonomy},
  doi={10.1109/FIE58773.2023.10343171},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{10350744,
  author={Guizzardi, Giancarlo},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={An Ontological View on Types}, 
  year={2023},
  volume={},
  number={},
  pages={634-634},
  abstract={Types are fundamental for modeling, being an essential construct in all major modeling languages. These include traditional conceptual modeling languages - such as Entity-Relationship models, UML class diagrams, or Object-Role-Modeling (ORM) specifications, and knowledge representation languages alike (e.g., the Web Ontology Language - OWL).},
  keywords={Unified modeling language;OWL;Knowledge representation;Model driven engineering;Ontological Foundations for Modeling;Types and Taxonomic Structures;Multi-Level Modeling},
  doi={10.1109/MODELS-C59198.2023.00103},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10350821,
  author={Henzgen, Arne and Strey, Lukas},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Model-Driven Approach for Automatic Model Information Aggregation in Structured Documents}, 
  year={2023},
  volume={},
  number={},
  pages={403-413},
  abstract={While models are widely used in software development projects originating from industry and academic research, their documentation can be a time-intensive process. This paper focuses on providing a Proof of Concept for the automatic aggregation of various model data in two different document types conforming to ISO/IEC/IEEE 42010 architecture descriptions or instructional information documents according to ISO/IEC/IEEE 26514. Therefore, this work leverages a model-driven mapping approach of model information to the required document structure, dynamic templating algorithms to transform model data into text and a prototypical implementation that executes the defined mapping and transformation logic in practice. The generation results show that most of the documentation standard requirements can be fulfilled automatically and therefore, reduce the manual processing effort while enhancing consistency.},
  keywords={Industries;ISO Standards;Heuristic algorithms;Documentation;Computer architecture;Transforms;Data models;Model-Driven Engineering;Documentation;BPMN;UML;GSN;Model-to-Document;Architecture Description;Instructional Information},
  doi={10.1109/MODELS-C59198.2023.00072},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10350471,
  author={Chen, Boqi and Yi, Fandi and Varró, Dániel},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction}, 
  year={2023},
  volume={},
  number={},
  pages={588-596},
  abstract={Taxonomies represent hierarchical relations between entities, frequently applied in various software modeling and natural language processing (NLP) activities. They are typically subject to a set of structural constraints restricting their content. However, manual taxonomy construction can be time-consuming, incomplete, and costly to maintain. Recent studies of large language models (LLMs) have demonstrated that appropriate user inputs (called prompting) can effectively guide LLMs, such as GPT-3, in diverse NLP tasks without explicit (re-)training. However, existing approaches for automated taxonomy construction typically involve fine-tuning a language model by adjusting model parameters. In this paper, we present a general framework for taxonomy construction that takes into account structural constraints. We subsequently conduct a systematic comparison between the prompting and fine-tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset. Our result reveals the following: (1) Even without explicit training on the dataset, the prompting approach outperforms fine-tuning-based approaches. Moreover, the performance gap between prompting and fine-tuning widens when the training dataset is small. However, (2) taxonomies generated by the fine-tuning approach can be easily post-processed to satisfy all the constraints, whereas handling violations of the taxonomies produced by the prompting approach can be challenging. These evaluation findings provide guidance on selecting the appropriate method for taxonomy construction and highlight potential enhancements for both approaches.},
  keywords={Training;Computer science;Systematics;Computational modeling;Taxonomy;Ontologies;Software;taxonomy construction;domain-specific constraints;large language models;few-shot learning;fine-tuning},
  doi={10.1109/MODELS-C59198.2023.00097},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10350365,
  author={Ali, Syed Juned and Gavric, Aleksandar and Proper, Henderik and Bork, Dominik},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Encoding Conceptual Models for Machine Learning: A Systematic Review}, 
  year={2023},
  volume={},
  number={},
  pages={562-570},
  abstract={Conceptual models are essential in Software and Information Systems Engineering to meet many purposes since they explicitly represent the subject domains. Machine Learning (ML) approaches have recently been used in conceptual modeling to realize, among others, intelligent modeling assistance, model transformation, and metamodel classification. These works en-code models in various ways, making the encoded models suitable for applying ML algorithms. The encodings capture the models' structure and/or semantics, making this information available to the ML model during training. Therefore, the choice of the encoding for any ML-driven task is crucial for the ML model to learn the relevant contextual information. In this paper, we report findings from a systematic literature review which yields insights into the current research in machine learning for conceptual modeling (ML4CM). The review focuses on the various encodings used in existing ML4CM solutions and provides insights into i) which are the information sources, ii) how is the conceptual model's structure and/or semantics encoded, iii) why is the model encoded, i.e., for which conceptual modeling task and, iv) which ML algorithms are applied. The results aim to structure the state of the art in encoding conceptual models for ML.},
  keywords={Training;Analytical models;Systematics;Machine learning algorithms;Bibliographies;Semantics;Machine learning;Machine learning;Model-driven engineering;Model Encoding;Systematic Literature Review},
  doi={10.1109/MODELS-C59198.2023.00094},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10350790,
  author={Majumder, Mainak},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={A Domain-Driven Model Generation Framework for Cyber-Physical Production Systems}, 
  year={2023},
  volume={},
  number={},
  pages={172-178},
  abstract={The growing influence of Information Technologies in the manufacturing domain has led to the fourth industrial revolution (Industry 4.0). Cyber-Physical Production System (CPPS) is one of the fundamental concepts of Industry 4.0 that aims to develop an intelligent manufacturing environment by leveraging concepts like the Internet of Things (IoT), cloud computing, virtualization, and Artificial Intelligence (AI). However, the challenges originating from the technological heterogeneity in the manufacturing domain remain primary obstacles towards realising a fully automated CPPS. Among them, semantic heterogeneity in manufacturing information is the most crucial which can be attributed to technology and vendor-specific information modelling mechanisms. A CPPS requires seamless machine-to-machine communication which could be hindered due to the non-interoperability among machine data on a semantic level. Therefore, the primary focus of this thesis work is to understand the semantic interoperability challenges of CPPS and propose solutions to address those challenges. The proposed solution revolves around the development of semantic domain models using the modelling philosophies of Domain-Driven Design (DDD).},
  keywords={Production systems;Machine-to-machine communications;Philosophical considerations;Semantics;Model driven engineering;Fourth Industrial Revolution;Internet of Things;CPPS;Domain-Driven Design (DDD);Information Model;Industry 4.0},
  doi={10.1109/MODELS-C59198.2023.00044},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10350785,
  author={Dhaouadi, Mouna and Oakes, Bentley James and Famelis, Michalis},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Towards Understanding and Analyzing Rationale in Commit Messages Using a Knowledge Graph Approach}, 
  year={2023},
  volume={},
  number={},
  pages={622-630},
  abstract={Extracting rationale information from commit messages allows developers to better understand a system and its past development. Here we present our ongoing work on the Kantara end-to-end rationale reconstruction pipeline to a) structure rationale information in an ontologically-based knowledge graph, b) extract and classify this information from commits, and c) produce analysis reports and visualizations for developers. We also present our work on creating a labelled dataset for our running example of the Out-of-Memory component of the Linux kernel. This dataset is used as ground truth for our evaluation of NLP classification techniques which show promising results, especially the multi-classification technique XGBoost.},
  keywords={Visualization;Analytical models;Linux;Pipelines;Knowledge graphs;Model driven engineering;Data mining;rationale structuring;rationale extraction;Natural Language Processing;Linux;ontology;dataset;openCAESAR},
  doi={10.1109/MODELS-C59198.2023.00101},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10350773,
  author={Lange, Arne and Atkinson, Colin},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Modeling in LML with DOCL: A Contribution to the MULTI Warehouse Challenge}, 
  year={2023},
  volume={},
  number={},
  pages={649-658},
  abstract={This paper responds to the “Warehouse” challenge that was posed to the community of multi-level modeling researchers for the MULTI 2023 workshop. Given the many flavors of multi-level modeling approaches, the purpose of this and other similar challenges defined by the MULTI workshop community is to clarify the trade-offs entailed by the design choices underpinning the different approaches. This challenge revolves around product copies, product specifications, and product type specifications and how to guarantee certain properties at the product instance level. After first providing an overview of our modeling approach, and summarising the requirements laid out in the challenge, we present our solution using the LML and DOCL languages. We then discuss how well the solution fulfills the requirements laid out in the challenge.},
  keywords={Conferences;Semantics;Syntactics;Model driven engineering;Complexity theory;Safety;Currencies;Multi-level modeling;LML;DOCL},
  doi={10.1109/MODELS-C59198.2023.00106},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10350799,
  author={Elaasar, Maged and Rouquette, Nicolas and Wagner, David and Oakes, Bentley James and Hamou-Lhadj, Abdelwahab and Hamdaqa, Mohammad},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={openCAESAR: Balancing Agility and Rigor in Model-Based Systems Engineering}, 
  year={2023},
  volume={},
  number={},
  pages={221-230},
  abstract={Model-Based System Engineering (MBSE) employs models and formal languages to support development of complex (systems-of-) systems. NASA Jet Propulsion Laboratory (JPL) sees MBSE as a key approach to managing the complexity of system development. However, balancing agility and rigor in MBSE has been reported as a challenging task not yet addressed by modeling tools and frameworks. This is because existing MBSE approaches may enable agility but compromise rigor, or enhance rigor but impede agility. We discuss the challenges of balancing agility and rigor in MBSE across seven systems engineering architectural functions defined by the JPL Integrated Model-Centric Engineering (IMCE) initiative. We demonstrate how openCAESAR, an open-source MBSE methodology and framework created at JPL, can strike a balance between agility and rigor through a case study of the Kepler16b project and discussion of lessons learned from past projects.},
  keywords={NASA;Formal languages;Propulsion;Model driven engineering;Complexity theory;Modeling;Task analysis;Systems Engineering;Model-Based Systems Engineering;Ontology-based Modeling;OML;openCAESAR},
  doi={10.1109/MODELS-C59198.2023.00051},
  ISSN={},
  month={Oct},}@ARTICLE{10367969,
  author={Fatemi, Bahareh and Rabbi, Fazle and Opdahl, Andreas L.},
  journal={IEEE Access}, 
  title={Evaluating the Effectiveness of GPT Large Language Model for News Classification in the IPTC News Ontology}, 
  year={2023},
  volume={11},
  number={},
  pages={145386-145394},
  abstract={News classification plays a vital role in newsrooms, as it involves the time-consuming task of categorizing news articles and requires domain knowledge. Effective news classification is essential for categorizing and organizing a constant flow of information, serving as the foundation for subsequent tasks, such as news aggregation, monitoring, filtering, and organization. The automation of this process can significantly benefit newsrooms by saving time and resources. In this study, we explore the potential of the GPT large language model in a zero-shot setting for multi-class classification of news articles within the widely accepted International Press Telecommunications Council (IPTC) news ontology. The IPTC news ontology provides a structured framework for categorizing news, facilitating the efficient organization and retrieval of news content. By investigating the effectiveness of the GPT language model in this classification task, we aimed to understand its capabilities and potential applications in the news domain. This study was conducted as part of our ongoing research in the field of automated journalism.},
  keywords={Task analysis;Annotations;Ontologies;Adaptation models;Tag clouds;Support vector machines;Sports;IPTC media topics;journalism;large language models;news classification},
  doi={10.1109/ACCESS.2023.3345414},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10348639,
  author={Palagin, Oleksandr and Kaverinsky, Vladislav and Petrenko, Mykola and Malakhov, Kyrylo},
  booktitle={2023 IEEE 12th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)}, 
  title={Digital Health Systems: Ontology-Based Universal Dialog Service for Hybrid E-Rehabilitation Activities Support}, 
  year={2023},
  volume={1},
  number={},
  pages={84-89},
  abstract={The medical rehabilitation system in Ukraine encountered a set of crucial challenges that demanded immediate attention and action. The primary objective revolves around rehabilitating patients with Combat stress reaction. Ukraine possesses a network of medical and preventive institutions that cater to the psycho-physiological rehabilitation needs of military personnel. These institutions employ contemporary rehabilitation technologies. Nonetheless, not all individuals have access to long-term rehabilitation within these centers. Hence, the integration of telerehabilitation technology becomes crucial for patients dealing with post-traumatic stress disorder and related conditions. This integration, combined with objective monitoring of the functional state, holds significant importance. Remote patient-centered rehabilitation emerges as one of the most effective approaches within the realm of medical rehabilitation assistance. Moreover, there is a need for efficient methods that support the “Physical therapist - Patient - Multidisciplinary team” system in the field of rehabilitation. Hence, in this paper, we not only explore conventional rehabilitation techniques but also present and elucidate the following advancements: a revised and comprehensive understanding of the hybrid e-rehabilitation concept and its underlying principles, an enhanced formalization notion of the Smart-system for remote support in hybrid e-rehabilitation services and activities, and the conceptual framework and software implementation of the ontology-based universal dialog service within the Smart-system.},
  keywords={Data acquisition;Ontologies;Software;Electronic healthcare;Personnel;IEEE activities;Stress;Ontology engineering;hybrid e-rehabilitation;Telerehabilitation;Transdisciplinary research;Computational linguistics;Universal dialog service},
  doi={10.1109/IDAACS58523.2023.10348639},
  ISSN={2770-4254},
  month={Sep.},}@INPROCEEDINGS{10357717,
  author={Sevastjanova, Rita and Vogelbacher, Simon and Spitz, Andreas and Keim, Daniel and El-Assady, Mennatallah},
  booktitle={2023 IEEE Visualization in Data Science (VDS)}, 
  title={Visual Comparison of Text Sequences Generated by Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={11-20},
  abstract={Causal language models have emerged as the leading technology for automating text generation tasks. Although these models tend to produce outputs that resemble human writing, they still suffer from quality issues (e.g., social biases). Researchers typically use automatic analysis methods to evaluate the model limitations, such as statistics on stereotypical words. Since different types of issues are embedded in the model parameters, the development of automated methods that capture all relevant aspects remains a challenge. To tackle this challenge, we propose a visual analytics approach that supports the exploratory analysis of text sequences generated by causal language models. Our approach enables users to specify starting prompts and effectively groups the resulting text sequences. To this end, we leverage a unified, ontology-driven embedding space, serving as a shared foundation for the thematic concepts present in the generated text sequences. Visual summaries provide insights into various levels of granularity within the generated data. Among others, we propose a novel comparison visualization that slices the embedding space and represents the differences between two prompt outputs in a radial layout. We demonstrate the effectiveness of our approach through case studies, showcasing its potential to reveal model biases and other quality issues.},
  keywords={Analytical models;Visual analytics;Semantics;Layout;Data visualization;Writing;Linguistics;Causal Language Models;Text Generation;Prompt Output Comparison},
  doi={10.1109/VDS60365.2023.00007},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10339738,
  author={Stoyanov, Stanimir and Kumurdjieva, Milena and Tabakova-Komsalova, Veneta and Doukovska, Lyubka},
  booktitle={2023 International Conference on Big Data, Knowledge and Control Systems Engineering (BdKCSE)}, 
  title={Using LLMs in Cyber-Physical Systems for Agriculture - ZEMELA}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents the idea of developing an advisory service using the capabilities of generative artificial intelligence and in particular of Large Language Model. The service will assess the risks for farmers when preparing projects under different programs, taking into account the Bulgarian legislation related to agriculture, as well as the requirements of the relevant program. The results of a feasibility analysis are summarized in the article. Furthermore, two architectural approaches are discussed. The service will be integrated in the platform for smart agriculture named ZEMELA. A brief overview of this platform is also given in the article.},
  keywords={Smart agriculture;Knowledge engineering;Prototypes;Legislation;Cyber-physical systems;Big Data;Control systems;generative artificial intelligence;large language model;advisory service;smart agriculture},
  doi={10.1109/BdKCSE59280.2023.10339738},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10339730,
  author={Samardzhiev, Georgi and Nisheva-Pavlova, Maria},
  booktitle={2023 International Conference on Big Data, Knowledge and Control Systems Engineering (BdKCSE)}, 
  title={Application of Machine Learning and Natural Language Technologies in Building Semantic Search Systems: Case Study of a Virtual Legal Assistant}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={Semantic search is a type of advanced information search that is based on the searcher's intent as well as on the meaning of the searched terms and phrases in the relevant context, rather than relying only on their individual dictionary meanings. Classical approaches to the design and implementation of semantic search systems are primarily associated with the appropriate use of different types of ontologies or knowledge graphs, but recently these approaches are increasingly enriched or replaced by the utilization of modern language technologies and machine learning techniques. The paper discusses a methodology for application of specific machine learning methods and language technologies and information retrieval techniques in the development of a type of semantic search systems and presents its application in the creation of a virtual legal assistant.},
  keywords={Semantic search;Law;Heuristic algorithms;Natural languages;Neural networks;Machine learning;User interfaces;semantic search;language technology;machine learning;information retrieval;virtual assistant},
  doi={10.1109/BdKCSE59280.2023.10339730},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10340611,
  author={Labbé, Thomas and Castel, Pierre and Sanner, Jean-Michel and Saleh, Majd},
  booktitle={2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={ChatGPT for phenotypes extraction: one model to rule them all?}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Information Extraction (IE) is a core task in Natural Language Processing (NLP) where the objective is to identify factual knowledge in textual documents (often unstructured), and feed downstream use cases with the resulting output. In genomic medicine for instance, being able to extract the most precise list of phenotypes associated to a patient allows to improve genetic disease diagnostic, which represents a vital step in the modern deep phenotyping approach. As most of the phenotypic information lies in clinical reports, the challenge is to build an IE pipeline to automatically recognize phenotype concepts from free-text notes. A new machine learning paradigm around large language models (LLM) has given rise of an increasing number of academic works on this topic lately, where sophisticated combinations of different technics have been employed to improve the phenotypes extraction accuracy. Even more recently released, the ChatGPT1 application nevertheless raises the question of the relevance of these approches compared to this new generic one based on an instruction-oriented LLM. In this paper, we propose a rigorous evaluation of ChatGPT and the current state-of-the-art solutions on this specific task, and discuss the possible impacts and the technical evolutions to consider in the medical domain.Clinical relevance— Deep phenotyping on electronic health records has proven its ability to improve genetic diagnosis by clinical exomes [10]. Thus, comparing state-of-the-art solutions in order to derive insights and improving research paths is essential.},
  keywords={Temperature distribution;Pipelines;Statistical distributions;Machine learning;Ontologies;Chatbots;Information retrieval},
  doi={10.1109/EMBC40787.2023.10340611},
  ISSN={2694-0604},
  month={July},}@INPROCEEDINGS{10327850,
  author={Tothfalusi, Tamas and Varga, Eszter and Csiszar, Zoltan and Varga, Pal},
  booktitle={2023 19th International Conference on Network and Service Management (CNSM)}, 
  title={ML-Based Translation Methods for Protocols and Data Formats}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={In order to exchange information between systems, the information must get encoded into a predefined data format, and it must be transferred in a protocol that the communicating parties have agreed upon. This works well if all parties follow the same protocol standard and use the same data description schemes. If systems use different data formats or protocols, then some sort of translation is required. Protocol and data format translation has been attempted previously through rule-based approaches, ontologies, and also by using machine learning (ML) techniques. Due to the current advances related to AI/ML methods, tools, and infrastructure, the accuracy and feasibility of “translation” with ML-approaches improved significantly. This paper introduces a generic approach and methodology for translating data formats and protocols with ML-based methods and presents our initial results through JSON-XML and JSON-SenML translation.},
  keywords={Protocols;Machine learning;Ontologies;Standards;protocol translation;machine learning;neural machine translation;natural language processing;LLM},
  doi={10.23919/CNSM59352.2023.10327850},
  ISSN={2165-963X},
  month={Oct},}@ARTICLE{10328733,
  author={Wang, Songsong and Xu, Ouguan},
  journal={IEEE Access}, 
  title={Semantic Information Modeling and Implementation Method for Water Conservancy Equipment}, 
  year={2023},
  volume={11},
  number={},
  pages={133879-133890},
  abstract={Water conservancy equipment (WCE) has a large amount of information, structural heterogeneity and complex relationship leads to the difficulty of semantic interoperability in smart water conservancy. To overcome this issue, we propose the WCE information interaction dimension theory, modeling process and instancing method. First, we analyze the smart water conservancy ontology and information factor, and propose semantic information interaction dimension structure of water conservancy Ontology. Second, we construct the network information model structure of water conservancy, through the relationship degree, a tree model which can realize semantic expression and interoperability is formed through the dimensionality reduction of the model. Third, the component attribute set hierarchical relationship architecture water conservancy information model is established, which use XML language to describe this model. Moreover, the three types of instancing methods are proposed. Through OPC unified architecture (OPC UA) technology, water conservancy information model can implement semantic interoperability. The experimental show that the proposed method of semantic information modeling and semantic interoperability of WCE is feasible, and obvious advantages of complete semantic interoperability than in the model architecture, semantic structure and technical implementation.},
  keywords={Water conservation;Ontologies;Optical wavelength conversion;Semantics;Data models;Interoperability;Water resources;Information model;semantics;smart water conservancy;water conservancy equipment;OPC UA},
  doi={10.1109/ACCESS.2023.3336817},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10313152,
  author={Ermakov, Ivan and Lanin, Viacheslav and Lyadova, Lyudmila and Proskuryakov, Kirill},
  booktitle={2023 IEEE 17th International Conference on Application of Information and Communication Technologies (AICT)}, 
  title={Approach to the Development of Ontology-Driven Language Toolkits Based on Metamodeling}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The information systems are to be conforming to the requirements defined by domain experts. These requirements are formalized as models created with modeling tools. Applying these tools is complicated for domain experts. Domain specific modeling (DSM) with domain specific languages (DSL) reduces the semantic gap. However, the system development complication shifts to the creation of languages and tools for transforming models and code generation. An approach to automating DSL creation and facilitating code generation based on using multifaceted ontology is proposed. The generalized description of the multifaceted ontology is given. Tools of automating generation of new DSL metamodels based on mapping the corresponding domain ontology onto the metamodels of the selected base languages are described. Metamodels of the visual languages, grammars of the target text languages and transformation rules are also included into the ontology. The proposed approach is implemented as a research prototype of the language toolkits. Examples of metamodels and rules described in the ontology, as well as the results of their application are shown. The results of experiments confirmed practical significance of the approach to the ontology-driven language toolkits development.},
  keywords={Visualization;Codes;Prototypes;Metamodeling;Ontologies;Grammar;DSL;domain-specific modeling;domain-specific languages;metamodeling;multifaceted ontology;metamodel generation;model transformation rules},
  doi={10.1109/AICT59525.2023.10313152},
  ISSN={2472-8586},
  month={Oct},}@INPROCEEDINGS{10309534,
  author={Nanwani, Laksh and Agarwal, Anmol and Jain, Kanishk and Prabhakar, Raghav and Monis, Aaron and Mathur, Aditya and Jatavallabhula, Krishna Murthy and Abdul Hafez, A. H. and Gandhi, Vineet and Krishna, K. Madhava},
  booktitle={2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)}, 
  title={Instance-Level Semantic Maps for Vision Language Navigation}, 
  year={2023},
  volume={},
  number={},
  pages={507-512},
  abstract={Humans have a natural ability to perform semantic associations with the surrounding objects in the environment. This allows them to create a mental map of the environment, allowing them to navigate on-demand when given linguistic instructions. A natural goal in Vision Language Navigation (VLN) research is to impart autonomous agents with similar capabilities. Recent works take a step towards this goal by creating a semantic spatial map representation of the environment without any labeled data. However, their representations are limited for practical applicability as they do not distinguish between different instances of the same object. In this work, we address this limitation by integrating instance-level information into spatial map representation using a community detection algorithm and utilizing word ontology learned by large language models (LLMs) to perform open-set semantic associations in the mapping representation. The resulting map representation improves the navigation performance by two-fold (233%) on realistic language commands with instance-specific descriptions compared to the baseline. We validate the practicality and effectiveness of our approach through extensive qualitative and quantitative experiments.},
  keywords={Measurement;Visualization;Three-dimensional displays;Navigation;Semantics;Linguistics;Ontologies},
  doi={10.1109/RO-MAN57019.2023.10309534},
  ISSN={1944-9437},
  month={Aug},}@INPROCEEDINGS{10309510,
  author={Wilcock, Graham and Jokinen, Kristiina},
  booktitle={2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)}, 
  title={To Err Is Robotic; to Earn Trust, Divine: Comparing ChatGPT and Knowledge Graphs for HRI}, 
  year={2023},
  volume={},
  number={},
  pages={1396-1401},
  abstract={The paper discusses two current approaches to conversational AI, using large language models and knowledge graphs, and compares types of errors that occur in human-robot interactions based on these approaches. It provides example dialogues and describes solutions to several error types including false implications, ontological errors, theory of mind errors, and handling of speech recognition errors. The paper addresses issues of particular concern for earning user trust.},
  keywords={Visualization;Terminology;Semantics;Human-robot interaction;Knowledge graphs;Oral communication;Speech recognition},
  doi={10.1109/RO-MAN57019.2023.10309510},
  ISSN={1944-9437},
  month={Aug},}@INPROCEEDINGS{10298592,
  author={Li, Linyu and Xu, Sihan and Liu, Yang and Gao, Ya and Cai, Xiangrui and Wu, Jiarun and Song, Wenli and Liu, Zheli},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={LiSum: Open Source Software License Summarization with Multi-Task Learning}, 
  year={2023},
  volume={},
  number={},
  pages={787-799},
  abstract={Open source software (OSS) licenses regulate the conditions under which users can reuse, modify, and distribute the software legally. However, there exist various OSS licenses in the community, written in a formal language, which are typically long and complicated to understand. In this paper, we conducted a 661-participants online survey to investigate the perspectives and practices of developers towards OSS licenses. The user study revealed an indeed need for an automated tool to facilitate license understanding. Motivated by the user study and the fast growth of licenses in the community, we propose the first study towards automated license summarization. Specifically, we released the first high quality text summarization dataset and designed two tasks, i.e., license text summarization (LTS), aiming at generating a relatively short summary for an arbitrary license, and license term classification (LTC), focusing on the attitude inference towards a predefined set of key license terms (e.g., Distribute). Aiming at the two tasks, we present LiSum, a multi-task learning method to help developers overcome the obstacles of understanding OSS licenses. Comprehensive experiments demonstrated that the proposed jointly training objective boosted the performance on both tasks, surpassing state-of-the-art baselines with gains of at least 5 points w.r.t. F1 scores of four summarization metrics and achieving 95.13% micro average F1 score for classification simultaneously. We released all the datasets, the replication package, and the questionnaires for the community.},
  keywords={Training;Measurement;Learning systems;Surveys;Formal languages;Licenses;Multitasking;Open Source Software Licenses;Multi-Task Learning;License comprehension},
  doi={10.1109/ASE56229.2023.00150},
  ISSN={2643-1572},
  month={Sep.},}@INPROCEEDINGS{10298429,
  author={Phokela, Kanchanjot Kaur and Sikand, Samarth and Singi, Kapil and Dey, Kuntal and Sharma, Vibhu Saujanya and Kaulgud, Vikrant},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Smart Prompt Advisor: Multi-Objective Prompt Framework for Consistency and Best Practices}, 
  year={2023},
  volume={},
  number={},
  pages={1846-1848},
  abstract={Recent breakthroughs in Large Language Models (LLM), comprised of billions of parameters, have achieved the ability to unveil exceptional insight into a wide range of Natural Language Processing (NLP) tasks. The onus of the performance of these models lies in the sophistication and completeness of the input prompt. Minimizing the enhancement cycles of prompt with improvised keywords becomes critically important as it directly affects the time to market and cost of the developing solution. However, this process inevitably has a trade-off between the learning curve/proficiency of the user and completeness of the prompt, as generating such a solutions is an incremental process. In this paper, we have designed a novel solution and implemented it in the form of a plugin for Visual Studio Code IDE, which can optimize this trade-off, by learning the underlying prompt intent to enhance with keywords. This will tend to align with developers' collection of semantics while developing a secure code, ensuring parameter and local variable names, return expressions, simple pre and post-conditions. and basic control and data flow are met.},
  keywords={Visualization;Codes;Costs;Semantics;Time to market;Natural language processing;Task analysis;Prompt Engineering;Artificial Intelligence;Deep Learning;LLM;Ontology},
  doi={10.1109/ASE56229.2023.00019},
  ISSN={2643-1572},
  month={Sep.},}@INPROCEEDINGS{10296153,
  author={Elkodssi, Iman and Sbai, Hanae},
  booktitle={2023 International Conference on Digital Age & Technological Advances for Sustainable Development (ICDATA)}, 
  title={Toward Semantic Framework for Internet of Things-Aware Business Process Discovery}, 
  year={2023},
  volume={},
  number={},
  pages={12-16},
  abstract={The Internet of Things (IoT) is often considered a disruptive technology [1]. By using smart devices, it has the potential to change everyone's daily life. With large sets of advanced sensors and actuators, it can create opportunities for commercial organizations to establish new business models. A fundamental barrier to automatic business process sensing is the lack of modeling concepts that explicitly express Internet elements as components of a business process model. Thus, there is a clear need to model these processes associated with IoT elements in a formal and unambiguous manner. However, in the context of business processes, there is a lack of formalized and explicit descriptions of IoT elements, which hinders their effective modeling and management. This article proposes a semantic formalization of the business process management perspective in an IoT environment by proposing Extended BPMNO for IoT and Domain Ontology. It uses standard semantic technologies to give a semantic representation that allows us to describe concepts relating to the IoT and the elements of an executable business process described in BPMN.},
  keywords={Annotations;Semantics;Ontologies;Data models;Business process management;Internet of Things;Sustainable development;Business Process Management Notation (BPMN);IoT element;The IoT-aware BP;Ontology},
  doi={10.1109/ICDATA58816.2023.00012},
  ISSN={},
  month={May},}@INPROCEEDINGS{10284477,
  author={Ayad, Sarah},
  booktitle={2023 9th International Conference on Control, Decision and Information Technologies (CoDIT)}, 
  title={Towards a Meta-Modeling Approach for Business Process Models Improvement Based on Ontological Analysis}, 
  year={2023},
  volume={},
  number={},
  pages={1021-1026},
  abstract={Business process modeling enable smoother and more efficient decision making in the organizations as it achieve consistency and standardization of their operations, facilitate communication and collaboration between different stakeholders. Therefore, it is important to be able to model real world aspects. To this end, we have carried out a thorough review of the relevant literature which focuses on real-world aspects that Business process modeling languages BPMLs are not able to model. These aspects are based on ontological analysis and characterization of process modeling constructs. Our research aims to propose an approach for business process model modeling improvement by defining Object Constraint Language OCL rules written at the meta-model level making them independent from specific notations. We exploited IS domain knowledge, defined a meta-model, and added semantics to the meta-model by the mean of OCL constraints. As formalism we use the (OCL) with Ecore from the Eclipse Modeling Framework (EMF).},
  keywords={Analytical models;Process modeling;Semantics;Standards organizations;Metamodeling;Transforms;Standardization},
  doi={10.1109/CoDIT58514.2023.10284477},
  ISSN={2576-3555},
  month={July},}
@INPROCEEDINGS{10286646,
  author={Sadirmekova, Zhanna and Sambetbayeva, Madina and Daiyrbayeva, Elmira and Yerimbetova, Aigerim and Altynbekova, Zhanar and Murzakhmetov, Aslanbek},
  booktitle={2023 8th International Conference on Computer Science and Engineering (UBMK)}, 
  title={Constructing the Terminological Core of NLP Ontology}, 
  year={2023},
  volume={},
  number={},
  pages={81-85},
  abstract={The basis of any intellectual resource is a knowledge base, which, based on the basic terms of the field under consideration, builds relationships between them. Therefore, the first task to building multilingual information system using Natural language processing (NLP)in scientific and educational activities will be the development of a multilingual dictionary on modern NLP methods, including terms in Kazakh, English and Russian. For its construction, linguistic models for semantic dictionaries and thesauruses will be used, as well as methods of automatic extraction of terms from the corpus of texts of a given subject area. In this paper, a system of concepts of the NLP domain will be formalized, which will form the terminological core of the NLP ontology. To systematize information and provide support for multilingualism and accessibility, we plan to apply ontological engineering methods to systematize information and build the upper levels of the NLP ontology (its terminological core) using the dictionary of terms obtained at the previous stage. The ontology developed by us can later become the conceptual basis for a multilingual information system used in scientific and educational activities using NLP. This system will provide systematization of all information, convenient navigation on it, integration into a single information space, as well as access to it.},
  keywords={Dictionaries;Navigation;Semantics;Knowledge based systems;Ontologies;Linguistics;Natural language processing;Natural language processing;ontology;conceptual model;scientific and educational information system;ontological design patterns},
  doi={10.1109/UBMK59864.2023.10286646},
  ISSN={2521-1641},
  month={Sep.},}@ARTICLE{10286838,
  author={Mohammadat, Tage},
  journal={IEEE Access}, 
  title={A Model of Design for Computing Systems: A Categorical Approach}, 
  year={2023},
  volume={11},
  number={},
  pages={116304-116347},
  abstract={This paper introduces the model of design (MoD), a framework that leverages category theory to study the design and development of computer-driven systems, to the academic and engineering communities dealing with computer systems. The model of design aims to offer a minimal framework for modelling the design and development of embedded computation across domains and abstractions, focusing on functional and extra-functional aspects as well as overarching concerns for automaticity, correctness and reuse. This nuanced approach provides insights into the theory and practice of computer systems design.},
  keywords={Computational modeling;Semantics;Symbols;Solid modeling;Ontologies;Grammar;Context modeling;Design automation;System-level design;Computing systems;computer-aided design (CAD);electronic design automation (EDA);embedded system design;architectural design;model-driven engineering;domain-specific modelling languages;model of computation;system-level design;hardware/software co-design},
  doi={10.1109/ACCESS.2023.3325349},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10275523,
  author={Dhouib, Saadia and Huang, Yining and Smaoui, Asma and Bhanja, Tapanta and Gezer, Volkan},
  booktitle={2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Papyrus4Manufacturing: A Model-Based Systems Engineering approach to AAS Digital Twins}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={As digital twins gain momentum in their usage in diverse domains, the concept of Asset Administration Shells (AAS) has become very relevant for achieving the digital twin approach, where Administration Shells are the digital representation of physical assets. Being a relatively new concept in the Industrial Internet of Things (IIoT) domain, the tools and approaches for creating and deploying AASs are likewise in infancy. This paper introduces an open-source tool, Papyrus4Manufacturing, which provides a model-based systems engineering approach to the AAS. This toolset supports the creation of AAS digital twins from modeling to automatic deployment and connection to assets using the OPC UA protocol. This paper also includes an evaluation of its usability, as it is put to test with an academic use case.},
  keywords={Protocols;Databases;Memory;Software;Digital twins;Servers;Modeling;Digital Twins;Asset Administration Shell;Model-Based System Engineering;Unified Modelling Language;UML Profiles;Generative Software Engineering;OPC UA;BaSyx;Eclipse Papyrus},
  doi={10.1109/ETFA54631.2023.10275523},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10275701,
  author={Knorr, Felix and Kastner, Wolfgang},
  booktitle={2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Towards a Uniform Exchange Format for Home and Building Automation using VDI 3814}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Exchanging technical documents in the building automation domain is a complicated process. Files are distributed either as drawings, spreadsheets, or text documents. Each stakeholder has to re-enter the data into their own system, and changes are revised manually, often even without revision control. This paper presents a uniform exchange format based on the ’graphical’ standard VDI 3814. To increase acceptance, the industry standards XSD and XML were chosen. As a result of this work, a model is provided that covers the concepts and exchange files provided in the VDI 3814 standard. Given a supporting tool, data can be entered, revised, and exchanged automatically. Based on this unified representation, it is subsequently possible to transfer the data into one of the already existing ontologies in this domain by using model transformations. Some of these ontologies are also referred to in this paper.},
  keywords={Industries;Buildings;XML;Ontologies;Data models;Stakeholders;Standards;Building Automation;Uniform Format;VDI 3813;VDI 3814},
  doi={10.1109/ETFA54631.2023.10275701},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10266116,
  author={Powell, James and Balakireva, Lyudmila},
  booktitle={2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL)}, 
  title={Measuring the Growth of Ideas in a Title Corpus}, 
  year={2023},
  volume={},
  number={},
  pages={291-292},
  abstract={Beyond bibliometrics, there is interest in characterizing the evolution of the number of ideas in scientific papers, or more generally, exploring the progress of science. We use various tokenization and phrase extraction strategies combined with lexical diversity metrics to analyze titles in our corpus. We compared four lexical diversity metrics for each corpora variants, to look for indications that new concepts might be emerging over time.},
  keywords={Measurement;Semantics;Ecosystems;Ontologies;Tokenization;Libraries;Ecology;lexical diversity;natural language processing;word embeddings;science of science},
  doi={10.1109/JCDL57899.2023.00063},
  ISSN={2575-8152},
  month={June},}@INPROCEEDINGS{10266269,
  author={Sierra-Múnera, Alejandro and Westphal, Jan and Krestel, Ralf},
  booktitle={2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL)}, 
  title={Efficient Ultrafine Typing of Named Entities}, 
  year={2023},
  volume={},
  number={},
  pages={205-214},
  abstract={Ultrafine named entity typing (UFET) refers to the assignment of predefined labels to entity mentions in a given context. In contrast to traditional named entity typing, the number of potential labels is in the thousands and one mention can have more than one assigned type. Previous approaches either depend on large training datasets, or require inefficient encoding of all input-type combinations. Therefore, there is a need for investigating the efficiency during training and prediction of entity typing models in the ultrafine-grained setting, considering its distinctively bigger search space, compared to the coarse- and fine-grained tasks. To efficiently solve UFET, we propose Decent, a lightweight model that encodes, using a pretrained language model, the input sentences separately from the type labels. Additionally, we make use of negative oversampling to speed up the training while improving the generalization of unseen types. Using an openly available UFET dataset, we evaluated the classification and runtime performance of Decent and observed that training and prediction runtime is orders of magnitude faster than the current state-of-the-art approaches, while maintaining a competitive classification performance.},
  keywords={Training;Vocabulary;Runtime;Limiting;Computational modeling;Semantics;Predictive models;ultrafine enity typing;named entity recognition},
  doi={10.1109/JCDL57899.2023.00038},
  ISSN={2575-8152},
  month={June},}@INPROCEEDINGS{10261615,
  author={Zeng, Ling and Liang, Zaoqing and Liang, Yun and Huang, Peijie},
  booktitle={2023 5th International Conference on Computer Science and Technologies in Education (CSTE)}, 
  title={Research on Key Technologies of Automated Instructional Design for Engineering Education Courses}, 
  year={2023},
  volume={},
  number={},
  pages={86-92},
  abstract={An Automated Instructional Design (AID) solution for engineering education courses instructional design is proposed in this research. By limiting the Domain of AID to engineering education courses, and limiting the course objectives to the graduate attributes and professional competence of engineering education programme, as well as limiting the instructional methods to task-and-activity-based methods, and applying the evaluation vocabulary regularly used in the field of engineering education, a complete set of instructional design vocabulary related to the instructional design domain can be abstracted. Using the Unified Modeling Language (UML) Profile mechanism, the complete set of instructional design vocabulary can be represented by a designed visual model language, which provide a complete instructional design description language, named Instructional Design Visual Model Language (IDVML), dedicated to the field of engineering education for AID tool implementation. Then the existed Model-Driven Architecture (MDA) tools can be applied to design IDVML-related Platform Independent Model (PIM) and various Platform Specific Model (PSM) meta-models, and the conversion templates from PIM to various PSM, as well as various conversion templates from PSM to executable code, database table, Web page, course Ontology, etc., so as to realize the conversion from IDVML to program code directly. Finally, using the Meta-model Object Facility (MOF), an independent AID can be implemented. This AID can help the teacher to design the teaching objectives, content, tasks and activities, evaluation of engineering education related courses which meet the requirements of engineering education accreditation. At the same time, the course Ontology can be generated to provide Ontology basis for the subsequent construction of individualized learning system.},
  keywords={Vocabulary;Visualization;Limiting;Unified modeling language;Computer architecture;Ontologies;Accreditation;automated instructional design;engineering education;instructional design visulized modleing language;instructional design meta-model},
  doi={10.1109/CSTE59648.2023.00022},
  ISSN={},
  month={April},}@INPROCEEDINGS{10223881,
  author={Ji, Wei and Cao, Qinghong and Shi, Jin and Zhu, Enyao and Xu, Tianyi and He, Hao},
  booktitle={2023 26th ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)}, 
  title={Research on Domain Knowledge Representation Techniques}, 
  year={2023},
  volume={},
  number={},
  pages={150-157},
  abstract={Knowledge is the fruit of human's knowledge of the objective world in practice and the crystallization of wisdom. A knowledge base makes a knowledge-based system (or expert system) intelligent by structuring and sorting out knowledge in a specific field and storing, organizing, managing and using it in a computer using a scientific knowledge representation. Based on the research of knowledge base construction in securities industry, this paper first summarizes and explains several representative knowledge representation models. Then, it summarizes the application scenarios of common knowledge representation techniques in the fields of product design, robot control, and natural language processing. In addition, based on the investigation of the knowledge characteristics of the securities industry, a knowledge representation model of the securities industry based on 5W1lH is proposed to organize and manage the multimodal information resources and provide high-value information for user needs, while the knowledge representation technology of the mechanical industry based on hypergraph embedding is examined and the specific processes and application scenarios are summarized.},
  keywords={Industries;Service robots;Robot control;Knowledge representation;Product design;Natural language processing;Security;knowledge representation;knowledge graph;industry knowledge base},
  doi={10.1109/SNPD-Winter57765.2023.10223881},
  ISSN={},
  month={July},}@INPROCEEDINGS{10224086,
  author={Jousselme, A-L. and de Villiers, J.P. and de Freitas, A. and Blasch, E. and Dragos, V. and Pavlin, G. and Costa, P. C. and Laskey, K. B. and Laudy, C.},
  booktitle={2023 26th International Conference on Information Fusion (FUSION)}, 
  title={Uncertain about ChatGPT: enabling the uncertainty evaluation of large language models}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={ChatGPT, OpenAI’s chatbot, has gained consider-able attention since its launch in November 2022, owing to its ability to formulate articulated responses to text queries and comments relating to seemingly any conceivable subject. As impressive as the majority of interactions with ChatGPT are, this large language model has a number of acknowledged shortcomings, which in several cases, may be directly related to how ChatGPT handles uncertainty. The objective of this paper is to pave the way to formal analysis of ChatGPT uncertainty handling. To this end, the ability of the Uncertainty Representation and Reasoning Framework (URREF) ontology is assessed, to support such analysis. Elements of structured experiments for reproducible results are identified. The dataset built varies Information Criteria of Correctness, Non-specificity, Self-confidence, Relevance and Inconsistency, and the Source Criteria of Reliability, Competency and Type. ChatGPT’s answers are analyzed along Information Criteria of Correctness, Non-specificity and Self-confidence. Both generic and singular information are sequentially provided. The outcome of this preliminary study is twofold: Firstly, we validate that the experimental setup is efficient in capturing aspects of ChatGPT uncertainty handling. Secondly, we identify possible modifications to the URREF ontology that will be discussed and eventually implemented in URREF ontology Version 4.0 under development.},
  keywords={Uncertainty;Ontologies;Media;Chatbots;Cognition;Reliability;Disruptive technologies;Uncertainty evaluation;Ontology;Information quality;Source quality;Large Language Models;NLP},
  doi={10.23919/FUSION52260.2023.10224086},
  ISSN={},
  month={June},}@INPROCEEDINGS{10208670,
  author={Srinivasan, Tejas and Ren, Xiang and Thomason, Jesse},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={Curriculum Learning for Data-Efficient Vision-Language Alignment}, 
  year={2023},
  volume={},
  number={},
  pages={5619-5624},
  abstract={Aligning image and text encoders from scratch using contrastive learning requires large amounts of paired image-text data. We alleviate this need by aligning individually pre-trained language and vision representation models using a much smaller amount of paired data with a curriculum learning algorithm to learn fine-grained vision-language alignments. TOnICS (Training with Ontology-Informed Contrastive Sampling) initially samples minibatches whose image-text pairs contain a wide variety of objects to learn object-level vision-language alignment, and progressively samples minibatches where all image-text pairs contain the same object to learn finer-grained contextual alignment. Aligning pre-trained BERT and VinVL-OD models to each other using TOnICS outperforms CLIP on downstream zero-shot image retrieval using < 1% as much training data.},
  keywords={Training;Computer vision;Conferences;Computational modeling;Image retrieval;Bit error rate;Training data},
  doi={10.1109/CVPRW59228.2023.00595},
  ISSN={2160-7516},
  month={June},}@INPROCEEDINGS{10205809,
  author={Nadhila, Fadiah and Alamsyah, Andry},
  booktitle={2023 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)}, 
  title={Mapping Personality Traits to Customer Complaints: Framework for Personalized Customer Service}, 
  year={2023},
  volume={},
  number={},
  pages={96-101},
  abstract={The study establishes utilizing the Big Five Personality framework and a Personality Measurement Platform (PMP) for personality analysis. Moreover, Customer Complaint Ontology (CCOntology) framework implements a Naive Bayes machine learning methodology to evaluate and scrutinize customer complaints. The algorithm works by calculating the probability of each complaint category. This association is measured in percentages, enabling the identification of specific personality traits related to customer complaints through identifying complaint characteristics and areas of concern. The study has found that individuals with neurotic personality traits who encounter customer complaints are often associated with problem categories such as Non-Contract, Privacy, and Contract and are more likely to express strong emotional dissatisfaction with a product or service. Linking customer complaints with their corresponding personalities can be an incredibly effective and innovative strategy for personalized customer service businesses in anticipating their needs and providing tailored recommendations that can improve the likelihood of customers making purchases. This approach involves educating employees on the importance of actively listening to customers, asking relevant questions, and anticipating their needs, ensuring that businesses can enhance customer satisfaction while building a loyal customer base.},
  keywords={Technological innovation;Privacy;Social networking (online);Customer services;Oral communication;Ontologies;Big Data;Big Five Personality;Customer Complaint Ontology (CCOntology);Naive Bayes;Personality Measurement Platform;Personalized Customer Service},
  doi={10.1109/IAICT59002.2023.10205809},
  ISSN={2834-8249},
  month={July},}@INPROCEEDINGS{10196264,
  author={Okazaki, Sho and Shirafuji, Shouhei and Yasui, Toshinori and Ota, Jun},
  booktitle={2023 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)}, 
  title={A Framework to Support Failure Cause Identification in Manufacturing Systems through Generalization of Past FMEAs}, 
  year={2023},
  volume={},
  number={},
  pages={858-865},
  abstract={This study proposes a framework for inferring the causes of failures occurring in manufacturing systems from past Failure Mode and Effect Analyses (FMEAs) conducted on other systems to assist in inspecting and maintaining the systems. Among various manufacturing systems, a framework to search past FMEAs and the corresponding causes of the failure requires solving the following problems. First, the difference in products, equipment, and wording to represent them make it difficult to search the similar failure phenomenon from FMEAs. Secondly, the causes of failure highly depend on the process flow of the system until the failure occurs. Therefore, it is also hard to find appropriate failure causes from FMEAs without reflecting on the process. The framework solves the first issue by generalizing descriptions in past FMEAs based on structured concepts of manufacturing systems in an ontology before inference of causes to address. Furthermore, the framework analyzes the correspondence of the process flows between the target manufacturing system and past FMEAs using a process order model generated by SysML diagrams to solve the second issue. The comparison between the causes inferred by the proposed framework and by skilled experts for three typical failures in the manufacturing system and the interview with them about the plausibility of the inference results showed that more than 73 % of them were valid.},
  keywords={Analytical models;Mechatronics;Ontologies;Maintenance engineering;Search problems;Cognition;Data models},
  doi={10.1109/AIM46323.2023.10196264},
  ISSN={2159-6255},
  month={June},}@ARTICLE{10198434,
  author={Jaradeh, Amer and Kurdy, Mohamad-Bassam},
  journal={IEEE Access}, 
  title={ArEmotive Bridging the Gap: Automatic Ontology Augmentation Using Zero-Shot Classification for Fine-Grained Sentiment Analysis of Arabic Text}, 
  year={2023},
  volume={11},
  number={},
  pages={81318-81330},
  abstract={Human-computer interaction remains one of the final frontiers to conquer while held in perspective with the rapid developments and technology growth over recent years. It is an arduous task to convey the true human intent to the machine in order to generate a computerized relevant decision in a certain field. In recent years, focus has shifted to cover fields of study that relate to Sentiment Analysis (SA) to improve and ease the tasks of our daily lives. We Propose ArEmotive (Arabic Emotive), a fine-grained sentiment analysis system that is human-independent which can automatically grow its source of information allowing for more precision and a greater dataset each time it is used through ontology augmentation and classification. Our proposed architecture relies on multiple data sources running through certain pipelines to generate a central online repository utilized by any mobile system to access this info-base. This system is important because many researchers in the field of automated ontology alignment and ontology mapping achieved a semi-automated approach to map new ontologies out of old ones or to extend already existing ontologies with data from new ones. ArEmotive identifies fine-grained emotions in text based on a dynamic ontology enriched through ontology alignment, mapping and machine learning assisted classification, resulting in a structure that contributes in: a centralized dataset ever growing to fit the need of the users, a sustainable structure able to allocate new data sources without the need to modify the system, ability to generate appropriate information even with the absence of “parent” sources.},
  keywords={Ontologies;Task analysis;Social networking (online);Sentiment analysis;Bridges;Blogs;Soft sensors;Emotion recognition;Arabic NLP;fine-grained emotions;ontology augmentation},
  doi={10.1109/ACCESS.2023.3300737},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10193435,
  author={Belani, Hrvoje and Šolić, Petar and Perković, Toni and Pleština, Vladimir},
  booktitle={2023 8th International Conference on Smart and Sustainable Technologies (SpliTech)}, 
  title={IoT Ontology Development Process for Well-Being, Aging and Health: Challenges and Opportunities}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Ontology development processes are not trivial, given the inherently complex nature of knowledge capturing and management, as well as the need to provide structural and methodical approach on the process methodology itself in order for it to be adopted and usable. If aiming to develop an ontology for multidimensional concepts, such as well-being, aging and health, it is certain that knowledge from multiple domains have to be included, which only extends the time needed for ontology engineering. If such environments aim to be supported by the Internet of Things, than challenges rise even more. This paper provides a scoping analysis of existing well-known ontology development methodologies, with a note on the extent of their adoption and readiness to be used in a multi-domain circumstances. The approach to IoT ontology development process tailoring has been presented and elaborated, as well as the challenges specific to IoT ontology development for well-being, aging and health. Finally, research opportunities have been presented and future directions given on providing more comprehensive, more tailored and more usable ontology development methodologies.},
  keywords={Knowledge engineering;Semantics;Ontologies;Aging;Reliability;Internet of Things;Internet of Things;ontology;well-being;e-health;development process},
  doi={10.23919/SpliTech58164.2023.10193435},
  ISSN={},
  month={June},}@INPROCEEDINGS{10184697,
  author={Geng, Yuxia and Chen, Jiaoyan and Pan, Jeff Z. and Chen, Mingyang and Jiang, Song and Zhang, Wen and Chen, Huajun},
  booktitle={2023 IEEE 39th International Conference on Data Engineering (ICDE)}, 
  title={Relational Message Passing for Fully Inductive Knowledge Graph Completion}, 
  year={2023},
  volume={},
  number={},
  pages={1221-1233},
  abstract={In knowledge graph completion (KGC), predicting triples involving emerging entities and/or relations, which are unseen when the KG embeddings are learned, has become a critical challenge. Subgraph reasoning with message passing is a promising and popular solution. Some recent methods have achieved good performance, but they (i) usually can only predict triples involving unseen entities alone, failing to address more realistic fully inductive situations with both unseen entities and unseen relations, and (ii) often conduct message passing over the entities with the relation patterns not fully utilized. In this study, we propose a new method named RMPI which uses a novel Relational Message Passing network for fully Inductive KGC. It passes messages directly between relations to make full use of the relation patterns for subgraph reasoning with new techniques on graph transformation, graph pruning, relation-aware neighborhood attention, addressing empty subgraphs, etc., and can utilize the relation semantics defined in the KG’s ontological schema. Extensive evaluation on multiple benchmarks has shown the effectiveness of RMPI’s techniques and its better performance compared with the existing methods that support fully inductive KGC. RMPI is also comparable to the state-of-the-art partially inductive KGC methods with very promising results achieved. Our codes, data and some supplementary experiment results are available at https://github.com/zjukg/RMPI.},
  keywords={Knowledge engineering;Codes;Message passing;Semantics;Knowledge graphs;Benchmark testing;Data engineering;Knowledge Graph;Inductive Knowledge Graph Completion;Message Passing;Link Prediction;Ontology},
  doi={10.1109/ICDE55515.2023.00098},
  ISSN={2375-026X},
  month={April},}@INPROCEEDINGS{10187511,
  author={Weigand, Hans and Johannesson, Paul},
  booktitle={2023 IEEE 25th Conference on Business Informatics (CBI)}, 
  title={How to Identify your Design Science Research Artifact}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  abstract={Design Science Research (DSR) is about the development and investigation of artifacts in context. However, in many articles that subscribe to a DSR approach, the artifact is not clearly classified and identified. Very little attention has been given in the DSR literature on this topic, so guidelines are lacking. Based on artifact ontology, this paper proposes guidelines for design researchers in the IS domain on how to specify both the artifact and the research objective. For validation, the guidelines have been applied to a range of DSR papers. Our results show that the artifact definition guidelines can add to the precision of the research object specification.},
  keywords={Philosophical considerations;Design methodology;Bibliographies;Ontologies;Data science;Data models;Informatics;Design Science;artifact ontology;research methodology},
  doi={10.1109/CBI58679.2023.10187511},
  ISSN={2378-1971},
  month={June},}@ARTICLE{10185050,
  author={Razzaq, Muhammad Saad and Maqbool, Fahad and Ilyas, Muhammad and Jabeen, Hajira},
  journal={IEEE Access}, 
  title={EvoRecipes: A Generative Approach for Evolving Context-Aware Recipes}, 
  year={2023},
  volume={11},
  number={},
  pages={74148-74164},
  abstract={Generative AI e.g. Large Language Models (LLMs) can be used to generate new recipes. However, LLMs struggle with more complex aspects like recipe semantics and process comprehension. Furthermore, LLMs have limited ability to account for user preferences since they are based on statistical patterns. As a result, these recipes may be invalid. Evolutionary algorithms inspired by the process of natural selection are optimization algorithms that use stochastic operators to generate new solutions. These algorithms can generate large number of solutions from the set of possible solution space. Moreover, these algorithms have the capability to incorporate user preferences in fitness function to generate novel recipes that are more aligned with the fitness objective. In this paper, we propose the  $EvoRecipes$  framework to generate novel recipes. The  $EvoRecipes$  framework utilizes both Genetic Algorithm and generative AI in addition to  $RecipeOn$  ontology, and  $RecipeKG$  knowledge graph. Genetic Algorithm explore the large solution space of encoded recipe solutions and are capable of incorporating user preferences, while LLMs are used to generate recipe text from encoded recipe solutions.  $EvoRecipes$  uses a population of context-aware recipe solutions from the  $RecipeKG$  knowledge graph.  $RecipeKG$  encodes recipes in RDF format using classes and properties as defined in the  $RecipeOn$  ontology. Moreover, to evaluate the alignment of  $EvoRecipe$  generated recipes with multiple intended objectives, we propose a fitness function that incorporates novelty, simplicity, visual appeal, and feasibility. Additionally, to evaluate the quality of the  $EvoRecipe$  generated recipes while considering the subjective nature of recipes, we conducted a survey using multi-dimensional metrics (i.e. contextual, procedural, and novelty). Results show that  $EvoRecipes$  generated recipes are novel, valid and incorporate user preferences.},
  keywords={Ontologies;Creativity;Resource description framework;Knowledge graphs;Genetic algorithms;Sociology;Semantics;Food products;Knowledge graph;ontology;computational creativity;recipe evolution;recipe;food},
  doi={10.1109/ACCESS.2023.3296144},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10145722,
  author={Bandara, H. M. R. L. and Ranathunga, L.},
  booktitle={2023 3rd International Conference on Advanced Research in Computing (ICARC)}, 
  title={Ontology Based Restaurant Recommendation Approach}, 
  year={2023},
  volume={},
  number={},
  pages={78-83},
  abstract={As the world moves forward, the restaurant industry is rapidly expanding. Customers may never physically evaluate a restaurant based on its services until that customer has practical experience with it. A better recommendation mechanism can always direct the customer to the correct location, resulting in a positive outcome. The paper discusses an approach of a context rich chatbot that can identify the customer’s mode of thinking using a restaurant ontology that suggests relevant restaurants and foods. Most importantly the defined methodology will use a hybrid version of knowledge bases along with a two-way bind to the primary knowledge base. The chatbot will proceed to find relationships in the ontology by tracing concept definitions and properties while feeding information from the database. The main components related to the proposed system are Natural Language Understanding (NLU) pipeline, dialog management module, action server, knowledge query module, and data repository (MongoDB). This mechanism was evaluated through information retrieval measures.},
  keywords={Industries;Databases;Knowledge based systems;Pipelines;Ontologies;Chatbots;Information retrieval;Rasa Framework;Ontology;Database;Knowledge Query Module;Dialog Management Module;Action Server},
  doi={10.1109/ICARC57651.2023.10145722},
  ISSN={},
  month={Feb},}@INBOOK{10144419,
  author={Godspower, Osaretin Ekuobase and Esingbemi, Princewill Ebietomere},
  booktitle={Semantic Technologies for Intelligent Industry 4.0 Applications}, 
  title={1 Semantic Search Engine in Industry 4.0}, 
  year={2023},
  volume={},
  number={},
  pages={1-24},
  abstract={As the world enters the era of big data, there is a serious need to give a semantic perspective to the data in order to find unseen patterns, derive meaningful information, and make intelligent decisions. Semantic technologies offer the richest machine-interpretable (rather than just machine-processable) and explicit semantics that are being extensively used in various domains and industries. These technologies reduce the problem of large semantic loss in the process of modelling knowledge, and provide sharable, reusable knowledge, and a common understanding of the knowledge. As a result, the interoperability and interconnectivity of the model make it priceless for addressing the issues of querying data. These technologies work with the concepts and relations that are very close to the working of the human brain. They provide a semantic representation of any data format: unstructured or semi-structured. As a consequence, data becomes real-world entity rather than a string of characters. For these reasons, semantic technologies are highly valuable tools to simplify the existing problems of the industry leading to new opportunities. However, there are some challenges that need to be addressed to make industrial applications and machines smarter. This book aims to provide a roadmap for semantic technologies and highlights the role of these technologies in industry. The book also explores the present and future prospects of these semantic technologies along with providing answers to various questions like: Are semantic technologies useful for the next era (industry 4.0)? Why are semantic technologies so popular and extensively used in the industry? Can semantic technologies make intelligent industrial applications? Which type of problem requires the immediate attention of researchers? Why are semantic technologies very helpful in people&#x2019;s future lives? This book will potentially serve as an important guide towards the latest industrial applications of semantic technologies for the upcoming generation, and thus becomes a unique resource for scholars, researchers, professionals and practitioners in the field.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770227810},
  url={https://ieeexplore.ieee.org/document/10144419},}@INPROCEEDINGS{10131050,
  author={Dunbar, Daniel and Vierlboeck, Maximilian and Blackburn, Mark},
  booktitle={2023 IEEE International Systems Conference (SysCon)}, 
  title={Use of Natural Language Processing in Digital Engineering Context to Aid Tagging of Model}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper uses Natural Language Processing to provide augmented intelligence assistance to the resource intensive task of aligning systems engineering artifacts, namely text requirements and system models, with ontologies. Ontologies are a key enabling technology for digital, multidisciplinary interoperability. The approach presented in this paper combines the efficiency of statistical based natural language processing to process large sets of data with expert verification of output to enable accurate alignment to ontologies in a time efficient manner. It applies this approach to an example from the telecommunications domain to demonstrate the workflows and highlight key points in the process. Enabling easier, faster alignment of systems engineering artifacts with ontologies allows for a holistic view of a system under design and enables interoperability between tools and domains.},
  keywords={Measurement;Ontologies;Tagging;Natural language processing;Telecommunications;Requirements engineering;Task analysis;ontology;natural language processing;semantic web;digital engineering;authoritative source of truth;augmented intelligence},
  doi={10.1109/SysCon53073.2023.10131050},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{10131078,
  author={Chen, Rui and Wang, Guoxin and Wu, Shouxuan and Lu, Jinzhi and Yan, Yan},
  booktitle={2023 IEEE International Systems Conference (SysCon)}, 
  title={A Service-oriented Approach Supporting Model Integration in Model-based Systems Engineering}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={When using Model-Based Systems Engineering (MBSE) to develop complex systems, models using different syntax and semantics are typically implemented in a heterogeneous environment which leads to difficulties to realize data integrations across the entire lifecycle. Specifically, seamless exchanges between models of different modeling tools are needed to support system lifecycle activities such as requirement analysis, function analysis, verification and validation. This article illustrates a service-oriented approach to support model integration for model-based systems engineering, especially between architecture design and system verification. First, a set of semantic mapping rules between architecture models and simulation models based on Open Service of Lifecycle Collaboration (OSLC) are proposed to support the formalization of technical resources (models, data, APIs). Then OSLC adapters are developed to transform models, data and APIs into web-based services. The services are deployed by a service discovering plug-in within a specific modeling tool for model information exchange. The approach is illustrated by a case study on KARMA architecture model and Modelica simulation model for a six-degree-of-freedom robot (RobotR3) system. We evaluate the availability and efficiency of this method from both qualitative and quantitative perspectives. The results show that our approach is effective in model and data integration.},
  keywords={Adaptation models;Analytical models;System verification;Semantics;Data integration;Transforms;Syntactics;Model integration;Model-Based Systems Engineering;Open Service for Lifecycle Collaboration;Modelica},
  doi={10.1109/SysCon53073.2023.10131078},
  ISSN={2472-9647},
  month={April},}@ARTICLE{10129977,
  author={Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Yang, Yumeng and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
  journal={IEEE Transactions on NanoBioscience}, 
  title={Protein Function Prediction With Functional and Topological Knowledge of Gene Ontology}, 
  year={2023},
  volume={22},
  number={4},
  pages={755-762},
  abstract={Gene Ontology (GO) is a widely used bioinformatics resource for describing biological processes, molecular functions, and cellular components of proteins. It covers more than 5000 terms hierarchically organized into a directed acyclic graph and known functional annotations. Automatically annotating protein functions by using GO-based computational models has been an area of active research for a long time. However, due to the limited functional annotation information and complex topological structures of GO, existing models cannot effectively capture the knowledge representation of GO. To solve this issue, we present a method that fuses the functional and topological knowledge of GO to guide protein function prediction. This method employs a multi-view GCN model to extract a variety of GO representations from functional information, topological structure, and their combinations. To dynamically learn the significance weights of these representations, it adopts an attention mechanism to learn the final knowledge representation of GO. Furthermore, it uses a pre-trained language model (i.e., ESM-1b) to efficiently learn biological features for each protein sequence. Finally, it obtains all predicted scores by calculating the dot product of sequence features and GO representation. Our method outperforms other state-of-the-art methods, as demonstrated by the experimental results on datasets from three different species, namely Yeast, Human and Arabidopsis. Our proposed method’s code can be accessed at: https://github.com/Candyperfect/Master.},
  keywords={Proteins;Feature extraction;Amino acids;Annotations;Predictive models;Biological system modeling;Protein sequence;Convolutional neural networks;Graph neural networks;Protein function prediction;gene ontology;multi-view GCN;pre-trained language model},
  doi={10.1109/TNB.2023.3278033},
  ISSN={1558-2639},
  month={Oct},}@ARTICLE{10123130,
  author={Zelina, Petr and Halámková, Jana and Nováček, Vít},
  journal={IEEE Transactions on NanoBioscience}, 
  title={Extraction, Labeling, Clustering, and Semantic Mapping of Segments From Clinical Notes}, 
  year={2023},
  volume={22},
  number={4},
  pages={781-788},
  abstract={This work is motivated by the scarcity of tools for accurate, unsupervised information extraction from unstructured clinical notes in computationally underrepresented languages, such as Czech. We introduce a stepping stone to a broad array of downstream tasks such as summarisation or integration of individual patient records, extraction of structured information for national cancer registry reporting or building of semi-structured semantic patient representations that can be used for computing patient embeddings. More specifically, we present a method for unsupervised extraction of semantically-labeled textual segments from clinical notes and test it out on a dataset of Czech breast cancer patients, provided by Masaryk Memorial Cancer Institute (the largest Czech hospital specialising exclusively in oncology). Our goal was to extract, classify (i.e. label) and cluster segments of the free-text notes that correspond to specific clinical features (e.g., family background, comorbidities or toxicities). Finally, we propose a tool for computer-assisted semantic mapping of segment types to pre-defined ontologies and validate it on a downstream task of category-specific patient similarity. The presented results demonstrate the practical relevance of the proposed approach for building more sophisticated extraction and analytical pipelines deployed on Czech clinical notes.},
  keywords={Task analysis;Semantics;Feature extraction;Ontologies;Nanobioscience;Measurement;Clinical diagnosis;Text categorization;Information retrieval;NLP;EHR;clinical notes;information extraction;text classification},
  doi={10.1109/TNB.2023.3275195},
  ISSN={1558-2639},
  month={Oct},}@INPROCEEDINGS{10095062,
  author={Abrougui, Rim and Damnati, Géraldine and Heinecke, Johannes and Béchet, Frédéric},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Abstract Representation for Multi-Intent Spoken Language Understanding}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Current sequence tagging models based on Deep Neural Network models with pretrained language models achieve almost perfect results on many SLU benchmarks with a flat semantic annotation at the token level such as ATIS or SNIPS. When dealing with more complex human-machine interactions (multi-domain, multi-intent, dialog context), relational semantic structures are needed in order to encode the links between slots and intents within an utterance and through dialog history. We propose in this study a new way to project annotation in an abstract structure with more compositional expressive power and a model to directly generate this abstract structure. We evaluate it on the MultiWoz dataset in a contextual SLU experimental setup. We show that this projection can be used to extend the existing flat annotations towards graph-based structures.},
  keywords={Human computer interaction;Deep learning;Annotations;Semantics;Neural networks;Natural languages;Tagging;Natural Language Understanding;Spoken Language Understanding;sequence tagging;sequence-to-sequence models},
  doi={10.1109/ICASSP49357.2023.10095062},
  ISSN={2379-190X},
  month={June},}@INPROCEEDINGS{10096669,
  author={Su, Ruolin and Yang, Jingfeng and Wu, Ting-Wei and Juang, Biing-Hwang},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Choice Fusion As Knowledge For Zero-Shot Dialogue State Tracking}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={With the demanding need for deploying dialogue systems in new domains with less cost, zero-shot dialogue state tracking (DST), which tracks user’s requirements in task-oriented dialogues without training on desired domains, draws attention increasingly. Although prior works have leveraged question-answering (QA) data to reduce the need for in-domain training in DST, they fail to explicitly model knowledge transfer and fusion for tracking dialogue states. To address this issue, we propose CoFunDST, which is trained on domain-agnostic QA datasets and directly uses candidate choices of slot-values as knowledge for zero-shot dialogue-state generation, based on a T5 pre-trained language model. Specifically, CoFunDST selects highly-relevant choices to the reference context and fuses them to initialize the decoder to constrain the model outputs. Our experimental results show that our proposed model achieves outperformed joint goal accuracy compared to existing zero-shot DST approaches in most domains on the MultiWOZ 2.1. Extensive analyses demonstrate the effectiveness of our proposed approach for improving zero-shot DST learning from QA.},
  keywords={Training;Costs;Fuses;Signal processing;Data models;Decoding;Task analysis;Dialogue state tracking;zero-shot;question answering;pre-trained language model;knowledge fusion},
  doi={10.1109/ICASSP49357.2023.10096669},
  ISSN={2379-190X},
  month={June},}@INPROCEEDINGS{10112187,
  author={Saraswat, Deepak},
  booktitle={2023 6th International Conference on Information Systems and Computer Networks (ISCON)}, 
  title={Ontology Based Agriculture Data Mining using IWO and RNN}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={An ontology is a machine-interpretable formal description of domain knowledge. In current years, ontologies have risen to prominence as a key tool for demonstrating domain knowledge and a key element of several knowledge management systems, decision-support systems (DSS) and other intelligent systems including in agriculture. However, a study of the current literature on agricultural ontologies suggests that the majority of research that suggest agricultural ontologies lack a clear assessment mechanism. This is unwanted because this is impossible to assess the value of ontologies in research and practise without well-structured assessment mechanisms. Furthermore, relying on such ontologies and sharing them on the Semantic Web or amongst semantic-aware apps is problematic. This paper presents a framework for selecting appropriate assessment techniques for Ontology Based Agriculture Data Mining utilizing Invasive Weed Optimization (IWO) and Re-current Neural Network (RNN) that appears to be absent from most recent agricultural ontology research. The framework facilitates the selection of relevant evaluation techniques for a particular ontology based on its intended user.},
  keywords={Decision support systems;Semantic Web;Knowledge engineering;Recurrent neural networks;Prototypes;Ontologies;Agriculture;Data Mining;Ontology;IWO;RNN;Agriculture 4.0;Agriculture 5.0},
  doi={10.1109/ISCON57294.2023.10112187},
  ISSN={2832-143X},
  month={March},}@ARTICLE{10107403,
  author={Li, Jijie and Shuang, Kai and Guo, Jinyu and Shi, Zengyi and Wang, Hongman},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Enhancing Semantic Relation Classification With Shortest Dependency Path Reasoning}, 
  year={2023},
  volume={31},
  number={},
  pages={1550-1560},
  abstract={Relation Classification (RC) is a basic and essential task of Natural Language Processing. Existing RC methods can be classified into two categories: sequence-based methods and dependency-based methods. Sequence-based methods identify the target relation based on the overall semantics of the whole sentence, which will inevitably introduce noisy features. Dependency-based methods extract indicative word-level features from the Shortest Dependency Path (SDP) between given entities and attempt to establish a statistical association between the words and the target relations. This pattern relatively eliminates the influence of noisy features and achieves a robust performance on long sentences. Nevertheless, we observe that majority of relation classification processes involve complex semantic reasoning which is hard to be achieved based on the word-level statistical association. To solve this problem, we categorize all relations into atomic relations and composed-relations. The atomic relations are the basic relations that can be identified based on the word-level features, while the composed-relation requires to be deducted from multiple atomic relations. Correspondingly, we propose the Atomic Relation Encoding and Reasoning Model (ATERM). In the atomic relation encoding stage, ATERM groups the word-level features and encodes multiple atomic relations in parallel. In the atomic relation reasoning stage, ATERM establishes the atomic relation chain where relation-level features are extracted to identify composed-relations. Experiments show that our method achieves state-of-the-art results on the three most popular relation classification datasets – TACRED, TACRED-Revisit, and SemEval 2010 task 8 with significant improvements.},
  keywords={Semantics;Knowledge based systems;Ontologies;Feature extraction;Cognition;Encoding;Natural language processing;Information extraction;graph convolution;shortest dependency path;semantic reasoning},
  doi={10.1109/TASLP.2023.3265205},
  ISSN={2329-9304},
  month={},}@ARTICLE{10100906,
  author={Huang, Heyan and Liu, Xiao and Shi, Ge and Liu, Qian},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Event Extraction With Dynamic Prefix Tuning and Relevance Retrieval}, 
  year={2023},
  volume={35},
  number={10},
  pages={9946-9958},
  abstract={We consider event extraction in a generative manner with template-based conditional generation. Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts, these generation-based methods have several significant challenges, including using suboptimal prompts, static event type information, and the overwhelming number of irrelevant event types. In this article, we propose a generative template-based method with dynamic prefixes and a relevance retrieval framework for event extraction (GREE) by first integrating context information with type-specific prefixes to learn a context-specific prefix for each context, and then retrieving the relevant event types with an adaptive threshold. Experimental results show that our model achieves competitive results with the state-of-the-art classification-based model OneIE on ACE 2005 and achieves the best performances on ERE. Additionally, our model is proven to be portable to new types of events effectively.},
  keywords={Task analysis;Tuning;Data mining;Adaptation models;Ontologies;Decoding;Feature extraction;Conditional generation;dense retrieval;event extraction;prompt tuning},
  doi={10.1109/TKDE.2023.3266495},
  ISSN={1558-2191},
  month={Oct},}@INPROCEEDINGS{10090855,
  author={Tang, Jin and Xu, Chengxian and Zhang, Wanda},
  booktitle={2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)}, 
  title={Construction and Accurate Retrieval Method of Knowledge Graph of Automobile Engine Fault}, 
  year={2023},
  volume={},
  number={},
  pages={336-345},
  abstract={In order to improve the efficiency and accuracy of automobile engine fault maintenance, an accurate retrieval method of automobile engine fault driven by knowledge graph was proposed. Firstly, the definition and framework of knowledge graph are discussed. The entity extraction of engine fault features was carried out by multi-source neural network, and the disambiguation of fault entities was carried out by integrating entity link technologies; Secondly, fault knowledge reasoning is carried out to eliminate the wrong knowledge in the knowledge base and infer new knowledge to form a complete knowledge graph.. On this basis, the retrieval subgraph of engine fault semantics is designed. Combined with the influence of physical distance and proximity, the retrieval result evaluation model is established, and the subgraph matching was carried out based on the similarity calculation of graph structure and semantic information. Finally, four knowledge graphs including entity equipment graph, ontology graph, maintenance rule graph and history graph were constructed by selecting some automobile engine fault cases from 2017 to 2020. Finally, the process architecture of engine fault search and analysis is constructed and the effectiveness of the proposed method was verified by precision rate and recall rata, which provides a new idea for accurate and efficient engine maintenance.},
  keywords={Knowledge engineering;Visualization;Semantic search;Decision making;Knowledge graphs;Maintenance engineering;Feature extraction;engine fault;knowledge graph;entity link;Subgraph matching;semantic search},
  doi={10.1109/EEBDA56825.2023.10090855},
  ISSN={},
  month={Feb},}@ARTICLE{10091124,
  author={Tu, Yamei and Wang, Xiaoqi and Qiu, Rui and Shen, Han-Wei and Miller, Michelle and Rao, Jinmeng and Gao, Song and Huber, Patrick R. and Hollander, Allan D. and Lange, Matthew and Garcia, Christian R. and Stubbs, Joe},
  journal={IEEE Computer Graphics and Applications}, 
  title={An Interactive Knowledge and Learning Environment in Smart Foodsheds}, 
  year={2023},
  volume={43},
  number={3},
  pages={36-47},
  abstract={The Internet of Food (IoF) is an emerging field in smart foodsheds, involving the creation of a knowledge graph (KG) about the environment, agriculture, food, diet, and health. However, the heterogeneity and size of the KG present challenges for downstream tasks, such as information retrieval and interactive exploration. To address those challenges, we propose an interactive knowledge and learning environment (IKLE) that integrates three programming and modeling languages to support multiple downstream tasks in the analysis pipeline. To make IKLE easier to use, we have developed algorithms to automate the generation of each language. In addition, we collaborated with domain experts to design and develop a dataflow visualization system, which embeds the automatic language generations into components and allows users to build their analysis pipeline by dragging and connecting components of interest. We have demonstrated the effectiveness of IKLE through three real-world case studies in smart foodsheds.},
  keywords={Knowledge graphs;Ontologies;Data visualization;Food products;Smart agriculture;Internet of Things;Data models},
  doi={10.1109/MCG.2023.3263960},
  ISSN={1558-1756},
  month={May},}@ARTICLE{10052691,
  author={Jha, Kanchan and Saha, Sriparna and Karmakar, Sourav},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
  title={Prediction of Protein-Protein Interactions Using Vision Transformer and Language Model}, 
  year={2023},
  volume={20},
  number={5},
  pages={3215-3225},
  abstract={The knowledge of protein-protein interaction (PPI) helps us to understand proteins’ functions, the causes and growth of several diseases, and can aid in designing new drugs. The majority of existing PPI research has relied mainly on sequence-based approaches. With the availability of multi-omics datasets (sequence, 3D structure) and advancements in deep learning techniques, it is feasible to develop a deep multi-modal framework that fuses the features learned from different sources of information to predict PPI. In this work, we propose a multi-modal approach utilizing protein sequence and 3D structure. To extract features from the 3D structure of proteins, we use a pre-trained vision transformer model that has been fine-tuned on the structural representation of proteins. The protein sequence is encoded into a feature vector using a pre-trained language model. The feature vectors extracted from the two modalities are fused and then fed to the neural network classifier to predict the protein interactions. To showcase the effectiveness of the proposed methodology, we conduct experiments on two popular PPI datasets, namely, the human dataset and the S. cerevisiae dataset. Our approach outperforms the existing methodologies to predict PPI, including multi-modal approaches. We also evaluate the contributions of each modality by designing uni-modal baselines. We perform experiments with three modalities as well, having gene ontology as the third modality.},
  keywords={Proteins;Feature extraction;Three-dimensional displays;Protein sequence;Amino acids;Deep learning;Transformers;Language model;protein-protein interaction;vision transformer},
  doi={10.1109/TCBB.2023.3248797},
  ISSN={1557-9964},
  month={Sep.},}@ARTICLE{10044673,
  author={Kim, Han Kyul and Park, Yujin and Park, Yeju and Choi, Eunji and Kim, Sodam and You, Hahyun and Bae, Ye Seul},
  journal={IEEE Access}, 
  title={Identifying Alcohol-Related Information From Unstructured Bilingual Clinical Notes With Multilingual Transformers}, 
  year={2023},
  volume={11},
  number={},
  pages={16066-16075},
  abstract={As a key modifiable risk factor, alcohol consumption is clinically crucial information that allows medical professionals to further understand their patients’ medical conditions and suggest appropriate lifestyle modifying interventions. However, identifying alcohol-related information from unstructured free-text clinical notes is often challenging. Not only are the formats of the notes inconsistent, but they also include a massive amount of non-alcohol-related information. Furthermore, for medical institutions outside of English-speaking countries, these clinical notes contain both a mixture of English and local languages, inducing additional difficulty in the extraction. Thanks to the increasing availability of electronic medical record (EMR), several previous works explored the idea of using natural language processing (NLP) to train machine learning models that automatically identify alcohol-related information from unstructured clinical notes. However, all these previous works are limited to English clinical notes, thereby able to leverage various large-scale external ontologies during the text preprocessing. Furthermore, they rely on simple NLP techniques such as the bag-of-words models that suffer from high dimensionality and out-of-vocabulary issues. Addressing these issues, we adopt fine-tuning multilingual transformers. By leveraging their linguistically rich contextual information learned during their pre-training, we are able to extract alcohol-related information from unstructured clinical notes without preprocessing the clinical notes on any external ontologies. Furthermore, our work is the first to explore the use of transformers in bilingual clinical notes to extract alcohol-related information. Even with minimal text preprocessing, we achieve extraction accuracy of 84.70% in terms of macro F-1 score.},
  keywords={Transformers;Data mining;Hospitals;Terminology;Symbols;Ontologies;Alcoholic beverages;Bioinformatics;Natural language processing;Informatics;Clinical diagnosis;Clinical informatics;alcohol information extraction;natural language processing;information extraction from clinical notes;multilingual transformers},
  doi={10.1109/ACCESS.2023.3245523},
  ISSN={2169-3536},
  month={},}@ARTICLE{10016758,
  author={Liu, Mingyi and Tu, Zhiying and Xu, Hanchuan and Xu, Xiaofei and Wang, Zhongjie},
  journal={IEEE Transactions on Services Computing}, 
  title={DySR: A Dynamic Graph Neural Network Based Service Bundle Recommendation Model for Mashup Creation}, 
  year={2023},
  volume={16},
  number={4},
  pages={2592-2605},
  abstract={An increasing number and diversity of services are available, which results in significant challenges to effectively reuse service during mashup creation. Many works have modeled the mashup creation problem as a service recommendation task and have achieved remarkable results. However, the performance of these methods can be further improved. The main problems affecting these methods include the constraints among recommended services, the evolution of services, and the semantic gap existing in services and mashups. In this article, we model the mashup creation problem as a service bundle recommendation task that is formally defined to address the constraints among recommended services. And then, a dynamic graph neural network based model called DySR is proposed to tackle the evolution of service and the semantic gap between services and mashups. In order to quantitatively measure how significant the semantic gap between mashups and services is, a measurement method of a semantic gap is given. With it, experiments show that to what extent DySR can reduce the semantic gap in the context of mashup creation. In addition, new evaluation metrics are introduced to overcome the preference for popular services in traditional service recommendations. Extensive experiments conducted on a real-world dataset from ProgrammableWeb, and the experiment results show that DySR outperforms existing state-of-the-art methods.},
  keywords={Mashups;Semantics;Task analysis;Graph neural networks;Computational modeling;Quality of service;Ontologies;Dynamic graph neural networks;evolving service;mashup creation;semantic gap;service bundle recommendation},
  doi={10.1109/TSC.2023.3234293},
  ISSN={1939-1374},
  month={July},}@ARTICLE{10013735,
  author={Benarab, Achref and Sun, Jianguo and Rafique, Fahad and Refoufi, Allaoua},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Global Ontology Entities Embeddings}, 
  year={2023},
  volume={35},
  number={11},
  pages={11449-11460},
  abstract={Ontologies are among the most widely used types of knowledge representation formalisms. The application of deep learning techniques in the field of ontology engineering has reinforced the need to learn and generate representations of ontological data. This allows ontologies to be exploited by such models, and thus automate various ontology engineering tasks, where most of the existing tools and machine learning approaches require a numerical feature vectors associated with each concept. This paper outlines a novel approach for learning global ontology entities embeddings by exploiting the structure and the various taxonomic and semantic relationships present in ontologies, taking into account all the information present in the ontological graph and carried by the OWL/RDF triples. Thus, producing global ontology entities embeddings capturing the global ontological graph semantics and similarities enclosed in the source ontology. Three different neural network models have been proposed based on two architectures: multi-input and multi-output, trained using the contrastive estimation technique. The evaluation on OWL/RDF ontologies and word semantic similarity tasks using various graph and WordNet based similarity measures, show that our approach yields competitive results outperforming the state-of-the-art ontology and word embedding models.},
  keywords={Ontologies;Semantics;Task analysis;Adaptation models;Predictive models;Neural networks;Deep learning;Concept embeddings;feature representation;neural networks;ontology embeddings;ontology entities vector representations},
  doi={10.1109/TKDE.2023.3235779},
  ISSN={1558-2191},
  month={Nov},}@ARTICLE{9961919,
  author={Sun, Guangzhi and Zhang, Chao and Woodland, Philip C.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Minimising Biasing Word Errors for Contextual ASR With the Tree-Constrained Pointer Generator}, 
  year={2023},
  volume={31},
  number={},
  pages={345-354},
  abstract={Contextual knowledge is essential for reducing speech recognition errors on high-valued long-tail words. This paper proposes a novel tree-constrained pointer generator (TCPGen) component that enables end-to-end ASR models to bias towards a list of long-tail words obtained using external contextual information. With only a small overhead in memory use and computation cost, TCPGen can structure thousands of biasing words efficiently into a symbolic prefix-tree, and creates a neural shortcut between the tree and the final ASR output to facilitate the recognition of the biasing words. To enhance TCPGen, we further propose a novel minimum biasing word error (MBWE) loss that directly optimises biasing word errors during training, along with a biasing-word-driven language model discounting (BLMD) method during the test. All contextual ASR systems were evaluated on the public Librispeech audiobook corpus and the data from the dialogue state tracking challenges (DSTC) with the biasing lists extracted from the dialogue-system ontology. Consistent word error rate (WER) reductions were achieved with TCPGen, which were particularly significant on the biasing words with around 40% relative reductions in the recognition error rates. MBWE and BLMD further improved the effectiveness of TCPGen, and achieved more significant WER reductions on the biasing words. TCPGen also achieved zero-shot learning of words not in the audio training set with large WER reductions on the out-of-vocabulary words in the biasing list.},
  keywords={Training;Generators;Decoding;Hidden Markov models;Context modeling;Speech recognition;Error analysis;Contextual speech recognition;end-to-end;language model discounting;minimum Bayes' risk;Pointer generator},
  doi={10.1109/TASLP.2022.3224286},
  ISSN={2329-9304},
  month={},}@ARTICLE{9950327,
  author={Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Improving Protein Function Prediction by Adaptively Fusing Information From Protein Sequences and Biomedical Literature}, 
  year={2023},
  volume={27},
  number={2},
  pages={1140-1148},
  abstract={Proteins are the main undertakers of life activities, and accurately predicting their biological functions can help human better understand life mechanism and promote the development of themselves. With the rapid development of high-throughput technologies, an abundance of proteins are discovered. However, the gap between proteins and function annotations is still huge. To accelerate the process of protein function prediction, some computational methods taking advantage of multiple data have been proposed. Among these methods, the deep-learning-based methods are currently the most popular for their capability of learning information automatically from raw data. However, due to the diversity and scale difference between data, it is challenging for existing deep learning methods to capture related information from different data effectively. In this paper, we introduce a deep learning method that can adaptively learn information from protein sequences and biomedical literature, namely DeepAF. DeepAF first extracts the two kinds of information by using different extractors, which are built based on pre-trained language models and can capture rudimentary biological knowledge. Then, to integrate those information, it performs an adaptive fusion layer based on a Cross-attention mechanism that considers the knowledge of mutual interactions between two information. Finally, based on the mixed information, DeepAF utilizes logistic regression to obtain prediction scores. The experimental results on the datasets of two species (i.e., Human and Yeast) show that DeepAF outperforms other state-of-the-art approaches.},
  keywords={Proteins;Protein engineering;Data mining;Biological system modeling;Amino acids;Semantics;Predictive models;Protein function prediction;deep learning;multiple data;pre-trained language models;cross-attention mechanism},
  doi={10.1109/JBHI.2022.3221988},
  ISSN={2168-2208},
  month={Feb},}@ARTICLE{9865204,
  author={Sun, Zequn and Hu, Wei and Wang, Chengming and Wang, Yuxin and Qu, Yuzhong},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Revisiting Embedding-Based Entity Alignment: A Robust and Adaptive Method}, 
  year={2023},
  volume={35},
  number={8},
  pages={8461-8475},
  abstract={Entity alignment—the discovery of identical entities across different knowledge graphs (KGs)—is a critical task in data fusion. In this paper, we revisit existing entity alignment methods in practical and challenging scenarios. Our empirical studies show that current work has a low level of robustness to long-tail entities and the lack of entity names or relation triples. We aim to develop a robust and adaptive entity alignment method, and the availability of relations, attributes, or names is not required. Our method consists of an attribute encoder and a relation encoder, representing an entity by aggregating its attributes or relational neighbors using the attention mechanisms that can highlight the useful attributes and relations in end-to-end learning. To let the encoders complement each other and produce a coherent representation space, we propose adaptive embedding fusion via a gating mechanism. We consider four evaluation settings, i.e., the conventional setting with both relation and attribute triples, as well as three challenging settings without attributes, without relations, without both relations and names, respectively. Results show that our method can achieve state-of-the-art performance. Even in the most challenging setting without relations and names, our method can still achieve promising results while existing methods fail.},
  keywords={Robustness;Task analysis;Logic gates;Sun;Ontologies;Manuals;Convolution;Knowledge graph embedding;entity alignment;adaptive embedding fusion},
  doi={10.1109/TKDE.2022.3200981},
  ISSN={1558-2191},
  month={Aug},}@ARTICLE{9792280,
  author={Guan, Saiping and Cheng, Xueqi and Bai, Long and Zhang, Fujun and Li, Zixuan and Zeng, Yutao and Jin, Xiaolong and Guo, Jiafeng},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={What is Event Knowledge Graph: A Survey}, 
  year={2023},
  volume={35},
  number={7},
  pages={7569-7589},
  abstract={Besides entity-centric knowledge, usually organized as Knowledge Graph (KG), events are also an essential kind of knowledge in the world, which trigger the spring up of event-centric knowledge representation form like Event KG (EKG). It plays an increasingly important role in many downstream applications, such as search, question-answering, recommendation, financial quantitative investments, and text generation. This paper provides a comprehensive survey of EKG from history, ontology, instance, and application views. Specifically, to characterize EKG thoroughly, we focus on its history, definition, schema induction, acquisition, related representative graphs/systems, and applications. The development processes and trends are studied therein. We further summarize prospective directions to facilitate future research on EKG.},
  keywords={Electrocardiography;History;Ontologies;Task analysis;Semantics;Internet;Standards;Event knowledge graph;schema;event acquisition;script event prediction;temporal knowledge graph prediction},
  doi={10.1109/TKDE.2022.3180362},
  ISSN={1558-2191},
  month={July},}@INBOOK{10787484,
  author={Renes, Joseph},
  booktitle={Quantum Information Theory: Concepts and Methods}, 
  title={2 Probability theory}, 
  year={2022},
  volume={},
  number={},
  pages={17-35},
  abstract={},
  keywords={Ontologies;Context modeling;OWL;Object oriented modeling;Context-aware services;Semantics;Resource description framework;Logic;Computational modeling;Cognition},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9783110570328},
  url={https://ieeexplore.ieee.org/document/10787484},}@INBOOK{10514903,
  author={Weilkiens, Tim and Lamm, Jesko G. and Roth, Stephan and Walker, Markus},
  booktitle={Model-Based System Architecture}, 
  title={Model&#x2010;Based Requirements Engineering and Use Case Analysis}, 
  year={2022},
  volume={},
  number={},
  pages={99-118},
  abstract={Summary <p>In this chapter, the authors give a brief description of requirements engineering and use case analysis. They define the most important requirements and use case terms. The authors present the requirements and use case analysis methodology steps of the &#x201c;Systems Modeling Toolbox&#x201d; (SYSMOD). The SYSMOD methodology defines common methods and is not specific to any modeling tool. The authors cover the Storyboard Activity Modeling for Systems (SAMS) method that provides an illustrative approach to identify use case activities with stakeholders. The SAMS method is intended to integrate the use of storyboards as a means of use case analysis into model&#x2010;based system development. The authors also present the Use Case 2.0 approach that works well with agile frameworks. The Use Case 2.0 concept introduces the use case slice that represents only one or some ways of the possible use case performances that can be handled as a unit from specification to implementation within an agile approach.</p>},
  keywords={Adaptation models;Stakeholders;Computer architecture;Systems architecture;Ontologies;Business;Analytical models},
  doi={10.1002/9781119746683.ch10},
  ISSN={},
  publisher={Wiley},
  isbn={9781119746669},
  url={https://ieeexplore.ieee.org/document/10514903},}@INPROCEEDINGS{10148526,
  author={Zhang, Xiang and Yu, Bruce X.B. and Liu, Yan and Chen, Gong and Ng, George Wing-Yiu and Chia, Nam-Hung and So, Eric Hang-Kwong and So, Sze-Sze and Cheung, Victor Kai-Lam},
  booktitle={2022 IEEE International Conference on Teaching, Assessment and Learning for Engineering (TALE)}, 
  title={Conversational System for Clinical Communication Training Supporting User-defined Tasks}, 
  year={2022},
  volume={},
  number={},
  pages={396-403},
  abstract={Effective clinical communication is essential for delivering safe and high-quality patient care, especially in emergent cases. Standard communication protocols have been developed to improve communication accuracy and efficiency. However, traditional training and evaluation require substantial manpower and time, which can be infeasible during public crises when training is most needed. This research aims to facilitate autonomous, low-cost, adaptive clinical communication training via artificial intelligence (AI)-powered techniques. We propose a conversational system for clinical communication training supporting user-defined tasks. Two data augmentation (DA) methods, term replacement and context expansion, are proposed to allow non-professional users to create Al models with a small number of samples. Equipped with biomedical ontology and pre-trained language models, our system is able to simulate clinical communication scenarios, provide timely evaluation, and adapt to new tasks with minimal editing. Various experiments demonstrate that our proposed algorithms can achieve satisfactory performance using a small amount of training data. Real-world practice in local hospitals shows that our system can provide expert-level evaluation and deliver effective clinical communication training.},
  keywords={Training;Adaptation models;Protocols;Biological system modeling;Training data;Ontologies;Data models;clinical communication;human-computer interaction;autonomous communication training;conversational system},
  doi={10.1109/TALE54877.2022.00071},
  ISSN={2470-6698},
  month={Dec},}@INPROCEEDINGS{10074709,
  author={Chen, Luming and Qi, Yifan and Wu, Aiping and Deng, Lizong and Jiang, Taijiao},
  booktitle={2022 IEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)}, 
  title={Enhancing Cross-lingual Medical Concept Alignment by Leveraging Synonyms and Translations of the Unified Medical Language System}, 
  year={2022},
  volume={},
  number={},
  pages={2078-2083},
  abstract={Well-developed medical terminology systems like the Unified Medical Language System (UMLS) improve the ability of language models to handle medical entity linking tasks. However, such magnificent terminology systems are only available for few languages, such as English. For Chinese, both simplified and traditional, the lack of well-developed terminology systems remains a big challenge to unify Chinese medical terminologies by linking medical entities as concepts. In this study, we purpose a translation enhanced contrastive learning scheme which leverages translations and synonyms of UMLS to infuse knowledge into the language model, and present a cross-lingual pre-trained language model called TeaBERT that aligns cross-lingual Chinese and English medical synonyms well at semantic level. Comparing with former cross-lingual language models, TeaBERT significantly outperforms on evaluation datasets, with 93.21%, 89.89% and 76.45% of Top 5 accuracy on ICDI0-CN, CHPO and RealWorld dataset respectively, and achieves new state-of-theart performance without task specific fine-tuning. Our contrastive learning scheme can not only be used for enhancing Chinese-English medical concepts alignment, but also be applied to other languages facing the same challenges.},
  keywords={Terminology;Biological system modeling;Unified modeling language;Semantics;Knowledge representation;Task analysis;NLP;pre-trained language model;cross-lingual medical entity linking;UMLS},
  doi={10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00309},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10076731,
  author={Taru, Uma and Patil, Archana},
  booktitle={2022 International Conference on Machine Learning, Computer Systems and Security (MLCSS)}, 
  title={Building Ontology for Toxic words}, 
  year={2022},
  volume={},
  number={},
  pages={241-246},
  abstract={Many online social media platforms have particular community guidelines for comment sections. The platforms that maintain commentary sections in various posts, videos, and blogs need to adhere to these guidelines. These comment sections may have specific comments that fail to satisfy the rules and regulations to maintain societal norms of communication. These comments are classified as toxic comments. Google's Perspective API defines toxic comments as comments that are rude, offensive, and likely to make someone leave the conversation. In this paper, we have built a toxic words ontology, which is as per our knowledge, first Ontology built on toxic words. This Ontology consists of toxic words and their antonyms and synonyms in increasing order of their toxicity levels. Traversing this ontology, we can find the best-suited word with less toxicity and similar meaning. This is a dynamic ontology and new words can be added easily. Thus letting us convey messages in a civil manner. We propose to reduce toxicity in the most straightforward way. After studying several papers, we found out that the toxicity mainly occurs because of use of toxic words. We also observed that use of less toxic synonyms or no toxic synonyms has huge effects on toxicity score given by the Perspective API, and results section proves that.},
  keywords={Toxicology;Social networking (online);Oral communication;Machine learning;Ontologies;Regulation;Internet;toxicity;ontology;similarity;antonyms;synonyms},
  doi={10.1109/MLCSS57186.2022.00052},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10043275,
  author={Mendil, Ismail and Rivière, Peter and Ait-Ameur, Yamine and Singh, Neeraj Kumar and Méry, Dominique and Palanque, Philippe},
  booktitle={2022 29th Asia-Pacific Software Engineering Conference (APSEC)}, 
  title={Non-Intrusive Annotation-Based Domain-Specific Analysis to Certify Event-B Models Behaviours}, 
  year={2022},
  volume={},
  number={},
  pages={129-138},
  abstract={System engineering advocates a thorough under-standing of the engineering domain or certification standards (aeronautics, railway, medical, etc.) associated to the system under design. In this context, engineering domain knowledge plays a predominant role in system design and/or certification. Furthermore, it is a prerequisite to achieve the effectiveness and performance of the designed system. This article proposes a formal method for describing and setting up domain-specific behavioural analyses. It defines a formal verification technique for dynamic properties entailed by engineering domain knowledge where Event-B formal models are annotated and analysed in a non-intrusive way, i.e. without destructive alteration. This method is based on the formalisation of behavioural properties analyses relying on domain knowledge as an ontology on the one hand and a meta-theory for Event-B on the other hand. The proposed method is illustrated using a critical interactive system.},
  keywords={Knowledge engineering;Analytical models;Interactive systems;Ontologies;Rail transportation;Proposals;Certification;Domain knowledge;formal methods;Event-B;refinement;proof;ontology;behavioural analyses},
  doi={10.1109/APSEC57359.2022.00025},
  ISSN={2640-0715},
  month={Dec},}@INPROCEEDINGS{10020417,
  author={Mijalcheva, Viktorija and Davcheva, Ana and Gramatikov, Sasho and Jovanovik, Milos and Trajanov, Dimitar and Stojanov, Riste},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Learning Robust Food Ontology Alignment}, 
  year={2022},
  volume={},
  number={},
  pages={4097-4104},
  abstract={In today’s knowledge society, large number of information systems use many different individual schemes to represent data. Ontologies are a promising approach for formal knowledge representation and their number is growing rapidly. The semantic linking of these ontologies is a necessary prerequisite for establishing interoperability between the large number of services that structure the data with these ontologies. Consequently, the alignment of ontologies becomes a central issue when building a worldwide Semantic Web. There is a need to develop automatic or at least semi-automatic techniques to reduce the burden of manually creating and maintaining alignments. Ontologies are seen as a solution to data heterogeneity on the Web. However, the available ontologies are themselves a source of heterogeneity. On the Web, there are multiple ontologies that refer to the same domain, and with that comes the challenge of a given graph-based system using multiple ontologies whose taxonomy is different, but the semantics are the same. This can be overcome by aligning the ontologies or by finding the correspondence between their components.In this paper, we propose a method for indexing ontologies as a support to a solution for ontology alignment based on a neural network. In this process, for each semantic resource we combine the graph based representations from the RDF2vec model, together with the text representation from the BERT model in order to capture the semantic and structural features. This methodology is evaluated using the FoodOn and OntoFood ontologies, based on the Food Onto Map alignment dataset, which contains 155 unique and validly aligned resources. Using these limited resources, we managed to obtain accuracy of 74% and F1 score of 75% on the test set, which is a promising result that can be further improved in future. Furthermore, the methodology presented in this paper is both robust and ontology-agnostic. It can be applied to any ontology, regardless of the domain.},
  keywords={Training;Semantic Web;Semantics;Neural networks;Taxonomy;Ontologies;Big Data;Ontology Alignment;Natural language processing;Text representation;Embeddings;Data normalization;Data linking},
  doi={10.1109/BigData55660.2022.10020417},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10020882,
  author={Hsiao, Yi-Hao and Chuang, Chia-Yi and Huang, Megn-Chi and Yang, Chia-Lee and Wu, Jyh-Horng},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Using Contextual Text Mining and Ontology Methods to Establish a Novel Technology Trend and Associative Analysis Framework for Sustainable Energy Development in Taiwan}, 
  year={2022},
  volume={},
  number={},
  pages={4491-4494},
  abstract={In 2015, the United Nations proposed 17 Sustainable Development Goals, SDGs, as the guidelines for all countries in the world to promote sustainable development before 2030. Government Research Bulletin (GRB), the research projects and technical reports sponsored by government, which has long-term, numerous, complete research method, technology development and policy analysis information in Taiwan. Therefore, it is an important and effective way to explore SDG-related information from a large amount of GRB text. In this paper, a novel technologies trend and associative analysis framework which uses contextual text mining and ontology methods is proposed and applied to SDG 7, which "Affordable and Clean Energy". First, we integrate dictionary-based method and semantic textual similarity analysis algorithm to obtain a SDG 7 classifier which can exactly and quickly classify a large amount number of GRB text to SDG 7. Then, two major SDG 7 analysis procedures based on the classification results are implemented. One is using contextual text mining algorithm to obtain energy technologies trend information. The other is adopting ontology method to establish energy technologies associative analysis concept map. According to the analysis results mentioned above, we are able to efficiently incorporate the energy technology with long-term trend, energy technology associative information, and the most influential authors on the specify energy technology in order to generate a global strategy for continuous improvement in Taiwan.},
  keywords={Text mining;Government;Semantics;Ontologies;Big Data;Writing;Market research;Contextual text mining;Ontology;SBERTs;Sustainable Development Goals (SDGs)},
  doi={10.1109/BigData55660.2022.10020882},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10020339,
  author={Wullschleger, Pascal and Lionetti, Simone and Daly, Donnacha and Volpe, Francesca and Caro, Grégoire},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Auto-Regressive Self-Attention Models for Diagnosis Prediction on Electronic Health Records}, 
  year={2022},
  volume={},
  number={},
  pages={1950-1956},
  abstract={Insufficient data and data lacking the diversity to represent the general public is a common challenge when modelling diagnosis prediction. We consider a much larger and more diverse database of commercial Electronic Health Records than what is prevalent in the literature. We formulate a simplified version of diagnosis prediction that focuses on major developments in medical histories of patients. To this end, we leverage Auto-Regressive Self-Attention models that have seen promising applications in language modelling and extend them to incorporate ontological representations of medical codes. Additionally, we include time-intervals between diagnoses into the attention calculation. We evaluate models and baselines at different levels of diagnostic granularity and our results suggest that using very detailed clinical classifications does not significantly degrade performance, possibly allowing their use in practice. Our model outperforms all baselines and we suggest that leveraging the ontology for generating diagnosis representations is mostly helpful for rare diagnoses.},
  keywords={Codes;Databases;Predictive models;Big Data;Ontologies;Data models;History;Electronic Health Record;Transformer;Ontological Representation;Diagnosis Prediction},
  doi={10.1109/BigData55660.2022.10020339},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10020513,
  author={Chen, Xianlai and Lin, Jiamiao and An, Ying},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={DL-BERT: a time-aware double-level BERT-style model with pre-training for disease prediction}, 
  year={2022},
  volume={},
  number={},
  pages={1801-1808},
  abstract={Disease prediction based on the Electronic Health Record (EHR) is an important task in healthcare. EHR records patients’ every visit by time, and there are many kinds of medical codes within a visit, therefore, EHR has characteristics of temporal irregularity and hierarchical structure. Some recent works employ BERT-style models to process EHR data for disease prediction. However, few of these models can give consideration to capture both the interaction between medical codes and the impact of temporal irregularity. To solve this problem, we propose the Double-Level BERT-style model (DL-BERT). Considering EHR’s hierarchical structure, the model contains a code-level and a visit-level representation layer which can learn the relationship between medical codes and temporal influence respectively. In the code-level representation layer, the model achieves the representation power by employing external medical ontologies to provide multi-resolution information of medical codes and the Transformer to embed medical codes. Besides, the model adopts two pre-training tasks to enhance the ability to capture the link between different kinds of codes. In the visit-level representation layer, DL-BERT utilizes a special time-aware Transformer to model temporal information. And the model adopts a visit-level pre-training task for better learning context information. Experiments are conducted on two real-world healthcare datasets and show that our model outperforms all baselines demonstrating the effectiveness of our model.},
  keywords={Codes;Medical services;Ontologies;Predictive models;Big Data;Transformers;Data models;Electronic Health Record;BERT;Transformer;medical ontology;pre-train},
  doi={10.1109/BigData55660.2022.10020513},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10020509,
  author={Parolin, Erick Skorupa and Hu, Yibo and Khan, Latifur and Brandt, Patrick T. and Osorio, Javier and D’Orazio, Vito},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Confli-T5: An AutoPrompt Pipeline for Conflict Related Text Augmentation}, 
  year={2022},
  volume={},
  number={},
  pages={1906-1913},
  abstract={Recent advances in natural language processing (NLP) and Big Data technologies have been crucial for scientists to analyze political unrest and violence, prevent harm, and promote global conflict management. Government agencies and public security organizations have invested heavily in deep learning-based applications to study global conflicts and political violence. However, such applications involving text classification, information extraction, and other NLP-related tasks require extensive human efforts in annotating/labeling texts. While limited labeled data may drastically hurt the models’ performance (over-fitting), large demands on annotation tasks may turn real-world applications impracticable. To address this problem, we propose Confli-T5, a prompt-based method that leverages the domain knowledge from existing political science ontology to generate synthetic but realistic labeled text samples in the conflict and mediation domain. Our model allows generating textual data from the ground up and employs our novel Double Random Sampling mechanism to improve the quality (coherency and consistency) of the generated samples. We conduct experiments over six standard datasets relevant to political science studies to show the superiority of Confli-T5. Our codes are publicly available 1.},
  keywords={Text categorization;Standards organizations;Pipelines;Big Data;Natural language processing;Data models;Safety;text augmentation;generation;classification;natural language processing;conflict;coding event data;CAMEO},
  doi={10.1109/BigData55660.2022.10020509},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10013619,
  author={Kulagin, Grigory and Ermakov, Ivan and Lyadova, Lyudmila},
  booktitle={2022 IEEE 16th International Conference on Application of Information and Communication Technologies (AICT)}, 
  title={Ontology-Based Development of Domain-Specific Languages via Customizing Base Language}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={The quality of the systems depends on compliance to the domain requirements. High quality is achieved only with involving experts in the relevant fields to the system design as experts. Modern design methods are based on using professional tools and modeling languages. Using these tools are difficult for domain experts. Domain-Specific Languages (DSLs) can be considered as "user interfaces" for experts because they bridge the gap between the domain experts and the software development tools via customizing modeling languages. Usability of DSLs by domain experts is a key factor for their successful adoption. But DSL creation is challenging task. An approach to DSL customization based on using multifaceted ontology is proposed. General scheme of DSL metamodel generation based on multifaceted ontology is described. Examples of created DSLs and models illustrating the applicability of the proposed method are shown. The DSL metamodels were developed and tested in several domains. The results of experiments confirmed practical significance of the ontology-based approach to DSL creation.},
  keywords={Visualization;Prototypes;Ontologies;User interfaces;Software;DSL;Task analysis;domain-specific modeling;DSM;domain-specific language;DSL;metamodel generation;multifaceted ontology;language customization;GalileoSky;algorithm description language},
  doi={10.1109/AICT55583.2022.10013619},
  ISSN={2472-8586},
  month={Oct},}@INPROCEEDINGS{10005324,
  author={Zindel, Andreas and Feo-Arenis, Sergio and Helle, Philipp and Schramm, Gerrit and Elaasar, Maged},
  booktitle={2022 IEEE International Symposium on Systems Engineering (ISSE)}, 
  title={Building a Semantic Layer for Early Design Trade Studies in the Development of Commercial Aircraft}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={To improve the adoption of Model-based Systems Engineering (MBSE), data that is distributed across engineering disciplines needs to be made available in an open and descriptive way. This paper describes a new approach to implementing a semantic layer that allows integrating and publishing MBSE data stored in heterogeneous models in a uniform way by means of Semantic Web technologies. The tool-independent views on engineering data provided by the semantic layer enable the implementation of services for accessing, classifying, checking and reuse of federated information. We report on the creation of a common vocabulary in the Ontology Modeling Language (OML) that can be automatically instantiated from distributed models into a knowledge graph. We demonstrate the benefits of our approach using a Systems Modeling Language (SysML) based early design trade study in the aeronautics domain.},
  keywords={Semantic Web;Vocabulary;Atmospheric modeling;Unified modeling language;Semantics;Ontologies;Data models},
  doi={10.1109/ISSE54508.2022.10005324},
  ISSN={2687-8828},
  month={Oct},}@INPROCEEDINGS{9994917,
  author={Choi, Kyudam and Lee, Yurim and Kim, Cheongwon},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={GCL-GO: A novel sequence-based hierarchy-aware method for protein function prediction}, 
  year={2022},
  volume={},
  number={},
  pages={51-56},
  abstract={Experimental protein functional annotation does not cover rapidly-expanding protein sequences. Sequence-based methods, one of the computational methods, have been developed for extending functional annotations to fast-growing sequence databases. We propose a novel sequence-based hierarchy-aware method, namely GCL-GO. GCL-GO applies a protein language model to represent sequences, applies graph contrastive learning to represent GO terms, and then predicts protein functions by combining these two features. By contrasting the GO graph and semantic features of GO terms, GCL-GO has generalizability and scalability by accurately embedding the features of GO terms while relying less on training data. We also suggest GCL-GO+, which combines a sequence similarity-based method with GCLGO, to improve performance. GCL-GO+ outperforms sequence-based competing methods on both the CAFA3 and the TALE datasets. Furthermore, GCL-GO and GCL-GO+ demonstrate functional generalization and scalability potential by having the best performance on new GO terms or on GO terms annotated infrequently in the training dataset. Our code is available in https://github.com/kch38896/GCL-GO},
  keywords={Proteins;Training;Protein engineering;Annotations;Databases;Scalability;Semantics;protein function prediction;gene ontology;graph constructive learning;protein language model},
  doi={10.1109/BIBM55620.2022.9994917},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9995651,
  author={Wang, Hong and Wang, Xiaoqi and Liu, Wenjuan and Xie, Xiaolan and Peng, Shaoliang},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={deepDGA: Biomedical Heterogeneous Network-based Deep Learning Framework for Disease-Gene Association Predictions}, 
  year={2022},
  volume={},
  number={},
  pages={601-606},
  abstract={Accurate prediction of disease-gene associations is a crucial tissue in the treatment of diseases. Currently, deep learning-based methods have been proposed to determine the associations between diseases and genes. However, previous network-based models do not consider the semantic characteristics of various biomedical entities and suffer from the problems of cold-start. To this end, this study proposes a heterogeneous network-based deep learning framework (termed deepDGA) to predict disease-gene associations. First, a heterogeneous network with four kinds of biological nodes and eight kinds of edges is constructed. Second, we develop a meta path-driven deep Transformer encoder to learn node representations which contains semantic characteristics of nodes in the heterogeneous network. Finally, the inductive matrix completion algorithm that can solve problem of cold-start, is used for disease-gene association prediction. The results of 5-flod cross-validation and top-ranked predictions suggest that deepDGA is superior to other methods. In addition, we further observe that deepDGA performs the highest predictive ability for specific diseases via the literature verification, KEGG human pathway analyses, and GO enrichment analyses. In summary, deepDGA is an effective framework for predicting the diseases-gene associations.},
  keywords={Deep learning;Biological system modeling;Semantics;Pipelines;Predictive models;Transformers;Prediction algorithms;disease-gene association;heterogeneous network;deep Transformer encoder;inductive matrix completion},
  doi={10.1109/BIBM55620.2022.9995651},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9995517,
  author={Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Adaptive Multi-view Graph Convolutional Network for Gene Ontology Annotations of Proteins}, 
  year={2022},
  volume={},
  number={},
  pages={90-93},
  abstract={Gene Ontology (GO) containing a set of standard concepts (or terms) is launched to unify the functional descriptions of proteins. Developing computational models based on GO to automatically annotate protein functions has been a longstanding active research area. In this paper, we propose a novel method to adaptively fuse functional and topological information between GO Terms. Our method is composed of a pre-trained language model for encoding protein sequences and an adaptive multi-view graph convolutional network (Multi-view GCN) for representing GO terms. Particularly, the Multi-view GCN considers multiple views from functional information, topological structures, and their combinations, and extracts multiple corresponding representations of GO terms. Then, an attention mechanism is applied to adaptively learn the importance weights of these representations. Finally, the predicted scores are calculated by using a dot product between protein sequence features and GO term representations. Experimental results on the datasets of two species (i.e., Human and Yeast) show that our method outperforms other state-of-the-art methods. The code of our proposed method is available at: https://github.com/Candyperfect/Master.},
  keywords={Convolutional codes;Adaptation models;Adaptive systems;Computational modeling;RNA;Ontologies;Feature extraction;gene ontology terms;protein function prediction;deep learning;adaptive multi-view graph convolutional network},
  doi={10.1109/BIBM55620.2022.9995517},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9994899,
  author={Huang, Zhijian and Zheng, Rongtao and Deng, Lei},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={DeepFusionGO: Protein function prediction by fusing heterogeneous features through deep learning}, 
  year={2022},
  volume={},
  number={},
  pages={12-17},
  abstract={Exploring the functions of proteins is crucial for explaining cellular mechanisms, treating diseases, and developing new drugs. Due to experimental limitations, large-scale identification of protein function remains a challenging task in cell biology. Here we propose DeepFusionGo, a novel protein function prediction method that adopts a graph representation learning approach (GraphSAGE) to extract features from heterogeneous data sources. First, we generate embeddings from protein sequences using the pre-trained protein language model and InterPro domains with scaling gradient. Then we integrate these two embeddings with adaptive feature weights to the PPI graph and use GraphSAGE to generate the representation vector. Finally, we build the classification model to predict protein function based on the concatenated feature vector. The experimental results show that DeepFusionGO outperforms existing state-of-the-art methods, including sequence-based DeepGOPLUS, and PPI-based DeepGraphGO. DeepFusionGO also performs well in difficult protein function prediction. We demonstrate that selecting an appropriate protein features fusion method can improve the prediction performance, and using the PPI network and the protein representation vector obtained from the protein language model through the GraphSAGE algorithm is an effective way to mine potential functional clues. The source code and data sets are available at: https://github.com/Hhhzj-7/DeepFusionGO.},
  keywords={Proteins;Representation learning;Adaptation models;Biological system modeling;Source coding;Soft sensors;Predictive models;Protein function prediction;graph representation learning;GraphSAGE;feature fusion},
  doi={10.1109/BIBM55620.2022.9994899},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9999076,
  author={Japa, Sai Sharath and Green, Sarah},
  booktitle={2022 IEEE Eighth International Conference on Multimedia Big Data (BigMM)}, 
  title={Question Answering over Knowledge Base with Variational Auto-Encoder}, 
  year={2022},
  volume={},
  number={},
  pages={29-36},
  abstract={Knowledge Base is a semantic data repos-itory made available over the web. Knowledge bases are multi-relational graphs consisting of entities and relations representing facts about the world. These facts follow a controlled vocabulary that drives the knowledge base, often called ontology. Entities are the graph nodes, while relations are the edges connecting the nodes. While knowledge base attention prolifer-ates, extracting information from these resources is challenging. One of the promising approaches is Question Answering systems. The Question Answering over Knowledge Base(KB-QA) system aims to provide a natural language interface to pose a query to the KB. QA systems use embeddings to map the posed Question to the facts stored in the knowledge bases. QA systems can be effectively improved using large transformer models. These transformer models are computationally expensive, which prevents their use in real-world applications. To solve this, we started our study with the use-case of Knowledge Base Question Answering systems. In KB-QA systems, representing the entities and relationships play a pivotal role in extracting the answer to the posed query. Traditional KB embedding techniques represent entities/relations as vectors, mapping them in different semantic spaces and ignoring the subtle cor-relation. A pre-training language model for encoding the KB facts will preserve the semantic and contextual correlation. While learning linguistic knowledge, these models also store relational knowledge in the training data. These Language Models are often trained on a massive corpus addressing the Out-Of- Vocabulary problem. In this paper, we incorporate a co-embedding model for knowledge base embedding, which learns low-dimensional representations of entities and relations in the same semantic space. We propose a Variational Auto-Encoder, an efficient transformer architecture that represents knowledge base representations as Gaussian distributions to address the issue of neglecting uncertainty. In addition, our method has high quality and interpretability advantages compared with previous methods. Our experimental results show the effectiveness and the superiority of the VAE approach for question-answering systems on knowledge bases over other well-known pre-trained embedding methods.},
  keywords={Vocabulary;Uncertainty;Computational modeling;Knowledge based systems;Semantics;Training data;Transformers;knowledge base question answering;Bert;Language Model;KBQA;Multi-Head Attention;VAE;Encoder;Transformers},
  doi={10.1109/BigMM55396.2022.00012},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9987258,
  author={Chen, Jun},
  booktitle={2022 Sixth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)}, 
  title={Selection Method of Fuzzy Semantics in Machine Translation and the Integration of LBP Algorithm}, 
  year={2022},
  volume={},
  number={},
  pages={963-966},
  abstract={This paper studies the accuracy and rationality of machine English translation based on the LBP algorithm, and proposes a machine English translation method based on the selection of the optimal solution of fuzzy semantics. Construct an information extraction model for machine English translation, establish a fuzzy semantic topic word attribute table for machine English translation, and use phrases as the basic granularity to produce paraphrase results that are semantically consistent with the translation hypothesis set. Extract phrase paraphrase resources by using massively parallel corpus. Experimental test results show that using this method for machine English translation improves the recall performance of semantic information by 6.7%, and the feature matching degree of topic words is higher.},
  keywords={Analytical models;Semantics;Coherence;Ontologies;Information retrieval;Feature extraction;Machine translation;Fuzzy Semantics;Machine Translation;LBP Algorithm;Selection Method},
  doi={10.1109/I-SMAC55078.2022.9987258},
  ISSN={2768-0673},
  month={Nov},}@INPROCEEDINGS{9980157,
  author={Vasantharajan, Charangan and Tun, Kyaw Zin and Thi-Nga, Ho and Jain, Sparsh and Rong, Tong and Siong, Chng Eng},
  booktitle={2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)}, 
  title={MedBERT: A Pre-trained Language Model for Biomedical Named Entity Recognition}, 
  year={2022},
  volume={},
  number={},
  pages={1482-1488},
  abstract={This paper introduces MedBERT, a new pre-trained transformer-based model for biomedical named entity recognition. MedBERT is trained with 57.46M tokens collected from biomedical-related data sources, i.e. datasets acquired from N2C2, BioNLP, CRAFT challenges, and biomedical-related articles crawled from Wikipedia. We validate the effectiveness of MedBERT by comparing it with four publicly available pre-trained models on ten biomedical datasets from BioNLP and CRAFT shared tasks. Our experimental results show that models fine-tuned on MedBERT achieve state-of-the-art performance in nine datasets that predict Protein, Gene, Chemical, Cellular/Component, Gene Ontology, and Taxonomy entities. Specifically, the model achieved an average of 84.04% F1-micro score on ten test sets from BioNLP and CRAFT challenges with an improvement of 3.7% and 7.83% as compared to models that were fine-tuned on BioBERT and Bio_ClinicalBERT, respectively.},
  keywords={Proteins;Protein engineering;Biological system modeling;Taxonomy;Predictive models;Ontologies;Transformers},
  doi={10.23919/APSIPAASC55919.2022.9980157},
  ISSN={2640-0103},
  month={Nov},}@INPROCEEDINGS{9973695,
  author={Filgueira, Rosa},
  booktitle={2022 IEEE 18th International Conference on e-Science (e-Science)}, 
  title={frances: A Deep Learning NLP and Text Mining Web Tool to Unlock Historical Digital Collections: A Case Study on the Encyclopaedia Britannica}, 
  year={2022},
  volume={},
  number={},
  pages={246-255},
  abstract={This work presents frances, an integrated text mining tool that combines information extraction, knowledge graphs, NLP, deep learning, parallel processing and Semantic Web techniques to unlock the full value of historical digital textual collections, offering new capabilities for researchers to use powerful analysis methods without being distracted by the technology and middleware details. To demonstrate these capabilities, we use the first eight editions of the Encyclopaedia Britannica offered by the National Library of Scotland (NLS) as an example digital collection to mine and analyse. We have developed novel parallel heuristics to extract terms from the original collection (alongside metadata), which provides a mix of unstructured and semi-structured input data, and populated a new knowledge graph with this information. Our Natural Language Processing models enable frances to perform advanced analyses that go significantly beyond simple search using the information stored in the knowledge graph. Furthermore, frances also allows for creating and running complex text mining analyses at scale. Our results show that the novel computational techniques developed within frances provide a vehicle for researchers to formalize and connect findings and insights derived from the analysis of large-scale digital corpora such as the Encyclopaedia Britannica.},
  keywords={Text mining;Deep learning;Semantic Web;Knowledge engineering;Parallel processing;Metadata;Information retrieval;information extraction;knowledge graphs;deep transfer learning;natural language processing;text mining;web tools;semantic web;parallel computing;digital tools;historical digital textual collections},
  doi={10.1109/eScience55777.2022.00038},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9970920,
  author={Cahyaningsih, Elin and Silalahi, Natascha Lestari Eunike and Rohajawati, Siti and Avianti, Yuliza Maulina},
  booktitle={2022 International Conference on Information Technology Systems and Innovation (ICITSI)}, 
  title={COMMONKADS for Knowledge Based System Development: A Literature Study}, 
  year={2022},
  volume={},
  number={},
  pages={213-218},
  abstract={COMMONKADS is a method for developing knowledge-based system. This method describes foundation, technique, modeling language and document structure for develop the knowledge-based system. COMMONKADS is people-oriented system development methodology, and this methodology is often used for developing organizational knowledge management system. COMMONKADS approach is divided based on context (organizational model, task model, agent model), concept (knowledge model) and artifact (design model). COMMONKADS have been used widely for knowledge-based system in several fields, such as COMMONKADS that integrated in tourism knowledge-based system, COMMONKADS for irrigation expert system, expertise model using COMMONKADS in manufactured company, COMMONKADS in energy management system and many more. Generally, there are eight strengths of COMMONKADS methodology for develop knowledge-based system. Its strength is flexible to use in any scope, represent knowledge (organizational, domain, task and inference knowledge), complete (representation, model, and form), powerful, accurate, comprehensive, represent KM process, systematic and effective. While the weakness of COMMONKADS methodology only three, there are don't have validation process and difficult to acquisition knowledge and use semi formal language, large data storage. Nevertheless, COMMONKADS is recommended methodology for develop knowledge-based system.},
  keywords={Technological innovation;Systematics;Knowledge based systems;Government;Memory;Software;Regulation;COMMONKADS;strength of COMMONKADS;knowledge-based system;weakness of COMMONKADS;methodology;knowledge-based engineering},
  doi={10.1109/ICITSI56531.2022.9970920},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9961280,
  author={Li, Jianli and Yilahun, Hankiz and Hamdulla, Askar},
  booktitle={2022 International Conference on Asian Language Processing (IALP)}, 
  title={Petroleum Exploration and Development Text Triplet Extraction Based on Deep Learning}, 
  year={2022},
  volume={},
  number={},
  pages={225-230},
  abstract={The petroleum exploration and development industry is moving from digital to intelligent. Under the guidance of AI, machine reading and automatic extraction of petroleum exploration and development knowledge are needed for unstructured datas. There are a large number of long entities and complex nested entities in petroleum exploration and development text, which increase the challenge of petroleum exploration and development triplet extraction task. To solve the above problems, (1) The ALBERT-BiLSTM-Attention-CRF method based on large-scale pre-trained Chinese language model is used to extract text triples for Petroleum exploration and development. (2) The ALBERT-BiGRU-Attention method is used to carry out text dichotomies to judge whether triplet extraction is effective. By collecting the datas of Petroleum exploration and development, the corpus of Petroleum exploration and development is established. Secondly, knowledge composition is analyzed and corpus is annotated under the guidance of Petroleum exploration and development theory. Finally, the deep learning training of Petroleum exploration and development descriptive corpus knowledge extraction is carried out. The experimental results show that the accuracy of SPO triplet named entity recognition is 85.49%. The recognition accuracy of extracted triples is 95.90%, which can achieve good recognition effect in small-scale Petroleum exploration and development corpus.},
  keywords={Deep learning;Training;Industries;Text recognition;Geology;Semantics;Ontologies;Deep learning;ALBERT-BiLSTM-Attention-CRF;Triplet extraction;ALBERT-BiGRU-Attention;Petroleum Exploration and Development},
  doi={10.1109/IALP57159.2022.9961280},
  ISSN={},
  month={Oct},}@ARTICLE{9940925,
  author={Borrego, Agustín and Dessì, Danilo and Hernández, Inma and Osborne, Francesco and Reforgiato Recupero, Diego and Ruiz, David and Buscaldi, Davide and Motta, Enrico},
  journal={IEEE Access}, 
  title={Completing Scientific Facts in Knowledge Graphs of Research Concepts}, 
  year={2022},
  volume={10},
  number={},
  pages={125867-125880},
  abstract={In the last few years, we have witnessed the emergence of several knowledge graphs that explicitly describe research knowledge with the aim of enabling intelligent systems for supporting and accelerating the scientific process. These resources typically characterize a set of entities in this space (e.g., tasks, methods, evaluation techniques, proteins, chemicals), their relations, and the relevant actors (e.g., researchers, organizations) and documents (e.g., articles, books). However, they are usually very partial representations of the actual research knowledge and may miss several relevant facts. In this paper, we introduce SciCheck, a new triple classification approach for completing scientific statements in knowledge graphs. SciCheck was evaluated against other state-of-the-art approaches on seven benchmarks, yielding excellent results. Finally, we provide a real-world use case and applied SciCheck to the Artificial Intelligence Knowledge Graph (AI-KG), a large-scale automatically-generated open knowledge graph including 1.2M statements extracted from the 333K most cited articles in the field of Artificial Intelligence, and generated a new version of this knowledge graph with 300K additional triples.},
  keywords={Machine learning;Feature extraction;Semantic Web;Task analysis;Context modeling;Computational modeling;Benchmark testing;Knowledge based systems;Knowledge graphs;science of science;knowledge graph completion;triple classification;machine learning;semantic web},
  doi={10.1109/ACCESS.2022.3220241},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9926710,
  author={Ji, Fan and Ocker, Felix and Zou, Minjie and Vogel-Heuser, Birgit and Oligschläger, Marius},
  booktitle={2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)}, 
  title={Identifying Inconsistencies in the Design of Large-scale Casting Systems – An Ontology-based Approach}, 
  year={2022},
  volume={},
  number={},
  pages={319-325},
  abstract={The development of modern automated production systems requires the close cooperation of engineers from different domains. Due to the large amount of domain-specific documents and heterogeneous data they create during the multidisciplinary engineering activities, ensuring the consistency of information is always challenging. Since most of these documents are texted-based and lack a standardized structure, extracting required information from these files is oftentimes problematic. This issue is particularly critical in the development of large-scale production plants due to the high complexity of the systems and the diversity of disciplines involved. To help engineers efficiently utilize unstructured data sources as well as identify potential information contradictions, we propose an ontology-based inconsistency management approach for large-scale production systems that generates the knowledge base from unstructured engineering data and (semi-) automatically detects multiple types of inconsistencies. In addition, the presented framework also supports the tracking of information changes during the system design process.},
  keywords={Production systems;Casting;Computer aided software engineering;Automation;Soft sensors;Knowledge based systems;Complexity theory},
  doi={10.1109/CASE49997.2022.9926710},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{9921564,
  author={Köcher, Aljosha and Da Silva, Luis Miguel Vieira and Fay, Alexander},
  booktitle={2022 IEEE 27th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Modeling and Executing Production Processes with Capabilities and Skills using Ontologies and BPMN}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Current challenges of the manufacturing industry require modular and changeable manufacturing systems that can be adapted to variable conditions with little effort. At the same time, production recipes typically represent important company know-how that should not be directly tied to changing plant configurations. Thus, there is a need to model general production recipes independent of specific plant layouts. For execution of such a recipe however, a binding to then available production resources needs to be made. In this contribution, we select a suitable modeling language to model and execute such recipes. Furthermore, we present an approach to solve the issue of recipe modeling and execution in modular plants using semantically modeled capabilities and skills as well as BPMN. We make use of BPMN to model production recipes using capability processes, i.e. production processes referencing abstract descriptions of resource functions. These capability processes are not bound to a certain plant layout, as there can be multiple resources fulfilling the same capability. For execution, every capability in a capability process is replaced by a skill realizing it, effectively creating a skill process consisting of various skill invocations. The presented solution is capable of orchestrating and executing complex processes that integrate production steps with typical IT functionalities such as error handling, user interactions and notifications. Benefits of the approach are demonstrated using a flexible manufacturing system.},
  keywords={Manufacturing industries;Adaptation models;Layout;Production;Companies;Ontologies;Flexible manufacturing systems;Capabilities;Skills;Skill-Based Production;Orchestration;BPMN;Ontologies;Semantic Web},
  doi={10.1109/ETFA52439.2022.9921564},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9909441,
  author={Rybiński, Kamil and Śmiałek, Michał},
  booktitle={2022 17th Conference on Computer Science and Intelligence Systems (FedCSIS)}, 
  title={Beyond Low-Code Development: Marrying Requirements Models and Knowledge Representations}, 
  year={2022},
  volume={},
  number={},
  pages={919-928},
  abstract={Typical Low-Code Development platforms enable model-driven generation of web applications from high-level visual notations. They normally express the UI and the application logic, which allows generating the frontend and basic CRUD operations. However, more complex domain logic (data processing) operations still necessitate the use of traditional programming. This paper presents a visual language, called RSL-DL, to represent domain knowledge with complex domain rules aligned with requirements models. The language synthesises and extends approaches found in knowledge representation (ontologies) and software modelling language engineering. Its purpose is to enable a fully automatic generation of domain logic code by reasoning over and reusing domain knowledge. The language’s abstract syntax is defined using a meta-model expressed in MOF. Its semantics is expressed with several translational rules that map RSL-DL models onto typical programming language constructs. The rules are explained informally in natural language and formalised using a graphical transformation notation. It is also supported by introducing an inference engine that enables processing queries to domain models and selecting appropriate invocations to generated code. The presented language was implemented by building a dedicated model editor and transformation engine. It was also initially validated through usability studies. Based on these results, we conclude that declarative knowledge representations can be successfully used to produce imperative back-end code with non-trivial logic.},
  keywords={Visualization;Codes;Computational modeling;Semantics;Syntactics;Programming;Ontologies},
  doi={10.15439/2022F129},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9905090,
  author={Pradeepani, M. K. T. and Jayawardena, C. and Rajapaksha, U. U. S.},
  booktitle={2022 International Research Conference on Smart Computing and Systems Engineering (SCSE)}, 
  title={Adding Commonsense to Robotic Application Using Ontology-Based Model Retraining}, 
  year={2022},
  volume={5},
  number={},
  pages={157-164},
  abstract={In terms of the level of technological capability in the world today, the use of automated robotics is common in various fields. There are large projects going on in many industries that collaborate between robots and other robots, as well as humans and robots. In hospital environments, care for people with medical needs and their needs and used to make appropriate suggestions to their problems. Robots can also be found in certain areas that can respond quickly as an emergency rescue agent. Furthermore, robots, which can be seen in the hotel industry as waiters and as farm assistants in agriculture, have a great tendency to be used as multi-tasking agents in many fields. In each of these areas, robots must co-operate with humans. In that situation, the importance of the exchange of mutual knowledge between robots-robots and between humans-robots comes into the picture. What matters here is not only the quantitative vastness of knowledge but also the ability to understand each other in the same medium. Although the common sense that people need in their day-to-day work is completely obvious to humans, the commonsense knowledge domain needs to be implanted in robots. Whatever concept is defined for adding commonsense to robotics, it should be a consistent concept that can be logically constructed so that it can be understood by a machine. As will be discussed later in the paper, different methods have been used in various related works to add a different kind of domain knowledge to robotics. The objective of this paper is to provide an improved retrained model for robotics in order to give them the ability to act more human-like when performing tasks. By using the proposed model robots are able to answer the incomplete command or inquiries related to a given context. One of the objectives of this work is to use the ontology-based, commonsense-support existing knowledge base as a mechanism to retrain and build a new model.},
  keywords={Training;Adaptation models;Service robots;Knowledge based systems;Robot sensing systems;Hardware;Sensors;BERT;commonsense;robotics;transfer learning},
  doi={10.1109/SCSE56529.2022.9905090},
  ISSN={2613-8662},
  month={Sep.},}@INPROCEEDINGS{9892075,
  author={Lin, RuiMing and Cheng, LiangLun and Wang, Tao and Deng, Jianfeng},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Trans-SBLGCN: A Transfer Learning Model for Event Logic Knowledge Graph Construction of Fault Diagnosis}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Taking fault diagnosis corpus as the research object, an event logic knowledge graph construction method is proposed in this paper. Firstly, we propose a data labeling strategy based on a constructed event logic ontology model, then collect large-scale robot transmission system fault diagnosis corpus, and label part of the data according to the strategy. Secondly, we propose a transfer learning model called Trans-SBLGCN for event argument entity and event argument relation joint extraction. A language model is trained based on large-scale unlabeled fault diagnosis corpus and transferred to a model based on stacked bidirectional long short term memory (BiLSTM) and bidirectional graph convolutional network (BiGCN). Experimental results show that the method is superior to other methods. Finally, an event logic knowledge graph of robot transmission system fault diagnosis is constructed to provide decision support for autonomous robot transmission system fault diagnosis.},
  keywords={Fault diagnosis;Knowledge engineering;Transfer learning;Neural networks;Ontologies;Data models;Labeling;Event Logic Knowledge Graph;Fault Diagnosis;Knowledge Joint Extraction;BiGCN},
  doi={10.1109/IJCNN55064.2022.9892075},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9896311,
  author={Rogachev, Aleksey and Melikhova, Elena},
  booktitle={2022 International Russian Automation Conference (RusAutoCon)}, 
  title={Automation of Architecture Justification and Parameters Selection of Artificial Neural Networks for Intelligent Detection of Cyber-Physical Threats}, 
  year={2022},
  volume={},
  number={},
  pages={908-912},
  abstract={The problems of improving the quality of training of deep artificial neural networks (ANN) for various applied tasks require automatization of the selection of hyperparameters of neural networks. The KerasTuner software toolkit can be used to automate the search for optimal values of ANN hyperparameters. It includes random search methods, Bayesian optimization, etc. The formation of training text samples for neural network identification of cyber-physical threats is a separate scientific and methodological task. The complexity of the problem is due to the diversity of the ontology of the key terms of the cyberphysical thesaurus, the variety of styles of lexicological content, as well as the partial intersection of the content of previously identified ontological categories. In the process of experimental study of hyperparameters of deep ANNs being developed, models of “embedding”, “bag of words” and dense vector representation in Python were compared. On the basis of a systematic approach, an information-morphological matrix of thematic blocks is constructed. In the conducted experiments, the values of parameters such as the number of convolutional blocks, the number of their filters, the type of activation functions, the parameters of the “dropout” layers, etc. were changed. The studied tools provided optimization of hyperparameters of the convolutional network, while the calculation time on the Colaboratory platform for the studied ANN architectures using GPU graphics accelerators was 5…9 o’clock. The developed modified algorithm for computer detection of cyberphysical threats in electronic resources allowed to substantiate alternative architectures and optimize the main hyperparameters of ANN.},
  keywords={Training;Automation;Graphics processing units;Artificial neural networks;Computer architecture;Ontologies;Cyber-physical systems;cyber-physical threat;artificial neural network;hyperparameter;intelligent detection;Automation},
  doi={10.1109/RusAutoCon54946.2022.9896311},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9887476,
  author={Dudija, Nidya and Natalia, Lezia and Alamsyah, Andry and Romadhony, Ade},
  booktitle={2022 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)}, 
  title={Identification of Extraversion and Neuroticism Personality Dimensions Using IndoBERT’s Deep Learning Model}, 
  year={2022},
  volume={},
  number={},
  pages={155-159},
  abstract={Human resources are essential for the business organization to adapt to change. Identifying the personality dimensions of new talent could help recruiters conduct the selection process of matching skilled talent to the organization’s needs. The objective of this study is to identify the personality dimensions corresponding to the job need, which correlates with extraversion and neuroticism. The legacy methodology to determine personality dimensions is through interviews or questionnaire surveys, but this process is costly and takes longer time to complete. This paper proposes a work on a person personality identification based on social media text as a complementary methodology. We utilize the textual data to support identifying new talent personality dimensions. In this study, we use IndoBERT model to capture person personality dimension based on their post on Twitter social media. As a result, our model achieves 96% accuracy in identifying extraversion and neuroticism personality dimensions. We also compare our result with the previous work based on the ontology model.},
  keywords={Deep learning;Costs;Social networking (online);Blogs;Ontologies;Communications technology;Fourth Industrial Revolution;Human Resource;Talent Selection;Personality Identification Dimension;Deep Learning;IndoBERT},
  doi={10.1109/IAICT55358.2022.9887476},
  ISSN={},
  month={July},}@INPROCEEDINGS{9874511,
  author={Faramarzi, Noushin Salek and Dara, Akanksha and Banerjee, Ritwik},
  booktitle={2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)}, 
  title={Combining Attention-based Models with the MeSH Ontology for Semantic Textual Similarity in Clinical Notes}, 
  year={2022},
  volume={},
  number={},
  pages={74-83},
  abstract={In this study, we present several transformer-based models as well as traditional machine learning methods to detect semantic textual similarity (STS) in clinical notes. We investigate transformer models pretrained on general English as well as clinical notes, and use generic English STS datasets as a supplemental corpus to clinical notes data. Our work is based on the 2019 National NLP Clinical Challenge (n2c2). We identify and annotate six types of sentences in the clinical notes corpus, and report an ensemble method that combines attention-based contextualized embeddings with a similarity score based on the MeSH ontology obtained by computing least common ancestors of clinical terms. Our approach does not need additional clinical data for model training, while still achieving comparable Pearson's correlation coefficient of 0.901.},
  keywords={Training;Drugs;Vocabulary;Computational modeling;Semantics;Machine learning;Ontologies;Electronic Health Records;Natural Language Processing;Clinical Semantic Textual Similarity;Transformers;MeSH},
  doi={10.1109/ICHI54592.2022.00023},
  ISSN={2575-2634},
  month={June},}@INPROCEEDINGS{9874698,
  author={Zheng, Can and Wang, Yanshan and Jia, Xiaowei},
  booktitle={2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)}, 
  title={Graph-Augmented Cyclic Learning Framework for Similarity Estimation of Medical Clinical Notes}, 
  year={2022},
  volume={},
  number={},
  pages={97-103},
  abstract={Semantic textual similarity (STS) in the clinical domain helps improve diagnostic efficiency and produce concise texts for downstream data mining tasks. However, given the high degree of domain knowledge involved in clinic text, it remains challenging for general language models to infer implicit medical relationships behind clinical sentences and output similarities correctly. In this paper, we present a graph-augmented cyclic learning framework for similarity estimation in the clinical domain. The framework can be conveniently implemented on a state-of-art backbone language model, and improve its performance by leveraging domain knowledge through co-training with an auxiliary graph convolution network (GCN) based network. We report the success of introducing domain knowledge in GCN and the co-training framework by improving the Bio-clinical BERT baseline by 16.3% and 27.9%, respectively.},
  keywords={Knowledge engineering;Biological system modeling;Semantics;Estimation;Medical services;Manuals;Ontologies;clinical notes;graph neural networks;BERT},
  doi={10.1109/ICHI54592.2022.00026},
  ISSN={2575-2634},
  month={June},}@INPROCEEDINGS{9865330,
  author={Dordiuk, Vladislav and Demicheva, Ekaterina and Espino, Fernando Polanco and Ushenin, Konstantin},
  booktitle={2022 Ural-Siberian Conference on Computational Technologies in Cognitive Science, Genomics and Biomedicine (CSGB)}, 
  title={Natural language processing for clusterization of genes according to their functions}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={There are hundreds of methods for analysis of data obtained in mRNA-sequencing. The most of them are focused on small number of genes. In this study, we propose an approach that reduces the analysis of several thousand genes to analysis of several clusters. The list of genes is enriched with information from open databases. Then, the descriptions are encoded as vectors using the pretrained language model (BERT) and some text processing approaches. The encoded gene function pass through the dimensionality reduction and clusterization. Aiming to find the most efficient pipeline, 180 cases of pipeline with different methods in the major pipeline steps were analyzed. The performance was evaluated with clusterization indexes and expert review of the results.},
  keywords={Dimensionality reduction;Databases;Pipelines;Bit error rate;Genomics;Natural language processing;Cognitive science;natural language processing;BERT;semantic analysis;differential gene expression analysis;gene ontology;gene expression;clusterization},
  doi={10.1109/CSGB56354.2022.9865330},
  ISSN={},
  month={July},}@INPROCEEDINGS{9863051,
  author={Ataei, Sima and Butler, Gregory},
  booktitle={2022 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)}, 
  title={Predicting the specific substrate for transmembrane transport proteins using BERT language model}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Transmembrane transport proteins play a vital role in cells' metabolism by the selective passage of substrates through the cell membrane. Metabolic network reconstruction requires transport reactions that describe the specific substrate transported as well as the metabolic reactions of enzyme catalysis. In this paper, we apply BERT (Bidirectional Encoder Representations from Transformers) language model for protein sequences to predict one of 12 specific substrates. Our UniProt-ICAT-100 dataset is automatically constructed from UniProt using the ChEBI and GO ontologies to identify 4,112 proteins transporting 12 inorganic anion or cation substrates. We classified this dataset using three different models including Logistic Regression with an MCC of 0.81 and accuracy of 97.5%; Feed-forward Neural Networks classifier with an MCC of 0.88 and accuracy of 98.5%. Our third model utilizes a Fine-tuned BERT language model to predict the specific substrate with an MCC of 0.95 and accuracy of 99.3% on an independent test set.},
  keywords={Proteins;Biological system modeling;Computational modeling;Bit error rate;Neural networks;Cells (biology);Predictive models;Classification;BERT model;Transport protein;Specific substrate Prediction;ChEBI ontology;Gene Ontology},
  doi={10.1109/CIBCB55180.2022.9863051},
  ISSN={},
  month={Aug},}@ARTICLE{9866769,
  author={Silega, Nemury and Noguera, Manuel and Rogozov, Yuri I. and Lapshin, Vyacheslav S. and González, Tahumara},
  journal={IEEE Access}, 
  title={Transformation From CIM to PIM: a Systematic Mapping}, 
  year={2022},
  volume={10},
  number={},
  pages={90857-90872},
  abstract={Model Driven Architecture (MDA) is the most prominent and accepted methodology based on the Model Driven Development (MDD) principles. MDA includes three abstraction levels: Computer Independent Models (CIM), Platform Independent models (PIM) and Platform specific models (PSM). MDA encourages the automatic transformation of models as a means to increase the speed of the software development process and to prevent human errors. There are plenty of solutions to transform PIMs to PSMs, however the CIM to PIM transformation does not receive a similar attention. In that sense, this paper aims to describe a systematic mapping to analyze the main characteristics of the approaches that deal with the CIM to PIM transformation as well as to discuss research directions stemming out from our analysis. The results of this mapping study could be a valuable information source for the scientific community in order to know the real advances in this topic and to avoid unnecessary effort dealing with problems that have already been addressed. For example, this study yielded the models at the CIM level that have already been transformed into models at the PIM level. Hence, with this information, the researchers could focus their attention on finding solutions to transform those models at CIM level that have not been transformed into models at PIM level. Likewise, this mapping study provides information regarding the technological support of this type of transformation. This information could be useful for those software projects interested to adopt MDA.},
  keywords={Computational modeling;Software;Systematics;Business;Software engineering;Mathematical models;Internet;Model driven architecture (MDA);computer independent models (CIM);platform independent models (PIM);systematic mapping},
  doi={10.1109/ACCESS.2022.3201556},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9861086,
  author={Morine, Melissa J. and Priami, Corrado and Coronado, Edith and Haber, Juliana and Kaput, Jim},
  booktitle={2022 IEEE International Conference on Digital Health (ICDH)}, 
  title={A Comprehensive and Holistic Health Database}, 
  year={2022},
  volume={},
  number={},
  pages={202-207},
  abstract={Health and the initiation, progression, and outcome of disease are the result of multiple environmental factors interacting with individual genetic makeups. Collectively, results from primary clinical research on health and disease represent the most compendious and reliable source of actionable knowledge on strategies to optimize health. However, the dispersal of this information as unstructured data, distributed across millions of documents, is a substantial challenge in bridging the gap between primary research and concrete recommendations for improving health. Described here is the development and implementation of a machine reading pipeline that builds a knowledge graph of causal relationships between a broad range of predictive/modifiable diet and lifestyle factors and health outcomes, extracted from the vast biomedical corpus in the National Library of Medicine.},
  keywords={Text mining;Systematics;Pipelines;Semantics;Genetics;Libraries;Environmental factors;Healthware;knowledge graphs;natural language processing},
  doi={10.1109/ICDH55609.2022.00039},
  ISSN={},
  month={July},}@INPROCEEDINGS{9856059,
  author={Rudwan, Mohammed Suleiman Mohammed and Fonou-Dombeu, Jean Vincent},
  booktitle={2022 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)}, 
  title={Ontology Reuse: Neural Network-Based Measurement of Concepts Representations and Similarities in Ontology Corpus}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Ontologies are the heart of the semantic web. They are designed to be reused in web applications. This paper aims to discover a given concept's representation against existing ontologies in a corpus, and if the concept is represented, other similar concepts and terms to it are extracted. A corpus formed of several ontologies in the agricultural domain was constructed. SPARQL queries were used to extract the required data from existing ontologies. And a machine learning technique, the Word2Vec, was employed for ontology reuse process to measure concepts similarity against the existing ontologies. The experimental results showed that the proposed methodology successfully detected previously seen vocabularies during the training on the data in the ontology corpus, and retrieved other similar concepts from the ontologies as well as their degree of similarity (Cosin similarity). Furthermore, the proposed model could process over two million terms in around one minute, reflecting its effectiveness in this context. The proposed method would be useful to ontology and knowledge engineers to conduct a preliminary investigation about which existing ontologies are suitable for reuse in the process of developing new ontologies. Other applications of the proposed method may include ontology alignment to measure the degree of similarity between existing ontologies.},
  keywords={Training;Semantic Web;Knowledge engineering;Heart;Vocabulary;Machine learning;Ontologies;automated ontology reuse;NLP;Word2Vec;artificial neural networks},
  doi={10.1109/icABCD54961.2022.9856059},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9845845,
  author={Yiming, Liu and Li, Duan},
  booktitle={2022 7th International Conference on Computer and Communication Systems (ICCCS)}, 
  title={Research on the Construction of Maritime Legal Knowledge Graph}, 
  year={2022},
  volume={},
  number={},
  pages={903-908},
  abstract={As the marine industry booms, the maritime legal documents are of great importance to the maneuver on the sea. However, the traditional way of consulting the text can not meet the demand of maritime operation nowadays. This paper aims to explore a way to extract and strengthen data from maritime legal texts to better support legal question answering. To mine knowledge from unstructured maritime laws and regulations, this paper proposes a method to build the maritime legal knowledge graph. To extract information from unstructured texts, BERT+BiLSTM+CRF is used for named entity recognition. DeepKE toolkit is used for relation extraction. And to strengthen the logics between entities, heterogeneous nodes are introduced to enhance the semantic associations in the maritime legal knowledge graph. The document-enhanced knowledge graph expanded in scale, so it can better support subsequent intelligent applications.},
  keywords={Law;Text recognition;Semantics;Pipelines;Ontologies;Information retrieval;Regulation;knowledge graph;maritime law;named entity recognition;heterogeneous entities},
  doi={10.1109/ICCCS55155.2022.9845845},
  ISSN={},
  month={April},}@INPROCEEDINGS{9843341,
  author={Mordecai, Yaniv and Markina–Khusid, Aleksandra and Quinn, Greg and Crawley, Edward F.},
  booktitle={2022 IEEE Aerospace Conference (AERO)}, 
  title={Applying Model-Based Ontology Coverage Analysis to Mission Architectures}, 
  year={2022},
  volume={},
  number={},
  pages={01-18},
  abstract={This paper introduces a method for Model-based Ontology Coverage Analysis (MOCA) and applies it to SysML models of mission architectures. An ontology is a set of concepts that constitute a common language, standard terminology, and consistent pattern reference across multiple models within an organization, industry, or domain. The purpose of MOCA is to assess the overlap between a system architecture model and a given ontology, and thereby the architecture model's compliance with the ontology and the ontology's utilization by the architecture. We demonstrate MOCA on a SysML model of a humanitarian airlift mission, using a conceptual mission architecting SysML profile model that serves as the ontology. MOCA automates and simplifies reasoning over models, and creates digital model-based artifacts that support stakeholders in concept validation, decision making, and system/mission design. Thus, MOCA enhances digital systems engineering.},
  keywords={Analytical models;Vocabulary;Visualization;Digital systems;Atmospheric modeling;Unified modeling language;Semantics;Digital Engineering;Model-Based Systems Engineering;MBSE;Mission Architecture;Mission Engineering;Ontology;Ontological Analysis},
  doi={10.1109/AERO53065.2022.9843341},
  ISSN={1095-323X},
  month={March},}@INPROCEEDINGS{9816191,
  author={Andreadis, Stelios and Elias, Mirette and Mavropoulos, Thanassis and Papadopoulos, Charis and Pantelidis, Nick and Gialampoukidis, Ilias and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
  booktitle={2022 IEEE 14th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP)}, 
  title={SPARQL querying for validating the usage of automatically georeferenced social media data as human sensors for air quality}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={The problem of air pollution is one of the countless topics discussed on social media on an everyday basis. This rich, crowdsourced information can be exploited to assess the air quality of urban areas, using humans as sensors. Nevertheless, the majority of social media data are falsely geotagged or completely lack geoinformation, which is an essential attribute, while the reliability of the air pollution events reported by online citizens has to be proven. The scope of this work is to present a framework that collects Twitter messages in German that refer to the atmosphere, automatically georeferences them, and finally validates them through semantic representation and SPARQL queries in order to associate them with real measurements of air quality sensors. The georeferencing models are evaluated against state-of-the-art works and the proposed framework is validated in a near-six-month scenario in Germany.},
  keywords={Multidimensional signal processing;Social networking (online);Atmospheric measurements;Atmospheric modeling;Urban areas;Semantics;Sensor phenomena and characterization;air quality;social media;georeferencing;semantic representation;SPARQL querying},
  doi={10.1109/IVMSP54334.2022.9816191},
  ISSN={},
  month={June},}@BOOK{9785669,
  author={Quamar, Abdul and Efthymiou, Vasilis and Lei, Chuan and Özcan, Fatma},
  booktitle={Natural Language Interfaces to Data},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Natural language interfaces provide an easy way to query and interact with data and enable non-technical users to investigate data sets without the need to know a query language. Recent advances in natural language understanding and processing have resulted in a renewed interest in natural language interfaces to data. The main challenges in natural language querying are identifying the entities involved in the user utterance, connecting the different entities in a meaningful way over the underlying data source to interpret user intents, and generating a structured query. There are two main approaches in the literature for interpreting a user’s natural language query. The first are rule-based systems that make use of semantic indices, ontologies, and knowledge graphs to identify the entities in the query, understand the intended relationships between those entities, and utilize grammars to generate the target queries. Second are hybrid approaches that utilize both rule-based techniques as well as deep learning models. Conversational interfaces are the next natural step to one-shot natural language querying by exploiting query context between multiple turns of conversation for disambiguation. In this monograph, the authors review the rule-based and hybrid technologies that are used in natural language interfaces and survey the different approaches to natural language querying. They also describe conversational interfaces for data analytics and discuss several benchmarks used for natural language querying research and evaluation. The monograph concludes with discussion on challenges that need to be addressed before these systems can be widely adopted.},
  keywords={},
  doi={},
  ISSN={},
  publisher={now},
  isbn={9781638280293},
  url={https://ieeexplore.ieee.org/document/9785669},}@INPROCEEDINGS{9776306,
  author={Ma, Ke and Qin, Bo and Wang, Hongwei},
  booktitle={2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
  title={A Profiling and Query Platform for Research Management Based on Knowledge Graph}, 
  year={2022},
  volume={},
  number={},
  pages={822-827},
  abstract={Researching is a process whereby a large amount of new and unstructured knowledge is created and accumulated. In this context, the capture of complex knowledge about detailed work and decision-making issues throughout a research project is very challenging for modern researchers. Knowledge graph technology can help machines better understand complicated relationships between entities, has great potential for helping researchers with organizing and automating such kinds of repetitive works, and even uncovering and providing new insights into related topics.This paper introduces a way to construct a research management platform by providing a profiling and query system visualized as a knowledge graph. With the scope of this platform being restricted to the research field, typical ontologies are proposed on different levels. For better and more meaningful visualization, specific modifications and improvements to the traditional knowledge graph structure are discussed. A prototype system that is under construction is then described based on the above work, with the extensive applications discussed.},
  keywords={Visualization;Conferences;Decision making;Prototypes;Ontologies;Collaborative work;knowledge graph;research management;ontology},
  doi={10.1109/CSCWD54268.2022.9776306},
  ISSN={},
  month={May},}@ARTICLE{9744572,
  author={Hsu, Hao-Hsuan and Huang, Nen-Fu},
  journal={IEEE Transactions on Learning Technologies}, 
  title={Xiao-Shih: A Self-Enriched Question Answering Bot With Machine Learning on Chinese-Based MOOCs}, 
  year={2022},
  volume={15},
  number={2},
  pages={223-237},
  abstract={This article introduces Xiao-Shih, the first intelligent question answering bot on Chinese-based massive open online courses (MOOCs). Question answering is critical for solving individual problems. However, instructors on MOOCs must respond to many questions, and learners must wait a long time for answers. To address this issue, Xiao-Shih integrates many novel natural language processing and machine learning approaches to achieve state-of-the-art performance. Furthermore, Xiao-Shih has a built-in self-enriched mechanism for expanding the knowledge base through open community-based question answering. This article proposes a novel approach, known as spreading question similarity (SQS), which iterates similar keywords on our keyword networks to find duplicate questions. Compared with BERT, an advanced neural language model, the results showed that SQS outperforms BERT on recall and accuracy above a prediction probability threshold of 0.8. After training, Xiao-Shih achieved a perfect correct rate. Furthermore, Xiao-Shih outperforms Jill Watson 1.0, which is a noted question answering bot, on answer rate with the self-enriched mechanism.},
  keywords={Chatbots;Standards;Electronic learning;Data science;Computer aided instruction;Bit error rate;Machine learning;Answer selection;machine learning (ML);massive open online courses (MOOCs);natural language processing (NLP);ontologies;question answering bot;question retrieval},
  doi={10.1109/TLT.2022.3162572},
  ISSN={1939-1382},
  month={April},}@INPROCEEDINGS{9736481,
  author={Lim, Chae-Gyun and Lee, Dongkun and Lee, Young-Jun and Choi, Ho-Jin},
  booktitle={2022 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Knowledge Management Approach for Memory Components Based on User-friendly Conversational System}, 
  year={2022},
  volume={},
  number={},
  pages={401-403},
  abstract={Due to the recent technological development and the growth of computing resources, there are various studies that apply large-scale language models in the field of natural language processing such as conversational systems. Also, there are researches that attempt to maintain a conversation flow and naturally lead a dialogue by treating the contextual information exchanged with users from the perspective of knowledge. In this paper, we propose a method for managing various contextual information such as chat history, situations, and preferred topics based on a knowledge base and generating a conversation customized for the specific user. We design a schema of memory components to deal with the user's contextual information so that implement a Web-based conversational system, which is friendly come to those users. It is expected that these systems using the user-specific memory components will be helpful in such domains like education or customer consultation.},
  keywords={Conferences;Computational modeling;Memory management;Knowledge based systems;Education;Lead;Big Data;memory component;ontology-based approach;conversation history;conversational system},
  doi={10.1109/BigComp54360.2022.00091},
  ISSN={2375-9356},
  month={Jan},}@INPROCEEDINGS{9736309,
  author={Burgdorf, Andreas and Paulus, Alexander and Pomp, André and Meisen, Tobias},
  booktitle={2022 IEEE 16th International Conference on Semantic Computing (ICSC)}, 
  title={DocSemMap: Leveraging Textual Data Documentations for Mapping Structured Data Sets into Knowledge Graphs}, 
  year={2022},
  volume={},
  number={},
  pages={209-216},
  abstract={Today, knowledge graphs have been proven to enable the efficient integration of heterogeneous data sets. An important step in creating such knowledge graphs is the mapping of the attributes of a data set to the knowledge graph's ontology. So far, numerous methods have been developed to support this mapping process by using both the schema information as well as the actual data values from a data set in conjunction with external knowledge bases or machine learning approaches. A third source of information, namely textual data documentations, has not yet been considered. In this paper, we present DocSemMap, a novel approach that utilizes textual data documentations of data sets as an additional source for the creation of semantic mappings. We train custom embeddings on the textual data documentations. Further, we utilize pre-trained embeddings that allow us to identify similarities between excerpts of the textual data documentations and descriptions of ontological concepts. Based on this, we build candidate sets of the best suitable concepts for mapping and finally use weighted similarity scores to identify the best fitting concept for each attribute of a data set. The evaluation of our approach outperforms existing approaches for semantic mapping but still has potential for improvement.},
  keywords={Semantics;Natural languages;Knowledge based systems;Fitting;Documentation;Machine learning;Syntactics;semantic mapping;knowledge graph construction;natural language processing;textual data documentation},
  doi={10.1109/ICSC52841.2022.00042},
  ISSN={2325-6516},
  month={Jan},}@INPROCEEDINGS{9721762,
  author={Sajid, Hira and Kanwal, Javeria and Bhatti, Saeed Ur Rehman and Qureshi, Saad Ali and Basharat, Amna and Hussain, Shujaat and Khan, Kifayat Ullah},
  booktitle={2022 16th International Conference on Ubiquitous Information Management and Communication (IMCOM)}, 
  title={Resume Parsing Framework for E-recruitment}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Modern approaches to improve networking and communication have given ways to the advancement of recruitment process through the development of e-recruitment recommender systems. The increasing expansion of internet- based recruiting has resulted in a large number of resumes being stored in recruitment systems. Most resumes are prepared in a variety of styles to attract the attention of recruiters, including different font sizes, font colors, and table formats. However, data mining operations such as resume information extraction, automatic profile matching, and applicant ranking are immensely affected by the variety of formats. Rule-based methods, supervised methods and semantics-based methods have been introduced to extract facts from resume accurately, however, these methods heavily depend on large amounts of annotated data that is usually difficult to collect Furthermore, these techniques are time-intensive and bear knowledge incompleteness that strongly affect the accuracy of resume parser. In this paper, we present a resume parsing framework that handles the limitations faced in the previous techniques. At first, the raw text is extracted from resumes and blocks are separated using text block classification. Furthermore, the entities are extracted using named entity recognition and enriched using ontology. The proposed resume parser accurately extracts information from resumes that directly contributes towards the selection of best candidate.},
  keywords={Training;Resumes;Layout;Ontologies;Information retrieval;Feature extraction;Information management;Resume parsing;Data Enrichment;Text Extraction;Ontology;Boolean Naive Bayes},
  doi={10.1109/IMCOM53663.2022.9721762},
  ISSN={},
  month={Jan},}@ARTICLE{9556591,
  author={Marschall, Benedikt and Ochsenkuehn, Daniel and Voigt, Tobias},
  journal={IEEE Journal of Emerging and Selected Topics in Industrial Electronics}, 
  title={Design and Implementation of a Smart, Product-Led Production Control Using Industrial Agents}, 
  year={2022},
  volume={3},
  number={1},
  pages={48-56},
  abstract={In theory, the design of modern production systems in the form of a cyber–physical production system (CPPS) allows more flexibility, simple expandability, quick adaptability, and intelligent production control by the product. Multiagent systems (MASs) are thereby recommended as control solution because of their autonomy and dynamic decentralized architecture. Although their potential use and technical excellence have been proven, the costs of implementation and maintenance still outweigh their supposed advantages. This results in low acceptance and usage of operational MAS in the industry. This article describes topics that need to be considered when designing and implementing an MAS as production control for a CPPS in the context of customized mass production. Generally developed approaches are presented, which were implemented in a commercially developed agent framework and validated on the basis of an industrial use case for a product-led filling process in lot size one. All implemented concepts aim to be reusable in comparable applications across industries. In combination with the MAS-internal testing approach also presented, this should contribute to faster, more cost-effective implementation of reliable MAS solutions and ultimately increase their technical maturity.},
  keywords={Production systems;Production control;XML;Radiofrequency identification;Costs;Companies;Testing;Industrial agents (IAs);cyber–physical product ion system (CPPS);multiagent system (MAS)},
  doi={10.1109/JESTIE.2021.3117121},
  ISSN={2687-9743},
  month={Jan},}@ARTICLE{9534721,
  author={Lu, Jinzhi and Ma, Junda and Zheng, Xiaochen and Wang, Guoxin and Li, Han and Kiritsis, Dimitris},
  journal={IEEE Systems Journal}, 
  title={Design Ontology Supporting Model-Based Systems Engineering Formalisms}, 
  year={2022},
  volume={16},
  number={4},
  pages={5465-5476},
  abstract={Model-based systems engineering (MBSE) provides an important capability for managing the complexities of system development. MBSE empowers the formalism of system architectures for supporting model-based requirement elicitation, specification, design, development, testing, fielding, etc. However, the modeling languages and techniques are heterogeneous, even within the same enterprise system, which leads to difficulties for data interoperability. The discrepancies among data structures and language syntaxes make information exchange among MBSE models more difficult, resulting in considerable information deviations when connecting data flows across the enterprise. Therefore, this article presents an ontology based upon graphs, objects, points, properties, roles, and relationships with extensions (GOPPRRE), providing metamodels that support the various MBSE formalisms across lifecycle stages. In particular, knowledge graph models are developed to support unified model representations to further implement ontological data integration based on GOPPRRE throughout the entire lifecycle. The applicability of the MBSE formalism is verified using quantitative and qualitative approaches. Moreover, the GOPPRRE ontologies are used to create the MBSE formalisms in a domain-specific modeling tool, MetaGraph, for evaluating its availability. The results demonstrate that the proposed ontology supports the formal structures and descriptive logic of the systems engineering lifecycle.},
  keywords={Modeling;Ontologies;Unified modeling language;Tools;Systems engineering and theory;Semantics;Data models;Formalism;interoperability;knowledge graph;model-based systems engineering;ontology},
  doi={10.1109/JSYST.2021.3106195},
  ISSN={1937-9234},
  month={Dec},}@ARTICLE{9525274,
  author={Nourani, Esmaeil and Asgari, Ehsaneddin and McHardy, Alice C. and Mofrad, Mohammad R.K.},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
  title={TripletProt: Deep Representation Learning of Proteins Based On Siamese Networks}, 
  year={2022},
  volume={19},
  number={6},
  pages={3744-3753},
  abstract={Pretrained representations have recently gained attention in various machine learning applications. Nonetheless, the high computational costs associated with training these models have motivated alternative approaches for representation learning. Herein we introduce TripletProt, a new approach for protein representation learning based on the Siamese neural networks. Representation learning of biological entities which capture essential features can alleviate many of the challenges associated with supervised learning in bioinformatics. The most important distinction of our proposed method is relying on the protein-protein interaction (PPI) network. The computational cost of the generated representations for any potential application is significantly lower than comparable methods since the length of the representations is significantly smaller than that in other approaches. TripletProt offers great potentials for the protein informatics tasks and can be widely applied to similar tasks. We evaluate TripletProt comprehensively in protein functional annotation tasks including sub-cellular localization (14 categories) and gene ontology prediction (more than 2000 classes), which are both challenging multi-class, multi-label classification machine learning problems. We compare the performance of TripletProt with the state-of-the-art approaches including a recurrent language model-based approach (i.e., UniRep), as well as a protein-protein interaction (PPI) network and sequence-based method (i.e., DeepGO). Our TripletProt showed an overall improvement of F1 score in the above mentioned comprehensive functional annotation tasks, solely relying on the PPI network. Availability: The source code and datasets are available at https://github.com/EsmaeilNourani/TripletProt.},
  keywords={Proteins;Task analysis;Computational modeling;Training;Protein engineering;Feature extraction;Computational efficiency;Protein representation learning;triplet loss;siamese networks},
  doi={10.1109/TCBB.2021.3108718},
  ISSN={1557-9964},
  month={Nov},}@ARTICLE{9237126,
  author={Eckhart, Matthias and Ekelhart, Andreas and Weippl, Edgar},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={Automated Security Risk Identification Using AutomationML-Based Engineering Data}, 
  year={2022},
  volume={19},
  number={3},
  pages={1655-1672},
  abstract={Systems integrators and vendors of industrial components need to establish a security-by-design approach, which includes the assessment and subsequent treatment of security risks. However, conducting security risk assessments along the engineering process is a costly and labor-intensive endeavor due to the complexity of the system(s) under consideration and the lack of automated methods. This, in turn, hampers the ability of security analysts to assess risks pertaining to cyber-physical systems (CPSs) in an efficient manner. In this work, we propose a method that automatically identifies security risks based on the CPS's data representation, which exists within engineering artifacts. To lay the foundation for our method, we present security-focused semantics for the engineering data exchange format AutomationML (AML). These semantics enable the reuse of security-relevant know-how in AML artifacts by means of a formal knowledge representation, modeled with a security-enriched ontology. Our method is capable of automating the identification of security risk sources and potential consequences in order to construct cyber-physical attack graphs that capture the paths adversaries may take. We demonstrate the benefits of the proposed method through a case study and an open-source prototypical implementation. Finally, we prove that our solution is scalable by conducting a rigorous performance evaluation.},
  keywords={Security;IEC Standards;Risk management;Topology;Semantics;Data models;Knowledge engineering;Cyber-physical systems;information security;AutomationML;security modeling;security risk assessment;industrial control systems;IEC 62443},
  doi={10.1109/TDSC.2020.3033150},
  ISSN={1941-0018},
  month={May},}@INPROCEEDINGS{9742010,
  author={Yang, Wansheng and Deng, Fei and Ma, Siyou and Wu, Linbo and Sun, Zhe and Hu, Chi},
  booktitle={2021 IEEE 21st International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
  title={Test Case Reuse Based on Software Testing Knowledge Graph and Collaborative Filtering Recommendation Algorithm}, 
  year={2021},
  volume={},
  number={},
  pages={67-76},
  abstract={As an important role of software test, the reuse of test cases is essential in terms of finding software defects and locating the causes of them. However, the existing related approaches are insufficient to establish an internal relationship between test cases and defects and their abilities to find or diagnose errors are limited. In this paper, an ontology model based on the software testing process is applied to establish a software testing knowledge graph, which serves as the foundation to build an recommendation system. Specifically, the recommendation system takes the functions of software under test as the “user”, and the defect-occurrence-chain which establishes the correlation between test cases and defects in the knowledge graph as the “item”. Both of them provide the evidence to build collaborative filtering recommendation algorithm based on the user-item scoring matrix. It aims to assist testers in recommending reusable test cases to identify software errors effectively. Against this background, the BERT+Bi-LSTM-CRF model is selected to extract the latent test requirements of the software under test, and an overt variable factorization model is built so as to iteratively optimize the user-item scoring matrix. Further, an empirical study has been conducted, and it is found that the recommended test cases can significantly help testers find software defects faster in a more efficient way, and locate defects more accurately.},
  keywords={Software testing;Collaborative filtering;Software algorithms;Semantics;Software quality;Ontologies;Software;software testing knowledge graph;collaborative filtering;BERT+Bi-LSTM-CRF;defect-occurrence-chain;overt variable factorization model},
  doi={10.1109/QRS-C55045.2021.00020},
  ISSN={2693-9371},
  month={Dec},}@INPROCEEDINGS{9688274,
  author={Kim, Seokhwan and Liu, Yang and Jin, Di and Papangelis, Alexandros and Gopalakrishnan, Karthik and Hedayatnia, Behnam and Hakkani-Tür, Dilek},
  booktitle={2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)}, 
  title={“How Robust R U?”: Evaluating Task-Oriented Dialogue Systems on Spoken Conversations}, 
  year={2021},
  volume={},
  number={},
  pages={1147-1154},
  abstract={Most prior work in dialogue modeling has been on written conversations mostly because of existing data sets. However, written dialogues are not sufficient to fully capture the nature of spoken conversations as well as the potential speech recognition errors in practical spoken dialogue systems. This work presents a new benchmark on spoken task-oriented conversations, which is intended to study multi-domain dialogue state tracking and knowledge-grounded dialogue modeling. We report that the existing state-of-the-art models trained on written conversations are not performing well on our spoken data, as expected. Furthermore, we observe improvements in task performances when leveraging $n$-best speech recognition hypotheses such as by combining predictions based on individual hypotheses. Our data set enables speech-based benchmarking of task-oriented dialogue systems.},
  keywords={Conferences;Benchmark testing;Data models;Robustness;Task analysis;Automatic speech recognition;spoken dialogue systems;dialogue state tracking;knowledge-grounded dialogue generation},
  doi={10.1109/ASRU51503.2021.9688274},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9686335,
  author={Palchunov, Dmitry and Tregubov, A.S.},
  booktitle={2021 International Symposium on Knowledge, Ontology, and Theory (KNOTH)}, 
  title={Semantic methods of intelligent assistant developing}, 
  year={2021},
  volume={},
  number={},
  pages={30-35},
  abstract={Human-computer interaction with people whose visual perception is limited is possible only with tactile and voice interfaces, the latter are being used more and more recently. The aim of the work is to create an intelligent assistant for an indoor navigation system designed for blind and visually impaired people. The development of an intelligent assistant is based on a semantic user model and a four-level ontological model of the subject domain. To build a dialogue between an intelligent assistant and a user, we use the theory of speech acts, argumentation theory and case-based reasoning. The developed software system is aimed at identifying the desires and user needs and proposing possible user actions aimed at achieving them. The system allows for the decomposition of user tasks and the formation of a sequence of their execution based on semantic models of the user and the subject domain.},
  keywords={Knowledge engineering;Uncertainty;Web services;Semantics;Speech recognition;Ontologies;Software systems;intelligent assistant;ontology;machine learning;natural language processing;intent recognition;argumentation theory;case-based reasoning},
  doi={10.1109/KNOTH54462.2021.9686335},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9679873,
  author={Sakhrani, Harsh and Parekh, Saloni and Ratadiya, Pratik},
  booktitle={2021 International Conference on Data Mining Workshops (ICDMW)}, 
  title={Transformer-based Hierarchical Encoder for Document Classification}, 
  year={2021},
  volume={},
  number={},
  pages={852-858},
  abstract={Document Classification has a wide range of applications in various domains like Ontology Mapping, Sentiment Analysis, Topic Categorization and Document Clustering, to mention a few. Unlike Text Classification, Document Classification works with longer sequences that typically contain multiple paragraphs. Previous approaches for this task have achieved promising results, but have often relied on complex recurrence mechanisms that are expensive and time-consuming in nature. Recently, self-attention based models like Transformers and BERT have achieved state-of-the-art performance on several Natural Language Understanding (NLU) tasks, but owing to the quadratic computational complexity of the self-attention mechanism with respect to the input sequence length, these approaches are generally applied to shorter text sequences. In this paper, we address this issue, by proposing a new Transformer-based Hierarchical Encoder approach for the Document Classification task. The hierarchical framework we adopt helps us extend the self-attention mechanism to long-form text modelling thereby reducing the complexity considerably. We use the Bidirectional Transformer Encoder (BTE) at the sentence-level to generate a fixed-size sentence embedding for each sentence in the document. A document-level Transformer Encoder is then used to model the global document context and learn the inter-sentence dependencies. We also carry out experiments with the BTE in a feature-extraction and a fine-tuning setup, allowing us to evaluate the trade-off between computation power and accuracy. Furthermore, we also conduct ablation experiments, and evaluate the impact of different pre-training strategies on the overall performance. Experimental results demonstrate that our proposed model achieves state-of-the-art performance on two standard benchmark datasets.},
  keywords={Training;Sentiment analysis;Computational modeling;Transfer learning;Text categorization;Natural languages;Ontologies;Transformer;Self-attention;Document Classification},
  doi={10.1109/ICDMW53433.2021.00109},
  ISSN={2375-9259},
  month={Dec},}@INPROCEEDINGS{9673037,
  author={Amini, M. Mohammad and Aldanondo, M. and Vareilles, E. and Coudert, T.},
  booktitle={2021 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)}, 
  title={Twenty Years of Configuration Knowledge Modeling Research. Main Works, What To Do Next?}, 
  year={2021},
  volume={},
  number={},
  pages={1328-1332},
  abstract={A configuration software (configurator) associates a knowledge base (KB) with a knowledge processing unit (PU). The KB describes all possible combinations of components while the PU overlays this knowledge with the customer requirements. Our work deals with the KB and the approaches, models, or tools for modeling configuration knowledge. Our goal is to present a small quantitative literature survey highlighting two work streams: the first one gathers modeling works dealing with constraint-based approaches while the second deals with ontologies, description logic, or object-oriented modeling approach. We will also consider hybrid approaches. We will present a quantitative analysis of published materials in Web of science over the last twenty years. The keywords occurrence versus time will also be studied in detail to identify tendencies in configuration knowledge modeling.},
  keywords={Knowledge engineering;Statistical analysis;Object oriented modeling;Engineering management;Knowledge based systems;Ontologies;Maintenance engineering;Configuration knowledge modeling;constraints satisfaction problem;ontology;UML;OWL;rules},
  doi={10.1109/IEEM50564.2021.9673037},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9669634,
  author={An, Ying and Zhang, Haojia and Sheng, Yu and Wang, Jianxin and Chen, Xianlai},
  booktitle={2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={MAIN: Multimodal Attention-based Fusion Networks for Diagnosis Prediction}, 
  year={2021},
  volume={},
  number={},
  pages={809-816},
  abstract={Predicting the future diagnoses from patients’ historical Electronic Health Records (EHR) is a significant task in healthcare. EHR consist of multiple modal data, each modality has different features and contains a wealth of information of patients. However, most of the existing EHR-based prediction methods either only use unimodal data, or fail to fully explore the correlation between different modalities when fusing multimodal data. To address these challenges, we propose a Multimodal Attention-based fusIon Networks (MAIN) for diagnosis prediction. In this model, we first design different feature extraction modules for each modality. Then, an inter-modal correlation module which contains two layers is applied to capture the intermodal correlation. Finally, a multimodal fusion module based on weighted averaging is utilized to integrate the representations derived from different modalities and their correlation to obtain the patient representation for diagnosis prediction. We evaluate our proposed model on two medical datasets, and the experimental results demonstrate the effectiveness of MAIN.},
  keywords={Correlation;Conferences;Medical services;Prediction methods;Ontologies;Feature extraction;Task analysis;Electronic Health Records;diagnosis prediction;multimodal fusion;attention mechanism},
  doi={10.1109/BIBM52615.2021.9669634},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9672080,
  author={Parolin, Erick Skorupa and Hu, Yibo and Khan, Latifur and Osorio, Javier and Brandt, Patrick T. and D’Orazio, Vito},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)}, 
  title={CoMe-KE: A New Transformers Based Approach for Knowledge Extraction in Conflict and Mediation Domain}, 
  year={2021},
  volume={},
  number={},
  pages={1449-1459},
  abstract={Knowledge discovery and extraction approaches attract special attention across industries and areas moving toward the 5V Era. In the political and social sciences, scholars and governments dedicate considerable resources to develop intelligent systems for monitoring, analyzing and predicting conflicts and affairs involving political entities across the globe. Such systems rely on background knowledge from external knowledge bases, that conflict experts commonly maintain manually. The high costs and extensive human efforts associated with updating and extending these repositories often compromise their correctness of. Here we introduce CoMe-KE (Conflict and Mediation Knowledge Extractor) to extend automatically knowledge bases about conflict and mediation events. We explore state-of-the-art natural language models to discover new political entities, their roles and status from news. We propose a distant supervised method and propose an innovative zero-shot approach based on a dynamic hypothesis procedure. Our methods leverage pre-trained models through transfer learning techniques to obtain excellent results with no need for a labeled data. Finally, we demonstrate the superiority of our method through a comprehensive set of experiments involving two study cases in the social sciences domain. CoMe-KE significantly outperforms the existing baseline, with (on average) double of the performance retrieving new political entities.},
  keywords={Knowledge based systems;Social sciences;Transfer learning;Natural languages;Big Data;Transformers;Knowledge discovery;knowledge base construction;knowledge extraction;ontologies;link and graph mining;transfer-learning;natural language processing;web search and mining;semantic-based data mining;CAMEO},
  doi={10.1109/BigData52589.2021.9672080},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9671503,
  author={Zhao, Xintong and Greenberg, Jane and McClellan, Scott and Hu, Yong-Jie and Lopez, Steven and Saikin, Semion K. and Hu, Xiaohua and An, Yuan},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)}, 
  title={Knowledge Graph-Empowered Materials Discovery}, 
  year={2021},
  volume={},
  number={},
  pages={4628-4632},
  abstract={In this position paper, we describe research on knowledge graph-empowered materials science prediction and discovery. The research consists of several key components including ontology mapping, materials data annotation, and information extraction from unstructured scholarly articles. We argue that although big data generated by simulations and experiments have motivated and accelerated the data-driven science, the distribution and heterogeneity of materials science-related big data hinders major advancements in the field. Knowledge graphs, as semantic hubs, integrate disparate data and provide a feasible solution to addressing this challenge. We design a knowledge-graph based approach for data discovery, extraction, and integration in materials science.},
  keywords={Materials science and technology;Vocabulary;Technological innovation;Semantics;Prototypes;Transforms;Big Data;Knowledge Graph;Materials Discovery;Information Extraction;Ontology;Natural Language Processing},
  doi={10.1109/BigData52589.2021.9671503},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9671788,
  author={Eggleston, Chloe and Abramson, Jeremy},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)}, 
  title={Woolery: Extending Frame Semantics to Structured Documents}, 
  year={2021},
  volume={},
  number={},
  pages={5597-5601},
  abstract={This paper presents Woolery, a system for semantic annotation and mapping of structured documents (such as JSON key-value pairs) to FrameNet. Implemented as a graphical interface, Woolery provides an annotator with a guided means to map keys in a JSON document to FrameNet elements, without the need for extensive knowledge of FrameNet's semantic structures. Candidate frame elements are identified via a search across FrameNet's internal representations, or via mapping keys to their potential WordNet synsets. Final element selection is automated via a pretrained language model. Initial results are promising, with the model giving an overall accuracy of 77.8% when labeling frames across a diverse corpus of JSON document schemas.},
  keywords={Annotations;Conferences;Semantics;Big Data;Labeling;FrameNet;natural language processing;annotation;lexical databases;JSON;ontology alignment;computational semantics},
  doi={10.1109/BigData52589.2021.9671788},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9643789,
  author={Jeusfeld, Manfred A. and Frank, Ulrich},
  booktitle={2021 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Unifying multi-level modeling: A position paper}, 
  year={2021},
  volume={},
  number={},
  pages={536-540},
  abstract={Multi-level modeling (MLM) as part of object-oriented modeling aims at fully utilizing the expressive power of multiple abstraction levels. While these levels where initially used to define domain-specific modeling languages, i.e. for linguistic purposes, the MLM community has long argued that there is much more to gain by tapping into ontological abstraction levels. While MLM is a rather specialized research field, there are now quite a number of different proposals. There is thus an opportunity to develop a uniform core of MLM that then possibly can become part of a standard and be taken up by the larger modeling community.},
  keywords={Limiting;Object oriented modeling;Education;Linguistics;Solids;Reflection;Model driven engineering;multi-level modeling;conceptual modeling;research agenda},
  doi={10.1109/MODELS-C53483.2021.00083},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9641021,
  author={K C, Hitha and V K, Kiran},
  booktitle={2021 Fifth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)}, 
  title={Topic Recognition and Correlation Analysis of Articles in Computer Science}, 
  year={2021},
  volume={},
  number={},
  pages={1115-1118},
  abstract={Topic identification and similarity detection are two related essential task in data mining, information retrieval, and bibliometric data analysis, which aims to identify significant topics and to find similarity between text collections.It is an essential activity to identify research papers according to their research topics to enhance their retrievability, help create smart analytics, and promote a range of approaches to evaluating the research environment and making sense of it.The proposed frame work deals with three main steps: text extraction, topic identification, and similarity detection.The PyPDF2 module is used to extract text from pdf file. CSO classifier is used for topic identification and similarity between documents is calculated using different models, such as Tf-Idf, Bert, Glove, Word2vec, and Doc2vec.and compared these models with respect to cosine similarity and Eucleadian distance obtained from these models.},
  keywords={Semantic search;Computational modeling;Manuals;Euclidean distance;Ontologies;Syntactics;Portable document format;PyPDF2 module;CSO Classifier;Tf-Idf;Bert;Glove;Word2Vec;Doc2Vec},
  doi={10.1109/I-SMAC52330.2021.9641021},
  ISSN={2768-0673},
  month={Nov},}
@INPROCEEDINGS{9620493,
  author={Lyadova, Lyudmila N. and Sukhov, Alexander O. and Nureev, Marsel R.},
  booktitle={2021 IEEE 15th International Conference on Application of Information and Communication Technologies (AICT)}, 
  title={An Ontology-Based Approach to the Domain Specific Languages Design}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Developing software systems for various domains is a complex task. The quality of the system, corresponding to the domain requirements, can only be achieved via involving the model development of experts in the relevant fields. Traditional design methods based on the using professional tools and modeling languages are difficult for subject matter experts. Using Domain Specific Languages (DSL) have been increasingly gaining attention of developers because DSLs are created to cope with specific domain particularities. However, DSL development consists of several steps to be performed can be hard. Identifying the correct set of elements and constructions of DSL, defining their constraints can be very error-prone. Automation of the new DSLs development is relevant task. The designing of new DSLs should be based on the knowledge of experts, which can be represented using an ontology. An approach to DSM platform development based on using multifaceted ontology to DSL design is proposed. Examples of DSLs and models illustrating the applicability of the proposed methodology are described.},
  keywords={Natural languages;Prototypes;Metamodeling;Ontologies;Tools;Software systems;DSL;domain specific modeling;DSM;domain specific language;DSL;visual language;metamodeling;DSM platform;language toolkits;metamodel generation;multifaceted ontology},
  doi={10.1109/AICT52784.2021.9620493},
  ISSN={2472-8586},
  month={Oct},}@ARTICLE{9623547,
  author={Loubach, Denis S. and Bonna, Ricardo and Ungureanu, George and Sander, Ingo and Söderquist, Ingemar},
  journal={IEEE Access}, 
  title={Classification and Mapping of Model Elements for Designing Runtime Reconfigurable Systems}, 
  year={2021},
  volume={9},
  number={},
  pages={156337-156360},
  abstract={Embedded systems are ubiquitous and control many critical functions in society. A fairly new type of embedded system has emerged with the advent of partial reconfiguration, i.e. runtime reconfigurable systems. They are attracting interest in many different applications. Such a system is capable of reconfiguring itself at the hardware level and without the need to halt the application’s execution. While modeling and implementing these systems is far from a trivial task, there is currently a lack of systematic approaches to tackle this issue. In other words, there is no unanimously agreed upon modeling paradigm that can capture adaptive behaviors at the highest level of abstraction, especially when regarding the design entry, namely, the initial high-level application and platform models. Given this, our paper proposes two domain ontologies for application and virtual platform models used to derive a classification system and to provide a set of rules on how the different model elements are allowed to be composed together. The application behavior is captured through a formal model of computation which dictates the semantics of execution, concurrency, and synchronization. The main contribution of this paper is to combine suitable formal models of computation, a functional modeling language, and two domain ontologies to create a systematic design flow from an abstract executable application model into a virtual implementation model based on a runtime reconfigurable architecture (virtual platform model) using well-defined mapping rules. We demonstrate the applicability, generality, and potential of the proposed model element classification system and mapping rules by applying them to representative and complete examples: an encoder/decoder system and an avionics attitude estimation system. Both cases yield a virtual implementation model from an abstract application model.},
  keywords={Computational modeling;Runtime;Unified modeling language;Ontologies;Embedded systems;Adaptation models;Hardware;Embedded systems;runtime reconfiguration;models of computation (MoC);domain ontology;mapping rules},
  doi={10.1109/ACCESS.2021.3129899},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9610664,
  author={Kiv, Soreangsey and Heng, Samedi and Wautelet, Yves and Kolp, Manuel},
  booktitle={2021 IEEE 23rd Conference on Business Informatics (CBI)}, 
  title={Towards a Systematic Socio-Intentional Framework for Agile Methods Tailoring}, 
  year={2021},
  volume={02},
  number={},
  pages={143-152},
  abstract={Agile has become one of the most popular software development approaches thanks to its flexible and evolutive features. To find further suitable practices, teams start to follow the tailoring approach by choosing only the fragments of different methods that fit their needs and context. Many tailoring approaches have been proposed by orienting different aspects such as process, resource and goal. While the interaction between team members is very important in agile methods, none of these approaches focuses on the socio-intentional aspect. In the literature, we can find many case studies that link socio-intentional aspects to the tailoring of agile practices. Even though it is helpful to know it, locating relevant information can be effort and time-consuming. This research proposes a socio-intentional framework that can analyze agile practices and indicate how to tailor them with the help of an evidence-based tool and a modeling language. This framework will allow practitioners to identify the right practices to achieve their goals and analyze their suitability and vulnerability. It will also indicate how to successfully implement them in the software development process.},
  keywords={Analytical models;Visualization;Systematics;Focusing;Tools;Software;Planning;Evidence-based System;Ontology;Tailoring Agile;Socio-intentional Modeling},
  doi={10.1109/CBI52690.2021.10065},
  ISSN={2378-1971},
  month={Sep.},}@INPROCEEDINGS{9582510,
  author={Odukoya, Kofoworola Adebowale and Whitfield, Robert Ian and Hay, Laura and Harrison, Neil and Robb, Malcolm},
  booktitle={2021 IEEE International Symposium on Systems Engineering (ISSE)}, 
  title={An Architectural Description For The Application Of Mbse In Complex Systems}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={The design of a complex warship is a multidisciplinary effort which often encounters major challenges, particularly with respect to integration across interfaces in the System of Systems (SoS). In principle, the goal of Model Based Systems Engineering (MBSE) with respect to system design is to provide a means of capturing and communicating the system design in a structured, consistent, and coherent fashion; that can be easily assessed by engineering teams and quickly analysed using queries and toolsets. The focus of this paper is to investigate the potential to achieve a consistent description, identify a viable methodology that minimises mismatch in requirements and to avoid an extended design lifecycle. This study highlights the need to develop a generic Architectural Description (AD) that is based on a common ontology which would clearly define the fundamental tenets of applying state-of-the-art Architectural Frameworks (AFs) in naval ship design. An investigation on the effectiveness and accuracy of a graph-based approach is needed to assess whether it is possible to create a ‘Rosetta stone’ for AFs, which links any two or more different model viewpoints in different AF’s using the approach.},
  keywords={Analytical models;Ontologies;Stakeholders;Marine vehicles;Complex systems;System analysis and design;System of systems;Systems architecture;System of systems;Complex systems1},
  doi={10.1109/ISSE51541.2021.9582510},
  ISSN={2687-8828},
  month={Sep.},}@INPROCEEDINGS{9582475,
  author={Mandel, Constantin and Böning, Jannis and Behrendt, Matthias and Albers, Albert},
  booktitle={2021 IEEE International Symposium on Systems Engineering (ISSE)}, 
  title={A Model-Based Systems Engineering Approach to Support Continuous Validation in PGE - Product Generation Engineering}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Increasing customer demands, especially regarding functionality, safety and environmental sustainability, are major drivers of nowadays product development processes. Those increasing demands as well as today’s product development context of distributed interdisciplinary development teams lead to an increasing complexity of systems as well as their respective development processes. MBSE - Model-Based Systems Engineering is regarded as a promising approach to cope with this complexity. MBSE aims at supporting during the whole product lifecycle in system analysis, requirements management, design as well as verification and validation. Especially validation plays a central role in product development as it is the only activity that can ensure customer satisfaction and thus a successful product on the market. However, comprehensive MBSE-approaches to support validation in product development seem to be missing. This paper describes such a MBSE approach to support validation in product development. The approach includes an ontology of terms and their interrelations in the context of validation. The ontology is used to construct viewpoints, views and a modeling framework to structure a system model in the understanding of MBSE. In addition, a modeling method interacting with the constructed views is developed and presented. The presented approach aims at enabling a continuous validation concept, starting in the early phase of PGE - Product Generation Engineering and continuing throughout the entire lifecycle. Furthermore, the approach should support in integrating the development of products and appropriate validation systems, creating a consistent traceability of information throughout the created models. Finally, a specific focus of the approach lies on usability in order to guarantee individual and organizational acceptance. This acceptance is of particular importance to realize a human centered development as it is envisioned in approaches such as ASE - Advanced Systems Engineering.},
  keywords={Requirements management;Green products;Customer satisfaction;Ontologies;Product development;Complexity theory;Safety;MBSE;Validation;Continuous Validation Concept;Modeling Framework;PGE;Product Generation Engineering},
  doi={10.1109/ISSE51541.2021.9582475},
  ISSN={2687-8828},
  month={Sep.},}@INPROCEEDINGS{9534398,
  author={Ahmad, Zishan and Ekbal, Asif and Sengupta, Shubhashis and Maitra, Anutosh and Ramnani, Roshni and Bhattacharyya, Pushpak},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Unsupervised Approach for Knowledge-Graph Creation from Conversation: The Use of Intent Supervision for Slot Filling}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={In this paper, we propose an unsupervised approach for knowledge graph (KG) creation from conversational data. We make use of intent classification and slot-filling, the two important components of any dialogue agent, exploit their interconnectedness, and finally construct a KG. We build a supervised intent classifier to extract the intent classes, and then on top of this we run our occlusion based slot-information extraction algorithm. Our algorithm is able to make use of supervised training of intent classifiers for extracting the relevant slot-information in an unsupervised way. To test the effectiveness of our system, we perform both automatic and manual evaluation of our intent-classifier and slot-filling system on three dialog datasets. Finally, we construct a knowledge graph from the dialogue conversation using an algorithm that makes use of our occlusion based slot-information extraction module. Empirical evaluation shows that our occlusion based method is able to successfully extract slot information from conversations, resulting in a high-quality KG.},
  keywords={Training;Codes;Neural networks;Bit error rate;Manuals;Information retrieval;Filling;Deep-learning;Unsupervised;Style-transfer;Text generation},
  doi={10.1109/IJCNN52387.2021.9534398},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9534250,
  author={Choudhary, Rishabh and Doboli, Simona and Minai, Ali A.},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={A Comparative Study of Methods for Visualizable Semantic Embedding of Small Text Corpora}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Text embedding has recently emerged as a very useful and successful method for semantic representation. Following initial word-level embedding methods such as Latent Semantic Analysis (LSA) and topic-based bag-of-words approaches like Latent Dirichlet Allocation (LDA), the focus has turned to language models and text encoders implemented as neural networks - ranging from word-level models to those embedding whole documents. The distinctive feature of these models is their ability to infer semantic spaces at all levels based purely on data, with no need for complexities such as syntactic analysis or ontology building. Many of these models are available pre-trained on enormous amounts of data, providing downstream applications with general-purpose semantic spaces. In particular, embedding models at the sentence level or higher are most useful in applications because the meaning of text only becomes clear at that level. Most text embedding methods produce text embeddings in high-dimensional spaces, with a dimensionality ranging from a few hundred to thousands. However, it is often useful to visualize semantic spaces in very low dimension, which requires the use of dimensionality reduction methods. It is not clear what language models and what method of dimensionality reduction would work well in these cases. In this paper, we compare four text embedding methods in combination with three methods of dimensionality reduction to map three related real-world datasets comprising textual descriptions of items in a particular domain (sports) to a 2-dimensional semantic visualization space. The results provide several insights into the utility of these methods for data of this type.},
  keywords={Dimensionality reduction;Training;Analytical models;Visualization;Semantics;Neural networks;Bit error rate;semantic spaces;text embedding;language models;semantic visualization},
  doi={10.1109/IJCNN52387.2021.9534250},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9525789,
  author={Weerakoon, Charmy and Ranathunga, Surangika},
  booktitle={2021 Moratuwa Engineering Research Conference (MERCon)}, 
  title={Question Classification for the Travel Domain using Deep Contextualized Word Embedding Models}, 
  year={2021},
  volume={},
  number={},
  pages={573-578},
  abstract={Question answering can be considered as a key area in Natural Language Processing and Information Retrieval, where users construct queries in natural language and receive suitable answers in return. In the travel domain, most questions are “content questions”, where the expected answer is not the equivalent of “yes” or “no”, but rather factual information. Replying to a free-form factual question based on a large collection of text is challenging. Previous research has shown that the accuracy of question answering systems can be improved by adding a classification phase based on the expected answer type. This paper focuses on implementing a multi-level, multi-class question classification system focusing on the travel domain. Existing research for the travel domain is conducted using language-specific features and traditional Machine Learning models. In contrast, this research employs transformer-based state-of-the-art deep contextualized word embedding models for question classification. The proposed method improves the coarse class Micro F1-Score by 5.43% compared to the baseline. Fine-grain Micro F1-Score has also improved by 3.8%. We also present an empirical analysis of the effectiveness of different transformer-based deep contextualized word embedding models for multi-level multi-class classification.},
  keywords={Analytical models;Focusing;Machine learning;Transformers;Knowledge discovery;Information retrieval;Natural language processing;question classification;expected answer type;ontology learning;transformers;RoBERTa},
  doi={10.1109/MERCon52712.2021.9525789},
  ISSN={2691-364X},
  month={July},}@INPROCEEDINGS{9529829,
  author={Neji, Sameh and Chenaina, Tarek and Shoeb, Abdullah M. and Ben Ayed, Leila},
  booktitle={2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={HIR: A Hybrid IR Ranking Model}, 
  year={2021},
  volume={},
  number={},
  pages={1717-1722},
  abstract={The aim of information retrieval (IR) process is to respond effectively to user queries by retrieving information that are better meets their expectations. A gap could exist between user information needs and his/her defined context as the level of the user's expertise in the search domain is directly influencing the query richness. The information retrieval system (IRS) must be intelligent enough to identify information needs and respond effectively to meet the expected needs, regardless the level of user's expertise level. This process is difficult and it remains a major and open challenge in the domain of IR. IR models integrate many sources to achieve an effective retrieval system. Semantic IR is an environment in which semantic techniques were applied to sort the documents according to their degree of relevance to the query. The present work proposes a hybrid model to rank documents. The proposed model is based on a query likelihood language model and the semantic similarity between concepts to assess the relevance between query-document pairs. Concepts were extracted by projection on WordNet ontology then word sense disambiguation was conducted. A semantic index was built to validate the proposed model. The conducted empirical experiments show that the proposed model is outperformed the compared benchmarks in the measured IR metrics.},
  keywords={Measurement;Semantic search;Computational modeling;Semantics;Ontologies;Benchmark testing;Information retrieval;semantic information retrieval;query-document relevance;language model;conceptual model;semantic similarity},
  doi={10.1109/COMPSAC51774.2021.00256},
  ISSN={0730-3157},
  month={July},}@ARTICLE{9524621,
  author={Cho, Hyejin and Choi, Dongha and Lee, Hyunju},
  journal={IEEE Access}, 
  title={Re-Ranking System with BERT for Biomedical Concept Normalization}, 
  year={2021},
  volume={9},
  number={},
  pages={121253-121262},
  abstract={In recent years, various neural network architectures have been successfully applied to natural language processing (NLP) tasks such as named entity normalization. Named entity normalization is a fundamental task for extracting information in free text, which aims to map entity mentions in a text to gold standard entities in a given domain-specific ontology; however, the normalization task in the biomedical domain is still challenging because of multiple synonyms, various acronyms, and numerous lexical variations. In this study, we regard the task of biomedical entity normalization as a ranking problem and propose an approach to rank normalized concepts. We additionally employ two factors that can notably affect the performance of normalization, such as task-specific pre-training (Task-PT) and calibration approach. Among five different biomedical benchmark corpora, our experimental results show that our proposed model achieved significant improvements over the previous methods and advanced the state-of-the-art performance for biomedical entity normalization, with up to 0.5% increase in accuracy and 1.2% increase in F-score.},
  keywords={Task analysis;Biological system modeling;Bit error rate;Unified modeling language;Dictionaries;Conferences;Context modeling;Named entity normalization;natural language processing;text mining;text recognition},
  doi={10.1109/ACCESS.2021.3108445},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9507178,
  author={Vlasenko, Sergey V.},
  booktitle={2021 XXIV International Conference on Soft Computing and Measurements (SCM)}, 
  title={Usage Features of Semantic Query and Rule Languages of Semantic Web in the Intelligent Systems, Based on Conceptual Graphs Technologies}, 
  year={2021},
  volume={},
  number={},
  pages={120-123},
  abstract={The article considers some problems related to applying languages of semantic query and rules, oriented towards their application within Semantic Web, in the information systems, based on conceptual graphs techniques and knowledge models of the appropriate type. At this, special attention is paid to the analysis of given languages interpretation correctness, as well as technological aspects of organizing for processing chosen-class knowledge models.},
  keywords={Semantic Web;ISO Standards;Semantics;W3C;Tools;Software;Regulation;intelligent systems;knowledge models;Semantic Web;conceptual graphs},
  doi={10.1109/SCM52931.2021.9507178},
  ISSN={},
  month={May},}@INPROCEEDINGS{9497473,
  author={Wu, Shouxuan and Lu, Jinzhi and Hu, Zhenchao and Yang, Pengfei and Wang, Guoxin and Kiritsis, Dimitris},
  booktitle={2021 16th International Conference of System of Systems Engineering (SoSE)}, 
  title={Cognitive Thread Supports System of Systems for Complex System Development}, 
  year={2021},
  volume={},
  number={},
  pages={82-87},
  abstract={Model-based Systems Engineering (MBSE) has been widely used in the development of complex systems. The system architectures, organizations, research and development processes using MBSE to design complex systems can be seen as a System of Systems (SoS), which has high complexity and hard to manage. The concept of digital thread is proposed to integrate all the models and data in the SoS. However, lack of cognition ability makes it hard to connect the models and data with human, processes and things in the SoS, which reduces the efficiency of complex system development. In this paper, a new concept named Cognitive Thread is first proposed as digital thread with augmented semantic capabilities for identifying the information of the SoS. Then a cognitive thread construction approach based on Open Services for Lifecycle Collaboration (OSLC) specification and knowledge graphs is proposed to support decision-making and management in the SoS. Finally, the feasibility of the proposed approach is verified through a case study of the advanced driver-assistance system development.},
  keywords={Instruction sets;Semantics;Systems architecture;Organizations;Cognition;Data models;Natural language processing;Digital Thread;Cognitive Thread;OSLC specification;System of Systems;Model-based Systems Engineering},
  doi={10.1109/SOSE52739.2021.9497473},
  ISSN={},
  month={June},}@INPROCEEDINGS{9496030,
  author={Alisa, Gisina and Dmitry, Devyatkin and Anton, Lukin and Alexey, Lupatov and Alexey, Molodchenkov and Irina, Kholodenko},
  booktitle={2021 IEEE Ural-Siberian Conference on Computational Technologies in Cognitive Science, Genomics and Biomedicine (CSGB)}, 
  title={Method for Biomedical Information Extraction of Immunosuppressive Cell Properties}, 
  year={2021},
  volume={},
  number={},
  pages={210-213},
  abstract={Automated extraction of cell populations' immunosuppressive properties from research articles is a basic problem, which requires specialized methods and tools for meta-analysis of publications. It is necessary to extract information about specific types of cells, their roles in the text, detect the immunosuppressive properties of cell populations, and certain types of relationships. It is also crucial to filter out those texts, which describe immunosuppressive features of different chemical compounds or drugs. Typically, efficient automatic information extraction requires a relatively large set of samples or marked-up texts. The paper presents a novel information extraction method that can be useful for such an analysis. This method uses external linguistic resources and can be trained on limited corpora. Namely, the method combines medical ontologies, rich unsupervised lexis representations (Fasttext word embeddings) with rule-based entity and relationship extraction, and supervised machine learning-based post-filtering. The developed method allows one to extract information about the target cells (for “in vitro” experiments), effector cells, diseases ("in vivo”), and arbitrary descriptions of immune suppression. In the paper, we also present a manually labeled corpus to train immunosuppressive information extraction methods. That corpus contains texts of 330 PubMed Central abstracts. The experiments on that corpus show the method has relatively high evaluation scores on the labeled dataset. Therefore, the proposed method makes it possible to identify descriptions of the immunosuppressive properties of cell populations in biomedical texts with sufficiently high quality. In the future, the method can be applied to perform an automatic meta-analysis of research in immunosuppressive cell therapy.},
  keywords={Sociology;Medical treatment;Genomics;Linguistics;Tools;Ontologies;Information filters;biomedical information extraction;relational-situational analysis;properties of immunosuppressive cells},
  doi={10.1109/CSGB53040.2021.9496030},
  ISSN={},
  month={May},}@INPROCEEDINGS{9447088,
  author={Giachetti, Ronald E. and Vaneman, Warren},
  booktitle={2021 IEEE International Systems Conference (SysCon)}, 
  title={Requirements for a System Model in the Context of Digital Engineering}, 
  year={2021},
  volume={},
  number={},
  pages={1-7},
  abstract={The vision of achieving digital engineering in the US Department of Defense has instigated work on defining the information content and structure of the system model. However, few seem to have asked what are the requirements for the system model? In this paper, we use a requirements process to elicit and define the requirements for the system model. The system model is a digital artifact containing descriptions of all the essential objects, their properties, and the relationships between them for the system-of-interest (SoI). The paper describes the context of the system model in relationships to the other components of model-based systems engineering (MBSE) consisting of a modeling language, schema, model-based process, presentation framework, MBSE tools, and knowledgeable workforce. The paper describes how these components interact to provide effective MBSE. Requirements are stated for each component. The paper additionally derives information requirements for the system model according to the systems engineering process’s information needs by examining the inputs and outputs of each activity in the systems engineering process. Lastly, the paper derives the quality characteristics for the system model from the literature on ontologies, modeling languages, and semiotics. The result is a set of requirements for the system model to support MBSE and the digital thread.},
  keywords={Context;Conferences;Project management;Tools;Ontologies;IEEE Standards;Data models},
  doi={10.1109/SysCon48628.2021.9447088},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{9435777,
  author={Sarsembayeva, Talshyn and Mansurova, Madina and Chikibayeva, Darya and Karymsakova, Dariya},
  booktitle={2020 IEEE 8th Workshop on Advances in Information, Electronic and Electrical Engineering (AIEEE)}, 
  title={The Problem of Named Entities Unification based on Geographical Ontologies}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={The subject of this research is to develop a system for extracting knowledge from both semi-structured and unstructured data and filling with this system a knowledge base that would provide support for decision-making on any problematic issues. The article deals with the problem of unification of named entities based on geographical ontologies.},
  keywords={Knowledge engineering;Decision making;Knowledge based systems;Machine learning;Ontologies;Information retrieval;Data models;semi-structured and unstructured data;decision support system;ontology;thesaurus;information extraction;knowledge extraction;knowledge base;machine learning;named entities},
  doi={10.1109/AIEEE51419.2021.9435777},
  ISSN={2689-7342},
  month={April},}@ARTICLE{9389549,
  author={Zeng, Kungan and Paik, Incheon},
  journal={IEEE Access}, 
  title={Semantic Service Clustering With Lightweight BERT-Based Service Embedding Using Invocation Sequences}, 
  year={2021},
  volume={9},
  number={},
  pages={54298-54309},
  abstract={Service clustering is an efficient method for facilitating service discovery and composition. Traditional approaches based on the self-description documents for services usually utilize service signatures. In Web service composition, service clustering can also be performed by the invocation relationship between services. Therefore, based on the successful development of several embedding techniques for words in several contexts, a novel deep learning-based service embedding using invocation sequences is devised for service clustering. Moreover, many microservices are being created because of the rapid development of the Internet of Things (IoT), and edge, and fog computing. Following these developments, Web service composition based on these environments has emerged in abundance. More efficient lightweight approaches to analyze large numbers of services are necessary for service clustering. Consequently, a lightweight deep learning-based approach for the semantic clustering of service composition is presented to address these requirements. In this paper, we first propose the concept of service embedding to capture semantic information from invocation sequences. Second, we suggest using state-of-the-art neural language sequence models for service embedding and develop a corresponding lightweight Bidirectional Encoder Representations of Transformers (BERT)-based model. Next, combined with K-means clustering, the semantic clustering of service composition is evaluated. Finally, the experimental results show that the clustering process can be effectively performed by the lightweight BERT-based model.},
  keywords={Web services;Semantics;Computational modeling;Feature extraction;Ontologies;Deep learning;Data mining;Semantic service clustering;service embedding;composition;lightweight BERT},
  doi={10.1109/ACCESS.2021.3069509},
  ISSN={2169-3536},
  month={},}@ARTICLE{9351987,
  author={Morales-Garzón, Andrea and Gómez-Romero, Juan and Martin-Bautista, Maria J.},
  journal={IEEE Access}, 
  title={A Word Embedding-Based Method for Unsupervised Adaptation of Cooking Recipes}, 
  year={2021},
  volume={9},
  number={},
  pages={27389-27404},
  abstract={Studying food recipes is indispensable to understand the science of cooking. An essential problem in food computing is the adaptation of recipes to user needs and preferences. The main difficulty when adapting recipes is in determining ingredients relations, which are compound and hard to interpret. Word embedding models can catch the semantics of food items in a recipe, helping to understand how ingredients are combined and substituted. In this work, we propose an unsupervised method for adapting ingredient recipes to user preferences. To learn food representations and relations, we create and apply a specific-domain word embedding model. In contrast to previous works, we not only use the list of ingredients to train the model but also the cooking instructions. We enrich the ingredient data by mapping them to a nutrition database to guide the adaptation and find ingredient substitutes. We performed three different kinds of recipe adaptation based on nutrition preferences, adapting to similar ingredients, and vegetarian and vegan diet restrictions. With a 95% of confidence, our method can obtain quality adapted recipes without a previous knowledge extraction on the recipe adaptation domain. Our results confirm the potential of using a specific-domain semantic model to tackle the recipe adaptation task.},
  keywords={Adaptation models;Task analysis;Ontologies;Databases;Computational modeling;Data models;Vocabulary;Data mapping;food computing;natural language processing;recipe adaptation;word embedding},
  doi={10.1109/ACCESS.2021.3058559},
  ISSN={2169-3536},
  month={},}@ARTICLE{9349177,
  author={Amato, Flora and Cozzolino, Giovanni and Moscato, Francesco and Moscato, Vincenzo and Xhafa, Fatos},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={A Model for Verification and Validation of Law Compliance of Smart Contracts in IoT Environment}, 
  year={2021},
  volume={17},
  number={11},
  pages={7752-7759},
  abstract={The interest of Industry 4.0 in smart contracts and blockchain technologies is growing up day by day. Smart contracts have enabled new kinds of interactions whereby contractors can even fully automate processes they agree on. This technology is really appealing in Internet of Things (IoT) domain because smart devices generate events for software agents involved in a smart contract execution, making full automation possible. However, smart contracts have to comply with national and international laws and accountability of participant's actions. Soundness of a smart contract has to be verified in terms of law compliance. Here, we propose a model for verification and validation of law compliance of smart contracts in IoT environments. The main goal of this article is to propose a formal model (based on multiagent logic and ontological description of contracts) for validating law compliance of smart contracts and to determine potential responsibilities of failures.},
  keywords={Unified modeling language;Law;Internet of Things;Insurance;Automobiles;Analytical models;Smart contracts;Blockchain;industry 4.0;Internet of Things (IoT);multiagent systems;smart contracts},
  doi={10.1109/TII.2021.3057595},
  ISSN={1941-0050},
  month={Nov},}@ARTICLE{9335254,
  author={Balaraman, Vevake and Magnini, Bernardo},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Domain-Aware Dialogue State Tracker for Multi-Domain Dialogue Systems}, 
  year={2021},
  volume={29},
  number={},
  pages={866-873},
  abstract={In task-oriented dialogue systems the dialogue state tracker component (DST) is responsible for predicting the current state of the dialogue based on the dialogue history and the user utterance. Current DST approaches rely on a predefined domain ontology, a fact that limits their effective usage for large scale conversational agents, where the DST constantly needs to be interfaced with ever-increasing services and APIs. Focused towards overcoming this drawback, we propose a domain-aware dialogue state tracker, that is completely data-driven and it is modeled to predict for dynamic service schemas, including zero-shot domains. Unlike approaches that propose separate models for prediction of intents, requested slots, slot status, categorical slots and non-categorical slots, we propose a single model in an end-to-end architecture. The proposed model utilizes domain and slot information to extract both domain and slot specific representations from a given dialogue, and then uses such representations to predict the values of the corresponding slot in a given domain. Integrating this mechanism with pretrained language models, our approach can effectively learn semantic relations and effectively perform transfer learning between domains or zero-shot tracking for domains not present in training.},
  keywords={Predictive models;Bit error rate;Task analysis;Ontologies;Virtual assistants;Semantics;Training data;Dialogue state tracking;multi-domain dialogue systems;zero-shot tracking;end-to-end},
  doi={10.1109/TASLP.2021.3054309},
  ISSN={2329-9304},
  month={},}@INPROCEEDINGS{10186650,
  author={Chondamrongkul, Nacha and Sun, Jing and Warren, Ian and Lee, Scott Uk-Jin},
  booktitle={2020 IEEE/ACM 8th International Conference on Formal Methods in Software Engineering (FormaliSE)}, 
  title={Semantic-based Architecture Smell Analysis}, 
  year={2020},
  volume={},
  number={},
  pages={109-118},
  abstract={Software smells have negative impacts on the reliability and modifiability of software systems. The smells in architecture design can be cascaded down to the implementation level and cause issues that require much effort to fix. Therefore, early detection of the architecture smells can benefit the overall quality of the software system. This paper presents an integration of methods that formally define the software architecture design towards architecture smell detection. Our approach serves as a framework that allows the architectural structures and behaviours to be formally analysed based on a coherent technique. We evaluated the accuracy and performance of our approach with the models generated from open source projects. The results show that our approach is effective and functions well.},
  keywords={Software architecture;OWL;Computer architecture;Ontologies;Model checking;Software systems;Cognition;Architecture Smells;Software Architecture;Ontology Web Language;Model Checking;Smell Detection},
  doi={},
  ISSN={2575-5099},
  month={May},}@INBOOK{9822105,
  author={Dori, Dov and Kohen, Hanan and Jbara, Ahmad and Wengrowicz, Niva and Lavi, Rea and Soskin, Natali Levi and Bernstein, Kfir and Shani, Uri},
  booktitle={Systems Engineering in the Fourth Industrial Revolution: Big Data, Novel Technologies, and Modern Systems Engineering}, 
  title={OPCloud: An OPM Integrated Conceptual‐Executable Modeling Environment for Industry 4.0}, 
  year={2020},
  volume={},
  number={},
  pages={243-271},
  abstract={This chapter presents Methodical Approach to Executable Integrated Modeling (MAXIM) and its implementation environment, OPCloud. The MAXIM framework enables concurrent modeling of the hardware and software system aspects, avoiding the need to make the painful and information‐leaking transition from the abstract, qualitative conceptual system architecting stage to the concrete, detailed, quantitative design stage. The MAXIM environment aims to overcome the widening hardware‐software modeling gap, stepping toward bringing systems engineering and software engineering closer together. OPCloud is revolutionary in that it is the first and only modeling environment that enables modeling systems not just conceptually; the same environment also provides the modeler with the ability to proceed with detailed, quantitative design that is integrated into the qualitative model. The chapter discusses the sharing of model data and describes collaboration facilities as built into OPCloud.},
  keywords={Modeling;Hardware;Software;Fourth Industrial Revolution;Systems engineering and theory;Software engineering;Computational modeling},
  doi={10.1002/9781119513957.ch11},
  ISSN={},
  publisher={Wiley},
  isbn={9781119513940},
  url={https://ieeexplore.ieee.org/document/9822105},}@INPROCEEDINGS{9378066,
  author={Stojanov, Riste and Kocev, Ilija and Gramatikov, Sasho and Popovski, Gorjan and Koroušić Seljak, Barbara and Eftimov, Tome},
  booktitle={2020 IEEE International Conference on Big Data (Big Data)}, 
  title={Toward Robust Food Ontology Mapping}, 
  year={2020},
  volume={},
  number={},
  pages={3596-3601},
  abstract={Data normalization methodologies are extremely welcome to link extracted information from textual data to different semantic resources. These methodologies have been previously well researched especially in the biomedical domain, where health concepts were normalized and described using semantic tags. Recently, a methodology for normalizing food concepts has been proposed, based on Named-Entity Recognition methods resulting in the FoodOntoMap semantic resource. In this paper, we propose and evaluate a new architecture for linking phrases (i.e. textual name for foods) to concepts from semantic resources in the Food and Nutrition domain. We represent the food phrases (i.e. their textual name) in continuous vector space using state-of-the-art Natural Language Processing (NLP) embedding algorithms, and evaluate their proximity with respect to the annotated semantic food concepts. Additionally, indexing was incorporated to improve efficiency.The GloVe embedding with mean pooling provided best evaluation results, with maximum recall of 74% for the Snomed CT semantic dataset, which is promising result, but also opens a space for future improvement of the phrase representations, and their incorporation in this system.},
  keywords={Semantics;Taxonomy;Ontologies;Big Data;Syntactics;Natural language processing;Indexing;Natural Language Processing;Text representation;Embeddings;Data normalization and linking},
  doi={10.1109/BigData50022.2020.9378066},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9340905,
  author={Jiang, Chen and Dehghan, Masood and Jagersand, Martin},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Understanding Contexts Inside Robot and Human Manipulation Tasks through Vision-Language Model and Ontology System in Video Streams}, 
  year={2020},
  volume={},
  number={},
  pages={8366-8372},
  abstract={Manipulation tasks in daily life, such as pouring water, unfold through human intentions. Being able to process contextual knowledge from these Activities of Daily Living (ADLs) over time can help us understand manipulation intentions, which are essential for an intelligent robot to transition smoothly between various manipulation actions. In this paper, to model the intended concepts of manipulation, we present a vision dataset under a strictly constrained knowledge domain for both robot and human manipulations, where manipulation concepts and relations are stored by an ontology system in a taxonomic manner. Furthermore, we propose a scheme to generate a combination of visual attentions and an evolving knowledge graph filled with commonsense knowledge. Our scheme works with real-world camera streams and fuses an attention-based Vision-Language model with the ontology system. The experimental results demonstrate that the proposed scheme can successfully represent the evolution of an intended object manipulation procedure for both robots and humans. The proposed scheme allows the robot to mimic human-like intentional behaviors by watching real-time videos. We aim to develop this scheme further for real-world robot intelligence in Human-Robot Interaction.},
  keywords={Visualization;Fuses;Ontologies;Real-time systems;Task analysis;Intelligent robots;Videos},
  doi={10.1109/IROS45743.2020.9340905},
  ISSN={2153-0866},
  month={Oct},}@INPROCEEDINGS{9338531,
  author={Fattouch, Najla and Ben Lahmar, Imen and Boukadi, Khouloud},
  booktitle={2020 IEEE 29th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE)}, 
  title={IoT-aware Business Process: comprehensive survey, discussion and challenges}, 
  year={2020},
  volume={},
  number={},
  pages={100-105},
  abstract={In the last years, the Internet of Things (IoT) know a huge widespread thanks to the increase of the connected objects number. The IoT technology has several benefits that make it among the proliferation technology. The major advantage of this technology is the communication between devices known as Machine-to-Machine (M2M) communication allowing them to be connected without human intervention. Thanks to this advantage, the technology become able to facilitate the people's lives that it become smoother through a seamless cooperation between virtual objects and physical ones. As well as, the IoT sweep various fields (e.g., industry, health) thanks to its capacity to automate tasks.In this setting, a tremendous number of business managers are interesting to integrate the IoT devices into their Business Processes (BPs), known in literature as IoT-aware BP. This integration gives the opportunity to the business managers to avail from the IoT technology in their process through an enhancement of the business performance and an achievement of the business competitiveness. Thus, several researchers competed to identify approaches and methods to integrate the IoT technology within the BP paradigm. In this paper, we present a review of the different proposed approaches that deal with the integration of the IoT technology within the BP. Furthermore, we give in this paper, a rich comparative analysis based on a set of criteria. Finally, we identify some initiatives and challenges in the IoT-aware BP paradigm.},
  keywords={Performance evaluation;Industries;Machine-to-machine communications;Internet of Things;Time factors;Task analysis;Business;IoT-aware Business Process;Internet of Things;Business Process;Industry 4.0},
  doi={10.1109/WETICE49692.2020.00027},
  ISSN={2641-8169},
  month={Sep.},}@INPROCEEDINGS{9256709,
  author={Batista, Leandro and Monsuez, Bruno},
  booktitle={2020 AIAA/IEEE 39th Digital Avionics Systems Conference (DASC)}, 
  title={The conception of a large-scale Systems Engineering environment}, 
  year={2020},
  volume={},
  number={},
  pages={1-7},
  abstract={With the rise of artificial intelligence, it is time to shape the systems engineering tooling environment for the future. In the last decade, we have seen several emerging technologies that will potentially have a great impact in complex systems. These new technologies are expected to cause a disruptive impact not only in the products but also in to the tools used across the whole product life cycle. For this reason, is imperative to perform a critical review of the current systems engineering tooling ecosystem. This assessment should also map the open research problems that could prevent the complete integration of the new technologies into the systems engineering framework. This paper proposes a new architecture for a system engineering environment to operate in large scale projects. The objective of this research is twofold: it will first identify the capabilities for the next generation platform, and secondly, it will evaluate how artificial intelligence applications can be integrated in compliance with DO-330. The concept developed by this research will drive tool design recommendations enabling the use of artificial intelligence driven applications in a systems engineering tooling ecosystem.},
  keywords={Tools;Systems engineering and theory;Modeling;Standards;Ecosystems;Complex systems;Vocabulary;Systems Engineering;MBSE;Artificial Intelligence;DO-330},
  doi={10.1109/DASC50938.2020.9256709},
  ISSN={2155-7209},
  month={Oct},}@INPROCEEDINGS{9256484,
  author={Helmke, Hartmut and Kleinert, Matthias and Ohneiser, Oliver and Ehr, Heiko and Shetty, Shruthi},
  booktitle={2020 AIAA/IEEE 39th Digital Avionics Systems Conference (DASC)}, 
  title={Machine Learning of Air Traffic Controller Command Extraction Models for Speech Recognition Applications}, 
  year={2020},
  volume={},
  number={},
  pages={1-9},
  abstract={Increasing digitization and automation is a widely accepted method to cope with the challenges of constantly increasing air traffic. The analogue communication of air traffic controllers (ATCo) to pilots has been excluded so far from the digitization process. However, the content of this communication is of decisive importance for various automation systems. Although Assistant Based Speech Recognition (ABSR) has recently significantly improved the recognition performance and, therefore, enables the digitization of ATCo-pilot-communication, its adaptation to other airports is a critical and costly process, This is even more important, if ATCos tend to deviate from the published ICAO phraseology: “start reducing to two fifty” instead of “reduce two five zero knots” is just an example. User acceptance requires that these deviations are also correctly recognized. Therefore, this paper presents an approach, which automatically learns a so-called Command Extraction Model from labelled controller utterances. The initial Command Extraction Model without learning only covers 60% of the commands, whereas the automatically learned Command Extraction Model covers more than 98%. With just six hours of training data we could achieve 94%.},
  keywords={Speech recognition;Atmospheric modeling;Adaptation models;Ontologies;Engines;Annotations;Airports;Automatic Speech Recognition;Machine Learning;Annotation;Ontology;Controller Command Extraction Model},
  doi={10.1109/DASC50938.2020.9256484},
  ISSN={2155-7209},
  month={Oct},}@INPROCEEDINGS{9226280,
  author={Novacek, Jan and Kühlwein, Arthur and Reiter, Sebastian and Viehl, Alexander and Bringmann, Oliver and Rosenstiel, Wolfgang},
  booktitle={2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
  title={Lemons: Leveraging Model-Based Techniques to Enable Non-Intrusive Semantic Enrichment in Wireless Sensor Networks}, 
  year={2020},
  volume={},
  number={},
  pages={561-568},
  abstract={The paper presents an efficient approach to the semantic enrichment of measured sensor data in Wireless Sensor Networks (WSNs), by bridging techniques from Model-driven Software Development (MDSD) and Semantic Web Technology (SWT). Our approach reinforces data interoperability, fostering data sharing and reuse, by utilizing SWT. Model-based and type-agnostic configuration reduces the overall effort for WSN setup and maintenance, which are traditionally complex and time-consuming tasks. The presented approach addresses the problem of large-scale WSN management through the application of SWT in WSN configuration and management without requiring expert knowledge. Additionally, we present a generic architecture and an implementation which is also supplemented by hands-on descriptions of an illustrative use case. Our experimental results demonstrate that our model-based approach provides non-intrusive semantic enrichment with sub-millisecond computational overhead, as well as partially automated configuration of WSNs.},
  keywords={Wireless sensor networks;Semantics;Ontologies;Software;Data models;Robot sensing systems;OWL;Internet of Things;Semantic Web of Things;Wireless Sensor Networks;Knowledge-based Engineering;Model-Driven Software Development;DevOps},
  doi={10.1109/SEAA51224.2020.00092},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9223143,
  author={Krungklang, Weerayut and Sinthupinyo, Sukree},
  booktitle={2020 12th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)}, 
  title={An Analysis of Natural Language Text Relating to Thai Criminal Law}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper analyses Thailand's criminal law enforcement in chapter 1, Offenses causing death section category section 288 and 289 of title 10 offenses affecting life and body under the Thai Criminal Code. The first part of this paper is using criminal law domain knowledge and supreme court judgment results, to be the initial domain information and result is the rules that humans can understand. The second part of this paper is bringing training data set from the final judgment to train with deep learning methods. Due to the training set which have severe imbalances, the Synthetic Minority Over-Sampling TEchnique (SMOTE) [1] is used to solve this problem. Models are trained on the training set using unidirectional Long Short-Term Memory (LSTM) [2] networks and bidirectional Long Short-Term Memory (BiLSTM) [3] are type of Recurrent Neural Networks (RNN) [2]. The word embeddings of the dataset can be learned while training a deep neural network. BiLSTM average F1 score is higher than LSTM. Pre-trained word embeddings are then used to make the average F1 score higher than before. Finally, using models to predict online crime news, the highest average probability of each model is selected by using Soft Voting as input to the rules. The test results compared with the predictions of our methods with the opinion of the lawyer, corresponding 76%.},
  keywords={Training;Criminal law;Decision trees;Data models;Feature extraction;Ontologies;Classification algorithms;Criminal law;Thai Supreme Court;Word embedding;Word2Vec;LSTM;BiLSTM;SMOTE;Pre-trained word embeddings;Deepcut;Decision tree;Soft Voting},
  doi={10.1109/ECAI50035.2020.9223143},
  ISSN={},
  month={June},}@INPROCEEDINGS{9218173,
  author={Parvizimosaed, Alireza},
  booktitle={2020 IEEE 28th International Requirements Engineering Conference (RE)}, 
  title={Towards the Specification and Verification of Legal Contracts}, 
  year={2020},
  volume={},
  number={},
  pages={445-450},
  abstract={A contract is a legally binding agreement that expresses high-level requirements of parties in terms of obligations, powers and constraints. Parties' actions influence the status of a contract and shall comply with its clauses. Manual contract monitoring is very laborious in real markets, such as transactive energy, where plenty of complex contracts are running concurrently. Furthermore, liability, right and performance transition through run-time operations such as subcontracting, assignment and substitution complicate contract interpretation. Automation is needed to ensure that contracts respect desirable properties and to support monitoring of compliance and handling of violations. In this thesis research, I propose an innovative ontology that defines fundamental contractual notions (such as the ones mentioned above) and their relationships, on which is built a specification language, called Symboleo, that provides syntax and axiomatic semantics of contracts via first-order logic. Symboleo enables the development of advanced automation tools such as a compliance checker that monitors contracts at runtime, and a model checking verification method that analyzes liveness and safety properties of contracts. This paper reports on the problem domain, research method, current status, expected contributions, and main foreseen challenges.},
  keywords={Contracts;Law;Ontologies;Monitoring;Tools;Legal Contract;Specification Language;Model Checking;Smart Contract;Ontology},
  doi={10.1109/RE48521.2020.00066},
  ISSN={2332-6441},
  month={Aug},}@INPROCEEDINGS{9216770,
  author={Jiang, Chen and Jagersand, Martin},
  booktitle={2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)}, 
  title={Bridging Visual Perception with Contextual Semantics for Understanding Robot Manipulation Tasks}, 
  year={2020},
  volume={},
  number={},
  pages={1447-1452},
  abstract={Understanding manipulation scenarios allows intelligent robots to plan for appropriate actions to complete a manipulation task successfully. It is essential for intelligent robots to semantically interpret manipulation knowledge by describing entities, relations and attributes in a structural manner. In this paper, we propose an implementing framework to generate high-level conceptual dynamic knowledge graphs from video clips. A combination of a Vision-Language model and an ontology system, in correspondence with visual perception and contextual semantics, is used to represent robot manipulation knowledge with Entity-Relation-Entity (E-R-E) and Entity-Attribute-Value (E-A-V) tuples. The proposed method is flexible and well-versed. Using the framework, we present a case study where robot performs manipulation actions in a kitchen environment, bridging visual perception with contextual semantics using the generated dynamic knowledge graphs.},
  keywords={Semantics;Ontologies;Manipulator dynamics;Visual perception;Robot kinematics;Task analysis},
  doi={10.1109/CASE48305.2020.9216770},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{9206698,
  author={Sai Sharath, Japa and Banafsheh, Rekabdar},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Question Answering over Knowledge Base using Language Model Embeddings}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={Knowledge Base, represents facts about the world, often in some form of subsumption ontology, rather than implicitly, embedded in procedural code, the way a conventional computer program does. While there is a rapid growth in knowledge bases, it poses a challenge of retrieving information from them. Knowledge Base Question Answering is one of the promising approaches for extracting substantial knowledge from Knowledge Bases. Unlike web search, Question Answering over a knowledge base gives accurate and concise results, provided that natural language questions can be understood and mapped precisely to an answer in the knowledge base. However, some of the existing embedding-based methods for knowledge base question answering systems ignore the subtle correlation between the question and the Knowledge Base (e.g., entity types, relation paths, and context) and suffer from the Out Of Vocabulary problem. In this paper, we focused on using a pre-trained language model for the Knowledge Base Question Answering task. Firstly, we used Bert base uncased for the initial experiments. We further fine-tuned these embeddings with a two way attention mechanism from the knowledge base to the asked question and from the asked question to the knowledge base answer aspects. Our method is based on a simple Convolutional Neural Network architecture with a Multi-Head Attention mechanism to represent the asked question dynamically in multiple aspects. Our experimental results show the effectiveness and the superiority of the Bert pre-trained language model embeddings for question answering systems on knowledge bases over other well-known embedding methods.},
  keywords={Knowledge based systems;Task analysis;Bit error rate;Knowledge discovery;Semantics;Natural languages;Context modeling;knowledge base question answering;BERT;Language Model;KBQA;Multi-Head Attention},
  doi={10.1109/IJCNN48605.2020.9206698},
  ISSN={2161-4407},
  month={July},}@ARTICLE{9205800,
  author={Larhrib, Mohamed and Escribano, Miguel and Cerrada, Carlos and Escribano, Juan Jose},
  journal={IEEE Access}, 
  title={Converting OCL and CGMES Rules to SHACL in Smart Grids}, 
  year={2020},
  volume={8},
  number={},
  pages={177255-177266},
  abstract={Models are first-class elements in Model-Driven Engineering (MDE). In this paradigm, the most widespread approaches adopted by the development community are Object-Oriented and ontological, formalized using Unified Modeling Language (UML) and Resource Description Framework (RDF), respectively. However, Object Management Group (OMG) does not provide a specific standard language for validating UML models against Object Constraints Language (OCL) constraints; meanwhile, World Wide Web Consortium (W3C) has defined Shapes Constraint Language (SHACL) as a standard validation language. Although the transformation between UML and RDF can be performed at the structural level, no effort has been made to transform OCL to SHACL. This paper addresses the transformation of OCL and text-based constraints to SHACL shapes in the context of Common Grid Model Exchange Standard (CGMES), a UML-based standard for electric utilities in Europe. This paper presents several contributions to the software engineering community. First, solving the validation problem in a standardized way. Second, facilitating European Network of Transmission System Operators for Electricity (ENTSO-E) the construction of an ontology associated with the CGMES standard. Third, allowing developers to integrate the two complementary approaches. Finally, Promoting the adoption and integration of the ontological approach in the software community.},
  keywords={Unified modeling language;Object oriented modeling;Resource description framework;IEC Standards;Ontologies;Common Information Model (electricity);CIM for ENTSO-E (CGMES);OCL rules;ontology;RDF/RDFS;SHACL standard},
  doi={10.1109/ACCESS.2020.3026941},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9172685,
  author={Mordecai, Yaniv and James, Nicholas K. and Crawley, Edward F.},
  booktitle={2020 IEEE Aerospace Conference}, 
  title={Object-Process Model-Based Operational Viewpoint Specification for Aerospace Architectures}, 
  year={2020},
  volume={},
  number={},
  pages={1-15},
  abstract={Remote-controlled or autonomous multi-rotor air vehicles, or drones, have become common and commercially available even to individual consumers, mostly for imaging purposes. Drones appeal to mission architects looking to extend the toolbox provided to operators performing challenging missions such as public safety operations. However, careful analysis of the operational context and concept of operations must take place before major acquisitions. The purpose of this paper is to propose a model-based operational architecture definition framework, which is based on the Department of Defense Architecture Framework (DoDAF) ontology and uses Object Process Methodology (OPM) as its underlying modeling language. Through careful mapping of DoDAF Operational Viewpoint (OV) ontology to OPM ontology, we were able to show that the entire OV ontology can be covered by a small set of objects, processes, relations among them, and constructs comprising them. We then show how to instantiate the ontology to create a model of an actual architecture of interest (AoI) while maintaining strong typing of the model elements to ensure validity, integrity, consistency, and continuous compliance with the OV. We demonstrate our approach on the case of using drones in public safety enterprises for the purpose of crowd management in massively attended events and locations. The proposed framework allows for capturing ConOps and OpsCon in a lightweight, yet robust and consistent manner, and improve communication and concept validation between operational stakeholders and enterprise architects.},
  keywords={Conferences;Imaging;Ontologies;Safety;Stakeholders;Drones},
  doi={10.1109/AERO47225.2020.9172685},
  ISSN={1095-323X},
  month={March},}@INPROCEEDINGS{9166221,
  author={Alamsyah, Andry and Bastikarana, Rafa Syafiq and Ramadhanti, Alya Rysda and Widiyanesti, Sri},
  booktitle={2020 8th International Conference on Information and Communication Technology (ICoICT)}, 
  title={Recognizing Personality from Social Media Linguistic Cues: A Case Study of Brand Ambassador Personality}, 
  year={2020},
  volume={},
  number={},
  pages={1-5},
  abstract={The burgeoning need of a brand ambassador (BA) as a company representative begin to rise in recent year. The phenomena followed by the increase of method to select the most suitable BA. The universal way of selecting one appropriate ambassador is by understanding their personality, therefore, measurement of a BA personality considered as one way to characterize a company credibility. This research proposes to design a method of measuring the BA personality from their social media data in Bahasa Indonesia. We enrich the methodology to measure human personality using the ontology modeling approach. The ontology model constructed under the ngram language model which provides a rapid and effective way of measuring a BA personality. The results of a BA personality measurement allow the utilization to portray of how an ambassador represent their brand and interact with their customer.},
  keywords={Weight measurement;Accuracy;Social networking (online);Design methodology;Psychology;Companies;Ontologies;Linguistics;Time measurement;Information and communication technology;Brand Ambassador;Personality Measurement;Ontology},
  doi={10.1109/ICoICT49345.2020.9166221},
  ISSN={},
  month={June},}@ARTICLE{9141288,
  author={Huang, Zhao and Zhao, Wei},
  journal={IEEE Access}, 
  title={Combination of ELMo Representation and CNN Approaches to Enhance Service Discovery}, 
  year={2020},
  volume={8},
  number={},
  pages={130782-130796},
  abstract={With the rapid growth of Web services, the demand for discovering the optimal services to satisfy the users' requirements is no longer an easy task. The critical issue in the process of service discovery is to conduct a similarity calculation. To solve such an issue, this study proposes an effective approach that combines the Embeddings from Language Models (ELMo) representation and Convolutional Neural Network (CNN) to obtain a more accurate similarity score for retrieving target Web services. More specifically, first, the study adopts the ELMo model to generate effective word representations for capturing the sufficient information from services and queries. Then, the word representations are used to compose a similarity matrix, which will be taken as the input for the CNN to learn the matching relationships. Finally, the combination of the ELMo representation and CNN is used to address the representation and interaction processes within the matching task to improve the service discovery performance. The results demonstrate the effectiveness of our proposed approach for retrieving better targeted Web services.},
  keywords={Semantics;Syntactics;Task analysis;Ontologies;Linguistics;Service-oriented architecture;Service discovery;ELMo;CNN;service similarity;web service},
  doi={10.1109/ACCESS.2020.3009393},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9140236,
  author={Torres, Victoria and Serral, Estefanía and Valderas, Pedro and Pelechano, Vicente and Grefen, Paul},
  booktitle={2020 IEEE 22nd Conference on Business Informatics (CBI)}, 
  title={Modeling of IoT devices in Business Processes: A Systematic Mapping Study}, 
  year={2020},
  volume={1},
  number={},
  pages={221-230},
  abstract={The Internet of Things (IoT) enables to connect the physical world to digital business processes (BP). By using the IoT, a BP can, e.g.: 1) take into account real-world data to take more informed business decisions, and 2) automate and/or improve BP tasks. To achieve these benefits, the integration of IoT and BPs needs to be successful. The first step to this end is to support the modeling of IoT-enhanced BPs. Although numerous researchers have studied this subject, it is unclear what is the current state of the art in terms of current modeling solutions and gaps. In this work, we carry out a Systematic Mapping Study (SMS) to find out how current solutions are modelling IoT into business processes. After studying 600 papers, we identified and analyzed in depth a total of 36 different solutions. In addition, we report on some important issues that should be addressed in the near future, such as, for instance the lack of standardization.},
  keywords={Conferences;Unified modeling language;Internet of Things;Systematics;Task analysis;Computers;Business process modeling;Internet of Things;IoT devices;IoT-enhanced BP;Systematic mapping study},
  doi={10.1109/CBI49978.2020.00031},
  ISSN={2378-1971},
  month={June},}@ARTICLE{9127914,
  author={Yonglin, Lei and Zhi, Zhu and Qun, Li},
  journal={Journal of Systems Engineering and Electronics}, 
  title={An ontological metamodeling framework for semantic simulation model engineering}, 
  year={2020},
  volume={31},
  number={3},
  pages={527-538},
  abstract={Recently, the ontological metamodel plays an increasingly important role to specify systems in two forms: ontology and metamodel. Ontology is a descriptive model representing reality by a set of concepts, their interrelations, and constraints. On the other hand, metamodel is a more classical, but more powerful model in which concepts and relationships are represented in a prescriptive way. This study firstly clarifies the difference between the two approaches, then explains their advantages and limitations, and attempts to explore a general ontological metamodeling framework by integrating each characteristic, in order to implement semantic simulation model engineering. As a proof of concept, this paper takes the combat effectiveness simulation systems as a motivating case, uses the proposed framework to define a set of ontological composable modeling frameworks, and presents an underwater targets search scenario for running simulations and analyzing results. Finally, this paper expects that this framework will be generally used in other fields.},
  keywords={Ontologies;Unified modeling language;Semantics;Metamodeling;Computational modeling;Analytical models;ontology;metamodeling;semantic composability;model-driven engineering (MDE)},
  doi={10.23919/JSEE.2020.000032},
  ISSN={1004-4132},
  month={June},}@INPROCEEDINGS{9112985,
  author={Brummett, Travis and An, Kyoungho and Gokhale, Aniruddha and Mertens, Sanders},
  booktitle={2020 IEEE 23rd International Symposium on Real-Time Distributed Computing (ISORC)}, 
  title={A Model-driven Middleware Integration Approach for Performance-Sensitive Distributed Simulations}, 
  year={2020},
  volume={},
  number={},
  pages={65-73},
  abstract={Complex simulation systems often comprise multiple distributed simulators that need to interoperate and synchronize states and events. In many cases, the simulation logics which are developed by different teams with specific expertise, need to be integrated to build a complete simulation system. Thus, supporting composability and reusability of simulation functionalities with minimal integration and performance overhead is a challenging but required capability. Middleware for game engines are promising to realize both the modular and reusable development criteria as well as the high performance requirements, while data-centric publish/subscribe middleware can support seamless integration and synchronization of the distributed artifacts. However, differences in the level of abstraction at which these middleware operate and the semantic differences in their underlying ontologies make it hard and challenging for simulation application developers and system integrators to realize a complete, operational system. To that end this paper presents a model-driven approach to blending the two middleware, wherein the modeling capabilities provide intuitive and higher-level abstractions for developers to reason about the composition and validation of the complete system, and the generative capabilities address the inherent and accidental complexities incurred in reconciling the semantic differences between the gaming and pub/sub middleware. We present a concrete implementation of our approach and illustrate its use and performance results using simple use cases.},
  keywords={Ports (computers);Semantics;Standardization;Ontologies;Data models;Real-time systems;Complexity theory;Composable Simulation;Distributed Simulation;Model Driven Engineering;Entity Component System;Data Distribution Service;Automation},
  doi={10.1109/ISORC49007.2020.00019},
  ISSN={2375-5261},
  month={May},}@INPROCEEDINGS{9095669,
  author={Chondamrongkul, Nacha and Sun, Jing and Warren, Ian},
  booktitle={2020 IEEE International Conference on Software Architecture Companion (ICSA-C)}, 
  title={Automated Security Analysis for Microservice Architecture}, 
  year={2020},
  volume={},
  number={},
  pages={79-82},
  abstract={Designing a software system that applied the microservice architecture style is a challenging task, as its characteristics are vulnerable to various security attacks. Software architect, therefore, needs to pinpoint the security flaws in the design before the implementation can proceed. This task is error-prone as it requires manual analysis on the design model, to identify security threats and trace possible attack scenarios. This paper presents an automated security analysis approach for microservice architecture. Our approach can automatically identify security threats according to a collection of formally defined security characteristics and provide an insightful result that demonstrates how the attack scenarios may happen. A collection of formally defined security characteristics can be extended to support other security characteristics not addressed in this paper.},
  keywords={Security;Computer architecture;Connectors;Ontologies;Containers;Tools;Analytical models;Microservice Architecture;Security Analysis;Ontology Web Language;Model Checking},
  doi={10.1109/ICSA-C50368.2020.00024},
  ISSN={},
  month={March},}@INPROCEEDINGS{9087557,
  author={Matveev, Anton and Makhnytkina, Olesia and Lizunova, Inna and Vinogradova, Taisiia and Chirkovskii, Artem and Svischev, Aleksei and Mamaev, Nikita},
  booktitle={2020 26th Conference of Open Innovations Association (FRUCT)}, 
  title={A Virtual Dialogue Assistant for Conducting Remote Exams}, 
  year={2020},
  volume={},
  number={},
  pages={284-290},
  abstract={In this paper, we demonstrate issues and possible solutions to building an Artificial Intelligence Dialogue Assistant for human-machine communication. We specialize it for conducting written exams at online education platforms, talk about the main logical components of the system: knowledge base, question encoder, question generation module, question analysis module. As a knowledge base we consider text fragments representing parts of the course of text fragments representing parts of a course and is a source for a format ontology; and is also sourced for neural network generation of fact-based questions in question generation module and building dependency trees for answer evaluation in question analysis module.},
  keywords={Technological innovation;Knowledge based systems;Buildings;Neural networks;Ontologies;Solids;Complexity theory},
  doi={10.23919/FRUCT48808.2020.9087557},
  ISSN={2305-7254},
  month={April},}@INPROCEEDINGS{9070680,
  author={Han, Seung-Ho and Choi, Ho-Jin},
  booktitle={2020 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Domain-Specific Image Caption Generator with Semantic Ontology}, 
  year={2020},
  volume={},
  number={},
  pages={526-530},
  abstract={Image captioning is the task of generating textual descriptions of a given image, requiring techniques of computer vision and natural language processing. Recent models have utilized deep learning techniques for this task to gain performance improvement. However, these models can neither fully use information included in a given image such as object and attribute, nor generate a domain-specific caption because existing methods use open dataset such as MSCOCO which include general images. To overcome these limitations, this paper proposes a domain-specific image caption generator, which generates a caption based on attention mechanism with object and attribute information, and reconstruct a generate caption using a semantic ontology to provide natural language description for given specific-domain. To show the effectiveness of the proposed model, we evaluate the image caption generator with a dataset, MSCOCO, quantitatively and qualitatively.},
  keywords={Semantics;Visualization;Generators;Ontologies;Feature extraction;Image reconstruction;Machine learning;image captioning;attention model;attribute predictionm;domain-specific ontology},
  doi={10.1109/BigComp48618.2020.00-12},
  ISSN={2375-9356},
  month={Feb},}@INPROCEEDINGS{9053975,
  author={Lai, Tuan Manh and Hung Tran, Quan and Bui, Trung and Kihara, Daisuke},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A Simple But Effective Bert Model for Dialog State Tracking on Resource-Limited Systems}, 
  year={2020},
  volume={},
  number={},
  pages={8034-8038},
  abstract={In a task-oriented dialog system, the goal of dialog state tracking (DST) is to monitor the state of the conversation from the dialog history. Recently, many deep learning based methods have been proposed for the task. Despite their impressive performance, current neural architectures for DST are typically heavily-engineered and conceptually complex, making it difficult to implement, debug, and maintain them in a production setting. In this work, we propose a simple but effective DST model based on BERT. In addition to its simplicity, our approach also has a number of other advantages: (a) the number of parameters does not grow with the ontology size (b) the model can operate in situations where the domain ontology may change dynamically. Experimental results demonstrate that our BERT-based model outperforms previous methods by a large margin, achieving new state-of-the-art results on the standard WoZ 2.0 dataset 1. Finally, to make the model small and fast enough for resource-restricted systems, we apply the knowledge distillation method to compress our model. The final compressed model achieves comparable results with the original model while being 8x smaller and 7x faster.},
  keywords={Bit error rate;Production;Ontologies;Signal processing;Task analysis;Speech processing;Standards;Task-Oriented Dialog Systems;Dialog State Tracking;BERT;Knowledge Distillation},
  doi={10.1109/ICASSP40776.2020.9053975},
  ISSN={2379-190X},
  month={May},}@ARTICLE{9050669,
  author={Bandyszak, Torsten and Daun, Marian and Tenbergen, Bastian and Kuhs, Patrick and Wolf, Stefanie and Weyer, Thorsten},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Orthogonal Uncertainty Modeling in the Engineering of Cyber-Physical Systems}, 
  year={2020},
  volume={17},
  number={3},
  pages={1250-1265},
  abstract={Software-intensive cyber-physical systems (CPS) perform essential tasks such as controlling automated production processes in industrial production plants. The required levels of autonomy, openness, and self-adaptation, as well as the dynamic nature of the context of such CPS, result in challenging tasks for their engineering. During operation, unexpected situations in which the system has insufficient knowledge about the current state of the system itself as well as its context may occur. Engineering CPS, e.g., for industrial production sites, must account for such uncertainties the system will have to cope with during its lifetime in a structured and systematic way. Since the development of CPS requires consideration of different system perspectives, current uncertainty modeling approaches cannot be applied right away, as they do not explicitly consider uncertainty aspects that affect different artifacts. To aid the engineering of CPS, this article presents a model-based approach to document uncertainty. We propose “Orthogonal Uncertainty Models,” which closely integrate with other engineering artifacts from different perspectives, as a means for capturing a dedicated uncertainty viewpoint. Our approach has been evaluated in the industry automation domain. The application shows that the idea of regarding uncertainty within a dedicated perspective is highly beneficial. Particularly, our approach helps to uncover and document uncertainties related to behavioral, functional, and structural properties of a system, as well as uncertainties related to business models that would otherwise possibly remain covert. Note to Practitioners-Identifying and documenting uncertainties, which may occur during operation of a system, is a common problem in engineering processes. Such uncertainties may lead to severe damage, and thus need to be mitigated appropriately. It is crucial to account for these uncertainties during engineering, especially in the early phases. Depending on the specific project characteristics, a multitude of different diagram types are used to model a system. Uncertainties thus reflect in many artifacts, which leads to: 1) redundancies in the specified uncertainty attached to diagram elements and 2) uncertainty information (e.g., about the cause or effect of uncertainty) that is spread across different diagrams. The latter makes it difficult to structure uncertainty information and trace it throughout the engineering process so that uncertainty can be systematically considered. Our approach provides a graphical modeling language that employs a dedicated perspective on uncertainty in separate diagrams that can be linked to any engineering artifact.},
  keywords={Uncertainty;Robot sensing systems;Context modeling;Collaboration;Production;Runtime;Cyber-physical systems;industry automation case study;model-based engineering;orthogonal modeling;uncertainty;uncertainty modeling},
  doi={10.1109/TASE.2020.2980726},
  ISSN={1558-3783},
  month={July},}@ARTICLE{9032420,
  author={},
  journal={IEEE Std 2413-2019}, 
  title={IEEE Standard for an Architectural Framework for the Internet of Things (IoT)}, 
  year={2020},
  volume={},
  number={},
  pages={1-269},
  abstract={An architecture framework description for the Internet of Things (IoT) which conforms to the international standard ISO/IEC/IEEE 42010:2011 is defined. The architecture framework description is motivated by concerns commonly shared by IoT system stakeholders across multiple domains (transportation, healthcare, Smart Grid, etc.). A conceptual basis for the notion of things in the IoT is provided and the shared concerns as a collection of architecture viewpoints is elaborated to form the body of the framework description.},
  keywords={IEEE Standards;Computer architecture;Internet of Things;architectural framework;IEEE 2413;Internet of Things (IoT)},
  doi={10.1109/IEEESTD.2020.9032420},
  ISSN={},
  month={March},}@INPROCEEDINGS{9081794,
  author={Annighoefer, Bjoern and Halle, Martin and Schweiger, Andreas and Reich, Marina and Watkins, Christopher and VanderLeest, Steven H. and Harwarth, Stefan and Deiber, Patrick},
  booktitle={2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC)}, 
  title={Challenges and Ways Forward for Avionics Platforms and their Development in 2019}, 
  year={2019},
  volume={},
  number={},
  pages={1-10},
  abstract={Today's air vehicles depend on digital technology. It accounts for more than 30% of their development costs. The number of functions, the lines of code, the degree of autonomy, and the number of vehicles rise. This is why there is a need for cutting-edge technology and development methods. There is a gap between academia's methods and industrial applications due to multi-disciplinary challenges. We summarize the state-of-the-art in avionics, namely avionics platforms, requirements engineering, model-based development, automated verification, emerging technologies, and emerging demands. Experts review the most demanding challenges, research gaps, and promising solutions. They provide recommendations for the enhancement of the cooperation between industry and academia and suggest necessary research topics. This article is an introduction for those who are new to avionics. It is an up-to-date summary, for insiders looking for most promising solutions to their current problems; and it is a guide for those advancing avionics research.},
  keywords={Adaptation models;Reviews;Computational modeling;Education;Transportation;Aerospace electronics;Requirements engineering;Security;Virtualization;Standards;avionics platforms;requirements engineering;model-based development;automated verification;multi-core},
  doi={10.1109/DASC43569.2019.9081794},
  ISSN={2155-7209},
  month={Sep.},}@INPROCEEDINGS{9031044,
  author={Vijayalakshmi, H C and Dixit, Bhavana S},
  booktitle={2019 4th International Conference on Computational Systems and Information Technology for Sustainable Solution (CSITSS)}, 
  title={Information Retrieval in Kannada using Ontology}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  abstract={As Internet technology has become a part of the lifestyle of the common man, research efforts are extensively made in the fields of Natural Language Processing (NLP) and Information Retrieval. Studying regional languages for developing the system to store, retrieve, extract the information from the database has gained lots of prominence nowadays. Case studies show that Ontological Information Retrieval has many advantages over keyword-based approach. In this paper we have focused on the general architecture of ontology-based Information Retrieval used for Kannada.},
  keywords={Ontologies;Information retrieval;Databases;Semantics;Structured Query Language;Natural language processing;Ontology;Information Retrieval;Kannada;NLP},
  doi={10.1109/CSITSS47250.2019.9031044},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8972267,
  author={Andryushkevich, Sergey K. and Kovalyov, Serge P. and Nefedov, Evgeny},
  booktitle={2019 IEEE 17th International Conference on Industrial Informatics (INDIN)}, 
  title={Composition and Application of Power System Digital Twins Based on Ontological Modeling}, 
  year={2019},
  volume={1},
  number={},
  pages={1536-1542},
  abstract={The approach to create power system digital twins is presented by the example of energy supply of a geographically localized R&D facility. In this paper, the six-layer digital twin architecture is proposed and its prototype software implementation is described. The architecture consists of an ontological model, a digital single line diagram, electronic documentation, master data, load measurement data, and mathematical models and simulations. The paper describes problems and principles of ontological modeling of the prosumer infrastructure, including customer load, low-voltage distribution network sections, small-scale generation equipment, and electric energy storage devices. The optimal configuration of the hybrid power supply system with renewable energy sources was computed using the digital twin that was composed according to the presented approach. For machine-readable representation of the digital twin, the ontological modeling language OWL is used.},
  keywords={Low voltage;Power supplies;Computational modeling;Prototypes;Computer architecture;Mathematical models;Data models;Digital twins;Power systems;Load modeling;modeling;digital twin;ontology;generative design;distributed energy resources;Internet of Energy},
  doi={10.1109/INDIN41052.2019.8972267},
  ISSN={2378-363X},
  month={July},}@INPROCEEDINGS{8945629,
  author={Daneth, Horn and Ali, Nazakat and Hong, Jang-Eui},
  booktitle={2019 26th Asia-Pacific Software Engineering Conference (APSEC)}, 
  title={Automatic Identifying Interaction Components in Collaborative Cyber-Physical Systems}, 
  year={2019},
  volume={},
  number={},
  pages={197-203},
  abstract={Due to diverse set of heterogeneous computing devices communicating with one another and fusing with physical components in Cyber-Physical Systems, software engineers may use different tools and/or modeling languages to formally describe or verify the system properties. As a result, the integration of these diverse constituents poses key challenges such as task for identifying interactions of components to be synthesized for a function in the systems. Although existing studies such as ontology and integration semantic languages have been used for specifying interactions of components in a Cyber-Physical System, these are still not applicable to discover the component interactions in collaborative Cyber-Physical Systems. It is due to the fact that functionalities of Cyber-Physical Systems are generally realized through interactions among multiple systems in a collaborative environment. This paper proposes a model interaction language, CyPhyML+ which can identify component interactions of realized functions in collaborative Cyber-Physical Systems. We show the proposed approach validity and applicability via an Automatic Incident Detection System.},
  keywords={Safety;Finite element analysis;Collaboration;Cyber-physical systems;Computational modeling;Software;Logic gates;Model Interaction Language, Component Interactions , Cyber-Physical Systems},
  doi={10.1109/APSEC48747.2019.00035},
  ISSN={2640-0715},
  month={Dec},}@INPROCEEDINGS{8945003,
  author={Nardi, Julio Cesar and Almeida, João Paulo A. and da Silva, Paulo Henrique A. and Guizzardi, Giancarlo},
  booktitle={2019 IEEE 23rd International Enterprise Distributed Object Computing Conference (EDOC)}, 
  title={An Ontology-Based Diagnosis of Mainstream Service Modeling Languages}, 
  year={2019},
  volume={},
  number={},
  pages={112-121},
  abstract={This paper presents a diagnosis of mainstream service modeling languages (SoaML, USDL, and ArchiMate) in light of UFO-S, a reference ontology for services. UFO-S is intended as a broad ontology for service phenomena, harmonizing different perspectives on services (e.g., "service as commitment", and "service as capability"), and addressing several phases of the service lifecycle (service offering, service agreement, and service delivery). As result, UFO-S is used as an "analysis theory" to identify choices in these languages concerning their focus and coverage of service phenomena. We identify a number of possible improvements concerning the representation of service participant (roles), the description of service offerings, service agreements and service delivery.},
  keywords={Computational modeling;Conferences;Ontologies;service modeling languages;service ontology;SoaML;USDL;ArchiMate},
  doi={10.1109/EDOC.2019.00023},
  ISSN={2325-6362},
  month={Oct},}@INPROCEEDINGS{8934256,
  author={Bikmullina, I. and Barkov, I.},
  booktitle={2019 International Multi-Conference on Industrial Engineering and Modern Technologies (FarEastCon)}, 
  title={Instrumentation and Control System for Test Bench}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={The main problem of the article is the lack of automated structural synthesis of information systems based on the semantic relations of subject area. As presented in the article, in technology creation software system expert developing the domain ontology with help user-friendly the program interfaces. This program is a mechanism for automatically formalizing domain description and automatically synthesizing the class diagram in the Unified Modeling Language. In the article discusses the method for automatic design based on automated synthesis of Unified Modeling Language models of the application program to control the test stand. The method of its use in solving the problems of designing an instrumentation and control system for test bench of an unmanned aerial vehicle is described. Expert inputs a domain ontology, then system automatically formalizes domain description and synthesizes the class diagram in the Unified Modeling Language.},
  keywords={Unified modeling language;Software systems;Semantics;Ontologies;Information systems;Complexity theory;Industrial engineering;UML;synthesis of class diagrams;ontology;domain;an unmanned aerial vehicle},
  doi={10.1109/FarEastCon.2019.8934256},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8926665,
  author={Jue, Wang and Song, Yineng and Wu, Xian and Dai, Wenbin},
  booktitle={IECON 2019 - 45th Annual Conference of the IEEE Industrial Electronics Society}, 
  title={A Semi-Formal Requirement Modeling Pattern for Designing Industrial Cyber-Physical Systems}, 
  year={2019},
  volume={1},
  number={},
  pages={2883-2888},
  abstract={Requirement engineering is a crucial part of the engineering process. The traditional methods of requirement engineering are time-consuming and human-centered. A well-established software requirement description model needs to ensure the accuracy and integrity of the transformation and is also hoped to be scalable, versatile, and efficient in transformation and transmission. This paper presents a method of requirement engineering, including constricted nature language requirement input pattern, and the formalized requirement description JSON model. This method provides convenience for requirement modification and validation that can satisfy the real-time constraints of industrial cyber-physical systems.},
  keywords={Unified modeling language;Software;Solid modeling;Requirements engineering;Analytical models;Real-time systems;requirement engineering;requirement component;formal models;system behavior decomposition},
  doi={10.1109/IECON.2019.8926665},
  ISSN={2577-1647},
  month={Oct},}@INPROCEEDINGS{8907289,
  author={Lê, Lam-Son and Truong, Thai-Minh and Wegmann, Alain},
  booktitle={2019 IEEE 23rd International Enterprise Distributed Object Computing Workshop (EDOCW)}, 
  title={A Novel Approach to Modeling Enterprise Services Leveraging Object Cloning and Multilevel Classification}, 
  year={2019},
  volume={},
  number={},
  pages={160-167},
  abstract={Object-oriented modeling is concerned with capturing common properties of objects. The dominant thinking in this realm is to classify objects that share certain properties into what is called a class, which in turn enables us to instantiate additional objects. Deep modeling takes a step further by introducing the notion of clabject that might be instantiated multiple times until its instantiation potency runs out. This initiative has gained a lot of momentum of late, primarily due to the inadequacy of the classical mechanics of two-level object instantiation. There exists a less familiar way of reasoning in object-orientation that takes its root from the prototype theory. We believe that they co-exist as two sides of the same coin. Unfortunately, prototype-based modeling still stays on the sidelines in the mainstream of conceptual modeling and related areas (e.g., enterprise modeling). In this paper, we argue that the two methods actually complement each other. We propose a hybrid modeling suite that allows for both instantiation and cloning in enterprise modeling. We formally state that a clabject not only features the so-called potency (i.e., for how many levels this clabject might further be classified) but also carries the notion of characteristics (i.e., the extent to which this clabject resembles those being represented). We demonstrate our novel ways of modeling for capturing business processes in a service-oriented enterprise architecture.},
  keywords={Process modeling;Object oriented modeling;Computational modeling;Conferences;Prototypes;Cloning;Computer architecture;Cognition;Business;Multilevel Classification, Business Process Modeling, Prototype Theory, Enterprise Modeling},
  doi={10.1109/EDOCW.2019.00036},
  ISSN={2325-6605},
  month={Oct},}@INPROCEEDINGS{8901302,
  author={Rui, Jiang},
  booktitle={2019 International Conference on Smart Grid and Electrical Automation (ICSGEA)}, 
  title={Research on Semantic Service Technology in Mobile Geographic Information System}, 
  year={2019},
  volume={},
  number={},
  pages={304-307},
  abstract={This paper combines the conceptual construction and classification methods of geo-ontology, explores the method of formal description language to express geo-domain ontology concepts, and defines the concept of geo-ontology in the process of geo-spatial information semantic expression. Controposing to the problem of inadequate description of spatial information services by traditional description methods, Construction of spatial information application ontology with Protege tool and description service information with OWL-S description language are adopted, to realize the semantic integration between geo-information system. It constructs spatial information application ontology and describes service information with OWL-S description language. The elements of service quality are expanded to improve the comprehensiveness of service description. To verify the feasibility of the combination of theory and practical application of geo-ontology, a mobile GIS is developed from the bottom under the support of current mobile development technology, and an example of spatial semantic retrieval is tested. The superiority of geo-ontology in semantic retrieval is verified, which has important theoretical and practical significance.},
  keywords={Semantics;Information services;Ontologies;Tools;Topology;Smart grids;Geographic information systems;semantic service;GIS;OWL;ontology;Protege},
  doi={10.1109/ICSGEA.2019.00076},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{8876971,
  author={Guizzardi, Giancarlo and Figueiredo, Guylerme and Hedblom, Maria M. and Poels, Geert},
  booktitle={2019 13th International Conference on Research Challenges in Information Science (RCIS)}, 
  title={Ontology-Based Model Abstraction}, 
  year={2019},
  volume={},
  number={},
  pages={1-13},
  abstract={In recent years, there has been a growth in the use of reference conceptual models to capture information about complex and critical domains. However, as the complexity of domain increases, so does the size and complexity of the models that represent them. Over the years, different techniques for complexity management in large conceptual models have been developed. In particular, several authors have proposed different techniques for model abstraction. In this paper, we leverage on the ontologically well-founded semantics of the modeling language OntoUML to propose a novel approach for model abstraction in conceptual models. We provide a precise definition for a set of Graph-Rewriting rules that can automatically produce much-reduced versions of OntoUML models that concentrate the models' information content around the ontologically essential types in that domain, i.e., the so-called Kinds. The approach has been implemented using a model-based editor and tested over a repository of OntoUML models.},
  keywords={Unified modeling language;Object oriented modeling;Complexity theory;Ontologies;Semantics;Context modeling;Clustering methods;Model Abstraction;Complexity Management in Conceptual Modeling;Ontology-Based Conceptual Modeling},
  doi={10.1109/RCIS.2019.8876971},
  ISSN={2151-1357},
  month={May},}@INPROCEEDINGS{8869059,
  author={Patzer, Florian and Volz, Friedrich and Usländer, Thomas and Blöcher, Immanuel and Beyerer, Jürgen},
  booktitle={2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={The Industrie 4.0 Asset Administration Shell as Information Source for Security Analysis}, 
  year={2019},
  volume={},
  number={},
  pages={420-427},
  abstract={One of the essential concepts of the Reference Architecture Model Industrie 4.0 (RAMI4.0) is the uniform modelling of assets by means of a common meta-data model called the Asset Administration Shell (AAS). However, important practical experience with this concept is still missing, as not many use cases for the AAS have yet been implemented. Thus, practical issues within the AAS concept and respective solutions are hard to identify. In this paper, presents our experience with the implementation of an AAS use case. The AAS is used as information source to create an ontology, which is then used for security analysis. The paper discusses the use-case-specific modelling language selection and provides a practical examination of several of our implementations that use OWL and OPC UA together. Furthermore, it provides recommendations for the implementation of Asset Administration Shells for this and similar use cases.},
  keywords={Security;Protocols;Data models;Analytical models;Tools;XML;Cognition;Asset Administration Shell;Security Ontology;Ontology;OPC UA;Semantic Web;Industrial Systems},
  doi={10.1109/ETFA.2019.8869059},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{8862131,
  author={Vijaya, B. and Gharpure, Prachi},
  booktitle={2019 International Conference on Computational Intelligence in Data Science (ICCIDS)}, 
  title={Candidate Generation for Instance Matching on Semantic Web}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  abstract={The growth of semantic web has given rise to proliferation of data sources wherein the task of recognizing real world entities and identifying multiple references of the same real world entity becomes an essential task in order to facilitate sharing and integration of data. Due to the heterogeneous nature of data on the semantic web, entities belonging to different sources are compared by assessing the similarity of features that are common in order to identify matches. With the increasing size of data sets Candidate generation methods are generally employed to avoid quadratic time complexity that would otherwise be incurred if pairwise similarity of all entities are computed. Here we propose a novel index based approach for candidate generation and reduction. The evaluation shows that the proposed method scales well and improves recall significantly.},
  keywords={Indexes;Semantic Web;Task analysis;Mathematical model;Computational intelligence;Data science;Couplings;Instance matching;Record Linkage;Blocking;Candidate generation;Semantic web},
  doi={10.1109/ICCIDS.2019.8862131},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{8843314,
  author={Paczona, Martin and Mayr, Heinrich C.},
  booktitle={2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)}, 
  title={Model-Driven Mechatronic System Development}, 
  year={2019},
  volume={},
  number={},
  pages={1730-1736},
  abstract={This paper presents an approach for model-driven mechatronic system development. The approach starts with the definition of a suitable domain-specific modeling language and its semantic foundation in a domain ontology. Models created in this language are used to generate application-specific artefacts. We illustrate our approach with the example of the development of Electric Vehicle Testbeds (EVTs), i.e. systems for testing high-voltage electric vehicle components. Companies in the electric vehicle industry (automobile, aircraft and rail vehicle manufacturers) mainly use such systems. Like many other mechatronic systems, EVTs are typically tailor-made solutions. Our approach automates manual development steps and can thus contribute to quality improvement, development time reduction and finally cost reduction.},
  keywords={Ontologies;Integrated circuit modeling;Unified modeling language;Software;Tools;Electric vehicles},
  doi={10.1109/COASE.2019.8843314},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{8836882,
  author={Lee, James D. and Matsumoto, Shou and Zaidi, Abbas K. and Laskey, Kathryn B.},
  booktitle={2019 IEEE International Systems Conference (SysCon)}, 
  title={Towards Automating Design and Development of Inference Enterprise Models}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={In this paper, a process improvement approach, which provides automated assistance in designing and developing Inference Enterprise Models (IEM), is described. An IEM forecasts and evaluates the performance of the modeled enterprise and can be used to improve the operational quality of an enterprise making mission-focused inferences. As part of developing the IEM methodology, a team of researchers built several Inference Enterprise Models and acquired a breadth of knowledge in the domain of modeling insider threat detection systems. Although there are many different ways this type of inference enterprise can be modeled, there are common elements that are reused from case to case. We propose a solution for formalizing and reusing this IEM domain knowledge by developing an IEM Process Ontology and Workflow Generator.},
  keywords={Ontologies;Unified modeling language;Modeling;Predictive models;Sociology;Statistics;Task analysis;Inference Enterprise Modeling;Ontology;Template;Business Process Improvement;Knowledge Reuse},
  doi={10.1109/SYSCON.2019.8836882},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{8816987,
  author={Oba, Atsushi and Paik, Incheon},
  booktitle={2019 IEEE International Conference on Cognitive Computing (ICCC)}, 
  title={Extraction of Taxonomic Relation of Complex Terms by Recurrent Neural Network}, 
  year={2019},
  volume={},
  number={},
  pages={70-72},
  abstract={In recent years, while the Internet has brought various technological evolutions, a lot of ontology is required to organize and systemize knowledge, and its generation is necessary. Especially, classification of hypernym-hyponym relation which describes taxonomy of ontology has received a lot of attention. As a method to automate the generation, word embedding based method was proposed recently. Although the method enabled high accuracy classification by using semantics, it does not correspond to complex term consisting of multiple words. Based on this background, in this paper, we proposed a new model combined word embedding and Recurrent Neural Network(RNN), evaluated the classification performance with data extracted from WordNet. For the result, it is indicated that the RNN approach is more effective and general for ontology generation.},
  keywords={Ontologies;Data models;Recurrent neural networks;Semantics;Training;Taxonomy;Support vector machines;Ontological Classification;Word Embedding;Word2Vector;Recurrent Neural Network;Natural Language Processing;Recurrent Neural Network Language Model},
  doi={10.1109/ICCC.2019.00024},
  ISSN={},
  month={July},}@INPROCEEDINGS{8808078,
  author={Kreines, Mikhail G. and Kreines, Elena M.},
  booktitle={2019 IEEE 21st Conference on Business Informatics (CBI)}, 
  title={Artificial Intelligence Tools for Business Applications: Objective Map of Science and Analysis of Texts}, 
  year={2019},
  volume={01},
  number={},
  pages={445-451},
  abstract={Business is looking for technological and investment possibilities in research and development (R&D). Here the basic problems are to find R&D's results and/or teams for solving the professional tasks and for making investment. But business has no personal view on scientific problems. So business is seeking the objective tools for forecasting and evaluation of R&D prospects and results. Experts have own interests and require a lot of funding. R&D reflections are the texts. The modern methods of computer analysis of texts can do a lot of the experts' work for making it more objective and cheaper. The tools for search, systematization and ranking R&D's results and teams are computer analysis of texts and the map of science. The map of science is the distribution of the collection of texts of a scientific nature by the topics. The map of science is a way to navigate through the world of scientific publications and R&D's teams, a tool for identifying trends and assessing R&D directions. The usual ways for the map of science formation use bibliometric/scientometric data, general probability models of the texts, expert's opinion or artificial intelligence (AI) models and methods based on the thesaurus or on the ontology of the subject domains. The interests of business are not in line with the orientation on a priori established ideas about possible topics or there number for rapidly changing scientific fields. Precisely these fields are of the greatest interest to business. On the basis of mathematical modeling of texts and large-scale text collections, an approach is proposed for the computational formation of the adaptive dynamic map of science that does not use a priori classification schemes and data of the scientific publications' citation. Topics (thematic groups), their number and the distribution of texts over the topics are determined computationally without experts' involvement. Examples of the maps of science for various collections of scientific publications are given. The original method is proposed for checking the adequacy of the text models and the map of science. The method uses the categorization of articles and their abstracts as the separate objects on the basis of computationally generated map (its topics). The results of the large-scale experiment confirmed the high efficiency of the proposed mathematical modeling of texts and text collections. The possibilities of practical use of the map of science for business applications are considered.},
  keywords={Research and development;Computational modeling;Analytical models;Business;Semantics;Tools;Adaptation models;text;collection;semantics;semiotics;modeling;interpretation;research and development;information retrieval;analytical support},
  doi={10.1109/CBI.2019.00058},
  ISSN={2378-1971},
  month={July},}@INPROCEEDINGS{8756968,
  author={Bučko, B. and Zábovská, K. and Zábovský, M.},
  booktitle={2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)}, 
  title={Ontology as a Modeling Tool within Model Driven Architecture Abstraction}, 
  year={2019},
  volume={},
  number={},
  pages={1525-1530},
  abstract={This paper is focused on automatic transformation process of top levels of Model Driven Architecture (MDA) within the information system development phase. System architects are always trying to find easier, complex and more united way of information system development. Although the Model Driven Architecture (MDA) provides a set of guidelines for the structuring of specifications it also comes with challenging tasks of transformations between the various levels of abstraction. The primary objective of this work is to design a universal automated approach within the Computer Independent Model (CIM) and Platform Independent Model (PIM) manual transformation. The manual process of the transformations within MDA could be automated using ontology model with the combination of mapping rules and Extensible Markup Process Definition Language (XPDL) and Extensible Markup Language Metadata interchange (XMI) conversion.},
  keywords={Unified modeling language;Ontologies;Business;Computational modeling;Data models;Computer architecture;Information systems;Model driven architecture;Computer independent model;Platform independent model;Ontology;Information system development},
  doi={10.23919/MIPRO.2019.8756968},
  ISSN={2623-8764},
  month={May},}@ARTICLE{8758422,
  author={Zafar, Iqra and Azam, Farooque and Anwar, Muhammad Waseem and Maqbool, Bilal and Butt, Wasi Haider and Nazir, Aiman},
  journal={IEEE Access}, 
  title={A Novel Framework to Automatically Generate Executable Web Services From BPMN Models}, 
  year={2019},
  volume={7},
  number={},
  pages={93653-93677},
  abstract={Enterprise resource planning (ERP) is a business process management system in which integrated applications are used to manage business processes in a shared data environment. ERP systems usually deal with the two types of business processes, i.e., exchange and conversion. In the exchange process, economic resource, such as product, exchanges to another economic resource, such as the sales process. In a conversion process, an enterprise consumes resources in order to produce new resources, such as the distribution process. Generally, the communication between ERP applications, based on the conversion and exchange processes, is accomplished through Web services. In this context, the implementation of Web services in ERP systems is a complex task. To manage this, the business process model and notation (BPMN) are frequently utilized to simplify the development of ERP applications. However, state-of-the-art BPMN approaches usually deal with the modeling of exchange processes without considering the conversion process. Furthermore, the model transformation solution to automatically generate Web services from the BPMN models are hard to find in the literature. Therefore, in this paper, a novel framework is proposed that supports the modeling of both exchanges as well as conversion processes through BPMN. Particularly, a modeling approach is introduced to represent the ERP processes through BPMN concepts. Subsequently, the rules are developed to convert source BPMN models into target Service-oriented architecture Modeling Language (SoaML) models. Finally, transformation rules are developed to generate fully functional executable Java Web services from SoaML models. As a part of the research, a complete open-source BPMN to Web services transformation (B2W) tool is developed to automatically generate the Web services from the high-level BPMN models. The proposed framework is validated through multiple case studies. The experimental results prove that the proposed framework accurately generates Web services from the BPMN models, which eventually helps in developing the ERP systems with simplicity.},
  keywords={Business;Biological system modeling;Unified modeling language;Economics;Service-oriented architecture;Ontologies;Model-driven engineering (MDE);business process model and notation (BPMN);service oriented architecture (SOA);ERP systems;service generation},
  doi={10.1109/ACCESS.2019.2927785},
  ISSN={2169-3536},
  month={},}@INBOOK{10789177,
  author={},
  booktitle={Context-Aware Computing}, 
  title={2. Modeling Context}, 
  year={2018},
  volume={},
  number={},
  pages={27-43},
  abstract={Modeling context is the first step in building context-aware computing systems and applications. It determines the organization and access manner of context information in context-aware applications. This chapter lists six typical context representation methods. An ontological context representation language OWL with its Description Logic foundation is particularly described. To tackle context dynamics, context can further be interpreted as context events, whose temporal properties and relationships can be captured and expressed through temporal operators.},
  keywords={Ontologies;Context modeling;OWL;Object oriented modeling;Context-aware services;Semantics;Resource description framework;Logic;Computational modeling;Cognition},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9783110555691},
  url={https://ieeexplore.ieee.org/document/10789177},}@INPROCEEDINGS{9018722,
  author={Qie, Yongiun and Zhu, Weijie and Liu, Aishan and Zhang, Yuchen and Wang, Jun and Li, Teng and Li, Yaqing and Ge, Yufei and Wang, Yufeng},
  booktitle={2018 IEEE CSAA Guidance, Navigation and Control Conference (CGNCC)}, 
  title={A Deep Learning Based Framework for Textual Requirement Analysis and Model Generation}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={Requirement analysis is a key part of systems engineering process. Analyzing requirements correctly and creating design model sequentially could be critical to the whole process of a product development. Nevertheless, requirement text handling and model transferring could be really time-consuming and error-prone. Thus, we proposed an artificial intelligence based framework to deal with textual requirement handling and model creation. With deep learning and natural language process skills, our approach could be able to analyze textual requirements automatically, and then create the related models. This would indeed alleviate the work of engineers and promote the efficiency and quality of product development process. With our limited knowledge, our paper is the first one to propose the deep learning and NLP based framework to automatically create requirement models.},
  keywords={Semantics;Computational modeling;Machine learning;Natural language processing;Air traffic control;Analytical models;requirement modeling;deep learning;NLP},
  doi={10.1109/GNCC42960.2018.9018722},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{8947785,
  author={Litvak, Claudia and Antonelli, Leandro and Rossi, Gustavo and Gigante, Nora},
  booktitle={2018 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
  title={Improving the Identification of Conflicts in Collaborative Requirements Engineering}, 
  year={2018},
  volume={},
  number={},
  pages={872-877},
  abstract={Requirements engineering has the aim of describing as accurately as possible the needs and expectations of all the stakeholders involved in the software development. The collaborative work of the stakeholders in this process allows them to improve the quality of the requirements. Nevertheless, collaborative work involves the raising of conflicts, and they must be solved in order to achieve the desired quality. This paper presents the evolution in our understanding of the process to identify and solve conflicts during the collaborative construction of the Language Extended Lexicon that captures the domain language. This process was validated with three different case studies and the usability SUS questionnaire.},
  keywords={Collaboration;Ontologies;Stakeholders;Requirements engineering;Software;Companies;Collaborative work;requirements engineering, collaboration, conflicts, natural language models},
  doi={10.1109/CSCI46756.2018.00173},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8710519,
  author={Landolfi, Giuseppe and Bami, Andrea and Izzo, Gabriele and Montini, Elias and Bettoni, Andrea and Vujasinovic, Marko and Gugliotta, Alessio and Soares, Antόnio Lucas and Diogo Silva, Henrique},
  booktitle={2018 International Conference on Intelligent Systems (IS)}, 
  title={An Ontology Based Semantic Data Model Supporting A Maas Digital Platform}, 
  year={2018},
  volume={},
  number={},
  pages={896-904},
  abstract={The integration of IoT infrastructures across production systems, together with the extensive digitalisation of industrial processes, are drastically impacting manufacturing value chains and the business models built on the top of them. By exploiting these capabilities companies are evolving the nature of their businesses shifting value proposition towards models relying on product servitization and share, instead of ownership. In this paper, we describe the semantic data-model developed to support a digital platform fostering the reintroduction in the loop and optimization of unused industrial capacity. Such data-model aims to establish the main propositions of the semantic representation that constitutes the essential nature of the ecosystem to depict their interactions, the flow of resources and exchange of production services. The inference reasoning on the semantic representation of the ecosystem allows to make emerge nontrivial and previously unknown opportunities. This will apply not only to the matching of demand and supply of manufacturing services, but to possible and unpredictable relations. For instance, a particular kind of waste being produced at an ecosystem node can be linked to the requirements for an input material needed in a new product being developed on the platform, or new technologies can be suggested to enhance processes under improvement. The overall architecture and individual ontologies are presented and their usefulness is motivated via the application to use cases.},
  keywords={US Department of Transportation;Technological innovation;Production;Reliability;Manganese;Ions;Stakeholders;servitization;manufacturing ontologies;semantic data-model;knowledge discovery},
  doi={10.1109/IS.2018.8710519},
  ISSN={1541-1672},
  month={Sep.},}@INPROCEEDINGS{8653655,
  author={Rajasekar, M. and Udhayakumar, A.},
  booktitle={2018 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 2018 2nd International Conference on}, 
  title={"E MARUTHUVACHI" – INFORMATION EXTRACTION FRAMEWORK FOR DATA ABOUT OBSTETRICS AND GYNECOLOGY IN TAMIL}, 
  year={2018},
  volume={},
  number={},
  pages={399-407},
  abstract={Technology is transforming the world from traditional into Artificial Intelligence. Human beings are adopting themselves into the change using Technology. India is famous for the name of unique traditional culture. The traditional culture protected people to do useful things. Especially for women, they were protected by the traditional culture to gain knowledge about maternity and gynecology. The target framework to extract the useful information from the raw documents. The extraction process extracts the NLP elements from the raw documents. The framework is developed using modified model of neural network language model (NNLM). The proposed model is evaluated with F-Test. The evaluation produces the good result for accuracy.},
  keywords={Computational modeling;Neural networks;Task analysis;Data mining;Taxonomy;Gynecology;Artificial Intelligence;Information Extraction;Obstetrics and Gynecology;Neural Networks;Neural Network Language Model},
  doi={10.1109/I-SMAC.2018.8653655},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{8632265,
  author={Blas, María J. and Gonnet, Silvio M. and Leone, Horacio P. and Zeigler, Bernard P.},
  booktitle={2018 Winter Simulation Conference (WSC)}, 
  title={A CONCEPTUAL FRAMEWORK TO CLASSIFY THE EXTENSIONS OF DEVS FORMALISM AS VARIANTS AND SUBCLASSES}, 
  year={2018},
  volume={},
  number={},
  pages={560-571},
  abstract={The Discrete Event System Specification (DEVS) is a general modeling formalism with sound semantics founded on a system theoretic basis. It can be used as a base for the development of specialized modeling formalisms. Usually, the extensions of DEVS expand the classes of systems models that can be represented in DEVS. However, with a growing number in new variants of DEVS and an increasing number of problems to be solved using discrete simulation techniques, it is necessary to define the relations among different approaches. This paper presents a conceptual modeling perspective applied to DEVS extensions that structure a framework over the traditional modeling and simulation approach. The framework provides a multilevel structure to analyze the features required for each extension type. Two main types of extensions are identified: variants and subclasses. In order to illustrate the proposed guidelines, the Routed DEVS formalism is presented as example of the subclass type.},
  keywords={Context modeling;Discrete-event systems;Computational modeling;Analytical models;Semantics;Guidelines;Ontologies},
  doi={10.1109/WSC.2018.8632265},
  ISSN={1558-4305},
  month={Dec},}@INPROCEEDINGS{8622503,
  author={Kharlamov, Evgeny and Martin-Recuerda, Francisco and Perry, Brandon and Cameron, David and Fjellheim, Roar and Waaler, Arild},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)}, 
  title={Towards Semantically Enhanced Digital Twins}, 
  year={2018},
  volume={},
  number={},
  pages={4189-4193},
  abstract={Digital twins (DTs) are a powerful mechanism for representing complex industrial assets such as oil platforms as digital models. These models can facilitate temporal analyses and computer simulations of assets. In order to enable this, DTs should be able to capture characteristics of an asset as specified by the manufacturer, its state during the run time, as well as how the asset interacts with other assets in a complex system. We argue that semantic technologies and in particular semantic models or ontologies is promising modelling paradigm for DTs. Semantic models allow to capture complex systems in an intuitive fashion, can be written in standardised ontology languages, and come with a wide range of off-the-shelf systems to design, maintain, query, and navigate semantic models. In this work we report our preliminary results on developing a system that would support semantic-based DTs. In particular, we plan to augment the PI System developed by OSIsoft with ontologies and show how the resulting solution can help in simplifying analytical and machine learning routines for DTs.},
  keywords={Data models;Semantics;Analytical models;Computational modeling;Context modeling;Ontologies;Turbines},
  doi={10.1109/BigData.2018.8622503},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8612870,
  author={Bouzidi, Aljia and Haddar, Nahla and Ben Abdallah, Mounira and Haddar, Kais},
  booktitle={2018 IEEE/ACS 15th International Conference on Computer Systems and Applications (AICCSA)}, 
  title={Alignment of Business Processes and Requirements Through Model Integration}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={Business process and requirement specification are crucial phases in the software development process. Yet, business and requirement modeling are often carried out separately by different languages and design teams, leading to misaligned models. The degree of misalignment grows continuously with their independent evolution. Thus, the potential of model-driven software development cannot be fully exploited. There is a considerable agreement among researchers about the integration technique roles to bridge the gap between heterogeneous models. Therefore, this approach is based on this technique to align the business world represented by BPMN and the software requirement world represented by the UML use case. We define an integrated meta-model that incorporates all BPMN and use case elements as well as new others to map traceable elements. Further, we define a new diagram that provides a means to visualize and combines their use within an integrated model. Our approach supplements CASE tools with additional information and relationships to maintain the global system consistency. To illustrate it, we implement an editor for designing the proposed diagram, and we apply it in a topical case study.},
  keywords={Unified modeling language;Business;Ontologies;Computational modeling;Software;Semantics;Bridges;meta-model integration;Alignment;business process models;requirements models;BPMN;UML use case},
  doi={10.1109/AICCSA.2018.8612870},
  ISSN={2161-5330},
  month={Oct},}@INPROCEEDINGS{8600201,
  author={Lohar, Nitin K. and Kar, Subrat},
  booktitle={2018 Twenty Fourth National Conference on Communications (NCC)}, 
  title={Control and Management of Optical Networks Using Optical Network Description Language}, 
  year={2018},
  volume={},
  number={},
  pages={1-5},
  abstract={Management and configuration of optical networks, implementing new policies to keep up with ever-changing network etc. have always been tedious tasks. Software-defined networking (SDN) has provided many network solutions in the electrical counterpart. SDN for optical networks can provide new opportunities to make the above mentioned tasks easier and faster. As a first step towards this goal, we develop an optical network description language (ONDL). We use it to describe various network components, and their configuration and run-time states, such as modulation schemes, wavelength and spectral-width of a transponder, switching matrix of an optical switch etc. The language is based on resource description framework (RDF). Furthermore, we develop a controller which understands and sends instructions in this language to different network devices to provide/change their states. We show the applicability of ONDL by simulating a network, controlling and managing its nodes using ONDL and developed controller.},
  keywords={Optical switches;Optical fiber networks;Resource description framework;Modulation},
  doi={10.1109/NCC.2018.8600201},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{8595070,
  author={Tueno Fotso, Steve Jeffrey and Frappier, Marc and Laleau, Régine and Mammar, Amel},
  booktitle={2018 23rd International Conference on Engineering of Complex Computer Systems (ICECCS)}, 
  title={Back Propagating B System Updates on SysML/KAOS Domain Models}, 
  year={2018},
  volume={},
  number={},
  pages={160-169},
  abstract={Nowadays, the usefulness of the formal verification and validation of system specifications is well established, at least for critical systems. However, one of the main obstacles to their adoption lies in obtaining the formal specification of the system, and, in the case of refinement-based formal methods such as B System or Event-B, in obtaining the most abstract specification that heads the development of the system. The SysML/KAOS requirements engineering method is proposed to overcome this difficulty. It includes a goal modeling language to model requirements from stakeholders needs. Translation rules from a goal model to a B System specification have already been defined. They allow to obtain a skeleton of the system specification. To complete it, a language has been defined to express the domain model associated to the goal model. Its translation gives the structural part of the B System specification. However, it very often appears that new elements must be added in the B System specification obtained from SysML/KAOS models, discovered for instance when specifying the body of events and/or by using formal validation and/or verification tools. We have therefore defined a set of rules allowing the back propagation, within domain models, of every newly added element. This paper describes these rules and how they are specified in Event-B. Their consistency is proved using the Rodin tool. We show that they are structure preserving: two related elements within the B System specification remain related within the domain model. This is done by proving various isomorphisms between the B System specification and the domain models.},
  keywords={Optimized production technology;Backpropagation;Actuators;Boilers;Sensor phenomena and characterization;Requirements engineering;Requirements Engineering;Domain Modeling;SysML/KAOS;Event-B;B System},
  doi={10.1109/ICECCS2018.2018.00025},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8595067,
  author={Singh, Neeraj Kumar and Ait-Ameur, Yamine and Mery, Dominique},
  booktitle={2018 23rd International Conference on Engineering of Complex Computer Systems (ICECCS)}, 
  title={Formal Ontology Driven Model Refactoring}, 
  year={2018},
  volume={},
  number={},
  pages={136-145},
  abstract={Refactoring, successfully used in the field of programming, can be used in maintenance and restructuring of the large and complex models. In this paper, we present a novel approach for model refactoring and a set of modelling patterns that are applicable for refinement-based formal development. In order to carry out this study, we investigate the previously developed large and complex model and required ontology to develop a domain model and a refactored system model. Further, we use the Rodin tools to check the internal consistency with respect to the desired functional behaviour and the required safety properties. Our main contributions are: to develop a refactoring technique related to the correct by construction approach; to use the domain specific knowledge in a system model explicitly; to define a set of modelling patterns; and to define a restructuring mechanism in the formal development. Finally, this proposed approach is evaluated through a complex medical case study: ECG clinical assessment protocol.},
  keywords={Ontologies;Computational modeling;Safety;Semantics;Tools;Electrocardiography;Analytical models;Refactoring, refinement and proofs, ontologies, do main theories, Event-B},
  doi={10.1109/ICECCS2018.2018.00022},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8588250,
  author={Papazoglou, Michael P.},
  booktitle={2018 Sixth International Conference on Enterprise Systems (ES)}, 
  title={Metaprogramming Environment for Industry 4.0}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={Industry 4.0 is blurring the lines between the physical, and digital spheres of global production systems. Industry 4.0 sets the foundations for a completely connected factories that are characterized by the digitization and interconnection of supply chains, production equipment and production lines, and the application of the latest advanced digital information technologies to manufacturing activities. To fully realize the promise of Industry 4.0, disparate manufacturing systems, devices, data and processes need to connect, communicate, and interoperate. This paper has dual purpose. It first introduces a model-based engineering that enables a concurrent, collaborative design process where users examine and define requirements, propose solution architectures, demonstrate and exchange ideas with stakeholders, and consider product feature tradeoffs. Subsequently, it proposes a novel programming paradigm and a flexible environment that helps developers develop design-to-production industrial automation solutions by employing structured higher-level modular software techniques.},
  keywords={Manufacturing;Industries;Solid modeling;Supply chains;Three-dimensional displays;Real-time systems;Industry 4.0;Enterprise Knowledge Engineering;Information integration and interoperability;Model-based development;Digital twins;Meta-programming},
  doi={10.1109/ES.2018.00008},
  ISSN={2572-6609},
  month={Oct},}@INPROCEEDINGS{8588279,
  author={Kurjakovic, Sabrina and Hinkelmann, Knut},
  booktitle={2018 Sixth International Conference on Enterprise Systems (ES)}, 
  title={Enterprise Architecture Driven and User-Friendly SaaS Service Selection}, 
  year={2018},
  volume={},
  number={},
  pages={196-203},
  abstract={In the last decade the number of cloud services has grown significantly, and its nature challenges the traditional process of requirements elicitation for software solutions. Most of the conducted cloud research address non-functional requirements, such as payment strategies, performance or security. Methods are required which provide an appropriate level of abstraction for describing the functional aspects of cloud services. We suggest an approach which exploits the potential of enterprise architecture modeling for cloud service selection. This study contributes to cloud service selection in two ways. First, we suggest a concept for the description of functional requirements for vertical SaaS services. Second, we provide a method that enables various kinds of stakeholders to describe their requirements considering their individual knowledge level.},
  keywords={Software as a service;Stakeholders;Ontologies;Biological system modeling;Computer architecture;enterprise architecture;enterprise ontologies;business IT alignment;SaaS service selection},
  doi={10.1109/ES.2018.00037},
  ISSN={2572-6609},
  month={Oct},}@INPROCEEDINGS{8577518,
  author={Wan, Guangxi and Wang, Peng and Xue, Lingling and Zeng, Peng},
  booktitle={2018 IEEE 3rd Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, 
  title={An Integrated Design Method For Cyber-Physical Production Systems}, 
  year={2018},
  volume={},
  number={},
  pages={791-796},
  abstract={The industrial cyber-physical production system contains a variety of heterogeneous devices and models, which increase the complexity of system design, particularly in software programming. Although the proposed model-driven engineering (MDE) is applied to industrial automation for this issue, the inherent complex dependencies and constraints between models and the availability of suitable tools for code generation limit the application of the model-driven approach in practice, especially as the system becomes more enormous. The component-based design (CBD) approach, which is a bottom-up approach, in contrast to the MDE that is top-down, is proposed to manage the complexity of software aspect based on the idea that building a system from existing components instead of the scratch. However, CBD can't provide the whole system model, so other ways should be adopted to design the system structure in advance. This paper presents an integrated design approach based on the combination of CBD and MDE to realize auto-design for cyber-physical production systems.},
  keywords={Unified modeling language;Automation;Control systems;Software;Ontologies;Testing;Integrated design;Component-based Design;IEC 61499;Industrial automation;Model-based design;Ontology;Semantic technology},
  doi={10.1109/IAEAC.2018.8577518},
  ISSN={2381-0947},
  month={Oct},}@INPROCEEDINGS{8569238,
  author={Helmke, Hartmut and Slotty, Michael and Poiger, Michael and Herrer, Damián Ferrer and Ohneiser, Oliver and Vink, Nathan and Cerna, Aneta and Hartikainen, Petri and Josefsson, Billy and Langr, David and Lasheras, Raquel García and Marin, Gabriela and Mevatne, Odd Georg and Moos, Sylvain and Nilsson, Mats N. and Pérez, Mario Boyero},
  booktitle={2018 IEEE/AIAA 37th Digital Avionics Systems Conference (DASC)}, 
  title={Ontology for Transcription of ATC Speech Commands of SESAR 2020 Solution PJ.16-04}, 
  year={2018},
  volume={},
  number={},
  pages={1-10},
  abstract={Nowadays Automatic Speech Recognition (ASR) applications are increasingly successful in the air traffic (ATC) domain. Paramount to achieving this is collecting enough data for speech recognition model training. Thousands of hours of ATC communication are recorded every day. However, the transcription of these data sets is resource intense, i.e. writing down the sequence of spoken words, and more importantly, interpreting the relevant semantics. Many different approaches including CPDLC (Controller Pilot Data Link Communications) currently exist in the ATC community for command transcription, a fact that e.g. complicates exchange of transcriptions. The partners of the SESAR funded solution PJ.16-04 are currently developing on a common ontology for transcription of controller-pilot communications, which will harmonize integration of ASR into controller working positions. The resulting ontology is presented in this paper.},
  keywords={Speech recognition;Training;Ontologies;Standards;Radar;Aircraft;Strips;Automatic Speech Recognition (ASR);CWP HMI;Transcription;Controller Command;Ontology;SESAR;PJ.16-04},
  doi={10.1109/DASC.2018.8569238},
  ISSN={2155-7209},
  month={Sep.},}@INPROCEEDINGS{8549977,
  author={Albab, Muhammad Ulil and Arman, Arry Akhmad},
  booktitle={2018 International Conference on ICT for Smart Society (ICISS)}, 
  title={Resource-Based and Value-Based Extension for Archimate}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={The concept of resource and value in the archimate that is too general allows for modeling and communication problems. Therefore, an ontological analysis is needed to evaluate and redesign the concept of resource and value on the archimate. In this paper, Unified foundational ontology (UFO) is used in conducting an ontological analysis on the concept of resource and value. An ontological analysis is done by considering the mapping between the concept on Archimate and ontology concept on UFO. Semantic problems in resource and value concepts are found when the ontological analysis is performed. The result of this study is Archimate extension with the improvement of resource and value concept. The resulting extension was successfully applied in case studies and was able to handle semantic problems that were identified.},
  keywords={Ontologies;Semantics;Business;Analytical models;Electrical engineering;Informatics;Information technology;resource;value;enterprise architecture modeling;ontology-based semantics;Archimate},
  doi={10.1109/ICTSS.2018.8549977},
  ISSN={},
  month={Oct},}@ARTICLE{8540835,
  author={Gómez, Francisco J. and Aguilera Chaves, Miguel and Vanfretti, Luigi and Olsen, Svein Harald},
  journal={IEEE Access}, 
  title={Multi-Domain Semantic Information and Physical Behavior Modeling of Power Systems and Gas Turbines Expanding the Common Information Model}, 
  year={2018},
  volume={6},
  number={},
  pages={72663-72674},
  abstract={Due to the rapid increase of intermittent energy resources (IERs), there is a need to have dispatchable production available to ensure secure operation and increase opportunity for energy system flexibility. Gas turbine-based power plants offer flexible operation that is being improved with new technology advancements. Those plants provide, in general, quick start together with significant ramping capability, which can be exploited to balance IERs. Consequently, to understand the potential source of flexibility, better models for gas turbines are required for power system studies and analysis. In this paper, both the required semantic information and physical behavior models of such multi-domain systems are considered. First, UML class diagrams and RDF schemas based on the common information model standards are used to describe the semantic information of the electrical power grid. An extension that exploits the ISO 15926 standard is proposed herein to derive the multi-domain semantics required by integrated electrical power grid with detailed gas turbine dynamic models. Second, the Modelica language is employed to create the equation-based models, which represent the behavior of a multi-domain physical system. A comparative simulation analysis between the power system domain model and the multi-domain model has been performed. Some differences between the turbine dynamics representation of the commonly used GGOV1 standard model and a more detailed gas turbine model are shown.},
  keywords={Unified modeling language;Mathematical model;Power system dynamics;Semantics;Turbines;Standards;Common Information Model (electricity);CIM;cyber-physical systems;dynamic simulation;equation-based modeling;IEC 61970;information modeling;ISO 15926;Modelica;power systems simulation;power systems modeling},
  doi={10.1109/ACCESS.2018.2882311},
  ISSN={2169-3536},
  month={},}@ARTICLE{8540089,
  author={Zhou, Bo and Maines, Curtis and Tang, Stephen and Shi, Qi and Yang, Po and Yang, Qiang and Qi, Jun},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={A 3-D Security Modeling Platform for Social IoT Environments}, 
  year={2018},
  volume={5},
  number={4},
  pages={1174-1188},
  abstract={Social Internet-of-Things (SIoT) environment comprises not only smart devices but also the humans who interact with these IoT devices. The benefits of such system are overshadowed due to the cyber security issues. A novel approach is required to understand the security implication under such a dynamic environment while taking both the social and technical aspects into consideration. This paper addressed such challenges and proposed a 3-D security modeling platform that can capture and model the security requirements in the SIoT environment. The modeling process is graphical notation based and works as a security extension to the Business Process Model and Notation. Still, it utilizes the latest 3-D game technology; thus, the security extensions are generated through the third dimension. Consequently, the introduction of security extensions will not increase the complexity of the original SIoT scenario, while keeping all the key information on the same platform. Together with the proposed security ontology, these comprehensive security notations created a unique platform that aims at addressing the ever complicated security issues in the SIoT environment.},
  keywords={Ontologies;Security;Complexity theory;Internet of Things;Process modeling;Business process;game technology;notation;security modeling;social Internet of Things (SIoT)},
  doi={10.1109/TCSS.2018.2878921},
  ISSN={2329-924X},
  month={Dec},}@INPROCEEDINGS{8536159,
  author={Prince Sales, Tiago and Almeida, João Paulo A. and Santini, Sebastiano and Baião, Fernanda and Guizzardi, Giancarlo},
  booktitle={2018 IEEE 22nd International Enterprise Distributed Object Computing Conference (EDOC)}, 
  title={Ontological Analysis and Redesign of Risk Modeling in ArchiMate}, 
  year={2018},
  volume={},
  number={},
  pages={154-163},
  abstract={Risk analysis is a complex and critical activity in various contexts, ranging from strategic planning to IT systems operation. Given its complexity, several Enterprise Architecture (EA) frameworks and modeling languages have been developed to help analysts in representing and analyzing risks. Yet, the notion of risk remains overloaded and conceptually unclear in most of them. In this paper, we investigate the real-world semantics underlying risk-related constructs in one of such approaches, namely ArchiMate's Risk and Security Overlay (RSO). We perform this investigation by means of ontological analysis to reveal semantic limitations in the overlay, such as ambiguity and missing constructs. Building on the results of this analysis, we propose a well-founded redesign of the risk modeling aspects of the RSO.},
  keywords={Security;Organizations;Ontologies;Risk management;Analytical models;Semantics;Risk Modeling;Enterprise Architecture;ArchiMate;Ontological Analysis;Unified Foundational Ontology},
  doi={10.1109/EDOC.2018.00028},
  ISSN={2325-6362},
  month={Oct},}@ARTICLE{8502923,
  author={Halawi, Bahia and Mourad, Azzam and Otrok, Hadi and Damiani, Ernesto},
  journal={IEEE Access}, 
  title={Few are as Good as Many: An Ontology-Based Tweet Spam Detection Approach}, 
  year={2018},
  volume={6},
  number={},
  pages={63890-63904},
  abstract={Due to the high popularity of Twitter, spammers tend to favor its use in spreading their commercial messages. In the context of detecting twitter spams, different statistical and behavioral analysis approaches were proposed. However, these techniques suffer from many limitations due to: 1) ongoing changes to Twitter's streaming API which constrains access to a user's list of followers/followees; 2) spammer's creativity in building diverse messages; 3) use of embedded links and new accounts; and 4) need for analyzing different characteristics about users without their consent. To address the aforementioned challenges, we propose a novel ontology-based approach for spam detection over Twitter during events by analyzing the relationship between ham user tweets versus spams. Our approach relies solely on public tweet messages while performing the analysis and classification tasks. In this context, ontologies are derived and used to generate a dictionary that validates real tweet messages from random topics. Similarity ratio among the dictionary and tweets is used to reflect the legitimacy of the messages. Experiments conducted on real tweet data illustrate that message-to-message techniques achieved a low detection rate compared with our ontology-based approach which outperforms them by approximately 200%, in addition to promising scalability for large data analysis.},
  keywords={Twitter;Ontologies;Electronic mail;Feature extraction;Uniform resource locators;Tagging;Analytical models;Twitter;meta-data;spam detection;text based analysis;event spammers;ontology},
  doi={10.1109/ACCESS.2018.2877685},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8501304,
  author={Shakeri Hossein Abad, Zahra and Gervasi, Vincenzo and Zowghi, Didar and Barker, Ken},
  booktitle={2018 5th International Workshop on Artificial Intelligence for Requirements Engineering (AIRE)}, 
  title={ELICA: An Automated Tool for Dynamic Extraction of Requirements Relevant Information}, 
  year={2018},
  volume={},
  number={},
  pages={8-14},
  abstract={Requirements elicitation requires extensive knowledge and deep understanding of the problem domain where the final system will be situated. However, in many software development projects, analysts are required to elicit the requirements from an unfamiliar domain, which often causes communication barriers between analysts and stakeholders. In this paper, we propose a requirements ELICitation Aid tool (ELICA) to help analysts better understand the target application domain by dynamic extraction and labeling of requirements-relevant knowledge. To extract the relevant terms, we leverage the flexibility and power of Weighted Finite State Transducers (WFSTs) in dynamic modeling of natural language processing tasks. In addition to the information conveyed through text, ELICA captures and processes non-linguistic information about the intention of speakers such as their confidence level, analytical tone, and emotions. The extracted information is made available to the analysts as a set of labeled snippets with highlighted relevant terms which can also be exported as an artifact of the Requirements Engineering (RE) process. The application and usefulness of ELICA are demonstrated through a case study. This study shows how pre-existing relevant information about the application domain and the information captured during an elicitation meeting, such as the conversation and stakeholders' intentions, can be captured and used to support analysts achieving their tasks.},
  keywords={Tools;Stakeholders;Task analysis;Interviews;Ontologies;Feature extraction;Data mining;Requirements elicitation, Natural language processing, Tool support, Dynamic information extraction},
  doi={10.1109/AIRE.2018.00007},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{8501495,
  author={Rabinia, Amin and Ghanavati, Sepideh},
  booktitle={2018 IEEE 8th International Model-Driven Requirements Engineering Workshop (MoDRE)}, 
  title={The FOL-Based Legal-GRL (FLG) Framework: Towards an Automated Goal Modeling Approach for Regulations}, 
  year={2018},
  volume={},
  number={},
  pages={58-67},
  abstract={In recent years, several goal modeling approaches have been used and extended to capture the complexity of legal requirements and help modeling them in notations familiar to the requirements engineers and analysts. Legal-GRL, which is an extension of the Goal-oriented Requirements Language (GRL), is used for modeling and analyzing legal requirements. However, creating Legal-GRL models is still a manual process, which limits its effectiveness and scalability. In this paper, we propose a new goal modeling framework based on GRL to facilitate the automation of the legal requirements modeling process. Our FOL-based Legal-GRL (FLG) framework uses a legal ontology, which entails a modal theory and First-order Logic (FOL) approach, for the purpose of extraction, refinement, and representation of legal requirements. Our FLG framework consists of a database design and a set of methods for automating the modeling process. We evaluate our work by modeling several statements from HIPAA, PHIPA, the EU GDPR and EU-US Privacy Shield.},
  keywords={Law;Ontologies;Analytical models;Complexity theory;Computational modeling;Databases;Goal Modeling, Legal Requirements, First Order Logic, GRL},
  doi={10.1109/MoDRE.2018.00014},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{8482078,
  author={Ali, Abbas Raza},
  booktitle={2018 IEEE 17th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)}, 
  title={Cognitive Computing to Optimize IT Services}, 
  year={2018},
  volume={},
  number={},
  pages={54-60},
  abstract={In this paper, the challenges of maintaining a healthy IT operational environment have been addressed by proactively analyzing IT Service Desk tickets, customer satisfaction surveys and social media data. A Cognitive solution goes beyond the traditional structured data analysis solutions by deep analyses of both structured and unstructured text. The salient features of the proposed platform include language identification, translation, hierarchical extraction of the most frequently occurring topics, entities and their relationships, text summarization, sentiments and knowledge extraction from the unstructured text using Natural Language Processing techniques. Moreover, the insights from unstructured text combined with structured data allows the development of various classification, segmentation and time-series forecasting use-cases on incident, problem and change datasets. The text and predictive insights together with raw data are used for visualization and exploration of actionable insights on a rich and interactive dashboard. However, it is hard not only to find these insights using traditional Analytics solutions but it might also take very long time to discover them, especially while dealing with massive amount of unstructured data. By taking actions on these insights, organizations can benefit from significant reduction of ticket volume, reduced operational costs and increased customer satisfaction. In various experiments, on average, up to 18-25 % of yearly ticket volume has been reduced using the proposed approach.},
  keywords={Feature extraction;Cognitive systems;Customer satisfaction;Data visualization;Semantics;Servers;Social network services;Knowledge Extraction;Optimizing IT Services;Cognitive Computing;Topic Clustering;Semantic Text Analytics;Service Desk},
  doi={10.1109/ICCI-CC.2018.8482078},
  ISSN={},
  month={July},}@INPROCEEDINGS{8472069,
  author={López, Mikel and Martín, Jon and Gangoiti, Unai and Armentia, Aintzane and Estévez, Elisabet and Marcos, Marga},
  booktitle={2018 IEEE 16th International Conference on Industrial Informatics (INDIN)}, 
  title={Supporting Product Oriented Manufacturing: a Model Driven and Agent based Approach}, 
  year={2018},
  volume={},
  number={},
  pages={133-139},
  abstract={Product oriented manufacturing is aligned with one of the Industry 4.0 trends consisting of integrating all production systems. This implies shifting from a traditional view of manufacturing processes focused on production line in order to reduce costs, to a more flexible and customized product manufacturing. On the other hand, Multi Agent Systems (MAS) have been proved to be a suitable way to fulfill these requirements. However, a key aspect of the use of novel technologies is to offer methodologies and tools for supporting the implementation of such systems. In this sense, this paper uses Model Driven Engineering and MAS technology to propose an architecture that is able to launch and execute a manufacturing execution plan. It focuses on the information models managed by architecture agents that can be customized to particular manufacturing plants as well as on the definition of agent templates.},
  keywords={Manufacturing processes;Tools;Ontologies;Systems engineering and theory;Industries;product oriented manufacturing;multi agent system;model driven engineering},
  doi={10.1109/INDIN.2018.8472069},
  ISSN={2378-363X},
  month={July},}@INPROCEEDINGS{8453933,
  author={Gand, Kai},
  booktitle={2018 IEEE 20th Conference on Business Informatics (CBI)}, 
  title={Towards Conceptual Enhancements of the Business Model Canvas: The Case of Health Information Technology}, 
  year={2018},
  volume={02},
  number={},
  pages={62-71},
  abstract={Business models (BM) describe mechanisms of value creation, delivery and capture. Business Model Representation (BMRs) in terms of conceptual models systematically and formally visualize BMs. The field of BMRs is characterized by conceptual inconsistencies such as heterogeneous notations and insufficiently described semantics. Describing and constructing structured and comparable BMs capturing concrete business cases is hampered. Even the success of the most prominent BMR approach - the business model canvas (BMC) - could only lead to slight advances. So, we aim to mature research on BMRs by enhancing the BMC with further conceptual modeling's concepts. We will focus on Health Information Technology (HIT) initiatives that are prone to fail. Analyses showed that the domain's networked nature of value creation requires to link and interrelate the BM's building blocks. Current BMR approaches, including the BMC, can only hardly provide such. For addressing BMC's conceptual weaknesses, we propose to substantiate the BMC by means of a layered concept making use of conceptual modeling's principles and considering the domain's specifics to a greater extent. Systematic and HITspecific BM(R) approaches are worthwhile as these identify and map all relevant stakeholders as well as their interrelations for value creation. That is a primary prerequisite for sustainable HIT solutions. So, we aim at bringing necessary complex information into a structured concept that allows to make profound managerial decisions. In turn, this provides more profound guidance for the implementation of HIT solutions that may lead to a higher success rates of such.},
  keywords={Organizations;Visualization;Stakeholders;Semantics;Information systems;Information technology;Business Models;Business Model Representations;Conceptual Modeling;Health Information Technology},
  doi={10.1109/CBI.2018.10047},
  ISSN={2378-1971},
  month={July},}@INPROCEEDINGS{8432294,
  author={Machado Lunardi, Gabriel and Medeiros Machado, Guilherme and Al Machot, Fadi and Maran, Vinícius and Machado, Alencar and C. Mayr, Heinrich and A. Shekhovtsov, Vladimir and Palazzo M. de Oliveira, José},
  booktitle={2018 IEEE 32nd International Conference on Advanced Information Networking and Applications (AINA)}, 
  title={Probabilistic Ontology Reasoning in Ambient Assistance: Predicting Human Actions}, 
  year={2018},
  volume={},
  number={},
  pages={593-600},
  abstract={Providing reminders to elderly people in their home environment, while they perform their daily activities, is considered as a user support activity, and thus a relevant topic in Active and Assisted Living (AAL) research and development. Determining such reminders implies decision-making, since the actions' flow (behavior) usually involves probabilistic branches. An automated system needs to decide which of the next actions is the best one for the user in a given situation. Problems of this nature involve uncertainty levels that have to be dealt with. Many approaches to this problem exploit statistical data only, thus ignoring important semantic data as, for instance, are provided by Ontologies. However, ontologies do not support reasoning over uncertainty natively. In this paper, we present a probabilistic semantic model that enables reasoning over uncertainty without losing semantic information. This model will be exemplified by an extension of the Human Behavior Monitoring and Support [HBMS] approach that provides a conceptual model for representing the user's behavior and its context in her/his living environment. The performance of this approach was evaluated using real data collected from a smart home prototype equipped with sensors. The experiments provided promising results which we will discuss regarding limits and challenges to overcome.},
  keywords={Ontologies;Uncertainty;Cognition;Probabilistic logic;Semantics;Hidden Markov models;Context modeling;Probabilistic ontologies;Uncertainty reasoning;Ambient assistance;Smart home;Context awareness},
  doi={10.1109/AINA.2018.00092},
  ISSN={2332-5658},
  month={May},}@INPROCEEDINGS{8406642,
  author={Figueiredo, Guylerme and Duchardt, Amelie and Hedblom, Maria M. and Guizzardi, Giancarlo},
  booktitle={2018 12th International Conference on Research Challenges in Information Science (RCIS)}, 
  title={Breaking into pieces: An ontological approach to conceptual model complexity management}, 
  year={2018},
  volume={},
  number={},
  pages={1-10},
  abstract={In recent years, there has been a growth in the use of reference conceptual models, in general, and domain ontologies, in particular, to capture information about complex and critical domains. These models play a fundamental role in different types of critical semantic interoperability tasks. Therefore, it is essential that domain experts are able to understand and reason using the models' content. In other words, it is important that conceptual models are cognitively tractable. However, it is unavoidable that when the information of the represented domain grows, so does the size and complexity of the artifacts and models that represent them. For this reason, more sophisticated techniques for complexity management in ontology-driven conceptual models, need to be developed. Some approaches are based on the notion of model modularization. In this paper, we follow the work on model modularization to present an approach for view extraction for the ontology-driven conceptual modeling language OntoUML. We provide a formal definition for ontological views over OntoUML conceptual models that completely leverages on the ontologically well-grounded real-world semantics of that language. Moreover, we present a plug-in tool, particularly developed for an OntoUML model-based editor that implements this formal view structure in terms of queries defined over the OntoUML metamodel embedded in that tool.},
  keywords={Unified modeling language;Ontologies;Semantics;Complexity theory;Marine vehicles;Tools;Computational modeling;Conceptual Model Modularization;Ontological Views;Complexity Management in Conceptual Modeling;On-toUML},
  doi={10.1109/RCIS.2018.8406642},
  ISSN={2151-1357},
  month={May},}@INPROCEEDINGS{8400301,
  author={Törsleff, Sebastian and Hildebrandt, Constantin and Daun, Marian and Brings, Jennifer and Fay, Alexander},
  booktitle={2018 4th International Workshop on Emerging Ideas and Trends in the Engineering of Cyber-Physical Systems (EITEC)}, 
  title={Developing Ontologies for the Collaboration of Cyber-Physical Systems: Requirements and Solution Approach}, 
  year={2018},
  volume={},
  number={},
  pages={25-32},
  abstract={Cyber-physical systems that interact with each other and form groups to achieve joint goals, are referred to as collaborative cyber-physical systems. They are exposed to highly dynamic and open contexts only partly known at design time. While model-based systems engineering already helped overcome various challenges regarding the development of conventional cyber-physical systems, challenges remain with respect to highly collaborative systems. This paper proposes an approach for modeling the context of collaborative cyber-physical systems and generating ontologies that they can use at runtime to communicate with each other and perform context-related reasoning. To account for the broad domain scope of such systems and unforeseeable future applications, the approach is not limited to a static set of prescribed ontological concepts. Instead, it facilitates the incorporation of custom concepts to meet the individual requirements of a broad spectrum of development projects concerned with collaborative cyber-physical systems.},
  keywords={Unified modeling language;Context modeling;Runtime;Ontologies;Cyber-physical systems;Collaboration;Embedded Systems;Ontologies;Collaborations;Distributed Control;Model-Based Systems Engineering;DSML;Context Modeling;Context Awareness},
  doi={10.1109/EITEC.2018.00009},
  ISSN={},
  month={April},}@INPROCEEDINGS{8374390,
  author={Bendib, Issam and Laouar, Mohammed Ridda},
  booktitle={2018 2nd International Conference on Natural Language and Speech Processing (ICNLSP)}, 
  title={A semantic indexing approach of multimedia documents content based partial transcription}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={The complexity of searching and indexing spoken document retrieval depends closely on content structure and access strategies. However, Spoken Document Retrieval (SDR) approaches based on the transcription of content by LVCSR systems must treat the impact of recognition errors on the performance of the information retrieval system. Currently, several multimedia resources (spoken document) are unexploited, because the errors generated in the transcription process, decrease the performance of the systems of searches for spoken documents. Our contribution in this paper is the proposition a novel approach semantic based indexing content of multimedia documents by using the results of a partial automatic transcription to ride out research and indexing problems on these resources. In this context, the main hypothesis is developed around the following question: is it possible to identify and retrieve spoken document by using a semantic content indexing system based on his partial transcription? Also, does the partial content transcription of a spoken document is sufficient for his indexing? First, we are interested to look for a linguistic syntactic representation of the whole of the content spoken document. Then, it will be used to define the most discriminating indexing terms for his content. Nevertheless, in our approach, we have an assumption that the use of segments of a spoken document with an efficient semantic enrichment process instead of the whole spoken document content is sufficient. Also, with this strategy, we can resolve OOV problems, recognition errors and technical terms. Finally, in validation and experimentation phase, we tested the different modules proposed on the TED-LIUM corpora. The results obtained are interesting and encourage us to lead off our future perspectives.},
  keywords={Semantics;Indexing;Multimedia communication;Speech recognition;Syntactics;Lattices;Semantic content Indexing;Spoken Term detection;L VCSR;Keyword Spotting;Spoken Document;WordNet Ontology;similarity measures},
  doi={10.1109/ICNLSP.2018.8374390},
  ISSN={},
  month={April},}@INPROCEEDINGS{8369586,
  author={Blackburn, Mark R. and Austin, Mark A. and Coelho, Maria},
  booktitle={2018 Annual IEEE International Systems Conference (SysCon)}, 
  title={Modeling and cross-domain dependability analysis of cyber-physical systems}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper discusses a novel method of modeling and formal verification to support dependability analyses. The method is demonstrated in an example of a fault management capability of robots that interacts with equipment and humans. Hazard analyses produce derived requirements for fault management capabilities. These include safety critical functions for collision avoidance and temporary autonomy. Derived requirements are represented formally in models that are used to produce dependability evidence using theorem proving, model-based test vector generation, test execution with code coverage analysis, and requirement-to-test traceability. To address the challenges of heterogeneity of modeling tools and languages, Semantic Web Technologies are used for model composition and model transformation from modeling tools to formal analysis tools.},
  keywords={Object oriented modeling;Analytical models;Robots;Mathematical model;Tools;Software;Hazards;component;formatting;style;styling;formal methods;modeling;formal verification;dependability;fault management;cyber physical systems},
  doi={10.1109/SYSCON.2018.8369586},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{8334444,
  author={Weigelt, Sebastian and Hey, Tobias and Steurer, Vanessa},
  booktitle={2018 IEEE 12th International Conference on Semantic Computing (ICSC)}, 
  title={Detection of Conditionals in Spoken Utterances}, 
  year={2018},
  volume={},
  number={},
  pages={85-92},
  abstract={State-of-the-art intelligent assistant systems such as Siri & Co. struggle with conditionals. They reliably react to ordinary commands. However, their architectures are not designed to cope with complex conditional queries. We propose a system to overcome these limitations. Our approach models if-then-else constructs in spoken utterances explicitly. The model bridges the gap between linguistic and programmatic semantics. To proof our concept, we apply a rule-based approach to extract conditionals. For our prototype we use part-of-speech and chunk tags provided by NLP tools. We make use of coreference information to determine the reference frame of a condition. The explicit modeling of conditionals allows us to evaluate the accuracy of our approach independently from other language understanding tasks. The prototype works well in the domain of humanoid robotics. In a user study we achieve F1 scores of 0.783 (automatic speech recognition) up to 0.898 (manual transcripts) on unrestricted utterances.},
  keywords={Semantics;Task analysis;Robots;Linguistics;Tools;Ontologies;Spoken Language Interfaces;Spoken Language Understanding;Language Model;Programming In Natural Language;End User Programming;Natural Language Processing;Knowledge Representation;Condition Model;Conditionals;Conditional Semantics;Natural Language Understanding;Semantic Parsing},
  doi={10.1109/ICSC.2018.00021},
  ISSN={},
  month={Jan},}@ARTICLE{8246712,
  author={Hafeez Khan, Abdul and Hyder Abbas Musavi, Sayed and Rehman, Aqeel-Ur and Shaikh, Asadullah},
  journal={IEEE Access}, 
  title={Ontology-Based Finite Satisfiability of UML Class Model}, 
  year={2018},
  volume={6},
  number={},
  pages={3040-3050},
  abstract={Software models are core artifacts in model driven engineering (MDE) and processable by computer. They are automatically transformed into other models and in MDE, programming code is also produced by the models. The automatic transformation provides a systematic reuse of existing artifacts. However, sometimes models are developed with defects and the defects can implicitly shift into the code, which may be difficult to discover and repair. A promising solution to this problem is model verification. UML class model is a key ingredient of MDE. However, UML only offers graphical components without the support of reasoning, due to lack of the formal foundation. Therefore, the verification of formal properties, such as consistency and finite satisfiability is not possible in UML. This paper proposes an ontology-based optimized verification method for important correctness property “finite satisfiability”of UML class model.},
  keywords={Unified modeling language;Ontologies;Computational modeling;Object oriented modeling;Software;Business;Semantics;Finite satisfiability;model satisfiability;ontology-based satisfiability;model checking;model verification},
  doi={10.1109/ACCESS.2017.2786781},
  ISSN={2169-3536},
  month={},}@ARTICLE{7571127,
  author={Mordecai, Yaniv and Orhof, Ori and Dori, Dov},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
  title={Model-Based Interoperability Engineering in Systems-of-Systems and Civil Aviation}, 
  year={2018},
  volume={48},
  number={4},
  pages={637-648},
  abstract={Interoperability is the capability of multiple parties and systems to collaborate and exchange information and matter to obtain their objectives. Interoperability challenges call for a model-based systems engineering approach. This paper describes a conceptual modeling framework for model-based interoperability engineering (MoBIE) for systems of systems, which integrates multilayered interoperability specification, modeling, architecting, design, and testing. Treating interoperability infrastructure as a system in its own right, MoBIE facilitates interoperability among agents, processes, systems, services, and interfaces. MoBIE is founded on ISO 19450 standard-object-process methodology, a holistic paradigm for modeling and architecting complex, dynamic, and multidisciplinary systems-and allows for synergistic integration of the interoperability model with system-centric models. We also discuss the implementation of MoBIE with the unified modeling language. We discuss the importance of interoperability in the civil aviation domain, and apply MoBIE to analyze the passenger departure process in an airport terminal as a case-in-point. The resulting model enables architectural and operational decision making and analysis at the system-of-systems level and adds significant value at the interoperability engineering program level.},
  keywords={Interoperability;Unified modeling language;Atmospheric modeling;Business;Ontologies;Airport terminals;interoperability;model-based systems engineering (MBSE);object process methodology (OPM);systems-of-systems (SoS)},
  doi={10.1109/TSMC.2016.2602543},
  ISSN={2168-2232},
  month={April},}@ARTICLE{7496983,
  author={Alvarez, María Luz and Sarachaga, Isabel and Burgos, Arantzazu and Estévez, Elisabet and Marcos, Marga},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={A Methodological Approach to Model-Driven Design and Development of Automation Systems}, 
  year={2018},
  volume={15},
  number={1},
  pages={67-79},
  abstract={The growing complexity of industrial automation demands the adoption of software engineering principles for improving the development process of control systems. This paper presents a methodological approach to the design and development of complex automation systems relying on model-driven engineering (MDE). A benefit of this approach is the integration of methods and techniques widespread within the automation discipline with modern MDE techniques guiding the designer through the development phases. A second advantage is to add flexibility enough to adapt the steps to the needs of the system under design. Finally, the architecture presented is prepared to be adapted to methodology extensions to cover other aspects of automation systems. The framework is based on domain models that are defined through the development phases using the terminology of the automation field. Using model transformations both documentation about system analysis and design and the skeleton of software units are automatically generated. A proof-of-concept tool has been developed that has been tested on the design of medium-complexity projects to assess the impact of its use with respect to project documentation and maintenance.Note to Practitioners—Control software development can be considered one of the challenges in automation field for achieving leadership in the future economic market. This work presents a model-driven engineering-based approach making use of both automation and software engineering methods and techniques for developing automation control systems. The framework implements the methodology for industrial automation systems ( ${\rm MeiA}_{\bullet }$ ) for guiding developers through the development phases and generates the analysis and design documentation using domain terminology, the design documentation that involves the minimal units of design, and the program organization units in one-to-one correspondence with the minimal units of design. From a practical point of view, it should be highly emphasized that developers of automation projects benefit from more structured designs, reduced number of errors, and improved project documentation.},
  keywords={Automation;Unified modeling language;Software engineering;Documentation;Software;Production;Control systems;Engineering frameworks;IEC 61131-3;industrial automation;methodology for industrial automation systems (  ${\mathrm{MeiA}}_{\bullet}$   );model-driven engineering (MDE);PLCopen XML},
  doi={10.1109/TASE.2016.2574644},
  ISSN={1558-3783},
  month={Jan},}

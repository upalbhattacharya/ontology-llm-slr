@article{DANG2023116313,
title = {Stochastic analysis of semi-rigid steel frames using a refined plastic-hinge model and Latin hypercube sampling},
journal = {Engineering Structures},
volume = {291},
pages = {116313},
year = {2023},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2023.116313},
url = {https://www.sciencedirect.com/science/article/pii/S0141029623007289},
author = {Huy-Khanh Dang and Duc-Kien Thai and Seung-Eock Kim},
keywords = {Stochastic advanced analysis, Steel structure, Monte Carlo simulation, Latin hypercube sampling, Statistic approach, Correlation matrix, Sensitivity},
abstract = {This paper proposes a stochastic analysis model that combines the refined plastic-hinge analysis method with the Monte Carlo simulation to predict the realistic resistance of steel structures. The model treats the uncertainty quantifications of the material properties, geometry, and semi-rigid connections as independent random variables in the discretized stochastic fields using Latin hypercube sampling. The critical resistance of the structures in each simulation is determined using the general displacement control method and advanced analysis algorithms. The statistical approach is employed to compute the mean values and the standard deviation of the load-carrying capacity of the steel frames. The study finds that the uncertainties in material properties are the most sensitive, and the correlation of these parameters with the strength of structures is also significant. To improve the load and resistance factor design criterion, a resistance factor of 0.93 is suggested for the beam-columns in the frames under axial force, torsion, and biaxial bending moments. A stochastic response spectrum is proposed to define the boundary of strength, where a clearly defined plastic mechanism is attained. Furthermore, this study provides valuable insights into the stochastic resistance of steel frames, which are essential for practical engineering applications.}
}
@article{ROUNSEVELL2021967,
title = {Identifying uncertainties in scenarios and models of socio-ecological systems in support of decision-making},
journal = {One Earth},
volume = {4},
number = {7},
pages = {967-985},
year = {2021},
issn = {2590-3322},
doi = {https://doi.org/10.1016/j.oneear.2021.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S2590332221003456},
author = {Mark D.A. Rounsevell and Almut Arneth and Calum Brown and William W.L. Cheung and Olivier Gimenez and Ian Holman and Paul Leadley and Criscely Luján and Stéphanie Mahevas and Isabelle Maréchaux and Raphaël Pélissier and Peter H. Verburg and Ghislain Vieilledent and Brendan A. Wintle and Yunne-Jai Shin},
keywords = {uncertainty, scenarios, socio-ecological models, decision-making},
abstract = {Summary
There are many sources of uncertainty in scenarios and models of socio-ecological systems, and understanding these uncertainties is critical in supporting informed decision-making about the management of natural resources. Here, we review uncertainty across the steps needed to create socio-ecological scenarios, from narrative storylines to the representation of human and biological processes in models and the estimation of scenario and model parameters. We find that socio-ecological scenarios and models would benefit from moving away from “stylized” approaches that do not consider a wide range of direct drivers and their dependency on indirect drivers. Indeed, a greater focus on the social phenomena is fundamental in understanding the functioning of nature on a human-dominated planet. There is no panacea for dealing with uncertainty, but several approaches to evaluating uncertainty are still not routinely applied in scenario modeling, and this is becoming increasingly unacceptable. However, it is important to avoid uncertainties becoming an excuse for inaction in decision-making when facing environmental challenges.}
}
@article{CAO2024120622,
title = {Unraveling the molecular relevance of brain phenotypes: A comparative analysis of null models and test statistics},
journal = {NeuroImage},
volume = {293},
pages = {120622},
year = {2024},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2024.120622},
url = {https://www.sciencedirect.com/science/article/pii/S1053811924001174},
author = {Zhipeng Cao and Guilai Zhan and Jinmei Qin and Renata B. Cupertino and Jonatan Ottino-Gonzalez and Alistair Murphy and Devarshi Pancholi and Sage Hahn and Dekang Yuan and Peter Callas and Scott Mackey and Hugh Garavan},
keywords = {Imaging-transcriptomics, Imaging-derived phenotypes, Gene set analysis, Competitive null models, Self-contained null models},
abstract = {Correlating transcriptional profiles with imaging-derived phenotypes has the potential to reveal possible molecular architectures associated with cognitive functions, brain development and disorders. Competitive null models built by resampling genes and self-contained null models built by spinning brain regions, along with varying test statistics, have been used to determine the significance of transcriptional associations. However, there has been no systematic evaluation of their performance in imaging transcriptomics analyses. Here, we evaluated the performance of eight different test statistics (mean, mean absolute value, mean squared value, max mean, median, Kolmogorov-Smirnov (KS), Weighted KS and the number of significant correlations) in both competitive null models and self-contained null models. Simulated brain maps (n = 1,000) and gene sets (n = 500) were used to calculate the probability of significance (Psig) for each statistical test. Our results suggested that competitive null models may result in false positive results driven by co-expression within gene sets. Furthermore, we demonstrated that the self-contained null models may fail to account for distribution characteristics (e.g., bimodality) of correlations between all available genes and brain phenotypes, leading to false positives. These two confounding factors interacted differently with test statistics, resulting in varying outcomes. Specifically, the sign-sensitive test statistics (i.e., mean, median, KS, Weighted KS) were influenced by co-expression bias in the competitive null models, while median and sign-insensitive test statistics were sensitive to the bimodality bias in the self-contained null models. Additionally, KS-based statistics produced conservative results in the self-contained null models, which increased the risk of false negatives. Comprehensive supplementary analyses with various configurations, including realistic scenarios, supported the results. These findings suggest utilizing sign-insensitive test statistics such as mean absolute value, max mean in the competitive null models and the mean as the test statistic for the self-contained null models. Additionally, adopting the confounder-matched (e.g., coexpression-matched) null models as an alternative to standard null models can be a viable strategy. Overall, the present study offers insights into the selection of statistical tests for imaging transcriptomics studies, highlighting areas for further investigation and refinement in the evaluation of novel and commonly used tests.}
}
@article{FERNANDEZCHAVES2021107440,
title = {ViMantic, a distributed robotic architecture for semantic mapping in indoor environments},
journal = {Knowledge-Based Systems},
volume = {232},
pages = {107440},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107440},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121007024},
author = {D. Fernandez-Chaves and J.R. Ruiz-Sarmiento and N. Petkov and J. Gonzalez-Jimenez},
keywords = {Semantic maps, Robotic architecture, Mobile robots, Unity 3D, ROS, Object detection, Detectron2, Robot@Home},
abstract = {Semantic maps augment traditional representations of robot workspaces, typically based on their geometry and/or topology, with meta-information about the properties, relations and functionalities of their composing elements. A piece of such information could be: fridges are appliances typically found in kitchens and employed to keep food in good condition. Thereby, semantic maps allow for the execution of high-level robotic tasks in an efficient way, e.g. “Hey robot, Store the leftover salad”. This paper presents ViMantic, a novel semantic mapping architecture for the building and maintenance of such maps, which brings together a number of features as demanded by modern mobile robotic systems, including: (i) a formal model, based on ontologies, which defines the semantics of the problem at hand and establishes mechanisms for its manipulation; (ii) techniques for processing sensory information and automatically populating maps with, for example, objects detected by cutting-edge CNNs; (iii) distributed execution capabilities through a client–server design, making the knowledge in the maps accessible and extendable to other robots/agents; (iv) a user interface that allows for the visualization and interaction with relevant parts of the maps through a virtual environment; (v) public availability, hence being ready to use in robotic platforms. The suitability of ViMantic has been assessed using Robot@Home, a vast repository of data collected by a robot in different houses. The experiments carried out consider different scenarios with one or multiple robots, from where we have extracted satisfactory results regarding automatic population, execution times, and required size in memory of the resultant semantic maps.}
}
@article{JIANG2023102878,
title = {Robotic ultrasound imaging: State-of-the-art and future perspectives},
journal = {Medical Image Analysis},
volume = {89},
pages = {102878},
year = {2023},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.102878},
url = {https://www.sciencedirect.com/science/article/pii/S136184152300138X},
author = {Zhongliang Jiang and Septimiu E. Salcudean and Nassir Navab},
keywords = {Ultrasound imaging, Robotic ultrasound, Telesonography, Medical robotics, Orientation optimization, Path planning, Visual servoing, Compliant control, Robotic US, Robot learning, Reinforcement learning, Learning from demonstrations, Ultrasound standard plane, Ultrasound imgaing quality, Teleoperated robotic ultrasound},
abstract = {Ultrasound (US) is one of the most widely used modalities for clinical intervention and diagnosis due to the merits of providing non-invasive, radiation-free, and real-time images. However, free-hand US examinations are highly operator-dependent. Robotic US System (RUSS) aims at overcoming this shortcoming by offering reproducibility, while also aiming at improving dexterity, and intelligent anatomy and disease-aware imaging. In addition to enhancing diagnostic outcomes, RUSS also holds the potential to provide medical interventions for populations suffering from the shortage of experienced sonographers. In this paper, we categorize RUSS as teleoperated or autonomous. Regarding teleoperated RUSS, we summarize their technical developments, and clinical evaluations, respectively. This survey then focuses on the review of recent work on autonomous robotic US imaging. We demonstrate that machine learning and artificial intelligence present the key techniques, which enable intelligent patient and process-specific, motion and deformation-aware robotic image acquisition. We also show that the research on artificial intelligence for autonomous RUSS has directed the research community toward understanding and modeling expert sonographers’ semantic reasoning and action. Here, we call this process, the recovery of the “language of sonography”. This side result of research on autonomous robotic US acquisitions could be considered as valuable and essential as the progress made in the robotic US examination itself. This article will provide both engineers and clinicians with a comprehensive understanding of RUSS by surveying underlying techniques. Additionally, we present the challenges that the scientific community needs to face in the coming years in order to achieve its ultimate goal of developing intelligent robotic sonographer colleagues. These colleagues are expected to be capable of collaborating with human sonographers in dynamic environments to enhance both diagnostic and intraoperative imaging.}
}
@article{BERGES2021100222,
title = {A Semantic Approach for Big Data Exploration in Industry 4.0},
journal = {Big Data Research},
volume = {25},
pages = {100222},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100222},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000393},
author = {Idoia Berges and Víctor Julio Ramírez-Durán and Arantza Illarramendi},
keywords = {Data exploration, Industry 4.0, Ontologies},
abstract = {The growing trends in automation, Internet of Things, big data and cloud computing technologies have led to the fourth industrial revolution (Industry 4.0), where it is possible to visualize and identify patterns and insights, which results in a better understanding of the data and can improve the manufacturing process. However, many times, the task of data exploration results difficult for manufacturing experts because they might be interested in analyzing also data that does not appear in pre-designed visualizations and therefore they must be assisted by Information Technology experts. In this paper, we present a proposal materialized in a semantic-based visual query system developed for a real Industry 4.0 scenario that allows domain experts to explore and visualize data in a friendly way. The main novelty of the system is the combined use that it makes of captured data that are semantically annotated first, and a 2D customized digital representation of a machine that is also linked with semantic descriptions. Those descriptions are expressed using terms of an ontology, where, among others, the sensors that are used to capture indicators about the performance of a machine that belongs to a Industry 4.0 scenario have been modeled. Moreover, this semantic description allows to: formulate queries at a higher level of abstraction, provide customized graphical visualizations of the results based on the format and nature of the data, and download enriched data enabling further types of analysis.}
}
@article{MURAWSKY2023115953,
title = {The struggle with transnormativity: Non-binary identity work, embodiment desires, and experience with gender dysphoria},
journal = {Social Science & Medicine},
volume = {327},
pages = {115953},
year = {2023},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2023.115953},
url = {https://www.sciencedirect.com/science/article/pii/S0277953623003106},
author = {Stef Murawsky},
keywords = {Transnormativity, Non-binary, Transgender, Healthcare, Gender dysphoria, Embodiment, Medicalization, United States},
abstract = {I examine how non-binary people who have considered, or accessed, gender-affirming health care experience accountability to transnormativity using 12 in-depth interviews conducted between 2018 and 2019 in a midwestern American city. I detail how non-binary people who want to embody genders that are still largely culturally unintelligible think about identity, embodiment, and gender dysphoria. Using grounded theory methodology, I find that non-binary identity work around medicalization differs from that of transgender men and women in three primary ways: 1) regarding how they understand and operationalize gender dysphoria, 2) in relation to their embodiment goals, and 3) concerning how they experience pressure to medically transition. Non-binary people describe increased ontological uncertainty about their gender identities when researching gender dysphoria that is contextualized by an internalized sense of accountability to the transnormative expectation for medicalization. They additionally anticipate a potential medicalization paradox, where accessing gender-affirming care leads to a different type of binary misgendering and risks making their gender identities less, rather than more, culturally intelligible to others. Non-binary people also experience external accountability to transnormativity as pressure from trans and medical communities to think about dysphoria as inherently binaristic, embodied, and medically treatable. These findings indicate that non-binary people experience accountability to transnormativity differently than trans men and women. Since non-binary people and their body projects often disrupt the transnormative tropes that are the framework for trans medicine, they find trans therapeutics, and the diagnostic experience of gender dysphoria, uniquely problematic. Non-binary experiences of accountability to transnormativity indicate the need to re-center trans medicine to better accommodate non-normative embodiment desires and focus future diagnostic revisions of gender dysphoria to emphasize the social aspects of trans and non-binary experience.}
}
@article{LI2023147,
title = {Construction of constitutive model and parameter determination of green Sichuan pepper (Zanthoxylum armatum) branches},
journal = {Biosystems Engineering},
volume = {227},
pages = {147-160},
year = {2023},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2023.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1537511023000338},
author = {Yexin Li and Peng Zhuo and Haobo Jiao and Pei Wang and Lihong Wang and Chengsong Li and Qi Niu},
keywords = {Green Sichuan pepper branch, Constitutive model, Parameter determination, Finite element simulation, Experimental validation},
abstract = {The harvesting of green Sichuan pepper (Zanthoxylum armatum) is achieved by cutting the branches, and it is important to investigate the biomechanical properties of the branches for harvesting. In this study, based on the elastic mechanics of composite materials, the Yamada and Sun yield strength criterion, hardening model, and flow law were selected, and a damage factor was introduced to establish the constitutive model of green Sichuan pepper branches. This model can characterise the complex biomechanical properties of green Sichuan pepper branches, which provides a new way and saves time for green Sichuan pepper branch testing, and lays a theoretical foundation for the design and optimisation of green Sichuan pepper production machinery. Furthermore, the constitutive parameters of the green Sichuan pepper branches were measured for the first time to meet the needs of the established model. In addition, an effective bending indenter-branch finite element model was established by compiling the constitutive model into the simulation software Abaqus through the Umat subroutine, and a three-point bending simulation test was conducted on the branch of green Sichuan pepper. Finally, relative deviation from maximum pressure founded to be less than 6% by comparing the experimental results with the simulation, which proved that the model was reliable and the measured constitutive parameters were accurate. Looking forward, these findings may contribute to the development of green Sichuan pepper harvesting equipment worldwide.}
}
@article{AYED2021312,
title = {Arabic text summarization via Knapsack balancing of effective retention},
journal = {Procedia Computer Science},
volume = {189},
pages = {312-319},
year = {2021},
note = {AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.05.100},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012242},
author = {Alaidine Ben Ayed and Ismaïl Biskri and Jean-Guy Meunier},
keywords = {Natural Language Processing, Automatic Text Summarization, Combinatorial Optimization},
abstract = {This paper presents a new extractive Arabic text summarization approach based on the Knapsack balancing of effective retention. Effective retention refers to maximizing retention while prioritizing salient themes. First, text segments are mapped to salient topics, and an effective retention score is computed for every sentence. Next, the summarization task is formulated as a combinatory optimization problem. The final output is generated through the maximization of effective retention. Aka selected sentences to be part of the summary are the best ones to encode salient ideas while covering most central themes. Experimental results show that the proposed approach outperforms three state-of-the-art Arabic summarization protocols.}
}
@article{SATTI2023103213,
title = {A semantic sequence similarity based approach for extracting medical entities from clinical conversations},
journal = {Information Processing & Management},
volume = {60},
number = {2},
pages = {103213},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103213},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322003144},
author = {Fahad Ahmed Satti and Musarrat Hussain and Syed Imran Ali and Misha Saleem and Husnain Ali and Tae Choong Chung and Sungyoung Lee},
keywords = {Clinical data mining, Semantic similarity, Natural language processing},
abstract = {Clinical conversations between physicians and patients can provide a rich source of data, information, and knowledge. A plethora of tools and technologies have been developed to identify attributes of interest in unstructured text. However, identifying the name and correct value of an attribute, from real world data, in a timely manner is a nontrivial task. In this manuscript we present a novel pipeline using transfer learning, clinical concept dictionaries, and pattern matching to provide an end-to-end solution for identifying attributes and extracting their values from natural clinical text. On real-world data, with 1176 instances, we achieve an accuracy of 56.21%, which is  3% higher than the baseline methodology.}
}
@article{KOVALEV202252,
title = {Vector Semiotic Model for Visual Question Answering},
journal = {Cognitive Systems Research},
volume = {71},
pages = {52-63},
year = {2022},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041721000632},
author = {Alexey K. Kovalev and Makhmud Shaban and Evgeny Osipov and Aleksandr I. Panov},
keywords = {Vector-symbolic architecture, Semiotic approach, Symbol grounding problem, Causal network, Visual Question Answering},
abstract = {In this paper, we propose a Vector Semiotic Model as a possible solution to the symbol grounding problem in the context of Visual Question Answering. The Vector Semiotic Model combines the advantages of a Semiotic Approach implemented in the Sign-Based World Model and Vector Symbolic Architectures. The Sign-Based World Model represents information about a scene depicted on an input image in a structured way and grounds abstract objects in an agent’s sensory input. We use the Vector Symbolic Architecture to represent the elements of the Sign-Based World Model on a computational level. Properties of a high-dimensional space and operations defined for high-dimensional vectors allow encoding the whole scene into a high-dimensional vector with the preservation of the structure. That leads to the ability to apply explainable reasoning to answer an input question. We conducted experiments are on a CLEVR dataset and show results comparable to the state of the art. The proposed combination of approaches, first, leads to the possible solution of the symbol-grounding problem and, second, allows expanding current results to other intelligent tasks (collaborative robotics, embodied intellectual assistance, etc.).}
}
@article{LI20237208,
title = {Integrating Hazard-oriented Knowledge Representation for Multilevel Flow Modelling in Process Hazard Identification},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {7208-7215},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.327},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323006948},
author = {Ruixue Li and Jing Wu and Xinxin Zhang and Ole Ravn},
keywords = {Safety Analysis, Multilevel Flow Modelling, System Control, Hazard, HAZOP, Functional Modeling, Process Systems},
abstract = {In addition to process control at the equipment level, the Identification of systemic hazards and the realization of process safety are also part of the operational problems of complex systems. To improve the effectiveness of hazard Identification based on the Multilevel Flow Modelling(MFM) models, this paper proposes that a hazard-oriented knowledge representation can represent more hazards than previous models used for online operational decision support in terms of latent intention, chemical, and physical hazard properties of materials and effects of multiple conditions in combination. Thus, the scope of application of the MFM model is extended from online operational decision support to hazard identification, and the application phase of the model is extended from the operational phase to the design phase. Moreover, the results are demonstrated by a Minxo process modeling and compared with its Hazard and Operability study(HAZOP) report results, and the representation rate of hazard of the MFM model incorporating hazard-oriented knowledge is improved, showing great potential for further optimization of the MFM-assisted HAZOP study.}
}
@incollection{FERMULLER2022373,
title = {Chapter 11 - Learning for action-based scene understanding},
editor = {E.R. Davies and Matthew A. Turk},
booktitle = {Advanced Methods and Deep Learning in Computer Vision},
publisher = {Academic Press},
pages = {373-403},
year = {2022},
series = {Computer Vision and Pattern Recognition},
isbn = {978-0-12-822109-9},
doi = {https://doi.org/10.1016/B978-0-12-822109-9.00020-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128221099000205},
author = {Cornelia Fermüller and Michael Maynord},
keywords = {Affordances, Actions, Activities, Action-based representations, Vector space embeddings},
abstract = {Action plays a central role in our lives and environments, yet most Computer Vision methods do not explicitly model action. In this chapter we outline an action-centric framework which spans multiple time scales and levels of abstraction, producing both action and scene interpretations constrained towards action consistency. At the lower level of the visual hierarchy we detail affordances – object characteristics which afford themselves to different actions. At mid-levels we model individual actions, and at higher levels we model activities through leveraging knowledge and longer term temporal relations. We emphasize the use of grasp characteristics, geometry, ontologies, and physics based constraints for generalizing to new scenes. Such explicit representations avoid overtraining on appearance characteristics. To integrate signal based perception with symbolic knowledge we align vectorized knowledge with visual features. We finish with a discussion on action and activity understanding, and discuss implications for future work.}
}
@article{SAMOURKASIDIS2020105171,
title = {A semantic approach for timeseries data fusion},
journal = {Computers and Electronics in Agriculture},
volume = {169},
pages = {105171},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.105171},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919318514},
author = {Argyrios Samourkasidis and Ioannis N. Athanasiadis},
keywords = {Environmental timeseries, Internet of Things, Legacy data, Semantic heterogeneity, Templates, FAIR data, Reasoning, Interoperability, Data reuse, APSIM, AgMIP, DSSAT, WOFOST},
abstract = {The data deluge following the rise of Internet of Things contributes towards the creation of non-reusable data silos. Especially in the environmental sciences domain, syntactic and semantic heterogeneity hinders data re-usability as most times manual labour and domain expertise is required. Both the different syntaxes under which environmental timeseries are formatted and the implicit semantics which are used to describe them contribute to this end. Usually, the real meaning of data is obscured in a combination of short data labels, titles and various value codes, that require domain or institutional knowledge to decipher. The FAIR data principles for scientific data sharing are stewardship offer a framework based on community-adopted metadata. In this work, we present the Environmental Data Acquisition Module (EDAM) which focuses on data interoperability and reuse, and deals with syntactic and semantic heterogeneity using a template approach. Data curators draft templates to describe in an abstract fashion the syntax of the timeseries datasets they want to acquire or disseminate. They complement each template with a metadata file, which is used to annotate observables and their properties (including physical quantities and units of measurement) with terms from an ontology. EDAM employs a reasoner to infer compatibility among syntactically and semantically heterogeneous datasets, and enables timeseries, format and units of measurement transformation on-the-fly. Our approach utilizes a local ontology to store metadata about datasets, which enables EDAM to acquire and transform datasets which were originally stored with different semantics and syntaxes. We demonstrate EDAM in a case study where we transform meteorological input files of four agricultural models. Our approach, allows to cut across environmental data silos and facilitate timeseries reusability, as it enables users to (a) discover datasets in other formats, (b) transform them and (c) reuse them in their scientific workflows. This directly contributes to the toolshed for FAIR data management in environmental sciences. EDAM implementation has been released under an open-source license.}
}
@article{TOILIER2024169,
title = {Driver survey vs GPS Tour data: Strength and weaknesses of the two sources in order to model the drivers’ journeys},
journal = {Transportation Research Procedia},
volume = {76},
pages = {169-182},
year = {2024},
note = {12th International Conference on Transport Survey Methods},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2023.12.047},
url = {https://www.sciencedirect.com/science/article/pii/S235214652301308X},
author = {Florence Toilier and Mathieu Gardrat},
keywords = {Declarative driver survey, GPS records, urban goods movements’ survey, travelled distances, stops location, data collection reliability},
abstract = {Large data collection campaigns are difficult to implement in a context of public finance scarcity. This is even more true for goods movements observation, as public authorities’ interest in this topic is recent and does not benefit from established survey protocols and modeling tools like those for passenger mobility. The use of GPS devices to capture goods’ mobility then appears to be an economic substitute for traditional surveys. By comparing the 2 collection modes on the same tours, this paper provides an overview of the reliability of each mode and proposes processing protocols to make the most of GPS data.}
}
@article{LIN20191880,
title = {Knowledge Reasoning for Intelligent Manufacturing Control System},
journal = {Procedia Manufacturing},
volume = {39},
pages = {1880-1888},
year = {2019},
note = {25th International Conference on Production Research Manufacturing Innovation: Cyber Physical Manufacturing August 9-14, 2019 | Chicago, Illinois (USA)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.250},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920303140},
author = {Yu-Ju Lin and Zheng-Xian Chen and Chin-Yin Huang},
keywords = {Knowledge-Driven Approach, Ontology, SCADA, Smart Factory Control, SPARQL},
abstract = {Manufacturing Control System (MCS) is regarded to have a capability to deliver a response in the manufacturing system to cope with the unexpected events, e.g., machine failures, order changes, etc. It is the core of smart factory control. However, most of today’s MCSs are still with the Supervisory Control and Data Acquisition (SCADA) architecture, which is in conflict with the flexible and adaptive characteristics. On the other hand, knowledge-driven approach is considered an alternative of flexibility, robustness, and re-configurability. By taking the knowledge-drive initiative, this research develops an intelligent MCS (iMCS). There are two cores in iMCS: Ontology and SPARQL. The ontology classifies and describes the relationships between objects of a particular domain with tree structure, whereas SPARQL plays a role of query/inference for data or metadata. This research applies SPARQL to trigger the ontology of the manufacturing system based on the given events. With such an approach, iMCS can timely respond and control the manufacturing conditions such as a new dispatching due to machine failure. iMCS adapts the manufacturing system to the dynamic situations.}
}
@article{HEISE2025103649,
title = {A graph-based systems-of-systems architecture enabling multi-scale Digital Twins for maintaining road infrastructure},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103649},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103649},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005427},
author = {Ina Heise and Sebastian Esser and André Borrmann},
keywords = {Digital Twin, Road infrastructure, Systems-of-systems, Spatial relations, Labeled Property Graphs},
abstract = {Road infrastructure constitutes a complex system characterized by multiple interacting subsystems. A comprehensive understanding of the correlations and dependencies among various road infrastructure elements is essential for enhancing infrastructure management and maintenance by providing a robust foundation for decision support. Digital Twins (DT) are recognized as effective tools for facilitating such decision-making. However, the development of comprehensive DTs considering road infrastructure in its entirety is still in its early stages. Hence, this paper focuses on the conceptualization of a digital representation of road infrastructure that enables the evaluation of relationships between the various heterogeneous subsystems. To accomplish this, we apply the systems-of-systems principle to road infrastructure. At its core, Labeled Property Graphs (LPG) are employed to capture intra-subsystem relationships and inter-system linkages, facilitating a holistic representation of interactions. Furthermore, we acknowledge the current organizational status of distributed responsibilities resulting in distributed data storage and maintenance by using the concept of federated databases. The presented approach enables multi-scale evaluations of relations among road infrastructure elements while preserving the system’s scalability and the distributed management of infrastructure data. Thus, previously separate data sets can be evaluated in relation to each other on a big scale. Doing so, the presented concept provides a foundation for extensive correlation studies between different heterogeneous infrastructure datasets. The concept is validated by applying it to a large-scale real-world data set stemming from multiple Bavarian road authorities, transferring into the proposed graph structure, and demonstrating the gained capabilities through cross-domain queries and analysis.}
}
@article{QUINN2020103257,
title = {Building automation system - BIM integration using a linked data structure},
journal = {Automation in Construction},
volume = {118},
pages = {103257},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103257},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519313172},
author = {Caroline Quinn and Ali Zargar Shabestari and Tony Misic and Sara Gilani and Marin Litoiu and J.J. McArthur},
keywords = {FM-BIM integration, Linked data, Building automation systems},
abstract = {Buildings Automation Systems (BAS) are ubiquitous in contemporary buildings, both monitoring building conditions and managing the building system control points. At present, these controls are prescriptive and pre-determined by the design team, rather than responsive to actual building performance. These are further limited by prescribed logic, possess only rudimentary visualizations, and lack broader system integration capabilities. Advances in machine learning, edge analytics, data management systems, and Facility Management-enabled Building Information Models (FM-BIMs) permit a novel approach: cloud-hosted building management. This paper presents an integration technique for mapping the data from a building Internet of Things (IoT) sensor network to an FM-BIM. The sensor data naming convention and time-series analysis strategies integrated into the data structure are discussed and presented, including the use of a 3D nested list to permit time-series data to be mapped to the FM-BIM and readily visualized. The developed approach is presented through a case study describing the scalability of the approach to integrate a building BAS system with a BIM. The resultant data structure and key visualizations are presented to demonstrate the value of this approach, which permits the end-user to select the desired timeframe for visualization and readily step through the spatio-temporal building performance data.}
}
@article{LIN2021101075,
title = {The landscape of Block-based programming: Characteristics of block-based environments and how they support the transition to text-based programming},
journal = {Journal of Computer Languages},
volume = {67},
pages = {101075},
year = {2021},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2021.101075},
url = {https://www.sciencedirect.com/science/article/pii/S259011842100054X},
author = {Yuhan Lin and David Weintrop},
keywords = {Computer science education, Block-based programming, Design of programming environments},
abstract = {Block-based programming (BBP) environments have become increasingly commonplace computer science education. Despite a rapidly expanding ecosystem of BBP environments, text-based languages remain the dominant programming paradigm, motivating the transition from BBP to text-based programming (TBP). Support students in transitioning from BBP to TBP is an important and open design question. This work identifies 101 unique BBP environments, analyzes the 46 of them and identifies different design approaches used to support the transition to TBP. The contribution of this work is to provide a snapshot of the current state of BBP environments and how they support learners in transitioning to TBP.}
}
@article{WANG2021135,
title = {Differences between common endothelial cell models (primary human aortic endothelial cells and EA.hy926 cells) revealed through transcriptomics, bioinformatics, and functional analysis},
journal = {Current Research in Biotechnology},
volume = {3},
pages = {135-145},
year = {2021},
issn = {2590-2628},
doi = {https://doi.org/10.1016/j.crbiot.2021.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2590262821000150},
author = {Dongdong Wang and Zhu Chen and Andy {Wai Kan Yeung} and Atanas G. Atanasov},
keywords = {Endothelial cells, Bioinformatics analysis, Rap1 signaling pathway, Ras signaling pathway, HDL cellular association},
abstract = {Endothelial cells (ECs) are involved in various physiological process. Both primary human ECs and immortal endothelial cells are used in various studies. Available genomic or transcriptomic information for difference in ECs is deficient. Therefore, in this study we aim to reveal the difference between primary human aortic ECs (HAECs) and immortal EA.hy926 cells. We identified 529 differentially expressed genes (DEGs) between HAECs and EA.hy926 cells. Gene Ontology (GO), KEGG Pathway and GSEA enrichment analysis suggest that DEGs highly expressed in HAECs are distributed in Rap1 signaling pathway and Ras signaling pathway, which are contributing to the endothelial barrier function and endocytosis, among other functions. We also established long non-coding (lncRNA)-miRNA-mRNA ceRNA network, and further set up protein–protein interaction (PPI) network. High-density lipoprotein (HDL) cellular association experiments were verified that HAECs have stronger response to HDL cellular binding and endocytosis compared to EA.hy926 cells. This study identified DEGs between HAECs and EA.hy926 cells, and found enrichment of the Ras signaling pathway and Rap1 signaling pathway in HAECs, established ceRNA network and suggested that HAECs may have a stronger response to endothelial binding and endocytosis compared to EA.hy926 cells. This work provides a genomic basis to choose suitable EC model to reach respective research goals.}
}
@article{SUN2023103865,
title = {Light dose effect of photodynamic therapy on growth inhibition and apoptosis induction in non-small cell lung cancer: A study in nude mouse model},
journal = {Photodiagnosis and Photodynamic Therapy},
volume = {44},
pages = {103865},
year = {2023},
issn = {1572-1000},
doi = {https://doi.org/10.1016/j.pdpdt.2023.103865},
url = {https://www.sciencedirect.com/science/article/pii/S1572100023005926},
author = {Wen Sun and Xiaoyu Ma and Yunxia Wang and Guosheng Yang and Jiping Liao and Yuan Cheng and Guangfa Wang},
keywords = {Photodynamic therapy (PDT), Non-small cell lung cancer (NSCLC), Light dose effect, Apoptosis},
abstract = {Background
Photodynamic therapy (PDT) is receiving increasing attention in treating non-small cell lung cancer (NSCLC) worldwide, but in clinical practice, the relationship between treatment effect and PDT light dose in NSCLC remains unclear. Therefore, we aimed to determine the optimal light dose for PDT by exploring molecular biomarkers and evaluating tumor growth data.
Methods
We applied bioinformatics to identify promising genes and pathways in NSCLC and PDT. Then, the human lung adenocarcinoma cell line A549-bearing BALB/c nude mice were treated with hematoporphyrin derivative (HPD, 3 mg/kg) that is currently used widely for lung cancer treatment in the world even with photosensitization issues. After 48 h, tumor-bearing mice were irradiated superficially at doses of 100, 200, 300, 400, and 500 J/cm2. The tumor growth data and apoptotic molecules were assessed and calculated.
Results
Bioinformatics results indicated that the apoptosis pathway was significantly enriched and caspase 3 was the most promising biomarker on prognosis in NSCLC-PDT. Compared to the untreated group, there was no difference in the relative tumor volume (RTV) of the 100 J/cm2 group, while the RTV of the other treatment groups (200–500 J/cm2) was significantly lower. In the 100 J/cm2 group, there were significant differences in the complete remission (CR, 0 %) and the percentage of tumor growth inhibition rate (TGI%) over 75 % (20 %) compared with the other treatment groups, especially the 300 and 400 J/cm2 groups (CR 70 %; TGI% 90 %). In the 300 and 400 J/cm2 groups, the expression of caspase 3, cleaved-caspase 3, PARP1, and Bax was increased significantly, while Bcl-2 expression was significantly lower.
Conclusions
Moderate doses of PDT (300 or 400 J/cm2) are more effective than low (100 or 200 J/cm2) or high doses (500 J/cm2) in the A549 tumor-bearing mice model. Since the A549 tumor is more akin to human tumors in pathological behavior, these experimental data may contribute to improving HPD-PDT illumination protocols for favorable clinical outcomes.}
}
@article{KINEBER2023104930,
title = {Modelling the relationship between digital twins implementation barriers and sustainability pillars: Insights from building and construction sector},
journal = {Sustainable Cities and Society},
volume = {99},
pages = {104930},
year = {2023},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2023.104930},
url = {https://www.sciencedirect.com/science/article/pii/S2210670723005413},
author = {Ahmed Farouk Kineber and Atul Kumar Singh and Abdulwahed Fazeli and Saeed Reza Mohandes and Clara Cheung and Mehrdad Arashpour and Obuks Ejohwomu and Tarek Zayed},
keywords = {Digital twin, Sustainability, Sustainable construction, Overall sustainable success, Structural equation modelling},
abstract = {A Digital Twin (DT) is a digital copy of a real-world object or process. Although DT has gained traction in construction, its relationship with sustainable success remains insufficiently studied. This research addresses this gap by investigating barriers to implementing DT in sustainable construction. The study employs a hybrid approach involving literature review, expert interviews, and modeling techniques, with data collected from 108 construction experts based on a number of criteria, including the experience, degree, and familiarity of the experts about the Hong Kong building and construction sector Hong Kong. The findings reveal 45 barriers categorized into six clusters, including notable obstacles such as "legacy systems," "data uncertainties," and "connectivity." The key clusters identified are "performance" and "security," while the "social" aspect of sustainable success is least supported. Recognizing these challenges assists decision-makers in navigating obstacles and utilizing DT for environmentally conscious construction, streamlined processes, and positive societal impacts. Future research could delve into integrating sustainability throughout the project lifecycle using technology adoption theories.}
}
@article{SENZON201852,
title = {The Chiropractic Vertebral Subluxation Part 4: New Perspectives and Theorists From 1916 to 1927},
journal = {Journal of Chiropractic Humanities},
volume = {25},
pages = {52-66},
year = {2018},
issn = {1556-3499},
doi = {https://doi.org/10.1016/j.echu.2018.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1556349918300123},
author = {Simon A. Senzon},
keywords = {Chiropractic, History},
abstract = {Objective
The purpose of this paper is to review and discuss the history of chiropractic vertebral subluxation (CVS) between the years 1916 and 1927.
Discussion
Theories during this period were shaped by many chiropractic school leaders and instructors. Unique contributions to theories during this period come primarily from 4 authors, John Craven, Jim Drain, Shelby Riley, and Ralph Stephenson. This period included the first thermographic instrumentation in chiropractic, which led to one of Craven’s modifications of CVS theory. He also added to the literature about spinal cord pressure and developed the restoration cycle. Drain and Stephenson also expanded on the cord pressure models of CVS. Drain wrote, in plain language, of many central B. J. Palmer theories and developed protocols for acute and chronic CVS. Stephenson made several contributions to models, including his expansion on B. J. Palmer’s theory of momentum of dis-ease. Stephenson’s main contribution to theory was likely his vertemere cycle, which was a precursor to proprioceptive models. Riley’s combination of Gregory’s theories with zone therapy had a significant impact on several reflex theories.
Conclusion
Chiropractic vertebral subluxation theory during this period grew in complexity and demonstrated several new perspectives on CVS, which may be still relevant today.}
}
@article{TAN2023103796,
title = {RNA-sequencing of peripheral whole blood of individuals at ultra-high-risk for psychosis – A longitudinal perspective},
journal = {Asian Journal of Psychiatry},
volume = {89},
pages = {103796},
year = {2023},
issn = {1876-2018},
doi = {https://doi.org/10.1016/j.ajp.2023.103796},
url = {https://www.sciencedirect.com/science/article/pii/S1876201823003520},
author = {Samuel Ming Xuan Tan and Jie Yin Yee and Sugam Budhraja and Balkaran Singh and Zohreh Doborjeh and Maryam Doborjeh and Nikola Kasabov and Edmund Lai and Alexander Sumich and Jimmy Lee and Wilson Wen Bin Goh},
abstract = {Background
The peripheral blood is an attractive source of prognostic biomarkers for psychosis conversion. There is limited research on the transcriptomic changes associated with psychosis conversion in the peripheral whole blood.
Study Design
We performed RNA-sequencing of peripheral whole blood from 65 ultra-high-risk (UHR) participants and 70 healthy control participants recruited in the Longitudinal Youth-at-Risk Study (LYRIKS) cohort. 13 UHR participants converted in the study duration. Samples were collected at 3 timepoints, at 12-months interval across a 2-year period. We examined whether the genes differential with psychosis conversion contain schizophrenia risk loci. We then examined the functional ontologies and GWAS associations of the differential genes. We also identified the overlap between differentially expressed genes across different comparisons.
Study results
Genes containing schizophrenia risk loci were not differentially expressed in the peripheral whole blood in psychosis conversion. The differentially expressed genes in psychosis conversion are enriched for ontologies associated with cellular replication. The differentially expressed genes in psychosis conversion are associated with non-neurological GWAS phenotypes reported to be perturbed in schizophrenia and psychosis but not schizophrenia and psychosis phenotypes themselves. We found minimal overlap between the genes differential with psychosis conversion and the genes that are differential between pre-conversion and non-conversion samples.
Conclusion
The associations between psychosis conversion and peripheral blood-based biomarkers are likely to be indirect. Further studies to elucidate the mechanism behind potential indirect associations are needed.}
}
@article{OHSAWA20244843,
title = {Semantic Cells: Evolutional process for item sense disambiguation},
journal = {Procedia Computer Science},
volume = {246},
pages = {4843-4852},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.350},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924023755},
author = {Yukio Ohsawa and Dingming Xue and Kaira Sekiguchi},
keywords = {evolutionary computing, word-sense disambiguation, item-sense disambiguation, bio-inspired computation},
abstract = {Previous models for learning the semantic vectors of items and their groups, such as words, sentences, nodes, and graphs, using distributed representation have been based on the assumption that the basic sense of an item corresponds to one vector composed of dimensions corresponding to hidden contexts in the target real world, from which multiple senses of the item are obtained by conforming to lexical databases or adapting to the context. However, there may be multiple senses of an item, which are hardly assimilated and change or evolve dynamically following the contextual shift even within a document or a restricted period. This is a process similar to the evolution or adaptation of a living entity with/to environmental shifts. Setting the scope of disambiguation of items for sensemaking, the author presents a method in which a word or item in the data embraces multiple semantic vectors that evolve via interaction with others, similar to a cell embracing chromosomes crossing over with each other. We obtained a preliminary result: the role of a word that evolves to acquire the largest or lower-middle variance of semantic vectors tends to be explainable by the author of the text.}
}
@article{JIANG2020101772,
title = {Medical knowledge embedding based on recursive neural network for multi-disease diagnosis},
journal = {Artificial Intelligence in Medicine},
volume = {103},
pages = {101772},
year = {2020},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2019.101772},
url = {https://www.sciencedirect.com/science/article/pii/S0933365718303919},
author = {Jingchi Jiang and Huanzheng Wang and Jing Xie and Xitong Guo and Yi Guan and Qiubin Yu},
keywords = {Electronic medical records, First-order logic, Knowledge embedding, Recursive neural network},
abstract = {The representation of knowledge based on first-order logic captures the richness of natural language and supports multiple probabilistic inference models. Although symbolic representation enables quantitative reasoning with statistical probability, it is difficult to utilize with machine learning models as they perform numerical operations. In contrast, knowledge embedding (i.e., high-dimensional and continuous vectors) is a feasible approach to complex reasoning that can not only retain the semantic information of knowledge, but also establish the quantifiable relationship among embeddings. In this paper, we propose a recursive neural knowledge network (RNKN), which combines medical knowledge based on first-order logic with a recursive neural network for multi-disease diagnosis. After the RNKN is efficiently trained using manually annotated Chinese Electronic Medical Records (CEMRs), diagnosis-oriented knowledge embeddings and weight matrixes are learned. The experimental results confirm that the diagnostic accuracy of the RNKN is superior to those of four machine learning models, four classical neural networks and Markov logic network. The results also demonstrate that the more explicit the evidence extracted from CEMRs, the better the performance. The RNKN gradually reveals the interpretation of knowledge embeddings as the number of training epochs increases.}
}
@article{CHAKRABARTY2024498,
title = {Imaging Analytics using Artificial Intelligence in Oncology: A Comprehensive Review},
journal = {Clinical Oncology},
volume = {36},
number = {8},
pages = {498-513},
year = {2024},
note = {Advances in Imaging for Oncology},
issn = {0936-6555},
doi = {https://doi.org/10.1016/j.clon.2023.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0936655523003345},
author = {N. Chakrabarty and A. Mahajan},
keywords = {Artificial intelligence, cancer, deep learning, diagnosis-genomic mutations-outcome prediction},
abstract = {The present era has seen a surge in artificial intelligence-related research in oncology, mainly using deep learning, because of powerful computer hardware, improved algorithms and the availability of large amounts of data from open-source domains and the use of transfer learning. Here we discuss the multifaceted role of deep learning in cancer care, ranging from risk stratification, the screening and diagnosis of cancer, to the prediction of genomic mutations, treatment response and survival outcome prediction, through the use of convolutional neural networks. Another role of artificial intelligence is in the generation of automated radiology reports, which is a boon in high-volume centres to minimise report turnaround time. Although a validated and deployable deep-learning model for clinical use is still in its infancy, there is ongoing research to overcome the barriers for its universal implementation and we also delve into this aspect. We also briefly describe the role of radiomics in oncoimaging. Artificial intelligence can provide answers pertaining to cancer management at baseline imaging, saving cost and time. Imaging biobanks, which are repositories of anonymised images, are also briefly described. We also discuss the commercialisation and ethical issues pertaining to artificial intelligence. The latest generation generalist artificial intelligence model is also briefly described at the end of the article. We believe this article will not only enrich knowledge, but also promote research acumen in the minds of readers to take oncoimaging to another level using artificial intelligence and also work towards clinical translation of such research.}
}
@article{KHAN20213607,
title = {Intelligent Model for Predicting the Quality of Services Violation},
journal = {Computers, Materials and Continua},
volume = {71},
number = {2},
pages = {3607-3619},
year = {2021},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.023480},
url = {https://www.sciencedirect.com/science/article/pii/S154622182100237X},
author = {Muhammad Adnan Khan and Asma Kanwal and Sagheer Abbas and Faheem Khan and T. Whangbo},
keywords = {Accountability, particle swarm optimization, mutant particle swarm optimization, quality of service, service level agreement},
abstract = {Cloud computing is providing IT services to its customer based on Service level agreements (SLAs). It is important for cloud service providers to provide reliable Quality of service (QoS) and to maintain SLAs accountability. Cloud service providers need to predict possible service violations before the emergence of an issue to perform remedial actions for it. Cloud users’ major concerns; the factors for service reliability are based on response time, accessibility, availability, and speed. In this paper, we, therefore, experiment with the parallel mutant-Particle swarm optimization (PSO) for the detection and predictions of QoS violations in terms of response time, speed, accessibility, and availability. This paper also compares Simple-PSO and Parallel Mutant-PSO. In simulation results, it is observed that the proposed Parallel Mutant-PSO solution for cloud QoS violation prediction achieves 94% accuracy which is many accurate results and is computationally the fastest technique in comparison of conventional PSO technique.}
}
@article{SLATER2021104904,
title = {Multi-faceted semantic clustering with text-derived phenotypes},
journal = {Computers in Biology and Medicine},
volume = {138},
pages = {104904},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104904},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521006983},
author = {Karin Slater and John A. Williams and Andreas Karwath and Hilary Fanning and Simon Ball and Paul N. Schofield and Robert Hoehndorf and Georgios V. Gkoutos},
keywords = {Ontology, Clustering, MIMIC-III, Semantic similarity, Cluster explanation},
abstract = {Identification of ontology concepts in clinical narrative text enables the creation of phenotype profiles that can be associated with clinical entities, such as patients or drugs. Constructing patient phenotype profiles using formal ontologies enables their analysis via semantic similarity, in turn enabling the use of background knowledge in clustering or classification analyses. However, traditional semantic similarity approaches collapse complex relationships between patient phenotypes into a unitary similarity scores for each pair of patients. Moreover, single scores may be based only on matching terms with the greatest information content (IC), ignoring other dimensions of patient similarity. This process necessarily leads to a loss of information in the resulting representation of patient similarity, and is especially apparent when using very large text-derived and highly multi-morbid phenotype profiles. Moreover, it renders finding a biological explanation for similarity very difficult; the black box problem. In this article, we explore the generation of multiple semantic similarity scores for patients based on different facets of their phenotypic manifestation, which we define through different sub-graphs in the Human Phenotype Ontology. We further present a new methodology for deriving sets of qualitative class descriptions for groups of entities described by ontology terms. Leveraging this strategy to obtain meaningful explanations for our semantic clusters alongside other evaluation techniques, we show that semantic clustering with ontology-derived facets enables the representation, and thus identification of, clinically relevant phenotype relationships not easily recoverable using overall clustering alone. In this way, we demonstrate the potential of faceted semantic clustering for gaining a deeper and more nuanced understanding of text-derived patient phenotypes.}
}
@article{HU2024103742,
title = {DLRGeoTweet: A comprehensive social media geocoding corpus featuring fine-grained places},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103742},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103742},
url = {https://www.sciencedirect.com/science/article/pii/S030645732400102X},
author = {Xuke Hu and Tobias Elßner and Shiyu Zheng and Helen Ngonidzashe Serere and Jens Kersten and Friederike Klan and Qinjun Qiu},
keywords = {Annotated twitter corpus, Geoparsing, Geocoding, Toponym resolution, Toponym disambiguation, Fine-grained places},
abstract = {Every day, many short text messages on social media are generated in response to real-world events, providing a valuable resource for various domains such as emergency response and traffic management. Since exact coordinates of social media posts are rarely attached by users, accurately recognizing and resolving fine-grained place names, such as home addresses and Points of Interest, from these posts is crucial for understanding the precise locations of critical events, such as rescue requests. This task, known as geoparsing, involves toponym recognition and toponym resolution or geocoding. However, existing social media datasets for evaluating geoparsing approaches often lack sufficient fine-grained place names with associated geo-coordinates or linked to gazetteers, making evaluating, comparing, and training geocoding methods for such locations challenging. Moreover, the absence of supportive annotation tools compounds this challenge. To address these gaps, we implemented a lightweight Python tool leveraging Nominatim. Using this tool, we annotated a comprehensive X (formerly Twitter) geocoding corpus called DLRGeoTweet. The corpus underwent a rigorous cross-validation process to guarantee its quality. This corpus includes a total of 7,364 tweets and 12,510 places, of which 6,012 are fine-grained. It comprises two global datasets encompassing worldwide events and three local datasets related to local events such as the 2017 Hurricane Harvey. The annotation process spanned over ten months and required approximately 1000 person-hours to complete. We then evaluate 15 latest and representative geocoding approaches, including many deep learning-based, on DLRGeoTweet. The results highlight the inherent challenges in resolving fine-grained places accurately. Despite increasing access constraints to Twitter data, our corpus’s focus on short, informal text makes it a valuable resource for geocoding across multiple social media platforms.}
}
@article{HARRIS2023632,
title = {Achieving integrated treatment: a realist synthesis of service models and systems for co-existing serious mental health and substance use conditions},
journal = {The Lancet Psychiatry},
volume = {10},
number = {8},
pages = {632-643},
year = {2023},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(23)00104-9},
url = {https://www.sciencedirect.com/science/article/pii/S2215036623001049},
author = {Jane Harris and Sonia Dalkin and Lisa Jones and Tom Ainscough and Michelle Maden and Angela Bate and Alexandre Copello and Gail Gilchrist and Emma Griffith and Luke Mitcheson and Harry Sumnall and Elizabeth Hughes},
abstract = {Summary
Approximately 30–50% of people with serious mental illness have co-existing drug or alcohol problems (COSMHAD), associated with adverse health and social care outcomes. UK guidelines advocate both co-occurring needs being met within mental health services, but uncertainty remains about how to operationalise this to improve outcomes. Various unevaluated service configurations exist in the UK. A realist synthesis was done to identify, test, and refine programme theories of how context shapes the mechanisms through which UK service models for COSMHAD work, for whom, and in what circumstances. Structured and iterative realist searches of seven databases identified 5099 records. A two-stage screening process identified 132 papers. Three broad contextual factors shaped COSMHAD services across 11 programme theories: committed leadership, clear expectations regarding COSMHAD from mental health and substance use workforces, and clear care-coordination processes. These contextual factors led to increased staff empathy, confidence, legitimisation, and multidisciplinary ethos, which improved care coordination and increased the motivation of people with COSMHAD to work towards their goals. Our synthesis highlights that integrating COSMHAD care is complex, and both individual and cultural behavioural shifts in leadership, workforce, and service delivery are essential to ensure people with COSMHAD receive compassionate, trauma-informed care that meets their needs.}
}
@article{HU20241049,
title = {English translation evaluation method based on BP neural network},
journal = {Procedia Computer Science},
volume = {243},
pages = {1049-1058},
year = {2024},
note = {The 4th International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.125},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924021331},
author = {Yang Hu},
keywords = {BP neural network, Machine translation, Information fusion, English translation},
abstract = {In order to solve the problems of machine translation efficiency and translation quality, this paper proposes an English translation evaluation system based on BP neural network algorithm. This method provides users with a more intelligent machine translation service experience. With the help of BP neural network algorithm, taking English online translation as the research object, Google's translation quality is the best, with an error frequency of only 167, while Baidu translation and iFLYTEK translation in China have a high error rate of 266 and 301 respectively, which is much higher than Google translation. A model of machine translation evaluation based on neural network algorithm is proposed to better solve the disadvantages of traditional English machine translation. The results show that the machine translation system based on neural network algorithm can further optimize the problems existing in machine translation, such as insufficient use of information and large scale of model parameters, and further improve the performance of neural network machine translation.}
}
@article{CAI2025113374,
title = {Convolutional neural networks for construction safety: A technical review of computer vision applications},
journal = {Applied Soft Computing},
volume = {180},
pages = {113374},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.113374},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625006854},
author = {Ruying Cai and Jingru Li and Yi Tan and Jingyuan Tang and Xiangsheng Chen},
keywords = {Computer vision, Convolutional neural network, Construction safety, Image classification, Object detection, Object segmentation, Object pose estimation, Object tracking},
abstract = {The increasing number of construction-related accidents underscores the urgent need to enhance global construction safety (CS) management. Monitoring activities during the construction process is essential for effective CS management. Traditional computer vision (CV) techniques, which rely on handcrafted features and rule-based algorithms, often struggle to capture the complex dynamics of construction sites. In contrast, deep learning-based CV approaches, especially convolutional neural networks (CNNs), offer end-to-end solutions that overcome these limitations and are increasingly adopted in CS applications. This paper systematically categorizes the application of CNN-based CV technologies into four key stages: data collection, data preprocessing, model construction, and practical application. From a technical perspective, this paper provides a comprehensive review of relevant methods and tools at each stage. Initially, this paper analyzes the literature on CNN-based CV applications in CS via the Python library pyBibX combined with artificial intelligence (AI) tools for bibliometric analysis and effective visualization. This paper subsequently summarizes various data acquisition and preprocessing methods to save researchers time in data-related aspects. Furthermore, to provide researchers with a rapid understanding of existing methods, this paper presents various CV and CNN techniques, summarizing classical CV models based on CNNs through extensive literature, data, web, and competition searches, and compares their performance on public datasets. Finally, this paper provides a detailed analysis of CNN-based CV technologies used in CS, tailored to the technical aspects of various downstream tasks, revealing current applications of advanced technologies and research progress. Additionally, this paper discusses current research challenges, including technical and other aspects, and proposes several directions for future research.}
}
@article{BARAK2024104401,
title = {Educational ideals and classroom realities: Developing teachers' concepts of dialogic pedagogy in real-world contexts},
journal = {Teaching and Teacher Education},
volume = {138},
pages = {104401},
year = {2024},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2023.104401},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X2300389X},
author = {Matan Barak},
keywords = {Teacher professional development, Teacher learning, Dialogic pedagogy, Conceptual change, Thematic analysis},
abstract = {Professional learning processes aim to change teachers' pedagogical concepts, yet this process is obscure and understudied. Building on a conceptual change perspective, this study explored teachers’ concepts of dialogic pedagogy that emerged during professional development. The year-long process included 17 teachers, coaches, and researchers codesigning dialogic language arts lessons. Thematic analysis was used to map teachers' concepts and to investigate forces shaping them. Findings show teachers' concepts are embedded within classroom affordances and constraints and are influenced by broader educational contexts. Implications for teacher conceptual change include emphasizing pedagogical tensions, advancing practice-oriented designs, and adapting academic concepts to classroom contexts.}
}
@article{CID2024e44,
title = {Development and validation of open-source deep neural networks for comprehensive chest x-ray reading: a retrospective, multicentre study},
journal = {The Lancet Digital Health},
volume = {6},
number = {1},
pages = {e44-e57},
year = {2024},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(23)00218-2},
url = {https://www.sciencedirect.com/science/article/pii/S2589750023002182},
author = {Yashin Dicente Cid and Matthew Macpherson and Louise Gervais-Andre and Yuanyi Zhu and Giuseppe Franco and Ruggiero Santeramo and Chee Lim and Ian Selby and Keerthini Muthuswamy and Ashik Amlani and Heath Hopewell and Das Indrajeet and Maria Liakata and Charles E Hutchinson and Vicky Goh and Giovanni Montana},
abstract = {Summary
Background
Artificial intelligence (AI) systems for automated chest x-ray interpretation hold promise for standardising reporting and reducing delays in health systems with shortages of trained radiologists. Yet, there are few freely accessible AI systems trained on large datasets for practitioners to use with their own data with a view to accelerating clinical deployment of AI systems in radiology. We aimed to contribute an AI system for comprehensive chest x-ray abnormality detection.
Methods
In this retrospective cohort study, we developed open-source neural networks, X-Raydar and X-Raydar-NLP, for classifying common chest x-ray findings from images and their free-text reports. Our networks were developed using data from six UK hospitals from three National Health Service (NHS) Trusts (University Hospitals Coventry and Warwickshire NHS Trust, University Hospitals Birmingham NHS Foundation Trust, and University Hospitals Leicester NHS Trust) collectively contributing 2 513 546 chest x-ray studies taken from a 13-year period (2006–19), which yielded 1 940 508 usable free-text radiological reports written by the contemporary assessing radiologist (collectively referred to as the “historic reporters”) and 1 896 034 frontal images. Chest x-rays were labelled using a taxonomy of 37 findings by a custom-trained natural language processing (NLP) algorithm, X-Raydar-NLP, from the original free-text reports. X-Raydar-NLP was trained on 23 230 manually annotated reports and tested on 4551 reports from all hospitals. 1 694 921 labelled images from the training set and 89 238 from the validation set were then used to train a multi-label image classifier. Our algorithms were evaluated on three retrospective datasets: a set of exams sampled randomly from the full NHS dataset reported during clinical practice and annotated using NLP (n=103 328); a consensus set sampled from all six hospitals annotated by three expert radiologists (two independent annotators for each image and a third consultant to facilitate disagreement resolution) under research conditions (n=1427); and an independent dataset, MIMIC-CXR, consisting of NLP-annotated exams (n=252 374).
Findings
X-Raydar achieved a mean AUC of 0·919 (SD 0·039) on the auto-labelled set, 0·864 (0·102) on the consensus set, and 0·842 (0·074) on the MIMIC-CXR test, demonstrating similar performance to the historic clinical radiologist reporters, as assessed on the consensus set, for multiple clinically important findings, including pneumothorax, parenchymal opacification, and parenchymal mass or nodules. On the consensus set, X-Raydar outperformed historical reporter balanced accuracy with significance on 27 of 37 findings, was non-inferior on nine, and inferior on one finding, resulting in an average improvement of 13·3% (SD 13·1) to 0·763 (0·110), including a mean 5·6% (13·2) improvement in critical findings to 0·826 (0·119).
Interpretation
Our study shows that automated classification of chest x-rays under a comprehensive taxonomy can achieve performance levels similar to those of historical reporters and exhibit robust generalisation to external data. The open-sourced neural networks can serve as foundation models for further research and are freely available to the research community.
Funding
Wellcome Trust.}
}
@article{BISHNU2023100111,
title = {Computational applications using data driven modeling in process Systems: A review},
journal = {Digital Chemical Engineering},
volume = {8},
pages = {100111},
year = {2023},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2023.100111},
url = {https://www.sciencedirect.com/science/article/pii/S2772508123000297},
author = {Sumit K. Bishnu and Sabla Y. Alnouri and Dhabia M. Al-Mohannadi},
keywords = {Modeling, Optimization, Machine learning, Data-driven modeling},
abstract = {Modeling and optimization of various processes enable more efficient operations and better planning activities for new process developments. With recent advances in computing power, data driven models, such as Machine Learning (ML), are being extensively applied in many areas of chemical engineering topics. Compared to mechanistic models that often do not reflect the realities of field conditions and the high costs associated with them, these techniques are relatively easier to implement. Data-driven models generated via ML techniques can be regularly updated, thereby giving an accurate picture of the system. Due to these inherent benefits, such tools are increasingly gaining a lot of traction in process systems. Even though data-driven models have the potential to be used as a replacement for traditional optimization tools that can be implemented in various process industries, it was found that applications of such models in process systems were quite limited to reactor modeling, molecular design, as well as safety, and relatability. The challenge still exists for data-driven modeling due to the lack of specialized tools tailored for macro systems and scale up. Most datasets were found to be derived from experimental studies which are limited in nature and only fit into microsystems. Hence, this paper provides a state of the art review on recent applications for data driven modeling research in process systems, and discusses the prominent challenges and future outlooks that were observed.}
}
@article{IGAMBERDIEV2020104219,
title = {The evolutionary dynamics of social systems via reflexive transformation of external reality},
journal = {Biosystems},
volume = {197},
pages = {104219},
year = {2020},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2020.104219},
url = {https://www.sciencedirect.com/science/article/pii/S0303264720301106},
author = {Abir U. Igamberdiev and Joseph E. Brenner},
keywords = {Externality, Frame of reference, Reflexivity, Social evolution, Structure of subject, Transaction},
abstract = {One of the main challenges of the social sciences is to explain metasystem transitions from biological to social systems in the process of evolution. These transitions correspond to the emergence of the structure of the subject in which the external world is internalized as a symbolic image. This structure has the potential of rationally reflecting the external world and encoding it in human language. The structure of the subject was defined by Freud and Lacan within the framework of psychoanalysis and modeled by Vladimir Lefebvre using the algebra of simple relations. In that context, the binary oppositions of the Western (W) and Eastern (E) types of cognitive reflection generate not only opposite types of societies but also form the spatiotemporal pattern within a society as a whole underlying its homeostasis and internal dynamics. This opposition between Western and Eastern types is not identical to, but mirrors the probably genetically determined opposition between individuals with primarily selfish or altruistic behavior. The structure of the reflexive subject represents the basis of a sociotype that can be considered as the third component of biosocial being in addition to genotype and phenotype; it is defined a social form of the individual bonded by the social environment and social interactions. The transition to a different type of social relations and structure is based on an anticipated reflexive choice by an individual that is accompanied by a coherent response from the society. This change is defined as a “social transaction” and can be modeled by the logic of the transactional interpretation of quantum mechanics. In the history of human civilization, social transactions have resulted in the advancement and distribution of new knowledge and technologies, and in the formation, merging, splitting and decline and re-emergence of particular types of societies.}
}
@article{SPUTZ202237,
title = {Classification of Simulation Models for the Model-based Design of Plastic-Metal Hybrid Joints},
journal = {Procedia CIRP},
volume = {109},
pages = {37-42},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.211},
url = {https://www.sciencedirect.com/science/article/pii/S221282712200659X},
author = {Kathrin Spütz and Julius Berges and Georg Jacobs and Joerg Berroth and Christian Konrad},
keywords = {Model-based system engineering, Laser-based hybrid joints, Model chains, Model integration, Model execution},
abstract = {Laser-based hybrid joints (LBHJ) are promising due to low processing time and costs compared to conventional joining technologies. During the design of LBHJs, considering interactions between different domains (engineering, production, controlling) is required. This yields potential for a cross-domain design and optimization. However, considering the interactions is challenging and leads to iterations and inefficiency in product development. To map the interactions and exploit the potential of LBHJs, model-based systems engineering (MBSE) is a promising approach. With MBSE, domain models are integrated into a central system model and the model parameters are interconnected. To support a model-based design of LBHJs in the different phases of product development, usually several models are required. Therefore, multiple models in different fidelities for the same purpose exist (e.g. finite element and analytical models for joint strength estimation). Currently, this results in complex system models that are difficult to understand, reuse, extend, and maintain. The application of MBSE for the design of LBHJ is therefore inhibited. To overcome this deficit, the integration and linkage of domain models in the system model must be formally structured. Therefore, this paper presents a SysML metamodel, which classifies the domain models, defines their standardized structure and specifies their integration and linkage within the system model. The formal structuring is based on the classification criteria purpose. For each purpose, a superordinate model is built and connected to the domain models. To avoid complex system models that are vulnerable to changes, only superordinate domain models are linked with each other. Domain models can thus be easily replaced, without affecting the consistency of the system model. The results show that system models, built with this approach, can be easily created and used for the model-based design of LBHJs. Furthermore, interactions between the domains can be handled and efficiency in product development is increased.}
}
@article{SLAWIK2018846,
title = {Establishing User-centric Cloud Service Registries},
journal = {Future Generation Computer Systems},
volume = {87},
pages = {846-867},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18304813},
author = {Mathias Slawik and Begüm İlke Zilci and Axel Küpper},
keywords = {Cloud service registry, Cloud computing, Service matchmaking, Cloud brokering},
abstract = {Many potential cloud consumers are overburdened by the challenges persisting when discovering, assessing, and selecting contemporary Cloud Service offerings: the cloud market is vast and fast-moving, the selection criteria are ambiguous, service knowledge is scattered through the Internet, and features as well as prices are complex and incomparable. Much research has been carried out to create cloud service registries to help users select cloud services for eventual consumption, especially within the field of semantic web services. Through analyzing real-world requirements of six use cases we identified a gap in research for user-centric technologies. We fill this gap by creating a business vocabulary reflecting common service selection criteria, defining a textual domain specific language to let any user describe services easily, and implementing a novel brokering and matchmaking component to support users in their selection process. As a combination of those technologies, we create the Open Service Compendium (OSC), a crowd-sourced cloud service registry. Our evaluation activities highlight how these developments solve real-world challenges in diverse near-production settings. All of this implies that a substantial benefit for service registry users can be created by following a simple architecture that is focused on their concrete needs — instead of aiming for highest sophistication and broadest applicability as observed in many of the related works.}
}
@article{SHAKHOVSKA2019229,
title = {Big Data analysis in development of personalized medical system},
journal = {Procedia Computer Science},
volume = {160},
pages = {229-234},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.461},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931676X},
author = {Natalia Shakhovska and Solomia Fedushko and Michal {Greguš ml.} and Natalia Melnykova and Iryna Shvorob and Yuriy Syerov},
keywords = {Big Data, medical system, personalized data, decision-making method, service science, information medical profile},
abstract = {This paper considers the actual problem of Big Data analysis of medical information. Big Data is characterized by heterogeneity and constant growth requires the use of non-standard approaches to storage and processing of data. The analysis of personalized medical information for system development is proposed. The methods of construction of patient information track model based on systematic verification of patient’s personal and medical data is developed. The semantics of the data personalization in decision-making consists of the following stags: stage of forming the ontology of medical knowledge of the medical process and stage of formalizing the process of finding standard solutions. A personalized approach to personalization of standard schemes is proposed by modifying the decision-making method based on decision trees, taking into account the relationship between patient parameters. The result of the method is presented in personalized scheme.}
}
@article{BLUME2021136,
title = {FLUID: A common model for semantic structural graph summaries based on equivalence relations},
journal = {Theoretical Computer Science},
volume = {854},
pages = {136-158},
year = {2021},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2020.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S0304397520307234},
author = {Till Blume and David Richerby and Ansgar Scherp},
keywords = {Structural graph summary, Semantic graphs, Parameterized formal model},
abstract = {Summarization is a widespread method for handling very large graphs. The task of structural graph summarization is to compute a concise but meaningful synopsis of the key structural information of a graph. As summaries may be used for many different purposes, there is no single concept or model of graph summaries. We have studied existing structural graph summaries for large-scale (semantic) graphs. Despite their different concepts and purposes, we found commonalities in the graph structures they capture. We use these commonalities to provide for the first time a formally defined common model, FLUID (FLexible graph sUmmarIes for Data graphs), that allows us to flexibly define structural graph summaries. FLUID allows graph summaries to be quickly defined, adapted, and compared for different purposes and datasets. To this end, FLUID provides features of structural summarization based on equivalence relations such as distinction of types and properties, direction of edges, bisimulation, and inference. We conduct a detailed complexity analysis of the features provided by FLUID. We show that graph summaries defined with FLUID can be computed in the worst case in time O(n2) w.r.t. n, the number of edges in the data graph. An empirical analysis of large-scale web graphs with billions of edges indicates a typical running time of Θ(n). Based on the formal FLUID model, one can quickly define and modify various structural graph summaries from the literature and beyond.}
}
@article{CHEONG2019183,
title = {Translating JSON Schema logics into OWL axioms for unified data validation on a digital manufacturing platform},
journal = {Procedia Manufacturing},
volume = {28},
pages = {183-188},
year = {2019},
note = {7th International conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.12.030},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918313726},
author = {Hyunmin Cheong},
keywords = {JSON, JSON Schema, OWL, Ontology, Mapping, Axiom translation, Data validation},
abstract = {JSON (JavaScript Object Notation) is a prevalent data format used in cloud-based platforms that support composable digital manufacturing workflows. The current work presents a method to translate the logics found in JSON Schema into OWL axioms, in order to facilitate ontology-based unified data validation with JSON data. The specific contributions of this paper include the demonstration of using a formal ontology for the logic translation and data validation, a technique for disambiguating implicit relations found in JSON Schema as explicit OWL properties, and mapping JSON Schema validation keywords to equivalent OWL expressions.}
}
@article{BARZEGAR2018119,
title = {Attack scenario reconstruction using intrusion semantics},
journal = {Expert Systems with Applications},
volume = {108},
pages = {119-133},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.04.030},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418302689},
author = {Mahdiyeh Barzegar and Mehdi Shajari},
keywords = {Alert correlation, Attack scenario, Ontology, Similarity, Semantic},
abstract = {Security information and event management (SIEM) systems receive a large number of alerts from different intrusion detection systems. They are expected, from these alerts, to make reliable and timely decisions regarding the types of ongoing attack scenarios and their priorities. However, the lack of an agreed-upon vocabulary for the representation of the domain knowledge makes it difficult for state-of-the-art SIEM systems to effectively manage these complex decisions. To overcome this problem, an ontology-based expert system approach can provide domain knowledge modeling as a foundation for disambiguation of meaning and automatic reasoning regarding ongoing attack scenarios. The proposed approach reconstructs attack scenarios by reasoning based on the evidences in the alert stream. The main idea of the proposed approach is to identify the causal relation between alerts using their similarity. This approach assumes that the similarity between two successive steps in an attack scenario is greater than that of two non-successive steps. Moreover, the similarity between the steps of the same attack scenario is greater than that between the steps of two different attack scenarios. The benefit of the proposed approach includes the fast and incremental reconstruction of known and unknown attack scenarios without expert intervention, which is an enormous step forward in developing expert and intelligent systems for cyber security. We evaluated the proposed technique by performing experiments on two known datasets: DARPA 2000 and MACCDC 2012. The results prove the advantages of the proposed approach with regard to completeness and soundness criteria.}
}
@article{QIAN2024104453,
title = {Evaluating resilience of urban lifelines against flooding in China using social media data},
journal = {International Journal of Disaster Risk Reduction},
volume = {106},
pages = {104453},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104453},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924002152},
author = {Jiale Qian and Yunyan Du and Fuyuan Liang and Jiawei Yi and Nan Wang and Wenna Tu and Sheng Huang and Tao Pei and Ting Ma and Keith Burghardt and Kristina Lerman},
keywords = {Early warning, Flood, Resilience, Rumor spreading, Social media, Urban lifelines},
abstract = {Urban lifelines are the backbone of fundamental services that require stability to enable all other aspects of society to function during natural hazards. However, few studies have focused on measuring lifeline performance and resilience to hazards, especially from the public perspective. In this study, taking flood as the research object, we developed an enhanced urban lifelines classification scheme that integrates the Federal Emergency Management Agency (FEMA)'s lifeline framework with the co-occurrence relationships of keywords derived from 430 million Weibo posts. Leveraging this framework, we formulated two key indicators: Public Concern Ratio and Public Emotion Ratio, to evaluate the resilience of urban lifelines during the 2017 and 2020 flood in China. The results demonstrate the robust resilience of urban lifelines against flooding, with notable improvements over time. The study also identifies certain vulnerable lifelines, notably in the ability of early warning and control of rumor spreading, which often lead to an increase in social media posts expressing negative emotions during flooding. These areas are pinpointed for necessary enhancements. Employing a data-driven methodology, the study provides a novel and insightful approach to assessing urban lifeline resilience against flooding.}
}
@article{ADOLPHS2024105645,
title = {Commentary on, “The Human Affectome,” by Schiller et al.},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {160},
pages = {105645},
year = {2024},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105645},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424001143},
author = {Ralph Adolphs},
keywords = {Emotion, Affect},
abstract = {I suggest that this project could benefit from a relational database of some sort to provide readers with a more formal ontology, and that the authors consider making a distinction between experiential and functional aspects of emotion.}
}
@article{BARBEROAPARICIO2024102035,
title = {Addressing data scarcity in protein fitness landscape analysis: A study on semi-supervised and deep transfer learning techniques},
journal = {Information Fusion},
volume = {102},
pages = {102035},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102035},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523003512},
author = {José A. Barbero-Aparicio and Alicia Olivares-Gil and Juan J. Rodríguez and César García-Osorio and José F. Díez-Pastor},
keywords = {Bioinformatics, Machine learning, Transfer learning, Semi-supervised learning, Protein fitness prediction, Small datasets},
abstract = {This paper presents a comprehensive analysis of deep transfer learning methods, supervised methods, and semi-supervised methods in the context of protein fitness prediction, with a focus on small datasets. The analysis includes the exploration of the combination of different data sources to enhance the performance of the models. While deep learning and deep transfer learning methods have shown remarkable performance in situations with abundant data, this study aims to address the more realistic scenario faced by wet lab researchers, where labeled data is often limited. The novelty of this work lies in its examination of deep transfer learning in the context of small datasets and its consideration of semi-supervised methods and multi-view strategies. While previous research has extensively explored deep transfer learning in large dataset scenarios, little attention has been given to its efficacy in small dataset settings or its comparison with semi-supervised approaches. Our findings suggest that deep transfer learning, exemplified by ProteinBERT, shows promising performance in this context compared to the rest of the methods across various evaluation metrics, not only in small dataset contexts but also in large dataset scenarios. This highlights the robustness and versatility of deep transfer learning in protein fitness prediction tasks, even with limited labeled data. The results of this study shed light on the potential of deep transfer learning as a state-of-the-art approach in the field of protein fitness prediction. By leveraging pre-trained models and fine-tuning them on small datasets, researchers can achieve competitive performance surpassing traditional supervised and semi-supervised methods. These findings provide valuable insights for wet lab researchers who face the challenge of limited labeled data, enabling them to make informed decisions when selecting the most effective methodology for their specific protein fitness prediction tasks. Additionally, the study investigated the combination of two different sources of information (encodings) through our enhanced semi-supervised methods, yielding noteworthy results improving their base model and providing valuable insights for further research. The presented analysis contributes to a better understanding of the capabilities and limitations of different learning approaches in small dataset scenarios, ultimately aiding in the development of improved protein fitness prediction methods.}
}
@article{ZHU2019479,
title = {Bibliometric analysis of patent infringement retrieval model based on self-organizing map neural network algorithm},
journal = {Library Hi Tech},
volume = {38},
number = {2},
pages = {479-491},
year = {2019},
issn = {0737-8831},
doi = {https://doi.org/10.1108/LHT-12-2018-0201},
url = {https://www.sciencedirect.com/science/article/pii/S0737883119000368},
author = {Dimin Zhu},
keywords = {Databases, Data analysis, Data mining, Social sciences, Data collection techniques, Hypertext},
abstract = {Purpose
The purpose of this paper is to quickly retrieve the same or similar patents in a large patent database.
Design/methodology/approach
The research is carried out through the analysis of the issue of patent examination, the type of patent infringement search and theories related to patent infringement determination and text mining.
Findings
The results show that the model improves the speed of patent search. It can quickly, accurately and comprehensively retrieve the same or equivalent patents as the imported patent claims.
Research limitations/implications
The patent infringement detection mainly focuses on the measurement of patent similarity in the implementation method. It is not mature, and there is still much room for improvement in research.
Practical implications
The model improves the efficiency of patent infringement detection, increases the accuracy of detection and protects the interests of patent stakeholders.
Originality/value
This study has great significance for improving the efficiency of patent examiners.}
}
@article{AN2021109685,
title = {Challenges, tasks, and opportunities in modeling agent-based complex systems},
journal = {Ecological Modelling},
volume = {457},
pages = {109685},
year = {2021},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2021.109685},
url = {https://www.sciencedirect.com/science/article/pii/S030438002100243X},
author = {Li An and Volker Grimm and Abigail Sullivan and B.L. Turner II and Nicolas Malleson and Alison Heppenstall and Christian Vincenot and Derek Robinson and Xinyue Ye and Jianguo Liu and Emilie Lindkvist and Wenwu Tang},
keywords = {Agent-based complex systems, Agent-based modelling, Socioecological systems, Data science, Artificial intelligence},
abstract = {Humanity is facing many grand challenges at unprecedented rates, nearly everywhere, and at all levels. Yet virtually all these challenges can be traced back to the decision and behavior of autonomous agents that constitute the complex systems under such challenges. Agent-based modeling has been developed and employed to address such challenges for a few decades with great achievements and caveats. This article reviews the advances of ABM in social, ecological, and socio-ecological systems, compare ABM with other traditional, equation-based models, provide guidelines for ABM novice, modelers, and reviewers, and point out the challenges and impending tasks that need to be addressed for the ABM community. We further point out great opportunities arising from new forms of data, data science and artificial intelligence, showing that agent behavioral rules can be derived through data mining and machine learning. Towards the end, we call for a new science of Agent-based Complex Systems (ACS) that can pave an effective way to tackle the grand challenges.}
}
@article{CHENG2025103098,
title = {Enhancing diagnosis prediction with adaptive disease representation learning},
journal = {Artificial Intelligence in Medicine},
volume = {163},
pages = {103098},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2025.103098},
url = {https://www.sciencedirect.com/science/article/pii/S0933365725000338},
author = {Hengliang Cheng and Shibo Li and Tao Shen and Weihua Li},
keywords = {Deep learning, Diagnosis prediction, Electronic health records},
abstract = {Diagnosis prediction predicts which diseases a patient is most likely to suffer from in the future based on their historical electronic health records. The time series model can better capture the temporal progression relationship of patient diseases, but ignores the semantic correlation between all diseases; in fact, multiple diseases that are often diagnosed at the same time reflect hidden patterns that are conducive to diagnosis, so predefined global disease co-occurrence graph can help the model understand disease relationships. But it may contain a lot of noise and ignore the semantic adaptation of the disease under the diagnosis target. To this end, we propose a graph-driven end-to-end framework, named Adaptive Disease Representation Learning (ADRL), obtain disease representation after learning complex disease relationships, and then use it to improve diagnosis prediction performance. This model introduces an adaptive mechanism to dynamically adjust and optimize disease relationships by performing self-supervised perturbations on a predefined global disease co-occurrence graph, thereby learning a global disease relationship graph that contains complex semantic association information between diseases. The computational burden of adaptive global disease graph can be further alleviated by the proposed SVD-based accelerator. Finally, experimental results on two real-world EHR datasets show that the proposed model outperforms existing models in diagnosis prediction.}
}
@article{HOU2023101434,
title = {How Boundary-spanning Paper Sparkles Citation: From Citation Count to Citation Network},
journal = {Journal of Informetrics},
volume = {17},
number = {3},
pages = {101434},
year = {2023},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2023.101434},
url = {https://www.sciencedirect.com/science/article/pii/S1751157723000597},
author = {Jianhua Hou and Bili Zheng and Dongyi Wang and Yang Zhang and Chaomei Chen},
keywords = {Boundary-spanning, Paper's role definition, Sentence-BERT, Knowledge diffusion, Citation network},
abstract = {Abstract
Previous studies diverged on whether boundary-spanning papers are cited more heavily, however, there is as yet no research that explains how boundary-spanning papers affect citation counts from a causal view and how they influence knowledge diffusion in citation network. To this end, we utilized Propensity Score Matching to clarify the relationship between the citation counts and the degree of boundary-spanning of 281,707 papers in the field of Astronomy (AS), Natural Language Processing (NLP), Library and Information Science (LIS) based on causal inference. We also adopted Sentence-BERT to compute the content similarity among seed papers, cited papers, citing papers, to imply how seed paper affects citation network when knowledge diffuses. According to the similarity between seed paper and citing paper, cited paper and citing paper, we defined paper's role in citation network as four types: Disseminator, Broker, Trigger, and Outlier. The major findings are as follows: (1) Papers with a higher degree of boundary-spanning are more likely to be cited heavily; (2) Disseminator and Outlier account for a larger proportion in three disciplines, while Broker and Trigger account for a smaller proportion; (3) The degree of boundary-spanning and citation counts of four types vary in three disciplines. This work, which reveals paper's value and role in citation network from the perspective of content, has implications to provide some enlightenment for the paper's evaluation.}
}
@article{NGUYEN20211017,
title = {Managing demand volatility during unplanned events with sentiment analysis: a case study of the COVID-19 pandemic},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {1017-1022},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.200},
url = {https://www.sciencedirect.com/science/article/pii/S240589632100971X},
author = {Angie Nguyen and Samir Lamouri and Robert Pellerin},
keywords = {sentiment analysis, news media, demand forecasting, crisis management, natural language processing, epidemic outbreak, analytics, artificial intelligence},
abstract = {Unplanned events such as natural disasters or epidemic outbreaks are usually accompanied by supply chain disruption and highly volatile markets. Besides, the recent COVID-19 crisis has shown that existing artificial intelligence systems and data analytics models, which normally provide valuable support in demand forecasting, have not been able to manage demand volatility. This study contributes addressing this issue and aims to determine whether sentiments conveyed by news media influence consumer behavior. It provides a case study conducted in three steps: (1) data were collected and prepared; (2) a sentiment analysis model was developed; and (3) a statistical analysis was performed to analyze the correlation between sentiments in news and drug consumption during the COVID-19 crisis. Findings highlighted a strong positive correlation between sentiments in news and consumption variability. They therefore suggest that sentiments in news have strong predictive power for demand forecasting in unplanned situations.}
}
@article{XIE2025103136,
title = {A modular configuration-based rapid verification and validation framework combining analytical and simulation methods for intelligent manufacturing systems},
journal = {Simulation Modelling Practice and Theory},
volume = {142},
pages = {103136},
year = {2025},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2025.103136},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X25000711},
author = {Shulian Xie and Weimin Zhang and Mulin Shen and Feng Xue},
keywords = {Intelligent manufacturing systems, Verification and validation, Modular configuration, Analytical method, Co-simulation},
abstract = {Verification and Validation (V&V) are crucial steps in the design to operation process of manufacturing systems to ensure the accuracy, safety, and reliability. The development of intelligent manufacturing technology and the acceleration of market changes are driving the increase of complexity and flexibility of manufacturing systems and the shortening of development cycles. These enhance the comprehensiveness, efficiency, and automation requirements of V&V. However, existing V&V schemes and cases cannot fully utilize the complementary advantages of analytical and simulation methods. It is difficult to meet the effective V&V requirements of complex intelligent manufacturing systems. This study proposes a modular configuration-based manufacturing system V&V framework combining analytical and simulation methods for rapid V&V of manufacturing systems. The construction method of modular configurations of manufacturing systems is explored. The modular configuration has been divided into the component function, system logic, and joint model sub-configurations and modularly packaged and integrated based on Asset Administration Shells (AAS). A rapid V&V process for manufacturing systems has been established. This process employs a test-module-driven approach to achieve configuration-based V&V testing of manufacturing systems. It supports synchronized co-simulation with equipment, software, and models in the loop. Finally, a simple manufacturing system V&V case was provided as an implementation reference. The proposed framework can demonstrate the rapid V&V of intelligent manufacturing systems and contribute to the development of automated V&V tools.}
}
@article{YU2022,
title = {Learning Disease Causality Knowledge From the Web of Health Data},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.297145},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000540},
author = {Hong Qing Yu and Stephan Reiff-Marganiec},
keywords = {Artificial Intelligent, Causality Analysis, Disease Diagnosis, Healthcare, Knowledge Graph, Natural Language Processing, Semantic Web},
abstract = {ABSTRACT
Health information becomes importantly valuable for protecting public health in the current coronavirus situation. Knowledge-based information systems can play a crucial role in helping individuals to practice risk assessment and remote diagnosis. The authors introduce a novel approach that will develop causality-focused knowledge learning in a robust and transparent manner. Then, the machine gains the causality and probability knowledge for inference (thinking) and accurate prediction later. In addition, the hidden knowledge can be discovered beyond the existing understanding of the diseases. The whole approach is built on a causal probability description logic framework that combines natural language processing (NLP), causality analysis, and extended knowledge graph (KG) technologies. The experimental work has processed 801 diseases in total (from the UK NHS website linking with DBpedia datasets). As a result, the machine learnt comprehensive health causal knowledge and relations among the diseases, symptoms, and other facts efficiently.}
}
@article{HASAN2020103568,
title = {Learning structured medical information from social media},
journal = {Journal of Biomedical Informatics},
volume = {110},
pages = {103568},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103568},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420301982},
author = {Abul Hasan and Mark Levene and David Weston},
keywords = {Social media mining, Medical concept extraction, Pharmacovigilance, Conditional random fields, Semi-supervised algorithm},
abstract = {Our goal is to summarise and aggregate information from social media regarding the symptoms of a disease, the drugs used and the treatment effects both positive and negative. To achieve this we first apply a supervised machine learning method to automatically extract medical concepts from natural language text. In an environment such as social media, where new data is continuously streamed, we need a methodology that will allow us to continuously train with the new data. To attain such incremental re-training, a semi-supervised methodology is developed, which is capable of learning new concepts from a small set of labelled data together with the much larger set of unlabelled data. The semi-supervised methodology deploys a conditional random field (CRF) as the base-line training algorithm for extracting medical concepts. The methodology iteratively augments to the training set sentences having high confidence, and adds terms to existing dictionaries to be used as features with the base-line model for further classification. Our empirical results show that the base-line CRF performs strongly across a range of different dictionary and training sizes; when the base-line is built with the full training data the F1 score reaches the range 84%–90%. Moreover, we show that the semi-supervised method produces a mild but significant improvement over the base-line. We also discuss the significance of the potential improvement of the semi-supervised methodology and found that it is significantly more accurate in most cases than the underlying base-line model.}
}
@article{HUANG2023167,
title = {Combining Deep Learning with Knowledge Graph for Design Knowledge Acquisition in Conceptual Product Design},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {138},
number = {1},
pages = {167-200},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.028268},
url = {https://www.sciencedirect.com/science/article/pii/S1526149223000073},
author = {Yuexin Huang and Suihuai Yu and Jianjie Chu and Zhaojing Su and Yangfan Cong and Hanyu Wang and Hao Fan},
keywords = {Conceptual product design, design knowledge acquisition, knowledge graph, entity extraction, relation extraction},
abstract = {The acquisition of valuable design knowledge from massive fragmentary data is challenging for designers in conceptual product design. This study proposes a novel method for acquiring design knowledge by combining deep learning with knowledge graph. Specifically, the design knowledge acquisition method utilises the knowledge extraction model to extract design-related entities and relations from fragmentary data, and further constructs the knowledge graph to support design knowledge acquisition for conceptual product design. Moreover, the knowledge extraction model introduces ALBERT to solve memory limitation and communication overhead in the entity extraction module, and uses multi-granularity information to overcome segmentation errors and polysemy ambiguity in the relation extraction module. Experimental comparison verified the effectiveness and accuracy of the proposed knowledge extraction model. The case study demonstrated the feasibility of the knowledge graph construction with real fragmentary porcelain data and showed the capability to provide designers with interconnected and visualised design knowledge.}
}
@article{ZHOU2021130,
title = {Network pharmacology research and experimental verification of Huangqi (Astragalus Radix) and Jinyingzi (Rosae Laevigatae Fructus) in treating benign prostatic hyperplasia},
journal = {Digital Chinese Medicine},
volume = {4},
number = {2},
pages = {130-143},
year = {2021},
issn = {2589-3777},
doi = {https://doi.org/10.1016/j.dcmed.2021.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S2589377721000215},
author = {Huan Zhou and Meng Yang and Yipin Yu and Hui Liu and Zhixing Qing and Qihua Chen},
keywords = {Huangqi (Astragalus Radix, HQ), Jinyingzi (Rosae Laevigatae Fructus, JYZ), Benign prostatic hyperplasia, Network pharmacology, Apoptosis, Angiogenesis},
abstract = {Objective
This study aimed to analyze the mechanism of action of Huangqi (Astragalus Radix, HQ) - Jinyingzi (Rosae Laevigatae Fructus, JYZ) in the treatment of benign prostatic hyperplasia (BPH) based on network pharmacology and to verify the prediction through animal experimentation.
Methods
Based on the Traditional Chinese Medicine Systems Pharmacology Database and Analysis Platform (TCMSP), Bioinformatics Analysis Tool for Molecular mechANism of Traditional Chinese Medicine (BATMAN-TCM) databases, and literature, the active components and related target genes of HQ and JYZ were screened. The BPH target genes were screened based on the DisGeNET and GeneGards databases, and Excel was used to merge and remove duplicates. The Perl language was used to obtain drug-BPH target genes by intersecting shared target genes. A drug-component-target gene network diagram was constructed using Cytoscape software. The drug-BPH intersection target genes were inputted into the STRING database, and the key target genes were selected according to the degree algorithm. The output formed the basis for Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway enrichment analyses to determine the potential mechanism of HQ and JYZ in BPH treatment. High, medium, and low doses of HQ and JYZ extract were used to intervene in BPH rats, and then the prostate volume, wet weight, and prostate index of the BPH rats were determined. Changes in prostate histopathology and microvessel density (MVD) were evaluated using immunohistochemistry, and the optimal HQ and JYZ extract dose was confirmed. Finally, the optimal dose was used to intervene in a BPH rat model, and AKT1 and VEGF expressions were examined by immunohistochemistry.
Results
Based on network pharmacology, 33 active components and 772 target genes were identified from HQ and JYZ, along with 817 BPH target genes and 112 drug-BPH common target genes. Among them were 10 key target genes, including AKT1, JUN, MAPK1, IL-6, TNF, ESR1, and VEGFA. KEGG enrichment analysis revealed 135 signaling pathways, including PI3K/AKT, IL-17, TNF, p53, MAPK, VEGF, JAK-STAT, and NF-κB pathways. The animal experiment showed that HQ and JYZ significantly improved prostate volume, wet weight, prostate index, and prostate histopathology of BPH rats, reducing MVD. In addition, HQ and JYZ inhibited the expression of AKT1 and VEGF in the prostate tissue of rats, promoted epithelial cell apoptosis, and inhibited angiogenesis, consistent with the prediction.
Conclusion
The combination of HQ and JYZ is effective for BPH therapy through multi-compound and multi-target collaboration. Its possible mechanism in treating BPH includes regulation of AKT1, VEGF protein, PI3K/Akt, and VEGF signaling pathways related to apoptosis, angiogenesis, and inflammation, with potential for clinical use and research.}
}
@article{NOEL2023179,
title = {Pluriversal Futures for Design Education},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {9},
number = {2},
pages = {179-196},
year = {2023},
note = {The Future of Design Education: Rethinking Design Education for the 21st Century},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2023.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872623000370},
author = {Lesley-Ann Noel and Adolfo Ruiz and Frederick M.C. {van Amstel} and Victor Udoewa and Neeta Verma and Nii Kommey Botchway and Arvind Lodaya and Shalini Agrawal},
keywords = {Pluriverse, Pluriversal design, Ontological design, Relationality, Lived experience, Decolonizing design},
abstract = {The Future of Design Education working group on pluriversal design—with members from Latin America, the Caribbean, Africa, South and Southeastern Asia, North America, Oceania, and Europe—developed recommendations for higher education design curricula. The group addresses the dominance of a Eurocentric design canon and worldwide colonization by a twentieth-century design monoculture grounded in the concept of universal human experience. Curricular recommendations honor Indigenous worlds and place-based ways of being, and chime with anthropologist Arturo Escobar’s premise that every community practices the design of itself, through participatory processes that are independent of experts. The authors posit that rather than a Cartesian rationalist perspective, the group advocates a relational view of situations in which the design responses to interdependent natural, social, economic, and technical systems, are specific to places and cultures. The recommendations assert a pluriversal design imperative in which multiple worldviews thrive and diverse lived experiences inform the entire field, as well as individual projects.}
}
@article{NOTORIO2025101404,
title = {A critical discourse analysis of the UN Sustainable Development Goals in destination management organization (DMO) websites: A study of provincial DMOs in the Philippines},
journal = {Tourism Management Perspectives},
volume = {59},
pages = {101404},
year = {2025},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2025.101404},
url = {https://www.sciencedirect.com/science/article/pii/S2211973625000698},
author = {Paul Anthony C. Notorio and Robert Charles G. Capistrano and Richard S. Aquino and Kelly S. Bricker},
keywords = {Critical discourse analysis, Destination management organizations, Destination websites, Sustainability communication, Sustainable Development Goals, Theory of communicative action},
abstract = {As destination management organizations (DMOs) adopt a destination stewardship approach, communication becomes more important in engaging stakeholders in the sustainable tourism discourse and achieving the United Nations (UN) Sustainable Development Goals (SDGs). This study examines the SDG-related content communicated in the websites of provincial DMOs in the Philippines. Texts from 1676 webpages mined from 51 provincial DMO websites were subjected to Habermasian critical discourse analysis (CDA). The findings showed that some SDGs were better represented due to the nature and role of DMOs. The CDA also showed that the SDG-related information fell short in terms of clarity, sincerity, and legitimacy. Based on the findings, the Spectrum of SDG Communication in DMO Websites model was conceptualized. The study contributes to the developing literature on tourism and the SDGs. It also provides practical insights into sustainability communication for DMOs as well as recommendations to address inherent challenges at a destination level.}
}
@incollection{VOROPAI20231,
title = {Chapter 1 - Methodological framework for hierarchical modeling of energy systems},
editor = {Nikolai I. Voropai and Valery A. Stennikov},
booktitle = {Hierarchical Modeling of Energy Systems},
publisher = {Elsevier},
pages = {1-78},
year = {2023},
isbn = {978-0-443-13917-8},
doi = {https://doi.org/10.1016/B978-0-44-313917-8.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443139178000109},
author = {N.I. Voropai and V.A. Stennikov and S.M. Senderov and B.G. Saneev and N.N. Novitsky and O.V. Khamisov and L.V. Massel and A.G. Massel and A.V. Mikheev},
keywords = {energy system, electric power system, smart grid, cyber-physical system, game theory, Danzig–Wolfe decomposition, bilevel programming, ontology modeling},
abstract = {This chapter outlines the distinctive features of energy systems as a subject of scholarly research in the context of substantiation of their development and control of the modes of operation of these systems. We analyze general methodological ideas and approaches to hierarchical modeling of complex systems. The chapter considers the generalized formalized technology of hierarchical modeling of complex systems with the use of operations of aggregation/disaggregation of models used when moving from one level to another in the hierarchy of models, as well as the transformation of decision-making criteria that takes place in this process. We outline the mathematical methods for decomposition of optimization problems to be employed when building a hierarchy of models for the study of complex systems and control over them. We analyze mathematical statements of bilevel mathematical programming problems as an example of adopting the ideology of the hierarchical approach. We consider the role and special aspects of the application of intelligent information technologies to the hierarchical modeling of energy systems as an ideology of solving the problems of development and operation of these complex systems.}
}
@article{BADGER2019103185,
title = {Machine learning for phenotyping opioid overdose events},
journal = {Journal of Biomedical Informatics},
volume = {94},
pages = {103185},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103185},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419301030},
author = {Jonathan Badger and Eric LaRose and John Mayer and Fereshteh Bashiri and David Page and Peggy Peissig},
keywords = {Machine learning, Opioid, Phenotype, Overdose, Electronic health record},
abstract = {Objective
To develop machine learning models for classifying the severity of opioid overdose events from clinical data.
Materials and methods
Opioid overdoses were identified by diagnoses codes from the Marshfield Clinic population and assigned a severity score via chart review to form a gold standard set of labels. Three primary feature sets were constructed from disparate data sources surrounding each event and used to train machine learning models for phenotyping.
Results
Random forest and penalized logistic regression models gave the best performance with cross-validated mean areas under the ROC curves (AUCs) for all severity classes of 0.893 and 0.882 respectively. Features derived from a common data model outperformed features collected from disparate data sources for the same cohort of patients (AUCs 0.893 versus 0.837, p value = 0.002). The addition of features extracted from free text to machine learning models also increased AUCs from 0.827 to 0.893 (p value < 0.0001). Key word features extracted using natural language processing (NLP) such as ‘Narcan’ and ‘Endotracheal Tube’ are important for classifying overdose event severity.
Conclusion
Random forest models using features derived from a common data model and free text can be effective for classifying opioid overdose events.}
}
@incollection{GYRARD2022199,
title = {Chapter 10 - Reasoning over personalized healthcare knowledge graph: a case study of patients with allergies and symptoms},
editor = {Sanju Tiwari and Fernando {Ortiz Rodriguez} and M.A. Jabbar},
booktitle = {Semantic Models in IoT and eHealth Applications},
publisher = {Academic Press},
pages = {199-225},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-91773-5},
doi = {https://doi.org/10.1016/B978-0-32-391773-5.00016-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917735000169},
author = {Amelie Gyrard and Utkarshani Jaimini and Manas Gaur and Saeedeh Shekharpour and Krishnaprasad Thirunarayan and Amit Sheth},
keywords = {Personalized healthcare knowledge graph, Contextualization, Ontology, Semantic data interoperability, Internet of things, FAIR principles, Reusability, Semantic web of things, Data analytics, Rule-based reasoning, Inference engine},
abstract = {Background: Current health applications (e.g., Google Fit) based on devices (e.g., Fitbit) often are limited to data visualization, summarization, or statistics-based models to understand and explore the patient data. The data must be more meaningful for the patient in a contextualized and personalized manner. Objective: A knowledge-based reasoning architecture is designed to collect, manage, integrate, and analyze multimodal health data from sources such as smart phone applications, wearable devices, Internet of Things (IoT) devices, and environmental sensors. The architecture and associated methods with several use cases are demonstrated in the chapter. Methods: A personalized health knowledge graph is developed to capture personalized and contextualized patient data from smartphone applications, wearables, and environmental sensors. It converts the data into knowledge using relevant medical knowledge from existing ontologies. The personalized health knowledge graph and its use cases consists of (1) Kno.e.sis asthma ontology, whose core model (e.g., patient) is generic enough to be reused for other use cases, and (2) a rule-based reasoning engine, which infers high-level abstractions from data annotated with Kno.e.sis asthma ontology to provide suggestions to patients. Results: The designed architecture is comprised of components compliant with each other: (1) A personalized health knowledge graph, which integrates cross-domain knowledge (asthma, weather, W3C SOSA SSN, smart home), (2) an automatic semantic annotation engine, (3) a rule (IF THEN ELSE) data set, which reuses domain expertise, (4) a rule-based inference engine to automatically infer new abstractions, and (5) a generic query engine to query inferred data and provides suggestions for a better health management. The architecture reuses and integrates knowledge in a machine-processable form to replace human interpretation as a long-term goal. Conclusion: The knowledge graph based reasoning is comprised of a set of tutorials with SPARQL queries and end-to-end scenarios (e.g., asthma, obesity, allergies to food). The methodology used to develop the knowledge graph can be generalized, refined, and reused for other diseases or even to other domains such as agriculture with smart irrigation to deal with different crop types, robotics in smart home for ageing people, smart energy, etc. The health knowledge graph can be used as a gold standard to design KG made with Machine Learning algorithms automatically.}
}
@article{KHALAF2025107125,
title = {Clinical microbiology and artificial intelligence: Different applications, challenges, and future prospects},
journal = {Journal of Microbiological Methods},
volume = {232-234},
pages = {107125},
year = {2025},
issn = {0167-7012},
doi = {https://doi.org/10.1016/j.mimet.2025.107125},
url = {https://www.sciencedirect.com/science/article/pii/S0167701225000417},
author = {Wafaa S. Khalaf and Radwa N. Morgan and Walid F. Elkhatib},
keywords = {Artificial intelligence, Machine learning, Deep learning, Microbiology, Staining, WGS, Antimicrobial peptides, Vaccines},
abstract = {Conventional clinical microbiological techniques are enhanced by the introduction of artificial intelligence (AI). Comprehensive data processing and analysis enabled the development of curated datasets that has been effectively used in training different AI algorithms. Recently, a number of machine learning (ML) and deep learning (DL) algorithms are developed and evaluated using diverse microbiological datasets. These datasets included spectral analysis (Raman and MALDI-TOF spectroscopy), microscopic images (Gram and acid fast stains), and genomic and protein sequences (whole genome sequencing (WGS) and protein data banks (PDBs)). The primary objective of these algorithms is to minimize the time, effort, and expenses linked to conventional analytical methods. Furthermore, AI algorithms are incorporated with quantitative structure-activity relationship (QSAR) models to predict novel antimicrobial agents that address the continuing surge of antimicrobial resistance. During the COVID-19 pandemic, AI algorithms played a crucial role in vaccine developments and the discovery of new antiviral agents, and introduced potential drug candidates via drug repurposing. However, despite their significant benefits, the implementation of AI encounters various challenges, including ethical considerations, the potential for bias, and errors related to data training. This review seeks to provide an overview of the most recent applications of artificial intelligence in clinical microbiology, with the intention of educating a wider audience of clinical practitioners regarding the current uses of machine learning algorithms and encouraging their implementation. Furthermore, it will discuss the challenges related to the incorporation of AI into clinical microbiology laboratories and examine future opportunities for AI within the realm of infectious disease epidemiology.}
}
@article{LEVISEN2019101173,
title = {Biases we live by: Anglocentrism in linguistics and cognitive sciences},
journal = {Language Sciences},
volume = {76},
pages = {101173},
year = {2019},
note = {Biases in Linguistics},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0388000117303558},
author = {Carsten Levisen},
keywords = {Anglocentrism, Anglo keywords, English as a global metalanguage, English as a language of knowledge production},
abstract = {This paper explores “Anglocentrism” as a bias in contemporary linguistics and cognitive sciences. Anglo concepts dominate international discourse on language and cognition, but the influence that this Anglocentric metalinguistic discourse has on global knowledge production, research methods, and the theoretical framing of research questions is rarely debated. Three case studies on heavily “Anglicised” discursive domains are provided: (i) “the mind” – and the Anglicisation of global discourse of human personhood; (ii) “happiness” – and the Anglicisation of the global discourse of human values; (iii) “community” – and the Anglicisation of the global discourse of human sociality. With cross-linguistic evidence from Europe (Danish), and the Pacific (Bislama), the paper denaturalises the English words mind, happiness, and community and the cognitive models they stand for, demonstrating that these words are not “neutral” nor “innocent” metalinguistic descriptors. Rather, they are quintessential Anglo constructs, and as such they provide a lens on humanity that is biased towards an Anglo interpretation of the world. Finally, the paper explores the “bias” concept. Paradoxically, the bias concept is in itself a product of the Anglosphere, as as such a part of the problem. However, due to this word’s meta-discursive function, the paper argues that the bias concept can become a useful Trojan Horse, a concept through which we can fight Anglocentrism from within, and pave the way for a more adequate representation of human diversity in linguistics and cognitive sciences.}
}
@article{LALANDER2025101508,
title = {Comparative reflections on contested hydro-territorial rights in Indigenous communities of Bolivia, India and Tanzania},
journal = {Social Sciences & Humanities Open},
volume = {11},
pages = {101508},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101508},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125002360},
author = {Rickard Lalander and Nandita Singh and J. Fernando Galindo and Faustin Maganga and Sara Sjöling and Kari Lehtilä},
keywords = {Hydro-territorial rights, Rural indigenous communities, Critical institutionalism- political ecology, Water and land rights, Bolivia, India, Tanzania},
abstract = {In Indigenous and rural communities of the Global South, relationships between humans, water, and life are understood and organized in various ways, with water often viewed as intrinsically linked to land. These resources not only serve the tangible purpose of supporting livelihoods but also form a fundamental basis for intangible aspects such as culture, identity, and epistemic-ontological foundations. In this article, the interconnected rights to both water and land for these communities are conceptualized as "hydro-territorial rights" (HTRs). This concept encompasses the formal and/or customary norms and practices related to the ownership, access, control, and use of both land and water, which are regarded as interrelated entities. Theoretically, this article draws on rights-based critical institutionalism and political ecology approaches to natural resource governance, including the legal-pluralist distinction between de jure rights on paper and de facto rights in practice. The aim is to identify and comparatively analyze contentious situations and conflicts surrounding water and land rights in rural Indigenous contexts across three postcolonial settings in the Global South. Methodologically, we employ a comparative strategy based on theory and literature reviews to examine conflictual hydro-territorial rights situations within selected Indigenous localities in Bolivia, India, and Tanzania. This analysis is complemented by interviews with local actors and observations in these three settings. Among our findings, we highlight both conflicts and temporary alliances between local and external interests, as well as practices and mechanisms related to the colonial legacy. We also explore how contemporary capitalist developmental interventions in these areas have impacted communities' access to and rights over local water and land resources, resulting in significant consequences for local livelihoods and ethno-cultural-territorial identities.}
}
@article{BAGHDADI2025100229,
title = {Comprehensive classification and analysis of s-commerce design research},
journal = {Telematics and Informatics Reports},
volume = {19},
pages = {100229},
year = {2025},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2025.100229},
url = {https://www.sciencedirect.com/science/article/pii/S277250302500043X},
author = {Youcef Baghdadi and Supriya Pulparambil},
keywords = {S-commerce design, Design framework, Design artifact, Behavioral design, S-commerce platform},
abstract = {Social media and social networks have a profound impact on business and created a new business model called social commerce (s-commerce). S-commerce is a type of electronic commerce (e-commerce) which utilizes social media platforms to facilitate commercial activities. S-commerce design deals with the structuring of online platforms that integrate social interaction and commercial interactions. S-commerce design is very significant in the transition of e-commerce to s-commerce as user preferences, social interactions, and user-generated content determine the business. This research conducts a systematic mapping study to classify s-commerce design articles published between 2015 and 2023 in online databases. A framework is proposed to analyze and categorize the existing body of knowledge on s-commerce design. The framework comprises three dimensions: design themes, design principles, and design artifacts. The results of the research show that the most prevalent design theme under s-commerce design is behavioral design (66 %), the commonly addressed design artifact is the behavior intention model (63 %), and the most emphasized design principle is usability (22 %). Among the least explored dimensions of s-commerce design are requirement design as the theme (4 %), playfulness as the design principle (6 %), and technical models and conceptual frameworks as the artifact (1 %). This indicates that human behavioral design has been well studied, while more research on technical, business, trust, and requirement design is needed for the efficient implementation of s-commerce platforms. The findings of this study guide future s-commerce research by highlighting gaps and potential directions in areas such as personalization, security, and interaction features.}
}
@incollection{MANN20241279,
title = {Hybrid Artificial Intelligence-based Process Flowsheet Synthesis and Design using Extended SFILES Representation},
editor = {Flavio Manenti and Gintaras V. Reklaitis},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {53},
pages = {1279-1284},
year = {2024},
booktitle = {34th European Symposium on Computer Aided Process Engineering / 15th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-28824-1.50214-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443288241502143},
author = {Vipul Mann and Mauricio Sales-Cruz and Rafiqul Gani and Venkat Venkatasubramanian},
keywords = {process design, flowsheet modeling, artificial intelligence, computer-aided flowsheet synthesis},
abstract = {Process flowsheet synthesis and design involves simultaneously solving several problems, including determining the unit operations and their sequence, underlying reactions and reaction stoichiometry, downstream separation design and operation parameters, sustainability factors, and many more. Naturally, this results in a large amount of data being associated with a given process flowsheet that captures the relevant process context and should be readily accessible. This data is useful for solving related problems both using data-driven and process knowledge-based methods. A hierarchical framework, called the extended SFILES (or eSFILES), proposed recently stores this information using a combination of text-based, graph-based, and ontology-based representations. Here, we provide details on a prototype software for automated flowsheet representation and generation across various levels in the eSFILES framework. The underlying methods include a novel flowsheet grammar, a set of inferencing algorithms, and interfacing with a commercial process simulator facilitating rigorous flowsheet simulation.}
}
@article{AKSE2024123796,
title = {Towards a conceptual model of uncertainty management for socio-technical innovations: A systematic review},
journal = {Technological Forecasting and Social Change},
volume = {209},
pages = {123796},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123796},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524005948},
author = {Ruben Akse},
keywords = {Uncertainty, Risk, Multi-actor decision-making, Sustainability transition, Public-private partnership, Infrastructure},
abstract = {Socio-technical innovations are necessary to establish a transition towards sustainable infrastructural systems. Actors developing and implementing these innovations experience considerable uncertainty whether innovations will technically work, are beneficiary for societal goals and how other actors will behave during the innovation process. Such uncertainties hamper the (successful) introduction of innovations, as actors struggle with handling uncertainty. There is a research gap that explains how public and private actors make specific choices regarding uncertainty interactively. Therefore, this paper has systematically reviewed literature on the interactions of actors in uncertain innovation processes. In total fifty-three articles out 2909 have been included in the full review. Based on these articles, a conceptual model has been proposed how actors experience, respond to, and consequently make decisions under uncertainty, in a cyclical interaction process with other actors. This process is influenced by uncertainty competencies (actor-specific characteristics), as well as uncertainty settings (formal and informal governance rules). The conceptual model will inform further research on the role of uncertainty in multi-actor innovation processes, and how actor competencies and uncertainty settings can be improved to stimulate a sustainability transition by socio-technical innovations.}
}
@article{SHAHANDASHTI2024107526,
title = {A PRISMA-driven systematic mapping study on system assurance weakeners},
journal = {Information and Software Technology},
volume = {175},
pages = {107526},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107526},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924001319},
author = {Kimya Khakzad Shahandashti and Alvine B. Belle and Timothy C. Lethbridge and Oluwafemi Odu and Mithila Sivakumar},
keywords = {Assurance cases, Assurance deficits, Uncertainty, Logical fallacies, PRISMA, GSN, SACM, Systematic mapping study, Cyber–physical systems, Safety, Reliability, Defeaters},
abstract = {Context:
An assurance case is a structured hierarchy of claims aiming at demonstrating that a mission-critical system supports specific requirements (e.g., safety, security, privacy). The presence of assurance weakeners (i.e., assurance deficits, logical fallacies) in assurance cases reflects insufficient evidence, knowledge, or gaps in reasoning. These weakeners can undermine confidence in assurance arguments, potentially hindering the verification of mission-critical system capabilities which could result in catastrophic outcomes (e.g., loss of lives). Given the growing interest in employing assurance cases to ensure that systems are developed to meet their requirements, exploring the management of assurance weakeners becomes beneficial.
Objective:
As a stepping stone for future research on assurance weakeners, we aim to initiate the first comprehensive systematic mapping study on this subject.
Methods:
We followed the well-established PRISMA 2020 and SEGRESS guidelines to conduct our systematic mapping study. We searched for primary studies in five digital libraries and focused on the 2012–2023 publication year range. Our selection criteria focused on studies addressing assurance weakeners from a qualitative standpoint, resulting in the inclusion of 39 primary studies in our systematic review.
Results:
Our systematic mapping study reports a taxonomy (map) that provides a uniform categorization of assurance weakeners and approaches proposed to manage them from a qualitative perspective. The taxonomy classifies weakeners in four categories: aleatory, epistemic, ontological, and argument uncertainty. Additionally, it classifies approaches supporting the management of weakeners in three main categories: representation, identification and mitigation approaches.
Conclusion:
Our study findings suggest that the SACM (Structured Assurance Case Metamodel) – a standard specified by the OMG (Object Management Group) – offers a comprehensive range of capabilities to capture structured arguments and reason about their potential assurance weakeners. Our findings also suggest novel assurance weakener management approaches should be proposed to better assure mission-critical systems.}
}
@article{KANDHRO2024e31488,
title = {Performance evaluation of E-VGG19 model: Enhancing real-time skin cancer detection and classification},
journal = {Heliyon},
volume = {10},
number = {10},
pages = {e31488},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e31488},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024075194},
author = {Irfan Ali Kandhro and Selvakumar Manickam and Kanwal Fatima and Mueen Uddin and Urooj Malik and Anum Naz and Abdulhalim Dandoush},
keywords = {Skin cancer detection, Health care, Image segmentation, Pre-trained models, Machine learning and deep learning},
abstract = {Skin cancer is a pervasive and potentially life-threatening disease. Early detection plays a crucial role in improving patient outcomes. Machine learning (ML) techniques, particularly when combined with pre-trained deep learning models, have shown promise in enhancing the accuracy of skin cancer detection. In this paper, we enhanced the VGG19 pre-trained model with max pooling and dense layer for the prediction of skin cancer. Moreover, we also explored the pre-trained models such as Visual Geometry Group 19 (VGG19), Residual Network 152 version 2 (ResNet152v2), Inception-Residual Network version 2 (InceptionResNetV2), Dense Convolutional Network 201 (DenseNet201), Residual Network 50 (ResNet50), Inception version 3 (InceptionV3), For training, skin lesions dataset is used with malignant and benign cases. The models extract features and divide skin lesions into two categories: malignant and benign. The features are then fed into machine learning methods, including Linear Support Vector Machine (SVM), k-Nearest Neighbors (KNN), Decision Tree (DT), Logistic Regression (LR) and Support Vector Machine (SVM), our results demonstrate that combining E-VGG19 model with traditional classifiers significantly improves the overall classification accuracy for skin cancer detection and classification. Moreover, we have also compared the performance of baseline classifiers and pre-trained models with metrics (recall, F1 score, precision, sensitivity, and accuracy). The experiment results provide valuable insights into the effectiveness of various models and classifiers for accurate and efficient skin cancer detection. This research contributes to the ongoing efforts to create automated technologies for detecting skin cancer that can help healthcare professionals and individuals identify potential skin cancer cases at an early stage, ultimately leading to more timely and effective treatments.}
}
@article{HUANG2023106105,
title = {Cell type- and region-specific translatomes in an MPTP mouse model of Parkinson's disease},
journal = {Neurobiology of Disease},
volume = {180},
pages = {106105},
year = {2023},
issn = {0969-9961},
doi = {https://doi.org/10.1016/j.nbd.2023.106105},
url = {https://www.sciencedirect.com/science/article/pii/S0969996123001195},
author = {Qiaoying Huang and Congmin Chen and Weizhao Chen and Chaoyu Cai and Hailin Xing and Junyu Li and Mingtao Li and Shanshan Ma},
keywords = {Parkinson's disease, Translatome, RiboTag, Glycosphingolipid},
abstract = {Parkinson's disease (PD) is the most common neurodegenerative movement disorder, characterized by the progressive loss of nigrostriatal dopaminergic neurons (DANs), involving the dysregulation of both neurons and glial cells. Cell type- and region-specific gene expression profiles can provide an effective source for revealing the mechanisms of PD. In this study, we adopted the RiboTag approach to obtain cell type (DAN, microglia, astrocytes)- and brain region (substantia nigra, caudate-putamen)-specific translatomes at an early stage in an MPTP-induced mouse model of PD. Through DAN-specific translatome analysis, the glycosphingolipid biosynthetic process was identified as a significantly downregulated pathway in the MPTP-treated mice. ST8Sia6, a key downregulated gene related to glycosphingolipid biosynthesis, was confirmed to be downregulated in nigral DANs from postmortem brains of patients with PD. Specific expression of ST8Sia6 in DANs exerts anti-inflammatory and neuroprotective effects in MPTP-treated mice. Through cell type (microglia vs. astrocyte) and brain region (substantia nigra vs. caudate-putamen) comparisons, nigral microglia showed the most intense immune responses. Microglia and astrocytes in the substantia nigra showed similar levels of activation in interferon-related pathways and interferon gamma (IFNG) was identified as the top upstream regulator in both cell types. This work highlights that the glycosphingolipid metabolism pathway in the DAN is involved in neuroinflammation and neurodegeneration in an MPTP mouse model of PD and provides a new data source for elucidating the pathogenesis of PD.}
}
@article{SEVILLALIU2022173,
title = {ACT and the Kyoto School of Philosophy:Interdisciplinary dialogues on personhood, ethics, and becoming},
journal = {Journal of Contextual Behavioral Science},
volume = {26},
pages = {173-180},
year = {2022},
issn = {2212-1447},
doi = {https://doi.org/10.1016/j.jcbs.2022.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S2212144722000941},
author = {Anton Sevilla-Liu},
keywords = {Philosophy, Eastern philosophy, Buddhism, Comparative philosophy},
abstract = {This paper examines the connections between Acceptance and Commitment Therapy (ACT) and Kyoto School Philosopher Mori Akira (1915–1976), in order to see how ACT and functional contextualism can engage other subfields in academic philosophy like philosophy of the human person, ethics, and philosophy of human becoming, and other areas such as eastern and continental philosophy. It first examines Mori's model of the layers of human existence (organic, conscious, reflective, and self-aware) and how it connects to ACT's views of the human person (workability, languaging, self-processes), presenting how these potentially critique modern ideas of the human being as a merely rational animal. It then proceeds to ACT and Mori's ethics of freely-chosen values and how these can critique utilitarian and deontological ethics. Finally, it proceeds to the philosophy of human becoming and how ACT and Mori can contribute to a contextual-existential view of the path of human development.}
}
@article{SON201858,
title = {Deep Phenotyping on Electronic Health Records Facilitates Genetic Diagnosis by Clinical Exomes},
journal = {The American Journal of Human Genetics},
volume = {103},
number = {1},
pages = {58-73},
year = {2018},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2018.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S000292971830171X},
author = {Jung Hoon Son and Gangcai Xie and Chi Yuan and Lyudmila Ena and Ziran Li and Andrew Goldstein and Lulin Huang and Liwei Wang and Feichen Shen and Hongfang Liu and Karla Mehl and Emily E. Groopman and Maddalena Marasa and Krzysztof Kiryluk and Ali G. Gharavi and Wendy K. Chung and George Hripcsak and Carol Friedman and Chunhua Weng and Kai Wang},
keywords = {electronic health records, phenotyping, biomedical informatics, natural language processing, knowledge engineering, precision medicine, diagnosis, next-generation sequencing, exome, genome},
abstract = {Integration of detailed phenotype information with genetic data is well established to facilitate accurate diagnosis of hereditary disorders. As a rich source of phenotype information, electronic health records (EHRs) promise to empower diagnostic variant interpretation. However, how to accurately and efficiently extract phenotypes from heterogeneous EHR narratives remains a challenge. Here, we present EHR-Phenolyzer, a high-throughput EHR framework for extracting and analyzing phenotypes. EHR-Phenolyzer extracts and normalizes Human Phenotype Ontology (HPO) concepts from EHR narratives and then prioritizes genes with causal variants on the basis of the HPO-coded phenotype manifestations. We assessed EHR-Phenolyzer on 28 pediatric individuals with confirmed diagnoses of monogenic diseases and found that the genes with causal variants were ranked among the top 100 genes selected by EHR-Phenolyzer for 16/28 individuals (p < 2.2 × 10−16), supporting the value of phenotype-driven gene prioritization in diagnostic sequence interpretation. To assess the generalizability, we replicated this finding on an independent EHR dataset of ten individuals with a positive diagnosis from a different institution. We then assessed the broader utility by examining two additional EHR datasets, including 31 individuals who were suspected of having a Mendelian disease and underwent different types of genetic testing and 20 individuals with positive diagnoses of specific Mendelian etiologies of chronic kidney disease from exome sequencing. Finally, through several retrospective case studies, we demonstrated how combined analyses of genotype data and deep phenotype data from EHRs can expedite genetic diagnoses. In summary, EHR-Phenolyzer leverages EHR narratives to automate phenotype-driven analysis of clinical exomes or genomes, facilitating the broader implementation of genomic medicine.}
}
@article{MALIK2022104290,
title = {A level-of-details framework for representing occupant behavior in agent-based models},
journal = {Automation in Construction},
volume = {139},
pages = {104290},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104290},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522001637},
author = {Jeetika Malik and Elie Azar and Ardeshir Mahdavi and Tianzhen Hong},
keywords = {Level of detail, Occupant behavior, Agent-based model, Building simulation, Behavioral theory},
abstract = {Agent-based modeling is an advanced computational technique capable of representing complex and dynamic processes of human behavior in building performance simulation. Though the agent-based approach supports diverse applications concerning human behavior modeling within the built environment, there is no consensus on the optimal amount of information or level of granularity needed for occupant information representation. This paper attempts to formalize the level of details (LoD) needed for occupant behavior representation in agent-based environments. A novel framework, grounded on the concept of LoD, is proposed to select the required details in representing occupants in agent-based models. Ten attributes related to occupants' presence, movement, behavioral processes, and repertoire are considered to define the LoD. The framework identifies use case parameters as the guiding principle and allows a hybrid approach for selecting varying degrees of occupant attributes to serve the purpose of simulation. A discussion on the pertinence of different occupant behavior LoDs in relation to the desired objective and simulation context is also presented. The study intends to support the occupant behavior research by advancing agent-based occupant modeling in building performance simulation.}
}
@article{DEVARAKONDA2024104627,
title = {Clinical trial recommendations using Semantics-Based inductive inference and knowledge graph embeddings},
journal = {Journal of Biomedical Informatics},
volume = {154},
pages = {104627},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104627},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000455},
author = {Murthy V. Devarakonda and Smita Mohanty and Raja Rao Sunkishala and Nag Mallampalli and Xiong Liu},
keywords = {Clinical trials, Knowledge graphs, Graph embeddings, Transductive inference, Inductive inference, Recommendation systems, Graph Neural Networks (GNNs), Graph Attention Networks (GATs)},
abstract = {Objective
Designing a new clinical trial entails many decisions, such as defining a cohort and setting the study objectives to name a few, and therefore can benefit from recommendations based on exhaustive mining of past clinical trial records. This study proposes an approach based on knowledge graph embeddings and semantics-driven inductive inference for generating such recommendations.
Method
The proposed recommendation methodology is based on neural embeddings trained on first-of-its-kind knowledge graph constructed from clinical trials data. The methodology includes design of a knowledge graph for clinical trial data, evaluation of various knowledge graph embedding techniques for it, application of a novel inductive inference method using these embeddings, and generation of recommendations for clinical trial design. The study uses freely available data from clinicaltrials.gov and related sources.
Results
The proposed approach for recommendations obtained relevance scores ranging from 70% to 83%. These scores were determined by evaluating the text similarity of recommended elements to actual elements used in clinical trials that are in progress. Furthermore, the most pertinent recommendations were consistently located towards the top of the list, indicating the effectiveness of our method.
Conclusion
Our study suggests that inductive inference using node semantics is a viable approach for generating recommendations using graphs neural embeddings, and that there is a potential for improvement in training graph embeddings using node semantics.}
}
@article{TAELMAN201843,
title = {On the Semantics of Tpf-qs towards Publishing and Querying rdf Streams at Web-scale},
journal = {Procedia Computer Science},
volume = {137},
pages = {43-54},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316090},
author = {Ruben Taelman and Riccardo Tommasini and Joachim Van Herwegena and Miel Vander Sandea and Emanuele Della Valle and Ruben Verborgh},
keywords = {Linked Data, rdf stream processing, continuous querying, , , },
abstract = {rdf Stream Processing (rsp) is a rapidly evolving area of research that focuses on extensions of the Semantic Web in order to model and process Web data streams. While state-of-the-art approaches concentrate on server-side processing of rdf streams, we investigate the Triple Pattern Fragments Query Streamer (tpf-qs) method for server-side publishing of rdf streams, which moves the workload of continuous querying to clients. We formalize tpf-qs in terms of the rsp-ql reference model in order to formally compare it with existing rsp query languages. We experimentally validate that, compared to the state of the art, the server load of tpf-qs scales better with increasing numbers of concurrent clients in case of simple queries, at the cost of increased bandwidth consumption. This shows that tpf-qs is an important first step towards a viable solution for Web-scale publication and continuous processing of rdf streams.}
}
@incollection{SASSI202197,
title = {Chapter 8 - The connected electronic health record: a semantic-enabled, flexible, and unified electronic health record},
editor = {Sarika Jain and Vishal Jain and Valentina Emilia Balas},
booktitle = {Web Semantics},
publisher = {Academic Press},
pages = {97-116},
year = {2021},
isbn = {978-0-12-822468-7},
doi = {https://doi.org/10.1016/B978-0-12-822468-7.00008-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128224687000080},
author = {Salma Sassi and Richard Chbeir},
keywords = {EHR, IoT, interoperability, HL7, UMLs, analysis, prediction, knowledge domain},
abstract = {One of the major evolutions on e-health is the digitization of the Electronic Health Record (EHR), which can thereby be shared more easily among hospitals via the Internet. During the last century, the Internet of Things for Healthcare (Healthcare-IoT) appeared. It includes health sensor devices as new sources of health information, as well as new technologies and applications for remote diagnostics and the monitoring of patients, equipment, and drugs. The adoption of IoT in health care, anywhere and anytime, will bring future health care services to a new level and great conveniences to both doctors and patients for effective illness monitoring and diagnosis. The emergence of the IoT has led to a myriad of Medical Connected Objects (MCOs). Accordingly, different associated challenges are emerging, including the heterogeneity of the gathered health data from these MCOs within ever changing contexts. Current studies are limited to specific cases and to specific sensors to deliver their services and do not present a standard integration between patients’ MCOs and EHR systems. This disparity limits cooperation between patients and general practitioners, leading to security, transparency, and privacy issues. These challenges are difficult to address due to the absence of a reference model for describing health data and their sources and for linking this data with their contexts. This book chapter addresses this problem and introduces a semantic-based model for flexible and customized EHR for patient monitoring with MCOs. In fact, this approach exploits the HL7 FHIR (Fast Healthcare Interoperability Resources) standard and UMLS (Unified Medical Language System) terminology to handle integration challenges and supports the semantic integration of the EHR system. The main goal is to provide semantic-connected EHR that will facilitate data interoperability, integration, information search and retrieval, and automatic inference and adaptation in real time.}
}
@article{HAN2019159,
title = {Immune checkpoint molecule herpes virus entry mediator is overexpressed and associated with poor prognosis in human glioblastoma},
journal = {EBioMedicine},
volume = {43},
pages = {159-170},
year = {2019},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2019.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352396419302361},
author = {Ming-Zhi Han and Shuai Wang and Wen-Bo Zhao and Shi-Lei Ni and Ning Yang and Yang Kong and Bin Huang and An-Jing Chen and Xin-Gang Li and Jian Wang and Dong-Hai Wang},
keywords = {Glioma, HVEM, Immune response, Prognosis, Tumour microenvironment},
abstract = {Background
Dysregulation of immune checkpoint molecules leads to immune evasion in human tumours but has become a viable target for tumour therapy. Here, we examined expression of Herpes virus entry mediator (HVEM), an immune checkpoint molecule, in human glioblastoma (GBM) to assess its potential as a molecular target for treatment.
Methods
Molecular and clinical data from publicly available genomic databases containing WHO grade II-IV human glioma cases (n = 1866) were analyzed. Immunohistochemistry was applied to assess HVEM protein levels in primary tumour sections. Statistical analysis was performed using Matlab and R language.
Findings
HVEM was found to be elevated in aggressive gliomas, particularly in the mesenchymal and isocitrate dehydrogenase (IDH) wild-type molecular subtypes of GBM. HVEMhigh tumours tended to be associated with amplification of EGFR and loss of PTEN, while HVEMlow tumours harbored mutations in IDH1 (93%). HVEM exhibited potential as a prognostic marker based on Cox regression and nomogram models. HVEM displayed intra-tumour heterogeneity and was more highly expressed in peri-necrotic and microvascular regions. Gene ontology and pathway analysis revealed enrichment of HVEM in multiple immune regulatory processes, such as suppression of T cell mediated immunity in GBM. Finally, in cell lineage analysis, HVEM was found to be tightly associated with several infiltrating immune and stromal cell types which localized to the tumour microenvironment.
Interpretation
Our data highlights the importance of HVEM in the development of GBM and as a potential molecular target in combination with current immune checkpoint blockades for treatment of GBM.}
}
@incollection{GARRO2025372,
title = {Intelligent Agents and Environment},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {372-378},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00039-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000397},
author = {Alfredo Garro and Alberto Falcone and Matteo Baldoni and Cristina Baroglio and Federico Bergenti and Stefano Mariani and Andrea Omicini and Giuseppe Vizzari},
keywords = {Artificial systems, Autonomy cooperation., Biological systems, Environment, Intelligent agent, Natural systems},
abstract = {This chapter aims to provide a panorama on the fundamentals related to Agents. Specifically, the most popular definitions behind the concept of Intelligent Agents and the main properties that characterize an agent are discussed along with different agent-based models. Furthermore, the concepts of Environment as well as its properties and the role that it plays in the context of the agent paradigm is presented. Actions and Interactions among agents and environment are also discussed before contextualizing the role of Intelligent Agents in the Computational Biology domain.}
}
@article{DING2023,
title = {Constructing a Knowledge Graph for the Chinese Subject Based on Collective Intelligence},
journal = {International Journal on Semantic Web and Information Systems},
volume = {19},
number = {1},
year = {2023},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.327355},
url = {https://www.sciencedirect.com/science/article/pii/S1552628323000108},
author = {Guozhu Ding and Peiying Yi and Xinru Feng},
keywords = {Knowledge Graph, Knowledge Ontology, Ontology Construction, Ontology Evolution, Subject Matter Learning Cell},
abstract = {ABSTRACT
Knowledge graphs are a valuable tool for intelligent tutoring systems and are typically constructed with a focus on objectivity and accuracy. However, they may not effectively capture the subjectivity and complex relationships often present in the humanities. To address this issue, a dynamic visualization of subject matter knowledge graph was developed using a collective intelligence approach that integrates the individual intelligence of learners and considers cognitive diversity to construct and evolve the knowledge graph. The approach resulted in the construction of 722 knowledge associations and the evolution of 584 triples. A survey assessed the effectiveness and user-friendliness, revealing that this approach is effective, easy to use, and can improve subject matter knowledge ontology. In conclusion, combining individual and collective intelligence is a promising approach for building effective knowledge graphs in subject areas with subjectivity and complexity.}
}
@article{BAWACK2020102179,
title = {The role of digital information use on student performance and collaboration in marginal universities},
journal = {International Journal of Information Management},
volume = {54},
pages = {102179},
year = {2020},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102179},
url = {https://www.sciencedirect.com/science/article/pii/S0268401219317700},
author = {Ransome Epie Bawack and Jean Robert {Kala Kamdjoug}},
keywords = {Information seeking, Information need, Information source, Information use, Digital literacy},
abstract = {Using technology to facilitate learning in universities and other higher education institutions (HEIs) has become common practice due to its ability to reduce barriers related to time and space in traditional learning environments. However, current literature mostly focuses on the use of the technology and not on the use of the information it conveys. Also, very few studies focus on technology adoption in universities and HEIs in developing countries, especially those in Africa. Thus, we propose a model that explains the changing information behaviors of students in this digital age and the effect this has on their learning outcomes. We collected questionnaire data from 303 students and analyzed the data using structural equation modelling partial least squares (SEM-PLS). We found that our proposed model explains 60.2 % of student satisfaction, 24.2 % of academic performance, 24.1 % of information sharing, and 19.8 % of their information exchange behavior. This study confirms that the use of digital information and its antecedent factors have significant effects on the college experience of students. This has several implications for information systems research and practice, especially in the design and assessment of technology use in learning environments.}
}
@article{QIU2023105278,
title = {Geological profile-text information association model of mineral exploration reports for fast analysis of geological content},
journal = {Ore Geology Reviews},
volume = {153},
pages = {105278},
year = {2023},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2022.105278},
url = {https://www.sciencedirect.com/science/article/pii/S0169136822005868},
author = {Qinjun Qiu and Bin Wang and Kai Ma and Zhong Xie},
keywords = {Mineral exploration report, Geological profile, Geological text, Knowledge graph, Geological information extraction},
abstract = {Mineral exploration reports include not only a large number of geological profiles but also geological text by offering valuable information and knowledge about the geological environments in which mineral deposits form. Extracting and understanding historical data can assist in the fast analysis of geological content and support 3D model construction. However, geological texts are written in unstructured form and have many correlations with geological profiles. It is a challenging task to derive meaningful geological information without manually reading through a large collection of reports, which is a formidable task for geologists. This paper proposes a geological profile-text association framework for constructing a knowledge graph, and it aims to understand the contents of the geological profile, transform a larger amount of textual data into structure form, and link the geological profile and text to a graph-based knowledge representation that assists further analysis of knowledge discovery. The concept of constructing vector geological profile rock layer objectification is proposed to make each rock layer with geometric features and attribute information for the geological profile, and the geological entity relationship is extracted to form a triple by deep learning and stored and expressed in the form of a graph structure for geological text. Finally, a geological profile and text association model is established by word vector similarity. The proposed approach is capable of rapidly and robustly understanding geological profiles, extracting geological texts, establishing correlations between them, and performing geological knowledge mining.}
}
@article{GROSS202125,
title = {The Business Process Design Space for exploring process redesign alternatives},
journal = {Business Process Management Journal},
volume = {27},
number = {8},
pages = {25-56},
year = {2021},
issn = {1463-7154},
doi = {https://doi.org/10.1108/BPMJ-03-2020-0116},
url = {https://www.sciencedirect.com/science/article/pii/S1463715421000157},
author = {Steven Gross and Katharina Stelzl and Thomas Grisold and Jan Mendling and Maximilian Röglinger and Jan {vom Brocke}},
keywords = {Business process redesign, Process improvement, Process innovation, Design space exploration},
abstract = {Purpose
Process redesign refers to the intentional change of business processes. While process redesign methods provide structure to redesign projects, they provide limited support during the actual creation of to-be processes. More specifically, existing approaches hardly develop an ontological perspective on what can be changed from a process design point of view, and they provide limited procedural guidance on how to derive possible process design alternatives. This paper aims to provide structured guidance during the to-be process creation.
Design/methodology/approach
Using design space exploration as a theoretical lens, the authors develop a conceptual model of the design space for business processes, which facilitates the systematic exploration of design alternatives along different dimensions. The authors utilized an established method for taxonomy development for constructing the conceptual model. First, the authors derived design dimensions for business processes and underlying characteristics through a literature review. Second, the authors conducted semi-structured interviews with professional process experts. Third, the authors evaluated their artifact through three real-world applications.
Findings
The authors identified 19 business process design dimensions that are grouped into different layers and specified by underlying characteristics. Guiding questions and illustrative real-world examples help to deploy these design dimensions in practice. Taken together, the design dimensions form the “Business Process Design Space” (BPD-Space).
Research limitations/implications
Practitioners can use the BPD-Space to explore, question and rethink business processes in various respects.
Originality/value
The BPD-Space complements existing approaches by explicating process design dimensions. It abstracts from specific process flows and representations of processes and supports an unconstrained exploration of various alternative process designs.}
}
@article{ZANOVELLO2025485,
title = {A Knowledge Based System for Intelligent Planning of e-motors Remanufacturing Activities},
journal = {Procedia CIRP},
volume = {134},
pages = {485-490},
year = {2025},
note = {58th CIRP Conference on Manufacturing Systems 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.03.063},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125005311},
author = {Matteo Zanovello and Tullio Tolio and Maria Chiara Magnanini},
keywords = {Knowledge Based System, Short-Term Production Planning, Remanufacturing, Circular Economy},
abstract = {The implementation of circular economy guidelines and new greener policies introduced by governments and agencies have underscored significant challenges in manufacturing systems fexibility. These challenges are particularly severe when considering de-remanufacturing processes, where unpredictable, product-specific variability plays a crucial role. Effiectively managing this input variability requires flexible and reconfg-urable systems that integrate both hardware and software, while meeting production standards and customer expectations. As products reach the end of their operational life and return for remanufacturing, their nominal characteristics are lost, requiring customized disassembly and remanufacturing plans tailored to the condition of each component. To address this need, this work proposes a knowledge-based system for generating activity schedules for de-remanufacturing, within the context of short-term production planning methods. A case study focused on the de-remanufacturing of electric motors is conducted to test and validate the effectiveness of the proposed approach.}
}
@article{RUMPLTUNJIC2025285,
title = {OP0350-PARE Do Patient Research Partners and Researchers share the same perspective on collaborative research Assessing implementation of the EULAR recommendations for PRP involvement in the STRATA-FIT Consortium},
journal = {Annals of the Rheumatic Diseases},
volume = {84},
pages = {285-286},
year = {2025},
note = {EULAR 2025: European Congress of Rheumatology},
issn = {0003-4967},
doi = {https://doi.org/10.1016/j.ard.2025.05.352},
url = {https://www.sciencedirect.com/science/article/pii/S0003496725013858},
author = {S. {Rumpl Tunjić} and D. Rodrigues and B. Maat and S. Valpereiro and N. Trehan and B. Barten and I. Pitsillidou and E. Mateus and K. Chingov and P. Studenic and P.M.J. Welsing and J. M {van Laar}},
keywords = {Patient-led research, Qualitative research, Diversity, Equity, and Inclusion (DEI), Education, Patient organisations},
abstract = {Background:
The STRATA-FIT consortium, a European Union funded research project, focuses on personalising management strategies for Difficult-To-Treat Rheumatoid Arthritis (D2T RA). It aims to develop and validate computational models that can help identify and group patients with D2T RA into relevant categories based on real-world clinical data. The STRATA-FIT Patient Advisory Panel, facilitated by EULAR, plays a pivotal role in overseeing project progress and providing input across all project phases. Patient involvement is central to the project, both for the success of its research objectives and for achieving the primary goal of delivering long-term benefits to patients through shared decision-making. However, several research projects nowadays involve patients as research partners (PRPs) without ever evaluating the quality of the collaboration and adapting engagement strategies, which can lead to tokenism, waste of resources and mismatched expectations.
Objectives:
To evaluate the perspectives of both PRPs and researchers on the collaboration during the initial phase of the project. Additionally, we aim to assess which 2023 EULAR recommendations for the involvement of PRPs in rheumatology research have been successfully implemented, identify any gaps, and determine whether any adjustments or improvements are needed for the future.
Methods:
After 6 months of collaboration, a 17-question survey was developed for a preliminary evaluation of the PRP engagement process and shared with the PRPs participating in STRATA-FIT.A full evaluation is now being conducted after 12 months of collaboration, including one 39-question survey for PRPs and one for researchers with the goal of collecting insights into the needs of both PRPs and researchers to improve the collaboration. Both surveys were adapted from the well-established Public and Patient Engagement Evaluation Tool (PPEET) and aligned with the respective 2023 EULAR recommendations.The questions for PRPs focused on topics such as: the perspective PRPs bring to the project, the frequency of interactions with researchers, the perceived level of collaboration within the project, whether they have opportunity to share their ideas and opinions, among others. The questions for researchers included: whether they worked with PRPs before this project, any challenges faced in collaborating with PRPs, whether they felt adequately knowledgeable to carry out their role and responsibilities toward the PRPs, among others. After this step, a list of key action points to improve the quality of the collaboration will be developed to be implemented by the consortium members.
Results:
The results of the preliminary evaluation showed that the six participating PRPs are overall satisfied with the STRATA-FIT patient engagement, assigning it an average rating of 4.6 out of 5. In this context, the following strengths of the patient involvement process were identified: support and communication provided by the EULAR Research team as PRP facilitator; possibility of PRPs giving input and influencing the project, which can potentially improve the lives of many patients; experience and knowledge learned by participating in such a large project; possibility of exchanging views and working together as a patient panel; diversity of PRPs regarding nationality, educational and professional background and experiences; discussions and skill share sessions with EULAR PRPs from other consortia. The following weaknesses of the process were also identified: lack of project information in lay language (both written and in meetings); amount of technical information often overwhelming; uncertainty about role of PRPs in meetings; uncertainty about tasks for PRPs to contribute to; lack of awareness of progress from other project areas; lack of follow-up information on meetings. Regarding the EULAR recommendations, five recommendations have been found to be fully implemented while the remaining five are considered partially implemented (see Table 1). After evaluating the strengths and weaknesses of the collaboration, the PRPs reached a consensus on the prioritization of improvement opportunities, and identified seven key action points for the future (see Table 2). Table 1. List of the 2023 EULAR recommendations for the involvement of PRPs in rheumatology research and their level of implementation in the STRATA-FIT consortium.  Table 2. List of seven consensually agreed preliminary action points for implementation in STRATA-FIT.  The analysis of responses from the full evaluation is currently ongoing and its results will iterate our action points.
Conclusion:
The 6-month evaluation of PRP involvement in the consortium received positive feedback but still prompted the phrasing of key action points for future activities. They should strengthen the areas of inclusiveness and communication, emphasizing the need for sustained patient engagement and the integration of patient feedback into all phases of the work plan. The PRP facilitator and the leading researchers will be responsible for implementing these action points within the next 12 months of the project. These efforts will ensure that STRATA-FIT continues to meet its objectives in a patient-centered manner. Hopefully, the STRATA-FIT experience can also inspire and advise future research consortia on how to establish effective and meaningful collaboration between PRPs and researchers.
REFERENCES:
NIL.
Acknowledgements:
This project has received funding from the European Union's Horizon Europe research and innovation programme under grant agreement no. 101080243, from the Swiss State Secretariat for Education, Research and Innovation (SERI) and from Hungary's National Research, Development and Innovation (NRDI) Fund. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them.
Disclosure of Interests:
Slađana Rumpl Tunjić: None declared, Diana Rodrigues: None declared, Bertha Maat: None declared, Sofia Valpereiro: None declared, Natasha Trehan AbbVie, UCB, AbbVie, Pfizer, Janssen, Organon, Birgit Barten: None declared, IRENE Pitsillidou: None declared, Elsa Mateus: None declared, Kristina Chingov: None declared, Paul Studenic AbbVie, AbbVie, Paco M.J. Welsing: None declared, Jacob M. van Laar Abbvie, Astra Zeneca, Alfasigma, Boehringer Ingelheim, GSK, Novartis, I have received research grants from Boehringer Ingelheim and Alfasigma. © The Authors 2025. This abstract is an open access article published in Annals of Rheumatic Diseases under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Neither EULAR nor the publisher make any representation as to the accuracy of the content. The authors are solely responsible for the content in their abstract including accuracy of the facts, statements, results, conclusion, citing resources etc.}
}
@article{FATEMIMOGHADDAM202038,
title = {A multi-layered policy generation and management engine for semantic policy mapping in clouds},
journal = {Digital Communications and Networks},
volume = {6},
number = {1},
pages = {38-50},
year = {2020},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352864817301931},
author = {Faraz {Fatemi Moghaddam} and Philipp Wieder and Ramin Yahyapour},
keywords = {Cloud computing, Security, Security management, Policy management, Access control, Policy mapping},
abstract = {The long awaited cloud computing concept is a reality now due to the transformation of computer generations. However, security challenges have become the biggest obstacles for the advancement of this emerging technology. A well-established policy framework is defined in this paper to generate security policies which are compliant to requirements and capabilities. Moreover, a federated policy management schema is introduced based on the policy definition framework and a multi-level policy application to create and manage virtual clusters with identical or common security levels. The proposed model consists in the design of a well-established ontology according to security mechanisms, a procedure which classifies nodes with common policies into virtual clusters, a policy engine to enhance the process of mapping requests to a specific node as well as an associated cluster and matchmaker engine to eliminate inessential mapping processes. The suggested model has been evaluated according to performance and security parameters to prove the efficiency and reliability of this multi-layered engine in cloud computing environments during policy definition, application and mapping procedures.}
}
@article{SPRUIT2018643,
title = {Applied data science in patient-centric healthcare: Adaptive analytic systems for empowering physicians and patients},
journal = {Telematics and Informatics},
volume = {35},
number = {4},
pages = {643-653},
year = {2018},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0736585318303666},
author = {Marco Spruit and Miltiadis Lytras},
keywords = {Applied data science, Knowledge discovery process, Patient-centric healthcare, Adaptive analytic system, Meta-algorithmic modelling, Big data analytics, Natural language processing},
abstract = {We define the emerging research field of applied data science as the knowledge discovery process in which analytic systems are designed and evaluated to improve the daily practices of domain experts. We investigate adaptive analytic systems as a novel research perspective of the three intertwining aspects within the knowledge discovery process in healthcare: domain and data understanding for physician- and patient-centric healthcare, data preprocessing and modelling using natural language processing and (big) data analytic techniques, and model evaluation and knowledge deployment through information infrastructures. We align these knowledge discovery aspects with the design science research steps of problem investigation, treatment design, and treatment validation, respectively. We note that the adaptive component in healthcare system prototypes may translate to data-driven personalisation aspects including personalised medicine. We explore how applied data science for patient-centric healthcare can thus empower physicians and patients to more effectively and efficiently improve healthcare. We propose meta-algorithmic modelling as a solution-oriented design science research framework in alignment with the knowledge discovery process to address the three key dilemmas in the emerging “post-algorithmic era” of data science: depth versus breadth, selection versus configuration, and accuracy versus transparency.}
}
@incollection{2025718,
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {718-800},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.09001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027090011}
}
@article{WEICHHART201836,
title = {Models for Interoperable Human Robot Collaboration},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {36-41},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.231},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318313557},
author = {Georg Weichhart and Magnus Åkerman and Sharath Chandra Akkaladevi and Matthias Plasch and Åsa Fast–Berglund and Andreas Pichler},
keywords = {Organizational Interoperability, Human Robot Collaboration},
abstract = {In this initial research work, different approaches that allow to represent production processes that can be shared by humans and robots are analyzed. Focus is given to approaches that support modular process model structures, allowing to hide details of how tasks are implemented. This is necessary, as a single task might be realized by a human or robot, and both agents need the task description on a different level of granularity.}
}
@incollection{GEETHA2025287,
title = {15 - Emerging graphical data management methodologies for automated driving},
editor = {Rajesh Kumar Dhanaraj and M. Nalini and Malathy Sathyamoorthy and Manar Mohaisen},
booktitle = {Knowledge Graph-Based Methods for Automated Driving},
publisher = {Elsevier},
pages = {287-309},
year = {2025},
isbn = {978-0-443-30040-0},
doi = {https://doi.org/10.1016/B978-0-443-30040-0.00015-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300400000151},
author = {Anbazhagan Geetha and S. Usha and A. Prasanth and Ahmed Elngar},
keywords = {Emerging graphical data, Automated driving systems, Machine learning models, Optimization techniques, Management approaches, Sensor fusion, Automated driving, Deep learning, Knowledge graph},
abstract = {This chapter examines emerging techniques of graphical data management approaches in enhancing the functionalities and ensuring the safety of automated driving systems. It places emphasis on the efficient arrangement, examination, and depiction of sensor data, with particular attention given to their influence on crucial elements such as sensor fusion, mapping, localization, simulation, machine learning model training, human-machine interface design, and system debugging. By conducting a thorough investigation into these areas, the study highlights the essential role of graphical data management in the developmental stages of autonomous cars. The results emphasize the ways in which these methodologies play a role in the development of strong mapping and localization algorithms, the building of realistic simulation environments for testing purposes, and the enhancement of machine learning models through optimization techniques.}
}
@article{LI2023100397,
title = {A Multi-View Filter for Relation-Free Knowledge Graph Completion},
journal = {Big Data Research},
volume = {33},
pages = {100397},
year = {2023},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2023.100397},
url = {https://www.sciencedirect.com/science/article/pii/S2214579623000308},
author = {Juan Li and Wen Zhang and Hongtao Yu},
keywords = {Knowledge graph, Knowledge graph embedding, Relation-free knowledge graph completion, Graph neural networks},
abstract = {As knowledge graphs are often incomplete, knowledge graph completion methods have been widely proposed to infer missing facts by predicting the missing element of a triple given the other two elements. However, the assumption that the two elements have to be correlated is strong. Thus in this paper, we investigate relation-free knowledge graph completion to predict relation-tail(r-t) pairs given a head entity. Considering the large scale of candidate relation-tail pairs, previous work proposed to filter r-t pairs before ranking them relying on entity types, which fails when entity types are missing or insufficient. To tackle the limitation, we propose a relation-free knowledge graph completion method that can cope with knowledge graphs without additional ontological information, such as entity types. Specifically, we propose a multi-view filter, including two intra-view modules and an inter-view module, to filter r-t pairs. For the intra-view modules, we construct head-relation and tail-relation graphs based on triples. Two graph neural networks are respectively trained on these two graphs to capture the correlations between the head entities and the relations, as well as the tail entities and the relations. The inter-view module is learned to bridge the embeddings of entities that appeared in the two graphs. In terms of ranking, existing knowledge graph embedding models are applied to score and rank the filtered candidate r-t pairs. Experimental results show the efficiency of our method in preserving higher-quality candidate r-t pairs for knowledge graphs and resulting in better relation-free knowledge graph completion.}
}
@article{SMITH2022101938,
title = {LILLIE: Information extraction and database integration using linguistics and learning-based algorithms},
journal = {Information Systems},
volume = {105},
pages = {101938},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101938},
url = {https://www.sciencedirect.com/science/article/pii/S030643792100137X},
author = {Ellery Smith and Dimitris Papadopoulos and Martin Braschler and Kurt Stockinger},
keywords = {Information extraction, Data integration, Machine learning for database systems},
abstract = {Querying both structured and unstructured data via a single common query interface such as SQL or natural language has been a long standing research goal. Moreover, as methods for extracting information from unstructured data become ever more powerful, the desire to integrate the output of such extraction processes with “clean”, structured data grows. We are convinced that for successful integration into databases, such extracted information in the form of “triples” needs to be both (1) of high quality and (2) have the necessary generality to link up with varying forms of structured data. It is the combination of both these aspects, which heretofore have been usually treated in isolation, where our approach breaks new ground. The cornerstone of our work is a novel, generic method for extracting open information triples from unstructured text, using a combination of linguistics and learning-based extraction methods, thus uniquely balancing both precision and recall. Our system called LILLIE (LInked Linguistics and Learning-Based Information Extractor) uses dependency tree modification rules to refine triples from a high-recall learning-based engine, and combines them with syntactic triples from a high-precision engine to increase effectiveness. In addition, our system features several augmentations, which modify the generality and the degree of granularity of the output triples. Even though our focus is on addressing both quality and generality simultaneously, our new method substantially outperforms current state-of-the-art systems on the two widely-used CaRB and Re-OIE16 benchmark sets for information extraction. We have made our code publicly available11https://github.com/OIELILLIE/LILLIE. to facilitate further research.}
}
@article{SEN2021115043,
title = {RDFM: An alternative approach for representing, storing, and maintaining meta-knowledge in web of data},
journal = {Expert Systems with Applications},
volume = {179},
pages = {115043},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115043},
url = {https://www.sciencedirect.com/science/article/pii/S095741742100484X},
author = {Sangeeta Sen and Devashish Katoriya and Animesh Dutta and Biswanath Dutta},
keywords = {RDF, Multi-dimensional meta-knowledge, Provenance, Semantic web, Web of data},
abstract = {The Web of Data needs additional information, i.e., meta-knowledge to ensure quality and build the trust in data. For representing meta-knowledge, there exist various approaches in the literature, for example, RDFt, tRDF, RDF Reification, Singleton Property and Named Graph. There are various issues associated with these approaches in representing the meta-knowledge, for example, the increasing graph size, additional statement generation, the representation of multi-dimensional and/or nested meta-knowledge, among others. In this work, we propose an approach called RDFM to represent, store, and manage meta-knowledge. RDFM integrates attributes to the predicate to represent nested and/or multi-dimensional meta-knowledge with lesser statements generation. A query language called SPARQLM is developed to extract RDFM data. The study analyzes the performance of RDFM in terms of the number of edges, number of statements generation, data redundancy, storage, query response time, required query length, representation of meta-knowledge in nested and different dimensions. The results show that the RDFM model performs significantly and gives advantage over storage management and graph data management.}
}
@article{ZHANG2021106605,
title = {Service skill improvement for home robots: Autonomous generation of action sequence based on reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {212},
pages = {106605},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106605},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120307346},
author = {Mengyang Zhang and Guohui Tian and Ying Zhang and Peng Duan},
keywords = {Home robots, Action sequence, Reinforcement learning, Natural language, A priori knowledge},
abstract = {It still remains a challenge for robots to obtain knowledge automatically for performing home services. In the human learning process, natural languages act as an outline in guiding human beings complete tasks. From this point, a conditional generation method transforming textual manipulation instructions into action sequences is proposed, to provide home robots with knowledge automatically and improve the service skills finally. Due to the limited learning ability of the generation model on understanding complex semantic information, we present a two-phase conditional generation strategy in which the action space is reduced at the syntax level before generating action sequences semantically. For representing action sequences effectively, functional labels (FLs) are designed according to the requirements of performing home services, to identify six relationships about objects and actions. In action sequence generation, reinforcement learning is employed to guide the action sequence generation by introducing hierarchical rewards related to a priori knowledge, semantic similarity, and action logic. Based on statistic learning, a priori knowledge is constructed by modeling the relationship about object co-occurrence, action collaboration, and action–object correlation. The semantic similarity with Semantic Role Labeling enables the similarity evaluation between textual sentences (inputs) and produced sequences (outputs). And action logic, represented by the verb sequence in instructions, guides the production of action sequences logically. Experimental results demonstrate that the proposed method can produce competitive action sequences from textual instructions, and produced action sequences can be applied to robot for performing services.}
}
@article{BOLTUC202057,
title = {BICA for AGI},
journal = {Cognitive Systems Research},
volume = {62},
pages = {57-67},
year = {2020},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S1389041719304863},
author = {Piotr Bołtuć and Marta Boltuc},
keywords = {Artificial general intelligence, AGI, BICA, Stochastic AI, Unconscious consciousness, Libet, The edge of chaos, Creativity engines, Thaler, Goertzel},
abstract = {BICAs for AI have been happening for decades, realized within multiple cognitive architectures. What is a BICA for AGI? It requires AI to go beyond the limitations of predicative human language, and of predicative logic based on it; also, above human reportable consciousness, to the subconscious/non-conscious level of human (and machine) mind; and then still beyond it. Within machine cognition this is the sub-symbolic level. It has to gauge gestalts, or patterns, directly from the processes, which goes beyond human-level observational capacities or human-understandable language, even beyond the language of human-readable mathematics (Boltuc, 2018). It requires versatile life-long learning (Siegelmann, 2018). Through complex stochastic processes, it needs to confabulate by creative permutations of multifarious gestalts and to select those with useful applications (Thaler, 1997). This is computing at the edge of chaos (Goertzel, 2006).}
}
@incollection{CARRIM20231,
title = {Diversity},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {1-7},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.08001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305080015},
author = {Nazir Carrim},
keywords = {Diversity, Apartheid, Race, Ethnicity, Culture, Identity, Inclusivity and difference},
abstract = {This chapter focuses on diversity. It traces diversity to its historical links with issues related to race relations. Using the apartheid experience in South Africa, it shows how diversity was used to construct racism and segregation. It also argues that diversity is more than being about race, ethnicity, and culture, but also includes gender, class, sexual orientation, dis/ability and many other aspects. In noting the broader way in which diversity is viewed, it shows that inclusivity provided a way to delink diversity as referring to race relations. It is also argued that inclusivity implies difference. For a more socially just order in the future, it is shown that viewing difference ontologically and epistemologically may be useful. Diversity when viewed as difference, it is argued, allows for the complexities and pluralities of people's lives to be recognized so that who they are is not essentialized and reduced to a fixed essence that is counter factual. Recognizing the complexities of human beings requires working with difference that allows such complexity to be viewed and appreciated, as central to being human and as always in a state of becoming and development.}
}
@article{ZAMBRANO2022100176,
title = {Industrial digitalization in the industry 4.0 era: Classification, reuse and authoring of digital models on Digital Twin platforms},
journal = {Array},
volume = {14},
pages = {100176},
year = {2022},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2022.100176},
url = {https://www.sciencedirect.com/science/article/pii/S2590005622000352},
author = {Valentina Zambrano and Johannes Mueller-Roemer and Michael Sandberg and Prasad Talasila and Davide Zanin and Peter Gorm Larsen and Elke Loeschner and Wolfgang Thronicke and Dario Pietraroia and Giuseppe Landolfi and Alessandro Fontana and Manuel Laspalas and Jibinraj Antony and Valerie Poser and Tamas Kiss and Simon Bergweiler and Sebastian {Pena Serna} and Salvador Izquierdo and Ismael Viejo and Asier Juan and Francisco Serrano and André Stork},
keywords = {Digital Twin, Cyber–Physical System, Industry 4.0, Reusable models},
abstract = {Digital Twins (DTs) are real-time digital models that allow for self-diagnosis, self-optimization and self-configuration without the need for human input or intervention. While DTs are a central aspect of the ongoing fourth industrial revolution (I4.0), this leap forward may be reserved for the established, large-cap companies since the adoption of digital technologies among Small and Medium-size Enterprises (SMEs) is still modest. The aim of the H2020 European Project ”DIGITbrain” is to support a modular construction of DTs by reusing their fundamental building blocks, i.e., the Models that describe the behavior of the DT, their associated Algorithms and the Data required for the evaluation. By offering these building blocks as a service via a DT Platform (a Digital Twin Environment), the technical barriers among SMEs to adopt these technologies are lowered. This paper describes how digital models can be classified, reused and authored on such DT Platforms. Through experimental analyses of three industrial cases, the paper exemplifies how DTs are employed in relation to product assembly of agricultural robots, polymer injection molding, as well as laser-cutting and sheet-metal forming of aluminum.}
}
@article{SCHORLEMMER2021118,
title = {A uniform model of computational conceptual blending},
journal = {Cognitive Systems Research},
volume = {65},
pages = {118-137},
year = {2021},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1389041720300759},
author = {Marco Schorlemmer and Enric Plaza},
keywords = {Conceptual blending, Computational creativity, Amalgams, Category theory, Case-based reasoning},
abstract = {We present a mathematical model for the cognitive operation of conceptual blending that aims at being uniform across different representation formalisms, while capturing the relevant structure of this operation. The model takes its inspiration from amalgams as applied in case-based reasoning, but lifts them into category theory so as to follow Joseph Goguen’s intuition for a mathematically precise characterisation of conceptual blending at a representation-independent level of abstraction. We prove that our amalgam-based category-theoretical model of conceptual blending is essentially equivalent to the pushout model in the ordered category of partial maps as put forward by Goguen. But unlike Goguen’s approach, our model is more suitable to capture computational realisations of conceptual blending, and we exemplify this by concretising our model to computational conceptual blends for various representation formalisms and application domains.}
}
@article{GORECKI2018655,
title = {Integrating HLA-Based Distributed Simulation for Management Science and BPMN},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {655-660},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.393},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318315180},
author = {Simon Gorecki and Youssef Bouanan and Gregory Zacharewicz and Judicael Ribault and Nicolas Perry},
keywords = {Modeling, Simulation, Distributed Simulation, High Level Architecture, Business Process Model, Notation, Interoperability},
abstract = {Modeling and Simulation are becoming more and more complex, making their design very challenging. Modeling and Simulation of complex systems requires simultaneous consideration of several points of view and the study of these systems needs skills belonging to different scientific fields. In Distributed Simulation domain, IEEE 1516-2010 - High Level Architecture (HLA) is one of the most used standard. However, it does not provide any official graphical language for defining distributed simulation behaviors. Business Process Model and Notation standard could be an interesting solution for defining HLA execution scenario. This paper present application experiment applied to solar power plant. Our proposition consists in restrict the HLA execution process in order to use one federate as Master, controlling the others as Slaves. This allows us to generate a component responsible for the simulation execution process with parsing a Business Process Model and Notation diagram. In this paper, we present an application of this concept.}
}
@article{KUTT2023119968,
title = {Loki – the semantic wiki for collaborative knowledge engineering},
journal = {Expert Systems with Applications},
volume = {224},
pages = {119968},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119968},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423004700},
author = {Krzysztof Kutt and Grzegorz J. Nalepa},
keywords = {Knowledge engineering, Semantic wiki, Software engineering, Unit tests, Prolog},
abstract = {We present Loki, a semantic wiki designed to support the collaborative knowledge engineering process with the use of software engineering methods. Designed as a set of DokuWiki plug-ins, it provides a variety of knowledge representation methods, including semantic annotations, Prolog clauses, and business processes and rules oriented to specific tasks. Knowledge stored in Loki can be retrieved via SPARQL queries, in-line Semantic MediaWiki-like queries, or Prolog goals. Loki includes a number of useful features for a group of experts and knowledge engineers developing the wiki, such as knowledge visualization, ontology storage, or code hint and completion mechanism. Reasoning unit tests are also introduced to validate knowledge quality. The paper is complemented by the formulation of the collaborative knowledge engineering process and the description of experiments performed during Loki development to evaluate its functionality. Loki is available as free software at https://loki.re.}
}
@article{XU2022116964,
title = {A novel framework of knowledge transfer system for construction projects based on knowledge graph and transfer learning},
journal = {Expert Systems with Applications},
volume = {199},
pages = {116964},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116964},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422003906},
author = {Jin Xu and Mengqi He and Ying Jiang},
keywords = {Construction project, Knowledge transfer, Project similarity, Knowledge graph, Transfer learning},
abstract = {For construction enterprises, efficient knowledge sharing among projects not only effectively improves enterprise technology, level of management and competitiveness, but also promotes their sustainable development. Given the many benefits of knowledge management, enterprises have an urgent need for project knowledge sharing methods and tools. In this study, we build an automated and intelligent framework for a construction project knowledge transfer system based on knowledge graph and transfer learning. This framework aims to solve the problem of ineffective knowledge transfer that is encountered in the management of construction project knowledge sharing. First, to discover the relationship among knowledge and further obtain the relationships among projects, we design a domain knowledge graph ontology for construction projects and build an example. Then, based on this domain knowledge graph and combining construction data distance distribution with construction project knowledge background, we design a new construction project similarity measurement algorithm (PBG-MMD), which can guide the selection of the knowledge transfer source domain. Finally, a new transfer learning method is developed to automatically select the transfer source domain according to the domain context. The framework provides an effective answer for the problem of “what to transfer” in transfer learning and provides an effective solution to address the problem of “how to transfer” during knowledge transfer. Through the verification of practical case data, the proposed framework successfully realizes knowledge transfer among construction projects and provides an automated and intelligent knowledge sharing approach for construction enterprises.}
}
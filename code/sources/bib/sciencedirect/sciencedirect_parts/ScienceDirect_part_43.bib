@article{EMUNDS2025103414,
title = {Spatial Link Prediction: Learning topological relationships in MEP systems},
journal = {Advanced Engineering Informatics},
volume = {66},
pages = {103414},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103414},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625003076},
author = {Christoph Emunds and Jérôme Frisch and Christoph {van Treeck}},
keywords = {BIM, MEP, Deep learning, Topology, Relationship modeling, Link prediction},
abstract = {In Building Information Modeling (BIM), interactions between building components are defined by parametric relationships. These relationships are critical for analyses and simulations throughout a building’s life-cycle. However, many of these relationships are implicit in BIM models, posing challenges when exchanging models between different BIM authoring tools. Existing query systems and languages for BIM can partially infer or deduce missing information, incorporating semantic, geometric, and topological data to make it more accessible. Nonetheless, they often lack a holistic understanding of the building model, focusing on individual vertices, edges, and faces of components’ 3D representations. In this paper, we present a novel approach for predicting connectivity relationships in mechanical, electrical and plumbing (MEP) systems using a candidate generation procedure followed by an auto-encoder model to identify relevant component connections. Our approach leverages both the topological structure of MEP systems and the geometric attributes of system components, allowing the model to learn effectively and accurately predict potential connections. We conduct extensive experiments across various settings to evaluate the practical applicability of our approach and demonstrate its effectiveness on graphs of real-world MEP systems. In addition, we design a decoder that explicitly considers the direction and distance between components in order to enhance spatial realism in the predicted connections. To train our models for the task of link prediction in MEP systems, we assembled a dataset of 27 BIM models from residential, industrial, and public buildings. Our results indicate that our approach can significantly enhance the prediction of connectivity relationships, offering valuable insights for improving BIM workflows and interoperability. The source code of our models and experiments is available at https://github.com/RWTH-E3D/SpatialLinkPrediction.}
}
@article{KJAERGAARD2020106848,
title = {Current practices and infrastructure for open data based research on occupant-centric design and operation of buildings},
journal = {Building and Environment},
volume = {177},
pages = {106848},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2020.106848},
url = {https://www.sciencedirect.com/science/article/pii/S0360132320302079},
author = {Mikkel B. Kjærgaard and Omid Ardakanian and Salvatore Carlucci and Bing Dong and Steven K. Firth and Nan Gao and Gesche Margarethe Huebner and Ardeshir Mahdavi and Mohammad Saiedur Rahaman and Flora D. Salim and Fisayo Caleb Sangogboye and Jens Hjort Schwee and Dawid Wolosiuk and Yimin Zhu},
keywords = {Open data, Data publishing, Data use, Occupant behavior, FAIR Data, Ontology, Anonymi, z, ation, Metadata schema},
abstract = {Many new tools for improving the design and operation of buildings try to realize the potential of big data. In particular, data is an important element for occupant-centric design and operation as occupants’ presence and actions are affected by a high degree of uncertainty and, hence, are hard to model in general. For such research, data handling is an important challenge, and following an open science paradigm based on open data can increase efficiency and transparency of scientific work. This article reviews current practices and infrastructure for open data-driven research on occupant-centric design and operation of buildings. In particular, it covers related work on open data in general and for the built environment in particular, presents survey results for existing scientific practices, reviews technical solutions for handling data and metadata, discusses ethics and privacy protection and analyses principles for the sharing of open data. In summary, this study establishes the status quo and presents an outlook on future work for methods and infrastructures to support the open data community within the built environment.}
}
@article{ALI2024100954,
title = {Cognitive systems and interoperability in the enterprise: A systematic literature review},
journal = {Annual Reviews in Control},
volume = {57},
pages = {100954},
year = {2024},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2024.100954},
url = {https://www.sciencedirect.com/science/article/pii/S1367578824000233},
author = {Jana Al Haj Ali and Ben Gaffinet and Hervé Panetto and Yannick Naudet},
keywords = {Cognition, Cognitive systems, Cognitive cyber–physical systems, Cognitive Digital Twin, Cognitive interoperability},
abstract = {The transition from automated processes to mechanisms that manifest intelligence through cognitive abilities such as memorisation, adaptability and decision-making in uncertain contexts, has marked a turning point in the field of industrial systems, particularly in the development of cyber–physical systems and digital twins. This evolution, supported by advances in cognitive science and artificial intelligence, has opened the way to a new era in which systems are able to adapt and evolve autonomously, while offering more intuitive interaction with human users. This article proposes a systematic literature review to gather and analyse current research on Cognitive Cyber–Physical Systems (CCPS), Cognitive Digital Twins (CDT), and cognitive interoperability, which are pivotal in a contemporary Cyber–Physical Enterprise (CPE). From this review, we first seek to understand how cognitive capabilities that are traditionally considered as human traits have been defined and modelled in cyber–physical systems and digital twins in the context of Industry 4.0/5.0, and what cognitive functions they implement. We explore their theoretical foundations, in particular in relation to cognitive psychology and humanities definitions and theories. Then we analyse how interoperability between cognitive systems has been considered, leading to cognitive interoperability, and we highlight the role of knowledge representation and reasoning.}
}
@article{JARVENPAA2019261,
title = {Implementation of capability matchmaking software facilitating faster production system design and reconfiguration planning},
journal = {Journal of Manufacturing Systems},
volume = {53},
pages = {261-270},
year = {2019},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2019.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0278612519300883},
author = {Eeva Järvenpää and Niko Siltala and Otto Hylli and Minna Lanz},
keywords = {Production system design, Production system reconfiguration, Resource representation, Capability modelling, Capability matchmaking, Matchmaking software, Matchmaking web service},
abstract = {Smart manufacturing calls for rapidly responding production systems which help the manufacturing companies to operate efficiently in a highly dynamic environment. Currently, the system design and reconfiguration planning are manual processes which rely heavily on the designers’ expertise and tacit knowledge to find feasible system configuration solutions by comparing the characteristics of the product to the technical properties of the available resources. Rapid responsiveness requires new computer-aided intelligent design and planning solutions that would reduce the time and effort put into system design, both in brownfield and greenfield scenarios. This article describes the implementation of a capability matchmaking approach and software which automatizes the matchmaking between product requirements and resource capabilities. The interaction of the matchmaking system with external design and planning tools, through its web service interface, is explained and illustrated with a case example. The proposed matchmaking approach supports production system design and reconfiguration planning by providing automatic means for checking if the existing system already fulfils the new product requirements, and/or for finding alternative resources and resource combinations for specific product requirements from large search spaces, e.g. from global resource catalogues.}
}
@article{ROEHNER20191519,
title = {Specifying Combinatorial Designs with the Synthetic Biology Open Language (SBOL)},
journal = {ACS Synthetic Biology},
volume = {8},
number = {7},
pages = {1519-1523},
year = {2019},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.9b00092},
url = {https://www.sciencedirect.com/science/article/pii/S2161506319001566},
author = {Nicholas Roehner and Bryan Bartley and Jacob Beal and James McLaughlin and Matthew Pocock and Michael Zhang and Zach Zundel and Chris J. Myers},
keywords = {combinatorial design, combinatorial libraries, biodesign automation, standards, SBOL},
abstract = {As improvements in DNA synthesis technology and assembly methods make combinatorial assembly of genetic constructs increasingly accessible, methods for representing genetic constructs likewise need to improve to handle the exponential growth of combinatorial design space. To this end, we present a community accepted extension of the SBOL data standard that allows for the efficient and flexible encoding of combinatorial designs. This extension includes data structures for representing genetic designs with “variable” components that can be implemented by choosing one of many linked designs for existing genetic parts or constructs. We demonstrate the representational power of the SBOL combinatorial design extension through case studies on metabolic pathway design and genetic circuit design, and we report the expansion of the SBOLDesigner software tool to support users in creating and modifying combinatorial designs in SBOL.
}
}
@article{GOU2023101506,
title = {FAM72 family proteins as poor prognostic markers in clear cell renal carcinoma},
journal = {Biochemistry and Biophysics Reports},
volume = {35},
pages = {101506},
year = {2023},
issn = {2405-5808},
doi = {https://doi.org/10.1016/j.bbrep.2023.101506},
url = {https://www.sciencedirect.com/science/article/pii/S2405580823000870},
author = {Hui Gou and Ping Chen and Wenbing Wu},
keywords = {FAM72, Clear cell renal carcinoma, TCGA, Mmune infiltration, Prognosis},
abstract = {Purpose
This study aimed to investigate the prognostic significance of the Family with Sequence Similarity 72 member (FAM72) gene family in clear cell renal carcinoma (ccRCC) using a bioinformatic approach.
Patients and methods
To investigate the association between FAM72 and ccRCC, we utilized various databases and analysis tools, including TCGA, GEPIA, Metscape, cBioPortal, and MethSurv. We conducted an analysis of FAM72 expression levels in ccRCC tissues compared to normal kidney tissues and performed univariate and multivariate Cox analysis to determine the relationship between FAM72 expression and patient prognosis. Furthermore, we carried out Gene Ontology (GO) and Gene Set Enrichment Analysis (GSEA) to identify enriched biological processes associated with FAM72 expression. Additionally, we analyzed immune cell infiltration and the level of methylation in ccRCC patients. Our bioinformatic analysis revealed that FAM72 expression levels were significantly higher in ccRCC tissues than in normal kidney tissues. High expression of FAM72 was associated with poor prognosis in ccRCC patients and was found to be an independent prognostic factor for ccRCC. GO and GSEA analyses indicated that FAM72 was enriched in biological processes related to mitosis, cell cycle, and DNA metabolism. Moreover, we found a significant correlation between FAM72 and immune cell infiltration and the level of methylation in ccRCC patients.
Conclusion
Our findings suggest that FAM72 could serve as an unfavorable prognostic molecular marker for ccRCC. A comprehensive understanding of FAM72 could provide crucial insights into tumor progression and prognosis.}
}
@article{TILL2024627,
title = {Enabling digitally integrated product design and production through digital continuity and feedback to design},
journal = {Procedia CIRP},
volume = {128},
pages = {627-632},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.03.041},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124007558},
author = {Tschiltschke Till and Manoury Marvin Michael and Riedelsheimer Theresa and Lindow Kai},
keywords = {product, production system architecture, production description language, Design methodology, tools, technologies},
abstract = {Due to global and political influence, resilient and versatile production systems are unavoidable. Based on existing Product Lifecycle Management (PLM) systems there is a growing need for a consistent communication between machines and product design to enable integrated product and production process design. Core aspects are integrating the product design information directly into the production equipment and their feedback into the product design as well as into other phases of the product creation and usage. Therefore, a generalized production description language as well as an architecture for the digital integrated product design and production need to be developed. This paper presents the first results of a use case in the automotive industry. The approach of developing the functional architecture is shown. Several feedback to X approaches were researched in literature and analyzed regarding functional requirements for integration in the developed architecture. The results are the main basis for further implementation within the automotive production. Moreover, the main challenges and next steps are discussed.}
}
@article{ZHANG2025104539,
title = {Decreased left brain specialization in bipolar disorder patients and its association with neurotransmitter and genetic profiles: A longitudinal study},
journal = {Asian Journal of Psychiatry},
volume = {109},
pages = {104539},
year = {2025},
issn = {1876-2018},
doi = {https://doi.org/10.1016/j.ajp.2025.104539},
url = {https://www.sciencedirect.com/science/article/pii/S1876201825001820},
author = {Leyi Zhang and Yiding Han and Haohao Yan and Chunguo Zhang and Xiaoling Li and Jiaquan Liang and Chaohua Tang and Weibin Wu and Wen Deng and Guojun Xie and Wenbin Guo},
keywords = {Bipolar disorder, Autonomy index, Lateralization, Language network, Allen Human Brain Atlas},
abstract = {Brain specialization plays a crucial role in human behavior and cognition. Previous studies have suggested abnormal specialization in psychiatric disorders; however, the specialization patterns of bipolar disorder (BD) and the effects of medication on these changes remain unclear. According to Crow’s hypothesis regarding the key role of language in the origin of psychoses, BD patients (BDPs) may exhibit abnormal language-related specialization. Here, we aimed to explore brain specialization alterations of BDPs before and after pharmacological treatment. The autonomy index, based on resting-state images, was used to assess brain specialization in 82 BDPs and 88 healthy controls (HCs). Among patients, 43 BDPs who underwent 3 months of pharmacological treatment completed the follow-up. Using autonomy index as input, support vector regression (SVR) analysis was conducted to predict treatment response. Additionally, we conducted cross-sample correlation analyses between autonomy index and genetic profiles or the densities of neurotransmitter receptors/transporters. At baseline, BDPs exhibited reduced autonomy index in the left middle temporal gyrus (MTG) relative to HCs. However, no significant alterations were observed following pharmacological treatment. Using autonomy index, the SVR model could predict treatment response for BDPs with a correlation coefficient of 0.705. Brain specialization patterns were correlated with six genes and neurotransmitters including dopaminergic (D1R, D2R, and DAT) and serotonergic (5-HT2A) transmission. In line with Crow’s hypothesis, we found reduced brain specialization in a key node of the language network (LN) in BDPs. We also provided potential genetic and biological mechanisms underlying BD.}
}
@article{GAO2025,
title = {Capturing Biological Interactions Improves Predictive Ability of Complex Traits via Epistatic Models},
journal = {Journal of Integrative Agriculture},
year = {2025},
issn = {2095-3119},
doi = {https://doi.org/10.1016/j.jia.2025.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S2095311925002400},
author = {Ning Gao and Jinyan Teng and Shaopan Ye and Qing Lin and Yahui Gao and Jiaying Wang and Shuwen Huang and Jun He and Jiaqi Li and Yaosheng Chen and Lingzhao Fang and Qin Zhang and Zhe Zhang},
keywords = {biBLUP, biological interaction, epistatic effect, complex trait, KEGG pathway},
abstract = {Although genome-wide interaction effects are critical for unraveling the underlying genetic architectures of complex traits, the rich landscape of biological interactions is often disregarded in statistical models for genomic dissecting and predicting complex traits/diseases. To bridge this gap, we introduce biBLUP (biological interaction Best Linear Unbiased Prediction), a novel epistatic model that integrates prior biological knowledge by focusing on interactions among genes within KEGG (Kyoto Encyclopedia of Genes and Genomes) pathways. Simulation experiments demonstrate that biBLUP effectively captures interaction effects across diverse genetic architectures, achieving up to a 62% increase in predictive accuracy compared to models ignoring such information. We validated the performance of biBLUP using real data across species. In a specific application using data from 6,642 yeast lines, biBLUP yielded a 40.36% improvement in prediction accuracy for growth rate by modeling genetic interaction effects within the KEGG pathway associated with allantoin utilization. Furthermore, incorporating KEGG into biBLUP successfully captures validated epistatic effects associated with rice flowering time. This integration results in an improvement of 16.29% in prediction accuracy for flowering time of rice. Our findings demonstrate that integrating KEGG pathway information into genomic prediction models enables the capture of biologically relevant interaction effects, thereby enhancing both predictive ability and our understanding of the genetic basis of complex traits.}
}
@article{CHEN2025,
title = {Standardizing Survey Data Collection to Enhance Reproducibility: Development and Comparative Evaluation of the ReproSchema Ecosystem},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/63343},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125009379},
author = {Yibei Chen and Dorota Jarecka and Sanu Ann Abraham and Remi Gau and Evan Ng and Daniel M Low and Isaac Bevers and Alistair Johnson and Anisha Keshavan and Arno Klein and Jon Clucas and Zaliqa Rosli and Steven M Hodge and Janosch Linkersdörfer and Hauke Bartsch and Samir Das and Damien Fair and David Kennedy and Satrajit S Ghosh},
keywords = {reproducibility, survey, data collection, schema, findability, accessibility, interoperability, and reusability, FAIR},
abstract = {Background
Inconsistencies in survey-based (eg, questionnaire) data collection across biomedical, clinical, behavioral, and social sciences pose challenges to research reproducibility. ReproSchema is an ecosystem that standardizes survey design and facilitates reproducible data collection through a schema-centric framework, a library of reusable assessments, and computational tools for validation and conversion. Unlike conventional survey platforms that primarily offer graphical user interface–based survey creation, ReproSchema provides a structured, modular approach for defining and managing survey components, enabling interoperability and adaptability across diverse research settings.
Objective
This study examines ReproSchema’s role in enhancing research reproducibility and reliability. We introduce its conceptual and practical foundations, compare it against 12 platforms to assess its effectiveness in addressing inconsistencies in data collection, and demonstrate its application through 3 use cases: standardizing required mental health survey common data elements, tracking changes in longitudinal data collection, and creating interactive checklists for neuroimaging research.
Methods
We describe ReproSchema’s core components, including its schema-based design; reusable assessment library with >90 assessments; and tools to validate data, convert survey formats (eg, REDCap [Research Electronic Data Capture] and Fast Healthcare Interoperability Resources), and build protocols. We compared 12 platforms—Center for Expanded Data Annotation and Retrieval, formr, KoboToolbox, Longitudinal Online Research and Imaging System, MindLogger, OpenClinica, Pavlovia, PsyToolkit, Qualtrics, REDCap, SurveyCTO, and SurveyMonkey—against 14 findability, accessibility, interoperability, and reusability (FAIR) principles and assessed their support of 8 survey functionalities (eg, multilingual support and automated scoring). Finally, we applied ReproSchema to 3 use cases—NIMH-Minimal, the Adolescent Brain Cognitive Development and HEALthy Brain and Child Development Studies, and the Committee on Best Practices in Data Analysis and Sharing Checklist—to illustrate ReproSchema’s versatility.
Results
ReproSchema provides a structured framework for standardizing survey-based data collection while ensuring compatibility with existing survey tools. Our comparison results showed that ReproSchema met 14 of 14 FAIR criteria and supported 6 of 8 key survey functionalities: provision of standardized assessments, multilingual support, multimedia integration, data validation, advanced branching logic, and automated scoring. Three use cases illustrating ReproSchema’s flexibility include standardizing essential mental health assessments (NIMH-Minimal), systematically tracking changes in longitudinal studies (Adolescent Brain Cognitive Development and HEALthy Brain and Child Development), and converting a 71-page neuroimaging best practices guide into an interactive checklist (Committee on Best Practices in Data Analysis and Sharing).
Conclusions
ReproSchema enhances reproducibility by structuring survey-based data collection through a structured, schema-driven approach. It integrates version control, manages metadata, and ensures interoperability, maintaining consistency across studies and compatibility with common survey tools. Planned developments, including ontology mappings and semantic search, will broaden its use, supporting transparent, scalable, and reproducible research across disciplines.}
}
@article{LIU2024102300,
title = {Emotion detection for misinformation: A review},
journal = {Information Fusion},
volume = {107},
pages = {102300},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102300},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524000782},
author = {Zhiwei Liu and Tianlin Zhang and Kailai Yang and Paul Thompson and Zeping Yu and Sophia Ananiadou},
keywords = {Sentiment analysis, Emotion detection, Misinformation, Rumor, Fake news, Stance detection},
abstract = {With the advent of social media, an increasing number of netizens are sharing and reading posts and news online. However, the huge volumes of misinformation (e.g., fake news and rumors) that flood the internet can adversely affect people’s lives, and have resulted in the emergence of rumor and fake news detection as a hot research topic. The emotions and sentiments of netizens, as expressed in social media posts and news, constitute important factors that can help to distinguish fake news from genuine news and to understand the spread of rumors. This article comprehensively reviews emotion-based methods for misinformation detection, with a particular focus on advanced fusion methods. We begin by explaining the strong links between emotions and misinformation. We subsequently provide a detailed analysis of a range of misinformation detection methods that employ a variety of emotion, sentiment and stance-based features, and describe their strengths and weaknesses. Finally, we discuss a number of ongoing challenges in emotion-based misinformation detection based on large language models, and suggest future research directions, including data collection (multi-platform, multilingual), annotation, benchmark, multimodality, and interpretability.}
}
@article{RICO2019100500,
title = {Evaluating the impact of semantic technologies on bibliographic systems: A user-centred and comparative approach},
journal = {Journal of Web Semantics},
volume = {59},
pages = {100500},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300174},
author = {Mariano Rico and Daniel Vila-Suero and Iuliana Botezan and Asunción Gómez-Pérez},
keywords = {User-centred evaluation, Usability, Cultural heritage, Digital humanities, Linked data, Ontologies},
abstract = {Semantic and linked-data technologies are currently used by several cultural heritage institutions to make their content available through the Web. Although these technologies are heavily oriented towards data reuse and integration, one clear benefit highlighted by recent literature is the enhancement of human cultural consumption and user experience through the development of novel cultural end-user applications like Online Public Access Catalogues (OPACs). However, to the best of our knowledge, studies into the impact of these technologies on end-user applications are scarce. In order to address this lack, we report the results of two within-group user-centred studies of two online bibliographic systems in a realistic setting — using a widely deployed OPAC and its counterpart linked-data based system, datos.bne.es. The results of our first within-group study show that users of the system based on linked data required significantly less time and visited fewer pages to complete a typical search and retrieval activity. Additionally, the results of our user satisfaction tests also provided significantly better results for this new system. These results are consistent with the hypothesis that semantic technologies applied to library catalogues provide an enhancement that helps satisfy users’ information needs.}
}
@article{STACHOWIAK202052,
title = {On the civilising of objectification. Language use, discursive patterns and the psychological expertise of work planning},
journal = {Language & Communication},
volume = {74},
pages = {52-60},
year = {2020},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2020.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S027153092030046X},
author = {Jerzy Stachowiak},
keywords = {Psychology, Objectification, Expertise, Discourse, Qualitative research, Critique},
abstract = {The aim of the paper is to describe how psychological expertise of work planning contributes to the process of the civilising of objectification. Firstly, it is argued that during this process, some forms of objectification have gained the public status of being reprehensible. Unexpectedly, other types of objectification though, connected with looking at work from the standpoint of psychology, enjoy acceptability. Secondly, qualitative discourse analysis of empirical examples is provided. Analysis is based on public statements in which psychology experts advise others on how to overcome ʻinternal limitationsʼ, ʻthought patternsʼ and ʻstereotypesʼ of subordinates. The study shows that language and discourse of psychology have become carriers of the civilising of objectification with specific societal and historical implications of this process.}
}
@article{DREISBACH20243246,
title = {Dietary Sugar and Saturated Fat Consumption Associated with the Gastrointestinal Microbiome during Pregnancy},
journal = {The Journal of Nutrition},
volume = {154},
number = {11},
pages = {3246-3254},
year = {2024},
issn = {0022-3166},
doi = {https://doi.org/10.1016/j.tjnut.2024.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0022316624010332},
author = {Caitlin Dreisbach and Tonja Nansel and Shyamal Peddada and Wanda Nicholson and Anna Maria Siega-Riz},
keywords = {Microbiota, diet, Healthy Eating Index, pregnancy, saturated fat, sugar, dairy},
abstract = {Background
Growing evidence supports changes in the gastrointestinal microbiome over the course of pregnancy may have an impact on the short- and long-term health of both the mother and the child.
Objective
Our objective was to explore the association of diet quality, as measured by the Healthy Eating Index (HEI), with the composition and gene ontology (GO) representation of microbial function in the maternal gastrointestinal microbiome during pregnancy.
Methods
We conducted a retrospective, observational analysis of n = 185 pregnant participants in the Pregnancy Eating Attributes Study. Maternal dietary intake was assessed in the first trimester using the automated self-administered 24-h recall method, from which the HEI 2015 was calculated. Rectal swabs were obtained in the second trimester and sequenced using the NovaSeq 6000 system shotgun platform. We used unsupervised clustering to identify microbial enterotypes representative of maternal taxa and GO functional term composition. Multivariable linear models were used to identify associations between taxa, functional terms, and food components while controlling for relevant covariates. Multinomial regression was then used to predict enterotype membership based on a participant’s HEI food component score.
Results
Those in the high diet quality tertile had a lower early pregnancy BMI [mean (M) = 23.48 kg/m2, SD = 3.38] compared with the middle (M = 27.35, SD = 6.01) and low (M = 27.49, SD = 6.99) diet quality tertiles (P < 0.01). There were no statistically significant associations between the HEI components or the total HEI score and the 4 alpha diversity measures. Differences in taxa and GO term enterotypes were found in participants with, but not limited to, a higher saturated fat component score (β = 1.35, P = 0.01), added sugar HEI component (β = 0.07, P < 0.001), and higher total dairy score (β = 1.58, P = 0.01).
Conclusions
Specific dietary components are associated with microbial composition and function in the second trimester of pregnancy. These findings provide a foundation for future testable hypotheses.}
}
@article{AN2020103065,
title = {BIM-based decision support system for automated manufacturability check of wood frame assemblies},
journal = {Automation in Construction},
volume = {111},
pages = {103065},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.103065},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519307174},
author = {Shi An and Pablo Martinez and Mohamed Al-Hussein and Rafiq Ahmad},
keywords = {Building information modeling, Ontologies, Intelligent manufacturing, Wood framing, Construction automation, Computer numerical control, Mass customization},
abstract = {As offsite construction is increasing in popularity, an increasing number of construction products are fabricated in a controlled factory environment. Due to the complexity of construction products and the rising amount of automation used in the industry, productivity has reached a peak because the process planning of manufacturing activities is still done manually, for example, building information models (BIM) do not provide manufacturing information for construction products. Knowing whether a machine can manufacture a construction product defined by the BIM model is a critical prerequisite for new products. This paper proposes a BIM-based framework for automating the evaluation of machine capabilities for the manufacturing of construction-oriented products. By identifying intersections of the building elements of the product, feasible manufacturing operations are determined, and manufacturing locations are calculated. These locations are then compared to the manufacturing capabilities of the machine. The proposed approach is validated using two wood frame assemblies. The results show that the system accurately determines whether a user-selected machine can manufacture a construction product pre-designed using BIM software.}
}
@article{LIU2022394,
title = {Construction and application of knowledge graph of Treatise on Febrile Diseases},
journal = {Digital Chinese Medicine},
volume = {5},
number = {4},
pages = {394-405},
year = {2022},
issn = {2589-3777},
doi = {https://doi.org/10.1016/j.dcmed.2022.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S2589377722000763},
author = {Dongbo LIU and Changfa WEI and Shuaishuai XIA and Junfeng YAN},
keywords = {Treatise on Febrile Diseases(Shang Han Lun,《伤寒论》), Knowledge graph, Ontology, Graph database, Knowledge extraction, Knowledge fusion},
abstract = {Objective
To establish the knowledge graph of “disease-syndrome-symptom-method-formula” in Treatise on Febrile Diseases (Shang Han Lun,《伤寒论》) for reducing the fuzziness and uncertainty of data, and for laying a foundation for later knowledge reasoning and its application.
Methods
Under the guidance of experts in the classical formula of traditional Chinese medicine (TCM), the method of “top-down as the main, bottom-up as the auxiliary” was adopted to carry out knowledge extraction, knowledge fusion, and knowledge storage from the five aspects of the disease, syndrome, symptom, method, and formula for the original text of Treatise on Febrile Diseases, and so the knowledge graph of Treatise on Febrile Diseases was constructed. On this basis, the knowledge structure query and the knowledge relevance query were realized in a visual manner.
Results
The knowledge graph of “disease-syndrome-symptom-method-formula” in the Treatise on Febrile Diseases was constructed, containing 6 469 entities and 10 911 relational triples, on which the query of entities and their relationships can be carried out and the query result can be visualized.
Conclusion
The knowledge graph of Treatise on Febrile Diseases systematically realizes its digitization of the knowledge system, and improves the completeness and accuracy of the knowledge representation, and the connection between “disease-syndrome-symptom-treatment-formula”, which is conducive to the sharing and reuse of knowledge can be obtained in a clear and efficient way.}
}
@article{MALDONADO2020105616,
title = {CLIN-IK-LINKS: A platform for the design and execution of clinical data transformation and reasoning workflows},
journal = {Computer Methods and Programs in Biomedicine},
volume = {197},
pages = {105616},
year = {2020},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105616},
url = {https://www.sciencedirect.com/science/article/pii/S0169260720314498},
author = {José Alberto Maldonado and Mar Marcos and Jesualdo Tomás Fernández-Breis and Vicente Miguel Giménez-Solano and María del Carmen Legaz-García and Begoña Martínez-Salvador},
keywords = {Data workflow, Electronic health records, Health information interoperability, Semantic Web},
abstract = {Background and Objective
Effective sharing and reuse of Electronic Health Records (EHR) requires technological solutions which deal with different representations and different models of data. This includes information models, domain models and, ideally, inference models, which enable clinical decision support based on a knowledge base and facts. Our goal is to develop a framework to support EHR interoperability based on transformation and reasoning services intended for clinical data and knowledge.
Methods
Our framework is based on workflows whose primary components are reusable mappings. Key features are an integrated representation, storage, and exploitation of different types of mappings for clinical data transformation purposes, as well as the support for the discovery of new workflows. The current framework supports mappings which take advantage of the best features of EHR standards and ontologies. Our proposal is based on our previous results and experience working with both technological infrastructures.
Results
We have implemented CLIN-IK-LINKS, a web-based platform that enables users to create, modify and delete mappings as well as to define and execute workflows. The platform has been applied in two use cases: semantic publishing of clinical laboratory test results; and implementation of two colorectal cancer screening protocols. Real data have been used in both use cases.
Conclusions
The CLIN-IK-LINKS platform allows the composition and execution of clinical data transformation workflows to convert EHR data into EHR and/or semantic web standards. Having proved its usefulness to implement clinical data transformation applications of interest, CLIN-IK-LINKS can be regarded as a valuable contribution to improve the semantic interoperability of EHR systems.}
}
@article{SER2023,
title = {Clinical Prediction Models for Hospital-Induced Delirium Using Structured and Unstructured Electronic Health Record Data: Protocol for a Development and Validation Study},
journal = {JMIR Research Protocols},
volume = {12},
year = {2023},
issn = {1929-0748},
doi = {https://doi.org/10.2196/48521},
url = {https://www.sciencedirect.com/science/article/pii/S1929074823004936},
author = {Sarah E Ser and Kristen Shear and Urszula A Snigurska and Mattia Prosperi and Yonghui Wu and Tanja Magoc and Ragnhildur I Bjarnadottir and Robert J Lucero},
keywords = {big data, machine learning, data science, hospital-acquired condition, hospital induced, hospital acquired, predict, predictive, prediction, model, models, natural language processing, risk factors, delirium, risk, unstructured, structured, free text, clinical text, text data},
abstract = {Background
Hospital-induced delirium is one of the most common and costly iatrogenic conditions, and its incidence is predicted to increase as the population of the United States ages. An academic and clinical interdisciplinary systems approach is needed to reduce the frequency and impact of hospital-induced delirium.
Objective
The long-term goal of our research is to enhance the safety of hospitalized older adults by reducing iatrogenic conditions through an effective learning health system. In this study, we will develop models for predicting hospital-induced delirium. In order to accomplish this objective, we will create a computable phenotype for our outcome (hospital-induced delirium), design an expert-based traditional logistic regression model, leverage machine learning techniques to generate a model using structured data, and use machine learning and natural language processing to produce an integrated model with components from both structured data and text data.
Methods
This study will explore text-based data, such as nursing notes, to improve the predictive capability of prognostic models for hospital-induced delirium. By using supervised and unsupervised text mining in addition to structured data, we will examine multiple types of information in electronic health record data to predict medical-surgical patient risk of developing delirium. Development and validation will be compliant to the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) statement.
Results
Work on this project will take place through March 2024. For this study, we will use data from approximately 332,230 encounters that occurred between January 2012 to May 2021. Findings from this project will be disseminated at scientific conferences and in peer-reviewed journals.
Conclusions
Success in this study will yield a durable, high-performing research-data infrastructure that will process, extract, and analyze clinical text data in near real time. This model has the potential to be integrated into the electronic health record and provide point-of-care decision support to prevent harm and improve quality of care.
International Registered Report Identifier (IRRID)
DERR1-10.2196/48521}
}
@incollection{NORTHOFF2024395,
title = {Chapter 29 - From icebergs to the self—point of view},
editor = {Georg Northoff},
booktitle = {From Brain Dynamics to the Mind},
publisher = {Academic Press},
pages = {395-407},
year = {2024},
isbn = {978-0-12-821935-5},
doi = {https://doi.org/10.1016/B978-0-12-821935-5.00023-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219355000235},
author = {Georg Northoff},
keywords = {Comparison with iceberg, Ecological background layer, Mental surface layer, Point of view, Self},
abstract = {The self is not just in the brain and or body. Rather it is closely related to its respective environmental context. This raises the question whether the self shows an analogous neuro-ecological basis in the world. We first demonstrate how environmental life shapes and impacts the neural basis of self including its topography and dynamic. That is followed by the assumption of a neuro-ecological background layer of self complementing its mental surface layer. Finally, we propose that the neuro-ecological background layer provides what philosophically has been described as the point of view, the anchoring of the self in the world from which it perceives and accesses the world in a perspectival way. This does not lay bare the neuro-ecological scale-free temporal nestedness of the self in the world but also carries major implications for our more philosophical, i.e., ontological understanding of self, brain, and world. The assumption of a deeper layer of self, a neuro-ecological background layer constituting its point of view, puts the self on a par with the iceberg: like the iceberg, the self is also exposed to various forces both from within itself and without itself by environmental context. Accordingly, here is yet another instance where even one of the key features of the mind, the self, exhibits features analogous to those characterizing the basic features of nature.}
}
@article{WERBROUCK2020103286,
title = {Scan-to-graph: Semantic enrichment of existing building geometry},
journal = {Automation in Construction},
volume = {119},
pages = {103286},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103286},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520300674},
author = {Jeroen Werbrouck and Pieter Pauwels and Mathias Bonduel and Jakob Beetz and Willem Bekers},
keywords = {Linked Data, Existing buildings, Scan-to-BIM, Building Information Modelling},
abstract = {Building Information Modelling (BIM) has changed the way in which buildings are conceived, planned and executed. Apart from their frequent use for as-planned buildings, BIM authoring tools have now been adopted for a number of years for digitising existing buildings as well, mostly by performing a ‘scan-to-BIM’ process: the creation of a BIM model, primarily based on point clouds. However, some inherent characteristics of existing buildings are complicating such a process: uncertainties, geometric irregularities, classification of heritage building components, linking sources about the real-world asset and an interdisciplinarity that may go beyond traditional Architecture, Engineering and Construction (AEC) topics (e.g. heritage, Facility Management, sensor data and damage assessment). In this paper, a framework called ‘scan-to-graph’ (STG) is proposed to integrate the concepts of scan-to-BIM with Semantic Web technologies, as these could provide answers to the above-mentioned challenges, most notably on documentation of uncertainties, sources and modelling decisions, element classification and cross-discipline information linking. In order to test the STG concept, a use case was developed where the Audience Room of the Gravensteen castle in Ghent was reconstructed from point clouds, semantically enriched and stored as an RDF graph. The resulting graph contains multiple interlinked geometry types, metadata about the reconstruction process and the sources and allows to unambiguously refer to other contextual data on the Web.}
}
@article{PURNOMOWP2024111558,
title = {Extraction and attribution of public figures statements for journalism in Indonesia using deep learning},
journal = {Knowledge-Based Systems},
volume = {289},
pages = {111558},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111558},
url = {https://www.sciencedirect.com/science/article/pii/S095070512400193X},
author = {Yohanes Sigit {Purnomo W.P.} and Yogan Jaya Kumar and Nur Zareen Zulkarnain and Basit Raza},
keywords = {Statement extraction and attribution, Named-entity recognition, Knowledge-based, Indonesian language, Deep learning},
abstract = {News articles are usually written by journalists based on statements taken from interviews with public figures. Attribution from such statements provides important information and it can be extracted from news articles to build a knowledge base by developing a sequential tagging scheme such as entity recognition. This research applies two deep learning architectures: recurrent neural networks-based and transformer-based, to establish public figures statement attribution and extraction models in the Indonesian Language. The experiments are conducted using five deep-learning model architectures with two different corpus sizes to investigate the impact of corpus size on each model's performance. The experiments show that the best model for the RNN-based architecture is PFSA-ID-BLWCA which achieves 81.34 % F1 score, and the best model for the transformer-based is PFSA-ID-TWCA which obtains 81.01 % F1 score. This research also discovers that the size of the corpus influences the model performances. Furthermore, the study lays a foundation to overcome the attribution extraction in another language, especially low-resource languages, with some necessary adjustments.}
}
@article{ZHANG2022107730,
title = {MKGE: Knowledge graph embedding with molecular structure information},
journal = {Computational Biology and Chemistry},
volume = {100},
pages = {107730},
year = {2022},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2022.107730},
url = {https://www.sciencedirect.com/science/article/pii/S1476927122001104},
author = {Yi Zhang and Zhouhan Li and Biao Duan and Lei Qin and Jing Peng},
keywords = {Knowledge graph embedding, Link prediction, Molecular representation learning, Drug-drug interaction prediction},
abstract = {To easier manipulate Knowledge Graphs (KGs), knowledge graph embedding (KGE) is proposed and wildly used. However, the relations between entities are usually incomplete due to the performance problems of knowledge extraction methods, which also leads to the sparsity of KGs and make it difficult for KGE methods to obtain reliable representations. Related research has not paid much attention to this challenge in the biomedicine field and has not sufficiently integrated the domain knowledge into KGE methods. To alleviate this problem, we try to incorporate the molecular structure information of the entity into KGE. Specifically, we adopt two strategies to obtain the vector representations of the entities: text-structure-based and graph-structure-based. Then, we spliced the two together as the input of the KGE models. To validate our model, we construct a KCCR knowledge graph and validate the model’s superiority in entity prediction, relation prediction, and drug-drug interaction prediction tasks. To the best of our knowledge, this is the first time that molecular structure information has been integrated into KGE methods. It is worth noting that researchers can try to improve the work based on KGE by fusing other feature annotations such as Gene Ontology and protein structure.}
}
@article{BERETA2019100514,
title = {Ontop-spatial: Ontop of geospatial databases},
journal = {Journal of Web Semantics},
volume = {58},
pages = {100514},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.100514},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300447},
author = {Konstantina Bereta and Guohui Xiao and Manolis Koubarakis},
keywords = {Spatial data, GeoSPARQL, Ontology-based data access, Ontop-spatial},
abstract = {In this paper, we propose an OBDA approach for accessing geospatial data stored in relational databases using the R2RML mappings and OGC standard GeoSPARQL. On the theoretical side, we provide a formalization of GeoSPARQL in terms of SPARQL entailment regime. For a practical query answering algorithm, we introduce an extension to the existing SPARQL-to-SQL translation method to support GeoSPARQL features. Our approach has been implemented in the system Ontop-spatial, an extension of the OBDA system Ontop for creating virtual geospatial RDF graphs on top of geospatial relational databases. We present an experimental evaluation of our system using and extending a state-of-the-art benchmark. To measure the performance of our system, we compare it to two state-of-the-art geospatial RDF stores –a free and open-source one, and a commercial one– and confirm its efficiency.}
}
@article{WU201868,
title = {Behind the scenes: A medical natural language processing project},
journal = {International Journal of Medical Informatics},
volume = {112},
pages = {68-73},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S138650561730446X},
author = {Joy T. Wu and Franck Dernoncourt and Sebastian Gehrmann and Patrick D Tyler and Edward T Moseley and Eric T Carlson and David W Grant and Yeran Li and Jonathan Welt and Leo Anthony Celi},
keywords = {Artificial intelligence in medicine, Natural language processing, Machine learning, Text analytics, Multidisciplinary teamwork, Cross-disciplinary research, Translational research},
abstract = {Advancement of Artificial Intelligence (AI) capabilities in medicine can help address many pressing problems in healthcare. However, AI research endeavors in healthcare may not be clinically relevant, may have unrealistic expectations, or may not be explicit enough about their limitations. A diverse and well-functioning multidisciplinary team (MDT) can help identify appropriate and achievable AI research agendas in healthcare, and advance medical AI technologies by developing AI algorithms as well as addressing the shortage of appropriately labeled datasets for machine learning. In this paper, our team of engineers, clinicians and machine learning experts share their experience and lessons learned from their two-year-long collaboration on a natural language processing (NLP) research project. We highlight specific challenges encountered in cross-disciplinary teamwork, dataset creation for NLP research, and expectation setting for current medical AI technologies.}
}
@article{WANG2025107926,
title = {Investigating the relationship between brain age and Alzheimer’s disease: A deep learning approach with multimodal MRI},
journal = {Biomedical Signal Processing and Control},
volume = {109},
pages = {107926},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2025.107926},
url = {https://www.sciencedirect.com/science/article/pii/S1746809425004379},
author = {Zhengning Wang and Jiaxin Liu and Fang Chen and Ke Wang and Yuhang Xu and Fengmei Lu and Yanling Li and Jingjing Gao},
keywords = {Alzheimer’s disease, Graph neural network, Magnetic resonance imaging, Brain aging, Disease classification, Biomarkers},
abstract = {Alzheimer’s disease (AD) is a progressive neurodegenerative disorder with potential life-threatening complications in its advanced stages. The brain’s estimated age, derived from magnetic resonance imaging (MRI), has emerged as a promising biomarker for the early detection of AD. However, existing studies have predominantly utilized unimodal structural MRI, overlooking the supplementary insights offered by early functional connectivity disruptions. Meanwhile, the limited availability of diseased samples has impeded the development of robust classification models. The advent of multimodal techniques and transfer learning has presented novel avenues for addressing these challenges. In this study, we introduce a multimodal graph neural network model with an autoencoder structure primarily focused on brain age prediction, with disease classification used to validate its effectiveness. We have harnessed transfer learning to cultivate a three-branch model that amalgamates non-imaging data for disease classification. Furthermore, we have employed attention maps and gene expression analysis to delve into AD-associated brain regions, genes and their ontological implications. Our findings underscore the model’s superiority in brain age prediction, highlighting its ability to extract and integrate structural and functional features, while disease classification serves as a validation of these predictions. The transfer learning-based three-branch model has proven to be a potent instrument for disease classification. Moreover, we have substantiated the intimate nexus between the amygdala, parahippocampal gyrus, CDH23, DNA damage, and AD.}
}
@article{FENG2023,
title = {A Schema for Digitized Surface Swab Site Metadata in Open-Source DNA Sequence Databases},
journal = {mSystems},
volume = {8},
number = {2},
year = {2023},
issn = {2379-5077},
doi = {https://doi.org/10.1128/msystems.01284-22},
url = {https://www.sciencedirect.com/science/article/pii/S2379507723003045},
author = {Jingzhang Feng and Devin Daeschel and Damion Dooley and Emma Griffiths and Marc Allard and Ruth Timme and Yi Chen and Abigail B. Snyder},
keywords = {genomic surveillance, epidemiology, informatics, foodborne pathogen},
abstract = {The regular analysis of whole-genome sequence data in collections such as NCBI’s Pathogen Detection Database is used by many public health organizations to detect outbreaks of infectious disease. However, isolate metadata in these databases are often incomplete and of poor quality.
ABSTRACT
Large, open-source DNA sequence databases have been generated, in part, through the collection of microbial pathogens by swabbing surfaces in built environments. Analyzing these data in aggregate through public health surveillance requires digitization of the complex, domain-specific metadata that are associated with the swab site locations. However, the swab site location information is currently collected in a single, free-text, “isolation source”, field-promoting generation of poorly detailed descriptions with various word order, granularity, and linguistic errors, making automation difficult and reducing machine-actionability. We assessed 1,498 free-text swab site descriptions that were generated during routine foodborne pathogen surveillance. The lexicon of free-text metadata was evaluated to determine the informational facets and the quantity of unique terms used by data collectors. Open Biological Ontologies (OBO) Foundry libraries were used to develop hierarchical vocabularies that are connected with logical relationships to describe swab site locations. 5 informational facets that were described by 338 unique terms were identified via content analysis. Term hierarchy facets were developed, as were statements (called axioms) about how the entities within these five domains are related. The schema developed through this study has been integrated into a publicly available pathogen metadata standard, facilitating ongoing surveillance and investigations. The One Health Enteric Package was available at NCBI BioSample, beginning in 2022. The collective use of metadata standards increases the interoperability of DNA sequence databases and enables large-scale approaches to data sharing and artificial intelligence as well as big-data solutions to food safety.
IMPORTANCE The regular analysis of whole-genome sequence data in collections such as NCBI’s Pathogen Detection Database is used by many public health organizations to detect outbreaks of infectious disease. However, isolate metadata in these databases are often incomplete and of poor quality. These complex, raw metadata must often be reorganized and manually formatted for use in aggregate analyses. These processes are inefficient and time-consuming, increasing the interpretative labor needed by public health groups to extract actionable information. The future use of open genomic epidemiology networks will be supported through the development of an internationally applicable vocabulary system with which swab site locations can be described.}
}
@article{HARTLEY2020597,
title = {Beyond the Stimulus: A Neurohumanities Approach to Language, Music, and Emotion},
journal = {Neuron},
volume = {108},
number = {4},
pages = {597-599},
year = {2020},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2020.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0896627320308175},
author = {Catherine A. Hartley and David Poeppel},
abstract = {Summary
The brain basis of language, music, and emotion can be studied from the perspective of the psychological and cognitive sciences. Does this approach link to concerns of the humanities meaningfully? We outline prospects of developing a genuine neurohumanities research program.}
}
@article{BARANYI2024101177,
title = {From data to models and predictions in food microbiology},
journal = {Current Opinion in Food Science},
volume = {57},
pages = {101177},
year = {2024},
issn = {2214-7993},
doi = {https://doi.org/10.1016/j.cofs.2024.101177},
url = {https://www.sciencedirect.com/science/article/pii/S2214799324000559},
author = {József Baranyi and Maha Rockaya and Mariem Ellouze},
abstract = {This paper emphasizes the importance of structured databases, visualization techniques, statistics, and mathematical models as milestones when developing predictive models of bacterial responses to food environments. Predictions generated by such models are vital in decision-making on food safety and quality issues. The paper suggests that while refinements, such as reparameterization, rescaling, and fine-tuning smoothness-characterizing parameters, are useful for numerical/statistical point of view, the result should not be considered as new models. It is proposed that novel predictive models should be linked to those widely accepted in related disciplines, such as biotechnology, systems biology, or biochemistry.}
}
@article{LIU2020297,
title = {Breaking Social Media Bubbles for Information Globalization: A Cross-Cultural and Cross-Language User-Centered Sense-Making Approach},
journal = {Data and Information Management},
volume = {4},
number = {4},
pages = {297-305},
year = {2020},
issn = {2543-9251},
doi = {https://doi.org/10.2478/dim-2020-0020},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122000432},
author = {Xiaozhong Liu and Daqing He and Dan Wu},
keywords = {social media, big data, cross culture, cross language, information globalization},
abstract = {With the globalization of data, online social media plays an active role in spreading information and classifying people, and thinking about how to break the solidification of algorithms becomes critical. Current algorithmic research in the social media space often focuses on a single service or language, mainly due to the lack of a way to connect the different bubbles. The panel speakers described their various research activities in which they presented different perspectives on how to break the bubble. This article provides a summary of this interactive panel.}
}
@article{SU2024102813,
title = {A methodology for information modelling and analysis of manufacturing processes for digital twins},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {90},
pages = {102813},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2024.102813},
url = {https://www.sciencedirect.com/science/article/pii/S0736584524001005},
author = {Shuo Su and Aydin Nassehi and Qunfen Qi and Ben Hicks},
keywords = {Digital twins, Manufacturing process information model, Information modelling and analysis, Material extrusion process},
abstract = {This paper introduces a methodology for information modelling and analysis of physical manufacturing processes for digital twins (DTs). It aims to establish a comprehensive and fundamental understanding of manufacturing processes regarding the specific purpose of the DT. Through this methodology, information entities within the manufacturing process that can be represented in DTs, along with their essential attributes, are systematically identified. To achieve this, an information model is firstly proposed to define such entities, termed as representative information. The attributes and hierarchy of entities are formulated based on a requirements analysis of the DT lifecycle. An Integration Definition for Process Modelling 0 (IDEF0) model, Petri nets, and a literature-based identification process are applied to represent the manufacturing process’s workflow and identify information entities. Moreover, the relative importance of representing each information entity in a DT is evaluated by integrating domain-specific knowledge with the specific purpose of the DT. Three types of information analysis are suggested, each with its corresponding methods: empirical analysis, theoretical analysis, and experimental analysis. Specifically, this study explores the material extrusion (MEX) process of the Prusa i3 MK3 printer, resulting in an information model consisting of 128 entities including 21 components, 25 activities and 82 properties. These information entities and associated attributes provide a reference for selecting and synchronizing specific physical information in a DT for estimating dimensional accuracy during the MEX process.}
}
@article{CACCHIARELLI2024113037,
title = {Transcriptomics of fruit ripening in a tomato wide cross and genetic analysis of differentially expressed genes among parents and hybrid},
journal = {Scientia Horticulturae},
volume = {330},
pages = {113037},
year = {2024},
issn = {0304-4238},
doi = {https://doi.org/10.1016/j.scienta.2024.113037},
url = {https://www.sciencedirect.com/science/article/pii/S0304423824001961},
author = {Paolo Cacchiarelli and Flavio E. Spetale and Débora P. Arce and Elizabeth Tapia and Guillermo R. Pratta},
keywords = {Tomato crop breeding, Ripening process, RNA-seq, Functional annotation, Gene action estimation, Prepotency},
abstract = {Omic techniques have been used in recent years to support and make robust progress in breeding programs for different species. RNA-Seq is a powerful method for the analysis of transcriptomes to reveal genes with differential expression massively. The cultivated tomato (Solanum lycopersicum) is a crop with a narrow genetic basis in which fruit quality and wild genotypes are usually important targets to improve through breeding programs for obtaining new varieties. A novel framework was carried out using differential expression data (at three tomato fruit ripening stages), automatic Gene Ontology (GO) annotation, and estimating the Gene Action in our genotypes (two parental genotypes: the cultivated cv. Caimanta, the exotic LA0722 of S. pimpinellifolium, and their interspecific F1). The results indicate differential expression in all three genotypes, with an average of 17.26% of genes expressed throughout the genome. Functionality prediction detected 84 significant GO-terms related to the ripening process, by the FGGA method. In addition, we discovered 1,364 common genes among the GO-terms detected previously. Finally, a gene action analysis was carried out and high levels of non-additive effects towards the wild genotype LA0722 (59.90% of the total genes, with 40.10% towards the cultivated genotype) were detected. This finding implies that the hybrid genotype is similar to the exotic parents for traits related to tomato fruit ripening, which could account for prepotent effects. Genomic regions underlying the expression of these transcripts could be target regions to develop molecular markers for assisting breeding programs.}
}
@article{SUCESKA2018179,
title = {A Gramscian reading of language in Bakhtin and Voloshinov},
journal = {Language Sciences},
volume = {70},
pages = {179-192},
year = {2018},
note = {Karl Marx and the Language Sciences – Critical Encounters},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0388000117303625},
author = {Alen Sućeska},
keywords = {Mikhail Bakhtin, Valentin Voloshinov, Antonio Gramsci, Hegemony, Multiaccentuality},
abstract = {Both Bakhtin and Voloshinov were very keen to show that language is socially stratified and that this stratification corresponds in form to the stratification between social groups or classes. Through a comparative analysis of their concepts such as ‘speech genres’/‘behavioural genres’, ‘heteroglossia’/‘multiaccentuality’, ‘refraction’ etc, this paper will aim to show how both of the authors offer a convincing theoretical framework for a social and historical approach to language which stresses its class character. It is exclusively through such an approach that a coherent ‘Marxist’ account of language can be developed, both contrary to vulgar Marxism which sees language as a simple ‘reflection’ of reality and to the mainstream linguistics which tends to abstract from these social aspects of language. However, it will be argued that Bakhtin's and Voloshinov's works also have their limitations, and that, in order to develop a coherent and rigorous Marxist account of language, it is necessary to move beyond them towards authors which link language to politics, such as, to a certain extent, Marx himself, or, to a much greater extent, Antonio Gramsci.}
}
@article{SCUTELNICU2023804,
title = {An Approach of Interconnecting Romanian Lexical Resources},
journal = {Procedia Computer Science},
volume = {225},
pages = {804-814},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.067},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923012255},
author = {Liviu Andrei Scutelnicu},
keywords = {Lexical resources, electronic dictionary, Romanian WordNet, Romanian Corpus, linked data},
abstract = {In this paper we will present an approach for interconnecting some of the most important lexical resources for the Romanian language, among which the Corpus for Contemporary Romanian Language (CoRoLa), Romanian WordNet, the Thesaurus Dictionary of Romanian Language in Electronic Format (eDTLR). We will show how they are indexed to each other in order to obtain a standard interconnected structure, and then, we will present a general architecture of a system dedicated to interconnecting lexical resources “on the fly”.}
}
@article{HRADECKY20232982,
title = {Description and evaluation of production goals},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {2982-2988},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.1423},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323018311},
author = {Pavel Hradecky and Vojtech Janu and Pavel Burget and Tomas Jochman and Tilman Becker},
keywords = {Multi-agent systems applied to industrial systems, Smart factory, Industry 4.0},
abstract = {Robotic cells in industrial automation use different programming and description languages, which are typically almost hard-wired in the program solutions and do not allow a goal product to be changed easily. Using a unification description language allows focusing on the solution itself. An independent description of capabilities and intentions offers a way to allow for changing the goal and also for distributing the process to different locations. Moreover, based on the description, a knowledge model can be created to check if a solution exists for the defined goal and resource capabilities. If the knowledge model is implemented in a database, the check can be performed in a very Efficient way, which allows it to be used in real-life production scenarios. A reasoner tries to reach the solution-defined goal based on an initial state and actions following the predicates’ rules. Using the database to search only for the first possible solution, it can be checked if the defined domain model can be realized. The check can be done automatically and much faster than using a planner. In an implementation in our Testbed for Industry 4.0 at CIIRC/CTU Prague, we employ a scalable system based on a PDDL description and an automated translation to TypeDB to efficiently compute production plans for changing goals, tools and resources.}
}
@article{WINKLER2021353,
title = {Between tragedy, romance, comedy and satire: narratives of axiological progress in public relations},
journal = {Journal of Communication Management},
volume = {25},
number = {4},
pages = {353-367},
year = {2021},
issn = {1363-254X},
doi = {https://doi.org/10.1108/JCOM-11-2020-0145},
url = {https://www.sciencedirect.com/science/article/pii/S1363254X2100016X},
author = {Peter Winkler and Jannik Kretschmer and Michael Etter},
keywords = {Axiology, Narrative approach, Disciplinary progress, Paradigms, Public relations},
abstract = {Purpose
Over recent years, public relations (PR) research has diversified in themes and theories. As a result, PR presents itself today as a multi-paradigmatic discipline with competing ideas of progress that mainly circle around questions of ontology and epistemology, i.e. around defining appropriate object and knowledge in PR research.
Design/methodology/approach
This conceptual article highlights a third crucial question underlying the debate drawing on a narrative approach: The question of axiology, hence, the normative question how PR research shall develop to contribute to societal progress.
Findings
The article presents a model, which describes how normative visions of progress in different PR paradigms – functional, co-creational, social-reflective and critical-cultural – manifest in each distinct combinations of four narrative plots – tragedy, romance, comedy and satire.
Originality/value
These findings complement the current debate on disciplinary progress in PR research by fostering reflection and debate on paradigm development and cross-paradigmatic tensions and exchange from an explicit axiological perspective.}
}
@article{MUTHALAGU2025126044,
title = {Detection and prevention of evasion attacks on machine learning models},
journal = {Expert Systems with Applications},
volume = {266},
pages = {126044},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126044},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424029117},
author = {Raja Muthalagu and Jasmita Malik and Pranav M. Pawar},
keywords = {Adversarial machine learning, Cybersecurity, Evasion attacks, Secure coding},
abstract = {With the increasing use of machine learning models in critical applications such as image classification, natural language processing, and cybersecurity, there is a growing concern about the vulnerability of these models to adversarial attacks. Evasion attacks, in particular, pose a significant threat by manipulating input data to mislead the model’s predictions. This paper presents an overview of evasion attacks on machine learning models, its variants and conducts an adaptive white-box evasion attack to highlight how defense measures can be superseded with stronger evasion attack algorithms. It first discusses the different types of evasion attack algorithms, including Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD), and Carlini–Wagner, highlighting their impact on model performance and security. It then reviews various detection and mitigation techniques which aim to identify adversarial examples and improve model robustness. Finally, the paper proposes effective mitigation techniques against evasion attacks and recommends a machine learning based cybersecurity architecture workflow that can be practically applied by organizations in real-world settings. Overall, this paper provides a comprehensive overview of evasion attacks on machine learning models and highlights the current state of research in defending against them.}
}
@article{DASILVA2018229,
title = {Context-Aware Recommendation for Industrial Alarm System},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {229-234},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.266},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318313909},
author = {Márcio J. {da Silva} and Carlos E. Pereira and Marcelo Götz},
keywords = {Recommendation, Context, Semantic Rules, Bayesian Network},
abstract = {The selection of alerts within an industrial plant is not an easy task, due to the huge amount of variables involved. By this fact, it is necessary to have the means to easiest the acquirement of necessary information throughout the extraction of the knowledge in order to assist operator in the decision-making process. In this sense, a context-sensitive recommendation system can identify a situation from patterns of events. From the recognition of a situation in a particular ontological context, a proactive approach based on knowledge seems to be more effective for this task. The industrial alarm management system, generally used in supervision systems and process controls, has a problem related to the cognitive overload of their operators. Unfortunately, this occurs mainly during perturbations in the process. In this context, this work aims to investigate and develop methods to adopt proper actions sensitive to the present situation in an industrial alarm management system through semantic web technology and machine learning techniques. This problem involves questions related to the way of obtaining the context data, the context model that describes the situation and the way in which the context data of the plant are related, which allows to infer a situation. In this sense, it is proposed the appliance of data mining techniques to obtain knowledge of the process, taken into consideration an adaptive interface that makes non-intrusive recommendations and help the operator in an industrial plant. To combine deterministic knowledge with probabilistic knowledge, a Bayesian Networks is used. For the concept evaluation, a case study was conducted based on a real historical database, which shows sound and promising results.}
}
@article{CASILLO2025107702,
title = {Beyond domain dependency in security requirements identification},
journal = {Information and Software Technology},
volume = {182},
pages = {107702},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107702},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925000412},
author = {Francesco Casillo and Vincenzo Deufemia and Carmine Gravino},
keywords = {Security Requirements Classification, Natural Language Processing, Machine Learning, Transformers},
abstract = {Context:
Early security requirements identification is crucial in software development, facilitating the integration of security measures into IT networks and reducing time and costs throughout software life-cycle.
Objectives:
This paper addresses the limitations of existing methods that leverage Natural Language Processing (NLP) and machine learning techniques for detecting security requirements. These methods often fall short in capturing syntactic and semantic relationships, face challenges in adapting across domains, and rely heavily on extensive domain-specific data. In this paper we focus on identifying the most effective approaches for this task, highlighting both domain-specific and domain-independent strategies.
Method:
Our methodology encompasses two primary streams of investigation. First, we explore shallow machine learning techniques, leveraging word embeddings. We test ensemble methods and grid search within and across domains, evaluating on three industrial datasets. Next, we develop several domain-independent models based on BERT, tailored to better detect security requirements by incorporating data on software weaknesses and vulnerabilities.
Results:
Our findings reveal that ensemble and grid search methods prove effective in domain-specific and domain-independent experiments, respectively. However, our custom BERT models showcase domain independence and adaptability. Notably, the CweCveCodeBERT model excels in Precision and F1-score, outperforming existing approaches significantly. It improves F1-score by ∼3% and Precision by ∼14% over the best approach currently in the literature.
Conclusion:
BERT-based models, especially with specialized pre-training, show promise for automating security requirement detection. This establishes a foundation for software engineering researchers and practitioners to utilize advanced NLP to improve security in early development phases, fostering the adoption of these state-of-the-art methods in real-world scenarios.}
}
@article{WITTEVEEN202470,
title = {Golden spikes, scientific types, and the ma(r)king of deep time},
journal = {Studies in History and Philosophy of Science},
volume = {106},
pages = {70-85},
year = {2024},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2024.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S003936812400013X},
author = {Joeri Witteveen},
keywords = {Geology, Stratigraphy, Metrology, Taxonomy, Stratotype, GSSP, Measurement prototype, Kilogram standard, Type specimen, Holotype, Nomenclature, Classification, Standardization, Reference fixing, Natural classification},
abstract = {Chronostratigraphy is the subfield of geology that studies the relative age of rock strata and that aims at producing a hierarchical classification of (global) divisions of the historical time-rock record. The ‘golden spike’ or ‘GSSP’ approach is the cornerstone of contemporary chronostratigraphic methodology. It is also perplexing. Chronostratigraphers define each global time-rock boundary extremely locally, often by driving a gold-colored pin into an exposed rock section at a particular level. Moreover, they usually avoid rock sections that show any meaningful sign of paleontological disruption or geological discontinuity: the less obvious the boundary, the better. It has been argued that we can make sense of this practice of marking boundaries by comparing the status and function of golden spikes to that of other concrete, particular reference standards from other sciences: holotypes from biological taxonomy and measurement prototypes from the metrology of weight and measures. Alisa Bokulich (2020b) has argued that these ‘scientific types’ are in an important sense one of a kind: they have a common status and function. I will argue that this picture of high-level conceptual unity is mistaken and fails to consider the diversity of aims and purposes of standardization and classification across the sciences. I develop an alternative, disunified account of scientific types that shows how differences in ontological attitudes and epistemic aims inform scientists’ choices between different kinds of scientific types. This perspective on scientific types helps to make sense of an intriguing mid-twentieth-century debate among chronostratigraphers about the very nature of their enterprise. Should chronostratigraphers conventionally make boundaries by designating golden spikes, or should they attempt to mark pre-existing ‘natural’ boundaries with the help of a different kind of scientific type?}
}
@article{WILKINSON2022100623,
title = {Radical contingency, radical historicity and the spread of ‘homosexualism’: A diachronic corpus-based critical discourse analysis of queer representation in The Times between 1957–1967 and 1979–1990},
journal = {Discourse, Context & Media},
volume = {48},
pages = {100623},
year = {2022},
issn = {2211-6958},
doi = {https://doi.org/10.1016/j.dcm.2022.100623},
url = {https://www.sciencedirect.com/science/article/pii/S2211695822000460},
author = {Mark Wilkinson},
keywords = {Discourse theory, Post-structuralist discourse theory, Laclau and Mouffe, Critical discourse analysis (CDA), Corpus-based critical discourse analysis, Corpus assisted discourse studies (CADS), Representation, Sexuality, LGBTQI, British Press},
abstract = {This paper suggests that LGBTQI representation in The Times does more than simply construct queer subjects. Rather, by representing a sexualised Other, the language of The Times necessarily indexes the presence of an unmarked heterosexual population. Moreover, while LGBTQI people have historically been criminalised and discriminated against, a comparison between two historical corpora (1957–1967 and 1979–1990) demonstrates that The Times has consistently used language to suggest that the heterosexual population is, in fact, vulnerable to the threat of non-normative desire and sexual practices. By considering which key phrases and collocations are consistent between the two corpora, it is revealed that the verb spread is used to position heterosexual people as vulnerable to both ‘homosexual conduct’ in the 1960s and the threat of HIV infection in the 1980s. This is significant because of the considerable influence broadsheet newspapers like The Times had on British public discourse during the latter half of the twentieth century. In order to frame the discussion, the analysis is supported by the theories of radical contingency and radical historicity (Laclau and Mouffe, 1985). The former posits that subject positions are necessarily constituted by what they are not while the latter posits that subjectivities available to us in the present are always the result of political processes from the past. The social ontology of discourse theory (Laclau and Mouffe 1985) therefore provides a lens through which to interpret what diachronic newspaper data reveals about how British social attitudes were changing or staying the same during this time.}
}
@article{TIMAKUM2025102462,
title = {Four decades of data & knowledge engineering: A bibliometric analysis and topic evolution study (1985–2024)},
journal = {Data & Knowledge Engineering},
volume = {159},
pages = {102462},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102462},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000576},
author = {Tatsawan Timakum and Soobin Lee and Dongha Kim and Min Song and Il-Yeol Song},
keywords = {Data and knowledge engineering journal, Bibliometric analysis, Citation analysis, Topic evolution analysis, Authorship network analysis},
abstract = {The Data and Knowledge Engineering (DKE) journal has established a significant global research presence over four decades, substantially contributing to the advancement of data and knowledge engineering disciplines. This comprehensive bibliometric study analyzes the journal’s publications over the past 40 years (1985–2024), employing bibliographic records and citation data from Scopus, Web of Science (WoS), and ScienceDirect. By utilizing CiteSpace for citation and co-citation mapping and Dirichlet Multinomial Regression (DMR) topic modeling for trend analysis, the research provides a multifaceted examination of the journal’s scholarly landscape. Over its 40-year history, DKE has published 1951 articles, accumulating 53,594 citations. The study comprehensively explores key bibliometric dimensions, including influential authors, author networks, citation patterns, topic clusters, institutional contributions, and research funding sponsors, as well as evolution of topics, showing increasing, decreasing, or constant trends. Comprehensive analysis offers a meta-analytical perspective on DKE’s scholarly contributions, positioning the journal as a pioneering publication platform that advances critical knowledge and methodological innovations in data and knowledge engineering research domains. Through an in-depth examination of the journal’s publication trajectory, the study provides insights into the field’s scholarly evolution, highlighting DKE’s pivotal role in shaping academic discourse and technological understanding.}
}
@article{KATTEPUR2025111657,
title = {DETROIT: Decomposition techniques for a hierarchy of 6G network intent management functions},
journal = {Computer Networks},
volume = {272},
pages = {111657},
year = {2025},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2025.111657},
url = {https://www.sciencedirect.com/science/article/pii/S1389128625006243},
author = {Ajay Kattepur and Snigdha Das and Danesh Daroui and Swarup Mohalik and Marin Orlic and Sultan Ertas},
keywords = {Hierarchical intent management, Network slicing, Decomposition algorithms, AI planning, Graph search, Intent reports},
abstract = {To manage the scale and complexity of 6G autonomous network deployments, the concept of intent-driven networking has been introduced. Intents are further autonomously handled by implementations of intent management functions. The scalable management of intents is foreseen to be hierarchical, with multiple intent management functions at the business, management and resource levels. This hierarchy brings in challenges in decomposing high-level intent expectations to sub-intents to be solved by lower-level intent managers. In this paper, we present DETROIT, a framework for hierarchical intent decomposition. In DETROIT, we examine the use of multiple decomposition algorithms to determine the optimal decomposition techniques. To provide correct decompositions, a QoS algebraic formulation is incorporated within the decomposition algorithms. Intent reports are further composed to provide feedback on the efficacy of the decomposition process. Further prediction and evaluation steps ensure careful deliberation of the outputs before actuation. The decomposition is implemented over a network slicing use case with multiple expectations. We demonstrate that through the use of the agents and QoS algebra proposed within DETROIT, the decomposed targets are generated within 2% deviation. This process further captures the tradeoff between reaching intent expectation targets and resource optimality. Such an automated process of intent decomposition would be crucial in a hierarchy of 6G intent management functions.}
}
@article{BALLSUNSTANTON201847,
title = {FAIMS Mobile: Flexible, open-source software for field research},
journal = {SoftwareX},
volume = {7},
pages = {47-52},
year = {2018},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2017.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352711017300869},
author = {Brian Ballsun-Stanton and Shawn A. Ross and Adela Sobotkova and Penny Crook},
keywords = {Android, Mobile software, Field research, Field science},
abstract = {FAIMS Mobile is a native Android application supported by an Ubuntu server facilitating human-mediated field research across disciplines. It consists of ‘core’ Java and Ruby software providing a platform for data capture, which can be deeply customised using ‘definition packets’ consisting of XML documents (data schema and UI) and Beanshell scripts (automation). Definition packets can also be generated using an XML-based domain-specific language, making customisation easier. FAIMS Mobile includes features allowing rich and efficient data capture tailored to the needs of fieldwork. It also promotes synthetic research and improves transparency and reproducibility through the production of comprehensive datasets that can be mapped to vocabularies or ontologies as they are created.}
}
@article{TCHOUKA2024200416,
title = {Differentially private de-identifying textual medical document is compliant with challenging NLP analyses: Example of privacy-preserving ICD-10 code association},
journal = {Intelligent Systems with Applications},
volume = {23},
pages = {200416},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200416},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324000905},
author = {Yakini Tchouka and Jean-François Couchot and David Laiymani and Philippe Selles and Azzedine Rahmani},
keywords = {De-identification, Clinical data, Local differential privacy, Metric-privacy, Natural language processing, ICD-10 code association, Machine learning},
abstract = {Medical research plays a crucial role within scientific research. Technological advancements, especially those related to the rise of machine learning, pave the way for the exploration of medical issues that were once beyond reach. Unstructured textual data, such as correspondence between doctors, operative reports, etc., often serve as a starting point for many medical applications. However, for obvious privacy reasons, researchers do not legally have the right to access these documents as long as they contain sensitive data, as defined by regulations like GDPR (General Data Protection Regulation) or HIPAA (Health Insurance Portability and Accountability Act). De-identification, meaning the detection, removal or substitution of all sensitive information, is therefore a necessary step to facilitate the sharing of these data between the medical field and research. Over the past decade, various approaches have been proposed to de-identify medical textual data. However, while entity detection is a well-known task in the natural language processing field, it presents some specific challenges in the medical context. Moreover, existing substitution methods proposed in the literature often pay little attention to the medical relevance of de-identified data or are not very resilient to attacks. This paper addresses these challenges. Firstly, an efficient system for detecting sensitive entities in French medical data and then accurately substitute them was implemented. Secondly, robust strategies for generating substitutes that incorporate the medical utility of the data were provided, thereby minimizing the difference in utility between the original and de-identified data, and that mathematically ensure privacy protection. Thirdly, the utility of the de-identification system in a context of ICD-10 code association was evaluated. Finally, various systems developed to tackle ICD-10 code association were presented while providing a state-of-the-art model in French.}
}
@article{HECKLER2022107095,
title = {Machine learning for suicidal ideation identification: A systematic literature review},
journal = {Computers in Human Behavior},
volume = {128},
pages = {107095},
year = {2022},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.107095},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221004180},
author = {Wesllei Felipe Heckler and Juliano Varella {de Carvalho} and Jorge Luis Victória Barbosa},
keywords = {Machine learning, Suicidal ideation identification, Suicide prevention, Mental health, Systematic literature review},
abstract = {Suicide causes approximately one death every 40 s. Suicidal ideation is the first stage in the risk scale, being a potential gate for suicide prevention. Machine learning emerged as a promising tool for helping in preventing suicide through the identification of individuals at risk. Therefore, this paper presents a systematic literature review aiming to answer how machine learning can help in suicidal ideation identification. This study addresses the state-of-the-art for this research field by filtering 4,002 articles from eleven databases published up to February 2021. We analyzed the 54 filtered articles to explore twelve research questions, addressing techniques, data, devices, explainability, and additional resources. We propose a taxonomy of machine learning techniques explored in this area and a taxonomy for highlighting the current research challenges. This review found a growing interest in suicidal ideation in the last few years. In a general way, studies explored data from social media and performed a text analysis to investigate suicidal tendencies in the individuals' language. Moreover, deep learning models seem to be a tendency in this area nowadays. Future studies in suicidal ideation should investigate generic and proactive models that do not depend on users' self-report.}
}
@article{MORADI202061,
title = {CaaSSET: A Framework for Model-Driven Development of Context as a Service},
journal = {Future Generation Computer Systems},
volume = {105},
pages = {61-95},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19305977},
author = {Hossein Moradi and Bahman Zamani and Kamran Zamanifar},
keywords = {Context as a Service (CaaS), Model-Driven Development (MDD), Context service, Software as a Service (SaaS), Domain Specific Language (DSL), Pervasive computing},
abstract = {Developing a stand-alone context-aware application is costly and time-consuming due to the diversity and complexity of the requirements. Context-as-a-Service (CaaS) seeks to overcome the issues by following the idea of Software-as-a-Service (SaaS). The CaaS delivery model attempts to separate context service consumers from context service providers. However, CaaS shifts the complexities of context service provisioning and management from consumers toward providers. Considering the merits of Model-Driven Development (MDD) in coping with the complexities of problems in different domains, we leverage MDD in the CaaS domain and propose the CaaSSET framework to ease the development of context services. A prerequisite for realizing this aim is to specify the CaaS domain as a meta-model. Our CaaSSET framework consists of a CaaS reference model, a context service meta-model, a graphical modeling tool, a code generation tool, and a context service management tool. Using CaaSSET, the context services are modeled in a graphical environment and transformed into executable context web services (semi-) automatically. To evaluate the applicability of the CaaSSET framework, we have modeled and developed a tourism context service as a case study. We have also compared the framework with the related work based on 44 evaluation criteria, which we have extracted from 14 context service requirements.}
}
@incollection{MASSEROLI20191092,
title = {Integrative Bioinformatics},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {1092-1098},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20388-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338203889},
author = {Marco Masseroli},
keywords = {Data integration, Data warehousing, Federated databases, Information linkage, Mediator-based integration, Multi-databases, Network-based integration, Semantic integration},
abstract = {The increasingly reliable and affordable high-throughput production of many different types of biomolecular data, and the development of pipelines for their automatic processing and annotation, have further stressed the need for integrative structures able to simplify accessibility and decrease redundancy and variety of semantic representations of life science data. Integrative bioinformatics addresses these aspects taking advantage of several different approaches and implementations proposed to integrate distributed heterogeneous data, with notable applications in the biological domain; they include information linkage, federated databases, multi-databases, mediator-based solutions, and data warehousing. Additionally, ontology-based, statistical and network-based data integration can also be effectively used for better biologically-driven data integration.}
}
@article{PREMPEH2022100248,
title = {Polishing the pearls of indigenous knowledge for inclusive social education in Ghana},
journal = {Social Sciences & Humanities Open},
volume = {5},
number = {1},
pages = {100248},
year = {2022},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2022.100248},
url = {https://www.sciencedirect.com/science/article/pii/S259029112200002X},
author = {Charles Prempeh},
keywords = {Education, Ghana, Tradition, Modernity, Akan culture},
abstract = {The goal of my paper is to argue for the need to polish the pearls of Akan indigenous wisdom as creative cultural eclecticism to constitute a theoretical base for destabilising cultural imperialism and ontological individualism in favour of inclusive social education. Since the sixteenth century, European expansion to the rest of the world, particularly, Africa has imposed an European single narrative on Africans. This is to the extent that, as a result of the colonialization of Africa, western cultural values have been homogenised and hegemonized as a template for the rest of the world. Consequently, the cultural uniqueness of different African societies has been marginalised – leading to the imposition of a western education system that undermines indigenous knowledge systems of Africa for collective living. And as part of the western dualization of time into “modernity” and “tradition”, African cultures have been profiled as “tradition” and concurrently relegated to the background. Through the deployment of the Akan historical philosophy “Tete wo bi kyere” and “Sankofa” as creative cultural eclecticism given Ghana's cultural pluralism, my paper provides a theoretical foundation for the need to integrate the Akan indigenous values of inclusiveness to foster inclusive social education. I argue that theory is as important as practice and activism in re-imaging social justice education that upsets neoliberal individualism embedded in western education.}
}
@article{RUSSO20181341,
title = {Technical problem identification for supervised state of the art},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {1341-1346},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.344},
url = {https://www.sciencedirect.com/science/article/pii/S240589631831468X},
author = {Davide Russo and Paolo Carrara and Giancarlo Facoetti},
keywords = {problem identification, information retrieval, syntactic dependency pattern, state of the art},
abstract = {This paper presents a method for extracting technical information from a patent pool. It was designed to support the construction of the state of the art of a technology or a product/process by automatically identifying the list of problems that the inventors have faced. The method is based on a strict ontology, which defines what a patent problem is, and a set of IR strategies, which identify all alternative ways adopted in the pool to describe problems. More in detail, the authors propose a set of syntactic dependency patterns, and lemmas in order to extract only the sentences including the information dealing with problems. The output is a coarse list of technical problems, automatically extracted without the user being an expert in the problems of the sector. An exemplary case dealing with injection molding field is proposed.}
}
@article{WELLS201881,
title = {Mind the gap: Bridging the two cultures with complex thought},
journal = {Ecological Complexity},
volume = {35},
pages = {81-97},
year = {2018},
note = {Rosennean Complexity},
issn = {1476-945X},
doi = {https://doi.org/10.1016/j.ecocom.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1476945X17301083},
author = {Jennifer Wells},
keywords = {Complex, Paradox, Pluralism, Perspectivism, Anthropocene, Transition},
abstract = {Robert Rosen’s work has implications for biology and the natural sciences, but also for vital issues in society, economics and politics. Today’s dominant knowledge paradigm and Anthropocene crisis are two sides of one coin: extant flaws of modern thought are part and parcel of the economic and political ideas and institutions driving both social and environmental global crises. Rosennean relational biology and Morinian complex thought shift the knowledge paradigm from modernity towards complexity, working to transcend the ontological flaws underlying the sciences to better grasp and address the social drivers of global crises. Rosen’s work bridges gaps between physics and biology, and between the sciences and humanities. Rosen's work indicates that the dominant posture of monist physicalism must be transcended with a more pluralist methodological articulation within and across disciplines. I focus on just three concepts that help to evoke and advance Rosennean complexity throughout the sciences and humanities: paradox, pluralism, and perspectivism. These three concepts help to demonstrate how Rosen’s work advances biology, but also, more broadly, how to more fully make crucial bridges across the two cultures gap, articulate transdisciplinary knowledge, and thus advance social, economic and political dimensions of urgent societal transition.}
}
@article{FARGHALY2019489,
title = {BIM-linked data integration for asset management},
journal = {Built Environment Project and Asset Management},
volume = {9},
number = {4},
pages = {489-502},
year = {2019},
issn = {2044-124X},
doi = {https://doi.org/10.1108/BEPAM-11-2018-0136},
url = {https://www.sciencedirect.com/science/article/pii/S2044124X19000465},
author = {Karim Farghaly and F.H. Abanda and Christos Vidalakis and Graham Wood},
keywords = {BIM, Building maintenance, Asset management, Information management, Information exchange, Building lifecycle},
abstract = {Purpose
The purpose of this paper is to investigate the transfer of information from the building information modelling (BIM) models to either conventional or advanced asset management platforms using Linked Data. To achieve this aim, a process for generating Linked Data in the asset management context and its integration with BIM data is presented.
Design/methodology/approach
The research design employs a participatory action research (PAR) approach. The PAR approach utilized two qualitative data collection methods, namely; focus group and interviews to identify and evaluate the required standards for the mapping of different domains. Also prototyping which is an approach of Software Development Methodology is utilized to develop the ontologies and Linked Data.
Findings
The proposed process offers a comprehensive description of the required standards and classifications in construction domain, related vocabularies and object-oriented links to ensure the effective data integration between different domains. Also the proposed process demonstrates the different stages, tools, best practices and guidelines to develop Linked Data, armed with a comprehensive use case Linked Data generation about building assets that consume energy.
Originality/value
The Linked Data generation and publications in the domain of AECO is still in its infancy and it also needs methodological guidelines to support its evolution towards maturity in its processes and applications. This research concentrates on the Linked Data applications with BIM to link across domains where few studies have been conducted.}
}
@incollection{NITHYAKALYANI2019113,
title = {Chapter 7 - Speech Summarization for Tamil Language},
editor = {Nilanjan Dey},
booktitle = {Intelligent Speech Signal Processing},
publisher = {Academic Press},
pages = {113-138},
year = {2019},
isbn = {978-0-12-818130-0},
doi = {https://doi.org/10.1016/B978-0-12-818130-0.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128181300000076},
author = {A. NithyaKalyani and S. Jothilakshmi},
keywords = {Tamil language, Speech data, Summarization, Speech recognition, Evaluation metrics},
abstract = {In the education field, the amount of information in the form of audio and video recordings is available for every topic of interest and has gained a lot of research interest in summarization. Summarization is defined as a series of actions performed to express information in a concise form that can help users review the information available from a huge quantity of multimedia content in a shorter span of time. Summarization of speech files can be an effective mechanism to manage the large volume of information available in audio recordings. Summarization of a speech document usually addresses the following problems: generating a transcript from the input speech data, summarization, and rendering the output. The output can be in the form of either speech or text. In case of speech, prosodic information such as emotion of speakers that is conveyed only by speech can be presented and in case of text, hearing impaired people will benefit. This chapter discusses certain approaches that have been developed so far for extractive and abstractive speech summarization, and investigates speech summarization in the Indian language, a domain that has not been explored thus far. Also, this chapter discusses various speech recognition techniques, and their performance on recognizing Tamil speech data is analyzed. Various features used in the summarization of a spoken document are described, and related work on summarizing Tamil documents is presented.}
}
@article{DUSHKIN2022149,
title = {The Structure of Associative Heterarchical Memory},
journal = {Procedia Computer Science},
volume = {213},
pages = {149-156},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.050},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922017410},
author = {Roman V. Dushkin and Vasilisa A. Lelekova and Vladimir Y. Stepankov and Sandra Fadeeva},
keywords = {artificial intelligence, natural language processing, associative heterarchical memory, AI-agent, abstract symbols, hypernet, predicate symbol control model, actant role classifier, hypergraph},
abstract = {Natural language processing by artificial intelligence (NLP) remains an urgent problem of our time. The main task of NLP is to create programs capable of processing and understanding natural languages. To solve these problems the authors have proposed a new mathematical formalism - the associative-heterarchical memory (AH-memory), which structure and functioning are based both on bionic principles and on the achievements of both paradigms of artificial intelligence. This article will provide a comprehensive description of AH-memory. The article will be of interest to developers of artificial intelligence and specialists in the field of NLP.}
}
@article{NATHANI2025125573,
title = {Targeting EGFR-TKI resistance in lung cancer: Role of miR-5193/miR-149-5p loaded NK-EVs and Carboplatin combination},
journal = {International Journal of Pharmaceutics},
volume = {675},
pages = {125573},
year = {2025},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2025.125573},
url = {https://www.sciencedirect.com/science/article/pii/S0378517325004107},
author = {Aakash Nathani and Li Sun and Yan Li and Jassy Lazarte and Mounika Aare and Mandip Singh},
keywords = {NK-EVs, miR-5193, miR-149-5p, PD-L1/PD-1 axis, FOXM1, Osimertinib resistance, EGFR mutation, NSCLC},
abstract = {Lung cancer remains the leading cause of cancer-related deaths, and there is an urgent need for innovative therapies. MicroRNA (miRNA)-based gene therapy has shown promise, but efficient delivery systems are required for its success. This study investigates the use of extracellular vehicles (EVs) secreted by natural killer (NK) cells as delivery systems for miRNAs targeting PD-L1/PD-1 immune checkpoint and FOXM1, in combination with Carboplatin, to enhance anticancer efficacy in lung cancer models. NK-EVs were isolated from NK92-MI cells and characterized using nanoparticle tracking analysis (NTA), proteomics and Western blotting, confirming their exosomal characteristics. Gene ontology profiling and RNA-seq identified highly expressed miRNAs such as miR-5193 and miR-149-5p, which were loaded into NK-EVs via electroporation. Agarose gel electrophoresis confirmed their entrapment and Quickdrop spectrophotometer was used to estimate the quantity. In vitro, miRNA-loaded NK-EVs demonstrated significant cytotoxicity against Osimertinib-resistant PDX (TM0019, Jackson Labs) and H1975R (with L858R mutations) lung cancer cells, with approximately 1.2 to 1.6-fold (p < 0.01) decrease in cell viability compared to NK-EVs alone. In vivo, the combination of miRNA-loaded NK-EVs and Carboplatin significantly reduced tumor volumes (3.5 to 4-fold, p < 0.001) in PDX and H1975R xenograft models, with the most pronounced effect observed in combination therapies. Western blot analysis showed downregulation of tumor-associated markers: PD-1/PD-L1, FOXM1, Survivin, NF-κB and others vs untreated group, p < 0.001) suggesting immune checkpoint inhibition, apoptosis and anti-inflammatory activity. These findings highlight the potential of NK-EVs as effective carriers for miRNAs in combination with chemotherapy, offering a promising therapeutic strategy for NSCLC with EGFR mutations.}
}
@article{GEARIN2023116171,
title = {Moving beyond a figurative psychedelic literacy: Metaphors of psychiatric symptoms in ayahuasca narratives},
journal = {Social Science & Medicine},
volume = {334},
pages = {116171},
year = {2023},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2023.116171},
url = {https://www.sciencedirect.com/science/article/pii/S0277953623005282},
author = {Alex K. Gearin},
keywords = {Ayahuasca, Metaphor, Ontology, Narrative medicine, Integration, Psychedelic therapy},
abstract = {Metaphors, analogies, and similes commonly appear in narratives of drinking the potent psychedelic “ayahuasca”, presenting an intriguing transcultural pattern. Based upon survey and field research at an ayahuasca healing center in Pucallpa, Peru, the article investigates conceptual metaphors in narratives of ayahuasca experiences made by the visiting international guests. Bodily metaphors and visionary analogies frequently appear in narrative plots where they can express the reappraisal, overcoming, and sometimes emboldening of symptoms diagnosed by psychiatry. Moving beyond the literal-figurative divide, the article explores the intrinsic “metaphoricity” of psychedelic experiences and advocates for a literacy of conceptual metaphors regarding both clinical and non-clinical psychedelic narratives. Developing this literacy can broaden approaches in psychedelic psychiatry that analyze and treat syndromes and disorders, while also being applicable to social science and humanities research that examine psychoactive drug use beyond medical frameworks.}
}
@article{ZHITOMIRSKYGEFFET20201459,
title = {A new framework for ethical creation and evaluation of multi-perspective knowledge organization systems},
journal = {Journal of Documentation},
volume = {76},
number = {6},
pages = {1459-1471},
year = {2020},
issn = {0022-0418},
doi = {https://doi.org/10.1108/JD-04-2020-0053},
url = {https://www.sciencedirect.com/science/article/pii/S0022041820000648},
author = {Maayan Zhitomirsky-Geffet and Lala Hajibayova},
keywords = {Ethics of care, Diversity, Ontologies, Ethical values},
abstract = {Purpose
This study aims to present a new framework for ethical creation and evaluation of multi-perspective knowledge organization systems.
Design/methodology/approach
Applying Held's understanding of the ethics of care, this paper proposes five operative criteria for ethical building and evaluation of multi-perspective knowledge representation and organization systems.
Findings
This paper argues that a carefully designed multipoint view of representation and organization conforms to the proposed ethical criteria and shifts concerns associated with the expectation of neutrality of library information professionals to the necessity to humanize and diversify the representation and organization of knowledge to build inclusive and equitable systems.
Originality/value
This paper presents multi-perspectiveness as key to ethical knowledge organization. The paper proposes a generic taxonomy of the main stages in the creation of multi-perspective knowledge representation and organization systems and demonstrates how to apply the proposed framework in each stage to ensure ethical outcomes.}
}
@article{HAMA2025,
title = {Enhancing Patient Outcome Prediction Through Deep Learning With Sequential Diagnosis Codes From Structured Electronic Health Record Data: Systematic Review},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/57358},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125003875},
author = {Tuankasfee Hama and Mohanad M Alsaleh and Freya Allery and Jung Won Choi and Christopher Tomlinson and Honghan Wu and Alvina Lai and Nikolas Pontikos and Johan H Thygesen},
keywords = {deep learning, electronic health records, EHR, diagnosis codes, prediction, patient outcomes, systematic review},
abstract = {Background
The use of structured electronic health records in health care systems has grown rapidly. These systems collect huge amounts of patient information, including diagnosis codes representing temporal medical history. Sequential diagnostic information has proven valuable for predicting patient outcomes. However, the extent to which these types of data have been incorporated into deep learning (DL) models has not been examined.
Objective
This systematic review aims to describe the use of sequential diagnostic data in DL models, specifically to understand how these data are integrated, whether sample size improves performance, and whether the identified models are generalizable.
Methods
Relevant studies published up to May 15, 2023, were identified using 4 databases: PubMed, Embase, IEEE Xplore, and Web of Science. We included all studies using DL algorithms trained on sequential diagnosis codes to predict patient outcomes. We excluded review articles and non–peer-reviewed papers. We evaluated the following aspects in the included papers: DL techniques, characteristics of the dataset, prediction tasks, performance evaluation, generalizability, and explainability. We also assessed the risk of bias and applicability of the studies using the Prediction Model Study Risk of Bias Assessment Tool (PROBAST). We used the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) checklist to report our findings.
Results
Of the 740 identified papers, 84 (11.4%) met the eligibility criteria. Publications in this area increased yearly. Recurrent neural networks (and their derivatives; 47/84, 56%) and transformers (22/84, 26%) were the most commonly used architectures in DL-based models. Most studies (45/84, 54%) presented their input features as sequences of visit embeddings. Medications (38/84, 45%) were the most common additional feature. Of the 128 predictive outcome tasks, the most frequent was next-visit diagnosis (n=30, 23%), followed by heart failure (n=18, 14%) and mortality (n=17, 13%). Only 7 (8%) of the 84 studies evaluated their models in terms of generalizability. A positive correlation was observed between training sample size and model performance (area under the receiver operating characteristic curve; P=.02). However, 59 (70%) of the 84 studies had a high risk of bias.
Conclusions
The application of DL for advanced modeling of sequential medical codes has demonstrated remarkable promise in predicting patient outcomes. The main limitation of this study was the heterogeneity of methods and outcomes. However, our analysis found that using multiple types of features, integrating time intervals, and including larger sample sizes were generally related to an improved predictive performance. This review also highlights that very few studies (7/84, 8%) reported on challenges related to generalizability and less than half (38/84, 45%) of the studies reported on challenges related to explainability. Addressing these shortcomings will be instrumental in unlocking the full potential of DL for enhancing health care outcomes and patient care.
Trial Registration
PROSPERO CRD42018112161; https://tinyurl.com/yc6h9rwu}
}
@article{VASQUEZFERNANDEZ202065,
title = {Resurgence of relationality: reflections on decolonizing and indigenizing ‘sustainable development’},
journal = {Current Opinion in Environmental Sustainability},
volume = {43},
pages = {65-70},
year = {2020},
note = {Indigenous Conceptualizations of ‘Sustainability’},
issn = {1877-3435},
doi = {https://doi.org/10.1016/j.cosust.2020.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877343520300178},
author = {Andrea M Vásquez-Fernández and Cash {Ahenakew pii tai poo taa}},
abstract = {Many Indigenous Peoples around the world find the dominant model of sustainable development disrespectful and hypocritical. The terms ‘sustainable’ and ‘development’ when combined reproduce patterns of exploitation that destroy Mother Earth while imposing a regime of colonial praxis on Indigenous Peoples and Lands under a benevolent appearance of civilization, salvation, novelty, and progress. Sustainable development models are rooted within western paradigms (a specific set of epistemology, ontology, axiology and methodology) such as the neoliberal capitalism approach, which structures relationships with Indigenous Peoples and Land based on disrespectful relationships. In this article, we offer an approach that aspires to be decolonial. First, we examine the current model of ‘sustainable development’ through Indigenous and modernity/coloniality approaches. Second, through Indigenous Peoples’ paradigms, along with the concepts respectful inter-being-relationality and Land revitalization, we provide Indigenous perspectives on ‘sustainability’ and ‘development’ that may strengthen sovereignty and wellbeing.}
}
@article{ZHANG2025106018,
title = {Dual-loop integration framework for model-based system design and reliability analysis using Bayesian networks},
journal = {Results in Engineering},
volume = {27},
pages = {106018},
year = {2025},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2025.106018},
url = {https://www.sciencedirect.com/science/article/pii/S2590123025020900},
author = {Yunpeng Zhang and Jian Chen},
keywords = {Dynamic Bayesian network (DBN), Integrated modeling, Model-based systems engineering (MBSE), Reliability analysis, System design, SysML},
abstract = {Integrating model-based systems engineering (MBSE) and reliability analysis is challenging due to the lack of life-cycle reliability consideration in system design, difficulty in tracking design changes in reliability modeling, and limited quantitative analysis support in system modeling. To address these issues, this paper proposes a dual-loop modeling framework integrating system design and reliability analysis based on models and Bayesian networks. This framework supports a systematic and iterative design process, facilitates quantitative analysis throughout the system life cycle, and enables upstream integration of system-related and mapped reliability parameters from dynamically evolving design architectures at both modeling method and framework levels. Specifically, we propose a novel integrated modeling approach based on existing theories and methods, detailing the methodological, procedural, and linguistic foundations. Furthermore, a mapping method based on SysML activity diagrams basic types is proposed to connect system models with dynamic Bayesian networks (DBN) for quantitative analysis. Finally, the effectiveness of the proposed method is validated through its application to a reaction wheel actuator and a remote sensing satellite project.}
}
@article{DEMEESTER2020946,
title = {Implementation-independent function reuse},
journal = {Future Generation Computer Systems},
volume = {110},
pages = {946-959},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19303723},
author = {Ben {De Meester} and Tom Seymoens and Anastasia Dimou and Ruben Verborgh},
keywords = {Function, Linked data, Reuse},
abstract = {Functions are essential building blocks of information retrieval and information management. However, efforts implementing these functions are fragmented: one function has multiple implementations, within specific development contexts. This inhibits reuse: metadata of functions and associated implementations need to be found across various search interfaces, and implementation integration requires human interpretation and manual adjustments. An approach is needed, independent of development context and enabling description and exploration of functions and (automatic) instantiation of associated implementations. In this paper, after collecting scenarios and deriving corresponding requirements, we (i) propose an approach that facilitates functions’ description, publication, and exploration by modeling and publishing abstract function descriptions and their links to concrete implementations; and (ii) enable implementations’ automatic instantiation by exploiting those published descriptions. This way, we can link to existing implementations, and provide a uniform detailed search interface across development contexts. The proposed model (the Function Ontology) and the publication method following the Linked Data principles using standards, are deemed sufficient for this task, and are extensible to new development contexts. The proposed set of tools (the Function Hub and Function Handler) are shown to fulfill the collected requirements, and the user evaluation proves them being perceived as a valuable asset during software retrieval. Our work thus improves developer experience for function exploration and implementation instantiation.}
}
@article{ZENG2020148,
title = {Implications of Knowledge Organization Systems for Health Information Exchange and Communication during the COVID-19 Pandemic},
journal = {Data and Information Management},
volume = {4},
number = {3},
pages = {148-170},
year = {2020},
issn = {2543-9251},
doi = {https://doi.org/10.2478/dim-2020-0009},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122000468},
author = {Marcia Lei Zeng and Yi Hong and Julaine Clunis and Shaoyi He and L.P. Coladangelo},
keywords = {knowledge organization systems, health terminologies, health information exchange, COVID-19 pandemic},
abstract = {This article aims to review the important roles of health knowledge organization systems (KOSs) during the COVID-19 pandemic. Different types of knowledge organization systems, including term lists, synonym rings, thesauri, subject heading systems, taxonomies, classification schemes, and ontologies are widely recognized and applied in both modern and traditional information systems. Apart from their usage in the management of data, information, and knowledge, KOSs are seen as valuable components for large information architecture, content management, findability improvement, and many other applications. After introducing the challenges of information overload and semantic conflicts, the article reviews the efforts of major health KOSs, illustrates various health coding schemes, explains their usages and implementations, and reveals their implications for health information exchange and communication during the COVID-19 pandemic. Some general examples of the applications, services, and analysis powered by KOSs are presented at the end. As revealed in this article, they have become even more critical to aid the frontline endeavors to overcome the obstacles due to information overload and semantic conflicts that can occur during devastating historic and worldwide events like the COVID-19 pandemic.}
}
@article{KUBLER2020258,
title = {Towards an Automated Product-Production System Design - Combining Simulation-based Engineering and Graph-based Design Languages},
journal = {Procedia Manufacturing},
volume = {52},
pages = {258-265},
year = {2020},
note = {System-Integrated Intelligence – Intelligent, Flexible and Connected Systems in Products and ProductionProceedings of the 5th International Conference on System-Integrated Intelligence (SysInt 2020), Bremen, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.11.043},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321867},
author = {Karl Kübler and Dominik Schopper and Oliver Riedel and Stephan Rudolph},
keywords = {Simulation-based Engineering, Graph-based Design Languages, Virtual Commissioning, Digital Twins, Product-Production Model},
abstract = {In this paper, the authors elaborate a combination of graph-based design and simulation-based engineering into a new concept called Executable Integrative Product-Production Model (EIPPM). Today, the first collaborative process in engineering for all mechatronic disciplines is the virtual commissioning phase. Therefore, Digital Twins (DT) are modeled and run in a simulation. The authors see a hitherto untapped potential for the earlier, integrated and iterative use of DTs in a simulation-based engineering for the development of production systems. Seamless generation of and exchange between Model-, Software- and Hardware-in-the-Loop simulations is necessary. Feedback from simulation results will go into the design decisions after each iteration. The presented approach combines knowledge of the domain "production systems technology" together with the knowledge of the corresponding "product" using a so called Graph-based Design Language (GBDL). Its central data model, which represents the entire life cycle, results of an automatic translation step in a compiler. Since the execution of the GBDL can be repeated as often as desired with modified boundary conditions (e.g. through feedback), a design of experiment is made possible, whereby also unconventional solutions are considered. The novel concept aims at the following advantages: Consistent linking of all mechatronic domains through a data model (graph) from the project start, automatic design cycles exploring multiple variants for optimized product-production system combinations, automatic generation of simulation models starting with the planning phase, feedback from simulation-based optimization back into the data model.}
}
@article{WEI2025273,
title = {Machine learning-assisted retrosynthesis planning: Current status and future prospects},
journal = {Chinese Journal of Chemical Engineering},
volume = {77},
pages = {273-292},
year = {2025},
issn = {1004-9541},
doi = {https://doi.org/10.1016/j.cjche.2024.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S1004954124003720},
author = {Yixin Wei and Leyu Shan and Tong Qiu and Diannan Lu and Zheng Liu},
keywords = {Retrosynthesis planning, Machine learning, Artificial intelligence, Synthetic pathway, Chemoinformatics},
abstract = {Machine learning-assisted retrosynthesis planning aims to utilize machine learning (ML) algorithms to find synthetic pathways for target compounds. In recent years, with the development of artificial intelligence (AI), especially ML, researchers’ interest in ML-assisted retrosynthesis planning has rapidly increased, bringing development and opportunities to the field. In this review, we aim to provide a comprehensive understanding of ML-assisted retrosynthesis planning. We first discuss the formal definition and the objective of retrosynthesis planning, and organize a modular framework which includes four modules: data preparation, data preprocessing, pathway generation and evaluation, and pathway verification. Then, we sequentially review the current status of the first three modules (except pathway verification) in the ML-assisted retrosynthesis planning framework, including ideas, methods, and latest progress. Following that, we specifically discuss large language models in retrosynthesis planning. Finally, we summarize the extant challenges that are faced by current ML-assisted retrosynthesis planning research and offer a perspective on future research directions and development.}
}
@article{ESPOSITO2018136,
title = {A smart mobile, self-configuring, context-aware architecture for personal health monitoring},
journal = {Engineering Applications of Artificial Intelligence},
volume = {67},
pages = {136-156},
year = {2018},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2017.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0952197617302336},
author = {Massimo Esposito and Aniello Minutolo and Rosario Megna and Manolo Forastiere and Mario Magliulo and Giuseppe {De Pietro}},
keywords = {Mobile health monitoring, Software architecture, Smart applications, Context-awareness, Reasoning, Ontologies},
abstract = {The last decade has witnessed an exponential increase in older adult population suffering from chronic life-long diseases and needing healthcare. This situation has highlighted a need to revolutionize healthcare and provide innovative, efficient, and affordable solutions to patients at any time and from anywhere in an economic and friendly manner. The recent developments in sensing, mobile, and embedded devices have attracted considerable attention toward mobile health monitoring applications. However, existing architectures aimed at facilitating the realization of these mobile applications have shown to be not suitable to address all these challenging issues: (i) the seamless integration of heterogeneous devices; (ii) the estimation of vital parameters not measurable directly or measurable with a low accuracy; (iii) the extraction of context information pertaining to the patient’s activity to be used for the interpretation of vital parameters; (iv) the correlation of physiological and contextual information to detect suspicious anomalies and supply alerts; (v) the notification of anomalies to doctors and caregivers only when their detection is accurate and appropriate. In light of the above, this paper presents a smart mobile, self-configuring, context-aware architecture devised to enable the rapid prototyping of personal health monitoring applications for different scenarios, by exploiting commercial wearable sensors and mobile devices as well as knowledge-based technologies. This architecture is organized as a composition of four tiers that operate on a layered fashion and it exploits an ontology-based data model to ensure intercommunication among these tiers and the monitoring applications built on the top of them. The proposed architecture has been implemented for mobile devices equipped with the Android platform and evaluated with respect to its modifiability by employing the ALMA (Architecture Level Modifiability Analysis) method, highlighting its capability of being rapidly customized, personalized or eventually modified by software developers in order to prototype, with a reduced effort, novel health monitoring applications on the top of its components. Finally, it has been employed to build, as case study, a mobile application aimed at monitoring and managing cardiac arrhythmias, such as bradycardia and tachycardia, confirming its effectiveness with respect to a real scenario.}
}
@article{VANDERSCHYFF2024104065,
title = {Privacy policy analysis: A scoping review and research agenda},
journal = {Computers & Security},
volume = {146},
pages = {104065},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104065},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824003705},
author = {Karl {van der Schyff} and Suzanne Prior and Karen Renaud},
keywords = {Privacy policy analysis, Privacy policy classification, Privacy policy benchmarking, Privacy policy completeness, Privacy policy rule, Privacy policy strategy, Machine learning, Privacy policy evaluation, Scoping review},
abstract = {Online users often neglect the importance of privacy policies - a critical aspect of digital privacy and data protection. This scoping review addresses this oversight by delving into privacy policy analysis, aiming to establish a comprehensive research agenda. The study's objective was to explore the analytic techniques employed in privacy policy analysis and to identify the associated challenges. Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses for Scoping Reviews (PRISMA-ScR) checklist, the review selected n = 97 relevant studies. The findings reveal a diverse array of techniques used, encompassing automated machine learning and natural language processing, and manual content analysis. Notably, researchers grapple with challenges like linguistic nuances, ambiguity, and complex data harvesting methods. Additionally, the lack of privacy-centric theoretical frameworks and a dearth of user evaluations in many studies limit their real-world applicability. The review concludes by proposing a set of research recommendations to shape the future research agenda in privacy policy analysis.}
}
@article{WAALER2025100042,
title = {Information Modelling Framework for Digital Engineering},
journal = {Digital Engineering},
volume = {4},
pages = {100042},
year = {2025},
issn = {2950-550X},
doi = {https://doi.org/10.1016/j.dte.2025.100042},
url = {https://www.sciencedirect.com/science/article/pii/S2950550X25000081},
author = {Arild Waaler and Dimitrios {Kyritsis (Kiritsis)}}
}
@article{RATAJCZAK2018118,
title = {Language and value: the philosophy of language in the post-Operaist critique of contemporary capitalism},
journal = {Language Sciences},
volume = {70},
pages = {118-130},
year = {2018},
note = {Karl Marx and the Language Sciences – Critical Encounters},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0388000117303790},
author = {Mikołaj Ratajczak},
keywords = {Philosophy of language, Enunciation, Value, Financialization, Contemporary capitalism, Immaterial labour},
abstract = {This paper offers a discussion of the role played by the philosophy of language in the post-Operaist critique of contemporary capitalism, focussing specifically on the relation between language and questions of labour and value. It argues that the post-Operaist philosophy of language is a philosophy of immaterial labour which enables us to make new diagnoses concerning the contradictions and antagonisms of late, cognitive, financialized capitalism. The first part of the article broadly outlines post-Operaist thinking about language in the context of contemporary Italian philosophy of language. In this part, the argument is made for a shift from using the term ‘philosophy of language’ to the notion of a ‘philosophy of the linguistic faculty’. Part two is devoted to the main aspects of the post-Operaist critique of contemporary capitalism. The third presents an analysis of this post-Operaist philosophy of the linguistic faculty as a philosophy of living, immaterial labour, by examining the post-Operaist view of language within the wider context of Marxist theory, stressing the political character of the post-Operaist approach.}
}
@article{GE2023104458,
title = {Few-shot learning for medical text: A review of advances, trends, and opportunities},
journal = {Journal of Biomedical Informatics},
volume = {144},
pages = {104458},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104458},
url = {https://www.sciencedirect.com/science/article/pii/S153204642300179X},
author = {Yao Ge and Yuting Guo and Sudeshna Das and Mohammed Ali Al-Garadi and Abeed Sarker},
keywords = {Few-shot learning, Natural language processing, Machine learning, Biomedical informatics},
abstract = {Background:
Few-shot learning (FSL) is a class of machine learning methods that require small numbers of labeled instances for training. With many medical topics having limited annotated text-based data in practical settings, FSL-based natural language processing (NLP) holds substantial promise. We aimed to conduct a review to explore the current state of FSL methods for medical NLP.
Methods:
We searched for articles published between January 2016 and October 2022 using PubMed/Medline, Embase, ACL Anthology, and IEEE Xplore Digital Library. We also searched the preprint servers (e.g., arXiv, medRxiv, and bioRxiv) via Google Scholar to identify the latest relevant methods. We included all articles that involved FSL and any form of medical text. We abstracted articles based on the data source, target task, training set size, primary method(s)/approach(es), and evaluation metric(s).
Results:
Fifty-one articles met our inclusion criteria—all published after 2018, and most since 2020 (42/51; 82%). Concept extraction/named entity recognition was the most frequently addressed task (21/51; 41%), followed by text classification (16/51; 31%). Thirty-two (61%) articles reconstructed existing datasets to fit few-shot scenarios, and MIMIC-III was the most frequently used dataset (10/51; 20%). 77% of the articles attempted to incorporate prior knowledge to augment the small datasets available for training. Common methods included FSL with attention mechanisms (20/51; 39%), prototypical networks (11/51; 22%), meta-learning (7/51; 14%), and prompt-based learning methods, the latter being particularly popular since 2021. Benchmarking experiments demonstrated relative underperformance of FSL methods on biomedical NLP tasks.
Conclusion:
Despite the potential for FSL in biomedical NLP, progress has been limited. This may be attributed to the rarity of specialized data, lack of standardized evaluation criteria, and the underperformance of FSL methods on biomedical topics. The creation of publicly-available specialized datasets for biomedical FSL may aid method development by facilitating comparative analyses.}
}
@article{SU2024822,
title = {Information Modelling of Extrusion-based 3D Printing Process for Digital Twins},
journal = {Procedia CIRP},
volume = {128},
pages = {822-827},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.04.025},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124007923},
author = {Shuo Su and Aydin Nassehi and Mark Goudswaard and Ben Hicks},
keywords = {Digital twins, IDEF0 model, Petri net, Material extrusion process},
abstract = {This paper focuses on the information modelling of the extrusion-based 3D printing process. It aims to facilitate the creation and management of a digital twin (DT) through a comprehensive understanding of the manufacturing process. The workflow of extrusion-based 3D printing, spanning from the CAD model to the printed part, is analysed and modelled using the Integration Definition for Process Modelling 0 (IDEF0) method. Building upon the IDEF0 model and relevant literature, a Petri net is developed to identify specific information and logic inherent to the printing process. The identified elements are subsequently filtered by the specific purpose of the DT and categorized into three classes: Component, Activity, and Property. Specifically, this study explores the material extrusion (MEX) process as implemented by the Prusa i3 MK3 printer. Derived from functional flow block diagrams and mathematical formulas, the information model consists of 21 components, 25 activities and 83 properties. This model can be considered as an information repository, which enables DTs to consider, select and synchronise specific physical information elements in their digital representation within the context of the MEX process.}
}
@article{LIU2025103392,
title = {More attention for computer-aided conceptual design: A multimodal data-driven interactive design method},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103392},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103392},
url = {https://www.sciencedirect.com/science/article/pii/S147403462500285X},
author = {Zhixin Liu and Shanhe Lou and Yixiong Feng and Wenhui Huang and Bingtao Hu and Chengyu Lu and Jianrong Tan},
keywords = {Conceptual design, Multimodal data, Multimodal transformer, Knowledge recommendation},
abstract = {Computer-aided conceptual design (CACD) is a core means for the development of new products, as it can materialize designers’ inherent thinking. However, when designers encounter stagnation during CACD, they need to consult third-party design knowledge to seek inspiration, which frequently disrupts their design thinking process. Deep learning-empowered design methods and design knowledge management can be a potential solution to address these issues. This study proposes a multimodal design data-driven interactive design method. Multimodal data are utilized to identify the designer’s implicit intentions while design attention is abstracted to match relevant knowledge as computer feedback. It achieves the “designer-computer-designer” closed-loop interactive design through the mediation of design attention. The multimodal design data (design images and design descriptions) is obtained through sketch modeling and verbal protocol analysis experiments. A multimodal Transformer based on T2T-ViT and Bert (TB-Multiformer) is constructed to capture multimodal features to identify conceptual design intentions by utilizing cross-modal design attention modules and self-design attention modules. Since the identified attention can be used to match the knowledge that designers are more concerned about, an attention-based design knowledge recommendation method (AbDKR) is proposed to provide proactive knowledge feedback. It can prevent designers from spending time searching for design knowledge and helps them maintain sufficient inspiration. A case study on the conceptual design of two types of mechanical structure is conducted to illustrate the feasibility and practicability of the proposed approach.}
}
@article{ETTALEB2018768,
title = {A Combination of Reduction and Expansion Approaches to Deal with Long Natural Language queries},
journal = {Procedia Computer Science},
volume = {126},
pages = {768-777},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918312894},
author = {Mohamed ETTALEB and Chiraz LATIRI and Patrice BELLOT},
keywords = {Quey Expansion, association rules, Verbose Query Reduction, Social Book Search},
abstract = {Most of the queries submitted to search engines are composed of keywords but it is not enough for users to express their needs. Through verbose natural language queries, users can express complex or highly specific information needs. However, it is difficult for search engine to deal with this type of queries. Moreover, the emergence of social medias allows users to get opinions, suggestions or recommendations from other users about complex information needs. In order to increase the understandability of user needs, tasks as the CLEF Social Book Search Suggestion Track have been proposed from 2011 to 2016. The aim is to investigate techniques to support users in searching for books in catalogs of professional metadata and complementary social media. In this context, we introduce in the current paper a statical approach to deal with long verbose queries in Social Information Retrieval (SIR) by taking Social Book Search(SBS) as a study case. firstly, a morphosyntactic analysis was introduced to reduce verbose queries, the second step is based on expanding the reduced queries using association rules mining combined with Pseudo relevance feedback. Experiments on SBS 2014 and 2016 collections show significant improvement in the retrieval performance.}
}
@incollection{DEWOSKIN2024779,
title = {Virtual models (aka: in silico or computational models)},
editor = {Philip Wexler},
booktitle = {Encyclopedia of Toxicology (Fourth Edition)},
publisher = {Academic Press},
edition = {Fourth Edition},
address = {Oxford},
pages = {779-793},
year = {2024},
isbn = {978-0-323-85434-4},
doi = {https://doi.org/10.1016/B978-0-12-824315-2.00094-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243152000944},
author = {Robert S. DeWoskin and Thomas B. Knudsen and Imran Shah},
keywords = {Adverse outcome pathways, Computational model, Emergent properties, In silico models, Microphysiological systems (MPS), PBPK models, Physiome project, Systems biology, Virtual embryo, Virtual liver, Virtual model (vM), Virtual physiological human},
abstract = {Virtual models (vM) are mathematical representations of biological processes that are numerically solved computationally, and are used to investigate and predict system behaviors that cannot be predicted solely from studying the nature of the individual parts, or from the domain of the available data. Computational power is now available to develop advanced vMs capable of supporting predictive toxicology and drug efficacy, and of reducing the dependence on in vivo animal studies for basic research and risk assessment purposes. The ultimate goal is to simulate in vivo responses of biological organisms to environmental change, drugs, toxins, or human activities, and to predict the effects of defined perturbations on system behaviors. Examples of virtual models are presented from research in the fields of physiology, pharmacology, toxicology and risk assessment.}
}
@article{HOLZINGER202316,
title = {AI for life: Trends in artificial intelligence for biotechnology},
journal = {New Biotechnology},
volume = {74},
pages = {16-24},
year = {2023},
issn = {1871-6784},
doi = {https://doi.org/10.1016/j.nbt.2023.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1871678423000031},
author = {Andreas Holzinger and Katharina Keiblinger and Petr Holub and Kurt Zatloukal and Heimo Müller},
keywords = {Artificial Intelligence, Biotechnology, Deep Learning, Digital Transformation, Machine Learning},
abstract = {Due to popular successes (e.g., ChatGPT) Artificial Intelligence (AI) is on everyone's lips today. When advances in biotechnology are combined with advances in AI unprecedented new potential solutions become available. This can help with many global problems and contribute to important Sustainability Development Goals. Current examples include Food Security, Health and Well-being, Clean Water, Clean Energy, Responsible Consumption and Production, Climate Action, Life below Water, or protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss. AI is ubiquitous in the life sciences today. Topics include a wide range from machine learning and Big Data analytics, knowledge discovery and data mining, biomedical ontologies, knowledge-based reasoning, natural language processing, decision support and reasoning under uncertainty, temporal and spatial representation and inference, and methodological aspects of explainable AI (XAI) with applications of biotechnology. In this pre-Editorial paper, we provide an overview of open research issues and challenges for each of the topics addressed in this special issue. Potential authors can directly use this as a guideline for developing their paper.}
}
@article{ARASTOOPOOR2018430,
title = {Domain-specific readability measures to improve information retrieval in the Persian language},
journal = {The Electronic Library},
volume = {36},
number = {3},
pages = {430-444},
year = {2018},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-01-2017-0007},
url = {https://www.sciencedirect.com/science/article/pii/S026404731800022X},
author = {Sholeh Arastoopoor},
keywords = {Information retrieval, Document cohesion, Document scope, Flesch–Dayani formula, Persian, Re-ranking search results, Readability scores},
abstract = {Purpose
The degree to which a text is considered readable depends on the capability of the reader. This assumption puts different information retrieval systems at the risk of retrieving unreadable or hard-to-be-read yet relevant documents for their users. This paper aims to examine the potential use of concept-based readability measures along with classic measures for re-ranking search results in information retrieval systems, specifically in the Persian language.
Design/methodology/approach
Flesch–Dayani as a classic readability measure along with document scope (DS) and document cohesion (DC) as domain-specific measures have been applied for scoring the retrieved documents from Google (181 documents) and the RICeST database (215 documents) in the field of computer science and information technology (IT). The re-ranked result has been compared with the ranking of potential users regarding their readability.
Findings
The results show that there is a difference among subcategories of the computer science and IT field according to their readability and understandability. This study also shows that it is possible to develop a hybrid score based on DS and DC measures and, among all four applied scores in re-ranking the documents, the re-ranked list of documents based on the DSDC score shows correlation with re-ranking of the participants in both groups.
Practical implications
The findings of this study would foster a new option in re-ranking search results based on their difficulty for experts and non-experts in different fields.
Originality/value
The findings and the two-mode re-ranking model proposed in this paper along with its primary focus on domain-specific readability in the Persian language would help Web search engines and online databases in further refining the search results in pursuit of retrieving useful texts for users with differing expertise.}
}
@article{DEARAUJO2025160872,
title = {Recent developments in the use of machine learning in catalysis: A broad perspective with applications in kinetics},
journal = {Chemical Engineering Journal},
volume = {508},
pages = {160872},
year = {2025},
issn = {1385-8947},
doi = {https://doi.org/10.1016/j.cej.2025.160872},
url = {https://www.sciencedirect.com/science/article/pii/S1385894725016936},
author = {Leandro Goulart {de Araujo} and Léa Vilcocq and Pascal Fongarland and Yves Schuurman},
keywords = {Catalytic reaction, Catalyst, Catalysis informatics, Machine learning},
abstract = {A thorough grasp of the underlying mechanisms of catalytic reactions is indispensable for furthering our understanding of chemical kinetics. However, traditional phenomenological models present certain difficulties, including the tendency to converge to local minima and a reliance on parameters that are difficult to measure, particularly in complex catalytic systems. These systems frequently comprise intricate feedstock compositions or catalyst structures that are challenging to anticipate through theory-driven approaches. This often results in the utilization of unrealistic models or the allocation of considerable computational resources. While traditional methods offer valuable insights, they are constrained by these challenges and the lack of robust uncertainty assessments. In view of these limitations, data-driven modeling, in particular through machine learning (ML), has emerged as a promising alternative in catalysis in the last five years. This review examines recent advancements in ML applications within the field of catalysis, encompassing a broad range of applications, including data generation, descriptor identification, and feature engineering. While the review takes a general perspective on ML in catalysis, particular attention is given to applications in chemical kinetics wherever relevant, recognizing the interconnection between reaction kinetics, catalyst design, reaction conditions, and reactor configurations. The discussion includes various ML models, including interpretable yet less flexible models and more complex black-box models, and considers their applications in catalysis. It also examines key factors in model selection, such as generalizability, computational efficiency, data quality, and interpretability. Finally, it outlines future directions for ML in catalysis, emphasizing how these technologies can further enhance the optimization, design, and improvement of catalytic systems.}
}
@article{HOFMAN201955,
title = {A Methodological Approach for Development and Deployment of Data Sharing in Complex Organizational Supply and Logistics Networks with Blockchain Technology},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {3},
pages = {55-60},
year = {2019},
note = {15th IFAC Symposium on Large Scale Complex Systems LSS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319300941},
author = {Wout J. Hofman},
keywords = {complex organizational networks, supply, logistics, supply chain visibility, blockchain technology},
abstract = {Each manufacturer, supplier, and retailer has its own chain of collaborating stakeholders to meet their customer demands. Many perspectives can be taken like food safety, security, and sustainability, which may each lead to separate solutions that are not necessarily interoperable with each other. Data can only be shared within the context of those solutions and only with extra effort, and thus costs, across these solutions. To address this solution, this paper proposes a methodological approach for specification of data that can be shared with rapid deployment by for instance a blockchain based - or a peer-to-peer infrastructure. The methodological approach is based on basic informatics principles like the Turing machine and ontologies.}
}
@article{PAN2020122819,
title = {Knowledge, attitude and practice towards zero carbon buildings: Hong Kong case},
journal = {Journal of Cleaner Production},
volume = {274},
pages = {122819},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.122819},
url = {https://www.sciencedirect.com/science/article/pii/S095965262032864X},
author = {Mi Pan and Wei Pan},
keywords = {Zero carbon building, Carbon emission, Sustainable practice, Energy efficiency, Stakeholder, Hong Kong},
abstract = {The ‘zero carbon building (ZCB)’ approach has been hailed as an innovative strategy for addressing climate change and achieving a low carbon society. There has been burgeoning research on the technical aspects of ZCBs. However, the aspect of stakeholder engagement, albeit imperative for zero carbon development, has been largely overlooked in the literature. This paper examines stakeholder knowledge, attitude and practice (KAP) with regard to ZCBs in a systems manner, employing a new KAP model that integrates ontology, axiology and epistemology as the theoretical lens. The research was carried out through a questionnaire survey and focus group discussions with multiple stakeholders in the building industry in Hong Kong, which is a typical example of a high-density city. The results indicate the generally ambiguous and inconsistent knowledge, relatively negative attitude, and poor practice of stakeholders towards ZCBs. The statistical analysis findings reveal that differentiation exists between different stakeholder groups as to their knowledge, and this highlights the significant role that explicit ZCB knowledge plays in creating a knowledge-induced attitude that favours changes in practice leading to zero carbon developments. The applicability of the developed KAP model is verified using the case of Hong Kong, enabling the dialectics and complexities of stakeholder KAP towards ZCBs to be further clarified. Practical implications are provided to enhance the knowledge, shape the attitudes, and encourage the practices of building towards zero carbon in future cities.}
}
@article{EGHAN2020106197,
title = {The missing link – A semantic web based approach for integrating screencasts with security advisories},
journal = {Information and Software Technology},
volume = {117},
pages = {106197},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.106197},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919302046},
author = {Ellis E. Eghan and Parisa Moslehi and Juergen Rilling and Bram Adams},
keywords = {Crowd-based documentation, Mining video content, Software security vulnerabilities, Software dependencies, Software traceability, Semantic knowledge modeling, Semantic web},
abstract = {Context
Collaborative tools and repositories have been introduced to facilitate open source software development, allowing projects, developers, and users to share their knowledge and expertise through formal and informal channels such as repositories, Q&A websites, blogs and screencasts. While significant progress has been made in mining and cross-linking traditional software repositories, limited work exists in making multimedia content in the form of screencasts or audio recordings an integrated part of software engineering processes.
Objective
The objective of this research is to provide a standardized ontological representation that allows for a seamless knowledge integration of screencasts with other software artifacts across knowledge resource boundaries.
Method
In this paper, we propose a modeling approach that takes advantage of the Semantic Web and its inference services to capture and establish traceability links between knowledge extracted from different resources such as vulnerability information in NVD, project dependency information from Maven Central, and YouTube screencasts.
Results
We performed a case study on 48 videos that illustrate attacks on vulnerable systems and show that our approach can successfully link relevant vulnerabilities and screencasts with an average precision of 98% and an average recall of 54% when vulnerability identifiers (CVE ID) are explicitly mentioned in the metadata (title and description) of videos. When no CVE ID is present, our initial results show that for a reduced search space (for one vulnerability), using only the textual content of the image frames, our approach is still able to link video-vulnerability pairs and rank the correct result within the top two positions of the result set.
Conclusion
Our approach not only establishes bi-directional, direct, and indirect traceability links from screencasts to these other software artifacts; these links can also be used to guide practitioners in comprehending the potential security impact of vulnerable components in their projects.}
}
@article{XU2024111189,
title = {Cross-domain coreference modeling in dialogue state tracking with prompt learning},
journal = {Knowledge-Based Systems},
volume = {283},
pages = {111189},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.111189},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123009395},
author = {Heng-Da Xu and Xian-Ling Mao and Puhai Yang and Fanshu Sun and Heyan Huang},
keywords = {Task-oriented dialogue, Dialogue state tracking, Cross-domain coreference, Prompt learning},
abstract = {Dialogue state tracking (DST) is to identify user goals from dialogue context and it is an essential component in task-oriented dialogue systems. In multi-domain task-oriented dialogues, a user often refers to the information mentioned in the previous context when the topic is transferred to another domain. Accurate modeling of the cross-domain coreference plays an important role in building qualified DST systems. As far as we know, there is only one work attempting to model the cross-domain coreference phenomenon in DST. However, it still has low efficiency and suffers from the difficulty of handling the complex reasoning over the dialogue context. To tackle these problems, in this paper, we propose a simple but effective DST model, called Coref-DST, to track the cross-domain coreference slots. Instead of predicting the actual values via complex reasoning, Coref-DST directly identifies the coreferred domains and slots from the dialogue context. Moreover, a domain-specific prompt method is proposed to predict all the slot values in a domain simultaneously, so as to better capture the relationship among the slots. The extensive experimental results on the MultiWOZ 2.3 dataset demonstrate that Coref-DST not only outperforms the state-of-the-art DST baselines but also has higher efficiency of training and inference.}
}
@article{VATHANALAOHA20221,
title = {Spatialisation of Text Worlds: Contrastive Interpretations in P.L. Travers’s Mary Poppins},
journal = {Manusya: Journal of Humanities},
volume = {25},
number = {1},
pages = {1-22},
year = {2022},
issn = {0859-9920},
doi = {https://doi.org/10.1163/26659077-25010018},
url = {https://www.sciencedirect.com/science/article/pii/S0859992022000112},
author = {Kriangkrai Vathanalaoha},
keywords = {Text World Theory, discourse world, deictic shift, narrative, },
abstract = {Fictional situations recounted by a narrator instantaneously trigger a mental representation of a text world (Gavins 2007) which aims to explicate how meaning is achieved by readers based on salient theories of stylistics. This study examines how text world creation is established while reading Mary Poppins (Travers 2014). Crucial excerpts are explored to explain how readers perceive events that constitute the narrative. As the author recounts all events through an omniscient perspective, a discourse world is established through schematic data of all participants in the discourse based on common ground information. Deictic shift (Segal 1995) is employed to demonstrate how a mental representation is spatially situated and to achieve rich presentations of the fictional world. The present study concludes that text-world approaches to Mary Poppins could explain interpretive controversies between the novel and the film, since participants, schema and ontological distance trigger spatialisation of the fictional worlds.}
}
@article{ISLAM202143,
title = {Identification of molecular biomarkers and pathways of NSCLC: insights from a systems biomedicine perspective},
journal = {Journal of Genetic Engineering and Biotechnology},
volume = {19},
number = {1},
pages = {43},
year = {2021},
issn = {1687-157X},
doi = {https://doi.org/10.1186/s43141-021-00134-1},
url = {https://www.sciencedirect.com/science/article/pii/S1687157X23007394},
author = {Rakibul Islam and Liton Ahmed and Bikash Kumar Paul and Kawsar Ahmed and Touhid Bhuiyan and Mohammad Ali Moni},
keywords = {Gene expression, Gene ontology, KEGG pathway analysis, PPI network, Molecular biomarkers},
abstract = {Background
Worldwide, more than 80% of identified lung cancer cases are associated to the non-small cell lung cancer (NSCLC). We used microarray gene expression dataset GSE10245 to identify key biomarkers and associated pathways in NSCLC.
Results
To collect Differentially Expressed Genes (DEGs) from the dataset GSE10245, we applied the R statistical language. Functional analysis was completed using the Database for Annotation Visualization and Integrated Discovery (DAVID) online repository. The DifferentialNet database was used to construct Protein–protein interaction (PPI) network and visualized it with the Cytoscape software. Using the Molecular Complex Detection (MCODE) method, we identify clusters from the constructed PPI network. Finally, survival analysis was performed to acquire the overall survival (OS) values of the key genes. One thousand eighty two DEGs were unveiled after applying statistical criterion. Functional analysis showed that overexpressed DEGs were greatly involved with epidermis development and keratinocyte differentiation; the under-expressed DEGs were principally associated with the positive regulation of nitric oxide biosynthetic process and signal transduction. The Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway investigation explored that the overexpressed DEGs were highly involved with the cell cycle; the under-expressed DEGs were involved with cell adhesion molecules. The PPI network was constructed with 474 nodes and 2233 connections.
Conclusions
Using the connectivity method, 12 genes were considered as hub genes. Survival analysis showed worse OS value for SFN, DSP, and PHGDH. Outcomes indicate that Stratifin may play a crucial role in the development of NSCLC.}
}
@article{SINGH2025102510,
title = {Identifying pan-cancer and cancer subtype miRNAs using interpretable convolutional neural network},
journal = {Journal of Computational Science},
volume = {85},
pages = {102510},
year = {2025},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2024.102510},
url = {https://www.sciencedirect.com/science/article/pii/S187775032400303X},
author = {Joginder Singh and Shubhra Sankar Ray and Sukriti Roy},
keywords = {CNN, Explainable AI, Optimization, Machine learning, Data mining, miRNA expression, Computational oncology, Pan-cancer, Cancer subtypes},
abstract = {Background:
MiRNAs are short-length (∼22nt) non-coding RNAs and are considered to be important biomarkers in pan-cancer analysis. Pan-cancer analysis is the study of finding the commonalities and differences in genetic and cellular alterations in various types of cancers. A common computational challenge in handling miRNA expression data is that it is high dimensional and complex (HDC) in nature. In this regard, convolutional neural networks are proven to be good performers due to their nature of finding patterns in complex data.
Methodology:
An interpretable convolutional neural network model (ICNNM) is developed for classifying miRNA expression based pan-cancer data. The ICNNM is a one dimensional model. The layers and other hyperparameters are optimized using Bayesian optimization with multivariate tree parzen estimator (BoMTPE). An interpretable approach is developed using SHapley Additive exPlanations (SHAP) values for explaining the behavior of ICNNM. This approach helps in introducing an attribution score for identifying relevant miRNAs using SHAP values. The attribution scores are assigned higher values for those miRNAs which help in the accurate prediction of tumor class of patients by utilizing the game theory concept in computing the SHAP values. The model is evaluated on 9 datasets among which 6 datasets (4 general pan cancer and two subtypes) are derived from a single TCGA pan-cancer dataset, one dataset is downloaded as Breast sub-type from TCGA, and two datasets, nasopharyngeal carcinoma and bone and soft tissue sarcoma, are downloaded from GEO as rare cancer ones.
Results:
The ICNNM is seen to perform better as compared to related techniques such as three variations of the CNN model, random forest RF, SVM, Gboost, XGboost, and Catboost. The performance is evaluated in terms of F1-score, discriminability power of expressions between normal and tumor classes, and biological significance of the selected miRNAs. The biological significance is established through existing literatures and online databases such as gene ontology and KEGG pathways after obtaining the target genes using miRDB database. While the performance of ICNNM in terms of F1-score varies from 0.95 to 0.99 for 4 general pan-cancer datasets, it varies from 0.91 to 0.99 for 3 subtype datasets and from 0.76 to 0.90 for rare cancer datasets. Many of the selected miRNAs are found to be the key biomarkers in various tumor classes according to existing investigations. Three miRNAs miR-503, miR-202, and miR-135a can be considered as novel predictions for cancer classes prostate and rectum, mesothelioma, and testicular germ cells, respectively, as their target genes are involved in related cancer pathways, obtained using miRDB database.}
}
@article{ZENG2025168210,
title = {Establishment of succinic acid production in recombinant Kluyveromyces marxianus enhanced by AI-guided enzyme specificity engineering and redox-controlled fermentation optimization},
journal = {Chemical Engineering Journal},
pages = {168210},
year = {2025},
issn = {1385-8947},
doi = {https://doi.org/10.1016/j.cej.2025.168210},
url = {https://www.sciencedirect.com/science/article/pii/S1385894725090527},
author = {Du-Wen Zeng and Yi-Fan Zhu and Yuan-Hong Jiang and Ya-Hao Ding and Zhen-Zhi Wang and Kai Li and Sha Liao and Ya-Chao Fan and Lin Zhang and Yu-Guang Wang and Xin-Qing Zhao},
keywords = {Artificial intelligence, Enzyme specificity, , Succinic acid (SA) bioproduction, Fumarate hydratase},
abstract = {Controlling enzyme activities toward desired directions is critical for manipulating cellular metabolism. Artificial intelligence (AI) has been applied to increase enzyme activities and stability. However, AI-based engineering of enzyme specificity, such as substrate specificity and catalytic bias, remains insufficiently explored. In this work, we developed an AI-driven EKAD approach and achieved efficient succinic acid (SA) production using the yeast Kluyveromyces marxianus. EKAD integrates the ESM-2 protein language model with two-ways Kcat activity prediction using the DLKcat model, together with Alphafold 2 structural modeling and Molecular Docking. Through only one round EKAD, we identified one single mutation (V365E) of the reversible enzyme fumarate hydratase (FHase) which shows much higher specificity toward SA synthesis. Introduction of the mutation with suitable copy numbers significantly increased the ratio of SA and the byproduct malic acid (up to 232.74 %). SA production of the recombinant K. marxianus carrying both the mutation and a heterologous transhydrogenase from Escherichia coli was further optimized in a 3-L bioreactor by an oxidation-reduction potential (ORP) control strategy. As a result, SA production of 103.9 g/L after 96 h at 37 °C was achieved, more than 1000-folds of that of the wild-type strain. The EKAD approach can be commonly applied to improve production of valuable metabolites by manipulating enzyme specificity.}
}
@article{FEHER2023103463,
title = {Few-shot entity linking of food names},
journal = {Information Processing & Management},
volume = {60},
number = {5},
pages = {103463},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103463},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323002005},
author = {Darius Feher and Faridz Ibrahim and Zhuyan Cheng and Viktor Schlegel and Tom Maidment and James Bagshaw and Riza Batista-Navarro},
keywords = {Entity linking, Natural language processing, Machine learning, Food knowledge base},
abstract = {Entity linking (EL), the task of automatically matching mentions in text to concepts in a target knowledge base, remains under-explored when it comes to the food domain, despite its many potential applications, e.g., finding the nutritional value of ingredients in databases. In this paper, we describe the creation of new resources supporting the development of EL methods applied to the food domain: the E.Care Knowledge Base (E.Care KB) which contains 664 food concepts and the E.Care dataset, a corpus of 468 cooking recipes where ingredient names have been manually linked to corresponding concepts in the E.Care KB. We developed and evaluated different methods for EL, namely, deep learning-based approaches underpinned by Siamese networks trained under a few-shot learning setting, traditional machine learning-based approaches underpinned by support vector machines (SVMs) and unsupervised approaches based on string matching algorithms. Combining the strengths of each of these approaches, we built a hybrid model for food EL that balances the trade-offs between performance and inference speed. Specifically, our hybrid model obtains 89.40% accuracy and links mentions at an average speed of 0.24 seconds per mention, whereas our best deep learning-based model, SVM model and unsupervised model obtain accuracies of 86.99%, 87.19% and 87.43% at inference speeds of 0.007, 0.66 and 0.02 seconds per mention, respectively.}
}
@article{AKAY2025354,
title = {Training a Foundation Model in Engineering Design Understanding},
journal = {Procedia CIRP},
volume = {136},
pages = {354-359},
year = {2025},
note = {35th CIRP Design 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.08.062},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125008157},
author = {Haluk Akay and Antonio J. Capezza and Billy W. Hoogendoorn and Maryna Henrysson},
keywords = {Foundation Model, Design Representation, Artificial Intelligence},
abstract = {Across industry, applications involving Artificial Intelligence are shifting from task-specific to general purpose foundation models able to perform a diverse set of previously unseen functions with minimal instruction or additional training. To develop such a foundation model for engineering design, training must be completed at a meaningful scale on artifacts of prior product development, which can be multimodal and sparsely annotated. This work presents a sequence learning framework for training a foundation model on contextual relationships between function, form, and fabrication in engineering design. This learning method is demonstrated with a case study in absorbent product design.}
}
@article{MODI201963,
title = {A QoS-based approach for cloud-service matchmaking, selection and composition using the Semantic Web},
journal = {Journal of Systems and Information Technology},
volume = {21},
number = {1},
pages = {63-89},
year = {2019},
issn = {1328-7265},
doi = {https://doi.org/10.1108/JSIT-01-2017-0006},
url = {https://www.sciencedirect.com/science/article/pii/S1328726519000065},
author = {Kirit J. Modi and Sanjay Garg},
keywords = {Cloud-service, Cloud ontology, Cloud-service composition, Cloud-service matchmaking, Cloud-service selection, Healthcare decision making system},
abstract = {Purpose
Cloud computing provides a dynamic, heterogeneous and elastic environment by offering accessible ‘cloud services’ to end-users. The tasks involved in making cloud services available, such as matchmaking, selection and composition, are essential and closely related to each other. Integration of these tasks is critical for optimal composition and performance of the cloud service platform. More efficient solutions could be developed by considering cloud service tasks collectively, but the research and academic community have so far only considered these tasks individually. The purpose of this paper is to propose an integrated QoS-based approach for cloud service matchmaking, selection and composition using the Semantic Web.
Design/methodology/approach
In this paper, the authors propose a new approach using the Semantic Web and quality of service (QoS) model to perform cloud service matchmaking, selection and composition, to fulfil the requirements of an end user. In the Semantic Web, the authors develop cloud ontologies to provide semantic descriptions to the service provider and requester, so as to automate the cloud service tasks. This paper considers QoS parameters, such as availability, throughput, response time and cost, for quality assurance and enhanced user satisfaction.
Findings
This paper focus on the development of an integrated framework and approach for cloud service life cycle phases, such as discovery, selection and composition using QoS, to enhance user satisfaction and the Semantic Web, to achieve automation. To evaluate performance and usefulness, this paper uses a scenario based on a Healthcare Decision-Making System (HDMS). Results derived through the experiment prove that the proposed prototype performs well for the defined set of cloud-services tasks.
Originality/value
As a novel concept, our proposed integrated framework and approach for cloud service matchmaking, selection and composition based on the Semantic Web and QoS characterisitcs (availability, response time, throughput and cost), as part of the service level agreement (SLA) will help the end user to match, select and filter cloud services and integrate cloud-service providers into a multi-cloud environment.}
}
@article{YANG202399,
title = {Exploring molecular mechanisms underlying the pathophysiological association between knee osteoarthritis and sarcopenia},
journal = {Osteoporosis and Sarcopenia},
volume = {9},
number = {3},
pages = {99-111},
year = {2023},
issn = {2405-5255},
doi = {https://doi.org/10.1016/j.afos.2023.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405525523000511},
author = {Jiyong Yang and Tao Jiang and Guangming Xu and Shuai Wang and Wengang Liu},
keywords = {Knee osteoarthritis, Sarcopenia, Pathogenesis, Informatics analyses, Hub genes},
abstract = {Objectives
Accumulating evidence indicates a strong link between knee osteoarthritis (KOA) and sarcopenia. However, the mechanisms involved have not yet been elucidated. This study primarily aims to explore the molecular mechanisms that explain the connection between these 2 disorders.
Methods
The gene expression profiles for KOA and sarcopenia were obtained from the Gene Expression Omnibus database, specifically from GSE55235, GSE169077, and GSE1408. Various bioinformatics techniques were employed to identify and analyze common differentially expressed genes (DEGs) across the 3 datasets. The techniques involved the analysis of Gene Ontology and pathways to enhance understanding, examining protein-protein interaction (PPI) networks, and identifying hub genes. In addition, we constructed the network of interactions between transcription factors (TFs) and genes, the co-regulatory network of TFs and miRNAs for hub genes, and predicted potential drugs.
Results
In total, 14 common DEGs were found between KOA and sarcopenia. Detailed information on biological processes and signaling pathways of common DEGs was obtained through enrichment analysis. After performing PPI network analysis, we discovered 4 hub genes (FOXO3, BCL6, CDKN1A, and CEBPB). Subsequently, we developed coregulatory networks for these hub genes involving TF-gene and TF-miRNA interactions. Finally, we identified 10 potential chemical compounds.
Conclusions
By conducting bioinformatics analysis, our study has successfully identified common gene interaction networks between KOA and sarcopenia. The potential of these findings to offer revolutionary understanding into the common development of these 2 conditions could lead to the identification of valuable targets for therapy.}
}
@article{PURSNANI2023100183,
title = {Performance of ChatGPT on the US fundamentals of engineering exam: Comprehensive assessment of proficiency and potential implications for professional environmental engineering practice},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100183},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100183},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000620},
author = {Vinay Pursnani and Yusuf Sermet and Musa Kurt and Ibrahim Demir},
keywords = {ChatGPT, Fundamentals of engineering exam, AI in education, Prompt modification techniques, Large language models (LLMs), Responsible AI integration},
abstract = {In recent years, advancements in artificial intelligence (AI) have led to the development of large language models like GPT-4, demonstrating potential applications in various fields, including education. This study investigates the feasibility and effectiveness of using ChatGPT, a GPT-4 based model, in achieving satisfactory performance on the Fundamentals of Engineering (FE) Environmental Exam. This study further shows a significant improvement in the model's accuracy when answering FE exam questions through noninvasive prompt modifications, substantiating the utility of prompt modification as a viable approach to enhance AI performance in educational contexts. Furthermore, the findings reflect remarkable improvements in mathematical capabilities across successive iterations of ChatGPT models, showcasing their potential in solving complex engineering problems. Our paper also explores future research directions, emphasizing the importance of addressing AI challenges in education, enhancing accessibility and inclusion for diverse student populations, and developing AI-resistant exam questions to maintain examination integrity. By evaluating the performance of ChatGPT in the context of the FE Environmental Exam, this study contributes valuable insights into the potential applications and limitations of large language models in educational settings. As AI continues to evolve, these findings offer a foundation for further research into the responsible and effective integration of AI models across various disciplines, ultimately optimizing the learning experience and improving student outcomes.}
}
@article{LIU2023100761,
title = {From tabular data to knowledge graphs: A survey of semantic table interpretation tasks and methods},
journal = {Journal of Web Semantics},
volume = {76},
pages = {100761},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100761},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000452},
author = {Jixiong Liu and Yoan Chabot and Raphaël Troncy and Viet-Phi Huynh and Thomas Labbé and Pierre Monnin},
keywords = {Semantic table interpretation, Table annotation, Tabular data, Knowledge graph},
abstract = {Tabular data often refers to data that is organized in a table with rows and columns. We observe that this data format is widely used on the Web and within enterprise data repositories. Tables potentially contain rich semantic information that still needs to be interpreted. The process of extracting meaningful information out of tabular data with respect to a semantic artefact, such as an ontology or a knowledge graph, is often referred to as Semantic Table Interpretation (STI) or Semantic Table Annotation. In this survey paper, we aim to provide a comprehensive and up-to-date state-of-the-art review of the different tasks and methods that have been proposed so far to perform STI. First, we propose a new categorization that reflects the heterogeneity of table types that one can encounter, revealing different challenges that need to be addressed. Next, we define five major sub-tasks that STI deals with even if the literature has mostly focused on three sub-tasks so far. We review and group the many approaches that have been proposed into three macro families and we discuss their performance and limitations with respect to the various datasets and benchmarks proposed by the community. Finally, we detail what are the remaining scientific barriers to be able to truly automatically interpret any type of tables that can be found in the wild Web.}
}
@article{LI2024104621,
title = {Artificial intelligence-powered pharmacovigilance: A review of machine and deep learning in clinical text-based adverse drug event detection for benchmark datasets},
journal = {Journal of Biomedical Informatics},
volume = {152},
pages = {104621},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104621},
url = {https://www.sciencedirect.com/science/article/pii/S153204642400039X},
author = {Yiming Li and Wei Tao and Zehan Li and Zenan Sun and Fang Li and Susan Fenton and Hua Xu and Cui Tao},
keywords = {Pharmacovigilance, Machine learning/Deep learning, Adverse drug event (ADE) extraction, named-entity recognition (NER), Relation extraction (RE), Natural language processing (NLP)},
abstract = {Objective
The primary objective of this review is to investigate the effectiveness of machine learning and deep learning methodologies in the context of extracting adverse drug events (ADEs) from clinical benchmark datasets. We conduct an in-depth analysis, aiming to compare the merits and drawbacks of both machine learning and deep learning techniques, particularly within the framework of named-entity recognition (NER) and relation classification (RC) tasks related to ADE extraction. Additionally, our focus extends to the examination of specific features and their impact on the overall performance of these methodologies. In a broader perspective, our research extends to ADE extraction from various sources, including biomedical literature, social media data, and drug labels, removing the limitation to exclusively machine learning or deep learning methods.
Methods
We conducted an extensive literature review on PubMed using the query “(((machine learning [Medical Subject Headings (MeSH) Terms]) OR (deep learning [MeSH Terms])) AND (adverse drug event [MeSH Terms])) AND (extraction)”, and supplemented this with a snowballing approach to review 275 references sourced from retrieved articles.
Results
In our analysis, we included twelve articles for review. For the NER task, deep learning models outperformed machine learning models. In the RC task, gradient Boosting, multilayer perceptron and random forest models excelled. The Bidirectional Encoder Representations from Transformers (BERT) model consistently achieved the best performance in the end-to-end task. Future efforts in the end-to-end task should prioritize improving NER accuracy, especially for 'ADE' and 'Reason'.
Conclusion
These findings hold significant implications for advancing the field of ADE extraction and pharmacovigilance, ultimately contributing to improved drug safety monitoring and healthcare outcomes.}
}
@article{BA20221606,
title = {Association of oxidative stress and Kashin–Beck disease integrated Meta and Bioinformatics analysis},
journal = {Osteoarthritis and Cartilage},
volume = {30},
number = {12},
pages = {1606-1615},
year = {2022},
issn = {1063-4584},
doi = {https://doi.org/10.1016/j.joca.2022.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S1063458422008366},
author = {Y. Ba and L. Sun and J. Zuo and S.-Y. Yu and S. Yang and L.-M. Ding and Z.-C. Feng and Z.-Y. Li and G.-Y. Zhou and F.-F. Yu},
keywords = {Kashin-beck disease, Oxidative stress, Meta-analysis, Oxidative stress related genes},
abstract = {Summary
Objective
To explore the association between oxidative stress (OS) and Kashin-Beck disease (KBD).
Methods
Terms associated with “KBD” and “OS” were searched in the six different databases up to October 2021. Stata 14.0 was used to pool the means and standard deviations using random-effect or fixed-effect model. The differentially expressed genes in the articular chondrocytes of KBD were identified, the OS related genes were identified by blasting with the GeneCards. The KEGG pathway and gene ontology enrichment analysis was conducted using STRING.
Results
The pooled SMD and 95% CI showed hair selenium (−4.59; −6.99, −2.19), blood selenium (−1.65; −2.86, −0.44) and glutathione peroxidases (−4.15; −6.97, −1.33) levels were decreased in KBD, whereas the malondialdehyde (1.12; 0.60, 1.64), nitric oxide (2.29; 1.31, 3.27), nitric oxide synthase (1.07; 0.81, 1.33) and inducible nitric oxide synthase (1.69; 0.62, 2.77) were increased compared with external controls. Meanwhile, hair selenium (−2.71; −5.32, −0.10) and glutathione peroxidases (−1.00; −1.78, −0.22) in KBD were decreased, whereas the malondialdehyde (1.42; 1.04, 1.80), nitric oxide (3.08; 1.93, 4.22) and inducible nitric oxide synthase (0.81; 0.00, 1.61) were elevated compared with internal controls. Enrichment analysis revealed apoptosis was significantly correlated with KBD. The significant biological processes revealed OS induced the release of cytochrome c from mitochondria. The cellular component of OS located in the mitochondrial outer membrane.
Conclusions
The OS levels in KBD were significantly increased because of selenium deficiency, OS mainly occurred in mitochondrial outer membrane, released of cytochrome c from mitochondria, and induced apoptotic signaling pathway.}
}
@article{OFFENHUBER2023264,
title = {Reconsidering Representation in College Design Curricula},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {9},
number = {2},
pages = {264-282},
year = {2023},
note = {The Future of Design Education: Rethinking Design Education for the 21st Century},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2023.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405872623000394},
author = {Dietmar Offenhuber and Joy Mountford},
keywords = {Representation, Data, Models, Maps, Visualization, Sensory modalities},
abstract = {The Future of Design Education working group on representation addressed the roles of data, maps, models, and interfaces as a continuum from representation to action. The article traces historical ideas of representation grounded by a linguistic paradigm to more recent approaches based on performance, embodiment, and sensory modalities other than vision. Discussions include the use of representations in the design process. Designers are able to use traditional forms of representation in the design of artifacts, such as sketches. These forms of representation are not sufficient for the design of systems. System design requires models that allow stakeholders to negotiate their view of a situation and design teams to iterate how things might work. Core ideas in the working group recommendations address issues of, substitution, formal rules, motivation, context dependency, materiality, provisionality, latency, performance, externalization, facilitation and negotiation, mediation, and measurement and evaluation. Discussions address the socio-political implications of representation and the expanding role of computing and data that call for a systems view.}
}
@article{DALONZO201845,
title = {Tran-Duc-Thao and the language of real life},
journal = {Language Sciences},
volume = {70},
pages = {45-57},
year = {2018},
note = {Karl Marx and the Language Sciences – Critical Encounters},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S038800011730298X},
author = {Jacopo D'Alonzo},
keywords = {Tran-Duc-Thao, Language of real life, German ideology, Saussurean linguistics, Phenomenology},
abstract = {From the 1950s, the Vietnamese philosopher Tran-Duc-Thao (1917–1993) became interested in language origins. To him, the way to investigate the roots of human language was to suggest a semiotics that would be free from the primacy of arbitrary signs. Thao called his own semiotic project the ‘semiology of real life’. The main source of that label is a passage of Marx's and Engels' German Ideology dealing with ‘the language of real life’. Thao's semiotic model may allow us to better understand some of the assumptions and implications of this little-known Marxian notion. Like Marx and Engels, Thao was deeply convinced of the social origins of linguistic skills and insisted that language arose during cooperative collective goal-oriented activities. Thus, Thao regarded the language of real life as coextensive with the material activity and the material intercourse of humans. He went on to describe the motivated structure, the denotative function, and the context-related nature of what he saw as the most fundamental signs in human languages. To him, such signs also illustrated the first step in the development of human-specific linguistic skills. At the same time, in this way, Thao called explicitly into question Saussure's semiotic model.}
}
@article{GANESAN20202060,
title = {A longitudinal footprint of genetic epilepsies using automated electronic medical record interpretation},
journal = {Genetics in Medicine},
volume = {22},
number = {12},
pages = {2060-2070},
year = {2020},
issn = {1098-3600},
doi = {https://doi.org/10.1038/s41436-020-0923-1},
url = {https://www.sciencedirect.com/science/article/pii/S1098360021008169},
author = {Shiva Ganesan and Peter D. Galer and Katherine L. Helbig and Sarah E. McKeown and Margaret O’Brien and Alexander K. Gonzalez and Alex S. Felmeister and Pouya Khankhanian and Colin A. Ellis and Ingo Helbig},
keywords = {electronic medical records, Human Phenotype Ontology, childhood epilepsy, neurogenetics},
abstract = {Purpose
Childhood epilepsies have a strong genetic contribution, but the disease trajectory for many genetic etiologies remains unknown. Electronic medical record (EMR) data potentially allow for the analysis of longitudinal clinical information but this has not yet been explored.
Methods
We analyzed provider-entered neurological diagnoses made at 62,104 patient encounters from 658 individuals with known or presumed genetic epilepsies. To harmonize clinical terminology, we mapped clinical descriptors to Human Phenotype Ontology (HPO) terms and inferred higher-level phenotypic concepts. We then binned the resulting 286,085 HPO terms to 100 3-month time intervals and assessed gene–phenotype associations at each interval.
Results
We analyzed a median follow-up of 6.9 years per patient and a cumulative 3251 patient years. Correcting for multiple testing, we identified significant associations between “Status epilepticus” with SCN1A at 1.0 years, “Severe intellectual disability” with PURA at 9.75 years, and “Infantile spasms” and “Epileptic spasms” with STXBP1 at 0.5 years. The identified associations reflect known clinical features of these conditions, and manual chart review excluded provider bias.
Conclusion
Some aspects of the longitudinal disease histories can be reconstructed through EMR data and reveal significant gene–phenotype associations, even within closely related conditions. Gene-specific EMR footprints may enable outcome studies and clinical decision support.}
}
@article{ZHANG2023104540,
title = {Transformer-based approach for automated context-aware IFC-regulation semantic information alignment},
journal = {Automation in Construction},
volume = {145},
pages = {104540},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104540},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522004113},
author = {Ruichuan Zhang and Nora El-Gohary},
keywords = {Information alignment, Automated code checking, Building codes, Building information modeling, Industry foundation classes, Deep learning, Transformers},
abstract = {One of the main challenges of automated compliance checking systems is aligning the semantics of the building information models (BIMs), in Industry Foundation Classes (IFC) format, and the semantics of the regulations, in natural language, to allow for checking the compliance of the BIM with the regulations. Existing information alignment methods typically require intensive manual effort and their ability to deal with the complex regulatory concepts in the regulations is limited. To address this gap, this paper proposes a deep learning method for IFC-regulation semantic information alignment. The proposed method uses a relation classification model to relate and align the IFC and regulatory concepts. The method uses a transformer-based model and leverages the definitions of the concepts and an IFC knowledge graph to provide additional contextual information and knowledge for improved classification and alignment. The proposed method was evaluated on IFC concepts from IFC 4 and regulatory concepts from different building codes and standards. The experimental results showed good information alignment performance.}
}
@article{REHAWI2025113342,
title = {Integrative gene and isoform co-expression networks reveal regulatory rewiring in stress-related psychiatric disorders},
journal = {iScience},
volume = {28},
number = {9},
pages = {113342},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.113342},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225016037},
author = {Ghalia Rehawi and Jonas Hagenberg and Philipp G. Sämann and Lambert Moyon and Elisabeth Binder and Markus List and Annalisa Marsico and Janine Knauer-Arloth},
keywords = {Genetics, Molecular biology, Neuroscience, Omics},
abstract = {Summary
Isoform-specific expression patterns have been linked to stress-related psychiatric disorders such as major depressive disorder (MDD). To further explore their involvement, we constructed co-expression networks using total gene expression (TE) and isoform ratio (IR) data from affected (n = 210, 81% with depressive symptoms) and unaffected (n = 95) individuals. Networks were validated using advanced graph generation methods. Our analysis revealed distinct differences in network topology and structure. Shared hubs exhibited unique co-regulatory patterns in each network, with key master hubs in the affected network showing association with psychiatric disorders. Gene Ontology enrichment highlighted condition-specific biological processes linked to each network’s master hubs. Notably, isoform-level data uncovered unique co-regulatory interactions and enrichments not observed at the gene level. This is the first study to show network-level differences of gene and isoform co-expression between affected and unaffected individuals of stress-related psychiatric disorders, emphasizing the importance of isoforms in understanding the molecular mechanisms of these conditions.}
}
@article{ALRASHID2022,
title = {Collaborative Computing-Based K-Nearest Neighbour Algorithm and Mutual Information to Classify Gene Expressions for Type 2 Diabetes},
journal = {International Journal of e-Collaboration},
volume = {18},
number = {2},
year = {2022},
issn = {1548-3673},
doi = {https://doi.org/10.4018/IJeC.304044},
url = {https://www.sciencedirect.com/science/article/pii/S1548367322000163},
author = {Sura Zaki {Al Rashid}},
keywords = {Diabetes Disease, Gene Expression, K-Nearest Neighbour, MMGMOS, Mutual Information},
abstract = {ABSTRACT
The classification process is used in gene expression data on venous endothelial cells of umbilical cords in humans to reveal the concepts of regulation of insulin using dynamic gene expression data for two classes, namely control and exposed to insulin. The mutual information statistical feature selection method is used on all available datasets to select these significant genes. The data reduction results are divided into training and testing and further supplemented to the KNN classifier for diabetes classification. The results show that the mutual information in KNN reaches the highest ranked 10,000 genes, and the test classification accuracy is 100%. Pathway analysis and gene ontology enrichment are used to evaluate the targeted genes. The results clearly exhibit the importance of finding the most informative genes in the database by using the statistical gene selection technique to achieve a reduction in time and cost and increase the efficiency of the classifier. This method exhibits these significant results that can be applied to other data and diseases.}
}
@article{KHAN20251504,
title = {A data model for population descriptors in genomic research},
journal = {The American Journal of Human Genetics},
volume = {112},
number = {7},
pages = {1504-1514},
year = {2025},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2025.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0002929725001922},
author = {Alyna T. Khan and Clement Adebamowo and Stephanie M. Fullerton and Jibril Hirbo and Iain R. Konigsberg and Peter Kraft and Iman Martin and Sarah C. Nelson and Michèle Ramsay and Genevieve L. Wojcik and Sally N. Adebamowo and Matthew P. Conomos and Burcu F. Darst and Micah R. Hysong and Yun Li and Alicia R. Martin and Rasika A. Mathias and Stephen S. Rich and Lori C. Sakoda and Daniel R. Schrider and Jayati Sharma and Johanna L. Smith and Quan Sun and Yuji Zhang and Stephanie M. Gogarten},
keywords = {population descriptors, data model, genomics, genetics},
abstract = {Summary
Population descriptors used in genetic studies have broad social and translational implications. There are no globally agreed-upon definitions or usages of common population descriptors (e.g., race, ethnicity, nationality, and tribe), many of which are applied ad hoc and/or derived from political or bureaucratic conventions. Recent recommendations have encouraged the retention of as much granularity in population descriptors as possible during data preparation, analysis, and interpretation of research results. However, genomic research infrastructures (i.e., current practices, resources, and workflows in genomic research) often lack systematic and flexible organization, structure, and harmonization of multifaceted and detailed population descriptor data. This can lead to loss of information, barriers to international collaboration, and potential issues in clinical translation. Here, we describe a data model, developed by the NIH-funded Polygenic Risk Methods in Diverse Populations (PRIMED) Consortium, that organizes and retains detailed population descriptor data for future research use. The model supports a versatile, traceable, and reproducible harmonization system that offers multiple benefits over existing data structures. This data model affords researchers the flexibility to thoughtfully choose and scientifically justify their choice of population descriptors. It avoids the conflation of social identities with biological categories and guards against harmful typological inferences. Genomic research tools of this kind will be crucial for producing scientifically robust findings that minimize potential harms of descriptor misuse while maximizing benefits for diverse communities.}
}
@article{KUMAR2025101322,
title = {MLAPW: A framework to assess the impact of feature selection and sampling techniques on anti-pattern prediction using WSDL metrics},
journal = {Journal of Computer Languages},
volume = {83},
pages = {101322},
year = {2025},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2025.101322},
url = {https://www.sciencedirect.com/science/article/pii/S2590118425000085},
author = {Lov Kumar and Vikram Singh and Lalita Bhanu Murthy and Aneesh Krishna and Sanjay Misra},
keywords = {Anti-pattern, Feature selection techniques, Data sampling, Aggregation measure, Machine learning, WSDL metrics},
abstract = {Context:
The quality and design of Service-Based Systems may be degraded because of frequent changes, and negatively impacts the software design quality called Anti-patterns. The existence of these Anti-patterns highly impacts the overall maintainability of Service-Based Systems. Hence, early detection of these anti-patterns’ presence becomes mandatory with co-located modifications. However, it is not easy to find these anti-patterns manually.
Objective:
The objective of this work is to explore the role of WSDL (Web Services Description Language) metrics (MLAPW) for anti-pattern prediction using a Machine Learning (ML) based framework. This framework encompasses different variants of feature selection techniques, data sampling techniques, and a wide range of ML algorithms. This work empirically investigates the predictive ability of anti-pattern prediction models developed using different sets of WSDL metrics. Our major focus is to investigate ’how these metrics accurately predict different types of Anti-patterns present in the WSDL file’.
Methods:
To achieve the objective, different sets of WSDL metrics such as Structural Quality Metrics, Procedural Quality Metrics, Data Quality Metrics, Quality Metrics, and Complexity metrics, are used as input for Anti-patterns prediction models. Since these models use WSDL metrics as input, we have also used feature selection methods to find the best sets of WSDL metrics. These models are trained using various machine-learning techniques. This study also shows the performance of these models trained on balanced data using data sampling techniques. Finally, the empirical investigation of these techniques was done using accuracy and ROC (receiver operating characteristic curve) curve (AUC) with hypothesis testing.
Results:
The empirical study’s observation is based on 226 WSDL files from various domains such as finance, tourism, health, education, etc. The assessment asserts that the models trained using WSDL metrics have 0.79 mean AUC and 0.90 Median AUC. However, the models trained using the selected feature with classifier feature subset selection (CFS) have a better mean AUC of 0.80 and median AUC of 0.97. The experimental results also confirm that the models trained on up-sampling (UPSAM) have a better mean AUC of 0.79 and median AUC of 0.91 with a low value of Friedman rank of 2.40. Finally, the models trained using the least square support vector machine (LSSVM) achieved 1 median AUC, 0.99 mean AUC, and a low Friedman rank of 1.30.
Conclusion:
The experimental results show that the AUC values of the models trained using Data and Procedural Quality Metrics are high as compared to the other sets of metrics. However, the models improved significantly in their prediction performance after employing feature selection techniques. The experimental results also show that the models trained using the advanced level of classifiers and ensemble learning have a higher value of AUC than other techniques. Based on this research, it is reasonable to claim that using data sampling techniques helps to improve the models’ prediction capability. The models trained on sampled data using UPSAM or up-sampling achieved 0.91 medians AUC and 0.79 average AUC.}
}
@article{HUKERIKAR2024117462,
title = {Prioritising genetic findings for drug target identification and validation},
journal = {Atherosclerosis},
volume = {390},
pages = {117462},
year = {2024},
issn = {0021-9150},
doi = {https://doi.org/10.1016/j.atherosclerosis.2024.117462},
url = {https://www.sciencedirect.com/science/article/pii/S0021915024000224},
author = {Nikita Hukerikar and Aroon D. Hingorani and Folkert W. Asselbergs and Chris Finan and Amand F. Schmidt},
keywords = {Human genetics, Mendelian randomisation, Drug development, Bioinformatics, NAFLD, Drug target validation, Colocalization, Loss-of-function},
abstract = {The decreasing costs of high-throughput genetic sequencing and increasing abundance of sequenced genome data have paved the way for the use of genetic data in identifying and validating potential drug targets. However, the number of identified potential drug targets is often prohibitively large to experimentally evaluate in wet lab experiments, highlighting the need for systematic approaches for target prioritisation. In this review, we discuss principles of genetically guided drug development, specifically addressing loss-of-function analysis, colocalization and Mendelian randomisation (MR), and the contexts in which each may be most suitable. We subsequently present a range of biomedical resources which can be used to annotate and prioritise disease-associated proteins identified by these studies including 1) ontologies to map genes, proteins, and disease, 2) resources for determining the druggability of a potential target, 3) tissue and cell expression of the gene encoding the potential target, and 4) key biological pathways involving the potential target. We illustrate these concepts through a worked example, identifying a prioritised set of plasma proteins associated with non-alcoholic fatty liver disease (NAFLD). We identified five proteins with strong genetic support for involvement with NAFLD: CYB5A, NT5C, NCAN, TGFBI and DAPK2. All of the identified proteins were expressed in both liver and adipose tissues, with TGFBI and DAPK2 being potentially druggable. In conclusion, the current review provides an overview of genetic evidence for drug target identification, and how biomedical databases can be used to provide actionable prioritisation, fully informing downstream experimental validation.}
}
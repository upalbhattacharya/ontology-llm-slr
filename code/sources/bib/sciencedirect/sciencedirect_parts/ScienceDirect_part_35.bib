@article{KOOLE2018,
title = {Mobile Learning as a Tool for Indigenous Language Revitalization and Sustainability in Canada:},
journal = {International Journal of Mobile and Blended Learning},
volume = {10},
number = {4},
year = {2018},
issn = {1941-8647},
doi = {https://doi.org/10.4018/IJMBL.2018100101},
url = {https://www.sciencedirect.com/science/article/pii/S1941864718000033},
author = {Marguerite Koole and Kevin wâsakâyâsiw Lewis},
keywords = {Cree, FRAME Model, Indigenous Languages, Mobile Learning, Nêhiyawêwin},
abstract = {ABSTRACT
In this article, the authors explore how mobile learning can complement the Certificate of Indigenous Languages program at the University of Saskatchewan in Western Canada. Through the FRAME model analysis, the authors extract salient cultural, pedagogical, environmental, and technological characteristics that should be considered in the development of mobile learning tools and approaches for Cree language teachers. It is hoped that this article will stimulate a dialogue amongst designers and Indigenous groups regarding language sustainability through mobile learning. The article concludes with key findings: the need to follow protocols, to establish good relationships, and to design for areas of low/no bandwidth. Finally, the examination of current Indigenous language learning methods provides ideas for the development of much needed “apps” appropriate for Cree learners and teachers.}
}
@article{ADAMCZYK2020103161,
title = {Knowledge-based expert system to support the semantic interoperability in smart manufacturing},
journal = {Computers in Industry},
volume = {115},
pages = {103161},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.103161},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519303963},
author = {Bruno Sérgio Adamczyk and Anderson Luis Szejka and Osiris Canciglieri},
keywords = {Semantic interoperability, Manufacturing system, Integrated engine calibration process, Information and knowledge formal representation, Semantic rules, Ontology},
abstract = {The manufacturing industry has been challenged to find new technologies solutions to improve their manufacturing systems to meet new customise product while ensuring high quality and low costs. Alongside, a considerable increase in the quantity, complexity and detailed information necessary to make all system work, it has increased the need for semantic interoperability over the entire process. The semantic interoperability is necessary to integrate the heterogenic nature of the information that comes from different sources, which could lead a divergent way of interpreting its meaning, causing misinformation, misleads and errors that increase project costs and development time. The semantic interoperability is necessary to integrate the heterogenic nature of the information that comes from different sources and have its meaning interpreted in a divergent way, causing misinformation, misleads and errors that increase project costs and development time. In this context, this work contributes to the development of a Semantic Interoperable Smart Manufacturing to promote semantic interoperability across the manufacturing system considering information and concepts from multiple domains as well as sharing them across different phases of the manufacturing process. The approach proposed was validates in an experimental case of the engine calibration process of an automotive industry. The case was focused on an integration of a new auxiliary brake system composed by an electrical vacuum pump. The experimental system aids gathering and sharing information across the domains of design definitions, project requirements, and calibration process. The results demonstrated the potential of ontology-based systems to promote interoperability, exchanging information and pointing out the inconsistency within a smart manufacturing system.}
}
@article{HOSAMO2023112992,
title = {Improving building occupant comfort through a digital twin approach: A Bayesian network model and predictive maintenance method},
journal = {Energy and Buildings},
volume = {288},
pages = {112992},
year = {2023},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2023.112992},
url = {https://www.sciencedirect.com/science/article/pii/S0378778823002220},
author = {Haidar Hosamo Hosamo and Henrik Kofoed Nielsen and Dimitrios Kraniotis and Paul Ragnar Svennevig and Kjeld Svidt},
keywords = {Digital Twin, Building information modeling (BIM), Occupants comfort, Predictive maintenance, Facility management, Decision-making},
abstract = {This study introduces a Bayesian network model to evaluate the comfort levels of occupants of two non-residential Norwegian buildings based on data collected from satisfaction surveys and building performance parameters. A Digital Twin approach is proposed to integrate building information modeling (BIM) with real-time sensor data, occupant feedback, and a probabilistic model of occupant comfort to detect and predict HVAC issues that may impact comfort. The study also uses 200000 points as historical data of various sensors to understand the previous building systems’ behavior. The study also presents new methods for using BIM as a visualization platform and for predictive maintenance to identify and address problems in the HVAC system. For predictive maintenance, nine machine learning algorithms were evaluated using metrics such as ROC, accuracy, F1-score, precision, and recall, where Extreme Gradient Boosting (XGB) was the best algorithm for prediction. XGB is on average 2.5% more accurate than Multi-Layer Perceptron (MLP), and up to 5% more accurate than the other models. Random Forest is around 96% faster than XGBoost while being relatively easier to implement. The paper introduces a novel method that utilizes several standards to determine the remaining useful life of HVAC, leading to a potential increase in its lifetime by at least 10% and resulting in significant cost savings. The result shows that the most important factors that affect occupant comfort are poor air quality, lack of natural light, and uncomfortable temperature. To address the challenge of applying these methods to a wide range of buildings, the study proposes a framework using ontology graphs to integrate data from different systems, including FM, CMMS, BMS, and BIM. This study’s results provide insight into the factors that influence occupant comfort, help to expedite identifying equipment malfunctions and point towards potential solutions, leading to more sustainable and energy-efficient buildings.}
}
@article{PRICE201837,
title = {The evolution of cognitive models: From neuropsychology to neuroimaging and back},
journal = {Cortex},
volume = {107},
pages = {37-49},
year = {2018},
note = {In Memory of Professor Glyn Humphreys},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2017.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0010945217304288},
author = {Cathy J. Price},
keywords = {Cognitive models, Neuropsychology, Neuroimaging, Degeneracy, Ontologies},
abstract = {This paper provides a historical and future perspective on how neuropsychology and neuroimaging can be used to develop cognitive models of human brain functions. Section 1 focuses on the emergence of cognitive modelling from neuropsychology, why lesion location was considered to be unimportant and the challenges faced when mapping symptoms to impaired cognitive processes. Section 2 describes how established cognitive models based on behavioural data alone cannot explain the complex patterns of distributed brain activity that are observed in functional neuroimaging studies. This has led to proposals for new cognitive processes, new cognitive strategies and new functional ontologies for cognition. Section 3 considers how the integration of data from lesion, behavioural and functional neuroimaging studies of large cohorts of brain damaged patients can be used to determine whether inter-patient variability in behaviour is due to differences in the premorbid function of each brain region, lesion site or cognitive strategy. This combination of neuroimaging and neuropsychology is providing a deeper understanding of how cognitive functions can be lost and re-learnt after brain damage – an understanding that will transform our ability to generate and validate cognitive models that are both physiologically plausible and clinically useful.}
}
@article{KUBOTA2022100946,
title = {Decolonizing second language writing: Possibilities and challenges},
journal = {Journal of Second Language Writing},
volume = {58},
pages = {100946},
year = {2022},
issn = {1060-3743},
doi = {https://doi.org/10.1016/j.jslw.2022.100946},
url = {https://www.sciencedirect.com/science/article/pii/S1060374322000819},
author = {Ryuko Kubota},
keywords = {Coloniality, Decoloniality, Diversity, Eurocentric norms, Indigenous methodologies, Second language writing, Southern perspectives},
abstract = {As the field of second language writing expands its scope, scholars as well as its flagship journal, the Journal of Second Language Writing (JSLW), are expected to embrace the diversity of inquiry topics, languages of inquiry, author identities, and geographical locations where research takes place and the authors are situated. While the diversity reflected in JSLW and the field in general has certainly grown during the last 30 years, L2 writing in English and normative writing practices in U.S. higher education settings predominate the research foci. Further diversifying the inquiry scope entails questions of power, which can be addressed through decolonial and Southern perspectives. These perspectives scrutinize the hegemony of White Eurocentric norms in issues of language and academic knowledge, and discern ideological meanings produced according to the locus of enunciation. Decolonizing second language writing, especially transforming linguistic norms and research methodologies, is challenging due to the entrenched normativity that creates pragmatic needs for learners and scholars. This is why translanguaging, an approach consistent with decolonial thinking, appears to stand at odds with second language writing. In negotiating between practical needs and decolonization, the field should strive to create new conceptual, methodological, and discursive spaces for Southern and Indigenous voices.}
}
@article{MAHIEU2019138,
title = {Semantics-based platform for context-aware and personalized robot interaction in the internet of robotic things},
journal = {Journal of Systems and Software},
volume = {149},
pages = {138-157},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302553},
author = {Christof Mahieu and Femke Ongenae and Femke {De Backere} and Pieter Bonte and Filip {De Turck} and Pieter Simoens},
keywords = {Internet of things, Personalization, Semantics, Ontology, Context-aware systems, Social robots},
abstract = {Robots are moving from well-controlled lab environments to the real world, where an increasing number of environments has been transformed into smart sensorized IoT spaces. Users will expect these robots to adapt to their preferences and needs, and even more so for social robots that engage in personal interactions. In this paper, we present declarative ontological models and a middleware platform for building services that generate interaction tasks for social robots in smart IoT environments. The platform implements a modular, data-driven workflow that allows developers of interaction services to determine the appropriate time, content and style of human-robot interaction tasks by reasoning on semantically enriched IoT sensor data. The platform also abstracts the complexities of scheduling, planning and execution of these tasks, and can automatically adjust parameters to the personal profile and current context. We present motivational scenarios in three environments: a smart home, a smart office and a smart nursing home, detail the interfaces and executional paths in our platform and present a proof-of-concept implementation.}
}
@article{SUN2025101707,
title = {Knowledge-aware audio-grounded generative slot filling for limited annotated data},
journal = {Computer Speech & Language},
volume = {89},
pages = {101707},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101707},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000901},
author = {Guangzhi Sun and Chao Zhang and Ivan Vulić and Paweł Budzianowski and Philip C. Woodland},
keywords = {Slot filling, Spoken language understanding, Audio-grounding, Contextual biasing, Knowledge base, Generative model, Limited data, Few-shot, Zero-shot},
abstract = {Manually annotating fine-grained slot-value labels for task-oriented dialogue (ToD) systems is an expensive and time-consuming endeavour. This motivates research into slot-filling methods that operate with limited amounts of labelled data. Moreover, the majority of current work on ToD is based solely on text as the input modality, neglecting the additional challenges of imperfect automatic speech recognition (ASR) when working with spoken language. In this work, we propose a Knowledge-Aware Audio-Grounded generative slot filling framework, termed KA2G, that focuses on few-shot and zero-shot slot filling for ToD with speech input. KA2G achieves robust and data-efficient slot filling for speech-based ToD by (1) framing it as a text generation task, (2) grounding text generation additionally in the audio modality, and (3) conditioning on available external knowledge (e.g. a predefined list of possible slot values). We show that combining both modalities within the KA2G framework improves the robustness against ASR errors. Further, the knowledge-aware slot-value generator in KA2G, implemented via a pointer generator mechanism, particularly benefits few-shot and zero-shot learning. Experiments, conducted on the standard speech-based single-turn SLURP dataset and a multi-turn dataset extracted from a commercial ToD system, display strong and consistent gains over prior work, especially in few-shot and zero-shot setups.}
}
@article{CHANDAN2025100777,
title = {A comprehensive survey on sentiment analysis: Framework, techniques, and applications},
journal = {Computer Science Review},
volume = {58},
pages = {100777},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100777},
url = {https://www.sciencedirect.com/science/article/pii/S157401372500053X},
author = {Manish Kumar Chandan and Shrabanti Mandal},
keywords = {Natural language processing, Sentiment analysis, Machine learning, Artificial neural network (ANN), IMDb, Yelp},
abstract = {The study of sentiment analysis (SA), also recognized as opinion mining, is a rapidly emerging area of study in natural language processing (NLP). This area focuses on identifying and extracting emotions and opinions from textual data, categorizing them as either positive, neutral, or negative. Nowadays, most of the people express their opinions on social networking platforms, often using their native languages. The rapid growth of Internet-based applications has given rise to a vast amount of personalized information and broad array of user reviews available online. There are a substantial number of pertinent reviews about a particular domain, which remain difficult for humans to process. Therefore, analyzing user opinions is crucial to extract meaningful insights and understand sentiments effectively. This survey comprehensively examines the spectrum of applications for sentiment analysis within the context of current studies. We then critically review experimental outcomes and limitations observed in cutting-edge studies. Furthermore, we explore lexicon-based methods, machine learning (ML), and deep learning (DL) strategies, as well as emerging techniques like transfer learning, large language models, and multimodal approaches, discussing their strengths as well as their weaknesses. In addition, we employed multiple ML and DL strategies, leveraging the two benchmark IMDb and Yelp datasets. Following this, we utilized a systematic framework that incorporated preprocessing techniques, feature extraction, and evaluation metrics to facilitate comprehensive understanding and ensure model generalization. Finally, this study bridges the gap between traditional methods and modern innovations, addressing various challenges in sentiment analysis and proposing a roadmap for future research to mitigate these issues. This article serves as a guiding resource for researchers aiming to build an effective sentiment analysis framework.}
}
@incollection{YADAV2021123,
title = {Chapter 8 - A research review on semantic interoperability issues in electronic health record systems in medical healthcare},
editor = {Sanjay Kumar Singh and Ravi Shankar Singh and Anil Kumar Pandey and Sandeep S. Udmale and Ankit Chaudhary},
booktitle = {IoT-Based Data Analytics for the Healthcare Industry},
publisher = {Academic Press},
pages = {123-138},
year = {2021},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-821472-5},
doi = {https://doi.org/10.1016/B978-0-12-821472-5.00009-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128214725000090},
author = {Rimmy Yadav and Saniksha Murria and Anil Sharma},
keywords = {Medical healthcare, Electronic health record, Interoperability, Semantic interoperability},
abstract = {With constantly diminishing costs and prolonged effectiveness of wireless communication and transmission techniques and, importantly, the Internet of Things (IoT) emerging as a powerful technology, some aspects of our lives have changed and broadened. The healthcare sector in particular is a developing and highly demanding application sector. IoT contributions to the medical healthcare domain include remote health and monitoring services, care for the elderly, recognition as well as tactical management of chronic illnesses, and offering of adaptive and self-regulated medical facilities. In medical healthcare, electronic health record (EHR) systems provide efficient management of clinical records in today’s clinical healthcare organizations. However, medical records are generating huge amounts of data, with every medical record having its own standard pattern, schema, and level of abstraction and interoperability. To interact with EHRs, medical stakeholders must use standard and well-structured methods and ontology-based languages to analyze and mine the useful information from huge data records. Much research has been done on interoperability issues, particularly syntactic interoperability and technical interoperability. After reviewing the research articles and chapters from respected medical databases such as IEEE Xplore, Elsevier, and Science Direct, the authors noted that semantic interoperability is, in the EHR framework, one of the critical issues. To achieve full semantic interoperability, researchers and scholars have developed and structured numerous methodologies, tools, and techniques. This research review thus includes methodologies, frameworks, tools, and models, along with their advantages and limitations, developed by researchers to cope with semantic interoperability issues in medical healthcare. Furthermore, in this chapter, the authors have focused on searching the papers related to the semantic web with a model-driven architecture (MDA) approach for semantic interoperability. Applications of the MDA approach and advanced features of the semantic web may be able to resolve the issue of semantic interoperability.}
}
@article{MAZZETTI202293,
title = {Knowledge formalization for Earth Science informed decision-making: The GEOEssential Knowledge Base},
journal = {Environmental Science & Policy},
volume = {131},
pages = {93-104},
year = {2022},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.12.023},
url = {https://www.sciencedirect.com/science/article/pii/S1462901122000326},
author = {Paolo Mazzetti and Stefano Nativi and Mattia Santoro and Gregory Giuliani and Denisa Rodila and Antonietta Folino and Susie Caruso and Giovanna Aracri and Anthony Lehmann},
keywords = {Sustainability, Knowledge, Open Science, Policy indicators, Essential variables},
abstract = {During the past two centuries, the world has undergone deep societal, political, and economical changes that heavily affected human life. The above changes contributed to an increased awareness about the deep impact that policy decisions have at the local and the global level. Therefore, there is a strong need that policy-making and decision-making processes for a sustainable development be based on the best available knowledge about Earth system and environment. The recent advance of information technologies enables running complex models that use the large amount of Earth Observation datasets available. However, data and model interoperability are still limited to the syntactic level allowing to access and process datasets independently of their structural characteristics (data format, coordinate reference systems, service interface, …) but with no clear reference to their content (the semantic level) and context of use (the pragmatic level). This poses heavy limitations to the reusability of scientific processes and related workflows. The paper presents a general framework to address this issue through the design of a Knowledge Base supporting data and model semantic (and pragmatic) interoperability. In this framework, a general ontology represents the knowledge generation process for policy relevant decision-making, while multiple vocabularies formalize the semantics of data and models, identifying different types of observables, process variables, and indicators/indices. To evaluate the proposed approach to semantic interoperability of data and models, the Knowledge Base has been integrated with an advanced model-sharing framework, and a proof-of-concept has been developed for the assessment of one of the indicators of the Sustainable Development Goals defined by the United Nations.}
}
@incollection{ANOOP2021165,
title = {Chapter 12 - A meaning-aware information search and retrieval framework for healthcare},
editor = {Sarika Jain and Vishal Jain and Valentina Emilia Balas},
booktitle = {Web Semantics},
publisher = {Academic Press},
pages = {165-176},
year = {2021},
isbn = {978-0-12-822468-7},
doi = {https://doi.org/10.1016/B978-0-12-822468-7.00003-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128224687000031},
author = {V.S. Anoop and Nikhil V. Chandran and S. Asharaf},
keywords = {Information retrieval, healthcare, semantic search, meaning-aware information retrieval, ontology, query processing, information extraction},
abstract = {The developments in information and communication technologies pave the way to the digitization of traditional handwritten records to the computerized methods of record creation and storage. The healthcare industry is no different in adopting digitization mechanisms and the rate of adoption to the electronic health records (EHR) is rapid. With the introduction of many acts and regulations such as the Health Information Technology for Economic and Clinical Health Act introduced in 2009 and associated regulations such as Health information exchange act and False Claim Act, the rate has become exponential. It is evident that EHR hold beneficial knowledge for healthcare researchers and practitioners, and thus it is desirable to extract interesting patterns from the text. Clinical decision support systems heavily make use of information extracted from unstructured EHR for decision-making purposes. Clinical research studies also use such patterns extracted from unstructured EHR for analysis and predictions. Information extraction generally deals with automatic extraction of keyphrases, concepts, events, entities, and their relationships from unstructured text. This is a highly empirical area in natural language processing that uses lexical, syntactic, and semantic features for the extraction of useful information. As the volume and variety of the data increase, traditional rule-based information extraction systems will not perform well on such data and thus we need better algorithms that operate on text taking into consideration the meaning or semantics. A meaning-aware text processing algorithm can better extract patterns from highly unstructured text and can aid in the process of creating ontologies which are considered to be the backbone of semantic computing systems. Therefore it is evident that the research and development of technological breakthroughs in the areas of meaning-aware information extraction from healthcare documents and other electronic records is a highly demanding opportunity. This chapter is a novel attempt in this dimension that discusses a framework for building a meaning-aware healthcare information extraction from unstructured EHR. The proposed framework uses medical ontologies, medical catalog-based terminology extractor, and a semantic reasoner to build the medical knowledge base that is used for enabling a semantic information search and retrieval experience in the healthcare domain. An illustration of the same is also included in this chapter to better explain the proposed framework along with some future research dimensions on enhancing the framework discussed in this chapter. The empirical studies conducted show that the proposed framework outperforms some of the information extraction systems already available in the literature.}
}
@incollection{LESLIE2022271,
title = {Chapter 11 - Modelling clinical knowledge},
editor = {Evelyn Hovenga and Heather Grain},
booktitle = {Roadmap to Successful Digital Health Ecosystems},
publisher = {Academic Press},
pages = {271-289},
year = {2022},
isbn = {978-0-12-823413-6},
doi = {https://doi.org/10.1016/B978-0-12-823413-6.00016-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234136000161},
author = {Heather Leslie},
keywords = {Modelling, Clinical models, Health data design, Interoperability, Quality, Ontology},
abstract = {There is a need to accurately represent health and related knowledge within a digital health ecosystem. Communicating this need to information system developers is best achieved through clinical model formalisms developed by clinicians and other domain experts able to deconstruct and analyse a clinical domain expert’s inherent knowledge and understanding. Principles related to model design, validation and verification as ‘fit for use’, and governance formed the foundation for a proactive design method supported by freely available tooling. The online Clinical Knowledge Manager tool has proved central to the success of the openEHR crowdsourcing methodology, the coordinating hub for collaborative review and content publication. This references the openEHR ontological information model. Evidence-based benefits of their use for all types of stakeholders, especially clinicians and application developers, are detailed. This open, global health data ecosystem facilitates the efficient use of scarce and expensive resources.}
}
@article{SENE201818,
title = {Data mining for decision support with uncertainty on the airplane},
journal = {Data & Knowledge Engineering},
volume = {117},
pages = {18-36},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X1730071X},
author = {A. Sene and B. Kamsu-Foguem and P. Rumeau},
keywords = {Dempster-Shafer theory, Frequent pattern mining, Semantic reasoning, Decision support system, In-flight medical incidents},
abstract = {This study describes the formalization of the medical decision-making process under uncertainty underpinned by conditional preferences, the theory of evidence and the exploitation of high-utility patterns in data mining. To assist a decision maker, the medical process (clinical pathway) was implemented using a Conditional Preferences Base (CPB). Then for knowledge engineering, a Dempster-Shafer ontology integrating uncertainty underpinned by evidence theory was built. Beliefs from different sources are established with the use of data mining. The result is recorded in an In-flight Electronic Health Records (IEHR). The IEHR contains evidential items corresponding to the variables determining the management of medical incidents. Finally, to manage tolerance to uncertainty, a belief fusion algorithm was developed. There is an inherent risk in the practice of medicine that can affect the conditions of medical activities (diagnostic or therapeutic purposes). The management of uncertainty is also an integral part of decision-making processes in the medical field. Different models of medical decisions under uncertainty have been proposed. Much of the current literature on these models pays particular attention to health economics inspired by how to manage uncertainty in economic decisions. However, these models fail to consider the purely medical aspect of the decision that always remains poorly characterized. Besides, the models achieving interesting decision outcomes are those considering the patient's health variable and other variables such as the costs associated with the care services. These models are aimed at defining health policy (health economics) without a deep consideration for the uncertainty surrounding the medical practices and associated technologies. Our approach is to integrate the management of uncertainty into clinical reasoning models such as Clinical Pathway and to exploit the relationships between the determinants of incident management using data mining tools. To this end, how healthcare professionals see and conceive uncertainty has been investigated. This allowed for the identification of the characteristics determining people under uncertainty and to understand the different forms and representations of uncertainty. Furthermore, what an in-flight medical incident is and how its management is a decision under uncertainty issues was defined. This is the first phase of common data mining that will provide an evidential transaction basis. Subsequently an evidential and ontological reasoning to manage this uncertainty has been established in order to support decision making processes on the airplane.}
}
@article{TRAPPEY2023101879,
title = {Digital transformation of technological IP portfolio analysis for complex domain of satellite communication innovations},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101879},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101879},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000071},
author = {Amy J.C. Trappey and Regan J.S. Pa and Neil K.T. Chen and Andy Z.C. Huang and Kuo-An Li and L.P. Hung},
keywords = {B5G satellite communication, Non-terrestrial network, NLP text mining, Transdisciplinary R&D},
abstract = {The latest mobile network protocols, currently adopted by most of global telecom-services, are at the level of fifth generation (5G) technological standards. However, 5G speed and bandwidth will not be likely to catch up with the advanced AI- and IoT-based applications, which require very low latency with wide coverage. Thus, industry is looking ahead beyond 5G (B5G) communication enabling technologies. Further, in order to provide global communications without disconnections due to geographical barriers (e.g., oceans and high mountains lacking base stations), low earth orbit (LEO) satellites become the ideal solution for connecting entire global network with terrestrial and non-terrestrial links. In this research, the integration of LDA topic modeling, clustering algorithm, technology-function matrix, and s-curve modeling is proposed to analyze complex IP portfolio for B5G satellite technologies. The domain ontology, which describes the transdisciplinary features of the B5G satellite technology taxonomy, is derived and refined in a hierarchical schema after thorough literature review and 2357 domain patents being collected and analyzed. This research aims to discover the main trends and the potentials of satellite communication technologies through novel tech-mining techniques in both macro- and micro-levels. During macro-analyses, trends of global inventions, the leading assignees, and the distribution of technology categories are depicted. We identify the leading patent assignees with their domain patents (published from 2012 to 2021) and distributions under sub-technical categories. Further, the micro-analyses discover the innovations of major technological themes, showing key R&D hot-spots, cold-spots and lifecycle maturities of sub-technical categories. Finally, the research strategies for technical-savvy countries (e.g., Taiwan, with advances in its semiconductor and ICT industries) are presented highlighting its strategic innovations related to the complex communication domain.}
}
@article{SCHUMANN2025105042,
title = {Overview and expansion of CEP85L-associated lissencephaly},
journal = {European Journal of Medical Genetics},
volume = {77},
pages = {105042},
year = {2025},
issn = {1769-7212},
doi = {https://doi.org/10.1016/j.ejmg.2025.105042},
url = {https://www.sciencedirect.com/science/article/pii/S1769721225000497},
author = {Isabell Schumann and Rami {Abou Jamra} and Robin-Tobias Jauss and Maike Karnstedt and Ulrich Specht and Pia Zacher and Friedrich Woermann and Yuval Yaron and Bernt Popp},
keywords = {lissencephaly, , Exome sequencing, Neurodevelopmental delay},
abstract = {Defective neuronal migration causes lissencephaly (LIS), a neurodevelopmental disorder (NDD) with a smooth cerebral surface and abnormal cortical thickness. Variants in CEP85L are linked to posterior predominant LIS, but the phenotype and genotype are unclear. Three new unrelated cases of CEP85L-associated LIS are presented, including the first prenatal diagnosis and a mosaic variant. Clinical, neuroimaging, and genetic analyses were recorded. Data from 29 previously reported individuals was used in a comprehensive literature review. Human phenotype ontology (HPO) terms were used to annotate phenotypic features, and American College of Medical Genetics and Genomics (ACMG) guidelines were used to evaluate variants. All postnatal individuals had variable NDD. Global developmental delay was observed in 71 % (22/31), speech or motor delay in 54 % (11/31 and 6/31, respectively), and intellectual disability in 74 % (23/31) of cases. Focused and generalized-onset seizures occurred in 90 % (28/31). Brain magnetic resonance imaging (MRI) revealed predominantly posteriorly predominant LIS in all evaluated individuals, with 55 % (17/31) of cases also exhibiting subcortical band heterotopia (SBH). A total of 18 different CEP85L variants were identified among all individuals, all clustering in a highly conserved N-terminal region between amino acids 1 and 103. These included 10 missense mutations, five splice-site alterations, two start-loss variants, and one stop variant. Nine de novo variants, 10 mother-father variants, and 13 variants with unknown inheritance. Genotype-phenotype correlations in CEP85L-associated LIS show that stronger splice and germline variants often cause more severe symptoms than mosaic variants. To reduce confusion caused by alternative CEP85L transcripts, we recommend NM_001042475 for variant interpretation. These findings improve this disorder's genetic diagnostics and counseling framework.}
}
@article{CHEN2025113575,
title = {Computational design of indoor lighting supported by artificial intelligence: Recent advances and future prospects},
journal = {Building and Environment},
volume = {285},
pages = {113575},
year = {2025},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2025.113575},
url = {https://www.sciencedirect.com/science/article/pii/S0360132325010479},
author = {Peng Chen and Lixiong Wang and Yuting Wu and Zelin Liang and Juan Yu and Tianyi Chen},
keywords = {Interior lighting, Computational design, Artificial intelligence, Generative design},
abstract = {The growing complexity of indoor lighting requirements demands innovative design approaches. AI-supported computational design has demonstrated potential in generating design solutions under complex constraints, yet the lighting society lacks a comprehensive understanding of this approach. Seventy-nine publications were collected for bibliometric analysis. Then a three-domain framework was proposed and reviewed. For lighting environment integration, (deep) neural networks enable analysis of light intensity, spectrum, and spatial distribution patterns through sparse sensors or RGB images. For lighting performance modeling, ML and ANN achieve real-time, personalized, and environment-aware prediction of lighting performance. For lighting design support, heuristic algorithm-dominated systems intelligently generate luminaire layouts, dimming strategies, and spectral compositions while balancing functional, perceptual, and energy-saving objectives. Deep learning demonstrates end-to-end generative capabilities but is limited by data availability. “Perception-as-generation” is proposed as the future direction for computational lighting design, emphasizing responsiveness to the individual and temporal diversity of perceptual needs. A roadmap is proposed to establishing a lighting decision-making pivot centered on large language models. The associated technical challenges and opportunities are outlined too. This research will help practitioners better understand and apply AI, promote interdisciplinary collaboration in the lighting industry, and inspire lighting design paradigm innovation under the "good lighting" vision.}
}
@article{DUDEK201934,
title = {Integrated quality assessment of services in an adaptive expert system with a rule-based knowledge base},
journal = {Transportation Research Procedia},
volume = {39},
pages = {34-41},
year = {2019},
note = {3rd International Conference "Green Cities – Green Logistics for Greener Cities", Szczecin, 13-14 September 2018},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2019.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352146519300936},
author = {Tomasz Dudek and Bożena Śmiałkowska},
keywords = {quality assessment, ontologies, knowledge base},
abstract = {One of the basic tasks of companies providing services is care for their quality. The quality of services is not only demonstrated by the requirements of service users, but also by applicable industry standards, specifications, economic and financial conditions. The process of quality assessment require knowledge about the quality measures (expressed in numerical form or as a text description). Acquiring that knowledge is the most significant step to quality assessment. The article presents architecture of expert systems used to explain and asses that knowledge with a rule-based knowledge base (built with integration of data text analysis modulatory methods and rule-based exploration of numerical data). Such databases, with additional ontology integration methods, are more adaptable to new quality measures and data sources. Expert systems form quality standards and a reference to the quality assessment forecast.}
}
@article{TU201865,
title = {IoT-based production logistics and supply chain system – Part 1},
journal = {Industrial Management & Data Systems},
volume = {118},
number = {1},
pages = {65-95},
year = {2018},
issn = {0263-5577},
doi = {https://doi.org/10.1108/IMDS-11-2016-0503},
url = {https://www.sciencedirect.com/science/article/pii/S0263557718000891},
author = {Mengru Tu and Ming K. Lim and Ming-Fang Yang},
keywords = {Internet of Things, RFID, Petri net, Manufacturing supply chain, Production logistics},
abstract = {Purpose
The lack of reference architecture for Internet of Things (IoT) modeling impedes the successful design and implementation of an IoT-based production logistics and supply chain system (PLSCS). The authors present this study in two parts to address this research issue. Part A proposes a unified IoT modeling framework to model the dynamics of distributed IoT processes, IoT devices, and IoT objects. The models of the framework can be leveraged to support the implementation architecture of an IoT-based PLSCS. The second part (Part B) of this study extends the discussion of implementation architecture proposed in Part A. Part B presents an IoT-based cyber-physical system framework and evaluates its performance. The paper aims to discuss this issue.
Design/methodology/approach
This paper adopts a design research approach, using ontology, process analysis, and Petri net modeling scheme to support IoT system modeling.
Findings
The proposed IoT system-modeling approach reduces the complexity of system development and increases system portability for IoT-based PLSCS. The IoT design models generated from the modeling can also be transformed to implementation logic.
Practical implications
The proposed IoT system-modeling framework and the implementation architecture can be used to develop an IoT-based PLSCS in the real industrial setting. The proposed modeling methods can be applied to many discrete manufacturing industries.
Originality/value
The IoT modeling framework developed in this study is the first in this field which decomposes IoT system design into ontology-, process-, and object-modeling layers. A novel implementation architecture also proposed to transform above IoT system design models into implementation logic. The developed prototype system can track product and different parts of the same product along a manufacturing supply chain.}
}
@article{KINAWY2018286,
title = {Customizing information delivery to project stakeholders in the smart city},
journal = {Sustainable Cities and Society},
volume = {38},
pages = {286-300},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717309800},
author = {S.N. Kinawy and T.E. El-Diraby and H. Konomi},
keywords = {Project communication system, Co-creation, Community engagement, Crowdsourcing, Ontology, Recommender systems},
abstract = {In the smart city, citizens are integral participants in the decision making process. They possess equally important knowledge to that of professionals. Effectively informing them about new project features is the first step in engaging them in the decision making and harnessing their knowledge. However, given the complexity and diversity of project information, citizens could face an information overload. Our objective is to support the delivery of the right information to the right person; and doing so in an adaptive manner that recognizes the needs of local context. We have developed a system that allows users to profile their information needs based on an ontology of user communications. Recommender algorithms are then used to match user profile to the most relevant knowledge items, such as documents, web pages, and tagged videos or images. Users who wish to rate or tag documents can do so through using concepts from the ontology or free text. If free text is used, semantic analysis is conducted to extract relevant tags. Tags are then fed-back into the recommender system to enhance its accuracy. Capturing community tags provides a good opportunity to use crowd input to contextualise the matching algorithms.}
}
@article{LIU2024108064,
title = {POLAT: Protein function prediction based on soft mask graph network and residue-Label ATtention},
journal = {Computational Biology and Chemistry},
volume = {110},
pages = {108064},
year = {2024},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2024.108064},
url = {https://www.sciencedirect.com/science/article/pii/S1476927124000525},
author = {Yang Liu and Yi Zhang and ZiHao Chen and Jing Peng},
keywords = {Protein function prediction, Gene Ontology, Protein contact map, Graph Neural Network},
abstract = {Motivation:
Elucidating protein function is a central problem in biochemistry, genetics, and molecular biology. Developing computational methods for protein function prediction is critical due to the significant gap between sequence and functional data. Recent advances in protein structure prediction, which strongly correlates with function, make it feasible to use structure to predict function. However, current structure-based methods overlook the fact that individual residues may contribute differently to the protein’s function and do not take into account the correlation between protein residues and their functions. The challenge of effectively utilizing the relationship between protein residues and function-level information to predict protein function remains unsolved.
Result:
We proposed a protein function prediction method based on Soft Mask Graph Networks and Residue-Label Attention (POLAT), which could combine sequence features, predicted structure features, and function-level information to get an accurate prediction. We use soft mask graph networks to adaptively extract the residues relevant to functions. A residue-label attention mechanism is adopted to obtain the protein-level encoded features of a protein, which are then concatenated with a protein-level embedding and fed into a dense classifier to determine the probabilities of each function. POLAT achieves 0.670, 0.515, 0.578 Fmax and 0.677, 0.409, 0.507 AUPR on the PDB cdhit test set for the MFO, BPO, and CCO domains, respectively, outperforming the existing structure-based SOTA method GAT-GO (Fmax 0.633, 0.492, 0.547; AUPR 0.660, 0.381, 0.479). POLAT is also competitive in extensive experiments among sequence-based and multimodal methods and achieves the SOTA performance in three out of six metrics.}
}
@article{DAVIDSON2019134,
title = {Writing: the re-construction of language},
journal = {Language Sciences},
volume = {72},
pages = {134-149},
year = {2019},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0388000118300123},
author = {Andrew Davidson},
keywords = {Written language bias, Chomsky, Literacy hypothesis, Extended cognition, Neuro-constructivism},
abstract = {This paper takes as its point of departure David Olson’s contention (as expressed in The Mind on Paper, (2016) CUP, Cambridge) that writing affords a meta-representation of language through allowing linguistic elements to become explicit objects of awareness. In so doing, a tradition of suspicion of writing (e.g. Rousseau and Saussure) that sees it as a detour from and contamination of language is disarmed: writing becomes innocent, becomes naturalised. Also disarmed are some of the concerns given rise to by the observation made in the title of Per Linell’s book of a ‘written language bias in linguistics’ (2005, Routledge, London) with its attendant criticisms of approaches (e.g. Chomsky’s) that assume written language to be transparent to the putative underlying natural object. Taking Chomsky’s position (an unaware scriptism) as a representative point of orientation and target of critique, the paper assembles evidence that problematises the first-order, natural reality of cardinal linguistic constructs: phonemes, words and sentences. It is argued that the facticity of these constructs is artefactual and that that facticity is achieved by way of the introjection of ideal objects which the mind constructs as denotations of elements of an alphabetic writing system: the mental representation of language is transformed by engagement with writing and it is this non-natural artefact to which Structuralist/Generativist linguistics has been answering. Evidence for this position from the psycholinguistic and neurolinguistic literature is presented and discussed. The conclusion arrived at is that the cultural practice of literacy re-configures the cognitive realisation of language. Olson takes writing to be a map of the territory; however, it is suggested that the literate mind re-constructs the territory to answer to the features of the map.}
}
@incollection{PUNJABI2025139,
title = {8 - Integrated machine learning architectures for a knowledge graph embeddings (KGEs) approach},
editor = {Rajesh Kumar Dhanaraj and M. Nalini and Malathy Sathyamoorthy and Manar Mohaisen},
booktitle = {Knowledge Graph-Based Methods for Automated Driving},
publisher = {Elsevier},
pages = {139-158},
year = {2025},
isbn = {978-0-443-30040-0},
doi = {https://doi.org/10.1016/B978-0-443-30040-0.00008-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300400000084},
author = {Mamta Punjabi and Ratnalata Gupta and Arti Patle},
keywords = {Knowledge graph embeddings, Artificial intelligence, Data analytics, Prediction, Accuracy, Machine learning, Knowledge management, Human-centered computing, Evolutionary computation, Computational intelligence, Natural language processing},
abstract = {An increasing number of data analytics and artificial intelligence applications are being enabled by knowledge graph embeddings (KGEs).These embeddings are designed to capture semantic relationships within knowledge graphs. The purpose of this chapter is to introduce a novel approach for enhancing the performance and interpretability of KGEs through the use of machine learning architectures. The framework presented here combines deep neural networks, graph neural networks, and traditional embedding methods. By leveraging the strengths of each technique, including their ability to handle sparse and incomplete information, adapt to evolving knowledge graphs, and capture hierarchical relationships, this architecture aims to overcome the limitations of existing KGE models. In order to assess how well the suggested integrated architectures perform, we have carried out thorough experiments on widely used knowledge graph datasets. The outcomes of our research reveal notable enhancements in the ability to handle larger amounts of data, accuracy of predictions, and the ability to apply these predictions to various fields. Additionally, we analyze our integrated approach in comparison with a state-of-the-art KGE model.}
}
@article{BODENBENNER2022100442,
title = {Model-driven development of interoperable communication interfaces for FAIR sensor services},
journal = {Measurement: Sensors},
volume = {24},
pages = {100442},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2022.100442},
url = {https://www.sciencedirect.com/science/article/pii/S2665917422000769},
author = {Matthias Bodenbenner and Benjamin Montavon and Robert H. Schmitt},
keywords = {internet of production, FAIR data, Sensor data, Model-based software engineering},
abstract = {To gain long-term knowledge on product and process quality the sustainable collection, processing and storage of sensor data is an essential requirement. Whereas the application of the FAIR principles as a guideline to provide all collected data with rich metadata that allow for finding, accessing, interoperating with and reusing the data effectively is constantly growing within science and research, the usage in an industrial scenario is largely unexplored. Today, collected industrial data is often stored in arbitrary data formats without appropriate metadata describing the data content and is therefore lost for future reuse because crucial information on how to find, access and interoperate with the data is missing. Moreover, insufficiently described or missing data can lead to wrong decisions. In order to implement the FAIR principles for industrial sensor data, this article derives the major deficits and challenges of making industrial sensor data FAIR. Furthermore, the authors propose a three-layer architecture of FAIR sensor services, which handles the discussed challenges, to acquire FAIR sensor data at the time of measurement. The conceptual draft is evaluated by a prototypical implementation of a FAIR sensor service.}
}
@article{HAY2022101078,
title = {Functional magnetic resonance imaging (fMRI) in design studies: Methodological considerations, challenges, and recommendations},
journal = {Design Studies},
volume = {78},
pages = {101078},
year = {2022},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2021.101078},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X21000892},
author = {L. Hay and A.H.B. Duffy and S.J. Gilbert and M.A. Grealy},
keywords = {design cognition, design studies, research methods, psychology of design, functional magnetic resonance imaging},
abstract = {Functional magnetic resonance imaging (fMRI) enables identification of the brain regions and networks underpinning cognitive tasks. It has the potential to significantly advance cognitive design science, but is challenging to apply in design studies and methodological guidance for design researchers is lacking. In this Research Note, we reflect on our experiences and other work to outline the activities involved in developing and executing fMRI design studies. The implications for research quality at each stage are highlighted. We then consider the challenges for fMRI research on design and make recommendations for addressing them. Four critical areas are identified: establishing experimental protocols; establishing a cognitive design ontology; generating foundational knowledge about brain activation; and balancing fMRI constraints against ecological validity.}
}
@article{GONG2021514,
title = {Construction and implementation of extraction rules for assembly hierarchy information of a product based on OntoSTEP},
journal = {Procedia CIRP},
volume = {97},
pages = {514-519},
year = {2021},
note = {8th CIRP Conference of Assembly Technology and Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120315006},
author = {Hanqing Gong and Lingling Shi and Dongmei Liu and Jiahui Qian and Zhijing Zhang},
keywords = {Intelligent assembly, ASP, OntoSTEP, Information extraction, SWRL rule},
abstract = {In the framework of an intelligent assembly information system, the three-dimensional model of a product serves as a critical information source for the system, which also provides initial product feature information for assembly sequence planning. At present, in the process of extracting information from the STEP neutral file, the products are rarely performed at the assembly level. In addition, the characteristic of semantic expression of the same parts included in a product is always ignored. In this paper, based on the theory of ontology, the OntoSTEP model is constructed to extract information from STEP neutral files at the assembly level. A set of extraction rules for the basic corresponding information of the same parts in the assembly are developed. Further, a SWRL rule library is established in Protégé to realize the comprehensive information extraction of the assembly hierarchy for a product from its 3D model, which lays the foundation for the assembly sequence planning.}
}
@article{WEBER2023100639,
title = {Sociomateriality, agential realism, and the metaphysics of accounting information systems: A response to Vosselman and De Loo},
journal = {International Journal of Accounting Information Systems},
volume = {51},
pages = {100639},
year = {2023},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2023.100639},
url = {https://www.sciencedirect.com/science/article/pii/S1467089523000313},
author = {Ron Weber},
keywords = {Agential realism, Sociomateriality, Representation theory, Actor-network theory, Accounting systems, Representationalism, Ontology, Epistemology, Ethics, General systems theory, Accountability, Performativity},
abstract = {I respond to Vosselman and De Loo’s (2023) critique of my earlier paper on agential realism, representation theory, and accounting information systems (Weber, 2020). In that paper, I argued that little is learned by using an agential realism lens to study accounting-related phenomena. I claimed that insights revealed using agential realism also could have been revealed through using existing lenses such as actor-network theory and general systems theory. In particular, I defended representation theory as a way of studying accounting information systems and representationalism as a way of studying the world. Contrariwise, Vosselman and De Loo argue that representation theory and representationalism are useful only in some respects when studying accounting-related phenomena. They contend that agential realism and sociomateriality lenses are needed if the entangled nature of phenomena in accounting domains is to be understood. They point to some assumptions that they claim underpin representationalism and representation theory—assumptions that inhibit their usefulness as a way of studying entangled phenomena. In this response, I present counter-arguments to their claims and defend representationalism and representation theory as ways of understanding the world.}
}
@article{KONDINSKI20241071,
title = {Hacking decarbonization with a community-operated CreatorSpace},
journal = {Chem},
volume = {10},
number = {4},
pages = {1071-1083},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2023.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S2451929423006198},
author = {Aleksandar Kondinski and Sebastian Mosbach and Jethro Akroyd and Andrew Breeson and Yong Ren Tan and Simon Rihm and Jiaru Bai and Markus Kraft},
keywords = {decarbonization, chemistry, knowledge graphs, agents, CreatorSpace},
abstract = {Summary
The pressing challenge of decarbonization encompasses a vast combinatorial space of interlinked technologies, thus necessitating an increased reliance on artificial intelligence (AI)-assisted molecular modeling and data analytics. Our backcasting analysis proposes a future rich in efficient decarbonization technologies, such as sustainable fuels for aviation and shipping, as well as carbon capture and utilization. We then retrace the path to this proposed future with the guidance of two constraints: the maximization of scientists’ creative capacities and the evolution of a world-centric AI. Our exploration leads us to the concept of a “CreatorSpace,” a distributed digital system resembling existing hackerspaces and makerspaces known for accelerating the prototyping of new technologies worldwide. The CreatorSpace serves as a virtual, semantic platform where chemists, engineers, and materials scientists can freely collaborate, integrating chemical knowledge with cross-scale, cross-technology tools, and operations. This streamlined molecular-to-process-design pathway facilitates a diverse array of solutions for decarbonization and other sustainability technologies.}
}
@article{WANG2025100274,
title = {NOC4L serves as a novel prognostic marker and therapeutic target for lung adenocarcinoma},
journal = {European Journal of Medicinal Chemistry Reports},
pages = {100274},
year = {2025},
issn = {2772-4174},
doi = {https://doi.org/10.1016/j.ejmcr.2025.100274},
url = {https://www.sciencedirect.com/science/article/pii/S2772417425000305},
author = {Mingming Wang and Yang Lv and Zhenyu Wang and Richard Mprah and Yinchuan Ding and Cui Li and Min Xue},
keywords = {Lung Adenocarcinoma (LUAD), Nucleolus complex-associated 4 homologue (NOC4L), single cell, immune microenvironment, prognosis},
abstract = {Background
Nucleolus complex-associated 4 homologue (NOC4L), a ribosome biogenesis factor essential for small subunit processome assembly, has demonstrated oncogenic potential across multiple malignancies. Nevertheless, the mechanistic contributions of NOC4L to lung adenocarcinoma (LUAD) pathogenesis remain poorly characterized. This study systematically investigates the expression, clinical relevance, and molecular functions of NOC4L in LUAD.
Methods
We analyzed RNA sequencing data from The Cancer Genome Atlas (TCGA) LUAD cohort and normal lung tissues from the Genotype-Tissue Expression (GTEx) project to quantify tumor-specific NOC4L expression. Functional enrichment analysis employing Gene Ontology and KEGG pathways revealed associations with ribosome biogenesis, cell cycle regulation, and transcriptional dysregulation. Single-cell RNA analysis were interrogated to identify cellular subpopulations with elevated NOC4L expression. Immune microenvironment alterations were evaluated using ssGSEA algorithms, while methylation aberrations were mapped via MethSurv. The STarBase database and prognostic model were utilized to construct competing endogenous RNA (ceRNA) networks regulating NOC4L. siRNA mediated NOC4L knockdown in A549 and H1299 LUAD cell lines was performed to assess phenotypic impacts through CCK-8 proliferation assays, colony formation analyses, and migration experiments.
Results
NOC4L exhibited significant overexpression in LUAD compared to normal tissues (P<0.001), correlating with advanced TNM staging. Enrichment analysis indicated that NOC4L is associated with key pathways involved in cancer development. Single-cell analysis localized NOC4L overexpression predominantly within fibroblast subpopulations (c-9 cluster). High NOC4L expression correlated with immune microenvironment disorders, abnormal methylation levels, and mutations. Mechanistic studies identified the SNHG1/LINC00662-miR-101-3p axis as the predominant ceRNA regulator of NOC4L. Functional silencing of NOC4L significantly inhibited the proliferation, clone formation, and migration abilities of LUAD cells.
Conclusions
NOC4L as an oncogenic protein promotes the cancer progression related to immune infiltrates, which underscores that NOC4L is a promising diagnostic and prognostic marker that may facilitate targeted therapies in LUAD.}
}
@article{MING2024104658,
title = {Enhancing the coverage of SemRep using a relation classification approach},
journal = {Journal of Biomedical Informatics},
volume = {155},
pages = {104658},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104658},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000765},
author = {Shufan Ming and Rui Zhang and Halil Kilicoglu},
keywords = {Biomedical relation extraction, Relation classification, Large language models, SemRep, SemMedDB},
abstract = {Objective:
Relation extraction is an essential task in the field of biomedical literature mining and offers significant benefits for various downstream applications, including database curation, drug repurposing, and literature-based discovery. The broad-coverage natural language processing (NLP) tool SemRep has established a solid baseline for extracting subject–predicate–object triples from biomedical text and has served as the backbone of the Semantic MEDLINE Database (SemMedDB), a PubMed-scale repository of semantic triples. While SemRep achieves reasonable precision (0.69), its recall is relatively low (0.42). In this study, we aimed to enhance SemRep using a relation classification approach, in order to eventually increase the size and the utility of SemMedDB.
Methods:
We combined and extended existing SemRep evaluation datasets to generate training data. We leveraged the pre-trained PubMedBERT model, enhancing it through additional contrastive pre-training and fine-tuning. We experimented with three entity representations: mentions, semantic types, and semantic groups. We evaluated the model performance on a portion of the SemRep Gold Standard dataset and compared it to SemRep performance. We also assessed the effect of the model on a larger set of 12K randomly selected PubMed abstracts.
Results:
Our results show that the best model yields a precision of 0.62, recall of 0.81, and F1 score of 0.70. Assessment on 12K abstracts shows that the model could double the size of SemMedDB, when applied to entire PubMed. We also manually assessed the quality of 506 triples predicted by the model that SemRep had not previously identified, and found that 67% of these triples were correct.
Conclusion:
These findings underscore the promise of our model in achieving a more comprehensive coverage of relationships mentioned in biomedical literature, thereby showing its potential in enhancing various downstream applications of biomedical literature mining. Data and code related to this study are available at https://github.com/Michelle-Mings/SemRep_RelationClassification.}
}
@article{BECKER2019141,
title = {Natural language processing of German clinical colorectal cancer notes for guideline-based treatment evaluation},
journal = {International Journal of Medical Informatics},
volume = {127},
pages = {141-146},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S1386505619301145},
author = {Matthias Becker and Stefan Kasper and Britta Böckmann and Karl-Heinz Jöckel and Isabel Virchow},
keywords = {Natural language processing, Electronic health records, Information extraction, Knowledge management},
abstract = {Background
Colorectal cancer is the most commonly occurring cancer in Germany, and the second and third most commonly diagnosed cancer in women and men, respectively. The therapy for this disease is based primarily on the tumor stages, which are usually documented in an unstructured form in medical information systems. In order to re-use this knowledge, the information must be extracted and annotated using the correct terminology.
Methods
In this study, a natural language processing pipeline is developed to identify specific guideline-based patient information and to annotate it with Unified Medical Language System concepts for manual evaluation by a physician. The gold standard for one-time evaluation is determined using the human abstraction of 2513 German clinical notes from electronic health records.
Results
Using this approach to process the narrative clinical notes on colorectal cancer for retrospective evaluation of the therapy recommendation, the algorithm achieves a precision value of 96.64% for tumor stage detection and 97.95% for diagnosis recognition with recall values of 94.89% and 99.54%, respectively. The average precision value across all concepts relevant to treatment decisions for patients with known cancer diagnoses (11 concept groups) achieved a precision value of 82.05% with a recall value of 82.45% and an F1-score of 81.81%, respectively.
Conclusions
The identification of guideline-based information from narrative clinical notes has the potential for implementation as clinical decision support tools.}
}
@article{WISE2018200,
title = {Respecting the language: digitizing Native American language materials},
journal = {Digital Library Perspectives},
volume = {34},
number = {3},
pages = {200-214},
year = {2018},
issn = {2059-5816},
doi = {https://doi.org/10.1108/DLP-02-2018-0006},
url = {https://www.sciencedirect.com/science/article/pii/S2059581618000107},
author = {Mary Wise and Sarah R. Kostelecky},
keywords = {Digitization, Collaboration, Digital humanities, Digital collection, Native American language, Zuni Pueblo},
abstract = {Purpose
Many academic libraries use digital humanities projects to disseminate unique materials in their collections; during project planning, librarians will consider platforms, scanning rates and project sustainability. Rarely, though, will academic librarians consider how members from the communities that created the materials can contribute to digitization projects. The purpose of this study is to explain how collaboration with Zuni Pueblo (a Native American tribe in the southwest) community members improved a digital humanities project to disseminate Zuni language learning materials.
Design/methodology/approach
Methodologically relying on critical making, which involved community member feedback throughout the process, the Zuni Language Materials Collection will provide digital access to 35 language learning items.
Findings
The authors argue that collaboration with members of the community of creation dramatically improved item description, collection discoverability and collection interactivity. This study historicizes CONTENTdm and describes how the team modified this content management system to meet user needs. This project produced a prototype digital collection, collaboratively authored metadata and an interactive portal that invites users to engage with the collection.
Practical implications
Libraries continue to struggle to reach and reflect their diverse users. This study describes a process that others may use and modify to engage nearby Native American communities.
Originality/value
This piece shares a unique strategy of partnering with Native American community members on all aspects of digital humanities project development and design. This case study attempts to fill a gap in the literature as the first study to describe a digitization process using CONTENTdm with a Native American community.}
}
@article{QORICH2025109720,
title = {Lightweight advanced deep-learning models for stress detection on social media},
journal = {Engineering Applications of Artificial Intelligence},
volume = {140},
pages = {109720},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109720},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624018785},
author = {Mohammed Qorich and Rajae {El Ouazzani}},
keywords = {Deep-learning, Mental health, Natural language processing, Social media, Stress, Text classification},
abstract = {Nowadays, stress reveals itself as a ubiquitous presence, manifesting in novel forms in our modern daily life. Indeed, digital platforms and social media collect various impressions, reactions, and feelings that could provide valuable real-time sentiment data. Nevertheless, understanding stress and mental states among people is difficult because it relies on self-reporting and detecting related expressions, statements, and articulations. In this paper, we consider extracting nuanced insights and stress expressions from Reddit and Twitter posts using lightweight advanced deep-learning methods and Bidirectional Encoder Representations from Transformers (BERT) embeddings. Our findings highlight the potency of transformer BERT models, whether utilized as embedding feature extractors or as text sentiment classifiers. Moreover, the proposed lightweight deep architectural models promoted the field of stress detection in social media, achieving high classification performance. Practically, the BERT Electra model reached 85.67% accuracy on the small Reddit dataset, while our Convolutional Neural Network (CNN) model obtained 97.62% on the large Twitter dataset. Our contributions are not only restricted to the scientific understanding of stress but also extend to the well-being of individuals and global mental health.}
}
@article{SHARMA2024102805,
title = {A framework for enhancing the replicability of behavioral MIS research using prediction oriented techniques},
journal = {International Journal of Information Management},
volume = {78},
pages = {102805},
year = {2024},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2024.102805},
url = {https://www.sciencedirect.com/science/article/pii/S0268401224000537},
author = {Pratyush Nidhi Sharma and Marko Sarstedt and Christian M. Ringle and Jun-Hwa Cheah and Anne Herfurth and Joseph F. Hair},
keywords = {Replicability, Exploration, Confirmation, Explanation, Prediction, EP-mixed, Open Science, PLS-SEM},
abstract = {The ongoing scientific discourse surrounding the replication crisis in behavioral research, including management information systems (MIS) research, underscores the importance of innovative and rigorous approaches to theory development and validation. This article proposes the EP-mixed framework, which addresses the necessity of an ontological distinction between explanation and prediction in MIS theories, along with the epistemological challenges associated with conflating exploratory and confirmatory research during the design of robust, replicable theories. EP-mixed refers to theories that explain and predict (i.e., EP theories) developed using a mixed mode that combines the strengths of both exploratory and confirmatory research. The EP-mixed framework guides researchers in selecting appropriate analytical approaches based on their research goals and the type of theory being developed. While it can be applied in conjunction with a broad spectrum of statistical methods to enhance the robustness and replicability of MIS theories, we elaborate on the predictive analytic tools available in partial least squares structural equation modeling (PLS-SEM) as an exemplar for operationalizing the framework.}
}
@article{LAZOGLOU2020100022,
title = {Development of a spatial decision support system for land-use suitability assessment: The case of complex tourism accommodation in Greece},
journal = {Research in Globalization},
volume = {2},
pages = {100022},
year = {2020},
issn = {2590-051X},
doi = {https://doi.org/10.1016/j.resglo.2020.100022},
url = {https://www.sciencedirect.com/science/article/pii/S2590051X20300113},
author = {Miltiades Lazoglou and Demos C. Angelides},
keywords = {Land-use planning, Suitability-assessment, Ontologies, SDSS, Decision-making},
abstract = {Land-use suitability assessment necessitates the analysis of multiple parameters as well as the compliance with the legal framework, the priorities, and the beliefs of all stakeholders involved in forming and implementing spatial policies. The approach followed in this paper is based on combining Ontologies, GIS and Object-oriented programming, to develop a SDSS capable of supporting decision-making in land-use planning. The applicability of the system is demonstrated using the case-study of a Greek island. The proposed approach promotes the integration of modern technologies into the processes of spatial planning and enhances the strong trend towards using the benefits of knowledge-management in decision-making.}
}
@article{MCGARRY2018113,
title = {Complex network theory for the identification and assessment of candidate protein targets},
journal = {Computers in Biology and Medicine},
volume = {97},
pages = {113-123},
year = {2018},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2018.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0010482518300969},
author = {Ken McGarry and Sharon McDonald},
keywords = {Complex network theory, Link-clustering, Protein interactions, Ontologies},
abstract = {In this work we use complex network theory to provide a statistical model of the connectivity patterns of human proteins and their interaction partners. Our intention is to identify important proteins that may be predisposed to be potential candidates as drug targets for therapeutic interventions. Target proteins usually have more interaction partners than non-target proteins, but there are no hard-and-fast rules for defining the actual number of interactions. We devise a statistical measure for identifying hub proteins, we score our target proteins with gene ontology annotations. The important druggable protein targets are likely to have similar biological functions that can be assessed for their potential therapeutic value. Our system provides a statistical analysis of the local and distant neighborhood protein interactions of the potential targets using complex network measures. This approach builds a more accurate model of drug-to-target activity and therefore the likely impact on treating diseases. We integrate high quality protein interaction data from the HINT database and disease associated proteins from the DrugTarget database. Other sources include biological knowledge from Gene Ontology and drug information from DrugBank. The problem is a very challenging one since the data is highly imbalanced between target proteins and the more numerous nontargets. We use undersampling on the training data and build Random Forest classifier models which are used to identify previously unclassified target proteins. We validate and corroborate these findings from the available literature.}
}
@article{BRISTY2022100871,
title = {Determination of molecular signatures and pathways common to brain tissues of autism spectrum disorder: Insights from comprehensive bioinformatics approach},
journal = {Informatics in Medicine Unlocked},
volume = {29},
pages = {100871},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.100871},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822000259},
author = {Sadia Afrin Bristy and A.M. Humyra Islam and K.M. Salim Andalib and Umama Khan and Md Abdul Awal and Md Habibur Rahman},
keywords = {Bioinformatics, Autism spectrum disorder, Hub gene, Protein-protein interaction, Transcriptional factors, Post-transcriptional factors, Gene ontology},
abstract = {Autism spectrum disorder (ASD) is a collection of neurological disabilities marked by difficulties with behavior, speech, language, and interaction. It is a complicated and behaviorally defined static disorder of the developing brain. Recently it has become a serious concern across the world. The goal of this project was to use bioinformatics tools and network biology to uncover the molecular signatures and pathways of ASD. We investigated brain transcriptomics gene expression datasets and determined 47 dysregulated differentially expressed common genes. Several kinds of crucial neurodegeneration-related molecular mechanisms in the signaling structures were determined as a result of these investigations. We implemented gene set enrichment analysis (GSEA) using bimolecular pathways and gene ontology (GO) terms to determine the role of these differentially expressed genes (DEGs), as well as protein-protein interactions (PPI), transcriptional factor interactions, and post-transcriptional factor interactions. PPI network collected the top ten hub genes including KIT, PIN1, GATA1, GRIN2A, PBX2, BLK, ATP6V1B1, TCF7L1, TRAF1, and HSPG2. The PPI network also revealed the existence of two sub-networks. Moreover, several transcription factors (NFIC, USF2, TFAP2A, RELA, FOXL1, GATA2, YY1, FOXC1, NFKB1, and E2F1) and post-transcription factors (mir-335-5p, mir-26b-5p, mir-124-3p, mir-192-5p, mir-1-3p, mir-215-5p, mir-6825-5p, mir-146a-5p, mir-8485, and mir-93-5p) were found throughout this study. Some drug-like molecules were also predicted that might have a beneficial effect against ASD. We detected potentially novel links between pathogenic conditions in ASD patient's brain tissues. This work offers molecular biomarkers at the gene expression level and protein bases that could aid in a better understanding of molecular pathways, as well as potential pharmacological approaches and therapies for developing effective ASD treatments.}
}
@article{VILA2022101699,
title = {Edge-to-cloud sensing and actuation semantics in the industrial Internet of Things},
journal = {Pervasive and Mobile Computing},
volume = {87},
pages = {101699},
year = {2022},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2022.101699},
url = {https://www.sciencedirect.com/science/article/pii/S1574119222001122},
author = {Marc Vila and Víctor Casamayor and Schahram Dustdar and Ernest Teniente},
keywords = {Industrial Internet of Things, Interoperability, Computing Continuum, Context-awareness, Semantics, Autonomous cars},
abstract = {There are billions of devices worldwide deployed, connected, and communicating to other systems. Sensors and actuators, which can be stationary or movable devices. These Edge devices are considered part of the Internet of Things (IoT) devices, which can be referred to as a tier of the Computing Continuum paradigm. There are two main concerns at stake in the success of this ecosystem. The interoperability between devices and systems is the first. Mainly, because most of them communicate uniquely and differently from each other, leading to heterogeneous data. The second issue is the lack of decision-making capacity to conduct actuations, such as communicating through different computing tiers based on latency constraints due to a certain measured factor. In this article, we propose an ontology to improve device interoperability in the IoT. In addition, we also explain how to ease data communication between Computing Continuum devices, providing tools to enhance data management and decision-making. A use case is also presented, using the automotive industry, where quickness in maneuver determination is key to avoid accidents. It is exemplified using two Raspberry Pi devices, connected using different networks and choosing the appropriate one depending on context-aware conditions.}
}
@article{BENESCH2023102995,
title = {Rescuing “emotion labor” from (and for) language teacher emotion research},
journal = {System},
volume = {113},
pages = {102995},
year = {2023},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2023.102995},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X23000179},
author = {Sarah Benesch and Matthew T. Prior},
keywords = {Emotion, Emotion labor, Emotion management, Emotion research, English language teaching, Criticality},
abstract = {Recent trends in language education research have brought renewed attention to the significant role of emotions in the experiences and practices of teachers and learners. One concept that has risen to particular prominence is “emotion labor.” As originally developed by sociologist Arlie Hochschild (1983), emotion labor refers to tension between specific feelings (and feeling “rules” and displays) that are required and rewarded by one's profession or workplace and one's training and/or beliefs. However, as the term has spread across academia and popular culture, it has increasingly become used in ways that ignore its critical and feminist origins. We perceive this as a loss for language researchers and educators, especially given the reduction in funding for education and increased teacher attrition in response to the global pandemic. Therefore, to cast a light on the recurrent uncritical use of emotion labor and to rescue this concept from (and for) language teacher emotion research, we review research on emotion labor from a poststructural perspective that takes power relations into account. We also seek to disentangle emotion labor from other seemingly resonant concepts such as emotion regulation, emotional intelligence, and emotional literacy. We conclude by proposing an agenda for restoring the critical impetus to emotion labor, especially for researchers and practitioners seeking to better engage with power, agency, social justice, teacher well-being, and educational reforms.}
}
@incollection{DELUISE202459,
title = {4 - A model of time in natural linguistic reasoning},
editor = {D. Jude Hemanth},
booktitle = {Computational Intelligence Methods for Sentiment Analysis in Natural Language Processing Applications},
publisher = {Morgan Kaufmann},
pages = {59-92},
year = {2024},
isbn = {978-0-443-22009-8},
doi = {https://doi.org/10.1016/B978-0-443-22009-8.00010-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443220098000100},
author = {Daniela López {De Luise} and Sebastian Cippitelli},
keywords = {Natural language processing, fractals, chaos, entropy, automatic reasoning},
abstract = {The modern conception of natural language processing intends to explain wording, semantics, and syntactical evolution in terms of entropy, chaos, and fractals. From this perspective, there are many authors who have already presented several clues in that way and approaches that serve as a basis to explain many of the phenomena existing in any language. Part of this proposal is a short review of main contributions and a set of four rules defined that are part of previous work of the author grouped with the name “Thermodynamic Rules for Linguistic Reasoning.” They are part of a larger set of rules mathematically describing main characteristics of language performance in native speakers, in dialog contexts. The focus of the current paper is on the time perspective and modeling of the language under the mentioned context. The proposal covers also a use case with data analysis and explanation from the model conception.}
}
@article{STALLINGS2024100271,
title = {How artificial intelligence can enable data classification for market sizing - Insights from applications in practice},
journal = {International Journal of Information Management Data Insights},
volume = {4},
number = {2},
pages = {100271},
year = {2024},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100271},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824000600},
author = {L. Stallings and P. Bhat and J. Jacobs and K. Lynch and Q. Risch},
keywords = {Competitive intelligence, Market intelligence, Information classification, Market sizing, Machine learning},
abstract = {Determining the size of the addressable market is a key aspect of market intelligence and requires identifying and delineating projected budget data from potential customers. The market intelligence arena is characterized by a wide range of disparate sources, many of which are unstructured, ranging across competitive, market, financial, and technology sources, and typically necessitating significant manual work to analyze, reconcile, and integrate. The authors present an approach for classification of data from one of these sources, facilitating aggregation and analysis of intelligence information. We describe a concept proof using machine learning that extends a model for automatic mapping of publicly available budget data to segments and subsegments of a market segmentation taxonomy. This approach automates the tagging of market and market segment for each program and cost element by training classification models on the manually labeled historical data. We describe the evaluation and use of multiple natural language processing (NLP) and classification modeling methods. This work's contribution is demonstrating how NLP and machine learning techniques can provide useful data classification and automatic classification even when source data diverges from its specified taxonomic description.}
}
@article{PARVEEN2023103991,
title = {Disease risk level prediction based on knowledge driven optimized deep ensemble framework},
journal = {Biomedical Signal Processing and Control},
volume = {79},
pages = {103991},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.103991},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422004906},
author = {Huma Parveen and Syed Wajahat Abbas Rizvi and Praveen Kumar Shukla},
keywords = {Disease Risk Level Prediction, Ontology, Knowledge-based Approach, Deep EC, RLDBO},
abstract = {Healthcare information in diverse formats has been steadily boosting the growth of health systems. This information covers a wide range of new sources, such as computer files, cell phones, and health monitoring gadgets. Big health data expands the additional possibilities for analyzing the health data that improve healthcare services. Deep learning can be used in collaboration with information fusion approaches to generate better comprehensive and trustworthy predictions from large healthcare information. In this study, a risk level prediction framework is established with four main stages: pre-processing, knowledge extraction, feature extraction, as well as classification and prediction. The obtained data from the Electronic Health Records (EHR) is first exposed to a pre-processing stage, where the data cleaning and tokenization operations are performed. Then, the knowledge extraction takes place via ontology-based knowledge extraction and improved semantic similarity. In addition, the statistical and information entropy features are retrieved. To forecast the health risks from HER, a novel Ensemble Classifier (EC) with Neural network (NN), optimized Deep Belief Network (DBN) & Fuzzy logic is introduced. The extracted features are utilized to train the NN, and the retrieved knowledge bases are utilized to train the fuzzy logic. The outcome from NN as well as fuzzy logic is given as a source of input to the DBN, where the final risk prediction of the disease takes place. Since DBN is the key indicator; its weight is fine-tuned utilizing a new hybrid approach known as Rain Leveraged Dynamic Butterfly Optimization (RLDBO) to improve risk level prediction accuracy.}
}
@article{BAO2025112356,
title = {Defining and generating operation and maintenance management requirements in digital twin applications using the DT-GPT framework},
journal = {Journal of Building Engineering},
volume = {104},
pages = {112356},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.112356},
url = {https://www.sciencedirect.com/science/article/pii/S2352710225005935},
author = {Sheng Bao and Hangdong Bu},
keywords = {Digital twin, Operation and maintenance, Generative pre-trained transformer, Virtual assistant, Large language model},
abstract = {Implementing Digital Twin (DT) technology in Operation and Maintenance (O&M) heavily relies on the continuous availability of information regarding asset conditions and performances. Therefore, this study aims to establish comprehensive guidelines for defining information requirements for various O&M tasks during the design phase. To achieve this objective, data collection, interviews, and investigations were conducted to develop an O&M requirements system, including model requirements, function requirements, and nongeometric data requirements. The importance and attributes of requirement factors were analyzed through the Kano-QFD model. Based on the O&M requirements system, a virtual assistant framework named “DT-GPT” that utilizes the Generative Pre-trained Transformer (GPT) was developed to assist owners in creating guidelines according to their requirements. Evaluations and a case study were introduced to validate the potential of DT-GPT. Results illustrated that DT-GPT had a mean semantic similarity of 87.28 % in generating function lists, a mean accuracy of 92.76 % in extracting components from design drawings, and a mean semantic similarity of 96.34 % in generating nongeometric data lists. The case study further demonstrated the effectiveness of the proposed framework in assisting owners with the creation of O&M guidelines. This study provides valuable insights for identifying O&M management requirements and advancing the practical application of DT in O&M processes.}
}
@article{NIMMAGADDA20191198,
title = {Design Science Information System Framework for Managing the Articulations of Digital Agroecosystems},
journal = {Procedia Computer Science},
volume = {159},
pages = {1198-1207},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.289},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314875},
author = {Shastri L Nimmagadda and Amboge Samson and Neel Mani and Torsten Reiners},
keywords = {Design Science Research, Information System, Data Artefacts, Digital Agroecosystems},
abstract = {Agriculture industries and their business ecosystems experience data and information overload because of complex network or interconnected domains linked to a variety of agro-based systems. Data search becomes tedious when specific queries are made to support crucial technical and financial decisions by agroecosystem service providers. Due to accumulated volumes of heterogeneous data and information in multiple primary sources, websites and company servers, the agriculture industry needs a robust and flexible digital agroecosystem development. To address the major challenges, a Design Science Research (DSR) approach is adopted, articulating systematic data mapping workflows and integrating their data structures in different knowledge domains. Purpose of the research is aimed at designing and developing an ontology-based data warehousing framework, with comprehensive multidimensional ontologies that motivated us to present various data modelling architectures in different knowledge-based domain applications. An emphasis is given to spatial-temporal dimensions in the modelling process that affect the structuring of data relationships in large geographic regions, which are typical in the agro-business environment.}
}
@article{BOUCHARD2021101384,
title = {Three conceptions of nativism and the faculty of language},
journal = {Language Sciences},
volume = {85},
pages = {101384},
year = {2021},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2021.101384},
url = {https://www.sciencedirect.com/science/article/pii/S0388000121000310},
author = {Denis Bouchard},
keywords = {Nativism, Biolinguistics, Linguistic sign, Merge, Evolvability},
abstract = {I compare three linguistic models based on different conceptions of nativism to determine whether they can provide revealing accounts of properties of natural languages, and whether the traits that these linguistic models require are evolutionarily plausible. The model based on Language-specific nativism in a broad sense contains rules, conditions, features and categories—Universal Grammar—to capture the empirical phenomena. Broad UG contains many domain-specific devices that are more descriptive than explanatory, and are unlikely to have evolved as irreducible properties of the brain. Language-specific nativism in a narrow sense tries to improve evolvability by restricting its core component to the operation set-Merge and a universal lexicon of innate concepts. But the transfer operations required to link the language-invariant expressions with actual expressions of natural languages turn out to be inapplicable. Evolvability is also problematic: the model ends up appealing to mystery on three key issues. Exapted Language Nativism posits that the human language capacity is not due to a brain development specifically devoted to language but is a side effect of a uniquely human capacity of detachment that enables an array of human-specific cognitive traits. Key among these traits is the capacity to form linguistic signs. Syntactic combinations emerge directly from the capacity to form combinatorial signs, a small natural subset of linguistic signs. The perceptual and conceptual elements of signs, and the extralinguistic cognition of speakers, have grounded prior properties that provide principled explanations of the data. The model has high evolvability since its core capacity of detachment is independently related to other phenotypic effects.}
}
@article{ELSAHN2022100957,
title = {Alternative ways of studying time in qualitative international business research: A review and future agenda},
journal = {Journal of International Management},
volume = {28},
number = {3},
pages = {100957},
year = {2022},
issn = {1075-4253},
doi = {https://doi.org/10.1016/j.intman.2022.100957},
url = {https://www.sciencedirect.com/science/article/pii/S1075425322000321},
author = {Ziad Elsahn and Anna Earl},
keywords = {Time, Temporality, Theorising, Systematic theoretical review, Subjective time, Objective time},
abstract = {Our study provides a systematic theoretical review of 304 qualitative-based articles published in seven international business journals from 2010 to 2020. We constructed a typology that provides alternative ways of studying time and is constituted by two dimensions: ontological conceptions of time (objective vs subjective) and theorising style of research (variance vs process). Our analysis and findings illustrate that time is mostly treated as objective and linear, and they highlight some concerning trends: lack of conceptual clarity; lack of diversity within and between paradigms; and lack of methodological clarity. We propose three pathways for advancing future research on time.}
}
@article{BEDIA2019445,
title = {The METACLIP semantic provenance framework for climate products},
journal = {Environmental Modelling & Software},
volume = {119},
pages = {445-457},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2019.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364815218305036},
author = {J. Bedia and D. San-Martín and M. Iturbide and S. Herrera and R. Manzanas and J.M. Gutiérrez},
keywords = {Metadata, Climate services, Resource description framework, Ontologies, Earth science information},
abstract = {Having an effective way of dealing with data provenance is a necessary condition to ensure reproducibility, helping to build trust and credibility in research outcomes and the data products delivered. METACLIP (METAdata for CLImate Products) is a language-independent framework envisaged to tackle the problem of climate product provenance description. The solution is based on semantics exploiting the web standard Resource Description Framework (RDF), building on domain-specific extensions of standard vocabularies (e.g., PROV-O) describing the different aspects involved in climate product generation. We illustrate METACLIP through an example application within the open source R computing environment, generating a climate product for which full provenance information is recorded. Finally, the METACLIP Interpreter, a web-based interactive front-end for metadata visualization is presented, helping a diversity of users with different levels of expertise to trace and understand the provenance of a wide variety of climate data products, and to fully reproduce them.}
}
@article{FOMIATTI2021103357,
title = {Holding ‘new recovery’ together: Organising relations and forms of coordination in professional sociomaterial practices of addiction recovery},
journal = {International Journal of Drug Policy},
volume = {97},
pages = {103357},
year = {2021},
issn = {0955-3959},
doi = {https://doi.org/10.1016/j.drugpo.2021.103357},
url = {https://www.sciencedirect.com/science/article/pii/S0955395921002620},
author = {Renae Fomiatti and David Moore and Suzanne Fraser and Adrian Farrugia},
keywords = {Recovery, Annemarie Mol, Coordination, Translation, Science and Technology Studies},
abstract = {Questions about what addiction recovery is and the mechanisms by which people ‘recover’ have long animated alcohol and other drug research and policy. These debates became even more intense following the advent, and increasing influence in some quarters, of the ‘new recovery’. Starting from the position that recovery is ontologically multiple (Mol & Law, 2002), we trace how alcohol and other drug professionals attempted to make sense of ‘new recovery’ as a concept and set of professional practices during a period of Australian drug treatment system reform. Drawing on Annemarie Mol's (2002) account of organising relations and forms of coordination (addition, translation and distribution), we explore how the new recovery was enacted and coordinated in alcohol and other drug professionals’ sociomaterial practices, and highlight the ontological work involved in holding such an unstable object together. First, we argue that the addition of multiple enactments of addiction and recovery contributed to the formation of a singular and serviceable problem (that was simultaneously heterogeneous and complex), making the ‘disease-to-be-treated’ amenable to diverse treatment approaches, including new recovery. Second, we analyse the role of metaphor in translating authoritative logics and obligations into an enactment of new recovery suitable for application in clinical settings. Lastly, we track how incompatible enactments of recovery, both new and old, were kept apart through distribution. Although new recovery ultimately failed to gain policy traction in the Australian context, we focus on the ontological work undertaken by professionals in response to its introduction as such case studies can be useful for analysing other powerfully governing policy objects and their operations.}
}
@article{NOARDO2018156,
title = {Architectural heritage semantic 3D documentation in multi-scale standard maps},
journal = {Journal of Cultural Heritage},
volume = {32},
pages = {156-165},
year = {2018},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2018.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1296207417303710},
author = {Francesca Noardo},
keywords = {Architectural heritage, Documentation standard, Semantics, Interoperability, Ontologies, 3D digital archive},
abstract = {The documentation of cultural heritage is acknowledged as a fundamental instrument to guarantee the monument preservation and promotion, and to educate people towards these aims. The recently evolved potentialities of information technologies and communication (standard data models, ontologies and formats, web technologies) permit the development of digital archives in which the information is also semantically specified in a shared and explicit way, so that it can be universally understood and correctly interpreted. However, some tools are missing for suitably archiving and communicating the architectural heritage information, including the representation potentialities of high-level-of-detail 3D models. A goal of this study is the suitable representation of both the thematic information about architectural heritage and its 3D geometric characteristics in an interoperable and understandable way. For this reason, the existing data models, available for the geometric and cartographic field, and for the cultural heritage domain, were considered. They are distinct standards, and some limits make them incomplete (in the spatial or semantic management). In this study, an extension is proposed of the standard data model CityGML to overcome these limits. CityGML is published by the Open Geospatial Consortium to represent urban objects and permits a multi-scale management of the information useful for the representation of architectural heritage multi-faceted, multi-temporal, complex knowledge. In the paper, the extension is described, and an example of its application on a portion of a highly detailed 3D model of a mediaeval church is presented.}
}
@article{HASSON2018135,
title = {Grounding the neurobiology of language in first principles: The necessity of non-language-centric explanations for language comprehension},
journal = {Cognition},
volume = {180},
pages = {135-157},
year = {2018},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2018.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0010027718301707},
author = {Uri Hasson and Giovanna Egidi and Marco Marelli and Roel M. Willems},
keywords = {Language, Neurobiology, Context},
abstract = {Recent decades have ushered in tremendous progress in understanding the neural basis of language. Most of our current knowledge on language and the brain, however, is derived from lab-based experiments that are far removed from everyday language use, and that are inspired by questions originating in linguistic and psycholinguistic contexts. In this paper we argue that in order to make progress, the field needs to shift its focus to understanding the neurobiology of naturalistic language comprehension. We present here a new conceptual framework for understanding the neurobiological organization of language comprehension. This framework is non-language-centered in the computational/neurobiological constructs it identifies, and focuses strongly on context. Our core arguments address three general issues: (i) the difficulty in extending language-centric explanations to discourse; (ii) the necessity of taking context as a serious topic of study, modeling it formally and acknowledging the limitations on external validity when studying language comprehension outside context; and (iii) the tenuous status of the language network as an explanatory construct. We argue that adopting this framework means that neurobiological studies of language will be less focused on identifying correlations between brain activity patterns and mechanisms postulated by psycholinguistic theories. Instead, they will be less self-referential and increasingly more inclined towards integration of language with other cognitive systems, ultimately doing more justice to the neurobiological organization of language and how it supports language as it is used in everyday life.}
}
@article{WAN2024103,
title = {Making knowledge graphs work for smart manufacturing: Research topics, applications and prospects},
journal = {Journal of Manufacturing Systems},
volume = {76},
pages = {103-132},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524001572},
author = {Yuwei Wan and Ying Liu and Zheyuan Chen and Chong Chen and Xinyu Li and Fu Hu and Michael Packianather},
keywords = {Smart manufacturing, Industry 4.0, Knowledge graph, Semantic modelling under industry 4.0, Knowledge reasoning},
abstract = {Smart manufacturing (SM) confronts several challenges inherently suited to knowledge graphs (KGs) capabilities. The first key challenge lies in the synthesis of complex and varied data surrounding the manufacturing context, which demands advanced semantic analysis and inference capabilities. The second main limitation is the contextualization of manufacturing systems and the exploitation of manufacturing domain knowledge, which requires a dynamic and holistic representation of knowledge. The last major obstacle arises from the facilitation of intricate decision-making processes towards correlated manufacturing ecosystems, which benefit from interconnected data structures that KGs excel at organizing. However, the existing survey studies concentrated on distinct facets of SM and offered isolated insights into KG applications while overlooking the interconnections between various KG technologies and their application across multiple domains. What specific role KGs should play in SM towards the aforementioned challenges, how to effectively harness KGs for these challenges, and the essential topics and methodologies required to make KGs functional remain underexplored. To explore the potential of KGs in SM, this study adopts a systematic approach to investigate, evaluate, and analyse current research on KGs, identifying core advancements and their implications for future manufacturing practices. Firstly, cutting-edge developments in the challenge-driven roles of KGs and KG techniques are identified, from knowledge extraction and mining to techniques for KG construction and updates, further extending to KG embedding, fusion, and reasoning—central to driving SM ecosystems. Specifically, the KG technologies for SM are depicted holistically, emphasizing the interplay of diverse KG techniques with a comprehensive framework. Subsequently, this foundation outlines and discusses key application scenarios of KGs from engineering design to predictive maintenance, covering the main representative stages of the manufacturing life cycle. Lastly, this study explores the intricate interplay of the practical challenges and advantages of KGs in manufacturing systems, pointing to emerging research avenues.}
}
@article{TAUJALE2024103894,
title = {Informatic challenges and advances in illuminating the druggable proteome},
journal = {Drug Discovery Today},
volume = {29},
number = {3},
pages = {103894},
year = {2024},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2024.103894},
url = {https://www.sciencedirect.com/science/article/pii/S1359644624000199},
author = {Rahil Taujale and Nathan Gravel and Zhongliang Zhou and Wayland Yeung and Krystof Kochut and Natarajan Kannan},
keywords = {protein evolution, orthology, network biology, machine learning, sequence embedding},
abstract = {The understudied members of the druggable proteomes offer promising prospects for drug discovery efforts. While large-scale initiatives have generated valuable functional information on understudied members of the druggable gene families, translating this information into actionable knowledge for drug discovery requires specialized informatics tools and resources. Here, we review the unique informatics challenges and advances in annotating understudied members of the druggable proteome. We demonstrate the application of statistical evolutionary inference tools, knowledge graph mining approaches, and protein language models in illuminating understudied protein kinases, pseudokinases, and ion channels.}
}
@incollection{HALER2025,
title = {Semantics: Pre-20th Century Theories},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00814-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041008140},
author = {Gerda Haßler},
keywords = {Meaning, Value, Extension, Intension, Arbitrary, Semantic change, Semasiology},
abstract = {The problem of how linguistic signs mean something and can designate real objects or relations is a central topic in theories of language. This article first examines the ontological question till the end of the 18th century and then concentrates on different approaches to the description of meaning. The development of theories of meaning shows a certain continuity of concepts such as ‘arbitrariness’, ‘extension’ and ‘intension’. The history of semantics in different countries did not unfold simultaneously. There are also major differences in the influences on the development of semantics, stemming from different philosophical traditions and different natural sciences.}
}
@article{SOUDY2020103613,
title = {UniprotR: Retrieving and visualizing protein sequence and functional information from Universal Protein Resource (UniProt knowledgebase)},
journal = {Journal of Proteomics},
volume = {213},
pages = {103613},
year = {2020},
issn = {1874-3919},
doi = {https://doi.org/10.1016/j.jprot.2019.103613},
url = {https://www.sciencedirect.com/science/article/pii/S1874391919303859},
author = {Mohamed Soudy and Ali Mostafa Anwar and Eman Ali Ahmed and Aya Osama and Shahd Ezzeldin and Sebaey Mahgoub and Sameh Magdeldin},
keywords = {Proteomics, UniProtKB, UniProt, Bioinformatics, R package},
abstract = {UniprotR is a software package designed to easily retrieve, cluster and visualize protein data from UniProt knowledgebase (UniProtKB) using R language. The package is implemented mainly to process, parse and illustrate proteomics data in a handy and time-saving approach allowing researchers to summarize all required protein information available at UniProtKB in a readable data frame, Excel CSV file, and/or graphical output. UniprotR generates a set of graphics including gene ontology, chromosomal location, protein scoring and status, protein networking, sequence phylogenetic tree, and physicochemical properties. In addition, the package supports clustering of proteins based on primary gene name or chromosomal location, facilitating additional downstream analysis.
Significance
In this work, we implemented a robust package for retrieving and visualizing information from multiple sources such UniProtKB, SWISS-MODEL, and STRING. UniprotR Contains functions that enable retrieving and cluster data in a handy way and visualize data in publishable graphs to facilitate researcher's work and fulfill their needs. UniprotR will aid in saving time for downstream data analysis instead of manual time consuming data analysis.
Availability and implementation
UniprotR released as free open source code under the license of GPLv3, and available in CRAN (The Comprehensive R Archive Network) and GitHub. (https://cran.r-project.org/web/packages/UniprotR/index.html). (https://github.com/Proteomicslab57357/UniprotR).}
}
@article{AMATO2020393,
title = {An abstract reasoning architecture for privacy policies monitoring},
journal = {Future Generation Computer Systems},
volume = {106},
pages = {393-400},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19324549},
author = {Flora Amato and Luigi Coppolino and Salvatore D’Antonio and Nicola Mazzocca and Francesco Moscato and Luigi Sgaglione},
keywords = {Monitoring architecture, Ontological model, Privacy},
abstract = {Privacy is a topic of increasing interest not only for scientific communities, but also for public opinion and for regulatory bodies from all Countries. The problem is that privacy management and enforcement are very dynamic activities and monitoring known fallacies of systems is really difficult, since their number grows up day by day. Moreover, monitoring systems for yet unknown vulnerabilities is, of course, even more difficult. Here we want to present a model and an architecture, based on semantics and reasoning, which is able to detect privacy problems by abstract reasoning on monitored information. The methodology we present here fetches data and evidences from logs and other files, applying model transformation techniques in order to populate semantics repository for enacting reasoning actions. We provide here some experimental results in order to prove the strength of the proposed approach.}
}
@article{CHEN2022100197,
title = {Research on multi-factory combination optimization based on DOSTAR},
journal = {Array},
volume = {15},
pages = {100197},
year = {2022},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2022.100197},
url = {https://www.sciencedirect.com/science/article/pii/S2590005622000480},
author = {Sen Chen and Jian Wang and Manting Yan and Chuntao Yang and Huihui Han},
keywords = {Combinatorial optimization, Knowledge discovery, Reinforcement learning, AHP, Domain ontology, Six-tuple, Ternary data fusion, CPS},
abstract = {With the development of industrial big data, it has become an important research direction to use combinatorial optimization to coordinate multi-objective problems in complex manufacturing scenarios with multiple factories. At present, most of the multi-objective problems are decomposed into single-objective solutions. However, it is difficult to resolve the contradiction between multiple goals. There are many participants in multi-objective problems and complex data types, so there is no suitable research method at present. Based on big data, this paper integrates various aspects of supply chain management of multiple factories, and proposes a DOSTAR combined model. On the one hand, it conducts knowledge discovery based on the fusion of human-cyber-physical ternary data, on the other hand, it conducts multi-objective optimization through knowledge structure. Among them, the most important thing is to establish the six-tuple as the basic model. Then the space weight, time weight and decision weight are obtained through the weight sub-model. Finally, the improved reinforcement learning algorithm is used to extract relevant new knowledge and complete multi-objective coordination. This article takes the supply chain management of Haier water heaters as an example, using the above-mentioned combined model, and the experimental results show that the purpose of improving performance has been achieved.}
}
@article{XIE2024,
title = {Identifying the Severity of Heart Valve Stenosis and Regurgitation Among a Diverse Population Within an Integrated Health Care System: Natural Language Processing Approach},
journal = {JMIR Cardio},
volume = {8},
year = {2024},
issn = {2561-1011},
doi = {https://doi.org/10.2196/60503},
url = {https://www.sciencedirect.com/science/article/pii/S2561101124000242},
author = {Fagen Xie and Ming-sum Lee and Salam Allahwerdy and Darios Getahun and Benjamin Wessler and Wansu Chen},
keywords = {echocardiography report, heart valve, stenosis, regurgitation, natural language processing, algorithm},
abstract = {Background
Valvular heart disease (VHD) is a leading cause of cardiovascular morbidity and mortality that poses a substantial health care and economic burden on health care systems. Administrative diagnostic codes for ascertaining VHD diagnosis are incomplete.
Objective
This study aimed to develop a natural language processing (NLP) algorithm to identify patients with aortic, mitral, tricuspid, and pulmonic valve stenosis and regurgitation from transthoracic echocardiography (TTE) reports within a large integrated health care system.
Methods
We used reports from echocardiograms performed in the Kaiser Permanente Southern California (KPSC) health care system between January 1, 2011, and December 31, 2022. Related terms/phrases of aortic, mitral, tricuspid, and pulmonic stenosis and regurgitation and their severities were compiled from the literature and enriched with input from clinicians. An NLP algorithm was iteratively developed and fine-trained via multiple rounds of chart review, followed by adjudication. The developed algorithm was applied to 200 annotated echocardiography reports to assess its performance and then the study echocardiography reports.
Results
A total of 1,225,270 TTE reports were extracted from KPSC electronic health records during the study period. In these reports, valve lesions identified included 111,300 (9.08%) aortic stenosis, 20,246 (1.65%) mitral stenosis, 397 (0.03%) tricuspid stenosis, 2585 (0.21%) pulmonic stenosis, 345,115 (28.17%) aortic regurgitation, 802,103 (65.46%) mitral regurgitation, 903,965 (73.78%) tricuspid regurgitation, and 286,903 (23.42%) pulmonic regurgitation. Among the valves, 50,507 (4.12%), 22,656 (1.85%), 1685 (0.14%), and 1767 (0.14%) were identified as prosthetic aortic valves, mitral valves, tricuspid valves, and pulmonic valves, respectively. Mild and moderate were the most common severity levels of heart valve stenosis, while trace and mild were the most common severity levels of regurgitation. Males had a higher frequency of aortic stenosis and all 4 valvular regurgitations, while females had more mitral, tricuspid, and pulmonic stenosis. Non-Hispanic Whites had the highest frequency of all 4 valvular stenosis and regurgitations. The distribution of valvular stenosis and regurgitation severity was similar across race/ethnicity groups. Frequencies of aortic stenosis, mitral stenosis, and regurgitation of all 4 heart valves increased with age. In TTE reports with stenosis detected, younger patients were more likely to have mild aortic stenosis, while older patients were more likely to have severe aortic stenosis. However, mitral stenosis was opposite (milder in older patients and more severe in younger patients). In TTE reports with regurgitation detected, younger patients had a higher frequency of severe/very severe aortic regurgitation. In comparison, older patients had higher frequencies of mild aortic regurgitation and severe mitral/tricuspid regurgitation. Validation of the NLP algorithm against the 200 annotated TTE reports showed excellent precision, recall, and F1-scores.
Conclusions
The proposed computerized algorithm could effectively identify heart valve stenosis and regurgitation, as well as the severity of valvular involvement, with significant implications for pharmacoepidemiological studies and outcomes research.}
}
@article{LI2020101277,
title = {Rethinking ecolinguistics from a distributed language perspective},
journal = {Language Sciences},
volume = {80},
pages = {101277},
year = {2020},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2020.101277},
url = {https://www.sciencedirect.com/science/article/pii/S0388000120300097},
author = {Jia Li and Sune Vork Steffensen and Guowen Huang},
keywords = {Ecolinguistics, Distributed language perspective, Naturalized view of language, Extended ecology hypothesis, Bio-ecology, Organism-environment system},
abstract = {In this article, we discuss the current status of ecolinguistics and key issues in the field regarding the positioning of ecolinguistics, and the research objects, aims and methodologies in studies conducted under the label of “ecolinguistics” or “language ecology” (Section 2). Having discussed the inconsistencies and problems in these studies, we present a naturalized view of language from a distributed language perspective (DLP) which has the potential to contribute to a new understanding of the research object in ecolinguistics (Section 3). Under this new understanding of the nature of language and ecology, we propose possible research topics to be explored and potential methodologies to be adopted in ecolinguistics (Section 4). From a distributed understanding of language, ecology, and the research domain and methodologies proposed for future ecolinguistic studies, we argue that ecolinguistics is both an alternative to linguistics and a branch of ecology (Section 5). Future work is needed to explore the research domain proposed for ecolinguistics here, and multidisciplinary cooperation is necessary for the future development of the field.}
}
@article{PULVERMULLER2023102511,
title = {Neurobiological mechanisms for language, symbols and concepts: Clues from brain-constrained deep neural networks},
journal = {Progress in Neurobiology},
volume = {230},
pages = {102511},
year = {2023},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2023.102511},
url = {https://www.sciencedirect.com/science/article/pii/S0301008223001120},
author = {Friedemann Pulvermüller},
keywords = {Neurocognition, Neurocomputation, Semantics, Language learning, Deep neural network, Brain-constrained model},
abstract = {Neural networks are successfully used to imitate and model cognitive processes. However, to provide clues about the neurobiological mechanisms enabling human cognition, these models need to mimic the structure and function of real brains. Brain-constrained networks differ from classic neural networks by implementing brain similarities at different scales, ranging from the micro- and mesoscopic levels of neuronal function, local neuronal links and circuit interaction to large-scale anatomical structure and between-area connectivity. This review shows how brain-constrained neural networks can be applied to study in silico the formation of mechanisms for symbol and concept processing and to work towards neurobiological explanations of specifically human cognitive abilities. These include verbal working memory and learning of large vocabularies of symbols, semantic binding carried by specific areas of cortex, attention focusing and modulation driven by symbol type, and the acquisition of concrete and abstract concepts partly influenced by symbols. Neuronal assembly activity in the networks is analyzed to deliver putative mechanistic correlates of higher cognitive processes and to develop candidate explanations founded in established neurobiological principles.}
}
@article{PORATREIN2021268,
title = {Early prediction of encephalopathic transformation in children with benign epilepsy with centro-temporal spikes},
journal = {Brain and Development},
volume = {43},
number = {2},
pages = {268-279},
year = {2021},
issn = {0387-7604},
doi = {https://doi.org/10.1016/j.braindev.2020.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S0387760420302424},
author = {Adi {Porat Rein} and Uri Kramer and Moran {Hausman Kedem} and Aviva Fattal-Valevski and Alexis Mitelpunkt},
keywords = {Machine learning, Benign childhood epilepsy with centro-temporal spikes (BECTS), Landau–Kleffner syndrome (LKS), Electrical status epilepticus during slow-wave sleep (ESES), Epileptic encephalopathy with continuous spike-and-wave during sleep (ECSWS), Ontology},
abstract = {Background
Most children with Benign epilepsy with centro-temporal spikes (BECTS) undergo remission during late adolescence and do not require treatment. In a small group of patients, the condition may evolve to encephalopathic syndromes including epileptic encephalopathy with continuous spike-and-wave during sleep (ECSWS), or Landau-Kleffner Syndrome (LKS). Development of prediction models for early identification of at-risk children is of utmost importance.
Aim
To develop a predictive model of encephalopathic transformation using data-driven approaches, reveal complex interactions to identify potential risk factors.
Methods
Data were collected from a cohort of 91 patients diagnosed with BECTS treated between the years 2005–2017 at a pediatric neurology institute. Data on the initial presentation was collected based on a novel BECTS ontology and used to discover potential risk factors and to build a predictive model. Statistical and machine learning methods were compared.
Results
A subgroup of 18 children had encephalopathic transformation. The least absolute shrinkage and selection operator (LASSO) regression Model with Elastic Net was able to successfully detect children with ECSWS or LKS. Sensitivity and specificity were 0.83 and 0.44. The most notable risk factors were fronto-temporal and temporo-parietal localization of epileptic foci, semiology of seizure involving dysarthria or somatosensory auras.
Conclusion
Novel prediction model for early identification of patients with BECTS at risk for ECSWS or LKS. This model can be used as a screening tool and assist physicians to consider special management for children predicted at high-risk. Clinical application of machine learning methods opens new frontiers of personalized patient care and treatment.}
}
@article{GOH20244175,
title = {Automated Compliance Checking System for Structural Design Codes in a BIM Environment},
journal = {KSCE Journal of Civil Engineering},
volume = {28},
number = {10},
pages = {4175-4189},
year = {2024},
issn = {1226-7988},
doi = {https://doi.org/10.1007/s12205-024-1121-5},
url = {https://www.sciencedirect.com/science/article/pii/S1226798825001072},
author = {Wonhui Goh and Jaeguk Jang and Sang I. Park and Bong-Hyuck Choi and Heuiseok Lim and Goangseup Zi},
keywords = {Automated compliance checking (ACC), Structural calculations, Building information modeling (BIM), open BIM format, Industry foundation classes (IFC)},
abstract = {This study proposes a framework for developing an automated compliance checking (ACC) system that supports the verification of structural calculations in openBIM format. The framework consists of four main aspects: 1) classification of design codes depending on structural components and limit states to be checked; 2) preparation of a structural model according to Industry Foundation Classes; 3) interpretation of conditions in design codes into a machine-readable rule language; and 4) design of an ACC system, with a focus on the rule flow in the structural design code. The framework was demonstrated through a detailed example of an ACC system using ifcOWL, SWQRL, and the Drools inference engine. The ACC system was developed to check the strength and serviceability of a prestressed concrete girder bridge and its components. The developed ACC system verified the actual design example. The proposed framework can help develop ACC systems in structural engineering and can be customized to meet user requirements and accommodate various data sources and design codes.}
}
@article{RUIZLOPEZ2025103950,
title = {A preliminary conceptual structure for Computer-based Process Maturity Models, using a Cone-Based Conceptual Network and NIAM diagrams},
journal = {Computer Standards & Interfaces},
volume = {93},
pages = {103950},
year = {2025},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2024.103950},
url = {https://www.sciencedirect.com/science/article/pii/S0920548924001193},
author = {Francisco Ruiz-Lopez and Jean-Pierre Micaelli and Eric Bonjour and Javier Ortiz-Hernandez},
keywords = {Process maturity model, Audit, Conceptual model, Trace-based system, NIAM},
abstract = {Process maturity models support audit and assessment missions (AAMs) focusing on organizational routines. Under Traditional Process Maturity Models (TP2Ms), auditors use document-based surveys whereas organizations produce data in their daily activities employing Information Technologies (IT). Therefore, how to bridge the gap between IT capabilities and AAMs? One answer could be to design a Trace-Based System (TBS) capturing raw data of the daily activity and transmuting it into transformed traces from which it can be possible to automatically reconstruct processes and evaluate their maturity. Despite its obvious practical value, this processing is not so easy. A first phase must be realized, which consists of modeling the conceptual structure of current TP2Ms and of future possible Computer-based Process Maturity Models (CP2Ms) based on TBSs. To achieve this goal, this paper proposes a Cone-Based Conceptual Network (CBCN) to give the big picture of TP2Ms' and CP2Ms' scopes, then it proposes to model this CBCN with the use of Nijssen Information Analysis Method (NIAM) and to verify the semantic consistency of this preliminary conceptual structure. The result is a first step (at an early stage) of the development of computer-based (or trace-based) process maturity assessment tools. It allows auditors and IT specialists to have a big picture of the domain of interest and to map the different knowledge areas they need to acquire and combine to perform AAMs.}
}
@article{LINDVALL2022e29,
title = {Natural Language Processing to Identify Advance Care Planning Documentation in a Multisite Pragmatic Clinical Trial},
journal = {Journal of Pain and Symptom Management},
volume = {63},
number = {1},
pages = {e29-e36},
year = {2022},
issn = {0885-3924},
doi = {https://doi.org/10.1016/j.jpainsymman.2021.06.025},
url = {https://www.sciencedirect.com/science/article/pii/S0885392421004280},
author = {Charlotta Lindvall and Chih-Ying Deng and Edward Moseley and Nicole Agaronnik and Areej El-Jawahri and Michael K. Paasche-Orlow and Joshua R. Lakin and Angelo Volandes and The ACP-PEACE Investigators, James A. Tulsky},
keywords = {Natural language processing, clinical notes, advance care planning, pragmatic clinical trial, advanced cancer},
abstract = {Context
Large multisite clinical trials studying decision-making when facing serious illness require an efficient method for abstraction of advance care planning (ACP) documentation from clinical text documents. However, the current gold standard method of manual chart review is time-consuming and unreliable.
Objectives
To evaluate the ability to use natural language processing (NLP) to identify ACP documention in clinical notes from patients participating in a multisite trial.
Methods
Patients with advanced cancer followed in three disease-focused oncology clinics at Duke Health, Mayo Clinic, and Northwell Health were identified using administrative data. All outpatient and inpatient notes from patients meeting inclusion criteria were extracted from electronic health records (EHRs) between March 2018 and March 2019. NLP text identification software with semi-automated chart review was applied to identify documentation of four ACP domains: (1) conversations about goals of care, (2) limitation of life-sustaining treatment, (3) involvement of palliative care, and (4) discussion of hospice. The performance of NLP was compared to gold standard manual chart review.
Results
435 unique patients with 79,797 notes were included in the study. In our validation data set, NLP achieved F1 scores ranging from 0.84 to 0.97 across domains compared to gold standard manual chart review. NLP identified ACP documentation in a fraction of the time required by manual chart review of EHRs (1-5 minutes per patient for NLP, vs. 30-120 minutes for manual abstraction).
Conclusion
NLP is more efficient and as accurate as manual chart review for identifying ACP documentation in studies with large patient cohorts.}
}
@incollection{GYRARD2022199,
title = {Chapter 10 - Reasoning over personalized healthcare knowledge graph: a case study of patients with allergies and symptoms},
editor = {Sanju Tiwari and Fernando {Ortiz Rodriguez} and M.A. Jabbar},
booktitle = {Semantic Models in IoT and eHealth Applications},
publisher = {Academic Press},
pages = {199-225},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-91773-5},
doi = {https://doi.org/10.1016/B978-0-32-391773-5.00016-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917735000169},
author = {Amelie Gyrard and Utkarshani Jaimini and Manas Gaur and Saeedeh Shekharpour and Krishnaprasad Thirunarayan and Amit Sheth},
keywords = {Personalized healthcare knowledge graph, Contextualization, Ontology, Semantic data interoperability, Internet of things, FAIR principles, Reusability, Semantic web of things, Data analytics, Rule-based reasoning, Inference engine},
abstract = {Background: Current health applications (e.g., Google Fit) based on devices (e.g., Fitbit) often are limited to data visualization, summarization, or statistics-based models to understand and explore the patient data. The data must be more meaningful for the patient in a contextualized and personalized manner. Objective: A knowledge-based reasoning architecture is designed to collect, manage, integrate, and analyze multimodal health data from sources such as smart phone applications, wearable devices, Internet of Things (IoT) devices, and environmental sensors. The architecture and associated methods with several use cases are demonstrated in the chapter. Methods: A personalized health knowledge graph is developed to capture personalized and contextualized patient data from smartphone applications, wearables, and environmental sensors. It converts the data into knowledge using relevant medical knowledge from existing ontologies. The personalized health knowledge graph and its use cases consists of (1) Kno.e.sis asthma ontology, whose core model (e.g., patient) is generic enough to be reused for other use cases, and (2) a rule-based reasoning engine, which infers high-level abstractions from data annotated with Kno.e.sis asthma ontology to provide suggestions to patients. Results: The designed architecture is comprised of components compliant with each other: (1) A personalized health knowledge graph, which integrates cross-domain knowledge (asthma, weather, W3C SOSA SSN, smart home), (2) an automatic semantic annotation engine, (3) a rule (IF THEN ELSE) data set, which reuses domain expertise, (4) a rule-based inference engine to automatically infer new abstractions, and (5) a generic query engine to query inferred data and provides suggestions for a better health management. The architecture reuses and integrates knowledge in a machine-processable form to replace human interpretation as a long-term goal. Conclusion: The knowledge graph based reasoning is comprised of a set of tutorials with SPARQL queries and end-to-end scenarios (e.g., asthma, obesity, allergies to food). The methodology used to develop the knowledge graph can be generalized, refined, and reused for other diseases or even to other domains such as agriculture with smart irrigation to deal with different crop types, robotics in smart home for ageing people, smart energy, etc. The health knowledge graph can be used as a gold standard to design KG made with Machine Learning algorithms automatically.}
}
@article{KADDOURA2024101911,
title = {EnhancedBERT: A feature-rich ensemble model for Arabic word sense disambiguation with statistical analysis and optimized data collection},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {1},
pages = {101911},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101911},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823004652},
author = {Sanaa Kaddoura and Reem Nassar},
keywords = {Arabic natural language processing, Word sense disambiguation, Machine learning, Knowledge-based, BERT, Performance evaluation},
abstract = {Accurate assignment of meaning to a word based on its context, known as Word Sense Disambiguation (WSD), remains challenging across languages. Extensive research aims to develop automated methods for determining word senses in different contexts. However, the literature lacks the presence of datasets generated for the Arabic language WSD. This paper presents a dataset comprising a hundred polysemous Arabic words. Each word in the dataset encompasses 3–8 distinct senses, with ten example sentences per sense. Some statistical operations are conducted to gain insights into the dataset, enlightening its characteristics and properties. Subsequently, a novel WSD approach is proposed to utilize similarity measures and find the overlap between contextual information and dictionary definitions. The proposed method uses the power of BERT, a pre-trained language model, to enable effective Arabic word disambiguation. In training, new features are integrated to improve the model's ability to differentiate between various senses of words. The proposed BERT models are combined to compose an ensemble model architecture to improve the classification performances. The performance of the WSD system outperforms state-of-the-art systems, achieving an approximate F1-score of 96 %. Statistical analyses are performed to evaluate the overall performance of the WSD approach by providing additional information on model predictions. A case study was implemented to test the effectiveness of WSD in sentiment analysis, a downstream task.}
}
@article{WANG2025100929,
title = {Adipose tissue may not be a major player in the inflammatory pathogenesis of Autism Spectrum Disorder},
journal = {Brain, Behavior, & Immunity - Health},
volume = {43},
pages = {100929},
year = {2025},
issn = {2666-3546},
doi = {https://doi.org/10.1016/j.bbih.2024.100929},
url = {https://www.sciencedirect.com/science/article/pii/S2666354624002072},
author = {Baojiang Wang and Yueyuan Qin and Yong Chen and Xiujie Zheng and Yanjuan Chen and Juan Zhao and Feng Zhang and Shan Duan},
keywords = {Inflammation, Adipokines, Autism spectrum disorder, Inflammatory cytokines, Insulin},
abstract = {Purpose
Autism spectrum disorder (ASD) is a complex neurodevelopmental disorder increasingly recognized for its strong association with chronic inflammation. Adipose tissue functions as an endocrine organ and can secrete inflammatory cytokines to mediate inflammation. However, its involvement in ASD-related inflammation remains unclear. The present study aimed to clarify the role of adipose tissue in inducing inflammatory responses associated with ASD.
Methods
A total of 36 children with ASD and 18 unrelated healthy controls, aged 2–14.5 years, were enrolled in the study. The up-regulated differentially expressed genes from the GSE18123 dataset were subjected to gene ontology (GO) enrichment analysis to explore ASD-associated pathways. Plasma cytokines and adipokines levels were quantified using Milliplex MAP immunoaffinity technology. The BTBR T + Itprtf/J (BTBR) mice that are known for their core ASD behavioral traits and inflammatory phenotypes were employed as an animal ASD model to verify the key clinical findings.
Results
GO enrichment analyses revealed immune dysfunction in ASD. Symptom analysis showed that the recruited individuals had typical autistic symptoms. Plasma analysis showed no significant difference in adipokines levels, including adiponectin, leptin, resistin, adipsin, and lipocalin-2, between the ASD and control groups. However, markedly elevated levels of IL-6, IL-8, and tumor necrosis factor (TNF-α) were detected in children with ASD, suggesting that the inflammatory state is independent of adipokines. Similar results were also observed in BTBR autistic mice. Notably, levels of insulin, which are closely related to the exertion of adipokines function, also showed no significant changes.
Conclusions
Our findings suggest that inflammation in ASD likely originates from non-adipocyte sources, implying that adipose tissue may not play a major role in inflammatory pathogenesis of ASD. Consequently, targeting adipose-related inflammation may not be an effective treatment approach, providing new directions for the development of targeted interventions.}
}
@article{SCHNEIDER2023102149,
title = {Large-scale identification of undiagnosed hepatic steatosis using natural language processing},
journal = {eClinicalMedicine},
volume = {62},
pages = {102149},
year = {2023},
issn = {2589-5370},
doi = {https://doi.org/10.1016/j.eclinm.2023.102149},
url = {https://www.sciencedirect.com/science/article/pii/S2589537023003267},
author = {Carolin V. Schneider and Tang Li and David Zhang and Anya I. Mezina and Puru Rattan and Helen Huang and Kate Townsend Creasy and Eleonora Scorletti and Inuk Zandvakili and Marijana Vujkovic and Leonida Hehl and Jacob Fiksel and Joseph Park and Kirk Wangensteen and Marjorie Risman and Kyong-Mi Chang and Marina Serper and Rotonya M. Carr and Kai Markus Schneider and Jinbo Chen and Daniel J. Rader},
keywords = {Liver disease, NAFLD, Biopsy, EHR, Natural language processing},
abstract = {Summary
Background
Nonalcoholic fatty liver disease (NAFLD) is a major cause of liver-related morbidity in people with and without diabetes, but it is underdiagnosed, posing challenges for research and clinical management. Here, we determine if natural language processing (NLP) of data in the electronic health record (EHR) could identify undiagnosed patients with hepatic steatosis based on pathology and radiology reports.
Methods
A rule-based NLP algorithm was built using a Linguamatics literature text mining tool to search 2.15 million pathology report and 2.7 million imaging reports in the Penn Medicine EHR from November 2014, through December 2020, for evidence of hepatic steatosis. For quality control, two independent physicians manually reviewed randomly chosen biopsy and imaging reports (n = 353, PPV 99.7%).
Findings
After exclusion of individuals with other causes of hepatic steatosis, 3007 patients with biopsy-proven NAFLD and 42,083 patients with imaging-proven NAFLD were identified. Interestingly, elevated ALT was not a sensitive predictor of the presence of steatosis, and only half of the biopsied patients with steatosis ever received an ICD diagnosis code for the presence of NAFLD/NASH. There was a robust association for PNPLA3 and TM6SF2 risk alleles and steatosis identified by NLP. We identified 234 disorders that were significantly over- or underrepresented in all subjects with steatosis and identified changes in serum markers (e.g., GGT) associated with presence of steatosis.
Interpretation
This study demonstrates clear feasibility of NLP-based approaches to identify patients whose steatosis was indicated in imaging and pathology reports within a large healthcare system and uncovers undercoding of NAFLD in the general population. Identification of patients at risk could link them to improved care and outcomes.
Funding
The study was funded by US and German funding sources that did provide financial support only and had no influence or control over the research process.}
}
@article{ZHANG2022104834,
title = {A scoping review of semantic integration of health data and information},
journal = {International Journal of Medical Informatics},
volume = {165},
pages = {104834},
year = {2022},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2022.104834},
url = {https://www.sciencedirect.com/science/article/pii/S1386505622001484},
author = {Hansi Zhang and Tianchen Lyu and Pengfei Yin and Sarah Bost and Xing He and Yi Guo and Mattia Prosperi and Willian R. Hogan and Jiang Bian},
keywords = {Semantic data integration, Common data model, Common data element, Ontology, Systematic review and -analyses (PRISMA) framework},
abstract = {Objective
We summarized a decade of new research focusing on semantic data integration (SDI) since 2009, and we aim to: (1) summarize the state-of-art approaches on integrating health data and information; and (2) identify the main gaps and challenges of integrating health data and information from multiple levels and domains.
Materials and Methods
We used PubMed as our focus is applications of SDI in biomedical domains and followed the Preferred Reporting Items for Systematic Review and Meta-Analyses (PRISMA) to search and report for relevant studies published between January 1, 2009 and December 31, 2021. We used Covidence—a systematic review management system—to carry out this scoping review.
Results
The initial search from PubMed resulted in 5,326 articles using the two sets of keywords. We then removed 44 duplicates and 5,282 articles were retained for abstract screening. After abstract screening, we included 246 articles for full-text screening, among which 87 articles were deemed eligible for full-text extraction. We summarized the 87 articles from four aspects: (1) methods for the global schema; (2) data integration strategies (i.e., federated system vs. data warehousing); (3) the sources of the data; and (4) downstream applications.
Conclusion
SDI approach can effectively resolve the semantic heterogeneities across different data sources. We identified two key gaps and challenges in existing SDI studies that (1) many of the existing SDI studies used data from only single-level data sources (e.g., integrating individual-level patient records from different hospital systems), and (2) documentation of the data integration processes is sparse, threatening the reproducibility of SDI studies.}
}
@article{MAO2023156390,
title = {Identification of IL-8 in CSF as a potential biomarker in sepsis-associated encephalopathy},
journal = {Cytokine},
volume = {172},
pages = {156390},
year = {2023},
issn = {1043-4666},
doi = {https://doi.org/10.1016/j.cyto.2023.156390},
url = {https://www.sciencedirect.com/science/article/pii/S1043466623002685},
author = {Yingying Mao and Amin Zhang and Haitao Yang and Chen Zhang},
keywords = {Sepsis-associated encephalopathy, Bioinformatics analysis, IL-8, Cytokines, Biomarkers},
abstract = {Background
Sepsis-associated encephalopathy (SAE) is frequently present at the acute and chronic phase of sepsis, which is characterized by delirium, coma, and cognitive dysfunction. Despite the increased morbidity and mortality of SAE, the pathogenesis of SAE remains unclear. This study aims to discover the potential biomarkers, so as to clear the pathogenesis potentially contributing to the development of SAE and provide new therapeutic strategies for the treatment of SAE.
Methods
The GSE135838 dataset was obtained from the Gene Expression Omnibus (GEO) database and utilized for analysis the differentially expressed genes (DEGs). The DEGs were analyzed by limma package of R language and the extracellular protein-differentially expressed genes (EP-DEGs) were screened by the Human Protein Atlas (HPA) and UniProt database. Gene Ontology (GO) analysis and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway enrichment analysis were carried out to analyze the function and pathway of EP-DEGs. STRING, Cytoscape, MCODE and Cytohubba were used to construct a protein–protein interaction (PPI) network and screen key EP-DEGs. Key EP-DEGs levels were detected in the cerebrospinal fluid (CSF) of SAE patients and non-sepsis patients with critical illness. ROC curve was used to evaluate the diagnostic of SAE.
Results
We screened 82 EP-DEGs from DEGs. EP-DEGs were enriched in cytokine-cytokine receptor interaction, IL-17 signaling pathway and NOD-like receptor signaling pathway. We identified 2 key extracellular proteins IL-1B and IL-8. We clinically verified that IL-6 and IL-8 levels were increased in CSF of SAE patients and CSF IL-8 (AUC = 0.882, 95 % CI = 0.775–0.988) had a higher accuracy in the diagnosis of SAE than CSF IL-6 (AUC = 0.824, 95 % CI = 0.686–0.961). Furthermore, we found that the IL-8 levels in CSF might not associated with Glasgow Coma Scale (GCS) scores of SAE patients.
Conclusion
IL-8 may be the key extracellular cytokine in the pathogenesis of SAE. Bioinformatics methods were used to explore the biomarkers of SAE and validated the results in clinical samples. Our findings indicate that the IL-8 in CSF might be the potential diagnostic biomarker and therapeutic target in SAE.}
}
@article{XIONG2019100966,
title = {Onsite video mining for construction hazards identification with visual relationships},
journal = {Advanced Engineering Informatics},
volume = {42},
pages = {100966},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100966},
url = {https://www.sciencedirect.com/science/article/pii/S1474034619305397},
author = {Ruoxin Xiong and Yuanbin Song and Heng Li and Yuxuan Wang},
keywords = {Scene graph, Hazards identification, Safety regulations, Ontology, Video mining},
abstract = {Widely-used video monitoring systems provide a large corpus of unstructured image data on construction sites. Although previous developed vision-based approaches can be used for hazards recognition in terms of detecting dangerous objects or unsafe operations, such detection capacity is often limited due to lack of semantic representation of visual relationships between/among the components or crews in the workplace. Accordingly, the formal representation of textural criteria for checking improper relationships should also be improved. In this regard, an Automated Hazards Identification System (AHIS) is developed to evaluate the operation descriptions generated from site videos against the safety guidelines extracted from the textual documents with the assistance of the ontology of construction safety. In particular, visual relationships are modeled as a connector between site components/operators. Moreover, both visual descriptions of site operations and semantic representations of safety guidelines are coded in the three-tuple format and then automatically converted into Horn clauses for reasoning out the potential risks. A preliminary implementation of the system was tested on two separate onsite video clips. The results showed that two types of crucial hazards, i.e., failure to wear a helmet and walking beneath the cane, were successfully identified with three rules from Safety Handbook for Construction Site Workers. In addition, the high-performance results of Recall@50 and Recall@100 demonstrated that the proposed visual relationship detection method is promising in enriching the semantic representation of operation facts extracted from site videos, which may lead to better automation in the detection of construction hazards.}
}
@article{GIMENEZMEDINA2023120611,
title = {The innovation challenge in Spain: A Delphi study},
journal = {Expert Systems with Applications},
volume = {230},
pages = {120611},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120611},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423011132},
author = {M. Giménez-Medina and J.G. Enríquez and M.A. Olivero and F.J. Domínguez-Mayo},
keywords = {Innovation assessment, Innovation framework, Capability maturity model, Delphi study},
abstract = {Public funding for information and communication technology (ICT) innovation in Spain appears to be slow, bureaucratic, highly restrictive, and not agile. Therefore, the innovation process is negatively affected. These restrictions could be attributed to inadequate trust from public funders toward executors and ontological problems regarding the definitions of ICT innovation (i.e., the I+D+i formula), affecting all Quadruple Helix stakeholders. In this study, a Delphi study was proposed to reach a consensus among 81 experts (i.e., innovation managers, public funders, and consultants) to validate this hypothesis. The study included 41 statements and 59 questions organized into the following five objectives: (1) concept of innovation, (2) public funding and its restrictions, (3) theoretical model of innovation, (4) public funders’ trust and executors’ freedom, (5) assessment of capabilities and maturity for innovation. The experts discussed, evaluated, and reached a consensus, after two rounds, on 52 of the 59 questions. The results revealed wide dispersion of the proposed ICT innovation questions. They demonstrated that the innovation management ecosystem in Spain’s ICT context is immature and the I+D+i formula did not represent the innovation process. The study reached a consensus on requirements for an Agile Innovation Funding Framework (AIFF) oriented toward obtaining an improved competitive advantage for ICT products or services based on trust, transparency, inspection, and adaptation principles. The results revealed that a joint framework involving public funders and executors based on organizational capability and maturity positively affects the innovation process. The capabilities of the executors should be standardized and measured, and public funders must move from supervisors to mentors to acquire new capabilities. Furthermore, innovation regulation and the various types of calls for proposals should be analyzed globally to change their fiscal and controlling nature restricting innovation.}
}
@article{NYBERG2025110368,
title = {Prenatal exposure to methadone or buprenorphine alters transcriptional networks associated with synaptic signaling in newborn rats},
journal = {Neuropharmacology},
volume = {270},
pages = {110368},
year = {2025},
issn = {0028-3908},
doi = {https://doi.org/10.1016/j.neuropharm.2025.110368},
url = {https://www.sciencedirect.com/science/article/pii/S0028390825000747},
author = {Henriette Nyberg and Inger Lise Bogen and Nur Duale and Jannike Mørch Andersen},
keywords = {Buprenorphine, Methadone, Opioid use disorder, Prenatal exposure, RNA sequencing, Synapse},
abstract = {While the use of methadone or buprenorphine during pregnancy is beneficial for the mother's health compared to illicit opioid use, prenatal exposure to these medications may have adverse consequences for the unborn child. However, the underlying molecular mechanisms of prenatal opioid exposure on neurodevelopment remain poorly understood. Hence, this study aimed to investigate gene expression changes, focusing on synapse-related genes, in cerebral tissue from newborn rats prenatally exposed to methadone or buprenorphine. Female Sprague-Dawley rats were exposed to methadone (10 mg/kg/day), buprenorphine (1 mg/kg/day), or sterile water through osmotic minipumps during pregnancy. Total RNA was isolated from the cerebrum on postnatal day 2 and analyzed using RNA-sequencing. Analyses of differentially expressed genes (DEGs) and enriched biological processes were conducted to compare the gene expression profiles between treatment groups within each sex. Prenatal buprenorphine exposure resulted in 598 DEGs (333 up- and 265 downregulated) in males and 175 (75 up- and 100 downregulated) in females, while prenatal methadone exposure resulted in 335 DEGs (224 up- and 111 downregulated) in males and 201 (57 up- and 144 downregulated) in females. Gene ontology analyses demonstrated that enriched biological processes included synaptic signaling, immune responses, and apoptosis. Analysis of the DEGs using the synapse database SynGO revealed that males prenatally exposed to buprenorphine displayed the highest number of enriched synapse-related biological process terms. Understanding gene expression changes following prenatal methadone or buprenorphine exposure is crucial to uncover the mechanisms underlying behavioral alterations and to develop interventions to mitigate the impact of opioid exposure on neurodevelopment.}
}
@article{PENG2023100380,
title = {What Is a Multi-Modal Knowledge Graph: A Survey},
journal = {Big Data Research},
volume = {32},
pages = {100380},
year = {2023},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2023.100380},
url = {https://www.sciencedirect.com/science/article/pii/S2214579623000138},
author = {Jinghui Peng and Xinyu Hu and Wenbo Huang and Jian Yang},
keywords = {Multi-modal, MMKG, Structure, Knowledge representation, Ontology},
abstract = {With the explosive growth of multi-modal information on the Internet, the multi-modal knowledge graph (MMKG) has become an important research topic in knowledge graphs to meet the needs of data management and application. Most research on MMKG has taken image-text data as the research object and used the multi-modal deep learning approach to process multi-modal data. In comparison, the structure of the MMKG is no uniform statement. This paper focuses on MMKG, introduces the related theories of multi-modal knowledge, and analyzes several common ideas about its construction. The survey also explains the structural evolution, proposes mirror node alignment to represent cross-modal knowledge for MMKG, lists some tasks' difficulties, and ultimately gives a sample MMKG for the news scene.}
}
@article{TUSHKANOVA20191150,
title = {Knowledge Net: Model and System for Accumulation, Representation, and Use of Knowledge},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {1150-1155},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.351},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319313291},
author = {Olga Tushkanova and Vladimir Samoylov},
keywords = {single shared information space, knowledge model, knowledge representation, aspect, ontology, knowledge graph, graph data model},
abstract = {The paper describes data and knowledge accumulation model named Knowledge Net developed for the company Info Wings LLC. It proposed as a model for formalizing, presenting, and using of knowledge and data within single shared information space of an intelligent enterprise, for example, within the framework of Industry 4.0. The key features of Knowledge Net are the support of multi-aspect description of entities, the possibility of expanding the network of concepts and entities, and the dynamic evolution of the aspects structure. The Knowledge Net is based on a graph data model and it is the part of ongoing open source application development of a digital enterprise platform. The paper provides an example of using Knowledge Net software to describe objects of a manufacturing enterprise. Current state analysis has not revealed complete analogues of Knowledge Net model for accumulation, presentation, and use of data and knowledge or the corresponding software that justifies the novelty of the model described in the paper.}
}
@article{BILAL2025e374,
title = {NLP for Analyzing Electronic Health Records and Clinical Notes in Cancer Research: A Review},
journal = {Journal of Pain and Symptom Management},
volume = {69},
number = {5},
pages = {e374-e394},
year = {2025},
issn = {0885-3924},
doi = {https://doi.org/10.1016/j.jpainsymman.2025.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0885392425000375},
author = {Muhammad Bilal and Ameer Hamza and Nadia Malik},
keywords = {Natural language processing, Electronic health records, Clinical notes, Cancer, Information extraction, Text classification},
abstract = {This review examines the application of natural language processing (NLP) techniques in cancer research using electronic health records (EHRs) and clinical notes. It addresses gaps in existing literature by providing a broader perspective than previous studies focused on specific cancer types or applications. A comprehensive literature search in the Scopus database identified 94 relevant studies published between 2019 and 2024. The analysis revealed a growing trend in NLP applications for cancer research, with information extraction (47 studies) and text classification (40 studies) emerging as predominant NLP tasks, followed by named entity recognition (7 studies). Among cancer types, breast, lung, and colorectal cancers were found to be the most studied. A significant shift from rule-based and traditional machine learning approaches to advanced deep learning techniques and transformer-based models was observed. It was found that dataset sizes used in existing studies varied widely, ranging from small, manually annotated datasets to large-scale EHRs. The review highlighted key challenges, including the limited generalizability of proposed solutions and the need for improved integration into clinical workflows. While NLP techniques show significant potential in analyzing EHRs and clinical notes for cancer research, future work should focus on improving model generalizability, enhancing robustness in handling complex clinical language, and expanding applications to understudied cancer types. The integration of NLP tools into palliative medicine and addressing ethical considerations remain crucial for utilizing the full potential of NLP in enhancing cancer diagnosis, treatment, and patient outcomes. This review provides valuable insights into the current state and future directions of NLP applications in cancer research.}
}
@article{WANG2025761,
title = {MMCSD: Multi-Modal Knowledge Graph Completion Based on Super-Resolution and Detailed Description Generation},
journal = {Computers, Materials and Continua},
volume = {83},
number = {1},
pages = {761-783},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2025.060395},
url = {https://www.sciencedirect.com/science/article/pii/S154622182500298X},
author = {Huansha Wang and Ruiyang Huang and Qinrang Liu and Shaomei Li and Jianpeng Zhang},
keywords = {Multi-modal knowledge graph, knowledge graph completion, multi-modal fusion},
abstract = {Multi-modal knowledge graph completion (MMKGC) aims to complete missing entities or relations in multi-modal knowledge graphs, thereby discovering more previously unknown triples. Due to the continuous growth of data and knowledge and the limitations of data sources, the visual knowledge within the knowledge graphs is generally of low quality, and some entities suffer from the issue of missing visual modality. Nevertheless, previous studies of MMKGC have primarily focused on how to facilitate modality interaction and fusion while neglecting the problems of low modality quality and modality missing. In this case, mainstream MMKGC models only use pre-trained visual encoders to extract features and transfer the semantic information to the joint embeddings through modal fusion, which inevitably suffers from problems such as error propagation and increased uncertainty. To address these problems, we propose a Multi-modal knowledge graph Completion model based on Super-resolution and Detailed Description Generation (MMCSD). Specifically, we leverage a pre-trained residual network to enhance the resolution and improve the quality of the visual modality. Moreover, we design multi-level visual semantic extraction and entity description generation, thereby further extracting entity semantics from structural triples and visual images. Meanwhile, we train a variational multi-modal auto-encoder and utilize a pre-trained multi-modal language model to complement the missing visual features. We conducted experiments on FB15K-237 and DB13K, and the results showed that MMCSD can effectively perform MMKGC and achieve state-of-the-art performance.}
}
@article{LI2024125090,
title = {GS-CBR-KBQA: Graph-structured case-based reasoning for knowledge base question answering},
journal = {Expert Systems with Applications},
volume = {257},
pages = {125090},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125090},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424019572},
author = {Jiecheng Li and Xudong Luo and Guangquan Lu},
keywords = {Knowledge base question answering, Case-based reasoning, Natural language processing, Deep learning, Large language model},
abstract = {Knowledge Base Question Answering (KBQA) task is an important research direction in natural language processing. Due to the flexibility and ambiguity of natural language, users’ questions often have more complex query types and richer semantic information. To address this issue, this paper proposes the GS-CBR-KBQA model, a Case-Based Reasoning model tailored for KBQA to improve the semantic parsing accuracy and interpretability of natural language questions. The model integrates Knowledge-oriented Programming Language (KoPL) reasoning graphs with query information, employing a Graph Auto-Encoder and the RoBERTa pretrained language model for a highly effective case retrieval. This integration leads to a more robust knowledge retrieval and application approach, particularly innovative in capturing the relationships within KoPL graphs. The model addresses explicitly complex questions such as multi-hop reasoning and questions involving intricate entity relationships. Finally, our extensive experiments show that the model performs excellently in accuracy and F1 metrics on benchmark datasets such as WebQSP and ComplexWebQuestions, particularly in complex question-answering. The code of our model is available at https://anonymous.4open.science/r/GS-CBR-KBQA.}
}
@article{HU2025519,
title = {Investigation of brain structures and potential mechanisms associated with ADHD: Insights from Mendelian randomization and genetic analysis},
journal = {Journal of Affective Disorders},
volume = {379},
pages = {519-528},
year = {2025},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2025.03.024},
url = {https://www.sciencedirect.com/science/article/pii/S0165032725003623},
author = {Xiaoyun Hu and Liyu Lin and Zilun Wu},
keywords = {Brain structure, ADHD, MRI, Mendelian randomization, Enrichment analysis},
abstract = {Objective
Despite extensive studies linking brain structure with attention deficit hyperactivity disorder (ADHD), the causal relationships remain unclear. This study employs Mendelian randomization (MR) to assess these associations and explores the underlying mechanisms.
Methods
Utilizing genetic instruments from genome - wide association study (GWAS) data of 83 magnetic resonance imaging (MRI) studies sourced from the psychiatric genomics consortium (PGC) and integrative epidemiology unit (IEU), MR analyses were conducted to investigate the link between brain structures and ADHD. The Allen Human Brain Atlas was used to identify genes associated with significant brain structures, followed by gene ontology (GO), Kyoto encyclopedia of genes and genomes (KEGG), and pathway enrichment analyses, and construction of protein - protein interaction (PPI) networks.
Results
Intersection analysis from two MR studies highlighted 17 brain structures, such as the left caudal and rostral middle frontal volumes and right medial orbitofrontal volume, exhibiting strong negative correlations with ADHD symptoms (FDR < 0.05). These structures span the frontal, temporal, and parietal lobes, among others. Differential expression analysis showed these genes predominantly relate to pervasive developmental and autistic disorders, with functions including modulation of synaptic transmission. KEGG pathways identified neuroactive ligand-receptor interaction as significantly involved. PPI analysis pinpointed key proteins like SLC17A7, CAMK2A, and SST as critical hubs.
Conclusion
This research confirms negative correlations between certain brain structures and ADHD and implicates neuroactive ligand-receptor interactions in its pathogenesis, enhancing our understanding of ADHD's anatomical and genetic bases.}
}
@article{KRAUER2022100656,
title = {Mapping the plague through natural language processing},
journal = {Epidemics},
volume = {41},
pages = {100656},
year = {2022},
issn = {1755-4365},
doi = {https://doi.org/10.1016/j.epidem.2022.100656},
url = {https://www.sciencedirect.com/science/article/pii/S1755436522000962},
author = {Fabienne Krauer and Boris V. Schmid},
keywords = {Plague, Infectious diseases, Historical epidemiology, Outbreaks, Natural language processing, Machine learning},
abstract = {Pandemic diseases such as plague have produced a vast amount of literature providing information about the spatiotemporal extent, transmission, or countermeasures. However, the manual extraction of such information from running text is a tedious process, and much of this information remains locked into a narrative format. Natural Language processing (NLP) is a promising tool for the automated extraction of epidemiological data, and can facilitate the establishment of datasets. In this paper, we explore the utility of NLP to assist in the creation of a plague outbreak dataset. We produced a gold standard list of toponyms by manual annotation of a German plague treatise published by Sticker in 1908. We investigated the performance of five pre-trained NLP libraries (Google, Stanford CoreNLP, spaCy, germaNER and Geoparser) for the automated extraction of location data compared to the gold standard. Of all tested algorithms, spaCy performed best (sensitivity 0.92, F1 score 0.83), followed closely by Stanford CoreNLP (sensitivity 0.81, F1 score 0.87). Google NLP had a slightly lower performance (F1 score 0.72, sensitivity 0.78). Geoparser and germaNER had a poor sensitivity (0.41 and 0.61). We then evaluated how well automated geocoding services such as Google geocoding, Geonames and Geoparser located these outbreaks correctly. All geocoding services performed poorly – particularly for historical regions – and returned the correct GIS information only in 60.4%, 52.7% and 33.8% of all cases. Finally, we compared our newly digitized plague dataset to a re-digitized version of the plague treatise by Biraben and provide an update of the spatio-temporal extent of the second pandemic plague outbreaks. We conclude that NLP tools have their limitations, but they are potentially useful to accelerate the collection of data and the generation of a global plague outbreak database.}
}
@article{SCHREYER2024100350,
title = {Treatment resistance in schizophrenia and depression as an interactive kind: Mapping the development of a classification through Meta-Narrative review},
journal = {SSM - Mental Health},
volume = {6},
pages = {100350},
year = {2024},
issn = {2666-5603},
doi = {https://doi.org/10.1016/j.ssmmh.2024.100350},
url = {https://www.sciencedirect.com/science/article/pii/S2666560324000550},
author = {Leighton Schreyer and Csilla Kalocsai and Oshan Fernando and Melanie Anderson and Vanessa Lockwood and Sophie Soklaridis and Gary Remington and Araba Chintoh and Suze Berkhout},
abstract = {Despite ongoing attempts to delineate and name treatment resistance (TR) in psychiatry, the term is increasingly deployed across diagnostic categories. Still, what it is that constitutes TR remains unclear and in flux. Through a meta-narrative review, we construct a sociohistorical map of the concept of TR as it is employed in schizophrenia (TRS) and major depressive disorders (TRD). We track debates about TR, identify underlying assumptions and influencing factors that shape how the concept has evolved over time, and consider the intended and unintended consequences of its conceptualization. We develop our findings as three unique threads that, braided together, offer insight into TR as an interactive kind. Each thread analyzes and plays with the notion of heterogeneity, which arises in the literature as both a theme and a problem to be solved. Thread one looks at prevailing controversies surrounding the definition of TR. Here, heterogeneity arises in relation to how TR is delineated. We also consider the notion of “pseudoresistance,” a novel concept that functions to manage and contain heterogeneity, defining the boundaries of TR through its exclusions. Thread two explores the range of actors whose interests and practices are coordinated to shape TR as a concept: the pharmaceutical industry, academic psychiatry, clinicians, and health systems. Each group has its own interests and orientations: a heterogenous range of actors contributing to the thing that TR is. Thread three examines the intended and unintended consequences that attempts to conceptualize TR have yielded, including a reification of the biomedical paradigm and the personification of TR. This paper offers a systematic approach to thinking about similarities, differences, particularities and tensions embedded within TR to understand the politics and possibilities of the concept.}
}
@article{LEGLAZ2021,
title = {Machine Learning and Natural Language Processing in Mental Health: Systematic Review},
journal = {Journal of Medical Internet Research},
volume = {23},
number = {5},
year = {2021},
issn = {1438-8871},
doi = {https://doi.org/10.2196/15708},
url = {https://www.sciencedirect.com/science/article/pii/S1438887121004295},
author = {Aziliz {Le Glaz} and Yannis Haralambous and Deok-Hee Kim-Dufor and Philippe Lenca and Romain Billot and Taylor C Ryan and Jonathan Marsh and Jordan DeVylder and Michel Walter and Sofian Berrouiguet and Christophe Lemey},
keywords = {machine learning, natural language processing, artificial intelligence, data mining, mental health, psychiatry},
abstract = {Background
Machine learning systems are part of the field of artificial intelligence that automatically learn models from data to make better decisions. Natural language processing (NLP), by using corpora and learning approaches, provides good performance in statistical tasks, such as text classification or sentiment mining.
Objective
The primary aim of this systematic review was to summarize and characterize, in methodological and technical terms, studies that used machine learning and NLP techniques for mental health. The secondary aim was to consider the potential use of these methods in mental health clinical practice
Methods
This systematic review follows the PRISMA (Preferred Reporting Items for Systematic Review and Meta-analysis) guidelines and is registered with PROSPERO (Prospective Register of Systematic Reviews; number CRD42019107376). The search was conducted using 4 medical databases (PubMed, Scopus, ScienceDirect, and PsycINFO) with the following keywords: machine learning, data mining, psychiatry, mental health, and mental disorder. The exclusion criteria were as follows: languages other than English, anonymization process, case studies, conference papers, and reviews. No limitations on publication dates were imposed.
Results
A total of 327 articles were identified, of which 269 (82.3%) were excluded and 58 (17.7%) were included in the review. The results were organized through a qualitative perspective. Although studies had heterogeneous topics and methods, some themes emerged. Population studies could be grouped into 3 categories: patients included in medical databases, patients who came to the emergency room, and social media users. The main objectives were to extract symptoms, classify severity of illness, compare therapy effectiveness, provide psychopathological clues, and challenge the current nosography. Medical records and social media were the 2 major data sources. With regard to the methods used, preprocessing used the standard methods of NLP and unique identifier extraction dedicated to medical texts. Efficient classifiers were preferred rather than transparent functioning classifiers. Python was the most frequently used platform.
Conclusions
Machine learning and NLP models have been highly topical issues in medicine in recent years and may be considered a new paradigm in medical research. However, these processes tend to confirm clinical hypotheses rather than developing entirely new information, and only one major category of the population (ie, social media users) is an imprecise cohort. Moreover, some language-specific features can improve the performance of NLP methods, and their extension to other languages should be more closely investigated. However, machine learning and NLP techniques provide useful information from unexplored data (ie, patients’ daily habits that are usually inaccessible to care providers). Before considering It as an additional tool of mental health care, ethical issues remain and should be discussed in a timely manner. Machine learning and NLP methods may offer multiple perspectives in mental health research but should also be considered as tools to support clinical practice.}
}
@article{ROCHE2021111,
title = {Lexical necropolitics: The raciolinguistics of language oppression on the Tibetan margins of Chineseness},
journal = {Language & Communication},
volume = {76},
pages = {111-120},
year = {2021},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2020.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0271530920300896},
author = {Gerald Roche},
keywords = {Raciolinguistics, Purism, Language oppression, Tibet, China},
abstract = {This article aims to expand raciolinguistic theory to examine the issue of language oppression, i.e., enforced language loss. I used Foucauldian theories of race and racism to establish a link between lexical purism and language oppression, giving rise to a raciolinguistic theory of language oppression that I refer to as ‘lexical necropolitics.’ This issue is explored through a case study from northeast Tibet. I describe how state racism and the subordination of minority languages in the People's Republic of China has led to a grass-roots lexical purism campaign among Tibetans, and argue that since 2008, this purism has been linked to language oppression by the emergence of a new, biosovereign configuration of state power.}
}
@article{LOBET201911,
title = {Demystifying roots: A need for clarification and extended concepts in root phenotyping},
journal = {Plant Science},
volume = {282},
pages = {11-13},
year = {2019},
note = {The 4th International Plant Phenotyping Symposium},
issn = {0168-9452},
doi = {https://doi.org/10.1016/j.plantsci.2018.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S016894521730910X},
author = {Guillaume Lobet and Ana Paez-Garcia and Hannah Schneider and Astrid Junker and Jonathan A. Atkinson and Saoirse Tracy},
keywords = {Root, Phenotyping, Plasticity, Ontology},
abstract = {Plant roots have major roles in plant anchorage, resource acquisition and offer environmental benefits including carbon sequestration and soil erosion mitigation. As such, the study of root system architecture, anatomy and functional properties is of crucial interest to plant breeding, with the aim of sustainable yield production and environmental stewardship. Due to the importance of the root system studies, there is a need for clarification of terms and concepts in the root phenotyping community. In particular in this contribution, we advocate for the use of a reference naming system (ontologies) for roots and root phenes. Such uniformity would not only allow better understanding of research results, but would also enable a better sharing of data. In addition, we highlight the need to incorporate the concept of plasticity in breeding programs, as it is an essential component of root system development in heterogeneous environments.}
}
@article{RAMOS2020103339,
title = {An Archetype Query Language interpreter into MongoDB: Managing NoSQL standardized Electronic Health Record extracts systems},
journal = {Journal of Biomedical Informatics},
volume = {101},
pages = {103339},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103339},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419302588},
author = {Miguel Ramos and Ricardo Sánchez-de-Madariaga and Jesús Barros and Lino Carrajo and Guillermo Vázquez and Santiago Pérez and Mario Pascual and Fernando Martín-Sánchez and Adolfo Muñoz-Carrero},
keywords = {Information storage and retrieval, Health information interoperability, Databases, Management information systems, Medical record systems, Electronic Health Records},
abstract = {The fast development of today’s healthcare and the need to extract new medical knowledge from exponentially-growing volumes of standardized Electronic Health Records data, as required by studies in Precision Medicine, brings up a challenge that may probably only be addressed using NoSQL DBMSs, due to the non-optimal performance of traditional relational DBMSs on standardized data; and these database systems operated by semantic archetype-based query languages, because of the expected generalized extension of standardized EHR systems. An AQL into MongoDB interpreter has been developed to its first version. It translates system-independent AQL queries posed on ISO/EN 13606 standardized EHR extracts into the NoSQL MongoDB query language. The new interpreter has the advantages of both the archetype-based system-independent AQL queries and the dual-model-based standardized EHR extracts stored on document-centric NoSQL DBMSs, such as MongoDB. AQL queries are independent of applications, programming languages and system environments due to the use of the dual model, but EHR extracts featuring this model are best persisted on document-based NoSQL databases. Consequently, the interpreter allows us to query standardized EHR extracts semantically, and also affording optimal performance.}
}
@article{FECHTER201839,
title = {Integrated Process Planning and Resource Allocation for Collaborative Robot Workplace Design},
journal = {Procedia CIRP},
volume = {72},
pages = {39-44},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.179},
url = {https://www.sciencedirect.com/science/article/pii/S221282711830338X},
author = {Manuel Fechter and Carsten Seeber and Shengjian Chen},
keywords = {Human Robot Collaboration, Integrated Process, Job Shop Scheduling, Assembly Planning, Ontology, AutomationML},
abstract = {The design of human-robot-collaborative workplaces is a challenging task, whose outcome is highly dependent on the assembly planners knowledge and experience, as usually only a small fraction of the design space is considered. This often results in unappropriated workplace designs with process-related and economical drawbacks. This paper outlines an approach to a collaborative workplace design tool-chain considering different strengths of robot and human, starting from assembly group CAD model data input to an ontology based resource allocation and permutations on workplace design proposals. All steps are connected by the open exchange data format of AutomationML.}
}
@article{ANDREWSTODD2020105759,
title = {Exploring social and cognitive dimensions of collaborative problem solving in an open online simulation-based task},
journal = {Computers in Human Behavior},
volume = {104},
pages = {105759},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S0747563218305156},
author = {Jessica Andrews-Todd and Carol M. Forsyth},
keywords = {Collaboration, Collaborative problem solving, Ontology, Assessment, Simulation-based assessment},
abstract = {Collaborative problem solving (CPS) is a complex construct comprised of skills associated with social and cognitive dimensions. The diverse set of skills within these dimensions make CPS difficult to measure. Typically, research on measuring CPS has used highly constrained environments that help narrow the problem space. In the current study, we applied the in-task assessment framework to support the exploration of CPS skills at a deep level in an open digital environment in which three students worked together to solve an electronics problem. The construct of CPS was defined in depth prior to the implementation of the environment through the development of a complex, hierarchical ontology. The features from the ontology were identified in the data and four theoretically-grounded profiles of types of collaborative problem-solvers were produced - high social/high cognitive, high social/low cognitive, low social/high cognitive, and low social/low cognitive. Results showed that students in the low social/low cognitive profile group demonstrated poorer performance than students in other profile groups. Further, having at least one high social/high cognitive member in a team facilitated performance. This study offers groundwork for future studies in measuring CPS with an approach suitable for less constrained collaborative environments.}
}
@article{ZHURENKOV2019349,
title = {Post-non-classical discourse of civilizations as a self-developing poly-subject environment for improving the mechanisms of international stability},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {25},
pages = {349-354},
year = {2019},
note = {19th IFAC Conference on Technology, Culture and International Stability TECIS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.548},
url = {https://www.sciencedirect.com/science/article/pii/S240589631932467X},
author = {Denis A. Zhurenkov and Yelena A. Trushkova},
keywords = {international stability, post-non-classical paradigm, scientific rationality, third-order Cybernetics},
abstract = {This paper discusses the principle of self-development in the context of the post-non-classical paradigm and researches the related ontological problems to improve the mechanisms of international stability. The aim of the study is to determine the form of the principle of self-development. The research procedure consists of three parts: the essence of the principle of self-development in the post-non-classical discourse of civilizations, ontological generalization of the principle of self-development, moral generalization of the principle of self-development in Russian modern science. The method of research is hermeneutical-phenomenological and structuralist complex and basic principles of Synergetics. The main result of the research is the substantiation of the new form of the principle of self-development - post-non-classical discourse of civilizations. Post-non-classical discourse of civilizations is one of the possible ways to improve the mechanisms of international stability, which is quite relevant for TECIS. The ontology of management in hybrid reality environments is substantiated, which can be an important component of IFAC activities. The theoretical and practical significance of post-non-classical discourse of civilizations consists in substantiating new principles of management and communication in polysubject environments.}
}
@article{VERMA20252200,
title = {Unveiling Biomedical Insights: A Semantic Pipeline from Document Entities to Knowledge Graph Analysis of Hallmarks of Cancer},
journal = {Procedia Computer Science},
volume = {258},
pages = {2200-2209},
year = {2025},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.470},
url = {https://www.sciencedirect.com/science/article/pii/S187705092501573X},
author = {Shikha Verma and Hemraj Kumawat and Aditi Sharan and Sunila Hooda and Nidhi Verma},
keywords = {Hallmarks of Cancer, Entities, UMLS, Knowledge Graph, Biomedical knowledge},
abstract = {This study aims to build a knowledge graph specifically customized to the characteristics or hallmarks of cancer. The aim is to converge on semantically and contextually valuable information, ensuring the insights generated are meaningful and relevant to cancer biology. Domain specialists play a critical role in leading this process by offering in-depth biomedical knowledge, particularly in identifying entities of interest within hallmark classes. We apply the Unified Medical Language System (UMLS) to standardize these things, assuring consistency across biomedical nomenclature. Finally, the knowledge graph is generated based on the expert-selected entities, focusing on familiar key entities highlighting critical links within the hallmark class characteristics. This approach provides a greater understanding of cancer hallmarks through structured and standardized knowledge representation.}
}
@article{DAVARPOUR2019204,
title = {Toward a semantic-based location tagging news feed system: Constructing a conceptual hierarchy on geographical hashtags},
journal = {Computers & Electrical Engineering},
volume = {78},
pages = {204-217},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2019.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0045790618327435},
author = {Mohammad Hossein Davarpour and Mohammad Karim Sohrabi and Milad Naderi},
keywords = {News feed, Ontology, Semantic web, Hashtags, Location tagging},
abstract = {Online news and social networking sites have been significantly used in recent years. There has been a lot of efforts to provide appropriate contents for the end users; however, they proved not to be effective. We believe the semantic web (as the third generation of the web) is mature enough to undertake the responsibility of generating more user-centric content. One way to exploit the semantic web's capabilities for such purpose is to construct an ontology that establishes the relationships between hashtags. In this paper, we present the construction process of a news feed system based on the hierarchical relationships of geographic hashtag. Our experiments demonstrated that our proposed semantic-based hierarchical location tagging news feed system increases the quality of the user experience as well as the publication rate of the news while boosting the content match rate to the target audiences. The proposed system of this paper can be considered as a real step towards the realization of semantic web.}
}
@article{ROMERO2021900,
title = {A Proposal for a Software Tool to Perform Business Process Smart Assessment in Enterprises},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {900-905},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.107},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321008569},
author = {Marcelo Romero and Wided Guédria and Hervé Panetto and Béatrix Barafort},
keywords = {Computer software, Artificial intelligence, Machine learning, Knowledge-based systems, Efficient evaluation, Assessment},
abstract = {The challenges faced by enterprises on a daily basis such as regulatory compliance, novel technology adoption, or cost optimisation, foster them to implement improvement initiatives. As a first step towards the implementation of those initiatives, there is a need to perform assessments to understand the As-Is state of the enterprise considering different aspects such as maturity, agility, performance, or readiness towards digitisation. However, assessments are expensive in terms of time and resources. Specifically when considering qualitative appraisals, such as maturity or capability assessments, since they often demand the participation of one or more human assessors to review documents, perform interviews, etc. Therefore, means to automate or semi-automate the assessment process are essential, since they could reduce the effort to perform it. In this sense, this work introduces a software tool to support assessments in enterprises using text data as assessment evidence. The tool is developed following a conceptual framework named Smart Assessment Framework, which introduces a metamodel with abstract components to be instantiated for the development of systems dedicated to organisational assessments. The elements defined by the framework are grounded on the capabilities of smart systems. The application domain of the tool is focused on Process Capability assessment, in compliance with the ISO/IEC 33020:2015 international standard. The tool uses a Natural Language Processing method to process the assessment evidence and an Ontology as Knowledge Base to support the calculation of capability levels and to provide improvement recommendations.}
}
@article{ELU2021107847,
title = {Inferring spatial relations from textual descriptions of images},
journal = {Pattern Recognition},
volume = {113},
pages = {107847},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.107847},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321000340},
author = {Aitzol Elu and Gorka Azkune and Oier Lopez {de Lacalle} and Ignacio Arganda-Carreras and Aitor Soroa and Eneko Agirre},
keywords = {Text-to-image synthesis, Natural language understanding, Spatial relations, Deep learning},
abstract = {Generating an image from its textual description requires both a certain level of language understanding and common sense knowledge about the spatial relations of the physical entities being described. In this work, we focus on inferring the spatial relation between entities, a key step in the process of composing scenes based on text. More specifically, given a caption containing a mention to a subject and the location and size of the bounding box of that subject, our goal is to predict the location and size of an object mentioned in the caption. Previous work did not use the caption text information, but a manually provided relation holding between the subject and the object. In fact, the used evaluation datasets contain manually annotated ontological triplets but no captions, making the exercise unrealistic: a manual step was required; and systems did not leverage the richer information in captions. Here we present a system that uses the full caption, and Relations in Captions (REC-COCO), a dataset derived from MS-COCO which allows to evaluate spatial relation inference from captions directly. Our experiments show that: (1) it is possible to infer the size and location of an object with respect to a given subject directly from the caption; (2) the use of full text allows to place the object better than using a manually annotated relation. Our work paves the way for systems that, given a caption, decide which entities need to be depicted and their respective location and sizes, in order to then generate the final image.}
}
@article{CABILI2021100031,
title = {Empirical validation of an automated approach to data use oversight},
journal = {Cell Genomics},
volume = {1},
number = {2},
pages = {100031},
year = {2021},
issn = {2666-979X},
doi = {https://doi.org/10.1016/j.xgen.2021.100031},
url = {https://www.sciencedirect.com/science/article/pii/S2666979X21000380},
author = {Moran N. Cabili and Jonathan Lawson and Andrea Saltzman and Greg Rushton and Pearl O’Rourke and John Wilbanks and Laura Lyman Rodriguez and Tommi Nyronen and Mélanie Courtot and Stacey Donnelly and Anthony A. Philippakis},
keywords = {Data Access Committee, Data Use, GA4GH, Passport},
abstract = {Summary
The current paradigm for data use oversight of biomedical datasets is onerous, extending the timescale and resources needed to obtain access for secondary analyses, thus hindering scientific discovery. For a researcher to utilize a controlled-access dataset, a data access committee must review her research plans to determine whether they are consistent with the data use limitations (DULs) specified by the informed consent form. The newly created GA4GH data use ontology (DUO) holds the potential to streamline this process by making data use oversight computable. Here, we describe an open-source software platform, the Data Use Oversight System (DUOS), that connects with DUO terminology to enable automated data use oversight. We analyze dbGaP data acquired since 2006, finding an exponential increase in data access requests, which will not be sustainable with current manual oversight review. We perform an empirical evaluation of DUOS and DUO on selected datasets from the Broad Institute’s data repository. We were able to structure 118/123 of the evaluated DULs (96%) and 52/52 (100%) of research proposals using DUO terminology, and we find that DUOS’ automated data access adjudication in all cases agreed with the DAC manual review. This first empirical evaluation of the feasibility of automated data use oversight demonstrates comparable accuracy to human-based data access oversight in real-world data governance.}
}
@article{DELAVARA2024103803,
title = {Assessment of the quality of the text of safety standards with industrial semantic technologies},
journal = {Computer Standards & Interfaces},
volume = {88},
pages = {103803},
year = {2024},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2023.103803},
url = {https://www.sciencedirect.com/science/article/pii/S0920548923000843},
author = {Jose Luis {de la Vara} and Hector Bahamonde and Clara Ayora},
keywords = {Safety-critical system, Safety standard, Quality, RQA - Quality studio, DO-178C},
abstract = {Most safety-critical systems are subject to rigorous assurance processes to justify that the systems are dependable. These processes are typically conducted in compliance with safety standards, e.g., DO-178C for software in aerospace. This can be a prerequisite so that a system is allowed to operate. However, following these standards can be challenging in practice because of issues in their text such as imprecision, ambiguity, and inconsistency. These issues can hinder compliance, delaying it and making it more expensive, or even preventing it. As a solution, we aim to define means that aid in the identification of the issues and thus facilitate their resolution. We have developed an approach for assessment of the quality of the text of safety standards with RQA - Quality Studio, an industrial tool for requirements quality analysis with semantic technologies. The approach is based on the extraction of analysis units from a standard, on the specification and exploitation of ontologies, and on the reuse of metrics provided by RQA - Quality Studio to evaluate text quality. The approach has been applied on the DO-178C standard, assessing its text as a whole and its different main individual parts. The quality of most of the text of the standard can be regarded as high. The most frequent issues in DO-178C are the use of passive voice, of synonyms, and of imprecise modal verbs. To the best of our knowledge, this is the first study that provides a means for a broad and detailed assessment of the quality of the text of safety standards, leading to the identification of specific aspects that could be improved in the text and indicating the extent to which quality issues affect it.}
}
@article{JANG201896,
title = {PISTON: Predicting drug indications and side effects using topic modeling and natural language processing},
journal = {Journal of Biomedical Informatics},
volume = {87},
pages = {96-107},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418301898},
author = {Giup Jang and Taekeon Lee and Soyoun Hwang and Chihyun Park and Jaegyoon Ahn and Sukyung Seo and Youhyeon Hwang and Youngmi Yoon},
keywords = {Text mining, Drug repositioning, Side effect prediction, Bioinformatics, Systems biology},
abstract = {The process of discovering novel drugs to treat diseases requires a long time and high cost. It is important to understand side effects of drugs as well as their therapeutic effects, because these can seriously damage the patients due to unexpected actions of the derived candidate drugs. In order to overcome these limitations, computational methods for predicting the therapeutic effects and side effects have been proposed. In particular, text mining is a widely used technique in the field of systems biology, because it can discover hidden relationships between drugs, genes and diseases from a large amount of literature data. Compared with in vivo/in vitro experiments, text mining derives meaningful results with less time and cost. In this study, we propose an algorithm for predicting novel drug-phenotype associations and drug-side effect associations using topic modeling and natural language processing (NLP). We extract sentences in which drugs and genes co-occur from the abstracts of the literature and identify words that describe the relationship between them using NLP. Considering the characteristics of the identified words, we determine if the drug has an up-regulation effect or a down-regulation effect on the gene. Based on genes that affect drugs and their regulatory relationships, we group the frequently occurring genes and regulatory relationships into topics, and build a drug-topic probability matrix by calculating the score that the drug will have a topic using topic modeling. Using the matrix, a classifier is constructed for predicting the novel indications and side effects of drugs considering the characteristics of known drug-phenotype associations or drug-side effect associations. The proposed method predicts both indications and side effects with a single algorithm, and it can exclude drugs with serious side effects or side effects that patients do not want to experience from among the candidate drugs provided for the treatment of the phenotype. Furthermore, lists of novel candidate drugs for phenotypes and side effects can be continuously updated with our algorithm every time a document is added. More than a thousand documents are produced per day, and it is possible for our algorithm to efficiently derive candidate drugs because it requires less cost than the existing drug repositioning methods. The resource of PISTON is available at databio.gachon.ac.kr/tools/PISTON.}
}
@article{THIBAULT201949,
title = {Simplex selves, functional synergies, and selving: Languaging in a complex world},
journal = {Language Sciences},
volume = {71},
pages = {49-67},
year = {2019},
note = {Simplexity, agency and language},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0388000118300329},
author = {Paul J. Thibault},
keywords = {Languaging, Microgenesis, Self, Simplexity, Value, Virtual internal ecology},
abstract = {In this paper, I present selves as simplex structures (Berthoz, 2012/2009) that construct themselves and are constructed in and through the embodied socio-cognitive dynamics of ‘selving’. Selves are, following Vygotsky (1986: 59–73; see also Ratner, 2017), individuations and crystallisations of the concrete social relations in which the self has participated along its life-trajectory. Selving arises and takes place in dialogically coordinated languaging activity. In complex social and cultural worlds, simplex selves-in-languaging constitute and stabilise their own and others' experience and living bodies in and through norm saturated languaging. Thus, while human subjectivity is foundational, a self emerges from an ontogenetic history –it is a bodily-based time-extended process that generates a sense of its felt agency. Given the agency of self, a person experiences an ‘inner’ life—a virtual internal ecology—that generates affective relations and dialogical resonances with its ‘objects’. The latter come to be co-articulated together with how the self contributes to their becoming. Far from being ‘representational’, mental objects derive from a microgenetic constructive process that arises under both endophasic and exophasic controlling factors. The self is thus empowered to enact an embodied and enduring anima that is intrinsic to a living human being: it appears in articulatory acts and, dramatically, when people engage with each other by means of what is generically called ‘languaging’. In illustration, I analyse an example of observable first-order languaging activity in which action, perception, and expression constitute a unitary field within which parties undertake selving. Having presented the example, I conclude by showing that, during first-order languaging, wilful acts can be traced to the dialectic of autonomy and heteronomy in which selves participate. The analysis shows how, on at least some occasions, selving is a matter of configuring personal meaning and adapting and integrating it to second-order cultural resources in ways that are amenable to a description of languaging activity in terms of a three-part structure.}
}
@article{MORTENSEN2024100316,
title = {NNI nanoinformatics conference 2023: Movement toward a common infrastructure for federal nanoEHS data computational toxicology: Short communication},
journal = {Computational Toxicology},
volume = {30},
pages = {100316},
year = {2024},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2024.100316},
url = {https://www.sciencedirect.com/science/article/pii/S2468111324000185},
author = {Holly M. Mortensen and Jaleesia D. Amos and Thomas E. Exner and Kenneth Flores and Stacey Harper and Annie M. Jarabek and Fred Klaessig and Vladimir Lobaskin and Iseult Lynch and Christopher S. Marcum and Marvin Martens and Branden Brough and Quinn Spadola and Rhema Bjorkland},
keywords = {Nanomaterials, Emerging materials, Safe/sustainable by design, Database, Interoperability, Infrastructure, FAIR},
abstract = {The National Nanotechnology Initiative organized a Nanoinformatics Conference in the 2023 Biden-Harris Administration’s Year of Open Science, which included interested U.S. and EU stakeholders, and preceded the U.S.-EU COR meeting on November 15th, 2023 in Washington, D.C. Progress in the development of a common nanoinformatics infrastructure in the European Union and United States were discussed. Development of contributing, individual database projects, and their strengths and weaknesses, were highlighted. Recommendations and next steps for a U.S. nanoEHS common infrastructure were discussed in light of the pending update of the National Nanotechnology Initiative (NNI)’s Environmental, Health and Safety Research Strategy, and U.S. efforts to curate and house nano Environmental Health and Safety (nanoEHS) data from U.S. federal stakeholder groups. Improved data standards, for reporting and storage have been identified as areas where concerted efforts could most benefit initially. Areas that were not addressed at the conference, but that are critical to progress of the U.S. federal consortium effort are the evaluation of data formats according to use and sustainability measures; modeler and end user, including risk-assessor and regulator perspectives; a need for a community forum or shared data location that is not hosted by any individual U.S. federal agency, and is accessible to the public; as well as emerging needs for integration with new data types such as micro and nano plastics, and interoperability with other data and meta-data, such as adverse outcome pathway information. Future progress will depend on continued interaction of the U.S. and EU CORs, stakeholders and partners in the continued development goals for shared or interoperable infrastructure for nanoEHS.}
}
@article{LV2025143473,
title = {UBTD2 protein molecules emerges as a key prognostic protein marker in glioma: Insights from integrated omics and machine learning analysis of GRM7, NCAPG, CEP55, and other biomarkers},
journal = {International Journal of Biological Macromolecules},
volume = {310},
pages = {143473},
year = {2025},
issn = {0141-8130},
doi = {https://doi.org/10.1016/j.ijbiomac.2025.143473},
url = {https://www.sciencedirect.com/science/article/pii/S0141813025040255},
author = {Jia Lv and Shaoxun Li and Hongrui Zhang},
keywords = {UBTD2 protein molecule, Prognosis of glioma, Protein marker, GRM7, NCAPG, CEP55, Synthetic omics, Machine learning},
abstract = {Glioma is a malignant brain tumor with poor prognosis, and there is an urgent need to find effective biomarkers for early diagnosis and treatment. The aim of this study was to explore the potential of UBTD2 as a key prognostic protein marker for gliomas through comprehensive omics and machine learning analysis, and to reveal its relationship with other biomarkers such as GRM7, NCAPG, CEP55, etc. In this study, relevant data were downloaded from a public database, and differential expression gene (DEG) analysis was used to identify the characteristic genes of glioma. Through gene expression standardization and Venn analysis, 80 overlapping genes were selected and enriched by gene ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG). Key genes were identified based on LASSO analysis, while SHAP models were used to evaluate key DEGs and their interactions that influence glioma prediction models. Further analysis of UBTD2, including gene set enrichment analysis (GSEA), cell culture experiments, transfection techniques and Westernblot, was performed to investigate its function in glioma cells. The results showed that UBTD2 was significantly upregulated in glioma cells and correlated with the expression of five key proteins. LASSO analysis identified key difference-expressed genes in gliomas, and SHAP analysis revealed the importance of these genes in predictive models. Further experiments showed that UBTD2 knockdown inhibited cell proliferation and invasion by up-regulating p53 expression, underscoring its potential function in gliomas.}
}
@article{HOWELL201894,
title = {Water utility decision support through the semantic web of things},
journal = {Environmental Modelling & Software},
volume = {102},
pages = {94-114},
year = {2018},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364815216307551},
author = {Shaun Howell and Yacine Rezgui and Thomas Beach},
keywords = {Water management, Decision support tool, Interoperability, Big data, Ontology, Semantic web, Internet of things, Smart water networks},
abstract = {Urban environments are urgently required to become smarter. However, building advanced applications on the Internet of Things requires seamless interoperability. This paper proposes a water knowledge management platform which extends the Internet of Things towards a Semantic Web of Things, by leveraging the semantic web to address the heterogeneity of web resources. Proof of concept is demonstrated through a decision support tool which leverages both the data-driven and knowledge-based programming interfaces of the platform. The solution is grounded in a comprehensive ontology and rule base developed with industry experts. This is instantiated from GIS, sensor, and EPANET data for a Welsh pilot. The web service provides discoverability, context, and meaning for the sensor readings stored in a scalable database. An interface displays sensor data and fault inference notifications, leveraging the complementary nature of serving coherent lower and higher-order knowledge.}
}
@article{GAO2019414,
title = {An equine disease diagnosis expert system based on improved reasoning of evidence credibility},
journal = {Information Processing in Agriculture},
volume = {6},
number = {3},
pages = {414-423},
year = {2019},
issn = {2214-3173},
doi = {https://doi.org/10.1016/j.inpa.2018.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214317318302385},
author = {Hongyan Gao and Guimiao Jiang and Xiang Gao and Jianhua Xiao and Hongbin Wang},
keywords = {Equine disease, Diagnosis, Expert system, Object-based ontology, Evidence credibility},
abstract = {In China, there is a troubling shortage of well-trained equine veterinarians, leaving the needs of many equine farmers unmet. This is especially true with respect to the diagnosis of equine diseases. To solve this shortcoming, an equine disease diagnosis expert system was developed. For the aspect of knowledge representation, the structure of equine disease diagnosis knowledge was analyzed using an ontology system. Next, the clinical signs were described using an object-attribute-value (O-A-V) format, and the knowledge representation was then expressed using production rules. With respect to the reasoning mechanism, the weights of the clinical signs and promoted confidence factors (PCF) were combined to express information and rules pertaining to clinical signs with an associated level of uncertainty. The model was established based on improved reasoning of evidence credibility. Finally, using the ASP.Net platform and the SQL Server 2008 database, the equine disease diagnosis expert system based on the B/S structure has been developed, and is capable of reliably diagnosing 40 of the most common equine diseases. A functional evaluation of the system was conducted, and the diagnostic accuracy was observed to be 88%. This study demonstrates a bright prospect for the popularization and application of the system through continuous system maintenance and knowledge-based updates.}
}
@article{ZHOU2025749,
title = {Collaborative optimization for multirobot manufacturing system reliability through integration of SysML simulation and maintenance knowledge graph},
journal = {Journal of Manufacturing Systems},
volume = {80},
pages = {749-775},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525000998},
author = {Jian Zhou and Lianyu Zheng and Yiwei Wang},
keywords = {Multirobot system, Reliability optimization, SysML model simulation, Maintenance knowledge graph, Similarity reasoning},
abstract = {In the rapidly advancing field of industrial automation, the reliability and maintenance of multirobot manufacturing systems are crucial. This paper proposes a collaborative optimization method for the reliability of multirobot system, combining SysML (System Modeling Language) model simulation with an operational and maintenance knowledge graph, aiming to ensure the reliable operation of multirobot manufacturing systems. The SysML model provides a comprehensive framework to represent the system architecture, workflows, and key parameters, identify critical components and potential bottlenecks, and perform detailed reliability analysis. Simultaneously, by embedding intelligent algorithms, the operational and maintenance knowledge graph enables automatic detection of operational anomalies and intelligent generation of maintenance strategies for industrial robots. By integrating the SysML model with the operational and maintenance knowledge graph, a collaborative optimization framework for the reliability of multirobot system is constructed. This framework not only dynamically adjusts key parameters in the simulation model, enhancing the accuracy and real-time performance of system reliability assessments, but also optimizes maintenance strategies based on system simulation indicators to ensure the reliable operation of multirobot system. Case studies validate that the proposed method improves the reliability of multirobot manufacturing systems, demonstrating that the combination of SysML simulation and the operational and maintenance knowledge graph can effectively address the complexity of modern manufacturing systems, offering significant reference value.}
}
@incollection{CHARITONIDOU2024391,
title = {Chapter 17 - Smart cities as spaces of flows and the digital turn in architecture and urban planning: Big data vis-à-vis environmental and social equity},
editor = {Zhihan Lyu},
booktitle = {Smart Spaces},
publisher = {Academic Press},
pages = {391-413},
year = {2024},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-443-13462-3},
doi = {https://doi.org/10.1016/B978-0-443-13462-3.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443134623000030},
author = {Marianna Charitonidou},
keywords = {Generative processes, Real-time simulation, Paperless studios, Interactivity, Greg Lynn, Asymptote Architecture, Embryological House, Big data, Second digital turn, Smart cities, Urban scale digital twins, Spatial flux, Spaces of flows, Objectile},
abstract = {This chapter aims to shed light on the epistemological mutations that are related to the digital turn in architecture and urban planning and their pedagogies. Special attention is paid to the “paperless studios” at Columbia University’s Graduate School of Architecture, Planning and Preservation (GSAPP). The chapter also examines Greg Lynn’s Embryological House (1997–2001), Asymptote Architecture’s 3DTF Virtual Trading Floor (VTF) (1997–99) commissioned by the New York Stock Exchange (NYSE) and Securities Industry Automation Corporation (SIAC), dECOi architects’ Aegis Hyposurface (1999–2001), and the National Digital Twin programme (NDTp). The analysis of these case studies intends to show how the digital turn in architecture and urban planning is related to certain ontological transformations concerning design processes. The chapter also examines the epistemological and ontological shifts related to the first digital turn in architecture and urban planning, as well as the second digital turn in these fields and the role of urban scale digital twins in shaping sustainable urban policies. It also places particular emphasis on how urban scale digital twins help develop new data-driven scenarios, promote sustainable development goals, and shape new participatory design methods. At the core of the chapter is the conviction that it is of pivotal importance to shape methodological tools that would offer the possibility of developing new forms of social advocacy around big data, and would create a shared terrain of reflections between the debates on smart cities and the debates around urban commons.}
}
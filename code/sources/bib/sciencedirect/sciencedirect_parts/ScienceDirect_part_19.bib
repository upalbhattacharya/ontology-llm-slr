@article{CHIOSA2024114802,
title = {A portable application framework for energy management and information systems (EMIS) solutions using Brick semantic schema},
journal = {Energy and Buildings},
volume = {323},
pages = {114802},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.114802},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824009186},
author = {Roberto Chiosa and Marco Savino Piscitelli and Marco Pritoni and Alfonso Capozzoli},
keywords = {Energy management and information systems, Portable application, Brick metadata schema, Anomaly detection, Machine learning},
abstract = {This paper introduces a portable framework for developing, scaling and maintaining energy management and information systems (EMIS) applications using an ontology-based approach. Key contributions include an interoperable layer based on Brick schema, the formalization of application constraints pertaining metadata and data requirements, and a field demonstration. The framework allows for querying metadata models, fetching data, preprocessing, and analyzing data, thereby offering a modular and flexible workflow for application development. Its effectiveness is demonstrated through a case study involving the development and implementation of a data-driven anomaly detection tool for the photovoltaic systems installed at the Politecnico di Torino, Italy. During eight months of testing, the framework was used to tackle practical challenges including: (i) developing a machine learning-based anomaly detection pipeline, (ii) replacing data-driven models during operation, (iii) optimizing model deployment and retraining, (iv) handling critical changes in variable naming conventions and sensor availability (v) extending the pipeline from one system to additional ones.}
}
@article{CLEMENTSCORTES2022101965,
title = {The impact of systemic language in music therapy and western healthcare systems: A Canadian perspective},
journal = {The Arts in Psychotherapy},
volume = {81},
pages = {101965},
year = {2022},
issn = {0197-4556},
doi = {https://doi.org/10.1016/j.aip.2022.101965},
url = {https://www.sciencedirect.com/science/article/pii/S0197455622000867},
author = {Amy Clements-Cortés and Joyce Jing Yee Yip},
keywords = {Language, Power, Decolonization, Racism, Oppression, Music therapy},
abstract = {Language is an active, multifaceted expression that allows individuals to develop intrapersonal and interpersonal relationships in the meaning of patterns of syntax structures. This form of communication can be expressed and interpreted with single or multiple ontologies, suggesting viewpoints of privilege of "Whiteness,” being a native-English speaker. Language and power can perpetuate inadequate advocacy of marginalized populations and theories and concepts, particularly for listeners with a positionality in Western World/Global North publications. Allied healthcare professions such as music therapy and psychotherapy are no strangers to the systemic issues language can perpetuate. Ableist texts that describe the therapeutic process and/or aim to legitimize the discipline of music therapy afford inherent systemic issues that do not support the Neurodiversity movement, and pathologize differences. This article explores the interface of standard terminology in Western healthcare systems and the field of music therapy. Approaching the paper from a Canadian perspective, we focus on issues surrounding labelling (diagnostics), assessment, standardized testing, and problematic language. Discussion on these topics highlights s the exclusion of a client’s cultural and musical practices, as well as representative sampling in research and barriers faced by non-English speaking researchers and clinicians. Recommendations towards reducing and removing systemic barriers to ensure an inclusive provision to music therapy service, which include reflexivity of language choice in clients and decolonization methodologies in practice are offered.}
}
@article{QIU2025113672,
title = {Explainable medical visual question answering via chain of evidence},
journal = {Knowledge-Based Systems},
volume = {324},
pages = {113672},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113672},
url = {https://www.sciencedirect.com/science/article/pii/S095070512500718X},
author = {Chen Qiu and Ke Huang and Zhiqiang Xie and Maofu Liu and Jinguang Gu and Xiaofen Zong},
keywords = {Medical visual question answering, Chain of evidence, Large language model},
abstract = {Medical Visual Question Answering (MedVQA) has delivered success in healthcare, aiming to answer questions about clinical findings from medical images. However, developing models that are evidence-based and can precisely interpret decision-making with medical knowledge remains a crucial objective. To address these challenges, we reframe the MedVQA problem as a task of generating evidence, which is intrinsically consistent with the medical diagnostic interaction process. We introduce Med-CoE, a framework that leverages the chain-of-thought capacity of LLMs along with medical knowledge sources to address the explanation gap in MedVQA tasks. Med-CoE establishes an evidence verification and interpretation mechanism under a three-stage training process with medical knowledge grounding. We pre-train our model on the PMC-OA corpus and generate auto-labeling Chain of Evidences via LLMs prompt and then fine-tune Med-CoE on two benchmarks. Experimental findings show that Med-CoE outperforms state-of-the-art methods by a large margin, e.g., 10.7% on direct-answer task of PMC-VQA and 4.0% on SLAKE.}
}
@article{CALVANESE2023102157,
title = {Conceptually-grounded mapping patterns for Virtual Knowledge Graphs},
journal = {Data & Knowledge Engineering},
volume = {145},
pages = {102157},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102157},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23000174},
author = {Diego Calvanese and Avigdor Gal and Davide Lanti and Marco Montali and Alessandro Mosca and Roee Shraga},
keywords = {Virtual knowledge graphs, Ontology-based data access, Mapping patterns, Data integration},
abstract = {Virtual Knowledge Graphs (VKGs) constitute one of the most promising paradigms for integrating and accessing legacy data sources. A critical bottleneck in the integration process involves the definition, validation, and maintenance of mapping assertions that link data sources to a domain ontology. To support the management of mappings throughout their entire lifecycle, we identify a comprehensive catalog of sophisticated mapping patterns that emerge when linking databases to ontologies. To do so, we build on well-established methodologies and patterns studied in data management, data analysis, and conceptual modeling. These are extended and refined through the analysis of concrete VKG benchmarks and real-world use cases, and considering the inherent impedance mismatch between data sources and ontologies. We validate our catalog on the considered VKG scenarios, showing that it covers the vast majority of mappings present therein.}
}
@article{YANG2023100086,
title = {Step-wise discriminative learning on uncertain annotations for word sense disambiguation},
journal = {Journal of Engineering Research},
volume = {11},
number = {2},
pages = {100086},
year = {2023},
issn = {2307-1877},
doi = {https://doi.org/10.1016/j.jer.2023.100086},
url = {https://www.sciencedirect.com/science/article/pii/S2307187723000871},
author = {Qihao Yang and Jiong Zheng},
keywords = {Morphological analysis, Agglutinative language, Sequence labeling, Graphic model, Linear model},
abstract = {Uncertain annotations with sparsity and uncertainty widely exist for many natural language processing tasks. In this paper, a new word sense disambiguation system is proposed, which integrates rich features and distributed discrimination algorithm, and constructs a method for learning on fuzzy annotations, which can effectively learn from sparse and uncertain annotations. The algorithm considers the elementary components in ascending order concerning their ambiguity degrees, which learns a concept labeler with promising performance with only an inventory extracted from an ontology. Experiments show that the performance of the algorithm significantly outperforms the intuitive generative unsupervised method with the same linguistic resource.}
}
@article{KONG2025111461,
title = {An innovative life cycle scenario-driven method for product carbon emissions prediction in the early design phase},
journal = {Computers & Industrial Engineering},
pages = {111461},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111461},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225006072},
author = {Lin Kong and Liming Wang and Fangyi Li and Qingliang Zeng and Xin Zhang and Lirong Wan and Jianfeng Li and Chenglong Wang and Lirong Zhou and Geng Wang and Haiyang Lu},
keywords = {Low carbon design, Carbon emissions prediction, Life cycle scenario, Convolutional neural network, Differential evolution algorithm},
abstract = {The early estimation of carbon emissions during the electromechanical product design phase holds pivotal significance in facilitating carbon reduction and emission mitigation efforts. Nevertheless, the challenges posed by the multi-source and heterogeneous information, and the scarcity of inventory data render it difficult to express design schemes and accurately assess carbon emissions. Hence, this study addresses this challenge by proposing an integrated convolutional neural network and differential evolution method to predict electromechanical product carbon emissions in the early design phase driven by life cycle scenarios. Specifically, the life cycle scenario is proposed to represent all activities for product design, and a life cycle scenario-oriented multi-layer low carbon design model is developed to comprehensively express all design information of the design scheme throughout product lifecycle stages. Upon this foundation, an ontology-based product design scheme case base is established to uniformly store and manage all life cycle scenarios and carbon emissions of historical products. Subsequently, an integrated convolutional neural network and differential evolution algorithm method is developed to efficiently predict electromechanical product carbon emissions. Here, the convolutional neural network model is trained by the life cycle scenarios and carbon emissions of historical design schemes stored in the case base, while the parameters of the model are iteratively optimized through the differential evolution algorithm. Furthermore, to validate the feasibility of the method, the carbon emissions prediction of a wind turbine gearbox is utilized as a case study. The result shows the correlation coefficient of the predicted and true value is 0.9788, which reveals that the proposed method performs superior in the aspect of accuracy of results. This work provides designers with a target method for achieving efficient and accurate carbon emission prediction in the early design phase, which can effectively support product low carbon design.}
}
@article{PENG2020106650,
title = {A new method for interoperability and conformance checking of product manufacturing information},
journal = {Computers & Electrical Engineering},
volume = {85},
pages = {106650},
year = {2020},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106650},
url = {https://www.sciencedirect.com/science/article/pii/S004579062030505X},
author = {Zhiguo Peng and Meifa Huang and Yanru Zhong and Leilei Chen and Guanghao Liu},
keywords = {PMI, Interoperability, Conformance checking, Ontology, SROIQ(D)},
abstract = {Product manufacturing information (PMI) is the most important information in the product digital model. Whether PMI can be clearly and unambiguous represented and exchanged between different computer-aided software (CAx) has become a topic of high interest in current research. To enhance the interoperability of PMI information among different CAx systems and offer framework software to check the PMI realization ability of different computer-aided design (CAD) software packages, this paper proposes a new method for interoperability and conformance checking by establishing a unified PMI ontology. Using web ontology language description logic (OWL2 DL) and semantic web rule language (SRWL), PMI can be described clearly and without ambiguity and can be read and interpreted automatically by the computer. In addition, the ontology also supplies the functions of consistency checking, knowledge inference, and semantic queries. The analysis shows that this ontology has great engineering significance in realization of PMI interoperability exchange and construction of framework software for PMI conformance checking.}
}
@article{KRUIPER2024102653,
title = {A platform-based Natural Language processing-driven strategy for digitalising regulatory compliance processes for the built environment},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102653},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102653},
url = {https://www.sciencedirect.com/science/article/pii/S147403462400301X},
author = {Ruben Kruiper and Bimal Kumar and Richard Watson and Farhad Sadeghineko and Alasdair Gray and Ioannis Konstas},
keywords = {Digital Regulatory Compliance, Natural Language Processing, Semantic Web, Machine Learning, Knowledge Graph, Automated Compliance Checking},
abstract = {The digitalisation of the regulatory compliance process has been an active area of research for several decades. However, more recently the level of activities in this area has increased considerably. In the UK, the tragic incident of Grenfell fire in 2017 has been a major catalyst for this as a result of the Hackitt report’s recommendations pointing a lot of the blame on the broken regulatory regime in the country. The Hackitt report emphasises the need to overhaul the building regulations, but the approach to do so remains an open research question. Existing work in this space tends to overlook the processing of actual regulatory documents, or limits their scope to solving a relatively small subtask. This paper presents a new comprehensive platform approach to the digitalisation of the regulatory compliance processing. We present i-ReC (intelligent Regulatory Compliance), a platform approach to digitalisation of regulatory compliance that takes into consideration the enormous diversity of all the stakeholders’ activities. A historical perspective on research in this area is first presented to put things in perspective which identifies the challenges in such an endeavour and identifies the gaps in state-of-the-art. After enumerating all the challenges in implementing a platform-based approach to digitalising the regulatory compliance process, the implementation of some parts of the platform is described. Our research demonstrates that the identification and extraction of all relevant requirements from the corpus of several hundred regulatory documents is a key part of the whole process which underlies the entire process from authoring to eventually compliance checking of designs. Some of the issues that need addressing in this endeavour include ambiguous language, inconsistent use of terms, contradicting requirements and handling multi-word expressions. The implementation of these tools is driven by NLP, ML and Semantic Web technologies. A semantic search engine was developed and validated against other popular and comparable engines with a corpus of 420 (out of about 800) documents used in the UK for compliance checking of building designs. In every search scenario, our search engine performed better on all objective criteria. Limitations of the approach are discussed which includes the challenges around licensing for all the documents in the corpus. Further work includes improving the performance of SPaR.txt (the tool created to identify multi-word expressions) as well as the information retrieval engine by increasing the dataset and providing the model with examples from more diverse formats of regulations. There is also a need to develop and align strategies to collect a comprehensive set of domain vocabularies to be combined in a Knowledge Graph.}
}
@article{LIM2025104207,
title = {BRepQL: Query language for searching topological elements in B-rep models},
journal = {Computers in Industry},
volume = {164},
pages = {104207},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104207},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524001350},
author = {Seungeun Lim and Changmo Yeo and Byung Chul Kim and Kyung Cheol Bae and Duhwan Mun},
keywords = {Automated topology search, Boundary representation model, Structural query language, Topological element, Topological query},
abstract = {Topological elements form the basis for tasks such as geometric calculations, feature analysis, and direct modeling in 3D CAD systems. Handling these elements is also essential in various automated systems. This study proposes a method to search for topological elements within a boundary representation (B-rep) model by employing topological queries. To address complex scenarios that are difficult to handle using a single query, a topological query procedure that sequentially executes a predefined set of topological queries is used. To verify the effectiveness of the proposed method, experiments were conducted on Test Cases 1, 2, and 3, confirming the successful search of all target topological elements. Furthermore, tests on modified Snap-fit hook A and Bridge B models demonstrated that the same queries remained effective, provided the topological relationships and geometric constraints expressed in the query were preserved. In addition, a search time comparison showed that the proposed method reduced search time by over 90 % compared to manual processes. Finally, in an experiment involving participants with varying levels of programming proficiency, the results indicated that, for a developer with high programming skills, writing topological queries reduced the time required to search for a single topological element by more than 95 % compared to writing the program code.}
}
@article{KALYAN2022103982,
title = {AMMU: A survey of transformer-based biomedical pretrained language models},
journal = {Journal of Biomedical Informatics},
volume = {126},
pages = {103982},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103982},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421003117},
author = {Katikapalli Subramanyam Kalyan and Ajit Rajasekharan and Sivanesan Sangeetha},
keywords = {Biomedical pretrained language models, BioBERT, Survey, PubMedBERT, Transformers, Self-supervised learning},
abstract = {Transformer-based pretrained language models (PLMs) have started a new era in modern natural language processing (NLP). These models combine the power of transformers, transfer learning, and self-supervised learning (SSL). Following the success of these models in the general domain, the biomedical research community has developed various in-domain PLMs starting from BioBERT to the latest BioELECTRA and BioALBERT models. We strongly believe there is a need for a survey paper that can provide a comprehensive survey of various transformer-based biomedical pretrained language models (BPLMs). In this survey, we start with a brief overview of foundational concepts like self-supervised learning, embedding layer and transformer encoder layers. We discuss core concepts of transformer-based PLMs like pretraining methods, pretraining tasks, fine-tuning methods, and various embedding types specific to biomedical domain. We introduce a taxonomy for transformer-based BPLMs and then discuss all the models. We discuss various challenges and present possible solutions. We conclude by highlighting some of the open issues which will drive the research community to further improve transformer-based BPLMs. The list of all the publicly available transformer-based BPLMs along with their links is provided at https://mr-nlp.github.io/posts/2021/05/transformer-based-biomedical-pretrained-language-models-list/.}
}
@article{FAN2025,
title = {Deep learning applications advance plant genomics research},
journal = {Horticultural Plant Journal},
year = {2025},
issn = {2468-0141},
doi = {https://doi.org/10.1016/j.hpj.2025.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S2468014125001839},
author = {Wenyuan Fan and Zhongwei Guo and Xiang Wang and Lingkui Zhang and Yuanhang Liu and Chengcheng Cai and Kang Zhang and Feng Cheng},
keywords = {Deep learning, Genomics, Transfer learning, Language model, Multi-omics, Neural network architecture},
abstract = {With the rapid development of high-throughput sequencing technologies and the accumulation of large-scale multi-omics data, deep learning (DL) has emerged as a powerful tool to solve complex biological problems, with particular promise in plant genomics. This review systematically examines the progress of DL applications in DNA, RNA, and protein sequence analysis, covering key tasks such as gene regulatory element identification, gene function annotation, and protein structure prediction, and highlighting how these DL applications illuminate research of plants, including horticultural plants. We evaluate the advantages of different neural network architectures and their applications in different biology studies, as well as the development of large language models (LLMs) in genomic modelling, such as the plant-specific models PDLLMs and AgroNT. We also briefly introduce the general workflow of the basic DL model for plant genomics study. While DL has significantly improved prediction accuracy in plant genomics, its broader application remains constrained by several challenges, including the limited availability of well-annotated data, computational capacity, innovative model architectures adapted to plant genomes, and model interpretability. Future advances will require interdisciplinary collaborations to develop DL applications for intelligent plant genomic research frameworks with broader applicability.}
}
@article{GALLEGO2025113211,
title = {Enhancing cross-encoders using knowledge graph hierarchy for medical entity linking in zero- and few-shot scenarios},
journal = {Knowledge-Based Systems},
volume = {314},
pages = {113211},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113211},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125002588},
author = {Fernando Gallego and Pedro Ruas and Francisco M. Couto and Francisco J. Veredas},
keywords = {Knowledge enrichment, Medical entity linking, Contrastive-learning, Knowledge bases, Candidate reranking, Zero-few shot},
abstract = {Medical Entity Linking (MEL) is a common task in natural language processing, focusing on the normalization of recognized entities from clinical texts using large knowledge bases (KBs). This task presents significant challenges, especially when working with electronic health records that often lack annotated clinical notes, even in languages like English. The difficulty increases in few-shot or zero-shot scenarios, where models must operate with minimal or no training data, a common issue when dealing with less-documented languages such as Spanish. Existing solutions that combine contrastive learning with external sources, like the Unified Medical Language System (UMLS), have shown competitive results. However, most of these methods focus on individual concepts from the KBs, ignoring relationships such as synonymy or hierarchical links between concepts. In this paper, we propose leveraging these relationships to enrich the training triplets used for contrastive learning, improving performance in MEL tasks. Specifically, we fine-tune several BERT-based cross-encoders using enriched triplets on three clinical corpora in Spanish : DisTEMIST, MedProcNER, and SympTEMIST. Our approach addresses the complexity of real-world data, where unseen mentions and concepts are frequent. The results show a notable improvement in lower top-k accuracies, surpassing the state-of-the-art by up to 5.5 percentage points for unseen mentions and by up to 5.9 points for unseen concepts. This improvement reduces the number of candidate concepts required for cross-encoders, enabling more efficient semi-automatic annotation and decreasing human effort. Additionally, our findings underscore the importance of leveraging not only the concept-level information in KBs but also the relationships between those concepts.}
}
@article{JUNG2024105483,
title = {VisualSiteDiary: A detector-free Vision-Language Transformer model for captioning photologs for daily construction reporting and image retrievals},
journal = {Automation in Construction},
volume = {165},
pages = {105483},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105483},
url = {https://www.sciencedirect.com/science/article/pii/S092658052400219X},
author = {Yoonhwa Jung and Ikhyun Cho and Shun-Hsiang Hsu and Mani Golparvar-Fard},
keywords = {Computer vision, Project controls, Image captioning, Project management, Machine learning, Natural language generation, Artificial intelligence},
abstract = {This paper presents VisualSiteDiary, a Vision Transformer-based image captioning model which creates human-readable captions for daily progress and work activity log, and enhances image retrieval tasks. As a model for deciphering construction photologs, VisualSiteDiary incorporates pseudo-region features, utilizes high-level knowledge in pretraining, and fine-tunes for diverse captioning styles. To validate VisualSiteDiary, a new image captioning dataset, VSD, is presented. This dataset includes many realistic yet challenging cases commonly observed in commercial building projects. Experimental results using five different metrics demonstrate that VisualSiteDiary provides superior-quality captions compared to the state-of-the-art image captioning models. Excluding the task of object recognition, the presented model also outperformed mPLUG –the state-of-the-art visual-language model– in the image retrieval task by 0.6% in precision and 0.9% in recall, respectively. Detailed discussions illustrate practical examples on how VisualSiteDiary improves the process of creating daily construction reports, paving the way for future developments in the field.}
}
@article{WU2021103036,
title = {Combining computer vision with semantic reasoning for on-site safety management in construction},
journal = {Journal of Building Engineering},
volume = {42},
pages = {103036},
year = {2021},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2021.103036},
url = {https://www.sciencedirect.com/science/article/pii/S2352710221008949},
author = {Haitao Wu and Botao Zhong and Heng Li and Peter Love and Xing Pan and Neng Zhao},
keywords = {Computer vision, Ontology, Semantic reasoning, Hazard identification, Construction safety management},
abstract = {Computer vision has been utilized to extract safety-related information from images with the advancement of video monitoring systems and deep learning algorithms. However, construction safety management is a knowledge-intensive task; for instance, safety managers rely on safety regulations and their prior knowledge during a jobsite safety inspection. This paper presents a conceptual framework that combines computer vision and ontology techniques to facilitate the management of safety by semantically reasoning hazards and corresponding mitigations. Specifically, computer vision is used to detect visual information from on-site photos while the safety regulatory knowledge is formally represented by ontology and semantic web rule language (SWRL) rules. Hazards and corresponding mitigations can be inferred by comparing extracted visual information from construction images with pre-defined SWRL rules. Finally, the example of falls from height is selected to validate the theoretical and technical feasibility of the developed conceptual framework. Results show that the proposed framework operates similar to the thinking model of safety managers and can facilitate on-site hazard identification and prevention by semantically reasoning hazards from images and listing corresponding mitigations.}
}
@article{JIANG2025101094,
title = {A two-stage retrieval-augmented generation framework for producing sustainable product design guidelines},
journal = {Sustainable Futures},
volume = {10},
pages = {101094},
year = {2025},
issn = {2666-1888},
doi = {https://doi.org/10.1016/j.sftr.2025.101094},
url = {https://www.sciencedirect.com/science/article/pii/S2666188825006586},
author = {Pingfei Jiang and Ji Han and Saeema Ahmed-Kristensen},
keywords = {Circular economy, Sustainable consumption and production, Large language models, Retrieval-augmented generation, Product design},
abstract = {Design is fundamental to product development and is widely recognised as a key driver for supporting the transition to a circular economy. As a problem-solving process, design relies heavily on knowledge retrieval and generation. Large language models (LLMs), such as ChatGPT, have demonstrated substantial capabilities in these areas, suggesting their potential to support sustainable product design. However, LLMs alone often produce ambiguous and overly general responses, limiting their effectiveness in delivering valuable sustainability insights. To address this limitation, retrieval-augmented generation (RAG), which integrates domain-specific information retrieval with generative modelling, has emerged as a promising approach to improve output accuracy and contextual relevance. This study introduces a novel two-stage RAG framework specifically designed to enhance sustainable product development by incorporating two external knowledge bases: one dedicated to systems design principles and the other to sustainable design strategies. This framework equips designers with practical sustainable design guidelines that align naturally with the design thinking process, requiring minimal prior sustainability expertise, an improvement over current approaches that depend heavily on specialised knowledge or assessment tools typically applied in later development stages. Expert-evaluated case studies demonstrate that this RAG-based framework covers 2.7 times more relevant product design specification topics than non-RAG approaches, with noticeable improvements across all performance metrics, particularly in product Desirability and Use lifecycle stage considerations. This research has implications for accelerating circular economy transitions by embedding sustainability considerations at the conceptual design stage, potentially transforming how designers approach sustainable product innovation with relatively low implementation costs.}
}
@article{MAN2021529,
title = {A decision support system for DM algorithm selection based on module extraction},
journal = {Procedia Computer Science},
volume = {186},
pages = {529-537},
year = {2021},
note = {14th International Symposium "Intelligent Systems},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.173},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921010103},
author = {T. Man and N.A. Zhukova and A.M. Thaw and S.A. Abbas},
keywords = {Decision support system, data analysis, ontology, module extraction},
abstract = {Data mining techniques are needed in various fields. However, most data researchers do not have sufficient knowledge and experience. Due to the significant number of algorithms and parameters for data analysis, intuition-based decisions can’t lead to optimal solutions. Ontology is widely adopted to build knowledge-driven decision support systems since it is suited to encapsulate the concepts and relationships of terms associated with various domains. It is suitable for capturing knowledge by computers, allowing sharing and reusing it whenever necessary. All concepts and relationships about data analysis can be described using Web Ontology Language (OWL). Its reasoning mechanism is vital in any knowledge-based system. Ontology can be reasoned to recommend suitable solutions for a specific analysis task by considering the data characteristics and task requirements. In the previous work, we have developed a system that describes comprehensive knowledge and logical internal relationships about data mining. We found that the reasoning and query complexity is high in the large size ontology, so this paper focuses on the modeling and implementation of an ontology-based decision support system for data mining algorithm selection that contains a new sub-ontology extraction method. Extracting effective content can significantly reduce the size of the ontology. This extraction method can improve query efficiency on the premise of ensuring the quality of information.}
}
@article{CONSOLI2023109375,
title = {Cultural gems linked open data: Mapping culture and intangible heritage in European cities},
journal = {Data in Brief},
volume = {49},
pages = {109375},
year = {2023},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2023.109375},
url = {https://www.sciencedirect.com/science/article/pii/S2352340923004912},
author = {Sergio Consoli and Valentina Alberti and Cinzia Cocco and Francesco Panella and Valentina Montalto},
keywords = {ICT and Society, Social applications of the Semantic Web, Linked Open Data for Cultural Heritage, Cultural domain ontologies, Semantic Web content creation, annotation, and extraction, Ontology mapping, merging, and alignment, RDF data processing},
abstract = {The recovery and resilience of the cultural and creative sectors after the COVID-19 pandemic is a current topic with priority for the European Commission. Cultural gems is a crowdsourced web platform managed by the Joint Research Centre of the European Commission aimed at creating community-led maps as well as a common repository for cultural and creative places across European cities and towns. More than 130,000 physical locations and online cultural activities in more than 300 European cities and towns are currently tracked by the application. The main objective of Cultural gems consists in raising a holistic vision of European culture, reinforcing a sense of belonging to a common European cultural space. This data article describes the ontology developed for Cultural gems, adopted to represent the domain of knowledge of the application by means of FAIR (Findable, Accessible, Interoperable, Reusable) principles and following the paradigms of Linked Open Data (LOD). We provide an overview of this dataset, and describe the ontology model, along with the services used to access and consume the data.}
}
@article{LIGUORI2025131191,
title = {Modeling events and interactions through temporal processes: A survey},
journal = {Neurocomputing},
volume = {653},
pages = {131191},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.131191},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225018636},
author = {Angelica Liguori and Luciano Caroprese and Marco Minici and Bruno Veloso and Francesco Spinnato and Mirco Nanni and Giuseppe Manco and João Gama},
keywords = {Point processes, Probabilistic models, Deep learning},
abstract = {In real-world scenarios, numerous phenomena generate a series of events that occur in continuous time. Point processes provide a natural mathematical framework for modeling these event sequences. In this comprehensive survey, we aim to explore probabilistic models that capture the dynamics of event sequences through temporal processes. We revise the notion of event modeling and provide the mathematical foundations that underpin the existing literature on this topic. To structure our survey effectively, we introduce an ontology that categorizes the existing approaches considering three horizontal axes: modeling, inference and estimation, and application. We conduct a systematic review of the existing approaches, with a particular focus on those leveraging deep learning techniques. Finally, we delve into the practical applications where these proposed techniques can be harnessed to address real-world problems related to event modeling. Additionally, we provide a selection of benchmark datasets that can be employed to validate the approaches for point processes.}
}
@article{SHAH2024102910,
title = {Re-conceptualizing climate maladaptation: Complementing social-ecological interactions with relational socionatures},
journal = {Global Environmental Change},
volume = {88},
pages = {102910},
year = {2024},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2024.102910},
url = {https://www.sciencedirect.com/science/article/pii/S0959378024001146},
author = {Sameer H. Shah and Leila M. Harris and K.J. Joy and Trevor Birkenholtz and Idowu Ajibade},
keywords = {Climate maladaptation, Vulnerability, Relational, Ontology, Socionature, India},
abstract = {Cases of climate maladaptation are increasingly documented. Its identification and redressal has become a priority for researchers and policymakers concerned with climate vulnerability reduction. The ability to address climate maladaptation hinges on being open to its diverse causes, manifestations, and impacts. This study argues that climate maladaptation analyses are dominated by an “interactional ontology”—the understanding that it can be explained as an observable outcome from how separate social, economic, and political systems interact in moments of time. Consequently, efforts to curb climate maladaptation often target the institutional contexts (e.g., rules, regulations) understood as enabling adaptation practices to aggravate climate risks. But this only captures a partial aspect of climate maladaptation, neglecting underlying causes and processes. We argue a “relational ontology” can complement the “why and how” of maladaptation. A relational ontology understands climate maladaptation as an evolving process constituted through dynamic material and discursive relations, versus an observable outcome from separately interacting systems. By analyzing how adaptation initiatives are related to, framed, and politicized, assembly processes are rendered visible. To demonstrate this, we study the Government of Maharashtra’s (India) Jalyukt Shivar Abhiyan, a program aimed at increasing water conservation to “free” 20,000 villages from drought impacts. From our theorization and empirical case, we discuss how a relational ontology contributes to debates in the climate maladaptation literature and invites approaches for mitigating this phenomenon.}
}
@article{PEREIRA2022235,
title = {A web-based Voice Interaction framework proposal for enhancing Information Systems user experience},
journal = {Procedia Computer Science},
volume = {196},
pages = {235-244},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S187705092102233X},
author = {Tiago F. Pereira and Arthur Matta and Carlos M. Mayea and Frederico Pereira and Nelson Monroy and João Jorge and Tiago Rosa and Carlos E. Salgado and Ana Lima and Ricardo J. Machado and Luís Magalhães and Telmo Adão and Miguel Ángel {Guevara López} and Dibet Garcia Gonzalez},
keywords = {Automatic Speech Recognition, Deep Learning, Voice Activity Detection, Ontologies, Graph Database, Ontological Component, Semantic Interoperability},
abstract = {Nowadays, numerous organisations of different dimensions and business sectors operate in highly challenging and dynamic environments, wherein the supporting information systems (IS) are becoming increasingly complex. In this context, assistive tools capable of tackling such complexity have the potential to aid users improving their performance and effectiveness, as well as to streamline businesses’ processes and promote entrepreneurial-level competitivity. Following this line of research, a web-based speech-to-term recognition approach is presented as a solution to endow IS with advanced capabilities for providing an easier (more natural) and straightforward interaction with baseline functionalities, by combining relevant techniques such as Voice Activity Detection (VAD), Automatic Speech Recognition (ASR), Natural Language Processing (NLP), and an Ontological Database (OD), mapping the IS’ functionalities and characteristics, is proposed. The developed interoperable system allows the conversion of speech to text - deriving into IS instructions - that is, in turn, submitted to on an ontological database wherein a term-based query is performed to elicit a set of available commands to be executed in the web context. These commands, fully mapped in the ontological database, are divided into three categories: a) navigation by menus/links, b) buttons interaction (e.g., submit forms) and c) completion of form fields. The proposed framework was experimentally tested in close to real conditions, resorting to an Enterprise Resource Planning (ERP) tool supplied by ERP Company.}
}
@article{MEJRI20231425,
title = {A proposed guidance approach for BP performance improvement},
journal = {Procedia Computer Science},
volume = {225},
pages = {1425-1437},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.131},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923012899},
author = {Sarra MEJRI and Sonia AYACHI GHANNOUCHI},
keywords = {Industry 4.0, Performance criteria, IBPM, Ontology},
abstract = {The emergence of Intelligent BPM (IBPM) is built upon adding smart technologies and business intelligence to BPM. In such a harsh environment, improving BPs using the most appropriate and adequate technologies is considered to be a complex task to deal with. Recently, for the sake of exploring the use of technologies 4.0 in different domains using specific languages and in order to efficiently improve BPs in the context of IBPM, we need to define a specific approach for deciding about the technologies 4.0 to adopt. Thereby, such an approach would improve business processes effectively in this context of IBPM. For this purpose, it is worth noting that considering performance criteria can help us to integrate a specific intelligent technology to a given business process and it is important to develop an IBPM ontology as a first contribution in our approach. This approach was implemented in a tool named BPIGuide. To evaluate this approach, we have studied the post-operative monitoring (POM) process. The results of this study would help users to select the most adequate technology 4.0 that best fit their needs on performance criteria.}
}
@article{CHEN2025102221,
title = {Tab2KGWiz: Interactive assistant for mapping tabular data to knowledge graphs},
journal = {SoftwareX},
volume = {31},
pages = {102221},
year = {2025},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2025.102221},
url = {https://www.sciencedirect.com/science/article/pii/S2352711025001888},
author = {Z. Chen and J. Chamorro-Padial and J.M. López-Gil and R.M. Gil and R. García},
keywords = {FAIR principles, Interoperability, Reusability, Knowledge graphs},
abstract = {The reusability of research data is a key aspect of science development. According to FAIR principles, to make data reusable, it is required to make it interoperable. In this work, we present Tab2KGWiz, a software designed to enrich tabular data by adding contextual information formalized as Knowledge Graphs and based on ontologies. The metadata to automate the mapping of tabular datasets into Knowledge Graphs is generated interactively by the users under the guidance of Tab2KGWiz. It is then stored and exported into the YARRRML format, which can then be reused by tools that automate the mapping from tabular to semantic graph data based on the RDF standard. Tab2KGWiz is designed to help researchers enrich the metadata of their dataset by using an intuitive graphical user interface. The software is designed with a microservice architecture with a Java Spring back-end and a React front-end. Our application helps researchers and other data-sharing actors make their data easier to discover, access, and integrate, fostering collaboration and innovation.}
}
@article{LIU2024111818,
title = {The fusion of fuzzy theories and natural language processing: A state-of-the-art survey},
journal = {Applied Soft Computing},
volume = {162},
pages = {111818},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111818},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624005921},
author = {Ming Liu and Hongjun Zhang and Zeshui Xu and Kun Ding},
keywords = {Fuzzy theory, Natural language processing, Fusion, Artificial intelligence},
abstract = {Recent years have witnessed a drastic surge in natural language processing (NLP), which is a popular research orientation in artificial intelligence. In contrast to precise numbers, human language is very complex and diverse, with millions of expressions, both spoken and written. It is due to this ambiguity and imprecision that most of the problems in NLP relating to cognition, translation, and understanding are non-trivial. Fuzzy theory, which accepts the fact that ambiguity exists, aims to address and actively quantify conceptual vagueness into messages that can be processed by computers. Following the thread of recent studies, we systematically review the fusion of fuzzy theory and NLP technologies from the aspects of commonly used fuzzy theories in NLP, the NLP tasks fuzzy theories are applied to, the application fields of fusion and the basic paradigms of fusion. Towards the end of this paper, we delineate the constraints and obstacles encountered in current researches, while also endeavoring to suggest avenues for enhancement that may serve as a reference for subsequent scholarly inquiry.}
}
@article{ALKHARIJI2023280,
title = {Semantics-based privacy by design for Internet of Things applications},
journal = {Future Generation Computer Systems},
volume = {138},
pages = {280-295},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22002746},
author = {Lamya Alkhariji and Suparna De and Omer Rana and Charith Perera},
keywords = {Privacy, Privacy by Design, Internet of Things, Semantic web, Ontology, Context awareness},
abstract = {As Internet of Things (IoT) technologies become more widespread in everyday life, privacy issues are becoming more prominent. The aim of this research is to develop a personal assistant that can answer software engineers’ questions about Privacy by Design (PbD) practices during the design phase of IoT system development. Semantic web technologies are used to model the knowledge underlying PbD measurements, their intersections with privacy patterns, IoT system requirements and the privacy patterns that should be applied across IoT systems. This is achieved through the development of the PARROT ontology, developed through a set of representative IoT use cases relevant for software developers. This was supported by gathering Competency Questions (CQs) through a series of workshops, resulting in 81 curated CQs. These CQs were then recorded as SPARQL queries, and the developed ontology was evaluated using the Common Pitfalls model with the help of the Protégé HermiT Reasoner and the Ontology Pitfall Scanner (OOPS!), as well as evaluation by external experts. The ontology was assessed within a user study that identified that the PARROT ontology can answer up to 58% of privacy-related questions from software engineers.}
}
@article{ARAZZI2025100765,
title = {NLP-based techniques for Cyber Threat Intelligence},
journal = {Computer Science Review},
volume = {58},
pages = {100765},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100765},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000413},
author = {Marco Arazzi and Dincy {R. Arikkat} and Serena Nicolazzo and Antonino Nocera and Rafidha {Rehiman K.A.} and Vinod P. and Mauro Conti},
keywords = {Cyber threat intelligence, Natural language processing, Security, Named entity recognition, Knowledge graph, Large language model},
abstract = {In the digital era, threat actors employ sophisticated techniques for which, often, digital traces in the form of textual data are available. Cyber Threat Intelligence (CTI) is related to all the solutions inherent to data collection, processing, and analysis useful to understand a threat actor’s targets and attack behavior. Currently, CTI is assuming an always more crucial role in identifying and mitigating threats and enabling proactive defense strategies. In this context, NLP, an artificial intelligence branch, has emerged as a powerful tool for enhancing threat intelligence capabilities. This survey paper provides a comprehensive overview of NLP-based techniques applied in the context of threat intelligence. It begins by describing the foundational definitions and principles of CTI as a major tool for safeguarding digital assets. It then undertakes a thorough examination of NLP-based techniques for CTI data crawling from Web sources, CTI data analysis, Relation Extraction from cybersecurity data, CTI sharing and collaboration, security threats of CTI, and role of LLM in this domain. Finally, the challenges and limitations of NLP in threat intelligence are exhaustively examined, including data quality issues and ethical considerations. This survey draws a complete framework and serves as a valuable resource for security professionals and researchers seeking to understand the state-of-the-art NLP-based threat intelligence techniques and their potential impact on cybersecurity.}
}
@article{ABADNAVARRO2024107918,
title = {A knowledge graph-based data harmonization framework for secondary data reuse},
journal = {Computer Methods and Programs in Biomedicine},
volume = {243},
pages = {107918},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107918},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723005849},
author = {Francisco Abad-Navarro and Catalina Martínez-Costa},
keywords = {Semantic interoperability, Ontologies, SNOMED CT, Knowledge graphs, Semantic query},
abstract = {Background and objective
The adoption of new technologies in clinical care systems has propitiated the availability of a great amount of valuable data. However, this data is usually heterogeneous, requiring its harmonization to be integrated and analysed. We propose a semantic-driven harmonization framework that (1) enables the meaningful sharing and integration of healthcare data across institutions and (2) facilitates the analysis and exploitation of the shared data.
Methods
The framework includes an ontology-based common data model (i.e. SCDM), a data transformation pipeline and a semantic query system. Heterogeneous datasets, mapped to different terminologies, are integrated by using an ontology-based infrastructure rooted in a top-level ontology. A graph database is generated by using these mappings, and web-based semantic query system facilitates data exploration.
Results
Several datasets from different European institutions have been integrated by using the framework in the context of the European H2020 Precise4Q project. Through the query system, data scientists were able to explore data and use it for building machine learning models.
Conclusions
The flexible data representation using RDF, together with the formal semantic underpinning provided by the SCDM, have enabled the semantic integration, query and advanced exploitation of heterogeneous data in the context of the Precise4Q project.}
}
@article{JASPERS2025157,
title = {Identification of machine parameters in battery cell production: A comparison of different methods and approaches for mapping parameters},
journal = {Procedia CIRP},
volume = {134},
pages = {157-162},
year = {2025},
note = {58th CIRP Conference on Manufacturing Systems 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125004676},
author = {Wilhelm Jaspers and Arno Schmetz and David Roth and Bandik Becker and Achim Kampker},
keywords = {battery cell production, identification, machine learning},
abstract = {Optimization of battery cell production processes is increasingly based on digital use cases that can reduce costs and improve quality. However, these requires accurate and semantically understandable data that is human- and machine-readable and stored in standardized formats. Machine manufacturers often use unique or proprietary parameter standards and naming conventions, making data integration difficult. Linking machine data to semantic database standards requires expert knowledge and significant manual effort. Automating this process could reduce the workload enormously. This paper evaluates approaches for automating the mapping and comparison of machine parameter sets within the context of battery cell production. Using real production data and semantic modelling from multiple partners, different methods were applied to link individual parameters. The outcomes were assessed with specific metrics, revealing that Large Language Models (LLMs) show significant promise due to their flexibility and effectiveness in handling complex semantic comparisons.}
}
@article{STEADMAN2025104292,
title = {Resigned reductionism: Reconceptualising digital imaginaries of automated natural capital},
journal = {Geoforum},
volume = {162},
pages = {104292},
year = {2025},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2025.104292},
url = {https://www.sciencedirect.com/science/article/pii/S0016718525000922},
author = {Hope Steadman},
keywords = {Digital imaginary, Resignation, Ontological reductionism, Automation technology, Natural capital, Rewilding},
abstract = {Natural capital accounting is widely adopted in the UK as a means of supporting environmental measurement and management, with digital automation technologies increasingly shaping its practice. Natural capital approaches have been critiqued by geographers for being ontologically reductive and for over-privileging certain technocratic knowledges. Nevertheless, this analysis can overlook environmental practitioners’ awareness of and attempts to address these critiques. This paper therefore makes two interventions. It first analyses what it terms the Automated Natural Capital (ANC) imaginary, exploring how stakeholders involved in the development of an ANC tool articulate, experience and envision it. It finds that such tools are indeed founded upon functional relations with nature, failing to acknowledge alternative knowledges, whilst also black boxing the socio-political logics behind the choice of reductionist valuations. Secondly, however, it identifies how and why stakeholders working at a Scottish rewilding site resign themselves to ANC, deemed as necessary to channel attention and support to nature recovery. It conceptualises this as “resigned reductionism”, understood as affectively co-constituting the ANC imaginary. The paper ends by suggesting that resignation is bound up in promises of the future digital optimisation of ANC, which risks displacing transformational alternatives to an unrealised future. It therefore argues for the wider significance of this concept in contemporary environmental governance.}
}
@article{HEREDIAALVARO2025103007,
title = {An advanced retrieval-augmented generation system for manufacturing quality control},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103007},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103007},
url = {https://www.sciencedirect.com/science/article/pii/S147403462400658X},
author = {José Antonio {Heredia Álvaro} and Javier González Barreda},
keywords = {Knowledge-based systems, Quality control, Ceramic tile, Retrieval-augmented generation, Large language models},
abstract = {The rise of Large Language Models (LLMs) with generative artificial intelligence has revolutionized the development of knowledge-based systems, enabling intuitive interactions through natural language. This paper explores the implementation of an advanced Retrieval-Augmented Generation (RAG) system, designed to improve manufacturing quality control by utilizing the capabilities of LLMs, particularly OpenAI’s GPT models. We focus on the ceramic tile manufacturing process, where the system retrieves and analyzes specialized bibliographic sources to diagnose defects and propose solutions. In addition to core RAG functionalities, the system incorporates tailored pre-processing and post-processing mechanisms to optimize document retrieval and response generation. The system’s effectiveness in solving quality issues is demonstrated through its application in identifying defect causes and generating actionable solutions, significantly improving non-conformities management. This approach not only streamlines troubleshooting but also enhances the quality control system, providing a comprehensive, scalable tool for manufacturers.}
}
@article{LI2025106796,
title = {Semi-supervised named entity recognition in low-resource domains: A case study of rare earth elements in coal},
journal = {Ore Geology Reviews},
volume = {185},
pages = {106796},
year = {2025},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2025.106796},
url = {https://www.sciencedirect.com/science/article/pii/S0169136825003567},
author = {Pengfei Li and Wenze Lin and Yuqing Wang and Na Xu and Wei Zhu and Wei Liu},
keywords = {Rare earth elements in coal, Low-resource named entity recognition, Semi-supervised learning, Self-training, Distant supervision, Large language models},
abstract = {Geological literature serves as a crucial repository of information for advancing geological understanding and guiding mineral exploration. However, manually extracting geological information from unstructured text is labor-intensive and inefficient. Named entity recognition (NER), a core task in information extraction (IE), offers an automated solution by identifying and classifying geological entities. While NER has been widely applied in geological fields, it remains limited in coal geology and coal-hosted critical metal deposits, which represent a typical low-resource domain. Therefore, this work proposes an efficient approach for the low-resource NER, using rare earth elements (REE) in coal as a representative case. An unlabeled text corpus is first collected from the Web of Science (WoS) database. Distant supervision and large language models (LLM) are then integrated to directly annotate part of the corpus. Domain experts subsequently validate and refine these annotations, yielding a high-quality labeled dataset for training the semi-supervised NER model. Specifically, four widely used NER models (i.e., BERT, BERT-CRF, BiLSTM-CRF, and BERT-BiLSTM-CRF models) are compared to identify the optimal model for self-training. This work focuses on seven geological entity types, including rock, mineral, element, maceral, location, stratum, and geologic time. Experimental results show that the BERT-CRF model achieves the best performance, with an F1-score of 0.8467. After applying the self-training algorithm, the F1-score improves to 0.8702, highlighting the effectiveness of the proposed approach in enhancing NER performance in the low-resource domain. Additionally, an entity-based literature retrieval system is developed to facilitate the accurate and efficient extraction of geological information from relevant sources.}
}
@article{JALDI2025100857,
title = {Education in the era of Neurosymbolic AI},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100857},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100857},
url = {https://www.sciencedirect.com/science/article/pii/S157082682400043X},
author = {Chris Davis Jaldi and Eleni Ilkou and Noah Schroeder and Cogan Shimizu},
keywords = {Education, Knowledge graphs, Large language models, Neurosymbolic AI, Agents},
abstract = {Education is poised for a transformative shift with the advent of neurosymbolic artificial intelligence (NAI), which will redefine how we support deeply adaptive and personalized learning experiences. The integration of Knowledge Graphs (KGs) with Large Language Models (LLMs), a significant and popular form of NAI, presents a promising avenue for advancing personalized instruction via neurosymbolic educational agents. By leveraging structured knowledge, these agents can provide individualized learning experiences that align with specific learner preferences and desired learning paths, while also mitigating biases inherent in traditional AI systems. NAI-powered education systems will be capable of interpreting complex human concepts and contexts while employing advanced problem-solving strategies, all grounded in established pedagogical frameworks. In this paper, we propose a system that leverages the unique affordances of KGs, LLMs, and pedagogical agents – embodied characters designed to enhance learning – as critical components of a hybrid NAI architecture. We discuss the rationale for our system design and the preliminary findings of our work. We conclude that education in the era of NAI will make learning more accessible, equitable, and aligned with real-world skills. This is an era that will explore a new depth of understanding in educational tools.}
}
@article{PASCAZIO2024105428,
title = {Question-answering system for combustion kinetics},
journal = {Proceedings of the Combustion Institute},
volume = {40},
number = {1},
pages = {105428},
year = {2024},
issn = {1540-7489},
doi = {https://doi.org/10.1016/j.proci.2024.105428},
url = {https://www.sciencedirect.com/science/article/pii/S1540748924002360},
author = {Laura Pascazio and Dan Tran and Simon D. Rihm and Jiaru Bai and Sebastian Mosbach and Jethro Akroyd and Markus Kraft},
keywords = {Automated kinetic modeling, Reaction mechanism, Cheminformatics, Artificial intelligence, Knowledge graph},
abstract = {In this paper, we introduce for the first time a natural language question-answering (QA) system specifically designed for the field of combustion kinetics. This system marks a significant step towards achieving the PrIMe vision as outlined by Frenklach in 2007, offering a user-friendly interface that allows researchers and practitioners to easily access and query information about chemical mechanisms. This QA system is a key component of “The World Avatar” (TWA), a dynamic framework built upon semantic web technologies. TWA is characterized by its layered structure, which includes a knowledge graph (KG), software agents, and real-world data integration. These layers collectively create a comprehensive unified system for managing and analyzing complex chemical data from various domains. We detail the enhancements made to TWA’s ontologies (OntoSpecies, OntoKin, and OntoCompChem) to meet specific challenges in chemical kinetics and improve their representation accuracy. By focusing on data provenance and interoperability, our approach ensures transparent and reliable data management that adheres to the FAIR principles, which is vital for precise information retrieval and analysis. The role of software agents in populating these ontologies is highlighted, showcasing how they transform raw data into meaningful structured knowledge and generate new insights within the TWA ecosystem. Additionally, the semantic web technologies’ interoperability feature facilitates data integration and exchange across different platforms and tools, making the data machine-actionable. We instantiated in the KG data on four H2/O2 and five CH4/O2 reaction mechanisms taken from the literature and we then demonstrate the QA system’s capabilities in answering questions related to these reaction mechanisms as a proof of concept. Lastly, we discuss the future directions of the TWA framework, which include not only future extensions of the QA system but also the integration of external tool to automate tasks such as generation of kinetic mechanism, further expanding TWA’s functionality and application in the field of chemical kinetics.}
}
@article{GONG2024108399,
title = {Advancing microbial production through artificial intelligence-aided biology},
journal = {Biotechnology Advances},
volume = {74},
pages = {108399},
year = {2024},
issn = {0734-9750},
doi = {https://doi.org/10.1016/j.biotechadv.2024.108399},
url = {https://www.sciencedirect.com/science/article/pii/S0734975024000934},
author = {Xinyu Gong and Jianli Zhang and Qi Gan and Yuxi Teng and Jixin Hou and Yanjun Lyu and Zhengliang Liu and Zihao Wu and Runpeng Dai and Yusong Zou and Xianqiao Wang and Dajiang Zhu and Hongtu Zhu and Tianming Liu and Yajun Yan},
keywords = {Microbial production, Synthetic biology, Genome annotation, Enzyme function prediction, Artificial protein design, Pathway prediction, Artificial intelligence (AI), Large language models (LLMs)},
abstract = {Microbial cell factories (MCFs) have been leveraged to construct sustainable platforms for value-added compound production. To optimize metabolism and reach optimal productivity, synthetic biology has developed various genetic devices to engineer microbial systems by gene editing, high-throughput protein engineering, and dynamic regulation. However, current synthetic biology methodologies still rely heavily on manual design, laborious testing, and exhaustive analysis. The emerging interdisciplinary field of artificial intelligence (AI) and biology has become pivotal in addressing the remaining challenges. AI-aided microbial production harnesses the power of processing, learning, and predicting vast amounts of biological data within seconds, providing outputs with high probability. With well-trained AI models, the conventional Design-Build-Test (DBT) cycle has been transformed into a multidimensional Design-Build-Test-Learn-Predict (DBTLP) workflow, leading to significantly improved operational efficiency and reduced labor consumption. Here, we comprehensively review the main components and recent advances in AI-aided microbial production, focusing on genome annotation, AI-aided protein engineering, artificial functional protein design, and AI-enabled pathway prediction. Finally, we discuss the challenges of integrating novel AI techniques into biology and propose the potential of large language models (LLMs) in advancing microbial production.}
}
@article{VERDONK20252881,
title = {Learning the language of plant immunity: opportunities and challenges for AI-assisted modelling of fungal effector x host protein complexes},
journal = {Computational and Structural Biotechnology Journal},
volume = {27},
pages = {2881-2889},
year = {2025},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2025.06.048},
url = {https://www.sciencedirect.com/science/article/pii/S2001037025002648},
author = {C. Verdonk and KK Gagalova and S. Raffaele and MC Derbyshire},
keywords = {Effector, Protein structure, Protein interaction, Plant receptor, Plant pathogen, AI structure prediction},
abstract = {Phytopathogenic fungi cause substantial crop losses worldwide. They secrete proteins called effectors, which enable infection through interactions with diverse host proteins. These interactions are fundamentally important to plant disease and its practical control. New artificial intelligence (AI) techniques can predict many individual protein structures to near experimental accuracy. Although these techniques also predict protein complexes, they are not as accurate as single-protein models. Use of AI to study interactions between fungal pathogen effectors and plant proteins is currently limited. However, despite some challenges, early adoption of AI has highlighted its potential. General improvements in AI-assisted protein complex modelling may create more opportunities in future. This review focuses on recent research using AI to study the interactions between fungal effectors and plant proteins, outlining challenges and emerging opportunities.}
}
@incollection{RENDON2025,
title = {The Study of Language and Culture in Amazonia},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00661-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032395504100661X},
author = {Jorge Gómez Rendón},
keywords = {Biocultural diversity, Ideophones, Language contact, Linguistic diversity, Multilingualism, Perspectivism, Unclassified languages, Verbal art},
abstract = {Amazonia is one of the world's most diverse biocultural areas. Biological diversity is paralleled by linguistic diversity, the area boasting over 300 living languages, 25 language families, and 30 unclassified languages. References to this diversity were recorded from the earliest chronicles of European conquerors. Still, the linguistic and cultural knowledge of Amazonia became established as a field of scientific inquiry only in the mid-to-late 20th century. With biocultural diversity as a background, Amazonia has become, in the last decades, a living laboratory for linguistic description, language classification, language contact, orality, and discourse.}
}
@article{JIANG2024103769,
title = {How far is reality from vision: An online data-driven method for brand image assessment and maintenance},
journal = {Information Processing & Management},
volume = {61},
number = {5},
pages = {103769},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103769},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001298},
author = {Xiaoyan Jiang and Jie Lin and Chao Wang and Lixin Zhou},
keywords = {Brand image assessment, Brand maintenance, Online data, Ontology learning, Text mining},
abstract = {Brand image assessment and maintenance are essential for branding. This paper constructs an online data-driven quantitative brand image assessment method using the classic brand image theory as a conceptual model. The method is organized as follows. First, with domain expert knowledge and deep learning, this paper constructs a task ontology to clearly describe the brand image constituent content, constituent relationship, properties, and property values. Then, using the task ontology as a priori knowledge, we identify the content of brand associations from User-generated Content (UGC) and Firm-generated content (FGC), respectively, and calculate the associations’ favorability, strength and uniqueness; classify brand associations into three categories: functional, experiential, and symbolic to achieve a dual-perspectives (consumer perceptions & corporate claims) brand image assessment. Finally, this study compares the dual-perspective brand images from the components and benefits to construct a brand image communication and maintenance strategy. The development and validation of the methodology take the Chinese New Energy Vehicle (NEV) market as the analysis object. The proposed dual-perspective brand image quantitative assessment model is a new development of brand image evaluation and maintenance theoretical method in the digital era. It is also a practical tool for brand management in enterprises.}
}
@article{CLEVELAND2018556,
title = {Global consumer culture: epistemology and ontology},
journal = {International Marketing Review},
volume = {36},
number = {4},
pages = {556-580},
year = {2018},
issn = {0265-1335},
doi = {https://doi.org/10.1108/IMR-10-2018-0287},
url = {https://www.sciencedirect.com/science/article/pii/S0265133518000746},
author = {Mark Cleveland and Fabian Bartsch},
keywords = {Global consumer culture, Appropriation, Globalization, Acculturation, Indigenization, Creolization},
abstract = {Purpose
The purpose of this paper is to propose a conceptual framework that highlights the reinforcing nature of global consumer culture (GCC). In doing so, this paper highlights a dialectic process in which consumers trade-off, appropriate, indigenize and creolize consumption into multiple GCCs.
Design/methodology/approach
The approach is conceptual with illustrative examples.
Findings
GCC is a reinforcing process shaped by global culture flows, acculturation, deterritorialization, and cultural and geographic specific entities. This process allows consumers to indigenize GCC, and GCC to contemporaneously appropriate aspects from myriad localized cultures, producing creolized cultures.
Research limitations/implications
Marketing research and practices need to shift away from the dichotomous view of global and local consumption fueled by a misleading view of segmentation. Instead, marketers should focus on identifying the permutations of emerging GCCs, how these operate according to the context and accordingly position their marketing mix to accommodate them.
Originality/value
The proposed model reviews and integrates existing literature to highlight fundamental research directions that present a comprehensive overview of GCCs, its shortcomings and future directions.}
}
@article{LI2025102692,
title = {From text to insight: A natural language processing-based analysis of burst and research trends in HER2-low breast cancer patients},
journal = {Ageing Research Reviews},
volume = {106},
pages = {102692},
year = {2025},
issn = {1568-1637},
doi = {https://doi.org/10.1016/j.arr.2025.102692},
url = {https://www.sciencedirect.com/science/article/pii/S1568163725000388},
author = {Muyao Li and Ang Zheng and Mingjie Song and Feng Jin and Mengyang Pang and Yuchong Zhang and Ying Wu and Xin Li and Mingfang Zhao and Zhi Li},
keywords = {HER2-low breast cancer, bibliometric analysis, visual analysis, antibodydrug conjugate (ADC), T-DXd},
abstract = {With the intensification of population aging, the proportion of elderly breast cancer patients is continuously increasing, among which those with low HER2 expression account for approximately 45 %-55 % of all cases within traditional HER2-negative breast cancer. Concurrently, the significant therapeutic effect of T-DXd on patients with HER2-low tumors has brought this group into the public spotlight. Since the clinical approval of T-DXd in 2019, there has been a significant vertical surge in the volume of publications within this domain. We analyzed 512 articles on HER2-low breast cancer from the Web of Science Core Collection using bibliometrics, topic modeling, and knowledge graph techniques to summarize the current state and trends of research in this domain. Research efforts are particularly concentrated in the United States and China. Our analysis revealed six main research directions: HER2 detection, omics and clinical biomarkers, basic and translational research, neoadjuvant therapy and prognosis, progress of ADC drugs and clinical trials. To enhance the therapeutic efficacy and safety of antibodydrug conjugates (ADCs), researchers are actively exploring potential drug candidates other than T-DXd, with numerous ADC drugs emerging in clinical practice and trials. By incorporating emerging treatment strategies such as immunotherapy and employing circulating tumor cell (CTC) detection techniques, progress has been made toward improving the prognosis of patients with low HER2 expression. We believe that these research efforts hold promise as compelling evidence that HER2-low breast cancer may constitute a distinct and independent subtype.}
}
@article{WEN2023102172,
title = {Systematic knowledge modeling and extraction methods for manufacturing process planning based on knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102172},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102172},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003002},
author = {Peihan Wen and Yan Ma and Ruiquan Wang},
keywords = {Knowledge graph, Process knowledge, Ontology construction, Knowledge extraction, Question answering},
abstract = {Process knowledge is an important part of intelligent manufacturing, which has become the trend of global manufacturing industry. However, process planning in manufacturing heavily relies on experience and historical knowledge that cannot be effectively reused. Also, process knowledge is complicatedly structured, widely distributed, and weakly associated, which makes it more difficult to be shared and reused. Moreover, existing methods of organizing process knowledge are labor-intensive, inefficient and inevitable to extensive manual annotation. To overcome the above problems, a systematic method of knowledge graph construction for process knowledge is presented. First, through key concepts and relationships analysis, a domain ontology for process knowledge is proposed. Second, a pattern-based bootstrapping framework with a 2-level masking technique is established to perform process knowledge extraction, which does not require manual annotation and avoids overfitting issues. Third, a meta path-based question answering over knowledge graph approach is presented to support process planning, which can effectively capture keywords of the input question, present the intention and match the designed path. Finally, taking the production process of radiator as a research object, a corresponding manufacturing process knowledge graph is constructed and applied to process planning scenario, and experiments validate the feasibility of the proposed methods.}
}
@article{SAFITRA2024381,
title = {Advancements in Artificial Intelligence and Data Science: Models, Applications, and Challenges},
journal = {Procedia Computer Science},
volume = {234},
pages = {381-388},
year = {2024},
note = {Seventh Information Systems International Conference (ISICO 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924003752},
author = {Muhammad Fakhrul Safitra and Muharman Lubis and Tien Fabrianti Kusumasari and Deyana Prastika Putri},
keywords = {Artificial intelligence, Machine learning, Data Science, Bioinspired systems, Smart cities},
abstract = {The text that follows gives an overview of the numerous uses of artificial intelligence (AI) and data science in a variety of fields, focusing on their potential advantages and drawbacks. Healthcare, finance, education, smart cities, and the development of disease therapies have all showed promise for AI. The study examines several AI models, such as ontological, statistical, hybrid, and biological models, in addition to the restrictions placed by deep learning techniques. The study on brain architecture and function is also included in the article, along with applications of multivariate pattern analysis, bioinspired systems, and machine learning to the analysis and processing of enormous datasets. The literature study emphasizes how critical it is to address societal and ethical issues with AI.}
}
@article{PU2022104433,
title = {Extending IFC for multi-component subgrade modeling in a railway station},
journal = {Automation in Construction},
volume = {141},
pages = {104433},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104433},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003065},
author = {Hao Pu and Xiaomeng Fan and Paul Schonfeld and Wei Li and Wei Zhang and Fanghua Wei and Peng Wang and Changhuai Li},
keywords = {Building information modeling, IFC extension, Ontology, Subgrade, Railway station},
abstract = {The BIM modeling of Multi-Component Subgrade in a Railway Station (MCSRS) suffers from an inefficient manual approach due to its specific features and absence of a data standard. The Industry Foundation Classes (IFC) has been widely used for describing various components in the building industry while its current version fails to depict an MCSRS. In this study, the IFC is extended to describe the geometric information and hierarchy for an MCSRS, based on which an automatic workflow for converting the original design data to the corresponding 3D BIM model complying with IFC standard is developed. To validate the methodology, a real-world case is used for testing the modeling result, in which several different modeling cases between two cross-sections are examined. The results show that the BIM modeling for an MCSRS is greatly accelerated compared with the manual modeling approach and can be used as a cross-platform tool.}
}
@article{QIU2024200308,
title = {ChatGPT and finetuned BERT: A comparative study for developing intelligent design support systems},
journal = {Intelligent Systems with Applications},
volume = {21},
pages = {200308},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200308},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323001333},
author = {Yunjian Qiu and Yan Jin},
keywords = {Language model, Knowledge transferring, Knowledge elicitation, Text classification, Text generation},
abstract = {Large Language Models (LLMs), like ChatGPT, have sparked considerable interest among researchers across diverse disciplines owing to their remarkable text processing and generation capabilities. While ChatGPT is typically employed for tasks involving general knowledge, researchers increasingly explore the potential of this LLM-based tool in specific domains to enhance productivity. This study aims to compare the performance of a finetuned BERT model with that of ChatGPT on a domain-specific dataset in the context of developing an intelligent design support system. Through experiments conducted on classification and generation tasks, the knowledge transfer and elicitation abilities of ChatGPT are examined and contrasted with those of the finetuned BERT model. The findings indicate that ChatGPT exhibits comparable performance to the finetuned BERT model in sentence-level classification tasks but struggles with short sequences. However, ChatGPT's classification performance significantly improves when a few-shot setting is applied. Moreover, it can filter out unrelated data and enhance dataset quality by assimilating the underlying domain knowledge. Regarding content generation, ChatGPT with a zero-shot setting produces informative and readable output for domain-specific questions, albeit with an excessive amount of unrelated information, which can burden readers. In conclusion, ChatGPT demonstrates a promising potential for application in facilitating data labeling, knowledge transfer, and knowledge elicitation tasks. With minimal guidance, ChatGPT can substantially enhance the efficiency of domain experts in accomplishing their objectives. The findings suggest a nuanced integration of artificial intelligence (AI) with human expertise, bridging the gap from mere classification models to sophisticated human-analogous text generation systems. This signals a future in AI-augmented engineering design where the robust capabilities of AI technologies integrate with human creativity and innovation, creating a dynamic interactions to redefine how we tackle design challenges.}
}
@article{BERTOLI2022116702,
title = {Semantic modeling and analysis of complex data-aware processes and their executions},
journal = {Expert Systems with Applications},
volume = {198},
pages = {116702},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116702},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422001804},
author = {Piergiorgio Bertoli and Francesco Corcoglioniti and Chiara {Di Francescomarino} and Mauro Dragoni and Chiara Ghidini and Marco Pistore},
keywords = {Semantic business process, Business process modeling, Semantic technologies, Business process monitoring, Business process analysis},
abstract = {Enriching business processes with the semantic knowledge of an ontology is recognized as a fundamental need in business process management in order to obtain better models and perform more expressive execution analyses. In our work, we model semantically annotated business processes in an OWL knowledge base formalizing the structure of business processes together with the associated business domain. We extend previous work in three substantial manners: first, we show how to exploit the OWL knowledge base to represent the complex data artifacts manipulated by a business process (together with the process itself); second, we show how execution traces can be added into the OWL knowledge base to perform a process execution analysis involving knowledge about both the structure and the domain of a business process; third, we show how to support the semantic modeling and analysis of complex data-aware business processes and their executions. In detail, we illustrate how we extended the MoKi tool for enabling the collaborative modeling of semantically annotated complex data-aware processes, and for supporting their execution analysis via semantic-based reasoning techniques capable of scaling to non-trivial amounts of data while maintaining their advantages. The feasibility and usefulness of the proposed conceptual framework and tool support is shown through a real use case and an experimental evaluation.}
}
@article{RUTA2022100709,
title = {A multiplatform reasoning engine for the Semantic Web of Everything},
journal = {Journal of Web Semantics},
volume = {73},
pages = {100709},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100709},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000087},
author = {Michele Ruta and Floriano Scioscia and Ivano Bilenchi and Filippo Gramegna and Giuseppe Loseto and Saverio Ieva and Agnese Pinto},
keywords = {Automated reasoning, Web Ontology Language, Internet of Everything, Semantic Web of Things, Software architecture},
abstract = {The Internet of Everything and Semantic Web can be joined by giving more intelligence to pervasive systems. To that end, reasoning capabilities should be enabled even for very resource-constrained embedded devices. This paper presents Tiny-ME (the Tiny Matchmaking Engine), a matchmaking and reasoning engine for the Web Ontology Language (OWL), designed and implemented with a compact and portable C core. Main features are high resource efficiency and multiplatform support, spanning containerized microservices, desktops, mobile devices, and embedded boards. The OWLlink interface has been extended to enable non-standard reasoning services for matchmaking in Web, Cloud, and Edge computing. A prototype evaluation is proposed, including a case study on the Pixhawk Unmanned Aerial Vehicle (UAV) autopilot and performance highlights.}
}
@article{FARIAS2018214,
title = {A rule-based methodology to extract building model views},
journal = {Automation in Construction},
volume = {92},
pages = {214-229},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.03.035},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517306738},
author = {Tarcisio Mendes de Farias and Ana Roxin and Christophe Nicolle},
keywords = {ifcOWL, Rule checking, Model view definition (MVD), Reasoning, Semantic web},
abstract = {In this paper, we present a novel approach called IfcView that relies on Semantic Web technologies for creating building views. To do so, we consider an ifcOWL ontology proposed by buildingSMART. The ifcOWL is an Industry Foundation Classes (IFC) based ontology. By combining the ifcOWL ontology with logical rules (expressed in Semantic Web Rule Language, SWRL), we demonstrate through several case studies that our approach can perform a more intuitive and flexible extraction of building views when compared to the Model View Definition (MVD) approach. This is because our rule-based approach dynamically creates sub-graphs (i.e. views) by specifying the IFC elements to extract as Globally Unique Identifiers (GUID), relationships or entities. Another benefit of our approach is the fact that it simplifies the maintenance and definition of building views. Once our rule-based system extracts such a building view (i.e. sub-graph), this view can be exported by using STEP (STandard for the Exchange of Product) or Turtle (a Resource Description Framework (RDF) syntax) formats.}
}
@article{TANG2025104886,
title = {An open-set semi-supervised multi-task learning framework for context classification in biomedical texts},
journal = {Journal of Biomedical Informatics},
volume = {169},
pages = {104886},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104886},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425001157},
author = {Difei Tang and Thomas Yu {Chow Tam} and Haomiao Luo and Cheryl A. Telmer and Natasa Miskov-Zivanov},
keywords = {Biological knowledge representation, Entity span annotation, Natural language processing, Multi-task learning, Semi-supervised learning, Out-of-distribution detection},
abstract = {Objective
In biomedical research, knowledge about the relationships between entities, including genes, proteins, and drugs, is vital for elucidating complex biological processes and intracellular pathway mechanisms. While natural language processing (NLP) methods have shown great success in biomedical relation extraction (RE), extracted relations often lack contextual information such as cell type, cell line, and intracellular location. Previous studies treated this problem as a post hoc context-relation association task, limited by the absence of a golden standard corpus and prone to error propagation. To address these challenges, we propose CELESTA (Context Extraction through LEarning with Semi-supervised multi-Task Architecture), an open-set semi-supervised multi-task learning (OSSL-MTL) framework for biomedical context classification.
Methods
We designed a multi-task learning (MTL) architecture that integrates with the semi-supervised learning (SSL) strategies to leverage unlabeled data containing both in-distribution (ID) and out-of-distribution (OOD) examples. We created a large-scale dataset consisting of five context classification tasks by curating two large Biological Expression Language (BEL) corpora and annotating them with our new entity span annotation method. Additionally, we developed an OOD detector to distinguish between ID and OOD instances within the unlabeled data and applied data augmentation with an external database to enrich our dataset.
Results
Extensive experiments show that our framework significantly improves context classification performance. Our best OSSL-MTL models achieve F1 scores of 77.75% and 82.87% on location and disease classification tasks, and the SSL-MTL models without OOD detection perform best for cell line and cell type classification. The OOD detection experiment confirms that the OOD detector effectively identifies unknown categories while maintaining ID accuracy. Qualitative analysis shows improved extraction of implicit contexts compared to baseline models.
Conclusion
Our analysis demonstrates the effectiveness of the framework CELESTA in improving context classification and extracting contextual information with high accuracy. The newly created dataset and code are publicly available on GitHub (https://github.com/pitt-miskov-zivanov-lab/CELESTA).}
}
@article{HU2025114192,
title = {Dynamic scheduling for dual-resource constrained flexible job-shop via semantic-aware graph modelling and deep reinforcement learning},
journal = {Knowledge-Based Systems},
volume = {328},
pages = {114192},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114192},
url = {https://www.sciencedirect.com/science/article/pii/S095070512501233X},
author = {Yiwen Hu and Jie Chen and Zequn Zhang and Dunbing Tang and Qixiang Cai},
keywords = {Knowledge graph, Graph attention network, Deep reinforcement learning, Dual-resource constraints, Dynamic scheduling},
abstract = {The dual-resource constrained flexible job-shop scheduling problem (DRCFJSP) poses significant challenges in dynamic manufacturing environments. Effective scheduling requires the integration of heterogeneous data from machines, workers, and operations and the ability to adapt to real-time changes. To address these challenges, this study proposes a hybrid scheduling method that combines knowledge graph (KG) techniques with deep reinforcement learning (DRL) to enable adaptive decision-making. A scheduling optimisation model is first formulated to minimise makespan and total production cost. To support real-time environmental perception, a manufacturing process dynamic knowledge graph (MPDKG) is constructed using a multi-level ontology framework. The MPDKG represents static attributes and dynamic states of scheduling entities, allowing continuous data updates during production. Semantic features are extracted using a semantic-aware heterogeneous graph attention network (SHGAT), which incorporates relation-aware and node-aware semantic aggregation to fuse multi-source data and capture global contextual information on operations, machines, and workers. The extracted features characterise the current state of the shop floor and serve as input to a multi-actor proximal policy optimisation (MA-PPO) algorithm. MA-PPO assigns specialised agents to different scheduling entities and learns coordinated decision policies through interaction with the environment. Experimental results from an aerospace structural component workshop show that the proposed method significantly outperforms baseline approaches in responsiveness and scheduling efficiency. These findings demonstrate the method’s effectiveness and applicability in complex, dynamic production scenarios.}
}
@article{SHU201829,
title = {A practical approach to modelling and validating integrity constraints in the Semantic Web},
journal = {Knowledge-Based Systems},
volume = {153},
pages = {29-39},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.04.021},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118301862},
author = {Yanfeng Shu},
keywords = {Semantic Web, Data validation, Integrity constraint, OWL, SWRL},
abstract = {Efforts have been made in the Semantic Web to combine rules with ontologies. One result of these efforts is the development of the Semantic Web Rule Language (SWRL) which is designed to integrate closely with the Web Ontology Language (OWL). Both SWRL and OWL adhere to the open-world semantics of first-order logic, and thus are not suitable for modelling integrity constraints in applications where complete knowledge about some parts of the domain can be assumed. In this paper, we investigate this problem and present a practical approach to modelling and validating constraints in the Semantic Web. Building on existing work, we show that by employing a constraint semantics for both OWL and SWRL, we can model common constraints as OWL axioms or SWRL rules. We also show that by using a query reduction technique, we can validate constraints using existing OWL/SWRL reasoners. Finally, we demonstrate the usefulness of our approach via a real-world case study.}
}
@article{CALVANESE2021100346,
title = {Accessing scientific data through knowledge graphs with Ontop},
journal = {Patterns},
volume = {2},
number = {10},
pages = {100346},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100346},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921002014},
author = {Diego Calvanese and Davide Lanti and Tarcisio {Mendes De Farias} and Alessandro Mosca and Guohui Xiao},
keywords = {ontology-based data access, virtual knowledge graphs, data integration, ontology language, biomedical data},
abstract = {Summary
In this tutorial, we learn how to set up and exploit the virtual knowledge graph (VKG) approach to access data stored in relational legacy systems and to enrich such data with domain knowledge coming from different heterogeneous (biomedical) resources. The VKG approach is based on an ontology that describes a domain of interest in terms of a vocabulary familiar to the user and exposes a high-level conceptual view of the data. Users can access the data by exploiting the conceptual view, and in this way they do not need to be aware of low-level storage details. They can easily integrate ontologies coming from different sources and can obtain richer answers thanks to the interaction between data and domain knowledge.}
}
@article{CASTELLDIAZ2023104297,
title = {Supporting SNOMED CT postcoordination with knowledge graph embeddings},
journal = {Journal of Biomedical Informatics},
volume = {139},
pages = {104297},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104297},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423000187},
author = {Javier Castell-Díaz and Jose Antonio Miñarro-Giménez and Catalina Martínez-Costa},
keywords = {SNOMED CT, Postcoordination, Knowledge graph embeddings, Ontology},
abstract = {SNOMED CT postcoordination is an underused mechanism that can help to implement advanced systems for the automatic extraction and encoding of clinical information from text. It allows defining non-existing SNOMED CT concepts by their relationships with existing ones. Manually building postcoordinated expressions is a difficult task. It requires a deep knowledge of the terminology and the support of specialized tools that barely exist. In order to support the building of postcoordinated expressions, we have implemented KGE4SCT: a method that suggests the corresponding SNOMED CT postcoordinated expression for a given clinical term. We leverage on the SNOMED CT ontology and its graph-like structure and use knowledge graph embeddings (KGEs). The objective of such embeddings is to represent in a vector space knowledge graph components (e.g. entities and relations) in a way that captures the structure of the graph. Then, we use vector similarity and analogies for obtaining the postcoordinated expression of a given clinical term. We obtained a semantic type accuracy of 98%, relationship accuracy of 90%, and analogy accuracy of 60%, with an overall completeness of postcoordination of 52% for the Spanish SNOMED CT version. We have also applied it to the English SNOMED CT version and outperformed state of the art methods in both, corpus generation for language model training for this task (improvement of 6% for analogy accuracy), and automatic postcoordination of SNOMED CT expressions, with an increase of 17% for partial conversion rate.}
}
@article{MORI2025,
title = {Knowledge Representation of Sensor Dataset with IoT Collaboration of Semantic Web and IoT: Storage of Temperature and Humidity Details},
journal = {Recent Patents on Engineering},
volume = {19},
number = {2},
year = {2025},
issn = {2212-4047},
doi = {https://doi.org/10.2174/0118722121242190230921070510},
url = {https://www.sciencedirect.com/science/article/pii/S2212404725000211},
author = {Gajendrasinh N. Mori and Priya R. Swaminarayan and Ronak Panchal},
keywords = {IoT, ontology, RDF triple, SPARQL, NodeMCU, DHT11 sensor, ThingSpeak, protege, Apache Jena Fuseki},
abstract = {Introduction
Today, Internet of Things applications offer new opportunities in all domains like home automation, transportation, medical diagnosis, agriculture, etc. According to McKinsey Global Institute research, IoT will cover a market share of over $11.1 trillion by 2025. Moreover, Semantic web technology approaches are used in IoT applications so that machines can understand and interpret sensor-collected data.
Methods
Our proposed system uses a DHT11 sensor, NodeMCU for data collection, and ThingSpeak cloud for data analysis and visualization. It utilizes the Protégé tool to develop semantic data modelling using Ontology/RDF graphs and retrieval for future SPARQL queries.
Results
This patent approach ensures the optimal presentation of sensor data and the meaning of data and controls the information for the Home Automation System. By semantic layer, we improved integration, interoperability, discovery, and data analysis.
Conclusion
As far as applications are concerned, semantic technologies and IoT sensor data can be transformed into a more valuable and practical format, enabling intelligent applications and systems development across multiple fields, such as smart cities, industrial automation, healthcare, and environmental monitoring.}
}
@article{CHRUBASIK2025101457,
title = {Linking aircraft structural testing data through semantic technologies},
journal = {Measurement: Sensors},
volume = {38},
pages = {101457},
year = {2025},
note = {Proceedings of the XXIV IMEKO World Congress},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2024.101457},
url = {https://www.sciencedirect.com/science/article/pii/S2665917424004331},
author = {Michael Chrubasik and Hannah Strassburg and Emily Rolfe and Ramani Sankar Rajendran and Jean-Laurent Hippolyte},
keywords = {Aircraft structural testing, Digital image correlation, Semantic technology, Ontology, Knowledge graph, Data integration, Interoperability},
abstract = {Difficulties in data integration and interoperability in aircraft structural testing arise from disparate systems and data silos. A modular Point-Based-Measurement (PBM) ontology was created to address these inefficiencies by enabling systematic domain knowledge linkage across systems. This study introduces a new Digital Image Correlation (DIC) module for PBM, developed in collaboration with Airbus and Zeiss. Using a DIC dataset, the ontology was validated through SPARQL queries, highlighting improved data accessibility and data linking. The validation process not only confirmed the ontology's effectiveness but also showcased its potential to improve operator efficiency. Preliminary results demonstrate enhanced querying capabilities and streamlined testing workflows. This underscores the benefits of semantic unification in structural testing and importance of increased industry collaboration for broader adoption. The work also demonstrates that semantic technologies, such as ontologies and knowledge graphs, provide key solutions to the challenges of FAIR data integration, thereby enabling knowledge-driven innovations like digital twins.}
}
@incollection{VAKAJ20221321,
title = {A semantic based decision support framework to enable model and data integration},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {1321-1326},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50220-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596502207},
author = {Edlira Vakaj and Linsey Koo and Franjo Cecelja},
keywords = {biorefining, semantic repository, model integration, OWL-S},
abstract = {Model reusability and integration with datasets are major contributors towards their interoperability, the concepts that follows process established by computer aided process engineering (CAPE) community (Belaud & Pons 2002). This paper proposes a semantic approach which enables model/data registration, their discovery and concomitantly model their integration. The functionality of the process is fully controlled by a biorefining domain ontology implemented using Ontology Web Language (OWL) and tested using biorefining related scenarios.}
}
@article{GENG2025126175,
title = {Prompting disentangled embeddings for knowledge graph completion with pre-trained language model},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126175},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126175},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424030422},
author = {Yuxia Geng and Jiaoyan Chen and Yuhang Zeng and Zhuo Chen and Wen Zhang and Jeff Z. Pan and Yuxiang Wang and Xiaoliang Xu},
keywords = {Knowledge graph completion, Pre-trained language model, Prompt tuning, Disentangled embedding},
abstract = {Both graph structures and textual information play a critical role in Knowledge Graph Completion (KGC). With the success of Pre-trained Language Models (PLMs) such as BERT, they have been applied for text encoding for KGC. However, the current methods mostly prefer to fine-tune PLMs, leading to huge training costs and limited scalability to larger PLMs. In contrast, we propose to utilize prompts and perform KGC on a frozen PLM with only the prompts trained. Accordingly, we propose a new KGC method named PDKGC with two prompts — a hard task prompt which is to adapt the KGC task to the PLM pre-training task of token prediction, and a disentangled structure prompt which learns disentangled graph representation so as to enable the PLM to combine more relevant structure knowledge with the text information. With the two prompts, PDKGC builds a textual predictor and a structural predictor, respectively, and their combination leads to more comprehensive entity prediction. Solid evaluation on three widely used KGC datasets has shown that PDKGC often outperforms the baselines including the state-of-the-art, and its components are all effective. Our codes and data are available at https://github.com/genggengcss/PDKGC.}
}
@article{YUKSEL2023197,
title = {A reference implementation for knowledge assisted robot development for planetary and orbital robotics},
journal = {Acta Astronautica},
volume = {210},
pages = {197-211},
year = {2023},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2023.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0094576523002485},
author = {Mehmed Yüksel and Thomas M. Roehr and Marko Jankovic and Wiebke Brinkmann and Frank Kirchner},
keywords = {Robotics, System design, Domain ontology, Space interface benchmark, In-orbit servicing, Planetary robotics},
abstract = {The use of modular, yet reusable and interchangeable components offers flexibility for system designers and thus leads to a higher degree of reconfigurability with cost effective, adaptable and scalable solutions. The development of a robotic system remains, however, a complex task with increasing demands depending on the targeted capabilities, functionalities and operational constraints of the robot. This requires expert knowledge in mechanical, electrical and software engineering, as well as tools that can collect, process and use relevant data. As a result, the amount of available and managed information is increasing, while its relevance, usability, and knowledge that can be extracted from it are consequently decreasing: a situation described as information overload paradox. The design of a modular system can be based on the use of standardized modules with universal interconnects, but it should be aligned with a strong formalism, e.g. the semantic representation of components, to allow for a better management of the overall complexity. An ontology-based knowledge representation, for instance, which provides domain and application specific knowledge for the design of robotic systems, can be used as a method of storing and sharing data in a uniform, machine-readable and standardized way. In this paper, we introduce the Knowledge-based Open Robot voCabulary as Utility Toolkit (korcut) as a core component to support the ontology-driven development of robotic systems as part of a reference implementation of the Q-Rock development cycle. Q-Rock gathers methods to assist users in designing robot configuration by: (a) automatically exploring robot capabilities based on robot hardware, (b) proposing robot designs to meet the user’s needs, (c) refining the proposed robot designs. We use korcut to improve the robot design process in orbital and planetary robotics from the requirements definition to the development phases of complex modular robotic structures (e.g. composition of an unmanned ground vehicle with robotic arms). This is implemented by embedding semantic component descriptions in a state-of-the-art open-source 3D modeling software. Furthermore, the korcut ontology family includes various sub-ontologies to address specific space-related tasks, such as the modeling of a Standard Interconnect (SI) for On Orbit Services (OOS) and Orbital Factory. The ontology design follows criteria that have been derived from a survey conducted as part of this work. This paper also covers an evaluation of its current applications from a methodological, knowledge representation, and software tool perspectives. In the application part, we introduce the use of the ontology to model a multi-functional SI that enables mechanical connections between various (modular) robotic components, as well as transfer of power and data. Finally, we provide a critical analysis of our work and outline its future work.}
}
@article{MEDLIN2025290,
title = {Socio-mental adapting in business relationships: A dialogical understanding},
journal = {Industrial Marketing Management},
volume = {125},
pages = {290-302},
year = {2025},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2025.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0019850125000112},
author = {Christopher J. Medlin and Jan-Åke Törnroos},
keywords = {Constructivism, Dialogical/process ontology, Elongated relational presents, In-between sensemaking, Relationship atmosphere, Shared sensemaking},
abstract = {In industrial marketing research the approaches to collective sensemaking in business relationships and to understanding network logics have strongly relied upon an individualistic/cognitive viewpoint. This conceptual paper introduces socio-mental adapting as a form of shared sensemaking inside business relationships considered with a dialogical ontological perspective. Our point of departure is the distinction between adaptation and adapting and to re-consider how sensemaking, as adopted within the IMP approach, occurs during elongated relational presents unfolding in human-relational times. Our contribution thus brings individuals as joint constructions (and constructors) to the heart of interactive business relationship change. These elaborations present a dialogical ontology for research of socio-mental adapting, through meaning changes, by human business actors. Socio-mental adapting in B2B relations is strongly future oriented, and always uncertain/incomplete, as new futures are already unfolding. In a proposed research frame, we present related concepts for researchers and managers to handle humanly shared future projections inside business relationships. The dialogical ontology opens adaptive time-spaces for managers, e. g. for developing more sustainable partnerships.}
}
@article{CHARBONNIER20242439,
title = {Towards a Semantic Approach to Detection of Quality Issues in Manufacturing 4.0},
journal = {Procedia Computer Science},
volume = {246},
pages = {2439-2448},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.479},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924025237},
author = {Léa Charbonnier and Franco Giustozzi and Julien Saunier and Cecilia Zanni-Merk},
keywords = {Quality Assurance, Industry 4.0 (Quality 4.0), Ontology, Stream Reasoning, Quality issues detection},
abstract = {Quality assurance in manufacturing companies is an essential process for ensuring that products meet established standards. It contributes to customer satisfaction, as well as the reduction of the costs associated with defects. With Quality 4.0, an extension of Industry 4.0 to quality assurance, new possibilities in terms of product quality management are emerging. Thanks to expert knowledge, data collected by machine sensors can be used to anticipate quality issues or manufacturing errors. To semantically detect those situations, an ontology representing manufacturing knowledge linked to quality detection situations is needed. Moreover, as heterogeneous data streams have to be integrated, a combination of stream processing and of-line reasoning can be used. This combination allows a continuous process of data and the use of expert knowledge to detect anomalies. This paper presents an approach for detecting manufacturing quality losses. Therefore, an ontology-based context for manufacturing is introduced to detect quality issues situations. Then, an extension of an existing model using stream reasoning to process heterogeneous data from sensors and predictions is presented to detect the situations continuously.}
}
@article{LI2024205469,
title = {Artificial general intelligence for the upstream geoenergy industry: A review},
journal = {Gas Science and Engineering},
volume = {131},
pages = {205469},
year = {2024},
issn = {2949-9089},
doi = {https://doi.org/10.1016/j.jgsce.2024.205469},
url = {https://www.sciencedirect.com/science/article/pii/S2949908924002656},
author = {Jimmy Xuekai Li and Tiancheng Zhang and Yiran Zhu and Zhongwei Chen},
keywords = {Artificial general intelligence (AGI), ChatGPT, Large language models (LLMs), Generative AI, Multimodal, Upstream geoenergy industry},
abstract = {Artificial General Intelligence (AGI) is set to profoundly impact the traditional upstream geoenergy industry (i.e., geothermal energy, oil and gas industry) by introducing unprecedented efficiencies and innovations. This paper explores AGI's foundational principles and its transformative applications, particularly focusing on the advancements brought about by large language models (LLMs) and extensive computer vision systems in the upstream sectors of the industry. The integration of Artificial Intelligence (AI) has already begun reshaping the upstream geoenergy landscape, offering enhancements in production optimization, downtime reduction, safety improvements, and advancements in exploration and drilling techniques. These technologies streamline logistics, minimize maintenance costs, automate monotonous tasks, refine decision-making processes, foster team collaboration, and amplify profitability through error reduction and actionable insights extraction. Despite these advancements, the deployment of AI technologies faces challenges, including the necessity for skilled professionals for implementation and the limitations of model training on constrained datasets, which affects the models' adaptability across different contexts. The advent of generative AI, exemplified by innovations like ChatGPT and the Segment Anything Model (SAM), heralds a new era of high-density innovation. These developments highlight a shift towards natural language interfaces and domain-knowledge-driven AI, promising more accessible and tailored solutions for the upstream geoenergy industry. This review articulates the vast potential AGI holds for tackling complex operational challenges within the upstream geoenergy industry, requiring near-human levels of intelligence. We discussed the promising applications, the hurdles of large-scale AGI model deployment, and the necessity for domain-specific knowledge in maximizing the benefits of these technologies.}
}
@article{PATEL2025,
title = {Transforming electrophysiology workflows with natural language processing and agentic artificial intelligence},
journal = {Heart Rhythm O2},
year = {2025},
issn = {2666-5018},
doi = {https://doi.org/10.1016/j.hroo.2025.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S2666501825002600},
author = {Akshar Patel and Stanley Joseph and Caryl Bailey and Ashish Sakharpe and Mallikarjuna Devarapalli},
keywords = {Artificial intelligence, Electrophysiology, Natural language processing, Sentiment analysis, Hugging Face, Named entity recognition}
}
@incollection{AYAMS2025607,
title = {Chapter 29 - Domain specific semantic categories in biomedical applications},
editor = {Sujata Dash and Subhendu Kumar Pani and Wellington Pinheiro Dos Santos and Jake Y. Chen},
booktitle = {Mining Biomedical Text, Images and Visual Features for Information Retrieval},
publisher = {Academic Press},
pages = {607-634},
year = {2025},
isbn = {978-0-443-15452-2},
doi = {https://doi.org/10.1016/B978-0-443-15452-2.00029-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443154522000297},
author = {Jude Ndanusa Ayams and Felix O. Okunlola and Konjerimam Ishaku Chimbekujwo and Damilare Isaiah Taiwo and Oluwafemi Adebayo Oyewole and Charles Oluwaseun Adetunji and Babatunde Oluwafemi Adetuyi and Chidume Charles Chinazaekpele and Tomi Gloria Adetuyi and Abimbola Rafiat Okunlola and Peace Abiodun Olajide and Victoria Olufunmi Adeyemo-Eleyode and Olusola Olawale Olaleye},
keywords = {Biomedical, Diagnosis, Disease, Healthcare, Natural language processing, Semantic, Treatment},
abstract = {Domain specific semantic categories, especially in the form of ontologies, are being used at an exponential rate in the rapidly developing field of biological sciences to structure and organize massive amounts of data. These semantic categories are essential for distinguishing specific vocabularies linked to biological processes, diseases, genes, proteins, and anatomical structures. This review aims to clarify the various types of ontologies with structured representations of domain specific knowledge, and the critical importance of domain specific semantic categories in biomedical applications while addressing the difficulties and factors that must be taken into account during implementation. It explores the nuances of incorporating semantic categories into healthcare systems, including acceptance, interoperability, standardization, ontology suggestion, multilingualism, and resource allocation. It also includes an extensive summary of research, techniques, and instruments applied in this field, highlighting the dynamic environment of semantic classifications in the field of biomedicine. This chapter delves into the profound impact of domain specific semantic categories in biomedical research and applications. It explores the development and utilization of a comprehensive taxonomy tailored to categorize diverse biomedical data, encompassing diseases, treatments, genes, biological processes, medical imaging, and patient demographics. The research identifies key challenges in this area, particularly the issues of ambiguity and variability in data interpretation. It proposes innovative solutions such as the use of advanced natural language processing (NLP) algorithms and the integration of ontologies to achieve precise data disambiguation and manage the dynamic nature of biomedical information. Further, the study examines the integration of these semantic categories within existing biomedical systems, highlighting the importance of data standardization, interoperability, and user-centric approaches for effective adoption. It also investigates the role of emerging technologies, including machine learning, explainable artificial intelligence, graph databases, and blockchain, in enhancing the utility and precision of these semantic categories in biomedical contexts. The findings demonstrate the transformative potential of domain specific semantic categories in organizing and interpreting complex biomedical data, thus facilitating advancements in precision medicine, automated literature review, clinical decision support, and population health management. By offering a dynamic and adaptive framework, these categories enable a deeper understanding of biomedical complexities, fostering innovative solutions and interdisciplinary collaboration. In conclusion, this study underscores the significance of domain specific semantic categories as vital tools in biomedical research, driving the field toward groundbreaking discoveries and the realization of personalized healthcare solutions.}
}
@article{WU2021103563,
title = {Developing a hybrid approach to extract constraints related information for constraint management},
journal = {Automation in Construction},
volume = {124},
pages = {103563},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103563},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521000145},
author = {Chengke Wu and Peng Wu and Jun Wang and Rui Jiang and Mengcheng Chen and Xiangyu Wang},
keywords = {Constraint management, Information extraction, Ontology, Deep learning, Natural language processing},
abstract = {Construction projects face various constraints (e.g., materials and equipment). Constraint management approaches such as advanced working packaging (AWP) can remove constraints and ensure smooth work. However, due to inefficient information extraction, the prerequisite of AWP, i.e., identifying and modelling constraints, are performed manually. Efforts that integrate constraint information into project knowledge bases are also limited. This paper proposes a hybrid approach to automatically extract and integrate constraint information from texts. The approach combines a deep learning model with pre-defined rules. The model extracts constraint entities whereas rules created based on domain knowledge are used to establish relations between these entities. Extracted information is encoded into the original ontologies. The approach can extract both entities and relations with over 90% accuracy. The original ontologies can be successfully enriched and support semantic queries. The approach improves AWP by partially automating constraint identification and modelling as well as ontology development for information integration.}
}
@article{ABBASIDASHTAKI2025100366,
title = {Retrieving and discovering new knowledge from documents' abstracts in scientific databases: Proposing a query-based abstractive summarization model},
journal = {International Journal of Information Management Data Insights},
volume = {5},
number = {2},
pages = {100366},
year = {2025},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2025.100366},
url = {https://www.sciencedirect.com/science/article/pii/S2667096825000485},
author = {Neda {Abbasi Dashtaki} and Mehrdad CheshmehSohrabi and Mitra Pashootanizadeh and Hamidreza {Baradaran Kashani}},
keywords = {Information Retrieval (IR), Knowledge Retrieval (KR), Knowledge Extraction (KE), Knowledge Discovery (KD), Scientific databases, Document' Abstract, Systematic review, Meta-analysis, PRISMA, Query-Based Abstractive Summarization (QBAS), Retrieval-Augmented Generation (RAG)},
abstract = {Current search engines for Knowledge Retrieval (KR) and Knowledge Discovery (KD) do not effectively utilize scientifically validated documents, especially those indexed in scientific databases. Scientific databases e.g., Scopus primarily consist of document-based content and provide documents' abstract. Their Information Retrieval (IR) system only perform document searches and lack the capability to extract and discover new knowledge from documents' abstract in these databases and responding to users’ queries. The aim is to introduce a model that can efficiently perform these tasks. The statistical population for this study encompasses all scientific databases, with a particular emphasis on Scopus. To clarify the process of KR and KD as we define it, we employed a systematic review and meta-analysis framework using 33 queries. We conducted the identification, screening, eligibility, and inclusion steps following the PRISMA protocol. Next, we performed extraction, labeling, grouping, analysis, and inference. The outcome of these processes provided us with novel insights, which contribute to our exploratory knowledge. To automate these processes, we have proposed a conceptual model from query-based indirect abstractive summarization approach. The outcomes of this research offer fresh insights to database designers, administrators, and researchers, enabling the development of tools for KR and KD within these invaluable knowledge repositories. The integration of such tools into scientific databases will enhance user access to scientific knowledge to meet their informational and research needs.}
}
@article{OUESLATI2023100516,
title = {A systematic review on moving objects’ trajectory data and trajectory data warehouse modeling},
journal = {Computer Science Review},
volume = {47},
pages = {100516},
year = {2023},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2022.100516},
url = {https://www.sciencedirect.com/science/article/pii/S1574013722000508},
author = {Wided Oueslati and Sonia Tahri and Hela Limam and Jalel Akaichi},
keywords = {Moving object, Moving point, Moving region, Trajectory data, Trajectory data warehouse, Conceptual modeling, Ontological modeling},
abstract = {The development of mobile technologies has paved the way for new and various applications taking advantage of trajectory data resulting from moving objects activities in their associated ecosystems. Such data can be mainly handled either by real time applications or by oriented decision-making tools going from trajectory data warehouse technology to data mining classical advanced instruments. Indeed, applications dealing with moving objects encompass hidden significant knowledge that can be made visible through analytical and mining tools. This precious knowledge could not come properly in hands only if, the trajectory data problem modeling is global, precise, and concise. The aim of this paper is to investigate the appropriate literature on moving objects, trajectory data, and trajectory data warehouse modeling going from classical to ontological existing patterns. A comparison will be made between them, through which strong and limited contributions will be shown. This work aims to be valuable for researchers aiming to select and use modeling approaches in mobile objects ecosystems.}
}
@article{GAO2024105634,
title = {Exploring bridge maintenance knowledge graph by leveraging GrapshSAGE and text encoding},
journal = {Automation in Construction},
volume = {166},
pages = {105634},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105634},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524003704},
author = {Yan Gao and Guanyu Xiong and Haijiang Li and Jarrod Richards},
keywords = {Bridge maintenance knowledge graph, Text encoding, Graph neural networks, Node classification, Link prediction},
abstract = {Knowledge graphs (KGs) are crucial in documenting bridge maintenance expertise. However, existing KG schemas lack integration of bridge design and practical inspection insights. Meanwhile, traditional methods for node feature initialization, relying on meticulous manual encoding or word embeddings, are inadequate for real-world maintenance textual data. To address these challenges, this paper introduces a bridge maintenance-oriented KG (BMKG) schema and approaches for graph data mining, including node-layer classification and link prediction. These methods leverage large language model (LLM)-based text encoding combined with GraphSAGE, demonstrating excellent performance in semantic enrichment and KG completion on deficient BMKGs. Additionally, ablation studies reveal the superiority of the pre-trained BERT text encoder and the L2 distance pairwise scoring calculator. Furthermore, a practical implementation framework integrating these approaches is developed for routine bridge maintenance, which can facilitate various practical applications, such as maintenance planning, and has the potential to enhance the efficiency of engineers' documentation work.}
}
@incollection{ZHANG2025,
title = {Language Learner Identity},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00451-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041004518},
author = {Yue Zhang},
keywords = {Identity, Language learning, Language learner, Learner identity, Investment, Second language (L2), L2 investment, Power, Capital, Ideology},
abstract = {This article introduces language learning and learner identity from a sociolinguistic perspective. Discussing the poststructural conceptualization of identity, it first explores how neoliberalism and native-speakerism ideologies and language learners' social, cultural, and economic capital can influence learners' identity construction in recent years. Drawing on Norton's conceptualization of identity and poststructuralist theories, a model of L2 investment at the intersection of ideology, capital, and identity is then introduced. The author continues to discuss the aim, sociological foundation, and components of this model and how it can be used to understand language learner identity. The article ends with some concluding remarks, pedagogical implications, and directions for future studies.}
}
@article{CONDESALAZAR2024111017,
title = {RDF graph pair profile dataset for the data linking community},
journal = {Data in Brief},
volume = {57},
pages = {111017},
year = {2024},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2024.111017},
url = {https://www.sciencedirect.com/science/article/pii/S235234092400979X},
author = {Raphaël {Conde Salazar} and Clément Jonquet and Danai Symeonidou},
keywords = {RDF graph statistics, RDF graph profiles, Dataset pairing, Ontology matching},
abstract = {As the number of RDF datasets published on the Web grows, it becomes increasingly important to link similar entities across these datasets. We present the “RDF graph pair profiles dataset”, designed to help the data linking community develop tools and carry out evaluation work. This dataset includes profiles of 30 RDF graph pairs, classified according to ontology matching (OM), instance matching (IM) or both (OM + IM). Each profile includes statistical measures and lists of qualitative and quantitative information and descriptive models generated using automated tools. These profiles help in understanding dataset characteristics, facilitating the development, selection and validation of data linking tools. They are particularly useful in machine learning applications where the profiles can serve as input parameters. The dataset includes both the quasi-original RDF graphs and their profiles represented in a specific described format offering a comprehensive resource for researchers and practitioners. The methodology applied to obtain the profiles is also briefly presented. Available publicly (DOI: 10.57745/K7JDGV) this dataset will facilitate data linking, hence contribute to the integration and enhancement of RDF data published in the Web of data.}
}
@article{THORNTON2025,
title = {Using Natural Language Processing to Identify Symptomatic Adverse Events in Pediatric Oncology: Tutorial for Clinician Researchers},
journal = {JMIR Bioinformatics and Biotechnology},
volume = {6},
year = {2025},
issn = {2563-3570},
doi = {https://doi.org/10.2196/70751},
url = {https://www.sciencedirect.com/science/article/pii/S2563357025000078},
author = {Clifton P Thornton and Maryam Daniali and Lei Wang and Spandana Makeneni and Allison {Barz Leahy}},
keywords = {neoplasms, artificial intelligence, natural language processing, interdisciplinary research, oncology},
abstract = {Artificial intelligence (AI) is poised to become an integral component in health care research and delivery, promising to address complex challenges with unprecedented efficiency and precision. However, many clinicians lack training and experience with AI, and for those who wish to incorporate AI into research and practice, the path forward remains unclear. Technical barriers, institutional constraints, and lack of familiarity with computer and data science frequently stall progress. In this tutorial, we present a transparent account of our experiences as a newly established interdisciplinary team of clinical oncology researchers and data scientists working to develop a natural language processing model to identify symptomatic adverse events during pediatric cancer therapy. We outline the key steps for clinicians to consider as they explore the utility of AI in their inquiry and practice, including building a digital laboratory, curating a large clinical dataset, and developing early-stage AI models. We emphasize the invaluable role of institutional support, including financial and logistical resources, and dedicated and innovative computer and data scientists as equal partners in the research team. Our account highlights both facilitators and barriers encountered spanning financial support, learning curves inherent with interdisciplinary collaboration, and constraints of time and personnel. Through this narrative tutorial, we intend to demystify the process of AI research and equip clinicians with actionable steps to initiate new ventures in oncology research. As AI continues to reshape the research and practice landscapes, sharing insights from past successes and challenges will be essential to informing a clear path forward.}
}
@article{GLAUER2024896,
title = {Chebifier: automating semantic classification in ChEBI to accelerate data-driven discovery},
journal = {Digital Discovery},
volume = {3},
number = {5},
pages = {896-907},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00238a},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000664},
author = {Martin Glauer and Fabian Neuhaus and Simon Flügel and Marie Wosny and Till Mossakowski and Adel Memariani and Johannes Schwerdt and Janna Hastings},
abstract = {Connecting chemical structural representations with meaningful categories and semantic annotations representing existing knowledge enables data-driven digital discovery from chemistry data. Ontologies are semantic annotation resources that provide definitions and a classification hierarchy for a domain. They are widely used throughout the life sciences. ChEBI is a large-scale ontology for the domain of biologically interesting chemistry that connects representations of chemical structures with meaningful chemical and biological categories. Classifying novel molecular structures into ontologies such as ChEBI has been a longstanding objective for data scientific methods, but the approaches that have been developed to date are limited in several ways: they are not able to expand as the ontology expands without manual intervention, and they are not able to learn from continuously expanding data. We have developed an approach for automated classification of chemicals in the ChEBI ontology based on a neuro-symbolic AI technique that harnesses the ontology itself to create the learning system. We provide this system as a publicly available tool, Chebifier, and as an API, ChEB-AI. We here evaluate our approach and show how it constitutes an advance towards a continuously learning semantic system for chemical knowledge discovery.}
}
@article{CAI2023120459,
title = {A risk identification model for ICT supply chain based on network embedding and text encoding},
journal = {Expert Systems with Applications},
volume = {228},
pages = {120459},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120459},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423009612},
author = {Chengcheng Cai and Limin Pan and Xinshuai Li and Senlin Luo and Zhouting Wu},
keywords = {ICT supply chain, Risk identification, Network embedding, Ontology},
abstract = {Public bidding for the information and communication technology (ICT) products offers an external regulatory perspective to identify endogenous risks of ICT supply chain. Owing to the multi-level outsourcing, the modeling methods of the first-order topological similarity are prone to misjudgment. Moreover, only using basic structural attributes (such as enterprise type, registered capital, etc.) is difficult to accurately quantify the enterprise' supply capacity, resulting in the models cannot identify the risky nodes with insufficient supply capacity. In this work, a risk identification Model for ICT supply chain based on Network Embedding and Text Encoding (NETEM) is proposed. In addition to the basic business features of enterprises, the DeepWalk is utilized to obtain the high-order topological representation of the ICT supply chain. Meanwhile, the TextCNN combined with keyword sequence extraction is utilized to supplement text semantic information, and excavate the association between business scope and supply capacity of enterprises. Finally, the basic business features, high-level topological features and text semantic features are spliced to obtain the final enterprise node representation vector, then the node risk classification model is trained. In addition, an ontology is developed for ICT supply chain integration and risk knowledge reuse. The experimental results show that NETEM improves the F1 value of the optimal baseline algorithm by 7.3% on the ICT risk identification data set, and the case study in financial industry demonstrates that the model can be practically applied to identify the risk of supply interruption.}
}
@article{LIU2025106445,
title = {Digital twin modelling approaches and applications in urban infrastructure operations and maintenance},
journal = {Automation in Construction},
volume = {179},
pages = {106445},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106445},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525004856},
author = {Zhansheng Liu and Yanchi Mo and Benwei Hou and Mingming Li and Weiyi Li and Chengshun Xu},
keywords = {Urban infrastructure, Digital twin, Multi-scale model, Operation and maintenance},
abstract = {Large scale and complex cascading effects make the operations and maintenance (O&M) of urban infrastructure extremely challenging. In order to identify general approaches to digital twin (DT) modelling and its applications, this review investigates 226 relevant publications as the main database. Using a systematic literature review (SLR) methodology, the database was qualitatively assessed through iterative refinements of keywords, titles, abstracts, and full texts. General procedures for DT modelling in buildings and infrastructure at different scales were summarized through quantitative analysis. To examine cascading effects in urban infrastructure O&M, DT applications were categorized into daily and abnormal scenarios and analyzed separately. Special cases, such as disasters and external disturbances, were also explored using automatic modelling and ontology fusion techniques. Finally, future research directions for DT applications in urban infrastructure are summarized to guide further development in this field.}
}
@article{KIRNER2024105699,
title = {Enhancing robotic steel prefabrication with semantic digital twins driven by established industry standards},
journal = {Automation in Construction},
volume = {167},
pages = {105699},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105699},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524004357},
author = {Lukas Kirner and Victoria Jung and Jyrki Oraskari and Sigrid Brell-Cokcan},
keywords = {Semantic Digital Twin, Robotic steel prefabrication, DSTV-NC, Steel construction, Linked data, Semantic web},
abstract = {To increase automation in steel construction, new approaches are needed to strengthen the robustness of robotic steel prefabrication processes against manufacturing tolerances. While Digital Twins (DTs) can enable the detection of deviations and the adaptation of machine control accordingly, an adaptive information model interface that can integrate cross-process and cross-machine considerations is missing. Therefore, this paper focuses on the development of an ontology based on an existing steel prefabrication standard to link process data, tolerances and deviations, thereby enabling the realisation of a semantic DT. The approach’s feasibility is proven by a case study that demonstrates process and tolerance modelling, as well as linking robot control, feedback data, and measured tolerances for robotic plasma cutting. The results show that by means of the resulting data model, a semantic DT can be realised, which allows making deviations and process knowledge available for downstream manufacturing, assembly and construction processes.}
}
@article{TAO2025105920,
title = {A geological knowledge-constrained entity and relation extraction method for text: A case study of granitic pegmatite-type lithium deposits},
journal = {Computers & Geosciences},
volume = {200},
pages = {105920},
year = {2025},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2025.105920},
url = {https://www.sciencedirect.com/science/article/pii/S0098300425000706},
author = {Jintao Tao and Nannan Zhang and Jinyu Chang and Li Chen and Hao Zhang and Shibin Liao and Siyuan Li and Jianpeng Jing},
keywords = {Geological text, Geological schema, Geological information extraction, Granitic pegmatite-type lithium deposit, Geological knowledge graph},
abstract = {Geological text data contain rich and valuable information about geological environments and mineral deposits. The automated extraction of geological information from these unstructured texts is crucial for constructing geological knowledge graphs and facilitating knowledge discovery. Numerous studies have introduced methods for geological entity and relation extraction from different perspectives. Although many of these studies effectively utilize geological ontologies or schemas for data labeling, fewer have explicitly examined how these frameworks can constrain and improve the information extraction process. In this study, we propose a Geological Knowledge-constrained Entity and Relation Extraction (GKERE) method that incorporates a geological schema to enhance the extraction process. The GKERE method uses the Robustly Optimize Bidirectional Encoder Representation from Transformers Pre-training Approach to generate character embeddings from geological sentences. It begins with a span-based named entity recognition model to identify entities, then generates entity pairs and predicts their relationships using the geological schema. The schema helps filter out redundant entity pairs and provides information about the types of head/tail entities and their possible relationships, guiding the relation extraction step. To validate the method, we conducted a case study on granitic pegmatite-type lithium deposits. A geological schema was designed, comprising 22 entity types, 16 relationships, and 184 knowledge rules. An entity-relation extraction dataset was then constructed using 68 geological journal articles and four mineral exploration reports. The proposed GKERE method achieves an impressive F1-score of 75.82 % on this dataset, outperforming several baseline models. Results show that the GKERE method significantly enhances geological entity and relation extraction. The introduction of the geological schema not only accelerates computation but also improves model accuracy, making this approach effective for extracting geological information from large-scale textual data and promoting geological knowledge discovery.}
}
@article{SCIOSCIA2022100694,
title = {A multiplatform energy-aware OWL reasoner benchmarking framework},
journal = {Journal of Web Semantics},
volume = {72},
pages = {100694},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100694},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000639},
author = {Floriano Scioscia and Ivano Bilenchi and Michele Ruta and Filippo Gramegna and Davide Loconte},
keywords = {Web Ontology Language, Automated reasoning, Benchmarking, Performance evaluation, Energy-aware computing},
abstract = {Performance evaluation is increasingly relevant for Web Ontology Language (OWL) reasoners, due to the expanding availability of knowledge corpuses on the Web, the growing variety of applications, and the rise to prominence of mobile and pervasive computing. Motivated mainly by the difficulty of comparing reasoning engines in the Semantic Web of Things (SWoT), this paper introduces evOWLuator, a novel approach and a multiplatform framework devised to be both flexible and expandable. It features integration of traditional and mobile/embedded engines as well as ontology dataset management, reasoning test execution, and report generation. A case study consisting of an experimental setting for time, memory peak and energy footprint evaluation with eight reasoners and four different platforms allows showcasing usage and validating features and usability of the tool.}
}
@article{CHATTERJEE2025201,
title = {Sovereignty in Automated Stroke Prediction and Recommendation System with Explanations and Semantic Reasoning},
journal = {Procedia Computer Science},
volume = {254},
pages = {201-210},
year = {2025},
note = {International Conference on Digital Sovereignty (ICDS)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.02.079},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925004296},
author = {Ayan Chatterjee},
keywords = {Predictive Analysis, Personalized Recommendation System, Stroke Monitoring, Streaming Data, Semantic Knowledge, Digital Sovereignty},
abstract = {Personalized approaches are required for stroke management due to the variability in symptoms, triggers, and patient characteristics. An innovative stroke recommendation system that integrates automatic predictive analysis with semantic knowledge to provide personalized recommendations for stroke management is proposed by this paper. Stroke exacerbation are predicted and the recommendations are enhanced by the system, which leverages automatic Tree-based Pipeline Optimization Tool (TPOT) and semantic knowledge represented in an OWL Ontology (StrokeOnto). Digital sovereignty is addressed by ensuring the secure and autonomous control over patient data, supporting data sovereignty and compliance with jurisdictional data privacy laws. Furthermore, classifications are explained with Local Interpretable Model-Agnostic Explanations (LIME) to identify feature importance. Tailored interventions based on individual patient profiles are provided by this conceptual model, aiming to improve stroke management. The proposed model has been verified using public stroke dataset, and the same dataset has been utilized to support ontology development and verification. In TPOT, the best Variance Threshold + DecisionTree Classifier pipeline has outperformed other supervised machine learning models with an accuracy of 95.2%, for the used datasets. The Variance Threshold method reduces feature dimensionality with variance below a specified threshold of 0.1 to enhance predictive accuracy. To implement and evaluate the proposed model in clinical settings, further development and validation with more diverse and robust datasets are required.}
}
@article{PATZAK2024103733,
title = {Towards digital twins: Design of an entity data model in the MuPIF simulation platform},
journal = {Advances in Engineering Software},
volume = {197},
pages = {103733},
year = {2024},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2024.103733},
url = {https://www.sciencedirect.com/science/article/pii/S0965997824001406},
author = {Bořek Patzák and Stanislav Šulc and Václav Šmilauer},
keywords = {Semantic digital twin model, Entity data model, Simulation platform},
abstract = {This paper describes the design and implementation of a digital twin model in the open-source MuPIF simulation platform. MuPIF enables a user-defined data model based on an ontology or schema to be created. A representation of the data model is generated in a target data management system. The data model, integrated with MuPIF, lets model entities to be linked, and model attributes can be assigned to simulation workflows inputs and outputs. The model is semantically-defined, provides full traceability, and has a web-based API for data discovery.}
}
@article{PEREIRA2023104272,
title = {Querying semantic catalogues of biomedical databases},
journal = {Journal of Biomedical Informatics},
volume = {137},
pages = {104272},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104272},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422002775},
author = {Arnaldo Pereira and João Rafael Almeida and Rui Pedro Lopes and José Luís Oliveira},
keywords = {Biomedical data, Knowledge bases, Semantic data, Linked data, Information extraction, Natural language interfaces, Question answering},
abstract = {Background
Secondary use of health data is a valuable source of knowledge that boosts observational studies, leading to important discoveries in the medical and biomedical sciences. The fundamental guiding principle for performing a successful observational study is the research question and the approach in advance of executing a study. However, in multi-centre studies, finding suitable datasets to support the study is challenging, time-consuming, and sometimes impossible without a deep understanding of each dataset.
Methods
We propose a strategy for retrieving biomedical datasets of interest that were semantically annotated, using an interface built by applying a methodology for transforming natural language questions into formal language queries. The advantages of creating biomedical semantic data are enhanced by using natural language interfaces to issue complex queries without manipulating a logical query language.
Results
Our methodology was validated using Alzheimer’s disease datasets published in a European platform for sharing and reusing biomedical data. We converted data to semantic information format using biomedical ontologies in everyday use in the biomedical community and published it as a FAIR endpoint. We have considered natural language questions of three types: single-concept questions, questions with exclusion criteria, and multi-concept questions. Finally, we analysed the performance of the question-answering module we used and its limitations. The source code is publicly available at https:// bioinformatics-ua.github.io/BioKBQA/.
Conclusion
We propose a strategy for using information extracted from biomedical data and transformed into a semantic format using open biomedical ontologies. Our method uses natural language to formulate questions to be answered by this semantic data without the direct use of formal query languages.}
}
@article{NAVEEN2024126,
title = {GeoNLU: Bridging the gap between natural language and spatial data infrastructures},
journal = {Alexandria Engineering Journal},
volume = {87},
pages = {126-147},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2023.12.027},
url = {https://www.sciencedirect.com/science/article/pii/S1110016823011195},
author = {Palanichamy Naveen and Rajagopal Maheswar and Pavel Trojovský},
keywords = {GeoNLU, Natural language processing, Spatial data infrastructure, GeoSpatial data},
abstract = {Integrating natural language processing (NLP) techniques with spatial data infrastructures (SDIs) potentially revolutionize the way users interact with geospatial data. This article presents GeoNLU, a comprehensive framework aimed at bridging the gap between natural language and SDIs. GeoNLU aims to enable seamless interaction and querying of geospatial data through natural language, thereby enhancing accessibility and usability for a wide range of users. This article delves into the theoretical foundations, architectural design, key components, and potential applications of GeoNLU, highlighting its significance in improving geospatial data exploration, analysis, and decision-making.}
}
@article{MOHAMMADHASSANZADEH2024111493,
title = {Plausible reasoning over large health datasets: A novel approach to data analytics leveraging semantics},
journal = {Knowledge-Based Systems},
volume = {289},
pages = {111493},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111493},
url = {https://www.sciencedirect.com/science/article/pii/S095070512400128X},
author = {Hossein Mohammadhassanzadeh and Samina {Raza Abidi} and Syed Sibte {Raza Abidi}},
keywords = {Plausible reasoning, Query answering, Semantic analytics, Knowledge Discovery, Semantic Web Technologies},
abstract = {Plausible reasoning is an interesting and viable approach for semantic data analytics as it provides a non-deterministic and exploratory approach to inferring new knowledge from large datasets. Plausible reasoning identifies meaningful associations between data elements by analyzing the underlying semantics of the data to identify knowledge that can assist in complex decision-making, especially when dealing with incomplete and noisy knowledge. In this work, we present a plausible reasoning approach that computerizes formal plausible patterns using semantic web methods for knowledge representation and reasoning over knowledge graphs to discover new knowledge from large datasets. We implemented six plausible patterns, representing three different types of semantic relationships, by developing PL-OWL as a plausible extension to the Web Ontology Language (OWL). We incorporated the plausible patterns within our plausible reasoning framework—SeDan (SEmantics-based Data Analytics) —to provide plausible reasoning-based query answering over knowledge graphs to generate new knowledge and provide answers to complex queries. We evaluated our plausible reasoning-based query answering approach by answering medical questions from BioASQ challenges using SeDan which incorporates the Semantic MEDLINE database, DrugBank, and Disease Ontology for supplementary semantic associations. Our experimental results show that our plausible reasoning method was able to expand the query answering coverage of the Semantic MEDLINE database by 37% to generate plausible answers to previously unanswered queries—importantly 88% of the plausible answers generated were clinically reasonable and verified by a domain expert.}
}
@article{CHENG2022695,
title = {Construction and Implementation of Matter-Element Matching Model for Research and Development Tasks and Resources},
journal = {Procedia CIRP},
volume = {109},
pages = {695-700},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.316},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122007661},
author = {Zhizhong Cheng and Daming Li and Yuhu Li and Zhicheng Huang and Lihong Qiao},
keywords = {R & D tasks, Ontology, Matter-element, Cascade, Resource matching},
abstract = {To accurately and efficiently match tasks and resources in the R & D process of complex products, a model using matter-elements is proposed. In the model, ontologies are established to provide a unified description of task and resource information. According to the concepts of extension engineering, the matter-elements of R & D tasks and resources are established by extracting key information from the obtained ontology instances. The methods of calculating the authenticity degree, extension distance, and correlation degree for different types of features are discussed; the matching problem between tasks and resources is converted to a calculation of the matching degree of related features. A cascade matching method is used to calculate the matching degrees between the task and resource matter-elements. The resources were selected from different perspectives to obtain the optimal matching scheme. The proposed model and method were verified through a case study.}
}
@article{HAGEDORN2023105106,
title = {Semantic rule checking of cross-domain building data in information containers for linked document delivery using the shapes constraint language},
journal = {Automation in Construction},
volume = {156},
pages = {105106},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105106},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523003667},
author = {Philipp Hagedorn and Pieter Pauwels and Markus König},
keywords = {AEC, BIM, Rule checking, SHACL, ICDD, Semantic web, Information container, Linked building data, Benchmark},
abstract = {Information exchange is a crucial process throughout the lifecycle of built assets. Building Information Modeling (BIM) and standardized information delivery enable exchanging, storing, and linking cross-domain information bundled in containers such as the standardized Information Container for Linked Document Delivery (ICDD). While assessing the quality of the interlinking and metadata is essential to allow all parties to interpret an information container correctly, most research efforts have focused on code compliance of domain-specific models. In order to address this gap, this paper proposes an approach for validating ICDD information containers using the Shapes Constraint Language (SHACL), the W3C recommendation for Resource Description Framework (RDF) data validation. For optimized querying and validation, a case study applies SHACL inference rules to the linking structure of ICDD to infer predicates between linked entities in four ICDD containers in varying dataset sizes and different linking methods. Afterward, the validation of these containers is analyzed using three types of validation rules with increasing complexity for another SHACL implementation. Results show the feasibility of cross-domain building information validation in ICDD with SHACL and how performance enhancements can be obtained on an inferred dataset compared to the original dataset.}
}
@article{YANG2024105817,
title = {Prompt-based automation of building code information transformation for compliance checking},
journal = {Automation in Construction},
volume = {168},
pages = {105817},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105817},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524005533},
author = {Fan Yang and Jiansong Zhang},
keywords = {Building code, Information transformation, Automated compliance checking, Prompt engineering, Natural language processing, Large language models (LLMs)},
abstract = {Transforming building code information into a machine-processable format is essential for automated compliance checking, yet it presents significant challenges. A prompt-based framework was developed to automate the conversion into a logic programming language. Its effectiveness was assessed by testing the framework on 51 requirements from the International Building Code (IBC) 2015, achieving 97.37 % precision and 95.88 % recall at the logic clause level, with only 2 % of the data used for training. Further testing on crash report transformation enhanced efficiency, reducing the average code generation time to approximately 60.8 s, thereby achieving a 27.8 % time savings compared to existing rule-based methods. This paper contributes to the body of knowledge by introducing an effective, versatile, and user-friendly approach to automated building code information transformation, markedly decreasing the reliance on training data, time, and manual efforts.}
}
@article{NGO2022107127,
title = {Knowledge representation in digital agriculture: A step towards standardised model},
journal = {Computers and Electronics in Agriculture},
volume = {199},
pages = {107127},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107127},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922004446},
author = {Quoc Hung Ngo and Tahar Kechadi and Nhien-An Le-Khac},
keywords = {Knowledge maps, Knowledge representation, Knowledge management system, Agriculture computing ontology, Digital agriculture, Data mining},
abstract = {In recent years, data science has evolved significantly. Data analysis and mining processes become routines in all sectors of the economy where datasets are available. Vast data repositories have been collected, curated, stored, and used for extracting knowledge. And this is becoming commonplace. Subsequently, we extract a large amount of knowledge, either directly from the data or through experts in the given domain. The challenge now is how to exploit all this large amount of knowledge that is previously known for efficient decision-making processes. Until recently, much of the knowledge gained through a number of years of research is stored in static knowledge bases or ontologies, while more diverse and dynamic knowledge acquired from data mining studies is not centrally and consistently managed. In this research, we propose a novel model called ontology-based knowledge map to represent and store the results (knowledge) of data mining in crop farming to build, maintain, and enrich the process of knowledge discovery. The proposed model consists of six main sets: concepts, attributes, relations, transformations, instances, and states. This model is dynamic and facilitates the access, updates, and exploitation of the knowledge at any time. This paper also proposes an architecture for handling this knowledge-based model. The system architecture includes knowledge modelling, extraction, assessment, publishing, and exploitation. This system has been implemented and used in agriculture for crop management and monitoring. It is proven to be very effective and promising for its extension to other domains.}
}
@article{CHANG2025104532,
title = {Engineering material failure analysis report generation based on QWen and Llama2},
journal = {Results in Engineering},
volume = {26},
pages = {104532},
year = {2025},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2025.104532},
url = {https://www.sciencedirect.com/science/article/pii/S2590123025006097},
author = {Sijie Chang and Meng Wan and Jiaxiang Wang and Hao Du and Pufen Zhang and Peng Shi},
keywords = {Failure analysis report generation, Large language model, Prompt engineering, Llama2, Qwen, Gpt evaluation},
abstract = {Engineering material failure carries risks to the public and personal safety and causes economic loss. Failure analysis report describes the causes of failure and gives the improvement strategy, but requiring a lot of manpower of experts. The powerful text understanding and generation capabilities of large language models make automatic analysis possible. This paper presents an automatic report generation method using large language models, including training on only a few samples and instruction fine-tuning method (prompt engineering). The experiments are executed on two popular large language model bases: QWen and Llama2, involving 127 failure reports in 9 fields such as petroleum and petrochemical, nuclear and thermal power, ocean engineering, energy engineering, infrastructure, aerospace, transportation, equipment manufacturing, and water conservancy. Using different questioning ways, GPT evaluation and analysis based on manual verification are performed on the generated results. The experimental results show that the model can generate valuable case reports after being trained with specific domain data. The introduction of artificial intelligence technology for engineering failure prediction can enhance the safety of engineering development, improve industrial production efficiency, reduce labor costs, provide decision support during data analysis, and foster the emergence of new markets and business models.}
}
@article{ZHOU2025104199,
title = {Bi-directional feature learning-based approach for zero-shot event argument extraction},
journal = {Information Processing & Management},
volume = {62},
number = {5},
pages = {104199},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104199},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325001402},
author = {Ji Zhou and Kai Shuang and Qiwei Wang and Bing Qian and Jinyu Guo},
keywords = {Event argument extraction, Zero shot, Text generation, Feature learning},
abstract = {Recent research has shown that event argument extraction (EAE) methods based on transfer learning and data augmentation emphasize the contribution of contextual features and labeled features to zero-shot EAE tasks, respectively. However, these methods suffer from knowledge transfer insufficiency and context generation bias challenges. In this paper, we propose a bi-directional feature learning-based approach for zero-shot event argument extraction (BiTer), which gains bi-directional transferable knowledge and mitigates context generation bias. Specifically, BiTer contains source and target model training. During source model training, BiTer co-trains the contextual and labeled feature learning tasks on the source dataset. This step enables the target model to acquire bi-directional transferable knowledge, providing more appropriate feature representations for target events. In target model training, BiTer leverages the large language model to produce pseudo-arguments, and then the knowledge-embedded model generates training data of the target events based on them. This step mitigates context generation bias and makes BiTer learn a more comprehensive and precise feature of the target event. Extensive experiments on RAMS, WIKIEVENTS and ACE2005 have demonstrated BiTer achieves a new state-of-the-art level, with F1 in the zero-shot setting outperforming the baseline model by 4.6%, 7.5% and 0.4%, respectively.}
}
@article{SOBHKHIZ2023104859,
title = {Dynamic integration of unstructured data with BIM using a no-model approach based on machine learning and concept networks},
journal = {Automation in Construction},
volume = {150},
pages = {104859},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104859},
url = {https://www.sciencedirect.com/science/article/pii/S092658052300119X},
author = {Soroush Sobhkhiz and Tamer El-Diraby},
keywords = {Text to BIM, Data integration, Linked data, Semantic networks, Graph machine learning, Text classification},
abstract = {Accessing unstructured information through BIM-based platforms is essential for achieving integrated analytics especially, for facilities management where a wide range of unstructured data is required for effective decision-making. Previous research has explored linking textual data with BIM through the use of relational schemas, concept mapping, and ontologies. However, these methods are structured and static, failing to match the non-parametric and evolutionary nature of unstructured data. This study proposes an alternative approach where concept networks are used to represent the IFC data model. Using graph theory and natural language processing a classifier is trained for assigning text documents to their relevant IFC classes based on their conceptual network distances. Given that the classifier is trained on conceptual distances rather than the concepts themselves, it has the potential to be generalized to unseen classes with unseen concepts. Both the performance and the generalizability of the approach are evaluated in a case study.}
}
@article{BENSAOUD2025100834,
title = {Advancing software security: DCodeBERT for automatic vulnerability detection and repair},
journal = {Journal of Industrial Information Integration},
volume = {45},
pages = {100834},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100834},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25000585},
author = {Ahmed Bensaoud and Jugal Kalita},
keywords = {Large Language Models, Adversarial attacks, Vulnerability detection, Data privacy, Natural language processing},
abstract = {The exponential growth of software complexity has led to a corresponding increase in software vulnerabilities, necessitating robust methods for automatic vulnerability detection and repair. This paper proposes DCodeBERT, a large language model (LLM) fine-tuned for vulnerability detection and repair in software code. Leveraging the pre-trained CodeBERT model, DCodeBERT is designed to understand both natural language and programming language context, enabling it to effectively identify vulnerabilities and suggest repairs. We conduct experiments to evaluate DCodeBERT’s performance, comparing it against several baseline models. The results demonstrate that DCodeBERT outperforms the baselines in both vulnerability detection and repair tasks across multiple programming languages, showcasing its effectiveness in enhancing software security.}
}
@article{WOSCHITZ201953,
title = {Language in and out of society: Converging critiques of the Labovian paradigm},
journal = {Language & Communication},
volume = {64},
pages = {53-67},
year = {2019},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2018.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0271530918303185},
author = {Johannes Woschitz},
keywords = {Social-semiotics, Sociolinguistic metatheory, Labov, Third-wave variationism},
abstract = {What separates classical variationism from recent ‘social-semiotic’ approaches is its commitment to clearly distinguishable linguistic and social spheres. This distinction, as argued in this paper, is constructed through a juxtaposition of a social patterning of linguistic factors, and other social factors, which, when narrowly construed as changes from above, hinge on the conscious awareness of a linguistic feature. Recently, such a dichotomy has been called into question, since sociolinguists have begun theorising social meaningfulness as a more complex phenomenon that goes beyond the traditional ‘unconscious/conscious’ dichotomy that seems to underlie such a distinction. Giving up this dichotomy inevitably challenges the whole ‘narrow interface between language and society’ that underlies the orthodox Labovian framework, representing an ontological breach with important consequences.}
}
@article{ABDOLLAHI2021102167,
title = {Substituting clinical features using synthetic medical phrases: Medical text data augmentation techniques},
journal = {Artificial Intelligence in Medicine},
volume = {120},
pages = {102167},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102167},
url = {https://www.sciencedirect.com/science/article/pii/S0933365721001603},
author = {Mahdi Abdollahi and Xiaoying Gao and Yi Mei and Shameek Ghosh and Jinyan Li and Michael Narag},
keywords = {Unified Medical Language System, Natural language processing, Machine learning, Data augmentation, Medical document classification},
abstract = {Biomedical natural language processing (NLP) has an important role in extracting consequential information in medical discharge notes. Detecting meaningful features from unstructured notes is a challenging task in medical document classification. The domain specific phrases and different synonyms within the medical documents make it hard to analyze them. Analyzing clinical notes becomes more challenging for short documents like abstract texts. All of these can result in poor classification performance, especially when there is a shortage of the clinical data in real life. Two new approaches (an ontology-guided approach and a combined ontology-based with dictionary-based approach) are suggested for augmenting medical data to enrich training data. Three different deep learning approaches are used to evaluate the classification performance of the proposed methods. The obtained results show that the proposed methods improved the classification accuracy in clinical notes classification.}
}
@article{XIE2025106496,
title = {AI applications for structural design automation},
journal = {Automation in Construction},
volume = {179},
pages = {106496},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106496},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525005369},
author = {Hao Xie and Qipei Mei and Ying Hei Chui},
keywords = {Artificial intelligence, Structural design, Machine learning, Literature review, Automated compliance checking, Large language models},
abstract = {In recent years, there has been an increase in artificial intelligence (AI) applications in the field of structural engineering. With the rapid development of AI, new opportunities have emerged to use this technology to generate and check the structural design. Over the last few years, there has been a sharp increase in the number of publications relating to AI applications in structural design. This paper presents a systematic review of previous studies on the applications of AI in structural design. A total of 134 papers were analyzed. The review shows that AI techniques have benefited engineers in generating optimized structural design solutions that meet the requirements of building codes and design standards. Additionally, challenges and future opportunities are presented and discussed. This review will contribute to the body of knowledge by summarizing the state-of-the-art of using AI technologies in structural design and exploring future research opportunities in this area.}
}
@article{MA2025128106,
title = {An unsupervised fusion framework of generation and retrieval for entity search},
journal = {Expert Systems with Applications},
volume = {286},
pages = {128106},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128106},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425017270},
author = {Denghao Ma and Xu Zhang and Xueqiang Lv and Yanhe Du and ChangYu Wang and Hongbin Pei},
keywords = {Entity search, Large language models, Traditional retrieval models, Fusion models},
abstract = {Entity search aims to precisely and comprehensively return the target entities for a query of natural language, thus emphasizing both precision and recall qualities in the results returned. With the rise of large language models (LLMs), for the entity search task, LLMs tend to achieve high precision but low recall. Conversely, traditional retrieval models (TRMs) excel in delivering high recall but often come with lower precision. To facilitate complementarity between LLMs and TRMs, we propose an unsupervised fusion framework of generation and retrieval (FUGER) with new insights and modeling solutions, which effectively fuses their results and achieves both high precision and recall metrics. In this framework, a novel query-centric entity-category graph is constructed to model the query-aware relationships among entities. Second, over the query-centric entity-category graph, we propose an insight of graph-based scaling mechanism and query-driven iterative weighted geometric mean model (QIWGM) to fuse the results from LLMs and TRMs, addressing the challenge of scaling inconsistencies beween the relevance probabilities provided by LLMs and TRMs. To earlier stop the iterations of QIWGM, we propose an insight of ranking-oriented relative convergence and top-n consistency based termination model to automatically identify the convergence of QIWGM. Extensive experimental results, performed on two widely-used test sets: INEX-XER and SemSearch LS, show that 1) FUGER can effectively fuse the results of LLMs (GPT4, LLama2, Claude) and TRMs, achieving better performances than LLMs and TRMs; 2) FUGER achieves a significant improvement of the entity search performance over the baseline fusion solutions.}
}
@article{WANG2024843,
title = {Large language models assisted multi-effect variants mining on cerebral cavernous malformation familial whole genome sequencing},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {843-858},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S200103702400014X},
author = {Yiqi Wang and Jinmei Zuo and Chao Duan and Hao Peng and Jia Huang and Liang Zhao and Li Zhang and Zhiqiang Dong},
keywords = {Whole genome sequencing, Cerebral cavernous malformation, Deep learning, Large language model, Natural language processing},
abstract = {Cerebral cavernous malformation (CCM) is a polygenic disease with intricate genetic interactions contributing to quantitative pathogenesis across multiple factors. The principal pathogenic genes of CCM, specifically KRIT1, CCM2, and PDCD10, have been reported, accompanied by a growing wealth of genetic data related to mutations. Furthermore, numerous other molecules associated with CCM have been unearthed. However, tackling such massive volumes of unstructured data remains challenging until the advent of advanced large language models. In this study, we developed an automated analytical pipeline specialized in single nucleotide variants (SNVs) related biomedical text analysis called BRLM. To facilitate this, BioBERT was employed to vectorize the rich information of SNVs, while a deep residue network was used to discriminate the classes of the SNVs. BRLM was initially constructed on mutations from 12 different types of TCGA cancers, achieving an accuracy exceeding 99%. It was further examined for CCM mutations in familial sequencing data analysis, highlighting an upstream master regulator gene fibroblast growth factor 1 (FGF1). With multi-omics characterization and validation in biological function, FGF1 demonstrated to play a significant role in the development of CCMs, which proved the effectiveness of our model. The BRLM web server is available at http://1.117.230.196.}
}
@article{MENDIL2023102798,
title = {Formal domain-driven system development in Event-B: Application to interactive critical systems},
journal = {Journal of Systems Architecture},
volume = {135},
pages = {102798},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2022.102798},
url = {https://www.sciencedirect.com/science/article/pii/S1383762122002831},
author = {Ismail Mendil and Yamine Aït-Ameur and Neeraj Kumar Singh and Guillaume Dupont and Dominique Méry and Philippe Palanque},
keywords = {Domain knowledge, Ontology, Interactive system, Formal methods, Refinement and proofs, TCAS, Event-B},
abstract = {The design of complex and/or critical systems requires handling the environment constraints in which these systems evolve. Formal methods allow system developers to design models of such systems. They provide constructs for modelling components and views of these systems. However, these formal methods do not include built-in constructs for modelling the environment, and more broadly, domain knowledge associated with system models. Although ontologies have demonstrated their efficiency in modelling domain-specific features, they are not available as built-in constructs in formal methods. This paper shows how formal ontologies can be used to model domain-specific knowledge, as well as how system models may refer to these ontologies through annotation. We rely on the Event-B refinement and proof state-based method, and the associated theories, to define a framework in which domain-specific knowledge ontologies are formalised as Event-B theories defining data types used to type Event-B system design models. Finally, this framework is deployed for the specific case of interactive critical systems. To illustrate the proposed approach, a case study of the Traffic Collision Avoidance System (TCAS) is developed.}
}
@article{OYELADE201872,
title = {Patient symptoms elicitation process for breast cancer medical expert systems: A semantic web and natural language parsing approach},
journal = {Future Computing and Informatics Journal},
volume = {3},
number = {1},
pages = {72-81},
year = {2018},
issn = {2314-7288},
doi = {https://doi.org/10.1016/j.fcij.2017.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2314728817300314},
author = {O.N. Oyelade and A.A. Obiniyi and S.B. Junaidu and S.A. Adewuyi},
keywords = {Medical terms, Semantic web, Natural language processing (NLP), Rule sets, Inference, Heuristics},
abstract = {Information gathering from patient by clinicians during diagnostic procedures may sometimes require some skills to adequately collect required information that will be sufficient for the procedure. A situation where this information gathering may proof difficult in when a diagnostic decision making support system (DDSS) will have to gather such information from patient before carrying out the diagnostic procedure. Research has proven that it is more challenging to ensure user or patient inputs, in their raw form, maps into the list of acceptable medical terms for diagnostic tasks. This paper therefore proposes a formalized input generating model that addresses this shortcoming through the creation of an inference process, breast cancer lexicon, rule set and natural language processing (NLP). We developed an input generation algorithm which uses the python natural language processing capability in first filtering and generation the first pre-input collection. Furthermore, this algorithm then feeds in the pre-input word collection as input into the inference engine which has in its memory the rule set and ontology-based lexicon developed. Finally, this generates a list of acceptable tokens that will be sent into the medical expert system or DDSS for the diagnosing breast cancer. This proposed model was tested on a breast cancer based DDSS earlier designed by this authors, and result shows that the inference support of this model generates additional input of about 64% compared to when the patient's input where sent in as input in is state.}
}
@article{NOUIRA2018566,
title = {An Enhanced xAPI Data Model Supporting Assessment Analytics},
journal = {Procedia Computer Science},
volume = {126},
pages = {566-575},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.291},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918312675},
author = {Azer Nouira and Lilia Cheniti-Belcadhi and Rafik Braham},
keywords = {xAPI, Learning Analytics, Assessment Analytics, Ontology, SWRL},
abstract = {In the learning analytics field, it is highly significant to track and collect the big educational data to improve the learning experience. In fact, one of the e-learning standards for data interoperability which had attracted a remarkable amount of attention in the last years is the Experience API (xAPI). In this paper, we explore the use of xAPI in the learning analytics field. Therefore assessment data represents an important proportion of the educational data generated. When we focus on assessment, we can launch a new source of data that can be analyzed and hence contribute to the improvement of the field of learning analytics. In fact, we discuss the suitability of xAPI standard to track the assessment data and try to enhance its data model to support effectively the assessment analytics. An ontological model is proposed supporting assessment analytics purpose based on the weaknesses of the xAPI data model from assessment point of view. Since our proposed pattern is an ontological model, this gives us the chance to reason about the assessment data by performing some logic rules edited with SWRL(Semantic Web Rule Language) for supporting inference mechanisms related to the leaner level according to its assessment performance.}
}
@article{HASSEN20222968,
title = {Extending BPMN Models with Sensitive Business Process Aspects},
journal = {Procedia Computer Science},
volume = {207},
pages = {2968-2979},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.355},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922012443},
author = {Mariam Ben Hassen and Mohamed Turki and Faïez Gargouri},
keywords = {Business Process modeling, Knowledge Management, Sensitive Business Process, Core Domain Ontologies, BPMN 2.0.2, Extension Mechanism, Design Science Research Methodology},
abstract = {This research article aims to apply the Business Process Model and Notation (BPMN) for the representation of Sensitive Business Processes (SBPs). Therefore, we propose a new extension “BPMN4SBP” explicitly integrate all the relevant issues and aspects relevant at the coupling of the business process modeling (BPM) domain and the knowledge management (KM) domain for improving the management of crucial knowledge which are mobilized and created by these processes. This article provides the analysis of requirements and relevant concepts for modeling SBP. Based on a core domain ontology, need for extension is identified and the valid BPMN4SBP extension is designed (according to the BPMN extension mechanism) by the construction of a conceptional domain model and the corresponding BPMN extension model.}
}
@article{AMINI20221189,
title = {System Configuration Models: Towards a Specialization Approach},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1189-1194},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.551},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322018584},
author = {M. Mohammad Amini and T. Coudert and E. Vareilles and M. Aldanondo},
keywords = {System configuration, Knowledge formalization, Ontology, CSP, Configuration model, Specialization, Abstraction level},
abstract = {Nowadays, system configuration helps to achieve mass customization and manage a wide variety of systems. System configuration is based on a model that gathers all relevant knowledge for a family of systems. This knowledge model can be difficult to formalize and keep up to date; indeed, the knowledge must be made explicit, and can come from different departments and experts within an organization. In addition, it must reflect the different variants and options of a family of systems. Therefore, we try to answer the following question: how to better formalize and structure knowledge for system configuration to define configuration models and use the benefits of specialization in terms of modeling? Thus, in our proposal, we introduce the elements required to formalize the knowledge and define configuration models. This formalization will be done in a structured way using the association of ontology and Constraint Satisfaction Problem (CSP). We use the abilities of ontology to model knowledge of the different artifacts, their characteristics, and composition, and the abilities of CSP to model the relations between artifacts. In our proposal, we also define artifacts at different levels of abstraction using specialization which allows experts to detail or refine the formalized knowledge. We illustrate our proposals on a simplified but realistic example of a bicycle.}
}
@article{DOYLE2025105731,
title = {Communicating natural hazards science advice: Understanding scientists', decision-makers’, and the public's perceptions of the scientific process},
journal = {International Journal of Disaster Risk Reduction},
volume = {128},
pages = {105731},
year = {2025},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2025.105731},
url = {https://www.sciencedirect.com/science/article/pii/S2212420925005552},
author = {Emma E. H. Doyle and Jessica Thompson and Stephen R. Hill and Matt Williams and Douglas Paton and Sara E. Harrison and Ann Bostrom and Julia S. Becker},
abstract = {How individuals perceive scientific processes impacts their interpretation of, trust in, and use of, science advice particularly when managing uncertain natural hazard risk. We explored a) how diverse stakeholders understand how science of natural hazards is produced, and b) how this relates to their ontological, epistemological, and philosophical views of science. Using inductive analysis of semi-structured interviews with 31 participants involved in the management of natural hazards in Aotearoa New Zealand (including non-scientists), we produced three leading themes describing their views: 1) ‘Science is a way of seeing the world’; 2) ‘Science has limitations’; and 3) ‘Knowledge evolves’. Across Scientist, non-Scientist, and Lay public groups, there was broad agreement on the fundamental steps of the scientific process, aligning mostly with a hypothetico-deductive process. However, many discussed how others may have different perspectives of scientific approaches, truth, and reality. These are informed by training, disciplinary biases, cultural practices, and personal experience of hazards and associated science. We propose that individuals who recognise different worldviews and philosophies of science will experience higher levels of communication and cognitive uncertainty, which encourages information seeking behaviour and can improve communication efficacy, particularly during high pressure events. We conclude with three communication lessons: 1) be transparent about the processes and causes of change in natural hazards science advice; 2) communicate as both trusted individuals as well as through collective Science Advisory Group (SAG) systems; and 3) provide accessible structures and language to help lay people articulate scientific processes they often intuitively understand, rather than just simplifying information.}
}
@article{HOU202471,
title = {Language model based on deep learning network for biomedical named entity recognition},
journal = {Methods},
volume = {226},
pages = {71-77},
year = {2024},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2024.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S1046202324001038},
author = {Guan Hou and Yuhao Jian and Qingqing Zhao and Xiongwen Quan and Han Zhang},
keywords = {Biomedical named entity recognition, Deep learning, Language model, Multi-task learning},
abstract = {Biomedical Named Entity Recognition (BioNER) is one of the most basic tasks in biomedical text mining, which aims to automatically identify and classify biomedical entities in text. Recently, deep learning-based methods have been applied to Biomedical Named Entity Recognition and have shown encouraging results. However, many biological entities are polysemous and ambiguous, which is one of the main obstacles to the task of biomedical named entity recognition. Deep learning methods require large amounts of training data, so the lack of data also affect the performance of model recognition. To solve the problem of polysemous words and insufficient data, for the task of biomedical named entity recognition, we propose a multi-task learning framework fused with language model based on the BiLSTM-CRF architecture. Our model uses a language model to design a differential encoding of the context, which could obtain dynamic word vectors to distinguish words in different datasets. Moreover, we use a multi-task learning method to collectively share the dynamic word vector of different types of entities to improve the recognition performance of each type of entity. Experimental results show that our model reduces the false positives caused by polysemous words through differentiated coding, and improves the performance of each subtask by sharing information between different entity data. Compared with other state-of-the art methods, our model achieved superior results in four typical training sets, and achieved the best results in F1 values.}
}
@article{SAVOJARDO2023102641,
title = {Finding functional motifs in protein sequences with deep learning and natural language models},
journal = {Current Opinion in Structural Biology},
volume = {81},
pages = {102641},
year = {2023},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2023.102641},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X2300115X},
author = {Castrense Savojardo and Pier Luigi Martelli and Rita Casadio},
abstract = {Recently, prediction of structural/functional motifs in protein sequences takes advantage of powerful machine learning based approaches. Protein encoding adopts protein language models overpassing standard procedures. Different combinations of machine learning and encoding schemas are available for predicting different structural/functional motifs. Particularly interesting is the adoption of protein language models to encode proteins in addition to evolution information and physicochemical parameters. A thorough analysis of recent predictors developed for annotating transmembrane regions, sorting signals, lipidation and phosphorylation sites allows to investigate the state-of-the-art focusing on the relevance of protein language models for the different tasks. This highlights that more experimental data are necessary to exploit available powerful machine learning methods.}
}
@article{MANTE20243051,
title = {SeqImprove: Machine-Learning-Assisted Curation of Genetic Circuit Sequence Information},
journal = {ACS Synthetic Biology},
volume = {13},
number = {9},
pages = {3051-3055},
year = {2024},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.4c00392},
url = {https://www.sciencedirect.com/science/article/pii/S2161506324002341},
author = {Jeanet Mante and Zach Sents and Duncan Britt and William Mo and Chunxiao Liao and Ryan Greer and Chris J. Myers},
keywords = {named entity recognition, named entity normalization, SBOL, ontologies, machine learning},
abstract = {The progress and utility of synthetic biology is currently hindered by the lengthy process of studying literature and replicating poorly documented work. Reconstruction of crucial design information through post hoc curation is highly noisy and error-prone. To combat this, author participation during the curation process is crucial. To encourage author participation without overburdening them, an ML-assisted curation tool called SeqImprove has been developed. Using named entity recognition, called entity normalization, and sequence matching, SeqImprove creates machine-accessible sequence data and metadata annotations, which authors can then review and edit before submitting a final sequence file. SeqImprove makes it easier for authors to submit sequence data that is FAIR (findable, accessible, interoperable, and reusable).
}
}
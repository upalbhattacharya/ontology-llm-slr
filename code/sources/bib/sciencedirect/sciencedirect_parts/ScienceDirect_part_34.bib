@article{SYRIANI2024101287,
title = {Screening articles for systematic reviews with ChatGPT},
journal = {Journal of Computer Languages},
volume = {80},
pages = {101287},
year = {2024},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2024.101287},
url = {https://www.sciencedirect.com/science/article/pii/S2590118424000303},
author = {Eugene Syriani and Istvan David and Gauransh Kumar},
keywords = {Generative AI, GPT, Empirical research, Large language model, Literature review, Mapping study, Screening},
abstract = {Systematic reviews (SRs) provide valuable evidence for guiding new research directions. However, the manual effort involved in selecting articles for inclusion in an SR is error-prone and time-consuming. While screening articles has traditionally been considered challenging to automate, the advent of large language models offers new possibilities. In this paper, we discuss the effect of using ChatGPT on the SR process. In particular, we investigate the effectiveness of different prompt strategies for automating the article screening process using five real SR datasets. Our results show that ChatGPT can reach up to 82% accuracy. The best performing prompts specify exclusion criteria and avoid negative shots. However, prompts should be adapted to different corpus characteristics.}
}
@article{SHEN2024101587,
title = {Bioinformatics and machine learning driven key genes screening for hepatocellular carcinoma},
journal = {Biochemistry and Biophysics Reports},
volume = {37},
pages = {101587},
year = {2024},
issn = {2405-5808},
doi = {https://doi.org/10.1016/j.bbrep.2023.101587},
url = {https://www.sciencedirect.com/science/article/pii/S2405580823001681},
author = {Ye Shen and Juanjie Huang and Lei Jia and Chi Zhang and Jianxing Xu},
keywords = {Hepatocellular carcinoma cells, Differentially expressed genes, Biomarkers, Machine learning},
abstract = {Liver cancer, a global menace, ranked as the sixth most prevalent and third deadliest cancer in 2020. The challenge of early diagnosis and treatment, especially for hepatocellular carcinoma (HCC), persists due to late-stage detections. Understanding HCC's complex pathogenesis is vital for advancing diagnostics and therapies. This study combines bioinformatics and machine learning, examining HCC comprehensively. Three datasets underwent meticulous scrutiny, employing various analytical tools such as Gene Ontology (GO) function and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway enrichment analysis, protein interaction assessment, and survival analysis. These rigorous investigations uncovered twelve pivotal genes intricately linked with HCC's pathophysiological intricacies. Among them, CYP2C8, CYP2C9, EPHX2, and ESR1 were significantly positively correlated with overall patient survival, while AKR1B10 and NQO1 displayed a negative correlation. Moreover, the Adaboost prediction model yielded an 86.8 % accuracy, showcasing machine learning's potential in deciphering complex dataset patterns for clinically relevant predictions. These findings promise to contribute valuable insights into the elusive mechanisms driving liver cancer (HCC). They hold the potential to guide the development of more precise diagnostic methods and treatment strategies in the future. In the fight against this global health challenge, unraveling HCC's intricacies is of paramount importance.}
}
@article{LIU2020167,
title = {Adaptive protocol generation for group collaborative in smart medical waste transportation},
journal = {Future Generation Computer Systems},
volume = {110},
pages = {167-180},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19302031},
author = {Wei Liu and Jingzhi Guo and Feng Yao and Deng Chen},
keywords = {Adaptive systems, Agents collaboration, Domain ontology expression, AGV-based simulation},
abstract = {This article addresses the challenge of collective decision making among the smart components in medical waste transportation. We search for an optimized alternation to generate a commitment according to the capabilities belonging to heterogeneous agents under ever-changing context conditions. we propose a Goal-Capability-Commitment based Executable Tree (GCC-ETree) approach to generate an adaptive protocol. We illustrate our approach in a real-world medical waste automated guided vehicle transportation scenario and evaluate the feasibility of our approach by comparison with other common approaches. Our work makes two main contributions. First, we show how ontology-based matching and calculation can be used to enact a semantic understanding of the alignment of environment, goal, capability and commitment. Second, we introduce run-time goal modeling into collective decision making to capture the real-time requirements and provide execution flow information for generating a decision flow at runtime.}
}
@article{YANG2023107475,
title = {A framework for structured semantic representation capable of active sensing and interpretable inference: A cancer prognostic analysis case study},
journal = {Computers in Biology and Medicine},
volume = {166},
pages = {107475},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107475},
url = {https://www.sciencedirect.com/science/article/pii/S001048252300940X},
author = {Xin Yang and Jie Jin and Qiaolei Yang and Xueling Shen and Xin Chen},
keywords = {Semantic representation, Natural language inference, Knowledge representation, Explainable artificial intelligence},
abstract = {Precise semantic representation is important for allowing machines to truly comprehend the meaning of natural language text, especially biomedical literature. Although the semantic relations among words in a single sentence may be accurately represented with existing approaches, relations between two sentences cannot yet be accurately modeled, which leads to a lack of contextual information and difficulty in performing interpretable semantic inference. Additionally, it is challenging to merge semantic representations curated by different experts. These critical challenges are insufficiently addressed by existing methods. In this paper, we present a framework for structured semantic representation (FSSR) to address these issues. FSSR uses a double-layer structure Construct that combines Paradigm and Instance to represent the semantics of a word or a sentence. It uses six types of rules to represent the semantic relations between sentence Constructs and uses a Computational Model to represent an action. FSSR is a graph-based representation of semantics, in which a node represents a Construct or a Paradigm. Two nodes are connected by an edge (a rule). In addition, FSSR enables interpretable inference and active acquisition of new information, as illustrated in a case study. This case study models the semantics of a cancer prognostic analysis article and reproduces its text results and charts. We provide a website that visualizes the inference process (http://cragraph.synergylab.cn).}
}
@article{KOOLE2018,
title = {Mobile Learning as a Tool for Indigenous Language Revitalization and Sustainability in Canada:},
journal = {International Journal of Mobile and Blended Learning},
volume = {10},
number = {4},
year = {2018},
issn = {1941-8647},
doi = {https://doi.org/10.4018/IJMBL.2018100101},
url = {https://www.sciencedirect.com/science/article/pii/S1941864718000033},
author = {Marguerite Koole and Kevin wâsakâyâsiw Lewis},
keywords = {Cree, FRAME Model, Indigenous Languages, Mobile Learning, Nêhiyawêwin},
abstract = {ABSTRACT
In this article, the authors explore how mobile learning can complement the Certificate of Indigenous Languages program at the University of Saskatchewan in Western Canada. Through the FRAME model analysis, the authors extract salient cultural, pedagogical, environmental, and technological characteristics that should be considered in the development of mobile learning tools and approaches for Cree language teachers. It is hoped that this article will stimulate a dialogue amongst designers and Indigenous groups regarding language sustainability through mobile learning. The article concludes with key findings: the need to follow protocols, to establish good relationships, and to design for areas of low/no bandwidth. Finally, the examination of current Indigenous language learning methods provides ideas for the development of much needed “apps” appropriate for Cree learners and teachers.}
}
@article{TAKAHASHI20232222,
title = {Catalysts informatics: paradigm shift towards data-driven catalyst design},
journal = {Chemical Communications},
volume = {59},
number = {16},
pages = {2222-2238},
year = {2023},
issn = {1359-7345},
doi = {https://doi.org/10.1039/d2cc05938j},
url = {https://www.sciencedirect.com/science/article/pii/S1359734523010819},
author = {Keisuke Takahashi and Junya Ohyama and Shun Nishimura and Jun Fujima and Lauren Takahashi and Takeaki Uno and Toshiaki Taniike},
abstract = {ABSTRACT
Designing catalysts is a challenging matter as catalysts are involved with various factors that impact synthesis, catalysts, reactor and reaction. In order to overcome these difficulties, catalysts informatics is proposed as an alternative way to design and understand catalysts. The underlying concept of catalysts informatics is to design the catalysts from trends and patterns found in catalysts data. Here, three key concepts are introduced: experimental catalysts database, knowledge extraction from catalyst data via data science, and a catalysts informatics platform. Methane oxidation is chosen as a prototype reaction for demonstrating various aspects of catalysts informatics. This work summarizes how catalysts informatics plays a role in catalyst design. The work covers big data generation via high throughput experiments, machine learning, catalysts network method, catalyst design from small data, catalysts informatics platform, and the future of catalysts informatics via ontology. Thus, the proposed catalysts informatics would help innovate how catalysts can be designed and understood.}
}
@article{BERGES2021100222,
title = {A Semantic Approach for Big Data Exploration in Industry 4.0},
journal = {Big Data Research},
volume = {25},
pages = {100222},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100222},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000393},
author = {Idoia Berges and Víctor Julio Ramírez-Durán and Arantza Illarramendi},
keywords = {Data exploration, Industry 4.0, Ontologies},
abstract = {The growing trends in automation, Internet of Things, big data and cloud computing technologies have led to the fourth industrial revolution (Industry 4.0), where it is possible to visualize and identify patterns and insights, which results in a better understanding of the data and can improve the manufacturing process. However, many times, the task of data exploration results difficult for manufacturing experts because they might be interested in analyzing also data that does not appear in pre-designed visualizations and therefore they must be assisted by Information Technology experts. In this paper, we present a proposal materialized in a semantic-based visual query system developed for a real Industry 4.0 scenario that allows domain experts to explore and visualize data in a friendly way. The main novelty of the system is the combined use that it makes of captured data that are semantically annotated first, and a 2D customized digital representation of a machine that is also linked with semantic descriptions. Those descriptions are expressed using terms of an ontology, where, among others, the sensors that are used to capture indicators about the performance of a machine that belongs to a Industry 4.0 scenario have been modeled. Moreover, this semantic description allows to: formulate queries at a higher level of abstraction, provide customized graphical visualizations of the results based on the format and nature of the data, and download enriched data enabling further types of analysis.}
}
@article{SANTAMARIAGARCIA2024194,
title = {Deductive Care Methodology: Describing and testing modes of care research},
journal = {Enfermería Clínica (English Edition)},
volume = {34},
number = {3},
pages = {194-206},
year = {2024},
issn = {2445-1479},
doi = {https://doi.org/10.1016/j.enfcle.2024.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2445147924000250},
author = {José María Santamaría-García and Alexandra González-Aguña and Marta Fernández-Batalla and Sara Herrero-Jaén and María Lourdes Jiménez-Rodríguez and León Atilano González-Sotos},
keywords = {Nursing, Health Information Management, Models, Nursing, Models, Theoretical, Nursing Theory, Enfermería, Gestión de la Información en Salud, Modelos de Enfermería, Modelos Teóricos, Teoría de Enfermería},
abstract = {Objective
Define the modes of procedure of the Deductive Care Methodology (DCM) in the generation of knowledge about person's health care.
Methodology
Design and test of the DCM modes based on three phases: mapping of the DCM, generation of models from this methodology and testing of the models through studies in a clinical context.
Results
The DCM presents five levels of abstraction with three modes broken down to 16 types. The modes are: Philosophical Mode to conceptualize and obtain generalities about reality, Mathematical Mode to operate with generalities, and Physical Mode to operationally verify, validating the results and the predictive capacity of the model. This MDC allows the creation of three models: Knowledge Model about Person Care, an ontology of care, Vulnerability Model about the person and Taxonomic Triangulation Model for knowledge management. All models generate products for computational knowledge management. In addition, the models are applied in teaching and generate research with more than a hundred participations in conferences and journals, of which five impact publications (from 2008 to 2022) classified in the categories of Nursing and Informatics are analysed.
Conclusions
The DCM collects prior knowledge to work with certainties, evidence and applying inferences that do not depend on the number of cases or inductive designs. This research presents a formal structure of the DCM with an interdisciplinary orientation between Health Sciences and Computer Sciences.
Resumen
Objetivo
Definir los modos de procedimiento de la Metodología Deductiva del Cuidado (MDC) en la generación de conocimiento acerca del cuidado de la salud de las personas.
Metodología
Diseño y prueba de los modos de la MDC basado en tres fases: mapeado de la MDC, generación de modelos desde esta metodología y prueba de los modelos a través de estudios en contexto clínico.
Resultados
La MDC presenta cinco niveles de abstracción con tres modos desglosados hasta 16 tipos. Los modos son: Modo Filosófico para conceptualizar y obtener generalidades acerca de la realidad, Modo Matemático para operar con las generalidades y Modo Físico para verificar operacionalmente, validando los resultados y la capacidad predictiva del modelo. Esta MDC permite crear tres modelos: Modelo de Conocimiento sobre el Cuidado de la Persona, una ontología de cuidado, Modelo de Vulnerabilidad sobre la persona y Modelo de Triangulación Taxonómica para gestión de conocimiento. Todos los modelos generan productos para gestión computacional del conocimiento. Además, los modelos se aplican en docencia y generan investigaciones con más de un centenar de participaciones en congresos y revistas, de las cuales se analizan cinco publicaciones de impacto (desde 2008 a 2022) clasificadas en las categorías de Enfermería e Informática.
Conclusiones
La MDC recoge conocimiento previo para trabajar con certezas, evidencias y aplicando inferencias que no dependen del número de casos o diseños inductivos. Esta investigación presenta una estructura formal de la MDC con orientación interdisciplinar entre Ciencias de la Salud y Ciencias de la Computación.}
}
@article{MURAWSKY2023115953,
title = {The struggle with transnormativity: Non-binary identity work, embodiment desires, and experience with gender dysphoria},
journal = {Social Science & Medicine},
volume = {327},
pages = {115953},
year = {2023},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2023.115953},
url = {https://www.sciencedirect.com/science/article/pii/S0277953623003106},
author = {Stef Murawsky},
keywords = {Transnormativity, Non-binary, Transgender, Healthcare, Gender dysphoria, Embodiment, Medicalization, United States},
abstract = {I examine how non-binary people who have considered, or accessed, gender-affirming health care experience accountability to transnormativity using 12 in-depth interviews conducted between 2018 and 2019 in a midwestern American city. I detail how non-binary people who want to embody genders that are still largely culturally unintelligible think about identity, embodiment, and gender dysphoria. Using grounded theory methodology, I find that non-binary identity work around medicalization differs from that of transgender men and women in three primary ways: 1) regarding how they understand and operationalize gender dysphoria, 2) in relation to their embodiment goals, and 3) concerning how they experience pressure to medically transition. Non-binary people describe increased ontological uncertainty about their gender identities when researching gender dysphoria that is contextualized by an internalized sense of accountability to the transnormative expectation for medicalization. They additionally anticipate a potential medicalization paradox, where accessing gender-affirming care leads to a different type of binary misgendering and risks making their gender identities less, rather than more, culturally intelligible to others. Non-binary people also experience external accountability to transnormativity as pressure from trans and medical communities to think about dysphoria as inherently binaristic, embodied, and medically treatable. These findings indicate that non-binary people experience accountability to transnormativity differently than trans men and women. Since non-binary people and their body projects often disrupt the transnormative tropes that are the framework for trans medicine, they find trans therapeutics, and the diagnostic experience of gender dysphoria, uniquely problematic. Non-binary experiences of accountability to transnormativity indicate the need to re-center trans medicine to better accommodate non-normative embodiment desires and focus future diagnostic revisions of gender dysphoria to emphasize the social aspects of trans and non-binary experience.}
}
@article{CONDE2018551,
title = {Teamwork assessment in the educational web of data: A learning analytics approach towards ISO 10018},
journal = {Telematics and Informatics},
volume = {35},
number = {3},
pages = {551-563},
year = {2018},
note = {SI: EduWebofData},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2017.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0736585316304932},
author = {Miguel A. Conde and Ricardo Colomo-Palacios and Francisco J. García-Peñalvo and Xabier Larrucea},
keywords = {Teamwork, Competence, Ontology, ISO 10018},
abstract = {The Web of Data is an emerging research field that contributes to make better decisions because it gathers, combines and analyses different data sources available worldwide. Educational data is an interesting domain because it deals with the quality of the education itself and educational institutions which are common goals for every country. This paper is devoted to present how this idea has been used to improve a learning analytics tool. By means of this tool, teachers can perform teamwork competence assessment of a group of students taking into account how the individuals acquire the essential components of such competence. In this sense, authors use the Comprehensive Training Model of the Teamwork Competence in Engineering Domain (CTMTC) method to gather competence evidences and improve the system with a learning analytics tool to support the process. This tool is able to transform competence evidences and stores them in a competence ontology built upon ISO 10018 concepts. The final result is the production of educational results for the web of data.}
}
@article{XUE2024105730,
title = {Question-answering framework for building codes using fine-tuned and distilled pre-trained transformer models},
journal = {Automation in Construction},
volume = {168},
pages = {105730},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105730},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524004667},
author = {Xiaorui Xue and Jiansong Zhang and Yunfeng Chen},
keywords = {Question and answering, Building codes, Deep learning, Artificial intelligence, Natural language processing, Information retrieval},
abstract = {Building code compliance checking is considered a bottleneck in construction projects, which calls for a novel approach to building code query and information retrieval. To address this research gap, the paper presents a question and answering framework comprising: (1) a ‘retriever’ for efficient context retrieval from building codes in response to an inquiry, and (2) a ‘reader’ for precise context interpretation and answer generation. The ‘retriever’, based on the BM25 algorithm, achieved a top-1 precision, recall, and F1-score of 0.95, 0.95, and 0.95, and a top-5 precision, recall, and F1-score of 0.97, 1.00, and 0.99, respectively. The ‘reader’, utilizing the transformer-based “xlm-roberta-base-squad2-distilled” model, achieved a top-4 accuracy of 0.95 and a top-1 F1-score of 0.84. A fine-tuning and model distillation process was used and shown to provide high performance on limited amount of training data, overcoming a common barrier in the development of domain-specific (e.g., construction) deep learning models.}
}
@article{LIMA2025103263,
title = {ULKB Logic: A HOL-based framework for reasoning over knowledge graphs},
journal = {Science of Computer Programming},
volume = {242},
pages = {103263},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2025.103263},
url = {https://www.sciencedirect.com/science/article/pii/S0167642325000024},
author = {Guilherme Lima and Alexandre Rademaker and Rosario Uceda-Sosa},
keywords = {HOL, Python, Wikidata, SPARQL, MRS, NLP},
abstract = {ULKB Logic is an open-source framework written in Python for reasoning over knowledge graphs. It provides an interactive theorem prover-like environment equipped with a higher-order language similar to the one used by HOL Light. The main goal of ULKB Logic is to ease the construction of applications that combine state-of-the-art computational logic tools with the knowledge available in knowledge graphs, such as Wikidata. To this end, the framework provides APIs for fetching statements from SPARQL endpoints and operating over the constructed theories using automated theorem provers and SMT solvers (such as the E prover and Z3). In this paper, we describe the design and implementation of ULKB Logic, present its interfaces for querying knowledge graphs and for calling external provers, and discuss a use case of commonsense reasoning in which ULKB Logic is used as the target logic for representing the semantics of English sentences.}
}
@article{CAPUANO2022,
title = {A Semantic Framework Supporting Multilayer Networks Analysis for Rare Diseases},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.297141},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000102},
author = {Nicola Capuano and Pasquale Foggia and Luca Greco and Pierluigi Ritrovato},
keywords = {Biomedical Ontologies, Human Genome, Linked Data, Multilayer Network Analysis, Neuroen-Docrine Neoplasms, Rare Diseases, Semantic Information Integration},
abstract = {ABSTRACT
Understanding the role played by genetic variations in diseases, exploring genomic variants, and discovering disease-associated loci are among the most pressing challenges of genomic medicine. A huge and ever-increasing amount of information is available to researchers to address these challenges. Unfortunately, it is stored in fragmented ontologies and databases, which use heterogeneous formats and poorly integrated schemas. To overcome these limitations, the authors propose a linked data approach, based on the formalism of multilayer networks, able to integrate and harmonize biomedical information from multiple sources into a single dense network covering different aspects on Neuroendocrine Neoplasms (NENs). The proposed integration schema consists of three interconnected layers representing, respectively, information on the disease, on the affected genes, on the related biological processes and molecular functions. An easy-to-use client-server application was also developed to browse and search for information on the model supporting multilayer network analysis.}
}
@article{HARDJOJO2018,
title = {Validation of a Natural Language Processing Algorithm for Detecting Infectious Disease Symptoms in Primary Care Electronic Medical Records in Singapore},
journal = {JMIR Medical Informatics},
volume = {6},
number = {2},
year = {2018},
issn = {2291-9694},
doi = {https://doi.org/10.2196/medinform.8204},
url = {https://www.sciencedirect.com/science/article/pii/S2291969418000327},
author = {Antony Hardjojo and Arunan Gunachandran and Long Pang and Mohammed Ridzwan Bin Abdullah and Win Wah and Joash Wen Chen Chong and Ee Hui Goh and Sok Huang Teo and Gilbert Lim and Mong Li Lee and Wynne Hsu and Vernon Lee and Mark I-Cheng Chen and Franco Wong and Jonathan Siung King Phang},
keywords = {natural language processing, communicable diseases, epidemiology, surveillance, syndromic surveillance, electronic health records},
abstract = {Background
Free-text clinical records provide a source of information that complements traditional disease surveillance. To electronically harness these records, they need to be transformed into codified fields by natural language processing algorithms.
Objective
The aim of this study was to develop, train, and validate Clinical History Extractor for Syndromic Surveillance (CHESS), an natural language processing algorithm to extract clinical information from free-text primary care records.
Methods
CHESS is a keyword-based natural language processing algorithm to extract 48 signs and symptoms suggesting respiratory infections, gastrointestinal infections, constitutional, as well as other signs and symptoms potentially associated with infectious diseases. The algorithm also captured the assertion status (affirmed, negated, or suspected) and symptom duration. Electronic medical records from the National Healthcare Group Polyclinics, a major public sector primary care provider in Singapore, were randomly extracted and manually reviewed by 2 human reviewers, with a third reviewer as the adjudicator. The algorithm was evaluated based on 1680 notes against the human-coded result as the reference standard, with half of the data used for training and the other half for validation.
Results
The symptoms most commonly present within the 1680 clinical records at the episode level were those typically present in respiratory infections such as cough (744/7703, 9.66%), sore throat (591/7703, 7.67%), rhinorrhea (552/7703, 7.17%), and fever (928/7703, 12.04%). At the episode level, CHESS had an overall performance of 96.7% precision and 97.6% recall on the training dataset and 96.0% precision and 93.1% recall on the validation dataset. Symptoms suggesting respiratory and gastrointestinal infections were all detected with more than 90% precision and recall. CHESS correctly assigned the assertion status in 97.3%, 97.9%, and 89.8% of affirmed, negated, and suspected signs and symptoms, respectively (97.6% overall accuracy). Symptom episode duration was correctly identified in 81.2% of records with known duration status.
Conclusions
We have developed an natural language processing algorithm dubbed CHESS that achieves good performance in extracting signs and symptoms from primary care free-text clinical records. In addition to the presence of symptoms, our algorithm can also accurately distinguish affirmed, negated, and suspected assertion statuses and extract symptom durations.}
}
@article{FAYOUMI2021100221,
title = {An integrated socio-technical enterprise modelling: A scenario of healthcare system analysis and design},
journal = {Journal of Industrial Information Integration},
volume = {23},
pages = {100221},
year = {2021},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100221},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000212},
author = {Amjad Fayoumi and Richard Williams},
keywords = {Enterprise Modelling, Socio-technical Systems, Enterprise Integrated Model, Conceptual Modelling, Healthcare System},
abstract = {One of the crucial issues facing enterprise modelling (EM) practices is that EM is considered technical, and rarely or never has a social focus. Social aspects referred to here are the soft aspects of the organisation that lead to organic organisation development (communication, collaboration, culture, skills and personal goals). There are many EM approaches and enterprise architecture frameworks were proposed recently. These cover different enterprise aspects, perspectives, artefacts and models with different qualities and levels of details. Yet, the imperative determination has overlaid the declarative exploration in EM as a necessity of the design effort. Rethinking the assumptions underlying EM should bring a new and different understanding on how EM can be tackled within the enterprise, in particular the joint development and optimisation of socio-technical systems. This paper discusses EM from a socio-technical systems (STS) perspective, and towards forming a new model of EM that is driven from STS theory and combined with STS practices. Then proposes a conceptual integrated model that incorporates the new concepts of STS toward building an EM framework for balanced socio-technical joint development and optimisation. The approach is illustrated in a scenario from healthcare industry. A combination between modelling and STS practices proved powerful for holistic IT modernisation, future work discussed toward the end of the paper.}
}
@article{MAO2024101988,
title = {A survey on semantic processing techniques},
journal = {Information Fusion},
volume = {101},
pages = {101988},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101988},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523003044},
author = {Rui Mao and Kai He and Xulang Zhang and Guanyi Chen and Jinjie Ni and Zonglin Yang and Erik Cambria},
keywords = {Semantic processing, Word sense disambiguation, Anaphora resolution, Named entity recognition, Concept extraction, Subjectivity detection},
abstract = {Semantic processing is a fundamental research domain in computational linguistics. In the era of powerful pre-trained language models and large language models, the advancement of research in this domain appears to be decelerating. However, the study of semantics is multi-dimensional in linguistics. The research depth and breadth of computational semantic processing can be largely improved with new technologies. In this survey, we analyzed five semantic processing tasks, e.g., word sense disambiguation, anaphora resolution, named entity recognition, concept extraction, and subjectivity detection. We study relevant theoretical research in these fields, advanced methods, and downstream applications. We connect the surveyed tasks with downstream applications because this may inspire future scholars to fuse these low-level semantic processing tasks with high-level natural language processing tasks. The review of theoretical research may also inspire new tasks and technologies in the semantic processing domain. Finally, we compare the different semantic processing techniques and summarize their technical trends, application trends, and future directions.}
}
@article{NIE2025100866,
title = {What can knowledge graph do for few-shot named entity recognition},
journal = {Journal of Web Semantics},
volume = {86},
pages = {100866},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100866},
url = {https://www.sciencedirect.com/science/article/pii/S157082682500006X},
author = {Binling Nie and Yiming Shao and Yigang Wang},
keywords = {Few-shot NER, Knowledge graph, Knowledge fusion},
abstract = {Due to its extensive applicability in various downstream domains, few-shot named entity recognition (NER) has attracted increasing attention, particularly in areas where acquiring sufficient labeled data poses a significant challenge. Recent studies have highlighted the potential of knowledge graphs (KGs) in enhancing natural language processing (NLP) tasks. However, a comprehensive understanding of whether and how KGs can effectively improve the NER performance under low-resource conditions remains elusive. In this paper, for the first time, we quantitatively investigate the effects of different kinds of extra KG features for few-shot NER. We enable our analysis by aggregating extra KG features into an NER framework. Through extensive experiments, we find that incorporating class features yields the best performance. To fully explore the potential of class features from KGs, we propose a novel network architecture, named KGen, to jointly leverage KG-based knowledge from both the input sentence side and the label semantic side for few-shot NER.The efficacy of our proposed method is validated through extensive experiments on five challenging datasets.}
}
@article{KERSIC2025486,
title = {A review on building blocks of decentralized artificial intelligence},
journal = {ICT Express},
volume = {11},
number = {3},
pages = {486-506},
year = {2025},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2025.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2405959525000463},
author = {Vid Keršič and Muhamed Turkanović},
keywords = {AI, Artificial intelligence, Blockchain, Cryptography, DEAI, Decentralization, Decentralized artificial intelligence},
abstract = {Artificial intelligence (AI) is one of the key technologies transforming our lives, while the transfer of knowledge and competencies from the academic sphere to the industry and real-world use cases are accelerating yearly. However, during that transition, several significant problems and questions need to be addressed for the field to develop ethically, such as digital privacy, ownership, and control. These are some of the reasons why the currently most popular approaches of artificial intelligence, i.e., centralized artificial intelligence (CEAI), are questionable, with other directions also being explored widely, such as decentralized artificial intelligence (DEAI), which aim to solve some of the most far-reaching problems. This paper aims to review and organize the knowledge in the field of DEAI, focusing solely on studies that fall within this category. A systematic literature review (SLR) was conducted using six scientific databases and additional gray literature to analyze and present the findings of 71 identified studies. The paper’s primary focus is identifying the building blocks of DEAI solutions and networks, tackling the DEAI analysis from a bottom-up approach. Future research directions and open problems are presented and proposed at the end.}
}
@article{SESBOUE20221667,
title = {An Operational Architecture for Knowledge Graph-Based Systems},
journal = {Procedia Computer Science},
volume = {207},
pages = {1667-1676},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.224},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922011085},
author = {Matthias Sesboüé and Nicolas Delestre and Jean-Philippe Kotowicz and Ali Khudiyev and Cecilia Zanni-Merk},
keywords = {Knowledge Graph, Ontology, Knowledge-Based System, Semantic},
abstract = {Knowledge Graphs (KG) are gaining in popularity recently, notably since big tech giants announced they are using the technology. While the term is becoming popular, it is not new, and its ideas are even older. The research community has extensively studied knowledge Graphs in their various forms. Furthermore, the approach has been applied and proved valuable in many different applications. However, we found a lack of papers presenting the integration of KGs in a system regardless of the downstream application. We explore how KGs can fit in an overall information system independently from any specific use case, i.e., what we will consider knowledge consumption. We propose an architecture to understand better the KG roles within a system and how they can be integrated and implemented in a business context. We introduce each element of the latter architecture and discuss some candidate technology to implement them. Our work implements Knowledge Graph-Based Systems considering the constraints of a small to medium-sized enterprise.}
}
@article{VANCAPELLEVEEN2021111430,
title = {Toward building recommender systems for the circular economy: Exploring the perils of the European Waste Catalogue},
journal = {Journal of Environmental Management},
volume = {277},
pages = {111430},
year = {2021},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2020.111430},
url = {https://www.sciencedirect.com/science/article/pii/S0301479720313554},
author = {Guido {van Capelleveen} and Chintan Amrit and Henk Zijm and Devrim Murat Yazan and Asad Abdi},
keywords = {Recommender system, Tag recommendation, European waste catalogue (EWC), Industrial symbiosis, Circular economy},
abstract = {The growth in the number of industries aiming at more sustainable business processes is driving the use of the European Waste Catalogue (EWC). For example, the identification of industrial symbiosis opportunities, in which a user-generated item description has to be annotated with exactly one EWC tag from an a priori defined tag ontology. This study aims to help researchers understand the perils of the EWC when building a recommender system based on natural language processing techniques. We experiment with semantic enhancement (an EWC thesaurus) and the linguistic contexts of words (learned by Word2vec) for detecting term vector similarity in addition to direct term matching algorithms, which often fail to detect an identical term in the short text generated by users. Our in-depth analysis provides an insight into why the different recommenders were unable to generate a correct annotation and motivates a discussion on the current design of the EWC system.}
}
@article{MOORE2024100114,
title = {A tale of two English teachers: Examining narrative in multilingual writing and teaching},
journal = {Research Methods in Applied Linguistics},
volume = {3},
number = {2},
pages = {100114},
year = {2024},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2024.100114},
url = {https://www.sciencedirect.com/science/article/pii/S277276612400020X},
author = {Miriam Moore},
keywords = {Multilingual writing, Narrative, English as a Foreign Language, Writing process, L2 writing, TEFL},
abstract = {This paper examines how multilingual English language teachers use narrative for learning and teaching writing given their own writing practices and beliefs. The data was collected and analyzed using different narrative approaches to explore the advantages and disadvantages of each of these three models: Labov, Ochs and Capps, and De Fina & Georgakopoulou's Social Interactional Approach (SIA). Each model highlights different aspects of narrative. However, due to the contextual nature of multilingual writing, the SIA model proved to be the most comprehensive for knitting together teacher writing practices, beliefs about language-learning, and their pedagogies. Nevertheless, applying all three models proved useful for highlighting the dynamics of language use in different contexts along with multidimensional aspects of storytelling.}
}
@article{MATHEWS2022109668,
title = {Conservation needs to include a ‘story about feeling’},
journal = {Biological Conservation},
volume = {272},
pages = {109668},
year = {2022},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2022.109668},
url = {https://www.sciencedirect.com/science/article/pii/S000632072200221X},
author = {Freya Mathews},
keywords = {Epistemology of science, Methodological solipsism, Relational epistemology, , Caring for Country, Lawlands},
abstract = {Can science properly serve as the exclusive or framing epistemology for conservation? It is argued here that, regardless of the ontological findings of science, its epistemology subtly reinforces anthropocentric bias by distancing the knower from the known in the name of value neutrality. If conservation is to escape the grip of anthropocentric bias, its underlying epistemology needs to be expanded to include very different ways of knowing, based on feeling and hence on caring, that may be found in certain Indigenous cultures.}
}
@article{LUO2024109820,
title = {Comprehensive analysis of the miRNA-mRNA regulatory network involved in spontaneous recovery of an H2O2-induced zebrafish cataract model},
journal = {Experimental Eye Research},
volume = {240},
pages = {109820},
year = {2024},
issn = {0014-4835},
doi = {https://doi.org/10.1016/j.exer.2024.109820},
url = {https://www.sciencedirect.com/science/article/pii/S0014483524000411},
author = {Jiawei Luo and Mu Zhang and Yanhua Chen and Guowei Zhang and Tianqiu Zhou and Lihua Kang and Xiaoqing Chen and Huaijin Guan},
keywords = {miRNA-mRNA network, Zebrafish cataract, Oxidative stress, Opacity reversal},
abstract = {Objective
To identify the hub miRNAs and mRNAs contributing to the spontaneous recovery of an H2O2-induced zebrafish cataract model.
Methods
Zebrafishes were divided into three groups, i.e., Group A, which included normal control fish (day 0), and Groups B and C, where fish were injected with 2.5% hydrogen peroxide into the anterior chamber and reared for 14 and 30 days, respectively. Fish eyes were examined by stereomicroscope photography and optical coherence tomography (OCT). RNA profiles of fish lenses were detected by RNA sequencing. Differentially expressed genes (DEGs) and differentially expressed miRNAs (DEmiRs) were identified among three groups. The DEGs and DEmiRs, which changed in opposite positions between “B vs. A” and “C vs. B” were defined as ODGs (opposite positions changed DEGs) and ODmiRs (opposite positions changed DEmiRs). Gene Ontology (GO) analysis and Kyoto Encyclopedia of Genes and Genomes pathway (KEGG) analysis were carried out by R language. The protein-protein interaction network (PPI) was constructed using STRING. Potential targets of miRNAs were obtained using miRanda. miRNA-mRNA networks were constructed by Cytoscape.
Results
The fish lens opacity formed on day 14 and recovered to transparent on day 30 after injection. Compared to group B, 1366 DEGs and 54 DEmiRs were identified in group C. “C vs. B” DEGs were enriched in gene clusters related to development and oxidative phosphorylation. Target genes of DEmiRs were enriched in clusters such as development and cysteine metabolism. Among three groups, 786 ODGs and 27 ODmiRs were identified, and 480 ODGs were predicted as targets of ODmiRs. Target ODGs were enriched in pathways related to methionine metabolism, ubiquitin, sensory system development, and structural constituents of the eye lens. In addition, we established an ODmiRs-ODGs regulation network.
Conclusion
We identified several hub mRNAs and altered miRNAs in the formation and reversal of zebrafish cataracts. These hub miRNAs/mRNAs could be potential targets for the non-surgical treatment of ARC.}
}
@article{DING2022104164,
title = {Prediction and evaluation of combination pharmacotherapy using natural language processing, machine learning and patient electronic health records},
journal = {Journal of Biomedical Informatics},
volume = {133},
pages = {104164},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104164},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422001769},
author = {Pingjian Ding and Yiheng Pan and Quanqiu Wang and Rong Xu},
keywords = {Combination pharmacotherapy, Tensor factorization, Retrospective Cohort study, Hypertension},
abstract = {Combination pharmacotherapy targets key disease pathways in a synergistic or additive manner and has high potential in treating complex diseases. Computational methods have been developed to identifying combination pharmacotherapy by analyzing large amounts of biomedical data. Existing computational approaches are often underpowered due to their reliance on our limited understanding of disease mechanisms. On the other hand, observable phenotypic inter-relationships among thousands of diseases often reflect their underlying shared genetic and molecular underpinnings, therefore can offer unique opportunities to design computational models to discover novel combinational therapies by automatically transferring knowledge among phenotypically related diseases. We developed a novel phenome-driven drug discovery system, named TuSDC, which leverages knowledge of existing drug combinations, disease comorbidities, and disease treatments of thousands of disease and drug entities extracted from over 31.5 million biomedical research articles using natural language processing techniques. TuSDC predicts combination pharmacotherapy by extracting representations of diseases and drugs using tensor factorization approaches. In external validation, TuSDC achieved an average precision of 0.77 for top ranked candidates, outperforming a state of art mechanism-based method for discovering drug combinations in treating hypertension. We evaluated top ranked anti-hypertension drug combinations using electronic health records of 84.7 million unique patients and showed that a novel drug combination hydrochlorothiazide-digoxin was associated with significantly lower hazards of subsequent hypertension as compared to the monotherapy hydrochlorothiazide alone (HR: 0.769, 95% CI [0.732, 0.807]) and digoxin alone (0.857, 95% CI [0.785, 0.936]). Data-driven informatics analyses reveal that the renin-angiotensin system is involved in the synergistical interactions of hydrochlorothiazide and digoxin on regulating hypertension. The prediction model’s code with PyTorch version 1.5 is available at http://nlp.case.edu/public/data/TuSDC/.}
}
@article{SMIRNOV20181409,
title = {Semantic Interoperability for Coalition Creation by Mobile Robots and Humans: an Approach and Case Study},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {1409-1414},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.319},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318314435},
author = {Alexander Smirnov and Alexey Kashevnik},
keywords = {Ontology, interoperability, robotics, coalitions, contest, interaction},
abstract = {Interaction between mobile robots and humans is an actual task in different areas of modern world. In manufacturing Industry 4.0 brings new possibilities and technologies that provide certain benefits and requires a development of new approaches to semantic interoperability between industrial Internet participants. The paper describes an ontology-based approach to semantic interoperability support between mobile robots and humans for joint task’s performance. The proposed approach is based on the cyber-physical-social system concept and context management technology. Cyber-physical-social systems tightly integrate physical, information (cyber), and social spaces based on their real-time interactions. To perform some task a coalition of mobile robots and humans has to be created. Mobile robots operate in physical space while their information communication is implemented in cyber space. Social space contains humans that can be involved in the coalition in case when the task cannot be implemented by mobile robots themselves. Context management technology is aimed at the current situation modeling and utilization at the coalition creation process. The paper describes a case study that considers a point exploring and obstacles overcoming. To implement the case study two mobile robots and a human are engaged. Mobile robots have been constructed based on the Lego Mindstorms EV3 educational kit. The human user interacts with the system via a smartphone.}
}
@article{OLIVEIRA2020,
title = {Natural Language Processing for Surveillance of Cervical and Anal Cancer and Precancer: Algorithm Development and Split-Validation Study},
journal = {JMIR Medical Informatics},
volume = {8},
number = {11},
year = {2020},
issn = {2291-9694},
doi = {https://doi.org/10.2196/20826},
url = {https://www.sciencedirect.com/science/article/pii/S229196942000040X},
author = {Carlos R Oliveira and Patrick Niccolai and Anette Michelle Ortiz and Sangini S Sheth and Eugene D Shapiro and Linda M Niccolai and Cynthia A Brandt},
keywords = {natural language processing, automated data extraction, human papillomavirus, surveillance, pathology reporting, cervical cancer, anal cancer, precancer, cancer, HPV, accuracy},
abstract = {Background
Accurate identification of new diagnoses of human papillomavirus–associated cancers and precancers is an important step toward the development of strategies that optimize the use of human papillomavirus vaccines. The diagnosis of human papillomavirus cancers hinges on a histopathologic report, which is typically stored in electronic medical records as free-form, or unstructured, narrative text. Previous efforts to perform surveillance for human papillomavirus cancers have relied on the manual review of pathology reports to extract diagnostic information, a process that is both labor- and resource-intensive. Natural language processing can be used to automate the structuring and extraction of clinical data from unstructured narrative text in medical records and may provide a practical and effective method for identifying patients with vaccine-preventable human papillomavirus disease for surveillance and research.
Objective
This study's objective was to develop and assess the accuracy of a natural language processing algorithm for the identification of individuals with cancer or precancer of the cervix and anus.
Methods
A pipeline-based natural language processing algorithm was developed, which incorporated machine learning and rule-based methods to extract diagnostic elements from the narrative pathology reports. To test the algorithm’s classification accuracy, we used a split-validation study design. Full-length cervical and anal pathology reports were randomly selected from 4 clinical pathology laboratories. Two study team members, blinded to the classifications produced by the natural language processing algorithm, manually and independently reviewed all reports and classified them at the document level according to 2 domains (diagnosis and human papillomavirus testing results). Using the manual review as the gold standard, the algorithm’s performance was evaluated using standard measurements of accuracy, recall, precision, and F-measure.
Results
The natural language processing algorithm’s performance was validated on 949 pathology reports. The algorithm demonstrated accurate identification of abnormal cytology, histology, and positive human papillomavirus tests with accuracies greater than 0.91. Precision was lowest for anal histology reports (0.87, 95% CI 0.59-0.98) and highest for cervical cytology (0.98, 95% CI 0.95-0.99). The natural language processing algorithm missed 2 out of the 15 abnormal anal histology reports, which led to a relatively low recall (0.68, 95% CI 0.43-0.87).
Conclusions
This study outlines the development and validation of a freely available and easily implementable natural language processing algorithm that can automate the extraction and classification of clinical data from cervical and anal cytology and histology.}
}
@article{MEDEIROS201975,
title = {LL-based query answering over RDF databases},
journal = {Journal of Computer Languages},
volume = {51},
pages = {75-87},
year = {2019},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X18301915},
author = {Ciro M. Medeiros and Martin A. Musicante and Umberto S. Costa},
keywords = {Context-Free graph patterns, Graph databases, RDF, SPARQL},
abstract = {We present a method based on top-down parsing techniques for evaluating context-free path queries on RDF Graph Databases. The syntax of the query language is based on SPARQL. The language extends SPARQL by allowing the use of non-terminal symbols of a context-free grammar to specify paths on the graph. In this manner, the language subsumes the definition of regular graph patterns present in SPARQL. Our query evaluator takes an RDF graph, a context-free grammar and a declarative query, and produces tuples of values. The query evaluator proceeds in two stages: Firstly, the RDF graph is enriched with edges representing paths which correspond to strings derived by the grammar. We show that this algorithm is correct and presents a cubic worst-case run-time complexity on the number of nodes in the graph, which is an improvement over some previous work. The second stage of the evaluator uses the produced graph to identify tuples of values defined by a declarative query. In order to validate our approach, we conducted experiments by using some popular ontologies as well as synthetic databases. We compare performance results of our method with some related work.}
}
@article{TEIXEIRA2018225,
title = {Data mart construction based on semantic annotation of scientific articles: A case study for the prioritization of drug targets},
journal = {Computer Methods and Programs in Biomedicine},
volume = {157},
pages = {225-235},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169260717309252},
author = {Marlon Amaro Coelho Teixeira and Kele Teixeira Belloze and Maria Cláudia Cavalcanti and Floriano P. Silva-Junior},
keywords = {Semantic annotation, Decision support systems, Drug target prioritization},
abstract = {Background and objectives
Semantic text annotation enables the association of semantic information (ontology concepts) to text expressions (terms), which are readable by software agents. In the scientific scenario, this is particularly useful because it reveals a lot of scientific discoveries that are hidden within academic articles. The Biomedical area has more than 300 ontologies, most of them composed of over 500 concepts. These ontologies can be used to annotate scientific papers and thus, facilitate data extraction. However, in the context of a scientific research, a simple keyword-based query using the interface of a digital scientific texts library can return more than a thousand hits. The analysis of such a large set of texts, annotated with such numerous and large ontologies, is not an easy task. Therefore, the main objective of this work is to provide a method that could facilitate this task.
Methods
This work describes a method called Text and Ontology ETL (TOETL), to build an analytical view over such texts. First, a corpus of selected papers is semantically annotated using distinct ontologies. Then, the annotation data is extracted, organized and aggregated into the dimensional schema of a data mart.
Results
Besides the TOETL method, this work illustrates its application through the development of the TaP DM (Target Prioritization data mart). This data mart has focus on the research of gene essentiality, a key concept to be considered when searching for genes showing potential as anti-infective drug targets.
Conclusions
This work reveals that the proposed approach is a relevant tool to support decision making in the prioritization of new drug targets, being more efficient than the keyword-based traditional tools.}
}
@article{ROGER2022104489,
title = {Missing links: The functional unification of language and memory (L∪M)},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {133},
pages = {104489},
year = {2022},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2021.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0149763421005601},
author = {Elise Roger and Sonja Banjac and Michel {Thiebaut de Schotten} and Monica Baciu},
keywords = {Language, Memory, Inter-functional relations, Interactive model, Cognitive neuroscience},
abstract = {The field of neurocognition is currently undergoing a significant change of perspective. Traditional neurocognitive models evolved into an integrative and dynamic vision of cognitive functioning. Dynamic integration assumes an interaction between cognitive domains traditionally considered to be distinct. Language and declarative memory are regarded as separate functions supported by different neural systems. However, they also share anatomical structures (notably, the inferior frontal gyrus, the supplementary motor area, the superior and middle temporal gyrus, and the hippocampal complex) and cognitive processes (such as semantic and working memory) that merge to endorse our quintessential daily lives. We propose a new model, "L∪M" (i.e., Language/union/Memory), that considers these two functions interactively. We fractionated language and declarative memory into three fundamental dimensions or systems (“Receiver-Transmitter”, “Controller-Manager” and “Transformer-Associative” Systems), that communicate reciprocally. We formalized their interactions at the brain level with a connectivity-based approach. This new taxonomy overcomes the modular view of cognitive functioning and reconciles functional specialization with plasticity in neurological disorders.}
}
@article{WARNER2019103239,
title = {HemOnc: A new standard vocabulary for chemotherapy regimen representation in the OMOP common data model},
journal = {Journal of Biomedical Informatics},
volume = {96},
pages = {103239},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103239},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419301583},
author = {Jeremy L. Warner and Dmitry Dymshyts and Christian G. Reich and Michael J. Gurley and Harry Hochheiser and Zachary H. Moldwin and Rimma Belenkaya and Andrew E. Williams and Peter C. Yang},
keywords = {Neoplasms, Ontologies, Knowledge engineering},
abstract = {Systematic application of observational data to the understanding of impacts of cancer treatments requires detailed information models allowing meaningful comparisons between treatment regimens. Unfortunately, details of systemic therapies are scarce in registries and data warehouses, primarily due to the complex nature of the protocols and a lack of standardization. Since 2011, we have been creating a curated and semi-structured website of chemotherapy regimens, HemOnc.org. In coordination with the Observational Health Data Sciences and Informatics (OHDSI) Oncology Subgroup, we have transformed a substantial subset of this content into the OMOP common data model, with bindings to multiple external vocabularies, e.g., RxNorm and the National Cancer Institute Thesaurus. Currently, there are >73,000 concepts and >177,000 relationships in the full vocabulary. Content related to the definition and composition of chemotherapy regimens has been released within the ATHENA tool (athena.ohdsi.org) for widespread utilization by the OHDSI membership. Here, we describe the rationale, data model, and initial contents of the HemOnc vocabulary along with several use cases for which it may be valuable.}
}
@article{SNIGDHA2023101247,
title = {Bioinformatics approach to analyse COVID-19 biomarkers accountable for generation of intracranial aneurysm in COVID-19 patients},
journal = {Informatics in Medicine Unlocked},
volume = {39},
pages = {101247},
year = {2023},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2023.101247},
url = {https://www.sciencedirect.com/science/article/pii/S2352914823000898},
author = {Mahajabin Snigdha and Azifa Akter and Md Al Amin and Md Zahidul Islam},
keywords = {Intracranial aneurysm, COVID-19, Bioinformatics, Association, Pathway, Ontology},
abstract = {COVID-19 became a health emergency on January 30, 2020. SARS-CoV-2 is the causative agent of the coronavirus disease known as COVID-19 and can develop cardiometabolic and neurological disorders. Intracranial aneurysm (IA) is considered the most significant reason for hemorrhagic stroke,and it accounts for approximately 85% of all subarachnoid hemorrhages (SAH). Retinoid signaling abnormalities may explain COVID-19's pathogenesis with inhibition of AEH2, from which COVID-19 infection may enhance aneurysm formation and rupture due to abrupt blood pressure changes, endothelial cell injury, and systemic inflammation. The objective of this study was to investigate the potential biomarkers, differentially expressed genes (DEGs), and metabolic pathways associated with both COVID-19 and intracranial aneurysm (IA) using simulation databases like DIsGeNET. The purpose was to confirm prior findings and gain a comprehensive understanding of the underlying mechanisms that contribute to the development of these conditions. We combined the regulated genes to describe intracranial aneurysm formation in COVID-19. To determine DEGs in COVID-19 and IA patient tissues, we compared gene expression transcriptomic datasets from healthy and diseased individuals. There were 41 differentially expressed genes (DEGs) shared by both the COVID-19 and IA datasets (27 up-regulated genes and 14 down-regulated genes). Using protein-protein interaction analysis, we were able to identify hub proteins (C3, NCR1, IL10RA, OXTR, RSAD2, CD38, IL10RB, MX1, IL10, GFAP, IFIT3, XAF1, USP18, OASL, IFI6, EPSTI1, CMPK2, and ISG15), which were not described as key proteins for both COVID-19 and IA before. We also used Gene Ontology analysis (6 significant ontologies were validated), Pathway analysis (the top 20 were validated), TF-Gene interaction analysis, Gene miRNA analysis, and Drug-Protein interaction analysis methods to comprehend the extensive connection between COVID-19 and IA. In Drug-Protein interaction analysis, we have gotten the following three drugs: LLL-3348, CRx139, and AV41 against IL10 which was both common for COVID-19 and IA disease. Our study with different cabalistic methods has showed the interaction between the proteins and pathways with drug analysis which may direct further treatment development for certain diseases.}
}
@article{COHEN20221283,
title = {Pain is Not a “thing”: How That Error Affects Language and Logic in Pain Medicine},
journal = {The Journal of Pain},
volume = {23},
number = {8},
pages = {1283-1293},
year = {2022},
issn = {1526-5900},
doi = {https://doi.org/10.1016/j.jpain.2022.03.235},
url = {https://www.sciencedirect.com/science/article/pii/S152659002200284X},
author = {Milton Cohen and Asaf Weisman and John Quintner},
keywords = {Misnomers, Fallacies, Speculative concepts},
abstract = {Effectiveness in academic and clinical communication depends upon agreement on what words and concepts denote and on the consequent ability to argue logically and accurately. In the pain medicine literature there are many examples of imprecision and confusion in this respect, including misnomers and fallacies in reasoning. This article firstly critically examines some of these misnomers. Identified themes include pain being conceptualised as a “thing,” conflation between nociception and pain, and confusion between stimulus and response and between the perspectives of the experiencer and the observer of “pain.” Secondly, fallacies in reasoning are identified that contribute to imprecision and confusion. These include reification of pain, attributing to the brain functions that belong to whole organisms, and the illusory truth effect. Thirdly, these themes are identified also in constructs that are shown to be based more on speculation than on fact. Taken together, these observations reveal a need to review and, where necessary, modify terminology and concepts used in Pain Medicine.
Perspective
This article examines a number of words and constructs commonly found in the pain literature from the perspective of accuracy in terms of their consistency of usage, concordance with fact, degree of speculation and logical argument. A common major theme is the error of considering pain as a “thing” that has agentive properties. A need to clarify much of the language used in Pain Medicine is identified.}
}
@article{KHOUFACHE2025131119,
title = {A distributed inference algorithm for Dirichlet process mixture models with exponential family components},
journal = {Neurocomputing},
volume = {653},
pages = {131119},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.131119},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225017916},
author = {Reda Khoufache and Mustapha Lebbah and Hanene Azzag and Etienne Goffinet and Djamel Bouchaffra},
keywords = {Horizontal federated learning, Distributed computing, Dirichlet process mixture models, Markov chain Monte Carlo, Bayesian non-parametric modeling},
abstract = {This article extends our earlier work published in [24]. Dirichlet Process Mixture Models (DPMMs) are widely used for clustering due to their ability to infer the number of clusters via a Bayesian non-parametric framework. However, MCMC inference with DPMMs becomes computationally intensive on large datasets. To address this, we propose a distributed inference algorithm, DisCGS, which approximates the collapsed Gibbs sampler using sufficient statistics and is specifically designed for horizontally distributed data across independent and potentially heterogeneous machines, making it well-suited for federated learning scenarios. Our contributions are threefold: first, we develop and evaluate DisCGS for the multivariate Gaussian DPMM, demonstrating significant computational gains—for example, reducing runtime from approximately 12 h to 3 min for 100 iterations on 100 K data points, a 200× speedup—while maintaining comparable clustering quality as measured by Adjusted Rand Index (ARI), Normalized Mutual Information (NMI), and Accuracy. Second, we introduce a multinomial DPMM formulation within the DisCGS framework tailored for discrete data such as text documents, showcasing its applicability to categorical and count-based data common in natural language processing and genomics. Third, we prove that DisCGS generalizes to all exponential family distributions, thereby extending its flexibility and applicability beyond Gaussian and multinomial models. The source code for the continuous (Gaussian) model is available at https://github.com/redakhoufache/DisCGS, and the discrete (multinomial) version for text clustering is provided at https://github.com/redakhoufache/DisCGS-for-Discrete-data.}
}
@article{YANG20201079,
title = {Perspective: Towards Automated Tracking of Content and Evidence Appraisal of Nutrition Research},
journal = {Advances in Nutrition},
volume = {11},
number = {5},
pages = {1079-1088},
year = {2020},
issn = {2161-8313},
doi = {https://doi.org/10.1093/advances/nmaa057},
url = {https://www.sciencedirect.com/science/article/pii/S2161831322000412},
author = {Chen Yang and Dana Hawwash and Bernard De Baets and Jildau Bouwman and Carl Lachat},
keywords = {STROBE-nut, reporting guidelines, graph database, research semantics, ontology, standardization},
abstract = {ABSTRACT
Robust recommendations for healthy diets and nutrition require careful synthesis of available evidence. Given the increasing volume of research articles generated, the retrieval and synthesis of evidence are increasingly becoming laborious and time-consuming. Information technology could help to reduce workload for humans. To guide supervised learning however, human identification of key study characteristics is necessary. Reporting guidelines recommend that authors include essential content in articles and could generate manually labeled training data for automated evidence retrieval and synthesis. Here, we present a semiautomated approach to annotate, link, and track the content of nutrition research manuscripts. We used the STROBE extension for nutritional epidemiology (STROBE-nut) reporting guidelines to manually annotate a sample of 15 articles and converted the semantic information into linked data in a Neo4j graph database through an automated process. Six summary statistics were computed to estimate the reporting completeness of the articles. The content structure, presence of essential study characteristics as well as the reporting completeness of the articles are visualized automatically from the graph database. The archived linked data are interoperable through their annotations and relations. A graph database with linked data on essential study characteristics can enable Natural Language Processing in nutrition.}
}
@article{KIM2025103236,
title = {Pathway information on methylation analysis using deep neural network (PROMINENT): An interpretable deep learning method with pathway prior for phenotype prediction using gene-level DNA methylation},
journal = {Artificial Intelligence in Medicine},
pages = {103236},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2025.103236},
url = {https://www.sciencedirect.com/science/article/pii/S093336572500171X},
author = {Soyeon Kim and Laizhi Zhang and Yidi Qin and Rebecca I. Caldino Bohn and Hyun Jung Park},
keywords = {Deep learning, DNA methylation, Phenotype prediction, Pathway, Gene},
abstract = {Background
DNA methylation is a key epigenetic marker that influences gene expression and phenotype regulation, and is affected by both genetic and environmental factors. Traditional linear regression methods such as elastic nets have been employed to assess the cumulative effects of multiple DNA methylation markers on phenotypes. However, these methods often fail to capture the complex nonlinear nature of the data. Recent deep learning approaches, such as MethylNet, have improved the prediction accuracy but lack interpretability and efficiency.
Findings
To address these limitations, we introduced Pathway Information on Methylation Analysis using a Deep Neural Network (PROMINENT), a novel interpretable deep learning method that integrates gene-level DNA methylation data with biological pathway information for phenotype prediction. PROMINENT enhances interpretability and prediction accuracy by incorporating gene- and pathway-level priors from databases such as Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG). It employs SHapley Additive exPlanations (SHAP) to prioritize significant genes and pathways. Evaluated across various datasets, childhood asthma, idiopathic pulmonary fibrosis (IPF), and first-episode psychosis (FEP)—PROMINENT consistently outperformed existing methods in terms of prediction accuracy and computational efficiency. PROMINENT also identified crucial genes and pathways involved in disease mechanisms.
Conclusions
PROMINENT represents a significant advancement in leveraging DNA methylation data for phenotype prediction, offering both high accuracy and interpretability within reasonable computational time. This method holds promise for elucidating the epigenetic underpinnings of complex diseases and enhancing the utility of DNA methylation data in biomedical research.}
}
@article{SIKSTROM2024105140,
title = {Pedagogical agents communicating and scaffolding students' learning: High school teachers' and students' perspectives},
journal = {Computers & Education},
volume = {222},
pages = {105140},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105140},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524001544},
author = {Pieta Sikström and Chiara Valentini and Anu Sivunen and Tommi Kärkkäinen},
keywords = {Pedagogical agent, Secondary education, User-centered design, Human–machine communication (HMC), Human-to-human communication script},
abstract = {Pedagogical agents (PAs) communicate verbally and non-verbally with students in digital and virtual reality/augmented reality learning environments. PAs have been shown to be beneficial for learning, and generative artificial intelligence, such as large language models, can improve PAs' communication abilities significantly. K-12 education is underrepresented in learning technology research and teachers' and students' insights have not been considered when developing PA communication. The current study addresses this research gap by conducting and analyzing semi-structured, in-depth interviews with eleven high school teachers and sixteen high school students about their expectations for PAs' communication capabilities. The interviewees identified relational and task-related communication capabilities that a PA should perform to communicate effectively with students and scaffold their learning. PA communication that is simultaneously affirmative and relational can induce immediacy, foster the relationship and engagement with a PA, and support students' learning management. Additionally, the teachers and students described the activities and technological aspects that should be considered when designing conversational PAs. The study showed that teachers and students applied human-to-human communication scripts when outlining their desired PA communication characteristics. The study offers novel insights and recommendations to researchers and developers on the communicational, pedagogical, and technological aspects that must be considered when designing communicative PAs that scaffold students’ learning, and discusses the contributions on human–machine communication in education.}
}
@article{YANG2025103041,
title = {LCDL: Classification of ICD codes based on disease label co-occurrence dependency and LongFormer with medical knowledge},
journal = {Artificial Intelligence in Medicine},
volume = {160},
pages = {103041},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.103041},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724002835},
author = {Yumeng Yang and Hongfei Lin and Zhihao Yang and Yijia Zhang and Di Zhao and Ling Luo},
keywords = {Automatic medical coding, Pre-trained language models, Multi-label classification, Label co-occurrence},
abstract = {Medical coding involves assigning codes to clinical free-text documents, specifically medical records that average over 3,000 markers, in order to track patient diagnoses and treatments. This is typically accomplished through manual assignments by healthcare professionals. To improve efficiency and accuracy while reducing the workload on these professionals, researchers have employed a multi-label classification approach. Since the long-tail phenomenon impacts tens of thousands of ICD codes, whereby only a few codes (representative of common diseases) are frequently assigned, while the majority of codes (representative of rare diseases) are infrequently assigned, this paper presents an LCDL model that addresses the challenge at hand by examining the LongFormer pre-trained language model and the disease label co-occurrence map. To enhance the performance of automated medical coding in the biomedical domain, hierarchies with medical knowledge, synonyms and abbreviations are introduced, improving the medical knowledge representation. Test evaluations are extensively conducted on the benchmark dataset MIMIC-III, and obtained the competitive performance compared to the previous state-of-the-art methods.}
}
@article{SENE201897,
title = {Discovering frequent patterns for in-flight incidents},
journal = {Cognitive Systems Research},
volume = {49},
pages = {97-113},
year = {2018},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041717301997},
author = {Alsane Sene and Bernard Kamsu-Foguem and Pierre Rumeau},
keywords = {Data mining, Association rule, Ontology reasoning, Decision system, Air transport},
abstract = {Objectives
In order to get a clearer idea of in-flight medical emergencies management, the application of Data mining tools can be useful to facilitate knowledge discovery from data collected by existing studies. The objective of this work is to conceptualize the construction of a Clinical Decision Support System (CDSS) in three stages corresponding to the representation levels necessary to extract knowledge from information and raw data.
Method
The method can be summarized in three parts: (1) in-flight medical incident data search, (2) the validation of this data using Data mining tools, (3) the construction of the CDSS in 3 steps corresponding to the levels of knowledge representation. These three steps will be carried out using tools such as EORCA (Event Oriented Representation for Collaborative Activities) which includes action codification with regard to an ontology and event representation.
Result
Data processing services provide a good structuration for information about in-flight medical incidents from which useful knowledge can be generated could improve the handling of other incidents by adapting the medical emergency equipments, for example. This structuring can be facilitated by the use of CDSS to fill in any gaps, increase coherency, and provide decision makers with a more complete picture of options that might be involved in a critical situation.
Conclusion
We proposed an evolving framework facilitating the description of in-flight medical emergencies with adequate data collection and appropriate information that are required for producing interesting rules and better decisions. The data collected nourishes the organization of information, which can be improved over time by continuous integration of evidence gained from the number of incidents treated. Finally, it is proposed to strengthen requirements concerning the medical equipments available on-board, particularly in the light of knowledge resulting from the selection and approval of interesting rules.}
}
@article{DANG201812,
title = {A data model for clinical legal medicine practice and the development of a dedicated software for both practitioners and researchers},
journal = {Journal of Forensic and Legal Medicine},
volume = {57},
pages = {12-18},
year = {2018},
note = {Thematic section: Big dataGuest editor: Thomas LefèvreThematic section: Health issues in police custodyGuest editors: Patrick Chariot and Steffen Heide},
issn = {1752-928X},
doi = {https://doi.org/10.1016/j.jflm.2016.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1752928X16301524},
author = {Catherine Dang and Thomas Phuong and Mahmoud Beddag and Anabel Vega and Céline Denis},
keywords = {Clinical legal medicine, Ontology, Data warehouse, Total incapacity to work, Personalized medicine, Predictive medicine},
abstract = {Objective
To present a data model for clinical legal medicine and the software based on that data model for both practitioners and researchers. The main functionalities of the presented software are computer-assisted production of medical certificates and data capture, storage and retrieval.
Methods
The data model and the software were jointly developed by the department of forensic medicine of the Jean Verdier Hospital (Bondy, France) and an bioinformatics laboratory (LIMICS, Paris universities 6–13) between November 2015 and May 2016. The data model was built based on four sources: i) a template used in our department for producing standardised medical certificates; ii) a random sample of medical certificates produced by the forensic department; iii) anterior consensus between four healthcare professionals (two forensic practitioners, a psychologist and a forensic psychiatrist) and iv) anatomical dictionaries. The trial version of the open source software was first designed for examination of physical assault survivors.
Results
An UML-like data model dedicated to clinical legal practice was built. The data model describes the terminology for examinations of sexual assault survivors, physical assault survivors, individuals kept in police custody and undocumented migrants for age estimation. A trial version of a software relying on the data model was developed and tested by three physicians.
Discussion
The software allows files archiving, standardised data collection, extraction and assistance for certificate generation. It can be used for research purpose, by data exchange and analysis. Despite some current limitations of use, it is a tool which can be shared and used by other departments of forensic medicine and other specialties, improving data management and exploitation. Full integration with external sources, analytics software and use of a semantic interoperability framework are planned for the next months.}
}
@article{HARRISON2019105058,
title = {EcoHealth and One Health: A theory-focused review in response to calls for convergence},
journal = {Environment International},
volume = {132},
pages = {105058},
year = {2019},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2019.105058},
url = {https://www.sciencedirect.com/science/article/pii/S0160412019305409},
author = {Sarah Harrison and Lucy Kivuti-Bitok and Alexandra Macmillan and Patricia Priest},
keywords = {EcoHealth, One Health, Theoretical approaches, Critical realism, Systems thinking, System dynamics modelling},
abstract = {Background
EcoHealth and One Health are two major approaches broadly aimed at understanding the links between human, animal, and environment health. There have been increasing calls for convergence between the two. If convergence is desired, greater clarity regarding the underlying theoretical assumptions of both approaches is required. This would also support integrated research to effectively address complex health issues at the human, animal and environment interface. To better understand the areas of overlap and alignment, we systematically compared and contrasted the theoretical assumptions of both approaches.
Objectives
We aimed to gain a more in-depth understanding of the ontological, epistemological and methodological underpinnings of EcoHealth and One Health in order to identify areas of difference and overlap, and consider the extent to which closer convergence between the two may be possible.
Methods
We undertook a scoping review of literature about the ontological, epistemological and methodological positions of EcoHealth and One Health, and analyzed these according to Lincoln, Lynham and Guba's paradigm framework.
Results
EcoHealth and One Health are both collaborative, systems-focused approaches at the human, animal, and ecosystem health interface. EcoHealth typically leans towards constructivist-leaning assumptions. Many consider this a necessary aspiration for One Health. However, in practice One Health remains dominated by the veterinary and medical disciplines that emphasize positivist-leaning assumptions.
Discussion
The aspirations of EcoHealth and One Health appear to overlap at the conceptual level, and may well warrant closer convergence. However, further shared discussions about their epistemological and ontological assumptions are needed to reconcile important theoretical differences, and to better guide scopes of practice. Critical realism may be a crucial theoretical meeting point. Systems thinking methods (with critical realist underpinnings), such as system dynamics modelling, are potentially useful methodologies for supporting convergent practice.}
}
@article{SUNG2021337,
title = {Applications of Semantic Web in integrating open data and bibliographic records: a development example of an infomediary of Taiwanese indigenous people},
journal = {The Electronic Library},
volume = {39},
number = {2},
pages = {337-353},
year = {2021},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-09-2020-0258},
url = {https://www.sciencedirect.com/science/article/pii/S0264047321000278},
author = {Han-Yu Sung and Yu-Liang Chi},
keywords = {Open data, Semantic Web, Linked data, Indigenous people},
abstract = {Purpose
This study aims to develop a Web-based application system called Infomediary of Taiwanese Indigenous Peoples (ITIP) that can help individuals comprehend the society and culture of indigenous people. The ITIP is based on the use of Semantic Web technologies to integrate a number of data sources, particularly including the bibliographic records of a museum. Moreover, an ontology model was developed to help users search cultural collections by topic concepts.
Design/methodology/approach
Two issues were identified that needed to be addressed: the integration of heterogeneous data sources and semantic-based information retrieval. Two corresponding methods were proposed: SPARQL federated queries were designed for data integration across the Web and ontology-driven queries were designed to semantically search by knowledge inference. Furthermore, to help users perform searches easily, three searching interfaces, namely, ethnicity, region and topic, were developed to take full advantage of the content available on the Web.
Findings
Most open government data provides structured but non-resource description framework data, Semantic Web consumers, therefore, require additional data conversion before the data can be used. On the other hand, although the library, archive and museum (LAM) community has produced some emerging linked data, very few data sets are released to the general public as open data. The Semantic Web’s vision of “web of data” remains challenging.
Originality/value
This study developed data integration from various institutions, including those of the LAM community. The development was conducted based on the mode of non-institution members (i.e. institutional outsiders). The challenges encountered included uncertain data quality and the absence of institutional participation.}
}
@article{RAMSTEAD20181,
title = {Answering Schrödinger's question: A free-energy formulation},
journal = {Physics of Life Reviews},
volume = {24},
pages = {1-16},
year = {2018},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2017.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1571064517301409},
author = {Maxwell James Désormeau Ramstead and Paul Benjamin Badcock and Karl John Friston},
keywords = {Free energy principle, Complex adaptive systems, Evolutionary systems theory, Hierarchically mechanistic mind, Physics of the mind, Variational neuroethology},
abstract = {The free-energy principle (FEP) is a formal model of neuronal processes that is widely recognised in neuroscience as a unifying theory of the brain and biobehaviour. More recently, however, it has been extended beyond the brain to explain the dynamics of living systems, and their unique capacity to avoid decay. The aim of this review is to synthesise these advances with a meta-theoretical ontology of biological systems called variational neuroethology, which integrates the FEP with Tinbergen's four research questions to explain biological systems across spatial and temporal scales. We exemplify this framework by applying it to Homo sapiens, before translating variational neuroethology into a systematic research heuristic that supplies the biological, cognitive, and social sciences with a computationally tractable guide to discovery.}
}
@article{RIHM2024100135,
title = {The digital lab manager: Automating research support},
journal = {SLAS Technology},
volume = {29},
number = {3},
pages = {100135},
year = {2024},
issn = {2472-6303},
doi = {https://doi.org/10.1016/j.slast.2024.100135},
url = {https://www.sciencedirect.com/science/article/pii/S2472630324000177},
author = {Simon D. Rihm and Yong Ren Tan and Wilson Ang and Markus Hofmeister and Xinhong Deng and Michael Teguh Laksana and Hou Yee Quek and Jiaru Bai and Laura Pascazio and Sim Chun Siong and Jethro Akroyd and Sebastian Mosbach and Markus Kraft},
keywords = {Laboratory automation, Lab management, LIMS, RFID, Asset tracking, Dynamic knowledge graphs},
abstract = {Laboratory management automation is essential for achieving interoperability in the domain of experimental research and accelerating scientific discovery. The integration of resources and the sharing of knowledge across organisations enable scientific discoveries to be accelerated by increasing the productivity of laboratories, optimising funding efficiency, and addressing emerging global challenges. This paper presents a novel framework for digitalising and automating the administration of research laboratories through The World Avatar, an all-encompassing dynamic knowledge graph. This Digital Laboratory Framework serves as a flexible tool, enabling users to efficiently leverage data from diverse systems and formats without being confined to a specific software or protocol. Establishing dedicated ontologies and agents and combining them with technologies such as QR codes, RFID tags, and mobile apps, enabled us to develop modular applications that tackle some key challenges related to lab management. Here, we showcase an automated tracking and intervention system for explosive chemicals as well as an easy-to-use mobile application for asset management and information retrieval. Implementing these, we have achieved semantic linking of BIM and BMS data with laboratory inventory and chemical knowledge. Our approach can capture the crucial data points and reduce inventory processing time. All data provenance is recorded following the FAIR principles, ensuring its accessibility and interoperability.}
}
@article{BROEKHUIS2021,
title = {Conceptualizing Usability for the eHealth Context: Content Analysis of Usability Problems of eHealth Applications},
journal = {JMIR Formative Research},
volume = {5},
number = {7},
year = {2021},
issn = {2561-326X},
doi = {https://doi.org/10.2196/18198},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X21003504},
author = {Marijke Broekhuis and Lex {van Velsen} and Linda Peute and Meilani Halim and Hermie Hermens},
keywords = {usability benchmarking, eHealth systems, content analysis, usability framework, summative evaluation, mobile phone},
abstract = {Background
Usability tests can be either formative (where the aim is to detect usability problems) or summative (where the aim is to benchmark usability). There are ample formative methods that consider user characteristics and contexts (ie, cognitive walkthroughs, interviews, and verbal protocols). This is especially valuable for eHealth applications, as health conditions can influence user-system interactions. However, most summative usability tests do not consider eHealth-specific factors that could potentially affect the usability of a system. One of the reasons for this is the lack of fine-grained frameworks or models of usability factors that are unique to the eHealth domain.
Objective
In this study, we aim to develop an ontology of usability problems, specifically for eHealth applications, with patients as primary end users.
Methods
We analyzed 8 data sets containing the results of 8 formative usability tests for eHealth applications. These data sets contained 400 usability problems that could be used for analysis. Both inductive and deductive coding were used to create an ontology from 6 data sets, and 2 data sets were used to validate the framework by assessing the intercoder agreement.
Results
We identified 8 main categories of usability factors, including basic system performance, task-technology fit, accessibility, interface design, navigation and structure, information and terminology, guidance and support, and satisfaction. These 8 categories contained a total of 21 factors: 14 general usability factors and 7 eHealth-specific factors. Cohen κ was calculated for 2 data sets on both the category and factor levels, and all Cohen κ values were between 0.62 and 0.67, which is acceptable. Descriptive analysis revealed that approximately 69.5% (278/400) of the usability problems can be considered as general usability factors and 30.5% (122/400) as eHealth-specific usability factors.
Conclusions
Our ontology provides a detailed overview of the usability factors for eHealth applications. Current usability benchmarking instruments include only a subset of the factors that emerged from our study and are therefore not fully suited for summative evaluations of eHealth applications. Our findings support the development of new usability benchmarking tools for the eHealth domain.}
}
@article{VILA2022101699,
title = {Edge-to-cloud sensing and actuation semantics in the industrial Internet of Things},
journal = {Pervasive and Mobile Computing},
volume = {87},
pages = {101699},
year = {2022},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2022.101699},
url = {https://www.sciencedirect.com/science/article/pii/S1574119222001122},
author = {Marc Vila and Víctor Casamayor and Schahram Dustdar and Ernest Teniente},
keywords = {Industrial Internet of Things, Interoperability, Computing Continuum, Context-awareness, Semantics, Autonomous cars},
abstract = {There are billions of devices worldwide deployed, connected, and communicating to other systems. Sensors and actuators, which can be stationary or movable devices. These Edge devices are considered part of the Internet of Things (IoT) devices, which can be referred to as a tier of the Computing Continuum paradigm. There are two main concerns at stake in the success of this ecosystem. The interoperability between devices and systems is the first. Mainly, because most of them communicate uniquely and differently from each other, leading to heterogeneous data. The second issue is the lack of decision-making capacity to conduct actuations, such as communicating through different computing tiers based on latency constraints due to a certain measured factor. In this article, we propose an ontology to improve device interoperability in the IoT. In addition, we also explain how to ease data communication between Computing Continuum devices, providing tools to enhance data management and decision-making. A use case is also presented, using the automotive industry, where quickness in maneuver determination is key to avoid accidents. It is exemplified using two Raspberry Pi devices, connected using different networks and choosing the appropriate one depending on context-aware conditions.}
}
@article{QIU2025100588,
title = {Understanding the comorbidities among psychiatric disorders, chronic low-back pain, and spinal degenerative disease using observational and genetically informed analyses},
journal = {Biological Psychiatry Global Open Science},
pages = {100588},
year = {2025},
issn = {2667-1743},
doi = {https://doi.org/10.1016/j.bpsgos.2025.100588},
url = {https://www.sciencedirect.com/science/article/pii/S2667174325001429},
author = {Dan Qiu and Eleni Friligkou and Jun He and Brenda Cabrera-Mendoza and Mihaela Aslan and Mihir Gupta and Renato Polimanti},
keywords = {Mental Health, Polygenic Risk, Shared Pathogenesis, Musculoskeletal Disorders},
abstract = {Background
Psychiatric disorders and symptoms are associated with differences in pain perception and sensitivity. These differences can have important implications in treating spinal degenerative disease (SDD) and chronic low-back pain (CLBP).
Methods
Leveraging UK Biobank (UKB, N=402,072) and All of Us Research Program (AoU, N=157,415), we investigated the effects linking psychiatric disorders to SDD and CLBP. We applied multi-nominal regression models, polygenic risk scoring (PRS), and one-sample Mendelian randomization (MR) to triangulate the effects underlying the associations observed. We also performed gene ontology and drug-repurposing analyses to dissect the biology shared among mental illnesses, SDD, and CLBP.
Results
Comparing individuals affected only by SDD, those affected only by CLBP, and those affected by both conditions to controls, observational and genetically informed analyses highlighted that the strongest effects across the three case groups were observed for alcohol use disorder, anxiety, depression, and posttraumatic stress disorder. Additionally, schizophrenia and its PRS appeared to have an inverse relationship with CLBP, SDD, and their comorbidity. One-sample MR highlighted a potential direct effect of internalizing disorders on the outcomes investigated that was particularly strong on SDD. Our drug-repurposing analyses identified histone deacetylase inhibitors as targeting molecular pathways shared among psychiatric disorders, SDD, and CLBP.
Conclusions
These findings support that the comorbidity among psychiatric disorders, SDD, and CLBP is due to the contribution of direct effects and shared biology linking these health outcomes. These pleiotropic mechanisms, together with sociocultural factors, play a key role in shaping the SDD-CLBP comorbidity patterns observed across the psychopathology spectrum.}
}
@article{KUGIC2024,
title = {Processing of Short-Form Content in Clinical Narratives: Systematic Scoping Review},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/57852},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124006009},
author = {Amila Kugic and Ingrid Martin and Luise Modersohn and Peter Pallaoro and Markus Kreuzthaler and Stefan Schulz and Martin Boeker},
keywords = {electronic health records, EHR, clinical narratives, natural language processing, machine learning, deep learning, rule-based approach, short-form expression, disambiguation, word embedding, vector representations, language modeling, human-in-the-loop, feature extraction},
abstract = {Background
Clinical narratives are essential components of electronic health records. The adoption of electronic health records has increased documentation time for hospital staff, leading to the use of abbreviations and acronyms more frequently. This brevity can potentially hinder comprehension for both professionals and patients.
Objective
This review aims to provide an overview of the types of short forms found in clinical narratives, as well as the natural language processing (NLP) techniques used for their identification, expansion, and disambiguation.
Methods
In the databases Web of Science, Embase, MEDLINE, EBMR (Evidence-Based Medicine Reviews), and ACL Anthology, publications that met the inclusion criteria were searched according to PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines for a systematic scoping review. Original, peer-reviewed publications focusing on short-form processing in human clinical narratives were included, covering the period from January 2018 to February 2023. Short-form types were extracted, and multidimensional research methodologies were assigned to each target objective (identification, expansion, and disambiguation). NLP study recommendations and study characteristics were systematically assigned occurrence rates for evaluation.
Results
Out of a total of 6639 records, only 19 articles were included in the final analysis. Rule-based approaches were predominantly used for identifying short forms, while string similarity and vector representations were applied for expansion. Embeddings and deep learning approaches were used for disambiguation.
Conclusions
The scope and types of what constitutes a clinical short form were often not explicitly defined by the authors. This lack of definition poses challenges for reproducibility and for determining whether specific methodologies are suitable for different types of short forms. Analysis of a subset of NLP recommendations for assessing quality and reproducibility revealed only partial adherence to these recommendations. Single-character abbreviations were underrepresented in studies on clinical narrative processing, as were investigations in languages other than English. Future research should focus on these 2 areas, and each paper should include descriptions of the types of content analyzed.}
}
@article{XING20252275,
title = {Multi-Head Encoder Shared Model Integrating Intent and Emotion for Dialogue Summarization},
journal = {Computers, Materials and Continua},
volume = {82},
number = {2},
pages = {2275-2292},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.056877},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825001596},
author = {Xinlai Xing and Junliang Chen and Xiaochuan Zhang and Shuran Zhou and Runqing Zhang},
keywords = {Dialogue summaries, dialogue state tracking, emotion recognition, task-oriented dialogue system, pre-trained language model},
abstract = {In task-oriented dialogue systems, intent, emotion, and actions are crucial elements of user activity. Analyzing the relationships among these elements to control and manage task-oriented dialogue systems is a challenging task. However, previous work has primarily focused on the independent recognition of user intent and emotion, making it difficult to simultaneously track both aspects in the dialogue tracking module and to effectively utilize user emotions in subsequent dialogue strategies. We propose a Multi-Head Encoder Shared Model (MESM) that dynamically integrates features from emotion and intent encoders through a feature fusioner. Addressing the scarcity of datasets containing both emotion and intent labels, we designed a multi-dataset learning approach enabling the model to generate dialogue summaries encompassing both user intent and emotion. Experiments conducted on the MultiWoZ and MELD datasets demonstrate that our model effectively captures user intent and emotion, achieving extremely competitive results in dialogue state tracking tasks.}
}
@article{BEARD2025145240,
title = {Which sustainable business model archetypes are more prevalent? An analysis of small and medium-sized enterprises},
journal = {Journal of Cleaner Production},
volume = {504},
pages = {145240},
year = {2025},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2025.145240},
url = {https://www.sciencedirect.com/science/article/pii/S0959652625005906},
author = {Maya Nikki Beard and Maria Rosa {De Giacomo}},
abstract = {There is still limited knowledge of business model archetypes for sustainability in small and medium-sized companies. This study aims to fill that gap by investigating empirically the sustainable business model archetypes adopted by small and medium-sized companies. An analysis of general company information and communication material data, consisting of searching for specific keywords frequency related to nine sustainable business model archetypes, has been conducted. With data over three years, the study finds that small and medium-sized companies prefer an economic archetype (repurpose for society and the environment) compared to an environmental one (creating value from waste) or to a social one across all three years. It also finds that social and environmental sustainability is an increasing trend for those enterprises. The study also considers potential drivers and barriers for small and medium-sized enterprises in adopting sustainable business models. This paper is expected to generate insights into the internal shift toward sustainable business models in small and medium-sized enterprises and contribute to the literature on sustainable business model archetypes and the role small and medium sized enterprises may have in the sustainability transition.}
}
@article{NIMMAGADDA20191198,
title = {Design Science Information System Framework for Managing the Articulations of Digital Agroecosystems},
journal = {Procedia Computer Science},
volume = {159},
pages = {1198-1207},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.289},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314875},
author = {Shastri L Nimmagadda and Amboge Samson and Neel Mani and Torsten Reiners},
keywords = {Design Science Research, Information System, Data Artefacts, Digital Agroecosystems},
abstract = {Agriculture industries and their business ecosystems experience data and information overload because of complex network or interconnected domains linked to a variety of agro-based systems. Data search becomes tedious when specific queries are made to support crucial technical and financial decisions by agroecosystem service providers. Due to accumulated volumes of heterogeneous data and information in multiple primary sources, websites and company servers, the agriculture industry needs a robust and flexible digital agroecosystem development. To address the major challenges, a Design Science Research (DSR) approach is adopted, articulating systematic data mapping workflows and integrating their data structures in different knowledge domains. Purpose of the research is aimed at designing and developing an ontology-based data warehousing framework, with comprehensive multidimensional ontologies that motivated us to present various data modelling architectures in different knowledge-based domain applications. An emphasis is given to spatial-temporal dimensions in the modelling process that affect the structuring of data relationships in large geographic regions, which are typical in the agro-business environment.}
}
@article{ROBSON2022105323,
title = {Principles of Quantum Mechanics for Artificial Intelligence in medicine. Discussion with reference to the Quantum Universal Exchange Language (Q-UEL)},
journal = {Computers in Biology and Medicine},
volume = {143},
pages = {105323},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105323},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522001159},
author = {Barry Robson and Jim {St. Clair}},
keywords = {Quantum mechanics, Quantum computing, Artificial intelligence, Clinical decision support, Hyperbolic complex, Split-complex},
abstract = {This paper reviews some basic principles of Quantum Mechanics, Quantum Computing, and Artificial Intelligence in terms of a specific unifying theme. This theme relates to the hyperbolic or split-complex imaginary numbers and their equivalent matrices, rediscovered by Dirac, and the underlying mathematics of the previously described Q-UEL language based on them. Hyperbolic imaginary numbers h have the property hh = +1: contrast the more familiar i such that ii = −1. Examples of analogous matrices include that for the Hadamard gate as used in quantum computing and the Pauli spin matrices, and all Hermitian matrices of interest in quantum computing can readily be derived from these. They also relate to Dirac dualization, spinor projectors of Quantum Field Theory, the non-wave-like part of quantum theory, collapse of the wave function, and a dualized form of classical probability theory that has advantages in automated reasoning for medicine.}
}
@article{WANG2025110977,
title = {Deep learning methods for protein representation and function prediction: A comprehensive overview},
journal = {Engineering Applications of Artificial Intelligence},
volume = {155},
pages = {110977},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110977},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625009777},
author = {Mingqing Wang and Zhiwei Nie and Yonghong He and Athanasios V. Vasilakos and Qiang (Shawn) Cheng and Zhixiang Ren},
keywords = {Protein function prediction, Protein representation, Deep learning, Representation learning, Protein interaction},
abstract = {Deep learning has revolutionized protein function prediction by capturing intricate protein relationships, yet a comprehensive survey of its methodologies remains elusive. In this review, we systematically dissect recent advances by addressing three pivotal questions: (a) which modalities are most critical for various function prediction tasks, (b) which deep learning strategies optimally model these modalities, (c) what common and task-specific challenges persist. We categorize protein data into eight distinct types – from fundamental representations to specialized expert knowledge – and provide an exhaustive analysis of state-of-the-art deep learning models alongside emerging self-supervised learning strategies. Moreover, we compare the evolution of architectures across different modeling paradigms, highlighting their respective strengths and limitations. Our investigation spans over fifteen downstream tasks across five key research areas, including protein function annotation, protein–protein interactions, protein–ligand interactions, mutation effect prediction, and remote homology detection. Finally, we discuss current challenges and propose potential solutions, offering strategic guidance for data selection, methodological innovation, and future research directions in the application of deep learning to protein function prediction.}
}
@article{FUELLEN2025102777,
title = {In-silico evaluation of aging-related interventions using omics data and predictive modeling},
journal = {Ageing Research Reviews},
volume = {110},
pages = {102777},
year = {2025},
issn = {1568-1637},
doi = {https://doi.org/10.1016/j.arr.2025.102777},
url = {https://www.sciencedirect.com/science/article/pii/S1568163725001230},
author = {Georg Fuellen and Daniel Palmer and Claudia Fruijtier and Roberto A. Avelar},
keywords = {Longevity interventions, Gene expression, Toxicity/safety, Predictive modeling, Interpretable features},
abstract = {A major challenge in aging research is identifying interventions that can improve lifespan and health and minimize toxicity. Clinical studies cannot usually consider decades-long follow-up periods, and therefore, in-silico evaluations using omics-based surrogate biomarkers are emerging as key tools. However, many current approaches train predictive models on observational data, rather than on intervention data, which can lead to biased conclusions. Yet, the first classifiers for lifespan extension by compounds are now available, learned on intervention data. Here, we review evaluation methodologies and we prioritize training on intervention data whenever available, highlight the importance of safety and toxicity assessments, discuss the role of standardized benchmarks, and present a range of feature processing and predictive modeling approaches. We consider linear and non-linear methods, automated machine learning workflows, and use of AI. We conclude by emphasizing the need for explainable and reproducible strategies, the integration of safety metrics, and the careful validation of predictors based on interventional benchmarks.}
}
@article{ALMUSTANJID2022101003,
title = {Systems biology models to identify the influence of SARS-CoV-2 infections to the progression of human autoimmune diseases},
journal = {Informatics in Medicine Unlocked},
volume = {32},
pages = {101003},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101003},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822001460},
author = {Md Al-Mustanjid and S. M. Hasan Mahmud and Farzana Akter and Md Shazzadur Rahman and Md Sajid Hossen and Md Habibur Rahman and Mohammad Ali Moni},
keywords = {COVID-19, SARS-CoV-2, Autoimmune diseases, Transcriptomic analysis, DEGs, Protein-protein interaction network (PPIN), Hub gene, Drug repurposing},
abstract = {Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has been circulating since 2019, and its global dominance is rising. Evidences suggest the respiratory illness SARS-CoV-2 has a sensitive affect on causing organ damage and other complications to the patients with autoimmune diseases (AD), posing a significant risk factor. The genetic interrelationships and molecular appearances between SARS-CoV-2 and AD are yet unknown. We carried out the transcriptomic analytical framework to delve into the SARS-CoV-2 impacts on AD progression. We analyzed both gene expression microarray and RNA-Seq datasets from SARS-CoV-2 and AD affected tissues. With neighborhood-based benchmarks and multilevel network topology, we obtained dysfunctional signaling and ontological pathways, gene disease (diseasesome) association network and protein-protein interaction network (PPIN), uncovered essential shared infection recurrence connectivities with biological insights underlying between SARS-CoV-2 and AD. We found a total of 77, 21, 9, 54 common DEGs for SARS-CoV-2 and inflammatory bowel disorder (IBD), SARS-CoV-2 and rheumatoid arthritis (RA), SARS-CoV-2 and systemic lupus erythematosus (SLE) and SARS-CoV-2 and type 1 diabetes (T1D). The enclosure of these common DEGs with bimolecular networks revealed 10 hub proteins (FYN, VEGFA, CTNNB1, KDR, STAT1, B2M, CD3G, ITGAV, TGFB3). Drugs such as amlodipine besylate, vorinostat, methylprednisolone, and disulfiram have been identified as a common ground between SARS-CoV-2 and AD from drug repurposing investigation which will stimulate the optimal selection of medications in the battle against this ongoing pandemic triggered by COVID-19.}
}
@article{ADOLPHS2024105645,
title = {Commentary on, “The Human Affectome,” by Schiller et al.},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {160},
pages = {105645},
year = {2024},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105645},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424001143},
author = {Ralph Adolphs},
keywords = {Emotion, Affect},
abstract = {I suggest that this project could benefit from a relational database of some sort to provide readers with a more formal ontology, and that the authors consider making a distinction between experiential and functional aspects of emotion.}
}
@article{XU2024154,
title = {Reconfigurable flexible assembly model and implementation for cross-category products},
journal = {Journal of Manufacturing Systems},
volume = {77},
pages = {154-169},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524001857},
author = {Zhaobo Xu and Chaoran Zhang and Song Hu and Zhaochun Han and Pingfa Feng and Long Zeng},
keywords = {Five-element assembly model, Reconfigurable flexible assembly system, Cross-category products, Knowledge graph, Systematic implementation framework, Assembly decision},
abstract = {As the production orders are becoming multi-category and small-batch in the era of product personalization, these require frequent reconfiguration of reconfigurable flexible assembly system for cross-category products (RFAS-CCP). However, there is no suitable theoretical assembly model and systematic implementation framework. We first propose a five-element assembly model (FAM) for RFAS-CCP, i.e. product, process, resource, knowledge, and decision. The product, process, and resource element describe the objects, steps to be assembled, and the tools, fixtures, and other equipment used for assembly, respectively. The knowledge element is a form representation of various heterogeneous data, such as a knowledge graph. The decision element includes various assembly methods to achieve assembly automation, flexibility, and intelligence. Then, in order to standardize and easy the frequent reconfiguration process, we reorganize various decision methods into a three-phase systematic implementation framework according to which stage they are used: design, configuration, and operation phases. The design phase methods primarily design various assembly modules for a product family, forming an assembly resource library. The configuration phase methods primarily configure suitable assembly lines for a specific product in the product family. The operation phase methods monitor the status of the assembly line and ensures its stable operation through health management. Finally, the effectiveness and practicality of the proposed five-element assembly model and three-phase systematic implementation framework are experimented with a pressure reducing valve product.}
}
@article{KLASSER2023211,
title = {Classification and Diagnosis of Temporomandibular Disorders and Temporomandibular Disorder Pain},
journal = {Dental Clinics of North America},
volume = {67},
number = {2},
pages = {211-225},
year = {2023},
note = {Temporomandibular Disorders: The Current Perspective},
issn = {0011-8532},
doi = {https://doi.org/10.1016/j.cden.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0011853222035273},
author = {Gary D. Klasser and Jean-Paul Goulet and Isabel Moreno-Hay},
keywords = {Classification systems, Diagnostic criteria, Temporomandibular disorders (TMD), Orofacial pain}
}
@article{SU2024e38076,
title = {Supergroup algorithm and knowledge graph construction in museum digital display platform},
journal = {Heliyon},
volume = {10},
number = {19},
pages = {e38076},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e38076},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024141072},
author = {Liping Su and Hongli Liu and Wenru Zhao},
keywords = {Knowledge graph, Museum digital display platform, Supergroup algorithm, K-means algorithm, Cultural relic knowledge extraction},
abstract = {In response to the problems of low entity recognition accuracy, low user satisfaction, and weak interactivity in the construction of knowledge graph for digital display of museum cultural relics, this article studied the application of supergroup algorithms and knowledge graph construction in museum digital display platforms to solve the existing problems. By utilizing the K-means algorithm in the supergroup algorithm to conduct a survey of visitors to Museum A and analyze the behavior of 180 selected visitors, the display effect and audience satisfaction can be improved. Various knowledge graph technologies were utilized to construct a knowledge graph of museum cultural relics. Various knowledge resources in museums were associated and integrated, and through the collection and processing of museum cultural relic data, cultural relic ontology construction and relationship extraction were achieved, providing viewers with richer and more in-depth display content. Through experiments, it was found that the visitor satisfaction rate based on the K-means algorithm was above 92.68 %, and the average visitor satisfaction rate after 10 experiments was 94.25 %. The accuracy, recall, and F1 values of the museum cultural relics knowledge graph studied in this article were 90.12 %, 84.69 %, and 82.23 %, respectively, which were much higher than other types of knowledge graphs. By applying these advanced technologies to the digital display platform of museums, not only can the visitor experience be improved, but also the digitalization process of museums can be promoted, contributing to cultural dissemination and development.}
}
@article{OZTURK2020103122,
title = {Interoperability in building information modeling for AECO/FM industry},
journal = {Automation in Construction},
volume = {113},
pages = {103122},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103122},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519314918},
author = {Gozde Basak Ozturk},
keywords = {Building information modeling (BIM), Interoperability, Scientometric analysis, Scientometric mapping},
abstract = {Data loss and communication problems may arise especially in complex construction projects, which in turn may result in poor project performance. Interoperability smooths communication while increasing information management quality. The aim of this paper is to investigate the interoperability research gaps and trends in Building Information Modeling (BIM) for architecture, engineering, construction, operation, and facility management (AECO/FM). This study adopted a holistic approach with a bibliometric search, a scientometric mapping and analysis of interoperability in BIM research. Data about interoperability in BIM were collected through an examination of 477 articles that were selected from the Scopus database. This study contributes to the body of knowledge by highlighting the research trend and gaps in key areas, and their relationship with interoperability in BIM. As results suggest, future studies should focus on interoperability in BIM to increase knowledge-integrated, ontology-based, IT-based managed, collaborative, automated, well-visualized, and sustainable outcomes throughout the project lifecycle.}
}
@article{DORDI2022132741,
title = {Mapping 70 Years of advancements in management research on sustainability},
journal = {Journal of Cleaner Production},
volume = {365},
pages = {132741},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.132741},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622023393},
author = {Truzaar Dordi and Nicholas Palaschuk},
keywords = {Sustainability management, Grand challenges, Qualitative big data, Research mapping, Burst detection},
abstract = {Sustainability provides a research paradigm that, when embedded into management language, theory, and method, provides a systems approach to drawing interconnections between business activities and grand societal challenges. With mounting evidence detailing the salience of sustainable business transformations, the gap between conventional management research and practice is stark. With little formal discussion on how this gap can be bridged, this study examines the application of sustainability in top business and management literature and details opportunities for synergizing these research agendas to inform future action-oriented research. Examining 46,856 publications across 27 top management journals, this comparative analysis finds that 800 articles (1.71%) speak specifically to sustainability. Adopting the Poisson burst detection algorithm and interdisciplinary mapping techniques, we trace the evolution and positionality of sustainability discourse in management literature. Though nascent, sustainability management distinguishes itself through a lens of pragmatism versus empirics, as well as through discernible theoretical and methodological approaches. We also find that sustainability discourse is growing at a faster rate, with higher aggregate citation counts and with prominence in select sub-disciplines relative to its conventional counterparts. We conclude with a research agenda - for grand societal challenges to become actionable, a new language is required, engaging a range of disciplines, theories, and methodologies in the form of sustainability management.}
}
@article{LI2025742513,
title = {Integrated landscape of accessible regions and transcriptomic profiles in Penaeus monodon during ovarian maturation},
journal = {Aquaculture},
volume = {605},
pages = {742513},
year = {2025},
issn = {0044-8486},
doi = {https://doi.org/10.1016/j.aquaculture.2025.742513},
url = {https://www.sciencedirect.com/science/article/pii/S0044848625003990},
author = {Yundong Li and Jing An and Shigui Jiang and Song Jiang and JianHua Huang and LiShi Yang and Qibin Yang and Jianzhi Shi and Zhenhua Ma and Falin Zhou},
keywords = {Shrimp, Ovary development, Eyestalk ablation, RNA-seq, Regulatory elements, ATAC-seq},
abstract = {The ovary maturation process poses a significant constraint on the commercial production and breeding of Penaeus monodon, one of the most extensively cultivated crustacean species. The reproductive stage is controlled by neuropeptide hormones and various proteins released from secretory sites (X-organ/sinus gland, XO/SG) within the eyestalk. Unilateral eyestalk ablation has been a widely employed method to artificially induce ovarian maturation in farmed P. monodon. To better understand the reproductive regulation mechanism in P. monodon, we have investigated the both the transcriptomes and regulatory mechanisms of the eyestalk ablated ovary developmental stages with RNA-seq and ATACseq. For RNA-seq data, a total of 182.29 G of clean data was obtained, with the effective data distribution for each sample ranging from 6.42 to 7.04 G. Meanwhile, the differentially expressed genes (DEGs) between ovarian development stages were identified. We examined, through DEG enrichment analysis, eyestalk gene expression patterns for Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathways. We also identified a variety of accessible regions that appear to be differentially expressed throughout ovarian maturation. These include Motifs that encode bZIP and C2H2 transciption factor families. Furthermore, by integrating DEGs and differential accessible regions, we found HIF-signaling pathway, AGE-RAGE signaling pathway, ABC transporters pathways, were involved during ovarian maturation. Our study offers new insights into the role of the eyestalk in the regulation of reproduction, providing a basis for selective breeding and exploring alternative methods to induce ovary maturation.}
}
@article{DING2024100034,
title = {Understanding molecular characteristics of extracellular vesicles derived from different types of mesenchymal stem cells for therapeutic translation},
journal = {Extracellular Vesicle},
volume = {3},
pages = {100034},
year = {2024},
issn = {2773-0417},
doi = {https://doi.org/10.1016/j.vesic.2024.100034},
url = {https://www.sciencedirect.com/science/article/pii/S2773041724000015},
author = {Zuo Ding and Zachary F. Greenberg and Maria Fernanda Serafim and Samantha Ali and Julia C. Jamieson and Dmitry O. Traktuev and Keith March and Mei He},
keywords = {Extracellular Vesicles, Mesenchymal Stem Cells, Molecular Characteristics, Therapeutic Translation},
abstract = {Mesenchymal stem cells (MSCs) have been studied for decades as candidates for cellular therapy, and their secretome, including secreted extracellular vesicles (EVs), has been identified to contribute significantly to regenerative and reparative functions. Emerging evidence has suggested that MSC-EVs alone, could be used as therapeutics that emulate the biological function of MSCs. However, just as with MSCs, MSC-EVs have been shown to vary in composition, depending on the tissue source of the MSCs as well as the protocols employed in culturing the MSCs and obtaining the EVs. Therefore, the importance of careful choice of cell sources and culture environments is receiving increasing attention. Many factors contribute to the therapeutic potential of MSC-EVs, including the source tissue, isolation technique, and culturing conditions. This review illustrates the molecular landscape of EVs derived from different types of MSC cells along with culture strategies. A thorough analysis of publicly available omic datasets was performed to advance the precision understanding of MSC-EVs with unique tissue source-dependent molecular characteristics. The tissue-specific protein and miRNA-driven Reactome ontology analysis was used to reveal distinct patterns of top Reactome ontology pathways across adipose, bone marrow, and umbilical MSC-EVs. Moreover, a meta-analysis assisted by an AI technique was used to analyze the published literature, providing insights into the therapeutic translation of MSC-EVs based on their source tissues.}
}
@article{DAI2024104744,
title = {MultiADE: A Multi-domain benchmark for Adverse Drug Event extraction},
journal = {Journal of Biomedical Informatics},
volume = {160},
pages = {104744},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104744},
url = {https://www.sciencedirect.com/science/article/pii/S153204642400162X},
author = {Xiang Dai and Sarvnaz Karimi and Abeed Sarker and Ben Hachey and Cecile Paris},
keywords = {Adverse drug event, Drug safety, Natural language processing, Information extraction, Named entity recognition},
abstract = {Objective:
Active adverse event surveillance monitors Adverse Drug Events (ADE) from different data sources, such as electronic health records, medical literature, social media and search engine logs. Over the years, many datasets have been created, and shared tasks have been organised to facilitate active adverse event surveillance. However, most – if not all – datasets or shared tasks focus on extracting ADEs from a particular type of text. Domain generalisation – the ability of a machine learning model to perform well on new, unseen domains (text types) – is under-explored. Given the rapid advancements in natural language processing, one unanswered question is how far we are from having a single ADE extraction model that is effective on various types of text, such as scientific literature and social media posts.
Methods:
We contribute to answering this question by building a multi-domain benchmark for adverse drug event extraction, which we named MultiADE. The new benchmark comprises several existing datasets sampled from different text types and our newly created dataset—CADECv2, which is an extension of CADEC (Karimi et al., 2015), covering online posts regarding more diverse drugs than CADEC. Our new dataset is carefully annotated by human annotators following detailed annotation guidelines.
Conclusion:
Our benchmark results show that the generalisation of the trained models is far from perfect, making it infeasible to be deployed to process different types of text. In addition, although intermediate transfer learning is a promising approach to utilising existing resources, further investigation is needed on methods of domain adaptation, particularly cost-effective methods to select useful training instances. The newly created CADECv2 and the scripts for building the benchmark are publicly available at CSIRO’s Data Portal (https://data.csiro.au/collection/csiro:62387). These resources enable the research community to further information extraction, leading to more effective active adverse drug event surveillance.}
}
@article{WU2022105215,
title = {LinkClimate: An interoperable knowledge graph platform for climate data},
journal = {Computers & Geosciences},
volume = {169},
pages = {105215},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2022.105215},
url = {https://www.sciencedirect.com/science/article/pii/S0098300422001649},
author = {Jiantao Wu and Fabrizio Orlandi and Declan O’Sullivan and Soumyabrata Dev},
keywords = {Knowledge graph, Climate data, Ontology, Linked data, SPARQL, Climate change},
abstract = {Climate science has become more ambitious in recent years as global awareness about the environment has grown. To better understand climate, historical climate(e.g. archived meteorological variables such as temperature, wind, water, etc.) and climate-related data (e.g. geographical features and human activities) are widely used by today’s climate research to derive models for an explainable climate change and its effects. However, such data sources are often dispersed across a multitude of disconnected data silos on the Web. Moreover, there is a lack of advanced climate data platforms to enable multi-source heterogeneous climate data analysis, therefore, researchers must face a stern challenge in collecting and analyzing multi-source data. In this paper, we address this problem by proposing a climate knowledge graph for the integration of multiple climate data and other data sources into one service, leveraging Web technologies (e.g. HTTP) for multi-source climate data analysis. The proposed knowledge graph is primarily composed of data from the National Oceanic and Atmospheric Administration’s daily climate summaries, OpenStreetMap, and Wikidata, and it supports joint data queries on these widely used databases. This paper shows, with a use case in Ireland and the United Kingdom, how climate researchers could benefit from this platform as it allows them to easily integrate datasets from different domains and geographical locations.}
}
@article{NOEL2023179,
title = {Pluriversal Futures for Design Education},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {9},
number = {2},
pages = {179-196},
year = {2023},
note = {The Future of Design Education: Rethinking Design Education for the 21st Century},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2023.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872623000370},
author = {Lesley-Ann Noel and Adolfo Ruiz and Frederick M.C. {van Amstel} and Victor Udoewa and Neeta Verma and Nii Kommey Botchway and Arvind Lodaya and Shalini Agrawal},
keywords = {Pluriverse, Pluriversal design, Ontological design, Relationality, Lived experience, Decolonizing design},
abstract = {The Future of Design Education working group on pluriversal design—with members from Latin America, the Caribbean, Africa, South and Southeastern Asia, North America, Oceania, and Europe—developed recommendations for higher education design curricula. The group addresses the dominance of a Eurocentric design canon and worldwide colonization by a twentieth-century design monoculture grounded in the concept of universal human experience. Curricular recommendations honor Indigenous worlds and place-based ways of being, and chime with anthropologist Arturo Escobar’s premise that every community practices the design of itself, through participatory processes that are independent of experts. The authors posit that rather than a Cartesian rationalist perspective, the group advocates a relational view of situations in which the design responses to interdependent natural, social, economic, and technical systems, are specific to places and cultures. The recommendations assert a pluriversal design imperative in which multiple worldviews thrive and diverse lived experiences inform the entire field, as well as individual projects.}
}
@article{CHIA2025100468,
title = {A design-based approach to analysing student engagement with a GenAI-Enabled brainstorming app},
journal = {Computers and Education: Artificial Intelligence},
volume = {9},
pages = {100468},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100468},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25001080},
author = {Joanne Chia and Angela Frattarola},
keywords = {GenAI for education, Writing assistant apps, Customised apps, Personalization in technology, Student engagement, Human-AI interactions, Design thinking, Data storytelling, Technological use and innovation in institutes of higher learning, Faculty and student collaborations},
abstract = {While there are several “writing buddy” Generative Artificial Intelligence (GenAI) apps that check grammar and language usage, not many focus exclusively on enhancing the brainstorming process for writing across disciplines. To fill this gap, a team of staff and student assistants with programming and User Interface (UI) and User Experience (UX) expertise designed and prototyped a web app named “Waai,” which rhymes with ‘why,’ that could assist students throughout the writing process for a first-year general writing module for all undergraduate students at a Singaporean university. Utilising surveys, focus group discussions, and app data that shows the nature and type of student engagement with the Waai app, this paper studies the impact of one aspect of the Waai app, an uni-directional chatbot named “Nudgy,” as a first step to optimising interactions with an AI chatbot for writing purposes. Overall, we found that students were able to benefit from the GenAI chatbot Nudgy in 5 distinct ways: 1) its pre-engineered prompts, which were tailored to the course assignment rubrics; 2) its tendency to recommend topics to research rather than give students answers; 3) its suggested research topics, which helped students to consider different perspectives on their topics; 4) how it modelled ways to ideate new insights; and 5) its constant availability. Students, however, expressed reservations about the Nudgy, particularly in terms of: 1) the limitations of pre-engineered prompts within the app; 2) difficulty in discerning the most relevant of the Nudgy feedback; 3) mistrust in GenAI and Aigiarism; and 4) a recognition of the limitations of GenAI in supporting argumentative writing. “Waai” essentially presents a decision-making framework for brainstorming based on cognitive socialisation, a method of learning that emphasises inductive as opposed to deductive experience that could be applied to online environments, as an ideology of learning that considers, among other aspects, the development of selfhood, where learning is both guided and mediated (Kesebir & Gardner, 2010). In the context of asynchronous learning, meaning is not intrinsic but rather picked up through interactions on online platforms. Interacting with a chatbot with pre-designed prompts result in a ritual that both define and explore the limits of knowledge building. Symbolic interactionism (Aksan et al., 2009) through the medium of technology is a key objective of 21st century education, where ‘learning’ is internalised as an individual experience. Waai offers educatros additional understanding of the effects of personalization (Mygland et al., 2021) as proposed by this design-based study of the role of instruction in the creation of online ‘learning’ experiences. The centrality of instruction and standards of reasoning through the process of brainstorming suggests that the developmental stages of ‘learning’ concepts could empower a process for self-regulation (Zimmerman, 1989) that goes beyond immediate causes and effects to inspire a spiral of reflection and change essential to ideation.}
}
@article{TIAN2025100810,
title = {From past to present: A survey of malicious URL detection techniques, datasets and code repositories},
journal = {Computer Science Review},
volume = {58},
pages = {100810},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100810},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000863},
author = {Ye Tian and Yanqiu Yu and Jianguo Sun and Yanbin Wang},
keywords = {Malicious URL detection, Algorithm benchmarking, URL datasets, Detection taxonomy, Open-source tools},
abstract = {Malicious URLs persistently threaten the cybersecurity ecosystem, by either deceiving users into divulging private data or distributing harmful payloads to infiltrate host systems. The detection of malicious URLs is a protracted arms race between defenders and attackers. Gaining timely insights into the current state of this ongoing battle holds significant importance. However, existing reviews suffer from four critical limitations: 1) Their reliance on algorithm-centric taxonomies obscures understanding of how detection approaches exploit specific modal information channels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses; 3) No open-source implementations are collected to facilitate benchmarking; 4) Insufficient dataset coverage. This paper presents a comprehensive review of malicious URL detection technologies, systematically analyzing methods from traditional blacklisting to advanced deep learning approaches (e.g., Transformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel modality-based taxonomy that categorizes existing works according to their primary data modalities (e.g., lexical URL features, HTML structure, JavaScript behavior, visual layout).For instance, we group models that parse DOM trees and extract HTML tag paths into the HTML modality, while those using rendered webpage screenshots are classified under the visual modality. This taxonomy reveals how distinct input channels inform model design, offering new perspectives that are obscured in algorithm-based classifications. This hierarchical classification enables both rigorous technical analysis and clear understanding of multimodal information utilization. Furthermore, to establish a profile of accessible datasets and address the lack of standardized benchmarking (where current studies often lack proper baseline comparisons), we curate and analyze two key resources: 1) publicly available datasets (2016–2024), and 2) open-source implementations from published works (2013–2025). To facilitate cross-method comparison and support future benchmarking efforts, we compile a comparative table summarizing key performance metrics (e.g., Accuracy, F1 Score, AUC) as reported in the original works or open-source repositories. While no formal evaluation protocol is proposed, this effort provides a practical reference point and highlights the need for more standardized benchmarking practices in malicious URL detection research. The review concludes by examining emerging challenges and proposing actionable directions for future research. We maintain a GitHub repository for ongoing curation of datasets and open-source implementations: https://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master.}
}
@article{CHEN2019368,
title = {Re-examination of a Bioinformatics Database Course: Engaging Blockchain Technology},
journal = {Procedia Computer Science},
volume = {162},
pages = {368-374},
year = {2019},
note = {7th International Conference on Information Technology and Quantitative Management (ITQM 2019): Information technology and quantitative management based on Artificial Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.297},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919320095},
author = {Zhengxin Chen},
keywords = {Blockchain technology, Bioinformatics education, Ontology, Granular aspects},
abstract = {Seven years have passed since publication of our original paper on the development of a database course in bioinformatics curriculum. In this paper, we re-examine this course for its offering during 2005-2017. The course has been developed around ontologies, and student projects have been a very important component in conducting the course. Although this course has made valuable contribution to our bioinformatics program, a lot of improvements can be made. A critical examination of our past experience suggests the need for emphasizing the importance of “think big” at big data era. In particular, it would be beneficial to incorporate blockchain technology to enrich the technical contents of this course. Since there is a close relationship between blockchain technology and ontologies (as enthusiastically emphasized by many researchers), engaging blockchain technology in this course should strengthen ontology-centered bioinformatics database education.}
}
@incollection{WELKER2023219,
title = {Imagining and reimagining focus group research in education},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {219-249},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.11023-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305110231},
author = {Alyson Welker and George Kamberelis},
keywords = {Becomings, Collective conversations, Collective self-inquiry, Data analysis, Data collection, Fabulative conversations, Figured worlds, Focus groups, Indexicality, Mapping, Paratactic commoning, Politics of affirmation, Rhizomatics, Strategic assembly, Transformation},
abstract = {We begin, in this chapter, by providing a brief history of conceptual dimensions of focus group research and how these conceptualizations have informed empirical research in education (and other social sciences). Next, we suggest that recent theoretical and empirical focus group scholarship (e.g., Welker and Kamberelis, 2020) has led to a radical methodological “break” or “turn” in how and why focus groups are defined and practiced. In the wake of this “break” or “turn,” experimental uses of focus groups have induced an ontological/epistemological/axiological cascading effect that has all but forced us to reimagine: (a) what focus group are, (b) how they might be deployed in research, and (c) to what ends. Finally, we engage in some musings about the future, transformative potentials of focus group research.}
}
@article{WANG2021762,
title = {Adapting topic map and social influence to the personalized hybrid recommender system},
journal = {Information Sciences},
volume = {575},
pages = {762-778},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518302718},
author = {Hei-Chia Wang and Hsu-Tung Jhou and Yu-Shan Tsai},
keywords = {Hybrid recommender system, Cold start, Social network, Ontology, Sentiment analysis},
abstract = {A recommender system utilizes information filtering techniques to help users obtain accurate information effectively and efficiently. The existing recommender systems, however, recommend items based on the overall ratings or the click-through rate, and emotions expressed by users are neglected. Conversely, the cold-start problem and low model scalability are the two main problems with recommender systems. The cold-start problem is encountered when the system lacks initial rating. Low model scalability indicates that a model is incapable of coping with high-dimensional data. These two problems may mislead the recommender system, and thus, users will not be satisfied with the recommended items. A hybrid recommender system is proposed to mitigate the negative effects caused by these problems. Additionally, ontologies are applied to integrate the extracted features into topics to reduce dimensionality. Topics mentioned in the items are displayed in the form of a topic map, and users can refer to these similar items for further information.}
}
@article{DEPAULA2020103495,
title = {A competency question-oriented approach for the transformation of semi-structured bioinformatics data into linked open data},
journal = {Engineering Applications of Artificial Intelligence},
volume = {90},
pages = {103495},
year = {2020},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.103495},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620300130},
author = {Gabriel C.S.G. {de Paula} and Cléver R.G. {de Farias}},
keywords = {Semi-structured bioinformatics data, Linked open data, Stepwise transformation approach, Competency questions},
abstract = {Bioinformatics data obtained using different molecular biology techniques must be processed through different analysis tools to discover new biological knowledge. Since plain processed data have no explicit semantic value, the extraction of additional knowledge through data exploration would benefit from the transformation of bioinformatics data into Linked Open Data (LOD). Different approaches have been proposed to support the transformation of different types of biomedical data into LOD. However, these approaches are not flexible enough so they can be easily adapted for the transformation of semi-structured bioinformatics data into LOD. Thus, this paper proposes a novel approach to support such transformation. According to this approach, a set of competency questions drive not only the definition of transformation rules, but also the data transformation and exploration afterwards. The paper also presents a support toolset and describes the successful application of the proposed approach in the functional genomics domain.}
}
@article{GUPTA2020S87,
title = {An Interactive Dialogue Agent to Assist African American and Hispanic/Latino Heart Failure Patients with Self-Care Needs},
journal = {Journal of Cardiac Failure},
volume = {26},
number = {10, Supplement },
pages = {S87-S88},
year = {2020},
note = {Abstracts From the Heart Failure Society of America's (HFSA) Annual Scientific Meeting 2020},
issn = {1071-9164},
doi = {https://doi.org/10.1016/j.cardfail.2020.09.257},
url = {https://www.sciencedirect.com/science/article/pii/S1071916420312045},
author = {Itika Gupta and Devika Salunke and Barbara Di Eugenio and Paula G. Allen-Meares and Carolyn A. Dickens and Olga Garcia-Bedoya and Andrew D. Boyd},
abstract = {Introduction
Heart failure (HF) is a public health problem, and self-care remains poor, especially among minorities. Access to personalized educational material via an interactive dialogue agent (DA), a system intended to converse with humans in natural language, has the potential to improve self-care. This abstract outlines the initial steps of developing an artificial intelligence-based DA to assist African American (AA) and Hispanic/Latino (H/L) HF patients with their self-care needs.
Hypothesis
Analysis of HF education sessions between health educators (HE) and patients can provide insight into the topics that AA and H/L patients value.
Method
In this IRB approved pilot study, we have recorded, transcribed, and verified 20 (18 AA and 2 H/L) HF education sessions between HE and HF patients. Latent Dirichlet Allocation (LDA), a topic modeling algorithm and Empath, a text analysis tool were used to identify common discussion topics in the transcripts. An initiative analysis was performed to identify conversation drivers where each turn (unit of speech by a single speaker, without interruption from the other speaker) was classified as either a question (ending with ‘?’), a prompt (having only filler words like umm, okay) or an assertion/command (others). Lastly, we compared the transcripts against a HF ontology published by the National Center for Biomedical Ontology and the Consumer Health Vocabulary (CHV) available in the Unified Medical Language System (UMLS) to identify the term overlap.
Results
On average, HE took 117 turns comprising 205 sentences and 2281 words per conversation, whereas patients took 108 turns comprising 131 sentences and 850 words. Per conversation, HE asked 26 questions and had 17 prompts as opposed to 3 questions and 39 prompts by the patients. LDA identified HF and heart function, effects of HF, low salt diet, and follow-up appointment and medication as the top 4 most common topics discussed by the HE. When looking at the entire transcripts, Empath identified eating, health, and cooking as the most common topics for both the HE and the patients. Patients frequently discussed children and family, whereas HE focused on providing HF information as indicated by LDA. Lastly, only 2.1% of HE terms overlapped with the HF ontology. Our analysis revealed that 25% and 22% of the terms (without stop words such as at, the) used by the patients and HE respectively match with the ‘preferred label’ in the CHV. For both, the high frequency terms included heart, heart failure, salt, water, and fluid. This also correlated with our topic analysis findings from Empath and LDA.
Conclusion
Our analysis helps triangulate the kind of information HE and HF patients value. Though mostly HE dominated the conversation, it is essential to incorporate the topics that patients brought up into the DA and use culturally sensitive vocabulary while communicating them. Next, we will collect more recordings, conduct focus groups, and evaluate the usability of our prototype DA.}
}
@article{KAPUGAMAGEEGANAGE2021106808,
title = {Semantic-based topic representation using frequent semantic patterns},
journal = {Knowledge-Based Systems},
volume = {216},
pages = {106808},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106808},
url = {https://www.sciencedirect.com/science/article/pii/S095070512100071X},
author = {Dakshi T. {Kapugama Geeganage} and Yue Xu and Yuefeng Li},
keywords = {Topic representation, Semantics, Concepts, Patterns},
abstract = {Topic modeling discovers the hidden topics in a document collection. Most of the existing topic models focus only on word usage and generate the topics based on the word frequency and co-occurrence without considering the meaning of the text. In this paper, we propose a novel approach to generate a semantic pattern-based topic representation based on the meaning of the text to represent the topics in a document collection. The proposed approach considers both the semantics and co-occurrence of words to generate a set of frequent semantic patterns to represent each topic. The semantics are captured by matching the words in each topic with concepts in the Probase ontology. A set of frequent semantic patterns in each topic is generated based on the co-occurrence of the matched words to represent the topic. Hence, our approach differs from traditional topic models because of the meaningful frequent semantic patterns generated based on the ontology. The proposed topic representation was evaluated in terms of topic quality and information filtering performance against a set of state-of-the-art systems. Perplexity, coherence, and topic word distribution were examined in the topic quality evaluation. The generated frequent semantic patterns were used as features for the information filtering evaluation. Our topic representation outperformed in all the evaluations.}
}
@article{ZENG2024123400,
title = {Research on the application of knowledge mapping and knowledge structure construction based on adaptive learning model},
journal = {Expert Systems with Applications},
volume = {249},
pages = {123400},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123400},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424002653},
author = {Xiyin Zeng and Shouqiang Liu},
keywords = {Personalized learing, Pedagogy, Interactive learning environments, Applications},
abstract = {This project has developed a geometry learning software that integrates multiple computer technologies to address the challenges of deep analysis of knowledge points and establishing connections in learning software. The software combines Long Short-Term Memory (LSTM) and Residual Neural Network (ResNet101) to encode text and image features. A self-attention mechanism is used to fuse information from both modalities, enabling decoding of geometric models and classification of corresponding knowledge points.This project uses LSTM and ResNet101 models to extract text and visual features for problem-solving using the Multi Mode Thinking Chain (CoT) method. Classification labels are utilized to generate text responses for problem-solving ideas. Furthermore, a recommendation module is proposed, which combines knowledge tracking and neural collaborative filtering algorithms to capture student behavior and knowledge point vectors. Implicit factors representing students' mastery of different knowledge points are used as inputs in neural collaborative filtering for personalized recommendations. The results demonstrate improvements in accuracy using the ResNet + LSTM multimodal algorithm, achieving a 13 % increase compared to single-modal classification. The multimodal CoT approach also outperforms language models like GPT3.5 and VisualBert by 10 %. Additionally, the combined algorithm of knowledge tracking and neural collaborative filtering shows a 13.3 % higher F1 value compared to ordinary algorithms, confirming the superiority of the adopted method in this project.}
}
@article{EID2024202,
title = {A-MASA: Arabic Multi-Domain Aspect-Based Sentiment Analysis Datasets},
journal = {Procedia Computer Science},
volume = {244},
pages = {202-211},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.193},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924029946},
author = {Yomna Eid and Hala Zayed and Walaa Medhat},
keywords = {Aspects Extraction, Arabic Aspect-Based Sentiment Analysis, Natural language processing, Datasets},
abstract = {The rapid growth of Natural Language Processing (NLP) applications has gained interest in various languages, including Arabic. One critical NLP task is Aspect-Based Sentiment Analysis (ABSA). ABSA involves identifying the sentiment expressed toward specific aspects or attributes of entities mentioned in a piece of text, rather than determining the overall sentiment. Despite the increased use of Semitic languages like Arabic in NLP over the past decade, there remains a lack of resources and datasets in Arabic, particularly in dialectical Arabic. Additionally, existing ABSA datasets are often limited to a single domain, which does not support all types of ABSA, such as multi-domain ABSA, nor do they allow for testing a model's generalization capabilities. This study addresses these limitations by constructing multi-domain ABSA datasets in Arabic for the tasks of aspect extraction and aspect polarity detection. The datasets include both real and synthetic data, covering dialectical and modern standard Arabic, and comprising 6,500 records across five domains. These datasets can be utilized in single-domain, cross-domain, and multi-domain experiments for ABSA. Furthermore, we evaluated the constructed datasets using various transformer-based models and assessed the impact of generating synthetic data on the diversity and balance of the data, as well as on model performance. The results reported in this research demonstrate that the data is valid for use in ABSA tasks. Moreover, the models used for evaluation achieved competitive or superior results compared to existing models on various datasets. Additionally, the annotation guidelines we created are presented as recommended practices applicable to different tasks. All datasets used in this study are publicly available for research purposes.}
}
@article{SCHUK2022100650,
title = {Technical specifications to meet the requirements of an Automatic Code Compliance Checking tool and current developments in infrastructure construction},
journal = {Results in Engineering},
volume = {16},
pages = {100650},
year = {2022},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2022.100650},
url = {https://www.sciencedirect.com/science/article/pii/S2590123022003206},
author = {V. Schuk and M.E. {Pombo Jiménez} and U. Martin},
keywords = {BIM, Formalization of engineering knowledge, Classification of regulation rules, Automatic Code Compliance Checking, Structured infrastructure data, Graph-based data modeling},
abstract = {Building Information Modeling has revolutionized the construction industry by, for example, contributing to the automation of different activities related to the design, planning, and construction of civil works. Tasks such as the review of construction-related codes and regulations, which today still require a great deal of human effort, have begun to be automated through the development of digital applications and tools, thus resulting in the BIM use case Automated Code Compliance Checking (ACCC). So far, however, the research and development around this topic have been focused mainly on the building area, leaving a gap for the infrastructure sector. With this article, the authors intend to contribute to closing this gap, by providing an overview of the current state of development of ACCC tools in infrastructure construction. This overview supports the identification of challenges and requirements for the development of such tools, which are also discussed in detail in the current work. Finally, the author's considerations regarding the formalization and structuring of the engineering knowledge and infrastructure data required for the development of ACCC tools, as well as its mapping in a graph database, which allows access to all the necessary and relevant information for the code checking process, are presented. The result of the research carried out is the detailed structuring of the regulations and their rules from infrastructure construction and the possible modeling implementation in a graph database. In practice, this should help the leading software manufacturers in cooperation with infrastructure engineers, to develop a full-fledged ACCC tool.}
}
@article{TURNER2022103593,
title = {Circular production and maintenance of automotive parts: An Internet of Things (IoT) data framework and practice review},
journal = {Computers in Industry},
volume = {136},
pages = {103593},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103593},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521002001},
author = {C. Turner and O. Okorie and C. Emmanouilidis and J. Oyekan},
keywords = {Sustainability, Intelligent maintenance systems, Maintenance models and services},
abstract = {The adoption of the Circular Economy paradigm by industry leads to increased responsibility of manufacturing to ensure a holistic awareness of the environmental impact of its operations. In mitigating negative effects in the environment, current maintenance practice must be considered for its potential contribution to a more sustainable lifecycle for the manufacturing operation, its products and related services. Focusing on the matching of digital technologies to maintenance practice in the automotive sector, this paper outlines a framework for organisations pursuing the integration of environmentally aware solutions in their production systems. This research sets out an agenda and framework for digital maintenance practice within the Circular Economy and the utilisation of Industry 4.0 technologies for this purpose.}
}
@article{MARTINI2024107783,
title = {Neuronal Spike Shapes (NSS): A straightforward approach to investigate heterogeneity in neuronal excitability states},
journal = {Computers in Biology and Medicine},
volume = {168},
pages = {107783},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107783},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523012489},
author = {Lorenzo Martini and Gianluca Amprimo and Stefano {Di Carlo} and Gabriella Olmo and Claudia Ferraris and Alessandro Savino and Roberta Bardini},
keywords = {Action potential, Patch-seq, Multimodal single-cell, Electrophysiology, Data analysis, Computational biology, Bioinformatics, Unsupervised learning},
abstract = {The mammalian brain exhibits a remarkable diversity of neurons, contributing to its intricate architecture and functional complexity. The analysis of multimodal single-cell datasets enables the investigation of cell types and states heterogeneity. In this study, we introduce the Neuronal Spike Shapes (NSS), a straightforward approach for the exploration of excitability states of neurons based on their Action Potential (AP) waveforms. The NSS method describes the AP waveform based on a triangular representation complemented by a set of derived electrophysiological (EP) features. To support this hypothesis, we validate the proposed approach on two datasets of murine cortical neurons, focusing it on GABAergic neurons. The validation process involves a combination of NSS-based clustering analysis, features exploration, Differential Expression (DE), and Gene Ontology (GO) enrichment analysis. Results show that the NSS-based analysis captures neuronal excitability states that possess biological relevance independently of cell subtype. In particular, Neuronal Spike Shapes (NSS) captures, among others, a well-characterized fast-spiking excitability state, supported by both electrophysiological and transcriptomic validation. Gene Ontology Enrichment Analysis reveals voltage-gated potassium (K+) channels as specific markers of the identified NSS partitions. This finding strongly corroborates the biological relevance of NSS partitions as excitability states, as the expression of voltage-gated K+ channels regulates the hyperpolarization phase of the AP, being directly implicated in the regulation of neuronal excitability.}
}
@article{CHEN2024105873,
title = {Knowledge graph for safety management standards of water conservancy construction engineering},
journal = {Automation in Construction},
volume = {168},
pages = {105873},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105873},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524006095},
author = {Yun Chen and Gengyang Lu and Ke Wang and Shu Chen and Chenfei Duan},
keywords = {Water conservancy construction engineering, Knowledge graph, Safety management standards, ALBERT-BiLSTM-CRF, Association rules},
abstract = {With the increasing demand for water conservancy engineering (WCE), the number of safety accidents during construction has continued to rise, requiring an urgent improvement in construction safety. The existing safety management regulations for water conservancy construction engineering (WCCE) comprise a considerable amount of text, with cross-references between different standards severely reducing their use efficiency. To address this issue, this paper proposes an ALBERT-BiLSTM-CRF model based on textual data from WCCE safety management standards. ALBERT, a lightweight pretrained language model, is integrated with the BiLSTM-CRF to construct an intelligent text entity recognition method. Association rules are used to extract entity relationships, and a knowledge graph representing the WCCE safety management standards is established. The results show that the ALBERT-BiLSTM-CRF algorithm improves the precision, with a recognition accuracy exceeding 85 %. Case studies validate that the constructed knowledge graph can quickly query safety standard knowledge, aiding in the generation of safety measures.}
}
@incollection{GALLA2023786,
title = {Community-centered Indigenous language recovery, restoration, revitalization, and renewal},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {786-796},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.07038-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818630507038X},
author = {Candace Kaleimamoowahinekapu Galla and Amanda Holmes and Joshua {Schwab Cartas} and William Rowluck},
keywords = {Community-centered language work, Decolonization, Elder praxis, Indian boarding or residential schools, Indigenous language curriculum, Indigenous language revitalization, Indigenous languages, Intergenerational knowledge relations, Media and technology, Relationality},
abstract = {Foregrounding Indigenous self-determination, decolonization, and community-centered language practices, we intend to disrupt the academy's assumptions about the necessity and centrality of Western academic research to Indigenous communities' efforts at revitalizing their languages. As Indigenous scholar-practitioners from distinct Indigenous communities - Hawaiʻi, Kanien'keha:ka, Lytton First Nation, Isthmus Zapotec - we share our perspectives and lived-experiences of community-centered language work in the areas of intergenerational knowledge relations, curriculum, and media and technology. We challenge the academy's desire to control the futurity of Indigenous languages by following the lead of the Indigenous community working toward language recovery, restoration, revitalization, and renewal.}
}
@article{QIN2025100016,
title = {Position paper: Domain knowledge-driven intelligentization of watershed modeling and scenario analysis for precise watershed management},
journal = {Information Geography},
volume = {1},
number = {1},
pages = {100016},
year = {2025},
issn = {3050-5208},
doi = {https://doi.org/10.1016/j.infgeo.2025.100016},
url = {https://www.sciencedirect.com/science/article/pii/S3050520825000168},
author = {Cheng-Zhi Qin and Liang-Jun Zhu and A-Xing Zhu},
keywords = {Watershed modeling, Intelligent geographical modeling, Domain knowledge-driven, Scenario analysis, Watershed management},
abstract = {Watershed modeling and scenario analysis based on the simulation–optimization framework, which comprehensively represents the most (if not all) of characteristics of geographical information science and geography, is crucial to support scientific decision-making for precise watershed management with increasing practical need. Meanwhile, it is still highly complex to be conducted in a way being not only scientific reasonable but also highly accessible for wide real applications with diverse users, especially those non-expert stakeholders. To narrow such a so-called “digital divide” between specialized modeling and real applications mainly with non-experts, this position paper advocates that intelligentization of watershed modeling and scenario analysis is the key. This position paper presents the state-of-the-art research landscape of intelligent methods for watershed modeling and scenario analysis, and particularly advocates that research on domain knowledge-driven intelligent methods is emerging and promising.}
}
@article{YANG2022105956,
title = {A novel multi-class classification model for schizophrenia, bipolar disorder and healthy controls using comprehensive transcriptomic data},
journal = {Computers in Biology and Medicine},
volume = {148},
pages = {105956},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105956},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522006904},
author = {Qingxia Yang and Yi Li and Bo Li and Yaguo Gong},
keywords = {Multi-class classification, Schizophrenia, Bipolar disorder, Gene signature, Partial least squares discriminant analysis},
abstract = {Two common psychiatric disorders, schizophrenia (SCZ) and bipolar disorder (BP), confer lifelong disability and collectively affect 2% of the world population. Because the diagnosis of psychiatry is based only on symptoms, developing more effective methods for the diagnosis of psychiatric disorders is a major international public health priority. Furthermore, SCZ and BP overlap considerably in terms of symptoms and risk genes. Therefore, the clarity of the underlying etiology and pathology remains lacking for these two disorders. Although many studies have been conducted, a classification model with higher accuracy and consistency was found to still be necessary for accurate diagnoses of SCZ and BP. In this study, a comprehensive dataset was combined from five independent transcriptomic studies. This dataset comprised 120 patients with SCZ, 101 patients with BP, and 149 healthy subjects. The partial least squares discriminant analysis (PLS-DA) method was applied to identify the gene signature among multiple groups, and 341 differentially expressed genes (DEGs) were identified. Then, the disease relevance of these DEGs was systematically performed, including (α) the great disease relevance of the identified signature, (β) the hub genes of the protein-protein interaction network playing a key role in psychiatric disorders, and (γ) gene ontology terms and enriched pathways playing a key role in psychiatric disorders. Finally, a popular multi-class classifier, support vector machine (SVM), was applied to construct a novel multi-class classification model using the identified signature for SCZ and BP. Using the independent test sets, the classification capacity of this multi-class model was assessed, which showed this model had a strong classification ability.}
}
@article{ZOU2023460,
title = {How do tourists’ heritage spatial perceptions affect place identity? A case study of Quanzhou, China},
journal = {Journal of Hospitality and Tourism Management},
volume = {55},
pages = {460-470},
year = {2023},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2023.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S1447677023000852},
author = {Yongguang Zou and Yong Yang and Yuan Li and Jinjin Liao and Honggen Xiao},
keywords = {Heritage spatial perception, Place identity, Functional satisfaction, Emotional experience, Cultural intelligence},
abstract = {The significance and social potential of cultural heritage tourism have been acknowledged worldwide, while less is known about the formation of tourists' place identity at cultural heritage sites. This research explores and examine how tourists' place identity is formed from a perceptual perspective. Two studies were conducted for this purpose. In Study 1, 30 tourists were interviewed, leading to three dimensions of tourists' heritage spatial perceptions (THSPs) (ontological spatial perceptions, representative spatial perceptions, and value-based spatial perceptions). Study 2 surveyed 336 tourists and found that THSPs could promote tourists' place identity through functional satisfaction and emotional involvement, while the impacts of THSPs on tourists' place identity demonstrated dimensional differences. Cultural intelligence positively moderated this process. These findings contribute to an enhanced understanding of tourists’ perceptions in cultural heritage tourism and present practical implications for managers on how to foster place identity at heritage sites.}
}
@article{DURANDDELACRE2024925,
title = {How does knowledge move? Investigating the epistemic mobilities of “climate migration” with diverse conceptual metaphors},
journal = {Mobilities},
volume = {19},
number = {6},
pages = {925-941},
year = {2024},
issn = {1745-0101},
doi = {https://doi.org/10.1080/17450101.2024.2328221},
url = {https://www.sciencedirect.com/science/article/pii/S1745010124000080},
author = {David Durand-Delacre},
keywords = {Knowledge, epistemic mobilities, conceptual metaphor theory, new mobilities paradigm, climate migration, climate mobilities},
abstract = {The production of knowledge is a mobile process. Efforts to conceptualise the mobilities of knowledge draw on a wide range of metaphors to conceptualise the ways in which knowledge moves and changes as it moves. In this paper, I present the theoretical origins and methodological implications – often tied to specific disciplines – of concepts in use. I distinguish between sedentarist metaphors (construction, transfer) and mobile metaphors (focusing on translation, contagion, friction, and circulation). I show that, although all these metaphors share a common attention to knowledge as mobile, they are neither synonymous nor interchangeable. They each structure how we think about and research epistemic mobilities in their own way. I find that mobile metaphors in particular are most compatible with, and can contribute to, the development of the mobile ontology that characterises the mobilities turn. I illustrate this using a case study of the epistemic mobilities of the idea of climate migration in the French context. From this example, I draw key lessons for studies of epistemic mobilities. I argue for a diverse, nuanced conceptual vocabulary of epistemic mobilities, leading to a nuanced, relational understanding of space, scale, and how to trace the mobilities of knowledge in practice.}
}
@article{FENG2025103217,
title = {Crafting user-centric prompts for UI generations based on Kansei engineering and knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103217},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103217},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625001107},
author = {Xuejing Feng and Huifang Du and Jun Ma and Haofen Wang and Lijuan Zhou and Meng Wang},
keywords = {Human-centered AI, User interface generations, Prompt engineering, Knowledge graph, Kansei engineering},
abstract = {Text-to-image (T2I) models are emerging as a powerful tool for designers to create user interface (UI) prototypes from natural language inputs (i.e., prompts). However, the discrepancy between designer inputs and model-preferred prompts makes it challenging for designers to consistently deliver effective results to end users. To bridge this gap, we introduce a novel hybrid method that assists designers in crafting user-centric prompts for T2I models, ensuring that the generated UIs align with end-user expectations. First, this method merges text mining and Kansei Engineering (KE) to analyze online user reviews and construct a Knowledge Graph (KG), mapping the intricate relationships between diverse affective requirements of users, design features, and corresponding text prompts for UI generation. Then, our approach automatically transforms designer inputs into model-preferred prompts through entity mention recognition and entity linking during the human-AI collaborative design process. Finally, we validate the proposed approach with a case study on automotive human–machine interface design. Experimental results demonstrate that our approach achieves high performance in perceived efficiency, satisfaction, and expectation disconfirmation. Overall, this study represents a step forward in integrating human and AI contributions in design and innovation within engineering disciplines, enabling AI to inspire, develop, and reinforce human creativity from a human factors perspective.}
}
@article{SUNG2020103504,
title = {A knowledge-based system to find over-the-counter medicines for self-medication},
journal = {Journal of Biomedical Informatics},
volume = {108},
pages = {103504},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103504},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420301325},
author = {Han-Yu Sung and Yu-Liang Chi},
keywords = {Self-medication, Over-the-counter medicine, Semantic Web, Open data, SPARQL},
abstract = {This study developed a medicine query system based on Semantic Web and open data especially for self-medication users to search over-the-counter (OTC) medicines. Most existing medicine query systems are based on keyword searches. If users are uncertain about the exact search words, these query systems do not offer effective help. Furthermore, most systems provide inadequate explanations of symptoms and ailments for users to use with confidence. To remedy these issues, this study builds a knowledge base to enable inference-based searches and data mashup for integrating information from across the Web. Three components were identified: (1) building an ontology model to describe the relationships between ailments and symptoms; (2) upgrading medicinal product datasets to link them with the ontology model on a semantic level; and (3) developing a data mashup to integrate web resources to help users to find references. Furthermore, the aim was to develop a web-based application that utilizes inference mechanisms to provide users with tools for interactive manipulation. A pilot experiment for skin ailments was implemented to learn the problem-solving skills of the system. Finally, two experts utilized a content validity index to rate a four-dimension 15-item scale. The evaluation results show that experts found the proposed system excellent for content validity.}
}
@article{ZHURENKOV2019349,
title = {Post-non-classical discourse of civilizations as a self-developing poly-subject environment for improving the mechanisms of international stability},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {25},
pages = {349-354},
year = {2019},
note = {19th IFAC Conference on Technology, Culture and International Stability TECIS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.548},
url = {https://www.sciencedirect.com/science/article/pii/S240589631932467X},
author = {Denis A. Zhurenkov and Yelena A. Trushkova},
keywords = {international stability, post-non-classical paradigm, scientific rationality, third-order Cybernetics},
abstract = {This paper discusses the principle of self-development in the context of the post-non-classical paradigm and researches the related ontological problems to improve the mechanisms of international stability. The aim of the study is to determine the form of the principle of self-development. The research procedure consists of three parts: the essence of the principle of self-development in the post-non-classical discourse of civilizations, ontological generalization of the principle of self-development, moral generalization of the principle of self-development in Russian modern science. The method of research is hermeneutical-phenomenological and structuralist complex and basic principles of Synergetics. The main result of the research is the substantiation of the new form of the principle of self-development - post-non-classical discourse of civilizations. Post-non-classical discourse of civilizations is one of the possible ways to improve the mechanisms of international stability, which is quite relevant for TECIS. The ontology of management in hybrid reality environments is substantiated, which can be an important component of IFAC activities. The theoretical and practical significance of post-non-classical discourse of civilizations consists in substantiating new principles of management and communication in polysubject environments.}
}
@article{WILLIAMS2024102057,
title = {Replacement of Dietary Fish Protein with Bacterial Protein Results in Decreased Adiposity Coupled with Liver Gene Expression Changes in Female Danio rerio},
journal = {Current Developments in Nutrition},
volume = {8},
number = {1},
pages = {102057},
year = {2024},
issn = {2475-2991},
doi = {https://doi.org/10.1016/j.cdnut.2023.102057},
url = {https://www.sciencedirect.com/science/article/pii/S247529912326641X},
author = {Michael B Williams and George BH Green and Joseph W Palmer and Christian X Fay and Sophie B Chehade and Addison L Lawrence and Robert J Barry and Mickie L Powell and Melissa L Harris and Stephen A Watts},
keywords = {nutrition, gene expression, body composition, animal nutrition, alternative protein sources, obesity},
abstract = {Background
Effective use of Danio rerio as a preclinical model requires standardization of macronutrient sources to achieve scientific reproducibility across studies and labs.
Objective
Our objective was to evaluate a bacterial-based single-cell protein (SCP) for the production of open-source standardized diets with defined health characteristics for the zebrafish research community.
Methods
We completed a 16-wk feeding trial using juvenile D. rerio 31 d postfertilization (10 tanks per diet and 14 D. rerio per tank) with formulated diets containing either a typical fish protein ingredient [standard reference (SR) diet] or a novel bacterial SCP source [bacterial protein (BP) diet]. At the end of the feeding trial, growth metrics, body composition, reproductive success, and bulk transcriptomics of the liver (RNAseq on female D. rerio with confirmatory rtPCR) were performed for each diet treatment.
Results
D. rerio fed the BP diet had body weight gains equivalent to the D. rerio fed fish protein, and females had significantly lower total carcass lipid, indicating reduced adiposity. Reproductive success was similar between treatments, suggesting normal physiological function. Genes differentially expressed in female D. rerio fed the BP diet compared with females fed the SR diet were overrepresented in the gene ontologies of metabolism, biosynthesis of cholesterol precursors and products, and protein unfolding responses.
Conclusion
Protein source substantially affected body growth metrics and composition as well as gene expression. These data support the development of an open-source diet utilizing an ingredient that correlates with improved health profiles and reduced variability in notable outcomes.}
}
@article{IDREES20241115,
title = {A Weighted Multi-Layer Analytics Based Model for Emoji Recommendation},
journal = {Computers, Materials and Continua},
volume = {78},
number = {1},
pages = {1115-1133},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.046457},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824001723},
author = {Amira M. Idrees and Abdul Lateef Marzouq Al-Solami},
keywords = {Social networks, text analytics, emoji prediction, features extraction, information retrieval},
abstract = {The developed system for eye and face detection using Convolutional Neural Networks (CNN) models, followed by eye classification and voice-based assistance, has shown promising potential in enhancing accessibility for individuals with visual impairments. The modular approach implemented in this research allows for a seamless flow of information and assistance between the different components of the system. This research significantly contributes to the field of accessibility technology by integrating computer vision, natural language processing, and voice technologies. By leveraging these advancements, the developed system offers a practical and efficient solution for assisting blind individuals. The modular design ensures flexibility, scalability, and ease of integration with existing assistive technologies. However, it is important to acknowledge that further research and improvements are necessary to enhance the system’s accuracy and usability. Fine-tuning the CNN models and expanding the training dataset can improve eye and face detection as well as eye classification capabilities. Additionally, incorporating real-time responses through sophisticated natural language understanding techniques and expanding the knowledge base of ChatGPT can enhance the system’s ability to provide comprehensive and accurate responses. Overall, this research paves the way for the development of more advanced and robust systems for assisting visually impaired individuals. By leveraging cutting-edge technologies and integrating them into a modular framework, this research contributes to creating a more inclusive and accessible society for individuals with visual impairments. Future work can focus on refining the system, addressing its limitations, and conducting user studies to evaluate its effectiveness and impact in real-world scenarios.}
}
@article{OLIVEIRA20182,
title = {OntoGenesis: an architecture for automatic semantic enhancement of data services},
journal = {International Journal of Web Information Systems},
volume = {15},
number = {1},
pages = {2-27},
year = {2018},
issn = {1744-0084},
doi = {https://doi.org/10.1108/IJWIS-04-2018-0020},
url = {https://www.sciencedirect.com/science/article/pii/S1744008418000447},
author = {Bruno C.N. Oliveira and Alexis Huf and Ivan Luiz Salvadori and Frank Siqueira},
keywords = {Semantic web service, Data service, Ontology alignment and construction, Property matching, Semantic representation, Service description},
abstract = {Purpose
This paper describes a software architecture that automatically adds semantic capabilities to data services. The proposed architecture, called OntoGenesis, is able to semantically enrich data services, so that they can dynamically provide both semantic descriptions and data representations.
Design/methodology/approach
The enrichment approach is designed to intercept the requests from data services. Therefore, a domain ontology is constructed and evolved in accordance with the syntactic representations provided by such services in order to define the data concepts. In addition, a property matching mechanism is proposed to exploit the potential data intersection observed in data service representations and external data sources so as to enhance the domain ontology with new equivalences triples. Finally, the enrichment approach is capable of deriving on demand a semantic description and data representations that link to the domain ontology concepts.
Findings
Experiments were performed using real-world datasets, such as DBpedia, GeoNames as well as open government data. The obtained results show the applicability of the proposed architecture and that it can boost the development of semantic data services. Moreover, the matching approach achieved better performance when compared with other existing approaches found in the literature.
Research limitations/implications
This work only considers services designed as data providers, i.e., services that provide an interface for accessing data sources. In addition, our approach assumes that both data services and external sources – used to enhance the domain ontology – have some potential of data intersection. Such assumption only requires that services and external sources share particular property values.
Originality/value
Unlike most of the approaches found in the literature, the architecture proposed in this paper is meant to semantically enrich data services in such way that human intervention is minimal. Furthermore, an automata-based index is also presented as a novel method that significantly improves the performance of the property matching mechanism.}
}
@article{ABDELRAHMAN2025112748,
title = {What is a Digital Twin anyway? Deriving the definition for the built environment from over 15,000 scientific publications},
journal = {Building and Environment},
volume = {274},
pages = {112748},
year = {2025},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2025.112748},
url = {https://www.sciencedirect.com/science/article/pii/S0360132325002306},
author = {Mahmoud Abdelrahman and Edgardo Macatulad and Binyu Lei and Matias Quintana and Clayton Miller and Filip Biljecki},
keywords = {Digital twin, Terminology, NLP, LLMs, Building digital twin, Urban digital twin, City digital twin},
abstract = {The concept of Digital Twins (DT) has attracted significant attention across various domains, particularly within the built environment. However, there is a sheer volume of definitions and the terminological consensus remains out of reach. The lack of a universally accepted definition leads to ambiguities in their conceptualization and implementation, and may cause miscommunication for both researchers and practitioners. We employed Natural Language Processing (NLP) techniques to systematically extract and analyze definitions of DTs from a corpus of more than 15,000 full-text articles spanning diverse disciplines. The study compares these findings with insights from an expert survey that included 52 experts. The study identifies concurrence on the components that comprise a “Digital Twin” from a practical perspective across various domains, contrasting them with those that do not, to identify deviations. We investigate the evolution of digital twin definitions over time and across different scales, including manufacturing, building, and urban/geospatial perspectives. We extracted the main components of Digital Twins using Text Frequency Analysis and N-gram analysis. Subsequently, we identified components that appeared in the literature and conducted a Chi-square test to assess the significance of each component in different domains. Our analysis identified key components of digital twins and revealed significant variations in definitions based on application domains, such as manufacturing, building, and urban contexts. The analysis of DT components reveal two major groups of DT types: High-Performance Real-Time (HPRT) DTs, and Long-Term Decision Support (LTDS) DTs. Contrary to common assumptions, we found that components such as simulation, AI/ML, real-time capabilities, and bi-directional data flow are not yet fully mature in the digital twins of the built environment. We derived two definitions for the Building/Architecture DT and the City/Urban DTs. Both definitions have a must-have components (such as spatial and temporal data updates) and good-to-have components such as prediction, AI, bi-directional data flow, and Real-time data exchange. One of the key findings is that the definition of digital twins has not yet reached its equilibrium phase, highlighting the need for ongoing revisions as technologies emerge or existing ones become obsolete. To address this, we introduce a novel, reproducible methodology that enables researchers to refine and adapt the current definitions in response to technological advancements or deprecations.}
}
@article{LIU2020101956,
title = {Web-based digital twin modeling and remote control of cyber-physical production systems},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {64},
pages = {101956},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.101956},
url = {https://www.sciencedirect.com/science/article/pii/S0736584519300134},
author = {Chao Liu and Pingyu Jiang and Wenlei Jiang},
keywords = {Digital twin, Cyber-physical production system, Ontology, Distributed cooperation, Web-based remote control},
abstract = {The rapid development new generation of information technologies facilitate the emergence of cyber-physical production system (CPPS) which could pave a way to exploring new smart manufacturing solutions. Digital twin (DT) is the technical core for establishing CPPS in the context of industry 4.0. Developing an easy-to-deploy and simple-to-use DT-based CPPS is a critical research gap. In this paper, a systemic framework is proposed to provide guidelines for rapid system configuration and easy runtime of DT-based CPPS by integrating CPS, DT modeling technologies, event-driven distributed cooperation mechanisms, and web technologies. The concept of CPS node (CPSN) for manufacturing resources is established by integrating semantic information model, 3D geometric model and function modules. Various CPSNs are orchestrated as an autonomous CPPS using dynamic resource registration and binding technologies. To achieve easy runtime of DT-based CPPS, event-driven distributed cooperation among CPSNs and web-based remote control of CPPS are proposed respectively. Finally, to verify the feasibility of the proposed framework, a prototype of DT-based CPPS is implemented, based on which an exemplary case is conducted.}
}
@incollection{CARRIM20231,
title = {Diversity},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {1-7},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.08001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305080015},
author = {Nazir Carrim},
keywords = {Diversity, Apartheid, Race, Ethnicity, Culture, Identity, Inclusivity and difference},
abstract = {This chapter focuses on diversity. It traces diversity to its historical links with issues related to race relations. Using the apartheid experience in South Africa, it shows how diversity was used to construct racism and segregation. It also argues that diversity is more than being about race, ethnicity, and culture, but also includes gender, class, sexual orientation, dis/ability and many other aspects. In noting the broader way in which diversity is viewed, it shows that inclusivity provided a way to delink diversity as referring to race relations. It is also argued that inclusivity implies difference. For a more socially just order in the future, it is shown that viewing difference ontologically and epistemologically may be useful. Diversity when viewed as difference, it is argued, allows for the complexities and pluralities of people's lives to be recognized so that who they are is not essentialized and reduced to a fixed essence that is counter factual. Recognizing the complexities of human beings requires working with difference that allows such complexity to be viewed and appreciated, as central to being human and as always in a state of becoming and development.}
}
@article{ISRANI2025100010,
title = {Precision medicine: Crossing the biomedical scales with AI},
journal = {The Journal of Precision Medicine: Health and Disease},
volume = {3},
pages = {100010},
year = {2025},
issn = {3050-6328},
doi = {https://doi.org/10.1016/j.premed.2025.100010},
url = {https://www.sciencedirect.com/science/article/pii/S3050632825000101},
author = {Sharat Israni and Gary D. Bader and Sergio E. Baranzini and John A. Capra and Marina Sirota and Christina V. Theodoris and Chun Jimmie Ye},
abstract = {Precision Medicine, targeted for each person, has presented a major challenge in scaling up to wide use. Reflecting the true behavior of the human body, it requires inquiry across the biomedical scales, from the genome all the way up to the environmental factors impinging on that person. In computation terms, this is a problem of unprecedented complexity. With AI showing its potential, the thoughtful application of advanced machine intelligence, knowledge networks, machine reasoning power and new computational paradigms—guided by human oversight at every stage—can help unlock major advances in treatment and prevention.}
}
@article{WICHMANN20181726,
title = {Towards automatically generating supply chain maps from natural language text},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {1726-1731},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.207},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318313284},
author = {Pascal Wichmann and Alexandra Brintrup and Simon Baker and Philip Woodall and Duncan McFarlane},
keywords = {supply chain management, supply chain map, supply chain mapping, natural language processing, text mining, supply chain visibility, supply chain mining},
abstract = {Supply chains are increasingly global, complex and multi-tiered. Consequently, companies often struggle to maintain complete visibility of their upstream supply network. This poses a problem as visibility of the network is required in order to effectively manage supply chain risk. In this paper, we discuss supply chain mapping as a means of maintaining (structural) visibility of a company’s supply chain, and we derive the requirements for automatically generating supply chain maps from openly available text sources. Early results show that supply chain mapping solutions generated by Natural Language Processing (NLP) could enable companies to a) automatically generate rudimentary supply chain maps, b) verify existing supply chain maps or c) augment existing maps with additional supplier information.}
}
@article{KOSTOPOLUS2025102894,
title = {Student use of generative AI as a composing process supplement: Concerns for intellectual property and academic honesty},
journal = {Computers and Composition},
volume = {75},
pages = {102894},
year = {2025},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2024.102894},
url = {https://www.sciencedirect.com/science/article/pii/S8755461524000707},
author = {Emma Kostopolus},
keywords = {Artificial intelligence, Intellectual property, Academic honesty, Multimodality},
abstract = {This article discusses the nuanced challenges of using Generative Artificial Intelligence in multimodal compositions while maintaining an ethical adherence to ideas of academic honesty and intellectual property. Through examining hypothetical scenarios, we can see that multimodality complicates the concept of “fair use” in academic contexts, since image or audio generation via AI functions differently than text generated by a Large Language Model. In thinking through the case studies, the article presents an argument for how educators can still use Generative AI in their multimodal composition assignments, through teaching students to us it as a process supplement and to always be critically aware of their citational responsibilities. This understanding of Generative AI use is placed in conversation with our understanding of intellectual property law as relates to both the classroom and broader digital composing environments, to better prepare students to create texts in their future careers.}
}
@article{SAFARI201813,
title = {Complex analyses on clinical information systems using restricted natural language querying to resolve time-event dependencies},
journal = {Journal of Biomedical Informatics},
volume = {82},
pages = {13-30},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418300686},
author = {Leila Safari and Jon D. Patrick},
keywords = {Data analytics, Time-event dependency, Scientific experiment},
abstract = {Purpose
This paper reports on a generic framework to provide clinicians with the ability to conduct complex analyses on elaborate research topics using cascaded queries to resolve internal time-event dependencies in the research questions, as an extension to the proposed Clinical Data Analytics Language (CliniDAL).
Methods
A cascaded query model is proposed to resolve internal time-event dependencies in the queries which can have up to five levels of criteria starting with a query to define subjects to be admitted into a study, followed by a query to define the time span of the experiment. Three more cascaded queries can be required to define control groups, control variables and output variables which all together simulate a real scientific experiment. According to the complexity of the research questions, the cascaded query model has the flexibility of merging some lower level queries for simple research questions or adding a nested query to each level to compose more complex queries. Three different scenarios (one of them contains two studies) are described and used for evaluation of the proposed solution.
Results
CliniDAL’s complex analyses solution enables answering complex queries with time-event dependencies at most in a few hours which manually would take many days.
Conclusion
An evaluation of results of the research studies based on the comparison between CliniDAL and SQL solutions reveals high usability and efficiency of CliniDAL’s solution.}
}
@article{SARICA2020112995,
title = {TechNet: Technology semantic network based on patent data},
journal = {Expert Systems with Applications},
volume = {142},
pages = {112995},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.112995},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419307122},
author = {Serhad Sarica and Jianxi Luo and Kristin L. Wood},
keywords = {Knowledge discovery, Word embedding, Technology semantic network, Knowledge representation},
abstract = {The growing developments in general semantic networks, knowledge graphs and ontology databases have motivated us to build a large-scale comprehensive semantic network of technology-related data for engineering knowledge discovery, technology search and retrieval, and artificial intelligence for engineering design and innovation. Specially, we constructed a technology semantic network (TechNet) that covers the elemental concepts in all domains of technology and their semantic associations by mining the complete U.S. patent database from 1976. To derive the TechNet, natural language processing techniques were utilized to extract terms from massive patent texts and recent word embedding algorithms were employed to vectorize such terms and establish their semantic relationships. We report and evaluate the TechNet for retrieving terms and their pairwise relevance that is meaningful from a technology and engineering design perspective. The TechNet may serve as an infrastructure to support a wide range of applications, e.g., technical text summaries, search query predictions, relational knowledge discovery, and design ideation support, in the context of engineering and technology, and complement or enrich existing semantic databases. To enable such applications, the TechNet is made public via an online interface and APIs for public users to retrieve technology-related terms and their relevancies.}
}
@article{EKAPUTRA2025100855,
title = {Pattern-based engineering of Neurosymbolic AI Systems},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100855},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100855},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000416},
author = {Fajar J. Ekaputra},
keywords = {Neurosymbolic AI, Knowledge graphs, AI system engineering, Design patterns},
abstract = {The symbiotic combination of sub-symbolic and symbolic AI techniques is a significant trend in AI, leading to the fast-paced development of various techniques that integrate these paradigms to build intelligent systems. However, the wealth of heterogeneous architectural options for combining the paradigms into Neurosymbolic AI (NeSy-AI) systems poses significant challenges. In particular, there is currently no standardized way to design, engineer, and document such systems that encompass visual and formal notations. Existing works aim to address this challenge by systematically modelling NeSy-AI systems as design patterns that include process, data, and human interactions. However, these works focus on capturing specific views of the system rather than aiming to support the broad process of AI system engineering. This paper outlines a vision of pattern-based AI Systems engineering, aiming to support the engineering process of NeSy-AI systems with tasks such as system documentation and artefact generation through interlinked visual and formal notations with Knowledge Graphs at its core.}
}
@article{ZHANG2023102455,
title = {An effective MBSE approach for constructing industrial robot digital twin system},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {80},
pages = {102455},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102455},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001375},
author = {Xi Zhang and Bo Wu and Xin Zhang and Jian Duan and Chenhui Wan and Youmin Hu},
keywords = {Digital twin, Variants management, Industrial robot digital twin, Model-base systems engineering, Multi-domain model},
abstract = {Recently, the rapid development of digital twin (DT) technology has been regarded significant in Cyber-physical systems (CPS) promotion. Scholars are focusing on the theoretical architecture and implementing applications, in order to establish a high-fidelity, dynamic, and full-lifecycle DT model and achieve a deep fusion of real and virtual. As a typical complex system with multi-disciplines, multi-physics, and multi-domain characteristics, industrial robot (IR) involves various processes and elements from the two other levels of the system: components and production lines. Their complex relationships lead to a huge challenge to build a comprehensive DT model. Current researchers usually concentrates on single-layer services because of limited construction methodology, which results in enormous isolated models, and leads to low reusable system blocks, finite scalability, and high costs of design, adjustment, upgrade, and maintenance. To address these issues, a standardized methodology and a hierarchical, modular, and generic architecture are proposed to depict comprehensive and variable industrial robot digital twin (IRDT). Firstly, the ontology information model is presented by analyzing variable factors systematically. Then, model-based system engineering (MBSE) based methodology is introduced, including construction process and variants management. After modeling process of three levels (problem domain, solution main, and implementation domain) and four viewpoints (requirement, structure, behavior, and parameter), a generic architecture of IRDT is constructed and a feature-based variants management method is described. Besides, a six-axis IRDTS is implemented to illustrate the mapping of logical architecture and physical system as a multi-level elements and processes representation example. And the steps of numerical evaluations consist of system delay and derivation. Finally, results show the effectiveness and the potential of the proposed theoretical methodology for constructing IRDTS and other industrial applications.}
}
@article{BALCH2023,
title = {Machine Learning–Enabled Clinical Information Systems Using Fast Healthcare Interoperability Resources Data Standards: Scoping Review},
journal = {JMIR Medical Informatics},
volume = {11},
year = {2023},
issn = {2291-9694},
doi = {https://doi.org/10.2196/48297},
url = {https://www.sciencedirect.com/science/article/pii/S229196942300025X},
author = {Jeremy A Balch and Matthew M Ruppert and Tyler J Loftus and Ziyuan Guan and Yuanfang Ren and Gilbert R Upchurch and Tezcan Ozrazgat-Baslanti and Parisa Rashidi and Azra Bihorac},
keywords = {ontologies, clinical decision support system, Fast Healthcare Interoperability Resources, FHIR, machine learning, ontology, interoperability, interoperable, decision support, information systems, review methodology, review methods, scoping review, clinical informatics},
abstract = {Background
Machine learning–enabled clinical information systems (ML-CISs) have the potential to drive health care delivery and research. The Fast Healthcare Interoperability Resources (FHIR) data standard has been increasingly applied in developing these systems. However, methods for applying FHIR to ML-CISs are variable.
Objective
This study evaluates and compares the functionalities, strengths, and weaknesses of existing systems and proposes guidelines for optimizing future work with ML-CISs.
Methods
Embase, PubMed, and Web of Science were searched for articles describing machine learning systems that were used for clinical data analytics or decision support in compliance with FHIR standards. Information regarding each system’s functionality, data sources, formats, security, performance, resource requirements, scalability, strengths, and limitations was compared across systems.
Results
A total of 39 articles describing FHIR-based ML-CISs were divided into the following three categories according to their primary focus: clinical decision support systems (n=18), data management and analytic platforms (n=10), or auxiliary modules and application programming interfaces (n=11). Model strengths included novel use of cloud systems, Bayesian networks, visualization strategies, and techniques for translating unstructured or free-text data to FHIR frameworks. Many intelligent systems lacked electronic health record interoperability and externally validated evidence of clinical efficacy.
Conclusions
Shortcomings in current ML-CISs can be addressed by incorporating modular and interoperable data management, analytic platforms, secure interinstitutional data exchange, and application programming interfaces with adequate scalability to support both real-time and prospective clinical applications that use electronic health record platforms with diverse implementations.}
}
@article{ELKIN2021,
title = {Using Artificial Intelligence With Natural Language Processing to Combine Electronic Health Record’s Structured and Free Text Data to Identify Nonvalvular Atrial Fibrillation to Decrease Strokes and Death: Evaluation and Case-Control Study},
journal = {Journal of Medical Internet Research},
volume = {23},
number = {11},
year = {2021},
issn = {1438-8871},
doi = {https://doi.org/10.2196/28946},
url = {https://www.sciencedirect.com/science/article/pii/S1438887121011092},
author = {Peter L Elkin and Sarah Mullin and Jack Mardekian and Christopher Crowner and Sylvester Sakilay and Shyamashree Sinha and Gary Brady and Marcia Wright and Kimberly Nolen and JoAnn Trainer and Ross Koppel and Daniel Schlegel and Sashank Kaushik and Jane Zhao and Buer Song and Edwin Anand},
keywords = {afib, atrial fibrillation, artificial intelligence, NVAF, natural language processing, stroke risk, bleed risk, CHA2DS2-VASc, HAS-BLED, bio-surveillance},
abstract = {Background
Nonvalvular atrial fibrillation (NVAF) affects almost 6 million Americans and is a major contributor to stroke but is significantly undiagnosed and undertreated despite explicit guidelines for oral anticoagulation.
Objective
The aim of this study is to investigate whether the use of semisupervised natural language processing (NLP) of electronic health record’s (EHR) free-text information combined with structured EHR data improves NVAF discovery and treatment and perhaps offers a method to prevent thousands of deaths and save billions of dollars.
Methods
We abstracted 96,681 participants from the University of Buffalo faculty practice’s EHR. NLP was used to index the notes and compare the ability to identify NVAF, congestive heart failure, hypertension, age ≥75 years, diabetes mellitus, stroke or transient ischemic attack, vascular disease, age 65 to 74 years, sex category (CHA2DS2-VASc), and Hypertension, Abnormal liver/renal function, Stroke history, Bleeding history or predisposition, Labile INR, Elderly, Drug/alcohol usage (HAS-BLED) scores using unstructured data (International Classification of Diseases codes) versus structured and unstructured data from clinical notes. In addition, we analyzed data from 63,296,120 participants in the Optum and Truven databases to determine the NVAF frequency, rates of CHA2DS2‑VASc ≥2, and no contraindications to oral anticoagulants, rates of stroke and death in the untreated population, and first year’s costs after stroke.
Results
The structured-plus-unstructured method would have identified 3,976,056 additional true NVAF cases (P<.001) and improved sensitivity for CHA2DS2-VASc and HAS-BLED scores compared with the structured data alone (P=.002 and P<.001, respectively), causing a 32.1% improvement. For the United States, this method would prevent an estimated 176,537 strokes, save 10,575 lives, and save >US $13.5 billion.
Conclusions
Artificial intelligence–informed bio-surveillance combining NLP of free-text information with structured EHR data improves data completeness, prevents thousands of strokes, and saves lives and funds. This method is applicable to many disorders with profound public health consequences.}
}
@article{MA2024102621,
title = {Requirements prioritization for complex products based on fuzzy associative predicate representation learning},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102621},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102621},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002696},
author = {Yufeng Ma and Yajie Dou and Xiangqian Xu and Yuejin Tan and Kewei Yang},
keywords = {Fuzzy associate predicate, Knowledge representation learning, Membership degree calculation, Fuzzy soft set},
abstract = {Complex products, integrating many structures and functions, face challenges in fulfilling all requirements due to limited time and resources, making Requirements Prioritization (RP) essential in product development. The complexity of RP increases with the need to consider a wider range of criteria and data from more stakeholders like developers and customers, introducing uncertainty in requirements and expert opinions. However, current research rarely explores systematic methods for addressing requirements in this uncertain environment. Based on that, this paper presents a hybrid framework for organizing knowledge related to RP and determining item priorities. Specifically, we build a multi-dimensional evaluation indicator ontology, model requirement knowledge based on fuzzy RDF Knowledge Graph(KG), generate fuzzy membership degree through representation learning, and then rank requirements by fuzzy soft set. Finally, the effectiveness of our framework is validated in two aspects: evaluation of the fuzzy associative predicate representation learning method and application through a practical case study.}
}
@article{SMIRNOV20191603,
title = {Task Model Representation: Approach and Prototype},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {1603-1608},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.429},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319314107},
author = {Alexander Smirnov and Nikolay Shilov and Maxim Shchekotov},
keywords = {Task Model, automation, handling, function, ontology},
abstract = {Due to the variety of possible solutions for handling applications, formalization of a customer request is not an easy task. The paper addresses this problem proposing a Task Model description independent on particular automation components. The model makes it possible to describe complex handling processes, handling function parameters, as well as interdependencies between these parameters. The developed prototype enables building a Task Model through a graphical user interface and also helps to find correct handling functions via interactive selection of function characteristics.}
}
@article{ZHAO2020198,
title = {Conjoint Feature Representation of GO and Protein Sequence for PPI Prediction Based on an Inception RNN Attention Network},
journal = {Molecular Therapy Nucleic Acids},
volume = {22},
pages = {198-208},
year = {2020},
issn = {2162-2531},
doi = {https://doi.org/10.1016/j.omtn.2020.08.025},
url = {https://www.sciencedirect.com/science/article/pii/S2162253120302547},
author = {Lingling Zhao and Junjie Wang and Yang Hu and Liang Cheng},
keywords = {GOSeqPPI, protein-protein interaction, ontology semantic representation, NCBI-blueBERT, inception RNN attention network, biomedical research},
abstract = {Protein-protein interactions (PPIs) are pivotal for cellular functions and biological processes. In the past years, computational methods using amino acid sequences and gene ontology (GO) annotations of proteins for prioritizing PPIs have provided important references for biological experiments in the wet lab. Despite the current success, sequence information and ontological annotation in semantic representation have not been integrated into current methods. We propose a deep-learning-based PPI prediction methodology conjointly featuring sequence information and GO annotation. First, we adopt a word-embedding tool, the NCBI-blueBERT model pre-trained on PubMed, to map the GO terms into their semantic vectors. Then, the GO semantic vectors and protein sequence vector serve as the input of the proposed inception recurrent neural network (RNN) attention network (IRAN). The IRAN captures the spatial relationship and the potential sequential feature of the protein sequence and ontological annotation semantics. The extensive experimental results on 12 benchmarks demonstrate that our method achieves superiority over state-of-the-art baselines. In the yeast dataset of a binary PPI prediction, our method improved the performance with the Matthews correlation coefficient increasing from 94.2% to 98.2% and the accuracy from 97.1% to 98.2%. The analogous results were also obtained in other comparison evaluations.}
}
@article{NOUAR2021108152,
title = {A Semantic virtualized network functions description and discovery model},
journal = {Computer Networks},
volume = {195},
pages = {108152},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108152},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621002140},
author = {Nour el houda Nouar and Sami Yangui and Noura Faci and Khalil Drira and Saïd Tazi},
keywords = {Network Function Virtualization (NFV), Ontology, Semantic matchmaking, Virtualized Network Function (VNF)},
abstract = {Network Function Virtualization (NFV) has increasingly gained importance to address some emerging networking challenges like agility and cost-effectiveness. NFV enables to run Virtualized Network Functions (VNF) on top of any generic, Commercial-Off-The-Shelf (COTS) hardware, anytime and anywhere in the network. Specific service providers offer VNFs to prospective network providers. Service providers publish VNFs in dedicated marketplaces where network providers search VNFs and instantiate them according to a pre-established service-level agreement. On top of being proprietary and specific to the service providers, the existing VNF description models include details on VNF deployment but fail to fit VNF functional and non-functional specifications. This description alters an efficient selection of the most relevant VNFs and prevents full automation of the VNFs provisioning. This paper introduces a novel domain-independent VIrtualized networK functIoN ontoloGy (VIKING for short) for VNF description and publication in federated repositories. It also proposes a semantic-based matchmaking algorithm to discover and select the most relevant VNFs that satisfy prospective VNF consumers’ requests. As for validation, a prototype called Mastermyr Chest, including VIKING’s instantiation along with the matchmaker in Content Delivery Networks (CDN) domain was implemented. This prototype illustrates a new way to contribute to the redesign of the CDN’s traditional architecture by enabling value-added CDN service provisioning in an agile and dynamic manner. A set of experiments was run to (i) evaluate the matchmaker performances and (ii) demonstrate its accuracy and precision.}
}
@article{SCUTELNICU2023815,
title = {A Word Sense Disambiguatin Approach For Romanian Language},
journal = {Procedia Computer Science},
volume = {225},
pages = {815-821},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.068},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923012267},
author = {Liviu Andrei Scutelnicu},
keywords = {Word Sense Disambiguateion, corpus, machine learning, context extraction, Romanian WordNet},
abstract = {The automatic word sense disambiguation has been a topic of interest since the 1950s. Meaning disambiguation is not an end in itself, it is an intermediate process, necessary at a certain level to use in natural language processing. It is obviously useful for applications that require language interpretation (message communication, human-machine interaction), but it is also used in fields whose main purpose is not to understand natural language. In this paper we propose an approach for word sense disambiguation for Romanian language, based on a supervised learning, a large number of features, and information gain to reduce the dimensions. The evaluation for the Romanian words subtask is produced by the extensive feature set in conjunction with a Maximum Entropy classifier and outcomes that are better than other similar researches.}
}
@article{HUANG2023227,
title = {Knowledge production of university-industry collaboration in academic capitalism: An analysis based on Hoffman's framework},
journal = {Asian Journal of Social Science},
volume = {51},
number = {4},
pages = {227-236},
year = {2023},
issn = {1568-4849},
doi = {https://doi.org/10.1016/j.ajss.2023.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1568484923000345},
author = {Jinghui Huang and Kui Xiong},
keywords = {Academic capitalism, Industry-academia collaboration, Knowledge production, Sociology of knowledge, The Greater Bay Area of China},
abstract = {As academic capitalism promotes the transformation of knowledge production globally, both parties in the industry-academia collaboration are repositioned as knowledge creators in the new knowledge circuit, in which the knowledge production process becomes an issue to be explored in depth. Based on the analytical framework of Hoffman and the experience of River City in Guangdong Province-Hong Kong-Macao Greater Bay Area, we analyse the ontological, epistemological, and applicative knowledge practices of both industry and academia. Both parties acknowledge the inherent uncertainties in knowledge production. Risk discourse for knowledge practices reflects influences of market/market-like logic on industry-academia collaboration. Around the three dimensions, the contradictions revealed in their interactions are multiple. Accordingly, both parties seek consensus on knowledge outcomes by suspending ontological discussions of knowledge, conceding epistemological issues, and imagining applied consensus. Nonetheless, these acts hardly achieve the desired goal of knowledge advancement and create obstacles to the knowledge circuit.}
}
@article{PAL20213981,
title = {Using DEMATEL for Contextual Learner Modeling in Personalized and Ubiquitous Learning},
journal = {Computers, Materials and Continua},
volume = {69},
number = {3},
pages = {3981-4001},
year = {2021},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2021.017966},
url = {https://www.sciencedirect.com/science/article/pii/S1546221821012169},
author = {Saurabh Pal and Pijush Kanti Dutta Pramanik and Musleh Alsulami and Anand Nayyar and Mohammad Zarour and Prasenjit Choudhury},
keywords = {Personalized e-learning, DEMATEL, learner model, ontology, learner context, personalized recommendation, adaptive decisions},
abstract = {With the popularity of e-learning, personalization and ubiquity have become important aspects of online learning. To make learning more personalized and ubiquitous, we propose a learner model for a query-based personalized learning recommendation system. Several contextual attributes characterize a learner, but considering all of them is costly for a ubiquitous learning system. In this paper, a set of optimal intrinsic and extrinsic contexts of a learner are identified for learner modeling. A total of 208 students are surveyed. DEMATEL (Decision Making Trial and Evaluation Laboratory) technique is used to establish the validity and importance of the identified contexts and find the interdependency among them. The acquiring methods of these contexts are also defined. On the basis of these contexts, the learner model is designed. A layered architecture is presented for interfacing the learner model with a query-based personalized learning recommendation system. In a ubiquitous learning scenario, the necessary adaptive decisions are identified to make a personalized recommendation to a learner.}
}
@article{BUCKHORST2022102301,
title = {Decentralized Holonic Control System Model for Line-less Mobile Assembly Systems},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {75},
pages = {102301},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102301},
url = {https://www.sciencedirect.com/science/article/pii/S0736584521001812},
author = {Armin F. Buckhorst and Lea Grahn and Robert H. Schmitt},
keywords = {Line-less Mobile Assembly System, Assembly planning and control, Control system architecture, Holonic Manufacturing System},
abstract = {The Line-less Mobile Assembly System paradigm (short LMAS) provides necessitated flexibility, especially for large-scale products as customer demands for individualized products persist and product life-cycles remain short. To make LMAS advantages operationally usable in an industrial context, it requires a suitable control system to connect multi-purpose assembly resources and to autonomously configure transient assembly stations. Therefore this paper reviews the relevant literature to identify the necessary components of such architectures. Depending on the control system’s intention and use cases, the properties of the organizational paradigm, an adequate ontology and basic design patterns regarding the distribution and order-relationships of the system entities are to be defined conceptually. For the implementation, a role model, an interaction model, and a data model are required. Due to the utilization of mobile multipurpose resources and the possibility of factory shopfloor reconfiguration by transient stations, existing approaches do not meet LMAS inherent properties. Consequently, we present a suitable decentralized multi-agent control system approach.}
}
@incollection{BIF20242275,
title = {A data visualization tool for biomass valorization in Brazil},
editor = {Flavio Manenti and Gintaras V. Reklaitis},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {53},
pages = {2275-2280},
year = {2024},
booktitle = {34th European Symposium on Computer Aided Process Engineering / 15th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-28824-1.50380-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044328824150380X},
author = {Roger Sampaio Bif and Larissa Thais Bruschi and Moisés Teles {dos Santos}},
keywords = {data visualization, biomass valorization, ontology},
abstract = {Brazil’s diverse climate zones, vast territory, varied soils, and abundant water resources contribute to its rich biodiversity, making it ideal for biomass exploitation. This work deals with the creation of a user-friendly, interactive data visualization tool for biomass valorization. This involves collecting georeferenced data on biomass production and availability and developing an ontology to represent the Brazilian biomass supply chain. The Python programming language with libraries like Pandas, Matplotlib, and Streamlit, are used for data analysis, mapping, and creating web-based visualization applications. The proposed tool provides dynamic maps, showing biomass distribution and availability, highlighting regional resource disparities, assisting in region-specific planning and decision-making. This tool will be further integrated into optimization frameworks, helping to identify opportunities and to develop effective strategies for biomass valorization in Brazil, supporting the decision-making process.}
}
@article{PANOUTSOPOULOS2024109268,
title = {Investigating the effect of different fine-tuning configuration scenarios on agricultural term extraction using BERT},
journal = {Computers and Electronics in Agriculture},
volume = {225},
pages = {109268},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109268},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924006598},
author = {Hercules Panoutsopoulos and Borja Espejo-Garcia and Stephan Raaijmakers and Xu Wang and Spyros Fountas and Christopher Brewster},
keywords = {Automatic term extraction, Agriculture, Agriculture-BERT, Fine tuning configuration scenarios, Silver standard corpus},
abstract = {This paper compares different transformer-based language models for automatic term extraction from agriculture-related texts. Agriculture is an important economic sector faced with severe environmental and societal challenges. The collection, annotation and sharing of agricultural scientific knowledge is key to enabling the agricultural sector to address its challenges. Automatic term extraction is a Natural Language Processing task that can provide solutions to text tagging and annotation towards better knowledge and information exchange. It is concerned with the identification of terms pertaining to a domain, or area of expertise, in text and is an important step in knowledge base creation and update pipelines. Transformer-based language modeling technologies like BERT have become popular for automatic term extraction, but limited work has been undertaken so far in applying these methods to agriculture. This paper systematically compares Agriculture-BERT to Sci-BERT, RoBERTa, and vanilla BERT, which were fine-tuned for the automatic extraction of agricultural terms from English texts. The greatest challenge faced in our research was the scarcity of agriculture-related gold standard corpora for measuring automatic term extraction performance. Our results show that, with a few exceptions, Agriculture-BERT performs better than the other models considered in our research. Our main contribution and novelty of the presented research is the investigation of the impact that different language model fine-tuning configuration scenarios had on the term extraction task. More specifically, we tested different scenarios related to the model layers kept frozen, or being updated, during training, to measure the impact they may have on Agriculture-BERT’s performance in automatic term extraction. Our results show that the best performance was achieved by: (i) the “embedding layer updated + all encoder layers updated” scenario for the identification of terms also seen during training; (ii) the “embedding layer frozen + all encoder layers updated” scenario for the identification of terms being synonyms to those seen during training; and (iii) the “embedding layer updated + top 4 encoder layers updated” scenario for identifying terms neither seen during training nor being synonyms to those seen during training (novel terms).}
}
@article{NECASKY2022100713,
title = {Interactive and iterative visual exploration of knowledge graphs based on shareable and reusable visual configurations},
journal = {Journal of Web Semantics},
volume = {73},
pages = {100713},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100713},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000105},
author = {Martin Nečaský and Štěpán Stenchlák},
keywords = {Knowledge graph, Graph visualization, Linked data, Open data, Ontology},
abstract = {Knowledge graphs denote structured data which represent entities and relationships between them in a form of a graph, often expressed in the RDF data model. It may be hard for lay users to explore existing knowledge graphs, especially when graphs from different data sources need to be integrated. In this paper, we present an approach to knowledge graph visual exploration based on the concept of shareable and reusable visual configurations. A visual configuration comprises domain specific views on a knowledge graph which define operations such as node detail or expansion. These operations are easy to understand for lay users who can use them to explore a graph while complexities unnecessary in a given application context remain hidden. We introduce an ontology which enables to express and publish visual configurations and reuse their components in other configurations. We also provide an experimental implementation called KGBrowser. We evaluate the proposed approach with real users. We also compare our implementation KGBrowser with other existing tools for knowledge graph visualization and exploration.}
}
@article{BATTISTELLI20202358,
title = {Building a formal model for hate detection in French corpora},
journal = {Procedia Computer Science},
volume = {176},
pages = {2358-2365},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.299},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920322092},
author = {Delphine Battistelli and Cyril Bruneau and Valentina Dragos},
keywords = {online hate speech, ontology, semantics},
abstract = {This paper investigates the development of a formal model in order to analyse online hate in French corpora. Relevant concepts are identified by exploiting several sources: the cognitive foundations of the appraisal theory, according to which people’s emotional response are based on their own evaluative judgments or appraisals of situations, events or objects; a linguistic model of how different kinds of modalities applied to predicative contents are expressed in textual data; several definitions of a hate speech. Based on those inputs, a formal model was developed to describe online hate speech. The model highlights different categories of hate targets and actions, and emphasizes the importance of context for online hate detection.}
}
@article{BURSE2022104870,
title = {Targeting stopwords for quality assurance of SNOMED-CT},
journal = {International Journal of Medical Informatics},
volume = {167},
pages = {104870},
year = {2022},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2022.104870},
url = {https://www.sciencedirect.com/science/article/pii/S1386505622001848},
author = {Rashmi Burse and Gavin McArdle and Michela Bertolotto},
keywords = {Lexical Auditing, Semantic Analysis, Biomedical Named Entity Recognition, Quality Assurance, Biomedical Ontologies, SNOMED-CT},
abstract = {Objective
We assess the potential of exploiting stopwords in biomedical concept names to complete the logical definitions of concepts that are not sufficiently defined.
Methods
Concepts containing stopwords are selected from the Disorder hierarchy of Systematized NOmenclature of MEDicine (SNOMED-CT). SNOMED-CT consists of two types of concepts: Fully Defined (FD) concepts which are sufficiently defined and Partially Defined (PD) concepts which are not sufficiently defined. In this work, FD concepts containing stopwords are treated as a source of ground truth to complete the definitions of, lexically and semantically similar, PD concepts. FD and PD concepts are lexically and semantically analysed to create sample-sets. Mandatory attribute-relationships are calculated by using an intersection-set logic for each FD sample-set. PD sample-sets are audited against this mandatory attribute-relationship template to identify inconsistencies in modelling styles and potentially missing attribute-relationships.
Results
Lexical and semantic patterns around 11 stopwords were analysed. 26 sample-sets were extracted for the 11 stopwords. Mandatory attribute-relationships were identified for 24 of the 26 sample-sets. The method identified 62.5% - 72.22% of the PD concepts, containing the stopwords in and due to, to be inconsistent in their modelling style and potentially missing at least one attribute-relationship according to the created template.}
}
@article{BELLANDI2024105904,
title = {An entity-centric approach to manage court judgments based on Natural Language Processing},
journal = {Computer Law & Security Review},
volume = {52},
pages = {105904},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2023.105904},
url = {https://www.sciencedirect.com/science/article/pii/S0267364923001140},
author = {Valerio Bellandi and Christian Bernasconi and Fausto Lodi and Matteo Palmonari and Riccardo Pozzi and Marco Ripamonti and Stefano Siccardi},
keywords = {Legal knowledge extraction, Semantic search, Named Entity Recognition, Zero-shot learning},
abstract = {In this paper, we present an entity-centric infrastructure to manage legal documents, especially court judgments, based on the organization of a textual document repository and on the annotation of these documents to serve a variety of downstream tasks. Documents are pre-processed and then iteratively annotated using a set of NLP services that combine complementary approaches based on machine learning and syntactic rules. We present a framework that has been designed to be developed and maintained in a sustainable way, allowing for multiple services and uses of the annotated document repository and considering the scarcity of annotated data as an intrinsic challenge for its development. The design activity is the result of a cooperative project where a scientific team, institutional bodies, and companies appointed to implement the final system are involved in co-design activities. We describe experiments to demonstrate the feasibility of the solution and discuss the main challenges to scaling the system at a national level. In particular, we report the results we obtained in annotating data with different low-resource methods and with solutions designed to combine these approaches in a meaningful way. An essential aspect of the proposed solution is a human-in-the-loop approach to control the output of the annotation algorithms in agreement with the organizational processes in place in Italian courts. Based on these results we advocate for the feasibility of the proposed approach and discuss the challenges that must be addressed to ensure the scalability and robustness of the proposed solution.}
}
@article{XIAO2023417,
title = {Knowledge graph-based manufacturing process planning: A state-of-the-art review},
journal = {Journal of Manufacturing Systems},
volume = {70},
pages = {417-435},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001577},
author = {Youzi Xiao and Shuai Zheng and Jiancheng Shi and Xiaodong Du and Jun Hong},
keywords = {Knowledge graph, Process planning, Process knowledge graph, CAPP},
abstract = {Computer-aided process planning is the bridge between computer-aided design and computer-aided manufacturing. With the advent of the intelligent manufacturing era, process knowledge is important for process planning. Knowledge graph is a semantic representation method of knowledge that has attracted extensive attention from the industry and academia. Process planning using the process knowledge graph has become an important development direction for computer-aided process planning. From the analysis of the published reviews, there have been many computer-aided process planning reviews with different focuses. We focus on the techniques and applications of knowledge graph in manufacturing process planning. Therefore, this paper comprehensively reviews knowledge graphs in manufacturing process planning. We analyze the key technologies of process knowledge graph, including process knowledge representation, process knowledge extraction, process knowledge graph construction, process knowledge graph refinement, process knowledge graph validation, and process generation. We also explore the combination of process knowledge graphs and large language models. Finally, potential future research directions are proposed.}
}
@article{WANG2024102502,
title = {Decision-guidance method for knowledge discovery and reuse in multi-goal engineering design problems},
journal = {Advanced Engineering Informatics},
volume = {61},
pages = {102502},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102502},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624001502},
author = {Ru Wang and Lin Guo and Yu Huang and Yan Yan},
keywords = {Multi-goal decision making, Knowledge discovery and reuse, Decision guidance, Decision preference, Ontology},
abstract = {When designing complex engineered systems, designers typically encounter different types of complexity in decision-making, such as learning trade-offs among conflicting system goals and concurrently meeting specifications of subsystems while complying with all constraints. Therefore, when managing knowledge of the concurrent design of multi-component systems, one must consider the decision interactions among designers with different interests and the coupling effects of subsystems. Conventional multi-objective optimization methods are inadequate in providing decision support on compromising the achievement of conflicting goals under various design circumstances; they are incompetent to provide knowledge from the supply side. They may also fail to solve the problems endorsed by decision-makers' evolving preferences; that is, they are incompetent in gaining knowledge from the demand side. Enhancing decision support and managing various stakeholders’ preferences is essential for engineering design problems. To address this issue, we propose a method to give systematic guidance for knowledge discovery and reuse in learning the trade-offs among conflicting goals with various preferences. Using the proposed method, a designer can (1) select appropriate design scenarios to compromise the achievement of system goals while maintaining an acceptable level of fidelity and (2) explore and refine the subsystems coupling in a structured and computable manner. We made the contributions above through (1) reusable knowledge identification and ontology development for creating and archiving the knowledge associated with the problem-solving process, (2) implementing interactive decision-making by adopting different guiding strategies, (3) working on satisfying solutions, and (4) developing icon-based knowledge representation and execution of decision workflows in the knowledge-based design guidance system. The whole process forms a loop, namely, the formulation-refinement-exploration-improvement loop. We demonstrate the efficacy of our method using a test problem, the design of a small-scale thermal system that is widely applied in freshwater production and agricultural irrigation.}
}
@article{ALSAIG2020106403,
title = {Contelog: A declarative language for modeling and reasoning with contextual knowledge},
journal = {Knowledge-Based Systems},
volume = {207},
pages = {106403},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106403},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120305347},
author = {Ammar Alsaig and Vangalur Alagar and Shiri Nematollaah},
keywords = {Knowledge-base systems, Contextual reasoning, Context, Contextual knowledge base systems, Knowledge representation, Declarative semantics, Datalog, Modularity, Reuse},
abstract = {Context-awareness is at the core of many modern-day applications in safety and secure-critical domains. In existing context-aware systems knowledge and context are not formally integrated, and consequently adaptation behaviors for safety-criticality cannot be formally reasoned. In modern day smart systems, such as healthcare and advanced manufacturing, context-awareness must be combined with contextual reasoning in order that new knowledge can be inferred and based on which strategic decisions can be made. Consequently, a rigorous approach is essential to represent contextual domain knowledge and inference rules in order to combine the logic of domain-based decision rules with contextual constraints for contextual reasoning, and decision making. In this paper we address this later issue and introduce a formal approach to achieve contextual reasoning. The framework that we create, called Contelog, conservatively extends the syntax and semantics of Datalog to reason with contextual knowledge. In this setting, contextual knowledge is reusable on its own. The significance is that by fixing the contextual knowledge, goal-specific analysis rules may be changed, and vice versa. By providing a theory of context, independent of the logic of the rules, we have developed a simple and sound context calculus using which it is possible to export knowledge reasoned in one context to another context. Query processing and implementation of Contelogprograms convince us that it has the capabilities to reason in systems where perception and cognition are formally combined for problem solving.}
}
@article{BENITEZANDRADES2020154,
title = {Social network analysis for personalized characterization and risk assessment of alcohol use disorders in adolescents using semantic technologies},
journal = {Future Generation Computer Systems},
volume = {106},
pages = {154-170},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19316796},
author = {José Alberto Benítez-Andrades and Isaías García-Rodríguez and Carmen Benavides and Héctor Alaiz-Moretón and Alejandro Rodríguez-González},
keywords = {Healthcare information system, Knowledge based systems, Ontologies, Semantic web, Social network analysis},
abstract = {Alcohol Use Disorder (AUD) is a major concern for public health organizations worldwide, especially as regards the adolescent population. The consumption of alcohol in adolescents is known to be influenced by seeing friends and even parents drinking alcohol. Building on this fact, a number of studies into alcohol consumption among adolescents have made use of Social Network Analysis (SNA) techniques to study the different social networks (peers, friends, family, etc.) with whom the adolescent is involved. These kinds of studies need an initial phase of data gathering by means of questionnaires and a subsequent analysis phase using the SNA techniques. The process involves a number of manual data handling stages that are time consuming and error-prone. The use of knowledge engineering techniques (including the construction of a domain ontology) to represent the information, allows the automation of all the activities, from the initial data collection to the results of the SNA study. This paper shows how a knowledge model is constructed, and compares the results obtained using the traditional method with this, fully automated model, detailing the main advantages of the latter. In the case of the SNA analysis, the validity of the results obtained with the knowledge engineering approach are compared to those obtained manually using the UCINET, Cytoscape, Pajek and Gephi to test the accuracy of the knowledge model.}
}
@article{SHRESTHA2025105110,
title = {Reimagining community-based indigenous tourism: Insights from the traditional knowledge of indigenous Newars of Nepal},
journal = {Tourism Management},
volume = {108},
pages = {105110},
year = {2025},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2024.105110},
url = {https://www.sciencedirect.com/science/article/pii/S0261517724002292},
author = {Roshis Krishna Shrestha and J.N. Patrick {L'Espoir Decosta} and Michelle Whitford},
keywords = {Community-based tourism, Indigenous knowledge, Indigenous methodology, Lived experiences, Reciprocity, Collective sensemaking},
abstract = {Community-based tourism serves as an economic catalyst for enhancing the socio-economic well-being of Indigenous peoples in developing countries. While extensive research has explored its functional and structural aspects, the potential of community-based tourism to foster cultural empowerment though Indigenous ontologies and traditions institutions remains unexplored. This ethnographic study investigates the role of community-based Indigenous tourism (CBIT) in reinforcing cultural identity among the Khokana Newars of Nepal through their traditional institution, Guthi. Employing Indigenous methodology and qualitative methods, the research reveals Guthi's role in disseminating cultural knowledge, preserving cultural memory, and strengthening individual and collective identity within tourism contexts. The study expands the discourse on CBIT by introducing cultural empowerment as a core element, highlighting the interplay between Indigenous institutions, community identity, and tourism development. It advocates for a paradigm shift towards culturally empowered tourism initiatives that actively integrate Indigenous voices and lived experiences, promoting cultural integrity, and self-determination.}
}
@incollection{MIZOGUCHI202472,
title = {Post-Processual Archaeology},
editor = {Efthymia Nikita and Thilo Rehren},
booktitle = {Encyclopedia of Archaeology (Second Edition) (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {72-83},
year = {2024},
isbn = {978-0-323-91856-5},
doi = {https://doi.org/10.1016/B978-0-323-90799-6.00184-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323907996001841},
author = {Koji Mizoguchi},
keywords = {Agency, Embodiment, Giddensian practice theory, Hermeneutics, Late-/post-modernity, Minorities, Multi-vocality, Ontology, Phenomenology, Power},
abstract = {This entry situates post-processual archaeology in the discursive space of contemporary society and analytically describes its constitutive characteristics as the consequences of shifts from the Modern to the Late-/Post-modern epistemic-ontological horizon. I describe how these shifts were implemented as concrete archaeological practices and examine the intended/unintended consequences generated by them. Then, I investigate how the post-processual movement impacted the ways in which archaeology is practiced in different parts of the world. The entry concludes by evaluating the impact of the inception of post-processual archaeology/archaeologies on Archaeology generally and considering what the future holds for archaeology after it.}
}
@article{KUNDU2024,
title = {A Comparative Study of Online Consumer Reviews Among Mainstream and Niche Products:},
journal = {International Journal of Business Analytics},
volume = {11},
number = {1},
year = {2024},
issn = {2334-4547},
doi = {https://doi.org/10.4018/IJBAN.353306},
url = {https://www.sciencedirect.com/science/article/pii/S2334454724000042},
author = {Supratim Kundu and Swapnajit Chakraborti},
keywords = {E-Commerce, Sentiment Analysis, Topic Modelling, Online Reviews, Word of Mouth, Ontology, Theme Generation, Consumer Motivation, Customer Relationship Management, Consumer Culture},
abstract = {ABSTRACT
The purpose of the paper is to understand the variations in the online consumer reviews among product categories across geographies and cultures. We consider the cases of mainstream products and their lesser-known niche counterparts providing similar utilities across emerging and developed markets. We find reviews vary in terms of characteristics and sentiments across the two categories and markets. The reviews for the niches contain in-depth discussions on some topics. We apply mixed method approach of quantitative analysis, topic modeling, sentiment analysis, and network analysis to arrive at the conclusion. The quantitative analyses confirm the variations, while the qualitative approach enables us to create a typology/ontology for the reviews. Based on the observations, we could successfully map various motives and cultural dimensions related to online reviews to our hierarchical typology. Managers may find our study interesting to boost their online review strategy according to the product category in a specific geography.}
}
@article{CAMIRE2023102505,
title = {Positive youth development as a guiding framework in sport research: Is it time to plan for a transition?},
journal = {Psychology of Sport and Exercise},
volume = {69},
pages = {102505},
year = {2023},
issn = {1469-0292},
doi = {https://doi.org/10.1016/j.psychsport.2023.102505},
url = {https://www.sciencedirect.com/science/article/pii/S1469029223001292},
author = {Martin Camiré and Fernando Santos and Tarkington Newman and Stewart Vella and Dany J. MacDonald and Michel Milistetd and Scott Pierce and Leisha Strachan},
keywords = {Ontology, Social justice, Posthumanism, Measurement, Life skills},
abstract = {Positive youth development is a popular guiding framework for studying the psychosocial development of youth. In sport research, for more than two decades, this framework has enhanced our understanding of the mechanisms involved in successful shifts from youth to adulthood. Nonetheless, scholars have recently taken a more critical stance on the positive youth development framework by elucidating some of its shortcomings. To help determine whether it may be warranted to plan for a transition from the positive youth development framework in sport research, a critical commentary is offered. The purpose of this commentary lies in situating three ontologically distinct arguments that depict the shortcomings of the positive youth development framework, namely the operationalization argument, the social justice argument, and the posthumanist argument. This paper is offered as an open invitation to instigate dialogue on what may come next for youth development in sport research and whether planning for a transition is warranted.}
}
@article{ABRAHAO2019106171,
title = {Assessing the effectiveness of goal-oriented modeling languages: A family of experiments},
journal = {Information and Software Technology},
volume = {116},
pages = {106171},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919301673},
author = {Silvia Abrahão and Emilio Insfran and Fernando González-Ladrón-de-Guevara and Marta Fernández-Diego and Carlos Cano-Genoves and Raphael {Pereira de Oliveira}},
keywords = {Requirements engineering, Goal modeling, GRL, I*, Controlled experiments},
abstract = {Context
Several goal-oriented languages focus on modeling stakeholders’ objectives, interests or wishes. However, these languages can be used for various purposes (e.g., exploring system solutions or evaluating alternatives), and there are few guidelines on how to use these models downstream to the software requirements and design artifacts. Moreover, little attention has been paid to the empirical evaluation of this kind of languages. In a previous work, we proposed value@GRL as a specialization of the Goal Requirements Language (GRL) to specify stakeholders’ goals when dealing with early requirements in the context of incremental software development.
Objective
This paper compares the value@GRL language with the i* language, with respect to the quality of goal models, the participants’ modeling time and productivity when creating the models, and their perceptions regarding ease of use and usefulness.
Method
A family of experiments was carried out with 184 students and practitioners in which the participants were asked to specify a goal model using each of the languages. The participants also filled in a questionnaire that allowed us to assess their perceptions.
Results
The results of the individual experiments and the meta-analysis indicate that the quality of goal models obtained with value@GRL is higher than that of i*, but that the participants required less time to create the goal models when using i*. The results also show that the participants perceived value@GRL to be easier to use and more useful than i* in at least two experiments of the family.
Conclusions
value@GRL makes it possible to obtain goal models with good quality when compared to i*, which is one of the most frequently used goal-oriented modeling languages. It can, therefore, be considered as a promising emerging approach in this area. Several insights emerged from the study and opportunities for improving both languages are outlined.}
}
@article{USODOMENECH2019938,
title = {A dialectical vision of mathematical models of complex systems},
journal = {Kybernetes},
volume = {49},
number = {3},
pages = {938-959},
year = {2019},
issn = {0368-492X},
doi = {https://doi.org/10.1108/K-01-2019-0032},
url = {https://www.sciencedirect.com/science/article/pii/S0368492X19000847},
author = {José Luis {Usó Doménech} and Josué Antonio Nescolarde-Selva and Lorena Segura-Abad and Hugh Gash},
keywords = {Feedback, Complex system, Data, Dialectics, Mathematical model, Ontological system, Reality, Simulation},
abstract = {Purpose
Mathematical models are constructed at the interface between practice, experience and theories. The function of models puts us on guard against the privilege granted to what is accepted as abstract and formal, and at the same time puts us on guard against a static and phenomenological conception of knowledge. The epistemology of models does not suppress in any way the objectives of science: only, a dogmatic conception concerning truth is removed, and dynamic and dialectical aspects of monitoring are stressed to establish the most viable model. The purpose of this paper is to examine hybrid methodologies (inductive-deductive) that may either propose hypothetical causal relations and seek support for them in field data or detect causal relations in field data and propose hypotheses for the relations detected.
Design/methodology/approach
The authors follow a dialectical analysis for a type of inductive-deductive model.
Findings
In this work, the authors present an inductive-deductive methodology whose practical result satisfies the Hegelian dialectic. The consequent implication of their mutual reciprocal integration produces abstractions from the concrete that enable thought. The real problem in this case is a given ontological system or reality.
Originality/value
The essential elements of the models – variables, equations, simulation and feedback – are studied using a dialectic Hegelian theory.}
}
@article{GARBA2022111120,
title = {Self-adaptive mobile web service discovery framework for Dynamic Mobile Environment},
journal = {Journal of Systems and Software},
volume = {184},
pages = {111120},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111120},
url = {https://www.sciencedirect.com/science/article/pii/S016412122100217X},
author = {Salisu Garba and Radziah Mohamad and Nor Azizah Saadon},
keywords = {Self-adaptive, Mobile web service, Service discovery, Negative Selection Algorithm, Dynamic Mobile Environment},
abstract = {This paper proposes a self-adaptive mobile web service (MWS) discovery framework for a dynamic mobile environment (DME) to deal with MWS proliferation, dynamic context, and irrelevant MWS discovery challenges. The main contribution of this research includes an improvement of the matchmaking algorithm, enhanced MWS categorization approach, and extensible meta-context ontology that represents the context information in DME. This was achieved by enabling the self-adaptive matchmaker to learn MWS relevance using a Modified-Negative Selection Algorithm (M-NSA) and retrieve the most relevant MWS based on the current context of the discovery. To assess the proposed framework, series of experiments was carried out using publicly-available datasets. The performance of the framework is evaluated against the state-of-the-art frameworks. It was found that the proposed framework is more effective and attained better binary and graded relevance when subjected to context variations which are prevalent in DME. This is useful for service-based application designers and other MWS clients.}
}
@article{GILLINGS2025100121,
title = {How humans and machines identify discourse topics: A methodological triangulation},
journal = {Applied Corpus Linguistics},
volume = {5},
number = {1},
pages = {100121},
year = {2025},
issn = {2666-7991},
doi = {https://doi.org/10.1016/j.acorp.2025.100121},
url = {https://www.sciencedirect.com/science/article/pii/S2666799125000048},
author = {Mathew Gillings and Sylvia Jaworska},
keywords = {Triangulation, Topic modelling, CADS, Concordance analysis, Close reading, LLMs},
abstract = {Identifying and exploring discursive topics in texts is of interest to not only linguists, but to researchers working across the full breadth of the social sciences. This paper reports on an exploratory study assessing the influence that analytical method has on the identification and labelling of topics, which might lead to varying interpretations of texts. Using a corpus of corporate sustainability reports, totalling 98,277 words, we asked 6 different researchers to interrogate the corpus and decide on its main ‘topics’ via four different methods: LLM-assisted analyses; topic modelling; concordance analysis; and close reading. These methods differ according to the amount of data that can be analysed at once, the amount of textual context available to the researcher, and the focus of the analysis (i.e., micro to macro). The paper explores how the identified topics differed both between analysts using the same method, and between methods. We conclude with a series of tentative observations regarding the benefits and limitations of each method, and offer recommendations for researchers in choosing which analytical technique to select.}
}
@article{XIE2025111331,
title = {The underlying neurobiological basis of gray matter volume alterations in schizophrenia with auditory verbal hallucinations: A meta-analytic investigation},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {138},
pages = {111331},
year = {2025},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2025.111331},
url = {https://www.sciencedirect.com/science/article/pii/S0278584625000855},
author = {Yuanjun Xie and Tian Zhang and Chaozong Ma and Muzhen Guan and Chenxi Li and Lingling Wang and Xinxin Lin and Yijun Li and Zhongheng Wang and Huaning Wang and Peng Fang},
keywords = {Schizophrenia, Auditory verbal hallucinations, Gray matter volume, Meta-analysis},
abstract = {Schizophrenia patients with auditory verbal hallucinations (AVH) frequently exhibit brain structural alterations, particularly reductions in gray matter volume (GMV).Understanding the neurobiological mechanisms underlying the changes is essential for advancing treatment strategies. To address this, a meta-analysis was conducted to identify GMV changes in schizophrenia patients with AVH and their associations with gene expression and neurotransmitter receptor profiles. The results indicated significant GMV reductions in the left and the right insula, as well as the left anterior cingulate cortex. Ontology analysis of genes associated with GMV alternations revealed enrichment in biological processes related to ion transport and synaptic transmission. Hub genes from the KCN, SCN, GN, and PRK families, along with neurotransmitter receptors such as D2, VAChT, and mGluR5, showed significant correlations with GMV changes. Furthermore, multivariate linear regression analysis demonstrated that GNB2, GNB4, PRKCG, D2, and mGluR5 significantly predicted GMV alternations. These findings suggest that GMV reductions in schizophrenia with AVH are linked to disruptions in neurobiological processes involving specific genes and neurotransmitter systems, highlighting the potential targets for therapeutic intervention.}
}
@article{DE2023100023,
title = {Towards Improvement of Grounded Cross-lingual Natural Language Inference with VisioTextual Attention},
journal = {Natural Language Processing Journal},
volume = {4},
pages = {100023},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100023},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000201},
author = {Arkadipta De and Maunendra Sankar Desarkar and Asif Ekbal},
keywords = {Attention mechanism, Textual entailment, Natural Language Inference, Grounded textual entailment, Cross-lingual textual entailment},
abstract = {Natural Language Inference (NLI) has been one of the fundamental tasks in Natural Language Processing (NLP). Recognizing Textual Entailment (RTE) between the two pieces of text is a crucial problem. It adds further challenges when it involves two languages, i.e., in the cross-lingual scenario. In this paper, we propose VisioTextual-Attention (VTA) — an effective visual–textual coattention mechanism for multi-modal cross-lingual NLI. Through our research, we show that instead of using only linguistic input features, introducing visual features supporting the textual inputs improves the performance of NLI models if an effective cross-modal attention mechanism is carefully constructed. We perform several experiments on a standard cross-lingual textual entailment dataset in Hindi–English language pairs and show that the addition of visual information to the dataset along with our proposed VisioTextual Attention (VTA) enhances performance and surpasses the current state-of-the-art by 4.5%. Through monolingual experiments, we also show that the proposed VTA mechanism surpasses monolingual state-of-the-art by a margin of 2.89%. We argue that our VTA mechanism is model agnostic and can be used with other deep learning-based architectures for grounded cross-lingual NLI.}
}
@article{MAN2023106244,
title = {Synthesis of multilevel knowledge graphs: Methods and technologies for dynamic networks},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106244},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106244},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623004281},
author = {Tianxing Man and Alexander Vodyaho and Dmitry I. Ignatov and Igor Kulikov and Nataly Zhukova},
keywords = {Knowledge graphs, Multilevel object model, Inductive synthesis, Deductive synthesis, Ontology, Telecommunication benchmark},
abstract = {Knowledge Graphs is one of the most popular techniques for knowledge-based modelling in various subdomains of modern AI technologies ranging from natural language processing to e-commerce recommendations and cyberphysical systems. Even complex technical systems like telecommunication networks could be modelled by means of Knowledge Graphs. However, there are serious challenges when we deal with such systems having a huge number of interconnected elements (e.g. technical objects and their groups) that change over time. Thus, up-to-date there is no adequate solution for not only telecommunication networks but for any complex dynamic systems where inductive and deductive synthesis of large Knowledge Graph based models that are easily reconfigurable and scalable is required. We state and solve the problem of building such models for one of the most common types of objects where models can be represented as hierarchical re-configurable structures. This representation enables recent advances in multilevel inductive–deductive synthesis for model building. From a methodological viewpoint, we propose a novel complex approach to multilevel synthesis for objects with dynamic hierarchical structure based on modified methods for inductive and deductive synthesis of Knowledge Graphs. From a practical perspective we present a real case-study on an interactive service for digital cable TV networks – which is especially interesting for data engineers and scientists – where various problems ranging from network health monitoring to channel advertising can be solved with the same hierarchical model. We release an openly available domain benchmark, which features two realistic datasets (namely, for SPARQL querying performance analysis, and for our case study on dynamic network monitoring). Last but not least, our experiments with recent state-of-the-art approaches to knowledge graph querying Abdelaziz et al. (2017) show that the developed models of multilevel synthesis reduce the time complexity up to 73% on practice compared to the baselines, and are lossless and able to beat their competitors based on parallel knowledge graph processing from 4% to 91% in terms of computational time (depending on the query type). Further parallelisation of our multilevel models is even more efficient (the reduction of query processing time is about 40%–45%) and opens promising prospects for the creation and exploitation of dynamic Knowledge Graphs in practice.}
}
@article{MAURYA2024990,
title = {Sentiment Analysis: A Hybrid Approach on Twitter Data},
journal = {Procedia Computer Science},
volume = {235},
pages = {990-999},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.094},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924007701},
author = {Chandra Gupta Maurya and Sudhanshu Kumar Jha},
keywords = {Sentiment Analysis, textual mining, hybrid approach, opnion clustring, twitter data analysis, visual ontology},
abstract = {In the present scenario, Internet communities, forum and blogging sites play a very crucial role to present opinions, views and the comments on various events. As the reachability of sites are beyond the control of national boundaries, sometimes this leads to the conviction and persuasion of thoughts without considering any legal subsequences, and also influence the belief of others as well, thus finding or identifying the sentiments of public from the social media content is one of the major research issue. Analysis of the sentiments of social media data is very difficult to understand as this typically does not exhibit a suspicious pattern in the flow of information like individual's actual opinion on a specific occasion etc. With the advancement of methodology and easy to use, the users of social media sites are now expressing their opinions, sharing their views and experiences through images, text, animation, audio, video etc. Because of the complexity the conventional text based sentiment analysis procedure, a lot of methods have been evolved, however studies suggest mostly more complicated procedure and approaches. Twitter® (or X), is one of the most common platform used widely by individual to express their opinions and sentiments on various events. Twitter sentiment analysis basically deals with the analysis of twitter quotes to find the hidden pattern in the sentiments expressed by the users in past. This paper aims to takes the challenges regarding social media sentiments analysis and developed a hybrid approach (Text and visual sentiment) on twitter data for sentiment analysis by using NLP-based opinion clustering, textual mining, emotion API and some machine learning techniques for visual ontology. Simulation result shows the significance of work.}
}
@article{SONGYANG2024101208,
title = {Simulation of association rule mining based on sensor networks in Chinese language learning recommendation system for college students},
journal = {Measurement: Sensors},
volume = {33},
pages = {101208},
year = {2024},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2024.101208},
url = {https://www.sciencedirect.com/science/article/pii/S2665917424001843},
author = {Lv Songyang},
keywords = {Reinforcement learning, Association rules, Online learning, Resource recommendation},
abstract = {Based on the increasing learning needs of students that cannot be met by traditional education in schools, the use of online learning platforms has become a way for many college students to learn Chinese, thereby improving their academic performance and literary literacy. However, it is difficult to guarantee the service quality of online platforms for Chinese learning at present, so this paper proposes an association rule mining algorithm to strengthen Chinese online learning. We using user service quality as a constraint to improve spectrum utilization and energy efficiency, and defining the user state space, borrowing the obtained resource allocation optimization function to reward a small portion of users' communication costs, ultimately obtaining user state space information and one-dimensional state space data. This algorithm performs grouping operations on the data estimated by a large amount of computation, and divides the data into balanced categories to reduce the amount of input data in the network. The performance test results show that this paper has made a great breakthrough in the personalization of Chinese learning, and has outstanding performance in the processing and classification of Big data. There are also some solutions to the problem of too much existing data. In a period of use experience analysis report, we found that the online platform for Chinese learning can give consideration to students' personalized needs and experience.}
}
@article{MARANGA2023,
title = {Comprehensive Functional Annotation of Metagenomes and Microbial Genomes Using a Deep Learning-Based Method},
journal = {mSystems},
volume = {8},
number = {2},
year = {2023},
issn = {2379-5077},
doi = {https://doi.org/10.1128/msystems.01178-22},
url = {https://www.sciencedirect.com/science/article/pii/S2379507723003471},
author = {Mary Maranga and Pawel Szczerbiak and Valentyn Bezshapkin and Vladimir Gligorijevic and Chris Chandler and Richard Bonneau and Ramnik J. Xavier and Tommi Vatanen and Tomasz Kosciolek},
keywords = {genome, metagenome, orthology, pangenome, deep learning, functional annotation, gene function, metagenome-assembled genomes, metagenomics, microbiome},
abstract = {The past decade has seen advancement in high-throughput sequencing technologies resulting in rapid accumulation of genomic data from microbial communities. While this growth in sequence data and gene discovery is impressive, the majority of microbial gene functions remain uncharacterized.
ABSTRACT
Comprehensive protein function annotation is essential for understanding microbiome-related disease mechanisms in the host organisms. However, a large portion of human gut microbial proteins lack functional annotation. Here, we have developed a new metagenome analysis workflow integrating de novo genome reconstruction, taxonomic profiling, and deep learning-based functional annotations from DeepFRI. This is the first approach to apply deep learning-based functional annotations in metagenomics. We validate DeepFRI functional annotations by comparing them to orthology-based annotations from eggNOG on a set of 1,070 infant metagenomes from the DIABIMMUNE cohort. Using this workflow, we generated a sequence catalogue of 1.9 million nonredundant microbial genes. The functional annotations revealed 70% concordance between Gene Ontology annotations predicted by DeepFRI and eggNOG. DeepFRI improved the annotation coverage, with 99% of the gene catalogue obtaining Gene Ontology molecular function annotations, although they are less specific than those from eggNOG. Additionally, we constructed pangenomes in a reference-free manner using high-quality metagenome-assembled genomes (MAGs) and analyzed the associated annotations. eggNOG annotated more genes on well-studied organisms, such as Escherichia coli, while DeepFRI was less sensitive to taxa. Further, we show that DeepFRI provides additional annotations in comparison to the previous DIABIMMUNE studies. This workflow will contribute to novel understanding of the functional signature of the human gut microbiome in health and disease as well as guiding future metagenomics studies.
IMPORTANCE The past decade has seen advancement in high-throughput sequencing technologies resulting in rapid accumulation of genomic data from microbial communities. While this growth in sequence data and gene discovery is impressive, the majority of microbial gene functions remain uncharacterized. The coverage of functional information coming from either experimental sources or inferences is low. To solve these challenges, we have developed a new workflow to computationally assemble microbial genomes and annotate the genes using a deep learning-based model DeepFRI. This improved microbial gene annotation coverage to 1.9 million metagenome-assembled genes, representing 99% of the assembled genes, which is a significant improvement compared to 12% Gene Ontology term annotation coverage by commonly used orthology-based approaches. Importantly, the workflow supports pangenome reconstruction in a reference-free manner, allowing us to analyze the functional potential of individual bacterial species. We therefore propose this alternative approach combining deep-learning functional predictions with the commonly used orthology-based annotations as one that could help us uncover novel functions observed in metagenomic microbiome studies.}
}
@article{ZHANG2024,
title = {Threat Attribution and Reasoning for Industrial Control System Asset},
journal = {International Journal of Ambient Computing and Intelligence},
volume = {15},
number = {1},
year = {2024},
issn = {1941-6237},
doi = {https://doi.org/10.4018/IJACI.333853},
url = {https://www.sciencedirect.com/science/article/pii/S1941623724000019},
author = {Shuqin Zhang and Peiyu Shi and Tianhui Du and Xinyu Su and Yunfei Han},
keywords = {Assets, Attribution, Industrial Control System, Reasoning, Threat Modeling},
abstract = {ABSTRACT
Due to the widespread use of the industrial internet of things, the industrial control system has steadily transformed into an intelligent and informational one. To increase the industrial control system’s security, based on industrial control system assets, this paper provides a method of threat modeling, attributing, and reasoning. First, this method characterizes the asset threat of an industrial control system by constructing an asset security ontology based on the asset structure. Second, this approach makes use of machine learning to identify assets and attribute the attacker’s attack path. Subsequently, inference rules are devised to replicate the attacker’s attack path, thereby reducing the response time of security personnel to threats and strengthening the semantic relationship between asset security within industrial control systems. Finally, the process is used in the simulation environment and real case scenario based on the power grid, where the assets and attacks are mapped. The actual attack path is deduced, and it demonstrates the approach’s effectiveness.}
}
@article{SLATER2023106425,
title = {Klarigi: Characteristic explanations for semantic biomedical data},
journal = {Computers in Biology and Medicine},
volume = {153},
pages = {106425},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.106425},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522011337},
author = {Karin Slater and John A. Williams and Paul N. Schofield and Sophie Russell and Samantha C. Pendleton and Andreas Karwath and Hilary Fanning and Simon Ball and Robert Hoehndorf and Georgios V. Gkoutos},
keywords = {Semantic explanation, Ontology, Semantic analysis, Enrichment analysis, Phenotypes, Explicability, Phenotype profiles},
abstract = {Annotation of biomedical entities with ontology classes provides for formal semantic analysis and mobilisation of background knowledge in determining their relationships. To date, enrichment analysis has been routinely employed to identify classes that are over-represented in annotations across sets of groups, such as biosample gene expression profiles or patient phenotypes, and is useful for a range of tasks including differential diagnosis and causative variant prioritisation. These approaches, however, usually consider only univariate relationships, make limited use of the semantic features of ontologies, and provide limited information and evaluation of the explanatory power of both singular and grouped candidate classes. Moreover, they are not designed to solve the problem of deriving cohesive, characteristic, and discriminatory sets of classes for entity groups. We have developed a new tool, called Klarigi, which introduces multiple scoring heuristics for identification of classes that are both compositional and discriminatory for groups of entities annotated with ontology classes. The tool includes a novel algorithm for derivation of multivariable semantic explanations for entity groups, makes use of semantic inference through live use of an ontology reasoner, and includes a classification method for identifying the discriminatory power of candidate sets, in addition to significance testing apposite to traditional enrichment approaches. We describe the design and implementation of Klarigi, including its scoring and explanation determination methods, and evaluate its use in application to two test cases with clinical significance, comparing and contrasting methods and results with literature-based and enrichment analysis methods. We demonstrate that Klarigi produces characteristic and discriminatory explanations for groups of biomedical entities in two settings. We also show that these explanations recapitulate and extend the knowledge held in existing biomedical databases and literature for several diseases. We conclude that Klarigi provides a distinct and valuable perspective on biomedical datasets when compared with traditional enrichment methods, and therefore constitutes a new method by which biomedical datasets can be explored, contributing to improved insight into semantic data.}
}
@article{ABEYNAYAKE20211122,
title = {A roadmap for business model adaptation in the construction industry: a structured review of business model research},
journal = {Construction Innovation: Information, Process, Management},
volume = {22},
number = {4},
pages = {1122-1137},
year = {2021},
issn = {1471-4175},
doi = {https://doi.org/10.1108/CI-05-2020-0077},
url = {https://www.sciencedirect.com/science/article/pii/S1471417521000439},
author = {Dilani Niroshika Abeynayake and BAKS Perera and Chandanie Hadiwattege},
keywords = {Construction industry, Business model, Roadmap, Business model ontology, Business model research, Construction business development},
abstract = {Purpose
Survival challenges compel construction firms to change their business thinking on adapting business models (BMs). Unlike in the other fields, in the construction field, it is still a novel concept, probably because BM adaptation in the industry has not been sufficiently studied. Hence, the purpose of this study is to set a roadmap for the effective adaptation of the BM concept in the construction industry.
Design/methodology/approach
To develop a roadmap, this study analysed BM literature through a comprehensive literature review by finding the BM research development stages using studies done in other fields and locating construction industry BM literature.
Findings
No stage-wise development of BM research has taken place in the construction industry. The four BM research development stages, namely, defining BMs, listing BM components, describing BM components and modelling BM components as a business model ontology (BMO) in relation to the construction industry have to be followed for proper BM adaptation in the construction industry.
Originality/value
This study provided an overview of and a roadmap for BM adaptation in the construction industry for the benefit of future researchers. The proper adaptation of the BM concept through a BMO will enable construction business managers to easily innovate, design and change construction BMs to streamline their business thinking.}
}
@article{KOMBOLONGAH2022108849,
title = {A new semantic resource responding to the principles of Open Science: The meat thesaurus as an IT tool for dialogue between sector actors},
journal = {Meat Science},
volume = {192},
pages = {108849},
year = {2022},
issn = {0309-1740},
doi = {https://doi.org/10.1016/j.meatsci.2022.108849},
url = {https://www.sciencedirect.com/science/article/pii/S0309174022001176},
author = {Moise {Kombolo Ngah} and Jérémy Yon and François Landrieu and Brigitte Richon and Sophie Aubin and Jean-François Hocquette},
keywords = {Meat, Dictionary, Thesaurus, Ontology},
abstract = {Nowadays, it is important to make the results of scientific research accessible in a simple and understandable way according to the Open Science policy. This movement uses tools to enhance findability and interoperability of data. This paper describes the transformation of the meat dictionary published by the French Meat Academy as a book into a machine actionable and freely accessible terminological resource based on the SKOS standard format. This thesaurus contains 1567 concepts describing the meat production chain. This work was carried out by experts in semantic web, meat biology and meat vocabulary. This thesaurus can be used to index articles, journals and datasets, thus facilitating consultation; it can also be used to facilitate interoperability of the indexed datasets and provide contextual definitions for building ontologies, i.e. formal descriptions of knowledge for reasoning on data. The thesaurus can be useful to enrich other vocabularies with new knowledge, such as French specificities in terms of meat cuts or definitions.}
}
@article{HUSSAIN2024101115,
title = {A state-of-the-art universal machine learning framework for decoding suspect coded messages},
journal = {Measurement: Sensors},
volume = {33},
pages = {101115},
year = {2024},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2024.101115},
url = {https://www.sciencedirect.com/science/article/pii/S2665917424000916},
author = {Syed Hussain and Pakkir {Mohideen S}},
keywords = {Machine learning, Cryptography, Linear and differential cryptanalysis, Encryption, And decryption, Instant message-based social networking, System for detecting suspicious messages, Or SMDs. statistical natural language processing (SNLP), Association rule mining (ARM)},
abstract = {This proposed framework aims to mitigate global terrorist threats, suspicious activities, and legal violations by introducing an innovative system that monitors encrypted short-text messages across instant messaging programs. In the current landscape, criminals, terrorists, and clandestine entities extensively employ encrypted messages for illicit endeavours on a global scale. Despite the escalating use of encrypted messages on social media platforms, a comprehensive solution for combating online crime is notably absent. The outlined framework addresses this gap by employing a cryptographic system that integrates semantic web ontology, a Predefined Dataset, and machine learning technology for encryption, decryption, and cryptanalysis. A key feature is the framework's ability to predict potential crimes by analyzing blogs for suspicious terminology conveyed through coded information. The identified information is then promptly communicated to law enforcement agencies, thereby alleviating the burden on numerous security entities and offering an efficient strategy against global crime. What sets this proposed framework apart is its unique utilization of encrypted text, a dimension that has remained unexplored in previous studies. The cohesive integration of its components expedites the process of anticipating crime types and significantly reduces the incidence of encrypted crimes on a global scale.}
}
@article{BEMTHUIS2025127571,
title = {Towards integrating process mining with agent-based modeling and simulation: State of the art and outlook},
journal = {Expert Systems with Applications},
volume = {281},
pages = {127571},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127571},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425011935},
author = {Rob H. Bemthuis and Sanja Lazarova-Molnar},
keywords = {Agent-based modeling, Agent-based simulation, Process mining, Systematic literature review},
abstract = {Agent-based modeling and simulation (ABMS) is a valuable tool for assessing complex socio-technical systems and is becoming increasingly advanced through the integration with data-driven capabilities. Process mining is an emerging data-driven discipline that combines elements from data mining and process modeling to gain insights into process execution through tasks such as process discovery, conformance checking, and process enhancement using event data. This study explores the role of process mining and its impact on the ABMS paradigm, identifying the current state of the art, gaps in the literature, and future directions for integrating process mining with ABMS. A systematic literature review is conducted to examine how ABMS and process mining techniques are jointly employed to address challenges reported in the literature. From an initial pool of 189 publications, a final set of 20 papers was synthesized, their primary contributions were discussed, and open issues and challenges for future research were identified. Although the integrated field of process mining and ABMS shows an upward trend in publications, it remains modest and requires further efforts to achieve synergistic improvements in socio-technical systems. The findings offer initial guidance for promising research directions.}
}
@article{QIU2022278,
title = {Adversarial attack and defense technologies in natural language processing: A survey},
journal = {Neurocomputing},
volume = {492},
pages = {278-307},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222003861},
author = {Shilin Qiu and Qihe Liu and Shijie Zhou and Wen Huang},
keywords = {Textual adversarial example, Adversarial attack, Adversarial defense, Natural language processing, Artificial intelligence},
abstract = {Recently, the adversarial attack and defense technology has made remarkable achievements and has been widely applied in the computer vision field, promoting its rapid development in other fields, primarily the natural language processing domain. However, discrete semantic texts bring additional restrictions and challenges to successfully implementing adversarial attacks and defenses. This survey systematically summarizes the current progress of adversarial techniques in the natural language processing field. We first briefly introduce the textual adversarial example’s particularity, vectorization, and evaluation metrics. More importantly, we categorize textual adversarial attacks according to the combination of semantic granularity and example generation strategy. Next, we present commonly used datasets and adversarial attack applications in diverse natural language processing tasks. Besides, we classify defense strategies as passive and active methods considering both input data and victim models. Finally, we present several challenging issues and future research directions in this domain.}
}
@article{ARTIGAPURCELL2024103076,
title = {Relational resources: Moving from plural to entangled extractivisms},
journal = {Political Geography},
volume = {110},
pages = {103076},
year = {2024},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2024.103076},
url = {https://www.sciencedirect.com/science/article/pii/S0962629824000258},
author = {James Alejandro Artiga-Purcell},
keywords = {Extractivism, Relational resources, Resource politics, Social movements, Decolonial},
abstract = {Increasing studies of extractivisms, in the plural, examine the diverse politics that permeate extractive activities. However useful, this appeal to plurality is problematic. Plurality without relationality risks justifying some extractive activities over others, without addressing how they relate. Drawing on anti-essentialist, decolonial, and political ecological scholarship, this article proposes an ontological reframing of extractivisms that troubles the division and comparison that pervade notions of taxonomic plurality, and emphasizes the mutual constitution articulated within the concept of relationality. Analyses of relational resources are better equipped to illuminate the power-laden associations through which supposedly distinct commodity values, extractive geographies, sectors, and governance regimes shape one another. Beyond complicating rigid definitions of extractive categories, this ontological shift requires distinct methodologies for understanding and contesting entangled extractive interests and logics. It implicates what forms of anti-extractive resistance, solidarities, and alternative futures become viable, thinkable, and necessary.}
}
@article{JING2023110524,
title = {An integrated implicit user preference mining approach for uncertain conceptual design decision-making: A pipeline inspection trolley design case study},
journal = {Knowledge-Based Systems},
volume = {270},
pages = {110524},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110524},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123002745},
author = {Liting Jing and Jingwei Yang and Junfeng Ma and Jing Xie and Jiquan Li and Shaofei Jiang},
keywords = {Natural language processing, Conceptual design decision-making, Implicit design preference, Semantic network, Rough VIKOR},
abstract = {User preference mining is one of the most crucial activities in the conceptual design process, aiming to gain customer attentions at the early design stage. Since user preference is usually ambiguous, existing methods of preference extraction and assessment strongly rely on design experience and hard to analyze the implicit design preferences at semantic level. In addition, the abstraction of the knowledge representation prevents a reasonable judgment of the scheme’s potential value. To fill these gaps, by utilizing natural language processing (NLP) technology and semantic network, an integrated implicit design preference (DP) mining approach is proposed to support the uncertain conceptual design decision-making. The proposed approach has three parts: first, the design knowledge semantic network (DKSN) is constructed by using NLP and patent data, which provides a new retrieval mode for function solving; second, TF–IDF model is introduced to extract the DPs, a similarity matrix between the principle solution (PS) retrieved from DKSN and design characteristic (DC) is constructed using word vector model, which can mine the potential satisfaction of various DPs in each PS; and third, based on the satisfaction matrix of PSs, a normalized comprehensive value matrix of the schemes is constructed, and the optimal scheme is obtained using rough VIKOR (VIseKriterijumska Optimizacija I Kompromisno Resenje). A case study of pipeline inspection trolley design is used to validate the proposed approach, sensitivity analysis shows that the proposed model truly captures the scheme’s intention. Additionally, the constructed DKSN has positive implications for assisting designers in exploring the design space.}
}
@article{LARIO2023104251,
title = {A method for structuring complex clinical knowledge and its representational formalisms to support composite knowledge interoperability in healthcare},
journal = {Journal of Biomedical Informatics},
volume = {137},
pages = {104251},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104251},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422002568},
author = {Robert Lario and Kensaku Kawamoto and Davide Sottara and Karen Eilbeck and Stanley Huff and Guilherme {Del Fiol} and Richard Soley and Blackford Middleton},
keywords = {Knowledge representation, Knowledge reference architecture, Medical knowledge, Clinical practice guidelines},
abstract = {Introduction
The use and interoperability of clinical knowledge starts with the quality of the formalism utilized to express medical expertise. However, a crucial challenge is that existing formalisms are often suboptimal, lacking the fidelity to represent complex knowledge thoroughly and concisely. Often this leads to difficulties when seeking to unambiguously capture, share, and implement the knowledge for care improvement in clinical information systems used by providers and patients.
Objectives
To provide a systematic method to address some of the complexities of knowledge composition and interoperability related to standards-based representational formalisms of medical knowledge.
Methods
Several cross-industry (Healthcare, Linguistics, System Engineering, Standards Development, and Knowledge Engineering) frameworks were synthesized into a proposed reference knowledge framework. The framework utilizes IEEE 42010, the MetaObject Facility, the Semantic Triangle, an Ontology Framework, and the Domain and Comprehensibility Appropriateness criteria. The steps taken were: 1) identify foundational cross-industry frameworks, 2) select architecture description method, 3) define life cycle viewpoints, 4) define representation and knowledge viewpoints, 5) define relationships between neighboring viewpoints, and 6) establish characteristic definitions of the relationships between components. System engineering principles applied included separation of concerns, cohesion, and loose coupling.
Results
A “Multilayer Metamodel for Representation and Knowledge“ (M*R/K) reference framework was defined. It provides a standard vocabulary for organizing and articulating medical knowledge curation perspectives, concepts, and relationships across the artifacts created during the life cycle of language creation, authoring medical knowledge, and knowledge implementation in clinical information systems such as electronic health records (EHR).
Conclusion
M*R/K provides a systematic means to address some of the complexities of knowledge composition and interoperability related to medical knowledge representations used in diverse standards. The framework may be used to guide the development, assessment, and coordinated use of knowledge representation formalisms. M*R/K could promote the alignment and aggregated use of distinct domain-specific languages in composite knowledge artifacts such as clinical practice guidelines (CPGs).}
}
@article{CHEEMA2023626,
title = {A natural language interface for automatic generation of data flow diagram using web extraction techniques},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {2},
pages = {626-640},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S131915782300006X},
author = {Sehrish Munawar Cheema and Saman Tariq and Ivan Miguel Pires},
keywords = {Data flow, Business logic, Decomposition, Data flow visualisation systems, NLP, Natural Language Interface (NLI), Web scraping, Crawler, Graph theory, Adjacency matrices},
abstract = {To model the data and functions in various computer science applications, the researcher uses a Data Flow Diagram (DFD). DFD has been constructed using [open-source software tools that provide users with different shapes and environments. However, the existing approaches require substantial human effort, the validity of the generated output is still a loophole, and they have never gained traction in practice. Our research objective is to develop a semi-automated tool for drawing complex Data Flow Diagrams in the shortest time according to the specified features of the intended system. We developed a Natural Language Interface (NLI) that allows the user to compose a query and identify the system functionality and constraints for the composition of DFD. Natural Language Processing (NLP) techniques are applied to scrapped data to extract the keywords and develop a data repository. Also, we developed rule-based algorithms to map user queries onto respective token shapes to draw the required functionality into appropriate levels of DFD. For verification, output DFDs were converted into conceptual digraphs using adjacency and permutation matrices to evaluate isomorphism. The empirical results reflect that the DFDs generated by the system are correct, complete, and significant.}
}
@article{CHEN2024102593,
title = {AskNatureNet: A divergent thinking tool based on bio-inspired design knowledge},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102593},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102593},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002416},
author = {Liuqing Chen and Zebin Cai and Zhaojun Jiang and Jianxi Luo and Lingyun Sun and Peter Childs and Haoyu Zuo},
keywords = {Bio-inspired design, Semantic network, Divergent thinking, Design creativity, Design ideation},
abstract = {Divergent thinking is a process in design by exploring multiple possible solutions, is crucial in the early stages of design to break fixation and expand the design ideation. Design-by-Analogy promotes divergent thinking, by studying solutions have solved similar problems and using this knowledge to make inferences and solve problems in new and unfamiliar situations. Bio-inspired design (BID) is a form of design by analogy and its knowledge provides diverse sources for analogy, making BID knowledge as a potential source for divergent thinking. Existing BID database has focused on collecting BID cases and facilitating the retrieval of biological knowledge. Despite its success, applying BID knowledge into divergent thinking still encounters challenge, as the association between source domain and target domain are always limited within a single case. In this work, a novel approach is proposed to support divergent thinking from three subsequent phases: encoding, retrieval and mapping. Specifically, biological knowledge is encoded in a triple form by employing a large language model (LLM) to extract key information from a well-known BID knowledge base. The created triples are implemented in a semantic network to facilitate bidirectional retrieval modes: problem-driven and solution-driven, as well as mapping for divergent thinking. The mapping algorithm calculates the semantic similarity between nodes in the semantic network based on their attributes in three progressive steps by following the paradigm of divergent thinking. The proposed approach is implemented as tool called AskNatureNet,11https://www.asknaturenet.com/. which supports divergent thinking by retrieving and mapping knowledge in a visualized interactive semantic network. An ideation case study on evaluating the effectiveness of AskNatureNet shows that our tool is capable of supporting divergent thinking efficiently.}
}
@article{DREXEL20222950,
title = {Three-Faceted Manufacturing Knowledge Representation in Cloud Environments*},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2950-2955},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.180},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322021942},
author = {Damian Drexel and Ralph Hoch},
keywords = {cloud manufacturing, knowledge representation, ontologies, abstraction, distributed manufacturing},
abstract = {A trend from centralized to decentralized production is emerging in the manufacturing domain leading to new and innovative approaches for long-established production methods. A technology supporting this trend is Cloud Manufacturing, which adapts technologies and concepts known from cloud computing to the manufacturing domain. A core aspect of Cloud Manufacturing is representing knowledge about manufacturing, e.g., machine capabilities, in a suitable form. This knowledge representation should be flexible and adaptable so that it fits across various manufacturing domains, but, at the same time, should also be specific and exhaustive. We identify three core capabilities that such a platform has to support, i.e., the product, the process and the production. We propose representing this knowledge in semantically specified knowledge graphs, essentially creating three through features interconnected ontologies each representing a facet of manufacturing. Finally, we present an exemplary implementation of a Cloud Manufacturing platform using this representation and its advantages.}
}
@article{LOU2023102236,
title = {A function-behavior mapping approach for product conceptual design inspired by memory mechanism},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102236},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102236},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003646},
author = {Shanhe Lou and Yixiong Feng and Yicong Gao and Hao Zheng and Tao Peng and Jianrong Tan},
keywords = {Conceptual design, Situated function-behavior-structure, Memory mechanism, Reinforcement learning},
abstract = {Conceptual design is a pivotal stage for new product development that relies more on designers to solve open-ended and ill-defined problems. Situated function-behavior-structure ontology is an acknowledged method to facilitate conceptual design in a goal-oriented way. However, it depends on the subjective cognition abilities of designers, which are influenced by limited memory and reasoning capacities. Developing computer-aided methods grounded in this ontology holds significant promise in enhancing designers' cognitive abilities. This study delves into the function-behavior (F-B) mapping process. It explores the effect of working memory and long-term memory on design cognition and introduces a memory-inspired reinforcement learning framework for F-B mapping. The Markov decision process is then adopted to formalize F-B mapping while motivation-driven Q learning is employed by the design agent to learn knowledge from historical design cases. The learned state-action value matrix can be applied to guide the designer in selecting feasible behaviors for the specific function requirement. The proposed approach empowers design agents with self-learning and self-evolving capacities. A case study on the F-B mapping of a traction system is conducted to illustrate the feasibility and practicability of the proposed approach.}
}
@article{CATLING201850,
title = {Towards automated clinical coding},
journal = {International Journal of Medical Informatics},
volume = {120},
pages = {50-61},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618304039},
author = {Finneas Catling and Georgios P. Spithourakis and Sebastian Riedel},
keywords = {Clinical coding, Recurrent neural networks, Hierarchical representation learning, Knowledge representation, Natural language processing, Machine learning},
abstract = {Background
Patients’ encounters with healthcare services must undergo clinical coding. These codes are typically derived from free-text notes. Manual clinical coding is expensive, time-consuming and prone to error. Automated clinical coding systems have great potential to save resources, and realtime availability of codes would improve oversight of patient care and accelerate research. Automated coding is made challenging by the idiosyncrasies of clinical text, the large number of disease codes and their unbalanced distribution.
Methods
We explore methods for representing clinical text and the labels in hierarchical clinical coding ontologies. Text is represented as term frequency-inverse document frequency counts and then as word embeddings, which we use as input to recurrent neural networks. Labels are represented atomically, and then by learning representations of each node in a coding ontology and composing a representation for each label from its respective node path. We consider different strategies for initialisation of the node representations. We evaluate our methods using the publicly-available Medical Information Mart for Intensive Care III dataset: we extract the history of presenting illness section from each discharge summary in the dataset, then predicting the International Classification of Diseases, ninth revision, Clinical Modification codes associated with these.
Results
Composing the label representations from the clinical-coding-ontology nodes increased weighted F1 for prediction of the 17,561 disease labels to 0.264–0.281 from 0.232–0.249 for atomic representations. Recurrent neural network text representation improved weighted F1 for prediction of the 19 disease-category labels to 0.682–0.701 from 0.662–0.682 using term frequency-inverse document frequency. However, term frequency-inverse document frequency outperformed recurrent neural networks for prediction of the 17,561 disease labels.
Conclusions
This study demonstrates that hierarchically-structured medical knowledge can be incorporated into statistical models, and produces improved performance during automated clinical coding. This performance improvement results primarily from improved representation of rarer diseases. We also show that recurrent neural networks improve representation of medical text in some settings. Learning good representations of the very rare diseases in clinical coding ontologies from data alone remains challenging, and alternative means of representing these diseases will form a major focus of future work on automated clinical coding.}
}
@article{STEINKAMP2020103354,
title = {Task definition, annotated dataset, and supervised natural language processing models for symptom extraction from unstructured clinical notes},
journal = {Journal of Biomedical Informatics},
volume = {102},
pages = {103354},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103354},
url = {https://www.sciencedirect.com/science/article/pii/S153204641930276X},
author = {Jackson M. Steinkamp and Wasif Bala and Abhinav Sharma and Jacob J. Kantrowitz},
keywords = {Machine learning, Natural language processing, Electronic medical record, Information extraction},
abstract = {Introduction
Machine learning (ML) and natural language processing have great potential to improve information extraction (IE) within electronic medical records (EMRs) for a wide variety of clinical search and summarization tools. Despite ML advancements, clinical adoption of real time IE tools for patient care remains low. Clinically motivated IE task definitions, publicly available annotated clinical datasets, and inclusion of subtasks such as coreference resolution and named entity normalization are critical for the development of useful clinical tools.
Materials and methods
We provide a task definition and comprehensive annotation requirements for a clinically motivated symptom extraction task. Four annotators labeled symptom mentions within 1108 discharge summaries from two public clinical note datasets for the tasks of named entity recognition, coreference resolution, and named entity normalization; these annotations will be released to the public. Baseline human performance was assessed and two ML models were evaluated on the symptom extraction task.
Results
16,922 symptom mentions were identified within the discharge summaries, with 11,944 symptom instances after coreference resolution and 1255 unique normalized answer forms. Human annotator performance averaged 92.2% F1. Recurrent network model performance was 85.6% F1 (recall 85.8%, precision 85.4%), and Transformer-based model performance was 86.3% F1 (recall 86.6%, precision 86.1%). Our models extracted vague symptoms, acronyms, typographical errors, and grouping statements. The models generalized effectively to a separate clinical note corpus and can run in real time.
Conclusion
To our knowledge, this dataset will be the largest and most comprehensive publicly released, annotated dataset for clinically motivated symptom extraction, as it includes annotations for named entity recognition, coreference, and normalization for more than 1000 clinical documents. Our neural network models extracted symptoms from unstructured clinical free text at near human performance in real time. In this paper, we present a clinically motivated task definition, dataset, and simple supervised natural language processing models to demonstrate the feasibility of building clinically applicable information extraction tools.}
}
@article{JIANG2019104967,
title = {Semantifying formal concept analysis using description logics},
journal = {Knowledge-Based Systems},
volume = {186},
pages = {104967},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.104967},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119303971},
author = {Yuncheng Jiang},
keywords = {Formal concept analysis, Ontologies, Description logics, Semantic operations, Attribute reduction},
abstract = {Formal Concept Analysis (FCA) is a field of applied mathematics with its roots in order theory, in particular the theory of complete lattices. Over the past 20 years, FCA has been widely studied. Description Logics (DLs) are a family of knowledge representation languages which can be used to represent the terminological knowledge of an application domain in a structured and formally well-understood way. Nowadays, properties and semantics of ontology constructs mainly are determined by DLs. The current research progress and the existing problems of FCA are analyzed. In this paper, we semantify FCA with DLs, in other words, we present an extended FCA (i.e., semantic FCA) by using the concepts of DLs to act as the attributes of formal contexts. Furthermore, we semantify the three components (i.e., formal concepts, attribute implications, and concept lattices) of traditional FCA. In addition, we also study the attribute reduction of formal contexts, formal concepts, and concept lattices from a semantics point of view.}
}
@article{XIE2025105161,
title = {Transforming intangible cultural heritage in destinations: A fashion communication perspective},
journal = {Tourism Management},
volume = {110},
pages = {105161},
year = {2025},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2025.105161},
url = {https://www.sciencedirect.com/science/article/pii/S0261517725000317},
author = {Chaowu Xie and Feifei Lai and Jiangchi Zhang and Songshan (Sam) Huang},
keywords = {Destination, Intangible cultural heritage (ICH), Fashion communication, Framework construction, Diffusion of innovation theory},
abstract = {Intangible cultural heritage (ICH) is a key attraction for the development of tourist destinations, but few studies have examined the popularization of ICH in destinations through the lens of fashion communication. This research pioneers the conceptualization of ICH fashion communication in tourist destinations. Using qualitative and quantitative methods, we identify and construct a theoretical framework of ICH fashion communication. Study 1 reveals that the fashion communication of ICH in tourist destinations follows a process framework of “fashion communication elements - fashion communication channels - fashion communication results,” involving six distinct constructs. Study 2 and Study 3 demonstrate that the fashion communication elements (fashion representation, fashion ontology, and fashion construction) significantly influence tourists’ fashion perception. Additionally, fashion communication channels (diffusion of exhibition spaces and participation of diverse groups) mediate the relationship between these elements and tourists’ fashion perception. This research enhances the theoretical understanding of ICH communication and marketing in tourism.}
}
@article{BELOZEROV2022617,
title = {Semantic Web Technologies: Issues and Possible Ways of Development},
journal = {Procedia Computer Science},
volume = {213},
pages = {617-622},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.112},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922018117},
author = {A.A. Belozerov and V.V. Klimov},
keywords = {Semantic Network, Semantic Web, Hybrid Web, Semantic Search, Semantic Technology, Ontology, Linked Data, Logical Inference, Microformats, Markup Language, OWL, RDF, RSS, FOAF, URI},
abstract = {Semantic digital technologies are as useful as they are difficult to implement. During its existence, the concept of the Semantic Web has still found very limited application and extremely slow development. The reasons for this are serious differences in the structure of hypertext and the Semantic Web as a superstructure above it, and the rules that circumscribed the development of the internet. Semantic search engines still do not implement semantic search in its pure form: various data organization schemes and data visualization formats make it a little easier to analyze facts, add them to databases, but do not make knowledge bases out of the latter, do not allow working with facts as with knowledge. In addition, the user interfaces of such systems tend to be either very complex or counter-intuitive. Nevertheless, there is every reason to believe that with the use of modern technologies, the ideas of semantization of the web and its content can already be improved and gradually implemented: for this it is necessary to get rid of some false expectations of semantic technologies, to direct efforts towards the development of web native formats of semantic markup and try to take advantage of the increasingly common voice input of household devices.}
}
@article{CHEN2021824,
title = {Development and application of the semantic annotation framework for digital images},
journal = {The Electronic Library},
volume = {39},
number = {6},
pages = {824-845},
year = {2021},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-07-2021-0131},
url = {https://www.sciencedirect.com/science/article/pii/S0264047321000217},
author = {Jinju Chen and Shiyan Ou},
keywords = {Digital images, Semantic annotation, Ontology, Knowledge organization, Semantic Web},
abstract = {Purpose
The purpose of this paper is to semantically annotate the content of digital images with the use of Semantic Web technologies and thus facilitate retrieval, integration and knowledge discovery.
Design/Methodology/Approach
After a review and comparison of the existing semantic annotation models for images and a deep analysis of the characteristics of the content of images, a multi-dimensional and hierarchical general semantic annotation framework for digital images was proposed. On this basis, taking histories images, advertising images and biomedical images as examples, by integrating the characteristics of images in these specific domains with related domain knowledge, the general semantic annotation framework for digital images was customized to form a domain annotation ontology for the images in a specific domain. The application of semantic annotation of digital images, such as semantic retrieval, visual analysis and semantic reuse, were also explored.
Findings
The results showed that the semantic annotation framework for digital images constructed in this paper provided a solution for the semantic organization of the content of images. On this basis, deep knowledge services such as semantic retrieval, visual analysis can be provided.
Originality/Value
The semantic annotation framework for digital images can reveal the fine-grained semantics in a multi-dimensional and hierarchical way, which can thus meet the demand for enrichment and retrieval of digital images.}
}
@incollection{KHARE2024221,
title = {Chapter 5 - Artificial intelligence and machine learning application in data analysis},
editor = {Vikas Khare and Sanjeet Kumar Dwivedi and Monica Bhatia},
booktitle = {Cognitive Science, Computational Intelligence, and Data Analytics},
publisher = {Morgan Kaufmann},
pages = {221-272},
year = {2024},
isbn = {978-0-443-16078-3},
doi = {https://doi.org/10.1016/B978-0-443-16078-3.00001-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443160783000010},
author = {Vikas Khare and Sanjeet Kumar Dwivedi and Monica Bhatia},
keywords = {Knowledge representation, artificial intelligence cycle, natural language processing, expert system},
abstract = {Artificial intelligence (AI) and machine learning (ML) have revolutionized the field of data analysis by enabling advanced techniques for extracting valuable insights and patterns from large datasets. This chapter explores the application of AI and ML in data analysis, focusing on key aspects such as knowledge representation, the AI cycle, and the intersection of cognitive science with AI. Knowledge representation plays a crucial role in data analysis, as it involves organizing and structuring information in a way that can be processed by AI algorithms. Various techniques, including semantic networks, ontologies, and probabilistic models, are employed to represent knowledge effectively. These representations serve as the foundation for AI and ML algorithms to reason, learn, and make informed decisions based on the available data. The AI cycle, consisting of data acquisition, preprocessing, modeling, evaluation, and deployment, forms a systematic framework for implementing AI and ML in data analysis. Data acquisition involves gathering relevant datasets, while preprocessing techniques, such as data cleaning, normalization, and feature selection, ensure data quality and readiness for analysis. Modeling encompasses the development and training of AI and ML models using appropriate algorithms and techniques. Evaluation is crucial to assess the model’s performance and refine it if necessary. Finally, deployment involves integrating the developed model into real-world applications for practical use. Furthermore, this chapter explores the intersection of cognitive science with AI in the context of data analysis. Cognitive science studies human cognition and aims to understand how humans perceive, learn, and reason. By incorporating cognitive science principles into AI and ML algorithms, researchers can develop more human-like intelligent systems. This integration allows for enhanced data analysis capabilities, including natural language processing, image recognition, and pattern recognition, by leveraging insights from cognitive science research. In summary, this chapter highlights the significant role of AI and ML in data analysis, focusing on knowledge representation, the AI cycle, and the synergy between cognitive science and AI. By understanding and leveraging these concepts, researchers and practitioners can harness the power of AI and ML to extract valuable insights and make informed decisions from complex datasets.}
}
@article{LIU2024108143,
title = {Towards key genes identification for breast cancer survival risk with neural network models},
journal = {Computational Biology and Chemistry},
volume = {112},
pages = {108143},
year = {2024},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2024.108143},
url = {https://www.sciencedirect.com/science/article/pii/S1476927124001312},
author = {Gang Liu and Xiao Yang and Nan Li},
keywords = {Breast cancer, Survival risk, Neural network},
abstract = {Breast cancer, one common malignant tumor all over the world, has a considerably high rate of recurrence, which endangers the health and life of patients. While more and more data have been available, how to leverage the gene expression data to predict the survival risk of cancer patients and identify key genes has become a hot topic for cancer research. Therefore, in this work, we investigate the gene expression and clinical data of breast cancer patients, specifically a novel framework is proposed focusing on the survival risk classification and key gene identification task. We firstly combine the differential expression and univariate Cox regression analysis to achieve dimensional reduction of gene expression data. The median survival time is subsequently proposed as the risk classification threshold and a learning model based on neural network is trained to classify the survival risk of patients. Innovatively, in this work, the activation region visualization technology is selected as the identification tool, which identify 20 key genes related to the survival risk of breast cancer patients. We further analyze the gene function of these 20 key genes based on STRING database. It is critical to learn that, the genetic biomarkers identified in this paper may possess value for the following clinical treatment of breast cancer according to the literature findings. Importantly, the genetic biomarkers identified in this paper may possess value for the following clinical treatment of breast cancer according to the literature findings. Our work accomplishes the objective of proposing a targeted approach to enhancing the survival analysis and therapeutic strategies in breast cancer through advanced computational techniques and gene analysis.}
}
@article{QUAN201934,
title = {A parallel computing architecture for high-performance OWL reasoning},
journal = {Parallel Computing},
volume = {83},
pages = {34-46},
year = {2019},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2018.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S016781911830142X},
author = {Zixi Quan and Volker Haarslev},
keywords = {Ontology classification, Parallel computing},
abstract = {The Web Ontology Language (OWL) is a widely used knowledge representation language for describing knowledge in application domains by using classes, properties, and individuals. Ontology classification is an important and widely used service that computes a taxonomy of all classes occurring in an ontology. It can require significant amounts of runtime, but most OWL reasoners do not support any kind of parallel processing. We present a novel thread-level parallel architecture for ontology classification, which is ideally suited for shared-memory SMP servers, but does not rely on locking techniques and thus avoids possible race conditions. We evaluated our prototype implementation with a set of real-world ontologies. Our experiments demonstrate a very good scalability resulting in a speedup that is linear to the number of available cores.}
}
@article{HENDAWI2023,
title = {A Mobile App That Addresses Interpretability Challenges in Machine Learning–Based Diabetes Predictions: Survey-Based User Study},
journal = {JMIR Formative Research},
volume = {7},
year = {2023},
issn = {2561-326X},
doi = {https://doi.org/10.2196/50328},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X23005322},
author = {Rasha Hendawi and Juan Li and Souradip Roy},
keywords = {disease prediction, explainable AI, artificial intelligence, knowledge graph, machine learning, ontology, diabetes},
abstract = {Background
Machine learning approaches, including deep learning, have demonstrated remarkable effectiveness in the diagnosis and prediction of diabetes. However, these approaches often operate as opaque black boxes, leaving health care providers in the dark about the reasoning behind predictions. This opacity poses a barrier to the widespread adoption of machine learning in diabetes and health care, leading to confusion and eroding trust.
Objective
This study aimed to address this critical issue by developing and evaluating an explainable artificial intelligence (AI) platform, XAI4Diabetes, designed to empower health care professionals with a clear understanding of AI-generated predictions and recommendations for diabetes care. XAI4Diabetes not only delivers diabetes risk predictions but also furnishes easily interpretable explanations for complex machine learning models and their outcomes.
Methods
XAI4Diabetes features a versatile multimodule explanation framework that leverages machine learning, knowledge graphs, and ontologies. The platform comprises the following four essential modules: (1) knowledge base, (2) knowledge matching, (3) prediction, and (4) interpretation. By harnessing AI techniques, XAI4Diabetes forecasts diabetes risk and provides valuable insights into the prediction process and outcomes. A structured, survey-based user study assessed the app’s usability and influence on participants’ comprehension of machine learning predictions in real-world patient scenarios.
Results
A prototype mobile app was meticulously developed and subjected to thorough usability studies and satisfaction surveys. The evaluation study findings underscore the substantial improvement in medical professionals’ comprehension of key aspects, including the (1) diabetes prediction process, (2) data sets used for model training, (3) data features used, and (4) relative significance of different features in prediction outcomes. Most participants reported heightened understanding of and trust in AI predictions following their use of XAI4Diabetes. The satisfaction survey results further revealed a high level of overall user satisfaction with the tool.
Conclusions
This study introduces XAI4Diabetes, a versatile multi-model explainable prediction platform tailored to diabetes care. By enabling transparent diabetes risk predictions and delivering interpretable insights, XAI4Diabetes empowers health care professionals to comprehend the AI-driven decision-making process, thereby fostering transparency and trust. These advancements hold the potential to mitigate biases and facilitate the broader integration of AI in diabetes care.}
}
@article{NANDY2025116174,
title = {From pages to patterns: Towards extracting catalytic knowledge from structure and text for transition-metal complexes and metal-organic frameworks},
journal = {Journal of Catalysis},
volume = {448},
pages = {116174},
year = {2025},
issn = {0021-9517},
doi = {https://doi.org/10.1016/j.jcat.2025.116174},
url = {https://www.sciencedirect.com/science/article/pii/S0021951725002398},
author = {Aditya Nandy},
keywords = {Literature mining, Catalysis informatics, Structural databases, Generative machine learning, Large language models}
}
@incollection{ELVE2018235,
title = {From Process Graph to Process Simulation with Proper Model Documentation},
editor = {Anton Friedl and Jiří J. Klemeš and Stefan Radl and Petar S. Varbanov and Thomas Wallek},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {43},
pages = {235-240},
year = {2018},
booktitle = {28th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64235-6.50042-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444642356500425},
author = {Arne Tobias Elve and Heinz A. Preisig},
keywords = {Model Documentation, Graph-based Modelling, Automatic Code Generation, Customised Modelling},
abstract = {This paper presents a new modelling framework called the Process Modeller Suite, which combines ontologies and graph-based modelling to do automatic code generation for models. The process models are represented by a process graph where the graph components capture system knowledge in the ontology in a systematic manner. The result is a software tool that promotes correct model formulation, interoperability between different modelling environment and reusability of the models and also generates a proper model documentation.}
}
@article{SANTOSAYLLON2025104047,
title = {Debates on the future of energy justice: Re-grounding the triumvirate},
journal = {Environmental Science & Policy},
volume = {167},
pages = {104047},
year = {2025},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2025.104047},
url = {https://www.sciencedirect.com/science/article/pii/S1462901125000632},
author = {Lara M. {Santos Ayllón}},
keywords = {Energy justice, Triumvirate of tenets, Restorative justice, Cosmopolitan justice, Critical theory},
abstract = {Energy justice is now a well-established pillar of the energy and social science field, primarily framed as the triumvirate of tenets of distributional, recognition and procedural justice. Triumvirate+ approaches also incorporate restorative and cosmopolitan justice. Most recently, the ‘five principles of energy justice’ was proposed to standardise the field. Prompted by the ‘five principles’ proposal alongside exchanges within critical energy justice scholarship, this perspective is framed by the debate between universalising or plural approaches to energy justice. It considers the triumvirate of tenets in relation to restorative justice, cosmopolitan justice and the triumvirate's broader pluralising potential. Particular attention is awarded to cosmopolitan justice and its anthropocentric tendency. Instead, this perspective proposes a re-grounding of energy justice in its environmental justice conceptual roots while maintaining the three tenets of distribution, procedure and recognition justice as a potential way forward. A re-grounded triumvirate of tenets, enriched by a decade of diverse scholarship and which departs from universalising approaches can continue to add important value across energy transition contexts, while preserving pluralising potential.}
}
@article{QIAN2025103614,
title = {Rural-urban interfaces and changing forms of relational and planetary rurality},
journal = {Journal of Rural Studies},
volume = {116},
pages = {103614},
year = {2025},
issn = {0743-0167},
doi = {https://doi.org/10.1016/j.jrurstud.2025.103614},
url = {https://www.sciencedirect.com/science/article/pii/S0743016725000543},
author = {Junxi Qian and Shenjing He and Darren Smith},
keywords = {Rurality, Rural-urban interface, Relationality, Worlding, Planetary rural geographies},
abstract = {This editorial introduction aims to frame the special issue entitled “Rural-urban interfaces and changing forms of relational and planetary rurality”. Problematising the dominant planetary urbanisation thesis, particularly its tendency in eliding alternative spaces, subjectivities, and politics to the global expansion of capitalist urban fabrics, this special issue seeks to rethink the rural-urban interface through the lenses of relationality and planetary rural geographies, highlighting the promiscuous interpenetration between the urban and the rural, the planetary significance of rurality, and, hence, the need to reassert the rural as a distinct spatial ontology and category. To move forward this theoretical agenda, this editorial situates our investigation of the rural-urban interface within a long pedigree of research on rural-urban interaction, coordination, or integration. However, such works are often leaned towards the dissemination of urban economic functions into rural places and often leave limited discursive space for a conceptual and theoretical rethinking of rurality per se. We advance the latter by building on a heuristic of “worlding” at the rural-urban interface, and bring this epistemology into a direct conversation with the six papers of this special issue.}
}
@article{WIJESINGHE2024108043,
title = {A phrase-based questionnaire–answering approach for automatic initial frailty assessment based on clinical notes},
journal = {Computers in Biology and Medicine},
volume = {170},
pages = {108043},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108043},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524001276},
author = {Yashodhya V. Wijesinghe and Yue Xu and Yuefeng Li and Qing Zhang},
keywords = {Frailty assessment, Questionnaire answering, Query expansion, Clinical notes},
abstract = {Frailty stands out as a particularly challenging multidimensional geriatric syndrome in the elderly population, often resulting in diminished quality of life and heightened mortality risk. Negative consequences encompass a heightened likelihood of hospitalization and institutionalization, as well as suboptimal post-hospitalization outcomes and elevated mortality rates. Using a questionnaire-based approach for assessing frailty has been shown to be an effective method for early diagnosis of frailty. Nonetheless, the majority of current frailty assessment tools necessitate in-person consultations. This poses a significant challenge for elderly patients residing in rural areas, who often encounter difficulties in accessing healthcare compared to their urban or suburban counterparts. Additionally, elderly patients face an elevated risk of contracting diseases as a result of frequent hospital visits, given that many of them are immunocompromised. An automated initial frailty assessment approach can help mitigate the challenges mentioned above and conserve clinical resources by circumventing the need for extensive manual assessments. The primary aim of this paper is to introduce an automatic initial frailty assessment method. This method efficiently identifies individuals who may necessitate further frailty evaluation by automatically extracting relevant information from a patient’s clinical notes and using it to complete the Tillburg Frailty Indicator (TFI) questionnaire. The introduced phrase-based query expansion technique is designed to identify the most pertinent phrases related to the frailty assessment questionnaire using Unified Medical Language System (UMLS) ontology and incorporates information from clinical notes to enhance its accuracy. Additionally, a method for retrieving pertinent clinical notes to automatically facilitate the frailty assessment process based on the identified phrases was also proposed. The proposed approaches are evaluated using a dataset containing a collection of clinical notes from elderly patients, assessing their effectiveness in terms of automating frailty assessment and question–answering tasks. This research underscores the significance of incorporating phrases as features in the automated frailty assessment process using clinical notes. The research empowers clinicians to conduct automatic frailty assessments utilizing medical data, thereby reducing the need for frequent hospital visits and in-patient consultations. This becomes particularly valuable during unusual or unexpected situations, such as the COVID-19 pandemic, where minimizing in-person interactions is crucial.}
}
@article{UVARAJAN2025100295,
title = {Arbutin in Alzheimer’s disease: A network-based approach to uncover drug targets through molecular mapping},
journal = {Pharmacological Research - Natural Products},
volume = {8},
pages = {100295},
year = {2025},
issn = {2950-1997},
doi = {https://doi.org/10.1016/j.prenap.2025.100295},
url = {https://www.sciencedirect.com/science/article/pii/S2950199725001557},
author = {Deenathayalan Uvarajan and Manish Ravikumar and Brindha Durairaj},
keywords = {Arbutin, Alzheimer’s Disease, Gene ontology, KEGG enrichment molecular docking and network pharmacology},
abstract = {Alzheimer’s disease (AD), a leading cause of dementia, is characterized by progressive neurodegeneration and cognitive decline. Current treatments, including cholinesterase and N-methyl-D-aspartate (NMDA) inhibitors, provide symptomatic relief but are often associated with adverse effects. This has driven interest in natural compounds like arbutin, an endogenous molecule, for their therapeutic potential in AD. This study employs a network pharmacology approach to explore arbutin’s molecular mechanisms and pathways. Arbutin targets were retrieved from SwissTargetPrediction and PharmMapper, while AD-related targets were obtained from DisGeNET and GeneCards. After merging and removing duplicates, 37 common targets were identified. Gene ontology and KEGG enrichment analyses, performed using ShinyGO, revealed key biological processes associated with AD, including phosphorus metabolic regulation, cell proliferation, apoptosis, oxidative response, and abiotic stress. Among the top five biological processes, nine out of ten hub genes were enriched. KEGG analysis highlighted ten significant pathways, with AD and cancer being the most predominant based on gene count. The Maximal Clique Centrality method in the CytoHubba plugin of Cytoscape identified the top ten hub genes. Molecular docking studies using PyRx and Discovery Studio revealed that AKT-1 exhibited the highest binding affinity with arbutin (-7.2 kcal/mol). These findings suggest that arbutin plays a crucial role in modulating key pathways associated with AD, offering potential therapeutic benefits. Targeting these pathways could reduce AD progression, highlighting arbutin as a promising candidate for further drug development.}
}
@article{LYU2023104298,
title = {Causal knowledge graph construction and evaluation for clinical decision support of diabetic nephropathy},
journal = {Journal of Biomedical Informatics},
volume = {139},
pages = {104298},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104298},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423000199},
author = {Kewei Lyu and Yu Tian and Yong Shang and Tianshu Zhou and Ziyue Yang and Qianghua Liu and Xi Yao and Ping Zhang and Jianghua Chen and Jingsong Li},
keywords = {Knowledge graph, Electronic health record, Causal knowledge, Diabetic nephropathy},
abstract = {Background
Many important clinical decisions require causal knowledge (CK) to take action. Although many causal knowledge bases for medicine have been constructed, a comprehensive evaluation based on real-world data and methods for handling potential knowledge noise are still lacking.
Objective
The objectives of our study are threefold: (1) propose a framework for the construction of a large-scale and high-quality causal knowledge graph (CKG); (2) design the methods for knowledge noise reduction to improve the quality of the CKG; (3) evaluate the knowledge completeness and accuracy of the CKG using real-world data.
Material and methods
We extracted causal triples from three knowledge sources (SemMedDB, UpToDate and Churchill's Pocketbook of Differential Diagnosis) based on rule methods and language models, performed ontological encoding, and then designed semantic modeling between electronic health record (EHR) data and the CKG to complete knowledge instantiation. We proposed two graph pruning strategies (co-occurrence ratio and causality ratio) to reduce the potential noise introduced by SemMedDB. Finally, the evaluation was carried out by taking the diagnostic decision support (DDS) of diabetic nephropathy (DN) as a real-world case. The data originated from a Chinese hospital EHR system from October 2010 to October 2020. The knowledge completeness and accuracy of the CKG were evaluated based on three state-of-the-art embedding methods (R-GCN, MHGRN and MedPath), the annotated clinical text and the expert review, respectively.
Results
This graph included 153,289 concepts and 1,719,968 causal triples. A total of 1427 inpatient data were used for evaluation. Better results were achieved by combining three knowledge sources than using only SemMedDB (three models: area under the receiver operating characteristic curve (AUC): p < 0.01, F1: p < 0.01), and the graph covered 93.9 % of the causal relations between diseases and diagnostic evidence recorded in clinical text. Causal relations played a vital role in all relations related to disease progression for DDS of DN (three models: AUC: p > 0.05, F1: p > 0.05), and after pruning, the knowledge accuracy of the CKG was significantly improved (three models: AUC: p < 0.01, F1: p < 0.01; expert review: average accuracy: + 5.5 %).
Conclusions
The results demonstrated that our proposed CKG could completely and accurately capture the abstract CK under the concrete EHR data, and the pruning strategies could improve the knowledge accuracy of our CKG. The CKG has the potential to be applied to the DDS of diseases.}
}
@article{BLUMENFELD20233738,
title = {Concepts for the Integration of Data from Asset Management Systems into BIM},
journal = {Transportation Research Procedia},
volume = {72},
pages = {3738-3745},
year = {2023},
note = {TRA Lisbon 2022 Conference Proceedings Transport Research Arena (TRA Lisbon 2022),14th-17th November 2022, Lisboa, Portugal},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2023.11.544},
url = {https://www.sciencedirect.com/science/article/pii/S2352146523008426},
author = {Tim Blumenfeld and Markus Stöckner and Liu Liu and Rade Hajdin and Markus König and Kenneth Gavin},
keywords = {BIM, IFC, Infrastructure Asset Management, Ontologies, Information Container},
abstract = {Even though BIM is still rarely used in operation over the lifetime of infrastructure assets, it promises significant benefits for national road authorities with regard to condition updating and the effects of maintenance measures required in contemporary asset management. This paper introduces new approaches for a task-related data exchange between BIM and Infrastructure Management Systems. Concepts to manage semantic information and to visualize geometric-related information are presented. For selected use cases, country-specific information containers were defined to support the general process map which differ fundamentally according to the ontologies, links and documents that are used and stored. The proposed approach was implemented in a web application and tested for three predefined use cases within the framework of a case study.}
}
@article{IGLESIAS2020101741,
title = {Comprehensive analysis of rule formalisms to represent clinical guidelines: Selection criteria and case study on antibiotic clinical guidelines},
journal = {Artificial Intelligence in Medicine},
volume = {103},
pages = {101741},
year = {2020},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2019.101741},
url = {https://www.sciencedirect.com/science/article/pii/S0933365719301873},
author = {Natalia Iglesias and Jose M. Juarez and Manuel Campos},
keywords = {CDSS, Rules, Drools, Arden, SPIN, RIF: SWRL, OWL, Ontology, Semantic Web, CG, HIS, FHIR, SHACL, KaaS},
abstract = {Background
The over-use of antibiotics in clinical domains is causing an alarming increase in bacterial resistance, thus endangering their effectiveness as regards the treatment of highly recurring severe infectious diseases. Whilst Clinical Guidelines (CGs) focus on the correct prescription of antibiotics in a narrative form, Clinical Decision Support Systems (CDSS) operationalize the knowledge contained in CGs in the form of rules at the point of care. Despite the efforts made to computerize CGs, there is still a gap between CGs and the myriad of rule technologies (based on different logic formalisms) that are available to implement CDSSs in real clinical settings.
Objective
To helpCDSS designers to determine the most suitable rule-based technology (medical-oriented rules, production rules and semantic web rules) with which to model knowledge from CGs for the prescription of antibiotics. We propose a framework of criteria for this purpose that is extensible to more generic CGs.
Materials and methods
Our proposal is based on the identification of core technical requirements extracted from both literature and the analysis of CGs for antibiotics, establishing three dimensions for analysis: language expressivity, interoperability and industrial aspects. We present a case study regarding the John Hopkins Hospital (JHH) Antibiotic Guidelines for Urinary Tract Infection (UTI), a highly recurring hospital acquired infection. We have adopted our framework of criteria in order to analyse and implement these CGs using various rule technologies: HL7 Arden Syntax, general-purpose Production Rules System (Drools), HL7 standard Rule Interchange Format (RIF), Semantic Web Rule Language (SWRL) and SParql Inference Notation (SPIN) rule extensions (implementing our own ontology for UTI).
Results
We have identified the main criteria required to attain a maintainable and cost-affordable computable knowledge representation for CGs. We have represented the JHH UTI CGs knowledge in a total of 12 Arden Syntax MLMs, 81 Drools rules and 154 ontology classes, properties and individuals. Our experiments confirm the relevance of the proposed set of criteria and show the level of compliance of the different rule technologies with the JHH UTI CGs knowledge representation.
Conclusions
The proposed framework of criteria may help clinical institutions to select the most suitable rule technology for the representation of CGs in general, and for the antibiotic prescription domain in particular, depicting the main aspects that lead to Computer Interpretable Guidelines (CIGs), such as Logic expressivity (Open/Closed World Assumption, Negation-As-Failure), Temporal Reasoning and Interoperability with existing HIS and clinical workflow. Future work will focus on providing clinicians with suggestions regarding new potential steps for CGs, considering process mining approaches and CGs Process Workflows, the use of HL7 FHIR for HIS interoperability and the representation of Knowledge-as- a-Service (KaaS).}
}
@article{MOHAMED2022934,
title = {QSST: A Quranic Semantic Search Tool based on word embedding},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {3},
pages = {934-945},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819311012},
author = {Ensaf Hussein Mohamed and Eyad Mohamed Shokry},
keywords = {Information Retrieval, Word Embedding, Concept-based Search, Ontology, Semantic Search, Arabic Natural Language Processing, Holy Quran},
abstract = {Retrieving information from the Quran is an important field for Quran scholars and Arabic researchers. There are two types of Quran searching techniques: semantic or concept-based and keyword-based. Concept-based search is a challenging task, especially in a complex corpus such as Quran. This paper presents a concept-based searching tool (QSST) for the Holy Quran. It consists of four phases. In the first phase, the Quran dataset is built by manually annotating Quran verses based on the ontology of Mushaf Al-Tajweed. The second phase is word Embedding, this phase generates features’ vectors for words by training a Continuous Bag of Words (CBOW) architecture on large Quranic and Classic Arabic corpus. The third phase includes calculating the features’ vectors of both input query and Quranic topics. Finally, retrieving the most relevant verses by computing the cosine similarity between both topic and query vectors. The performance of the proposed QSST is measured by comparing results against Mushaf Al-Tajweed. Then, precision, recall, and F-score are computed and their percentages were 76.91%, 72.23% 69.28% respectively. In addition, the results are evaluated by three Islamic experts and the average precision was 91.95%. Finally, QSST results are compared with the recent existing tools; QSST outperformed them.}
}
@article{LI20252790,
title = {DNMKG: A method for constructing domain of nonferrous metals knowledge graph based on multiple corpus},
journal = {Transactions of Nonferrous Metals Society of China},
volume = {35},
number = {8},
pages = {2790-2802},
year = {2025},
issn = {1003-6326},
doi = {https://doi.org/10.1016/S1003-6326(25)66848-8},
url = {https://www.sciencedirect.com/science/article/pii/S1003632625668488},
author = {Hai-liang LI and Hai-dong WANG},
keywords = {knowledge graph, nonferrous metals, thesaurus, word vector model, multi-source heterogeneous corpus},
abstract = {To address the underutilization of Chinese research materials in nonferrous metals, a method for constructing a domain of nonferrous metals knowledge graph (DNMKG) was established. Starting from a domain thesaurus, entities and relationships were mapped as resource description framework (RDF) triples to form the graph’s framework. Properties and related entities were extracted from open knowledge bases, enriching the graph. A large-scale, multi- source heterogeneous corpus of over 1×109 words was compiled from recent literature to further expand DNMKG. Using the knowledge graph as prior knowledge, natural language processing techniques were applied to the corpus, generating word vectors. A novel entity evaluation algorithm was used to identify and extract real domain entities, which were added to DNMKG. A prototype system was developed to visualize the knowledge graph and support human−computer interaction. Results demonstrate that DNMKG can enhance knowledge discovery and improve research efficiency in the nonferrous metals field.}
}
@article{RODLER2023103988,
title = {Sequential model-based diagnosis by systematic search},
journal = {Artificial Intelligence},
volume = {323},
pages = {103988},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103988},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223001340},
author = {Patrick Rodler},
keywords = {Model-based diagnosis, Sequential diagnosis, Query-based debugging, Interactive debugging, Combinatorial search, Diagnostic decision-making, Fault localization, Measurement selection, Knowledge-base debugging, Ontologies, Query computation, Active learning, Heuristics},
abstract = {Model-based diagnosis aims at identifying the real cause of a system's malfunction based on a formal system model and observations of the system behavior. To discriminate between multiple fault hypotheses (diagnoses), sequential diagnosis approaches iteratively pose queries to an oracle to acquire additional knowledge about the diagnosed system. Depending on the system type, queries can capture, e.g., system tests, probes, measurements, or expert questions. As the determination of optimal queries is NP-hard, state-of-the-art sequential diagnosis methods rely on a myopic one-step-lookahead analysis which has proven to constitute a particularly favorable trade-off between computational efficiency and diagnostic effectivity. Yet, this solves only a part of the problem, as various sources of complexity, such as the reliance on costly reasoning services and large numbers of or not explicitly given query candidates, remain. To deal with such issues, existing approaches often make assumptions about the (i) type of diagnosed system, (ii) formalism to describe the system, (iii) inference engine, (iv) type of query to be of interest, (v) query quality criterion to be adopted, or (vi) diagnosis computation algorithm to be employed. Moreover, they (vii) often cannot deal with large or implicit query spaces or with expressive logics, or (viii) require inputs that cannot always be provided. As a remedy, we propose a novel one-step lookahead query computation technique for sequential diagnosis that overcomes the said issues of existing methods. Our approach (1) is based on a solid theory, (2) involves a systematic search for optimal queries, (3) can operate on implicit and huge query spaces, (4) allows for a two-stage optimization of queries (wrt. their number and cost), (5) is designed to reduce expensive logical inferences to a minimum, and (6) is generally applicable. The latter means that it can deal with any type of diagnosis problem as per Reiter's theory, is applicable with any monotonic knowledge representation language, can interact with a multitude of diagnosis engines and logical reasoners, and allows for a quality optimization of queries based on any of the common criteria in the literature. We extensively study the performance of the novel technique using a benchmark of real-world diagnosis problems. Our findings are that our approach enables the computation of optimal queries with hardly any delay, independently of the size and complexity of the considered benchmark problem. Moreover, it proves to be highly scalable, and it outperforms the state-of-the-art method in the domain of our benchmarks by orders of magnitude in terms of computation time while always returning a qualitatively as good or better query.}
}
@article{CHEN202537,
title = {Throw out an oligopeptide to catch a protein: Deep learning and natural language processing-screened tripeptide PSP promotes Osteolectin-mediated vascularized bone regeneration},
journal = {Bioactive Materials},
volume = {46},
pages = {37-54},
year = {2025},
issn = {2452-199X},
doi = {https://doi.org/10.1016/j.bioactmat.2024.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S2452199X2400495X},
author = {Yu Chen and Long Chen and Jinyang Wu and Xiaofeng Xu and Chengshuai Yang and Yong Zhang and Xinrong Chen and Kaili Lin and Shilei Zhang},
keywords = {Deep learning, Natural language processing, Bioactive peptides, Vascular-osteo communication, Osteolectin},
abstract = {Angiogenesis is imperative for bone regeneration, yet the conventional cytokine therapies have been constrained by prohibitive costs and safety apprehensions. It is urgent to develop a safer and more efficient therapeutic alternative. Herein, utilizing the methodologies of Deep Learning (DL) and Natural Language Processing (NLP), we proposed a paradigm algorithm that amalgamates Word2vec with a TF-IDF variant, TF-IIDF, to deftly discern potential pro-angiogenic peptides from intrinsically disordered regions (IDRs) of 262 related proteins, where are fertile grounds for developing safer and highly promising bioactive peptides. After the evaluation of the candidate oligopeptides, one tripeptide, PSP, emerged as particularly notable for its exceptional ability to stimulate the vascularization of endothelial cells (ECs), enhance vascular-osteo communication, and then boost the osteogenic differentiation of bone marrow stem cells (BMSCs), evidenced in mouse critical-sized cranial model. Moreover, we found that PSP serves as a ‘priming’ agent, activating the body's innate ability to produce Osteolectin (Oln) — prompting ECs to release small extracellular vesicles (sEVs) enriched with Oln to facilitate bone formation. In summary, our study established a precise and efficient composite model of DL and NLP to screen bioactive peptides, opening an avenue for the development of various peptide-based therapeutic strategies applicable to a broader range of diseases.}
}
@article{MOYANO2022104551,
title = {Systematic approach to generate Historical Building Information Modelling (HBIM) in architectural restoration project},
journal = {Automation in Construction},
volume = {143},
pages = {104551},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104551},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522004216},
author = {Juan Moyano and Eva Carreño and Juan E. Nieto-Julián and Ignacio Gil-Arizón and Silvana Bruno},
keywords = {HBIM, Parametric objects, Revit, Digital twin, Restoration project},
abstract = {Working with heritage is complex and projects often include scientific, structural, and documentary studies. Every step requires a systematic approach in which the contribution of the diverse experts that participate in the conservation of heritage assets is analysed following the logical sequence of the project execution phase. However, there are still no scientific contributions that record the complete process. A logical sequence, called a ‘systematic approach’ in this paper, should start with data acquisition using remote sensing techniques, the insertion of point cloud data in a Building Information Modelling (BIM) environment, ontologies applied to HBIM (Historic BIM), data classification, and evolution towards the Digital Twin (DT) concept. This paper also analyses the modelling workflow of a case study, the construction of a column base of the Santiago church in Jerez de la Frontera (Spain). This component has been automatically modelled from a mesh in an Autodesk Revit family and through an adaptive mesh process in ArchiCAD. Furthermore, one of the objectives was the data organisation by following and extending a standard data structure of the column components. This paper presents a novel holistic approach from data capture, analysis of complex geometry, construction of the semantically enriched parametric model, and exchange of it among all involved professionals in open source IFC formats. This systematic approach therefore makes it possible to obtain a validated DT for the life-cycle management of architectural elements.}
}
@article{WANG2023190,
title = {Fusing external knowledge resources for natural language understanding techniques: A survey},
journal = {Information Fusion},
volume = {92},
pages = {190-204},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522002354},
author = {Yuqi Wang and Wei Wang and Qi Chen and Kaizhu Huang and Anh Nguyen and Suparna De and Amir Hussain},
keywords = {Natural language understanding, Knowledge graph, Knowledge fusion, Representation learning, Deep learning},
abstract = {Knowledge resources, e.g. knowledge graphs, which formally represent essential semantics and information for logic inference and reasoning, can compensate for the unawareness nature of many natural language processing techniques based on deep neural networks. This paper provides a focused review of the emerging but intriguing topic that fuses quality external knowledge resources in improving the performance of natural language processing tasks. Existing methods and techniques are summarised in three main categories: (1) static word embeddings, (2) sentence-level deep learning models, and (3) contextualised language representation models, depending on when, how and where external knowledge is fused into the underlying learning models. We focus on the solutions to mitigate two issues: knowledge inclusion and inconsistency between language and knowledge. Details on the design of each representative method, as well as their strength and limitation, are discussed. We also point out some potential future directions in view of the latest trends in natural language processing research.}
}
@article{VITTORI2025116368,
title = {BIM-to-BRICK: Using graph modeling for IoT/BMS and spatial semantic data interoperability within digital data models of buildings},
journal = {Energy and Buildings},
pages = {116368},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2025.116368},
url = {https://www.sciencedirect.com/science/article/pii/S0378778825010989},
author = {Filippo Vittori and Chuan Fu Tan and Anna Laura Pisello and Adrian Chong and Cristina Piselli and Clayton Miller},
keywords = {Digital Twin, Building Management System, comfort, automation, occupant behavior, data interoperability, building operation},
abstract = {The holistic management of a building requires data from heterogeneous sources such as building management systems (BMS), Internet-of-Things (IoT) sensor networks, and building information models (BIM), all aimed at environmental well-being. Data interoperability is a key component to eliminate silos of information, and using semantic web technologies like the BRICK schema, an effort to standardize semantic descriptions of the physical, logical, and virtual assets in buildings and the relationships between them, is a suitable approach. However, current data integration processes can involve significant manual interventions. This paper presents a methodology to automatically collect, assemble, and integrate information from a building information model into a knowledge graph. The resulting application, BIM-to-BRICK, is implemented on a higher education case study building in a tropical climate. BIM-to-BRICK generated a bidirectional link between a BIM model of 932 instances and experimental data collected for 17 subjects into 458 BRICK objects and 1219 relationships in 17 seconds. The automation of this approach can be compared to traditional manual mapping of data types. This scientific innovation incentivizes the convergence of disparate data types required for achieving and enhancing environmental comfort conditions and other strategic built-environment applications.}
}
@article{DUAN2020726,
title = {The development of standardized models of digital twin},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {5},
pages = {726-731},
year = {2020},
note = {3rd IFAC Workshop on Cyber-Physical & Human Systems CPHS 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.04.164},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321003050},
author = {Haibo Duan and Feng Tian},
keywords = {Digital Twin, standardization, concept system, reference architecture, application framework, maturity model},
abstract = {Digital twin is becoming a General Purpose Technology of the coming fourth industrial revolution. However, there is no consistent set of definition, concept system, reference architecture (model), application framework (model), and maturity model of digital twin yet. This paper identifies the relationship among these models, proposes a universal architectural and ontological concept system of digital twin, and a set of unified above models to facilitate the standardization of digital twin.}
}
@article{PANZARELLA2024116522,
title = {MAATrica: a measure for assessing consistency and methods in medicinal and nutraceutical chemistry papers},
journal = {European Journal of Medicinal Chemistry},
volume = {273},
pages = {116522},
year = {2024},
issn = {0223-5234},
doi = {https://doi.org/10.1016/j.ejmech.2024.116522},
url = {https://www.sciencedirect.com/science/article/pii/S0223523424004021},
author = {Giulia Panzarella and Alessandro Gallo and Sandra Coecke and Maddalena Querci and Francesco Ortuso and Martin Hofmann-Apitius and Pierangelo Veltri and Jürgen Bajorath and Stefano Alcaro},
keywords = {Medicinal chemistry, Nutraceuticals, Information extraction, Text mining, Research metrics},
abstract = {The growing number of scientific papers and document sources underscores the need for methods capable of evaluating the quality of publications. Researchers who are looking for relevant papers for their studies need ways to assess the scientific value of these documents. One approach involves using semantic search engines that can automatically extract important knowledge from the growing body of text. In this study, we introduce a new metric called “MAATrica,” which serves as the foundation for an innovative method designed to evaluate research papers. MAATrica offers a new way to analyze and categorize text, focusing on the consistency of research documents in the life sciences, particularly in the fields of medicinal and nutraceutical chemistry. This method utilizes semantic descriptions to cover in silico experiments, as well as in vitro and in vivo essays. Created to aid in evaluation processes like peer review, MAATrica uses toolkits and semantic applications to build the proposed measure, identify scientific entities, and gather information. We have applied MAATrica to roughly 90,000 papers and present our findings here.}
}
@article{BITTERMAN2021641,
title = {Clinical Natural Language Processing for Radiation Oncology: A Review and Practical Primer},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {110},
number = {3},
pages = {641-655},
year = {2021},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2021.01.044},
url = {https://www.sciencedirect.com/science/article/pii/S0360301621001188},
author = {Danielle S. Bitterman and Timothy A. Miller and Raymond H. Mak and Guergana K. Savova},
abstract = {Natural language processing (NLP), which aims to convert human language into expressions that can be analyzed by computers, is one of the most rapidly developing and widely used technologies in the field of artificial intelligence. Natural language processing algorithms convert unstructured free text data into structured data that can be extracted and analyzed at scale. In medicine, this unlocking of the rich, expressive data within clinical free text in electronic medical records will help untap the full potential of big data for research and clinical purposes. Recent major NLP algorithmic advances have significantly improved the performance of these algorithms, leading to a surge in academic and industry interest in developing tools to automate information extraction and phenotyping from clinical texts. Thus, these technologies are poised to transform medical research and alter clinical practices in the future. Radiation oncology stands to benefit from NLP algorithms if they are appropriately developed and deployed, as they may enable advances such as automated inclusion of radiation therapy details into cancer registries, discovery of novel insights about cancer care, and improved patient data curation and presentation at the point of care. However, challenges remain before the full value of NLP is realized, such as the plethora of jargon specific to radiation oncology, nonstandard nomenclature, a lack of publicly available labeled data for model development, and interoperability limitations between radiation oncology data silos. Successful development and implementation of high quality and high value NLP models for radiation oncology will require close collaboration between computer scientists and the radiation oncology community. Here, we present a primer on artificial intelligence algorithms in general and NLP algorithms in particular; provide guidance on how to assess the performance of such algorithms; review prior research on NLP algorithms for oncology; and describe future avenues for NLP in radiation oncology research and clinics.}
}
@article{MANSILLAQUINONES2024103922,
title = {Confronting coloniality of nature: Strategies to recover water and life in mapuche territory},
journal = {Geoforum},
volume = {148},
pages = {103922},
year = {2024},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2023.103922},
url = {https://www.sciencedirect.com/science/article/pii/S0016718523002488},
author = {Pablo Mansilla-Quiñones and Miguel Melín Pehuén and Alberto Curamil Millanao},
keywords = {Coloniality of nature, Hydroelectric plants, Decolonizing nature, Decolonial geography, Mapuche, Indigeonus geography},
abstract = {Indigenous peoples dispute the material and symbolic control of territory and nature in confronting the “coloniality of nature,” proposing respect for knowledge and environmental relations that modernity-coloniality-capitalism have denied. In these disputes, they ways that nature, energy and water are used and their meaning are at the center of the debate. This work presents a decade of participatory action research with Mapuche organizations, with which strategies against the installation of extractive projects have been deployed. In these experiences, we highlight the use of the concept of ixofillmongen as a category of Mapuche knowledge that proposes a relational ontological approach to other life forms that are part of nature. Here we present its use as part of a Mapuche strategy to decolonize nature implemented against the installation of hydroelectric plants. Based on this work, we developed a participatory action research methodology that other indigenous communities can replicate for the design of collective research experiences and the creation of alternative territorial strategies. The research results show that it is possible to build strategies for the recovery of water based on native peoples’ territorial and natural knowledge and get them to dialogue with critical and decolonial geography.}
}
@article{ZHOU2021103328,
title = {Semantic-based discovery method for high-performance computing resources in cyber-physical systems},
journal = {Microprocessors and Microsystems},
volume = {80},
pages = {103328},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103328},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120304877},
author = {Aolong Zhou and Kaijun Ren and Xiaoyong Li and Wen Zhang and Xiaoli Ren and Kefeng Deng},
keywords = {Cyber-physical system, High-performance computing, Ontology, Resource discovery},
abstract = {High-performance computing (HPC) systems with powerful computing capabilities are becoming increasingly significant in the large-scale cyber-physical systems (CPS), helping CPS process a huge number of real-time data. Nowadays, the efficiency of HPC resource management and discovery becomes a challenging problem in CPS, due to the complex characteristics of HPC resources and the growing demands of the users. Although lots of recent efforts have been conducted to the resource discovery in distributed systems, they cannot be well adapted for the cross-regional HPC environments, due to the lack of unified model for the resources description and consideration for the usability demands of non-expert users. In this paper, we propose novel techniques to try to solve the problem. Specifically, we first propose a unified semantic model named HPCRO for specifying cross-regional HPC resources, and apply ontology reasoning to obtain more semantic information for queries. Moreover, we propose a WordNet-based quick resource index list data structure called WQRIL to improve the query. Finally, according to the proposed model and data structure, we propose an efficient discovery method called ROLD for cross-regional HPC resources. Extensive experimental results demonstrate that, our proposals not only maintain efficient resource discovery performance, but also achieve the highest precision rate (94.76%), recall rate (92.34%) and F1-score (93.53%).}
}
@article{WALTHER201929,
title = {The Research Core Dataset (KDSF) in the Linked Data context},
journal = {Procedia Computer Science},
volume = {146},
pages = {29-38},
year = {2019},
note = {14th International Conference on Current Research Information Systems, CRIS2018, FAIRness of Research Information},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.074},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930078X},
author = {Tatiana Walther and Christian Hauschke and Anna Kasprzik},
keywords = {research information management, Linked Data, VIVO, Research Core Dataset, ontology matching},
abstract = {This paper describes our efforts to implement the Research Core Dataset (“Kerndatensatz Forschung”; KDSF) as an ontology in VIVO. KDSF is used in VIVO to record the required metadata on incoming data and to produce reports as an output. While both processes need an elaborate adaptation of the KDSF specification, this paper focusses on the adaptation of the KDSF basic data model for recording data in VIVO. In this context, the VIVO and KDSF ontologies were compared with respect to domain, syntax, structure, and granularity in order to identify correspondences and mismatches. To produce an alignment, different matching approaches have been applied. Furthermore, we made necessary modifications and extensions on KDSF classes and properties.}
}
@article{SYRJALA2025115387,
title = {From consumers to consumption: The socio-technical assemblage of the persona in market segmentation},
journal = {Journal of Business Research},
volume = {194},
pages = {115387},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2025.115387},
url = {https://www.sciencedirect.com/science/article/pii/S0148296325002103},
author = {Henna Syrjälä and Carlos Diaz Ruiz and Hanna Leipämaa-Leskinen and Harri T. Luomala},
keywords = {Market segmentation, Audience segmentation, Customer segmentation, Assemblage thinking, Persona},
abstract = {Market segmentation is a fundamental concept in marketing research and practice, which, over the past seventy years, has become synonymous with customer segmentation—the act of classifying homogeneous consumer groups and representing them as idealized personas. However, customer segmentation and market segmentation are not identical, as the former focuses on different groups of people and the latter on strategically distinct markets. To address this issue, this paper contributes to marketing research by reclaiming market segmentation’s original meaning: distinguishing markets. Using assemblage thinking as an analytical tool, the paper proposes that socio-technical personas are proxies that represent markets as assemblages constituted by people (consumers), spaces, devices, and agendas that afford recurring practices (consumption). Drawing from an illustrative case of chocolate consumption, the study builds upon a recent ontological shift in consumer research and strategy that distributes agency from the consumer to a socio-technical assemblage that intertwines as a consumption practice is actualized.}
}
@article{AMOSA2025100198,
title = {Centering Pacific knowledge of time, space, and relationality in community-based translanguaging research: An engaged and fluid temporal methodology},
journal = {Research Methods in Applied Linguistics},
volume = {4},
number = {1},
pages = {100198},
year = {2025},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2025.100198},
url = {https://www.sciencedirect.com/science/article/pii/S2772766125000199},
author = {Honiara Amosa and Sam Amosa and Corinne A. Seals},
keywords = {Pacific methodologies, Temporality, Qualitative research, Translanguaging, Community-based research},
abstract = {In Western perspectives, space and place are often viewed as distinct concepts. Place refers to a specific physical location, whereas space describes the interactions that take place within that location. Time is typically understood as a fixed moment or the interval between moments. In contrast, Pacific perspectives see time, space, and place as interconnected with each other and with individuals. In this worldview, a space or place continues to resonate even after one leaves it, and time is seen as circular rather than linear, linking the past, present, and future. This interconnectedness is embodied in the concept of 'va', which emphasizes the importance of understanding relationships and responsibilities within the community involved in research (cf. Airini et al., 2010). This article argues for the importance of recognizing Indigenous epistemologies and ontologies as valid in their own right, which in this case means recognizing the importance of va in Pacific research. We also present our theorization of an Engaged and Fluid Temporal Methodology, which outlines via five principles how Pacific understandings guided our work with Pacific communities. We then walk through each of the five principles as a methodological guide for community-based research, providing illustrative examples from the Wellington Translanguaging Project. By focusing on principles to research that are imbued with respective Pacific understandings of time, space, and relationships (e.g. va), we argue that this supports the research to remain in and for the communities with which we worked, a core component of Indigenous research methodology.}
}
@article{BUGHIO20233471,
title = {Knowledge Organization System for Partial Automation to Improve the Security Posture of IoMT Networks},
journal = {Procedia Computer Science},
volume = {225},
pages = {3471-3478},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.342},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923015004},
author = {Kulsoom Saima Bughio},
keywords = {Internet of Medical Things, Remote Patient Monitoring, Medical Devices, Knowledge Organization System, SWRL Rules, Automated Reasoning, SPARQL},
abstract = {Remote patient monitoring is a healthcare delivery model that uses technology to collect and transmit patient data from a remote location to healthcare providers for analysis and treatment. Remote patient monitoring systems rely on a network infrastructure to gather and transmit data from patients to healthcare providers through a network. While these systems become more prevalent, they may also become targets for cyberattacks. This paper deals with the development of a domain ontology to facilitate partial automation to improve the security posture of IoT networks used in remote patient monitoring. For this purpose, it captures the semantics of the concepts and properties of the main security aspects of IoT medical devices. This is complemented by a comprehensive ruleset, evaluated by using SPARQL queries, and automated reasoning over the aggregated knowledge.}
}
@article{LAADHAR2022997,
title = {Web of Things Semantic Interoperability in Smart Buildings},
journal = {Procedia Computer Science},
volume = {207},
pages = {997-1006},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.155},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922010377},
author = {Amir Laadhar and Junior Dongo and Søren Enevoldsen and Frédéric Revaz and Dominique Gabioud and Torben Bach Pedersen and Martin Meyer and Brian Nielsen and Christian Thomsen},
keywords = {Web of Things, Internet of Things, Web of Things Discovery, Semantic Interoperability, Semantic Web, Ontology},
abstract = {Buildings are the largest energy consumers in Europe and are responsible for approximately 40% of EU energy consumption and 36% of the greenhouse gas emissions in Europe. Two-thirds of the building consumption is for residential buildings. To achieve energy efficiency, buildings are being integrated with IoT devices through the use of smart IoT services. For instance, a smart space heating service reduces energy consumption by dynamically heating apartments based on indoor and outdoor temperatures. The W3C recommends the use of the Web of Things (WoT) standard to enable IoT interoperability on the Web. However, in the context of a smart building, the ability to search and discover building metadata and IoT devices available in the WoT ecosystems remains a challenge due to the limitation of the current WoT Discovery, which only includes a directory containing only IoT devices metadata without including building metadata. Integrating the IoT device's metadata with building metadata in the same directory can provide better discovery capabilities to the IoT services providers. In this paper, we integrate building metadata into the W3C WoT Discovery through the construction of a Building Description JSON-LD file. This Building Description is integrated into the W3C WoT Discovery and based on the domOS Common Ontology (dCO) to achieve semantic interoperability in smart residential buildings for the WoT IoT ecosystem within the Horizon 2020 domOS project. This integration results in a Thing and Building Description Directory. dCO integrates the SAREF core ontology with the Thing Description ontology, devices, and building metadata. We have implemented and validated the WoT discovery on top of a WoT Thing and Building Description Directory. The WoT Discovery implementation is also made available for the WoT community.}
}
@article{GRASSI2022103938,
title = {Knowledge triggering, extraction and storage via human–robot verbal interaction},
journal = {Robotics and Autonomous Systems},
volume = {148},
pages = {103938},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103938},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021002207},
author = {Lucrezia Grassi and Carmine Tommaso Recchiuto and Antonio Sgorbissa},
keywords = {Social robotics, Human–robot interaction, Knowledge-grounded conversation, Knowledge extraction},
abstract = {This article describes a novel approach to expand in run-time the knowledge base of an Artificial Conversational Agent. A technique for automatic knowledge extraction from the user’s sentence and four methods to insert the new acquired concepts in the knowledge base have been developed and integrated into a system that has already been tested for knowledge-based conversation between a social humanoid robot and residents of care homes. The run-time addition of new knowledge allows overcoming some limitations that affect most robots and chatbots: the incapability of engaging the user for a long time due to the restricted number of conversation topics. The insertion in the knowledge base of new concepts recognized in the user’s sentence is expected to result in a wider range of topics that can be covered during an interaction, making the conversation less repetitive. Two experiments are presented to assess the performance of the knowledge extraction technique, and the efficiency of the developed insertion methods when adding several concepts in the Ontology.}
}
@article{ASPRINO2022100683,
title = {A reference architecture for social robots},
journal = {Journal of Web Semantics},
volume = {72},
pages = {100683},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100683},
url = {https://www.sciencedirect.com/science/article/pii/S157082682100055X},
author = {Luigi Asprino and Paolo Ciancarini and Andrea Giovanni Nuzzolese and Valentina Presutti and Alessandro Russo},
keywords = {Social robots design, Software architectures, Architectural patterns, Interoperability, Ontologies, Linked open data},
abstract = {Social robotics poses tough challenges to software designers who are required to take care of difficult architectural drivers like acceptability, trust of robots as well as to guarantee that robots establish a personalized interaction with their users. Moreover, in this context recurrent software design issues such as ensuring interoperability, improving reusability and customizability of software components also arise. Designing and implementing social robotic software architectures is a time-intensive activity requiring multi-disciplinary expertise: this makes it difficult to rapidly develop, customize, and personalize robotic solutions. These challenges may be mitigated at design time by choosing certain architectural styles, implementing specific architectural patterns and using particular technologies. Leveraging on our experience in the MARIO project, in this paper we propose a series of principles that social robots may benefit from. These principles lay also the foundations for the design of a reference software architecture for social robots. The goal of this work is twofold: (i) Establishing a reference architecture whose components are unambiguously characterized by an ontology thus allowing to easily reuse them in order to implement and personalize social robots; (ii) Introducing a series of standardized software components for social robots architecture (mostly relying on ontologies and semantic technologies) to enhance interoperability, to improve explainability, and to favor rapid prototyping.}
}
@article{FERRO2023102174,
title = {AStar: A modeling language for document-oriented geospatial data warehouses},
journal = {Data & Knowledge Engineering},
volume = {145},
pages = {102174},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102174},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23000344},
author = {Marcio Ferro and Edson Silva and Robson Fidalgo},
keywords = {Geospatial data warehouse, Document-oriented databases, Logical schemas, DSML, Metamodel},
abstract = {A Geospatial Data Warehouse (GDW) is an extension of a traditional Data Warehouse that includes geospatial data in the decision-making processes. Several studies have proposed the use of document-oriented databases in a GDW as an alternative to relational databases. This is due to the ability of non-relational databases to scale horizontally, allowing for the storage and processing of large volumes of data. In this context, modeling the manner in which facts and dimensions are structured is important in order to understand, maintain, and evolve the Document-oriented GDW (DGDW) through visual analysis. However, to the best of our knowledge, there are no modeling languages that support the design of aggregated data as facts and dimensions, that can be represented as referenced or embedded documents, partitioned into one or more collections. To overcome this lack, we propose Aggregate Star (AStar), a Domain-Specific Modeling Language for designing DGDW logical schemas. AStar is defined from a concrete syntax (graphical notation), an abstract syntax (metamodel), and static semantics (well-formedness rules). In order to describe the semantics of the concepts defined in AStar, translational semantics map the graphical notation to the metamodel and the respective code, to define the schema in MongoDB (using JSON Schema). We evaluate the graphical notation using Physics of Notations (PoN), which provides a set of principles for designing cognitively effective visual notations. This evaluation revealed that AStar is in accordance with seven of the nine PoN principles, an adequate level of cognitive effectiveness. As a proof of concept, the metamodel and well-formedness rules were implemented in a prototype of Computer-Assisted Software Engineering tool, called AStarCASE. In its current version, AStarCASE can be used to design DGDW logical schemas and to generate their corresponding code in the form of JSON Schemas.}
}
@article{BRIDGEWATER2025101096,
title = {Making sense of meaning, making meaning of sense: Re-centring care in research},
journal = {Emotion, Space and Society},
volume = {56},
pages = {101096},
year = {2025},
issn = {1755-4586},
doi = {https://doi.org/10.1016/j.emospa.2025.101096},
url = {https://www.sciencedirect.com/science/article/pii/S1755458625000350},
author = {Grace Bridgewater},
keywords = {Care ethics, Relationality, Reflexivity, Positionality, Knowledge production, Lived experience},
abstract = {Utilising narrative form and ethnographic method, this paper is a reflexive plea for an academic community which foregrounds care and relationality. Contributing to literature on the politics of knowledge and continuing the pre-existing metaphor of knowledge-as-vision, I argue for a more unified, binocular gaze which draws just as much on meaning as it does on sense. The evidence for this argument stems from phenomenological reflection on my experiences and positionality within research settings, in the field, and as both a mental health patient and practitioner. This paper is a methodological account which aims to embrace the messy complexities of research and draws on a wide range of literature, principally, feminist care ethics, relational ontologies, philosophies of science and knowledge production, and democratic therapeutic communities. Ultimately, I outline how a neoliberal and production-orientated academy encourages a dangerous level of emotional repression and disavowal of meaning, harming a multiplicity of actors.}
}
@article{ZHOU2019106577,
title = {An agent composition framework for the J-Park Simulator - A knowledge graph for the process industry},
journal = {Computers & Chemical Engineering},
volume = {130},
pages = {106577},
year = {2019},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2019.106577},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419305058},
author = {Xiaochi Zhou and Andreas Eibeck and Mei Qi Lim and Nenad B. Krdzavac and Markus Kraft},
keywords = {Semantic web, Semantic web service composition, Agent, Cross-domain, Linked data, Knowledge graph},
abstract = {Digital twins, Industry 4.0 and Industrial Internet of Things are becoming ever more important in the process industry. The Semantic Web, linked data, knowledge graphs and web services/agents are key technologies for implementing the above concepts. In this paper, we present a comprehensive semantic agent composition framework. It enables automatic agent discovery and composition to generate cross-domain applications. This framework is based on a light-weight agent ontology, OntoAgent, which is an adaptation of the Minimal Service Model (MSM) ontology. The MSM ontology was extended with grounding components to support the execution of an agent while keeping the compatibility with other existing web service description standards and extensibility. We illustrate how the comprehensive agent composition framework can be integrated into the J-Park Simulator (JPS) knowledge graph, for the automatic creation of a composite agent that simulates the dispersion of the emissions of a power plant within a selected spatial area.}
}
@article{BRYANT2020101817,
title = {Improving SIEM alert metadata aggregation with a novel kill-chain based classification model},
journal = {Computers & Security},
volume = {94},
pages = {101817},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101817},
url = {https://www.sciencedirect.com/science/article/pii/S016740482030095X},
author = {Blake D. Bryant and Hossein Saiedian},
keywords = {Network monitoring, Intrusion detection, Kill-chain, Advanced persistent threat, APT, Security information and event management, SIEM, Security log ontology, Computer network defense, Attack ontology, Threat framework},
abstract = {Today’s information networks face increasingly sophisticated and persistent threats, where new threat tools and vulnerability exploits often outpace advancements in intrusion detection systems. Current detection systems often create too many alerts, which contain insufficient data for analysts. As a result, the vast majority of alerts are ignored, contributing to security breaches that might otherwise have been prevented. Security Information and Event Management (SIEM) software is a recent development designed to improve alert volume and content by correlating data from multiple sensors. However, insufficient SIEM configuration has thus far limited the promise of SIEM software for improving intrusion detection. The focus of our research is the implementation of a hybrid kill-chain framework as a novel configuration of SIEM software. Our research resulted in a new log ontology capable of normalizing security sensor data in accordance with modern threat research. New SIEM correlation rules were developed using the new log ontology, and the effectiveness of the new configuration was tested against a baseline configuration. The novel configuration was shown to improve detection rates, give more descriptive alerts, and lower the number of false positive alerts.}
}
@article{YU2022664,
title = {Detecting conflict of heterogeneous access control policies},
journal = {Digital Communications and Networks},
volume = {8},
number = {5},
pages = {664-679},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822001778},
author = {Mingjie Yu and Fenghua Li and Nenghai Yu and Xiao Wang and Yunchuan Guo},
keywords = {Conflict detection, Access control, Decision diagram, Policy translation, Heterogeneous policy, Ontology reasoner},
abstract = {Policy conflicts may cause substantial economic losses. Although a large amount of effort has been spent on detecting intra-domain policy conflict, it can not detect conflicts of heterogeneous policies. In this paper, considering background knowledge, we propose a conflict detection mechanism to search and locate conflicts of heterogeneous policies. First, we propose a general access control model to describe authorization mechanisms of cloud service and a translation scheme designed to translate a cloud service policy to an Extensible Access Control Markup Language (XACML) policy. Then the scheme based on Multi-terminal Multi-data-type Interval Decision Diagram (MTMIDD) and Extended MTMIDD (X-MTMIDD) is designed to represent XACML policy and search the conflict among heterogeneous policies. To reduce the rate of false positives, the description logic is used to represent XACML policy and eliminate false conflicts. Experimental results show the efficiency of our scheme.}
}
@article{HAN2024102965,
title = {CMCN: Chinese medical concept normalization using continual learning and knowledge-enhanced},
journal = {Artificial Intelligence in Medicine},
volume = {157},
pages = {102965},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102965},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724002070},
author = {Pu Han and Xiong Li and Zhanpeng Zhang and Yule Zhong and Liang Gu and Yingying Hua and Xiaoyan Li},
keywords = {Medical concept normalization, Deep learning, Continual learning},
abstract = {Medical Concept Normalization (MCN) is a crucial process for deep information extraction and natural language processing tasks, which plays a vital role in biomedical research. Although MCN in English has achieved significant research achievements, Chinese medical concept normalization (CMCN) remains insufficiently explored due to its complex syntactic structure and the paucity of Chinese medical semantic and ontology resources. In recent years, deep learning has been extensively applied across numerous natural language processing tasks, owing to its robust learning capabilities, adaptability, and transferability. It has proven to be well suited for intricate and specialized knowledge discovery research in the biomedical field. In this study, we conduct research on CMCN through the lens of deep learning. Specifically, our research introduces a model that leverages polymorphic semantic information and knowledge enhanced through multi-task learning and retain more important medical features through continual learning. As the cornerstone of CMCN, disease names are the main focus of this research. We evaluated various methodologies on Chinese disease dataset built by ourselves, finally achieving 76.12 % on Accuracy@1, 87.20 % on Accuracy@5 and 90.02 % on Accuracy@10 with our best-performing model GCBM-BSCL. This research not only advances the fields of knowledge mining and medical concept normalization but also enhances the integration and application of artificial intelligence in the medical and health field. We have published the source code and results on https://github.com/BearLiX/CMCN.}
}
@article{SCHMIDTKE202027,
title = {TextMap: A general purpose visualization system},
journal = {Cognitive Systems Research},
volume = {59},
pages = {27-36},
year = {2020},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041719304620},
author = {H.R. Schmidtke},
keywords = {Language-perception interface, Mental models, Visualization},
abstract = {Human language is a versatile tool for communicating mental models between speakers of a language. This paper presents the TextMap system, a logic-based system for generating visuospatial representations from textual input. TextMap combines a minimalistic parser with a simple model counting mechanism to automatically extract coordinate information from propositional Horn-logic knowledge bases encoding spatial predications. The system is based on a biologically inspired low-level bit vector mechanism, the activation bit vector machine (ABVM). It does not require an ontology apart from a list of which tokens indicate relations. Its minimalism and simplicity make it a general purpose visualization or imagery tool. The paper describes the TextMap application architecture as well as the key algorithms forming the ABVM core. The system is evaluated with respect to a larger case study of a geographic layout of 13 cities demonstrating capabilities as well as current shortcomings for a complex scenario of a human mental map description.}
}
@article{DIAC2019640,
title = {Relational Model for Parameter Description in Automatic Semantic Web Service Composition},
journal = {Procedia Computer Science},
volume = {159},
pages = {640-649},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.219},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314036},
author = {Paul Diac and Liana Ţucăr and Andrei Netedu},
keywords = {web service composition, semantic web, relational concepts, automatic composition algorithm, knowledge-based systems},
abstract = {Automatic Service Composition is a research direction aimed at facilitating the usage of atomic web services. Particularly, the goal is to build workflows of services that solve specific queries, which cannot be resolved by any single service from a known repository. Each of these services is described independently by their providers that can have no interaction with each other, therefore some common standards have been developed, such as WSDL, BPEL, OWL-S. Our proposal is to use such standards together with JSON-LD to model a next level of semantics, mainly based on binary relations between parameters of services. Services relate to a public ontology to describe their functionality. Binary relations can be specified between input and/or output parameters in service definition. The ontology includes some relations and inference rules that help to deduce new relations between parameters of services. To our knowledge, it is for the first time that parameters are matched not only based on their type, but on a more meaningful semantic context considering such type of relations. This enables the automation of a large part of the reasoning that a human person would do when manually building a composition. Moreover, the proposed model and the composition algorithm can work with multiple objects of the same type, a fundamental feature that was not possible before. We believe that the poor model expressiveness is what is keeping service composition from reaching large-scale application in practice.}
}
@article{VALCAMONICO2025110834,
title = {A systematic procedure for the analysis of maintenance reports based on a taxonomy and BERT attention mechanism},
journal = {Reliability Engineering & System Safety},
volume = {257},
pages = {110834},
year = {2025},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2025.110834},
url = {https://www.sciencedirect.com/science/article/pii/S0951832025000377},
author = {Dario Valcamonico and Piero Baraldi and July Bias Macêdo and Márcio Das Chagas Moura and Jonathan Brown and Stéphane Gauthier and Enrico Zio},
keywords = {Maintenance, Natural Language Processing, BERT, DBSCAN, Freight transport trains},
abstract = {This work proposes a systematic procedure for analyzing maintenance reports to support maintenance decision-making for a fleet of similar systems. The proposed procedure allows achieving three objectives: (1) grouping maintenance interventions, (2) identifying common characteristics in the maintenance interventions, and (3) recognizing occurrences of rare events of maintenance intervention. Specifically, the attention mechanism of Bidirectional Encoder Representation from Transformer (BERT) and the Density Based Spatial Clustering Applications with Noise (DBSCAN) methods are combined to group maintenance interventions according to their similarity of stated features. A taxonomy of the words used in the textual reports to state the maintenance interventions is developed to systematically identify common features of the clusters, such as the involved components, their working state, the occurred failures or malfunctions, the performed maintenance actions and the personnel that has performed the intervention. The proposed procedure is applied to a repository of reports of maintenance interventions performed on mechanical and electric components of traction systems of a fleet of trains. The obtained results show that it can effectively support decision-making on the maintenance of traction systems.}
}
@article{VARGA2018240,
title = {Analytical metadata modeling for next generation BI systems},
journal = {Journal of Systems and Software},
volume = {144},
pages = {240-254},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.06.039},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218301274},
author = {Jovan Varga and Oscar Romero and Torben Bach Pedersen and Christian Thomsen},
keywords = {Business intelligence, Metadata, Ontological metamodeling},
abstract = {Business Intelligence (BI) systems are extensively used as in-house solutions to support decision-making in organizations. Next generation BI 2.0 systems claim for expanding the use of BI solutions to external data sources and assisting the user in conducting data analysis. In this context, the Analytical Metadata (AM) framework defines the metadata artifacts (e.g., schema and queries) that are exploited for user assistance purposes. As such artifacts are typically handled in ad-hoc and system specific manners, BI 2.0 argues for a flexible solution supporting metadata exploration across different systems. In this paper, we focus on the AM modeling. We propose SM4AM, an RDF-based Semantic Metamodel for AM. On the one hand, we claim for ontological metamodeling as the proper solution, instead of a fixed universal model, due to (meta)data models heterogeneity in BI 2.0. On the other hand, RDF provides means for facilitating defining and sharing flexible metadata representations. Furthermore, we provide a method to instantiate our metamodel. Finally, we present a real-world case study and discuss how SM4AM, specially the schema and query artifacts, can help traversing different models instantiating our metamodel and enabling innovative means to explore external repositories in what we call metamodel-driven (meta)data exploration.}
}
@article{PERZYLO20191590,
title = {Capability-based semantic interoperability of manufacturing resources: A BaSys 4.0 perspective},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {1590-1596},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.427},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319314089},
author = {Alexander Perzylo and Julian Grothoff and Levi Lucio and Michael Weser and Somayeh Malakuti and Pierre Venet and Vincent Aravantinos and Torben Deppe},
keywords = {Intelligent manufacturing systems, modeling of assembly units, device integration technologies, knowledge modelling, knowledge based systems, ontology-based models interoperability},
abstract = {In distributed manufacturing systems, the level of interoperability of hardware and software components depends on the quality and flexibility of their information models. Syntactic descriptions of input and output parameters, e.g., using interface description languages (IDL), are not sufficient when it comes to evaluating whether a manufacturing resource provides the capabilities that are required for performing a particular process step on a product. The semantics of capabilities needs to be explicitly modelled and must be provided together with manufacturing resources. In this paper, we introduce concepts developed by the German BaSys 4.0 initiative dealing with semantically describing manufacturing skills, orchestrating higher-level skills from basic skills, and using them in a cognitive manufacturing framework.}
}
@article{CANOVASSEGURA201998,
title = {A lightweight acquisition of expert rules for interoperable clinical decision support systems},
journal = {Knowledge-Based Systems},
volume = {167},
pages = {98-113},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119300073},
author = {Bernardo Cánovas-Segura and Antonio Morales and Jose M. Juarez and Manuel Campos and Francisco Palacios},
keywords = {Clinical decision support systems, Rule-based systems, Knowledge acquisition, Semantic interoperability, Antimicrobial Susceptibility Testing},
abstract = {Background:
The process of adding new knowledge in the form of rules to already running Clinical Decision Support Systems (CDSSs) in hospitals is extremely costly and time consuming. There are two principal limitations: (1) the lack of a broad consensus regarding a uniform representation of clinical rules; and (2) the integration of new rule-based knowledge into hospital information systems.
Objective:
To provide a guideline with which to support knowledge acquisition for rule-based CDSSs and to facilitate the integration of that knowledge into hospital datasets using standard clinical terminologies and ontologies as reference elements.
Materials and Methods:
We have designed a straightforward 4-step methodology with which to incorporate the external knowledge sources and data integration required to run CDSSs in hospitals. This lightweight methodology is based on a reference ontology that integrates standard clinical terminologies and its objective is to effectively acquire procedural knowledge in the form of rules.
Results:
We have applied the methodology in the context of antimicrobial stewardship at a hospital. Recommendations from the European Committee on Antimicrobial Susceptibility Testing (EUCAST) were added to WASPSS, a CDSS running at the hospital. The reference ontology combines a subset of ATC terminologies for antibiotics and those of NCBI for microorganisms, including 584 and 1714 concepts, respectively. A total of 94 new rules were added to the CDSS so as to represent EUCAST knowledge. We also evaluated different implementations in order to study their scalability, during which time we analysed Drools 7.5 as a production rule engine, HermiT as an ontology reasoner and RuQAR as an integration tool. Our experiments show that the combination of a production rule engine and an ontology reasoner in runtime is more efficient than using a single rule engine with a knowledge base derived from the reference ontology (1.9 times faster than the next approach when executing 1000 expert rules on an ontology of 1000 concepts).
Discussion:
The methodology proposed helped to implement the knowledge acquisition process of EUCAST rules in a running CDSS. This methodology is applicable to other clinical domains when knowledge can be modelled with rules. Since it is a lightweight methodology, different implementation strategies are possible. The use of clinical standards also facilitates the future interoperation between CDSSs, particularly when using SNOMED as a reference ontology and employing future rule-sharing standards.}
}
@article{MITTAL2023227,
title = {Acoustic Based Emergency Vehicle Detection Using Ensemble of deep Learning Models},
journal = {Procedia Computer Science},
volume = {218},
pages = {227-234},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923000054},
author = {Usha Mittal and Priyanka Chawla},
keywords = {Audio recognition, CNN, DNN, emergency vehicle detection, MFCC, RNN, siren sound},
abstract = {The temporal and spectral structure is possessed in the time-frequency domain by sound events. Analyzing and classifying acoustic environment using sound recording is an emerging research area. Convolutional layers can quickly extract high-level features and shift-invariant features from the time-frequency domain. In this work, emergency vehicle detection (EVD) like fire brigades, ambulances, and police cars is done based upon their siren sounds. Dataset from Google Audioset ontology was collected and features are extracted by Mel-frequency Cepstral Coefficient (MFCC). Three deep neural networks (DNN) models (dense layer, Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN)) with different configurations and parameters have been investigated. Then, an ensemble model has been designed with optimum selected models by performing experimental tests on various configurations with hyper-parameter tuning. The proposed ensemble model provides the highest accuracy of 98.7%, while the recurrent neural network (RNN) model provides an accuracy of 94.5%. Also, performance analysis of deep learning models is done with various machine learning models like Perceptron, SVM, decision tree etc.}
}
@article{ZHAO2022104736,
title = {Construction of petrochemical knowledge graph based on deep learning},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {76},
pages = {104736},
year = {2022},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2022.104736},
url = {https://www.sciencedirect.com/science/article/pii/S0950423022000134},
author = {Yuchao Zhao and Beike Zhang and Dong Gao},
keywords = {HAZOP, Knowledge, Graph, Deep learning, BI-GAT-CRF, Named entity recognition},
abstract = {Hazard and operability analysis (HAZOP) is a safety evaluation method that is vital in the chemical process. HAZOP analysis uses the form of “brainstorming” and “counterfactual reasoning” to determine the potential events for danger in the chemical process. However, this method relies too much on expert experience and a large amount of existing HAZOP information has not been shared and reused. Herein we prompt a semi-automated HAZOP knowledge graph to overcome the problem of information reuse and sharing. In the implementation of the knowledge graph, the construction of the HAZOP ontology is completed by the seven-step rule in the top-down ontology construction. The named entity recognition task is one of the key tasks for the construction of knowledge graphs. The deep learning method is adopted in this paper and is based on the existing HAZOP data to recognize the named entity of HAZOP text. We adopt the deep neural network of BI-GAT-CRF, which has an accuracy of 90.75, a recall rate of 91.53, and an F1 score of 91.14 in the HAZOP Chinese text. BI-GAT-CRF aims to solve the problem of unclear entity boundary recognition and word ambiguity in named entity recognition tasks. The results show that the model performs well in HAZOP named entity recognition. BI-GAT-CRF can effectively determine entity boundaries and recognize named entity categories. BI-GAT-CRF also solves the problem of word ambiguity and improves the accuracy of named entity recognition tasks. Finally, a HAZOP analysis report of a certain oil equipment was used as an example to construct a HAZOP knowledge graph to verify the effectiveness of the construction method.}
}
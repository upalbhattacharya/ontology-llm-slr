@article{HUANG2023,
title = {A Semantic Matching Method of E-Government Information Resources Knowledge Fusion Service Driven by User Decisions},
journal = {Journal of Organizational and End User Computing},
volume = {35},
number = {1},
year = {2023},
issn = {1546-2234},
doi = {https://doi.org/10.4018/JOEUC.317082},
url = {https://www.sciencedirect.com/science/article/pii/S1546223423000552},
author = {Xinping Huang and Siyuan Zhu and Yue Ren},
keywords = {e-Government Information, Knowledge Fusion, Semantic Similarity, Service Matching, User Decision},
abstract = {ABSTRACT
This study focuses on the knowledge fusion model of e-government information resources that supports user decision-making information needs, it discusses the user decision-making information needs model, the knowledge fusion service model, and the relationship between them. The inter-layer mapping matching mechanism realizes the ultimate value of knowledge fusion. Therefore, this paper analyses and studies the mapping mechanism between the user information demand model and the knowledge fusion service model. A semantic, similarity-based knowledge fusion service matching method for e-government information resources is proposed to address the problem of lack of semantics in traditional web service matching methods. This method uses the ontology description language OWL-S to map information requirement documents of user decisions and knowledge fusion service function documents into an ontology tree structure. The authors then use this as the basis to calculate the concept similarity and relationship similarity measures, and the service matching based on semantic similarity can be realized.}
}
@article{ZHANG2023349,
title = {Threat Modeling and Application Research Based on Multi-Source Attack and Defense Knowledge},
journal = {Computers, Materials and Continua},
volume = {77},
number = {1},
pages = {349-377},
year = {2023},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.040964},
url = {https://www.sciencedirect.com/science/article/pii/S1546221823001340},
author = {Shuqin Zhang and Xinyu Su and Peiyu Shi and Tianhui Du and Yunfei Han},
keywords = {Multi-source data fusion, threat modeling, threat propagation path, knowledge graph, intelligent defense decision-making},
abstract = {Cyber Threat Intelligence (CTI) is a valuable resource for cybersecurity defense, but it also poses challenges due to its multi-source and heterogeneous nature. Security personnel may be unable to use CTI effectively to understand the condition and trend of a cyberattack and respond promptly. To address these challenges, we propose a novel approach that consists of three steps. First, we construct the attack and defense analysis of the cybersecurity ontology (ADACO) model by integrating multiple cybersecurity databases. Second, we develop the threat evolution prediction algorithm (TEPA), which can automatically detect threats at device nodes, correlate and map multi-source threat information, and dynamically infer the threat evolution process. TEPA leverages knowledge graphs to represent comprehensive threat scenarios and achieves better performance in simulated experiments by combining structural and textual features of entities. Third, we design the intelligent defense decision algorithm (IDDA), which can provide intelligent recommendations for security personnel regarding the most suitable defense techniques. IDDA outperforms the baseline methods in the comparative experiment.}
}
@article{SAHIN2023104264,
title = {Unpacking teachers’ orientations toward a knowledge generation approach: Do we need to go beyond epistemology?},
journal = {Teaching and Teacher Education},
volume = {132},
pages = {104264},
year = {2023},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2023.104264},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X23002524},
author = {Ercin Sahin and Jee Kyung Suh and Brian Hand and Gavin Fulmer},
keywords = {Teacher change, Epistemology, Axiology, Ontology, Generative learning, Professional development},
abstract = {This study aimed to assess the effectiveness of a professional development program rooted in knowledge generation theory. Specifically, it sought to examine the changes in teachers' three orientations following the completion of the first-year workshop, and how these changes impacted their classroom implementation. This investigation takes the form of a multiple-case study of twelve K-5 teachers. Data were collected through semi-structured interviews and classroom observations. The findings suggest that achieving changes toward generative learning in teachers is not just about altering their epistemological orientations and procedural pedagogical practices, but also involves a shift towards ontological and axiological perspectives.}
}
@article{SINGLA2024,
title = {Developing a Chatbot to Support Individuals With Neurodevelopmental Disorders: Tutorial},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/50182},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124003224},
author = {Ashwani Singla and Ritvik Khanna and Manpreet Kaur and Karen Kelm and Osmar Zaiane and Cory Scott Rosenfelt and Truong An Bui and Navid Rezaei and David Nicholas and Marek Z Reformat and Annette Majnemer and Tatiana Ogourtsova and Francois Bolduc},
keywords = {chatbot, user interface, knowledge graph, neurodevelopmental disability, autism, intellectual disability, attention-deficit/hyperactivity disorder},
abstract = {Families of individuals with neurodevelopmental disabilities or differences (NDDs) often struggle to find reliable health information on the web. NDDs encompass various conditions affecting up to 14% of children in high-income countries, and most individuals present with complex phenotypes and related conditions. It is challenging for their families to develop literacy solely by searching information on the internet. While in-person coaching can enhance care, it is only available to a minority of those with NDDs. Chatbots, or computer programs that simulate conversation, have emerged in the commercial sector as useful tools for answering questions, but their use in health care remains limited. To address this challenge, the researchers developed a chatbot named CAMI (Coaching Assistant for Medical/Health Information) that can provide information about trusted resources covering core knowledge and services relevant to families of individuals with NDDs. The chatbot was developed, in collaboration with individuals with lived experience, to provide information about trusted resources covering core knowledge and services that may be of interest. The developers used the Django framework (Django Software Foundation) for the development and used a knowledge graph to depict the key entities in NDDs and their relationships to allow the chatbot to suggest web resources that may be related to the user queries. To identify NDD domain–specific entities from user input, a combination of standard sources (the Unified Medical Language System) and other entities were used which were identified by health professionals as well as collaborators. Although most entities were identified in the text, some were not captured in the system and therefore went undetected. Nonetheless, the chatbot was able to provide resources addressing most user queries related to NDDs. The researchers found that enriching the vocabulary with synonyms and lay language terms for specific subdomains enhanced entity detection. By using a data set of numerous individuals with NDDs, the researchers developed a knowledge graph that established meaningful connections between entities, allowing the chatbot to present related symptoms, diagnoses, and resources. To the researchers’ knowledge, CAMI is the first chatbot to provide resources related to NDDs. Our work highlighted the importance of engaging end users to supplement standard generic ontologies to named entities for language recognition. It also demonstrates that complex medical and health-related information can be integrated using knowledge graphs and leveraging existing large datasets. This has multiple implications: generalizability to other health domains as well as reducing the need for experts and optimizing their input while keeping health care professionals in the loop. The researchers' work also shows how health and computer science domains need to collaborate to achieve the granularity needed to make chatbots truly useful and impactful.}
}
@article{SANTOS2025112407,
title = {Requirements extraction from model-based systems engineering: A systematic literature review},
journal = {Journal of Systems and Software},
volume = {226},
pages = {112407},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2025.112407},
url = {https://www.sciencedirect.com/science/article/pii/S0164121225000755},
author = {Jefferson L. Santos and Luiz Eduardo G. Martins and Jefferson Seide Molléri},
keywords = {Model-based systems engineering, Requirements engineering, Requirements extraction, Systematic literature review, Systems engineering},
abstract = {Collaboration and easy data exchange are crucial in modern systems that involve hardware, electronics, software, and users. Requirement Engineering (RE) and Systems Engineering (SE) are challenging fields that require tool support to automate activities. Natural language (NL) requirement documents can create processing issues. To address these issues, detailed models have been developed to represent a system effectively. These models are intend to replace inconsistent documents over time by using model-based methodologies like Model-Based SE (MBSE). Within the MBSE methodologies, Arcadia/Capella has proven its capabilities as a comprehensive tool in the SE community to define and validate complex system architecture. Thus, this paper aims to investigate the tools, methods, techniques, or processes for extracting requirements from the MBSE environment or model generation from NL requirements. Furthermore, this discusses how these approaches are applied specifically in the Arcadia/Capella and how transforming requirements are addressed to support textual requirements. We conducted a systematic literature review (SLR) by selecting 97 articles to examine advances in this field in various aspects of these approaches. The results presented in this SLR uncovered several key findings that have important implications for future research, such as the dominance of the model generation from NL; transforming model-based requirements to NL requires more data; and the fact that requirements extraction in Arcadia/Capella needs more evidence.}
}
@article{ZHANG2024102812,
title = {Detecting mental and physical disorders using multi-task learning equipped with knowledge graph attention network},
journal = {Artificial Intelligence in Medicine},
volume = {149},
pages = {102812},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102812},
url = {https://www.sciencedirect.com/science/article/pii/S093336572400054X},
author = {Wei Zhang and Ling Kong and Soobin Lee and Yan Chen and Guangxu Zhang and Hao Wang and Min Song},
keywords = {Mental disorder, Physical disorder, Muti-task learning, Knowledge graph, Graph attention network},
abstract = {Mental and physical disorders (MPD) are inextricably linked in many medical cases; psychosomatic diseases can be induced by mental concerns and psychological discomfort can ensue from physiological diseases. However, existing medical informatics studies focus on identifying mental or physical disorders from a unilateral perspective. Consequently, no existing domain knowledge base, corpus, or detection modeling approach considers mental as well as physical aspects concurrently. This paper proposes a joint modeling approach to detect MPD. First, we crawl through online medical consultation records of patients from websites and build an MPD knowledge ontology by extracting the core conceptual features of the text. Based on the ontology, an MPD knowledge graph containing 12,673 nodes and 82,195 relations is obtained using term matching with a domain thesaurus of each concept. Subsequently, an MPD corpus with fine-grained severities (None, Mild, Moderate, Severe, Dangerous) and 8909 records is constructed by formulating MPD classification criteria and a data annotation process under the guidance of domain experts. Taking the knowledge graph and corpus as the dataset, we design a multi-task learning model to detect the MPD severity, in which a knowledge graph attention network (KGAT) is embedded to better extract knowledge features. Experiments are performed to demonstrate the effectiveness of our model. Furthermore, we employ ontology-based and centrality-based methods to discover additional potential inferred knowledge, which can be captured by KGAT so as to improve the prediction performance and interpretability of our model. Our dataset has been made publicly available, so it can be further used as a medical informatics reference in the fields of psychosomatic medicine, psychiatrics, physical co-morbidity, and so on.}
}
@article{RICHTER2025114878,
title = {Microstructure informatics: Using computer vision for the characterization of dendrite growth phenomena in Ni-base single crystal Superalloys},
journal = {Materials Characterization},
volume = {223},
pages = {114878},
year = {2025},
issn = {1044-5803},
doi = {https://doi.org/10.1016/j.matchar.2025.114878},
url = {https://www.sciencedirect.com/science/article/pii/S1044580325001676},
author = {A.R. Richter and F. Scholz and G. Eggeler and J. Frenzel and P. Thome},
keywords = {Microstructure informatics, Deep learning, Relational geometric ontology, Ni-base single crystal superalloys, Collective dendrite growth phenomena},
abstract = {Microstructure informatics, an emerging field, combines traditional quantitative metallography with computer vision, algorithmic geometry and data science. It uses automated procedures to retrieve statistically relevant information from micrographs. Its power is demonstrated in a case study which focusses on competitive dendrite growth during directional solidification of single crystal Ni-base superalloys (SXs) in 3D. We show how microstructure informatics allows to follow the evolution of all dendrites in a cylindric SX bar (diameter: 12 mm, analyzed length: 76 mm), evaluating serial cross sections taken in 1 mm distances. The method presented in this work relies on three basic components: (1) A deep learning object detection network for detecting dendrite core positions. (2) A 3D image reconstruction routine for tracing dendrite paths and (3) a relational geometric ontological (RGO) database, documenting all relevant relationships between individual dendrites. The method allows to characterize crystal mosaicity, individual dendrite growth directions, interactions between dendrites and dendrite deformation. The performance of different deep learning classification architectures (AlexNet, GoogleNet and MobileNetV2) in combination with a YOLOv2 subdetection network is investigated. The network hyper parameters were optimized to achieve detection rates >99 %. A resulting ontological database of 16,631 individual dendrites provides a foundation for further automatic quantitative microstructural characterization.}
}
@article{CHEN2020769,
title = {Research on the construction of the semantic model for Chinese ancient architectures based on architectural narratives},
journal = {The Electronic Library},
volume = {38},
number = {4},
pages = {769-784},
year = {2020},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-02-2020-0039},
url = {https://www.sciencedirect.com/science/article/pii/S026404732000003X},
author = {Jinju Chen and Shiyan Ou},
keywords = {Ontology, Semantic Web, Semantic annotation, Visual analysis, Architectural narratives, Chinese ancient architectures, Semantic retrieval, Semantic reasoning},
abstract = {Purpose
This paper aims to reorganize the relevant information of Chinese ancient architectures with the use of Semantic Web technologies and thus facilitate its deep discovery and usage.
Design/methodology/approach
This paper proposes an ontology model for Chinese ancient architectures based on architectural narratives theory. To verify the availability of the ancient architecture ontology, we designed and implemented three experiments, including semantic retrieval based on SPARQL query, semantic reasoning with the use of Jena reasoner and visual analysis based on the Chinese Online Digital Humanities Resources Platform.
Findings
The proposed ontology provided a solution for the semantic annotation of the unstructured information of Chinese ancient architectures. On this basis, deep knowledge services such as semantic retrieval, semantic reasoning and visual analysis can be provided.
Practical implications
The proposed semantic model of ancient architectures can effectively improve the organization and access quality of the semantic content of Chinese ancient architectures.
Originality/value
This paper focuses on the semantic modelling for the unstructured information of Chinese ancient architectures to semantically describe the related entities (e.g. persons, events, places and times) and uncover their relationships, and thus it made contribution to the deep semantic annotations on ancient architectures.}
}
@article{BERETTA2021104807,
title = {A user-centric metadata model to foster sharing and reuse of multidisciplinary datasets in environmental and life sciences},
journal = {Computers & Geosciences},
volume = {154},
pages = {104807},
year = {2021},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.104807},
url = {https://www.sciencedirect.com/science/article/pii/S0098300421001060},
author = {Valentina Beretta and Jean-Christophe Desconnets and Isabelle Mougenot and Muhammad Arslan and Julien Barde and Véronique Chaffard},
keywords = {Interdisciplinary datasets, Semantic metadata model, Semantics, FAIR principles},
abstract = {The recent technological advancements and emergence of the open data in environmental and life sciences are opening new research opportunities while creating new challenges around data management. They make available an unprecedented amount of data that can be exploited for studying complex phenomena. However, new challenges related to data management need to be addressed to ensure effective data sharing, discovery and reuse, especially when dealing with interdisciplinary research contexts. These issues are magnified in interdisciplinary context, by the fact that each discipline has its practices, e.g., specific formats and metadata standards. Moreover, the majority of current data management practices do not consider semantic heterogeneity existing among disciplines. For this reason, we introduce a flexible metadata model that describes the datasets of various disciplines using a common paradigm based on the observation concept. It provides a key vision for articulating the user point of view and underlying scientific domains. In this study, we therefore decide to mainly reuse the SOSA lightweight ontology (Sensor, Observation, Sample, and Actuator) to efficiently leverage others existing ontologies to improve datasets discovery and reuse coming from Earth and life observation. The main benefit of the proposed metadata model is that it extends the technical description, usually provided by existing metadata models, with the observation context description enabling the need of a user viewpoint. Moreover, following the FAIR principles, the metadata model specifies the semantics of its elements using ontologies and vocabularies, and reuses as much as possible ontological and terminological existing resources. We show the benefit and applicability of the model through a case study we identified as representative after interviewing researchers in environmental and life sciences.}
}
@article{VOLCHENKOV20202583,
title = {Mathematical modeling of systems, its nature and limits of applicability},
journal = {Kybernetes},
volume = {50},
number = {9},
pages = {2583-2596},
year = {2020},
issn = {0368-492X},
doi = {https://doi.org/10.1108/K-06-2020-0393},
url = {https://www.sciencedirect.com/science/article/pii/S0368492X20000754},
author = {Evgeny Volchenkov},
keywords = {Mathematical modeling, Language, Information, Epistemology, Systems theory, System dynamics},
abstract = {Purpose
The purpose of this paper is to establish the nature of mathematical modeling of systems within the framework of the object-semantic methodology.
Design/methodology/approach
The initial methodological position of the object-semantic approach is the principle of constructing concepts of informatics proceeding from fundamental categories and laws. As the appropriate foundation, this paper accepts the system-physical meta-ontology is being developed in this paper.
Findings
The genesis of system modeling is considered in the aspect of the evolution of language tools in the direction of objectification. A new conception of formalized knowledge is being put forward as the mathematical form of fixing time-invariant relations of the universe, reflecting regularity of the dynamics of natural or anthropogenic organization. Object knowledge is considered as a key component of the mathematical model, and the solving of system information problems with its use is characterized as “work of knowledge.” The establishment of the meta-ontological essence of modern mathematical modeling allows us to formulate its fundamental limitations.
Originality/value
The establishment of system-physical limitations of modern mathematical modeling outlines the boundaries from which it is necessary to proceed in the development of future paradigms of cognition of the surrounding world, which presuppose convergence, synthesis of causal (physicalism) and target (elevationism) determination.}
}
@article{KLEIN2025110152,
title = {Combining informed data-driven anomaly detection with knowledge graphs for root cause analysis in predictive maintenance},
journal = {Engineering Applications of Artificial Intelligence},
volume = {145},
pages = {110152},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110152},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625001526},
author = {Patrick Klein and Lukas Malburg and Ralph Bergmann},
keywords = {Hybrid artificial intelligence, Knowledge-based diagnosis, Data-driven anomaly detection, Predictive maintenance, Knowledge graph, Semantic web technologies},
abstract = {Industry 4.0 has facilitated the access to sensor and actuator data from manufacturing systems, leading to studies on data-driven anomaly detection, but limited attention has been paid to finding root causes and automating this process using formalized expert knowledge. This is crucial due to the scarcity of qualified engineers and the time-consuming nature of diagnosing issues in large production systems. To address this gap, we present a framework that combines data-driven anomaly detection with a knowledge graph that provides domain knowledge by leveraging typical explanations of such models (i.e.,data streams potentially caused the detection) for further diagnosis. The framework’s usefulness to infer affected components or data set labels has been evaluated using two deep anomaly detection approaches. For knowledge-based diagnosis, three query strategies that utilize various knowledge graph relationships are implemented through three Artificial Intelligence (AI) techniques. The proposed anomaly detection approach, informed by integrating expert knowledge via the graph structure of the knowledge graph and node embeddings for encoding time series, outperforms baselines and a deep autoencoder in detecting anomalies and in identifying anomalous data streams. In subsequent diagnosis, it achieves the best performance on a complete knowledge graph in combination with a graph pattern matching query by identifying the label or affected component in 60% of detected anomalies by providing 4.1 labels or 2.3 components until the correct one is identified. In case of a corrupted one, Symbolic-Driven Neural Reasoning (SDNR) and Case-Based Reasoning (CBR) with knowledge graph embeddings demonstrate advantages by halving the number of incorrect labels and unaffected components.}
}
@article{WANG2025,
title = {Exploring Semantic Web Tools in Education to Boost Learning and Improve Organizational Efficiency},
journal = {International Journal on Semantic Web and Information Systems},
volume = {21},
number = {1},
year = {2025},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.370315},
url = {https://www.sciencedirect.com/science/article/pii/S1552628325000225},
author = {Liqun Wang and Wei Han and Zhimin Xi},
keywords = {Education, Semantic Web Tools, Digital Learning, Knowledge Sharing, Personalized Learning, Ontologies, Adaptive Learning, Intelligent, Landscape},
abstract = {ABSTRACT
Semantic Web technologies in education provide opportunities to improve learning outcomes and organizational efficiency via organized and significant information representation. However, with progress in educational technology, there is an absence of frameworks using Semantic Web capabilities to provide tailored learning experiences and enhance administrative procedures efficiently. Hence, this study proposes a framework called “Education using Semantic Web Tools (E-SWT),” employing ontologies, metadata tagging, and intelligent reasoning systems. The method supports seamless platform interoperability, adaptive content delivery, and efficient resource management. The experimental results demonstrate that the proposed model increases the learning experience ratio by 96.12%, Organizational Efficiency ratio by 97.53%, Fostering Collaboration by 98.82%, Data Management ratio by 97.68%, and Personalized Learning Paths by 96.66% compared to other existing models.}
}
@article{FURLAN2021,
title = {A Natural Language Processing–Based Virtual Patient Simulator and Intelligent Tutoring System for the Clinical Diagnostic Process: Simulator Development and Case Study},
journal = {JMIR Medical Informatics},
volume = {9},
number = {4},
year = {2021},
issn = {2291-9694},
doi = {https://doi.org/10.2196/24073},
url = {https://www.sciencedirect.com/science/article/pii/S2291969421001320},
author = {Raffaello Furlan and Mauro Gatti and Roberto Menè and Dana Shiffer and Chiara Marchiori and Alessandro {Giaj Levra} and Vincenzo Saturnino and Enrico Brunetta and Franca Dipaola},
keywords = {COVID-19, intelligent tutoring system, virtual patient simulator, natural language processing, artificial intelligence, clinical diagnostic reasoning},
abstract = {Background
Shortage of human resources, increasing educational costs, and the need to keep social distances in response to the COVID-19 worldwide outbreak have prompted the necessity of clinical training methods designed for distance learning. Virtual patient simulators (VPSs) may partially meet these needs. Natural language processing (NLP) and intelligent tutoring systems (ITSs) may further enhance the educational impact of these simulators.
Objective
The goal of this study was to develop a VPS for clinical diagnostic reasoning that integrates interaction in natural language and an ITS. We also aimed to provide preliminary results of a short-term learning test administered on undergraduate students after use of the simulator.
Methods
We trained a Siamese long short-term memory network for anamnesis and NLP algorithms combined with Systematized Nomenclature of Medicine (SNOMED) ontology for diagnostic hypothesis generation. The ITS was structured on the concepts of knowledge, assessment, and learner models. To assess short-term learning changes, 15 undergraduate medical students underwent two identical tests, composed of multiple-choice questions, before and after performing a simulation by the virtual simulator. The test was made up of 22 questions; 11 of these were core questions that were specifically designed to evaluate clinical knowledge related to the simulated case.
Results
We developed a VPS called Hepius that allows students to gather clinical information from the patient’s medical history, physical exam, and investigations and allows them to formulate a differential diagnosis by using natural language. Hepius is also an ITS that provides real-time step-by-step feedback to the student and suggests specific topics the student has to review to fill in potential knowledge gaps. Results from the short-term learning test showed an increase in both mean test score (P<.001) and mean score for core questions (P<.001) when comparing presimulation and postsimulation performance.
Conclusions
By combining ITS and NLP technologies, Hepius may provide medical undergraduate students with a learning tool for training them in diagnostic reasoning. This may be particularly useful in a setting where students have restricted access to clinical wards, as is happening during the COVID-19 pandemic in many countries worldwide.}
}
@article{LEE2025128263,
title = {SPARKLE: Enhancing SPARQL generation with direct KG integration in decoding},
journal = {Expert Systems with Applications},
volume = {289},
pages = {128263},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128263},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425018822},
author = {Jaebok Lee and Hyeonjeong Shin},
keywords = {Knowledge graph, Question answering, SPARQL, Constrained decoding},
abstract = {Existing KBQA methods have traditionally relied on multi-stage methodologies, involving tasks such as entity linking, subgraph retrieval, and query structure generation. However, multi-stage approaches are dependent on the accuracy of preceding steps, leading to cascading errors and increased inference time. Although a few studies have explored the use of end-to-end models, they often suffer from lower accuracy and generate inoperative queries that are not supported by the underlying data. Furthermore, most prior approaches are limited to static training data, potentially overlooking the evolving nature of knowledge bases over time. To address these challenges, we present a novel end-to-end natural language to SPARQL framework, SPARKLE. Notably, SPARKLE leverages the structure of the knowledge base directly during decoding, effectively integrating knowledge into the query generation. Our study reveals that simply referencing knowledge base during inference significantly reduces the occurrence of inexecutable query generations. SPARKLE achieves new state-of-the-art results on SimpleQuestions-Wiki and the highest F1 score on LCQuAD 1.0 (among models not using gold entities), while getting slightly lower results on the WebQSP dataset.1 Finally, we demonstrate SPARKLE’s fast inference speed and its ability to adapt when the knowledge base differs between the training and inference stages.}
}
@article{ONEILL2023270,
title = {Anatomic nomenclature and 3-dimensional regional model of the human ovary: call for a new paradigm},
journal = {American Journal of Obstetrics and Gynecology},
volume = {228},
number = {3},
pages = {270-275.e4},
year = {2023},
issn = {0002-9378},
doi = {https://doi.org/10.1016/j.ajog.2022.09.040},
url = {https://www.sciencedirect.com/science/article/pii/S0002937822007955},
author = {Kathleen E. O’Neill and Jacqueline Y. Maher and Monica M. Laronda and Francesca E. Duncan and Richard D. LeDuc and Marla E. Lujan and Kutluk H. Oktay and Alison M. Pouch and James H. Segars and Elizabeth L. Tsui and Mary B. Zelinski and Lisa M. Halvorson and Veronica Gomez-Lobo},
keywords = {bioinformatics, female reproductive anatomy, fertility preservation, gynecology, Human BioMolecular Atlas Program, oncofertility, oncopreservation, ontology, ovarian follicle, ovarian mapping, ovary atlas, Senescence Network, Systematized Nomenclature of Medicine Clinical Terms, Uber-anatomy ontology},
abstract = {The ovaries are the female gonads that are crucial for reproduction, steroid production, and overall health. Historically, the ovary was broadly divided into regions defined as the cortex, medulla, and hilum. This current nomenclature lacks specificity and fails to consider the significant anatomic variations in the ovary. Recent technological advances in imaging modalities and high-resolution omic analyses have brought about the need for revision of the existing definitions, which will facilitate the integration of generated data and enable the characterization of organ subanatomy and function at the cellular level. The creation of these high-resolution multimodal maps of the ovary will enhance collaboration and communication among disciplines and between clinicians and researchers. Beginning in March 2021, the Pediatric and Adolescent Gynecology Program of the Eunice Kennedy Shriver National Institute of Child Health and Human Development invited subject-matter experts to participate in a series of workshops and meetings to standardize ovarian nomenclature and define the organ’s features. The goal was to develop a spatially defined and semantically consistent terminology of the ovary to support collaborative, team science–based endeavors aimed at generating reference atlases of the human ovary. The group recommended a standardized, 3-dimensional description of the ovary and an ontological approach to the subanatomy of the ovary and definition of follicles. This new greater precision in nomenclature and mapping will better reflect the ovary’s heterogeneous composition and function, support the standardization of tissue collection, facilitate functional analyses, and enable clinical and research collaborations. The conceptualization process and outcomes of the effort, which spanned the better part of 2021 and early 2022, are introduced in this article. The institute and the workshop participants encourage researchers and clinicians to adopt the new systems in their everyday work to advance the overarching goal of improving human reproductive health.}
}
@article{ZHAO2024103524,
title = {A survey on cybersecurity knowledge graph construction},
journal = {Computers & Security},
volume = {136},
pages = {103524},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103524},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823004340},
author = {Xiaojuan Zhao and Rong Jiang and Yue Han and Aiping Li and Zhichao Peng},
keywords = {Cybersecurity knowledge graphs, Knowledge graph construction, Cybersecurity ontology, Named entity recognition, Relation extraction},
abstract = {The development of key technologies of knowledge graph (KG) has promoted the development of machine cognition technology, and the combination of KG and industry as well as scenario-based landing have also made breakthroughs in succession. In the field of cybersecurity, with the intelligent upgrading of defense technology, there is an urgent need for a mature and effective technical system to provide knowledge and intelligent reasoning support for the offensive and defensive games in strong adversarial and high dynamic environment. As a domain KG, cybersecurity KG (CKG) just meets this requirement. KG for cybersecurity is not a recent invention; its predecessor is the earliest semantic networks and ontologies, and the KG is small in scale and the relations are relatively simple. Through investigation, we found that most studies that explicitly mentioned CKG, still tend to construct a cybersecurity ontology first, and then extract semantic triples based on ontology. Of course, there are also some studies that try to build such a graph from a higher dimension to express the rich semantics. In order to apply mature techniques of KG construction and reasoning, we believe that the construction of CKG should also follow the Open-domain KG. Therefore, we conducted a comprehensive review and detailed comparison of CKG-related works, discussed the dilemma of CKG application, and then proposed future research opportunities for CKG. This work can help researchers keep up with recent research trends.}
}
@article{FUMAGALLI2025102453,
title = {Philosophical reflections on conceptual modeling as communication},
journal = {Data & Knowledge Engineering},
volume = {159},
pages = {102453},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102453},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000485},
author = {Mattia Fumagalli and Giancarlo Guizzardi},
keywords = {Conceptual modeling, Information systems design, Conceptual modeling foundations, Philosophical foundations},
abstract = {Conceptual modeling is a complex and demanding task. It is a task centered around the challenge of representing a portion of the world in a way that is objective, understandable, shareable, and reusable by a community of practitioners, who rely on models to design and implement software or to clarify the concepts within a given domain. The difficulty of conceptual modeling stems from the inherent limitations of human representation abilities, which cannot fully capture the infinite richness and diversity of the world, nor the endless possibilities for description enabled by language. Significant effort has been invested in addressing these challenges, particularly in the creation of effective and reusable conceptual models, which have presented numerous difficulties. This paper explores conceptual modeling from a philosophical standpoint, proposing that conceptual models should not be viewed merely as the static representational output of an a priori activity, subject to modification only during a preliminary design phase. Instead, they should be seen as dynamic artifacts that require continuous design, adaptation, and evolution from their inception to their application, which may account for multiple purposes. The paper seeks to highlight the importance of understanding conceptual modeling primarily as an act of communication, rather than just a process of information transmission. It also aims to clarify the distinction between these two aspects and to examine the potential implications of adopting a communicative approach to modeling. These implications extend not only to the tools and methodologies used in modeling but also to the ethical considerations that arise from such an approach.}
}
@article{FRENCH2023104252,
title = {An overview of biomedical entity linking throughout the years},
journal = {Journal of Biomedical Informatics},
volume = {137},
pages = {104252},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104252},
url = {https://www.sciencedirect.com/science/article/pii/S153204642200257X},
author = {Evan French and Bridget T. McInnes},
keywords = {Natural language processing, Entity linking, Normalization},
abstract = {Biomedical Entity Linking (BEL) is the task of mapping of spans of text within biomedical documents to normalized, unique identifiers within an ontology. This is an important task in natural language processing for both translational information extraction applications and providing context for downstream tasks like relationship extraction. In this paper, we will survey the progression of BEL from its inception in the late 80s to present day state of the art systems, provide a comprehensive list of datasets available for training BEL systems, reference shared tasks focused on BEL, discuss the technical components that comprise BEL systems, and discuss possible directions for the future of the field.}
}
@article{CHEN2024e29048,
title = {A hyper-knowledge graph system for research on AI ethics cases},
journal = {Heliyon},
volume = {10},
number = {7},
pages = {e29048},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e29048},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024050795},
author = {Chuan Chen and Yu Feng and Mengyi Wei and Zihan Liu and Peng Luo and Shengkai Wang and Liqiu Meng},
keywords = {AI ethics, Hyper-knowledge graph system, Case-oriented ontological model, Knowledge-based system, Visual analytics},
abstract = {Current studies on the artificial intelligence (AI) ethics focus either on very broad guidelines or on a very special domain. Therefore, the research outcome can hardly be converted into actionable measures or transferred to other domains. Potential correlations between various cases of AI ethics at different granularity levels are unexplored. To overcome these deficiencies, the authors designed a case-oriented ontological model (COOM) and a hyper-knowledge graph system (HKGS) for the research of collected AI ethics cases. COOM describes criteria for modelling cases by attributes from three perspectives: event attributes, relational attributes, and positional attributes on the value chain. Based on it, HKGS stores the correlation between cases as knowledge and allows advanced visual analysis. The correlations between cases and their dynamic changes on value chain can be observed and explored. In HKGS's implementation part, one of the collected ethics cases is used as an example to demonstrate how to generate a hyper-knowledge graph and to visually analyze it. The authors also anticipated how different practitioners of AI ethics, can achieve the desired outputs from HKGS in their diverse scenarios.}
}
@article{HAUCK201876,
title = {The origin of language among the Aché},
journal = {Language & Communication},
volume = {63},
pages = {76-88},
year = {2018},
note = {Language in the Amerindian Imagination},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2018.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0271530917301349},
author = {Jan David Hauck},
keywords = {Language ideology, Ontology, Objectification, Aché, Guaraní},
abstract = {For the Aché of Paraguay, language seems to never have been an issue, much less a topic. Aché verbal art emphasizes non-communicative and non-representational functions of speech. The activity of speaking was not related to ethnic or personal identity and there is no account of language in their mythology. This stands in stark contrast to their neighbors, the Guaraní, who have the concept of the word-soul, which relates to names and personhood, and a myth about the origin of language. In the twentieth century, contact with Paraguayan society, settlement in reservation communities, and the influence of missionaries led to profound sociocultural transformations among the Aché, including language shift. And as their speech practices were changing, these same practices were attended to in novel ways, as “language.” This paper analyzes the origin of language among the Aché and how it became similar to what it is in many other places of the world where people struggle for the maintenance and revitalization of their ancestral ways of speaking: a decontextualized cultural object and emblem of ethnic identity.}
}
@article{MCGLINN2022105313,
title = {FAIRVASC: A semantic web approach to rare disease registry integration},
journal = {Computers in Biology and Medicine},
volume = {145},
pages = {105313},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105313},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522001056},
author = {Kris McGlinn and Matthew A. Rutherford and Karl Gisslander and Lucy Hederman and Mark A. Little and Declan O'Sullivan},
keywords = {Knowledge engineering, Linked data, Ontologies, Federated queries, Rare diseases},
abstract = {Rare disease data is often fragmented within multiple heterogeneous siloed regional disease registries, each containing a small number of cases. These data are particularly sensitive, as low subject counts make the identification of patients more likely, meaning registries are not inclined to share subject level data outside their registries. At the same time access to multiple rare disease datasets is important as it will lead to new research opportunities and analysis over larger cohorts. To enable this, two major challenges must therefore be overcome. The first is to integrate data at a semantic level, so that it is possible to query over registries and return results which are comparable. The second is to enable queries which do not take subject level data from the registries. To meet the first challenge, this paper presents the FAIRVASC ontology to manage data related to the rare disease anti-neutrophil cytoplasmic antibody (ANCA) associated vasculitis (AAV), which is based on the harmonisation of terms in seven European data registries. It has been built upon a set of key clinical questions developed by a team of experts in vasculitis selected from the registry sites and makes use of several standard classifications, such as Systematized Nomenclature of Medicine - Clinical Terms (SNOMED-CT) and Orphacode. It also presents the method for adding semantic meaning to AAV data across the registries using the declarative Relational to Resource Description Framework Mapping Language (R2RML). To meet the second challenge a federated querying approach is presented for accessing aggregated and pseudonymized data, and which supports analysis of AAV data in a manner which protects patient privacy. For additional security the federated querying approach is augmented with a method for auditing queries (and the uplift process) using the provenance ontology (PROV-O) to track when queries and changes occur and by whom. The main contribution of this work is the successful application of semantic web technologies and federated queries to provide a novel infrastructure that can readily incorporate additional registries, thus providing access to harmonised data relating to unprecedented numbers of patients with rare disease, while also meeting data privacy and security concerns.}
}
@article{JIANG2023104728,
title = {Intelligent control of building fire protection system using digital twins and semantic web technologies},
journal = {Automation in Construction},
volume = {147},
pages = {104728},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104728},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522005982},
author = {Liu Jiang and Jianyong Shi and Chaoyu Wang and Zeyu Pan},
keywords = {Digital twins (DTs), Sematic web, Ontology, Building automated system, Fire protection system, Intelligent control, Building information modelling (BIM)},
abstract = {A fire protection system takes on a critical significance to building operation. This paper describes the use of digital twins (DTs) and semantic web technologies for the intelligent control of building fire protection (BFP) systems in fire accidents. Specifically, a data fusion stage and several information-based control mechanisms are involved in the use of the two technologies. A designed BFP ontology is considered as the semantic model and basis of data fusion between the static building geometric information and the dynamic sensing data. The above information is incorporated into a DT data model, which is considered as the mapping of physical space. Moreover, rule models and process models are developed to achieve intelligent control mechanisms and keep the DT data model synced with the physical space. A case study based on a fire accident simulation was conducted to verify the feasibility of the use of DTs and semantic web technologies.}
}
@article{NDINGAOKINA2023106665,
title = {Using Conceptual Graph modeling and inference to support the assessment and monitoring of bridge structural health},
journal = {Engineering Applications of Artificial Intelligence},
volume = {125},
pages = {106665},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106665},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623008497},
author = {Sylvain {Ndinga Okina} and Franck Taillandier and Louis Ahouet and Quynh Anh Hoang and Denys Breysse and Paul Louzolo-Kimbembe},
keywords = {Ontology of bridge deterioration, Predictive modeling of bridge, SHM (Structural health monitoring), Graphic inference, CGs (Conceptual Graphs), Decision support},
abstract = {Effective bridge maintenance requires sufficient and accurate knowledge on structural health. However, despite the development of structural health monitoring (SHM) and inspection aids, bridge structural health monitoring remains challenging. This work proposes a comprehensive predictive description based on Conceptual Graphs (CG), of the bridge condition deterioration mechanism, with the formalization of the inspection history. The model combines formalized​ assumptions with several deterioration factors and expert knowledge to assess the evolution of the structural condition. The model is based on logical and graphical descriptions of the deterioration of the structural condition thanks to the use of the CG modeling. Through the graphical inference, the evolution of the apparent condition and the deterioration time can be assessed. The application to two bridge cases highlights the interest of this approach to help understanding the sequence of deterioration. The actual condition obtained and the formalized inspection history allow a better decision.}
}
@article{LE2024108055,
title = {DeepPLM_mCNN: An approach for enhancing ion channel and ion transporter recognition by multi-window CNN based on features from pre-trained language models},
journal = {Computational Biology and Chemistry},
volume = {110},
pages = {108055},
year = {2024},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2024.108055},
url = {https://www.sciencedirect.com/science/article/pii/S1476927124000434},
author = {Van-The Le and Muhammad-Shahid Malik and Yi-Hsuan Tseng and Yu-Cheng Lee and Cheng-I Huang and Yu-Yen Ou},
keywords = {Convolutional neural networks, Multiple windows scanning, Deep learning, Pre-trained language model, Membrane proteins},
abstract = {Accurate classification of membrane proteins like ion channels and transporters is critical for elucidating cellular processes and drug development. We present DeepPLM_mCNN, a novel framework combining Pretrained Language Models (PLMs) and multi-window convolutional neural networks (mCNNs) for effective classification of membrane proteins into ion channels and ion transporters. Our approach extracts informative features from protein sequences by utilizing various PLMs, including TAPE, ProtT5_XL_U50, ESM-1b, ESM-2_480, and ESM-2_1280. These PLM-derived features are then input into a mCNN architecture to learn conserved motifs important for classification. When evaluated on ion transporters, our best performing model utilizing ProtT5 achieved 90% sensitivity, 95.8% specificity, and 95.4% overall accuracy. For ion channels, we obtained 88.3% sensitivity, 95.7% specificity, and 95.2% overall accuracy using ESM-1b features. Our proposed DeepPLM_mCNN framework demonstrates significant improvements over previous methods on unseen test data. This study illustrates the potential of combining PLMs and deep learning for accurate computational identification of membrane proteins from sequence data alone. Our findings have important implications for membrane protein research and drug development targeting ion channels and transporters. The data and source codes in this study are publicly available at the following link: https://github.com/s1129108/DeepPLM_mCNN.}
}
@article{GOUDAMOHAMED2024110416,
title = {Revolutionizing semantic integration of maintenance cost prediction for building systems using artificial neural networks},
journal = {Journal of Building Engineering},
volume = {96},
pages = {110416},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.110416},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224019843},
author = {Ahmed {Gouda Mohamed} and Joseph Ehab Ghaly and Mohamed Marzouk},
keywords = {Semantic web technology, Semantic ontology, Artificial neural network, Building services, Maintenace costs prediction},
abstract = {The maintenance cost management for building services has significantly advanced with the advent of computational techniques and semantic web technologies. Building services, including HVAC, plumbing, firefighting, and electrical systems, ensure building safety and efficiency. However, predicting maintenance costs is challenging due to the complexity and variability in usage patterns and environmental conditions. To address this challenge, this study introduces a Semantically Integrated Knowledge-based Artificial Neural Network (SIKB-ANN) framework. The SIKB-ANN framework integrates Semantic Web Technology (SWT) and Artificial Neural Networks (ANN) to improve the accuracy of maintenance cost predictions. SWT enhances data interoperability and standardization, while ANN manages complex non-linear data relationships. The model was trained with historical maintenance data from a firm (2017–2022) and validated using metrics such as Coefficient of Determination (R2), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE). The model achieved high predictive accuracy, with R2 values of 0.986 for the test set and 0.944 for the training set, indicating its robustness and reliability. An essential contribution of this research is the innovative integration of semantic ontologies with ANN, significantly enhancing predictive capabilities and providing a structured data management framework. This approach improves prediction accuracy and supports better decision-making and resource allocation in building management. The study highlights the potential of combining semantic technologies with machine learning to address complex predictive challenges in the built environment. Future research should consider integrating real-time data streams, advanced machine learning techniques, and broader applications in facility management.}
}
@article{LYNDA2023101700,
title = {Towards a semantic structure for classifying IoT agriculture sensor datasets : An approach based on machine learning and web semantic technologies},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {8},
pages = {101700},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101700},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823002549},
author = {Djakhdjakha Lynda and Farou Brahim and Seridi Hamid and Cissé Hamadoun},
keywords = {IoT agriculture dataset, Semantic web, Ontology, OWL, SWRL, Precision agriculture, IoT agriculture sensors application, Machine learning},
abstract = {With the increase in the number of IoT farming datasets, it has become so difficult to identify the right data for IoT agriculture applications. Therefore, a meaningful structure is needed to well understand, interpret and index IoT farming datasets. This paper proposes a new IoT farming ontology that allows the organization, the understanding, and the classification of IoT agriculture datasets knowledge as well as meta-data storage. For this, we have developed a new IoT agriculture taxonomy that helps to identify an IoT agriculture application based on the combination of various IoT agriculture sensors. The evaluation of the semantic IoT agriculture datasets classification, based on the background knowledge provided by the proposed ontology, was achieved using Machine Learning algorithms, including Logistic Regression, Decision Tree Classifier, K-Neighbors Classifier, Linear Discriminant Analysis, Gaussian NB, SVM, and Random Forest Regressor. The obtained results clearly show the effectiveness of the proposed ontology to classify IoT agriculture datasets with high performances and accuracy (0.98), (0.99) using Decision tree classifier and SVM respectively.}
}
@article{RUTA2025100335,
title = {Blockchain and Knowledge Representation for Service-oriented Smart Mobility Platforms},
journal = {Blockchain: Research and Applications},
pages = {100335},
year = {2025},
issn = {2096-7209},
doi = {https://doi.org/10.1016/j.bcra.2025.100335},
url = {https://www.sciencedirect.com/science/article/pii/S2096720925000624},
author = {Michele Ruta and Floriano Scioscia and Saverio Ieva and Giuseppe Loseto and Agnese Pinto and Arnaldo Tomasino},
keywords = {Blockchain, Knowledge Representation, Service Discovery, Internet of Things, Smart Mobility},
abstract = {The Smart Mobility vision calls for dynamic resource and service discovery to cope with the intrinsic topology volatility of Internet of Things (IoT) platforms without sacrificing the required business continuity and service flexibility. For an extended automation of collaboration within and across enterprise boundaries, trust management is equally important, granting security, reliability and scalability at the same time. To tackle the above challenges, this paper proposes the integration of a semantic-based service management layer in an IoT infrastructure grounded on the Hyperledger Sawtooth blockchain. Every service in the outlined framework is annotated with reference to a domain ontology, so that smart contracts can exploit knowledge representation and non-standard reasoning for service registration, discovery, outcomes explanation and service selection. A case study on power management of Plug-in Electric Vehicles (PEVs) is proposed to clarify the benefits of the proposal. Early performance evaluation results support the feasibility and sustainability of the approach.}
}
@article{XIAO2025103618,
title = {Generative knowledge-guided review system for construction disclosure documents},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103618},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103618},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005117},
author = {Hongru Xiao and Jiankun Zhuang and Bin Yang and Jiale Han and Yantao Yu and Songning Lai},
keywords = {Construction documents review, Large language model (LLM), Knowledge-guided retrieval, Natural Language Processing (NLP)},
abstract = {Construction disclosure documents are crucial for the safe and orderly execution of construction projects, making their effective review indispensable. However, accurately retrieving and applying regulatory compliance information from unstructured repositories of construction knowledge remains a significant challenge in this review process. To address this issue, this paper proposes a generative knowledge-guided review system that incorporates a Dynamic Semantic Knowledge Chunking (DSKC) strategy, designed to enhance the semantic association of knowledge within construction text knowledge bases. Building upon this foundation, a Generative Knowledge-Guided Retrieval (GKGR) framework is introduced to improve the accuracy of knowledge retrieval during the review process, thereby enhancing the overall reliability of document review. Experiments conducted on four newly established datasets and benchmarks demonstrate substantial improvements of 21.5 % in MRR@3 and 10.9 % in F1-Score compared to the baseline method, and further outperform state-of-the-art retrieval techniques.}
}
@article{WANG2024102664,
title = {A few-shot word-structure embedded model for bridge inspection reports learning},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102664},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102664},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003124},
author = {Yuchen Wang and Yanjie Zhu and Wen Xiong and C.S. Cai},
keywords = {Bridge maintenance, Deep learning, Information extraction, Integrated embedding, Pre-trained language model, Smart bridge},
abstract = {Intelligent bridge maintenance requires the comprehensive utilization of inspection records, as they contain valuable insights into structures’ long-term service conditions. To efficiently focus and utilize this data from extensive reports, reliable extraction methods are highly sought after. However, the heavy reliance on large amounts of manually annotated data limits the applicability and practicability of existing information extraction methods from bridge inspection reports, especially given the non-uniformity in report formats and expressions. To address this issue, this study proposed a few-shot information extraction model for bridge inspection reports understanding, comprising Word and Structure Integration Embeddings, Bi-directional Long Short-Term Memory (BiLSTM), and Condition Random Field (CRF). The model’s strength lies in its easy-to-implement word-structure embedding approach, which combines domain-specific word representations and sentence structure information. Specifically, the bridge inspection-domain pre-trained language model was further pre-trained and fine-tuned to obtain word embeddings, containing prior knowledge of the domain and tasks. Moreover, a novel encoding method was designed to generate sentence structure embeddings from dependency syntactic analysis results, providing textual representation information. Finally, the integrated word-structure embeddings, created by aligning dimensions for concatenation, were fed into the BiLSTM-CRF architecture to capture contextual dependencies and constrain extraction results. Empirical evaluations conducted on four few-shot datasets with 10, 30, 50, and 100 samples demonstrate that the proposed model achieved high accuracy and F1 score, outperforming prior methods, general domain models, and large language models. Specifically, in a dataset containing 50 sentences, our model achieved an accuracy of up to 0.9357 and an F1 score of 0.8683, representing an average increase of 38.4% higher than these methods. Ablation experiments revealed the contributions of each model component. These results suggest that the proposed model can accurately extract key information from bridge inspection reports even with limited training data scenarios, thereby facilitating applications such as structural condition evaluation and maintenance decision-making.}
}
@article{DIRKSON2023104228,
title = {How do others cope? Extracting coping strategies for adverse drug events from social media},
journal = {Journal of Biomedical Informatics},
volume = {139},
pages = {104228},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104228},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422002337},
author = {Anne Dirkson and Suzan Verberne and Gerard {van Oortmerssen} and Hans Gelderblom and Wessel Kraaij},
keywords = {Adverse drug reaction, Natural language processing, Coping strategies, Social media mining, GIST, Machine learning, Patient forum},
abstract = {Patients advise their peers on how to cope with their illness in daily life on online support groups. To date, no efforts have been made to automatically extract recommended coping strategies from online patient discussion groups. We introduce this new task, which poses a number of challenges including complex, long entities, a large long-tailed label space, and cross-document relations. We present an initial ontology for coping strategies as a starting point for future research on coping strategies, and the first end-to-end pipeline for extracting coping strategies for side effects. We also compared two possible computational solutions for this novel and highly challenging task; multi-label classification and named entity recognition (NER) with entity linking (EL). We evaluated our methods on the discussion forum from the Facebook group of the worldwide patient support organization ‘GIST support international’ (GSI); GIST support international donated the data to us. We found that coping strategy extraction is difficult and both methods attain limited performance (measured with F1 score) on held out test sets; multi-label classification outperforms NER+EL (F1=0.220 vs F1=0.155). An inspection of the multi-label classification output revealed that for some of the incorrect predictions, the reference label is close to the predicted label in the ontology (e.g. the predicted label ‘juice’ instead of the more specific reference label ‘grapefruit juice’). Performance increased to F1=0.498 when we evaluated at a coarser level of the ontology. We conclude that our pipeline can be used in a semi-automatic setting, in interaction with domain experts to discover coping strategies for side effects from a patient forum. For example, we found that patients recommend ginger tea for nausea and magnesium and potassium supplements for cramps. This information can be used as input for patient surveys or clinical studies.}
}
@incollection{KOSTOULAS2024,
title = {Complex Dynamic Systems Theory and Language Education},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00168-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032395504100168X},
author = {Achilleas Kostoulas and Juup Stelma},
keywords = {Applied linguistics, Complex dynamics systems theory, Complexity, Language education, Language teaching and learning},
abstract = {Originally deriving from the sciences, Complex Dynamic Systems Theory (CDST) has developed into a coherent theoretical frame to understand the structure, activity, and outcomes of language education. CDST encourages holistic descriptions of language education, including its interconnected structural, relational and intentional aspects. Language education as a Complex Dynamic System (CDS) is constituted by interacting physical and social entities, which are related in various ways, and with human agents determining what the system “is for.” Finally, CDST has been used in both restricted and generalized ways, and has emerged as a unique meta-theory, able to bridge between diverse disciplinary perspectives.}
}
@incollection{NIKIFOROVA202239,
title = {Chapter 3 - Evaluation and visualization of healthcare semantic models},
editor = {Sanju Tiwari and Fernando {Ortiz Rodriguez} and M.A. Jabbar},
booktitle = {Semantic Models in IoT and eHealth Applications},
publisher = {Academic Press},
pages = {39-68},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-91773-5},
doi = {https://doi.org/10.1016/B978-0-32-391773-5.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917735000091},
author = {Anastasija Nikiforova and Vita Rovite and Sanju Tiwari and Janis Klovins and Normunds Kante},
keywords = {Healthcare semantic model, Ontology visualization, Internet of medical things, Internet of things, Knowledge representation, Data management, Biomedicine},
abstract = {Today, the popularity of semantic models and ontologies is increasing rapidly. This leads not only to the high number of general ontologies, but also to a variety of domain-specific ontologies where the medical or healthcare domains play a major role. Their increasing popularity, particularly in different non-IT domains, has an impact on the need for support tools such as visualization. However, what are the benefits of ontology visualization? And how to choose not only the most suitable ontology, but also its visualization and evaluate its suitability for a given case? whether the “silver bullet” exists? and whether they are well suited to non-IT experts? Ontology itself is relatively complex concept that requires a special set of expertise to involve and maintain it. This chapter focuses on the current visualization techniques that make it easier to understand and process health-care data with a focus on the biomedical subdomain. The chapter should allow researchers and practitioners to understand the purposes and priorities of existing techniques by establishing support to choose the most appropriate depending on a use-case.}
}
@article{KRIJNEN201846,
title = {A SPARQL query engine for binary-formatted IFC building models},
journal = {Automation in Construction},
volume = {95},
pages = {46-63},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S092658051731049X},
author = {Thomas Krijnen and Jakob Beetz},
keywords = {BIM, IFC, Querying, SPARQL, Performance, HDF5},
abstract = {To date, widely implemented and full-featured query languages for building models in their native exchange formats do not exist. While interesting proposals exist for querying Industry Foundation Classes (IFC) models, their functionality is often incomplete and their semantics not precisely defined. With the introduction of the ifcOWL ontology as an equivalent to the IFC schema in the Web Ontology Language (OWL), an option to represent such models in RDF (Resource Description Framework, a general information modeling method) is provided, and such models can be queried using SPARQL (SPARQL Protocol and RDF Query Language). The size of data sets in complex building projects, however, renders the use of clear-text encoded RDF infeasible in many cases. A SPARQL implementation, compatible with ifcOWL, is proposed, directly atop a standardized binary serialization format for IFC building models. This novel format is the binary equivalent of traditional IFC serialization formats but with more compact storage and less overhead than the graph serialization in RDF. The format is based on ISO 10303-26 and relies on an open standard for organizing large amounts of data: Hierarchical Data Format version 5 (HDF5). Due to hierarchical partitioning and fixed-length records, only small subsets of the data are read to answer queries, improving efficiency. A prototypical implementation of the query engine is provided in the Python programming language. In several realistic use cases, the proposed system performs equivalent to or better than the state of the art in SPARQL querying on building models. For large datasets, the proposed storage format results in files that are 2–3 times smaller than the current, most concise, RDF databases while offering a platform-neutral, containerized exchange file.}
}
@article{DAS2024111981,
title = {Extracting goal models from natural language requirement specifications},
journal = {Journal of Systems and Software},
volume = {211},
pages = {111981},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.111981},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224000244},
author = {Souvick Das and Novarun Deb and Agostino Cortesi and Nabendu Chaki},
keywords = {Natural language requirements, Natural language processing, Transformer model, Entity type recognition, Contextual vector, Synonymy vector},
abstract = {Unstructured (or, semi-structured) natural language is mostly used to capture the requirement specifications both for legacy software systems and for modern day software systems. The adoption of a formal approach to the specification of the requirements, using goal models, enables rigorous and formal inspections while analyzing the requirements for satisfiability, consistency, completeness, conflicts and ambiguities. However, such a formal approach is often considered burdening for the analysts’ activity as it requires additional skills, and is therefore, discarded a priori. This works aims to bridge the gap between natural language requirement specifications and efficient goal model analysis techniques. We propose a framework that uses extensive natural language processing techniques to transform a set of unstructured natural language requirement specifications to the corresponding goal model. We combine techniques such as parts-of-speech tagging, dependency parsing, contextual and synonymy vector generation with the FiBER transformer model. An extensive unbiased crowd-sourced evaluation of the proposed framework has been performed, showing an acceptability rate (total and partial combined) of 95%. Time and space analyses of our framework also demonstrate the scalability of the proposed solution.}
}
@article{CHENG2019331,
title = {On the role of generating textual description for design intent communication in feature-based 3D collaborative design},
journal = {Advanced Engineering Informatics},
volume = {39},
pages = {331-346},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S1474034618305093},
author = {Yuan Cheng and Fazhi He and Xiao Lv and Weiwei Cai},
keywords = {Feature-based collaborative design, Design intent communication, Semantic network, Natural language generation},
abstract = {Modern manufacturing firms are more inclining to promote the product quality, save costs and reduce times of product design by both collaborative designing and model reuse. If CAD components constructed collaboratively have information representing their developers’ design intents embedded in the model, people’s understanding over the product should be improved and the product model should be best reused. Until now, capturing, recording and presenting design intents still remains a challenge. It has been shown by empirical studies that textual summarisations can lead to improved decision making. In this paper, we propose an approach to generation the natural language description about design intents of collaboratively developed product. The approach brings together techniques from different areas of collaborative designing, ontology and semantic network, and natural language generation. The language generation process is guided by an information model we established to give a structured description about design intents of collaboratively products. In order to record information related to the design intents, we build a common CAD model ontology and then generate a semantic network to describe dependencies, component structures and design history which are components of the design intent information model. The techniques of natural language generation, namely discourse planning and sentence planning, are adopted for the eventual linguistic generation of design intents. Finally, we use several case studies to prove the advantages of natural language in helping people better understanding the design intents.}
}
@article{BRINDHAMERIN2022103292,
title = {An efficient web service annotation for domain classification and information retrieval systems using HADLNN classifier},
journal = {Advances in Engineering Software},
volume = {174},
pages = {103292},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103292},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822001934},
author = {J {Brindha Merin} and W {Aisha Banu}},
keywords = {Web Service Description Language (WSDL), Hadoop Distributed File System (HDFS), Hybrid Artificial Deep Learning Neural Network (HADLNN), Cat swarm Optimization (CSO) and Modified K-means (MK-means)},
abstract = {Aimed at exchanging the dissimilar data between distributed applications, web services (WSs) annotations have progressed as a versatile and cost-effectual solution. Information retrieval (IR)assists in establishing the user-essential related information's searching impacts. However, rendering quick and efficient IR is a challenging problem. Also, the existent system yields slow accuracy, and as well the training time is high. Aimed at overcoming these problems, implemented an effective WS annotation aimed at domain classification and IR systems utilizing the Hybrid Artificial Deep Learning Neural Network (HADLNN). Firstly, the Semantic annotation (SA)stage is executed that comprises text preprocessing, repetitive data removal, feature extraction, and as well Ontology Construction. The text preprocessing offers the partitioning, stop word removal, and as well the stemming procedure aimed at the WSDL dataset. Next, continual WSs utilizing Hadoop Distributed File System (HDFS)is eliminated. After that, the CFC, confidence, support, and as well entropy attributes are taken out;next, the (Web Ontology Language) OWL files as of ontology construction are generated utilizing the protégé tool. After producing the OWL, the owl file is visualized utilizing Eclipse IDE and extracted the values utilizing the reasoner in the protege tool. After WS annotations, the domain is categorized centered on the connecting of WSs utilizing HADLNN. Lastly, the IR procedure is executed on MK-means that groups identical services as of the categorized domain. Preliminary outcomes exhibit that the system proposed offers efficient performance analogized to the existent techniques.}
}
@article{LUPI2023109497,
title = {Blockchain-based Shared Additive Manufacturing},
journal = {Computers & Industrial Engineering},
volume = {183},
pages = {109497},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109497},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223005211},
author = {Francesco Lupi and Mario G.C.A. Cimino and Tomaž Berlec and Federico A. Galatolo and Marko Corn and Nejc Rožman and Andrea Rossi and Michele Lanzetta},
keywords = {Distributed Manufacturing, Additive Manufacturing, Shared Manufacturing, Resilience, Smart Contract, Resource Sharing},
abstract = {Today, globalized markets require more resilient and agile manufacturing systems, as well as customized and virtualized features. Classical self-standing manufacturing systems are evolving into collaborative networks such as Cloud Manufacturing (based on centralized knowledge and distributed resources) or Shared Manufacturing (based on fully decentralized knowledge and distributed resources) as a solution to ensure business continuity under normal as well as special circumstances. Additive Manufacturing (AM), one of the enablers of Industry 4.0 (I4.0), is a promising technology for innovative production models due to its inherent distributed capabilities, digital nature, and product customization ability. To increase the adaptivity of distributed resources using AM technology, this paper proposes a mechanism for sharing workload and resources under unexpected behaviours in the supply chain. Smart contracts and blockchain technology in this concept are used to provide decentralized, transparent, and trusted operation of such systems, which provide more resilience to disruptive factors. In this paper, the proposed Blockchain-based Shared Additive Manufacturing (BBSAM) protocol, ontology, and workflow for AM capacity pooling are discussed and analysed under special conditions such as anomalous demand. Discrete-time Python simulation on a real Italian AM market dataset, also provided, is available on GitHub.}
}
@article{LI2025104336,
title = {SAMAC-R3-MED: Semantic alignment and multi-agent collaboration of retriever-reranker-responder models for multimodal engineering documents},
journal = {Computers in Industry},
volume = {171},
pages = {104336},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2025.104336},
url = {https://www.sciencedirect.com/science/article/pii/S0166361525001010},
author = {Fei Li and Xinyu Li and Sijie Wen and Haoyang Huang and Jinsong Bao},
keywords = {Multimodal engineering documents, Multimodal semantic alignment, Retriever-Reranker-Responder, Multi-agent collaboration, Retrieval-augmented generation},
abstract = {In the manufacturing industry's lifecycle, a vast amount of engineering documents in text, table, and image formats is generated. Retrieval-augmented generation (RAG) models can enhance retrieval efficiency and adapt to evolving document knowledge. However, challenges in understanding multimodal semantic associations and the absence of engineering-semantic-aligned RAG models result in suboptimal accuracy. This paper introduces a novel approach, namely SAMAC-R3-MED, to tackle these challenges. First, a fine-grained context enhancement strategy is applied to multimodal large language models (MLLMs), bridging multimodal semantic understanding by constructing multi-modal semantic trees (MMST) and multi-modal knowledge graphs (MMKG), forming a hybrid retrieval base. Second, to bridge the semantic gap in RAG models, a new training framework, retriever-reranker-responder (R3), is proposed, utilizing supervised and reinforcement learning with ranking feedback to enhance alignment. Third, a multi-channel hybrid retrieval strategy is implemented for the multi-agent collaboration R3 models, integrating expert feedback, semantic trees, and graphs to optimize the RAG pipeline and improve the accuracy of retrieving multimodal associative semantic contexts. An engineering documents chat (eDoChat) system is implemented, in the case of wind turbine assembly, validating the effectiveness in retrieving and generating accurate multimodal answers. Ablation experiments show R3 models outperform traditional RAG models, and SAMAC-R3-MED achieves state-of-the-art results in multimodal retrieval and generation tasks.}
}
@article{LENZERINI2021103432,
title = {Metamodeling and metaquerying in OWL2QL},
journal = {Artificial Intelligence},
volume = {292},
pages = {103432},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2020.103432},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218301875},
author = {Maurizio Lenzerini and Lorenzo Lepore and Antonella Poggi},
keywords = {Metamodeling, Metaquerying, },
abstract = {OWL2QL is a standard profile of the OWL2 ontology language, specifically tailored to Ontology-Based Data Management. Inspired by recent work on higher-order Description Logics, in this paper we present a new semantics for OWL2QL ontologies, called Metamodeling Semantics (MS), and show that, in contrast to the official Direct Semantics (DS) for OWL2, it allows exploiting the metamodeling capabilities natively offered by the OWL2 punning. We then extend unions of conjunctive queries with both metavariables, and the possibility of using TBox atoms, with the purpose of expressing meaningful metalevel queries. We first show that under MS both satisfiability checking and answering queries including only ABox atoms, have the same complexity as under DS. Second, we investigate the problem of answering general metaqueries, and single out a new source of complexity coming from the combined presence of a specific type of incompleteness in the ontology, and of TBox axioms among the query atoms. Then we focus on a specific class of ontologies, called TBox-complete, where there is no incompleteness in the TBox axioms, and show that general metaquery answering in this case has again the same complexity as under DS. Finally, we move to general ontologies and show that answering general metaqueries is coNP-complete with respect to ontology complexity, Π2p-complete with respect to combined complexity, and remains AC0 with respect to ABox complexity.}
}
@article{LIU2025112941,
title = {Vision–language representation learning with breadth and depth attention pre-training},
journal = {Knowledge-Based Systems},
volume = {310},
pages = {112941},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112941},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124015752},
author = {Yun Liu and Bo Zhang and ChenCheng Wang and Genglong Yan and Ke Zhou and Zhoujun Li and LeiLei Zhang},
keywords = {Representation learning, Attention, Pre-training},
abstract = {The rapid advances in computer vision and natural language processing have led to increased attention toward the challenge of understanding vision and language together across multiple domains. Representation learning has become a major focus of research on cross-modal information understanding. However, current methods often fall short of providing comprehensive interaction and meaningful supervised guidance that would allow for effective learning of visual-linguistic joint representation. In this paper, we introduce the Breadth and Depth Attention Pre-training (BDAP) model for vision–language representation learning. Our model includes a breadth attention network designed to model feature associations between text sentences and image regions across different image levels. It uses fine-grained image features to promote more effective cross-modal feature interactions. Additionally, a depth attention network, which repeatedly calculates attention scores, is designed to deeply capture the complementarity between the image and text by gradually refining important image regions related to the text. Furthermore, we propose an attention pre-training network that leverages attention annotated distribution maps as prior knowledge to supervise the learning process of the breadth and depth attention networks, thereby enabling weight initialization of both types of attention networks. Extensive experiments on datasets of visual question answering and multi-modal sentiment analysis demonstrate the promising superiority of our BDAP model for vision–language representation learning.}
}
@article{WALTERS2022119610,
title = {Predicting brain activation maps for arbitrary tasks with cognitive encoding models},
journal = {NeuroImage},
volume = {263},
pages = {119610},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119610},
url = {https://www.sciencedirect.com/science/article/pii/S105381192200725X},
author = {Jonathon Walters and Maedbh King and Patrick G. Bissett and Richard B. Ivry and Jörn Diedrichsen and Russell A. Poldrack},
keywords = {Encoding models, Computational modeling, Cognition, Ontologies, functional MRI},
abstract = {ABSTRACT
A deep understanding of the neural architecture of mental function should enable the accurate prediction of a specific pattern of brain activity for any psychological task, based only on the cognitive functions known to be engaged by that task. Encoding models (EMs), which predict neural responses from known features (e.g., stimulus properties), have succeeded in circumscribed domains (e.g., visual neuroscience), but implementing domain-general EMs that predict brain-wide activity for arbitrary tasks has been limited mainly by availability of datasets that 1) sufficiently span a large space of psychological functions, and 2) are sufficiently annotated with such functions to allow robust EM specification. We examine the use of EMs based on a formal specification of psychological function, to predict cortical activation patterns across a broad range of tasks. We utilized the Multi-Domain Task Battery, a dataset in which 24 subjects completed 32 ten-minute fMRI scans, switching tasks every 35 s and engaging in 44 total conditions of diverse psychological manipulations. Conditions were annotated by a group of experts using the Cognitive Atlas ontology to identify putatively engaged functions, and region-wise cognitive EMs (CEMs) were fit, for individual subjects, on neocortical responses. We found that CEMs predicted cortical activation maps of held-out tasks with high accuracy, outperforming a permutation-based null model while approaching the noise ceiling of the data, without being driven solely by either cognitive or perceptual-motor features. Hierarchical clustering on the similarity structure of CEM generalization errors revealed relationships amongst psychological functions. Spatial distributions of feature importances systematically overlapped with large-scale resting-state functional networks (RSNs), supporting the hypothesis of functional specialization within RSNs while grounding their function in an interpretable data-driven manner. Our implementation and validation of CEMs provides a proof of principle for the utility of formal ontologies in cognitive neuroscience and motivates the use of CEMs in the further testing of cognitive theories.}
}
@article{BIBI2025105979,
title = {Enhancing code search through query expansion: A fusion of LSTM with GloVe and BERT model (ECSQE)},
journal = {Results in Engineering},
volume = {27},
pages = {105979},
year = {2025},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2025.105979},
url = {https://www.sciencedirect.com/science/article/pii/S2590123025020511},
author = {Nazia Bibi and Muhammad Usman Tariq and Zabeeh Ullah and Muhammad Babar and Zahid Khan},
keywords = {Codesearchnet, Natural language, Deep neural networks, Semantic matching, LSTM, GloVe, BERT, Source code selection, Source code reuse, Recommendation system},
abstract = {In software engineering efficient code retrieval is essential for developers to quickly access relevant code snippets from vast repositories. This study focuses on enhancing code search through query expansion, leveraging advanced word embedding techniques such as GloVe and BERT to improve search accuracy and relevance. The main objective is to assess how the proposed model can outperform traditional models in terms of evaluation metrics by expanding query terms based on contextual understanding. The proposed model was evaluated using three query datasets to enable a comprehensive performance comparison with baseline models, including UNIF, CNN-CS and DeepCS. The results demonstrate that the proposed model significantly enhances code search effectiveness by generating more contextually relevant queries, which in turn leads to improved retrieval outcomes. Nonetheless, challenges such as language dependency and data diversity, were identified. The study concludes by emphasizing the potential of the proposed model to transform code search while highlighting areas for future work including expanding the model's applicability across more languages and addressing scalability issues for larger datasets.}
}
@article{ATKINSON2021101440,
title = {A deep view-point language and framework for projective modeling},
journal = {Information Systems},
volume = {101},
pages = {101440},
year = {2021},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2019.101440},
url = {https://www.sciencedirect.com/science/article/pii/S0306437919304922},
author = {Colin Atkinson and Christian Tunjic},
keywords = {View-based modeling, Enterprise architecture modeling, Multi-level modeling, Orthographic software modeling},
abstract = {Most view-based modeling approaches are today based on a “synthetic” approach in which the views hold all the information modeled about a system and are kept consistent using explicit, inter-view correspondence rules. The alternative “projective” approach, in which the contents of views are “projected” from a single underlying model on demand, is far less widely used due to the lack of suitable conceptual frameworks and languages. In this paper we take a step towards addressing this problem by presenting the foundations of a suitable language and conceptual framework for defining and applying views for projective modeling. The framework leverages deep modeling in order to seamlessly support views that exist at, and span, multiple levels of classification. The viewpoint language was developed in the context of Orthographic Software Modeling but is more generally applicable to any projective modeling approach.}
}
@incollection{FANEGOPALAT2025,
title = {Space and Its Cultural and Cognitive Imprint in Language},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00623-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041006232},
author = {Axel {Fanego Palat} and Angelika Mietzner},
keywords = {Spatial cognition, Spatial frames of reference, Spatial deixis, Cardinal directions, Cultural conceptualization, Deictic expressions, Itive and ventive, Path and manner of motion, Motion verbs},
abstract = {Space is a fundamental concept in anthropological, cognitive, and descriptive linguistics. Spatial understanding and communication can vary widely across linguistic communities, reflecting the diverse ways humans perceive, interpret, and express spatial dimensions. This article overviews existing research on language, culture, and spatial cognition. The examples focus on African languages, as scholars studying them have made significant theoretical contributions to the field.}
}
@article{SPOLADORE2024109001,
title = {A knowledge-based decision support system to support family doctors in personalizing type-2 diabetes mellitus medical nutrition therapy},
journal = {Computers in Biology and Medicine},
volume = {180},
pages = {109001},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109001},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524010862},
author = {Daniele Spoladore and Francesco Stella and Martina Tosi and Erna Cecilia Lorenzini and Claudio Bettini},
keywords = {Decision support system, Ontology-based system, Clinical decision support, Type-2 diabetes mellitus, Medical nutrition therapy},
abstract = {Background
Type-2 Diabetes Mellitus (T2D) is a growing concern worldwide, and family doctors are called to help diabetic patients manage this chronic disease, also with Medical Nutrition Therapy (MNT). However, MNT for Diabetes is usually standardized, while it would be much more effective if tailored to the patient. There is a gap in patient-tailored MNT which, if addressed, could support family doctors in delivering effective recommendations. In this context, decision support systems (DSSs) are valuable tools for physicians to support MNT for T2D patients – as long as DSSs are transparent to humans in their decision-making process. Indeed, the lack of transparency in data-driven DSS might hinder their adoption in clinical practice, thus leaving family physicians to adopt general nutrition guidelines provided by the national healthcare systems.
Method
This work presents a prototypical ontology-based clinical Decision Support System (OnT2D- DSS) aimed at assisting general practice doctors in managing T2D patients, specifically in creating a tailored dietary plan, leveraging clinical expert knowledge. OnT2D-DSS exploits clinical expert knowledge formalized as a domain ontology to identify a patient's phenotype and potential comorbidities, providing personalized MNT recommendations for macro- and micro-nutrient intake. The system can be accessed via a prototypical interface.
Results
Two preliminary experiments are conducted to assess both the quality and correctness of the inferences provided by the system and the usability and acceptance of the OnT2D-DSS (conducted with nutrition experts and family doctors, respectively).
Conclusions
Overall, the system is deemed accurate by the nutrition experts and valuable by the family doctors, with minor suggestions for future improvements collected during the experiments.}
}
@article{LIAO2025113676,
title = {A knowledge-data driven method for in-situ quality prediction in laser directed energy deposition using DfKDED and YOLOv7-tinyR},
journal = {Optics & Laser Technology},
volume = {192},
pages = {113676},
year = {2025},
issn = {0030-3992},
doi = {https://doi.org/10.1016/j.optlastec.2025.113676},
url = {https://www.sciencedirect.com/science/article/pii/S0030399225012678},
author = {Zhengjingxuan Liao and Yonggang Yu and Shitong Peng and Jianan Guo and Mengyue He and Bowen Chen and Zixin Liu and Weiwei Liu and Hongchao Zhang and Fengtao Wang},
keywords = {Laser directed energy deposition, Knowledge graph, TB6 titanium alloy, Melt pool},
abstract = {The Laser Directed Energy Deposition (L-DED) process involves multiple interacting factors, complicating quality assurance. Existing data-driven online monitoring methods often lack interpretability, hindering real-time melt pool quality assessment and impeding process optimization. To address these issues, we developed an ontology-based L-DED knowledge graph, Design for Knowledge graph Directed Energy Deposition (DfKDED), using TB6 titanium alloy. DfKDED integrates key entities, including geometric characters, effects, defects, process parameters, and product quality, to clarify their interrelationships and support structured classification. Furthermore, we proposed a YOLOv7-tinyR-based method for in-situ quality prediction and melt pool tracking, which collects and processes melt pool image data from an off-axis camera. This approach achieves 99.70 % accuracy and 99.59 % mean average precision with a 2.1 ms inference time, enabling faster detection speeds and reduced computational resource consumption. This study provides an innovative framework for L-DED data administration and retrieval, enhancing real-time monitoring and efficient process control.}
}
@article{HASSEN202496,
title = {Graphical Specification of Sensitive Business Processes},
journal = {Procedia Computer Science},
volume = {237},
pages = {96-106},
year = {2024},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.05.084},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924011001},
author = {Mariam Ben Hassen and Faïez Gargouri},
keywords = {Business Process modeling, Knowledge Management, Sensitive Business Process, Core Domain Ontologies, BPMN 2.0.2, Extension Mechanism, Design Science Research Methodology},
abstract = {This paper aims at developing a new BPMN extension, called “BPMN4SBP”, supporting the multi-dimensional modeling of Sensitive Business Processes (SBPs) (i.e., the knowledge, functional, organizational, behavioral, informational and intentional dimensions). It aims to explicitly integrate all the relevant issues and aspects relevant at the coupling of the business process modeling (BPM) domain and the knowledge management (KM) domain for improving the identification and management of crucial knowledge which are mobilized and created by these processes. This paper provides the analysis of requirements and relevant concepts for modeling SBP. Based on a core domain ontology, need for extension is identified and the valid BPMN4SBP extension is designed (according to the BPMN extension mechanism) by the construction of a conceptional domain model and the corresponding BPMN extension model. This extended BPMN4SBP meta-model is derived by applying model transformation rules and by adapting, also, the UML profile mechanism to BPMN. The applicability of the proposed BPMN extension is demonstrated by modeling an SBP in a real case study in the healthcare domain.}
}
@article{SONG2024118882,
title = {Integrating situation-aware knowledge maps and dynamic window approach for safe path planning by maritime autonomous surface ships},
journal = {Ocean Engineering},
volume = {311},
pages = {118882},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.118882},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824022200},
author = {Rongxin Song and Eleonora Papadimitriou and Rudy R. Negenborn and Pieter van Gelder},
keywords = {Decision-making, Knowledge map, Situational awareness, Maritime autonomous surface ships, Dynamic window approach, Collision avoidance},
abstract = {This study investigates the enhancement of Maritime Autonomous Surface Ships (MASS) navigation and path-planning through the integration of ontology-based knowledge maps (KM) with the Dynamic Window Approach (DWA), a fusion termed KM-DWA. The ontology-based KM model is important for MASS navigation, offering a framework for situational awareness, including contextual information fusion and decision-making evidence. This research enriches the KM model with collision avoidance rules from the International Regulations for Preventing Collisions at Sea (COLREGs), building upon our previous work on MASS's efficient and COLREGs-compliant navigation in encounter scenarios. The model provides navigational context, covers COLREGs rules and environmental factors, and recommends MASS actions for various scenarios as suggested by COLREGs. Moreover, an adapted DWA, tailored to maritime navigation, accounts for specific constraints and safety measures for MASS, utilising KM-derived situational awareness as constraints in its cost function for path planning. A significant innovation introduced here is a tiered safety distance model featuring proactive, defensive, and collision buffers to ensure rule-compliant and effective collision avoidance. This scheme enables MASS to take timely collision avoidance actions at both proactive and defensive distances, in line with COLREGs recommendations. The effectiveness of the KM-DWA algorithm is validated by comparing it with the basic DWA algorithm in single- and multi-vessel encounter scenarios. The experiment outcomes illustrate the integrated approach's superiority in terms of COLREGs compliance and collision avoidance rate, emphasising its ability to support COLREGs-compliant decision-making and enhance situational awareness in autonomous maritime operations.}
}
@incollection{VERDICCHIO2025,
title = {Language of Artificial Intelligence Discourses},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00392-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041003926},
author = {Mario Verdicchio},
keywords = {Artificial intelligence, Figures of speech, Machine learning, Philosophy of mind},
abstract = {Language has played a fundamental role in Artificial Intelligence discourses from the very beginning of the establishment of the field. An early assumption was that every aspect of intelligence could be described in a manner compatible with machine operation. This assumption is critical for comparing humans and machines, given that, on the one hand, our understanding of how human brains work is insufficient to define intelligence clearly, and on the other hand, a focus on computational artifacts may lead to a limited conceptualization that overlooks significant aspects of what it means to be conscious and conscientious humans. A mindful analysis of the metaphors used to describe AI systems is key to navigating the intricate entanglements between society and technology that contribute to this endeavor.}
}
@article{MURUKUTLA2023102717,
title = {Text-to-movie authoring of anatomy lessons},
journal = {Artificial Intelligence in Medicine},
volume = {146},
pages = {102717},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102717},
url = {https://www.sciencedirect.com/science/article/pii/S0933365723002312},
author = {Vaishnavi Ameya Murukutla and Elie Cattan and Benjamin Lecouteux and Remi Ronfard and Olivier Palombi},
keywords = {Anatomy pedagogy, Domain specific languages, Finite state machines, Computer animation},
abstract = {There is a need for a simple yet comprehensive tool to produce and edit pedagogical anatomy video courses, given the widespread usage of multimedia and 3D content in anatomy instruction. Anatomy teachers have minimal control over the present anatomical content generation pipeline. In this research, we provide an authoring tool for instructors that takes text written in the Anatomy Storyboard Language (ASL), a novel domain-specific language (DSL) and produces an animated video. ASL is a formal language that allows users to describe video shots as individual sentences while referencing anatomic structures from a large-scale ontology linked to 3D models. We describe an authoring tool that translates anatomy lessons written in ASL to finite state machines, which are then used to automatically generate 3D animation with the Unity 3D game engine. The proposed text-to-movie authoring tool was evaluated by four anatomy professors to create short lessons on the knee. Preliminary results demonstrate the ease of use and effectiveness of the tool for quickly drafting narrated video lessons in realistic medical anatomy teaching scenarios.}
}
@article{OLIVEIRA2022107818,
title = {Extracting data models from background knowledge graphs},
journal = {Knowledge-Based Systems},
volume = {237},
pages = {107818},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107818},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121010145},
author = {Daniela Oliveira and Mathieu d’Aquin},
keywords = {Knowledge graphs, Ontologies, Data modelling},
abstract = {Knowledge Graphs have emerged as a core technology to aggregate and publish knowledge on the Web. However, integrating knowledge from different sources, not specifically designed to be interoperable, is not a trivial task. Finding the right ontologies to model a dataset is a challenge since several valid data models exist and there is no clear agreement between them. In this paper, we propose to facilitate the selection of a data model with the RICDaM (Recommending Interoperable and Consistent Data Models) framework. RICDaM generates and ranks candidates that match entity types and properties in an input dataset. These candidates are obtained by aggregating freely available domain RDF datasets in a knowledge graph and then enriching the relationships between the graph’s entities. The entity type and object property candidates are obtained by exploiting the instances and structure of this knowledge graph to compute a score that considers both the accuracy and interoperability of the candidates. Datatype properties are predicted with a random forest model, trained on the knowledge graph properties and their values, so to make predictions on candidate properties and rank them according to different measures. We present experiments using multiple datasets from the library domain as a use case and show that our methodology can produce meaningful candidate data models, adaptable to specific scenarios and needs.}
}
@article{MA2024102329,
title = {Multicriteria requirement ranking based on uncertain knowledge representation and reasoning},
journal = {Advanced Engineering Informatics},
volume = {59},
pages = {102329},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102329},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623004573},
author = {Yufeng Ma and Yajie Dou and Xiangqian Xu and Jiang Jiang and Kewei Yang and Yuejin Tan},
keywords = {Fuzzy requirement knowledge graph, Predicate fuzziness, Attribute confidence, Grey relational analysis, Multicriteria ranking},
abstract = {The abundance of complex natural language descriptions related to requirements data presents major challenges for requirement analysis. Knowledge graphs (KGs), as the latest achievement in symbolic studies, are widely used in various fields due to their rich semantic expressive abilities. However, existing methods that mine resource description framework (RDF) triples in classical KGs cannot effectively capture the fuzzy semantic information in the product development system. To address this, we propose a multicriteria requirement ranking method based on uncertain knowledge representation and reasoning (KRR). First, we defined a model for representing uncertain knowledge to organize diverse data from multiple sources. This process was accomplished by constructing the requirement ontology, which is based on the function-behaviour-structure (FBS) model and requirements modelling-related documents. Next, a knowledge representation approach named the fuzzy requirement knowledge graph (FRKG) was devised by combining attribute confidence and predicate fuzziness. Then, knowledge reasoning rules were designed to enhance the edges in the FRKG, unveiling potential relationships between nodes. Utilizing the enriched FRKG (EFRKG), we proposed a multicriteria requirement ranking method based on grey relational analysis (GRA). To validate the effectiveness of the proposed approach, we conducted a case study involving unmanned aerial vehicles (UAVs). Furthermore, the semantic extension capability of FRKG was evaluated, and a comparison with traditional multicriteria requirement ranking methods was performed to demonstrate the efficiency of the proposed approach from both perspectives.}
}
@article{ZHOU2022100032,
title = {Question answering system for chemistry—A semantic agent extension},
journal = {Digital Chemical Engineering},
volume = {3},
pages = {100032},
year = {2022},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2022.100032},
url = {https://www.sciencedirect.com/science/article/pii/S2772508122000230},
author = {Xiaochi Zhou and Daniel Nurkowski and Angiras Menon and Jethro Akroyd and Sebastian Mosbach and Markus Kraft},
keywords = {Question answering, Semantic agent, Knowledge graph},
abstract = {This paper introduces an extension of a previously developed question answering (QA) system for chemistry, operating on a knowledge graph (KG) called Marie. This extension enables the automatic invocation of semantic agents to answer questions when static data is absent from the KG. The agents are semantically described using the agent ontology, OntoAgent, to enable automated agent discovery and invocation. The natural language processing (NLP) models of the QA system need to be trained in order to interpret questions to be answered by new agents. For this purpose, we extend OntoAgent so that it becomes possible to automatically create training material for the NLP models. We evaluate the extended QA system with two example chemistry-related agents and an evaluation question set. The evaluation result shows that the extension allows the QA system to discover the suitable agent and to invoke the agent by automatically constructing requests from the semantic agent description, thereby increasing the range of questions the QA system can answer.}
}
@article{BAEK2021103915,
title = {A critical review of text-based research in construction: Data source, analysis method, and implications},
journal = {Automation in Construction},
volume = {132},
pages = {103915},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103915},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521003666},
author = {Seungwon Baek and Wooyong Jung and Seung H. Han},
keywords = {Construction, Review, Text-based research, Natural language processing, Text mining, Unstructured text data, Data source, Text analysis method},
abstract = {The advancement of natural language processing and text mining techniques facilitate automatic non-trivial pattern extraction and knowledge discovery from text data. However, text-based research has received less attention compared to image- and sensor-based research in the construction industry. Hence, this paper performs a comprehensive review to understand the current state and future insights of text analytics focusing on the data source and analysis method. This study identifies various kinds of text data sources from project documents as well as open data in the websites. In addition, the review finds that the ontology- and rule-based approach has been dominant, at the same time, recent research has attempted to apply the state-of-the-art machine learning methods. It is envisioned that there are potential advancements in construction engineering and management based on the latest text analysis methods along with the enriched data by the digital transformation.}
}
@article{WANG201817,
title = {Change propagation analysis for system modeling using Semantic Web technology},
journal = {Advanced Engineering Informatics},
volume = {35},
pages = {17-29},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2017.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617301970},
author = {Haoqi Wang and Vincent Thomson and Chengtong Tang},
keywords = {Model-based systems engineering, System modeling language, Engineering change management, Change propagation analysis, Ontology, Semantic Web technology},
abstract = {Change propagation potentially affects many aspects of a SysML-based system model during the iterative process of Model-Based Systems Engineering (MBSE). However, few authors have addressed the implication of engineering change and its impact. To address having a successful change process, this article analyzes and explicitly represents different scenarios of how a system model is changed from a formal perspective, i.e., how a system model should be changed, and how model elements should be added, deleted or modified in response to design changes. A workflow is introduced to guide the change process taking change propagation into account. Second, change impact relationships among requirements, behaviors, and structures of the system model are formalized by an ontology to make the semantics both human-understandable and machine-readable. Reasoning rules are defined as well in order to improve automation of the change process. Finally, an experiment using a water distiller system showed that the identification of change impact information could help designers complete the change in less time and with higher quality.}
}
@article{IGAMBERDIEV2024105346,
title = {Reflexive neural circuits and the origin of language and music codes},
journal = {BioSystems},
volume = {246},
pages = {105346},
year = {2024},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2024.105346},
url = {https://www.sciencedirect.com/science/article/pii/S0303264724002314},
author = {Abir U. Igamberdiev},
keywords = {Aristotle, Language, Consciousness, Musical code, Reflexive psychology, Self-awareness},
abstract = {Conscious activity is grounded in the reflexive self-awareness in sense perception, through which the codes signifying sensual perceptive events operate and constrain human behavior. These codes grow via the creative generation of hypertextual statements. We apply the model of Vladimir Lefebvre (Lefebvre, V.A., 1987, J. Soc. Biol. Struct. 10, 129–175) to reveal the underlying structures on which the perception and creative development of language and music codes are based. According to this model, the reflexive structure of conscious subject is grounded in three thermodynamic cycles united by the control of the basic functional cycle by the second one, and resulting in the internal action that it turn is perceived by the third cycle evaluating this action. In this arrangement, the generative language structures are formed and the frequencies of sounds that form musical phrases and patterns are selected. We discuss the participation of certain neural brain structures and the establishment of reflexive neural circuits in the ad hoc transformation of perceptive signals, and show the similarities between the processes of perception and of biological self-maintenance and morphogenesis. We trace the peculiarities of the temporal encoding of emotions in music and musical creativity, as well as the principles of sharing musical information between the performing and the perceiving individuals.}
}
@article{INAN2025102692,
title = {Making hierarchically aware decisions on short findings for automatic summarisation},
journal = {Journal of Computational Science},
volume = {91},
pages = {102692},
year = {2025},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2025.102692},
url = {https://www.sciencedirect.com/science/article/pii/S1877750325001693},
author = {Emrah Inan},
keywords = {Radiology summarisation, Hierarchical text classification, Prompt engineering},
abstract = {An impression in a typical radiology report emphasises critical information by providing a conclusion and reasoning based on the findings. However, the findings and impression sections of these reports generally contain brief texts, as they highlight crucial observations derived from the clinical radiograph. In this scenario, abstractive summarisation models often experience a degradation in performance when generating short impressions. To address this challenge in the summarisation task, our work proposes a method that combines well-known fine-tuned text classification and abstractive summarisation language models. Since fine-tuning a language model requires an extensive, well-defined training dataset and is a time-consuming task dependent on high GPU resources, we employ prompt engineering, which uses prompt templates to programme language models and improve their performance. Our method first predicts whether the given findings text is normal or abnormal by leveraging a fine-tuned language model. Then, we apply a radiology-specific BART model to generate the summary for abnormal findings. In the zero-shot setting, our method achieves remarkable results compared to existing approaches on a real-world dataset. In particular, our method achieves scores of 37.43 for ROUGE-1, 21.72 for ROUGE-2, and 35.52 for ROUGE-L.}
}
@article{POSTANOGOV2019511,
title = {Towards Automating the Creation of OBDA Systems},
journal = {Procedia Computer Science},
volume = {150},
pages = {511-517},
year = {2019},
note = {Proceedings of the 13th International Symposium “Intelligent Systems 2018” (INTELS’18), 22-24 October, 2018, St. Petersburg, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.02.086},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919304338},
author = {I.S. Postanogov},
keywords = {ontology-based data access, ontology enrichment, reusable ontologies, sparql, query rewriting},
abstract = {In the paper, we consider automating the process of constructing and debugging ontology-based data access systems by using the information stored in external ontology resources. In particular, for the constructing stage, we propose to use a tool that suggests concepts from external repositories for those tables from a relational database, which haven’t been matched with concepts from a domain ontology by ontology alignment techniques. For the debugging stage, we show how a tool for navigation in a collection of ontologies can be used to simplify SPARQL query writing by generating lacking parts of query thus enhancing ontology engineer’s productivity. The proposed approaches have been implemented and used in our system, named Reply, which transforms traditional information systems into intelligent systems with a natural language query interface.}
}
@article{ZHANG20238,
title = {Does protein pretrained language model facilitate the prediction of protein–ligand interaction?},
journal = {Methods},
volume = {219},
pages = {8-15},
year = {2023},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2023.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S1046202323001469},
author = {Weihong Zhang and Fan Hu and Wang Li and Peng Yin},
keywords = {Protein–ligand interaction, Protein pretrained language model, Transfer learning, Transferability},
abstract = {Protein-ligand interaction (PLI) is a critical step for drug discovery. Recently, protein pretrained language models (PLMs) have showcased exceptional performance across a wide range of protein-related tasks. However, a significant heterogeneity exists between the PLM and PLI tasks, leading to a degree of uncertainty. In this study, we propose a method that quantitatively assesses the significance of protein PLMs in PLI prediction. Specifically, we analyze the performance of three widely-used protein PLMs (TAPE, ESM-1b, and ProtTrans) on three PLI tasks (PDBbind, Kinase, and DUD-E). The model with pre-training consistently achieves improved performance and decreased time cost, demonstrating that enhance both the accuracy and efficiency of PLI prediction. By quantitatively assessing the transferability, the optimal PLM for each PLI task is identified without the need for costly transfer experiments. Additionally, we examine the contributions of PLMs on the distribution of feature space, highlighting the improved discriminability after pre-training. Our findings provide insights into the mechanisms underlying PLMs in PLI prediction and pave the way for the design of more interpretable and accurate PLMs in the future. Code and data are freely available at https://github.com/brian-zZZ/PLM-PLI.}
}
@article{ELSHANI2025106226,
title = {AEC Co-design workflow for cross-domain querying and reasoning using Semantic Web Technologies},
journal = {Automation in Construction},
volume = {176},
pages = {106226},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106226},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525002663},
author = {Diellza Elshani and Alessio Lombardi and Daniel Hernandez and Steffen Staab and Al Fisher and Thomas Wortmann},
keywords = {Data integration, Co-design workflow, Ontology, Knowledge graphs, Tool integration, Federated querying},
abstract = {The Architecture, Engineering, and Construction (AEC) industry faces data integration challenges due to fragmented silos and diverse data representations, hindering cross-domain queries and early detection of design constraints. Semantic Web Technologies (SWTs) address data integration challenges. This paper evaluates the impact of SWTs on co-design workflows by comparing them with alternative approaches to assess their effectiveness in supporting interdisciplinary collaboration and design constraint detection. Using Design Science Research, a co-design methodology is developed that integrates SWTs with AEC tools for reasoning and federated querying. A component of this methodology is a bidirectional mapping strategy for translating object-oriented data models, demonstrated with the Building Habitat Object Model (BHoM), an AEC interoperability framework. Findings reveal that integrating SWTs enables reasoning and complex queries across federated datasets, improving co-design efficiency. These findings support AEC professionals in advancing co-design and data-driven decision-making, while also informing future research on integrating SWTs into AEC design workflows.}
}
@article{KRALJEVIC2021102083,
title = {Multi-domain clinical natural language processing with MedCAT: The Medical Concept Annotation Toolkit},
journal = {Artificial Intelligence in Medicine},
volume = {117},
pages = {102083},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102083},
url = {https://www.sciencedirect.com/science/article/pii/S0933365721000762},
author = {Zeljko Kraljevic and Thomas Searle and Anthony Shek and Lukasz Roguski and Kawsar Noor and Daniel Bean and Aurelie Mascio and Leilei Zhu and Amos A. Folarin and Angus Roberts and Rebecca Bendayan and Mark P. Richardson and Robert Stewart and Anoop D. Shah and Wai Keong Wong and Zina Ibrahim and James T. Teo and Richard J.B. Dobson},
keywords = {Electronic health record information extraction, Clinical natural language processing, Clinical concept embeddings, Clinical ontology embeddings},
abstract = {Electronic health records (EHR) contain large volumes of unstructured text, requiring the application of information extraction (IE) technologies to enable clinical analysis. We present the open source Medical Concept Annotation Toolkit (MedCAT) that provides: (a) a novel self-supervised machine learning algorithm for extracting concepts using any concept vocabulary including UMLS/SNOMED-CT; (b) a feature-rich annotation interface for customizing and training IE models; and (c) integrations to the broader CogStack ecosystem for vendor-agnostic health system deployment. We show improved performance in extracting UMLS concepts from open datasets (F1:0.448–0.738 vs 0.429–0.650). Further real-world validation demonstrates SNOMED-CT extraction at 3 large London hospitals with self-supervised training over ∼8.8B words from ∼17M clinical records and further fine-tuning with ∼6K clinician annotated examples. We show strong transferability (F1 > 0.94) between hospitals, datasets and concept types indicating cross-domain EHR-agnostic utility for accelerated clinical and research use cases.}
}
@article{WANG2025100287,
title = {The application of natural language processing technology in hospital network information management systems: Potential for improving diagnostic accuracy and efficiency},
journal = {SLAS Technology},
volume = {32},
pages = {100287},
year = {2025},
issn = {2472-6303},
doi = {https://doi.org/10.1016/j.slast.2025.100287},
url = {https://www.sciencedirect.com/science/article/pii/S2472630325000457},
author = {Shiyong Wang and Hong Luo},
keywords = {Electronic health records (EHR), Scanned document, Natural language processing (NLP), Hidden Bayesian integrated dense Bi-LSTM (HB-DBi-LSTM), Optical character recognition (OCR), Bag of words (BoW)},
abstract = {Background
Processing scanned documents in electronic health records (EHR) was one of the problem in hospital network information management systems (HNIMS). To overcome this difficulty, the complex interactions among natural language processing (NLP), optical character recognition (OCR) and image preprocessing was used.
Objective
The goal is to investigate the possibilities of improving diagnostic efficiency and accuracy in healthcare settings by using NLP technologies into HNIMS. These individuals received diagnoses for a wide range of sleep problems. The data collected were converted into scanned PDF images which were then preprocessed by using gray scaling and OCR. Bag of Words (BoW) is used to extract the featured data.
Method
Reports are divided among 70 % training and 30 % test sets for NLP model evaluation. By employing a hidden Bayesian technique on the development set, we suggest a novel hidden Bayesian integrated dense Bi-LSTM (HB-DBi-LSTM) strategy for optimizing bag-of-words models. A 6:1 ratio is further separated for training and validation sets in deep learning-based sequence models because of their high computing requirements. After 100 epochs of Adam optimization, the dense Bi-LSTM model is trained.
Result
The models are evaluated assessed at the segment level for AHI and SaO2 for ROC and AUROC on test sets. In the finding assessment phase, the detection capacity of the suggested model is evaluated using many criteria, such as F1-score (0.9637), accuracy (0.9321), recall (0.9421) and precision (0.9532). To evaluate information extraction, a document-level examination is also carried out.
Conclusion
To improve diagnostic speed and accuracy, especially when handling scanned documents in EHR, it emphasizes the critical need for strong natural language processing (NLP) systems inside HNIMS.}
}
@article{HAMMOUDA2024388,
title = {SinaTools: Open Source Toolkit for Arabic Natural Language Processing},
journal = {Procedia Computer Science},
volume = {244},
pages = {388-396},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.213},
url = {https://www.sciencedirect.com/science/article/pii/S187705092403014X},
author = {Tymaa Hammouda and Mustafa Jarrar and Mohammed Khalilia},
keywords = {Toolkit, Arabic, Named Entity Recognition, Word Sense Disambiguation, Semantic Relatedness, Synonymy Extraction, Lemmatization, Part-of-speech Tagging, Root tagging, Morphology, NLP, NLU},
abstract = {We introduce SinaTools, an open-source Python package for Arabic natural language processing and understanding. SinaTools is a unified package allowing people to integrate it into their system workflow, offering solutions for various tasks such as flat and nested Named Entity Recognition (NER), fully-flagged Word Sense Disambiguation (WSD), Semantic Relatedness, Synonymy Extractions and Evaluation, Lemmatization, Part-of-speech Tagging, Root Tagging, and additional helper utilities such as corpus processing, text stripping methods, and diacritic-aware word matching. This paper presents SinaTools and its benchmarking results, demonstrating that SinaTools outperforms all similar tools on the aforementioned tasks, such as Flat NER (87.33%), Nested NER (89.42%), WSD (82.63%), Semantic Relatedness (0.49 Spearman rank), Lemmatization (90.5%), POS tagging (93.8%), among others. SinaTools can be downloaded from (https://sina.birzeit.edu/sinatools).}
}
@article{KANG2021102990,
title = {Extraction of Formal Manufacturing Rules from Unstructured English Text},
journal = {Computer-Aided Design},
volume = {134},
pages = {102990},
year = {2021},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2021.102990},
url = {https://www.sciencedirect.com/science/article/pii/S0010448521000014},
author = {SungKu Kang and Lalit Patil and Arvind Rangarajan and Abha Moitra and Tao Jia and Dean Robinson and Farhad Ameri and Debasish Dutta},
keywords = {Rule extraction, Semantic technology, Natural Language Processing (NLP), Ontology},
abstract = {Semantics-based approaches – founded on the idea of explicitly encoding meaning separately from the data or the application code – are being applied to manufacturing, for example, to enable early manufacturability feedback. These approaches rely on formal, i.e., computer-interpretable, knowledge and rules along with the context or semantics, which facilitates the reuse and sharing of the knowledge via semantic web technologies. On the other hand, manufacturing knowledge has been maintained primarily in the form of unstructured English text. It is considered impractical for engineers to author accurate, formal, and structured manufacturing rules. However, previous efforts on extracting semantics from unstructured text in manufacturing have mainly focused on basic concept names and hierarchies for ontology creation, rather than extracting complex manufacturing rules. In this context, this paper focuses on the development of a semantics-based framework for acquiring formal manufacturing rules from English text, such as those written in manufacturing handbooks, by guiding standard Natural Language Processing (NLP) techniques with formal manufacturing knowledge (i.e., controlled vocabulary and domain ontology). Specifically, this paper studies the problem of rule extraction in the manufacturing domain, proposes the formal rule extraction framework, and demonstrates its feasibility. From the dataset of 133 sentences with a manufacturing rule, the proposed framework was able to extract correct rules from approximately 57% of the sentences. This paper also demonstrates the extensibility of the framework. Specifically, the framework was initially developed using the three sections of a manufacturing handbook, including milling, metal stamping, and die-casting sections, and could be successfully applied to the rest of the book after just updating the formal manufacturing knowledge to cover the other sections. This paper provides meaningful results in terms of formalization, thus will contribute to the development, sharing, and reuse of formal manufacturing knowledge that includes complex manufacturing rules.}
}
@article{FAN2025131292,
title = {PHLA: A pre-trained model based method incorporating HLA-peptide binding and immunogenicity},
journal = {Neurocomputing},
volume = {654},
pages = {131292},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.131292},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225019642},
author = {Jixiang Fan and Jianzhang Zhang and Ying Huang and Xiu-Xiu Zhan and Huajun Hu and Chuang Liu},
keywords = {HLA-peptide binding, Pre-trained model, Immunogenicity, Deep learning},
abstract = {The discovery of neoantigen peptides plays a vital role in tumor immunotherapy. Currently, the prediction of neoantigen peptides mainly considers the binding relationship between human leukocyte antigen (HLA) and peptides. However, by ignoring more comprehensive biological considerations, some results may not be sufficiently reliable in tumor immunotherapy. In this study, we applied natural language processing encoding technology and deep learning techniques for prediction, considering both the HLA-peptide binding probability (binding model) and the immunogenicity of the peptide-HLA complex (immune model). Experimental results show that the binding model of the peptide-HLA complex achieves better performance than some benchmarks on two external datasets. Meanwhile, the immunogenicity model of the peptide-HLA complex significantly improves the prediction performance. Finally, we perform data analysis on the prediction results and discover a close correlation with tumors, suggesting that the prediction results of the peptide-HLA complex may provide substantial assistance for tumor vaccine design.}
}
@article{PADUA20241,
title = {Ethnomethodology of written discourse: An analytical model for treating written discourse as ongoing social action},
journal = {Journal of Pragmatics},
volume = {222},
pages = {1-16},
year = {2024},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2023.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S037821662300320X},
author = {Joao Pedro Padua},
keywords = {Written discourse, Text, Discourse analysis, Ethnomethodology, Qualitative methods},
abstract = {Ethnomethodology has been influential in many social science fields, including and especially applied linguistics, where conversation analysis is a major subfield for analyzing oral data. Nevertheless, ethnomethodology can also be—and has been—fruitfully applied to written discursive data, using its tools and methods to describe in detail how written texts are assembled and reflexively used, as members’ methods, for doing and displaying social actions. This paper proposes an open analytical model to capture existing ethnomethodological analyses of written discourse and create a more transparent and replicable protocol for new analyses. I reconstruct ethnomethodology as set of assumptions and principles, review seminal research that used these principles to analyze written discourse, and present the master concept of “active text” that encapsulates them. I then construct a hierarchical prospective-retrospective model to formalize the methods used in ethnomethodology of written discourse. I use the model to reconstruct the analytical methods of two papers to show how it can be used for transparency and replication. The discussion section covers modelling in qualitative applied linguistics and ethnomethodology, and ends with some conclusions.}
}
@article{BABAIE2018213,
title = {Semantic modeling of plastic deformation of polycrystalline rock},
journal = {Computers & Geosciences},
volume = {111},
pages = {213-222},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0098300416308482},
author = {Hassan A. Babaie and Armita Davarpanah},
keywords = {Process ontology, Experimental plastic deformation of rock, Structural geology, Knowledge representation, Semantic modeling},
abstract = {We have developed the first iteration of the Plastic Rock Deformation (PRD) ontology by modeling the semantics of a selected set of deformational processes and mechanisms that produce, reconfigure, displace, and/or consume the material components of inhomogeneous polycrystalline rocks. The PRD knowledge model also classifies and formalizes the properties (relations) that hold between instances of the dynamic physical and chemical processes and the rock components, the complex physio-chemical, mathematical, and informational concepts of the plastic rock deformation system, the measured or calculated laboratory testing conditions, experimental procedures and protocols, the state and system variables, and the empirical flow laws that define the inter-relationships among the variables. The ontology reuses classes and properties from several existing ontologies that are built for physics, chemistry, biology, and mathematics. With its flexible design, the PRD ontology is well positioned to incrementally develop into a model that more fully represents the knowledge of plastic deformation of polycrystalline rocks in the future. The domain ontology will be used to consistently annotate varied data and information related to the microstructures and the physical and chemical processes that produce them at different spatial and temporal scales in the laboratory and in the solid Earth. The PRDKB knowledge base, when built based on the ontology, will help the community of experimental structural geologists and metamorphic petrologists to coherently and uniformly distribute, discover, access, share, and use their data through automated reasoning and integration and query of heterogeneous experimental deformation data that originate from autonomous rock testing laboratories.}
}
@article{SADEGHI2025101769,
title = {Implementing practical argumentation framework as an analytic/evaluative tool in research Articles: The role of critical discourse analysis in language education},
journal = {Social Sciences & Humanities Open},
volume = {12},
pages = {101769},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101769},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125004978},
author = {Karim Sadeghi and Javad Belali},
keywords = {Dominance, Power relations, Practical argumentation, Research papers, Self-regulated learning},
abstract = {The Implications Section (IS) of research articles serves as a vital means for conveying the significance of findings to readers. This study employs the practical argumentation framework to analyze the IS of three research articles focusing on self-regulated learning. Self-regulated learning encompasses learners' ability to set goals, monitor their progress, and reflect on their learning strategies and outcomes, fostering autonomy and effective educational practices. By examining these sections, this study aims to uncover how ideological assumptions are embedded in practical arguments and the ways they influence educational policies and practices. The findings suggest that the promotion of competence, motivation, and autonomy may be subtly intertwined with regulatory discourses that could influence the autonomy of educators and learners. This research sheds light on the intersection of language, power, and ideology in academic discourse, offering valuable insights for educators, policymakers, and researchers in language education contexts.}
}
@article{BONACIN2024104704,
title = {The reuse of electronic health records information models in the oncology domain: Studies with the bioframe framework},
journal = {Journal of Biomedical Informatics},
volume = {157},
pages = {104704},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104704},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001229},
author = {Rodrigo Bonacin and Elaine Barbosa {de Figueiredo} and Ferrucio {de Franco Rosa} and Julio Cesar {dos Reis} and Mariangela Dametto},
keywords = {Electronic health record, Knowledge organization systems, Information model reuse, Semantics},
abstract = {Objective:
The reuse of Electronic Health Records (EHR) information models (e.g., templates and archetypes) may bring various benefits, including higher standardization, integration, interoperability, increased productivity in developing EHR systems, and unlock potential Artificial Intelligence applications built on top of medical records. The literature presents recent advances in standards for modeling EHR, in Knowledge Organization Systems (KOS) and EHR data reuse. However, methods, development processes, and frameworks to improve the reuse of EHR information models are still scarce. This study proposes a software engineering framework, named BioFrame, and analyzes how the reuse of EHR information models can be improved during the development of EHR systems.
Methods:
EHR standards and KOS, including ontologies, identified from systematic reviews were considered in developing the BioFrame framework. We used the structure of the OpenEHR to model templates and archetypes, as well as its relationship to international KOS used in the oncology domain. Our framework was applied in the context of pediatric oncology. Three data entry forms concerning nutrition and one utilized during the first pediatric oncology consultations were analyzed to measure the reuse of information models.
Results:
There was an increase in the adherence rate to international KOS of 18% to the original forms. There was an increase in the concepts reused in all 12 scenarios analyzed, with an average reuse of 6.55% in the original forms compared to 17.1% using BioFrame, resulting in significant differences.
Conclusions:
Our results point to higher reuse rates achieved due to an engineering process that provided greater adherence to EHR standards combined with semantic artifacts. This reveals the potential to develop new methods and frameworks aimed at EHR information model reuse. Additional research is needed to evaluate the impacts of the reuse of the EHR information model on interoperability, EHR data reuse, and data quality and assess the proposed framework in other health domains.}
}
@article{PEIVASTE2025119419,
title = {Artificial intelligence in materials science and engineering: Current landscape, key challenges, and future trajectories},
journal = {Composite Structures},
volume = {372},
pages = {119419},
year = {2025},
issn = {0263-8223},
doi = {https://doi.org/10.1016/j.compstruct.2025.119419},
url = {https://www.sciencedirect.com/science/article/pii/S0263822325005847},
author = {Iman Peivaste and Salim Belouettar and Francesco Mercuri and Nicholas Fantuzzi and Hamidreza Dehghani and Razie Izadi and Halliru Ibrahim and Jakub Lengiewicz and Maël Belouettar-Mathis and Kouider Bendine and Ahmed Makradi and Martin Horsch and Peter Klein and Mohamed El Hachemi and Heinz A. Preisig and Yacine Rezgui and Natalia Konchakova and Ali Daouadji},
keywords = {Machine learning, Materials modeling, Materials design, Predictive modeling, Deep learning, Supervised learning, Unsupervised learning, Neural networks, Graph neural networks (GNNs), Convolutional neural networks (CNNs), Featurization, Property prediction, Materials discovery, Process optimization, Autonomous experimentation, Sustainability, Lifecycle assessment, Digital product passport, Data integration, Standardization},
abstract = {Artificial Intelligence is rapidly transforming materials science and engineering, offering powerful tools to navigate complexity, accelerate discovery, and optimize material design in ways previously unattainable. Driven by the accelerating pace of algorithmic advancements and increasing data availability, AI is becoming an essential competency for materials researchers. This review provides a comprehensive and structured overview of the current landscape, synthesizing recent advancements and methodologies for materials scientists seeking to effectively leverage these data-driven techniques. We survey the spectrum of machine learning approaches, from traditional algorithms to advanced deep learning architectures, including CNNs, GNNs, and Transformers, alongside emerging generative AI and probabilistic models such as Gaussian Processes for uncertainty quantification. The review also examines the pivotal role of data in this field, emphasizing how effective representation and featurization strategies, spanning compositional, structural, image-based, and language-inspired approaches, combined with appropriate preprocessing, fundamentally underpin the performance of machine learning models in materials research. Persistent challenges related to data quality, quantity, and standardization, which critically impact model development and application in materials science and engineering, are also addressed. Key applications are discussed across the materials lifecycle, including property prediction at multiple scales, high-throughput virtual screening, inverse design, process optimization, data extraction by large language models, and sustainability assessment. Critical challenges such as model interpretability, generalizability, and scalability are addressed, alongside promising future directions involving hybrid physics-ML models, autonomous experimentation, collaborative platforms, and human-AI synergy.}
}
@article{SAHLAB2022463,
title = {Extending the Intelligent Digital Twin with a context modeling service: A decision support use case},
journal = {Procedia CIRP},
volume = {107},
pages = {463-468},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S221282712200292X},
author = {Nada Sahlab and Dominik Braun and Christian Köhler and Nasser Jazdi and Michael Weyrich},
keywords = {context-awareness, Digital Twin, ontology, labeled property graph, intelligent warehouse, cyber-physical systems},
abstract = {Considering the surrounding context of manufacturing systems at runtime enhances their adaptability, which is advantageous given the increasing demand for flexible production. Context-awareness presents a viable approach to continuously model the surrounding context and relate it to the system’s operation. As the surrounding context is both acquirable from heterogeneous data sources and is dynamic in nature, a coherent and evolving context model is necessary. The context modeling approach needs to be both: system-centric to enable re-usability of the model by different applications as well as continuously and efficiently extendable to continuously represent the environment. Some approaches to model context within the manufacturing domain exist, but are mostly tailored to specific applications or do not consider an extension at runtime. To include the surrounding context of a system effectively and adapt it accordingly, a relation between the modeled context and the system during runtime is necessary. The intelligent Digital Twin provides essential interdisciplinary models of a physical system, which can be used for its analysis and monitoring. Adding a further service in the form of a model to represent the surrounding context enhances the intelligent Digital Twin to be used by various applications, e.g., for reconfiguration or decision-support. In our previous work, we have presented a tier-based model to realize context-awareness for intelligent Digital Twins. The presented tier model has different levels corresponding to different context scopes. In this contribution, we present an approach to realize an extendible and evolving tier-based context model with a generic ontology and model transformation to a labeled property graph with focus on the external context. To show the applicability and added value of the context model, a use case is presented, which addresses an intelligent warehouse’s Digital Twin and its context-model supported decision-making.}
}
@article{GARRIDO2020105242,
title = {Using LEL and scenarios to derive mathematical programming models. Application in a fresh tomato packing problem},
journal = {Computers and Electronics in Agriculture},
volume = {170},
pages = {105242},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105242},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919317338},
author = {Alejandra Garrido and Leandro Antonelli and Jonathan Martin and M.M.E. Alemany and Josefa Mula},
keywords = {Language extended lexicon (LEL), Scenarios, Software engineering, Mathematical programming, Fresh tomato packing},
abstract = {Mathematical programming models are invaluable tools at decision making, assisting managers to uncover otherwise unattainable means to optimize their processes. However, the value they provide is only as good as their capacity to capture the process domain. This information can only be obtained from stakeholders, i.e., clients or users, who can hardly communicate the requirements clearly and completely. Besides, existing conceptual models of mathematical programming models are not standardized, nor is the process of deriving the mathematical programming model from the concept model, which remains ad hoc. In this paper, we propose an agile methodology to construct mathematical programming models based on two techniques from requirements engineering that have been proven effective at requirements elicitation: the language extended lexicon (LEL) and scenarios. Using the pair of LEL + scenarios allows to create a conceptual model that is clear and complete enough to derive a mathematical programming model that effectively captures the business domain. We also define an ontology to describe the pair LEL + scenarios, which has been implemented with a semantic mediawiki and allows the collaborative construction of the conceptual model and the semi-automatic derivation of mathematical programming model elements. The process is applied and validated in a known fresh tomato packing optimization problem. This proposal can be of high relevance for the development and implementation of mathematical programming models for optimizing agriculture and supply chain management related processes in order to fill the current gap between mathematical programming models in the theory and the practice.}
}
@article{TEIXEIRA2025126675,
title = {Semantic rule-based approach for automated energy resource management in buildings},
journal = {Applied Energy},
volume = {401},
pages = {126675},
year = {2025},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2025.126675},
url = {https://www.sciencedirect.com/science/article/pii/S0306261925014059},
author = {Brígida Teixeira and Gabriel Santos and David Araújo and Letícia Gomes and Zita Vale},
keywords = {Artificial intelligence, Building energy management, Machine learning, Semantic knowledge and reasoning},
abstract = {The widespread use of renewable energy sources leads to the adoption of more sophisticated and intelligent real-time energy management solutions. Automated energy management systems allow consumers to play an active role in their flexibility management while ensuring their energy needs are met. However, a lack of trust in autonomous decision-making poses a significant challenge to their adoption. This work presents a novel semantic-based framework for automated energy management in buildings, integrating semantic rules and ontologies, expert knowledge, and machine learning models to enhance decision transparency and adaptability. By leveraging a semantic-based model, the proposed framework improves real-time decision-making, facilitates interoperability between data sources, and provides context-aware explanations, fostering user trust and system reliability. The framework has been tested and validated within a cyber-physical infrastructure, ensuring its robustness in real-world scenarios. A case study on the management of lighting and air conditioning demonstrates the advantages of this approach. The results confirm that the framework effectively adapts to evolving conditions, ensures reliable decision-making, and fosters user trust by providing interpretable justifications for automated actions. This facilitates a more efficient use of energy resources, reduces costs, and supports the transition toward a more sustainable and renewable-based power sector.}
}
@incollection{RZEPKA2024209,
title = {Chapter Twelve - Obtaining hints to understand language model-based moral decision making by generating consequences of acts},
editor = {Peggy Wu and Michael Salpukas and Hsin-Fu Wu and Shannon Ellsworth},
booktitle = {Trolley Crash},
publisher = {Academic Press},
pages = {209-223},
year = {2024},
isbn = {978-0-443-15991-6},
doi = {https://doi.org/10.1016/B978-0-44-315991-6.00018-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443159916000182},
author = {Rafal Rzepka and Kenji Araki},
keywords = {Artificial moral agents, context generation, sentiment analysis},
abstract = {In this chapter we discuss the limitations and dangers of language models in the task of moral decision making and suggest possible steps for building artificial moral agents that can explain their decisions. It is difficult to recognize triggers which influence moral decisions in our daily lives and the same is true about current artificial moral agents based on deep learning. Systems like Delphi show relatively high accuracy in recognizing potentially problematic acts, but its user cannot be sure why a given label was chosen by the system. In the past we have been retrieving possible consequences of a human act from blogs, but such approach is limited to short queries and suffers from low recall. With the advent of large language models, now it is possible to process acts with richer context, but unlike in the previous approach, it is difficult to investigate the reasons of errors. To partially alleviate this problem, we use another language model (GPT-2) to generate possible consequences of an act and perform sentiment analysis on these consequences to observe how they influence the automatic evaluation. Our experiments with Japanese language show that language model-based classification yields much higher accuracy than lexicon word matching, but we discovered that the longer generated consequences are, the more accurate the lexicon-based approach becomes. This steady increase suggests that a richer consequence description might help to improve moral estimation of acts in the future, but the artificially added context does not represent usual consequences of the input act.}
}
@article{AGATE2025100135,
title = {Artificial intelligence methods and approaches to improve data quality in healthcare data},
journal = {Artificial Intelligence in the Life Sciences},
volume = {8},
pages = {100135},
year = {2025},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2025.100135},
url = {https://www.sciencedirect.com/science/article/pii/S266731852500011X},
author = {Jarmakoviča Agate},
keywords = {Data quality, Artificial intelligence, Healthcare data, Deep learning, Federated learning, Ontology-based governance, Data completeness, Consistency, Systematic review, PRISMA},
abstract = {This study explores artificial intelligence (AI) methods and approaches used to improve data quality, with a particular focus on healthcare data. Applying a systematic literature review based on the PRISMA framework, the research examines publications from 2020 to 2025 that analyze AI applications across key data quality dimensions—accuracy, completeness, consistency, timeliness, uniqueness, and validity. The study aims to identify which AI methods are most commonly employed and how they align with these quality attributes. A conceptual map was developed to visualize the relationships between dimensions and AI techniques such as deep learning, federated learning, data-centric AI, and ontology-based data governance. Findings reveal that accuracy and consistency are the most emphasized dimensions in the literature, with methods like supervised learning, NLP, and isolation forest frequently applied. In contrast, dimensions like timeliness and validity receive comparatively limited attention. The study concludes that certain AI methods—particularly data-centric and cross-cutting approaches—are effective in addressing multiple data quality challenges simultaneously. These insights offer practical guidance for selecting AI strategies in healthcare data quality improvement and highlight areas for future research.}
}
@article{MADRA2025106875,
title = {Decolonizing development economics: A critique of the late neoclassical reason},
journal = {World Development},
volume = {188},
pages = {106875},
year = {2025},
issn = {0305-750X},
doi = {https://doi.org/10.1016/j.worlddev.2024.106875},
url = {https://www.sciencedirect.com/science/article/pii/S0305750X24003462},
author = {Yahya M. Madra and Bengi Akbulut and Fikret Adaman},
keywords = {Developmentalism, Colonialism, Economic difference, Decolonization, Poor economics},
abstract = {In this paper, we scrutinize contemporary development economics to render visible the colonial impulses that lead to forms of silencing and disavowal of economic differences in ontological and epistemological terms. As decolonizing economies and decolonizing economics are interwoven, we open our discussion with a history of decolonization efforts at the level of the political economy to form a background for the discussion of the ontological and epistemological issues on the coloniality of development economics. We then first engage with neoclassical economics and its antecedents in classical political economy—the orthodox and hegemonic stream in the discipline of economics—highlighting how its individualistic and mechanistic nature implicates the discipline with the coloniality of knowledge production, and second, unpack the current state of mainstream development economics after the late neoclassical turn (incorporating to standard economic models the problems emanating from market failures, cognitive biases, and multiple equilibria) in the discipline by focusing on two of its prominent research agendas: new institutional economics of development divergence, and the poor economics of development. Finally, we formulate some perspectives for decolonial development economics.}
}
@article{BARKI2021818,
title = {Model-based prediction of oncotherapy risks and side effects in bladder cancer},
journal = {Procedia Computer Science},
volume = {181},
pages = {818-826},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.235},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002787},
author = {Chamseddine Barki and Hanene Boussi Rahmouni and Salam Labidi},
keywords = {Bladder cancer, side-effects, evidence-based medicine, knowledge, modelling, oncotherapy, ontology, treatment, risks},
abstract = {The prediction of cancer treatment side-effects requires the capturing of complex biophysical therapy parameters and the integration of different medical knowledge elements. In relation with radiotherapy, it is widely observed that the uncontrolled processes or undefined radiation therapy dose can decline the state of treatment. Precisely, the inability to manage the flow of available information, usually provided in heterogeneous formats, made it complicated to oversee and predict risks and effects of a prescribed treatment protocol. We think that, the optimization of knowledge representation and modelling in the context of evidence-based medicine can support the automated prediction of risks and side effects in oncotherapy. The following manuscript describes our methodology used for the design of a bladder cancer treatment side effects ontology embedded with evidence-based semantic rules and queries. Treatment knowledge is represented along with a particular consideration to the modelling of its referred risks and side effects. Our ontology model helps in improving the streamlining of medical practices and clinical decision-making. Within our semantic web approach, better strategies are applied for treatment selection with reference to possible side effects. Our ontology depicts real world scenario of developing treatment-related side effects. Furthermore, it is a clinical decision support system founding tool that highlights treatments efficiency. Our model shares treatment knowledge, facts and effects. Moreover, it includes medical evidence and incorporates a semantic rule base for systemic prediction results.}
}
@incollection{AOKIKINOSHITA2025301,
title = {Semantic Web Integration in Life Science Data},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {301-312},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00136-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027001366},
author = {Kiyoko F. Aoki-Kinoshita and Achille Zappa and Yukie Akune-Taylor},
keywords = {Data integration, Interoperability, Knowledge discovery, Knowledge engineering, Linked data, Ontologies, RDF, Semantic bioinformatics, Semantic computational biology, Semantic knowledge graphs, Semantic web, SPARQL, Standards, Translational research},
abstract = {This chapter delves into the applications of Semantic Web technologies and Linked Data principles in bioinformatics and computational biology. It explores how these technologies facilitate data integration, analysis, and sharing in the life sciences domain, emphasizing the importance of ontologies and standards like RDF and SPARQL. This chapter highlights the goals and perspectives in bioinformatics, focusing on the adoption of Semantic Web technologies to address data integration challenges and to enhance research productivity. It also discusses the evolution of Semantic Web initiatives in bio-domains and the potential impact of these technologies on advancing translational research and personalized medicine.}
}
@article{KAN2025118682,
title = {ZFinfer: A novel chemical-phenotype inference system for zebrafish for filling data gaps in environmental pollutant research},
journal = {Ecotoxicology and Environmental Safety},
volume = {302},
pages = {118682},
year = {2025},
issn = {0147-6513},
doi = {https://doi.org/10.1016/j.ecoenv.2025.118682},
url = {https://www.sciencedirect.com/science/article/pii/S0147651325010279},
author = {Hung-Lin Kan and Shan-Shan Wang and Chun-Wei Tung},
keywords = {Chemical-gene-phenotype inference, Enrichment analysis, Environmental pollutant,  approach, PFAS, Zebrafish phenotype ontology},
abstract = {Zebrafish is an effective model organism for toxicological investigations due to their tiny size, quick reproduction, and conserved vertebrate biology. As environmental pollutants continue to increase, it becomes challenging to detect all chemical-related hazards using zebrafish models. In silico models can facilitate prioritizing chemicals prior to further experimental evaluations and providing potential underlying mechanisms. Chemical-phenotype inference system for zebrafish (ZFinfer), an enrichment analysis tool that can predict affected endpoints, was developed by integrating chemical-protein interaction data from the Search Tool for Interacting Chemicals (STITCH) database and gene-phenotype annotation data from the Zebrafish Information Network (ZFIN). Currently, 419,328 chemicals, 23,180 zebrafish proteins, and 3,104 phenotypes for zebrafish were curated and included in the system. ZFinfer has been validated using 777 ToxCast chemicals and 51 priority pollutants from the USEPA. The inference results demonstrated a sensitivity of 0.72 in critical morphological endpoints and a 93 % rediscovery rate for known toxicity records in the ECOTOX knowledgebase. Furthermore, the affected endpoints of 5,195 PFAS chemical exposures were inferred to fill data gaps. ZFinfer could be useful to prioritize chemicals that should be further evaluated and may be applicable in drug discovery and environmental chemical hazard prediction.}
}
@article{WANG2024115035,
title = {A knowledge graph-based framework to automate the generation of building energy models using geometric relation checking and HVAC topology establishment},
journal = {Energy and Buildings},
volume = {325},
pages = {115035},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.115035},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824011514},
author = {Meng Wang and Georgios N. Lilis and Dimitris Mavrokapnidis and Kyriakos Katsigarakis and Ivan Korolija and Dimitrios Rovas},
keywords = {Building digital twins, BIM2BEM, Ontology, HVAC topology, Geometric relation checking},
abstract = {Building Energy Models (BEM) are widely utilized throughout all stages of a building's lifecycle to understand and enhance energy usage. However, creating these models demands significant effort, particularly for larger buildings or those with complex HVAC systems. While a substantial amount of information can be extracted from Building Information Models (BIM) — which are increasingly accessible and provide necessary data for geometric and HVAC contexts — this information is not readily usable in setting up BEM and typically requires manual translation. To address this challenge, this paper introduces a BIM-to-BEM (BIM2BEM) framework that focuses on automating the generation of HVAC parts of BEM models from BIM data. Core to the methodology is the extraction of HVAC system topologies from the BIM model and the creation of a knowledge graph with the HVAC topology. The topology transformation unfolds in three key stages: first, a geometry-induced knowledge graph is established by examining the geometric relationships among HVAC elements; second, this graph is converted into an informative HVAC topology with enhanced properties from additional data sources; and finally, the informative topology is simplified into a BEM-oriented HVAC topology compliant with BEM platforms such as EnergyPlus. A case study of a large university building with a complex HVAC system showcases that the proposed framework achieves automatic and precise generation of building performance simulation models. The model's predictions are then validated against actual measurements from the building.}
}
@article{MISHRA20228765,
title = {Service-oriented architecture for Internet of Things: A semantic approach},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {10, Part A},
pages = {8765-8776},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.09.024},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821002755},
author = {Sugyan Kumar Mishra and Anirban Sarkar},
keywords = {Internet of Things, Service-oriented architecture, Semantic sensor network ontology, Service composition, Clinical decision support system},
abstract = {The Internet of Things (IoT) aids an interconnection between systems, humans, and services to develop computation-intensive autonomous applications. Service-oriented architecture (SOA) concepts have been used as defacto software architecture to develop IoT-based systems. Though SOA facilitates various benefits, several challenges exist for integration with IoT-based systems, including configurability, interoperability, and manageability. The ontology-based approach provides a semantically enriched and rigorous platform for interoperating different heterogeneous IoT-based systems and applications. The semantic modelling research has focused on IoT resource management without concerning the method of accessing and utilizing IoT information. In this context, an IoT-based large-scale SOA (IoT-LSS) ontology is proposed that supports interoperability, heterogeneity, flexibility, manageability, extendibility, scalability. This ontology integrates the concept of large-scale SOA (LSS) with semantic sensor network (SSN) ontology. Clinical decision support system (CDSS) ontology has considered as a case study to illustrate the proposed ontology. A detailed comparative analysis between the proposed system and several existing IoT-based service ontologies has been performed based on different characteristics. This analysis shows that IoT-LSS supports dynamic features and exposes to service with its different mechanisms. This article presents an ontological framework for the IoT service-based environment to efficiently use for service composition mechanisms in the large service domain.}
}
@article{MCGRANAGHAN2023100142,
title = {The cultural-social nucleus of an open community: A multi-level community knowledge graph and NASA application},
journal = {Applied Computing and Geosciences},
volume = {20},
pages = {100142},
year = {2023},
issn = {2590-1974},
doi = {https://doi.org/10.1016/j.acags.2023.100142},
url = {https://www.sciencedirect.com/science/article/pii/S2590197423000319},
author = {Ryan M. McGranaghan and Ellie Young and Cameron Powers and Swapnali Yadav and Edlira Vakaj},
keywords = {Knowledge representation, Knowledge graph, Collaboration, Community, Data science, Open science, Inclusivity, Accessibility, Information organization, Heliophysics, Space physics, Collective intelligence},
abstract = {The challenges faced by science, engineering, and society are increasingly complex, requiring broad, cross-disciplinary teams to contribute to collective knowledge, cooperation, and sensemaking efforts. However, existing approaches to collaboration and knowledge sharing are largely manual, inadequate to meet the needs of teams that are not closely connected through personal ties or which lack the time to respond to dynamic requests for contextual information sharing. Nonetheless, in the current remote-first, complexity-driven, time-constrained workplace, such teams are both more common and more necessary. For example, the NASA Center for HelioAnalytics (CfHA) is a growing and cross-disciplinary community that is dedicated to aiding the application of emerging data science techniques and technologies, including AI/ML, to increase the speed, rigor, and depth of space physics scientific discovery. The members of that community possess innumerable skills and competencies and are involved in hundreds of projects, including proposals, committees, papers, presentations, conferences, groups, and missions. Traditional structures for information and knowledge representation do not permit the community to search and discover activities that are ongoing across the Center, nor to understand where skills and knowledge exist. The approaches that do exist are burdensome and result in inefficient use of resources, reinvention of solutions, and missed important connections. The challenge faced by the CfHA is a common one across modern groups and one that must be solved if we are to respond to the grand challenges that face our society, such as complex scientific phenomena, global pandemics and climate change. We present a solution to the problem: a community knowledge graph (KG) that aids an organization to better understand the resources (people, capabilities, affiliations, assets, content, data, models) available across its membership base, and thus supports a more cohesive community and more capable teams, enables robust and responsible application of new technologies, and provides the foundation for all members of the community to co-evolve the shared information space. We call this the Community Action and Understanding via Semantic Enrichment (CAUSE) ontology. We demonstrate the efficacy of KGs that can be instantiated from the ontology together with data from a given community (shown here for the CfHA). Finally, we discuss the implications, including the importance of the community KG for open science.}
}
@article{SPEYER2025,
title = {Recovery-oriented psychiatry: oxymoron or catalyst for change?},
journal = {The Lancet Psychiatry},
year = {2025},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(25)00092-6},
url = {https://www.sciencedirect.com/science/article/pii/S2215036625000926},
author = {Helene Speyer and David Roe and Mike Slade},
abstract = {Summary
This Personal View provides a normative and conceptual analysis of the intersection between the recovery movement and psychiatry. Although recovery emerged as a grassroots social justice movement emphasising empowerment and systemic change, psychiatry remains rooted in the medical paradigm. We aim to develop a nuanced conceptual framework that fosters academic debate and meaningful implementation, while avoiding superficial or tokenistic adoption of recovery principles. Our analysis explores the contrasting values, ontologies, and epistemologies of these perspectives, identifying points of tension and areas of compatibility. We examine and discuss integrative and non-integrative pluralistic approaches, and we conclude with actionable recommendations for transformation at different organisational levels.}
}
@article{ROBERTS2024100224,
title = {Natural language processing of clinical notes enables early inborn error of immunity risk ascertainment},
journal = {Journal of Allergy and Clinical Immunology: Global},
volume = {3},
number = {2},
pages = {100224},
year = {2024},
issn = {2772-8293},
doi = {https://doi.org/10.1016/j.jacig.2024.100224},
url = {https://www.sciencedirect.com/science/article/pii/S2772829324000201},
author = {Kirk Roberts and Aaron T. Chin and Klaus Loewy and Lisa Pompeii and Harold Shin and Nicholas L. Rider},
keywords = {Natural language processing, machine learning, text mining, inborn errors of immunity, primary immunodeficiency, diagnosis, artificial intelligence},
abstract = {Background
There are now approximately 450 discrete inborn errors of immunity (IEI) described; however, diagnostic rates remain suboptimal. Use of structured health record data has proven useful for patient detection but may be augmented by natural language processing (NLP). Here we present a machine learning model that can distinguish patients from controls significantly in advance of ultimate diagnosis date.
Objective
We sought to create an NLP machine learning algorithm that could identify IEI patients early during the disease course and shorten the diagnostic odyssey.
Methods
Our approach involved extracting a large corpus of IEI patient clinical-note text from a major referral center’s electronic health record (EHR) system and a matched control corpus for comparison. We built text classifiers with simple machine learning methods and trained them on progressively longer time epochs before date of diagnosis.
Results
The top performing NLP algorithm effectively distinguished cases from controls robustly 36 months before ultimate clinical diagnosis (area under precision recall curve > 0.95). Corpus analysis demonstrated that statistically enriched, IEI-relevant terms were evident 24+ months before diagnosis, validating that clinical notes can provide a signal for early prediction of IEI.
Conclusion
Mining EHR notes with NLP holds promise for improving early IEI patient detection.}
}
@article{NIKIEMA2024104614,
title = {Improving the interoperability of drugs terminologies: Infusing local standardization with an international perspective},
journal = {Journal of Biomedical Informatics},
volume = {151},
pages = {104614},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104614},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000327},
author = {Jean Noël Nikiema and James Liang and Man Qing Liang and Davllyn {dos Anjos} and Aude Motulsky},
keywords = {OCRx, FAIR KOS, Interoperability, IDMP},
abstract = {Objectives:
The objective of this study is to describe how OCRx (Canadian Drug Ontology) has been built to address the dual need for local drug information integration in Canada and alignment with international standards requirements.
Methods:
This paper delves into (i) the implementation efforts to meet the Identification of Medicinal Product (IDMP) requirements in OCRx, alongside the ontology update strategy, (ii) the structure of the ontology itself, (iii) the alignment approach with several reference Knowledge Organization Systems, including SNOMED CT, RxNorm, and the list of “Code Identifiant de Spécialité” (CIS-Code), and (iv) the look-up services developed to facilitate its access and utilization.
Results:
Each OCRx release contains two distinct versions: the full and the up-to-date version. The full version encompasses all drugs with a DIN code sanctioned by Health Canada, while the up-to-date version is limited to drugs currently marketed in Canada. In the last release of OCRx, the full version comprises 162,400 classes; meanwhile, the up-to-date version consists of 36,909 classes. In terms of mappings with OCRx, substances in RxNorm and SNOMED CT fall below 40%, registering at 37% and 22% respectively. Meanwhile, mappings for CIS-Code achieve coverage of 61%. The strength mappings are notably low for RxNorm at 40% and for CIS-code at 28%. This affects the mapping of clinical drugs, which are predominantly alignable through post-coordinated expressions: 56% for RxNorm, 80% for SNOMED CT, and 35% for CIS-Code. The main support service of OCRx is a look-up service known as PaperRx that displays OCRx’s entities based on description logic queries (DL-queries) performed through the classified structure of OCRx. The look-up services also contain a SPARQL endpoint, an OCRx OWL file downloader, and a RESTful API.
Discussion:
The OCRx ontology demonstrates a significant effort towards integrating Canadian drug information with international standards. However, there are areas for improvement. In the future, our focus will be on refining the structure of OCRx for better classification capability and improvement of dosage conversion. Additionally, we aim to harness OCRx in constructing an ontology-based annotator, setting our sights on its deployment in real-world data integration scenarios.}
}
@article{ERGENE2025101439,
title = {Sustainability-in-the-making: Enduring commitment to socio-ecological matters of concerns},
journal = {Scandinavian Journal of Management},
pages = {101439},
year = {2025},
issn = {0956-5221},
doi = {https://doi.org/10.1016/j.scaman.2025.101439},
url = {https://www.sciencedirect.com/science/article/pii/S0956522125000442},
author = {Seray Ergene and Marta B. Calás and Erim Ergene},
keywords = {Actor-network theory, Assemblage, Fashion industry, Grand challenges, Matters of concern, Process studies, Relational practices, Sustainability},
abstract = {In this paper, we move beyond the economic reductionism of the “business case” paradigm in sustainability research and explore complexity in organizations’ engagement with socio-ecological matters. For this, we draw from Latour (2004; 2005; 2008) where he argues for an ontological shift from observing reality as matters of fact to engaging with its complexities as matters of concern and utilize Actor-Network Theory to re-frame sustainability from the business case to sustainability as relational practices. We study empirically how commitment to socio-ecological matters of concern is maintained, with a longitudinal case study of an exemplar fashion company’s sustainability initiative between 2012 and 2020. Analyses of our in-depth qualitative data led to articulating sustainability-in-the-making: an ongoing process where the agency of human and nonhuman collective assemblages produced organizational transformation and endured commitment to socio-ecological matters of concern. Our findings contribute to organizational sustainability literature by: 1- offering an ontological shift from the business case to relational practices, which enables focusing on emerging and evolving organizational transformations overlooked by the business case; 2- challenging the hierarchical conception of “scale” by illustrating how human and nonhuman actors in their own settings composed collectives and produced and expanded impact around matters of concern; and 3- showing the significance of dealing with controversies – disagreements and debates around particular matters of concern – in enduring commitment to socio-ecological issues.}
}
@article{HEYLIGHEN2023104937,
title = {Modeling autopoiesis and cognition with reaction networks},
journal = {Biosystems},
volume = {230},
pages = {104937},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.104937},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723001120},
author = {Francis Heylighen and Evo Busseniers},
keywords = {Autopoietic systems, Resilience, Chemical organization theory, Origin of life, Reaction networks, Cybernetics, Self-organization, Cognition, Epistemology, Process ontology},
abstract = {Maturana and Varela defined an autopoietic system as a self-regenerating network of processes. We reinterpret and elaborate this conception starting from a process ontology and its formalization in terms of reaction networks and chemical organization theory. An autopoietic organization can be modelled as a network of “molecules” (components) undergoing reactions, which is (operationally) closed and self-maintaining. Such organizations, being attractors of a dynamic system, tend to self-organize—thus providing a model for the origin of life. However, in order to survive in a variable environment, they must also be resilient, i.e. able to compensate perturbations. According to the “good regulator theorem” this requires some form of cognition, i.e. knowing which action to perform for which perturbation. Such cognition becomes more effective as it learns to anticipate perturbations by discovering invariant patterns in its interactions with the environment. Nevertheless, the resulting predictive model remains a subjective construction. Such implicit model cannot be interpreted as an objective representation of external reality, because the autopoietic system does not have direct access to that reality, and there is in general no isomorphism between internal and external processes.}
}
@article{LI2025111842,
title = {Advanced dialog state tracking with noetic graphs for complex human-machine interactions},
journal = {Pattern Recognition},
volume = {168},
pages = {111842},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2025.111842},
url = {https://www.sciencedirect.com/science/article/pii/S0031320325005023},
author = {Jingyang Li and Shengli Song and Sitong Yan and Guangneng Hu and Chengen Lai and Yulong Zhou},
keywords = {Task-oriented dialog system, Dialog state tracking, Noetic state representation graph, DialoGPT, Graph attention network, Multiple domains},
abstract = {Dialog State Tracking (DST) is a crucial process in task-oriented dialog systems, evaluating the current state of a conversation based on preceding interactions. Two effective techniques for DST are Large Language Models (LLMs) and neural networks. However, conventional neural networks lack explainability and reasoning abilities, limiting their adaptability to unknown domains and complex scenarios, posing challenges in real-time human-machine interfaces. This paper formulates the DST problem by assembling and representing the belief state using an explicit model and integrating it to enhance the neural network’s DST capabilities. A novel technique, the Noetic State Representation (NSR) Graph, is proposed to address these challenges. The NSR graph offers a dynamic and explicit representation of dialog context that synchronizes with multi-turn dialogues. To improve reasoning ability and semantic augmentation, a pre-trained language model, DialoGPT, is employed as the encoder for utterance sequences. The core NSR graph is built and encoded using a graph attention network to ensure the explicit representation of dialog context. To generate the belief state, the proposed model utilizes a classical sequence decoder, which is guided by the context information from the NSR graph and utterances. Experimental results demonstrate the effectiveness of this approach, achieving a 0.8 % improvement on unknown domains and a 1.7 % improvement across all domains in the Schema-Guided Dialogue (SGD) dataset, outperforming advanced techniques and showing strong results on the MultiWOZ dataset.}
}
@article{LOCH2025103382,
title = {Building connections: Exploring social network research in forest sciences},
journal = {Forest Policy and Economics},
volume = {170},
pages = {103382},
year = {2025},
issn = {1389-9341},
doi = {https://doi.org/10.1016/j.forpol.2024.103382},
url = {https://www.sciencedirect.com/science/article/pii/S1389934124002363},
author = {Theresa Klara Loch and Daniela Kleinschmit},
keywords = {Forest governance, Forest policy research, Relationalism, Social network analysis, Social network theories},
abstract = {This study evaluates the role of social network research in exploring its current application within forest research and identify potential for building connections. Through a systematic literature review of 135 articles, we investigate the theoretical and methodological nuances of social network research, highlighting the predominance of ontological and epistemological underpinnings of network theories as well as the pre-eminence of structural approaches. Our review identifies a significant emphasis on second-generation social network analysis (SNA) methods in the literature, which primarily focus on network structures. We find a limited application of relational and ideational perspectives offered by first and third-generation approaches. The literature review reveals that social network theories, although crucial, are underutilized beyond their ontological and epistemological underpinnings in forest research. Our findings demonstrate that social networks are essential for knowledge exchange, trust and power. However, the integration of comprehensive social network theories into forest research remains limited, suggesting the potential for further application in forest research. We emphasize the need for a more diversified methodological approach that extends beyond structural analysis to include more qualitative and ideational frameworks. By broadening the scope of social network applications, forest research can more effectively tackle the complexities of sustainable management and governance. This shift could lead to more robust strategies to cope with the challenges posed by environmental changes and complex stakeholder dynamics in forest governance.}
}
@article{XU2022104080,
title = {A simple neural vector space model for medical concept normalization using concept embeddings},
journal = {Journal of Biomedical Informatics},
volume = {130},
pages = {104080},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104080},
url = {https://www.sciencedirect.com/science/article/pii/S153204642200096X},
author = {Dongfang Xu and Timothy Miller},
keywords = {Deep Learning, Natural Language Processing, Medical Concept Normalization, Vector Space Model, Normalized Temperature-scaled Softmax},
abstract = {Objective
Medical concept normalization (MCN), the task of linking textual mentions to concepts in an ontology, provides a solution to unify different ways of referring to the same concept. In this paper, we present a simple neural MCN model that takes mentions as input and directly predicts concepts.
Materials and Methods
We evaluate our proposed model on clinical datasets from ShARe/CLEF eHealth 2013 shared task and 2019 n2c2/OHNLP shared task track 3. Our neural MCN model consists of an encoder, and a normalized temperature-scaled softmax (NT-softmax) layer that maximizes the cosine similarity score of matching the mention to the correct concept. We adopt SAPBERT as the encoder and initialize the weights in the NT-softmax layer with pre-computed concept embeddings from SAPBERT.
Results
Our proposed neural model achieves competitive performance on ShARe/CLEF 2013 and establishes a new state-of-the-art on 2019-n2c2-MCN. Yet this model is simpler than most prior work: it requires no complex pipelines, no hand-crafted rules, and no preprocessing, making it simpler to apply in new settings.
Discussion
Analyses of our proposed model show that the NT-softmax is better than the conventional softmax on the MCN task, and both the CUI-less threshold parameter and the initialization of the weight vectors in the NT-softmax layer contribute to the improvements.
Conclusion
We propose a simple neural model for clinical MCN, an one-step approach with simpler inference and more effective performance than prior work. Our analyses demonstrate future work on MCN may require more effort on unseen concepts.}
}
@article{PUSSI2025111557,
title = {Reference framework for metadata description to commensurate data in grain production},
journal = {Data in Brief},
volume = {60},
pages = {111557},
year = {2025},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2025.111557},
url = {https://www.sciencedirect.com/science/article/pii/S2352340925002896},
author = {Katariina Pussi and Petri Linna and Pasi Suomi and Kim Kaustell and Liisa Pesonen},
keywords = {Agriculture, Smart farming, Grain production, Data, Metadata, Data space},
abstract = {Data spaces will bring the need to harmonize the farm collected data for better interoperability. Attention needs also to be paid to data accessibility, since the value of data is strongly linked to its use. The evolving data space technologies will bring service providers that help farmers to translate farm born datasets to data products that can have measurable value. Data products can then be published to the data space catalog, allowing other people to discover and consume them. Data used for data products, decision-making, reporting, or analysis should be reliable and trustworthy. Common metadata standards, catalogs and ontologies will help to achieve this goal. The scope of this paper is to discuss requirements for metadata in grain production.}
}
@article{INVERNICI2024,
title = {Searching COVID-19 Clinical Research Using Graph Queries: Algorithm Development and Validation},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/52655},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124002802},
author = {Francesco Invernici and Anna Bernasconi and Stefano Ceri},
keywords = {big data corpus, clinical research, co-occurrence network, COVID-19 Open Research Dataset, CORD-19, graph search, Named Entity Recognition, Neo4j, text mining},
abstract = {Background
Since the beginning of the COVID-19 pandemic, >1 million studies have been collected within the COVID-19 Open Research Dataset, a corpus of manuscripts created to accelerate research against the disease. Their related abstracts hold a wealth of information that remains largely unexplored and difficult to search due to its unstructured nature. Keyword-based search is the standard approach, which allows users to retrieve the documents of a corpus that contain (all or some of) the words in a target list. This type of search, however, does not provide visual support to the task and is not suited to expressing complex queries or compensating for missing specifications.
Objective
This study aims to consider small graphs of concepts and exploit them for expressing graph searches over existing COVID-19–related literature, leveraging the increasing use of graphs to represent and query scientific knowledge and providing a user-friendly search and exploration experience.
Methods
We considered the COVID-19 Open Research Dataset corpus and summarized its content by annotating the publications’ abstracts using terms selected from the Unified Medical Language System and the Ontology of Coronavirus Infectious Disease. Then, we built a co-occurrence network that includes all relevant concepts mentioned in the corpus, establishing connections when their mutual information is relevant. A sophisticated graph query engine was built to allow the identification of the best matches of graph queries on the network. It also supports partial matches and suggests potential query completions using shortest paths.
Results
We built a large co-occurrence network, consisting of 128,249 entities and 47,198,965 relationships; the GRAPH-SEARCH interface allows users to explore the network by formulating or adapting graph queries; it produces a bibliography of publications, which are globally ranked; and each publication is further associated with the specific parts of the query that it explains, thereby allowing the user to understand each aspect of the matching.
Conclusions
Our approach supports the process of query formulation and evidence search upon a large text corpus; it can be reapplied to any scientific domain where documents corpora and curated ontologies are made available.}
}
@article{SOUSA2024108076,
title = {Explaining protein–protein interactions with knowledge graph-based semantic similarity},
journal = {Computers in Biology and Medicine},
volume = {170},
pages = {108076},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108076},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524001604},
author = {Rita T. Sousa and Sara Silva and Catia Pesquita},
keywords = {Machine learning, Explainable artificial intelligence, Knowledge graph, Semantic similarity, Protein–protein interaction prediction},
abstract = {The application of artificial intelligence and machine learning methods for several biomedical applications, such as protein–protein interaction prediction, has gained significant traction in recent decades. However, explainability is a key aspect of using machine learning as a tool for scientific discovery. Explainable artificial intelligence approaches help clarify algorithmic mechanisms and identify potential bias in the data. Given the complexity of the biomedical domain, explanations should be grounded in domain knowledge which can be achieved by using ontologies and knowledge graphs. These knowledge graphs express knowledge about a domain by capturing different perspectives of the representation of real-world entities. However, the most popular way to explore knowledge graphs with machine learning is through using embeddings, which are not explainable. As an alternative, knowledge graph-based semantic similarity offers the advantage of being explainable. Additionally, similarity can be computed to capture different semantic aspects within the knowledge graph and increasing the explainability of predictive approaches. We propose a novel method to generate explainable vector representations, KGsim2vec, that uses aspect-oriented semantic similarity features to represent pairs of entities in a knowledge graph. Our approach employs a set of machine learning models, including decision trees, genetic programming, random forest and eXtreme gradient boosting, to predict relations between entities. The experiments reveal that considering multiple semantic aspects when representing the similarity between two entities improves explainability and predictive performance. KGsim2vec performs better than black-box methods based on knowledge graph embeddings or graph neural networks. Moreover, KGsim2vec produces global models that can capture biological phenomena and elucidate data biases.}
}
@article{BITSCH2021582,
title = {Open semantic modeling for smart production systems},
journal = {Procedia CIRP},
volume = {104},
pages = {582-587},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.098},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121009963},
author = {Günter Bitsch and Pascal Senjic},
keywords = {Ontology, Semantics, Modelling},
abstract = {Conventional production systems are evolving through cyber-physical systems and application-oriented approaches of AI, more and more into "smart" production systems, which are characterized among other things by a high level of communication and integration of the individual components. The exchange of information between the systems is usually only oriented towards the data content, where semantics is usually only implicitly considered. The adaptability required by external and internal influences requires the integration of new or the redesign of existing components. Through an open application-oriented ontology the information and communication exchange are extended by explicit semantic information. This enables a better integration of new and an easier reconfiguration of existing components. The developed ontology, the derived application and use of the semantic information will be evaluated by means of a practical use case.}
}
@article{PACHECOLOPEZ2023108255,
title = {Integrated synthesis, modeling, and assessment (iSMA) of waste-to-resource alternatives towards a circular economy: The case of the chemical recycling of plastic waste management},
journal = {Computers & Chemical Engineering},
volume = {175},
pages = {108255},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108255},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423001254},
author = {Adrián Pacheco-López and Edward Gómez-Reyes and Moisés Graells and Antonio Espuña and Ana Somoza-Tornos},
keywords = {Circular economy, Waste-to-resource, Ontologies, Chemical recycling, Plastic waste treatment, Integrated modeling},
abstract = {The need to transform economic models to implement a circular use of resources is crucial due to the current waste accumulation crisis. New waste-to-resource alternatives are constantly emerging to close material loops; therefore, tools are needed to identify the best synergies to upcycle waste. An approach has been developed to identify and assess waste-to-resource processing routes not currently implemented at the industrial level to valorize waste. The proposed framework consists of several interconnected modules that include ontologies for knowledge management, graph theory and short-path algorithms for the generation of paths and pre-assessment of processes, a Mixed-Integer Linear Programming (MILP) model for superstructure optimization; and the rigorous design, simulation, and optimization exclusively of those alternatives that show the best performance in previous steps. A case study for the treatment of mixed plastic waste reveals chemical recycling and the production of pyrolytic fuels as tentatively favorable options, both environmentally and economically.}
}
@article{JAHN2023,
title = {A Linked Open Data–Based Terminology to Describe Libre/Free and Open-source Software: Incremental Development Study},
journal = {JMIR Medical Informatics},
volume = {11},
year = {2023},
issn = {2291-9694},
doi = {https://doi.org/10.2196/38861},
url = {https://www.sciencedirect.com/science/article/pii/S2291969423000716},
author = {Franziska Jahn and Elske Ammenwerth and Verena Dornauer and Konrad Höffner and Michelle Bindel and Thomas Karopka and Alfred Winter},
keywords = {health informatics, ontology, free/libre open-source software, software applications, health IT, terminology},
abstract = {Background
There is a variety of libre/free and open-source software (LIFOSS) products for medicine and health care. To support health care and IT professionals select an appropriate software product for given tasks, several comparison studies and web platforms, such as Medfloss.org, are available. However, due to the lack of a uniform terminology for health informatics, ambiguous or imprecise terms are used to describe the functionalities of LIFOSS. This makes comparisons of LIFOSS difficult and may lead to inappropriate software selection decisions. Using Linked Open Data (LOD) promises to address these challenges.
Objective
We describe LIFOSS systematically with the help of the underlying Health Information Technology Ontology (HITO). We publish HITO and HITO-based software product descriptions using LOD to obtain the following benefits: (1) linking and reusing existing terminologies and (2) using Semantic Web tools for viewing and querying the LIFOSS data on the World Wide Web.
Methods
HITO was incrementally developed and implemented. First, classes for the description of software products in health IT evaluation studies were identified. Second, requirements for describing LIFOSS were elicited by interviewing domain experts. Third, to describe domain-specific functionalities of software products, existing catalogues of features and enterprise functions were analyzed and integrated into the HITO knowledge base. As a proof of concept, HITO was used to describe 25 LIFOSS products.
Results
HITO provides a defined set of classes and their relationships to describe LIFOSS in medicine and health care. With the help of linked or integrated catalogues for languages, programming languages, licenses, features, and enterprise functions, the functionalities of LIFOSS can be precisely described and compared. We publish HITO and the LIFOSS descriptions as LOD; they can be queried and viewed using different Semantic Web tools, such as Resource Description Framework (RDF) browsers, SPARQL Protocol and RDF Query Language (SPARQL) queries, and faceted searches. The advantages of providing HITO as LOD are demonstrated by practical examples.
Conclusions
HITO is a building block to achieving unambiguous communication among health IT professionals and researchers. Providing LIFOSS product information as LOD enables barrier-free and easy access to data that are often hidden in user manuals of software products or are not available at all. Efforts to establish a unique terminology of medical and health informatics should be further supported and continued.}
}
@article{RATA2025100067,
title = {Takarangi: Developing a framework for a large program of research towards decolonisation and racial justice},
journal = {First Nations Health and Wellbeing - The Lowitja Journal},
volume = {3},
pages = {100067},
year = {2025},
issn = {2949-8406},
doi = {https://doi.org/10.1016/j.fnhli.2025.100067},
url = {https://www.sciencedirect.com/science/article/pii/S2949840625000257},
author = {Arama Rata and Waikaremoana Waitoki and Nabilah Husna {Binte Abdul Rahman}},
keywords = {Methodology, Anti-racist, Decolonising, Indigenous},
abstract = {Large, multi-year research programs have the potential to yield transformative and impactful research outcomes, particularly for research programs working towards emancipatory and decolonial aims. In large, multi-year projects, there is a need for epistemological, ontological and axiological consistencies across the projects involved. However, few studies have been conducted to guide researchers working in such programs through the critical, ethical and reflexive processes needed to achieve theoretical coherence. Drawing from the works of Indigenous scholars and literature on anti-oppressive research approaches within the fields of anti-racism and decolonisation, this paper outlines how WERO (Working to End Racial Oppression), a multi-year research program based in Aotearoa New Zealand, developed the Takarangi research framework to address this large knowledge gap. The framework is based on the Takarangi, an ancient double spiral pattern prominent in Māori carving that circles inwards and outwards, visually capturing how multiple and interrelated elements are at play in the production of knowledge. The Takarangi aids researchers within teams to reflexively consider how their social positioning, ontology, axiology, ethics, epistemology and research goals shape all aspects of their research process, from community engagement to transformative action. While the Takarangi framework was built within and for a particular context of knowledge production, in detailing the processes involved in its development and implementation, this paper aimed to enable researchers working with Indigenous, racialised and minoritised communities to flexibly interpret, build upon and implement the Takarangi model to support their own projects, institutes and programs of research.}
}
@article{CASTANEDATRUJILLO2025103566,
title = {In-service English language teacher-researchers’ ideologies, realities, and practices concerning qualitative research},
journal = {System},
volume = {129},
pages = {103566},
year = {2025},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2024.103566},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X24003518},
author = {Jairo Enrique Castañeda-Trujillo and Jhon Jairo Losada-Rivas},
keywords = {English language teacher-researchers, Ideologies, Practices, Qualitative research, Realities},
abstract = {This article explores the ideologies, realities, and practices of six English language teacher-researchers concerning research. The participants of this study are all Colombian in-service teachers who currently work at different local schools and universities and had formerly participated as guests in a set of educational podcasts about qualitative research. The data collection instruments comprised individual interviews from the podcast episodes and reflective questionnaires inquiring about the participants’ experiences as researchers. The findings illustrate how the teacher-researchers struggled to understand qualitative research beyond the scientific method at the beginning of their academic lives. However, after experiencing qualitative research, teacher-researchers transformed how they conducted qualitative research and the way they viewed doing qualitative research in ELT and education. Consequently, the teacher-researchers acknowledge qualitative research as a social practice that can lead to comprehending and transforming the communities involved. We conclude that the influence of scientific method-oriented epistemologies on qualitative research in ELT underscores the need for a more critical stance, advocating for research that unveils societal injustices and inequalities. By embracing a broader vision of research, particularly in undergraduate programs, teacher-researchers can cultivate a pragmatic perspective that empowers pre-service and in-service teachers alike to contribute meaningfully to their communities through research. This entails deconstructing traditional ideologies and promoting research practices that enrich classroom teaching and foster social impact, thereby transforming teachers into active agents of change within their contexts.}
}
@article{AHMED2023102428,
title = {Recursive approach to combine expert knowledge and data-driven RSW weldability certification decision making process},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {79},
pages = {102428},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102428},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001132},
author = {Fahim Ahmed and Kyoung-Yun Kim},
keywords = {Resistance spot welding, Expert knowledge integration, Ontological knowledge, Weldability certification, Recursive approach},
abstract = {Data-driven techniques have shown promising results in the analysis and understanding of complex welding processes. Data analytics play a significant role to turn data into valuable insights to assist in the weldability certification decision-making for Resistance Spot Welding (RSW) as well. However, to successfully perform the associated data analytics, domain knowledge is essential to construct more ‘sense-making’ analytics models, as often the models cannot properly capture the nuances of the domain and do not properly indicate the relationship among the RSW concepts and parameters. Thus, machine learning models developed from rough experimental data often do not provide models meaningful and sensible to the domain expert. In this article, we employ a recursive approach between the domain experts and data-driven models so that the knowledge of the domain experts can be integrated into the weldability certification decision-making process. An ontology-based semantic knowledge framework supports this recursive communication while helping the experts to instil more confidence in the developed analytics models. The collaborative and recursive approach implemented in this study helps the domain experts to tap into their domain knowledge and form expert opinions using the formalized semantic RSW concepts and decision rules. The expert opinions are then used to learn new knowledge about the RSW domain and transform the RSW datasets by incorporating significant features that were not included in the earlier models. The transformed datasets help us to develop improved machine learning models, which in turn work as a new source of semantic knowledge, as we have discovered through our pilot implementation.}
}
@article{LIGA2023105864,
title = {Fine-tuning GPT-3 for legal rule classification},
journal = {Computer Law & Security Review},
volume = {51},
pages = {105864},
year = {2023},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2023.105864},
url = {https://www.sciencedirect.com/science/article/pii/S0267364923000742},
author = {Davide Liga and Livio Robaldo},
keywords = {Rule classification, GPT-3, AI&Law},
abstract = {In this paper, we propose a Legal Rule Classification (LRC) task using one of the most discussed language model in the field of Artificial Intelligence, namely GPT-3, a generative pretrained language model. We train and test the proposed LRC task on the GDPR encoded in LegalDocML (Palmirani and Vitali, 2011) and LegalRuleML (Athan et al., 2013), two widely used XML standards for the legal domain. We use the LegalDocML and LegalRuleML annotations provided in Robaldo et al. (2020) to fine-tuned GPT-3. While showing the ability of large language models (LLMs) to easily learn to classify legal and deontic rules even on small amount of data, we show that GPT-3 can significantly outperform previous experiments on the same task. Our work focused on a multiclass task, showing that GPT-3 is capable to recognize the difference between obligation rules, permission rules and constitutive rules with performances that overcome previous scores in LRC.}
}
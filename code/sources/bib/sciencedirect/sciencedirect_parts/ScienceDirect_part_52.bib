@article{NASSEF2025,
title = {Boosting Intrusion Detection Against DDoS Attacks Using a Feature Engineering-Based Fine-Tuned XGBoost Model},
journal = {International Journal on Semantic Web and Information Systems},
volume = {21},
number = {1},
year = {2025},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.383062},
url = {https://www.sciencedirect.com/science/article/pii/S1552628325000341},
author = {Mohammad Nassef},
keywords = {Canadian Institute for Cybersecurity–Intrusion Detection System–2017 Data Set, Correlation-Based Feature Selection, Cybersecurity, Distributed Denial-of-Service, Intrusion Detection, Network Security Laboratory–Knowledge Discovery Databases Data Set, University of New South Wales–Network Behavior15 Data Set, XGBoost},
abstract = {ABSTRACT
Network security is seriously threatened by distributed denial-of-service (DDoS) attacks, which calls for sophisticated intrusion detection systems that can rapidly identify and mitigate such threats. Despite their widespread use in intrusion detection against DDoS attacks, machine learning methods still suffer accuracy degradation due to inadequate data pre-processing and computational inefficiency. This study combined a fine-tuned extreme gradient boosting (XGBoost) model with correlation-based feature selection—for efficient feature selection—to effectively maximize detection accuracy while lowering computing overhead. Both correlation-based feature selection and XGBoost contribute to boosting the final model’s efficiency. To evaluate the proposed model, different metrics were employed over three DDoS data sets, considering both binary and multi-classification scenarios. Experimental findings demonstrate that the proposed XGBoost achieves highly competitive accuracy. For the Network Security Laboratory–Knowledge Discovery Databases data set, University of New South Wales–Network Behavior15 data set, and Canadian Institute for Cybersecurity–Intrusion Detection System–2017 data set, the model secures 0.995, 1.000, and 0.999 and 0.996, 0.885, 0.998 for binary and multi-classification, respectively, outperforming its rival models.}
}
@article{LI2025308,
title = {Trustworthy AI for human-centric smart manufacturing: A survey},
journal = {Journal of Manufacturing Systems},
volume = {78},
pages = {308-327},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524002747},
author = {Dongpeng Li and Shimin Liu and Baicun Wang and Chunyang Yu and Pai Zheng and Weihua Li},
keywords = {Industry 5.0, Human-centric smart manufacturing, Human–machine symbiosis, Trustworthy AI},
abstract = {Human-centric smart manufacturing (HCSM) envisions a symbiotic relationship between humans and machines, leveraging human capability and Artificial Intelligence (AI)’s precision and computational power to achieve mutual enhancement. Trustworthy AI (TAI) is a promising enabler in this transition, ensuring that the integration of AI technologies within manufacturing scenarios is safe, transparent, and participatory. This paper systematically reviews TAI within the context of HCSM by adopting a progressive 3-layer framework. This framework aligns with the developmental stages of HCSM and includes basic safety (protection), advancing to explainability, accountability, and uncertainty awareness (perception), and culminating in continuous updating with human involvement (participation). The review explores the role of TAI across key stages of the product lifecycle, demonstrating how TAI can empower humans and highlighting current advancements while identifying ongoing challenges. The paper concludes by discussing future directions and offering insights for developing TAI-integrated HCSM.}
}
@article{GOURA2022442,
title = {An efficient and enhancement of recent approaches to build an automated essay scoring system},
journal = {Procedia Computer Science},
volume = {215},
pages = {442-451},
year = {2022},
note = {4th International Conference on Innovative Data Communication Technology and Application},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.046},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922021172},
author = {V.M.K. Prasad Goura and M Moulesh and N Madhusudanarao and Xiao-Zhi Gao},
keywords = {Natural language processing, essay evaluation, Regression, tfidf, word2vec},
abstract = {In this paper, we present an efficient system (models) to automate the task of essay grading through a comprehensive study and enhancement of recent approaches namely multiple linear regression (MLR) and gradient boosted regression (Xg-boost) which can positively impact the operational efficiency, time consumption, labour dependency, Unbiased grading and consistency. We also illustrate the efficiency of the proposed systems(models) through a fair comparison of recent models with respect to the evaluation metric, features applied and limitations.}
}
@incollection{PANDA202599,
title = {6 - Role of knowledge graph-based methods in human—AI systems for automated driving},
editor = {Rajesh Kumar Dhanaraj and M. Nalini and Malathy Sathyamoorthy and Manar Mohaisen},
booktitle = {Knowledge Graph-Based Methods for Automated Driving},
publisher = {Elsevier},
pages = {99-118},
year = {2025},
isbn = {978-0-443-30040-0},
doi = {https://doi.org/10.1016/B978-0-443-30040-0.00006-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300400000060},
author = {Sankarsan Panda and Kiran Shrimant Kakade and M. Nalini and Sheshang Degadwala},
keywords = {Knowledge graph, AI systems, Autonomous driving, highway traffic, Information systems, Autonomous vehicles, modular autonomy, collision avoidance, Information management, Artificial intelligence, Machine learning},
abstract = {This article discusses the knowledge-driven autonomous driving technologies that are currently under development. The limits of the currently available autonomous driving systems are found in our analysis. Some of these problems are that these systems can be affected by data bias, it can be hard to deal with long-tail cases, and the results can’t be easily understood. As an alternative, knowledge-based methods that include the skills of reasoning, generalization, and learning that lasts a lifetime seem like they could be a good way to solve these issues. The study’s goal is to look into the most important parts of knowledge-driven autonomous driving, including the dataset and standard, the surroundings, and the driver agent. Many advanced artificial intelligence techniques, such as large language models, world models, neural modeling, and others, are used by these parts to make an autonomous driving system that is more complete, flexible, and smart. The goal of this study is to give ideas and suggestions for further research and real-world applications of self-driving cars. It will also organize and evaluate previous research in this area in an organized way. As well as sharing the newest and most useful open-source materials related to the subject at hand, the newest developments in knowledge-driven autonomous driving are discussed.}
}
@article{WEN2024780,
title = {Fine-grained decomposition of complex digital twin systems driven by semantic-topological-dynamic associations},
journal = {Journal of Manufacturing Systems},
volume = {77},
pages = {780-797},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524002462},
author = {Xiaojian Wen and Yicheng Sun and Shimin Liu and Jinsong Bao and Dan Zhang},
keywords = {Digital Twin, Fine-grained Decomposition, Semantic Association, Topological Association, Dynamic Association},
abstract = {Complex digital twin (DT) systems offer a robust solution for design, optimization, and operational management in industrial domains. However, in an effort to faithfully replicate the dynamic changes of the physical world with high fidelity, the excessively intricate and highly coupled system components present modeling challenges, making it difficult to accurately capture the system's dynamic characteristics and internal correlations. Particularly in scenarios involving multi-scale and multi-physics coupling, complex systems lack adequate fine-grained decomposition (FGD) methods. This results in cumbersome information exchange and consistency maintenance between models of different granularities. To address these limitations, this paper proposes a method for multi-level decomposition of complex twin models. This method constructs a FGD model for DTs by integrating three key correlation mechanisms between components: semantic association, dynamic association, and topological association. The decomposed model achieves reasonable simplification and abstraction while maintaining the accuracy of the complex system, thereby balancing computational efficiency and simulation precision. The case study validation employed a marine diesel engine piston production line to test the proposed decomposition method, verifying the effectiveness of the approach.}
}
@article{ALMAHMOUD2024122959,
title = {The effect of clustering algorithms on question answering},
journal = {Expert Systems with Applications},
volume = {243},
pages = {122959},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122959},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423034619},
author = {Rana Husni AlMahmoud and Marwah Alian},
keywords = {Question answering, Clustering, Arabic language, Similarity measures},
abstract = {Question answering (QA) is one of the essential fields in information retrieval where specific answers are provided instead of large documents. The relations among questions and answers are determined using natural language processing techniques while clustering algorithms can be helpful in improving the effectiveness of result retrieval by reducing the amount of required comparisons for a specific question or answer. In this work, we introduce a clustering-based approach for a QA system. This approach groups related questions into clusters using different clustering algorithms, specifies the appropriate answer using similarity methods between the answers and the generated clusters, and then assigns answers to their most related questions. Different clustering algorithms, such as k-means, spherical k-means, single-linkage hierarchical clustering (SLHA), unweighted pair group method with arithmetic mean (UPGMA), expectation–maximization (EM), and clustering Arabic documents based on bond energy (CADBE), are tested. The effectiveness of a clustering algorithm is investigated with respect to certain factors, including number of clusters, text representation, similarity measure between answers and clusters, and similarity measure between answers and questions in a selected cluster. In addition, a comprehensive ranking system is introduced to evaluate the performance of clustering algorithms. Evaluation is performed using the Dataset of Arabic Why Question Answering System (DAWQAS) and the Multilingual Question Answering (MLQA) dataset. Results show that CADBE achieves the highest accuracy and the first rank, followed by SLHA and UPGMA, while spherical k-means has the lowest rank. The performance of clustering algorithms for MLQA dataset is affected by its characteristics, such as short questions, long and varied answers, and diverse subject domains. Unigram and bigram intersection measures perform well in most cases. Term frequency inverse document frequency representation outperforms word embedding in DAWQAS. Overall, the experiments provide insights into the performance of clustering algorithms in QA systems.}
}
@article{REDDY2020100774,
title = {Meditative introspection promotes the First-person's science of consciousness via intuitive pathways: A hypothesis based on traditional Buddhist and contemporary Monist frameworks},
journal = {New Ideas in Psychology},
volume = {58},
pages = {100774},
year = {2020},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2019.100774},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X19301023},
author = {J. Shashi Kiran Reddy and Alfredo Pereira and Edilene {de Souza Leite} and Sisir Roy},
keywords = {Introspection, Meditation, Intuition, Consciousness, Meta-awareness, First-person, Feeling, Triple-aspect Monism},
abstract = {What forms the basis for validating any knowledge? Should it be always verified based on the objective and analytical methods that we adapt according to our progressive advancements in science or is there any other way of conceiving knowledge? This is the context where modern sophistication should embrace an ancient perspective. Recently, there have been great advancements in the science of consciousness and meditation. Meditation received much attention as a practice for wellbeing and as a tool for cognitive enhancement. Even though, hundreds of objective studies have been conducted on different practices of meditation across different traditions, there is one essential element missing in almost all of these studies: the discussion of the subjective experience of meditation. Embracing the traditional insights on meditation, we study this element by defining meditation based on the concept of introspection. In addition, we hypothesize that introspective meta-awareness associated with the non-conceptual experience of meditation may result in the conceptual understanding of natural phenomena via pathways of intuition. The proposed advancement bears implications for the ontological and epistemological basis of experiential knowledge, as well as, for developing introspection and meditation-based interventions for self and consciousness disorders.}
}
@article{LIU2023138806,
title = {Self-assembly of gelatin microcarrier-based MSC microtissues for spinal cord injury repair},
journal = {Chemical Engineering Journal},
volume = {451},
pages = {138806},
year = {2023},
issn = {1385-8947},
doi = {https://doi.org/10.1016/j.cej.2022.138806},
url = {https://www.sciencedirect.com/science/article/pii/S1385894722042875},
author = {Haifeng Liu and Xiaojun Yan and Jingwei Jiu and Jiao Jiao Li and Yuanyuan Zhang and Guishan Wang and Dijun Li and Lei Yan and Yanan Du and Bin Zhao and Bin Wang},
keywords = {Spinal cord injury, Mesenchymal stem cells, Tissue engineering, Self-assemble, Microtissues},
abstract = {Current approaches for treating spinal cord injury (SCI) are mainly based on cell transplantation. Mesenchymal stem cells (MSCs) can help slow the progression of SCI due to their trophic function. However, SCI creates a complex microenvironment that reduces cell activity and hence cellular function, ultimately resulting in poor therapeutic outcomes. To help maintain function in transplanted cells, we produced functional tissue constructs by self-assembly of MSC microtissues comprising of porous gelatin microcarriers (GM) and MSCs. These microtissues maintained cellular activity without incurring an excessive amount of apoptosis and delayed senescence in vitro. The paracrine function of MSCs also improved within microtissues, shown by the increased secretion of nerve regeneration-related factors. Microtissues were transplanted in a rat model of complete spinal cord transection, and therapeutic effects were evaluated through behavioral measurements, imaging, histology, and western blot analysis. RNA-seq of spinal cord tissues using Gene Ontology analysis further revealed that the microtissues may have induced repair in SCI through mechanisms related to neurotrophin-3 (NT-3) regulation of response mediator protein 2 (CRMP2) phosphorylation, and inhibition of inflammatory response through interleukin-17 (IL-17), Chemokine C-X-C motif Ligand 1 (CXCL1) axis. The gelatin microcarrier-based MSC microtissues we developed may be effective in providing a new treatment strategy for SCI.}
}
@article{SLEDZIESKI2021969,
title = {D-SCRIPT translates genome to phenome with sequence-based, structure-aware, genome-scale predictions of protein-protein interactions},
journal = {Cell Systems},
volume = {12},
number = {10},
pages = {969-982.e6},
year = {2021},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2021.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S2405471221003331},
author = {Samuel Sledzieski and Rohit Singh and Lenore Cowen and Bonnie Berger},
keywords = {protein-protein interaction, deep learning, language models, interpretability, genome to phenome, module detection, function prediction, cow rumen, metabolism, embedding},
abstract = {Summary
We combine advances in neural language modeling and structurally motivated design to develop D-SCRIPT, an interpretable and generalizable deep-learning model, which predicts interaction between two proteins using only their sequence and maintains high accuracy with limited training data and across species. We show that a D-SCRIPT model trained on 38,345 human PPIs enables significantly improved functional characterization of fly proteins compared with the state-of-the-art approach. Evaluating the same D-SCRIPT model on protein complexes with known 3D structure, we find that the inter-protein contact map output by D-SCRIPT has significant overlap with the ground truth. We apply D-SCRIPT to screen for PPIs in cow (Bos taurus) at a genome-wide scale and focusing on rumen physiology, identify functional gene modules related to metabolism and immune response. The predicted interactions can then be leveraged for function prediction at scale, addressing the genome-to-phenome challenge, especially in species where little data are available.}
}
@article{POOBALAN2025101667,
title = {A novel and secured email classification using deep neural network with bidirectional long short-term memory},
journal = {Computer Speech & Language},
volume = {89},
pages = {101667},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101667},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000500},
author = {A. Poobalan and K. Ganapriya and K. Kalaivani and K. Parthiban},
keywords = {Email classification, DNN-BiLSTM, AES algorithm, Rabit algorithm, Random forests (RF)},
abstract = {Email data has some characteristics that are different from other social media data, such as a large range of answers, formal language, notable length variations, high degrees of anomalies, and indirect relationships. The main goal in this research is to develop a robust and computationally efficient classifier that can distinguish between spam and regular email content. The benchmark Enron dataset, which is accessible to the public, was used for the tests. The six distinct Enron data sets we acquired were combined to generate the final seven Enron data sets. The dataset undergoes early preprocessing to remove superfluous sentences. The proposed model Bidirectional Long Short-Term Memory (BiLSTM) apply spam labels and to examine email documents for spam. On seven Enron datasets, DNN-BiLSTM performs better than other classifiers in the performance comparison in terms of accuracy. DNN-BiLSTM and convolutional neural networks demonstrated that they can classify spam with 96.39 % and 98.69 % accuracy, respectively, in comparison to other machine learning classifiers. The risks associated with cloud data management and potential security flaws are also covered in the paper. This research presents hybrid encryption as a means of protecting cloud data while preserving privacy by using the hybrid AES-Rabit encryption algorithm which is based on symmetric session key exchange.}
}
@article{VELASCOHERREJON2022105725,
title = {Challenging dominant sustainability worldviews on the energy transition: Lessons from Indigenous communities in Mexico and a plea for pluriversal technologies},
journal = {World Development},
volume = {150},
pages = {105725},
year = {2022},
issn = {0305-750X},
doi = {https://doi.org/10.1016/j.worlddev.2021.105725},
url = {https://www.sciencedirect.com/science/article/pii/S0305750X21003405},
author = {Paola Velasco-Herrejón and Thomas Bauwens and Martin {Calisto Friant}},
keywords = {Indigenous communities, Energy justice, Sustainability, Sustainable development, Ecological modernization, Energy transition, Mexico},
abstract = {Little research exists on how alternative understandings of sustainability and societal well-being, such as those developed by marginalized Indigenous populations, can enrich and possibly challenge dominant visions of sustainability anchored in Western discourses on sustainable development and ecological modernization. This paper addresses this research gap in the context of the transition towards low-carbon energy sources by addressing the following question: how do Indigenous worldviews contrast with modernist visions of sustainability in the context of the energy transition? To do so, it first builds a conceptual framework contrasting modernist and Indigenous sustainability worldviews. Second, it applies this framework to the case of wind energy developments within the territory of three Zapotec communities located in southern Mexico, with the discussion relying on 103 interviews with key stakeholders, six focus groups and participant observation. Results show that the Zapotec sustainability worldview contrasts strikingly with wind developers’ modernist propositions, which tend to reproduce the region’s past colonial arrangements in terms of cultural domination, non-recognition of Indigenous identities and disrespect for local customs. This contrast has led to many conflicts and misunderstandings around wind energy projects. The paper concludes that different conceptualizations of sustainability must be recognized to ensure an inclusive and just energy transition, and advances the concept of “pluriversal technologies” to emphasize the need for technologies that embrace ontological and epistemological diversity by being co-designed, co-produced and co-owned by the inhabitants of the socio-cultural territory in which they are embedded.}
}
@article{KLIMOV2020388,
title = {Application of Long-Short Memory Neural Networks in Semantic Search Engines Development},
journal = {Procedia Computer Science},
volume = {169},
pages = {388-392},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.234},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303574},
author = {Valentin Klimov and Anita Balandina and Artem Chernyshov},
keywords = {neural networks, Bi-LSTM, natural language processing, natural language understanding},
abstract = {This article provides an overview and description of the long-short memory approaches for the neural networks modelling and development. The authors show the possible application of these models and methods and consider its usage during the process of natural language understanding as the part of the semantic search system.}
}
@article{XUE2023101403,
title = {Proteomics reveals that cell density could affect the efficacy of drug treatment},
journal = {Biochemistry and Biophysics Reports},
volume = {33},
pages = {101403},
year = {2023},
issn = {2405-5808},
doi = {https://doi.org/10.1016/j.bbrep.2022.101403},
url = {https://www.sciencedirect.com/science/article/pii/S2405580822002035},
author = {Zhichao Xue and Jiaming Zeng and Yongshu Li and Bo Meng and Xiaoyun Gong and Yang Zhao and Xinhua Dai},
keywords = { cell Experiments, Liquid chromatography-mass spectrometry (LC-MS/MS), Cell density, Drug treatment effect},
abstract = {In vitro cell biology study plays a fundamental role in biological and drug development research, but the repeatability and accuracy of cell studies remain to be low. Various uncertainties during the cell culture process could introduce bias into drug research. In this study, we evaluate the potential effects and underlying mechanisms induced by cell number differences in the cell seeding process. Normally, drug experiments are initiated 24 h after cell seeding, and the difference in the cell number at the time of inoculation leads to the difference in cell confluence (cell density) when drug research is conducted. While cell confluence is closely related to intercellular communication, surface protein interaction, cell autocrine as well as paracrine protein expression of cells, it might have a potential impact on the effect of biological studies such as drug treatment. This study used proteomics technology to comprehensively explore the different protein expression patterns between cells with different confluences. Due to the high sensitivity and high throughput of liquid chromatography-mass spectrometry (LC-MS/MS) detection, it was hired to evaluate the protein expression differences of Hep3B cells with 3 different confluences (30%, 50%, and 70%). The differential expressed proteins were analyzed by the Reactome pathway and the Gene Ontology (GO) pathway. Significant differences were identified across three confluences in terms of the number of proteins identified, the protein expression pattern, and the expression level of certain KEGG pathways. We found that those proteins involved in the cell cycle pathway were differently expressed: the higher the cell confluence, the higher these proteins expressed. A cell cycle inhibitor palbociclib was selected to further verify this observation. Palbociclib in the same dose was applied to cells with different confluence, the results indicated that the growth inhibition effect of palbociclib increases along with the increasing trend of cell cycle protein expression. The result indicated that cell density did influence the effect of drug treatment. Furthermore, three other drugs, cisplatin, paclitaxel, and imatinib, were used to treat the three liver cancer cell lines Hep3B, SUN387, and MHCC97, and a similar observation was obtained that drug effect would be different when the cell confluences were different. Therefore, selecting an appropriate number of cells for plating is vitally important at the beginning of a drug study.}
}
@article{RAMOGLOU2021106090,
title = {Knowable opportunities in an unknowable future? On the epistemological paradoxes of entrepreneurship theory},
journal = {Journal of Business Venturing},
volume = {36},
number = {2},
pages = {106090},
year = {2021},
issn = {0883-9026},
doi = {https://doi.org/10.1016/j.jbusvent.2020.106090},
url = {https://www.sciencedirect.com/science/article/pii/S0883902620306984},
author = {Stratos Ramoglou},
keywords = {Knightian uncertainty, Knowledge problems, Linguistic philosophy, Ontology/epistemology, Principle of ineliminable unknowability, Opportunity Ingredients, Opportunity discovery, Opportunity actualization},
abstract = {It is often assumed that opportunities can be known ex ante in spite of the fact that the future is simultaneously acknowledged to be unknowable. This paper endeavors to resolve this epistemological paradox in a manner that facilitates a more meaningful treatment of the knowledge problems of entrepreneurship. To this end, we draw from linguistic philosophy and undertake three interrelated analytical steps at the conceptual foundations of entrepreneurship theory. First, we clarify subtle logical aspects underlying the meaningful use of the word “uncertainty” qua unknowability. When properly used, uncertainty reflects the epistemological assessment that enterprising actors may only believe – not know – that new ventures can succeed. When incorrectly used, uncertainty is misrepresented as an obstacle that can be overcome by some and not others. Second, we explain how prevalent linguistic practices (“opportunity discovery”, “opportunity recognition”) lie at the root of epistemological tensions in opportunity theory. They act as a distorting mirror that trivializes the unknowability of the future and nourishes impressions of mental agencies allowing entrepreneurs to know the unknowable. Third, we urge a more nuanced understanding of the knowledge problems of entrepreneurship. On the one hand, we submit that opportunities are ineliminably unknowable. On the other hand, however, we argue that there exist knowable Opportunity-Ingredients (OIs) whose knowability varies across contexts. These analytical developments further contribute to the ongoing “opportunity wars”, strengthen the epistemological foundations of opportunity-actualization, improve construct clarity, and reveal new possibilities for research.}
}
@article{LOUREIRO2022103661,
title = {LMMS reloaded: Transformer-based sense embeddings for disambiguation and beyond},
journal = {Artificial Intelligence},
volume = {305},
pages = {103661},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103661},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222000017},
author = {Daniel Loureiro and Alípio {Mário Jorge} and Jose Camacho-Collados},
keywords = {Semantic representations, Neural language models},
abstract = {Distributional semantics based on neural approaches is a cornerstone of Natural Language Processing, with surprising connections to human meaning representation as well. Recent Transformer-based Language Models have proven capable of producing contextual word representations that reliably convey sense-specific information, simply as a product of self-supervision. Prior work has shown that these contextual representations can be used to accurately represent large sense inventories as sense embeddings, to the extent that a distance-based solution to Word Sense Disambiguation (WSD) tasks outperforms models trained specifically for the task. Still, there remains much to understand on how to use these Neural Language Models (NLMs) to produce sense embeddings that can better harness each NLM's meaning representation abilities. In this work we introduce a more principled approach to leverage information from all layers of NLMs, informed by a probing analysis on 14 NLM variants. We also emphasize the versatility of these sense embeddings in contrast to task-specific models, applying them on several sense-related tasks, besides WSD, while demonstrating improved performance using our proposed approach over prior work focused on sense embeddings. Finally, we discuss unexpected findings regarding layer and model performance variations, and potential applications for downstream tasks.}
}
@article{MA2024e27368,
title = {Bioinformatics analysis and clinical significance of NRP-1 in triple-negative breast cancer},
journal = {Heliyon},
volume = {10},
number = {5},
pages = {e27368},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e27368},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024033991},
author = {Xiao Ma and Haonan Liu and Congcong Shi and Yang Zhao and Hongmei Wang and Zhengxiang Han},
keywords = {Biomarkers, Immune infiltrates, NRP-1, Therapeutic targets, Triple-negative breast cancer, tumor purity},
abstract = {Purpose
This study aimed to investigate the diagnostic and prognostic values of neuropilin-1 (NRP-1) in triple-negative breast cancer (TNBC) and analyze its immune function in the tumor microenvironment.
Methods
Based on The Cancer Genome Atlas (TCGA), Gene Expression Omnibus, Genotype Tissue Expression, Immune Cell Abundance Identifier (ImmuCellAI), Reactome, and Genomics of Drug Sensitivity in Cancer databases, the cancer tissues from 50 patients with TNBC and corresponding adjacent noncancerous tissues from 10 patients (tissue microarrays were purchased from Shanghai Xinchao Biotechnology Co., Ltd.) were collected for validation. Bioinformatics combined with immunohistochemistry was used to analyze the relationship among NRP-1 expression, prognosis, tumor immune cell infiltration, immune genes, and drug resistance so as to investigate the role of NRP-1 in the development of TNBC.
Results
A significant difference in NRP-1 gene expression was found between the cancerous and noncancerous tissues (p-value < 0.05); NRP-1 expression was high in carcinoma. No significant correlation was found between NRP-1 protein expression levels and each stage in the TCGA database. Prognostic expression survival analysis showed that the survival probability of patients with high NRP-1 expression was significantly lower than that of patients with low NRP-1 expression (p-value < 0.05), suggesting that the gene might be a pro-oncogene. The data from 50 clinical samples also confirmed that the NRP-1 expression was significantly higher in triple-negative breast cancer (TNBC) tissues than in adjacent noncancerous tissues. The NRP-1 expression significantly correlated with the tumor diameter and pathological grade (p-value < 0.05), but not with age, stage, and ki67 (p-value > 0.05). The Kaplan–Meier survival curves suggested that the median overall survival was significantly shorter in patients with high NRP-1 expression than in those with low NRP-1 expression (13.6 months vs 15.2 months, p-value < 0.05). The 300 genes most significantly positively associated with this gene were selected for Gene Ontology (including Biological Process, Molecular Function, and Cellular Component groups) and Kyoto Encyclopedia of Genes and Genomics enrichment analysis. The findings showed that NRP-1 was involved in immune regulation in TNBC. In addition, the NRP-1 expression in TNBC positively correlated with a variety of immune cells and checkpoints.
Conclusion
NRP-1 can be used as a potential biomarker and therapeutic target in TNBC.}
}
@article{SCHMIDTKE2018805,
title = {A Canvas for Thought},
journal = {Procedia Computer Science},
volume = {145},
pages = {805-812},
year = {2018},
note = {Postproceedings of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2018 (Ninth Annual Meeting of the BICA Society), held August 22-24, 2018 in Prague, Czech Republic},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.11.027},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918323135},
author = {Hedda R. Schmidtke},
keywords = {Common Model of Cognition, Mental Imagery, Mental Maps, Logic-Perception Interface},
abstract = {A Common Model of Cognition needs to be open so as to encompass a broad range of approaches and provide a firm fundament to which researchers who study one particular aspect of cognition at more detail can refer. It should also be open to incorporate new findings even, or in particular, if they come from an unconventional angle. A recent cognitive systems experiment has surprisingly shown that there is a fundamental link between logic and analogous representations. This paper discusses a field of functionality called here the Canvas, comprising or overlapping what is called in other models the Visual Buffer, the Visuospatial Sketchpad, the Spatial Visual System, or Imagery, leveraging this recent result. The paper suggests that a module like the Canvas is a fundamental component for any model of cognition and of crucial importance to understanding the origins of higher cognitive abilities, such as language or geometry. The paper also demonstrates, how a fine-level research result benefits from the Common Model as a common context within which fine-level research can be embedded, relieving the author from having to provide a full cognitive architecture beyond the aspect studied.}
}
@article{HEISE2025103649,
title = {A graph-based systems-of-systems architecture enabling multi-scale Digital Twins for maintaining road infrastructure},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103649},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103649},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005427},
author = {Ina Heise and Sebastian Esser and André Borrmann},
keywords = {Digital Twin, Road infrastructure, Systems-of-systems, Spatial relations, Labeled Property Graphs},
abstract = {Road infrastructure constitutes a complex system characterized by multiple interacting subsystems. A comprehensive understanding of the correlations and dependencies among various road infrastructure elements is essential for enhancing infrastructure management and maintenance by providing a robust foundation for decision support. Digital Twins (DT) are recognized as effective tools for facilitating such decision-making. However, the development of comprehensive DTs considering road infrastructure in its entirety is still in its early stages. Hence, this paper focuses on the conceptualization of a digital representation of road infrastructure that enables the evaluation of relationships between the various heterogeneous subsystems. To accomplish this, we apply the systems-of-systems principle to road infrastructure. At its core, Labeled Property Graphs (LPG) are employed to capture intra-subsystem relationships and inter-system linkages, facilitating a holistic representation of interactions. Furthermore, we acknowledge the current organizational status of distributed responsibilities resulting in distributed data storage and maintenance by using the concept of federated databases. The presented approach enables multi-scale evaluations of relations among road infrastructure elements while preserving the system’s scalability and the distributed management of infrastructure data. Thus, previously separate data sets can be evaluated in relation to each other on a big scale. Doing so, the presented concept provides a foundation for extensive correlation studies between different heterogeneous infrastructure datasets. The concept is validated by applying it to a large-scale real-world data set stemming from multiple Bavarian road authorities, transferring into the proposed graph structure, and demonstrating the gained capabilities through cross-domain queries and analysis.}
}
@article{MESSAOUDI20222628,
title = {A Deep Learning Model for Opinion mining in Twitter Combining Text and Emojis},
journal = {Procedia Computer Science},
volume = {207},
pages = {2628-2637},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.321},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922012108},
author = {Chaima Messaoudi and Zahia Guessoum and Lotfi ben Romdhane},
keywords = {Opinion mining, emojis, machine learning, social networks, Twitter},
abstract = {Several approaches have been proposed to study opinions on Social Network Sites (SNS). Unfortunately, those works are not topic-sensitive and do not investigate the impact of emojis on text-based classification. In this paper, we propose a novel approach to predict the users’ opinions expressed through textual tweets and emojis. Thus, we construct an emoji sentiment lexicon. Then, we extract opinions from the text before considering both the text and emojis to see how they enhance the expression of opinions in SNS discussions. We conduct a set of benchmarks using several well-known machine learning algorithms, leading to an accuracy of 83, 7%.}
}
@article{FERNANDES2022101965,
title = {Extracting value from Brazilian Court decisions},
journal = {Information Systems},
volume = {106},
pages = {101965},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101965},
url = {https://www.sciencedirect.com/science/article/pii/S030643792100154X},
author = {William Paulo Ducca Fernandes and Isabella Zalcberg Frajhof and Guilherme da Franca Couto Fernandes {de Almeida} and Ariane Moraes Bueno Rodrigues and Simone Diniz Junqueira Barbosa and Carlos Nelson Konder and Rafael Barbosa Nasser and Gustavo Robichez {de Carvalho} and Hélio Côrtes Vieira Lopes},
keywords = {Natural language processing, Deep learning, Recurrent neural networks, Long short-term memory, Machine learning, Conditional random fields, Information extraction, Law},
abstract = {We propose a methodology to extract value from Brazilian Court decisions to support judges and lawyers in their decision-making. We instantiate our methodology in one information system we have developed. Such system (i) extracts plaintiff’s legal claims and each specific provision on legal opinions enacted by lower and Appellate Courts, and (ii) connects each legal claim with the corresponding judicial provision. The information system presents the results through visualizations. Information Extraction for legal texts has been previously approached in the literature for different languages, using different methods. Our proposal is different from previous work, since our corpora comprise Brazilian lower and Appellate Court decisions, in which we look for a set of plaintiff’s legal claims and judicial provisions commonly judged by the Court. We use the following methods to tackle the information extraction tasks: Bidirectional Long Short-Term Memory network; Conditional Random Fields; and a combination of Bidirectional Long Short-Term Memory network and Conditional Random Fields. In addition to the well-known distributed representation of words in word embeddings, we use character-level representation of words in character embeddings. We have built three corpora – Kauane Insurance Report, Kauane Insurance Lower, and Kauane Insurance Upper – to train and evaluate the system, using public data from the State Court of Rio de Janeiro. Our methods achieved good quality for Kauane Insurance Lower and Kauane Insurance Upper, and promising results for Kauane Insurance Report.}
}
@article{GUETTERMAN2018,
title = {Augmenting Qualitative Text Analysis with Natural Language Processing: Methodological Study},
journal = {Journal of Medical Internet Research},
volume = {20},
number = {6},
year = {2018},
issn = {1438-8871},
doi = {https://doi.org/10.2196/jmir.9702},
url = {https://www.sciencedirect.com/science/article/pii/S1438887118002510},
author = {Timothy C Guetterman and Tammy Chang and Melissa DeJonckheere and Tanmay Basu and Elizabeth Scruggs and VG Vinod Vydiswaran},
keywords = {qualitative research, natural language processing, text data, methodology, coding},
abstract = {Background
Qualitative research methods are increasingly being used across disciplines because of their ability to help investigators understand the perspectives of participants in their own words. However, qualitative analysis is a laborious and resource-intensive process. To achieve depth, researchers are limited to smaller sample sizes when analyzing text data. One potential method to address this concern is natural language processing (NLP). Qualitative text analysis involves researchers reading data, assigning code labels, and iteratively developing findings; NLP has the potential to automate part of this process. Unfortunately, little methodological research has been done to compare automatic coding using NLP techniques and qualitative coding, which is critical to establish the viability of NLP as a useful, rigorous analysis procedure.
Objective
The purpose of this study was to compare the utility of a traditional qualitative text analysis, an NLP analysis, and an augmented approach that combines qualitative and NLP methods.
Methods
We conducted a 2-arm cross-over experiment to compare qualitative and NLP approaches to analyze data generated through 2 text (short message service) message survey questions, one about prescription drugs and the other about police interactions, sent to youth aged 14-24 years. We randomly assigned a question to each of the 2 experienced qualitative analysis teams for independent coding and analysis before receiving NLP results. A third team separately conducted NLP analysis of the same 2 questions. We examined the results of our analyses to compare (1) the similarity of findings derived, (2) the quality of inferences generated, and (3) the time spent in analysis.
Results
The qualitative-only analysis for the drug question (n=58) yielded 4 major findings, whereas the NLP analysis yielded 3 findings that missed contextual elements. The qualitative and NLP-augmented analysis was the most comprehensive. For the police question (n=68), the qualitative-only analysis yielded 4 primary findings and the NLP-only analysis yielded 4 slightly different findings. Again, the augmented qualitative and NLP analysis was the most comprehensive and produced the highest quality inferences, increasing our depth of understanding (ie, details and frequencies). In terms of time, the NLP-only approach was quicker than the qualitative-only approach for the drug (120 vs 270 minutes) and police (40 vs 270 minutes) questions. An approach beginning with qualitative analysis followed by qualitative- or NLP-augmented analysis took longer time than that beginning with NLP for both drug (450 vs 240 minutes) and police (390 vs 220 minutes) questions.
Conclusions
NLP provides both a foundation to code qualitatively more quickly and a method to validate qualitative findings. NLP methods were able to identify major themes found with traditional qualitative analysis but were not useful in identifying nuances. Traditional qualitative text analysis added important details and context.}
}
@article{SPREAFICO2022889,
title = {Can TRIZ (Theory of Inventive Problem Solving) strategies improve material substitution in eco-design?},
journal = {Sustainable Production and Consumption},
volume = {30},
pages = {889-915},
year = {2022},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2022.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S2352550922000112},
author = {Christian Spreafico},
keywords = {Material substitution, Eco-design, TRIZ, Life cycle assessment (LCA)},
abstract = {The material substitution (MS) is a very effective eco-design strategy for reducing the environmental impacts in a product, albeit its application can be hindered because of the other product requirements, e.g. mechanical strengths, aesthetics, etc. However, approaches that explicitly support a strategic MS in problem-solving are missing in the literature. This paper compares the reduction of the environmental impacts in 153 case studies of comparative life cycle assessment (LCA), extracted from 113 scientific articles, associated with generic MS or strategic MS according to TRIZ (Russian acronym for “Theory of Inventive Problem Solving”) strategies. The association was manually performed by following a structured and step-divided procedure, where the case studies are reformulated and compared to the TRIZ strategies, by exploiting the analogy of some common ontological terms between TRIZ and design. The obtained results showed how TRIZ can be used to perform a more rational and strategic MS to meet both environmental sustainability and other product requirements, better than generic MS. The impact reduction is instead greater in all impact categories (+21% on average), whether the introduced materials are synthetic (+19% on average), natural (+13% on average), and recycled materials (+18% on average). Furthermore, the associations between the solutions that guarantee the greatest reductions in environmental impacts and the revised TRIZ strategies for MS have been determined in relation with application fields, types of products and materials. Compared to other contributions in the literature, the main novelties of this study are: the intersection between TRIZ and MS and its environmental evaluation, quantitative and enlarged to different standard categories and based on a wider and heterogeneous set of case studies. In conclusions, this study associated more quantitative environmental advantages to the provided set of revised TRIZ strategies for material substitution than generic material substitution on the basis of analogies with historical cases that inspired their formulation.}
}
@article{QIU2024106070,
title = {An interoperable software system to store, associate, visualize, and publish global open science data of earth surface system},
journal = {Environmental Modelling & Software},
volume = {178},
pages = {106070},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106070},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224001312},
author = {Qinjun Qiu and Jiandong Liu and Mengqi Hao and Weijie Li and Yang Wang and Zhong Xie and Liufeng Tao},
keywords = {Earth surface system, Open science data, Data association network, Open-source software, Scientific data recommendations},
abstract = {Multi-source heterogeneous, multi-modal, and multi-type open scientific data (e.g., thematic sharing sites, metadata, journal articles, etc.) on earth surface systems (EES) provide important data sources for knowledge mining, discovery, and accurate recommendations, and also pose increasing challenges, resulting in the need to develop appropriate tools to address these challenges and support decision-making. This paper constructs an interoperable software system to store, visualize, and publish open science data of ESS. Utilizing an open scientific data catalogue repository encompassing EES information as foundational input and employing an integrated modeling methodology, this system endeavors to synthesize heterogeneous surface data of diverse linguistic, sourced, and typological origins. The objective is to facilitate multidimensional data retrieval and precise data auto-recommendation, thereby fostering the dissemination of scientific data and facilitating value-added services within EES domain. The tool may be used by stakeholders including researchers, data analysts, policymakers and national authorities to support decision-making on questions ranging from locating the location of open data related to the topic, to discovering high-quality data, selecting the data with the better overall evaluation. Along with a description of the system/platform design process, its structure, and the constituent models, key results are presented relating to the user interface, and several application examples. Software systems can help modelers to use the best features of a single software tool to answer open scientific data-related questions that seek to discovery, use, comparison or synthesis within or across topics of ESS.}
}
@article{SMAJEVIC2024103007,
title = {CM2KGcloud – An open web-based platform to transform conceptual models into knowledge graphs},
journal = {Science of Computer Programming},
volume = {231},
pages = {103007},
year = {2024},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2023.103007},
url = {https://www.sciencedirect.com/science/article/pii/S0167642323000898},
author = {Muhamed Smajevic and Syed Juned Ali and Dominik Bork},
keywords = {Model transformation, Conceptual modeling, Cloud platform, Knowledge graph, Artificial intelligence, Model-driven engineering},
abstract = {Semantic processing of conceptual models is a focus of research for several years, bridging the disciplines of knowledge-based systems, conceptual modeling, and model-driven software engineering. With Knowledge Graphs, this research area gained momentum. In this paper, we introduce CM2KGcloud, a generic and extensible Web-based platform for transforming conceptual models into Knowledge Graphs. The platform can work on models created by state-of-the-art metamodeling platforms (e.g., EMF, Papyrus, ADOxx) and transforms models created with them into standardized Knowledge Graph representations like GraphML, RDF, and OWL. CM2KGcloud can be used as a service and can be integrated into software systems by its exposed API.}
}
@article{CASHEEKAR2024100632,
title = {A contemporary review on chatbots, AI-powered virtual conversational agents, ChatGPT: Applications, open challenges and future research directions},
journal = {Computer Science Review},
volume = {52},
pages = {100632},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100632},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000169},
author = {Avyay Casheekar and Archit Lahiri and Kanishk Rath and Kaushik Sanjay Prabhakar and Kathiravan Srinivasan},
keywords = {Computational intelligence, Artificial intelligence, Chatbots, Conversational agents, ChatGPT},
abstract = {This review paper offers an in-depth analysis of AI-powered virtual conversational agents, specifically focusing on OpenAI’s ChatGPT. The main contributions of this paper are threefold: (i) an exhaustive review of prior literature on chatbots, (ii) a background of chatbots including existing chatbots/conversational agents like ChatGPT, and (iii) a UI/UX design analysis of prominent chatbots. Another contribution of this review is the comprehensive exploration of ChatGPT’s applications across a multitude of sectors, including education, business, public health, and more. This review highlights the transformative potential of ChatGPT, despite the challenges it faces such as hallucination, biases in training data, jailbreaks, and anonymous data collection. The review paper then presents a comprehensive survey of prior literature reviews on chatbots, identifying gaps in the prior work and highlighting the need for further research in areas such as chatbot evaluation, user experience, and ethical considerations. The paper also provides a detailed analysis of the UI/UX design of prominent chatbots, including their conversational flow, visual design, and user engagement. The paper also identifies key future research directions, including mitigating language bias, enhancing ethical decision-making capabilities, improving user interaction and personalization, and developing robust governance frameworks. By solving these issues, we can ensure that AI chatbots like ChatGPT are used responsibly and effectively across a broad variety of applications. This review will be a valuable resource for researchers and practitioners in understanding the current state and future potential of AI chatbots like ChatGPT.}
}
@article{CHANGYONG2024103735,
title = {Synchronous integration method of mechatronic system design, geometric design, and simulation based on SysML},
journal = {Computer-Aided Design},
volume = {174},
pages = {103735},
year = {2024},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2024.103735},
url = {https://www.sciencedirect.com/science/article/pii/S0010448524000629},
author = {Chu Changyong and Zhang Chunjia and Yin Chengfang},
keywords = {SysML, CAD, Simulink, M2T, TTRS, Integrated design},
abstract = {The benefits of integrated design using the Model Based System Engineering (MBSE) approach in the design process of mechatronic systems have gradually become apparent. The automatic generation of simulation models and geometric models based on System Modeling Languages (SysML) models enables system engineers to swiftly analyze and simulate system performance, visually depict design outcomes, and expedite the product development process. Due to the current system modeling's lack of model integration and geometric design functions, this paper proposes an integrated design and simulation method for mechatronic systems that can carry out complete model synchronization and verification, rapid geometric solution generation, and visual representation. Furthermore, a corresponding model synchronization integration framework is established. This framework primarily encompasses system de-sign, system simulation, and geometric design, with its model integration method being model transformation and model synchronization. The paper concludes with an example of the design process of a quadruped robot to validate the framework and its supported methods, providing a reference for other system design and integration endeavors.}
}
@article{ALKHALIL2025529,
title = {A Neural Network Framework for Classifying Educational Content Aligned with Learning Outcomes},
journal = {Procedia Computer Science},
volume = {257},
pages = {529-536},
year = {2025},
note = {The 16th International Conference on Ambient Systems, Networks and Technologies Networks (ANT)/ the 8th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.03.068},
url = {https://www.sciencedirect.com/science/article/pii/S187705092500804X},
author = {Adel Alkhalil},
keywords = {Neural Networks, Text Classification, Educational Content, Curriculum Mapping, Transformer Models, Learning Outcomes},
abstract = {Rapid growth of digital educational content necessitates efficient and accurate methods for organizing and mapping resources to ensure well-alignment with targeted learning outcomes, academic standards, and competency frameworks. Traditional text classification approaches, including rule-based and classical machine learning techniques, often fail to address the semantic diversity and scalability demands of modern educational systems. This study investigates the application of neural networks for text classification to automate the mapping of educational content into predefined categories. Leveraging state-of-the-art architectures such as Long Short-Term Memory (LSTM) networks, and transformers like BERT, we present an architecture of a systematic Classification of educational materials. We discuss the implications of this work for adaptive learning environments, emphasizing the potential of neural networks to enhance the efficiency and scalability of content mapping. This study contributes to the growing body of research in artificial intelligence for education and sets the stage for further exploration into multilingual and domain-specific content classification methods.}
}
@article{LIANG2025224,
title = {Screening of potential markers for vitiligo based on bioinformatics and LASSO regression and prediction of Chinese medicine},
journal = {Journal of Holistic Integrative Pharmacy},
volume = {6},
number = {2},
pages = {224-234},
year = {2025},
issn = {2707-3688},
doi = {https://doi.org/10.1016/j.jhip.2025.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S270736882500024X},
author = {Wei Liang and Minni Huang and Yue Sun and Shuyu Guan},
keywords = {Bioinformatics, Vitiligo, Immune infiltration, LASSO regression analysis, Weighted gene co-expression network analysis},
abstract = {Objective
This study aimed to use bioinformatics techniques to screen biomarkers related to vitiligo.
Methods
Firstly, the gene expression profiles of vitiligo were obtained from the GEO database, and differentially expressed genes (DEGs) were identified. Subsequently, Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analyses were performed on these differentially expressed genes. Through weighted gene co-expression network analysis (WGCNA), the core genes in the module most closely related to vitiligo were identified, and an intersection analysis was conducted with the DEGs. Next, a protein-protein interaction (PPI) network analysis was carried out on the intersection genes. Key genes were further screened using Cytohubba and least absolute shrinkage and selection operator (LASSO) regression analysis, and the roles of these key genes in immune cell infiltration were explored through single-sample gene set enrichment analysis (ssGSEA). In addition, the diagnostic effectiveness of the key genes was verified by the receiver operating characteristic (ROC) curve, and drugs related to the key genes were predicted using databases. Finally, the expression levels of these key genes were verified through reverse transcription quantitative polymerase chain reaction (RT-qPCR) and Western blot experiments.
Results
A total of 667 DEGs were identified, and the enrichment analysis mainly involved cell adhesion molecules, T cell receptor signaling pathway, etc. Nineteen core genes were screened out from the five algorithms of Cytohubba, and LASSO regression analysis further determined four key genes (IL7R, GZMH, CD3G, and UBD). Immune cell infiltration analysis showed that these four key genes had high expression in immune cells. The prediction results of traditional Chinese medicine showed that 15 traditional Chinese medicines were related to the key genes. The results of RT-qPCR showed that the expressions of IL7R, GZMH, and CD3G were significantly upregulated (∗P ​< ​0.05, ∗∗P ​< ​0.01, ∗∗∗P ​< ​0.001), and Western blot showed obvious expressions of IL7R, GZMH, CD3G, and UBD.
Conclusion
This study used bioinformatics methods to explore the biomarkers of vitiligo, and verified the potential of IL7R, GZMH, and CD3G as novel candidate genes through in vitro experiments. These genes may become new targets for the diagnosis, prognosis, and treatment of vitiligo.}
}
@article{BI2025861,
title = {Syntax-Enhanced Entity Relation Extraction with Complex Knowledge},
journal = {Computers, Materials and Continua},
volume = {83},
number = {1},
pages = {861-876},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2025.060517},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825003005},
author = {Mingwen Bi and Hefei Chen and Zhenghong Yang},
keywords = {Entity relation extraction, complex knowledge, syntax-enhanced, semantic interaction, pre-trained BERT},
abstract = {Entity relation extraction, a fundamental and essential task in natural language processing (NLP), has garnered significant attention over an extended period., aiming to extract the core of semantic knowledge from unstructured text, i.e., entities and the relations between them. At present, the main dilemma of Chinese entity relation extraction research lies in nested entities, relation overlap, and lack of entity relation interaction. This dilemma is particularly prominent in complex knowledge extraction tasks with high-density knowledge, imprecise syntactic structure, and lack of semantic roles. To address these challenges, this paper presents an innovative “character-level” Chinese part-of-speech (CN-POS) tagging approach and incorporates part-of-speech (POS) information into the pre-trained model, aiming to improve its semantic understanding and syntactic information processing capabilities. Additionally, A relation reference filling mechanism (RF) is proposed to enhance the semantic interaction between relations and entities, utilize relations to guide entity modeling, improve the boundary prediction ability of entity models for nested entity phenomena, and increase the cascading accuracy of entity-relation triples. Meanwhile, the “Queue” sub-task connection strategy is adopted to alleviate triplet cascading errors caused by overlapping relations, and a Syntax-enhanced entity relation extraction model (SE-RE) is constructed. The model showed excellent performance on the self-constructed E-commerce Product Information dataset (EPI) in this article. The results demonstrate that integrating POS enhancement into the pre-trained encoding model significantly boosts the performance of entity relation extraction models compared to baseline methods. Specifically, the F1-score fluctuation in subtasks caused by error accumulation was reduced by 3.21%, while the F1-score for entity-relation triplet extraction improved by 1.91%.}
}
@article{FUENTESLOPEZ2023161,
title = {Biomaterials text mining: A hands-on comparative study of methods on polydioxanone biocompatibility},
journal = {New Biotechnology},
volume = {77},
pages = {161-175},
year = {2023},
issn = {1871-6784},
doi = {https://doi.org/10.1016/j.nbt.2023.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1871678423000444},
author = {Carla V. Fuenteslópez and Austin McKitrick and Javier Corvi and Maria-Pau Ginebra and Osnat Hakimi},
keywords = {Biomaterials, Text mining, Polydioxanone, Biocompatibility, Information extraction},
abstract = {Scientific information extraction is fundamental for research and innovation, but is currently mostly a manual, time-consuming process. Text Mining tools (TMTs) enable automated, accurate and quick information extraction from text, but there is little precedent of their use in the biomaterials field. Here, we compare the ability of various TMTs to extract useful information from biomaterials abstracts. Focusing on the biocompatibility of polydioxanone, a biodegradable polymer for which there are relatively few scientific publications, we tested several tools ranging from machine learning approaches and statistical text analysis to MeSH indexing and domain-specific semantic tools for Named Entity Recognition. We also evaluated their output alongside a manual review of systematic reviews and meta-analyses. The findings show that TMTs can be highly efficient and powerful for mapping biomaterials texts and rapidly yield up-to-date information. Here, TMTs enable one to identify dominating themes, see the evolution of specific terms and topics, and learn about key medical applications in biomaterials literature over the years. The analysis also shows that ambiguity around biomaterials nomenclature is a significant challenge in mining biomedical literature that is yet to be tackled. This research showcases the potential value of using Natural Language Processing and domain-specific tools to extract and organize biomaterials data.}
}
@article{ZHENG2019141,
title = {Interactive natural language question answering over knowledge graphs},
journal = {Information Sciences},
volume = {481},
pages = {141-159},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518309848},
author = {Weiguo Zheng and Hong Cheng and Jeffrey Xu Yu and Lei Zou and Kangfei Zhao},
keywords = {Interactive query, Natural language question and answering, Knowledge graph, Question understanding},
abstract = {As many real-world data are constructed into knowledge graphs, providing effective and convenient query techniques for end users is an urgent and important task. Although structured query languages, such as SPARQL, offer a powerful expression ability to query RDF datasets, they are difficult to use. Keywords are simple but have a very limited expression ability. Natural language question (NLQ) is promising for querying knowledge graphs. A huge challenge is how to understand the question clearly so as to translate the unstructured question into a structured query. In this paper, we present a data + oracle approach to answer NLQs over knowledge graphs. We let users verify the ambiguities during the query understanding. To reduce the interaction cost, we formalize an interaction problem and design an efficient strategy to solve the problem. We also propose a query prefetching technique by exploiting the latency in the interactions with users. Moreover, we devise a hybrid approach that incorporates NLP-based, data-driven, and interaction techniques together to complete the question understanding. Extensive experiments over real datasets demonstrate that our proposed approach is effective as it outperforms state-of-the-art methods significantly.}
}
@article{COSTA2025102886,
title = {Unifying clustering and representation learning for unsupervised text analysis: A Bayesian knowledge-enhanced approach},
journal = {Information Fusion},
volume = {117},
pages = {102886},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102886},
url = {https://www.sciencedirect.com/science/article/pii/S156625352400664X},
author = {Gianni Costa and Riccardo Ortale},
keywords = {Text analysis, Text clustering, Text representation learning, Knowledge graph (embeddings), Word embeddings},
abstract = {Clustering and representation learning are foundational tasks in natural language processing and text mining, aiming to structure and semantically encode text documents, respectively. While clustering organizes documents into cohesive groups based on similarity, representation learning generates low-dimensional embeddings that capture the nuances of document semantics. This paper presents a novel knowledge-enhanced approach that synergistically integrates these two tasks, improving performance in both areas. Our method employs a latent-factor Bayesian generative model, named MINING (docuMent clusterINg and embeddING), along with a specialized collapsed Gibbs sampling algorithm. We enrich the learned representations by incorporating external knowledge from word and entity embeddings, enhancing their semantic and syntactic richness. Our approach treats clustering and representation learning as interdependent tasks, allowing them to inform and refine one another. Extensive experiments on benchmark datasets demonstrate that our integrated approach outperforms traditional methods that carry out clustering and representation learning as separate tasks.}
}
@article{ALONSOGARRIDO2018437,
title = {Enniatin B induces expression changes in the electron transport chain pathway related genes in lymphoblastic T-cell line},
journal = {Food and Chemical Toxicology},
volume = {121},
pages = {437-443},
year = {2018},
issn = {0278-6915},
doi = {https://doi.org/10.1016/j.fct.2018.09.018},
url = {https://www.sciencedirect.com/science/article/pii/S0278691518306628},
author = {M. Alonso-Garrido and L. Escrivá and L. Manyes and G. Font},
keywords = {Transcriptomics, Jurkat, In vitro, RT-qPCR, Gene ontology},
abstract = {Enniatin B is a ionophoric and lipophilic mycotoxin which reaches the bloodstream and has the ability to penetrate into cellular membranes. The purpose of this study was to reveal changes in the gene expression profile caused by enniatin B in human Jurkat lymphoblastic T-cells after 24 h of exposure at 1.5, 3 and 5 μM by next generation sequencing. It was found that up to 27% of human genome expression levels were significantly altered (5750 genes for both down-regulation and up-regulation). In the three enniatin B concentrations studied 245 differentially expressed genes were found to be overlapped, 83 were down and 162 up-regulated. ConsensusPathDB analysis of over-representation of differentially expressed genes provided a list of gene ontology terms in which several biological processes related to nucleoside monophosphate metabolic process, respiratory chain complex, electron transport chain, oxidative phosphorylation and cellular respiration were the most altered. Also, an interesting correlation was found between enniatin B toxicity and the up-regulation of the UCP protein complex. In summary, the transcriptomic analysis revealed that mitochondria are the organelles showing more related differentially expressed genes. Consequently, differentially expressed genes involved in biological processes, molecular functions and pathways related to mitochondrial metabolism and respiration were significantly changed.}
}
@article{HU2018363,
title = {Natural language aggregate query over RDF data},
journal = {Information Sciences},
volume = {454-455},
pages = {363-381},
year = {2018},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.04.042},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516310611},
author = {Xin Hu and Depeng Dang and Yingting Yao and Luting Ye},
keywords = {RDF, Question answering, Natural language, Aggregate query},
abstract = {Natural language question/answering over RDF (Resource Description Framework) data has received widespread attention. Although several studies can address a small number of aggregate queries, these studies have many restrictions (e.g., interactive information, controlled questions or query templates). Thus far, there has been no natural language querying mechanism that can process general aggregate queries over RDF data. Therefore, we propose a framework called NLAQ (Natural Language Aggregate Query). First, we propose a novel algorithm to automatically understand a user's query intention, which primarily contains semantic relations and aggregations. Second, to build a better bridge between the query intention and RDF data, we propose an extended paraphrase dictionary ED to obtain more candidate mappings for semantic relations, and we introduce a predicate-type adjacent set PT to filter out inappropriate candidate mapping combinations in semantic relations and basic graph patterns. Third, we design a suitable translation plan for each aggregate category and effectively distinguish whether an aggregate item is numeric, which will greatly affect the aggregate result. Finally, we conduct extensive experiments over real datasets (QALD benchmark and DBpedia). The experimental results demonstrate that our solution is effective.}
}
@article{SUBHASHREE2025127555,
title = {Enhanced quantum long short-term memory neural network based multi-task learning for sentimental analysis and cyberbullying detection},
journal = {Expert Systems with Applications},
volume = {282},
pages = {127555},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127555},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425011777},
author = {K. Subhashree and S.Manoj Kumar},
keywords = {Sentiment classification, Customer review, Cyberbullying detection, Social media, Multi task learning},
abstract = {Increasing usage of social media by individuals led to a significant rise in cyberbullying. Detecting sarcasm is challenging because many comments contain sarcasm or aggressive language. Text sentiment classification helps in the identification of abusive words using some beneficial features. Several machine learning algorithms are used in the detection of cyberbullying by using natural language processing mechanism. However, Deep Learning (DL) algorithms provides significant improvement in outcomes due to various reasons such as effectively segments text and image data, handling of large dataset, automatic extraction of features. Hence, a novel DL method Hybrid averaged and weighted averaged review vector Quantum long short-term memory neural based Multi-task Learning with Black-winged kite Optimization (HQMLBO) is proposed. Pre-processing is performed to clean the raw data. Next, features are extracted using hybrid multi-scale with hash vectorization, and relevant features are selected via the hybrid pine cone geyser-inspired optimization algorithm. Finally, sentiment classification and cyberbullying detection are performed using HQMLBO. Various DL methods are analysed and compared over three datasets using Python software. The proposed model outperforms existing methods in terms of accuracy of 95.68% for internet movie database, 92.5% for yelp polarity and 97.86% for cyberbullying classification dataset.}
}
@incollection{PANCHALINGAM2025869,
title = {Chapter 22 - Computational modeling and molecular dynamic simulations of gold nanoparticles},
editor = {S. K. Khadheer Pasha and Kalim Deshmukh and Chaudhery {Mustansar Hussain}},
booktitle = {Gold Nanoparticles, Nanomaterials and Nanocomposites},
publisher = {Elsevier},
pages = {869-893},
year = {2025},
series = {Micro and Nano Technologies},
isbn = {978-0-443-15897-1},
doi = {https://doi.org/10.1016/B978-0-443-15897-1.00022-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443158971000224},
author = {Santhiya Panchalingam and Govindaraju Kasivelu and Manikandan Jayaraman},
keywords = {Computational modeling, Gold nanoparticles, Molecular dynamic, Nanotechnology},
abstract = {All technology that can resolve or influence matter at the nanoscale is considered to fall under the overarching concept of nanotechnology. The advances in nanotechnology, nanosystems engineering (liposomes, Carbon Nano Tubes, micelles, etc), and bioinformatics will lead to the creation of nanosystems design. The study of “nanostructured materials” made it possible to create cutting-edge medical equipment that might be used for illness detection, management, and prophylaxis. In the developing field of personalized medicine, recent developments in biocomputing and nanotechnology hastened the discovery of novel biomarkers. The field of personalized medicine focuses on identifying diseases and treating patients based on their unique molecular profiles. The discovery of combinatorial markers in a nano particle was supported by recent advances in bio computing. The use of bioinformatics in nanotechnology was thoroughly covered in this chapter, with an emphasis on insilico drug development and delivery at the nanoscale. In addition to the bioinformatics applications now in use, attempts have been made to extend our understanding of the fundamental biological and medical ramifications of nanoscience and nanotechnology.}
}
@article{XI2023102075,
title = {Unveiling the mechanisms of nephrotoxicity caused by nephrotoxic compounds using toxicological network analysis},
journal = {Molecular Therapy Nucleic Acids},
volume = {34},
pages = {102075},
year = {2023},
issn = {2162-2531},
doi = {https://doi.org/10.1016/j.omtn.2023.102075},
url = {https://www.sciencedirect.com/science/article/pii/S2162253123002937},
author = {Kexing Xi and Mengqing Zhang and Mingrui Li and Qiang Tang and Qi Zhao and Wei Chen},
keywords = {MT: Bioinformatics, nephrotoxicity, network localization, network separation, disease, compound},
abstract = {Billions of people worldwide have experienced irreversible kidney injuries, which is mainly attributed to the complexity of drug-induced nephrotoxicity. Consequently, there is an urgent need for uncovering the mechanisms of nephrotoxicity caused by compounds. In the present study, a network-based methodology was applied to explore the mechanisms of nephrotoxicity induced by specific compounds. Initially, a total of 42 nephrotoxic compounds and 60 kinds of syndromes associated with nephrotoxicity were collected from public resources. Afterward, network localization and separation algorithms were used to map the targets of compounds and diseases into the human interactome. By doing so, 199 statistically significant nephrotoxic networks displaying the interaction between compound targets and disease genes were obtained, which played pivotal roles in compounds-induced nephrotoxicity. Subsequently, enrichment analysis pinpointed core Gene Ontology and Kyoto Encyclopedia of Genes and Genomes pathways that highlight commonalities in nephrotoxicity induced by nephrotoxic compounds. It was found that nephrotoxic compounds primarily induce nephrotoxicity by mediating the advanced glycosylation end products-receptor for advanced glycosylation end products signaling pathway in diabetic complications, human cytomegalovirus infection, lipid and atherosclerosis, Kaposi sarcoma–associated herpesvirus infection, apoptosis, and the phosphatidylinositol 3-kinase-Akt pathways. These results provide valuable insights for preventing drug-induced nephrotoxicity. Furthermore, the approaches we used are also helpful in conducting research on other kinds of toxicities.}
}
@article{SCHLEGEL2025102495,
title = {Capturing end-to-end provenance for machine learning pipelines},
journal = {Information Systems},
volume = {132},
pages = {102495},
year = {2025},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2024.102495},
url = {https://www.sciencedirect.com/science/article/pii/S0306437924001534},
author = {Marius Schlegel and Kai-Uwe Sattler},
keywords = {Machine learning model development, Data science, ML pipelines, Artifact management, Provenance, MLflow, W3C PROV},
abstract = {Modern workflows for developing ML pipelines utilize ML artifact management systems (ML AMSs) such as MLflow in addition to traditional version control systems such as Git. ML AMSs collect data, model, metadata and software artifacts used and produced in pipeline development workflows. While ensuring repeatability and reproducibility, the provenance capabilities are still rudimentary, mainly due to incomplete traces, coarse granularity, and limited query capabilities. In this paper, we introduce a comprehensive PROV-compliant provenance model that captures end-to-end provenance traces of ML pipelines, their artifacts, and their relationships based on MLflow and Git activities. Moreover, we present the tool MLflow2PROV for continuously extracting provenance graphs according to our model, enabling querying, analyzing, and processing of the collected provenance information.}
}
@article{FENG2022474,
title = {Computing Sufficient and Necessary Conditions in CTL: A Forgetting Approach},
journal = {Information Sciences},
volume = {616},
pages = {474-504},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.10.124},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522012518},
author = {Renyan Feng and Erman Acar and Yisong Wang and Wanwei Liu and Stefan Schlobach and Weiping Ding},
keywords = {Computation tree logic, Forgetting, Weakest sufficient condition, Model checking},
abstract = {Computation tree logic (CTL) is an essential specification language in the field of formal verification. In systems design and verification, it is often important to update existing knowledge with new attributes and subtract the irrelevant content while preserving the given properties on a known set of atoms. Under the scenario, given a specification, the weakest sufficient condition (WSC) and the strongest necessary condition (SNC) are dual concepts and very informative in formal verification. In this article, we generalize our previous results (i.e., the decomposition, homogeneity properties, and the representation theorem) on forgetting in bounded CTLto the unbounded one. The cost we pay is that, unlike the bounded case, the result of forgetting in CTLmay no longer exist. However, SNC and WSC can be obtained by the new forgetting machinery we are presenting. Furthermore, we complement our model-theoretic approach with a resolution-based method to compute forgetting results in CTL. This method is currently the only way to compute forgetting results for CTLand temporal logic. The method always terminates and is sound. That way, we set up the resolution-based approach for computing WSC and SNC in CTL.}
}
@article{KUMAR2025108364,
title = {Knowledge graph applications and multi-relation learning for drug repurposing: A scoping review},
journal = {Computational Biology and Chemistry},
volume = {115},
pages = {108364},
year = {2025},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2025.108364},
url = {https://www.sciencedirect.com/science/article/pii/S1476927125000246},
author = {A.Arun Kumar and Samarth Bhandary and Swathi Gopal Hegde and Jhinuk Chatterjee},
keywords = {Drug repurposing, Knowledge graph, Embedding, Multimodal frameworks},
abstract = {Objective
Development of novel drug solutions has always been an expensive endeavour, hence drug repurposing as an approach has gained popularity in recent years. In this review we intend to examine one of the most unique computational methods for drug repurposing, that being knowledge graphs.
Method
Through literature review we looked at the application of knowledge graphs in medicine, specifically at its use in drug repurposing. We also looked at literature embedding methods, integration of machine learning models and approaches to completion of knowledge graphs.
Result
After filtering 43 papers were used for analysis. Timeline, country distribution, application areas of knowledge graph was highlighted. General trends in the use of knowledge graphs for drug repurposing and any shortcomings of the approach was discussed.
Conclusion
This approach has gained popularity only very recently; hence it is in a nascent phase.}
}
@article{SFAR2018400,
title = {Activity Recognition for Anomalous Situations Detection},
journal = {IRBM},
volume = {39},
number = {6},
pages = {400-406},
year = {2018},
note = {JETSAN},
issn = {1959-0318},
doi = {https://doi.org/10.1016/j.irbm.2018.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S1959031818300897},
author = {H. Sfar and A. Bouzeghoub},
keywords = {Activity recognition, Situation anomaly detection, Dempster Shafer, Markov logic network, Ontologies, Reasoning, Smart homes},
abstract = {Background
Living alone can be tough and risky for the elderly, typically, a fall can have serious consequences for them. Consequently, smart homes are becoming more and more popular. Such sensors-enriched environments can be exploited for health-care applications, in particular, Anomaly Detection (AD). Currently, most AD solutions only focus on detecting anomalies in the user daily activities while omitting the ones coming from the environment itself. However, it appears that serious anomalies can be caused by the environment during the user activity such as getting sick during sleeping when it is cold and the window is open.
Methods
In order to consider environmental context with user activities, in this paper, we present a novel approach for detecting anomalous situations occurring in the smart home environment. To that end, we propose as a first step, an activity recognition method based on an hybridization of a knowledge-based technique, taking full advantage of the semantic representation and the reasoning properties of ontologies and a data driven technique based on Dempster Shafer theory. In the second step, given the recognized activity and its surrounding context, we propose an approach that is able to built situations and detect anomalies with a level of uncertainty.
Results
Our system is implemented, tested and evaluated using real data obtained from the Hadaptic platform11http://hadaptic.telecom-sudparis.eu/. and opportunity dataset. The former dataset is used to evaluate the detection of anomalies and the latter is for the recognition of activities. Experimental results prove that with suitable time window size, the activity recognizer and the anomaly detector are efficient having respectively 91% of recognition rate and 100% of precision.
Conclusion
Our method allows, on one hand, recognizing user activities and, on the other hand, detecting eventual occurrence of anomalies in the user's situation. It proves to be efficient using the tested datasets for each module. However, in order to obtain a more general conclusion we plan to evaluate the method using more different datasets.}
}
@article{KHORSHIDI2025104238,
title = {Half a Century of Information Processing & Management: A bibliometric retrospective},
journal = {Information Processing & Management},
volume = {62},
number = {6},
pages = {104238},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104238},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325001797},
author = {Mohammad Sadegh Khorshidi and José M. Merigó and Ghassan Beydoun},
keywords = {Information Processing, Management, Bibliometrics, Web of Science, Scopus, VOS viewer},
abstract = {Established in 1963 under the title Information Storage and Retrieval, the journal adopted its current name, Information Processing & Management (IPM), in 1975, reflecting a broadening scope aligned with computational and cognitive developments in information science. This study uses data from Web of Science and Scopus databases to deliver a longitudinal, multi-perspective bibliometric and science mapping analysis of IPM’s evolution from 1963 to 2023. Employing co-citation analysis, bibliographic coupling, keyword co-occurrence, and thematic mapping via VOSviewer and Bibliometrix, the analysis delineates the structural, conceptual, and topical transformation of the journal content. Co-citation networks uncover foundational cores in information retrieval, relevance theory, and evaluation methodologies, while also revealing temporal shifts toward natural language processing, deep learning, and social media analytics. Bibliographic coupling identifies coherent intellectual clusters centered on GNN-based recommendation systems, blockchain-secured infrastructures, and sentiment-aware retrieval frameworks. Keyword co-occurrence and topic evolution trajectories illustrate the journal’s recent pivot toward transformer models, misinformation detection, ethical AI, and interdisciplinary convergence across cognitive science, machine learning, and computational linguistics. Regional co-word analysis underscores epistemological diversity and geographic differentiation across North America, Europe, and East Asia. Productivity and influence metrics highlight the ascent of East Asian institutions and the emergence of globally distributed citation impact. Finally, SciVal-based topic and topic cluster analyses reveal the journal’s role in advancing highly cited research (as measured by FWCI) in areas such as ABSA, multi-view clustering, and health informatics. This work not only charts IPM’s conceptual landscape and disciplinary diffusion but also provides actionable intelligence on the journal’s strategic positioning within the broader information and computational sciences.}
}
@article{WU2025125981,
title = {A semantic-driven approach for maintenance digitalization in the pharmaceutical industry},
journal = {International Journal of Pharmaceutics},
volume = {683},
pages = {125981},
year = {2025},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2025.125981},
url = {https://www.sciencedirect.com/science/article/pii/S037851732500818X},
author = {Ju Wu and Xiaochen Zheng and Marco Madlena and Dimitris Kiritsis},
keywords = {Semantic technology, Pharma 4.0, Quality 4.0, Zero Defect Manufacturing, Digitalization, Pharmaceutical industry, Maintenance automation},
abstract = {The digital transformation of pharmaceutical industry is a challenging task due to the high complexity of involved elements and the strict regulatory compliance. Maintenance activities in the pharmaceutical industry play an essential role in ensuring product quality and integral functioning of equipment and premises. This paper first identifies the key challenges of digitalization in pharmaceutical industry and creates the corresponding problem space for key involved elements. A semantic-driven digitalization framework is proposed aiming to improve the digital continuity of digital resources and technologies for maintenance activities. This framework aligns with Quality 4.0 principles and supports the industry’s pursuit of zero manufacturing defects. A case study is conducted to verify the feasibility of the proposed framework based on the water sampling activities in Merck Serono facility in Switzerland. A tool-chain is presented to enable the functional modules of the framework. Some of the key functional modules within the framework are implemented and have demonstrated satisfactory performance. As one of the outcomes, a digital sampling assistant with web-based services is created to support the automated workflow of water sampling activities. The implementation result proves the potential of the proposed framework to solve the identified problems of maintenance digitalization in the pharmaceutical industry.}
}
@article{KIM202297,
title = {Goal-driven scheduling model in edge computing for smart city applications},
journal = {Journal of Parallel and Distributed Computing},
volume = {167},
pages = {97-108},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0743731522001009},
author = {Yongho Kim and Seongha Park and Sean Shahkarami and Rajesh Sankaran and Nicola Ferrier and Pete Beckman},
keywords = {Goal-driven scheduling, Context-aware scheduling, Edge computing, Logical reasoning},
abstract = {A formidable challenge in scheduling user applications lies in collecting and representing the user's goals and requirements. We introduce a “science goal” as a mechanism for users to define scientific objectives and conditions of interest. To provide an abstraction to run applications on an ensemble of edge computing nodes, we implement a two-layered scheduler—cloud and edge scheduler. In this scheduling model, the users submit their goals to the cloud scheduler. These goals are conveyed to the appropriate nodes based on a variety of constraints including geographical area, resource availability, node capabilities, and applicability. The edge scheduler, with complete understanding of the current conditions, assumes the responsibility for executing the applications on the nodes so that the users' science goals are met. This paper provides a framework for the two-layered scheduling model for goal-driven edge computing and motivates and informs its architecture through a case study.}
}
@incollection{COUCLELIS2020357,
title = {Computational Human Geography},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {357-363},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10619-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955106195},
author = {Helen Couclelis},
keywords = {Agent-based models, Ambient computing, Cellular automata, Computer modeling, Data revolution, GIS, Geocomputation, Simulation, Urban informatics, Visualization},
abstract = {Computational human geography refers to the use of computational methods and techniques to solve problems in human geography research and applications. The approach goes back to the beginnings of the quantitative revolution in geography and is philosophically related though methodologically distinct from it. Geographic information systems (GIS) and science are a big part of computational human geography, but the latter notion is considerably broader, encompassing spatial process modeling and simulation, the modeling of spatial decision and behavior, visualization techniques, spatial analysis, and an increasing number of new research areas and methods enabled by the most recent technological developments. The latter are discussed under the rubrics of The Data Revolution, Urban (Spatial) Informatics, and Ambient Computing. Two major thrusts have persisted throughout the years: the use of numerical techniques to solve large, complex quantitative problems; the development of models of complex spatial processes expressed directly in computational terms. Both have evolved with the times and continue to be central to computational human geography. Critiques originate from both within the field and from the humanities and social theory perspectives. These address epistemological and methodological problems as well as issues of ontology and representation.}
}
@article{JUMAELFLOW2025102756,
title = {Intersectional conflicts and strategic agency: Female English teachers in post-conflict Libya},
journal = {International Journal of Educational Research},
volume = {133},
pages = {102756},
year = {2025},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2025.102756},
url = {https://www.sciencedirect.com/science/article/pii/S0883035525002290},
author = {Khadija Juma-Elflow and Ulker Vanci Osam},
keywords = {Teacher identity, Emotional labor, Intersectionality, EFL, Post-conflict contexts, Libya, Gender},
abstract = {This study examines how female English as a Foreign Language (EFL) teachers in post-conflict Libya construct and negotiate their professional identities amid intersecting pedagogical, institutional, and sociocultural challenges. Using a theoretical framework that integrates Gendered Professional Identity Theory, Social Identity Theory, and Intersectionality, the research employs Interpretative Phenomenological Analysis (IPA) of in-depth interviews with twelve female Libyan EFL teachers. The findings reveal the profound emotional and ideological labor demanded of these educators, leading to four novel conceptual insights: ‘cultural boundary’ work, where student resistance acts as socio-political identity assertion; ‘intersectional emotional labor’, highlighting the compounded emotional demands shaped by gender, surveillance, and conflict fragility; ‘proclivitic conformity’, reflecting strategic compliance for self-preservation; and ‘gendered professional invisibility’, where teachers minimize public visibility to conform to societal norms. These findings challenge dominant Western-centric views of teacher identity and emotional labor, emphasizing the need for culturally responsive educational policies and teacher development programs in fragile, conflict-affected contexts. This study contributes to scholarship on teacher identity, language education, and educational development in Libya.}
}
@article{IZADI2024e24972,
title = {Discovering conserved epitopes of Monkeypox: Novel immunoinformatic and machine learning approaches},
journal = {Heliyon},
volume = {10},
number = {3},
pages = {e24972},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e24972},
url = {https://www.sciencedirect.com/science/article/pii/S240584402401003X},
author = {Mohammad Izadi and Fatemeh Mirzaei and Mohammad Aref Bagherzadeh and Shamim Ghiabi and Alireza Khalifeh},
keywords = {Monkeypox, Conserved epitopes, Machine learning, Immunoinformatic},
abstract = {The Monkeypox virus, an Orthopoxvirus with zoonotic origins, has been responsible for a growing number of human infections reminiscent of smallpox since May 2022, as reported by the World Health Organization. As of now, there are no established medical treatments for managing Monkeypox infections. In this study, we used machine learning to select conserved epitopes. Proteins were determined using Reverse Vaccinology and Gene Ontology subcellular localization, and their epitopes were predicted. NextClade was used to calculate the number of mutations in each amino acid position using 2433 Monkeypox sequences. The Unsupervised Nearest Neighbor machine learning algorithm and ideal matrix [0 0] were used to calculate the conservancy score of epitopes. Six proteins were determined for epitope prediction. Finally, 47 MHC-I epitopes, 5 MHC-II epitopes, and 10 Linear B cell epitopes were discovered. Our method can select epitopes for vaccine design to prevent viruses with accelerated evolution and high mutation rate.}
}
@article{ABDOU201894,
title = {A semi-automated framework for semantically annotating web content},
journal = {Future Generation Computer Systems},
volume = {81},
pages = {94-102},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17307343},
author = {Mohamed Abdou and Sayed AbdelGaber and Marwa Farhan},
keywords = {Semantic Markup, Semantic annotation, Schema.org, JSON-LD},
abstract = {Today’s web is growing very fast and having a strong online presence is becoming critical for businesses. In contrast, websites in search results using traditional Search Engine Optimization (SEO) techniques became less effective in achieving the desired visibility. To overcome the limitations of traditional SEO, some recent trends are adopting the technologies of Semantic Web to annotate web content with Semantic Markup that can be understood by search engines. However, the balance between the accuracy of annotations and the automation level still under investigation. This research proposes a semi-automated framework that provides a high level of accurate annotations with minimum user interaction, based on Schema.org; a well-accepted ontology for ordinary things in life. The proposed framework aims to analyze the contents of web documents and extract the unique keywords and key phrases that best describe that content. Then, annotate those keywords and key phrases with the appropriate Schema.org vocabularies. This will reflect on the understanding level of the search engines to the web contents and accordingly a better visibility in the search results.}
}
@article{LIU2022103744,
title = {VoCSK: Verb-oriented commonsense knowledge mining with taxonomy-guided induction},
journal = {Artificial Intelligence},
volume = {310},
pages = {103744},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103744},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222000844},
author = {Jingping Liu and Tao Chen and Chao Wang and Jiaqing Liang and Lihan Chen and Yanghua Xiao and Yunwen Chen and Ke Jin},
keywords = {Commonsense knowledge, Verb phrases, Probabilistic taxonomy},
abstract = {Commonsense knowledge acquisition is one of the fundamental issues in realizing human-level AI. However, commonsense knowledge is difficult to obtain because it is a human consensus and rarely explicitly appears in texts or other data. In this paper, we focus on the automatic acquisition of a typical kind of implicit verb-oriented commonsense knowledge (e.g., “person eats food”), which is the concept-level knowledge of verb phrases. For this purpose, we propose a taxonomy-guided induction method to mine verb-oriented commonsense knowledge from verb phrases with the help of a probabilistic taxonomy. First, we design an entropy-based triplet filter to cope with noisy verb phrases. Then, we propose a joint model based on the minimum description length principle and a neural language model to generate verb-oriented commonsense knowledge. Besides, we introduce two strategies to accelerate the computation, including the simulated annealing-based approximate solution and the verb phrase clustering method. Finally, we conduct extensive experiments to prove that our solution is more effective than competitors in mining verb-oriented commonsense knowledge. We construct a commonsense knowledge base called VoCSK, containing 259 verbs and 18,406 verb-oriented commonsense knowledge. To verify the usefulness of VoCSK, we utilize the knowledge in this KB to improve the model performance on two downstream applications.}
}
@article{THOMAS2024100118,
title = {Process Tracing for applied linguistics},
journal = {Research Methods in Applied Linguistics},
volume = {3},
number = {2},
pages = {100118},
year = {2024},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2024.100118},
url = {https://www.sciencedirect.com/science/article/pii/S2772766124000247},
author = {Nathan Thomas and Anna Kristina Hultgren and Beatrice Zuaro and Dogan Yuksel and Peter Wingrove and Marion Nao and Derek Beach},
keywords = {Process-tracing, Causal process, Causal mechanism, Causality, Qualitative},
abstract = {A key mission of many applied linguists is to understand how language-related processes work. The inner workings of a process can be explained by theorizing about the underlying causal mechanism that enables the process to unfold and evidencing the mechanism with empirical material. However, the methodological repertoire of applied linguistics is limited in this regard. Thus, we step outside of our field-specific literature and showcase Process Tracing, a family of qualitative, within-case research methods often used in political science to trace causal mechanisms. We describe Process Tracing's key tenets that are applicable to most variations. We then refer specifically to the systems understanding of Process Tracing. This variation views mechanisms as systems with interlocking parts. Each part is theorized to transmit causal force from a hypothesized cause to an outcome. Our concentrated focus enables us to provide specific recommendations for researchers looking to maximize the method's practical explanatory power without over-formalizing its procedures. As such, our coverage bridges the gap between broad introductions to Process Tracing currently available in applied linguistics and an increasingly technical literature found elsewhere. We also provide a preliminary state-of-the-art review of studies that have claimed to use Process Tracing in applied linguistics, often to very different ends. Finally, we demonstrate how we have applied the systems understanding of Process Tracing in one of our own studies as a methodological exemplar. We conclude with a call for researchers to explore the affordances of Process Tracing alongside us as we adapt it for future use in applied linguistics.}
}
@article{LIU201815,
title = {Generating machine-executable plans from end-user's natural-language instructions},
journal = {Knowledge-Based Systems},
volume = {140},
pages = {15-26},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2017.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S0950705117304926},
author = {Rui Liu and Xiaoli Zhang},
keywords = {Semantic analysis, Machine-executable plan, Natural language instruction, Advanced manufacturing machine},
abstract = {It is critical for advanced manufacturing machines to autonomously execute a task by following an end-user's natural language (NL) instructions. However, NL instructions are usually ambiguous and abstract so that the machines may misunderstand and incorrectly execute the task. To address this NL-based human-machine communication problem and enable the machines to appropriately execute tasks by following the end-user's NL instructions, we developed a Machine-Executable-Plan-Generation (exePlan) method. The exePlan method conducts task-centered semantic analysis to extract task-related information from ambiguous NL instructions. In addition, the method specifies machine execution parameters to generate a machine-executable plan by interpreting abstract NL instructions. To evaluate the exePlan method, an industrial robot Baxter was instructed by NL to perform three types of industrial tasks {“drill a hole”, “clean a spot”, “install a screw”}. The experiment results proved that the exePlan method was effective in generating machine-executable plans from the end-user's NL instructions. Such a method has the promise to endow a machine with the ability of NL-instructed task execution.}
}
@article{BOZIC2020201,
title = {Comparing tagging suggestion models on discrete corpora},
journal = {International Journal of Web Information Systems},
volume = {16},
number = {2},
pages = {201-221},
year = {2020},
issn = {1744-0084},
doi = {https://doi.org/10.1108/IJWIS-08-2019-0035},
url = {https://www.sciencedirect.com/science/article/pii/S1744008420000245},
author = {Bojan Bozic and Andre Rios and Sarah Jane Delany},
keywords = {Tag prediction, Natural language processing, Multi-label classification, k-nearest neighbor, Naïve Bayes},
abstract = {Purpose
This paper aims to investigate the methods for the prediction of tags on a textual corpus that describes diverse data sets based on short messages; as an example, the authors demonstrate the usage of methods based on hotel staff inputs in a ticketing system as well as the publicly available StackOverflow corpus. The aim is to improve the tagging process and find the most suitable method for suggesting tags for a new text entry.
Design/methodology/approach
The paper consists of two parts: exploration of existing sample data, which includes statistical analysis and visualisation of the data to provide an overview, and evaluation of tag prediction approaches. The authors have included different approaches from different research fields to cover a broad spectrum of possible solutions. As a result, the authors have tested a machine learning model for multi-label classification (using gradient boosting), a statistical approach (using frequency heuristics) and three similarity-based classification approaches (nearest centroid, k-nearest neighbours (k-NN) and naive Bayes). The experiment that compares the approaches uses recall to measure the quality of results. Finally, the authors provide a recommendation of the modelling approach that produces the best accuracy in terms of tag prediction on the sample data.
Findings
The authors have calculated the performance of each method against the test data set by measuring recall. The authors show recall for each method with different features (except for frequency heuristics, which does not provide the option to add additional features) for the dmbook pro and StackOverflow data sets. k-NN clearly provides the best recall. As k-NN turned out to provide the best results, the authors have performed further experiments with values of k from 1–10. This helped us to observe the impact of the number of neighbours used on the performance and to identify the best value for k.
Originality/value
The value and originality of the paper are given by extensive experiments with several methods from different domains. The authors have used probabilistic methods, such as naive Bayes, statistical methods, such as frequency heuristics, and similarity approaches, such as k-NN. Furthermore, the authors have produced results on an industrial-scale data set that has been provided by a company and used directly in their project, as well as a community-based data set with a large amount of data and dimensionality. The study results can be used to select a model based on diverse corpora for a specific use case, taking into account advantages and disadvantages when applying the model to your data.}
}
@article{ZHIVKOPLIAS20252105,
title = {How transdisciplinarity can help biotech-driven biodiversity research},
journal = {Trends in Biotechnology},
volume = {43},
number = {9},
pages = {2105-2116},
year = {2025},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2025.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167779925001350},
author = {Erik Zhivkoplias and Jessica M. {da Silva} and Robert Blasiak},
keywords = {capacity building, CBD, data management, NGS, research ethics, sequencing centres},
abstract = {The Kunming-Montreal Global Biodiversity Framework marks a significant step toward conserving genetic diversity on a global scale. Sequencing advancements have broadened biodiversity studies by enabling the mapping of species distributions, increasing understanding of ecological interactions, and monitoring genetic diversity. However, these tools are hindered by inequalities and biases, particularly in biodiversity-rich developing countries. To navigate these challenges, we propose strategies using the existing biotechnological toolbox to make biodiversity data more accessible and useful for research and development. This includes increasing funding for database curation, improving metadata standards, addressing inequalities in technological capacity, and supporting holistic capacity-building programmes. Implementing these strategies can unlock new opportunities for biodiversity research aligned with sustainable development principles and can contribute to improved conservation outcomes.}
}
@article{LIANG2025114769,
title = {Quantitative evaluation of China’s energy storage policies: A ChatGPT-based PMC index modelling approach},
journal = {Energy Policy},
volume = {206},
pages = {114769},
year = {2025},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2025.114769},
url = {https://www.sciencedirect.com/science/article/pii/S0301421525002769},
author = {Jing Liang and Yuqi Wang and Wei Li and Weihan Wang},
keywords = {Energy storage system, Policy evaluation, Policy modelling consistency, ChatGPT-based},
abstract = {Efficient energy grid systems can improve operational efficiency and reduce carbon emissions by integrating diverse renewable energy generation sources. As a distinct asset class within the electric grid, energy storage necessitates well-defined regulatory and financial policies to support its development and large-scale deployment. This makes it essential to establish an effective and consistent policy evaluation framework to support the growth of the energy storage industry. In this study, we propose a ChatGPT-based Policy Model Consistency framework to evaluate 203 energy supply policies issued by China’s central and local governments during the “14th Five-Year Plan” period (2021–2024). The results demonstrate the effectiveness of AI-powered policy analysis in building quantitative and objective policy evaluation systems. In addition, the findings highlight the ability of the system to provide a comprehensive analysis and practical recommendations for the development of energy storage systems in China.}
}
@article{HOSKERE2025106214,
title = {Unified framework for digital twins of bridges},
journal = {Automation in Construction},
volume = {175},
pages = {106214},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106214},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525002547},
author = {Vedhus Hoskere and Delaram Hassanlou and Asad Ur Rahman and Reza Bazrgary and Muhammad Taseer Ali},
keywords = {Digital twins, Bridges, Artificial intelligence, Bridge information model, Cyber-physical systems},
abstract = {Digital twin technology is transforming bridge infrastructure management through virtual replicas that integrate sensor data with intelligent analytics to enable proactive decision-making. At the same time, rapid advances in sensing, artificial intelligence, robotics, and automated bridge modeling offer new opportunities to improve digital twin outcomes. This paper introduces a unified development framework for bridge digital twins, grounded in a systematic literature review and bibliometric mapping of cyber-physical interfaces, reality modeling, and twin-building methods. Advances in bridge information modeling, as well as 4D, geometric, condition-based, and structural digital twins, are surveyed. Key digital services enabled by digital twins for both new and existing bridges are highlighted, including design, construction, inspection, structural health monitoring, digital maintenance, disaster vulnerability assessment, and fatigue assessment. Critical challenges and research gaps are identified to guide future efforts toward the widespread adoption of digital twins in bridge infrastructure.}
}
@article{MACHULIN2024105196,
title = {Variation in base composition, structure-function relationships, and origins of structural repetition in bacterial rpsA gene},
journal = {BioSystems},
volume = {238},
pages = {105196},
year = {2024},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2024.105196},
url = {https://www.sciencedirect.com/science/article/pii/S0303264724000819},
author = {Andrey V. Machulin and Evgeniya I. Deryusheva and Oxana V. Galzitskaya},
keywords = {Gene , Repetitive S1 domain, Ribosomal S1 proteins, RNA-Binding site, Structural genomics},
abstract = {Protein domain repeats are known to arise due to tandem duplications of internal genes. However, the understanding of the underlying mechanisms of this process is incomplete. The goal of this work was to investigate the mechanism of occurrence of repeat expansion based on studying the sequences of 1324 rpsA genes of bacterial S1 ribosomal proteins containing different numbers of S1 structural domains. The rpsA gene encodes ribosomal S1 protein, which is essential for cell viability as it interacts with both mRNA and proteins. Gene ontology (GO) analysis of S1 domains in ribosomal S1 proteins revealed that bacterial protein sequences in S1 mainly have 3 types of molecular functions: RNA binding activity, nucleic acid activity, and ribosome structural component. Our results show that the maximum value of rpsA gene identity for full-length proteins was found for S1 proteins containing six structural domains (58%). Analysis of consensus sequences showed that parts of the rpsA gene encoding separate S1 domains have no a strictly repetitive structure between groups containing different numbers of S1 domains. At the same time, gene regions encoding some conserved residues that form the RNA-binding site remain conserved. The detected phylogenetic similarity suggests that the proposed fold of the rpsA translation initiation region of Escherichia coli has functional value and is important for translational control of rpsA gene expression in other bacterial phyla, but not only in gamma Proteobacteria.}
}
@article{LILIS2025106081,
title = {BIM-based semantic enrichment and knowledge graph generation via geometric relation checking},
journal = {Automation in Construction},
volume = {173},
pages = {106081},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106081},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525001219},
author = {Georgios Nektarios Lilis and Meng Wang and Kyriakos Katsigarakis and Dimitrios Mavrokapnidis and Ivan Korolija and Rovas Dimitrios},
keywords = {BIM, IFC, Geometry, Interoperability, Digital twins, Semantic web technologies},
abstract = {Building Information Models support information exchange and collaboration between designers, engineers and stakeholders of the built environment. Due to the large scale and multi-domain requirements of building projects, building information is often fragmented to multiple files, prone to modeling errors and poor level of detail. As a result, BIM data cannot be reused and remain siloed across the building lifecycle. This paper introduces the Geometric Relation Checking tool, a novel tool that automatically detects geometric relations between IFC objects. These relationships can be used to infer missing semantic relationships among these objects, increasing the inter-connectivity among semantic graphs of different domains and at the same time breaking their siloed structures. The tool is tested on MEP and architecture domain data, but its use can be generalized to any other data domain that contains elements with geometric representations.}
}
@article{MIAO2020,
title = {Identification prognosis-associated immune genes in colon adenocarcinoma},
journal = {Bioscience Reports},
volume = {40},
number = {11},
year = {2020},
issn = {1573-4935},
doi = {https://doi.org/10.1042/BSR20201734},
url = {https://www.sciencedirect.com/science/article/pii/S1573493520002635},
author = {Yandong Miao and Jiangtao Wang and Xueping Ma and Yuan Yang and Denghai Mi},
keywords = {Bioinformatic analysis, Colon adenocarcinoma, Immune cell infiltration, Immune gene, TCGA, tumor microenvironments},
abstract = {Colon adenocarcinoma (COAD) is one of the most prevalent malignant tumors worldwide. Immune genes (IGs) have a considerable correlation with tumor initiation and prognosis. The present paper aims to identify the prognosis value of IGs in COAD and conduct a prognosis model for clinical utility. Gene expression data of COAD were downloaded from The Cancer Genome Atlas (TCGA), screening and analyzing differentially expressed IGs by bioinformatics. Core genes were screened by univariate and multivariate Cox regression analyses. Survival analysis was appraised by the Kaplan–Meier method and the log-rank test. Gene Ontology, Kyoto Encyclopedia of Genes and Genomes, and Gene Set Enrichment Analysis (GSEA) were used to identify IGs’ relevant signal pathways. We predicted the overall survival (OS) by nomogram. Finally, a prognosis model was conducted based on 12 IGs (SLC10A2, CXCL3, NOX4, FABP4, ADIPOQ, IGKV1-33, IGLV6-57, INHBA, UCN, VIP, NGFR, and TRDC). The risk score was an independent prognostic factor, and a nomogram could accurately predict the OS of individual COAD patients. These results were validated in GSE39582, GSE12945, and GSE103479 cohorts. Functional enrichment analysis demonstrated that these IGs are mainly enriched in hormone secretion, hormone transport, lipid transport, cytokine–cytokine receptor interaction, and peroxisome proliferators-activated receptor signaling pathway. In summary, the risk score is an independent prognostic biomarker. We also excavated several IGs related to COAD’s survival and maybe potential biomarkers for COAD diagnosis and treatment.}
}
@article{SANEMETERIODELAPARTE2024101030,
title = {Spatio-temporal semantic data management systems for IoT in agriculture 5.0: Challenges and future directions},
journal = {Internet of Things},
volume = {25},
pages = {101030},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.101030},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523003530},
author = {Mario {San Emeterio de la Parte} and José-Fernán Martínez-Ortega and Pedro Castillejo and Néstor Lucas-Martínez},
keywords = {Data science, Internet of Things (IoT), Big data, Agriculture, Spatio-temporal semantic data management system (STSDMS)},
abstract = {The Agri-Food sector is in a stressful situation due to the high demand for food from the growing population around the world. The agricultural sector is facing a challenging situation; it must increase production and reduce its impact on the environment by appropriately allocating resources, adapting to climate change, and avoiding food waste. Agriculture 5.0, as the fifth agricultural evolution, aims to offer a perfect symbiosis between agriculture, advanced technologies, and sustainability. The most advanced technologies in automation, monitoring, and decision support are driven by the collection and processing of large volumes of agricultural data, such as weather information, farm machinery, soil and crop conditions, and marketing demand for higher profits. Taking advantage of the technological paradigm of the Internet of Things, agricultural data provides information on spatial, temporal, and semantic dimensions. Spatio-temporal semantic data management systems have become the cornerstone for the achievement of Agriculture 5.0 through advanced Internet of Things technologies. This paper aims to review the current literature on spatio-temporal semantic data management systems for Agriculture 5.0. This paper uses a systematic literature review technique to study eleven representative spatio-temporal semantic data management systems. A comprehensive evaluation of the aspects of interoperability, accessibility, scalability, real-time operation capability, etc. is carried out. Based on the evaluation results, future challenges are detected and development trends and possible improvements are proposed for future research. Finally, a distributed architecture capable of satisfying the above needs and challenges is proposed. The paper aims to inspire further research and development efforts to improve the efficiency, accessibility, and performance of spatio-temporal semantic data management systems.}
}
@article{MCARTHUR2018676,
title = {Lean-Agile FM-BIM: a demonstrated approach},
journal = {Facilities},
volume = {36},
number = {1314},
pages = {676-695},
year = {2018},
issn = {0263-2772},
doi = {https://doi.org/10.1108/F-04-2017-0045},
url = {https://www.sciencedirect.com/science/article/pii/S0263277218000107},
author = {J. J. McArthur and Brandon Bortoluzzi},
keywords = {Facilities management Lean, Agile, Information management, Building information modeling (BIM), Data transfer},
abstract = {Purpose
This paper aims to respond to the high cost of facility management-enabled building information model (FM-BIM) creation and maintenance, a significant and under-researched barrier to adoption for existing buildings. The resultant approach focuses on only value-adding content (“Lean”) developed flexibly and iteratively in collaboration with end-users (“Agile”).
Design/methodology/approach
Five case studies were developed for university and hospital buildings in collaboration with end-users, guided by the process presented. These informed the refinement of a robust and flexible approach to increase BIM functionality with minimal geometry, focusing instead on the development of specific parameters to map semantic information necessary for each desired FM use.
Findings
The resulting BIM provided a breadth of model functionality with minimal modeling effort: 15 hours average implementation time per supported FM use. This low level of effort was achieved by limiting geometry to where it is necessary for the FM use implementation. Instead, the model incorporated the majority of geometry by reference and focused on semantic and topological parameters to house FM information.
Research limitations/implications
This study provides the basis for a new ontology structure focused on defining the rules for hosting asset management data (host entity, parameter type and characteristics) to reduce the reliance on complex geometric model development.
Practical implications
By prioritizing highly beneficial applications, early investment is minimized, providing quick returns at low risk, demonstrating the value of FM-BIM to end-users.
Originality/value
The Lean-Agile approach addresses the known research gap of low-effort, flexible approaches to FM-BIM model creation and maintenance and its effectiveness is analyzed through five case studies.}
}
@article{DAI2025110599,
title = {Retinal proteomic analysis reveals ON/OFF visual stimulation-specific changes in a Guinea pig myopia model},
journal = {Experimental Eye Research},
volume = {260},
pages = {110599},
year = {2025},
issn = {0014-4835},
doi = {https://doi.org/10.1016/j.exer.2025.110599},
url = {https://www.sciencedirect.com/science/article/pii/S0014483525003707},
author = {Qin Dai and Qiqi Xie and Frank Schaeffel and Sisi Zheng and Qi Sun and Jinfei Wei and Bingyan Shen and Yenan Fang and Ziwei Li and Xi Chen and Xiangtian Zhou and Min Wang},
keywords = {ON OFF pathways, Guinea pig, Myopia, Retina, Proteomic},
abstract = {This study aimed to explore the effect of artificial dynamic ON/OFF stimulation on the development of myopia and associated retinal proteomic changes in guinea pigs, and elucidate the underlying mechanisms. Myopia was induced in one eye of guinea pigs using −4 diopters lenses, with the other eye used as the untreated control. 53 guinea pigs were randomly given balanced ON/OFF (natural control, NC), ON, or OFF stimulation for 7 days. The axial lengths and refractions were measured. The choroidal thickness was measured using HE-staining in 4 guinea pigs per group. The retinal proteome was analyzed by LC-MS/MS, and protein expression compared across groups. Myopic eyes exposed to ON and OFF stimulation exhibited a lower degree of myopia compared with NC myopic eyes. HE staining revealed that NC myopic eyes had a thinner choroid; there was no difference between the two eyes in the ON/OFF group. A total of 646 differentially expressed proteins (DEPs) were observed in the ON, OFF, and NC groups. ON/OFF-specific DEPs affected glycometabolism and RNA function. Reduced activity of the Rap1 signaling pathway was enriched in the both ON/NC DEPs. GZMK, NDFIP2, PARP12, ZC3HAV1, and ID1 were identified as common proteins that remained unaffected by visual stimulations but were implicated in myopia induction solely through negative lens exposure. The DEPs in the three groups also overlapped with proteins associated with human myopia. In conclusion, ON and OFF stimulation suppressed myopia in guinea pigs, with unique DEPs affected; the Rap1 signaling pathway may be a myopia intervention target; and DEPs common across groups are related to oxidative stress and inflammation.
Trial registration
wydw2023-0063/2023.3.5.}
}
@article{HUSSONG2025176,
title = {Selection of manufacturing processes using graph neural networks},
journal = {Journal of Manufacturing Systems},
volume = {80},
pages = {176-193},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525000469},
author = {Marco Hussong and Patrick Ruediger-Flore and Matthias Klar and Marius Kloft and Jan C. Aurich},
keywords = {Manufacturing process selection, Deep learning, Graph neural network, Process planning},
abstract = {The increasing complexity of modern manufacturing, driven by trends such as product customization and shorter product life cycles, presents significant challenges in process planning. Traditional methods for selecting manufacturing processes in industry rely on expert knowledge and manual intervention, which can be time-consuming and error-prone. Systems that can automate the selection of manufacturing processes become increasingly important. Current approaches for the selection of manufacturing processes focus on deep learning that convert the 3D CAD models to intermediate representations such as voxels, point clouds or dexels. However, this transformation can result in the loss of topological, geometrical, or Product and Manufacturing Information (PMI). To address these challenges, this paper proposes a neural network architecture MaProNet. MaProNet is a graph attention neural network (GAT) designed to capture topological and geometrical information through the analysis of Attributed Adjacency Graphs (AAG) and Mesh structures. MaProNet also incorporates a wide range of PMI information.}
}
@article{TSCHILTSCHKE2025407,
title = {Automated LCA through semantic integration of design and process data},
journal = {Procedia CIRP},
volume = {136},
pages = {407-412},
year = {2025},
note = {35th CIRP Design 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.08.071},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125008248},
author = {Till Tschiltschke and Sebastian Wehking and Theresa Riedelsheimer and Kai Lindow},
keywords = {Design for Sustainability, Automated LCA, Semantics in Design, Digital Integration},
abstract = {Global challenges including climate crisis, resource scarcity and regulations are forcing companies to accelerate their sustainable transformation. This paper introduces a digital solution for sustainable design to assess an d optimize the environmental impact of products. An approach for automated LCA based on design data is proposed to overcome the limitations of heterogenous data formats and IT systems. The main results are the semantic mapping of product and LCA process data and its integration into a functional architecture. An implementation in the use case of fuel cell production is described. Moreover, the main challenges and next steps are discussed.}
}
@article{GOLO202273,
title = {Religion and Sustainable Development in Africa: Neo-Pentecostal Economies in Perspective},
journal = {Religion and Development},
volume = {1},
number = {1},
pages = {73-95},
year = {2022},
issn = {2750-7947},
doi = {https://doi.org/10.30965/27507955-20220005},
url = {https://www.sciencedirect.com/science/article/pii/S2750794722000069},
author = {Ben-Willie Kwaku Golo and Ernestina Novieto},
keywords = {neo-Pentecostal economies, Sustainable Development, neo-Pentecostalism, Africa, transformation, entrepreneurship},
abstract = {The secular approach to development has treated religion as anti-developmental. However, the history of how development was part of missionary activity, such as the provision of health and educational infrastructure in some African countries, has been widely acknowledged. In this paper, therefore, we contend that the marginalisation of religion in development discourse is a result of a faulty and fractured understanding of religion. We argue that sustainable development, if attainable in contemporary Africa, would require that organised and institutional religions in Africa as well as their religious cosmologies, convictions and orientations feature and remain integral to such processes. With reference to neo-Pentecostal economies in Africa, we intend to discuss why and how religion – religious cosmologies, ontologies and institutions – is indispensable in the sustainable development process in Africa. Specifically, keeping in focus the human dimensions of development, we intend to argue that the beliefs, teachings and activities of neo-Pentecostal churches on human salvation, progress and/or transformation, such as prosperity and wealth creation, which has seen them emerge on the socioeconomic scene, indicate the potentials of neo-Pentecostals in particular, and religion in general, to contribute immensely to sustainable development. This, however, is not to gloss over some of the challenges they potentially pose to sustainable development.}
}
@article{WALSH20201587,
title = {Transformative education: towards a relational, justice-oriented approach to sustainability},
journal = {International Journal of Sustainability in Higher Education},
volume = {21},
number = {7},
pages = {1587-1606},
year = {2020},
issn = {1467-6370},
doi = {https://doi.org/10.1108/IJSHE-05-2020-0176},
url = {https://www.sciencedirect.com/science/article/pii/S1467637020000188},
author = {Zack Walsh and Jessica Böhme and Brooke D. Lavelle and Christine Wamsler},
keywords = {Sustainability education, Systems thinking, Anti-oppression pedagogy, Contemplative pedagogy, Eco-justice, Relational ontology},
abstract = {Purpose
This paper aims to increase related knowledge across personal, social and ecological dimensions of sustainability and how it can be applied to support transformative learning.
Design/methodology/approach
The paper provides a reflexive case study of the design, content and impact of a course on eco-justice that integrates relational learning with an equity and justice lens. The reflexive case study provides a critical, exploratory self-assessment, including interviews, group discussions and surveys with key stakeholders and course participants.
Findings
The results show how relational approaches can support transformative learning for sustainability and provide concrete practices, pathways and recommendations for curricula development that other universities/training institutions could follow or learn from.
Originality/value
Sustainability research, practice and education generally focuses on structural or systemic factors of transformation (e.g. technology, governance and policy) without due consideration as to how institutions and systems are shaping and shaped by the transformation of personal agency and subjectivity. This presents a vast untapped and under-studied potential for addressing deep leverage points for change by using a relational approach to link personal, societal and ecological transformations for sustainability.}
}
@incollection{CAVALIERE2019586,
title = {Data-Information-Concept Continuum From a Text Mining Perspective},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {586-601},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20408-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338204081},
author = {Danilo Cavaliere and Sabrina Senatore and Vincenzo Loia},
keywords = {Concept mining, Data mining, Information retrieval, Knowledge modelling, Natural language processing, Semantic web},
abstract = {The recent Web panorama reveals a tangible proliferation of “social” data, in form of posts, opinions, feelings, experiences. Most of the available data is unstructured text, unsuitable to be processed by computers, especially due to ambiguity and vagueness of the natural language. Research developments highlight the difficulty in capturing semantics of terms, linguistic expressions, and sentences and their consequent representation as a finite concept. This article presents an open-minded overview of the Text Mining approaches, targeted at transforming unstructured textual data into explicit knowledge, with a special focus on the conceptualization, i.e., the concept identification by analysing syntactic and semantic relations among terms as well as the contextual surrounding information. Different knowledge granulation is described in a layered knowledge model, where the term, the information and the concept represent the basic knowledge granules that cover most Text Mining approaches, in an evolving knowledge continuum.}
}
@article{EVANGELISTALEITE2023122338,
title = {The molecular mechanisms of extracellular matrix-derived hydrogel therapy in idiopathic pulmonary fibrosis models},
journal = {Biomaterials},
volume = {302},
pages = {122338},
year = {2023},
issn = {0142-9612},
doi = {https://doi.org/10.1016/j.biomaterials.2023.122338},
url = {https://www.sciencedirect.com/science/article/pii/S0142961223003460},
author = {Daniele Evangelista-Leite and Ana C.O. Carreira and Milton Y. Nishiyama and Sarah E. Gilpin and Maria A. Miglino},
keywords = {Hydrogel, Decellularized extracellular matrix, Idiopathic pulmonary fibrosis, Therapy},
abstract = {Idiopathic Pulmonary Fibrosis (IPF) is a progressively debilitating lung condition characterized by oxidative stress, cell phenotype shifts, and excessive extracellular matrix (ECM) deposition. Recent studies have shown promising results using decellularized ECM-derived hydrogels produced through pepsin digestion in various lung injury models and even a human clinical trial for myocardial infarction. This study aimed to characterize the composition of ECM-derived hydrogels, assess their potential to prevent fibrosis in bleomycin-induced IPF models, and unravel their underlying molecular mechanisms of action. Porcine lungs were decellularized and pepsin-digested for 48 h. The hydrogel production process, including visualization of protein molecular weight distribution and hydrogel gelation, was characterized. Peptidomics analysis of ECM-derived hydrogel contained peptides from 224 proteins. Probable bioactive and cell-penetrating peptides, including collagen IV, laminin beta 2, and actin alpha 1, were identified. ECM-derived hydrogel treatment was administered as an early intervention to prevent fibrosis advancement in rat models of bleomycin-induced pulmonary fibrosis. ECM-derived hydrogel concentrations of 1 mg/mL and 2 mg/mL showed subtle but noticeable effects on reducing lung inflammation, oxidative damage, and protein markers related to fibrosis (e.g., alpha-smooth muscle actin, collagen I). Moreover, distinct changes were observed in macroscopic appearance, alveolar structure, collagen deposition, and protein expression between lungs that received ECM-derived hydrogel and control fibrotic lungs. Proteomic analyses revealed significant protein and gene expression changes related to cellular processes, pathways, and components involved in tissue remodeling, inflammation, and cytoskeleton regulation. RNA sequencing highlighted differentially expressed genes associated with various cellular processes, such as tissue remodeling, hormone secretion, cell chemotaxis, and cytoskeleton engagement. This study suggests that ECM-derived hydrogel treatment influence pathways associated with tissue repair, inflammation regulation, cytoskeleton reorganization, and cellular response to injury, potentially offering therapeutic benefits in preventing or mitigating lung fibrosis.}
}
@article{AMONOO2023334.e1,
title = {Delirium and Healthcare Utilization in Patients Undergoing Hematopoietic Stem Cell Transplantation},
journal = {Transplantation and Cellular Therapy},
volume = {29},
number = {5},
pages = {334.e1-334.e7},
year = {2023},
issn = {2666-6367},
doi = {https://doi.org/10.1016/j.jtct.2023.01.028},
url = {https://www.sciencedirect.com/science/article/pii/S2666636723000660},
author = {Hermioni L. Amonoo and Netana H. Markovitz and P. Connor Johnson and Anne Kwok and Ciara Dale and Emma C. Deary and Elizabeth Daskalakis and Joanna J. Choe and Nikka Yamin and Maanasi Gothoskar and Katherine G. Cronin and Carlos Fernandez-Robles and William F. Pirl and Yi-Bin Chen and Corey Cutler and Charlotta Lindvall and Areej El-Jawahri},
keywords = {Delirium, Hematopoietic Stem Cell Transplantation, Health care utilization, Natural Language Processing, Clinical Outcomes},
abstract = {ABSTRACT
Delirium, a common neuropsychiatric syndrome among hospitalized patients, has been associated with significant morbidity and mortality in patients undergoing hematopoietic stem cell transplantation (HSCT). Although delirium is often reversible with prompt diagnosis and appropriate management, timely screening of hospitalized patients, including HSCT recipients at risk for delirium, is lacking. The association between delirium symptoms and healthcare utilization among HSCT recipients is also limited. We conducted a retrospective analysis of 502 hospitalized patients admitted for allogeneic or autologous HSCT at 2 tertiary care hospitals between April 2016 and April 2021. We used Natural Language Processing (NLP) to identify patients with delirium symptoms, as defined by an NLP-assisted chart review of the electronic health record (EHR). We used multivariable regression models to examine the associations between delirium symptoms, clinical outcomes, and healthcare utilization, adjusting for patient-, disease-, and transplantation-related factors. Overall, 44.4% (124 of 279) of patients undergoing allogeneic HSCT and 39.0% (87 of 223) of those undergoing autologous HSCT were identified as having delirium symptoms during their index hospitalization. Two-thirds (139 of 211) of the patients with delirium symptoms were prescribed treatment with antipsychotic medications. Among allogeneic HSCT recipients, delirium symptoms were associated with longer hospital length of stay (β = 7.960; P < .001), fewer days alive and out of the hospital (β = -23.669; P < .001), and more intensive care unit admissions (odds ratio, 2.854; P = .002). In autologous HSCT recipients, delirium symptoms were associated with longer hospital length of stay (β = 2.204; P < .001). NLP-assisted EHR review is a feasible approach to identifying hospitalized patients, including HSCT recipients at risk for delirium. Because delirium symptoms are negatively associated with health care utilization during and after HSCT, our findings underscore the need to efficiently identify patients hospitalized for HSCT who are at risk of delirium to improve their outcomes. © 2023 American Society for Transplantation and Cellular Therapy. Published by Elsevier Inc.}
}
@incollection{20251,
title = {Authors},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {1-11},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.09011-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027090114}
}
@article{ALY2025100044,
title = {MBSE for robust decision support systems: A resilient, mission-centric reference model},
journal = {Digital Engineering},
volume = {6},
pages = {100044},
year = {2025},
issn = {2950-550X},
doi = {https://doi.org/10.1016/j.dte.2025.100044},
url = {https://www.sciencedirect.com/science/article/pii/S2950550X2500010X},
author = {Ebrahim Aly and Emiliya Suprun and Hasan Hüseyin Turan and Sondoss Elsawah},
keywords = {Model Based Systems Engineering(MBSE), Complex systems, Decision support systems(DSS), Mission engineering(ME), Resilience, Resilient by design},
abstract = {Decision support systems (DSS) play a key role in navigating complex decisions in various critical domains. Model Based Systems Engineering (MBSE) is proposed to enhance the development of complex software-intensive systems like DSS, focusing on agility and robustness of such systems. This research introduces a reference model for architecting DSS geared toward addressing socio-technical issues. It integrates two well-known frameworks, namely the Department of Defense Architecture Framework (DoDAF) and Integrated Systems Engineering and Pipelines of Processes in Object-Oriented Architecture (ISE&PPOOA) to ensure the alignment of the system design with both overarching strategic objectives and specific stakeholder needs. Emphasizing key non-functional requirements and mission engineering aspects, the model promotes a resilient-by-design and mission-aware architecture. The model also utilizes a service-oriented architecture (SOA) to organize the DSS functionalities into modular services improving its interoperability and robustness. The proposed model offers MBSE practitioners a customizable blueprint to maintain key system qualities, facilitating the creation of robust and adaptable DSS.}
}
@article{ABUSALIH2023e15926,
title = {Emotion detection of social data: APIs comparative study},
journal = {Heliyon},
volume = {9},
number = {5},
pages = {e15926},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e15926},
url = {https://www.sciencedirect.com/science/article/pii/S240584402303133X},
author = {Bilal Abu-Salih and Mohammad Alhabashneh and Dengya Zhu and Albara Awajan and Yazan Alshamaileh and Bashar Al-Shboul and Mohammad Alshraideh},
keywords = {Emotion detection, Emotion analysis, Application programming interfaces, Social emotion analysis, Commercial tools, Comparative study},
abstract = {The development of emotion detection technology has emerged as an efficient possibility in the corporate sector due to the nearly limitless uses of this new discipline, particularly with the unceasing propagation of social data. In recent years, the electronic marketplace has witnessed the establishment of various start-up businesses with an almost sole focus on building new commercial and open-source tools and APIs for emotion detection and recognition. Yet, these tools and APIs must be continuously reviewed and evaluated, and their performances should be reported and discussed. There is a lack of research to empirically compare current emotion detection technologies in terms of the results obtained from each model using the same textual dataset. Also, there is a lack of comparative studies that apply benchmark comparisons to social data. This study compares eight technologies: IBM Watson Natural Language Understanding, ParallelDots, Symanto – Ekman, Crystalfeel, Text to Emotion, Senpy, Textprobe, and Natural Language Processing Cloud. The comparison was undertaken using two different datasets. The emotions from the chosen datasets were then derived using the incorporated APIs. The performance of these APIs was assessed using the aggregated scores they delivered and the theoretically proven evaluation metrics such as the micro-average of accuracy, classification error, precision, recall, and f1-score. Lastly, the assessment of these APIs incorporating the evaluation measures is reported and discussed.}
}
@article{PUZYN2018478,
title = {Perspectives from the NanoSafety Modelling Cluster on the validation criteria for (Q)SAR models used in nanotechnology},
journal = {Food and Chemical Toxicology},
volume = {112},
pages = {478-494},
year = {2018},
issn = {0278-6915},
doi = {https://doi.org/10.1016/j.fct.2017.09.037},
url = {https://www.sciencedirect.com/science/article/pii/S0278691517305562},
author = {Tomasz Puzyn and Nina Jeliazkova and Haralambos Sarimveis and Richard L. {Marchese Robinson} and Vladimir Lobaskin and Robert Rallo and Andrea-N. Richarz and Agnieszka Gajewicz and Manthos G. Papadopulos and Janna Hastings and Mark T.D. Cronin and Emilio Benfenati and Alberto Fernández},
keywords = {Nano-QSAR, QNTR, QNAR, QSAR, Validation},
abstract = {Nanotechnology and the production of nanomaterials have been expanding rapidly in recent years. Since many types of engineered nanoparticles are suspected to be toxic to living organisms and to have a negative impact on the environment, the process of designing new nanoparticles and their applications must be accompanied by a thorough risk analysis. (Quantitative) Structure-Activity Relationship ([Q]SAR) modelling creates promising options among the available methods for the risk assessment. These in silico models can be used to predict a variety of properties, including the toxicity of newly designed nanoparticles. However, (Q)SAR models must be appropriately validated to ensure the clarity, consistency and reliability of predictions. This paper is a joint initiative from recently completed European research projects focused on developing (Q)SAR methodology for nanomaterials. The aim was to interpret and expand the guidance for the well-known “OECD Principles for the Validation, for Regulatory Purposes, of (Q)SAR Models”, with reference to nano-(Q)SAR, and present our opinions on the criteria to be fulfilled for models developed for nanoparticles.}
}
@article{VANHECKEN2018314,
title = {Silencing Agency in Payments for Ecosystem Services (PES) by Essentializing a Neoliberal ‘Monster’ Into Being: A Response to Fletcher & Büscher's ‘PES Conceit’},
journal = {Ecological Economics},
volume = {144},
pages = {314-318},
year = {2018},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2017.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S0921800917309667},
author = {Gert {Van Hecken} and Vijay Kolinjivadi and Catherine Windey and Pamela McElwee and Elizabeth Shapiro-Garza and Frédéric Huybrechs and Johan Bastiaensen},
keywords = {Payments for Ecosystem Services (PES), Neoliberalism, Conservation, Performativity, Actor-oriented approach, Weak theory, Governmentality},
abstract = {In this commentary we respond to Fletcher and Büscher's (2017) recent article in this journal on Payments for Ecosystem Services (PES) as neoliberal ‘conceit’. The authors claim that focusing attention on the micro-politics of PES design and implementation fails to expose an underlying neoliberal governmentality, and therefore only reinforces neoliberal capitalism as both the problem and solution of ecological crises. In response, we argue that a focus on the actions of local actors is key to understanding how and why such governmentality fails or succeeds in performing as theorized. Grand generalizations fixated on a particular hegemonic and neoliberal PES ontology overlook how actors intertwine theory and practice in ways which cannot be explained by a dominant structural theory. Such generalizations risk obscuring the complexity and situational history, practice and scale of the processes involved. Rather than relegating variegated and hybrid forms of what actually emerges from PES interventions as neoliberal conceit, we argue that an actor-oriented, ‘weak theory’ approach permits PES praxis to inform knowledge generation. This would open up a more inclusive and politically engaging space for thinking about and realizing political change.}
}
@article{SON2022101731,
title = {Automated generation of a model view definition from an information delivery manual using idmXSD and buildingSMART data dictionary},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101731},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101731},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001896},
author = {Seungwoo Son and Ghang Lee and Jeaeun Jung and Jungdae Kim and Kahyun Jeon},
keywords = {Information delivery manual, Information delivery specification, Model view definition, buidlingSMART data dictionary, idmXSD, COBie},
abstract = {The development of an integrated information delivery manual (IDM) and model view definition (MVD) using the buildingSMART Data Dictionary (bSDD) as a lexicon has been identified as an ideal way of developing IDMs and MVDs. Several previous studies proposed methods to integrate the IDM and MVD development process. However, they were inherently limited in various ways because they were proposed before the core relevant technologies were ready. This study proposes an integrated IDM and MVD development method using bSDD as a lexicon based on three international standards—ISO 12006-3, ISO 16739-1, and ISO 29481-3. In particular, ISO 29481-3 is deployed as a core data schema to integrate the entire process. ISO 29481-3, of which the authors were the main developers, is a new ISO standard that specifies the information delivery manual XML schema definition (idmXSD). In addition, the concept-based MVD generation algorithm was adopted to generate a syntactically valid MVD. The proposed method was validated against the basic Facility Management (FM) Handover MVD and the Construction–Operations Building Information Exchange (COBie) MVD. The proposed approach is expected to help facilitate the development of IDM and MVD by allowing developers to utilize bSDD as a common lexicon and share and repurpose existing IDMs and their relationships to MVDs in a machine-readable format using idmXSD.}
}
@incollection{FERNANDINO2025421,
title = {Concept representation},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {421-441},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00155-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001558},
author = {Leonardo Fernandino and Jeffrey R. Binder},
keywords = {Language, Categorization, Word meaning, Word comprehension, Lexical semantics, Semantic cognition, Semantic memory, Knowledge representation, Embodiment, Embodied semantics, Embodied cognition, Semantic deficits, Category-specific semantic impairments, Aphasia},
abstract = {The ability to communicate knowledge about the world through language is arguably the most defining trait of the human species, and a neuroscientific account of how language meaning is encoded in the brain is a major goal of modern neuroscience. How can the information content conveyed by a word or a sentence be objectively characterized? How is this information stored in the brain? How does it interface with the brain's stored representations of facts and past experiences? This article provides an overview of recent research bearing on this topic, arguing that a characterization of lexical concepts in terms of multimodal representations derived from sensory, motor, and affective experiences is an essential step toward a mechanistic, neurobiologically rooted account of language meaning.}
}
@article{AYADI2024287,
title = {A Web-based Methodology to Automate foundation of business process model in MDA by AI},
journal = {Procedia Computer Science},
volume = {231},
pages = {287-292},
year = {2024},
note = {14th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 13th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (EUSPN/ICTH 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.12.205},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923022172},
author = {Mohamed EL Ayadi and Yassine Rhazali and Mohammed Lahmer},
keywords = {MDA;CIM;PIM;PSM;Model Transformations},
abstract = {MDA represents a fundamental standard established by the Object Management Group (OMG) that serves as a foundation for model-driven development. Within the MDA framework, the computation-independent model (CIM) holds the position of the initial abstraction level. As a business model, the CIM describes the requirements and environment of a business, playing a pivotal role in influencing all other levels of the MDA approach. Recognizing the significance of this stage in MDA, the primary objective of this paper is to propose a web-based approach for automating the construction of CIM. This approach utilizes web interfaces to gather relevant information from users about the business model. Furthermore, an illustrative example is presented to provide a clearer demonstration of the application of this approach.}
}
@article{KIFETEW2021106635,
title = {Automating user-feedback driven requirements prioritization},
journal = {Information and Software Technology},
volume = {138},
pages = {106635},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106635},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921001014},
author = {Fitsum Meshesha Kifetew and Anna Perini and Angelo Susi and Aberto Siena and Denisse Muñante and Itzel Morales-Ramirez},
keywords = {Automated user-feedback analysis, Automated requirements prioritization, Empirical study},
abstract = {Context:
Feedback from end users of software applications is a valuable resource in understanding what users request, what they value, and what they dislike. Information derived from user-feedback can support software evolution activities, such as requirements prioritization. User-feedback analysis is still mostly performed manually by practitioners, despite growing research in automated analysis.
Objective:
We address two issues in automated user-feedback analysis: (i) most of the existing automated analysis approaches that exploit linguistic analysis assume that the vocabulary adopted by users (when expressing feedback) and developers (when formulating requirements) are the same; and (ii) user-feedback analysis techniques are usually experimentally evaluated only on some user-feedback dataset, not involving assessment by potential software developers.
Method:
We propose an approach, ReFeed, that computes, for each requirement, the set of related user-feedback, and from such user-feedback extracts quantifiable properties which are relevant for prioritizing the requirement. The extracted properties are propagated to the related requirements, based on which ranks are computed for each requirement. ReFeed relies on domain knowledge, in the form of an ontology, helping mitigate the gap in the vocabulary of end users and developers. The effectiveness of ReFeed is evaluated on a realistic requirements prioritization scenario in two experiments involving graduate students from two different universities.
Results:
ReFeed is able to synthesize reasonable priorities for a given set of requirements based on properties derived from user-feedback. The implementation of ReFeed and related resources are publicly available.
Conclusion:
The results from our studies are encouraging in that using only three properties of user-feedback, ReFeed is able to prioritize requirements with reasonable accuracy. Such automatically determined prioritization could serve as a good starting point for requirements experts involved in the task of prioritizing requirements Future studies could explore additional user-feedback properties to improve the effectiveness of computed priorities.}
}
@article{NAPIER2021100944,
title = {Accounting for heritage assets: Thomas Holloway's picture collection, 1881–2019},
journal = {The British Accounting Review},
volume = {53},
number = {2},
pages = {100944},
year = {2021},
issn = {0890-8389},
doi = {https://doi.org/10.1016/j.bar.2020.100944},
url = {https://www.sciencedirect.com/science/article/pii/S0890838920300640},
author = {Christopher J. Napier and Elena Giovannoni},
keywords = {Heritage assets, Thomas Holloway, Victorian painting, Royal Holloway College, Counterfeit, Simulacrum, Financial reporting, University accounts},
abstract = {In recent years, there has been a debate about whether the owners of “heritage assets” should include them on their balance sheets. We present a longitudinal study of the collection of 77 pictures donated by Thomas Holloway to Royal Holloway College between 1881 and 1883. We draw on archival material to analyse accounting practices for Holloway's picture collection, finding that the collection remained effectively invisible as an accounting object until 1999, when accounting requirements for heritage assets were first applied. We use Jean Baudrillard's “orders of simulacra” to study the relationship between accounting signs and their referents, and we draw on Bruno Latour's notion of “matters of concern” to investigate how changes in the accounting sign render the referent a complicating, agitating and provoking “matter” in different ways. The Royal Holloway financial statements currently present the picture collection by an accounting sign that we suggest is a “counterfeit” (signifying the money that could, counterfactually, be made from selling the paintings) but not a “simulation” (creating a hyperreality detached from the referent). This relationship between the sign and the referent makes up the ontological status of “assets” in accounting reports, rendering assets capable of triggering actual (rather than hyperreal) material effects.}
}
@article{ILYIN2025149329,
title = {Neurotranscriptomic and behavioral effects of ISRIB, and its therapeutic effects in the traumatic brain injury model in zebrafish},
journal = {Brain Research},
volume = {1848},
pages = {149329},
year = {2025},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2024.149329},
url = {https://www.sciencedirect.com/science/article/pii/S0006899324005833},
author = {Nikita P. Ilyin and Anton D. Shevlyakov and Galina A. Boyko and Anastasia M. Moskalenko and Aleksey N. Ikrin and David S. Galstyan and Tatiana O. Kolesnikova and Nataliia V. Katolikova and Sergei A. Chekrygin and Lee Wei Lim and LongEn Yang and Murilo S. {De Abreu} and Konstantin B. Yenkoyan and Allan V. Kalueff and Konstantin A. Demin},
keywords = {ISRIB, Integrated Stress Response, Traumatic brain injury, Transcriptome, Zebrafish, Behavioral deficits},
abstract = {Traumatic brain injury (TBI) is a global medical concern and has a lasting impact on brain activity with high risks of mortality. Current treatments are inadequate for repairing damaged brain cells or correcting cognitive and behavioral disabilities in TBI patients. Mounting evidence links TBI to the activation of the Integrated Stress Response (ISR) signaling in the brain. A novel small molecule, ISRIB, is an effective inhibitor of the ISR pathway, offering potential advantages for brain health. Here, we investigated how ISRIB affects brain transcriptome and behavior in zebrafish TBI model evoked by telencephalic brain injury. Overall, while TBI diminished memory and social behavior in zebrafish, administering ISRIB post-injury markedly reduced these behavioral deficits, and modulated brain gene expression, rescuing TBI-activated pathways related to inflammation and brain cell development. Collectively, this supports the role of brain ISR in TBI, and suggests potential utility of ISRIB for the treatment of TBI-related states.}
}
@article{KONG2025100560,
title = {A network analysis framework for traditional Chinese medicine in treating Alzheimer’s disease: From core herbal pairs to key component networks},
journal = {Chinese Journal of Analytical Chemistry},
volume = {53},
number = {8},
pages = {100560},
year = {2025},
issn = {1872-2040},
doi = {https://doi.org/10.1016/j.cjac.2025.100560},
url = {https://www.sciencedirect.com/science/article/pii/S1872204025000702},
author = {Fanjing KONG and Weiming ZHANG and Tianyu WU and Jingyi DAI and Ying XU and Tao SUN},
keywords = {Alzheimer’s disease, Traditional Chinese medicine, Data mining, Network pharmacology},
abstract = {Alzheimer’s disease (AD) is a chronic neurodegenerative disorder for which traditional Chinese medicine (TCM) has shown potential therapeutic advantages. This study aimed to explore the medication patterns and core herbal pairs used in the treatment of AD by integrating data mining, network pharmacology, and molecular docking techniques. Relevant literature on TCM interventions for AD was retrieved from five major databases, and core herbal pairs were identified. To further investigate the multi-target and multi-pathway mechanisms of these pairs and their key components, a comprehensive network was constructed. A core herbal pair—Acori Tatarinowii Rhizoma-Polygalae Radix was selected, and its component–target–pathway network was established. Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analyses were conducted to elucidate biological functions and signaling pathways, while molecular docking was performed to validate the interactions between key compounds and their targets. A total of 482 TCM formulas for AD were analyzed, identifying Acori Tatarinowii Rhizoma-Polygalae Radix as the most representative core pair. Its major active components, including phenylpropanoids (e.g., caffeic acid, β-asarone) and saponins (e.g., tenuigenin), were found to modulate inflammation, apoptosis, and synaptic plasticity through critical pathways such as MAPK and PI3K-Akt. Molecular docking demonstrated strong binding affinities between these key components and pivotal targets such as AKT1 and MAPK3. This study establishes a multi-layered framework, from big data analysis to functional validation of specific components, offering a clear pathway for investigating the structure-effect relationships in TCM. Furthermore, it proposes an innovative computational approach for analyzing the complex mechanisms of TCM, providing new insights into optimizing TCM formulations and developing novel drugs for AD.}
}
@article{SUN2023160019,
title = {Metabolic and metatranscriptional characteristics of corals bleaching induced by the most severe marine heatwaves in the South China Sea},
journal = {Science of The Total Environment},
volume = {858},
pages = {160019},
year = {2023},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.160019},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722071194},
author = {Fulin Sun and Hongqiang Yang and Xiyang Zhang and Qi Shi},
keywords = {High temperature, Coral bleaching, Metabolomics analysis, Metatranscriptomic analysis, Metabolic pathway},
abstract = {Coral bleaching significantly affects the function and health of coral reef ecosystems; however, the mechanisms underlying metabolism and transcription in corals remain unclear. In this study, untargeted metabolomics and metatranscriptomic analyses were performed to analyze the differences between unbleached and bleached Pocillopora corals during the most severe marine heatwaves. Difference analysis showed that bleached corals had significant metabolomic characteristics compared with those in unbleached corals. These differences were significant (p < 0.05) according to partial least squares discriminant analysis (PLS-DA). Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analysis revealed that the metabolites were significantly enriched in numerous pathways in bleached or unbleached corals, such as steroid hormone biosynthesis, biosynthesis of unsaturated fatty acids, and pyrimidine metabolism. Bleaching greatly affects coral reproduction as well as the tolerance of coral symbionts to heat stress. In metatranscriptomic analysis, we observed large gene expression differences between unbleached and bleached corals. Three Gene Ontology directed acyclic graphs (DAGs) were constructed to show the significantly differentially expressed genes (DEGs). Many biological and molecular processes were significantly enriched between bleached corals to unbleached corals, such as metabolic processes, lipid metabolic processes, oxidation-reduction processes, single-organism metabolic processes, and protein metabolic processes. Metabolome and metatranscriptome analyses showed that bleaching caused substantial physiological damage to corals. This study provides insight into the metabolic and transcriptional changes that occur in corals during bleaching.}
}
@article{ZHANG2025999,
title = {Development and application of a nonlinear stress dilatancy model for geocell-reinforced soil via the FEM},
journal = {Geotextiles and Geomembranes},
volume = {53},
number = {4},
pages = {999-1020},
year = {2025},
issn = {0266-1144},
doi = {https://doi.org/10.1016/j.geotexmem.2025.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0266114425000408},
author = {Bingbing Zhang and Fei Song and Junding Liu},
keywords = {Geocells, UMAT subroutine, Jacobian stiffness matrix, Triaxial test, Foundation/retaining wall model test},
abstract = {To address the ambiguities in current ontological models of geocell-reinforced soil and the limitations inherent in finite element analysis methods, a nonlinear stress dilatancy model (NSDM) encompassing geocell-reinforced soil was successfully formulated. This model is based on the interaction between the geocell and the infilled soil, which can consider the confining pressures provided by the geocells and the stress dilatancy model of the soil. A finite element method (FEM) implementation of the model was achieved via the User-defined Material (UMAT) subroutine interface provided by ABAQUS software. Validation of the model was achieved via triaxial tests on geocell-reinforced sand with varying relative densities, as well as reinforced foundation and retaining wall model tests. Concurrently, the model calculation results were compared and analyzed with those obtained from a conventional separated model, and an in-depth exploration of the sensitivity of the model's key parameters was carried out. The findings demonstrate that the UMAT subroutine of the model can accurately predict the reinforced sand triaxial test, the reinforced foundation model test and the retaining wall model test results. Compared with the reinforced soil-separated model, the model delineated in this paper is easier to construct and has markedly improved computational efficiency. Additionally, the model can capture failure within the geocell fill, thereby affording a more precise depiction in the near-failure stage. This research offers an efficient and practical novel methodology for numerical analysis within the domain of geocell-reinforced soil.}
}
@article{JI2020102305,
title = {An end-to-end joint model for evidence information extraction from court record document},
journal = {Information Processing & Management},
volume = {57},
number = {6},
pages = {102305},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102305},
url = {https://www.sciencedirect.com/science/article/pii/S0306457320308001},
author = {Donghong Ji and Peng Tao and Hao Fei and Yafeng Ren},
keywords = {Natural language processing, Information extraction, Court record document, Neural networks, Joint model},
abstract = {Information extraction is one of the important tasks in the field of Natural Language Processing (NLP). Most of the existing methods focus on general texts and little attention is paid to information extraction in specialized domains such as legal texts. This paper explores the task of information extraction in the legal field, which aims to extract evidence information from court record documents (CRDs). In the general domain, entities and relations are mostly words and phrases, indicating that they do not span multiple sentences. In contrast, evidence information in CRDs may span multiple sentences, while existing models cannot handle this situation. To address this issue, we first add a classification task in addition to the extraction task. We then formulate the two tasks as a multi-task learning problem and present a novel end-to-end model to jointly address the two tasks. The joint model adopts a shared encoder followed by separate decoders for the two tasks. The experimental results on the dataset show the effectiveness of the proposed model, which can obtain 72.36% F1 score, outperforming previous methods and strong baselines by a large margin.}
}
@article{YAMAMOTO2019973,
title = {Aspect Analysis towards ArchiMate Diagrams},
journal = {Procedia Computer Science},
volume = {159},
pages = {973-980},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.264},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314607},
author = {Shuichiro Yamamoto and Qiang Zhi and Zhengshu Zhou},
keywords = {Enterprise Architecture, ArchiMate, Aspect Analysis, Workshop Design},
abstract = {ArchiMate is the language to model Enterprise Architecture using diagrams. Although ArchiMate is the powerful language to describe the whole content of Enterprise Architecture, it is not easy to learn and use correctly ArchiMate because it has approximately 60 graphical icons. The application knowledge how to use ArchiMate is highly expected. In this paper, we propose the aspect analysis approach to design ArchiMate diagrams. A simple example to develop ArchiMate diagram using the approach is also explained. Moreover, a workshop design to educate ArchiMate using the approach for software engineers is discussed.}
}
@article{PARIMBELLI2021102111,
title = {A review of AI and Data Science support for cancer management},
journal = {Artificial Intelligence in Medicine},
volume = {117},
pages = {102111},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102111},
url = {https://www.sciencedirect.com/science/article/pii/S0933365721001044},
author = {E. Parimbelli and S. Wilk and R. Cornet and P. Sniatala and K. Sniatala and S.L.C. Glaser and I. Fraterman and A.H Boekhout and M. Ottaviano and M. Peleg},
keywords = {Cancer, Decision support system, Data science, Data integration, Patient reported outcomes, Quality of life, Artificial intelligence, Predictive modeling, Patient coaching},
abstract = {Introduction
Thanks to improvement of care, cancer has become a chronic condition. But due to the toxicity of treatment, the importance of supporting the quality of life (QoL) of cancer patients increases. Monitoring and managing QoL relies on data collected by the patient in his/her home environment, its integration, and its analysis, which supports personalization of cancer management recommendations. We review the state-of-the-art of computerized systems that employ AI and Data Science methods to monitor the health status and provide support to cancer patients managed at home.
Objective
Our main objective is to analyze the literature to identify open research challenges that a novel decision support system for cancer patients and clinicians will need to address, point to potential solutions, and provide a list of established best-practices to adopt.
Methods
We designed a review study, in compliance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, analyzing studies retrieved from PubMed related to monitoring cancer patients in their home environments via sensors and self-reporting: what data is collected, what are the techniques used to collect data, semantically integrate it, infer the patient’s state from it and deliver coaching/behavior change interventions.
Results
Starting from an initial corpus of 819 unique articles, a total of 180 papers were considered in the full-text analysis and 109 were finally included in the review. Our findings are organized and presented in four main sub-topics consisting of data collection, data integration, predictive modeling and patient coaching.
Conclusion
Development of modern decision support systems for cancer needs to utilize best practices like the use of validated electronic questionnaires for quality-of-life assessment, adoption of appropriate information modeling standards supplemented by terminologies/ontologies, adherence to FAIR data principles, external validation, stratification of patients in subgroups for better predictive modeling, and adoption of formal behavior change theories. Open research challenges include supporting emotional and social dimensions of well-being, including PROs in predictive modeling, and providing better customization of behavioral interventions for the specific population of cancer patients.}
}
@article{MADSEN2022103671,
title = {Soft City Sensing: A turn to computational humanities in data-driven urbanism},
journal = {Cities},
volume = {126},
pages = {103671},
year = {2022},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2022.103671},
url = {https://www.sciencedirect.com/science/article/pii/S026427512200110X},
author = {Anders Koed Madsen and Anders Grundtvig and Sofie Thorsen},
keywords = {Soft City Sensing, Urban studies, Big data, Social web, Digital city, Computational humanities},
abstract = {Data-driven urbanism is often entangled with the smart city and practiced in a way that prioritizes control over physical objects and downplays the human and political aspects of data. We label this approach ‘hard city sensing’ (HCS) and we argue that the rise of the ‘digital city’ offers the empirical foundation for more humanistic approaches. Driven by the ambition to untangle data-driven urbanism from HCS, this paper reviews two decades of scholarship that has used digital traces as an empirical ground for understanding urban phenomena. The review identifies four distinct ways of working with digital traces of which three pave the way for new ways of problematizing the city. Instead of abandoning the idea of data-driven urbanism, we propose the framework of 'soft city sensing' (SCS) as way to re-engage with it with inspiration from these pioneering works. However, this requires a willingness to revisit central epistemological commitments that currently serve as standards for how to “properly” do data projects. We therefore urge qualitative urban scholars to ponder the possibilities of furthering their urban interest by ‘thinking with algorithms’ while retaining their interpretative ambitions just as we identify a need for urban decion-makers to expand their criteria for what serves as valid data inputs to urban planning.}
}
@article{CAO2024108201,
title = {DCSGMDA: A dual-channel convolutional model based on stacked deep learning collaborative gradient decomposition for predicting miRNA-disease associations},
journal = {Computational Biology and Chemistry},
volume = {113},
pages = {108201},
year = {2024},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2024.108201},
url = {https://www.sciencedirect.com/science/article/pii/S1476927124001890},
author = {Xu Cao and Pengli Lu},
keywords = {miRNA-disease, Deep learning, Gradient decomposition network, Convolutional neural network},
abstract = {Numerous studies have shown that microRNAs (miRNAs) play a key role in human diseases as critical biomarkers. Its abnormal expression is often accompanied by the emergence of specific diseases. Therefore, studying the relationship between miRNAs and diseases can deepen the insights of their pathogenesis, grasp the process of disease onset and development, and promote drug research of specific diseases. However, many undiscovered relationships between miRNAs and diseases remain, significantly limiting research on miRNA-disease correlations. To explore more potential correlations, we propose a dual-channel convolutional model based on stacked deep learning collaborative gradient decomposition for predicting miRNA-disease associations (DCSGMDA). Firstly, we constructed similarity networks for miRNAs and diseases, as well as an association relationship network. Secondly, potential features were fully mined using stacked deep learning and gradient decomposition networks, along with dual-channel convolutional neural networks. Finally, correlations were scored by a multilayer perceptron. We performed 5-fold and 10-fold cross-validation experiments on DCSGMDA using two datasets based on the Human MicroRNA Disease Database (HMDD). Additionally, parametric, ablation, and comparative experiments, along with case studies, were conducted. The experimental results demonstrate that DCSGMDA performs well in predicting miRNA-disease associations.}
}
@incollection{BADRY202263,
title = {Chapter Three - Enhancing collaboration across the knowledge system boundaries of ecosystem governance},
editor = {Jennifer M. Holzer and Julia Baird and Gordon M. Hickey},
series = {Advances in Ecological Research},
publisher = {Academic Press},
volume = {66},
pages = {63-88},
year = {2022},
booktitle = {Pluralism in Ecosystem Governance},
issn = {0065-2504},
doi = {https://doi.org/10.1016/bs.aecr.2022.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0065250422000101},
author = {Nathan A. Badry and Gordon M. Hickey},
keywords = {Social-ecological systems, Natural resource management, Traditional ecological knowledge, Collaborative governance, Boundary spanning},
abstract = {In ecosystem governance, due to the ecological, social, and jurisdictional complexity of these systems, pluralities of knowledge are increasingly necessary for informed decision-making. However, there are frictions between different kinds of knowledges, whether scientific, bureaucratic, or local. The bringing together of Indigenous and Western knowledge systems is an especially intractable challenge. Indigenous knowledge is frequently incommensurable with dominant scientific frameworks, particularly in how many Indigenous Peoples conceptualize the social roles of nonhumans. Nonetheless, Indigenous and scientific knowledges are regularly brought together in contexts ranging from wildlife co-management to global environmental assessments, and this can involve Indigenous knowledge being selectively adopted, integrated, translated, or just ignored to fit within those frameworks. In this literature review we suggest that Actor-Network Theory—a way of accounting for the webs of relations constituting and generating social and natural worlds—has the potential to help researchers and practitioners challenge the ontological boundaries that make this work difficult. ANT, as well as other approaches under the wider umbrella of posthumanism, could fit within existing pluralist frameworks like Two-Eyed Seeing which strive to address entrenched power dynamics in work involving Indigenous and non-Indigenous Peoples, and help inspire new participatory methods for understanding and enhancing knowledge pluralism in governance.}
}
@article{MOLENAAR2025107648,
title = {Concept definition review: A method for studying terminology in software engineering},
journal = {Information and Software Technology},
volume = {180},
pages = {107648},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107648},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924002532},
author = {Sabine Molenaar and Nikita {van den Berg} and Fabiano Dalpiaz and Sjaak Brinkkemper},
keywords = {Literature review, Research method, Concept definition, Software engineering, Requirements engineering},
abstract = {Context:
In scientific domains, definitions provide a precise description of fundamental concepts. Although the debate within the philosophy of computer science regarding the scientific nature of software engineering (SE) is inconclusive, SE researchers have laid down important steps toward treating SE as a scientific paradigm.
Objective:
We aim to support precise and effective communication among SE researchers and practitioners by providing a systematic process for the identification and analysis of definitions, in order to support the selection of a suitable definition for a certain use case.
Method:
Inspired by methods for the planning and execution of systematic literature reviews, we construct a method that is specific for concept definition reviews (CDRs). These reviews are performed whenever a research team wishes to obtain a detailed understanding of an SE concept that may have been characterized by dozens, if not hundreds, definitions.
Results:
We built our method via two design science iterations. The first one focused on the concept feature and resulted in the definitive version of the CDR method presented in this paper. We then applied the revised method to two, related concepts: quality requirement and non-functional requirement. Besides showing the applicability of the CDR method, our results include findings regarding the characteristics and evolution of the terms.
Conclusions:
The two applications of the CDR method highlight the existence and citation of hundreds of definitions, many of which are nearly (but not exactly) identical. We put forward our method for other researchers to shed light on the key terminology in other sub-fields of SE.}
}
@article{WEI2024150570,
title = {Bioinformatics analysis and validation of RNA methylation-related genes in osteogenic and adipogenic differentiation of rat bone marrow mesenchymal stem cells},
journal = {Biochemical and Biophysical Research Communications},
volume = {739},
pages = {150570},
year = {2024},
issn = {0006-291X},
doi = {https://doi.org/10.1016/j.bbrc.2024.150570},
url = {https://www.sciencedirect.com/science/article/pii/S0006291X24011069},
author = {Li Wei and Yuping Xie and Peiyang Yu and Qiang Zhu and Xiaorong Lan and Jingang Xiao},
keywords = {Bioinformatics analysis, rat bone marrow mesenchymal stem cells (rBMSCs), Osteogenic differentiation, Adipogenic differentiation, RNA methylation, mA},
abstract = {Background
The regulatory mechanisms of RNA methylation during the processes of osteogenic and adipogenic differentiation of bone marrow mesenchymal stem cells (BMSCs) have yet to be fully understood. The objective of our study was to analyze and validate the contribution of RNA methylation regulators to the mechanisms underlying the osteogenic and adipogenic differentiation of rat BMSCs.
Methods
We downloaded the GSE186026 from the Gene Expression Omnibus (GEO). Differentially expressed genes (DEGs) were screened using the DESeq2 package in R software (version 3.6.3). A total of 50 RNA methylation genes obtained from literature review and summary were intersected with the previous DEGs to obtain RNA methylation genes, which have different expressions (RM-DEGs). Gene Ontology (GO) analysis and Kyoto Encyclopedia of Genes and Genomes (KEGG) analysis were utilized to reveal the functional enrichment. Quantitative real-time polymerase chain reaction (qRT-PCR) was performed to validate RM-DEGs. Protein-protein interaction network (PPI) analysis and visual analysis were performed using STRING and Cytoscape. RM-DEGs regulatory network was constructed to analyze the top 10 hub genes. The relationship between RM-DEGs, some enriched GO and pathways was also been analyzed. The miRNAs and RM-DEGs regulatory networks were established by using miRWalk and TargetScan.
Results
As part of our research, we detected varying levels of expression for m6A regulators Mettl3 and Rbm15, as well as m7G regulators Mettl1 and Wdr4, in relation to osteogenic differentiation, along with m6A regulator Fmr1 in adipogenic differentiation. The protein-protein interaction (PPI) networks were constructed for 49 differentially expressed genes (DEGs) related to RNA methylation during the process of osteogenic differentiation, and 13 DEGs for adipogenic differentiation. Moreover, top10 hub genes were calculated. In osteogenic differentiation, Mettl3 regulated the Wnt pathway and Hippo pathway by regulating Smad3, Rbm15 regulated the Notch pathway by Notch1, Mettl1 regulated the PI3K-Akt pathway by Gnb4. In adipogenic differentiation, Fmr1 regulated the PI3K-Akt pathway by Egfr. M6A methylation sites of Smad3, Notch1 and Gnb4 were predicted, and the results showed that all three genes were possibly methylated by m6A, and more than 9 sites per gene were possibly methylated. Finally, we constructed the regulatory networks of Mettl3, Rbm15, Mettl1, and Wdr4 and 109 miRNAs in osteogenic differentiation, Fmr1 and 118 miRNAs in adipogenic differentiation.
Conclusions
Mettl3(m6A), Rbm15(m6A), Wdr4 and Mettl1(m7G) were differentially expressed in osteogenic differentiation, while Fmr1(m6A) was differentially expressed in adipogenic differentiation. These findings offered potential candidates for further research on the involvement of RNA methylation in the osteogenic and adipogenic differentiation of BMSCs.}
}
@article{RIEGER201843,
title = {A process-oriented modeling approach for graphical development of mobile business apps},
journal = {Computer Languages, Systems & Structures},
volume = {53},
pages = {43-58},
year = {2018},
issn = {1477-8424},
doi = {https://doi.org/10.1016/j.cl.2018.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1477842417301215},
author = {Christoph Rieger and Herbert Kuchen},
keywords = {Graphical DSL, Mobile application, Business app, Model-driven software development, Data model inference},
abstract = {Mobile app development is an activity predominantly performed by software developers. Domain experts and future users are merely considered in early development phases as source of requirements or consulted for evaluating the resulting product. In the domain of business apps, many cross-platform programming frameworks exist but approaches also targeted at non-technical users are rare. Existing graphical notations for describing apps either lack the simplicity to be understandable by domain experts or are not expressive enough to support automated processing. The MAML framework is proposed as model-driven approach for describing mobile apps in a platform-agnostic fashion not only for software developers but also for process modelers and domain experts. Data, views, business logic, and user interactions are jointly modeled from a process perspective using a graphical domain-specific language. To aggregate multiple use cases and provide advanced modeling support, an inference mechanism is utilized to deduce a global data model. Through model transformations, native apps are then automatically generated for multiple platforms without manual programming. Our approach is compared to the IFML notation in an observational study, with promising results regarding readability and usability.}
}
@article{ZAGHOUANI2025101591,
title = {SemChain : A Blockchain-based semantic discovery on distributed resource directories for the Internet of Things},
journal = {Internet of Things},
volume = {31},
pages = {101591},
year = {2025},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2025.101591},
url = {https://www.sciencedirect.com/science/article/pii/S2542660525001040},
author = {Kheireddine Zaghouani and Badis Djamaa and Ali Yachir and Saïd Mahmoudi},
keywords = {Semantic Web of Things, Internet of Things, Blockchain, CoRE Resource Directory, CoAP},
abstract = {As the Web evolves towards Web 3.0, integrating the Internet of Things (IoT), the Semantic Web, and Blockchain (BC) technology, managing the growing number of IoT devices and ensuring their interoperability and trust becomes increasingly critical. Centralized solutions are prone to single points of failure, while distributed systems face synchronization and consensus issues. Although integrating BC into IoT has shown promise in addressing these challenges, existing approaches often overlook the resource limitations of IoT devices, the importance of standardized IoT protocols, and the impact of BC consensus mechanisms on network performance. To bridge these gaps, this paper presents SemChain: a framework integrating BC within the semantic web of things in a robust, resource-friendly, and trustworthy manner. Leveraging the CoAP standard, a distributed network of resource directories, and a permissioned BC with Smart Contracts (SC), SemChain strengthens the security and trust of semantic resource registration and discovery in IoT environments. Other key contributions include proposing two data storage approaches, namely SemChain-Full and SemChain-Hash, devising multiple SC transactions, and providing a detailed description of a prototype implementation. The framework’s performance is evaluated against state-of-the-art approaches within a smart hospital use case, demonstrating notable improvements, including an average precision of 92% and a recall of 88% in resource discovery.}
}
@article{MALHAS2022103068,
title = {Arabic machine reading comprehension on the Holy Qur’an using CL-AraBERT},
journal = {Information Processing & Management},
volume = {59},
number = {6},
pages = {103068},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103068},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322001704},
author = {Rana Malhas and Tamer Elsayed},
keywords = {Classical Arabic, Reading comprehension, Answer extraction, Partial matching evaluation, Pre-trained language models, Cross-lingual transfer learning},
abstract = {In this work, we tackle the problem of machine reading comprehension (MRC) on the Holy Qur’an to address the lack of Arabic datasets and systems for this important task. We construct QRCD as the first Qur’anic Reading Comprehension Dataset, composed of 1,337 question-passage-answer triplets for 1,093 question-passage pairs, of which 14% are multi-answer questions. We then introduce CLassical-AraBERT (CL-AraBERT for short), a new AraBERT-based pre-trained model, which is further pre-trained on about 1.0B-word Classical Arabic (CA) dataset, to complement the Modern Standard Arabic (MSA) resources used in pre-training the initial model, and make it a better fit for the task. Finally, we leverage cross-lingual transfer learning from MSA to CA, and fine-tune CL-AraBERT as a reader using two MSA-based MRC datasets followed by our QRCD dataset to constitute the first (to the best of our knowledge) MRC system on the Holy Qur’an. To evaluate our system, we introduce Partial Average Precision (pAP) as an adapted version of the traditional rank-based Average Precision measure, which integrates partial matching in the evaluation over multi-answer and single-answer MSA questions. Adopting two experimental evaluation setups (hold-out and cross validation (CV)), we empirically show that the fine-tuned CL-AraBERT reader model significantly outperforms the baseline fine-tuned AraBERT reader model by 6.12 and 3.75 points in pAP scores, in the hold-out and CV setups, respectively. To promote further research on this task and other related tasks on Qur’an and Classical Arabic text, we make both the QRCD dataset and the pre-trained CL-AraBERT model publicly available.}
}
@article{LI202018,
title = {Teaching Natural Language Processing through Big Data Text Summarization with Problem-Based Learning},
journal = {Data and Information Management},
volume = {4},
number = {1},
pages = {18-43},
year = {2020},
issn = {2543-9251},
doi = {https://doi.org/10.2478/dim-2020-0003},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122000572},
author = {Liuqing Li and Jack Geissinger and William A. Ingram and Edward A. Fox},
keywords = {information system education, computer science education, problem-based learning, natural language processing, NLP, big data text analytics, machine learning, deep learning},
abstract = {Natural language processing (NLP) covers a large number of topics and tasks related to data and information management, leading to a complex and challenging teaching process. Meanwhile, problem-based learning is a teaching technique specifically designed to motivate students to learn efficiently, work collaboratively, and communicate effectively. With this aim, we developed a problem-based learning course for both undergraduate and graduate students to teach NLP. We provided student teams with big data sets, basic guidelines, cloud computing resources, and other aids to help different teams in summarizing two types of big collections: Web pages related to events, and electronic theses and dissertations (ETDs). Student teams then deployed different libraries, tools, methods, and algorithms to solve the task of big data text summarization. Summarization is an ideal problem to address learning NLP since it involves all levels of linguistics, as well as many of the tools and techniques used by NLP practitioners. The evaluation results showed that all teams generated coherent and readable summaries. Many summaries were of high quality and accurately described their corresponding events or ETD chapters, and the teams produced them along with NLP pipelines in a single semester. Further, both undergraduate and graduate students gave statistically significant positive feedback, relative to other courses in the Department of Computer Science. Accordingly, we encourage educators in the data and information management field to use our approach or similar methods in their teaching and hope that other researchers will also use our data sets and synergistic solutions to approach the new and challenging tasks we addressed.}
}
@article{CAO2021101352,
title = {A function-oriented biologically analogical approach for constructing the design concept of smart product in Industry 4.0},
journal = {Advanced Engineering Informatics},
volume = {49},
pages = {101352},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101352},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621001051},
author = {Guozhong Cao and Yindi Sun and Runhua Tan and Jinpu Zhang and Wei Liu},
keywords = {Smart product design, Conceptual design, Bologically analogical approach, Creative design, Industry 4.0},
abstract = {The development of the Industry 4.0 paradigm and the advancement of information technology have aroused new consumer requirements for smart products that are capable of context awareness and autonomous control. Nature holds huge potential for inspiring innovative design concepts that can meet the ever-growing need for smart products since biology perceive and interact with their living environment for survival. However, to date, very few studies have explored the application of natural wisdom in building innovative design concepts for smart products. This paper proposes a function-oriented design approach for smart products, by analogizing to biological prototypes. To do so, a unified functional representation, based on the Function–behavior–structure (FBS) ontology, is proposed to abstract biological prototypes, followed by a fuzzy triangular numbers-based algorithm designed to locate appropriate biological prototypes as analogical sources for smart product development. Moreover, functional innovative strategies and a hybrid design process are formulated to develop design concepts of smart products, by integrating several existing engineering design methods. Finally, an illustrative design case of a smart natural resource collecting system is used to demonstrate the workability of the proposed method.}
}
@article{CHEN2021,
title = {Exploration of the molecular targets and mechanisms of suxiao xintong dropping pills for myocardial infarction by network pharmacology method},
journal = {Bioscience Reports},
volume = {41},
number = {8},
year = {2021},
issn = {1573-4935},
doi = {https://doi.org/10.1042/BSR20204211},
url = {https://www.sciencedirect.com/science/article/pii/S157349352100271X},
author = {Daqiu Chen and Yanqing Wu and Yixing Chen and Qiaoxing Chen and Xianhua Ye and Shanghua Xu and Shunxiang Luo},
keywords = {Chinese Traditional, Medicine, Molecular Docking Simulation, Myocardial Infarction, Pharmacology},
abstract = {Background: Suxiao Xintong dropping pills (SXXTDP), a traditional Chinese medicine, is widely applied for treating myocardial infarction (MI). However, its therapy mechanisms are still unclear. Therefore, this research is designed to explore the molecular mechanisms of SXXTDP in treating MI.
Methods: The active ingredients of SXXTDP and their corresponding genes of the active ingredients were retrieved from the Traditional Chinese Medicine Systems Pharmacology (TCMSP) database. MI-related genes were identified via analyzing the expression profiling data (accession number: GSE97320). Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway enrichment analysis were performed to study the shared genes of drug and disease. Through protein–protein interaction (PPI) network and the Cytoscape plugin cytoHubba, the hub genes were screened out. The compounds and hub targets binding were simulated through molecular docking method.
Results: We obtained 21 active compounds and 253 corresponding target genes from TCMSP database. 1833 MI-related genes were identified according to P<0.05 and |log2FC| ≥ 0.5. 27 overlapping genes between drug and disease were acquired. GO analysis indicated that overlapping genes were mainly enriched in MAP kinase activity and antioxidant activity. KEGG analysis indicated that overlapping genes were mainly enriched in IL-17 signaling pathway and TNF signaling pathway. We obtained 10 hub genes via cytoHubba plugin. Six of the 10 hub genes, including PTGS2, MAPK14, MMP9, MAPK1, NFKBIA, and CASP8, were acted on molecular docking verification with their corresponding compounds of SXXTDP.
Conclusion: SXXTDP may exert cardioprotection effect through regulating multiple targets and multiple pathways in MI.}
}
@article{DUBAYAN2025110209,
title = {Automated classification of thyroid disease using deep learning with neuroevolution model training},
journal = {Engineering Applications of Artificial Intelligence},
volume = {146},
pages = {110209},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110209},
url = {https://www.sciencedirect.com/science/article/pii/S095219762500209X},
author = {Mohammad Rashid Dubayan and Sara Ershadi-Nasab and Mariam Zomorodi and Pawel Plawiak and Ryszard Tadeusiewicz and Mohammad Beheshti Roui},
keywords = {Classification, Genetic algorithm, Neural networks, Neuroevolution, Hypothyroid disease},
abstract = {Background:
Thyroid disease is a common endocrine disorder; its timely and accurate diagnosis is important. Using clinical and laboratory data input, we developed an artificial neural network (ANN) for thyroid disease classification, incorporating an evolutionary algorithm for network optimization.
Methods:
The proposed model combined ANN with a genetic algorithm (GA), which iteratively modified the weights and biases of the ANN architecture. The weights, encoded as genes in a chromosome and represented as one-dimensional vectors, were updated in each iteration. Binary cross-entropy loss was used as the fitness function to calculate the suitability of solutions in the genetic algorithm. The model was trained and tested on an open-access Hypothyroid disease dataset comprising multiparametric variables of 3772 samples (291 thyroid patients and 3481 controls).
Results:
Our model attends %99.14 accuracy for binary classification (thyroid disease vs. normal), outperforming published models.
Conclusion:
Incorporating GA optimization into ANN enabled the model to explore diverse solutions and escape local optima more effectively, leading to better performance and generalizability. The excellent results support the feasibility of implementing the proposed model for thyroid disease screening in the clinical setting.}
}
@article{DUSEK2020123,
title = {Evaluating the state-of-the-art of End-to-End Natural Language Generation: The E2E NLG challenge},
journal = {Computer Speech & Language},
volume = {59},
pages = {123-156},
year = {2020},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2019.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0885230819300919},
author = {Ondřej Dušek and Jekaterina Novikova and Verena Rieser},
abstract = {This paper provides a comprehensive analysis of the first shared task on End-to-End Natural Language Generation (NLG) and identifies avenues for future research based on the results. This shared task aimed to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena. Introducing novel automatic and human metrics, we compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures – with the majority implementing sequence-to-sequence models (seq2seq) – as well as systems based on grammatical rules and templates. Seq2seq-based systems have demonstrated a great potential for NLG in the challenge. We find that seq2seq systems generally score high in terms of word-overlap metrics and human evaluations of naturalness – with the winning Slug system (Juraska et al., 2018) being seq2seq-based. However, vanilla seq2seq models often fail to correctly express a given meaning representation if they lack a strong semantic control mechanism applied during decoding. Moreover, seq2seq models can be outperformed by hand-engineered systems in terms of overall quality, as well as complexity, length and diversity of outputs. This research has influenced, inspired and motivated a number of recent studies outwith the original competition, which we also summarise as part of this paper.}
}
@article{LERNER2020103356,
title = {Terminologies augmented recurrent neural network model for clinical named entity recognition},
journal = {Journal of Biomedical Informatics},
volume = {102},
pages = {103356},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103356},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419302734},
author = {Ivan Lerner and Nicolas Paris and Xavier Tannier},
keywords = {Clinical natural language processing, Named entity recognition, Information extraction, Machine learning, APcNER},
abstract = {Objective
We aimed to enhance the performance of a supervised model for clinical named-entity recognition (NER) using medical terminologies. In order to evaluate our system in French, we built a corpus for 5 types of clinical entities.
Methods
We used a terminology-based system as baseline, built upon UMLS and SNOMED. Then, we evaluated a biGRU-CRF, and a hybrid system using the prediction of the terminology-based system as feature for the biGRU-CRF. In French, we built APcNER, a corpus of 147 documents annotated for 5 entities (Drug names, Signs or symptoms, Diseases or disorders, Diagnostic procedures or lab tests and Therapeutic procedures). We evaluated each NER systems using exact and partial match definition of F-measure for NER. The APcNER contains 4,837 entities, which took 28 h to annotate. The inter-annotator agreement as measured by Cohen’s Kappa was substantial for non-exact match (Κ = 0.61) and moderate considering exact match (Κ = 0.42). In English, we evaluated the NER systems on the i2b2-2009 Medication Challenge for Drug name recognition, which contained 8,573 entities for 268 documents, and i2b2-small a version reduced to match APcNER number of entities.
Results
For drug name recognition on both i2b2-2009 and APcNER, the biGRU-CRF performed better that the terminology-based system, with an exact-match F-measure of 91.1% versus 73% and 81.9% versus 75% respectively. For i2b2-small and APcNER, the hybrid system outperformed the biGRU-CRF, with an exact-match F-measure of 87.8% versus 85.6% and 86.4% versus 81.9% respectively. On APcNER corpus, the micro-average F-measure of the hybrid system on the 5 entities was 69.5% in exact match and 84.1% in non-exact match.
Conclusion
APcNER is a French corpus for clinical-NER of five types of entities which covers a large variety of document types. The extension of the supervised model with terminology has allowed an easy increase in performance, especially for rare entities, and established near state of the art results on the i2b2-2009 corpus.}
}
@article{WOODLIEF2025103252,
title = {The SGSM framework: Enabling the specification and monitor synthesis of safe driving properties through scene graphs},
journal = {Science of Computer Programming},
volume = {242},
pages = {103252},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2024.103252},
url = {https://www.sciencedirect.com/science/article/pii/S0167642324001758},
author = {Trey Woodlief and Felipe Toledo and Sebastian Elbaum and Matthew B. Dwyer},
keywords = {Runtime verification, Autonomous vehicles, Safe driving properties, Scene graphs},
abstract = {As autonomous vehicles (AVs) become mainstream, assuring that they operate in accordance with safe driving properties becomes paramount. The ability to specify and monitor driving properties is at the center of such assurance. Yet, the mismatch between the semantic space over which typical driving properties are asserted (e.g., vehicles, pedestrians) and the sensed inputs of AVs (e.g., images, point clouds) poses a significant assurance gap. Related efforts bypass this gap by either assuming that data at the right semantic level is available, or they develop bespoke methods for capturing such data. Our recent Scene Graph Safety Monitoring (SGSM) framework addresses this challenge by extracting scene graphs (SGs) from sensor inputs to capture the entities related to the AV, specifying driving properties using a domain-specific language that enables building propositions over those graphs and composing them through temporal logic, and synthesizing monitors to detect property violations. Through this paper we further explain, formalize, analyze, and extend the SGSM framework, producing SGSM++. This extension is significant in that it incorporates the ability for the framework to encode the semantics of resetting a property violation, enabling the framework to count the quantity and duration of violations. We implemented SGSM++ to monitor for violations of 9 properties of 3 AVs from the CARLA Autonomous Driving Leaderboard, confirming the viability of the framework, which found that the AVs violated 71% of properties during at least one test including almost 1400 unique violations over 30 total test executions, with violations lasting up to 9.25 minutes. Artifact available at https://github.com/less-lab-uva/ExtendingSGSM.}
}
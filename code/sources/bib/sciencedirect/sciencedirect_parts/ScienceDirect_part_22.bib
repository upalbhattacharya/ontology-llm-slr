@article{MATTHEWS2022102495,
title = {Smart data and business analytics: A theoretical framework for managing rework risks in mega-projects},
journal = {International Journal of Information Management},
volume = {65},
pages = {102495},
year = {2022},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2022.102495},
url = {https://www.sciencedirect.com/science/article/pii/S0268401222000263},
author = {Jane Matthews and Peter E.D. Love and Stuart R. Porter and Weili Fang},
keywords = {Business analytics, Machine learning, Rework, Risk, Smart data, Topic modelling},
abstract = {Within construction, we have become increasingly accustomed to relying on the benefits of digital technologies, such as Building Information Modelling, to improve the performance and productivity of projects. We have, however, overlooked the problems that technology is unable to redress. One such problem is rework, which has become so embedded in practice that technology adoption alone can not resolve the issue without fundamental changes in how information is managed for decision-making. Hence, the motivation of this paper is to bring to the fore the challenges of classifying and creating an ontology for rework that can be used to understand its patterns of occurrence and risks and provide a much-needed structure for decision-making in transport mega-projects. Using an exploratory case study approach, we examine ‘how’ rework information is currently being managed by an alliance that contributes significantly to delivering a multi-billion dollar mega-transport project. We reveal the challenges around location, format, structure, granularity and redundancy hindering the alliance’s ability to classify and manage rework data. We use the generative machine learning technique of Correlation Explanation to illustrate how we can make headway toward classifying and then creating an ontology for rework. We propose a theoretical framework utilising a smart data approach to generate an ontology that can effectively use business analytics (i.e., descriptive, predictive and prescriptive) to manage rework risks.}
}
@article{MERINO2023194,
title = {Data integration for digital twins in the built environment based on federated data models},
journal = {Proceedings of the Institution of Civil Engineers - Smart Infrastructure and Construction},
volume = {176},
number = {4},
pages = {194-211},
year = {2023},
issn = {2397-8759},
doi = {https://doi.org/10.1680/jsmic.23.00002},
url = {https://www.sciencedirect.com/science/article/pii/S2397875923000157},
author = {Jorge Merino and Xiang Xie and Nicola Moretti and Janet Yoon Chang and Ajith Parlikad},
keywords = {Building Information Modelling (BIM), built environment, data, digital twin, modelling, sensors},
abstract = {Improving the efficiency of operations is a major challenge in facility management given the limitations of outsourcing individual building functions to third-party companies. The status of each building function is isolated in silos that are controlled by these third-party companies. Companies provide access to aggregated information in the form of reports through web portals, emails or bureaucratic processes. Digital twins represent an emerging approach to returning awareness and control to facility managers by automating all levels of information access (from granular data to defined key performance indicators and reports) and actuation. This paper proposes a low-latency data integration method that supports actuation and decision making in facility management, including construction, operation and maintenance data, and Internet of things. The method uses federated data models and semantic web ontologies, and it is implemented within a data lake architecture with connections to siloed data to keep the delegation of responsibilities of data owners. A case study in the Alan Reece Building (Cambridge, UK) demonstrates the approach by enabling fault detection and diagnosis of the heating, ventilation and air-conditioning system for facility management.}
}
@article{MISHRA2019933,
title = {Data-driven method to enhance craniofacial and oral phenotype vocabularies},
journal = {The Journal of the American Dental Association},
volume = {150},
number = {11},
pages = {933-939.e2},
year = {2019},
issn = {0002-8177},
doi = {https://doi.org/10.1016/j.adaj.2019.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S000281771930443X},
author = {Rashmi Mishra and Andrea Burke and Bonnie Gitman and Payal Verma and Mark Engelstad and Melissa A. Haendel and Ilias Alevizos and William A. Gahl and Michael T. Collins and Janice S. Lee and Murat Sincan},
keywords = {Natural language processing, evidence-based dentistry, craniofacial and oral phenotypes, ontology},
abstract = {Background
A significant amount of clinical information captured as free-text narratives could be better used for several applications, such as clinical decision support, ontology development, evidence-based practice, and research. The Human Phenotype Ontology (HPO) is specifically used for semantic comparisons for diagnostic purposes. All these functions require quality coverage of the domain of interest. The authors used natural language processing to capture craniofacial and oral phenotype signatures from electronic health records and then used these signatures for evaluation of existing oral phenotype ontology coverage.
Methods
The authors applied a text-processing pipeline based on the clinical Text Analysis and Knowledge Extraction System to annotate the clinical notes with Unified Medical Language System codes. The authors extracted the disease or disorder phenotype terms, which were then compared with HPO terms and their synonyms.
Results
The authors retrieved 2,153 deidentified clinical notes from 558 patients. Finally, 2,416 unique diseases or disorders phenotype terms were extracted, which included 210 craniofacial or oral phenotype terms. Twenty-six of these phenotypes were not found in the HPO.
Conclusions
The authors demonstrated that natural language processing tools could extract relevant phenotype terms from clinical narratives, which could help identify gaps in existing ontologies and enhance craniofacial and dental phenotyping vocabularies.
Practical Implications
The expansion of terms in the dental, oral, and craniofacial domains in the HPO is particularly important as the dental community moves toward electronic health records.}
}
@article{AHMETAJ2020103220,
title = {Polynomial rewritings from expressive Description Logics with closed predicates to variants of Datalog},
journal = {Artificial Intelligence},
volume = {280},
pages = {103220},
year = {2020},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2019.103220},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218301462},
author = {Shqiponja Ahmetaj and Magdalena Ortiz and Mantas Šimkus},
keywords = {Description Logics, Datalog, Closed predicates, Ontology-mediated query answering, Query rewriting, Succinctness, Relative expressiveness},
abstract = {In many scenarios, complete and incomplete information coexist. For this reason, the knowledge representation and database communities have long shown interest in simultaneously supporting the closed- and the open-world views when reasoning about logic theories. Here we consider the setting of querying possibly incomplete data using logic theories, formalized as the evaluation of an ontology-mediated query (OMQ) that pairs a query with a theory, sometimes called an ontology, expressing background knowledge. This can be further enriched by specifying a set of closed predicates from the theory that are to be interpreted under the closed-world assumption, while the rest are interpreted with the open-world view. In this way we can retrieve more precise answers to queries by leveraging the partial completeness of the data. The central goal of this paper is to understand the relative expressiveness of ontology-mediated query languages in which the ontology part is written in the expressive Description Logic (DL) ALCHOI and includes a set of closed predicates. We consider a restricted class of conjunctive queries. Our main result is to show that every query in this non-monotonic query language can be translated in polynomial time into Datalog with negation as failure under the stable model semantics. To overcome the challenge that Datalog has no direct means to express the existential quantification present in ALCHOI, we define a two-player game that characterizes the satisfaction of the ontology, and design a Datalog query that can decide the existence of a winning strategy for the game. If there are no closed predicates—in the case of querying an ALCHOI knowledge base—our translation yields a positive disjunctive Datalog program of polynomial size. To the best of our knowledge, unlike previous translations for related fragments with expressive (non-Horn) DLs, these are the first polynomial time translations.}
}
@article{EKRAMIPOOYA2024105310,
title = {Predicting possible recommendations related to causes and consequences in the HAZOP study worksheet using natural language processing and machine learning: BERT, clustering, and classification},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {89},
pages = {105310},
year = {2024},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2024.105310},
url = {https://www.sciencedirect.com/science/article/pii/S0950423024000688},
author = {Ali Ekramipooya and Mehrdad Boroushaki and Davood Rashtchian},
keywords = {HAZOP study automation, Recommendation, Natural language processing, Machine learning, Class imbalance problem},
abstract = {A set of recommendations is one of the most valuable outputs of the hazard and operability (HAZOP) study. The HAZOP study team provides recommendations when deficiencies are detected in the chemical process plant. These deficiencies can cause chemical process accidents and operability issues. This study employed a data-driven approach using natural language processing (NLP) and machine learning (ML) to predict potential recommendations based on causes and consequences. The dataset had no label; thus, clustering was used to label it. Firstly, bidirectional encoder representations from transformers (BERT) converted recommendation sentences into vectors. Secondly, uniform manifold approximation and projection (UMAP) and hierarchical density-based spatial clustering of applications with noise (HDBSCAN) were utilized to determine recommendation categories and label the dataset. Then, BERT was used to convert causes and consequences into vectors. Finally, a multi-layer perceptron (MLP) classifier was employed to predict possible recommendations based on causes and consequences. The class imbalance problem was handled by random over-sampling. The prediction accuracy of possible recommendations based on causes and consequences equals 93.7% and 89.5%, respectively. As a result of predicting potential recommendations utilizing causes and consequences, major recommendations will not be overlooked during the HAZOP study. This can further expand NLP and ML applications in HAZOP study automation.}
}
@article{TECLAW2024102770,
title = {Federating cross-domain BIM-based knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102770},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102770},
url = {https://www.sciencedirect.com/science/article/pii/S147403462400418X},
author = {Wojciech Teclaw and James O’Donnel and Ville Kukkonen and Pieter Pauwels and Nathalie Labonnote and Eilif Hjelseth},
keywords = {Building information modelling, Semantics, IFC, Knowledge integration, BIM},
abstract = {Despite decades of the digitization of the construction industry, interoperability challenges prevail. One of which is the lack of sufficient integration between domain-specific BIM models. Therefore, the paper proposes a novel methodology for federating cross-domain BIM-based knowledge graphs. The approach leverages the integration of multiple models into a single knowledge graph, aiming to bridge the existing gaps in data exchange and enhance the semantic richness of building information models. Through a detailed exploration of existing data exchange mechanisms, popular ontologies, and schema limitations, this research addresses the interoperability dilemma. A practical demonstration, encapsulated in the “LBD-Online-Merge” demo application, validates the methodology’s efficacy in real-world scenarios, illustrating its potential to revolutionize building operations management and data preparation for the operational phase of a building’s lifecycle. This study significantly contributes to the academic and practical discourse by providing a robust framework for achieving a more integrated, intelligent building information management ecosystem, setting a new benchmark for future research and applications in constructing semantically rich digital twins of the built environment.}
}
@article{MCCORMICK2023100140,
title = {The British Geological Survey Rock Classification Scheme, its representation as linked data, and a comparison with some other lithology vocabularies},
journal = {Applied Computing and Geosciences},
volume = {20},
pages = {100140},
year = {2023},
issn = {2590-1974},
doi = {https://doi.org/10.1016/j.acags.2023.100140},
url = {https://www.sciencedirect.com/science/article/pii/S2590197423000290},
author = {Tim McCormick and Rachel E. Heaven},
keywords = {Ontology, Vocabulary, Lithology, Linked data},
abstract = {Controlled vocabularies are critical to constructing FAIR (findable, accessible, interoperable, re-useable) data. One of the most widely required, yet complex, vocabularies in earth science is for rock and sediment type, or ‘lithology’. Since 1999 the British Geological Survey has used its own Rock Classification Scheme in many of its workflows and products including the national digital geological map. This scheme pre-dates others that have been published, and is deeply embedded in BGS’ processes. By publishing this classification scheme now as a Simple Knowledge Organisation System (SKOS) machine-readable informal ontology, we make it available for ourselves and third parties to use in modern semantic applications, and we open the future possibility of using the tools SKOS provides to align our scheme with other published schemes. These include the IUGS-CGI Simple Lithology Scheme, the European Commission INSPIRE Lithology Code List, the Queensland Geological Survey Lithotype Scheme, the USGS Lithologic Classification of Geologic Map Units, and Mindat.org. The BGS lithology classification was initially based on four narrative reports that can be downloaded from the BGS website, although it has been added to subsequently. The classification is almost entirely mono-hierarchical in nature and includes 3454 currently valid concepts in a classification 11 levels deep. It includes igneous rocks and sediments, metamorphic rocks, sediments and sedimentary rocks, and superficial deposits including anthropogenic deposits. The SKOS informal ontology built on it is stored in a triplestore and the triples are updated nightly by extracting from a relational database where the ontology is maintained. Bulk downloads and version history are available on github. The RCS concepts themselves are used in other BGS linked data, namely the Lexicon of Named Rock Units and the linked data representation of the 1:625 000 scale geological map of the UK. Comparing the RCS with the other published lithology schemes, all are broadly similar but show characteristics that reveal the interests and requirements of the groups that developed them, in terms of their level of detail both overall and in constituent parts. It should be possible to align the RCS with the other classifications, and future work will focus on automated mechanisms to do this, and possibly on constructing a formal ontology for the RCS.}
}
@incollection{CASCIANELLI2025704,
title = {Integrative Bioinformatics},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {704-711},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00096-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000968},
author = {Silvia Cascianelli and Marco Masseroli},
keywords = {Data integration, Data warehousing, Federated databases, Information linkage, Mediator-based integration, Multi-databases, Network-based integration, Semantic integration},
abstract = {The increasingly reliable and affordable high-throughput production of many different types of biomolecular data, and the development of pipelines for their automatic processing and annotation, have further stressed the need for integrative structures able to simplify accessibility and decrease redundancy and variety of semantic representations of life science data. Integrative bioinformatics addresses these aspects taking advantage of several different approaches and implementations proposed to integrate distributed heterogeneous data with notable applications in the biological domain; they include information linkage, federated databases, multi-databases, mediator-based solutions, and data warehousing. Additionally, ontology-based, statistical and network-based data integration can also be effectively used for better biologically driven data integration.}
}
@article{WANG2021101248,
title = {Automatic modeling and fault diagnosis of car production lines based on first-principle qualitative mechanics and semantic web technology},
journal = {Advanced Engineering Informatics},
volume = {49},
pages = {101248},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101248},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000033},
author = {Liyu Wang and Jack Hodges and Dan Yu and Ronald S. Fearing},
keywords = {Fault diagnosis, Intelligent manufacturing systems, Numerical optimization, Semantic web, Ontology, Time series analysis},
abstract = {Fault diagnosis is critical for intelligent manufacturing by monitoring the status of a production line and preventing financial loss. Model-based fault diagnosis has the advantage of being able to explain the cause and propagation of faults over model-free diagnosis, but would need knowledge about the configuration model and context-specific information of the production line. Ontology modelling can provide context-specific information on top of a configuration model to benefit fault diagnosis. Typically ontologies are manually constructed and then used by a reasoner based on a set of predefined rules. From the perspective of fault diagnosis, this approach works as an expert system where both the ontology models and predefined rules are specific to a given system. Once the system has changed which happens from time to time as repairs and updates in a production line, or in the case of a different system, the ontology models and predefined rules would need to be manually modified or reconstructed. Here a model-based method is proposed to automate generation of configuration models with context-specific information using semantic web technology when a production line is healthy, and to use the generated configuration model and information for diagnosis when the production line has a fault. The method does not rely on predefined rules and reasoners, but rather uses dynamics models that are based on first-principle qualitative mechanics. It uses numerical optimization to minimize the discrepancy between sensor data from the production line and from simulation running the dynamics model to achieve automatic configuration modelling and fault diagnosis. With three use cases commonly found for a production line, i.e. automatic sensor placement modeling or misplacement diagnosis, motor fault diagnosis with single sensor modality, and motor fault diagnosis with sensory substitution, the feasibility of the proposed method is demonstrated. The method’s faster computational speed and comparable accuracy to a quantitative model-based approach suggests it may complement and accelerate the latter with early-stage selection of candidate models for both modelling and fault diagnosis.}
}
@article{SERAKU2025104008,
title = {Placeholders in written language: The case of Japanese kō and otsu},
journal = {Lingua},
volume = {325},
pages = {104008},
year = {2025},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2025.104008},
url = {https://www.sciencedirect.com/science/article/pii/S0024384125001330},
author = {Tohru Seraku},
keywords = {Placeholder, Written language, Legal text, Japanese, Relevance Theory, Procedural meaning},
abstract = {A communicator is sometimes unable or unwilling to produce a particular expression. One of the lexical resources for overcoming such word-formulation difficulties is the use of placeholders like whatchamacallit and you-know-what. Whilst recent years have seen a proliferation of research on placeholders across diverse languages, prior studies have largely focussed on spoken language, with little attention paid to their use in writing. This paper reveals that Japanese has placeholders that appear almost exclusively in written language, predominantly in legal texts. First, we describe the distributional and grammatical properties of the placeholders kō and otsu through a theory-neutral analysis of data retrieved from The Balanced Corpus of Contemporary Written Japanese. Second, building on this descriptive foundation, we propose a cognitive account of their semantic and pragmatic aspects within the framework of Relevance Theory. We argue that kō and otsu encode procedural meaning and demonstrate how specific interpretations, both explicit and implicit, arise from the interaction among procedural meaning, contextual assumptions, and pragmatic principles. The present work advances the scholarship of placeholders by providing the first detailed account of placeholders in written language and by developing a semantic–pragmatic integrated approach to their contextual interpretation.}
}
@article{KHOURI2023217,
title = {Knowledge base construction for the semantic management of environment-enriched built heritage: The case of Algerian traditional houses architecture},
journal = {Journal of Cultural Heritage},
volume = {63},
pages = {217-229},
year = {2023},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2023.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S1296207423001589},
author = {Selma Khouri and Houda Oufaida and Racha Amrani and Sabrina Kacher and Safia Ouahab and Mouna Cherrad},
keywords = {Knowledge base, Built heritage, Traditional house architecture, Environmental devices, Algeria},
abstract = {Traditional domestic architecture in Algeria reflects ancestral know-how which could serve as a model for new architectural achievements concerned with respecting the environment. This multidisciplinary knowledge covering different contexts (architectural, physical and environmental), is available but scattered through various sources of different formats (texts, tables, illustrations, etc.). Capturing such knowledge through an efficient knowledge repository is required. Knowledge base (KB) repositories have shown many significant contributions for managing the complexity, the variety of contexts and the querying of Built Heritage (BH) data. Creating a KB is a complex process that requires identifying and extracting the relevant knowledge from raw sources of information, then structuring, cleaning and integrating the extracted information into the KB repository. The management of these issues and their inter-dependencies are not always detailed in related BH literature. We propose in this paper a complete process for creating a KB dedicated to the built heritage consisting of Algerian traditional houses. Two main contributions are emphasized in this study: (i) the construction process of the KB is detailed at each step using two main suites: the ontology suite for identifying and structuring the relevant concepts, and the data suite that allows the flow of knowledge from the sources to the KB. (ii) the semantic variety of contexts of the input sources, their linguistic complexity and their format heterogeneity is handled all along the process, and is reflected at two levels: at the schema level (ontology model), and at the data level using automatic information extraction techniques. The definition of the KB includes fine-grained provenance metadata. Different results are proposed to evaluate the content and the quality of the KB.}
}
@article{GARCIA20231180,
title = {Integrating artificial intelligence and natural language processing for computer-assisted reporting and report understanding in nuclear cardiology},
journal = {Journal of Nuclear Cardiology},
volume = {30},
number = {3},
pages = {1180-1190},
year = {2023},
issn = {1071-3581},
doi = {https://doi.org/10.1007/s12350-022-02996-5},
url = {https://www.sciencedirect.com/science/article/pii/S1071358123000119},
author = {Ernest V. Garcia},
keywords = {Natural language processing, Natural language understanding, Natural language generation, Structured reports},
abstract = {Natural language processing (NLP) offers many opportunities in Nuclear Cardiology. These opportunities include applications in converting nuclear cardiology imaging reports to digital searchable information that may be used as Big Data for machine learning and registries. Another major NLP application is, with the support of AI, in automatically translating MPI image features directly into nuclear cardiology reports. This review describes the symbiotic relationship between AI and NLP in that NLP is being used to facilitate AI applications and, AI techniques are being used to facilitate NLP. This article reviews the fundamentals of NLP and describes various conventional and AI techniques that have been applied in imaging. Key nuclear cardiology applications are reviewed such as conversion of MPI free-text reports to digital documents as well as direct conversion of MPI images into structured medical reports.}
}
@article{CHHETRI2023120955,
title = {Towards improving prediction accuracy and user-level explainability using deep learning and knowledge graphs: A study on cassava disease},
journal = {Expert Systems with Applications},
volume = {233},
pages = {120955},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120955},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423014574},
author = {Tek Raj Chhetri and Armin Hohenegger and Anna Fensel and Mariam Aramide Kasali and Asiru Afeez Adekunle},
keywords = {Explainable AI (XAI), Agricultural sustainability, Knowledge graphs, Deep learning, Cassava},
abstract = {Food security is currently a major concern due to the growing global population, the exponential increase in food demand, the deterioration of soil quality, the occurrence of numerous diseases, and the effects of climate change on crop yield. Sustainable agriculture is necessary to solve this food security challenge. Disruptive technologies, such as of artificial intelligence, especially, deep learning techniques can contribute to agricultural sustainability. For example, applying deep learning techniques for early disease classification allows us to take timely action, thereby helping to increase the yield without inflicting unnecessary environmental damage, such as excessive use of fertilisers or pesticides. Several studies have been conducted on agricultural sustainability using deep learning techniques and also semantic web technologies such as ontologies and knowledge graphs. However, the three major challenges remain: (i) the lack of explainability of deep learning-based systems (e.g. disease information), especially to non-experts like farmers; (ii) a lack of contextual information (e.g. soil or plant information) and domain-expert knowledge in deep learning-based systems; and (iii) the lack of pattern learning ability of systems based on the semantic web, despite their ability to incorporate domain knowledge. Therefore, this paper presents the work on disease classification, addressing the challenges as mentioned earlier by combining deep learning and semantic web technologies, namely ontologies and knowledge graphs. The findings are: (i) 0.905 (90.5%) prediction accuracy on large noisy dataset; (ii) ability to generate user-level explanations about disease and incorporate contextual and domain knowledge; (iii) the average prediction latency of 3.8514 s on 5268 samples; (iv) 95% of users finding the explanation of the proposed method useful; and (v) 85% of users being able to understand generated explanations easily—show that the proposed method is superior to the state-of-the-art in terms of performance and explainability and is also suitable for real-world scenarios.}
}
@article{CHENG2021104400,
title = {A location conversion method for roads through deep learning-based semantic matching and simplified qualitative direction knowledge representation},
journal = {Engineering Applications of Artificial Intelligence},
volume = {104},
pages = {104400},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104400},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621002487},
author = {Ruozhen Cheng and Jing Chen},
keywords = {Natural language descriptions, Road location conversion, Geocoding, Qualitative direction reasoning, Deep learning, Simplified qualitative direction knowledge},
abstract = {Qualitative direction knowledge that appears in natural language descriptions of road-related locations could point to the interior of individual roads or associate multiple roads. Interpreting such descriptions to perform location conversion for roads will support intelligent road-related location services. Existing geocoding technologies could perform textual or semantic matching to transform road names to spatial locations, and research on qualitative direction reasoning could perform efficient location conversion based on semantic queries of qualitative direction knowledge between roads. However, research on geocoding lacks the consideration of matching the described internal direction knowledge of a road to a part of the road. Moreover, efficient location conversion based on semantic queries cannot scale to large road datasets due to the retrieval efficiency of a large amount of qualitative direction knowledge between roads. To accomplish this goal, this study proposes a location conversion method for roads, wherein a road ontology is designed to model the interior direction knowledge of the roads, a deep learning-based road semantic matching model is trained to match the internal direction knowledge descriptions and road segments, and a simplified qualitative direction knowledge representation between roads is performed to support rapid location conversion between roads based on efficient semantic queries. The proposed method was implemented on a road dataset of New York State. The results demonstrate that the proposed method can be effectively applied in road location conversion based on descriptions that contain qualitative direction knowledge inside individual roads or between multiple roads, which expands the scope of artificial intelligence applications.}
}
@article{KARLANDER202582,
title = {The art and politics of micronational language planning},
journal = {Language & Communication},
volume = {104},
pages = {82-96},
year = {2025},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2025.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0271530925000618},
author = {David Karlander},
keywords = {Constructed languages, Invented languages, Language and art, Language politics, Micronations, Subversive affirmation},
abstract = {This article discusses the language politics of micronations. It argues that micronational language planning offers a three-pronged satirical rejoinder to mainstream language politics. First, micronational language politics pushes back at attempts to frame nation-states and national languages as irrelevant in a globalized world. Second, it rebuffs neo-romantic sociolinguistic critiques of globalization. Third, it troubles technocratic approaches to language policy and planning (LPP). This argument is grounded in a close analysis of two micronational art projects: Elgaland-Vargaland and Ladonia. These micronations simultaneously appropriate and debase traditional LPP, creating both a defamiliarization of well-worn language ideologies and a destabilization of technocratic linguistic expertise. This is a promising starting point for reimagining research into the politics of language.}
}
@article{KOSSE2024102677,
title = {A Semantic Digital Twin for the Dynamic Scheduling of Industry 4.0-based Production of Precast Concrete Elements},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102677},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102677},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003252},
author = {Simon Kosse and Vincent Betker and Philipp Hagedorn and Markus König and Thorsten Schmidt},
keywords = {Modular construction, Precast concrete, Industry 4.0, Semantic digital twin, Dynamic Scheduling, Linked Data},
abstract = {Precast concrete construction enhances project efficiency, sustainability, and durability by leveraging a controlled production environment that ensures high-quality outputs. Still, the sequential nature of off-site production, encompassing casting, curing, and storage of the precast elements, is subject to significant uncertainties, including supply chain variability and environmental factors affecting curing times. Dynamic adjustments to the production schedule are essential for aligning with real-time changes, necessitating a robust framework for real-time data acquisition and analysis. Dynamic Scheduling (DS) is a responsive and adaptive approach that accommodates real-time changes, optimizes the production flow, and minimizes downtime or delays. However, the DS approach demands real-time data to quickly and efficiently respond to unforeseen challenges, which requires acquiring and analyzing production-relevant data throughout the production process. The Digital Twin (DT) emerges in Industry 4.0 (I4.0) as a bridge between physical operations and digital capabilities, enabling a seamless flow of information, for which the Asset Administration Shell (AAS) is a reference implementation. This study introduces a DS framework to optimize precast element production, utilizing a DT for real-time data aggregation across the production system. The framework implements a Semantic DT based on the AAS and the Linked Data approach. It employs Resource Description Framework (RDF) serialization of the AAS and an ontological representation of the production system for data integration. The framework leverages a simulation-based scheduler, which exchanges data with the DT in a Service-oriented Architecture (SoA) using SPARQL, a language for querying and updating graph databases. The approach is evaluated through a proof of concept, demonstrating effective uncertainty management in a dynamic production environment.}
}
@article{ZHOU2021100664,
title = {SemML: Facilitating development of ML models for condition monitoring with semantics},
journal = {Journal of Web Semantics},
volume = {71},
pages = {100664},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100664},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000391},
author = {Baifan Zhou and Yulia Svetashova and Andre Gusmao and Ahmet Soylu and Gong Cheng and Ralf Mikut and Arild Waaler and Evgeny Kharlamov},
keywords = {Condition monitoring, Ontologies, Templates, Data integration, Machine learning, Software architecture, Industry 4.0},
abstract = {Monitoring of the state, performance, quality of operations and other parameters of equipment and production processes, which is typically referred to as condition monitoring, is an important common practice in many industries including manufacturing, oil and gas, chemical and process industry. In the age of Industry 4.0, where the aim is a deep degree of production automation, unprecedented amounts of data are generated by equipment and processes, and this enables adoption of Machine Learning (ML) approaches for condition monitoring. Development of such ML models is challenging. On the one hand, it requires collaborative work of experts from different areas, including data scientists, engineers, process experts, and managers with asymmetric backgrounds. On the other hand, there is high variety and diversity of data relevant for condition monitoring. Both factors hampers ML modelling for condition monitoring. In this work, we address these challenges by empowering ML-based condition monitoring with semantic technologies. To this end we propose a software system SemML that allows to reuse and generalise ML pipelines for conditions monitoring by relying on semantics. In particular, SemML has several novel components and relies on ontologies and ontology templates for ML task negotiation and for data and ML feature annotation. SemML also allows to instantiate parametrised ML pipelines by semantic annotation of industrial data. With SemML, users do not need to dive into data and ML scripts when new datasets of a studied application scenario arrive. They only need to annotate data and then ML models will be constructed through the combination of semantic reasoning and ML modules. We demonstrate the benefits of SemML on a Bosch use-case of electric resistance welding with very promising results.}
}
@article{KOSSE2025103483,
title = {Semantic Digital Twins in Construction: Developing a modular System Reference Architecture based on Information Containers},
journal = {Advanced Engineering Informatics},
volume = {67},
pages = {103483},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103483},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625003763},
author = {Simon Kosse and Philipp Hagedorn and Markus König},
keywords = {Digital Twin, Construction, Reference Architecture, Linked data, Asset Administration Shell, Information Container for Linked Document Delivery},
abstract = {The construction industry is increasingly adopting Digital Twin (DT) technology to support the design, construction, and operation of buildings and structures. In this context, a key challenge for DTs is integrating heterogeneous data sources to address requirements that evolve across different life cycle phases and use cases. A modular approach for deploying DTs offers a flexible and scalable solution that can adapt to these changing requirements. However, a clear definition and structure of DT modules for the built environment are still missing. This research presents a modular System Reference Architecture (SRA) for implementing Semantic DTs in the construction industry. As its central component, the SRA leverages the inherently modular Asset Administration Shell (AAS) reference model for asset DTs in Industry 4.0. Built on submodels, each addressing a specific use case or aspect, the AAS serves as a high-level framework for DTs. The SRA extends the AAS with standardized Information Containers for Linked Document Delivery (ICDD), integrated through a Linked Data approach employing a semantic layer of ontologies. The feasibility of the proposed SRA is demonstrated through a case-specific implementation for the precast concrete production. Two submodels are developed within the SRA: one for accessing dynamic sensor data via time series databases and another for integrating BIM-derived semantic data using ICDD. The architecture is evaluated through a simulated curing process, where SPARQL and REST-based queries enable real-time monitoring and feedback control. The results confirm the SRA’s ability to integrate heterogeneous data sources, support semantic interoperability, and facilitate lifecycle-oriented feedback mechanisms.}
}
@article{GALADIMA2025103970,
title = {Evaluating Incident Response in CSIRTs using Cube Socio-technical Systems Analysis},
journal = {Computer Standards & Interfaces},
volume = {93},
pages = {103970},
year = {2025},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2024.103970},
url = {https://www.sciencedirect.com/science/article/pii/S0920548924001399},
author = {Haula Sani Galadima and Cormac Doherty and Nick McDonald and Junli Liang and Rob Brennan},
keywords = {Incident response, Computer Security Incident Response Team (CSIRT), Cube Socio-technical Systems Analysis (STSA), Access Risk Knowledge (ARK) platform},
abstract = {This paper provides a novel method for evaluating Incident Response (IR) teams through the application of the Cube Socio-technical Systems Analysis (STSA) methodology. Cube is a form of structured Human Factors enquiry and has previously been successfully applied in both aviation and healthcare. By utilising STSA, this study aims to understand and evaluate incident knowledge across the IR socio-technical domain. Traditional approaches to IR improvement often focus solely on technical aspects, neglecting social factors that may significantly influence IR effectiveness. This research presents the results of extending the ARK platform for a cybersecurity IR Cube STSA of IR activities in a case study involving a large, accredited Computer Security Incident Response Team (CSIRT). It evaluates the IR system and team needs before the development of a technological intervention to improve IR learning and preparation capabilities. We present an extended Cube questionnaire, that defines specialised IR questions, an ontology, and terminology for the cybersecurity domain based on the ISO27000 series of standards. The case study demonstrates the ARK platform's capability to capture and analyse IR systems using a Multi-stage Cube STSA analysis shared in a reusable knowledge graph based on W3C standards. This provides a shared knowledge base based on FAIR (Findable, Accessible, Interoperable, Reusable) linked data, that may support generation of training materials, playbooks, and best practices to enhance IR capabilities and CSIRT operations. We show how this approach provides new insights and reusable artefacts for CSIRTs to enhance organisational cyber resilience and learning.}
}
@article{PETREY2022130,
title = {The affective flows of the sublime in Martina Menegon's new machinic subjectivity},
journal = {New Techno Humanities},
volume = {2},
number = {2},
pages = {130-135},
year = {2022},
note = {New Media, Interactive Audiences, and the Virtual. Next Generation Narratives; Edited by Keyan G Tomaselli & Damien R Tomaselli},
issn = {2664-3294},
doi = {https://doi.org/10.1016/j.techum.2022.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2664329422000061},
author = {Grant Petrey},
keywords = {Martina Menegon, Machinic subjectivity, Photogrammetry, Affect, New languages, Sublime},
abstract = {What can the use of emerging digital technologies for the creation of art exploring subjectivity offer up for research into new affective flows within the temporal structures created by machinic assemblages? An examination of the work by the artist Martina Menegon who explores the self to engage with the problematics of representation, cognition and sensation within machinic assemblages using photogrammetry, raises ontological issues. Menegon uses photogrammetry to capture her body as data, to experience her body and represent her body as art in the digital virtual space, link the ontological, with the material and corporeal. This recent virtual reality output of Menegon's explores emerging post screen digital aesthetics and manifestations of digital subjectivity, where “affect” becomes central to the form, content and reception of the work. Menegon uses the virtual digital space to take pleasure is confusing the boundaries of the body via its digital capturing, re-presentation and plays with notions of the self and its limits. Menegon's explorations can be positioned as action research within a theoretical framework of Donna Haraway and the assemblage theory of Giles Deleuze and Felix Guatarri where a new “image” of the self can be mediated or remediated and where Menegon can take responsibility for production and dissemination within her networks of distribution to consider what this entails. The temporal and affective relationships of production become key to this new machinic subjectivity.}
}
@article{YAN2021182,
title = {Extended intelligent Su-Field analysis based on fuzzy inference},
journal = {Procedia Computer Science},
volume = {192},
pages = {182-191},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015064},
author = {W. Yan and C. Zanni-Merk and D. Cavallucci and L. Zhang and J.H. Wang},
keywords = {Fuzzy ontology, Fuzzy SWRL(Semantic Web Rule Language) rules, Fuzzy inference, Su-Field analysis, Inventive standards},
abstract = {Su-Field analysis, as one of TRIZ analytical tool for solving inventive problems, can be used to improve the performance of the technical system effectively. Generally, choosing an appropriate inventive standard is critical to solving inventive problems efficiently and accurately. However, these standards are summarized and categorized based on the enormous amount of patents in different domains, and they are built in the high level of abstraction, independently of the specific application field, making their use require much more technical knowledge than other TRIZ tools. In order to facilitate the use of inventive standards, especially for capturing the uncertainty or imprecision depicted in the standards, a rule-based heuristic methodology is proposed in this paper. Firstly, Su-Field analysis ontology and fuzzy analysis ontology are built to represent the precise and fuzzy knowledge in the process of solving inventive problems respectively. Then, SWRL (Semantic Web Rule Language) inference and fuzzy inference are performed for generating heuristic concept solution. Finally, a prototype is developed and the resolution of the case of the “Auguste Piccard’s Stratostat” in prototype is elaborated in detail.}
}
@article{NAIR20242876,
title = {Investigating Natural Language Techniques for Accurate Noun and Verb Extraction},
journal = {Procedia Computer Science},
volume = {235},
pages = {2876-2885},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.272},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924009517},
author = {Reshma P Nair and M G Thushara},
keywords = {Natural Language Processing(NLP), Noun, Verb phrase, Part Of Speech(POS), NLP Libraries, Rule based Approaches},
abstract = {Natural language processing (NLP) has witnessed significant advancements in recent decades. Automatically classifying parts of speech, like nouns and verbs, from textual input has transformed text analysis and language understanding. Using natural language processing techniques, we explore various methods for identifying noun and verb phrases automatically, with an emphasis on high accuracy. Our study explores rule-based, statistical, and Machine Learning (ML) approaches for determining the nouns and verbs from sentences. The effectiveness of these approaches is clearly evident, especially when NLP libraries such as SpaCy and the Natural Language Toolkit (NLTK) are used. As well as demonstrating their potential applications across diverse language processing tasks and industries, we conduct comparative research to showcase their advantages and disadvantages. The performance of these methods is also examined in terms of retrieving subject and action terms. SpaCy achieves an impressive accuracy of 95% in noun and verb extraction, while Part-Of-Speech (POS) technology tagging delivers an even higher accuracy of 96%. The results obtained with these methods illustrate how nouns, verbs, and names can be classified in text successfully.}
}
@article{SACKS2022101711,
title = {Toward artificially intelligent cloud-based building information modelling for collaborative multidisciplinary design},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101711},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101711},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001690},
author = {Rafael Sacks and Zijian Wang and Boyuan Ouyang and Duygu Utkucu and Siyu Chen},
keywords = {Building information modelling, Concurrent engineering, Design collaboration, Knowledge graphs, Semantic enrichment},
abstract = {The technological tools people use for designing buildings have progressed from drawings to descriptive geometry, and from computer-aided drafting and design (CAD) to building information modelling (BIM). Yet despite their use of state-of-the-art BIM technology, the multidisciplinary teams that design modern buildings still face numerous challenges. Building models lack sufficient semantic content to properly express design intent, concurrent design is difficult due to the need for operators to maintain model consistency and integrity manually, managing design variations is cumbersome due to the packaging of information in files, and collaboration requires making-do with imperfect interoperability between application software. In response, we propose a ‘Cloud BIM’ (CBIM) approach to building modelling that seeks to automate maintenance of consistency across federated discipline-specific models by enriching models with semantic information that encapsulates design intent. The approach requires a new ontology to represent knowledge about the relationships between building model objects within and across disciplines. Discipline-specific building models are stored together with their data schema in knowledge graphs, and linked using objects and relationships from the CBIM ontology. The links are established using artificially intelligent semantic enrichment methods that recognize patterns of location, geometry, topology and more. Software methods that operate along CBIM relationship chains can detect inconsistencies that arise across disciplines and act to inform users, propose meaningful corrections, and apply them if approved. Future CBIM systems may provide designers with the functionality for collaborative multidisciplinary design by maintaining model consistency and managing versioning at the object level.}
}
@article{CABALLEROOTEYZA2024831,
title = {GenIA, the Genetic Immunology Advisor database for inborn errors of immunity},
journal = {Journal of Allergy and Clinical Immunology},
volume = {153},
number = {3},
pages = {831-843},
year = {2024},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2023.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0091674923015130},
author = {Andrés Caballero-Oteyza and Laura Crisponi and Xiao P. Peng and Kevin Yauy and Stefano Volpi and Stefano Giardino and Alexandra F. Freeman and Bodo Grimbacher and Michele Proietti},
keywords = {Inborn error of immunity, immune disease, immunogenetics, genotype-phenotype, genetic paradigms, natural history, curation, database, resource, patient-centered},
abstract = {Background
To date, no publicly accessible platform has captured and synthesized all of the layered dimensions of genotypic, phenotypic, and mechanistic information published in the field of inborn errors of immunity (IEIs). Such a platform would represent the extensive and complex landscape of IEIs and could increase the rate of diagnosis in patients with a suspected IEI, which remains unacceptably low.
Objective
Our aim was to create an expertly curated, patient-centered, multidimensional IEI database that enables aggregation and sophisticated data interrogation and promotes involvement from diverse stakeholders across the community.
Methods
The database structure was designed following a subject-centered model and written in Structured Query Language (SQL). The web application is written in Hypertext Preprocessor (PHP), Hypertext Markup Language (HTML), Cascading Style Sheets (CSS), and JavaScript. All data stored in the Genetic Immunology Advisor (GenIA) are extracted by manually reviewing published research articles.
Results
We completed data collection and curation for 24 pilot genes. Using these data, we have exemplified how GenIA can provide quick access to structured, longitudinal, more thorough, comprehensive, and up-to-date IEI knowledge than do currently existing databases, such as ClinGen, Human Phenotype Ontology (HPO), ClinVar, or Online Mendelian Inheritance in Man (OMIM), with which GenIA intends to dovetail.
Conclusions
GenIA strives to accurately capture the extensive genetic, mechanistic, and phenotypic heterogeneity found across IEIs, as well as genetic paradigms and diagnostic pitfalls associated with individual genes and conditions. The IEI community's involvement will help promote GenIA as an enduring resource that supports and improves knowledge sharing, research, diagnosis, and care for patients with genetic immune disease.}
}
@incollection{KONSTANTINIDIS2025,
title = {Using AI for personalized nutrition: Customizing diets for optimal consumer well-being},
booktitle = {Reference Module in Food Science},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-08-100596-5},
doi = {https://doi.org/10.1016/B978-0-443-15976-3.00042-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443159763000428},
author = {Dimitrios Konstantinidis and Achilleas Blekos and Martha Kotaidou and Periklis Papaioannou and Lazaros Gymnopoulos and Kosmas Dimitropoulos},
keywords = {Artificial Intelligence, Dietary Recommendations, Large Language Models, Nutritional Guidelines, Personalized Nutrition},
abstract = {Proper nutrition is vital for individual health and well-being, driving a growing demand for recommendation systems that provide personalized dietary advice tailored to each consumer's unique needs and preferences. This chapter presents an extensive literature review of nutrition recommendation systems, categorizing them into three distinct types: knowledge-based, AI-based, and LLM-based. It then critically examines the strengths and limitations of each approach in terms of diet recommendation capabilities, algorithmic performance, and ethical considerations. Knowledge-based systems rely on established nutritional guidelines, offering reliability, transparency, and robust data privacy. In contrast, AI-based systems process large data volumes to uncover hidden insights, while LLM-based systems leverage extensive internet sources and their interactive capabilities to deliver personalized dietary recommendations. The chapter concludes by outlining future research directions aimed at mitigating current drawbacks and enhancing the performance of the systems, ultimately offering nutrition recommendations that are more reliable, adaptable, trustworthy and transparent for consumers.}
}
@article{ANDERSON2025101483,
title = {Integrative genre-based pedagogy: Enhancing social responsiveness in English medium of instruction and STEM education},
journal = {Journal of English for Academic Purposes},
volume = {74},
pages = {101483},
year = {2025},
issn = {1475-1585},
doi = {https://doi.org/10.1016/j.jeap.2025.101483},
url = {https://www.sciencedirect.com/science/article/pii/S1475158525000141},
author = {Karoline Anita Anderson},
keywords = {Diversity, Genre-based pedagogy, Decolonization, English medium of instruction, ESP/EAP, STEM, Student agency, Social responsiveness},
abstract = {Genre-based pedagogy (GBP) is frequently employed to instruct second language (L2) English speakers to communicate through genre conventions, yet doing so may inadvertently subjugate culturally diverse ways of knowing and communicating knowledge, reinforcing predominant social norms. English for Specific Purposes (ESP) and English for Academic Purposes (EAP) traditions of GBP are characterized by broad subdisciplines, including critical perspectives aimed at promoting sociocultural diversity. Yet, current approaches may be limited in their critiquing of epistemological and ontological biases and practical application to broader social and educational contexts. The present work introduces integrative genre-based pedagogy (IGBP) to enhance social responsiveness in ESP/EAP genre instruction, particularly in English as a medium of instruction (EMI) and STEM contexts. IGBP is founded on three pillars of practice: identifying, enhancing, and critiquing. Cumulatively, these pillars of practice aim to elevate teachers' and students’ critical consciousness, authenticity, and collaboration, using a holistic approach to establish a positive learning environment for enhancing genre and cultural knowledge and engaging in ongoing critiques of genre.}
}
@article{PURSNANI2025105422,
title = {A conversational intelligent assistant for enhanced operational support in floodplain management with multimodal data},
journal = {International Journal of Disaster Risk Reduction},
volume = {122},
pages = {105422},
year = {2025},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2025.105422},
url = {https://www.sciencedirect.com/science/article/pii/S2212420925002468},
author = {Vinay Pursnani and Yusuf Sermet and Ibrahim Demir},
keywords = {Floodplain management, AI assistant, Large language models, Flood maps, Decision support, Community resilience},
abstract = {Floodplain management is crucial for mitigating flood risks and enhancing community resilience, yet floodplain managers often face significant challenges, including the complexity of data analysis, regulatory compliance, and effective communication with diverse stakeholders. This study introduces Floodplain Manager AI, an innovative artificial intelligence (AI) based virtual assistant designed to support floodplain managers in their decision-making processes and operations. Utilizing advanced large language models and semantic search techniques, the AI Assistant provides accurate, location-specific guidance tailored to the unique regulatory environments of different states. It is capable of interpreting Federal Emergency Management Agency (FEMA) flood maps through multimodal capabilities, allowing users to understand complex visual data and its implications for flood risk assessment. The AI Assistant also simplifies access to comprehensive floodplain management resources, enabling users to quickly find relevant information and streamline their workflows. Experimental evaluations demonstrated substantial improvements in accuracy and relevance of the AI Assistant's response, underscoring its effectiveness in addressing the specific needs of floodplain managers. By facilitating informed decision-making and promoting proactive measures, Floodplain Manager AI aims to enhance flood risk mitigation operations and support sustainable community development in the context of increasing flood events driven by climate change. Ultimately, this research highlights the transformative potential of AI technologies in improving floodplain management practices and fostering community resilience.}
}
@incollection{DASILVASINHA2025,
title = {Time and Its Cultural Conceptualization in Language},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00622-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041006220},
author = {Vera {da Silva Sinha} and Chris Sinha},
keywords = {Time conceptualization, Diversity, Language, Cognition, Cultural practices, Linguistic relativity, Event-based time, Temporal metaphors},
abstract = {The domain of time exemplifies the importance of cultural variation in motivating linguistic variation. Although all languages express deictic and sequential inter-event relationships, time interval concepts are embedded in cultural contexts and shaped by cultural artifacts and practices. Some traditional societies use event-based time systems that are indexed to social and natural environmental happenings rather than being based on measurement units in calendric and clock time. The space-time metaphor is widespread, but it is not universal. In event-based time languages, space-time metaphoric mapping is largely absent, and the source domain for temporal metaphors is embodied human perception and cognition.}
}
@article{GASPAR2025,
title = {Natural Language Processing and ICD-10 Coding for Detecting Bleeding Events in Discharge Summaries: Comparative Cross-Sectional Study},
journal = {JMIR Medical Informatics},
volume = {13},
year = {2025},
issn = {2291-9694},
doi = {https://doi.org/10.2196/67837},
url = {https://www.sciencedirect.com/science/article/pii/S2291969425001838},
author = {Frederic Gaspar and Mehdi Zayene and Claire Coumau and Elliott Bertrand and Marie Bettex and Marie Annick {Le Pogam} and Chantal Csajka},
keywords = {decision support systems, deep learning, hemorrhage, international classification of diseases, machine learning, ML, natural language processing, NLP, cross-sectional, logistic regression, bleeding, discharge summaries, adverse drug events, older adults, elderly, electronic medical records, medical records, artificial intelligence, AI, healthcare, decision-making},
abstract = {Background
Bleeding adverse drug events (ADEs), particularly among older inpatients receiving antithrombotic therapy, represent a major safety concern in hospitals. These events are often underdetected by conventional rule-based systems relying on structured electronic medical record data, such as the ICD-10 (International Statistical Classification of Diseases and Related Health Problems 10th Revision) codes, which lack the granularity to capture nuanced clinical narratives.
Objective
This study aimed to develop and evaluate a natural language processing (NLP) model to detect and categorize bleeding ADEs in discharge summaries of older adults. Specifically, the model was designed to distinguish between “clinically significant bleeding,” “severe bleeding,” “history of bleeding,” and “no bleeding,” and was compared with a rule-based algorithm using ICD-10 codes.
Methods
Clinicians manually annotated 400 discharge summaries, comprising 65,706 sentences, into four categories: “no bleeding,” “clinically significant bleeding,” “severe bleeding,” and “history of bleeding.” The dataset was divided into a training set (70%, 47,100 sentences) and a test set (30%, 18,606 sentences). Two detection approaches were developed and evaluated: (1) an NLP model using binary logistic regression and support vector machine classifiers, and (2) a traditional rule-based algorithm relying exclusively on predefined ICD-10 codes. To address class imbalance, with most sentences categorized as irrelevant (“no bleeding”), a class-weighting strategy was applied in the NLP model. Model performance was assessed using accuracy, precision, recall, F1-score, and receiver operating characteristic (ROC) curve analyses, with manual annotations as the gold standard.
Results
The NLP model significantly outperformed the rule-based approach across all evaluation metrics. At the document level, the NLP model achieved macro-average scores of 0.81 for accuracy and 0.80 for F1-score. Precision was particularly high for detecting severe (0.92) and clinically significant bleeding events (0.87), demonstrating strong classification capability despite class imbalance. ROC analyses confirmed the model’s robust diagnostic performance, yielding an area under the curve (AUC) of 0.91 when distinguishing irrelevant sentences from potential bleeding events, 0.88 for identifying historical mentions of bleeding, and notably, 0.94 for differentiating clinically significant from severe bleeding. In contrast, the rule-based ICD-10 model demonstrated high precision (0.94) for clinically significant bleeding but poor recall (0.03) for severe bleeding events, reflecting frequent missed detections. This limitation arose due to its reliance on commonly used ICD-10 codes (eg, gastrointestinal hemorrhage) and inadequate capture of rare severe bleeding conditions such as shock due to hemorrhage.
Conclusions
This study highlights the considerable advantage of NLP over traditional ICD-10–based methods for detecting bleeding ADEs within electronic medical records. The NLP model effectively captured nuanced clinical narratives, including severity, negations, and historical bleeding events, demonstrating substantial promise for improving patient safety surveillance and clinical decision-making. Future research should extend validation across multiple institutions, diversify annotated datasets, and further refine temporal reasoning capabilities within NLP algorithms.
International Registered Report Identifier (IRRID)
RR2-10.2196/40456}
}
@article{MOUICHE2025114346,
title = {TIJERE: A novel threat intelligence joint extraction model based on analyst expert knowledge},
journal = {Knowledge-Based Systems},
volume = {329},
pages = {114346},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114346},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125013851},
author = {Inoussa Mouiche and Sherif Saad},
keywords = {Threat intelligence joint entity and relation extraction, Multisequence labeling representation, Expert domain features, Cyber threat intelligence, Cyber knowledge graphs, Pipeline extraction, Joint extraction},
abstract = {The extraction of entities and relationships from threat intelligence reports into structured formats, such as cybersecurity knowledge graphs, is essential for automated threat analysis, detection, and mitigation. However, existing joint extraction methods struggle with feature confusion, language ambiguity, noise propagation, and overlapping relations, resulting in low accuracy and poor model performance. This paper presents TIJERE, an innovative joint entity and relation extraction framework that formulates joint extraction as a multisequence labeling representation (MSLR) problem. Specifically, separate sequences are generated for each entity pair. Unlike prior tagging schemes, MSLR integrates expert domain features to enrich positional, contextual, and semantic representations of entities, thereby enhancing feature distinction and classification accuracy. Additionally, TIJERE reduces language ambiguity and enhances domain-specific generalization by leveraging SecureBERT+, a contextual language model fine-tuned on cybersecurity text. This improves both named entity recognition (NER) and relation extraction (RE). This paper also introduces DNRTI-JE, the first publicly available jointly labeled dataset for cybersecurity entity and RE, filling a crucial gap in cyber threat intelligence automation. Empirical evaluations on the curated DNRTI-JE dataset demonstrate that TIJERE achieves state-of-the-art performance, with F1-scores exceeding 0.93 for NER and 0.98 for RE, outperforming existing methods. Together, TIJERE and the standardized benchmarking DNRTI-JE dataset enable high-performance cybersecurity intelligence extraction, with transferable applications in healthcare, finance, and bioinformatics.}
}
@article{WANG2024102797,
title = {Research on Bio-inspired product design based on knowledge graph and semantic fusion diffusion model},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102797},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102797},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004452},
author = {Zeng Wang and Cong Fu and Shi-fan Niu and Shi-jie Hu},
keywords = {Bio-inspired design, Knowledge graph, Semantic fusion, Diffusion model, Product design, Design evaluation},
abstract = {Bio-inspired design (BID), as an innovative design methodology, holds a significant position in the field of design, providing essential means for exploring problem-solving approaches and creating aesthetically pleasing product appearances. However, two major challenges exist in the current domain of BID: accurately matching suitable biomimetic organisms and efficiently generating high-quality biomimetic design solutions. To address these issues, this study simulates the process of designers engaging in BID and proposes a novel approach to bio-inspired product design (BIPD) based on a knowledge graph and a semantic fusion diffusion model (SFDM). First, by thoroughly analyzing textual data describing the physiological and behavioral characteristics of various organisms, along with design requirements, we construct a biological knowledge ontology layer and a corresponding graph database tailored for BIPD. This allows for the accurate recommendation of biomimetic organisms by inputting different product design requirements into the BIPD knowledge graph. This solves the problem of accurate matching between the target domain and source domain in BID, ensuring consistency between the recommended biomimetic organisms and product design requirements. Second, by processing and filtering the images of recommended biomimetic organisms and product names, and inputting them into the SFDM, we enable the rapid generation of diverse biomimetic design solutions. Furthermore, to ensure the systematic nature and practical effectiveness of this method, we introduce a corresponding BIPD evaluation system. Evaluation metrics related to BIPD are refined to comprehensively assess and select the optimal biomimetic design solution. Finally, through three different case studies, we demonstrate the efficiency and potential of the proposed method in addressing the challenges of target and source domain matching and the rapid fusion generation of biomimetic design solutions in the field of BIPD. This validation confirms the feasibility and effectiveness of the method. Our approach injects new vitality into the innovation of BID, providing designers with intelligent design system tools for efficient design based on specific needs, thereby enhancing the design quality and productivity of BIPD.}
}
@article{NEJI2018685,
title = {A semantic approach for constructing valid composition scenarios of linguistic Web services},
journal = {Procedia Computer Science},
volume = {126},
pages = {685-694},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918312808},
author = {Mariem NEJI and Bilel GARGOURI and Mohammed JMAIEL},
keywords = {Linguistic Web Services, Services Composition, NLP domain ontology, Semantic approach, Ontological Views},
abstract = {Constructing Linguistic Web Services (LingWS for short) is a difficult task that requires expertise in both linguistic and software engineering. The challenge is to manage the various and richness linguistic knowledge and the heterogeneous Natural Language Processing (NLP for short) using the web service technology. In this paper, we propose a semantic approach to improve the composition of LingWS. First, we provide a rich NLP domain ontology that covers the data and processing concepts in a fine granularity and richness relationships between these concepts. Second, we proceed with the generation of an ontological view from the rich proposed NLP domain ontology using a set of selection criteria. A view delimits the linguistic ontology to the user’s needs. Starting from this view, the user will be assisted in selecting a valid processing scenario, assimilated to a scenario of linguistic services, with respect to the coherence, completeness and redundancy properties. In order to prove the feasibility of the proposed approach, we carried out an experiment on the morphological level.}
}
@article{DU2025102755,
title = {Natural language processing in finance: A survey},
journal = {Information Fusion},
volume = {115},
pages = {102755},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102755},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005335},
author = {Kelvin Du and Yazhi Zhao and Rui Mao and Frank Xing and Erik Cambria},
keywords = {Natural language processing, Finance, Financial sentiment analysis, Financial narrative processing, Financial forecasting, Portfolio management, Question answering, Risk management, Regulatory compliance, ESG and sustainable finance, Explainable AI, Digital assets},
abstract = {This survey presents an in-depth review of the transformative role of Natural Language Processing (NLP) in finance, highlighting its impact on ten major financial applications: (1) financial sentiment analysis, (2) financial narrative processing, (3) financial forecasting, (4) portfolio management, (5) question answering, virtual assistant and chatbot, (6) risk management, (7) regulatory compliance monitoring, (8) Environmental, Social, Governance (ESG) and sustainable finance, (9) explainable artificial intelligence (XAI) in finance and (10) NLP for digital assets. With the integration of vast amounts of unstructured financial data and advanced NLP techniques, the study explores how NLP enables data-driven decision-making and innovation in the financial sector, alongside the limitations and challenges. By providing a comprehensive analysis of NLP applications combining both academic and industrial perspectives, this study postulates the future trends and evolution of financial services. It introduces a unique review framework to understand the interaction of financial data and NLP technologies systematically and outlines the key drivers, transformations, and emerging areas in this field. This survey targets researchers, practitioners, and professionals, aiming to close their knowledge gap by highlighting the significance and future direction of NLP in enhancing financial services.}
}
@article{GARCIASANCHEZ2020102153,
title = {A social-semantic recommender system for advertisements},
journal = {Information Processing & Management},
volume = {57},
number = {2},
pages = {102153},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2019.102153},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319307265},
author = {Francisco García-Sánchez and Ricardo Colomo-Palacios and Rafael Valencia-García},
keywords = {Knowledge-based systems, Recommender systems, Natural language processing, Advertising, Social network services},
abstract = {Social applications foster the involvement of end users in Web content creation, as a result of which a new source of vast amounts of data about users and their likes and dislikes has become available. Having access to users’ contributions to social sites and gaining insights into the consumers’ needs is of the utmost importance for marketing decision making in general, and to advertisement recommendation in particular. By analyzing this information, advertisement recommendation systems can attain a better understanding of the users’ interests and preferences, thus allowing these solutions to provide more precise ad suggestions. However, in addition to the already complex challenges that hamper the performance of recommender systems (i.e., data sparsity, cold-start, diversity, accuracy and scalability), new issues that should be considered have also emerged from the need to deal with heterogeneous data gathered from disparate sources. The technologies surrounding Linked Data and the Semantic Web have proved effective for knowledge management and data integration. In this work, an ontology-based advertisement recommendation system that leverages the data produced by users in social networking sites is proposed, and this approach is substantiated by a shared ontology model with which to represent both users’ profiles and the content of advertisements. Both users and advertisement are represented by means of vectors generated using natural language processing techniques, which collect ontological entities from textual content. The ad recommender framework has been extensively validated in a simulated environment, obtaining an aggregated f-measure of 79.2% and a Mean Average Precision at 3 (MAP@3) of 85.6%.}
}
@article{ARAGON2025101691,
title = {“Weaving” the tupi: The study of kʷaẽ language and the persistence of pottery-making knowledge among the Akuntsu women, southwestern Amazon},
journal = {Journal of Anthropological Archaeology},
volume = {79},
pages = {101691},
year = {2025},
issn = {0278-4165},
doi = {https://doi.org/10.1016/j.jaa.2025.101691},
url = {https://www.sciencedirect.com/science/article/pii/S0278416525000364},
author = {Carolina Coelho Aragon and Roseline Mezacasa and Juliana Salles Machado},
keywords = {Indigenous persistence, , Women’s corporeality, Akuntsu, Tupi},
abstract = {Grounded in the archaeology of persistence, decolonial perspectives, and technological approaches to ceramic manufacture, this study examines pottery-making as both a material expression of resilience and a site of ongoing identity negotiation. Integrating notions of intersubjectivity and intercorporeality, this paper explores the interrelation between technical processes, embodied knowledge, and territorial experiences. Focusing on the persistence of pottery-making knowledge among Akuntsu women (Tupi, Tupari), the study highlights its ties to cosmology. The Akuntsu, a recently-contacted Indigenous people, faced genocidal attacks as their ancestral territory in the Rondônia State, Brazil, was violated. Today, only three Akuntsu women—Pugapia, Aiga, and Babawro—remain as survivors of this group. To understand the chaîne opératoire of pottery production among Akuntsu women, this research promotes interdisciplinary dialogue, recognizing the fusion of knowledge with territorial, language, and material experiences from an ethnoarchaeological perspective. We explore the interplay between women featured in historical narratives, fermented beverage, saliva, tupi (clay), and pottery-making process as carriers of subjectivity and potentiality. The findings reveal that the chaîne opératoire of production of kʷaẽ (clay pots) endures as a cultural practice despite historical disruption from contact-related violence. The persistence of the three Akuntsu women in tupi shaping exemplifies the resilience of their traditions and illustrates how Indigenous material practices engage with broader debates on persistence, coloniality, and the interconnections between bodies, artifacts, and territories.}
}
@article{MALIK2022,
title = {Knowledge-Infused Text Classification for the Biomedical Domain},
journal = {International Journal of Information System Modeling and Design},
volume = {13},
number = {10},
year = {2022},
issn = {1947-8186},
doi = {https://doi.org/10.4018/IJISMD.306635},
url = {https://www.sciencedirect.com/science/article/pii/S1947818622000497},
author = {Sonika Malik and Sarika Jain},
keywords = {Artificial Neural Network, Classifier, Natural Language Processing, Ontology, Text Classification},
abstract = {ABSTRACT
Extracting knowledge from unstructured text and then classifying it is gaining importance after the data explosion on the web. The traditional text classification approaches are becoming ubiquitous, but the hybrid of semantic knowledge representation with statistical techniques can be more promising. The developed method attempts to fabricate neural networks to expedite and improve the simulation of ontology-based classification. This paper weighs upon the accurate results between the ontology-based text classification and traditional classification based on the artificial neural network (ANN) using distinguished parameters such as accuracy, precision, etc. The experimental analysis shows that the proposed findings are substantially better than the conventional text classification, taking the course of action into account. The authors also ran tests to compare the results of the proposed research model with one of the latest researches, resulting in a cut above accuracy and F1 score of the proposed model for various experiments performed at the different number of hidden layers and neurons.}
}
@article{PAN2024101697,
title = {Version [2.0.0]-[ENTIRETY—sEmanNTIc pRovisioning and govErning ioT devices in smart energY domain]},
journal = {SoftwareX},
volume = {26},
pages = {101697},
year = {2024},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2024.101697},
url = {https://www.sciencedirect.com/science/article/pii/S2352711024000682},
author = {Zhiyu Pan and Syed Junayed Ahmed Anta and Antonello Monti},
keywords = {Internet of Things, Device provisioning, Semantic IoT provisioning, Ontology},
abstract = {ENTIRETY 1 is a web application for governing and provisioning IoT devices. Due to the rapid evolution and digitization of energy systems, energy domain ontology is required to manage the data in a structured way. This paper presents the ontology-based web application ENTIRETY 2, which is the updated version of ENTIRETY 1. This updated version provides an interface for data collection and transmission. The data collected by the IoT device are provided to the ICT platform according to the ontology. ENTIRETY 2 overcomes the challenges of the mapping between the different network protocols and automates the generation of JSON-LD based on ontology files. Therefore, the ENTIRETY 2 can be easily extended to support other ontologies.}
}
@article{GOODWIN2025104199,
title = {Revisiting the challenges to monitoring, evaluation, reporting, and learning for climate adaptation},
journal = {Environmental Science & Policy},
volume = {172},
pages = {104199},
year = {2025},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2025.104199},
url = {https://www.sciencedirect.com/science/article/pii/S1462901125002151},
author = {Sean Goodwin and Marta Olazabal},
abstract = {Monitoring, evaluation, reporting, and learning (MERL) is a key mechanism for advancing climate change adaptation by enabling reflection, preparation and improvement. While challenges to MERL are typically grouped into conceptual, empirical, and methodological categories, emerging issues demand attention as adaptation practice matures. These challenges are pressing for two reasons. First, the maturing field of adaptation introduces complex challenges beyond technical considerations, yet risks perpetuating structural problems seen in other MERL domains. Second, adaptation is inherently transversal, spanning multiple policy arenas—local to global, and across sectors such as environment, development, transport, water management, and education—further multiplying the diversity of challenges. Through a scoping review of scientific and grey literature, we synthesised a framework of emerging challenges to designing, implementing, and using MERL for adaptation. These include ontological challenges (how adaptation and related concepts are defined and contested), epistemological challenges (whose knowledge counts, and how it is valued), axiological challenges (nature and types of value being placed on MERL), social challenges (equity and justice aspects in MERL processes affecting effectiveness), material challenges (use of financial and human resources), political challenges (dynamics affecting transparent evaluation), and spatio-temporal challenges (integration of complex spatial and temporal dimensions). This framework highlights key gaps and needs in current MERL practices. By addressing these challenges, adaptation policy and science can advance in ways that are more grounded, equitable, and effective, enabling improved responses to the complex and evolving demands of climate change adaptation.}
}
@article{ALMOHAIMEED2025100762,
title = {Abstractive text summarization: A comprehensive survey of techniques, systems, and challenges},
journal = {Computer Science Review},
volume = {57},
pages = {100762},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100762},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000383},
author = {Norah Almohaimeed and Aqil M. Azmi},
keywords = {Summarization techniques, Abstractive summarization, System architectures, Taxonomy, Evaluation methods, Datasets},
abstract = {Abstractive text summarization addresses information overload by generating paraphrased content that mimics human expression, yet it faces significant computational and linguistic challenges. This paper presents a detailed functional taxonomy of abstractive summarization, structured along four dimensions: techniques (including structure-based, semantic, and deep learning approaches, including large language models), system architectures (ranging from single-model to multi-agent and human-in-the-loop interactive systems), evaluation methods (covering lexical, semantic, and human-centered assessments), and datasets. Our taxonomy explicitly distinguishes techniques from architectures to clarify how methodological strategies are operationalized in practice. We examine pressing multilingual challenges such as linguistic complexity, data scarcity, and performance disparities in cross-lingual transfer, particularly for low-resource languages. Additionally, we address persistent issues such as factual inaccuracies, content hallucinations, and biases in widely used evaluation metrics. The paper highlights emerging trends—including cross-lingual summarization, interactive summarization systems, and ethically grounded frameworks—as key directions for future research. This synthesis not only maps the current landscape but also outlines pathways to enhance the accuracy, reliability, and applicability of abstractive summarization in real-world settings.}
}
@article{HAMDAN2021103739,
title = {A semantic modeling approach for the automated detection and interpretation of structural damage},
journal = {Automation in Construction},
volume = {128},
pages = {103739},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103739},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521001904},
author = {Al-Hakam Hamdan and Jakob Taraben and Marcel Helmrich and Tobias Mansperger and Guido Morgenthal and Raimar J. Scherer},
keywords = {Building information modeling, Structural damage, Point cloud, Ontology, Multimodel, Semantic web},
abstract = {During the life-cycle of constructions various influences induce material defects that could affect the behavior of the structural system. Therefore, anomalies that affect heavily stressed constructions, such as bridges, need to be inspected and evaluated regarding their impact on the structural capacity. By using new technologies in the field of damage detection, e.g. laser scanners or unmanned aircraft systems (UAS), this process can be facilitated. However, the classification and assessment of detected anomalies must still be performed in a manual way by human experts due to the lack of machine-processable evaluation methods. In this paper an approach is proposed towards a machine-based damage evaluation, applying semantic web technologies on a new developed method for damage detection on constructions. Thereby, anomalies are detected based on a large amount of high-resolution images from which georeferenced point clouds are calculated by using photogrammetric methods. Using the geometric relations among the image positions and reconstructed points, image features such as anomalies are localized on a 3 dimensional surface. Based on these image features, a web ontology as semantic representation of the recorded damages is generated and linked with an ontology that contains information about the affected construction and its environment. By using predefined rules based on expert knowledge, the detected anomalies are classified and assessed automatically. The inferred information is then used to generate damage representations in a structural analysis model. Furthermore, the geometrical data, which are represented in a model created according to Building Information Modeling (BIM) standards, the semantic data as well as the structural data are linked by utilizing the Multimodel approach.}
}
@article{CHUNG2023105020,
title = {Comparing natural language processing (NLP) applications in construction and computer science using preferred reporting items for systematic reviews (PRISMA)},
journal = {Automation in Construction},
volume = {154},
pages = {105020},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105020},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523002807},
author = {Sehwan Chung and Seonghyeon Moon and Junghoon Kim and Jungyeon Kim and Seungmo Lim and Seokho Chi},
keywords = {Natural language processing, NLP methods, NLP tasks, Systematic comparison, Preferred reporting items for systematic reviews, Bibliometric analysis, VOSViewer},
abstract = {Despite the increasing use of natural language processing (NLP) in the construction domain, no systematic comparison has been conducted between NLP applications in construction and the latest advancements in NLP within the computer science domain. Therefore, this study compares NLP studies in these two domains. Firstly, a bibliometric analysis was performed on 55 publications in state-of-the-art NLP studies, which identified four main research areas in NLP. Secondly, a systematic review of 202 NLP studies in construction was conducted, presenting representative application areas of NLP and their current technical status. The results reveal a decreasing technology gap between NLP in construction and the state-of-the-art. However, the comparison also highlighted gaps in application areas and methodologies, and eight future research opportunities were proposed. This review serves as a foundation for future studies that aim to apply state-of-the-art NLP technologies in the construction domain.}
}
@article{SCHUCHTER202561,
title = {Application of artificial intelligence in model-based systems engineering of automated production systems},
journal = {Procedia CIRP},
volume = {136},
pages = {61-66},
year = {2025},
note = {35th CIRP Design 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125007656},
author = {Timo Schuchter and Patrick Saft and Ralf Stetter and Markus Pfeil and Wolfram Höpken and Markus Till and Stephan Rudolph},
keywords = {artificial intelligence, model based systems engineering, behavior modelling, design methodology, tools, technologies},
abstract = {Despite the incontestable appeal, the application of artificial intelligence (AI) in engineering processes is still limited to isolated applications and, in some fields, enthusiasm has given way to disillusionment. This paper aims to contribute to a concept of a framework that allows the application of AI in model-based systems engineering (MBSE) processes of automated production systems; the main focus is hereby on the MBSE processes. The aim of the complete framework is to realize an AI-based, self-learning digital twin that automatically adapts to the real system behavior and represents an optimal image of a product and its production process at all times. An expressive, semantic overall model serves as the basis for new approaches to artificial intelligence. In the complete framework, knowledge gained using AI methods is integrated into the overall model and thus brought into an overall context. Such an overall model improves the interpretability and explainability of the AI models and enables complex analyses, simulations and forecasts. The core element of the approach is a novel, AI-based, self-learning engineering model consisting of a product and production model that maps function, behavior and product geometry. Graph-based design languages are used for forming a central data model and functional mock-up units are applied for continuous co-simulation. The approach is underlined by means of an application to the design of automated assembly systems.}
}
@article{VANWOENSEL2025100320,
title = {Semantic Interoperability on Blockchain by Generating Smart Contracts Based on Knowledge Graphs},
journal = {Blockchain: Research and Applications},
pages = {100320},
year = {2025},
issn = {2096-7209},
doi = {https://doi.org/10.1016/j.bcra.2025.100320},
url = {https://www.sciencedirect.com/science/article/pii/S2096720925000478},
author = {W {Van Woensel} and O Seneviratne},
keywords = {Health 3.0, Knowledge Graphs, Blockchain, Smart contract, Code generation},
abstract = {Health 3.0 enables decision-making to be based on longitudinal data from multiple institutions spanning the patient's healthcare journey. Blockchain smart contracts can act as neutral and trustworthy intermediaries to implement such decision-making. In this distributed healthcare setting, transmitted data are structured using standards, such as Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR), for semantic interoperability. Hence, the smart contract will require interoperability with the domain standard. However, it will also have to implement a complex communication setup to work in a distributed environment (e.g., using oracles), and be developed using special-purpose blockchain languages (e.g., Solidity). To support these requirements, we propose the encoding of smart contract logic using a high-level semantic Knowledge Graph (KG), which uses concepts and relations from a domain standard and additionally lists distributed data requirements. We subsequently deploy this semantic KG on blockchain via a hybrid on-/off-chain code-generation approach. We applied our approach to generate smart contracts for three health insurance cases from Medicare. We evaluated the generated contracts in terms of correctness and execution cost (i.e., gas) on blockchain. Finally, we discuss the suitability of blockchain—and by extension, our approach—for a number of healthcare use cases.}
}
@article{WANG2024242,
title = {An Intelligent Quality Control Method for Manufacturing Processes Based on a Human–Cyber–Physical Knowledge Graph},
journal = {Engineering},
volume = {41},
pages = {242-260},
year = {2024},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2024.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S2095809924003710},
author = {Shilong Wang and Jinhan Yang and Bo Yang and Dong Li and Ling Kang},
keywords = {Quality control, Human–cyber–physical ternary data, Knowledge graph},
abstract = {Quality management is a constant and significant concern in enterprises. Effective determination of correct solutions for comprehensive problems helps avoid increased backtesting costs. This study proposes an intelligent quality control method for manufacturing processes based on a human–cyber–physical (HCP) knowledge graph, which is a systematic method that encompasses the following elements: data management and classification based on HCP ternary data, HCP ontology construction, knowledge extraction for constructing an HCP knowledge graph, and comprehensive application of quality control based on HCP knowledge. The proposed method implements case retrieval, automatic analysis, and assisted decision making based on an HCP knowledge graph, enabling quality monitoring, inspection, diagnosis, and maintenance strategies for quality control. In practical applications, the proposed modular and hierarchical HCP ontology exhibits significant superiority in terms of shareability and reusability of the acquired knowledge. Moreover, the HCP knowledge graph deeply integrates the provided HCP data and effectively supports comprehensive decision making. The proposed method was implemented in cases involving an automotive production line and a gear manufacturing process, and the effectiveness of the method was verified by the application system deployed. Furthermore, the proposed method can be extended to other manufacturing process quality control tasks.}
}
@article{HANKS201963,
title = {Communicative interaction in terms of ba theory: Towards an innovative approach to language practice},
journal = {Journal of Pragmatics},
volume = {145},
pages = {63-71},
year = {2019},
note = {Quo Vadis, Pragmatics?},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2019.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0378216618306222},
author = {William F. Hanks and Sachiko Ide and Yasuhiro Katagiri and Scott Saft and Yoko Fujii and Kishiko Ueno},
keywords = { theory, , Context, Interaction, Imputed intention, Phenomenology},
abstract = {This paper proposes an innovative approach to pragmatics that breaks away from the notion of individual as starting point in order to understand interactive context as a single integrated whole. This break is made by introducing the Japanese philosophical concept of ba and basho. The conceptualization of basho was initiated by Nishida Kitaro and his fellow philosophers at the Kyoto School in the first half of the twentieth century, and the ba principle, a theory of the emergence of information in dynamical systems, has been theorized by Shimizu. A ba and basho approach is a way of rethinking context that lies at the heart of pragmatics. A ba and basho approach presupposes ‘primary ba,’ ‘secondary ba’ and ‘ba theory.’ The uniqueness of this approach is the level of ‘primary ba,’ which is basically an ontology of mutual dependence, impermanence and ultimately non-separation. Nishida referred to this level of ba as ‘Basho of Absolute Nothingness.’ By applying ba theory to language practices, we analyze and present new interpretations of interactive discourse.}
}
@article{YAN2025106383,
title = {Digital twin-based bridge geometric quality inspection using knowledge mapping and data-driven method},
journal = {Automation in Construction},
volume = {178},
pages = {106383},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106383},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525004236},
author = {Junwei Yan and Hao Zhang and Qingsong Ai and Yongyang Xu and Jun Yang and Wei Meng and Tuyu Bao},
keywords = {Knowledge mapping, Point cloud segmentation, Digital twin, Geometric quality inspection, Bridge},
abstract = {Accurate geometric quality inspection is vital for detecting bridge defects during construction. To address the challenges of limited point cloud segmentation accuracy at bridge component connections and insufficient detection efficiency, a digital twin bridge geometric quality inspection method based on knowledge mapping and data-driven is proposed. In the digital space, a reusable bridge geometric quality inspection knowledge model is established to enhance the scalability of knowledge. In the twin data processing space, a large-scale point cloud segmentation network based on hybrid feature aggregation and neighbor feature enhancement (HANE-Net) is proposed to improve the segmentation accuracy. The network achieves superior performance in the S3DIS dataset and real bridge point cloud, with mean intersection over union of 66.8 % and 95.44 %, respectively, surpassing the baseline method RandLANet by 3.2 % and 0.79 %, respectively. Finally, a prototype system is designed based on Revit to prove the feasibility of the proposed method.}
}
@article{HAHNLEIN2025100640,
title = {Transforming formal knowledge to language and graphs to promote mathematics learning: A repeated-measures mixed design quasi-experiment},
journal = {Computers in Human Behavior Reports},
volume = {18},
pages = {100640},
year = {2025},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2025.100640},
url = {https://www.sciencedirect.com/science/article/pii/S2451958825000557},
author = {Inka Sara Hähnlein and Clara Luleich and Philipp Reiter and Nils Waterstraat and Pablo Pirnay-Dummer},
keywords = {Mathematics education, Higher education, Knowledge maps, Natural language processing, Repeated-measures design experiment},
abstract = {The transition from school to university mathematics presents a significant challenge for students, as both the demands on mathematical reasoning and the level of abstraction increase. This often makes it difficult for learners to construct the mental models necessary for understanding mathematical content and meeting academic requirements. Research has shown that incorporating a second level of content representation—particularly graphical representations—can help students develop more viable mental models. This longitudinal quasi-experimental study aims to enhance mathematical learning in higher education by supporting students' mental modeling. We use a new approach called natural-language conceptual Graph (NaGra), which translates mathematical formalism into natural language. Using computer-linguistic software, we then generate knowledge maps from these texts, providing two distinct types of additional representations to complement traditional instruction. In a 6-point repeated-measures control-group design, 139 math undergraduates received either (a) a natural language text, (b) a knowledge map, (c) both the natural language text and the knowledge map, or (d) the traditional instruction based solely on mathematical formalism. Results from non-parametric longitudinal analyses indicate that students in the experimental conditions consistently outperformed those in the control group over time in mathematical performance. However, students did not perceive the added value of these representations. These findings suggest that the NaGra method can contribute to students’ understanding of STEM subjects (science, technology, engineering, and mathematics), where first-year students often struggle to adapt to abstract formal content.}
}
@article{LONGO202364,
title = {A framework for cognitive chatbots based on abductive–deductive inference},
journal = {Cognitive Systems Research},
volume = {81},
pages = {64-79},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723000359},
author = {Carmelo Fabio Longo and Paolo Marco Riela and Daniele Francesco Santamaria and Corrado Santoro and Antonio Lieto},
keywords = {Chatbot, Question answering, Artificial intelligence, First-order logic, Cognitive architectures, Meta-reasoning},
abstract = {This paper presents a framework based on natural language processing and first-order logic aiming at instantiating cognitive chatbots. The proposed framework leverages two types of knowledge bases interacting with each other in a meta-reasoning process. The first one is devoted to the reactive interactions within the environment, while the second one to conceptual reasoning. The latter exploits a combination of axioms represented with rich semantics and abduction as pre-stage of deduction, dealing also with some of the state-of-the-art issues in the natural language ontology domain. As a case study, a Telegram chatbot system has been implemented, supported by a module which automatically transforms polar and wh-questions into one or more likely assertions, so as to infer Boolean values or snippets with variable length as factoid answer. The conceptual knowledge base is organized in two layers, representing both long- and short-term memory. The knowledge transition between the two layers is achieved by leveraging both a greedy algorithm and the engine’s features of a NoSQL database, with promising timing performance if compared with the adoption of a single layer. Furthermore, the implemented chatbot only requires the knowledge base in natural language sentences, avoiding any script updates or code refactoring when new knowledge has to income. The framework has been also evaluated as cognitive system by taking into account the state-of-the art criteria: the results show that AD-Caspar is an interesting starting point for the design of psychologically inspired cognitive systems, endowed of functional features and integrating different types of perception.}
}
@article{SCHMITT2025111332,
title = {Relationships and representations of brain structures, connectivity, dynamics and functions},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {138},
pages = {111332},
year = {2025},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2025.111332},
url = {https://www.sciencedirect.com/science/article/pii/S0278584625000867},
author = {Oliver Schmitt},
keywords = {Brain theory, Functions, Hierarchies, Ontologies, Connectomics, Imaging, Modeling, Simulation, Computational neuroscience, Neuronal dynamics, Behavior},
abstract = {The review explores the complex interplay between brain structures and their associated functions, presenting a diversity of hierarchical models that enhances our understanding of these relationships. Central to this approach are structure-function flow diagrams, which offer a visual representation of how specific neuroanatomical structures are linked to their functional roles. These diagrams are instrumental in mapping the intricate connections between different brain regions, providing a clearer understanding of how functions emerge from the underlying neural architecture. The study details innovative attempts to develop new functional hierarchies that integrate structural and functional data. These efforts leverage recent advancements in neuroimaging techniques such as fMRI, EEG, MEG, and PET, as well as computational models that simulate neural dynamics. By combining these approaches, the study seeks to create a more refined and dynamic hierarchy that can accommodate the brain’s complexity, including its capacity for plasticity and adaptation. A significant focus is placed on the overlap of structures and functions within the brain. The manuscript acknowledges that many brain regions are multifunctional, contributing to different cognitive and behavioral processes depending on the context. This overlap highlights the need for a flexible, non-linear hierarchy that can capture the brain’s intricate functional landscape. Moreover, the study examines the interdependence of these functions, emphasizing how the loss or impairment of one function can impact others. Another crucial aspect discussed is the brain’s ability to compensate for functional deficits following neurological diseases or injuries. The investigation explores how the brain reorganizes itself, often through the recruitment of alternative neural pathways or the enhancement of existing ones, to maintain functionality despite structural damage. This compensatory mechanism underscores the brain’s remarkable plasticity, demonstrating its ability to adapt and reconfigure itself in response to injury, thereby ensuring the continuation of essential functions. In conclusion, the study presents a system of brain functions that integrates structural, functional, and dynamic perspectives. It offers a robust framework for understanding how the brain’s complex network of structures supports a wide range of cognitive and behavioral functions, with significant implications for both basic neuroscience and clinical applications.}
}
@article{NGO2025102966,
title = {Integrating personalized and contextual information in fine-grained emotion recognition in text: A multi-source fusion approach with explainability},
journal = {Information Fusion},
volume = {118},
pages = {102966},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.102966},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525000399},
author = {Anh Ngo and Jan Kocoń},
keywords = {Emotion recognition, Sentence sequence classification, Personalization, Data cartography, Natural Language Processing (NLP), Explainable artificial, Intelligence (XAI)},
abstract = {Emotion recognition in textual data is a rapidly evolving field with diverse applications. While the state-of-the-art (SOTA) models based on pre-trained large language models (LLMs) have demonstrated significant achievements, the existing approaches often overlook fine-grained emotional nuances within individual sentences and the influence of contextual information. Additionally, despite the growing interest in personalized Natural Language Processing, recent studies have highlighted limitations in the literature, particularly the lack of explainability methods to interpret the improvements observed in these models. This study explores the CLARIN-Emo dataset to demonstrate the effectiveness of integrating personalized and contextual information for accurate emotion detection. By framing textual emotion recognition as a sequence sentence classification (SSC) task and leveraging transformer-based architectures, the proposed multi-source fusion approach significantly outperformed the baseline model, which considers each sentence in isolation. Furthermore, a personalized method, referred to as UserID, captures user-specific characteristics by assigning each annotator a unique identifier, significantly enhancing emotion prediction accuracy. This work also introduces an extension of Data Maps by differentiating dynamic training metrics to analyze the models’ training behaviors. The results validate the capability of this approach in visually interpreting and facilitating performance comparisons between models.}
}
@article{ELHOSARY2025100026,
title = {Evaluating Natural Language Processing Algorithms for Improved Hazard and Operability Analysis},
journal = {Geodata and AI},
volume = {4},
pages = {100026},
year = {2025},
issn = {3050-483X},
doi = {https://doi.org/10.1016/j.geoai.2025.100026},
url = {https://www.sciencedirect.com/science/article/pii/S3050483X25000255},
author = {Ehab Elhosary and Osama Moselhi},
keywords = {Hazard and Operability reports (HAZOP), Natural Language Processing (NLP), Machine Learning Classifiers},
abstract = {Automating Hazard and Operability (HAZOP) reports is crucial for enhancing efficiency and reducing human biases in hazard identification process. Recent HAZOP studies have applied various Natural Language Processing (NLP) algorithms to optimize hazard identification and reporting. These studies exhibit divergent textual inputs and results and overlook the mining of all HAZOP report components. Furthermore, traditional NLP algorithms such as Bag of Words (BOW) and Term Frequency-inverse Document Frequency (TF-IDF) often fail to capture polysemy and semantic relationships between words. This study aims to evaluate NLP algorithms’ efficacy in automating HAZOP reports to improve safety levels in infrastructure projects. These algorithms, including BOW, TF-IDF, the global vectors for word representation (GloVe), and sentence bidirectional encoder representation from transformers (SBERT), were combined with machine learning classifiers such as random forest (RF), gaussian naive bayes (NB), decision tree (DT), and k-nearest neighbors (KNN), using two small HAZOP datasets with varied inputs. A zero-shot text classification model was further evaluated for its ability to assign labels to HAZOP data without prior training. Results demonstrate that GloVe combined with RF achieves the highest accuracy (83 %), significantly outperforming other models. We further observe that KNN degrades on short text features, while DT underperforms on longer descriptions. The zero-shot model achieves low performance (52 % accuracy), lacking the precision needed for fine-grained, jargon-heavy labels. These findings indicate that GloVe embeddings remain a robust foundation for HAZOP automation under data-scarce conditions in the operations of infrastructure projects.}
}
@article{WEN2023e20390,
title = {A systematic knowledge graph-based smart management method for operations: A case study of standardized management},
journal = {Heliyon},
volume = {9},
number = {10},
pages = {e20390},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e20390},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023075989},
author = {Peihan Wen and Yiming Zhao and Jin Liu},
keywords = {Knowledge graph, Operation management, Quality management system, Knowledge management},
abstract = {Standardized routine operation management (SROM) has been widely accepted and applied by kinds of enterprises and played a key supporting role. With full use of the emerging knowledge-based smart management technology, SROM will further increase comprehensive efficiency and save human resources greatly at the same time, especially for small and medium enterprises (SMEs). Hence, we propose a systematic knowledge-based smart management method to transfer SROM activities from human operations to automatic response by means of knowledge explicitation, organization, sharing and reusing, which can be further achieved by employing knowledge graph. We took a typical SROM instance, ISO 9000 implementation management, as an example to validate the transformation from human activities to knowledge graph-based automatic operation. We firstly analyzed characteristics of domain knowledge and constructed an ontology model according to the knowledge stability. Secondly, a hybrid knowledge graph construction and dynamic updating framework together with related algorithms were designed by deliberately integrating semantic similarity calculation and natural language processing. Thirdly, we developed a question-answering mechanism and reasoning system based on the ISO 9000 implementation knowledge graph to support automatic decision and feedback for ISO 9000 routine operation management including knowledge learning and processes auditing. Finally, the practicability and effectiveness of SROM knowledge graph has been validated in a SME in China, realizing the application of question-answering, job responsibility recommendation, conflict detection, semantic detection, multidimensional statistical analysis. The proposed method can also be generalized to support auxiliary optimization decision, vertical risk control, operation mode analysis, optimization model improvement experience and so on.}
}
@article{RAMONELL2023105109,
title = {Knowledge graph-based data integration system for digital twins of built assets},
journal = {Automation in Construction},
volume = {156},
pages = {105109},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105109},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523003692},
author = {Carlos Ramonell and Rolando Chacón and Héctor Posada},
keywords = {Digital twin, BIM, Data integration, Knowledge graph, Microservice architecture},
abstract = {The emergence of digital twin technologies offers a promising avenue for improving decision-making through the integrated use of up-to-date physical or synthetically simulated data. Nevertheless, the practical implementation of digital twins in the built environment remains a significant challenge. This paper describes a system that seamlessly integrates data into digital twins of built assets. The system uses a knowledge graph to achieve data integration, which is designed to be modular, flexible, and interoperable. The graph includes BIM models, metadata from an external IoT platform, and process-related information. The system is microservice-based and revolves around a graph database housing the knowledge graph. It employs dynamic operations to update the knowledge graph and is tested using civil engineering infrastructure examples. Results from this work can be used to create pipelines that extract and operate with data connecting computational agents integrated into the system as microservices or connected through the system API.}
}
@article{NUNDLOLL2023,
title = {A Dynamic Interoperability Model for an Emergent Middleware Framework},
journal = {International Journal of Distributed Systems and Technologies},
volume = {14},
number = {1},
year = {2023},
issn = {1947-3532},
doi = {https://doi.org/10.4018/IJDST.317420},
url = {https://www.sciencedirect.com/science/article/pii/S194735322300003X},
author = {Vatsala Nundloll and Gordon Blair},
keywords = {distributed systems, emergent middleware, interoperability, middleware framework, ontologies, semantic, vanet},
abstract = {ABSTRACT
Standard middleware platforms are unable to cope with extreme heterogeneity and dynamicity of distributed systems. With new trends in mobile/pervasive applications, distributed systems are required to connect to one another at run time, implying that heterogeneities arising in systems need to be resolved on the fly. This ability of a system to interact with a different system is known as interoperability. More advanced solutions, which exceed the state-of-the-art in middleware, are required to handle interoperability on the fly. This paper investigates the challenges of enabling dynamic interoperability for the domain of vehicular ad-hoc networks (VANETs). The paper uses semantic web technologies to help devise an emergent middleware to enable different VANETs to interact with each other at runtime. An ontology-based framework coupled with an experimental evaluation of the framework is presented. The need for linguistic techniques in assisting ontologies is also emphasized in the framework.}
}
@article{LOPEZ2025100848,
title = {Context in abusive language detection: On the interdependence of context and annotation of user comments},
journal = {Discourse, Context & Media},
volume = {63},
pages = {100848},
year = {2025},
issn = {2211-6958},
doi = {https://doi.org/10.1016/j.dcm.2024.100848},
url = {https://www.sciencedirect.com/science/article/pii/S2211695824000941},
author = {Holly Lopez and Sandra Kübler},
keywords = {Automated abusive language detection, Corpora development, Annotation, Data curation, Contextualization},
abstract = {One of the challenges for automated abusive language detection is combating unintended bias, which can be easily introduced through the annotation process, especially when what is (not) considered abusive is subjective and heavily context dependent. Our study incorporates a fine-grained, socio-pragmatic perspective to data modeling by taking into consideration contextual elements that impact the quality of abusive language corpora. We use a fine-grained annotation scheme that distinguishes between different types of non-abuse along with explicit and implicit abuse. We include the following non-abusive categories: meta, casual profanity, argumentative language, irony, and non-abusive language. Experts and minimally trained annotators use this scheme to manually re-annotate instances originally considered abusive by crowdsourced annotators in a standard corpus. After re-annotation, we investigate discrepancies between experts and minimally trained annotators. Our investigation shows that minimally trained annotators have difficulty interpreting contextual aspects and distinguishing between content performing abuse and content about abuse or instances of casual profanity. It also demonstrates how missing information or contextualization cues are often a source of disagreement across all types of annotators and poses a significant challenge for developing robust, nuanced corpora and annotation guidelines for abusive language detection.}
}
@article{WU2025103490,
title = {Cognitive digital thread tool-chain for model versioning in model-based systems engineering},
journal = {Advanced Engineering Informatics},
volume = {67},
pages = {103490},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103490},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625003830},
author = {Shouxuan Wu and Guoxin Wang and Jinzhi Lu and Jianyu Huang and Jiaxing Qiao and Yan Yan and Dimitris Kiritsis},
keywords = {Cognitive digital thread, Model-based systems engineering, Model versioning, Open service for lifeCycle collaboration, KARMA language, Knowledge graph},
abstract = {Model-based systems engineering (MBSE) allows system models to formalize end-to-end systems engineering implementation while developing complex engineering system. The evolution of MBSE models, including changes and conflicts, provides important historical knowledge to support design decisions. Model versioning is an efficient approach to manage the evolution of MBSE models. However, the heterogeneous data structure and semantics used in MBSE practices hinder the tool interoperability that is required in model versioning, which also decreases the effectiveness and efficiency of system development. This paper proposes a tool-chain for model versioning of MBSE models based on a cognitive digital thread (CDT). In this tool-chain, the graph–object–point–property-relationship-role-extension (GOPPRR-E) modeling approach is adopted because it is compatible with heterogeneous modeling languages used in model versioning. To promote tool interoperability, this tool-chain adopts the Open Services for Lifecycle Collaboration to support conflict detection or resolution during model versioning. In particular, knowledge graphs are generated along with the model versioning workflow to develop a CDT, which provides the cognitive reasoning ability required for model versioning behaviors. A case study of landing gear system development is used to evaluate the feasibility of the proposed tool-chain through qualitative and quantitative analyses. The results demonstrate that the proposed tool-chain has better efficiency than traditional model versioning using Git tools.}
}
@article{MAKANDA20241091,
title = {Leveraging natural language processing and community detection for shaping manufacturing communities in social manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {74},
pages = {1091-1105},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524001171},
author = {Inno Lorren Désir Makanda and Maolin Yang and Haoliang Shi and Pingyu Jiang},
keywords = {Social manufacturing, Collaborative network, Overlapping community detection, Multi-label text classification, Recommender system},
abstract = {Social manufacturing (SocialMfg), as an emerging paradigm, leverages socialized manufacturing resource nodes (SMRNs) grouped into manufacturing communities (MCs) through cyber-physical-social connections to collectively create, produce, and share goods and services. Despite its nascent stage, the potential impact of SocialMfg on mass personalization and sustainability is significant, prompting manufacturing enterprises to increasingly adopt this model facilitated by various industrial Internet platforms (IIPs). However, SMRNs face challenges such as trust and reputation and alignment of interests when forming MCs. This article proposes an approach that integrates natural language processing and community detection algorithm to autonomously form relevant MCs among SMRNs on IIPs. A novel BERT-like model, SoManBERT, is introduced to accurately classify the manufacturing interests and roles of SMRNs, revealing their expertise. Subsequently, a recommender system, integrating trust scores and a modified-density-peaks-based overlapping community detection (DPOCD) algorithm, is designed to recommend reliable SMRNs with similar manufacturing interests or roles to each other. The effectiveness of the proposed approach is verified through a case study on a SocialMfg prototype system. Empirical evaluations reveal that this approach surpasses baseline methods, demonstrating its potential for SocialMfg environments.}
}
@article{WANG2023103509,
title = {MsPrompt: Multi-step prompt learning for debiasing few-shot event detection},
journal = {Information Processing & Management},
volume = {60},
number = {6},
pages = {103509},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103509},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323002467},
author = {Siyuan Wang and Jianming Zheng and Fei Cai and Chengyu Song and Xueshan Luo},
keywords = {Few-shot event detection, Prompt learning, Debiasing},
abstract = {Event detection (ED) is aimed to identify the key trigger words in unstructured text and predict the event types accordingly. Traditional ED models are too data-hungry to accommodate real applications with scarce labeled data. Besides, typical ED models are facing the context-bypassing and disabled generalization issues caused by the trigger bias stemming from ED datasets. Therefore, we focus on the true few-shot paradigm targeting to construct a novel training set that accommodates the low-resource scenarios. In particular, we propose a multi-step prompt learning model (MsPrompt) for debiasing few-shot event detection (FSED), consisting of the following two components: a multi-step prompt module equipped with a knowledge-enhanced ontology to leverage the event semantics and latent prior knowledge in the pretrained language models (PLMs) sufficiently for tackling the context-bypassing problem, and a prototypical network module compensating for the weakness of classifying events with sparse data and boost the generalization performance. Experiments on two public datasets ACE-2005 and FewEvent show that MsPrompt can outperform the state-of-the-art models, especially in the strict low-resource scenarios reporting 11.43% improvement in terms of weighted F1-score against the best baseline and achieving an outstanding debiasing performance.}
}
@article{VERNADAT2020103265,
title = {Enterprise modelling: Research review and outlook},
journal = {Computers in Industry},
volume = {122},
pages = {103265},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103265},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520304991},
author = {François Vernadat},
keywords = {Enterprise modelling, Enterprise engineering, Enterprise architecture, Enterprise modelling languages, Modelling constructs, Enterprise ontology, ISO 19439:2006, ISO 19440:2007},
abstract = {Enterprise models are essential for the understanding, analysis, engineering, improvement, optimisation, maintenance and even management and control of enterprise systems. In that sense, Enterprise Modelling is foundational for Enterprise Engineering, Integration and Management and closely associated to Enterprise Architectures. This synthesis paper reviews and summarises important research works and contributions made to Enterprise Modelling over the last four decades, outlining major modelling constructs and their extensions as well as prominent modelling tools and methods and discussing future developments in the context of smart manufacturing or Industry 4.0.}
}
@article{SHEEBA20181592,
title = {Semantic Predictive Model of Student Dynamic Profile Using Fuzzy Concept},
journal = {Procedia Computer Science},
volume = {132},
pages = {1592-1601},
year = {2018},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.05.124},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918308561},
author = {T. Sheeba and Reshmy Krishnan},
keywords = {Dynamic studentprofile, Ontology, WordNet, Reasoning, Fuzzy linguistic variable, Fuzzy IF THEN rules},
abstract = {Student profile describes the best way a student prefers to learn. It includes information on student’s characteristics such as background knowledge, learning preference, styles, interest, goals etc. The major challenge that the students face in learning system is that they are unable to retrieve relevant information based on their requirements. One of the methods used to obtain the requirement of the students is to construct an efficient student profile which would reflect the true student needs. The proposed work is to develop an intelligent ontology-based dynamic student profile that provides semantic retrieval using fuzzy concepts. The approach starts with the collection of both static and dynamic data of students. The dynamic dataof students particularly student interest and learning style are obtained by weblog analysis using algorithms such as semantic based representation using WordNet and decision tree classifier algorithm based on Felder-Silverman learning style model (FSLSM). The retrieved data is then used to construct student profile using ontology in which automatic student profile updating is obtained using ontology -based semantic similarity algorithm using WordNet. Finally, semantic retrieval of student information from ontology is achieved by integrating fuzzy concepts using fuzzy linguistic variable and ‘fuzzy IF THEN’ rules. Fuzzy linguistic variable is used to make precise representation on the existing ontology concepts which facilitate more specific classification and semantic retrieval of information. The predictive model of student profile is designed with the implementation of ‘fuzzy IF THEN’ rules using forward chaining reasoning process in the existing ontology model. The inference engine predicts the preference of a new student based on the reasoning process done for specific conditions particularly on student interest and learning style. The experiments were performed using NetBeans IDE, OWL API and Protégé 4.2 beta editor. The experiment result shows the successful completion of student profile generation, updating, fuzzy semantic retrieval and prediction through utilization of fuzzy concepts in student profile ontology.}
}
@article{AI2025108666,
title = {Self-mourning in the digital age: Insights from Douban's Online Graveyard in Chinese},
journal = {Computers in Human Behavior},
volume = {168},
pages = {108666},
year = {2025},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2025.108666},
url = {https://www.sciencedirect.com/science/article/pii/S074756322500113X},
author = {Qi Ai and Yansheng Mao and Huimin Huang},
keywords = {Self-mourning, Digital age, Douban, Online Graveyard, Chinese},
abstract = {This study provides an in-depth analysis of self-mourning discourse within the “Tomb/Grave for Oneself” section of Douban's Online Graveyard, focusing on the types, temporal patterns, and emotional self-regulation processes involved in Chinese. Using a mixed-methods approach, it draws on the Affective Lexicon Ontology to reveal the multidimensionality of emotional expressions in Chinese online self-mourning, encompassing both negative emotions (e.g., pain, depression, anxiety) and positive emotions (e.g., hope, like). These findings underscore the dual role of online self-mourning platforms as spaces for emotional disclosure and psychological healing. Furthermore, the study identifies a temporal trajectory in emotional expressions, transitioning from past-oriented negative emotions to future-oriented positive ones, signaling a shift from trauma to self-renewal. This emotional transformation aligns with the process model of self-focused emotion regulation (Gross, 1998, 2015, 2024), highlighting the mechanisms through which online self-mourners achieve psychological resilience. The findings offer new insights into the emotional dynamics of online self-disclosure and its potential for fostering mental well-being.}
}
@article{SCHEUER2022127780,
title = {A trait-based typification of urban forests as nature-based solutions},
journal = {Urban Forestry & Urban Greening},
volume = {78},
pages = {127780},
year = {2022},
issn = {1618-8667},
doi = {https://doi.org/10.1016/j.ufug.2022.127780},
url = {https://www.sciencedirect.com/science/article/pii/S1618866722003235},
author = {Sebastian Scheuer and Jessica Jache and Martina Kičić and Thilo Wellmann and Manuel Wolff and Dagmar Haase},
keywords = {Urban forest, Nature-based solution, Typology, Trait-based modelling, Semantics, Ontology},
abstract = {Urban forests as nature-based solutions (UF-NBS) are important tools for climate change adaptation and sustainable development. However, achieving both effective and sustainable UF-NBS solutions requires diverse knowledge. This includes knowledge on UF-NBS implementation, on the assessment of their environmental impacts in diverse spatial contexts, and on their management for the long-term safeguarding of delivered benefits. A successful integration of such bodies of knowledge demands a systematic understanding of UF-NBS. To achieve such an understanding, this paper presents a conceptual UF-NBS model obtained through a semantic, trait-based modelling approach. This conceptual model is subsequently implemented as an extendible, re-usable and interoperable ontology. In so doing, a formal, trait-based vocabulary on UF-NBS is created, that allows expressing spatial, morphological, physical, functional, and institutional UF-NBS properties for their typification and a subsequent integration of further knowledge and data. Thereby, ways forward are opened for a more systematic UF-NBS impact assessment, management, and decision-making.}
}
@article{FREEDMAN2020100086,
title = {A novel tool for standardizing clinical data in a semantically rich model},
journal = {Journal of Biomedical Informatics},
volume = {112},
pages = {100086},
year = {2020},
note = {Articles initially published in Journal of Biomedical Informatics: X 5-8, 2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.yjbinx.2020.100086},
url = {https://www.sciencedirect.com/science/article/pii/S2590177X20300214},
author = {Hayden G. Freedman and Heather Williams and Mark A. Miller and David Birtwell and Danielle L. Mowery and Christian J. Stoeckert},
keywords = {Clinical data, Biomedical ontologies, Common data model, Data interoperability, Semantic Web technologies},
abstract = {Standardizing clinical information in a semantically rich data model is useful for promoting interoperability and facilitating high quality research. Semantic Web technologies such as Resource Description Framework can be utilized to their full potential when a model accurately reflects the semantics of the clinical situation it describes. To this end, ontologies that abide by sound organizational principles can be used as the building blocks of a semantically rich model for the storage of clinical data. However, it is a challenge to programmatically define such a model and load data from disparate sources. The PennTURBO Semantic Engine is a tool developed at the University of Pennsylvania that transforms concise RDF data into a source-independent, semantically rich model. This system sources classes from an application ontology and specifically defines how instances of those classes may relate to each other. Additionally, the system defines and executes RDF data transformations by launching dynamically generated SPARQL update statements. The Semantic Engine was designed as a generalizable data standardization tool, and is able to work with various data models and incoming data sources. Its human-readable configuration files can easily be shared between institutions, providing the basis for collaboration on a standard data model.}
}
@article{GIORDANO20241170,
title = {POPCORN: Fictional and Synthetic Intelligence Reports for Named Entity Recognition and Relation Extraction Tasks},
journal = {Procedia Computer Science},
volume = {246},
pages = {1170-1180},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.542},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924025870},
author = {Bastien Giordano and Maxime Prieur and Nakanyseth Vuth and Sylvain Verdy and Kévin Cousot and Gilles Sérasset and Guillaume Gadek and Didier Schwab and Cédric Lopez},
keywords = {Synthetic Data Generation, Dataset, Natural Language Processing, Large Language Models, Information Extraction},
abstract = {POPCORN is a research project aiming at maturing Information Extraction (IE) solutions for intelligence services. Due to defense security constraints, reports analyzed by intelligence services are not to be accessible to the scientific community. To address this challenge, we propose a dataset made of “fictional” (handcrafted) and “synthetic” (AI generated) French reports. Those synthetic reports are produced by an innovative approach that generates texts closely resembling real-world intelligence reports, facilitating the training and evaluation of IE tasks such as Entity and Relation Extraction. Experiments demonstrate the interest of synthetic reports to enhance the performance of IE models, showcasing their potential to augment real-world intelligence operations.}
}
@article{LI2022101487,
title = {Towards a unifying domain model of construction safety, health and well-being: SafeConDM},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101487},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101487},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002366},
author = {Beidi Li and Carl Schultz and Jochen Teizer and Olga Golovina and Jürgen Melzner},
keywords = {Fall hazards, Occupational health and safety, Building information modelling, Rule checking, Logic programming},
abstract = {Specific occupational construction safety, health, and well-being related knowledge and information are scattered and fragmented. Despite technological advancements of information and knowledge management, a link between safety management and information models is still missing. In this paper we present first steps towards a unifying formal (logic-based) domain model of construction safety, called SafeConDM, that consists of: (1) a semantically rich ontology of hazard, safety concepts, and concept relationships that builds on, and integrates with, existing construction safety ontologies and building information models; (2) a set of first-order if-then rules linking construction site states with the potential for specific hazards to occur that we define in a novel way using spatial artefacts. We present a prototype software tool, based on our ASP4BIM tool that implements SafeConDM for construction hazard analysis and safe construction planning decision support, and empirically evaluate our tool on three real-world construction building models.}
}
@article{ZHAO2023225,
title = {A Survey of Knowledge Graph Construction Using Machine Learning},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {139},
number = {1},
pages = {225-257},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.031513},
url = {https://www.sciencedirect.com/science/article/pii/S152614922300098X},
author = {Zhigang Zhao and Xiong Luo and Maojian Chen and Ling Ma},
keywords = {Knowledge graph (KG), semantic network, relation extraction, entity linking, knowledge reasoning},
abstract = {Knowledge graph (KG) serves as a specialized semantic network that encapsulates intricate relationships among real-world entities within a structured framework. This framework facilitates a transformation in information retrieval, transitioning it from mere string matching to far more sophisticated entity matching. In this transformative process, the advancement of artificial intelligence and intelligent information services is invigorated. Meanwhile, the role of machine learning method in the construction of KG is important, and these techniques have already achieved initial success. This article embarks on a comprehensive journey through the last strides in the field of KG via machine learning. With a profound amalgamation of cutting-edge research in machine learning, this article undertakes a systematical exploration of KG construction methods in three distinct phases: entity learning, ontology learning, and knowledge reasoning. Especially, a meticulous dissection of machine learning-driven algorithms is conducted, spotlighting their contributions to critical facets such as entity extraction, relation extraction, entity linking, and link prediction. Moreover, this article also provides an analysis of the unresolved challenges and emerging trajectories that beckon within the expansive application of machine learning-fueled, large-scale KG construction.}
}
@article{KASTRATI2019552,
title = {Performance analysis of machine learning classifiers on improved concept vector space models},
journal = {Future Generation Computer Systems},
volume = {96},
pages = {552-562},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1831745X},
author = {Zenun Kastrati and Ali Shariq Imran},
keywords = {Document representation, CVS, iCVS, Document classification, Deep learning, Ontology},
abstract = {This paper provides a comprehensive performance analysis of parametric and non-parametric machine learning classifiers including a deep feed-forward multi-layer perceptron (MLP) network on two variants of improved Concept Vector Space (iCVS) model. In the first variant, a weighting scheme enhanced with the notion of concept importance is used to assess weight of ontology concepts. Concept importance shows how important a concept is in an ontology and it is automatically computed by converting the ontology into a graph and then applying one of the Markov based algorithms. In the second variant of iCVS, concepts provided by the ontology and their semantically related terms are used to construct concept vectors in order to represent the document into a semantic vector space. We conducted various experiments using a variety of machine learning classifiers for three different models of document representation. The first model is a baseline concept vector space (CVS) model that relies on an exact/partial match technique to represent a document into a vector space. The second and third model is an iCVS model that employs an enhanced concept weighting scheme for assessing weights of concepts (variant 1), and the acquisition of terms that are semantically related to concepts of the ontology for semantic document representation (variant 2), respectively. Additionally, a comparison between seven different classifiers is performed for all three models using precision, recall, and F1 score. Results for multiple configurations of deep learning architecture are obtained by varying the number of hidden layers and nodes in each layer, and are compared to those obtained with conventional classifiers. The obtained results show that the classification performance is highly dependent upon the choice of a classifier, and that the Random Forest, Gradient Boosting, and Multilayer Perceptron are among the classifiers that performed rather well for all three models.}
}
@article{NOLL2023120,
title = {Machine translation of standardised medical terminology using natural language processing: A scoping review},
journal = {New Biotechnology},
volume = {77},
pages = {120-129},
year = {2023},
issn = {1871-6784},
doi = {https://doi.org/10.1016/j.nbt.2023.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1871678423000432},
author = {Richard Noll and Lena S. Frischen and Martin Boeker and Holger Storf and Jannik Schaaf},
keywords = {Controlled vocabulary, NLP, Machine translation},
abstract = {Standardised medical terminologies are used to ensure accurate and consistent communication of information and to facilitate data exchange. Currently, many terminologies are only available in English, which hinders international research and automated processing of medical data. Natural language processing (NLP) and Machine Translation (MT) methods can be used to automatically translate these terms. This scoping review examines the research on automated translation of standardised medical terminology. A search was performed in PubMed and Web of Science and results were screened for eligibility by title and abstract as well as full text screening. In addition to bibliographic data, the following data items were considered: 'terminology considered', 'terms considered', 'source language', 'target language', 'translation type', 'NLP technique', 'NLP system', 'machine translation system', 'data source' and 'translation quality'. The results showed that the most frequently translated terminology is SNOMED CT (39.1%), followed by MeSH (13%), ICD (13%) and UMLS (8.7%). The most common source language is English (55.9%), and the most common target language is German (41.2%). Translation methods are often based on Statistical Machine Translation (SMT) (41.7%) and, more recently, Neural Machine Translation (NMT) (30.6%), but can also be combined with various MT methods. Commercial translators such as Google Translate (36.4%) and automatic validation methods such as BLEU (22.2%) are frequently used tools for translation and subsequent validation.}
}
@article{ISMAILOVA2022463,
title = {Applicative approach to construe a computational model of concepts and individuals},
journal = {Procedia Computer Science},
volume = {213},
pages = {463-470},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.092},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922017835},
author = {Larisa Ismailova and Viacheslav Wolfengagen and Sergey Kosikov},
keywords = {concept-as-a-process, application, abstraction, conceptual minimalism, computational model},
abstract = {This paper considers the application of conceptual mathematics to construct a computational model of concepts and individuals. An applicative approach is systematically applied to build a concept-as-process design. Since modern computing considers information processes as the main objects of modeling, the developed design is indeed representative of the semantic processing of information. The nature of concepts – what concepts are – and the constraints that govern the theory of concepts have been, and continue to be, the subject of debate. It is especially interesting to discuss the nature of concepts in connection with the recently established fundamental nature of information processes that are attributable to all phenomena and events occurring in the world around us. The current trend elevates information processes to forms of computing, which can also be implemented through practices, for example, in the form of programming. The deep component is computational models, one way or another expressed by means of mathematics and metamathematics. The main meta-operations used are abstraction and application. Of greatest interest is functional abstraction and application in the form of applying a function to an argument. Despite this “conceptual minimalism”, a rich theory of concepts can be developed. Using this theory, it is possible to focus further discussion not only on the nature of concepts, but also to characterize the position on each of the five important issues that are central to many theories of concepts: (1) ontology of concepts, (2) structure of concepts, (3) empiricism, and nativism about concepts, (4) concepts and natural language, and (5) concepts and conceptual analysis.}
}
@article{NABLI2018233,
title = {Efficient cloud service discovery approach based on LDA topic modeling},
journal = {Journal of Systems and Software},
volume = {146},
pages = {233-248},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.09.069},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302103},
author = {Hajer Nabli and Raoudha {Ben Djemaa} and Ikram Amous {Ben Amor}},
keywords = {LDA Model, TF-IDF, Semantic focused crawler, Cloud service ontology, Self-adaptive ontology, URLs priority},
abstract = {With the rapid development of Cloud-based services, the necessity of a Cloud service discovery engine becomes a fundamental requirement. A semantic focused crawler is one of the most key components of Cloud service discovery engines. However, the huge size and varied functionalities of Cloud services on the Web have a great effect on crawlers to provide effective Cloud services. It is a challenge for semantic crawlers to search only for URLs that offer Cloud services from this explosion of information. To solve these issues, this paper proposes a self-adaptive semantic focused crawler based on Latent Dirichlet Allocation (LDA) for efficient Cloud service discovery. In this paper, we present a Cloud Service Ontology (CSOnt) that defines Cloud service categories. CSOnt contains a set of concepts, allowing the crawler to automatically collect and categorize Cloud services. Moreover, our proposed crawler adopts URLs priority techniques to maintain the order of URLs to be parsed for efficient retrieval of the relevant Cloud services. Additionally, we create a self-adaptive semantic focused crawler, which has an ontology-learning function to automatically improve the proposed Cloud Service Ontology and maintain the crawler’s performance.}
}
@article{ABAZOGLU2025101947,
title = {The use of language corpora in teaching Arabic to Turkish speakers within the framework of computational linguistics},
journal = {Social Sciences & Humanities Open},
volume = {12},
pages = {101947},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101947},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125006758},
author = {Muhammet Abazoglu and Mohammad Issa Alhourani},
keywords = {Corpus linguistics, Second language acquisition, Pedagogical corpora, Turkish learners, Arabic as a foreign language},
abstract = {Modern technologies have significantly transformed various knowledge domains, facilitating the integration of digital tools into educational practices, particularly in foreign-language instruction. This convergence has strengthened interdisciplinary approaches such as applied and computational linguistics. Computational linguistics, which synthesizes linguistic theory with advanced technological applications, offers innovative methodologies for generating, analyzing, and utilizing linguistic data to support language education. This study examines the potential application of corpus linguistics in teaching Arabic as a foreign language to Turkish speakers. It proposes a theoretical framework for developing a bilingual Arabic–Turkish corpus tailored to the linguistic and pedagogical needs of Turkish learners in academic settings. The paper reviews the fundamental concepts of linguistic corpora, their classifications, and their pedagogical functions within second-language acquisition. It also discusses essential corpus-design criteria, emphasizing usability, representativeness, and learner relevance. As a practical contribution, a mini-corpus developed using digital linguistic software is presented to illustrate the proposed approach. Theoretically, the study contributes to the literature by integrating corpus linguistics with applied and computational linguistics and by outlining a bilingual corpus model for Arabic Turkish contexts. Empirically, the corpus analysis suggests that such tools can facilitate data-driven learning and support the development of listening, speaking, reading, and writing skills. While no experimental intervention was conducted, the findings indicate that carefully designed corpora can serve as effective resources for non-native Arabic learners, providing authentic input and encouraging learner autonomy. The study also underscores the importance of interdisciplinary collaboration and technological investment to advance corpus-based Arabic language pedagogy.}
}
@article{KUMHAR20223899,
title = {Translation of English Language into Urdu Language Using LSTM Model},
journal = {Computers, Materials and Continua},
volume = {74},
number = {2},
pages = {3899-3912},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.032290},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822004180},
author = {Sajadul Hassan Kumhar and Syed Immamul Ansarullah and Akber Abid Gardezi and Shafiq Ahmad and Abdelaty Edrees Sayed and Muhammad Shafiq},
keywords = {Machine translation, Urdu language, word embedding},
abstract = {English to Urdu machine translation is still in its beginning and lacks simple translation methods to provide motivating and adequate English to Urdu translation. In order to make knowledge available to the masses, there should be mechanisms and tools in place to make things understandable by translating from source language to target language in an automated fashion. Machine translation has achieved this goal with encouraging results. When decoding the source text into the target language, the translator checks all the characteristics of the text. To achieve machine translation, rule-based, computational, hybrid and neural machine translation approaches have been proposed to automate the work. In this research work, a neural machine translation approach is employed to translate English text into Urdu. Long Short Term Short Model (LSTM) Encoder Decoder is used to translate English to Urdu. The various steps required to perform translation tasks include preprocessing, tokenization, grammar and sentence structure analysis, word embeddings, training data preparation, encoder-decoder models, and output text generation. The results show that the model used in the research work shows better performance in translation. The results were evaluated using bilingual research metrics and showed that the test and training data yielded the highest score sequences with an effective length of ten (10).}
}
@article{ADDEPALLI202333,
title = {Automation of knowledge extraction for degradation analysis},
journal = {CIRP Annals},
volume = {72},
number = {1},
pages = {33-36},
year = {2023},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2023.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0007850623000070},
author = {Sri Addepalli and Tillman Weyde and Bernadin Namoano and Oluseyi Ayodeji Oyedeji and Tiancheng Wang and John Ahmet Erkoyuncu and Rajkumar Roy},
keywords = {Knowledge management, Decision making, Knowledge graph},
abstract = {Degradation analysis relies heavily on capturing degradation data manually and its interpretation using knowledge to deduce an assessment of the health of a component. Health monitoring requires automation of knowledge extraction to improve the analysis, quality and effectiveness over manual degradation analysis. This paper proposes a novel approach to achieve automation by combining natural language processing methods, ontology and a knowledge graph to represent the extracted degradation causality and a rule based decision-making system to enable a continuous learning process. The effectiveness of this approach is demonstrated by using an aero-engine component as a use-case.}
}
@article{GUTIERREZ2024107854,
title = {KD SENSO-MERGER: An architecture for semantic integration of heterogeneous data},
journal = {Engineering Applications of Artificial Intelligence},
volume = {132},
pages = {107854},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.107854},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624000125},
author = {Yoan Gutiérrez and José I. Abreu Salas and Andrés Montoyo and Rafael Muñoz and Suilan Estévez-Velarde},
keywords = {Heterogeneous data, Knowledge discovery, NERC, Natural language processing, Ontology and knowledge representation, Semantic data integration},
abstract = {This paper presents KD SENSO-MERGER, a novel Knowledge Discovery (KD) architecture that is capable of semantically integrating heterogeneous data from various sources of structured and unstructured data (i.e. geolocations, demographic, socio-economic, user reviews, and comments). This goal drives the main design approach of the architecture. It works by building internal representations that adapt and merge knowledge across multiple domains, ensuring that the knowledge base is continuously updated. To deal with the challenge of integrating heterogeneous data, this proposal puts forward the corresponding solutions: (i) knowledge extraction, addressed via a plugin-based architecture of knowledge sensors; (ii) data integrity, tackled by an architecture designed to deal with uncertain or noisy information; (iii) scalability, this is also supported by the plugin-based architecture as only relevant knowledge to the scenario is integrated by switching-off non-relevant sensors. Also, we minimize the expert knowledge required, which may pose a bottleneck when integrating a fast-paced stream of new sources. As proof of concept, we developed a case study that deploys the architecture to integrate population census and economic data, municipal cartography, and Google Reviews to analyze the socio-economic contexts of educational institutions. The knowledge discovered enables us to answer questions that are not possible through individual sources. Thus, companies or public entities can discover patterns of behavior or relationships that would otherwise not be visible and this would allow extracting valuable information for the decision-making process.}
}
@article{TERHORST2023102491,
title = {Automatic knowledge graph population with model-complete text comprehension for pre-clinical outcomes in the field of spinal cord injury},
journal = {Artificial Intelligence in Medicine},
volume = {137},
pages = {102491},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102491},
url = {https://www.sciencedirect.com/science/article/pii/S0933365723000052},
author = {Hendrik {ter Horst} and Nicole Brazda and Jessica Schira-Heinen and Julia Krebbers and Hans-Werner Müller and Philipp Cimiano},
keywords = {Information extraction, Pre-clinical outcomes, Structured prediction, Spinal cord injury, Deep knowledge graph population},
abstract = {The paradigm of evidence-based medicine requires that medical decisions are made on the basis of the best available knowledge published in the literature. Existing evidence is often summarized in the form of systematic reviews and/or meta-reviews and is rarely available in a structured form. Manual compilation and aggregation is costly, and conducting a systematic review represents a high effort. The need to aggregate evidence arises not only in the context of clinical trials, but is also important in the context of pre-clinical animal studies. In this context, evidence extraction is important to support translation of the most promising pre-clinical therapies into clinical trials or to optimize clinical trial design. Aiming at developing methods that facilitate the task of aggregating evidence published in pre-clinical studies, in this paper a new system is presented that automatically extracts structured knowledge from such publications and stores it in a so-called domain knowledge graph. The approach follows the paradigm of model-complete text comprehension by relying on guidance from a domain ontology creating a deep relational data-structure that reflects the main concepts, protocol, and key findings of studies. Focusing on the domain of spinal cord injuries, a single outcome of a pre-clinical study is described by up to 103 outcome parameters. Since the problem of extracting all these variables together is intractable, we propose a hierarchical architecture that incrementally predicts semantic sub-structures according to a given data model in a bottom-up fashion. At the heart of our approach is a statistical inference method that relies on conditional random fields to infer the most likely instance of the domain model given the text of a scientific publication as input. This approach allows modeling dependencies between the different variables describing a study in a semi-joint fashion. We present a comprehensive evaluation of our system to understand the extent to which our system can capture a study in the depth required to enable the generation of new knowledge. We conclude the article with a brief description of some applications of the populated knowledge graph and show the potential implications of our work for supporting evidence-based medicine.}
}
@article{WANG2022104317,
title = {Improving knowledge capture and retrieval in the BIM environment: Combining case-based reasoning and natural language processing},
journal = {Automation in Construction},
volume = {139},
pages = {104317},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104317},
url = {https://www.sciencedirect.com/science/article/pii/S092658052200190X},
author = {Hao Wang and Xianhai Meng and Xingyu Zhu},
keywords = {Building information modeling (BIM), Knowledge management (KM), Natural language processing (NLP), Case-based reasoning (CBR), Deep learning, Construction project, Neural network},
abstract = {Most knowledge management (KM) techniques capture knowledge at the end of a project, leading to knowledge loss. Building information modeling (BIM) is a building information management process throughout the project lifecycle. This study uses BIM for lifecycle knowledge capture to address knowledge loss. Knowledge in construction projects is intricate, intensifying the challenges of knowledge retrieval. This study combines natural language processing (NLP) and case-based reasoning (CBR) to improve knowledge retrieval, which not only takes advantage of CBR for retrieving knowledge case context through attributes but also obtains the benefits of NLP for retrieving text descriptions of knowledge cases. The parameters created in BIM correspond to the CBR attributes and NLP case descriptions. Quantitative test and case study confirm that the novel approach proposed in this study can improve knowledge retrieval performance and prevent knowledge loss. It also provides new insights into the transformation from BIM to building knowledge modeling (BKM).}
}
@article{LAGOS2018463,
title = {Towards semantically-aided domain specific business process modeling},
journal = {Data Technologies and Applications},
volume = {52},
number = {4},
pages = {463-481},
year = {2018},
issn = {2514-9288},
doi = {https://doi.org/10.1108/DTA-01-2018-0007},
url = {https://www.sciencedirect.com/science/article/pii/S2514928818000238},
author = {Nikolaos Lagos and Adrian Mos and Mario Cortes-cornax},
keywords = {Business process management, Ontology, Domain-specific process modelling, Multi-context systems, Multi-layer process, Semantic process modelling},
abstract = {Purpose
Domain-specific process modeling has been proposed in the literature as a solution to several problems in business process management. The problems arise when using only the generic Business Process Model and Notation (BPMN) standard for modeling. This language includes domain ambiguity and difficult long-term model evolution. Domain-specific modeling involves developing concept definitions, domain-specific processes and eventually industry-standard BPMN models. This entails a multi-layered modeling approach, where any of these artifacts can be modified by various stakeholders and changes done by one person may influence models used by others. There is therefore a need for tool support to keep track of changes done and their potential impacts. The paper aims to discuss these issues.
Design/methodology/approach
The authors use a multi-context systems-based approach to infer the impacts that changes may cause in the models; and alsothe authors incrementally map components of business process models to ontologies.
Findings
Advantages of the framework include: identifying conflicts/inconsistencies across different business modeling layers; expressing rich information on the relations between two layers; calculating the impact of changes taking place in one layer to the rest of the layers; and selecting incrementally the most appropriate semantic models on which the transformations can be based.
Research limitations/implications
The authors consider this work as one of the foundational bricks that will enable further advances toward the governance of multi-layer business process modeling systems. Extensive usability tests would enable to further confirm the findings of the paper.
Practical implications
The approach described here should improve the maintainability, reuse and clarity of business process models and in extension improve data governance in large organizations. The approaches described here should improve the maintainability, reuse and clarity of business process models. This can improve data governance in large organizations and for large collections of processes by aiding various stakeholders to understand problems with process evolutions, changes and inconsistencies with business goals.
Originality/value
This paper fulfills an identified gap to enabling semantically aided domain–specific process modeling.}
}
@article{FORTINEAU201922,
title = {Automated business rules and requirements to enrich product-centric information},
journal = {Computers in Industry},
volume = {104},
pages = {22-33},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518301477},
author = {Virginie Fortineau and Thomas Paviot and Samir Lamouri},
keywords = {Business rule, Requirements, Ontology, PLM, BIM},
abstract = {Current PLM or BIM based information systems suffer from a lack of checking components for business rules. One reason is the misunderstanding of the role and nature of business rules, and how they should be treated in a product-centric information system. This paper intends to provide both a process and a related model to build such a component and enrich future systems. Rules and requirements process management enables the unambiguous formalization of implicit knowledge contained in business rules, generally expressed in easily understandable language, and leads to the formal expression of requirements. In this paper, the requirements are considered a consequence of the application of a business rule. A conceptual model is then introduced, called DALTON (DAta Linked Through Occurrences Network), which supports this process. In this ontology, concepts and product data, coming for instance from an existing product database, are represented using instances and occurrences, connected together with triples built from business rules and requirements according to previous management processes. An experiment involving a set of SWRL rules is conducted in the Protégé environment that validates the model and the process.}
}
@article{LAI2023104392,
title = {KEBLM: Knowledge-Enhanced Biomedical Language Models},
journal = {Journal of Biomedical Informatics},
volume = {143},
pages = {104392},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104392},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423001132},
author = {Tuan Manh Lai and ChengXiang Zhai and Heng Ji},
keywords = {Pre-trained language models, Knowledge bases, Domain knowledge},
abstract = {Pretrained language models (PLMs) have demonstrated strong performance on many natural language processing (NLP) tasks. Despite their great success, these PLMs are typically pretrained only on unstructured free texts without leveraging existing structured knowledge bases that are readily available for many domains, especially scientific domains. As a result, these PLMs may not achieve satisfactory performance on knowledge-intensive tasks such as biomedical NLP. Comprehending a complex biomedical document without domain-specific knowledge is challenging, even for humans. Inspired by this observation, we propose a general framework for incorporating various types of domain knowledge from multiple sources into biomedical PLMs. We encode domain knowledge using lightweight adapter modules, bottleneck feed-forward networks that are inserted into different locations of a backbone PLM. For each knowledge source of interest, we pretrain an adapter module to capture the knowledge in a self-supervised way. We design a wide range of self-supervised objectives to accommodate diverse types of knowledge, ranging from entity relations to description sentences. Once a set of pretrained adapters is available, we employ fusion layers to combine the knowledge encoded within these adapters for downstream tasks. Each fusion layer is a parameterized mixer of the available trained adapters that can identify and activate the most useful adapters for a given input. Our method diverges from prior work by including a knowledge consolidation phase, during which we teach the fusion layers to effectively combine knowledge from both the original PLM and newly-acquired external knowledge using a large collection of unannotated texts. After the consolidation phase, the complete knowledge-enhanced model can be fine-tuned for any downstream task of interest to achieve optimal performance. Extensive experiments on many biomedical NLP datasets show that our proposed framework consistently improves the performance of the underlying PLMs on various downstream tasks such as natural language inference, question answering, and entity linking. These results demonstrate the benefits of using multiple sources of external knowledge to enhance PLMs and the effectiveness of the framework for incorporating knowledge into PLMs. While primarily focused on the biomedical domain in this work, our framework is highly adaptable and can be easily applied to other domains, such as the bioenergy sector.}
}
@article{DAQUIN2025100854,
title = {On the role of knowledge graphs in AI-based scientific discovery},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100854},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100854},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000404},
author = {Mathieu d’Aquin},
keywords = {Scientific discovery, Knowledge graphs, Machine learning, Interpretability},
abstract = {Research and the scientific activity are widely seen as an area where the current trends in AI, namely the development of deep learning models (including large language models), are having an increasing impact. Indeed, the ability of such models to extrapolate from data, seemingly finding unknown patterns relating implicit features of the objects under study to their properties can, at the very least, help accelerate and scale up those studies as demonstrated in fields such as molecular biology and chemistry. Knowledge graphs, on the other hand, have more traditionally been used to organize information around the scientific activity, keeping track of existing knowledge, of conducted experiments, of interactions within the research community, etc. However, for machine learning models to be truly used as a tool for scientific advancement, we have to find ways for the knowledge implicitly gained by these models from their training to be integrated with the explicitly represented knowledge captured through knowledge graphs. Based on our experience in ongoing projects in the domain of material science, in this position paper, we discuss the role that knowledge graphs can play in new methodologies for scientific discovery. These methodologies are based on the creation of large and opaque neural models. We therefore focus on the research challenges we need to address to support aligning such neural models to knowledge graphs for them to become a knowledge-level interface to those neural models.}
}
@article{KAYES2019237,
title = {Context-aware access control with imprecise context characterization for cloud-based data resources},
journal = {Future Generation Computer Systems},
volume = {93},
pages = {237-255},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.10.036},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18300785},
author = {A.S.M. Kayes and Wenny Rahayu and Tharam Dillon and Elizabeth Chang and Jun Han},
keywords = {Context-aware access control, Cloud-based data resources, Fuzzy contextual conditions, Context model, Fuzzy reasoning model, Policy model, Ontology},
abstract = {Computing technologies are increasingly dynamic and ubiquitous in everyday life nowadays. Context information plays a crucial role in such dynamically changing environments and the different types of contextual conditions bring new challenges to context-sensitive access control. This information mostly can be derived from the crisp sets. For example, we can utilize a crisp set to derive a patient and nurse are co-located in the general ward of the hospital or not. Some of the context information characterizations cannot be made using crisp sets, however, they are equally important in order to make access control decisions. Towards this end, this article proposes an approach to Context-Aware Access Control using Fuzzy logic (FCAAC) for data and information resources. We introduce a formal context model to represent the fuzzy and other contextual conditions. We also introduce a formal policy model to specify the policies by utilizing these conditions. Using our formal approach, we combine the fuzzy model with an ontology-based approach that captures such contextual conditions and incorporates them into the policies, utilizing the ontology languages and the fuzzy logic-based reasoning. We introduce a unified data ontology and its associated mapping ontology in terms of facilitating access control to cloud-based data resources. We justify the feasibility of our approach by demonstrating the practicality through a prototype implementation, several healthcare case studies and a usability study. Finally, we demonstrate an experimental evaluation in terms of query response time. The experiment results demonstrate the satisfactory performance of our proposed FCAAC approach.}
}
@article{STEINBERG2021103637,
title = {Language models are an effective representation learning technique for electronic health record data},
journal = {Journal of Biomedical Informatics},
volume = {113},
pages = {103637},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103637},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420302653},
author = {Ethan Steinberg and Ken Jung and Jason A. Fries and Conor K. Corbin and Stephen R. Pfohl and Nigam H. Shah},
keywords = {Electronic health record, Representation learning, Transfer learning, Risk stratification, Machine learning},
abstract = {Widespread adoption of electronic health records (EHRs) has fueled the development of using machine learning to build prediction models for various clinical outcomes. However, this process is often constrained by having a relatively small number of patient records for training the model. We demonstrate that using patient representation schemes inspired from techniques in natural language processing can increase the accuracy of clinical prediction models by transferring information learned from the entire patient population to the task of training a specific model, where only a subset of the population is relevant. Such patient representation schemes enable a 3.5% mean improvement in AUROC on five prediction tasks compared to standard baselines, with the average improvement rising to 19% when only a small number of patient records are available for training the clinical prediction model.}
}
@article{ALIDRA2023104,
title = {A feature-based survey of Fog modeling languages},
journal = {Future Generation Computer Systems},
volume = {138},
pages = {104-119},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22002710},
author = {Abdelghani Alidra and Hugo Bruneliere and Thomas Ledoux},
keywords = {Fog Computing, Cloud Computing, Internet of Things, Modeling language, Survey},
abstract = {Fog Computing is a new paradigm aiming at decentralizing the Cloud by geographically distributing away computation, storage and network resources as well as related services. In order to design, develop, deploy, maintain and evolve Fog systems, languages are required for properly modeling both their entities (e.g., infrastructures, topologies, resources configurations) and their specific features such as the locality concept, QoS constraints applied on resources (e.g., energy, data privacy, latency) and their dependencies, the dynamicity of considered workloads, the heterogeneity of both applications and devices, etc. This paper provides a detailed overview of the current state-of-the-art in terms of Fog modeling languages. We relied on our long-term experience in Cloud Computing and Cloud Modeling to contribute a feature model describing what we believe to be the most important characteristics of Fog modeling languages. We also performed a systematic scientific literature search and selection process to obtain a list of already existing Fog modeling languages. Then, we evaluated and compared these Fog modeling languages according to the characteristics expressed in our feature model. As a result, we discuss in this paper the main capabilities of these Fog modeling languages and propose a corresponding set of open research challenges in this area. We expect the presented work to be helpful to both current and future researchers or engineers working on/with Fog systems, as well as to anybody genuinely interested in Fog Computing or more generally in distributed systems.}
}
@article{ALBUKHITAN2018589,
title = {Semantic Annotation of Arabic Web Documents using Deep Learning},
journal = {Procedia Computer Science},
volume = {130},
pages = {589-596},
year = {2018},
note = {The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.108},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918304708},
author = {Saeed Albukhitan and Ahmed Alnazer and Tarek Helmy},
keywords = {Deep Learning, Semantic Annotation, Arabic Language, Ontology},
abstract = {The vision of Semantic Web is to have a Web of things instead of Web of documents in a form that can be processed by machines. This vision could be achieved on the existing Web using semantic annotation based on common and public ontologies. Due to exponential growth and the huge size of the Web sources, there is a need to have fast and automatic semantic annotation of Web documents. Arabic language received less attention in semantic Web research as compared to Latin languages especially in the field of semantic annotation. The aim of this paper is to investigate the feasibility of using word embeddings from deep learning algorithms for semantic annotation of Arabic Web documents. To evaluate the performance of the proposed framework, food, nutrition, and health ontologies were used to annotate some related Web documents. For a given set of Arabic documents and ontologies, the framework produces annotations of these documents using different output formats. The initial results show a promising performance which will support the research in the Semantic Web with respect to Arabic language. The proposed framework could be used for building semantic Web application and semantic search engines for Arabic Language.}
}
@article{MUSIARI2024111776,
title = {Towards computer-aided hygienic design: Definition of a knowledge-based system for food processing equipment},
journal = {Journal of Food Engineering},
volume = {363},
pages = {111776},
year = {2024},
issn = {0260-8774},
doi = {https://doi.org/10.1016/j.jfoodeng.2023.111776},
url = {https://www.sciencedirect.com/science/article/pii/S0260877423003746},
author = {Francesco Musiari and Fabrizio Moroni and Alessandro Pirondi and Claudio Favi},
keywords = {Hygienic design, Knowledge-based system, Design for manufacturing, Food equipment, Food industry, Engineering design},
abstract = {Hygienic design requires the definition of rules allowing the correct development of food processing systems. The knowledge collection in this field would certainly help designers and engineers in developing hygienic-compliant systems. This paper aims to provide a knowledge-based (KB) system for gathering hygienic design guidelines for the design of food processing machinery and equipment. The KB system is based on a specific ontology that has been used to collect 78 hygienic design rules from different sources. The rules repository can be considered a backbone for the subsequent development of a CAD-based tool for an automatic search and detection of non-compliant design features. Starting with a CAD model, the KB system was used to check the compliance of a fish stick production machinery. Results highlight how the adoption of the KB system in the early design phase would anticipate hygienic design issues avoiding several design reviews.}
}
@article{BOY2023102298,
title = {An epistemological approach to human systems integration},
journal = {Technology in Society},
volume = {74},
pages = {102298},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102298},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23001033},
author = {Guy André Boy},
keywords = {Human systems integration, Epistemology, Complexity, Procedural and declarative knowledge, Ontology, Sociotechnical maturity},
abstract = {Can we contribute to developing a consistent terminology and, to some extent, an acceptable ontology in the rapidly expanding field of human systems integration (HSI)? We often define HSI as a process and a product at the confluence of several areas, such as systems engineering, human factors and ergonomics, information technology, and specific sectors, such as aerospace, health, and energy. It is a broader transdisciplinary field in our increasingly complex human-machine world that focuses on integrating technology, organizations, and people within a complex sociotechnical system throughout its life cycle. Therefore, HSI is no longer a question of usability and user interface design once a complex machine is technologically developed, but also about considering people and organizations early on in the design and development processes. Indeed, rooted in industrial engineering research and operational worlds, HSI requires a deeper foundation based on an epistemological approach. This assertion is even more crucial today as technology has become predominantly digital, and, more specifically, the concept of the digital twin is emphasized because it has become essential to support model-based HSI. In other words, software-based assistant systems are replacing traditional tools. Therefore, appropriate social-cognitive (multi-agent) models and methods are helpful throughout the life cycle of contemporary sociotechnical designs to account for the complexity and tangibility of their human-centered context-sensitive architectures, combining procedural and declarative knowledge. By considering these reasons, this article provides a set of fundamental axioms, some theoretical abstractions, and valuable practical models, which are presented and illustrated through the lens of an evolutionary HSI ontology.}
}
@article{GAHRNANDERSEN2025121,
title = {Enlanguaged affordances in social practices: A critical rethinking of Gibson's approach to language},
journal = {Language & Communication},
volume = {104},
pages = {121-130},
year = {2025},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2025.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0271530925000692},
author = {Rasmus Gahrn-Andersen},
keywords = {Affordances, Gibson, Radical embodied cognition, Non-representationalism, Practices},
abstract = {Jones and Read argue that Gibson's concept of affordances can significantly contribute to radical, non-representationalist approaches in linguistics. In line with their perspective, this paper asserts that a crucial initial step must be taken: a critical examination of Gibson's account of the relationship between affordances and language. Specifically, I argue that we should question the assumption that affordance-relative awareness necessarily precedes linguistic behavior, a hypothesis I term Gibson's ‘awareness first, words second’ hypothesis. The paper develops a positive argument around the concept of ‘enlanguaged affordance’, demonstrating how insights from practice theory, radical cognitive science, and distributed approaches to language can converge around this central idea. The paper aims to illuminate how linguistic skills and competencies contribute to shaping social practices.}
}
@article{COSTELLO2023,
title = {Leveraging Knowledge Graphs and Natural Language Processing for Automated Web Resource Labeling and Knowledge Mobilization in Neurodevelopmental Disorders: Development and Usability Study},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/45268},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123002388},
author = {Jeremy Costello and Manpreet Kaur and Marek Z Reformat and Francois V Bolduc},
keywords = {knowledge graph, natural language processing, neurodevelopmental disorders, autism spectrum disorder, intellectual disability, attention deficit hyperactivity disorder, named entity recognition, topic modeling, aggregation operator},
abstract = {Background
Patients and families need to be provided with trusted information more than ever with the abundance of online information. Several organizations aim to build databases that can be searched based on the needs of target groups. One such group is individuals with neurodevelopmental disorders (NDDs) and their families. NDDs affect up to 18% of the population and have major social and economic impacts. The current limitations in communicating information for individuals with NDDs include the absence of shared terminology and the lack of efficient labeling processes for web resources. Because of these limitations, health professionals, support groups, and families are unable to share, combine, and access resources.
Objective
We aimed to develop a natural language–based pipeline to label resources by leveraging standard and free-text vocabularies obtained through text analysis, and then represent those resources as a weighted knowledge graph.
Methods
Using a combination of experts and service/organization databases, we created a data set of web resources for NDDs. Text from these websites was scraped and collected into a corpus of textual data on NDDs. This corpus was used to construct a knowledge graph suitable for use by both experts and nonexperts. Named entity recognition, topic modeling, document classification, and location detection were used to extract knowledge from the corpus.
Results
We developed a resource annotation pipeline using diverse natural language processing algorithms to annotate web resources and stored them in a structured knowledge graph. The graph contained 78,181 annotations obtained from the combination of standard terminologies and a free-text vocabulary obtained using topic modeling. An application of the constructed knowledge graph is a resource search interface using the ordered weighted averaging operator to rank resources based on a user query.
Conclusions
We developed an automated labeling pipeline for web resources on NDDs. This work showcases how artificial intelligence–based methods, such as natural language processing and knowledge graphs for information representation, can enhance knowledge extraction and mobilization, and could be used in other fields of medicine.}
}
@article{GIRALDO201848,
title = {Evaluating the quality of a set of modelling languages used in combination: A method and a tool},
journal = {Information Systems},
volume = {77},
pages = {48-70},
year = {2018},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306437916302071},
author = {Fáber D. Giraldo and Sergio España and William J. Giraldo and Óscar Pastor},
keywords = {Quality, Model-driven engineering, Information systems, The MMQEF method, Reference taxonomy, Model analytics},
abstract = {Modelling languages have proved to be an effective tool to specify and analyse various perspectives of enterprises and information systems. In addition to modelling language designs, works on model quality and modelling language quality evaluation have contributed to the maturity of the model-driven engineering (MDE) field. Although consolidated knowledge on quality evaluation is still relevant to this scenario, in previous works, we have identified misalignments between the topics that academia is addressing and the needs of industry in applying MDE, thus identifying some remaining challenges. In this paper, we focus on the need for a method to evaluate the quality of a set of modelling languages used in combination within a MDE environment. This paper presents MMQEF (Multiple Modelling language Quality Evaluation Framework), describing its foundations, presenting its method components and discussing its trade-offs.}
}
@article{DING2025126232,
title = {Tagging knowledge concepts for math problems based on multi-label text classification},
journal = {Expert Systems with Applications},
volume = {267},
pages = {126232},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126232},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424030999},
author = {Ziqi Ding and Xiaolu Wang and Yuzhuo Wu and Guitao Cao and Liangyu Chen},
keywords = {Hierarchical multi-label classification, Deep learning, Attention mechanism, K12 math problems},
abstract = {Tagging knowledge concepts for course problems is essential for intelligent tutoring systems. Traditional manual tagging methods, usually performed by domain experts, are time-consuming and subject to individual biases. Consequently, research on automatic tagging technology is of substantial practical importance. Recently, text classification techniques have been applied to this task; however, these methods are inadequate for math problems due to their complexity, which includes formulaic content and hierarchical relationships among knowledge concepts. Although large language models (LLMs) have also been explored for this purpose, their generative nature and high computational cost pose challenges for direct application in tutoring systems. In this paper, we propose an automatic knowledge concept tagging model LHABS based on RoBERTa. This model integrates hierarchical label-semantic attention, which captures hierarchical knowledge concepts information, and multi-label smoothing, which combines textual features to help reduce overfitting, thus enhancing text classification performance. Our experimental evaluation on four datasets demonstrates that our model outperforms state-of-the-art methods. We also validate the effectiveness of hierarchical label-semantic attention and multi-label smoothing through our experiments. The code and data are available at: https://github.com/xuqiang124/atmk_system.}
}
@article{XIANG2022105397,
title = {Flood Markup Language – A standards-based exchange language for flood risk communication},
journal = {Environmental Modelling & Software},
volume = {152},
pages = {105397},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105397},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222001037},
author = {Zhongrun Xiang and Ibrahim Demir},
keywords = {Flood, Markup language, Data exchange, Alert communication, Risk communication},
abstract = {Floods are one of the most frequently occurring natural disasters. There are numerous studies devoted to comprehending and forecasting flooding in order to aid in preparedness and response. It is critical to share and communicate datasets generated by various systems and organizations for flood forecasting and modeling. The majority of organizations share limited metadata and details for flood risk data to support research and operational activities. However, there is no standardized way for various stakeholders and automated systems to exchange flood forecast and alert data. This article proposes the Flood Markup Language (FloodML) as a data communication specification for extensively describing and exchanging flood forecasts and alerts with corresponding stakeholders. FloodML is applicable to a broad range of data sharing use cases and requirements for emergency management, the research and modeling communities, and the general public.}
}
@article{ROHRS2025110939,
title = {Identifying food safety risks with a novel digitally-driven food safety early warning tool – A retrospective study on the pesticide ethylene oxide},
journal = {Food Control},
volume = {168},
pages = {110939},
year = {2025},
issn = {0956-7135},
doi = {https://doi.org/10.1016/j.foodcont.2024.110939},
url = {https://www.sciencedirect.com/science/article/pii/S095671352400656X},
author = {Sina Röhrs and Kornél Nagy and Martin Kreutzer and Richard Stadler and Sascha Rohn and Yvonne Pfeifer},
keywords = {Food risk evaluation, Digital food safety, Early warning, Ethylene oxide, Sesame, Black pepper},
abstract = {Foodborne diseases present a major global health challenge, with 600 million annual cases and 420,000 deaths worldwide in 2023 reported by the World Health Organization, underscoring the critical need for early risk detection and swift measures. Novel solutions for early risk detection in food are emerging due to artificial intelligence-based data models. By applying diverse algorithms and ontologies in data processing, the continually expanding amount of available data can be harnessed in a precise manner. To assess the efficacy of such technologies in early risk detection, we evaluated whether the risk to exceed a legal limit for ethylene oxide – specifically in sesame seeds – could have been identified sooner with the assistance of a commercially available Artificial Intelligence (AI)-based platform. The non-compliance of sesame seeds led to various product recalls in the year 2020. The result of this retrospective case study shows that the first indirect indications of the issue started to emerge from the year 2018 with initial Rapid Alert System for Food and Feed (RASFF) notification of ethylene oxide limit value exceedances in black pepper powder. Based on these promising findings, the subsequent challenge is to develop a methodology for systematically categorizing and evaluating similar risks in the light of the exponentially growing volume of accessible data.}
}
@article{LI2024100266,
title = {A systematic review of the first year of publications on ChatGPT and language education: Examining research on ChatGPT’s use in language learning and teaching},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100266},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100266},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000699},
author = {Belle Li and Victoria L. Lowell and Chaoran Wang and Xiangning Li},
keywords = {Systematic review, Artificial intelligence (AI), ChatGPT, Language learning},
abstract = {This systematic review aims to explore published research on the use of ChatGPT in language learning between November 2022 and November 2023, outlining the types of papers, methodologies adopted, publishing journals, major research trends, topics of interest, and existing gaps demanding attention. The PRISMA framework was utilized to capture the latest published articles, selecting 36 articles that met the inclusion criteria. Findings extracted from this review include (1) authors worldwide contribute to this topic, with Asia and North America leading; (2) the wide distribution across various journals underscores the interdisciplinary nature of this research topic, such as computer science, psychology, linguistics, education, and other social sciences; (3) empirical research dominates the literature that is published, with the majority focusing on higher education and ethical considerations. Other findings include that ChatGPT plays multifaceted roles, supporting self-directed language learning, content generation, and teacher workflows. Research gaps include the need for diversified scopes, longitudinal studies, exploration of stakeholders’ perceptions, and assessments of feedback quality.}
}
@article{HUSKOVA2025115111,
title = {Improvement of data and metadata quality in catalysis research: A use case-driven methodology},
journal = {Catalysis Today},
volume = {446},
pages = {115111},
year = {2025},
issn = {0920-5861},
doi = {https://doi.org/10.1016/j.cattod.2024.115111},
url = {https://www.sciencedirect.com/science/article/pii/S0920586124006059},
author = {Nadiia Huskova and Yuliia Dikova and Taras Petrenko and Thomas Bönisch},
keywords = {NFDI4Cat, Research data infrastructure, Catalysis, Use cases, Standardization, Metadata, Semantic representation},
abstract = {The goal of the NFDI4Cat project is to establish a National Research Data Infrastructure for catalysis research in Germany that ensures the data and metadata collected and shared by researchers are of high quality and adhere to established standards. To achieve this goal, a comprehensive use case (UC) collection methodology has been developed. The methodology is based on the collection and analysis of use cases for research workflows and data therein provided by the researchers working in the field of catalysis. The proposed methodology includes detailed guidelines for the information collected within a particular UC, ensuring that it is relevant, accurate and complete. The collected UC data are then evaluated based on established criteria for data and metadata quality. Any identified issues are addressed through the collaboration with the respective researchers, to ensure that the use cases meet the required standards. The collected use cases are then standardized, which in particular includes mapping the data and metadata to relevant ontologies and vocabularies, as well as ensuring consistency across different use cases. The standardization process is coupled with a semantic representation of metadata within the Resource Description Framework (RDF) followed by appropriate extension of the ontology being developed. The semantic framework allows for easy integration and cross-referencing of data. It ensures that the data are machine-readable, linked, and can be easily integrated with other datasets, making it more discoverable and useful for the catalysis research community. Within the project, special attention is paid on the collection of UCs from different fields, including biocatalysis, homogeneous catalysis, and heterogeneous catalysis. This will provide a comprehensive representation of the metadata related to catalysis. The proposed methodology serves as a valuable resource for the catalysis research community, promoting adherence to established standards and ensuring that the data and metadata shared by researchers are of high quality.}
}
@article{XIANG2023110015,
title = {Zero-shot language extension for dialogue state tracking via pre-trained models and multi-auxiliary-tasks fine-tuning},
journal = {Knowledge-Based Systems},
volume = {259},
pages = {110015},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110015},
url = {https://www.sciencedirect.com/science/article/pii/S095070512201108X},
author = {Lu Xiang and Yang Zhao and Junnan Zhu and Yu Zhou and Chengqing Zong},
keywords = {Dialogue state tracking, Zero-shot language extension, Multilingual DST, Pre-trained models, Multi-auxiliary-tasks fine-tuning},
abstract = {Dialogue state tracking (DST), a crucial component of the task-oriented dialogue system (TOD), is designed to track the user’s goal. Existing DST models mainly focus on monolingual dialogue input, failing to meet the growing needs of a TOD to provide multilingual services. Therefore, this paper proposes a novel Zero-shot Language Extension scenario for DST, extending the monolingual DST to multilingual DST without extra high-cost dialogue data annotation. In this scenario, the multilingual DST only needs a single shared model to handle multilingual input and generate a unified dialogue state. This setting makes deploying a complete multilingual TOD easy since it could be reused by the downstream components from existing monolingual TOD. Specifically, we achieve the language extension by multi-auxiliary-tasks fine-tuning of multilingual pre-trained models, where five relevant auxiliary tasks are jointly designed, including monolingual DST, cross-lingual DST, forward word translation, utterance recovery, and semantic similarity. The extended multilingual DST model can be enhanced through joint optimization with all the auxiliary tasks by capturing multilingual context understanding and cross-lingual alignment characteristics. Comprehensive experiments on Multilingual WOZ dataset (English → German and English → Italian) and cross-lingual MultiWOZ dataset (English → Chinese and Chinese → English) demonstrate the effectiveness and superiority of the proposed method.}
}
@article{HUSSAIN2023100863,
title = {SCMD suspicious cryptographic message detection},
journal = {Measurement: Sensors},
volume = {29},
pages = {100863},
year = {2023},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2023.100863},
url = {https://www.sciencedirect.com/science/article/pii/S266591742300199X},
author = {Syed Hussain and Pakkir {Mohideen S}},
keywords = {Cryptography, Cryptanalysis, Encryption, Decryption, Machine learning, Statistical natural language processing (SNLP), Social networking with instant messengers, Association rule mining (ARM), Suspicious message detection system, SMDs},
abstract = {The proposed framework aims to prevent global crimes, specifically terrorist attacks, bomb blasts, and drone attacks, by using the Suspicious Cryptographic Message Detection (SCMD) system to detect and decrypt suspicious messages. Terrorists often use encrypted messages to communicate with their teammates in different parts of the world, making it difficult for authorities to monitor their activities. The framework addresses this issue by detecting with gradient boosting algorithm and cryptanalyzing by using Simple Substitution Cipher techniques to decrypt suspicious messages and predict potential criminal activity. The framework utilizes various emerging trends in information technology, including machine learning techniques, pre-defined decision rules, wordnet, and semantic web ontology, to predict the type of crime, the criminal's name and location, and other criminal details. It comprises multiple components, including Semantic web Ontology, Suspicious Database, and machine learning techniques, to facilitate the detection and reporting of criminal activity. When the server receives a suspicious encrypted message, the framework detect and decrypts it and predicts the type of crime based on microblogs before it can be executed by the criminals. The details of the criminals are then alerted to the cybercrime department, reducing the burden on security departments and enhancing public safety. In summary, the framework provides a comprehensive solution to the challenges posed by encrypted messages in criminal activities. It uses advanced technologies to identify and predict potential criminal activities, enabling law enforcement agencies to take preemptive actions to prevent major and minor attacks, including terrorist attacks.}
}
@article{SCHILLER2023366,
title = {Blisk Specific Query Language (BLISQL) – An approach for domain specific data querying in Blisk Manufacturing},
journal = {Procedia CIRP},
volume = {118},
pages = {366-371},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.063},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123002871},
author = {Sven Schiller and Markus Landwehr and Georg Vinogradov and Philipp Ganser and Thomas Bergs},
keywords = {Computer Aided Manufacturing (CAM), Internet of Things (Iot), Onotology, Semantic Web Technologies, Product Lifecycle Management (PLM)},
abstract = {Product lifecycle management (PLM) is constantly improved by a steadily growing amount of data collected along the product and process development chain. This data supports designing optimized geometries and manufacturing processes. For the optimization of the manufacturing process, data from process design and manufacturing are extracted and processed. The use of a query language is helpful to make the extraction more efficient. Query languages, referring to the specific domain of a component, simplify the formulation of the queries. We present an approach for domain specific data querying in blisk manufacturing based on the Resource Description Framework (RDF) using SPARQL.}
}
@article{SIVASHANMUGAM2024105842,
title = {BIM-integrated semantic framework for construction waste quantification and optimisation},
journal = {Automation in Construction},
volume = {168},
pages = {105842},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105842},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524005788},
author = {Subarna Sivashanmugam and Sergio {Rodriguez Trejo} and Farzad Rahimian},
keywords = {Construction and demolition waste, Quantification and classification, Information and communication technologies, Conceptual framework, Semantic Web Technologies, BIM},
abstract = {Quantification and optimisation of Construction Waste (CW) in the design stages are vital to implementing preventive CW management measures. Previous ICT-integrated CW models are not efficiently upscaled to achieve an interoperable and automated workflow. Therefore, this paper presents a BIM-integrated semantic framework for CW quantification and optimisation from the early design stages. A CW data model using Semantic-Web-Technologies (SWT) was developed and integrated with BIM. The results proved that unified data structure, standardised and granular information, established semantic relationships between building material and CW data, and diverse measurement units proposed in the framework facilitate seamless and dynamic information flows between BIM and CW platforms. The research outcomes are critical to improving interoperability and automation across the CW assessment process, enhancing the accuracy and reliability of results, supporting timely and integrated decision-making, and easing communication and collaboration among the supply-chain members. A test-case building demonstrates the application of the framework.}
}
@article{MASSAI2024111183,
title = {Evaluation of semantic relations impact in query expansion-based retrieval systems},
journal = {Knowledge-Based Systems},
volume = {283},
pages = {111183},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.111183},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123009334},
author = {Lorenzo Massai},
keywords = {Natural language processing, Query reformulation, Classification systems, Query classification, Semantic expansion, Pseudo-relevance feedback, Semantic search engines, Intent estimation, Ontology-based retrieval systems},
abstract = {With the increasing demand of intelligent systems capable of operating in different contexts (e.g. users on the move) the correct interpretation of the user-need by such systems has become crucial to give consistent answers to the user questions. The most effective applications addressing such task are in the fields of natural language processing and semantic expansion of terms. These techniques are aimed at estimating the goal of an input query reformulating it as an intent, commonly relying on textual resources built exploiting different semantic relations like synonymy, antonymy and many others. The aim of this paper is to generate such resources using the labels of a given taxonomy as source of information. The obtained resources are integrated into a plain classifier for reformulating a set of input queries as intents and tracking the effect of each relation, in order to quantify the impact of each semantic relation on the classification. As an extension to this, the best tradeoff between improvement and noise introduction when combining such relations is evaluated. The assessment is made generating the resources and their combinations and using them for tuning the classifier which is used to reformulate the user questions as labels. The evaluation employs a wide and varied taxonomy as a use-case, exploiting its labels as basis for the semantic expansion and producing several corpora with the purpose of enhancing the pseudo-queries estimation.}
}
@article{WANG2025127246,
title = {Aligning sequence and structure representations leveraging protein domains for function prediction},
journal = {Expert Systems with Applications},
volume = {278},
pages = {127246},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127246},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425008681},
author = {Mingqing Wang and Zhiwei Nie and Yonghong He and Athanasios V. Vasilakos and Zhixiang Ren},
keywords = {Protein function prediction, Protein domain, Deep learning, Functional priors, Contrastive learning},
abstract = {Protein function prediction is traditionally approached through sequence or structural modeling, often neglecting the effective fusion of diverse data sources. Protein domains, as functionally independent building blocks, determine a protein’s biological function, yet their potential has not been fully exploited in function prediction tasks. To address this, we introduce a modality-fused neural network leveraging function-aware domain embeddings as a bridge. We pre-train these embeddings by aligning domain semantics with Gene Ontology (GO) terms and textual descriptions. Additionally, we partition proteins into sub-views based on continuous domain regions for contrastive learning, supervised by a novel triplet InfoNCE loss. Our method outperforms state-of-the-art approaches across various benchmarks, and clearly differentiates proteins carrying distinct functions compared to the competitor.}
}
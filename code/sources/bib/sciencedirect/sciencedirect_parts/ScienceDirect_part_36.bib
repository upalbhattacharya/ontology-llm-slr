@article{STRUPCZEWSKI2021105143,
title = {Defining cyber risk},
journal = {Safety Science},
volume = {135},
pages = {105143},
year = {2021},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.105143},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520305397},
author = {Grzegorz Strupczewski},
keywords = {Cyber risk, Definition, Meta model of cyber risk, Cybersecurity, Cyber threats},
abstract = {Rapid digitization of the economy and social relations is the main reason why the issues of cyber risk, cyber threats and cybersecurity are continually gaining importance. Despite the increase in the number of research papers in these areas, scholarly articles defining cyber risk are relatively scarce. Moreover, the uniform broadly accepted definition of cyber risk has not been adopted yet, probably due to the interdisciplinary nature of this concept and the dynamics of its change. The paper contributes to the literature on the cyber risk, cybersecurity and cyber risk management. The author presents a comparative content analysis of existing definitions of cyber risk. Based on identification of three key characteristics of the cyber risk concept (source of cyber risk, cyber risk object, impact of cyber risk) in each definition, the analysed definitions are categorised as one-dimensional, two-dimensional or comprehensive definition. Among the collected 20 definitions of cyber risk, there is only one that can be called comprehensive. The remaining definitions address only selected aspects of this notion. The author proposes a new, comprehensive and universal definition of cyber risk. As an extension to the proposed approach, the ontological meta model of the cyber risk concept is developed. It supports deeper description of the cyber risk concept by depicting functional interdependencies with other terms and factors that constitute the cyber risk framework.}
}
@article{ALOMARI2022103637,
title = {Online perceptual learning and natural language acquisition for autonomous robots},
journal = {Artificial Intelligence},
volume = {303},
pages = {103637},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103637},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221001880},
author = {Muhannad Alomari and Fangjun Li and David C. Hogg and Anthony G. Cohn},
keywords = {Language and vision, Language acquisition, Language grounding, Grammar induction},
abstract = {In this work, the problem of bootstrapping knowledge in language and vision for autonomous robots is addressed through novel techniques in grammar induction and word grounding to the perceptual world. In particular, we demonstrate a system, called OLAV, which is able, for the first time, to (1) learn to form discrete concepts from sensory data; (2) ground language (n-grams) to these concepts; (3) induce a grammar for the language being used to describe the perceptual world; and moreover to do all this incrementally, without storing all previous data. The learning is achieved in a loosely-supervised manner from raw linguistic and visual data. Moreover, the learnt model is transparent, rather than a black-box model and is thus open to human inspection. The visual data is collected using three different robotic platforms deployed in real-world and simulated environments and equipped with different sensing modalities, while the linguistic data is collected using online crowdsourcing tools and volunteers. The analysis performed on these robots demonstrates the effectiveness of the framework in learning visual concepts, language groundings and grammatical structure in these three online settings.}
}
@article{LIU202123,
title = {Rethinking the geography of gentrification: From a scale perspective},
journal = {Geoforum},
volume = {118},
pages = {23-29},
year = {2021},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2020.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0016718520302529},
author = {Cheng Liu and Yu Deng and Weixuan Song and Jian Gong and Jie Zeng},
keywords = {Comparative urbanism, Gentrification, Scale, Re-scaling, Resistance, COVID-19},
abstract = {This article proposes that the lens of scale may potentially yield fresh insights into the comparative research on gentrification. First, the flows of capital, people, and knowledge in re-scaling furnish a point of departure for a holistic theory of planetary gentrification and a vital reference point for comparison. Second, to harness upscaling and downscaling in tandem, comparative studies should adequately connect gentrification with similar regional/local ontologies to obtain greater ontological power. The disadvantaged should take advantage of a loose and shifting alliance among the pro-gentrification forces to find a “real choice” (instead of a “perfect choice”) for those affected. Moreover, a pragmatic solution is opprobrium against predatory flows, particularly the flows of capital, whose nature is defined by local peculiarities.}
}
@article{ANDOR202389,
journal = {Journal of Pragmatics},
volume = {214},
pages = {89-91},
year = {2023},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2023.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S0378216623001741},
author = {József Andor}
}
@article{VISHWAKARMA2025102468,
title = {Automatic query expansion for enhancing document retrieval system in healthcare application using GAN based embedding and hyper-tuned DAEBERT algorithm},
journal = {Data & Knowledge Engineering},
volume = {160},
pages = {102468},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102468},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000631},
author = {Deepak Vishwakarma and Suresh Kumar},
keywords = {Automatic query expansion, Information retrieval, Generative Adversial Network, Modified Page Ranking Algorithm, Proximity based Keyword Extraction},
abstract = {Query expansion is a useful technique for improving document retrieval systems' dependability and performance. Search engines frequently employ query expansion strategies to improve Information Retrieval (IR) performance and elucidate users' information requirements. Although there are several methods for automatically expanding queries, the list of documents that are returned can occasionally be lengthy and contain a lot of useless information, particularly when searching the Web. As the size of medical document grows, Automatic Query Expansion might struggle with efficiency and real-time application. Thus, Hyper-Tuned Dual Attention Enhanced Bi-directional Encoder Representation from Transformers (HT-DAEBERT) with automatic ranking based query expansion system is created for enhancing medical document retrieval system. Initially, the user's query from the medical corpus document was collected, and it was augmented using the Generative Adversarial Network (GAN) approach. Then augmented text is pre-processed to improve the original text's quality through tokenization, acronym expansion, stemming, stop word removal, hyperlink removal, and spell correction. After that, Keywords are extracted using the Proximity-based Keyword Extraction (PKE) technique from the pre-processed text. Afterwards, the words are converted into vector form by utilizing the Hyper-Tuned Dual Attention Enhanced Bi-directional Encoder Representation from Transformers (HT-DAEBERT) model. In DAEBERT, key parameters such as dropout rate and weight decay were optimally selected by using the Election Optimization Algorithm (EOA). After that, a ranking-based query expansion approach was employed to enhance the document retrieval system. The proposed method achieves an accuracy of 97.60 %, a Hit Rate of 98.30 %, a PPV of 93.40 %, an F1-Score of 95.79 %, and an NPV of 97.50 %. This approach improves the accuracy and relevance of document retrieval in healthcare, potentially leading to better patient care and enhanced clinical outcomes.}
}
@article{MAILLOT2023100775,
title = {IndeGx: A model and a framework for indexing RDF knowledge graphs with SPARQL-based test suits},
journal = {Journal of Web Semantics},
volume = {76},
pages = {100775},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100775},
url = {https://www.sciencedirect.com/science/article/pii/S1570826823000045},
author = {Pierre Maillot and Olivier Corby and Catherine Faron and Fabien Gandon and Franck Michel},
keywords = {Semantic index, Metadata extraction, Dataset description, Endpoint description, Knowledge graph},
abstract = {In recent years, a large number of RDF datasets have been built and published on the Web in fields as diverse as linguistics or life sciences, as well as general datasets such as DBpedia or Wikidata. The joint exploitation of these datasets requires specific knowledge about their content, access points, and commonalities. However, not all datasets contain a self-description, and not all access points can handle the complex queries used to generate such a description. In this article, we provide a standard-based approach to generate the description of a dataset. The generated descriptions as well as the process of their computation are expressed using standard vocabularies and languages. We implemented our approach into a framework, called IndeGx, where each indexing feature and its computation is collaboratively and declaratively defined in a GitHub repository. We have experimented IndeGx on a set of 339 RDF datasets with endpoints listed in public catalogs, over 8 months. The results show that we can collect, as much as possible, important characteristics of the datasets depending on their availability and capacities. The resulting index captures the commonalities, variety and disparity in the offered content and services and it provides an important support to any application designed to query RDF datasets.}
}
@article{WATROBSKI20191591,
title = {Towards Knowledge Handling in Sustainable Management Domain},
journal = {Procedia Computer Science},
volume = {159},
pages = {1591-1601},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.330},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919315315},
author = {Jarosław Wątróbski},
keywords = {sustainable management, knowledge management, ontology, knowledge engineering, sustainable decision problem},
abstract = {Recent years the term of sustainability has received increased meaning in successful management. Consequently, a lot of attention is paid for developing effective means to support this process. The concept of sustainable management is an important focal point for decision-makers in business. To advance sustainable management activities, obviously an adequate and modern approach is needed. Actual research trends confirm that knowledge management gives a way to handle domain knowledge practically and effectively. Thus, the practical application of knowledge-based mechanisms may provide a relevant instrument. This paper addresses the specific kind of problem of domain knowledge handling in sustainable management. In scientific terms, the paper presents an attempt of knowledge handling in the form of domain ontology for sustainable management. Presented case studies show the practical capabilities of proposed approach.}
}
@article{JIANG2024100700,
title = {Enterprise architecture modeling for cybersecurity analysis in critical infrastructures — A systematic literature review},
journal = {International Journal of Critical Infrastructure Protection},
volume = {46},
pages = {100700},
year = {2024},
issn = {1874-5482},
doi = {https://doi.org/10.1016/j.ijcip.2024.100700},
url = {https://www.sciencedirect.com/science/article/pii/S1874548224000416},
author = {Yuning Jiang and Manfred A. Jeusfeld and Michael Mosaad and Nay Oo},
keywords = {Enterprise architecture, Enterprise model, Cybersecurity, Critical infrastructure},
abstract = {As digital landscapes become increasingly complex, safeguarding sensitive information and systems against cyber threats has become a paramount concern for organizations. This paper provides a comprehensive review of how enterprise architecture modeling is used in the context of cybersecurity assessment, particularly focusing on critical infrastructures. The use of enterprise architecture models for cybersecurity is motivated by the main purpose of enterprise architecture, namely to represent and manage business and IT assets and their interdependence. While enterprise architecture modeling originally served to assess Business/IT alignment, they are increasingly used to assess the cybersecurity of the enterprise. The research questions explored include the types of enterprise architecture models used for cybersecurity assessment, how security aspects are incorporated into these models, the theoretical frameworks and reference theories applied, the research methods used for evaluation, and the strengths and limitations of these models in supporting cybersecurity assessment. This review encompasses research papers published before 2024, focusing on high-quality research from peer-reviewed journals and reputable conferences, thereby providing a structured and comprehensive overview of the current state of research in this domain.}
}
@article{ZHANG2024104860,
title = {Cultural distortion risks at heritage sites: Scale development and validation},
journal = {Tourism Management},
volume = {102},
pages = {104860},
year = {2024},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2023.104860},
url = {https://www.sciencedirect.com/science/article/pii/S0261517723001425},
author = {Shu-Ning Zhang and Wen-Qi Ruan and Yong-Quan Li and Honggen Xiao},
keywords = {Cultural distortion risk, Heritage site, Cultural sustainability, Crisis, Scale development},
abstract = {Cultural risk measurement is an urgent theoretical and practical issue related to cultural sustainability of heritage sites. This research conducted rigorous scale development of cultural distortion risk at heritage sites (CDRHS) with four stages. The results further emphasize the risk prediction direction of cultural crisis. Validated with three dimensions and 12 items, the CDRHS scale has high-level and convergent, construct, and discriminant validity. The scale is confirmed with three constructs: cultural ontology distortion risks, cultural representation distortion risks, and cultural constructive distortion risks. Moreover, the ‘risk-trust-intention’ negative influence framework indicates that CDRHS has a significant negative effect on tourists' behavioral intention. This research highlights the structural dimension of the CDRHS scale, presenting a new measurement tool and methodological contribution to cultural sustainability of heritage sites. It provides indicators and diagnostic tools for timely prediction and supervision of cultural crises.}
}
@article{DONG2020203,
title = {Knowledge base enrichment by relation learning from social tagging data},
journal = {Information Sciences},
volume = {526},
pages = {203-220},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520303017},
author = {Hang Dong and Wei Wang and Frans Coenen and Kaizhu Huang},
keywords = {Knowledge discovery, Knowledge base enrichment, Ontology learning, Social tagging, Probabilistic association analysis, Classification},
abstract = {There has been considerable interest in transforming unstructured social tagging data into structured knowledge for semantic-based retrieval and recommendation. Research in this line mostly exploits data co-occurrence and often overlooks the complex and ambiguous meanings of tags. Furthermore, there have been few comprehensive evaluation studies regarding the quality of the discovered knowledge. We propose a supervised learning method to discover subsumption relations from tags. The key to this method is quantifying the probabilistic association among tags to better characterise their relations. We further develop an algorithm to organise tags into hierarchies based on the learned relations. Experiments were conducted using a large, publicly available dataset, Bibsonomy, and three popular, human-engineered or data-driven knowledge bases: DBpedia, Microsoft Concept Graph, and ACM Computing Classification System. We performed a comprehensive evaluation using different strategies: relation-level, ontology-level, and knowledge base enrichment based evaluation. The results clearly show that the proposed method can extract knowledge of better quality than the existing methods against the gold standard knowledge bases. The proposed approach can also enrich knowledge bases with new subsumption relations, having the potential to significantly reduce time and human effort for knowledge base maintenance and ontology evolution.}
}
@article{TREISTMAN2022118157,
title = {Word embedding dimensionality reduction using dynamic variance thresholding (DyVaT)},
journal = {Expert Systems with Applications},
volume = {208},
pages = {118157},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118157},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422013318},
author = {Avraham Treistman and Dror Mughaz and Ariel Stulman and Amit Dvir},
keywords = {Dimensionality reduction, Machine learning, Natural language processing, Learning styles, Feature selection},
abstract = {Natural language processing (NLP) provides a framework for large-scale text analysis. One common processing method uses vector space models (VSMs) which embed word attributes, called features, into highly dimensional vectors. Comprehensive VSMs are generated on sources such as the GoogleNews archive. A thesaurus, a collection of semantically-related words, can be created for a particular root word using cosine similarity with a given VSM. Many methods have been developed to reduce the complexity of these models by maintaining useful semantic information while discarding non-informative features. One such method, variance thresholding, retains high-variance features above a manually-determined threshold, providing higher differentiation between words for classification purposes. Our research developed a dimension-reducing methodology called dynamic variance thresholding (DyVaT). DyVaT reduces the specificity of word embeddings by maintaining low-variance features, allowing for a broader thesaurus preserving semantic similarity. A dynamic variance threshold, determining which low-variance features are retained, is selected using the kneedle algorithm, improving the current results. Our test case for examining the efficiency of DyVat in creating a contextual thesaurus is the visual, auditory and kinesthetic learning style context. We conclude that DyVaT is a valid method for generating loosely-connected word collections with potential uses in NLP classification or clustering tasks.}
}
@article{KE2020123269,
title = {An intelligent design for remanufacturing method based on vector space model and case-based reasoning},
journal = {Journal of Cleaner Production},
volume = {277},
pages = {123269},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123269},
url = {https://www.sciencedirect.com/science/article/pii/S095965262033314X},
author = {Chao Ke and Zhigang Jiang and Hua Zhang and Yan Wang and Shuo Zhu},
keywords = {Design for remanufacturing, Knowledge reuse, VSM, CBR},
abstract = {Design for Remanufacturing (DfRem) plays an important role in remanufacturing, which promotes the product remanufacturability, and enhance the efficiency of remanufacturing processes. However, due to the large and fuzzy demand data, it is difficult to accurately extract DfRem targets from the customer demand data. Moreover, the process of DfRem scheme generation includes conceptual design, general design and detailed design. The remanufacturability of products needs be considered at the design process, which makes the DfRem scheme solution process very complicated. For the purpose of accurately extracting DfRem targets and shortening design cycle, it is necessary to apply intelligent technology for customer demand analysis and DfRem solution. To address this, an intelligent DfRem method based on vector space model (VSM) and case-based reasoning (CBR) is proposed. Firstly, for accurate extraction of DfRem targets, VSM is employed to extract customer demand data features from the mass customer demand data embedded with remanufacturing information, and K-means technique is applied to classify customer demand data features thus to extract DfRem targets. After extraction of DfRem targets, CBR is utilized to retrieve the case that is most similar to the DfRem targets from DfRem and remanufacturing process knowledge bases. In order to improve the accuracy of the retrieval, ontology is used to construct standard knowledge expression. Finally, this method has been evaluated utilizing the DfRem of clutch remanufacturing as case studies. The results show that the method can accurately generate design scheme to satisfy the customer demands. In this paper, the intelligent DfRem method has been developed by Visual Studio and Microsoft SQL Server, which can quickly generate the most suitable solution.}
}
@article{KONUR20211931,
title = {Toward Full-Stack In Silico Synthetic Biology: Integrating Model Specification, Simulation, Verification, and Biological Compilation},
journal = {ACS Synthetic Biology},
volume = {10},
number = {8},
pages = {1931-1945},
year = {2021},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.1c00143},
url = {https://www.sciencedirect.com/science/article/pii/S2161506321000978},
author = {Savas Konur and Laurentiu Mierla and Harold Fellermann and Christophe Ladroue and Bradley Brown and Anil Wipat and Jamie Twycross and Boyang Peter Dun and Sara Kalvala and Marian Gheorghe and Natalio Krasnogor},
abstract = {We present the Infobiotics Workbench (IBW), a user-friendly, scalable, and integrated computational environment for the computer-aided design of synthetic biological systems. It supports an iterative workflow that begins with specification of the desired synthetic system, followed by simulation and verification of the system in high-performance environments and ending with the eventual compilation of the system specification into suitable genetic constructs. IBW integrates modeling, simulation, verification, and biocompilation features into a single software suite. This integration is achieved through a new domain-specific biological programming language, the Infobiotics Language (IBL), which tightly combines these different aspects of in silico synthetic biology into a full-stack integrated development environment. Unlike existing synthetic biology modeling or specification languages, IBL uniquely blends modeling, verification, and biocompilation statements into a single file. This allows biologists to incorporate design constraints within the specification file rather than using decoupled and independent formalisms for different in silico analyses. This novel approach offers seamless interoperability across different tools as well as compatibility with SBOL and SBML frameworks and removes the burden of doing manual translations for standalone applications. We demonstrate the features, usability, and effectiveness of IBW and IBL using well-established synthetic biological circuits.
}
}
@article{ZHENG2022102242,
title = {Knowledge-based program generation approach for robotic manufacturing systems},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {73},
pages = {102242},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102242},
url = {https://www.sciencedirect.com/science/article/pii/S073658452100123X},
author = {Chen Zheng and Jiajian Xing and Zhanxi Wang and Xiansheng Qin and Benoît Eynard and Jing Li and Jing Bai and Yicha Zhang},
keywords = {Knowledge-based engineering, Robotic manufacturing system, Manufacturing programming, Program generation},
abstract = {In recent decades, robotic manufacturing systems have been considered as effective solutions for providing more productive manufacturing processes, but with less cost and risk. However, the programming for robotic manufacturing systems is a time-consuming task, and hinders the implementation of robotic manufacturing systems in today's industry. This paper proposes a knowledge-based program-generation approach for robotic manufacturing systems. The proposed approach provides effective support for the standardization of the rules and knowledge related to manufacturing programs that have proven successful in previous manufacturing cases; this can not only increase the programming efficiency, but can also improve the manufacturing stability and production quality. First, an ontological knowledge model is developed to provide an explicit semantic description of the relevant concepts for the robotic manufacturing system, basic instruction units for the program, and product models of the workpieces. Second, a rule-based reasoning mechanism is established to infer the implicit relationships between the basic instruction units of the manufacturing program. Finally, based on the semantic descriptions and reasoning mechanism of the proposed knowledge model, the basic instruction units of the manufacturing program are instantiated based on data extracted from the product models and integrated according to the relationships inferred by the reasoning mechanism, thereby generating the robotic manufacturing program.}
}
@article{MORSHED2021100058,
title = {8R Resilience Model: A stakeholder-centered approach of disaster resilience for transportation infrastructure and network},
journal = {Transportation Engineering},
volume = {4},
pages = {100058},
year = {2021},
issn = {2666-691X},
doi = {https://doi.org/10.1016/j.treng.2021.100058},
url = {https://www.sciencedirect.com/science/article/pii/S2666691X21000142},
author = {Syed Ahnaf Morshed and Mahmoud Arafat and Seyedmirsajad Mokhtarimousavi and Sifat Shahriar Khan and Kamar Amine},
keywords = {Disaster resilience, MCDA, Resilience metrics, 8R Resilience Model, Stakeholder-centered Assessment},
abstract = {Infrastructure and network resilience directly or indirectly deal with the impact of risks, criticalities, emergencies, and stakeholders. It is evaluated based on the retrieval of system functionality during global setbacks such as natural disasters, man-made disasters, etc. Despite inherent complexities and posed threats, proper scientific methods to evaluate the transportation infrastructure and network resilience from the users’ point of view is absent. This study developed a unique ‘8R Resilience Model’, which is an information management tool based on resilient concepts for assessing transportation infrastructure and network resilience. The findings of this study present opportunities to influence policymakers to reassess the disaster resilience of the transportation system during a global crisis. The proposed 8R Resilience Model provides a dynamic stakeholder-centered assessment strategy of disaster resilience. A multi-criteria decision analysis (MCDA) process comprising of the 8R Resilience Model, statistical (survey) model, and social media reaction (Twitter) model has been performed for validation. MCDA result suggests that adopting the dynamic 8R Resilience Model combined with operational resilience metrics is more beneficial in terms of assessing the criticality of any transportation infrastructure than using models developed from survey data and social media reactions.}
}
@article{BAKHSHI2022107626,
title = {SParseQA: Sequential word reordering and parsing for answering complex natural language questions over knowledge graphs},
journal = {Knowledge-Based Systems},
volume = {235},
pages = {107626},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107626},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121008881},
author = {Mahdi Bakhshi and Mohammadali Nematbakhsh and Mehran Mohsenzadeh and Amir Masoud Rahmani},
keywords = {RDF complex question answering, Uncertain question graph construction, Sequential word reordering and parsing, Relation pattern-based similarity},
abstract = {One of the effective approaches for answering natural language questions (NLQs) over knowledge graphs consists of two main stages. It first creates a query graph based on the NLQ and then matches this graph over the knowledge graph to construct a structured query. An obstacle in the first stage is the need to build question interpretations with candidate resources, even if some implicit phrases exist in the sentence. In the second stage, a serious problem is to map diverse NLQ relations to their corresponding predicates. To overcome these problems, in this paper, we propose a novel sequential word parsing-based method to construct and refine an uncertain question graph that is disambiguated directly over the knowledge graph. Instead of relying on the syntactic dependency relations and some predefined rules that recognize the relations and their arguments, we consider the identified entities and variables in the NLQ as well as their corresponding place in the structure of a query graph pattern to build question triples. First, by leveraging the ordered dependency tree of an NLQ, sentence words are reordered. Then the question graph structure is constructed by parsing the new sequence backward, starting from the identified items. Subsequently, the question graph is refined by eliminating the useless elements. Additionally, to improve the relation similarity measure in the graph similarity process, we exploit the knowledge hidden in a relation pattern taxonomy. Experimental studies over several benchmarks demonstrate that our proposed approach is effective as it achieves promising results in answering the complex NLQs.}
}
@article{EIVAZZADEH2018,
title = {Most Influential Qualities in Creating Satisfaction Among the Users of Health Information Systems: Study in Seven European Union Countries},
journal = {JMIR Medical Informatics},
volume = {6},
number = {4},
year = {2018},
issn = {2291-9694},
doi = {https://doi.org/10.2196/11252},
url = {https://www.sciencedirect.com/science/article/pii/S2291969418000613},
author = {Shahryar Eivazzadeh and Johan S Berglund and Tobias C Larsson and Markus Fiedler and Peter Anderberg},
keywords = {health information systems, telemedicine, evaluation studies as topic, consumer behavior, treatment outcome, safety, efficiency, health care costs, ontology engineering, equation models},
abstract = {Background
Several models suggest how the qualities of a product or service influence user satisfaction. Models such as the Customer Satisfaction Index (CSI), Technology Acceptance Model (TAM), and Delone and McLean Information Systems Success demonstrate those relations and have been used in the context of health information systems.
Objective
This study aimed to investigate which qualities foster greater satisfaction among patient and professional users. In addition, we are interested in knowing to what extent improvement in those qualities can explain user satisfaction and whether this makes user satisfaction a proxy indicator of those qualities.
Methods
The Unified eValuation using ONtology (UVON) method was used to construct an ontology of the required qualities for 7 electronic health (eHealth) apps being developed in the Future Internet Social and Technological Alignment Research (FI-STAR) project, a European Union (EU) project in electronic health (eHealth). The eHealth apps were deployed across 7 EU countries. The ontology included and unified the required qualities of those systems together with the aspects suggested by the Model for ASsessment of Telemedicine apps (MAST) evaluation framework. Moreover, 2 similar questionnaires for 87 patient users and 31 health professional users were elicited from the ontology. In the questionnaires, the user was asked if the system has improved the specified qualities and if the user was satisfied with the system. The results were analyzed using Kendall correlation coefficients matrices, incorporating the quality and satisfaction aspects. For the next step, 2 partial least squares structural equation modeling (PLS-SEM) path models were developed using the quality and satisfaction measure variables and the latent construct variables that were suggested by the UVON method.
Results
Most of the quality aspects grouped by the UVON method are highly correlated. Strong correlations in each group suggest that the grouped qualities can be measures that reflect a latent quality construct. The PLS-SEM path analysis for the patients reveals that the effectiveness, safety, and efficiency of treatment provided by the system are the most influential qualities in achieving and predicting user satisfaction. For the professional users, effectiveness and affordability are the most influential. The parameters of the PLS-SEM that are calculated allow for the measurement of a user satisfaction index similar to CSI for similar health information systems.
Conclusions
For both patients and professionals, the effectiveness of systems highly contributes to their satisfaction. Patients care about improvements in safety and efficiency, whereas professionals care about improvements in the affordability of treatments with health information systems. User satisfaction is reflected more in the users’ evaluation of system output and fulfillment of expectations but slightly less in how far the system is from ideal. Investigating satisfaction scores can be a simple and fast way to infer if the system has improved the abovementioned qualities in treatment and care.}
}
@article{AKDEMIR2024124029,
title = {A review on deep learning applications with semantics},
journal = {Expert Systems with Applications},
volume = {251},
pages = {124029},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124029},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424008959},
author = {Emre Akdemir and Necaattin Barışçı},
keywords = {Deep Learning, Semantic Web and Natural Language Processing},
abstract = {In recent years, improvements in hardware and software which increased the speed of computing and production of large amounts of data that meet the needs of educational data caused deep learning to become popular. In addition to the developments brought by deep learning, the applications performed with the support of semantic technologies proved significant increases in performance. In this systematic review, 81 studies in which semantic technologies and deep learning are used together were investigated. Deep learning architectures and semantic methods used in the studies were presented by dividing into categories. Besides, the transformations of data obtained from semantic technologies were examined in order to use neural networks.}
}
@article{CROWTHER2020962,
title = {ShortBOL: A Language for Scripting Designs for Engineered Biological Systems Using Synthetic Biology Open Language (SBOL)},
journal = {ACS Synthetic Biology},
volume = {9},
number = {4},
pages = {962-966},
year = {2020},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.9b00470},
url = {https://www.sciencedirect.com/science/article/pii/S2161506320001047},
author = {Matthew Crowther and Lewis Grozinger and Matthew Pocock and Christopher P. D. Taylor and James A. McLaughlin and Göksel Mısırlı and Bryan A. Bartley and Jacob Beal and Angel Goñi-Moreno and Anil Wipat},
keywords = {programming language, biodesign, Synthetic Biology Open Language (SBOL), synthetic biology, RDF},
abstract = {The Synthetic Biology Open Language (SBOL) is an emerging synthetic biology data exchange standard, designed primarily for unambiguous and efficient machine-to-machine communication. However, manual editing of SBOL is generally difficult for nontrivial designs. Here, we describe ShortBOL, a lightweight SBOL scripting language that bridges the gap between manual editing, visual design tools, and direct programming. ShortBOL is a shorthand textual language developed to enable users to create SBOL designs quickly and easily, without requiring strong programming skills or visual design tools.
}
}
@article{BEETZ2025101375,
title = {Robot manipulation in everyday activities with the CRAM 2.0 cognitive architecture and generalized action plans},
journal = {Cognitive Systems Research},
volume = {92},
pages = {101375},
year = {2025},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2025.101375},
url = {https://www.sciencedirect.com/science/article/pii/S1389041725000555},
author = {Michael Beetz and Gayane Kazhoyan and David Vernon},
keywords = {Cognitive robotics, Cognitive architecture, Robot manipulation, Everyday activity},
abstract = {The CRAM 2.0 robot cognitive architecture provides a framework for knowledge-based instantiation of robot manipulation design patterns for everyday activities. These design patterns take the form of generalized action plans, which are transformed by CRAM 2.0 into parameterized low-level motion plans, using knowledge and reasoning with a contextual model to identify the motion parameter values that will successfully perform the actions required to accomplish the task. In this way, CRAM 2.0 performs implicit-to-explicit manipulation, mapping an under-specified high-level goal to the specific low-level motions required to accomplish the goal. We demonstrate the ability of a CRAM-controlled robot to carry out everyday activities in a kitchen environment.}
}
@article{RATHORE2024108926,
title = {ToxinPred 3.0: An improved method for predicting the toxicity of peptides},
journal = {Computers in Biology and Medicine},
volume = {179},
pages = {108926},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108926},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524010114},
author = {Anand Singh Rathore and Shubham Choudhury and Akanksha Arora and Purva Tijare and Gajendra P.S. Raghava},
keywords = {Toxic motifs, Virtual screening, Machine learning, Deep learning, Large language models, Ensemble/hybrid method},
abstract = {Toxicity emerges as a prominent challenge in the design of therapeutic peptides, causing the failure of numerous peptides during clinical trials. In 2013, our group developed ToxinPred, a computational method that has been extensively adopted by the scientific community for predicting peptide toxicity. In this paper, we propose a refined variant of ToxinPred that showcases improved reliability and accuracy in predicting peptide toxicity. Initially, we utilized a similarity/alignment-based approach employing BLAST to predict toxic peptides, which yielded satisfactory accuracy; however, the method suffered from inadequate coverage. Subsequently, we employed a motif-based approach using MERCI software to uncover specific patterns or motifs that are exclusively observed in toxic peptides. The search for these motifs in peptides allowed us to predict toxic peptides with a high level of specificity with poor sensitivity. To overcome the coverage limitations, we developed alignment-free methods using machine/deep learning techniques to balance sensitivity and specificity of prediction. Deep learning model (ANN - LSTM with fixed sequence length) developed using one-hot encoding achieved a maximum AUROC of 0.93 with MCC of 0.71 on an independent dataset. Machine learning model (extra tree) developed using compositional features of peptides achieved a maximum AUROC of 0.95 with MCC of 0.78. We also developed large language models and achieved maximum AUC of 0.93 using ESM2-t33. Finally, we developed hybrid or ensemble methods combining two or more methods to enhance performance. Our specific hybrid method, which combines a motif-based approach with a machine learning-based model, achieved a maximum AUROC of 0.98 with MCC 0.81 on an independent dataset. In this study, all models were trained and tested on 80 % of data using five-fold cross-validation and evaluated on the remaining 20 % of data called independent dataset. The evaluation of all methods on an independent dataset revealed that the method proposed in this study exhibited better performance than existing methods. To cater to the needs of the scientific community, we have developed a standalone software, pip package and web-based server ToxinPred3 (https://github.com/raghavagps/toxinpred3 and https://webs.iiitd.edu.in/raghava/toxinpred3/).}
}
@article{JACKSON2021388,
title = {On achieving human-level knowledge representation by developing a natural language of thought},
journal = {Procedia Computer Science},
volume = {190},
pages = {388-407},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.048},
url = {https://www.sciencedirect.com/science/article/pii/S187705092101293X},
author = {Philip C. Jackson},
keywords = {knowledge representation, understanding, ambiguity, natural language, language of thought, mentalese, TalaMind},
abstract = {What is the nature of knowledge representation needed for human-level artificial intelligence? This position paper contends that to achieve human-level AI, a system architecture for human-level knowledge representation would benefit from a neuro-symbolic approach combining deep neural networks with a ‘natural language of thought’ and would be greatly handicapped if relying only on formal logic systems.}
}
@article{LU2025100012,
title = {Geographic information discipline development: Demands, Strategy and challenges},
journal = {Information Geography},
volume = {1},
number = {1},
pages = {100012},
year = {2025},
issn = {3050-5208},
doi = {https://doi.org/10.1016/j.infgeo.2025.100012},
url = {https://www.sciencedirect.com/science/article/pii/S3050520825000120},
author = {Guonian Lü and Juhua Xiong and Mingguang Wu and Linwang Yuan and Jonathan Li and Min Chen and Zhaoyuan Yu and Liangchen Zhou and Songshan Yue and Xueying Zhang and Xudong Li and Xin Li and Fahu Chen and Chenghu Zhou and Zhonghao Zhang and Yang Gao},
keywords = {Geographic information discipline, Geographic information system, Geographic information science, Information geography, Geolinguistics, Geographic scene, Ternary world},
abstract = {Geographic Information Discipline serves as a critical pillar driving continuous innovation in Earth system science, geography, smart cities, and ecological conservation. Against the backdrop of intensified global environmental changes and diversified societal demands, this discipline faces a dual-challenge of theoretical innovation and technological breakthrough. This paper analyzes geographic information discipline from three perspectives: disciplinary demands, development strategies, and frontier challenges. In terms of disciplinary demands, we explores the requirements of Earth system science for multi-source data integration, dynamic model construction, and cross-scale analysis, as well as the evolving trends of geography within the framework of the ternary world (i.e., physical, human, and information). Regarding development strategies, we proposes strategic pathways from a multidisciplinary perspective encompassing information science, information geography, and geolinguistics. These include advancing the discipline through geographic language development, multi-modal representation, virtual-real integration, and the establishment of a universal language framework. Meanwhile, the paper addresses current theoretical and technical bottlenecks, such as insufficient modeling of information world, limited capacity for high-dimensional dynamic data processing, and the technical demand for real-time 3D scene representation. We emphasizes that under the globalization and digital transformation, geoinformatics is undergoing a comprehensive leap from the physical-human duality to the physical-human-information triad by integrating theories from information science, geography, and cognitive science. Moving forward, the discipline must focus on intelligent systems, multidimensional dynamic analysis, and multidisciplinary collaborative innovation to better address complex geographic phenomena and provide scientific and technological support for global environmental management, resource optimization, and sustainable development.}
}
@article{BEATTIE2020140,
title = {Human Dignity and Rights in the Context of Gender and the Sacramental Priesthood},
journal = {Interdisciplinary Journal for Religion and Transformation in Contemporary Society},
volume = {6},
number = {1},
pages = {140-157},
year = {2020},
issn = {2365-3140},
doi = {https://doi.org/10.30965/23642807-00601009},
url = {https://www.sciencedirect.com/science/article/pii/S2365314020000285},
author = {Tina Beattie},
keywords = {Baptism, dignity, gender, priesthood, rights, sacraments, women},
abstract = {This paper considers the question of women’s ordination to the sacramental priesthood in the context of human dignity and rights. Differentiating between two forms of ontological or intrinsic dignity – the universal dignity of the human being made in the imago Dei, and the particular dignity of those baptised into the imago Trinitatis – it argues that the refusal of ordination to women is a violation of baptismal dignity that constitutes a refusal of women’s rights. It analyses the arguments against women’s ordination and shows them to be based on a misreading of Thomas Aquinas, on the innovative concept of sexual complementarity which has replaced the earlier hierarchical model of sexual difference, and on appeals to mystery that might be better described as mystification. It concludes that the refusal to allow women to respond to the call to ordination is based on a modern form of essentialised sexual difference that is alien to the Catholic tradition and that violates Christological orthodoxy, insofar as it suggests that women are not able to image Christ.}
}
@article{LASSITER201927,
title = {Language and simplexity: A powers view},
journal = {Language Sciences},
volume = {71},
pages = {27-37},
year = {2019},
note = {Simplexity, agency and language},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0388000118300366},
author = {Charles Lassiter},
keywords = {Distributed language, Simplexity, Causal powers, Speech acts},
abstract = {The notion of simplexity is that complex problems are often solved by novel combinations of simple mechanisms. These solutions aren't simple; they're simplex. Language use, as a complex behavior, is ripe for simplex analysis. In this paper, I argue that the notion of powers—an organism's capacity to instigate or undergo change—is doubly useful. First, powers, as opposed to mental representations, are a suitable object for simplex analysis. So conceptualizing languaging in terms of powers gets us one step closer to a simplex analysis of language. But thinking of languaging in terms of powers has an additional payoff. Berthoz asserts that the concept of simplexity is related to the concept of meaning. How they're related is unclear. Conceptualizing languaging in terms of powers injects meaningfulness into lived world of the organism. Consequently, the concept of powers can act as a bridge between the concepts of meaningfulness and simplexity.}
}
@article{MURUGAN2022100993,
title = {Automatic Morpheme-based Distractors Generation for Fill-in-the-Blank Questions using Listwise Learning-To-Rank Method for Agglutinative Language},
journal = {Engineering Science and Technology, an International Journal},
volume = {26},
pages = {100993},
year = {2022},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2021.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S2215098621001026},
author = {Shanthi Murugan and Balasundaram {Sadhu Ramakrishnan}},
keywords = {MCQ, Distractor, Morpheme Learning, Learning-to-Rank, Word embedding, Morphological processing, Inflectional morphology},
abstract = {Automatic question generation facilitates the smart assessment for the evaluator to assess the student skills. Several methods were proposed to generate distractors for non-factoid cloze question using different similarity measures. This study presents a method for automatic generation of affix based distractor for Tamil fill-in-the-blank questions which are mainly used for learning Tamil grammar morphological details and vocabulary. In this study, affix based distractor generation is proposed as two step pipelined process: 1) Distractor candidate collection: This generation mainly relies on certain regularities manifest in high dimensional spaces which implicitly hybrids the orthographic and semantic features. 2) Distractor filtering: Filtering is trained as Learning-to-Rank models to persist the reliability in distractor generation. Feature based Listwise approaches (ListNet and ListMLE) were used which uses caserole relationship, subject-verb agreement, POS tag in addition to similarity measures. Experiments were done with annotated dataset (TamilMCQs) taken from 5th to 12th grade Tamil text books. Experimental results show that hybridization of spelling and semantic features highly improves the plausibility in distractor generation and then, ListMLE method improves the reliability in distractor generation compared to ListNet method. As a overall, our proposed pipelined process increases plausibility and reliability in distractor generation.}
}
@article{FUTIA2020100516,
title = {SeMi: A SEmantic Modeling machIne to build Knowledge Graphs with graph neural networks},
journal = {SoftwareX},
volume = {12},
pages = {100516},
year = {2020},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2020.100516},
url = {https://www.sciencedirect.com/science/article/pii/S2352711019302626},
author = {Giuseppe Futia and Antonio Vetrò and Juan Carlos {De Martin}},
keywords = {Knowledge Graphs, Semantic Modeling, Graph neural networks},
abstract = {SeMi (SEmantic Modeling machIne) is a tool to semi-automatically build large-scale Knowledge Graphs from structured sources such as CSV, JSON, and XML files. To achieve such a goal, SeMi builds the semantic models of the data sources, in terms of concepts and relations within a domain ontology. Most of the research contributions on automatic semantic modeling is focused on the detection of semantic types of source attributes. However, the inference of the correct semantic relations between these attributes is critical to reconstruct the precise meaning of the data. SeMi covers the entire process of semantic modeling: (i) it provides a semi-automatic step to detect semantic types; (ii) it exploits a novel approach to inference semantic relations, based on a graph neural network trained on background linked data. At the best of our knowledge, this is the first technique that exploits a graph neural network to support the semantic modeling process. Furthermore, the pipeline implemented in SeMi is modular and each component can be replaced to tailor the process to very specific domains or requirements. This contribution can be considered as a step ahead towards automatic and scalable approaches for building Knowledge Graphs.}
}
@article{NOGUEIRA2018533,
title = {FrameSTEP: A framework for annotating semantic trajectories based on episodes},
journal = {Expert Systems with Applications},
volume = {92},
pages = {533-545},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417306796},
author = {Tales P. Nogueira and Reinaldo B. Braga and Carina T. {de Oliveira} and Hervé Martin},
keywords = {Trajectory modeling, Semantic web, Trajectory annotation},
abstract = {We are witnessing an increasing usage of location data by a variety of applications. Consequently, information systems are required to deal with large datasets containing raw data to build high level abstractions. Semantic Web technologies offer powerful representation tools for pervasive applications. The convergence of location-based services and Semantic Web standards allows an easier interlinking and annotation of trajectories. However, due to the wide range of requirements on modeling mobile object trajectories, it is important to define a high-level data model for representing trajectory episodes and contextual elements with multiple levels of granularity and different options to represent spatial and temporal extents, as well as to express quantitative and qualitative semantic descriptions. In this article, we focus on modeling mobile object trajectories in the context of Semantic Web. First, we introduce a new version of the Semantic Trajectory Episodes (STEP) ontology to represent generic spatiotemporal episodes. Then, we present FrameSTEP as a new framework for annotating semantic trajectories based on episodes. As a result, we combine our ontology, which can represent spatiotemporal phenomena at different levels of granularity, with annotation algorithms, which allow to create instances of our model. The proposed spatial annotation algorithm explores the Linked Open Data cloud and OpenStreetMap tags to find relevant types of spatial features in order to describe the environment where the trajectory took place. Our framework can guide the development of future expert systems in trajectory analysis. It enables reasoning about knowledge gathered from large trajectory data and linked datasets in order to create several intelligent services.}
}
@article{LIU2022103623,
title = {The architectural design and implementation of a digital platform for Industry 4.0 SME collaboration},
journal = {Computers in Industry},
volume = {138},
pages = {103623},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103623},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000185},
author = {Zixu Liu and Pedro Sampaio and Grigory Pishchulov and Nikolay Mehandjiev and Sonia Cisneros-Cabrera and Arnd Schirrmann and Filip Jiru and Nisrine Bnouhanna},
keywords = {Digital platforms, Industry 4.0, Digitalization, Enterprise systems architecture, Ontologies, Application integration, SMEs},
abstract = {This paper presents the architectural design and implementation of DIGICOR — a collaborative Industry 4.0 (I4.0) platform aimed at enabling SMEs to dynamically form supply-chain collaborations so as to pool production capacities and capabilities and jointly address complex supply chain requests. The DIGICOR architecture builds on the event-driven service-oriented architecture (EDSOA) model to support the collaboration between SMEs, dynamic modelling of their systems and services, and their integration in the supply chains of large OEMs, enforcing digital platform governance rules for knowledge protection and security. In contrast to the extant platforms assessed through our systematic review, the proposed architecture supports the entire lifecycle of I4.0 collaborations, from creation of viable teams to deployment and operation. The architecture provides an open and extensible solution for (i) creating a marketplace for the collaboration partners, (ii) providing services for planning and controlling the collaborative production, logistics, and risk management, while supporting APIs for third parties to provide complementary services such as advanced analytics, simulation, and optimization; and (iii) seamless connectivity to automation solutions, smart objects and real-time data sources. We report on the design of the architecture and its innovative artefacts such as the component model description and the semantic model constructs created for meaningful event exchanges between architectural end-points. We also describe a running use case demonstrating implementation scenarios.}
}
@article{PETTIGREW20181437,
title = {What we talk about when we talk about numbers},
journal = {Annals of Pure and Applied Logic},
volume = {169},
number = {12},
pages = {1437-1456},
year = {2018},
note = {Logic Colloquium 2015},
issn = {0168-0072},
doi = {https://doi.org/10.1016/j.apal.2018.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0168007218300940},
author = {Richard Pettigrew},
keywords = {Mathematical structuralism, Foundations of mathematics, Philosophy of mathematics, Nominalism, Set-theoretic foundations},
abstract = {In this paper, I describe and motivate a new species of mathematical structuralism, which I call Instrumental Nominalism about Set-Theoretic Structuralism. As the name suggests, this approach takes standard Set-Theoretic Structuralism of the sort championed by Bourbaki, and removes its ontological commitments by taking an instrumental nominalist approach to that ontology of the sort described by Joseph Melia and Gideon Rosen. I argue that this avoids all of the problems that plague other versions of structuralism.}
}
@incollection{PRANAV2021253,
title = {Chapter 18 - Security issues for the Semantic Web},
editor = {Sarika Jain and Vishal Jain and Valentina Emilia Balas},
booktitle = {Web Semantics},
publisher = {Academic Press},
pages = {253-267},
year = {2021},
isbn = {978-0-12-822468-7},
doi = {https://doi.org/10.1016/B978-0-12-822468-7.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012822468700002X},
author = {Prashant Pranav and Sandip Dutta and Soubhik Chakraborty},
keywords = {Security, privacy, semantic web, IoT, healthcare},
abstract = {Security is one of the key requirements in the current state of technological advancement. Sharing of data at any level has been deemed unfit from time to time due to various security breaches. Healthcare sector is not an exception. Confidential medical data of individuals can be leaked and various techniques are then deployed by the third party to spam the individuals. This information can be further used by medical agencies for their personal benefits. In the current scenario, remotely monitoring of patients has gained some sort of momentum due to its many potential features. The term Semantic Web refers to a web where all data in a database are linked over the network of computers. It enables users to store their confidential and private data over the web, define some meaningful vocabulary for the created data, and formulate some rules to handle the stored data. Some of the technologies used to link the data in the Semantic Web are Resource Description Framework, SPARQL, Ontology Web Language, and Simple Knowledge Organization System. Likewise, the security issues of the Semantic web are of great concern in the current scenario. In this chapter, we have analyzed critically various security protocols as employed as of now in semantic web used in healthcare and suggested some ways of potentially improving the protocols to make them more robust to be used for these services.}
}
@article{ZHU2019671,
title = {A novel approach based on Neo4j for multi-constrained flexible job shop scheduling problem},
journal = {Computers & Industrial Engineering},
volume = {130},
pages = {671-686},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2019.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0360835219301585},
author = {Zhenwei Zhu and Xionghui Zhou and Kang Shao},
keywords = {Flexible job shop scheduling, Neo4j, Semantic graph, Precedence constraint, Stock constraint, Ant colony optimization},
abstract = {To accommodate the need for scheduling complex fabricated products manufacturing, this paper studies the flexible job shop scheduling problem with additional job precedence constraints, time constraints, and stock constraints. As a powerful graph database which deals with connected data and embraces relationships in flexible graphs, Neo4j is creatively introduced to tackle this problem. This paper proposes a semantic graph model which can not only represent the scheduling problem with extended constraints but also integrate the entire lifecycle data. In the semantic graph model, diverse specific data linked with semantic relationships are stored in Neo4j while the semantics of conceptual data model are recorded in the ontology. Based on Neo4j, a scheduling application framework incorporating graph database, semantic web and knowledge capture is also developed. By means of parsing the table header’s semantics, an automatic conversion mechanism is achieved between tabular data in Excel spreadsheets and graph data in Neo4j. Inspired by the similarity between ants finding food sources along paths scattered with pheromone trails and assigning operations on resources one by one in line with the time under the guidance of accumulated knowledge, a simulation-based ant colony algorithm is carried out to acquire a feasible and nearly optimal schedule solution.}
}
@article{MADSEN2021491,
title = {Gaining customer centric understanding of retail displays for future innovations},
journal = {International Journal of Retail & Distribution Management},
volume = {49},
number = {4},
pages = {491-513},
year = {2021},
issn = {0959-0552},
doi = {https://doi.org/10.1108/IJRDM-08-2019-0280},
url = {https://www.sciencedirect.com/science/article/pii/S095905522100070X},
author = {Signe Mørk Madsen},
keywords = {Display, Sensemaking, Process, Flat ontology, Systems thinking, Omnichannel, Retail design, Customer centric},
abstract = {Purpose
The aim of this research is to provide insights for future display design through understanding the processes of sensemaking of retail displays in digitised retail.
Design/methodology/approach
The research applies media elicited interviews and engages thematic analysis to understand agency and advance mental models of retail display. Actor Network Theory (ANT) is engaged to flatten the ontology to traverse digital and physical realms as well as more semiotic sources.
Findings
The article presents a system comprising sensemaking processes of displays in digitised retail and traces the blending traits of physical and digital displays labelling an emerging display terminology applicable across realms.
Research limitations/implications
The participating retail concepts' limited resources for technological innovations plus the customers all being local and recruited through the physical store represent this study's limitations.
Practical implications
The developed system reveals a process for abandoning the familiar but obsolete understanding of retail displays to replace it with new insights to support the judgement and decision process for designing innovative future displays with a customer centric logic.
Originality/value
The article is novel in flattening the ontology of retail displays to fit an organisational interface perception of the link between customer and retailer.}
}
@article{KIRAN2018205,
title = {Enabling intent to configure scientific networks for high performance demands},
journal = {Future Generation Computer Systems},
volume = {79},
pages = {205-214},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1730626X},
author = {Mariam Kiran and Eric Pouyoul and Anu Mercian and Brian Tierney and Chin Guok and Inder Monga},
keywords = {Intent-based networking, Natural language processing, Ontology engineering, SDN north-bound interface},
abstract = {Globally distributed scientific experiments involve movement of massive data volumes and many collaborators performing distributed data analysis. With complex workloads and heterogeneous resources, each user may desire certain behavior characteristics for their network paths. In this paper, we present the iNDIRA tool, which interacts with SDN north-bound interfaces to enable intent-based networking. It provides reliable, simple, and technology-agnostic communication between users and networks. Focusing particularly on science applications, iNDIRA uses natural language processing to construct semantic RDF graphs to understand, interact, and create the required network services. The technical challenges addressed by iNDIRA are: (1) development of a high-level descriptive language to query network-application requirements, (2) provides keyword identification and condition checking based on user profiles and topology details, (3) allows user negotiation based on the current network state, and (4) integrates network provisioning and service tools used by the application. iNDIRA is implemented on the ESnet network, where it interacts with OpenNSA (aka the NSI client) and Globus data transfer tools, to build complex cross-domain network paths for heterogeneous science applications, and perform secure data transfer. We argue that iNDIRA’s approach presents users with an alternative approach to interact and communicate their network demands, allowing seamless network service integration.}
}
@article{SINGH2025100430,
title = {Intrinsic Gene Expression Correlates of the Biophysically Modeled Diffusion Magnetic Resonance Imaging Signal},
journal = {Biological Psychiatry Global Open Science},
volume = {5},
number = {2},
pages = {100430},
year = {2025},
issn = {2667-1743},
doi = {https://doi.org/10.1016/j.bpsgos.2024.100430},
url = {https://www.sciencedirect.com/science/article/pii/S2667174324001435},
author = {Ajay P. Singh and Michael Fromandi and Daniel Pimentel-Alarcón and Donna M. Werling and Audrey P. Gasch and John-Paul J. Yu},
keywords = {ASD, Diffusion MRI, Imaging transcriptomics, Imaging-gene correlates, NODDI, Quantitative neuroimaging},
abstract = {Magnetic resonance imaging (MRI) is a powerful tool to identify the structural and functional correlates of neurological illness but provides limited insight into molecular neurobiology. Using rat genetic models of autism spectrum disorder, we show that image texture–processed neurite orientation dispersion and density imaging (NODDI) diffusion MRI possesses an intrinsic relationship with gene expression that corresponds to the biophysically modeled cellular compartments of the NODDI diffusion signal. Specifically, we demonstrate that neurite density index and orientation dispersion index signals are correlated with intracellular and extracellular gene expression, respectively. Moreover, we further demonstrate that these imaging signals correlate with genes specifically relevant to the etiopathogenesis of autism spectrum disorder. In sum, our data suggest fundamental relationships between gene expression and diffusion MRI, implicating the potential of diffusion MRI to probe causal neurobiological mechanisms in neuroimaging phenotypes in autism spectrum disorder.}
}
@article{RECKNAGEL2023102039,
title = {Cyberinfrastructure for sourcing and processing ecological data},
journal = {Ecological Informatics},
volume = {75},
pages = {102039},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102039},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000687},
author = {Friedrich Recknagel},
keywords = {Cyberinfrastructure, Adaptive management, Ecological entities, Ecological monitoring, Data management, Ecological synthesis, Ecological forecasting, Machine learning, Deep learning, Hybrid modelling},
abstract = {Cyberinfrastructure is a product of the information age that provides a framework for informing adaptive management of ecological entities under the impact of regional and global change. It supports proximity monitoring, user-friendly data management, knowledge discovery by data synthesis, and decision making by forecasting. A workflow is proposed that suits the iterative nature of adaptive management. It takes advantage of novel sensor, genomics, and communication technology for ecological monitoring, of ontologies, semantic webs and blockchain for data management, of hybrid, machine and deep learning concepts for data synthesis and forecasting. Forecasting at different time horizons is guiding decision making for adjusting management and continuing monitoring. This review aims to make researchers, decision makers and stakeholders aware of currently existing technology to make better use of ecological data and models for timely and evidence-based decisions.}
}
@article{HOSSAIN2023106649,
title = {Natural Language Processing in Electronic Health Records in relation to healthcare decision-making: A systematic review},
journal = {Computers in Biology and Medicine},
volume = {155},
pages = {106649},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.106649},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523001142},
author = {Elias Hossain and Rajib Rana and Niall Higgins and Jeffrey Soar and Prabal Datta Barua and Anthony R. Pisani and Kathryn Turner},
keywords = {Machine learning, Electronic Health Records, Medical natural language processing, Artificial intelligence in medicine, Automated tools, State-of-the-art deep learning},
abstract = {Background:
Natural Language Processing (NLP) is widely used to extract clinical insights from Electronic Health Records (EHRs). However, the lack of annotated data, automated tools, and other challenges hinder the full utilisation of NLP for EHRs. Various Machine Learning (ML), Deep Learning (DL) and NLP techniques are studied and compared to understand the limitations and opportunities in this space comprehensively.
Methodology:
After screening 261 articles from 11 databases, we included 127 papers for full-text review covering seven categories of articles: (1) medical note classification, (2) clinical entity recognition, (3) text summarisation, (4) deep learning (DL) and transfer learning architecture, (5) information extraction, (6) Medical language translation and (7) other NLP applications. This study follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines.
Result and Discussion:
EHR was the most commonly used data type among the selected articles, and the datasets were primarily unstructured. Various ML and DL methods were used, with prediction or classification being the most common application of ML or DL. The most common use cases were: the International Classification of Diseases, Ninth Revision (ICD-9) classification, clinical note analysis, and named entity recognition (NER) for clinical descriptions and research on psychiatric disorders.
Conclusion:
We find that the adopted ML models were not adequately assessed. In addition, the data imbalance problem is quite important, yet we must find techniques to address this underlining problem. Future studies should address key limitations in studies, primarily identifying Lupus Nephritis, Suicide Attempts, perinatal self-harmed and ICD-9 classification.}
}
@incollection{MAZZOTTA2025351,
title = {Knowledge and Reasoning},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {351-359},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00073-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000737},
author = {Giuseppe Mazzotta and Francesco Ricca and Giorgio Terracina},
keywords = {ASP, Knowledge representation and reasoning, Logic programming},
abstract = {Research on knowledge representation and reasoning (KR&R) dates back to mathematical logic and is one of the foundations of Artificial Intelligence. Several data models and inference tools have been proposed in the literature, varying from general purpose languages to ad-hoc reasoning systems. Recently, bioinformatics ignited new interest in this research area. Indeed, several bioinformatics problems can be effectively solved via KR&R methods. In this chapter we provide an overview of KR&R, focusing on one important paradigm named Answer Set Programming (ASP). We also showcase some interesting bioinformatics and medicine problems solved with ASP.}
}
@article{DOWRICK2023100306,
title = {Sharing uncertainty: Comparing patient narratives of help-seeking in the first year of the Covid-19 pandemic across the UK, USA, Brazil, Germany and Spain},
journal = {SSM - Qualitative Research in Health},
volume = {4},
pages = {100306},
year = {2023},
issn = {2667-3215},
doi = {https://doi.org/10.1016/j.ssmqr.2023.100306},
url = {https://www.sciencedirect.com/science/article/pii/S2667321523000902},
author = {Anna Dowrick and Jane Alice Evered and Alicia {Navarro Dias de Souza} and Anne Thier and Maria {Inês Gandolfo Conceição} and Christine Holmberg and Vinita Mahtani-Chugani},
keywords = {Covid-19, Cross-country comparison, Uncertainty, pandemics, Qualitative},
abstract = {The early stages of the Covid-19 pandemic generated profound global uncertainty that disrupted health systems. This paper examines uncertainty about Covid-19 from the perspective of patients who sought clinical help in Spain, the UK, the USA, Brazil and Germany in 2020. We conduct a narrative analysis to explore how patients sought to involve health care teams in addressing the ontological and epistemological uncertainties of Covid illness. Patients wanted clinical support to make sense of Covid as a novel illness and interpret their journey to recovery. Access to this support varied. Help-seeking was enabled when health services perceived patient needs as legitimate, alongside an infrastructure that enabled them access to care despite health system strain. In Brazil and Spain, candidacy for support in the early stages of illness was unquestioned, whereas in Germany, the UK and USA patients had to convince health professionals to support them. Where patients did access clinical support, they valued clinicians sharing the work of developing knowledge that would address epistemological uncertainty about Covid. Patients valued clinicians’ potential to acquire relevant expertise, rather than what they knew about Covid in a given encounter. Comparing experiences across different national settings demonstrates that patients wanted health systems to engage in the uncertainty of the pandemic through being accessible and present during novel illness experiences, sharing responsibility for learning more, and having a curiosity about the unknown.}
}
@article{NUYTS2024102443,
title = {Comparative analysis of approaches for automated compliance checking of construction data},
journal = {Advanced Engineering Informatics},
volume = {60},
pages = {102443},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102443},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624000910},
author = {Emma Nuyts and Mathias Bonduel and Ruben Verstraeten},
keywords = {Automated compliance checking, Building information management, Comparative analysis},
abstract = {While the domain of Automated Compliance Checking (ACC) has gained track, the construction industry has been flooded with different approaches. This paper studies these different approaches for use in compliance checking of construction data. The approaches are compared by defining constraints for the same set of five requirements, each of a different category, stemming from the Flemish building regulation on accessibility. Eight approaches have been selected for comparison: two IFC-based approaches (Solibri Model Checker and the upcoming buildingSMART standard IDS), two general data standards and their accompanying schema definition languages (JSON Schema and XSD), and four Linked Data approaches (OWL, SWRL, SPARQL, and SHACL). Besides the pure functional analysis, the relative uptake and support in tooling are also considered. While XML/XSD and JSON/JSON Schema and the Linked Data approaches are in essence domain-independent, only the latter has an extra layer for agreeing on high-level data modeling (and thus data validation) patterns in the construction domain with the EN17632-1:2022 standard. SHACL is considered the most adept method from the Linked Data approaches since it is fully standardized for both inputs and outputs and was developed for validation use cases.}
}
@incollection{LLINAS2025119,
title = {5 - Credition, uncertainty, consciousness, and communication},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Marco Brambilla},
booktitle = {Bi-directionality in Human-AI Collaborative Systems},
publisher = {Academic Press},
pages = {119-134},
year = {2025},
isbn = {978-0-443-40553-2},
doi = {https://doi.org/10.1016/B978-0-44-340553-2.00011-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443405532000113},
author = {James Llinas},
keywords = {Human–AI communications, Interaction models, Synergy},
abstract = {This chapter discusses several aspects of the communication process inherent in communication among/between humans and between humans and computationally-based processes, especially as reflected in AI systems, i.e., so-called “bidirectional” communication between such entities. A major topic is the notion of belief and how it is that, through communications, two agents come to believe each other. Common belief among agents is clearly needed to enable action-taking and decision-making. To understand such ideas, participating agents need to have a common comprehension of the concept of belief. Further, such communicating agents also need to appreciate that there is an overarching issue regarding the uncertainty in such beliefs and how that uncertainty needs to be accounted for. In interhuman communication, there are the subtle aspects of empathy and consciousness that have significant effects on the communication process and its effectiveness in the agents coming to a common understanding and belief. The chapter also briefly addresses this topic of consciousness and empathy in AI systems, and the current state-of-the-art in this area. A major section reviews a number of factors affecting human–AI communication and process models, and the aspects of team structures, humanness, and complexity in the design of such bidirectional processes. A final segment introduces the importance of appreciating whether and how AI-based constructions of natural language, to the level of how language is used in interhuman communication, is even feasible as a basis for potential bidirectional communication.}
}
@article{PAQUET2023,
title = {Social Prescription Interventions Addressing Social Isolation and Loneliness in Older Adults: Meta-Review Integrating On-the-Ground Resources},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/40213},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123003667},
author = {Catherine Paquet and Jocelyne Whitehead and Rishabh Shah and Alayne Mary Adams and Damion Dooley and R Nathan Spreng and Anna-Liisa Aunio and Laurette Dubé},
keywords = {social prescription, social isolation, loneliness, intervention, older adults, knowledge mobilization, database management, ontology},
abstract = {Background
Social prescription programs represent a viable solution to linking primary care patients to nonmedical community resources for improving patient well-being. However, their success depends on the integration of patient needs with local resources. This integration could be accelerated by digital tools that use expressive ontology to organize knowledge resources, thus enabling the seamless navigation of diverse community interventions and services tailored to the needs of individual users. This infrastructure bears particular relevance for older adults, who experience a range of social needs that impact their health, including social isolation and loneliness. An essential first step in enabling knowledge mobilization and the successful implementation of social prescription initiatives to meet the social needs of older adults is to incorporate the evidence-based academic literature on what works, with on-the-ground solutions in the community.
Objective
This study aims to integrate scientific evidence with on-the-ground knowledge to build a comprehensive list of intervention terms and keywords related to reducing social isolation and loneliness in older adults.
Methods
A meta-review was conducted using a search strategy combining terms related to older adult population, social isolation and loneliness, and study types relevant to reviews using 5 databases. Review extraction included intervention characteristics, outcomes (social [eg, loneliness, social isolation, and social support] or mental health [eg, psychological well-being, depression, and anxiety]), and effectiveness (reported as consistent, mixed, or not supported). Terms related to identified intervention types were extracted from the reviewed literature as well as descriptions of corresponding community services in Montréal, Canada, available from web-based regional, municipal, and community data sources.
Results
The meta-review identified 11 intervention types addressing social isolation and loneliness in older adults by either increasing social interactions, providing instrumental support, promoting mental and physical well-being, or providing home and community care. Group-based social activities, support groups with educational elements, recreational activities, and training or use of information and communication technologies were the most effective in improving outcomes. Examples of most intervention types were found in community data sources. Terms derived from the literature that were the most commonly congruent with those describing existing community services were related to telehealth, recreational activities, and psychological therapy. However, several discrepancies were observed between review-based terms and those addressing the available services.
Conclusions
A range of interventions found to be effective at addressing social isolation and loneliness or their impact on mental health were identified from the literature, and many of these interventions were represented in services available to older residents in Montréal, Canada. However, different terms were occasionally used to describe or categorize similar services across data sources. Establishing an efficient means of identifying and structuring such sources is important to facilitate referrals and help-seeking behaviors of older adults and for strategic planning of resources.}
}
@article{BIKAKI2024111823,
title = {Building an open-source collaborative platform for migration research: A metadata modeling approach using XML},
journal = {Knowledge-Based Systems},
volume = {299},
pages = {111823},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111823},
url = {https://www.sciencedirect.com/science/article/pii/S095070512400457X},
author = {Athina Bikaki and Mark Peters and Jimmy Krozel and Ioannis A. Kakadiaris},
keywords = {Collaborative and open-source platform, Data, FAIR data principles, Migration research, Metadata, Metadata modeling, XML},
abstract = {Public access to well-organized migration data repositories is rare. While open migration data are available, most are dispersed, challenging their organization. This work uses a metadata modeling approach to organize migration-related data and research components hosted in an open-source, collaborative research environment. The proposed platform integrates data, computational models, application libraries, and research projects in a unified environment for research. An overview of the metadata modeling process and the XML technology used to integrate the platform components was provided, along with descriptions of the platform’s essential features, such as standardized exchange of information for data organization and sharing and collaborative authoring. The design of a prototype collaborative, open-source migration data and modeling research platform using sample data and software components was built and hosted on GitHub for testing and refinement purposes. It allows the authors to collaborate, improve, and assess the platform’s functionalities and capabilities. Our platform, COSMOS (Collaborative Open-Source Modeling System), adopted the creation and use of open data formats for alignment with FAIR (Findable, Accessible, Interoperable, and Reusable) data principles to ensure that data at the prototype level is consistent with and can be used by the international migration research community in the future. By providing a standardized framework for data organization, data sharing, and collaboration, the platform can help accelerate scientific discovery and advance our understanding of complex social and cultural phenomena such as immigration.}
}
@article{HU2022,
title = {Evaluation and Comparative Analysis of Semantic Web-Based Strategies for Enhancing Educational System Development},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.302895},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000333},
author = {Bin Hu and Akshat Gaurav and Chang Choi and Ammar Almomani},
keywords = {Artificial Intelligence, Big Data, E-Learning, Machine Learning, Ontology},
abstract = {ABSTRACT
Educators have been calling for reform for a decade. Recent technical breakthroughs have led to various improvements in the semantic web-based education system. After last year's COVID-19 outbreak, development quickened. Many countries and educational systems now concentrate on providing students with online education, which differs greatly from traditional classroom education. Online education allows students to learn at their own pace. As a consequence, education has become more dynamic. In the educational system, this changing nature makes user demands difficult to identify. Many instructors suggest using machine learning, artificial intelligence, or ontology to improve traditional teaching methods. Due to the lack of survey studies examining and comparing all of the researcher's semantic web-based teaching methodologies, the authors decided to conduct this survey. This paper's goal is to analyse all available possibilities for semantic web-based education systems that enable new researchers to develop their knowledge.}
}
@article{HERNANDEZVELAZQUEZ2024104569,
title = {The future is fermented: Microbial biodiversity of fermented foods is a critical resource for food innovation and human health},
journal = {Trends in Food Science & Technology},
volume = {150},
pages = {104569},
year = {2024},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2024.104569},
url = {https://www.sciencedirect.com/science/article/pii/S0924224424002450},
author = {Rodrigo Hernández-Velázquez and Lena Flörl and Anton Lavrinienko and Zuzana Sebechlebská and Liana Merk and Anna Greppi and Nicholas A. Bokulich},
keywords = {Microbial diversity, Food fermentation, Microbial conservation, Microbiome},
abstract = {Background
Fermented foods and beverages have been a key component of the human diet for thousands of years, and play an important nutritional, cultural, and economic role in most human societies. Differences in ingredients, local practices, and environmental conditions can impact microbial communities in fermented foods resulting in distinct flavors, textures, and nutritional properties. Despite their ancient roots, omnipresence, potential health benefits, recent hype, and the plethora of tools to study microbial diversity, the microbial and functional diversity of most fermented foods currently remain unknown, while changing lifestyle practices threaten the loss of some fermented foods, with dire implications for gastrointestinal health.
Scope and approach
In this commentary we discuss the nutritional, cultural, and economic values of fermented foods and (i) introduce FermDB, the largest interoperable database and map of fermented foods to date (https://bokulich-lab.github.io/FermDB/), intended as an extensible community resource for documenting and studying food fermentations; (ii) provide a consistent fermented food ontology for classification of these foods; and (iii) estimate global production values and biomass of some of the most common fermented foods to contextualize the social and economic importance of microorganisms for human food production.
Key findings and conclusions
We provide a new perspective on global diversity of fermented foods and associated microbial communities as a resource for sustainable food production. Additionally, the introduced database and ontology can act as a roadmap to the path toward conserving microbial biodiversity of traditional foods and studying their associated health benefits.}
}
@article{SHEIKHALISHAHI2019,
title = {Natural Language Processing of Clinical Notes on Chronic Diseases: Systematic Review},
journal = {JMIR Medical Informatics},
volume = {7},
number = {2},
year = {2019},
issn = {2291-9694},
doi = {https://doi.org/10.2196/12239},
url = {https://www.sciencedirect.com/science/article/pii/S2291969419000504},
author = {Seyedmostafa Sheikhalishahi and Riccardo Miotto and Joel T Dudley and Alberto Lavelli and Fabio Rinaldi and Venet Osmani},
keywords = {electronic health records, clinical notes, chronic diseases, natural language processing, machine learning, deep learning, heart disease, stroke, cancer, diabetes, lung disease},
abstract = {Background
Novel approaches that complement and go beyond evidence-based medicine are required in the domain of chronic diseases, given the growing incidence of such conditions on the worldwide population. A promising avenue is the secondary use of electronic health records (EHRs), where patient data are analyzed to conduct clinical and translational research. Methods based on machine learning to process EHRs are resulting in improved understanding of patient clinical trajectories and chronic disease risk prediction, creating a unique opportunity to derive previously unknown clinical insights. However, a wealth of clinical histories remains locked behind clinical narratives in free-form text. Consequently, unlocking the full potential of EHR data is contingent on the development of natural language processing (NLP) methods to automatically transform clinical text into structured clinical data that can guide clinical decisions and potentially delay or prevent disease onset.
Objective
The goal of the research was to provide a comprehensive overview of the development and uptake of NLP methods applied to free-text clinical notes related to chronic diseases, including the investigation of challenges faced by NLP methodologies in understanding clinical narratives.
Methods
Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were followed and searches were conducted in 5 databases using “clinical notes,” “natural language processing,” and “chronic disease” and their variations as keywords to maximize coverage of the articles.
Results
Of the 2652 articles considered, 106 met the inclusion criteria. Review of the included papers resulted in identification of 43 chronic diseases, which were then further classified into 10 disease categories using the International Classification of Diseases, 10th Revision. The majority of studies focused on diseases of the circulatory system (n=38) while endocrine and metabolic diseases were fewest (n=14). This was due to the structure of clinical records related to metabolic diseases, which typically contain much more structured data, compared with medical records for diseases of the circulatory system, which focus more on unstructured data and consequently have seen a stronger focus of NLP. The review has shown that there is a significant increase in the use of machine learning methods compared to rule-based approaches; however, deep learning methods remain emergent (n=3). Consequently, the majority of works focus on classification of disease phenotype with only a handful of papers addressing extraction of comorbidities from the free text or integration of clinical notes with structured data. There is a notable use of relatively simple methods, such as shallow classifiers (or combination with rule-based methods), due to the interpretability of predictions, which still represents a significant issue for more complex methods. Finally, scarcity of publicly available data may also have contributed to insufficient development of more advanced methods, such as extraction of word embeddings from clinical notes.
Conclusions
Efforts are still required to improve (1) progression of clinical NLP methods from extraction toward understanding; (2) recognition of relations among entities rather than entities in isolation; (3) temporal extraction to understand past, current, and future clinical events; (4) exploitation of alternative sources of clinical knowledge; and (5) availability of large-scale, de-identified clinical corpora.}
}
@article{BARZEGAR2018119,
title = {Attack scenario reconstruction using intrusion semantics},
journal = {Expert Systems with Applications},
volume = {108},
pages = {119-133},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.04.030},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418302689},
author = {Mahdiyeh Barzegar and Mehdi Shajari},
keywords = {Alert correlation, Attack scenario, Ontology, Similarity, Semantic},
abstract = {Security information and event management (SIEM) systems receive a large number of alerts from different intrusion detection systems. They are expected, from these alerts, to make reliable and timely decisions regarding the types of ongoing attack scenarios and their priorities. However, the lack of an agreed-upon vocabulary for the representation of the domain knowledge makes it difficult for state-of-the-art SIEM systems to effectively manage these complex decisions. To overcome this problem, an ontology-based expert system approach can provide domain knowledge modeling as a foundation for disambiguation of meaning and automatic reasoning regarding ongoing attack scenarios. The proposed approach reconstructs attack scenarios by reasoning based on the evidences in the alert stream. The main idea of the proposed approach is to identify the causal relation between alerts using their similarity. This approach assumes that the similarity between two successive steps in an attack scenario is greater than that of two non-successive steps. Moreover, the similarity between the steps of the same attack scenario is greater than that between the steps of two different attack scenarios. The benefit of the proposed approach includes the fast and incremental reconstruction of known and unknown attack scenarios without expert intervention, which is an enormous step forward in developing expert and intelligent systems for cyber security. We evaluated the proposed technique by performing experiments on two known datasets: DARPA 2000 and MACCDC 2012. The results prove the advantages of the proposed approach with regard to completeness and soundness criteria.}
}
@article{HSU2022100123,
title = {Classification of cervical biopsy free-text diagnoses through linear-classifier based natural language processing},
journal = {Journal of Pathology Informatics},
volume = {13},
pages = {100123},
year = {2022},
issn = {2153-3539},
doi = {https://doi.org/10.1016/j.jpi.2022.100123},
url = {https://www.sciencedirect.com/science/article/pii/S2153353922007179},
author = {Jim Wei-Chun Hsu and Paul Christensen and Yimin Ge and S. Wesley Long},
keywords = {Natural language processing, Machine learning, Computational pathology, Linear classifier, FastText, Cervical biopsy},
abstract = {Routine cervical cancer screening has significantly decreased the incidence and mortality of cervical cancer. As selection of proper screening modalities depends on well-validated clinical decision algorithms, retrospective review correlating cytology and HPV test results with cervical biopsy diagnosis is essential for validating and revising these algorithms to changing technologies, demographics, and optimal clinical practices. However, manual categorization of the free-text biopsy diagnosis into discrete categories is extremely laborious due to the overwhelming number of specimens, which may lead to significant error and bias. Advances in machine learning and natural language processing (NLP), particularly over the last decade, have led to significant accomplishments and impressive performance in computer-based classification tasks. In this work, we apply an efficient version of an NLP framework, FastText™, to an annotated cervical biopsy dataset to create a supervised classifier that can assign accurate biopsy categories to free-text biopsy interpretations with high concordance to manually annotated data (>99.6%). We present cases where the machine-learning classifier disagrees with previous annotations and examine these discrepant cases after referee review by an expert pathologist. We also show that the classifier is robust on an untrained external dataset, achieving a concordance of 97.7%. In conclusion, we demonstrate a useful application of NLP to a real-world pathology classification task and highlight the benefits and limitations of this approach.}
}
@article{GOSSELIN2023104976,
title = {Modeling and integrating interactions involving the CYP450 enzyme system in a multi-terminology server: Contribution to information extraction from a clinical data warehouse},
journal = {International Journal of Medical Informatics},
volume = {170},
pages = {104976},
year = {2023},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2022.104976},
url = {https://www.sciencedirect.com/science/article/pii/S1386505622002908},
author = {Laura Gosselin and Catherine Letord and Romain Leguillon and Lina F. Soualmia and Badisse Dahamna and Abdelmalek Mouazer and Flavien Disson and Stéfan J. Darmoni and Julien Grosjean},
keywords = {Health data warehousing, Cytochrome P-450, Drug interactions, Knowledge representation, Information retrieval},
abstract = {Introduction
The cytochrome P450 (CYP450) enzyme system is involved in the metabolism of certain drugs and is responsible for most drug interactions. These interactions result in either an enzymatic inhibition or an enzymatic induction mechanism that has an impact on the therapeutic management of patients. Detecting these drug interactions will allow for better predictability in therapeutic response. Therefore, computerized solutions can represent a valuable help for clinicians in their tasks of detection.
Objective
The objective of this study is to provide a structured data-source of interactions involving the CYP450 enzyme system. These interactions are aimed to be integrated in the cross-lingual multi-terminology server HeTOP (Health Terminologies and Ontologies Portal), to support the query processing of the clinical data warehouse (CDW) EDSaN (Entrepôt de Données de Santé Normand).
Material and methods
A selection and curation of drug components (DCs) that share a relationship with the CYP450 system was performed from several international data sources. The DCs were linked according to the type of relationship which can be substrate, inhibitor, or inducer. These relationships were then integrated into the HeTOP server. To validate the CYP450 relationships, a semantic query was performed on the CDW, whose search engine is founded on HeTOP data (concepts, terms, and relations).
Results
A total of 776 DCs are associated by a new interaction relationship, integrated in HeTOP, by 14 enzymes. These are CYP450 1A2, 2A6, 2B6, 2C8, 2C9, 2C18, 2C19, 2D6, 2E1, 3A4, 3A7, 11B1,11B2 mitochondrial and P-glycoprotein, constituting a total of 2,088 relationships. A general modelling of cytochromic interactions was performed. From this model, 233,006 queries were processed in less than two hours, demonstrating the usefulness and performance of our CDW implementation. Moreover, they showed that in our university hospital, the concurrent prescription that could cause a cytochromic interaction is Bisoprolol with Amiodarone by enzymatic inhibition for 2,493 patients.
Discussion
The queries submitted to the CDW EDSaN allowed to highlight the most prescribed molecules simultaneously and potentially responsible for cytochromic interactions. In a second step, it would be interesting to evaluate the real clinical impact by looking for possible adverse effects of these interactions in the patients' files. Other computational solutions for cytochromic interactions exist. The impact of CYP450 is particularly important for drugs with narrow therapeutic window (NTW) as they can lead to increased toxicity or therapeutic failure. It is also important to define which drug component is a pro-drug and to considerate the many genetic polymorphisms of patients.
Conclusion
The HeTOP server contains a non-negligible number of relationships between drug components and CYP450 from multiple reference sources. These data allow us to query our Clinical Data Warehouse to highlight these cytochromic interactions. It would be interesting in the future to assess the actual clinical impact in hospital reports.}
}
@article{ZEB2021360,
title = {Data harmonisation as a key to enable digitalisation of the food sector: A review},
journal = {Food and Bioproducts Processing},
volume = {127},
pages = {360-370},
year = {2021},
issn = {0960-3085},
doi = {https://doi.org/10.1016/j.fbp.2021.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0960308521000237},
author = {Akhtar Zeb and Juha-Pekka Soininen and Nesli Sozer},
keywords = {Food sector, Data harmonisation, Food ontology, Harmonisation challenges},
abstract = {The food sector is driven by a large number of actors, including primary producers, manufacturers, logistics providers, retailers, and consumers. At each phase of the food value chain, a significant amount of data is generated that provides important information to the agents involved in processing and flow of food products from farm to fork. Proper handling of food data has a crucial role in providing safe, quality and affordable products to the increasing world population. The independent production of food data, without following any specific guidelines and procedures, often results in inconsistent and incomparable datasets that cannot be directly utilised by multiple users. Data harmonisation means reconciling various types, levels and sources of data in formats that are compatible and comparable, and thus useful for better decision making. In the food sector, one way of performing data harmonisation is to represent food data according to reliable classification and description systems. Another approach towards harmonisation is to match various food concepts to the existing and widely used ontologies. Furthermore, harmonisation is facilitated by following specific guidelines and procedures during data collection processes. This study explores some of the most important tools, frameworks and methodologies for data harmonisation in the food sector.}
}
@article{LIMA2019142,
title = {A logic-based relational learning approach to relation extraction: The OntoILPER system},
journal = {Engineering Applications of Artificial Intelligence},
volume = {78},
pages = {142-157},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2018.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0952197618302276},
author = {Rinaldo Lima and Bernard Espinasse and Fred Freitas},
keywords = {Relation extraction, Rule induction, Information extraction, Inductive logic programming, Relational learning},
abstract = {Relation Extraction (RE), the task of detecting and characterizing semantic relations between entities in text, has gained much importance in the last two decades, mainly in the biomedical domain. Many papers have been published on Relation Extraction using supervised machine learning techniques. Most of these techniques rely on statistical methods, such as feature-based and tree-kernels-based methods. Such statistical learning techniques are usually based on a propositional hypothesis space for representing examples, i.e., they employ an attribute–value representation of features. This kind of representation has some drawbacks, particularly in the extraction of complex relations which demand more contextual information about the involving instances, i.e., it is not able to effectively capture structural information from parse trees without loss of information. In this work, we present OntoILPER, a logic-based relational learning approach to Relation Extraction that uses Inductive Logic Programming for generating extraction models in the form of symbolic extraction rules. OntoILPER takes profit of a rich relational representation of examples, which can alleviate the aforementioned drawbacks. The proposed relational approach seems to be more suitable for Relation Extraction than statistical ones for several reasons that we argue. Moreover, OntoILPER uses a domain ontology that guides the background knowledge generation process and is used for storing the extracted relation instances. The induced extraction rules were evaluated on three protein–protein interaction datasets from the biomedical domain. The performance of OntoILPER extraction models was compared with other state-of-the-art RE systems. The encouraging results seem to demonstrate the effectiveness of the proposed solution.}
}
@article{CISNEROSCABRERA2021103391,
title = {An approach and decision support tool for forming Industry 4.0 supply chain collaborations},
journal = {Computers in Industry},
volume = {125},
pages = {103391},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103391},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520306254},
author = {Sonia Cisneros-Cabrera and Grigory Pishchulov and Pedro Sampaio and Nikolay Mehandjiev and Zixu Liu and Sophia Kununka},
keywords = {Digitalization, Supply chain collaboration, Industry 4.0, Decision support systems, Interoperability, Ontology},
abstract = {Industry 4.0 technologies, process digitalisation and automation can be applied to support the formation of supply chain collaborations in manufacturing. Underpinned by information and communication technologies, collaborations of independent companies can dynamically pool production capacities and capabilities to jointly react to new business opportunities. These collaborations may involve a wide range of enterprises with different sizes and scope that individually would not be able to tender for such new business opportunities. To form these collaborative teams, assistive processes and technologies can underpin the effort towards exploring the tender requirements, unbundling the tender into smaller tasks and finding a suitable supplier for each task. In this paper, we present an approach and a tool to support decision making concerning forming supply chain collaborations in Industry 4.0. The approach proposed is unique in integrating industry domain ontologies, assistive human-computer interaction tools and multi-criteria decision support techniques to form team compositions speeding-up the collaboration process whilst maximising the chances of forming a viable team to fulfil the tender requirements. We also show evaluation results involving stakeholders from the supply chain function pointing to the effectiveness of the proposed solution, available online as a demo11http://130.88.97.225:4200 (username: TDMS@uniman.eu; password: uniman)..}
}
@article{AGGARWAL2022100444,
title = {R-classify: Extracting research papers’ relevant concepts from a controlled vocabulary},
journal = {Software Impacts},
volume = {14},
pages = {100444},
year = {2022},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2022.100444},
url = {https://www.sciencedirect.com/science/article/pii/S2665963822001282},
author = {Tanay Aggarwal and Angelo Salatino and Francesco Osborne and Enrico Motta},
keywords = {Topic detection, Topic extraction, Scholarly data, Science of science, Text mining, Scholarly ontologies},
abstract = {In the past few decades, we saw a proliferation of scientific articles available online. This data-rich environment offers several opportunities but also challenges, since it is problematic to explore these resources and identify all the relevant content. Hence, it is crucial that they are appropriately annotated with their relevant concepts so to increase their chance of being properly indexed and retrieved. In this paper, we present R-Classify, a web tool that assists users in identifying the most relevant concepts according to a large-scale ontology of research areas in the field of Computer Science.}
}
@article{DUERR2021168,
title = {The physics and metaphysics of Tychistic Bohmian Mechanics},
journal = {Studies in History and Philosophy of Science Part A},
volume = {90},
pages = {168-183},
year = {2021},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2021.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0039368121001540},
author = {Patrick M. Duerr and Alexander Ehmann},
keywords = {Bohmian mechanics, Underdetermination, Selective realism, Temporal solipsism, Many worlds, Bell's everett (?) theory, Primitive ontology},
abstract = {The paper takes up Bell's (1987) “Everett (?) theory” and develops it further. The resulting theory is about the system of all particles in the universe, each located in ordinary, 3-dimensional space. This many-particle system as a whole performs random jumps through 3N-dimensional configuration space – hence “Tychistic Bohmian Mechanics” (TBM). The distribution of its spontaneous localisations in configuration space is given by the Born Rule probability measure for the universal wavefunction. Contra Bell, the theory is argued to satisfy the minimal desiderata for a Bohmian theory within the Primitive Ontology framework (for which we offer a metaphysically more perspicuous formulation than is customary). TBM's formalism is that of ordinary Bohmian Mechanics (BM), without the postulate of continuous particle trajectories and their deterministic dynamics. This “rump formalism” receives, however, a different interpretation. We defend TBM as an empirically adequate and coherent quantum theory. Objections voiced by Bell and Maudlin are rebutted. The “for all practical purposes”-classical, Everettian worlds (i.e. quasi-classical histories) exist sequentially in TBM (rather than simultaneously, as in the Everett interpretation). In a temporally coarse-grained sense, they quasi-persist. By contrast, the individual particles themselves cease to persist.}
}
@article{BARBOSA2025103378,
title = {Data quality issues in data used in species distribution models: A systematic literature review},
journal = {Ecological Informatics},
volume = {91},
pages = {103378},
year = {2025},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2025.103378},
url = {https://www.sciencedirect.com/science/article/pii/S1574954125003875},
author = {Wesley Lourenco Barbosa and Solange Nice Alves-Souza},
keywords = {Citizen science, Data quality, Location errors, Misidentification, Species distribution models},
abstract = {Species distribution models (SDM) are important tools for decision-making in several application areas, being essential for managing biodiversity resources in the world. The ability of these models to represent the reality is strongly dependent on the fitness of the data from which they are generated. Although scientific literature recognizes the occurrence of several data quality (DQ) problems, little work has focused on conducting a comprehensive survey to identify and quantify these challenges. Thus, this paper conducts a systematic review of the literature to examine the DQ problems observed in species occurrence and environmental data applied to the SDM context. It also identifies and discusses solutions that have been proposed to address these problems. A total of 212 articles were selected and analyzed to identify 14 recurring DQ problems. Misidentification errors and spatial or geographical bias were the most prevalent. Data gathered through Citizen Science initiatives continue to be a subject of scrutiny, with observer skill identified as the third most frequent challenge. Resolving data quality issues remains a significant research challenge due to the specific characteristics of the data types involved. Our findings highlight the need for a more detailed examination of the impact of data quality on SDMs and call for the development of robust methodologies for data quality assessment and improvement. The paper emphasizes the importance of context-specific knowledge for the effective management of data quality, which is essential for enhancing the reliability of SDMs and supporting more accurate ecological forecasting and conservation planning. Consequently, a substantial body of research remains to be conducted, particularly at the intersection of computational methodologies and the specialized domain of biogeography.}
}
@article{CHEN2022104191,
title = {Graph-based linguistic and visual information integration for on-site occupational hazards identification},
journal = {Automation in Construction},
volume = {137},
pages = {104191},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104191},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522000644},
author = {Shi Chen and Kazuyuki Demachi and Feiyan Dong},
keywords = {Construction safety, Occupational hazards identification, Personal protective equipment (PPE), Deep learning, Graph},
abstract = {Construction sites are hazardous with various potential hazards that can occur at any time. The combination of different factors always causes the construction fatalities, and the majority of these fatalities could be prevented if workers followed on-site regulatory rules. However, compliance of regulatory rules is not strictly enforced among workers due to all kinds of reasons. Although previously proposed vision-based approaches are available for occupational hazards identification, the practicality is limited by the lack of automated understanding and adaptability to regulatory rules changes. In response to these gaps, this paper proposes a novel graph-based framework that integrates linguistic and visual information to process regulatory rule sentences and images for on-site occupational hazards identification. Particularly, a regulatory rules processing approach is presented to automatically extract and represent the key linguistic information of regulatory rules and a vision-based image scene information understanding approach is introduced to process on-site images by the combination of deep learning-based object detection and individual detection using geometric relationships analysis. Additionally, an automated reasoning approach is proposed to provide the integration of the processed linguistic and visual information and perform hazards identification. The hazards of two scenes, i.e., “working on height” and “operating a grinder”, were successfully identified with significantly higher performance compared to the baseline model.}
}
@article{SONG2024123927,
title = {A deep learning-based approach to similarity calculation for UML use case models},
journal = {Expert Systems with Applications},
volume = {251},
pages = {123927},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123927},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424007930},
author = {Shizhe Song and Ye Wang and Xiaoyang Wang and Chengyi Lin and Kun Hu},
keywords = {Use case model, Similarity calculation, Structural similarity, Semantic similarity, Neural network},
abstract = {Unified Modeling Language (UML) models are main artifacts of software applications and valuable assets of software projects. The similarity calculation of UML models is an effective way to model reuse, model validation, model retrieval and pattern detection, which can help developers reduce development cost. Existing studies mainly focus on UML design models such as class diagrams and object diagrams. Yet, there is little research on calculating similarity for use case models despite their importance for software requirements reuse and validation. Thus, this paper proposes a similarity calculation approach for UML use case models. This approach first transforms the use case models into use case graphs and then adopts the deep learning technique — Similarity Graph Neural Network (SimGNN) to calculate the structural similarity between use case graphs. Then it calculates the semantic similarity of use case models with Term Frequency-Inverse Document Frequency (TF-IDF) and Cosine similarity. Finally it syntheses the results of semantic similarity and structural similarity of use case graphs. The effectiveness and efficiency of this approach was evaluated by comparing to two other approaches: (1) Cosine similarity+TF-IDF and (2) Cosine similarity+edit distance. The results showed that the our approach has better performance in terms of both effectiveness and efficiency.}
}
@article{ANDRZEJEWSKI20213657,
title = {Implementation of an example of Hierarchical Petri Net (HPN) in LAD language in TIA Portal},
journal = {Procedia Computer Science},
volume = {192},
pages = {3657-3666},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.139},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921018780},
author = {Grzegorz Andrzejewski and Wojciech Zając and Kazimierz Krzywicki and Artur Karasiński and Tomasz Królikowski and Błażej Bałasz},
keywords = {Hierarchical Petri Net, Petri Net, Discrete Process Control, LAD, TIA},
abstract = {In the paper there is described a process of implementation of an example of Hierarchical Petri Net (HPN) in Ladder Diagram, implemented with Totally Integrated Portal engineering software by Siemens. There is described the process of modeling the HPN for an example control problem, subsequent modeling steps are described, signals handling and system behavior is presented. The implementation process is described and discussed and a conclusion is drawn.}
}
@article{USMANIRANA2024e40744,
title = {Integrative bioinformatic analysis to identify potential phytochemical candidates for glioblastoma},
journal = {Heliyon},
volume = {10},
number = {24},
pages = {e40744},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e40744},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024167758},
author = {Hafiza Maria {Usmani Rana} and Haseeb Nisar and Jignesh Prajapati and Dweipayan Goswami and Ravi Rawat and Volkan Eyupoglu and Samiah Shahid and Anum Javaid and Wardah Nisar},
keywords = {Glioblastoma, Phytochemicals, Hub genes, Molecular docking, MD simulation},
abstract = {Glioblastoma (GBM) is one of the most malignant forms of cancer with the lowest survival ratio. Our study aims to utilize an integrated bioinformatic analysis to identify hub genes against GBM and explore the active phytochemicals with drug-like properties in treating GBM. The study employed databases of DisGenet, GeneCards, and Gene Expression Omnibus to retrieve GBM-associated genes, revealing 142 overlapping genes. Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment were used to analyze the role of these genes, which were involved in cancer-associated cell signaling pathways with tyrosine kinase activities and mainly enriched in the Nucleus. Furthermore, the hub genes identification through Cytoscape identified the top 10 ranked genes in a network, which were used as targets to dock against phytochemicals retrieved from the NPACT database having the ability to pass the blood-brain barrier and drug-likeness properties. The molecular docking and dynamics simulation studies predicted the binding of Isochaihulactone and VismioneB to the active site residues of EGFR and SRC genes. In contrast, Resveratrol binds to key residues of PIK3CA. Further, the binding free energy of the docked complex was calculated by performing MM-GBSA analysis, providing a detailed understanding of the underlying molecular interactions. The results offer interactional and structural insights into candidate phytochemicals towards GBM-associated top-ranked proteins. However, validation studies must be done through both in vitro and in vivo disease models to strengthen our computational results.}
}
@article{YI2025102105,
title = {Network pharmacology and experimental validation reveals the potential therapeutic effects of Polygonum cuspidatum against odontogenic keratocyst},
journal = {Journal of Stomatology Oral and Maxillofacial Surgery},
volume = {126},
number = {3},
pages = {102105},
year = {2025},
issn = {2468-7855},
doi = {https://doi.org/10.1016/j.jormas.2024.102105},
url = {https://www.sciencedirect.com/science/article/pii/S2468785524003938},
author = {Jing-Rui Yi and Bang Zeng and Jian-Feng Liu and Qi-Wen Man},
keywords = {Polygonum cuspidatum, Odontogenic keratocysts, Network pharmacology, Molecular docking},
abstract = {This study aimed to explore active ingredients in Polygonum cuspidatum with potential effects on odontogenic keratocysts (OKCs) using network pharmacological approach and bioinformatic gene analysis. The active ingredients and targets of P. cuspidatum were selected from the Traditional Chinese Medicine Systems Pharmacology Database and Analysis Platform (TCMSP) database, and the ingredient–target network was constructed using Cytoscape software. Differentially expressed genes (DEGs) of OKC were selected and Gene Ontology (GO) enrichment analysis were performed through bioinformatic analysis using Gene Expression Omnibus (GEO) dataset GSE38494. The STRING database platform was used to draw protein–protein interaction network diagram, then the hub gene analysis was performed by Cytoscape software. AutoDock Vina software was used to perform molecular docking verification of the effects of the active ingredients on potential core targets. Finally, we use OKC nude animal model to testify the potential effects of P. cuspidatum. Ten active ingredients of P. cuspidatum were obtained. A total of 205 drug targets and 38 potential core targets of P. cuspidatum were confirmed in OKCs. The hub genes included PPARG, SPP1, COL3A1, MMP2, HMOX1, CCL2, CXCL10, VCAM1, RUNX2 and IRF1. Molecular docking showed that the key active ingredients including luteolin and quercetin which exhibited good docking activity with key target proteins (VCAM1, HMOX1 and MMP2). GO enrichment revealed that the pathways of P. cuspidatum acting on OKCs included the response to toxic substance, response to nutrient levels and response to xenobiotic stimulus. P. cuspidatum treatment in OKC could significantly down-regulate COL3A1 and MMP2 expressions in vivo and vitro. Our study indicated that P. cuspidatum is a potential therapeutic candidate for OKCs.}
}
@article{MAYAMPURATH2025108376,
title = {Identification of neurological text markers associated with risk of stroke},
journal = {Journal of Stroke and Cerebrovascular Diseases},
volume = {34},
number = {8},
pages = {108376},
year = {2025},
issn = {1052-3057},
doi = {https://doi.org/10.1016/j.jstrokecerebrovasdis.2025.108376},
url = {https://www.sciencedirect.com/science/article/pii/S1052305725001545},
author = {Anoop Mayampurath and Avery Rosado and Elida Romo and Philip Silberman and Jay Patel and Samantha Jankowski and Matthew Maas and Jane L. Holl and Ava L. Liberman and Shyam Prabhakaran},
keywords = {Stroke, Diagnosis, Natural language processing, Neurological signs and symptoms},
abstract = {Background
Delayed or missed stroke diagnosis is associated with poor outcomes. We utilized natural language processing of notes from non-neurological emergency department (ED) encounters to identify text phrases indicating stroke presentations that are associated with stroke hospitalization 30 days after ED discharge.
Methods
We conducted a retrospective analysis of stroke (case) and gastroenteritis (matched-control) patients at two academic medical centers who had an ED encounter 30 days before index admission diagnosis. Medical concepts were extracted from the ED encounter notes. Statistical analysis was used to detect neurological text markers indicating stroke signs and symptoms using data from one hospital (discovery cohort) and validated in the second (validation cohort). We further compared the coefficients and the predictive performance of an elastic net model of both cohorts.
Results
We detected 58 medical concepts with a statistically significant positive association with stroke cases in the discovery cohort of 987 patients (51 % stroke). Expert review was used to combine these medical concepts into 11 text markers indicative of stroke presentations (e.g., coordination, language). Markers demonstrated external validity in terms of positive association when analyzed in the validation cohort of 433 patients (24 % stroke). Elastic net models derived at each center demonstrated equivalence in coefficient magnitudes and predictive performance, demonstrating generalizability.
Conclusion
We detected and validated neurologic text markers characteristic of stroke signs and symptoms at an ED encounter 30 days before the stroke diagnosis. The presence of these markers could be used to prompt additional neurologic evaluation to prevent delayed stroke diagnosis.}
}
@article{SIVAKUMAR2024124653,
title = {Prompting GPT–4 to support automatic safety case generation},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124653},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124653},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424015203},
author = {Mithila Sivakumar and Alvine B. Belle and Jinjun Shan and Kimya {Khakzad Shahandashti}},
keywords = {Safety cases, Safety assurance, Machine learning, Large language models, Generative AI, Requirements engineering},
abstract = {In the ever-evolving field of software engineering, the advent of large language models and conversational interfaces, exemplified by ChatGPT, represents a significant revolution. While their potential is evident in various domains, this paper expands upon our previous research, where we experimented with GPT–4, on its ability to create safety cases. A safety case is a structured argument supported by a body of evidence to demonstrate that a given system is safe to operate in a given environment. In this paper, we first determine GPT–4’s comprehension of the Goal Structuring Notation (GSN), a well-established notation for visually representing safety cases. Additionally, we conduct four distinct experiments using GPT–4 to evaluate its ability to generate safety cases within a specified system and application domain. To assess GPT–4’s performance in this context, we compare the results it produces with the ground-truth safety cases developed for an X-ray system, a machine learning-enabled component for tire noise recognition in a vehicle, and a lane management system from the automotive domain. This comparison enables us to gain valuable insights into the model’s generative capabilities. Our findings indicate that GPT–4 is able to generate moderately accurate and reasonable safety cases.}
}
@incollection{MANN20241279,
title = {Hybrid Artificial Intelligence-based Process Flowsheet Synthesis and Design using Extended SFILES Representation},
editor = {Flavio Manenti and Gintaras V. Reklaitis},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {53},
pages = {1279-1284},
year = {2024},
booktitle = {34th European Symposium on Computer Aided Process Engineering / 15th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-28824-1.50214-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443288241502143},
author = {Vipul Mann and Mauricio Sales-Cruz and Rafiqul Gani and Venkat Venkatasubramanian},
keywords = {process design, flowsheet modeling, artificial intelligence, computer-aided flowsheet synthesis},
abstract = {Process flowsheet synthesis and design involves simultaneously solving several problems, including determining the unit operations and their sequence, underlying reactions and reaction stoichiometry, downstream separation design and operation parameters, sustainability factors, and many more. Naturally, this results in a large amount of data being associated with a given process flowsheet that captures the relevant process context and should be readily accessible. This data is useful for solving related problems both using data-driven and process knowledge-based methods. A hierarchical framework, called the extended SFILES (or eSFILES), proposed recently stores this information using a combination of text-based, graph-based, and ontology-based representations. Here, we provide details on a prototype software for automated flowsheet representation and generation across various levels in the eSFILES framework. The underlying methods include a novel flowsheet grammar, a set of inferencing algorithms, and interfacing with a commercial process simulator facilitating rigorous flowsheet simulation.}
}
@article{LI2023102226,
title = {A graphical model for formalizing health maintenance activities in the context of the whole equipment lifecycle},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102226},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102226},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003543},
author = {Qingzong Li and Yuqian Yang and Maolin Yang and Pingyu Jiang},
keywords = {Maintenance, Graphical model, Event-state triggering mechanism, Maintenance activities formalizing, Bill of Materials},
abstract = {Formalizing health maintenance activities for the whole equipment lifecycle is significantly challenging in Maintenance, Repair, and Overhaul (MRO). It can support computer-aided managing and tracking maintenance activities, reducing the frequency of equipment failures and ensuring stable production operation, thereby reducing production costs and increasing enterprise profit. There are two problems formalizing health maintenance activities for the equipment lifecycle. The common formalizing modeling approaches, such as the Unified Modeling Language (UML), are readable and writable for computers but not friendly for human beings. There are various maintenance strategies adopted to assure equipment reliability in a real production scenario, and it is complex to formalize massive maintenance activities under different strategies. Thus, the research aims to propose a model for formalizing health maintenance activities that is friendly to both computer and human reading and writing and can manage and track massive maintenance activities under different strategies. In this regard, an event-state triggering mechanism-based graphical model for formalizing health maintenance activities in the context of the whole equipment lifecycle is proposed. First, the meta model of the Maintenance Graphical Model (MGM) based on the event-state triggering mechanism is introduced, clarifying the model unit and lifecycle model. Second, the Bill of Material (BOM) based configuration mechanism of the graphical model for specific equipment is presented. Third, the operation mechanism of the graphical model for managing and tracking the maintenance activities of equipment is illustrated. Finally, a real case of kneader, the production equipment of prebaked anode for aluminum electrolysis, is applied to verify our theory. The case demonstrates that the proposed model can reduce the frequency of equipment failures, maintenance time, and maintenance costs.}
}
@article{GARCIA2022,
title = {Integration and Open Access System Based on Semantic Technologies:},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.309422},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000394},
author = {Ana María Fermoso García and Maria Isabel Manzano García and Roberto Berjón Gallinas and Montserrat Mateos Sánchez and María Encarnación Beato Gutiérrez},
keywords = {CRIS (Current Research Information System), RIM (Research Information Management), CERIF (Current European Research Information Format), SPARQL Point, Ontology, Open Data, Information integration},
abstract = {ABSTRACT
The aim of this work is the development of an information system that, by integrating data from different sources and applying semantic technologies, makes it possible to publish and share with society the scientific production generated in the university environment, promoting its dissemination and thus contributing to the knowledge society, among others. In practice, this is the implementation of a CRIS (current research information system). This CRIS presents advanced features. On one hand it applies semantic technologies, providing a query service through a SPARQL Point, besides the reuse of shared data by exporting them in different formats. In this sense, it is also based on a European ontology or semantic standard such as CERIF, which facilitates its portability. On the other hand, CRIS also presents an alternative to the lack of a single data system by allowing data from different sources to be integrated and managed.}
}
@article{PANAGIOTOPOULOU2019,
title = {Conceptualizing Small and Medium-Sized Smart Cities in the Mediterranean Region:},
journal = {International Journal of E-Planning Research},
volume = {8},
number = {4},
year = {2019},
issn = {2160-9918},
doi = {https://doi.org/10.4018/IJEPR.2019100102},
url = {https://www.sciencedirect.com/science/article/pii/S2160991819000036},
author = {Maria Panagiotopoulou and Margarita Kokla and Anastasia Stratigea},
keywords = {Mediterranean Small and Medium-Sized Cities, Ontology, Semantic Exploration, Semantic Interoperability, Smart City, Sustainability Objectives, Urban Spatial Planning},
abstract = {ABSTRACT
Despite the remarkable interest in smart cities, noticed during the last decade, a consistent comprehension of the concept is not yet fully realized. Various definitions, ranging from exclusively technology-oriented perceptions to broader views, have been introduced, establishing a definitional polyphony and causing lack of semantic interoperability. Empirical evidence witnesses the prevalence of technology-pushed smart city initiatives as well as their failure to meet expectations in several urban domains. When planning “smart,” the relevance of ICT and their applications should be in alignment with spatial and other urban peculiarities and sub-systems’ interactions, implying the need for getting deep insight into the city’s ontology. The paper focuses on the extension/enrichment of an existing smart city ontology, with concepts and relationships stemming from Mediterranean small and medium-sized cities, in an attempt to outline their main key drivers and their interrelationships and fully grasp the smart city concept in the particular spatial context.}
}
@article{LIAN2024317,
title = {Hub genes, a diagnostic model, and immune infiltration based on ferroptosis-linked genes in schizophrenia},
journal = {IBRO Neuroscience Reports},
volume = {16},
pages = {317-328},
year = {2024},
issn = {2667-2421},
doi = {https://doi.org/10.1016/j.ibneur.2024.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S2667242124000071},
author = {Kun Lian and Yongmei Li and Wei Yang and Jing Ye and Hongbing Liu and Tianlan Wang and Guangya Yang and Yuqi Cheng and Xiufeng Xu},
keywords = {Schizophrenia, Ferroptosis, WGCNA, Hub gene, Diagnostic model, Immune infiltration},
abstract = {Background
Schizophrenia (SCZ) is a prevalent and serious mental disorder, and the exact pathophysiology of this condition is not fully understood. In previous studies, it has been proven that ferroprotein levels are high in SCZ. It has also been shown that this inflammatory response may modify fibromodulin. Accumulating evidence indicates a strong link between metabolism and ferroptosis. Therefore, the present study aims to identify ferroptosis‐linked hub genes to further investigate the role that ferroptosis plays in the development of SCZ.
Material and methods
From the GEO database, four microarray data sets on SCZ (GSE53987, GSE38481, GSE18312, and GSE38484) and ferroptosis‐linked genes were extracted. Using the prefrontal cortex expression matrix of SCZ patients and healthy individuals as the control group from GSE53987, weighted gene co‐expression network analysis (WGCNA) was performed to discover SCZ‐linked module genes. From the feed, genes associated with ferroptosis were retrieved. The intersection of the module and ferroptosis-linked genes was done to obtain the hub genes. Then, Gene Ontology (GO), Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway enrichment analyses, and Gene Set Enrichment Analysis (GSEA) were conducted. The SCZ diagnostic model was established using logistic regression, and the GSE38481, GSE18312, and GSE38484 data sets were used to validate the model. Finally, hub genes linked to immune infiltration were examined.
Results
A total of 13 SCZ module genes and 7 hub genes linked to ferroptosis were obtained: DECR1, GJA1, EFN2L2, PSAT1, SLC7A11, SOX2, and YAP1. The GO/KEGG/GSEA study indicated that these hub genes were predominantly enriched in mitochondria and lipid metabolism, oxidative stress, immunological inflammation, ferroptosis, Hippo signaling pathway, AMP‐activated protein kinase pathway, and other associated biological processes. The diagnostic model created using these hub genes was further confirmed using the data sets of three blood samples from patients with SCZ. The immune infiltration data showed that immune cell dysfunction enhanced ferroptosis and triggered SCZ.
Conclusion
In this study, seven critical genes that are strongly associated with ferroptosis in patients with SCZ were discovered, a valid clinical diagnostic model was built, and a novel therapeutic target for the treatment of SCZ was identified by the investigation of immune infiltration.}
}
@article{PANAITE2022,
title = {The Value of Extracting Clinician-Recorded Affect for Advancing Clinical Research on Depression: Proof-of-Concept Study Applying Natural Language Processing to Electronic Health Records},
journal = {JMIR Formative Research},
volume = {6},
number = {5},
year = {2022},
issn = {2561-326X},
doi = {https://doi.org/10.2196/34436},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X22005005},
author = {Vanessa Panaite and Andrew R Devendorf and Dezon Finch and Lina Bouayad and Stephen L Luther and Susan K Schultz},
keywords = {depression, affect, natural language processing, electronic health records, vocabularies},
abstract = {Background
Affective characteristics are associated with depression severity, course, and prognosis. Patients’ affect captured by clinicians during sessions may provide a rich source of information that more naturally aligns with the depression course and patient-desired depression outcomes.
Objective
In this paper, we propose an information extraction vocabulary used to pilot the feasibility and reliability of identifying clinician-recorded patient affective states in clinical notes from electronic health records.
Methods
Affect and mood were annotated in 147 clinical notes of 109 patients by 2 independent coders across 3 pilots. Intercoder discrepancies were settled by a third coder. This reference annotation set was used to test a proof-of-concept natural language processing (NLP) system using a named entity recognition approach.
Results
Concepts were frequently addressed in templated format and free text in clinical notes. Annotated data demonstrated that affective characteristics were identified in 87.8% (129/147) of the notes, while mood was identified in 97.3% (143/147) of the notes. The intercoder reliability was consistently good across the pilots (interannotator agreement [IAA] >70%). The final NLP system showed good reliability with the final reference annotation set (mood IAA=85.8%; affect IAA=80.9%).
Conclusions
Affect and mood can be reliably identified in clinician reports and are good targets for NLP. We discuss several next steps to expand on this proof of concept and the value of this research for depression clinical research.}
}
@article{LUGER2021151,
title = {Cafés, cocktail coves, and “empathy walls”: Comparing urban and exurban everyday life through a Lefebvrian lens},
journal = {Geoforum},
volume = {127},
pages = {151-161},
year = {2021},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2021.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0016718521002815},
author = {Jason Luger and Tilman Schwarze},
keywords = {Urban geography, Comparative urbanism, Political geography, Everyday life, Exurbs, Relationality},
abstract = {This article utilizes a Lefebvrian framework – specifically, his notions of “implosion and explosion”, the triad of the production of space, and his conceptions of “everyday life” and “oeuvre” – to comparatively engage two case studies alongside each other: urban South Chicago, and exurban North Carolina. Drawing from ethnographic (in-person and digital) observations and anecdotes, we suggest that these concepts are dynamically and flexibly applicable to the shifting terrains of urban and exurban relations and offer ontological pathways for productive comparison across difference. Conceptually, the article is also undergirded by Hochschild's (2016) notion of the “empathy wall”, a sociological barrier which divides polarized and socio-spatially- segregated geographies, within the context of recent urban and anti-urban insurrections and demonizations of one and other. These divides are viscerally evident in post-Trumpian America, but extend to many global contexts; thus, our comparison speaks to wider relevance. We argue that oppositional geographies like urban and exurban are inextricably linked and mutually constitutive, despite representing different sides of the “empathy wall” (in a political and cultural sense), and inhabiting distinct urban morphologies, geographical settings, historical lineages and political borders and boundaries. Furthermore, we suggest that such a relational pairing of urban alongside exurban is vital to overcome the seemingly insurmountable ontological, cultural and political borders between them. By reflecting on everyday life and social formations around ephemeral centers and sites such as cafés, backyard barbeques, lake parties and gyms, we offer some areas where dialogue and political solidarities might emerge across, and despite, the “empathy wall’s” steadfast insurmountability.}
}
@article{LI2023126583,
title = {Embracing ambiguity: Improving similarity-oriented tasks with contextual synonym knowledge},
journal = {Neurocomputing},
volume = {555},
pages = {126583},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126583},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223007063},
author = {Yangning Li and Jiaoyan Chen and Yinghui Li and Tianyu Yu and Xi Chen and Hai-Tao Zheng},
keywords = {Natural language processing, Pre-trained language model, Similarity-oriented tasks, Synonym knowledge enhancement},
abstract = {Contextual synonym knowledge is crucial for those similarity-oriented tasks whose core challenge lies in capturing semantic similarity between entities in their contexts, such as entity linking and entity matching. However, most Pre-trained Language Models (PLMs) lack synonym knowledge due to inherent limitations of their pre-training objectives such as masked language modeling (MLM). Existing works which inject synonym knowledge into PLMs often suffer from two severe problems: (i) Neglecting the ambiguity of synonyms, and (ii) Undermining semantic understanding of original PLMs, which is caused by inconsistency between the exact semantic similarity of the synonyms and the broad conceptual relevance learned from the original corpus. To address these issues, we propose PICSO, a flexible framework that supports the injection of contextual synonym knowledge from multiple domains into PLMs via a novel entity-aware Adapter which focuses on the semantics of the entities (synonyms) in the contexts. Meanwhile, PICSO stores the synonym knowledge in additional parameters of the Adapter structure, which prevents it from corrupting the semantic understanding of the original PLM. Extensive experiments demonstrate that PICSO can dramatically outperform the original PLMs and the other knowledge and synonym injection models on four different similarity-oriented tasks. In addition, experiments on GLUE prove that PICSO also benefits general natural language understanding tasks. Codes and data will be public.}
}
@article{SIMOULIN2023107693,
title = {From free‐text electronic health records to structured cohorts: Onconum, an innovative methodology for real‐world data mining in breast cancer},
journal = {Computer Methods and Programs in Biomedicine},
volume = {240},
pages = {107693},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107693},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723003589},
author = {Antoine Simoulin and Nicolas Thiebaut and Karl Neuberger and Issam Ibnouhsein and Nicolas Brunel and Raphaël Viné and Nicolas Bousquet and Jules Latapy and Nathalie Reix and Sébastien Molière and Massimo Lodi and Carole Mathelin},
keywords = {Breast cancer, Natural language processing, Electronic health record, Artificial intelligence, Dictionaries, Ontology},
abstract = {Purpose
A considerable amount of valuable information is present in electronic health records (EHRs) however it remains inaccessible because it is embedded into unstructured narrative documents that cannot be easily analyzed. We wanted to develop and evaluate a methodology able to extract and structure information from electronic health records in breast cancer.
Methods
We developed a software platform called Onconum (ClinicalTrials.gov Identifier: NCT02810093) which uses a hybrid method relying on machine learning approaches and rule-based lexical methods. It is based on natural language processing techniques that allows a targeted analysis of free-text medical data related to breast cancer, independently of any pre-existing dictionary, in a French context (available in N files). We then evaluated it on a validation cohort called Senometry.
Findings
Senometry cohort included 9,599 patients with breast cancer (both invasive and in situ), treated between 2000 and 2017 in the breast cancer unit of Strasbourg University Hospitals. Extraction rates ranged from 45 to 100%, depending on the type of each parameter. Precision of extracted information was 68%-94% compared to a structured cohort, and 89%-98% compared to manually structured databases and it retrieved more rare occurrences compared to another database search engine (+17%).
Interpretation
This innovative method can accurately structure relevant medical information embedded in EHRs in the context of breast cancer. Missing data handling is the main limitation of this method however multiple sources can be incorporated to reduce this limit. Nevertheless, this methodology does not need neither pre-existing dictionaries nor manually annotated corpora. It can therefore be easily implemented in non-English-speaking countries and in other diseases outside breast cancer, and it allows prospective inclusion of new patients.}
}
@article{BENSOUISSI2019103304,
title = {PARS, a system combining semantic technologies with multiple criteria decision aiding for supporting antibiotic prescriptions},
journal = {Journal of Biomedical Informatics},
volume = {99},
pages = {103304},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103304},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419302230},
author = {Souhir {Ben Souissi} and Mourad Abed and Lahcen {El Hiki} and Philippe Fortemps and Marc Pirlot},
keywords = {Clinical decision support system, Antibiotic prescription, Multiple criteria decision aiding, Ontology, Explanation},
abstract = {Objective
Motivated by the well documented worldwide spread of adverse drug events, as well as the increased danger of antibiotic resistance (caused mainly by inappropriate prescribing and overuse), we propose a novel recommendation system for antibiotic prescription (PARS).
Method
Our approach is based on the combination of semantic technologies with MCDA (Multiple Criteria Decision Aiding) that allowed us to build a two level decision support model. Given a specific domain, the approach assesses the adequacy of an alternative/action (prescription of antibiotic) for a specific subject (patient) with an issue (bacterial infection) in a given context (medical). The goal of the first level of the decision support model is to select the set of alternatives which have the potential to be suitable. Then the second level sorts the alternatives into categories according to their adequacy using an MCDA sorting method (MR–Sort with Veto) and a structured set of description logic queries.
Results
We applied this approach in the domain of antibiotic prescriptions, working closely with the EpiCura Hospital Center (BE). Its performance was compared to the EpiCura recommendation guidelines which are currently in use. The results showed that the proposed system is more consistent in its recommendations when compared with the static EpiCura guidelines. Moreover, with PARS the antibiotic prescribing workflow becomes more flexible. PARS allows the user (physician) to update incrementally and dynamically a patient’s profile with more information, or to input knowledge modifications that accommodate the decision context (like the introduction of new side effects and antibiotics, the development of germs that are resistant, etc). At the end of our evaluation, we detail a number of limitations of the current version of PARS and discuss future perspectives.}
}
@article{TIYYAGURA2022981,
title = {Development and Validation of a Natural Language Processing Tool to Identify Injuries in Infants Associated With Abuse},
journal = {Academic Pediatrics},
volume = {22},
number = {6},
pages = {981-988},
year = {2022},
issn = {1876-2859},
doi = {https://doi.org/10.1016/j.acap.2021.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S1876285921005404},
author = {Gunjan Tiyyagura and Andrea G. Asnes and John M. Leventhal and Eugene D. Shapiro and Marc Auerbach and Wei Teng and Emily Powers and Amy Thomas and Daniel M. Lindberg and Justin McClelland and Carol Kutryb and Thomas Polzin and Karen Daughtridge and Virginia Sevin and Allen L. Hsiao},
keywords = {child abuse, emergency department, natural language processing, test characteristics},
abstract = {Objectives
Medically minor but clinically important findings associated with physical child abuse, such as bruises in pre-mobile infants, may be identified by frontline clinicians yet the association of these injuries with child abuse is often not recognized, potentially allowing the abuse to continue and even to escalate. An accurate natural language processing (NLP) algorithm to identify high-risk injuries in electronic health record notes could improve detection and awareness of abuse. The objectives were to: 1) develop an NLP algorithm that accurately identifies injuries in infants associated with abuse and 2) determine the accuracy of this algorithm.
Methods
An NLP algorithm was designed to identify ten specific injuries known to be associated with physical abuse in infants. Iterative cycles of review identified inaccurate triggers, and coding of the algorithm was adjusted. The optimized NLP algorithm was applied to emergency department (ED) providers’ notes on 1344 consecutive sample of infants seen in 9 EDs over 3.5 months. Results were compared with review of the same notes conducted by a trained reviewer blind to the NLP results with discrepancies adjudicated by a child abuse expert.
Results
Among the 1344 encounters, 41 (3.1%) had one of the high-risk injuries. The NLP algorithm had a sensitivity and specificity of 92.7% (95% confidence interval [CI]: 79.0%–98.1%) and 98.1% (95% CI: 97.1%–98.7%), respectively, and positive and negative predictive values were 60.3% and 99.8%, respectively, for identifying high-risk injuries.
Conclusions
An NLP algorithm to identify infants with high-risk injuries in EDs has good accuracy and may be useful to aid clinicians in the identification of infants with injuries associated with child abuse.}
}
@article{MAHMOODPOUR2019283,
title = {A knowledge-based approach to the IoT-driven data integration of enterprises},
journal = {Procedia Manufacturing},
volume = {31},
pages = {283-289},
year = {2019},
note = {Research. Experience. Education. 9th Conference on Learning Factories 2019 (CLF 2019), Braunschweig, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.03.045},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919304093},
author = {Mehdi Mahmoodpour and Andrei Lobov},
keywords = {Internet of Things, Knowledge sharing, Learning factory, Ontology},
abstract = {Internet of Things (IoT) as a state-of-the-art technology has introduced businesses to new possibilities, thus allowing them to increase the efficiency and productivity of operational processes. Furthermore, the experiences gained by the employees of an organization can be shared among multiple corporations to facilitate the educational processes for employees through establishing learning environments within their businesses. In this study, we discuss the opportunities that IoT offers to businesses to integrate and share the massive amount of data generated by learning factories in enterprises as well as ongoing challenges in this domain. We further present the design and implementation of an ontology-based architecture for the development of IoT solution facilitating the collaborative business-to-business (B2B) knowledge sharing among enterprises to be used in their learning factory environments for educational matters. The proposed solution in this paper allows organizations to pursue their didactic purposes through the creation of an effective learning environment.}
}
@article{WOLK202262,
title = {Survey on dialogue systems including slavic languages},
journal = {Neurocomputing},
volume = {477},
pages = {62-84},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.11.076},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221017549},
author = {Krzysztof Wołk and Agnieszka Wołk and Dominika Wnuk and Tomasz Grześ and Ida Skubis},
keywords = {Slavic languages, Task-oriented dialogue systems, Non-task-oriented dialogue systems, Chatbots, Machine learning, Artificial intelligence},
abstract = {Slavic languages pose a challenge to the researchers in the domain of dialogue technology. A relatively free word order with a large degree of inflection, such as conjugation of verbs, and declension of adjectives, pronouns, and nouns are exhibited by the Slavic languages, which has a significant impact on the size of lexical inventories that significantly complicate the design of dialogue systems. This article conducts an empirical study on the state-of-the-art dialogue systems within Slavic languages. Moreover, we review the existing models in recent dialogue systems, pinpoint the current main challenges and identify potential research directions of practical and intelligent systems within low-resourced languages.}
}
@article{LUO2020447,
title = {Review of Natural Language Processing in Radiology},
journal = {Neuroimaging Clinics of North America},
volume = {30},
number = {4},
pages = {447-458},
year = {2020},
note = {Machine Learning and Other Artificial Intelligence Applications},
issn = {1052-5149},
doi = {https://doi.org/10.1016/j.nic.2020.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1052514920300563},
author = {Jack W. Luo and Jaron J.R. Chong},
keywords = {Natural language processing, Deep learning, Machine learning, Radiology, Artificial intelligence}
}
@article{KWABENA2023101935,
title = {An automated method for developing search strategies for systematic review using Natural Language Processing (NLP)},
journal = {MethodsX},
volume = {10},
pages = {101935},
year = {2023},
issn = {2215-0161},
doi = {https://doi.org/10.1016/j.mex.2022.101935},
url = {https://www.sciencedirect.com/science/article/pii/S2215016122003120},
author = {Antwi Effah Kwabena and Owusu-Banahene Wiafe and Boakye-Danquah John and Asare Bernard and Frimpong A.F. Boateng},
keywords = {Search Strategy, Search Terms, Data Deduplication, Software Implementation, Evidence Synthesis, Systematic Literature Review},
abstract = {The design and implementation of systematic reviews and meta-analyses are often hampered by high financial costs, significant time commitment, and biases due to researchers' familiarity with studies. We proposed and implemented a fast and standardized method for search term selection using Natural Language Processing (NLP) and co-occurrence networks to identify relevant search terms to reduce biases in conducting systematic reviews and meta-analyses.•The method was implemented using Python packaged dubbed Ananse, which is benchmarked on the search terms strategy for naïve search proposed by Grames et al. (2019) written in “R”. Ananse was applied to a case example towards finding search terms to implement a systematic literature review on cumulative effect studies on forest ecosystems.•The software automatically corrected and classified 100% of the duplicate articles identified by manual deduplication. Ananse was applied to the cumulative effects assessment case study, but it can serve as a general-purpose, open-source software system that can support extensive systematic reviews within a relatively short period with reduced biases.•Besides generating keywords, Ananse can act as middleware or a data converter for integrating multiple datasets into a database.}
}
@article{MAHASWA2025102801,
title = {Bioinspired technology and the uncanny Anthropocene},
journal = {Technology in Society},
volume = {81},
pages = {102801},
year = {2025},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102801},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X2400349X},
author = {Rangga Kala Mahaswa and Novan Gebbyano and  Hardiyanti},
keywords = {Bioinspired, Uncanny, Anthropocene, Technological mediation},
abstract = {This research explores the Anthropocene epoch, a period marked by significant environmental shifts due to human activity, which consequently brings forth a myriad of inherent risks. These risks are further complicated by a concurrent technological crisis, one that appears omnipresent and omnipotent, inducing a pervasive sense of disquiet. This ubiquitous crisis, paired with the stark reality of the Anthropocene, incites critical contemplation of our existing technological paradigms, particularly those of bioinspired technologies. Bioinspired technologies, which emulate nature's processes and structures, engender both promises and apprehensions. The uncanny phenomenon of naturalising technology, and in turn, technologising nature, disrupts our ontological comprehension of the human-technology-world mediation. This disconcerting symbiotic integration necessitates a philosophical examination of its implications and challenges within our Anthropocene epoch existence. To sufficiently navigate these complexities, our investigation transitions from focusing solely on bioinspired technologies to a consideration of Anthropocene-inspired approaches. In addition, in the face of the uncanny of the Anthropocene, such vagueness is critically interrupting. It not only reshapes the domain of bioinspired technologies but also challenges our understanding of the role of technology, our relationships with non-humans, and the potential futures we are forging within this Anthropocene world. This exploration extends an invitation to delve into these intricacies, encouraging a pursuit of ethically cognisant and ecologically responsible technological practises.}
}
@article{CAMARILLO2018115,
title = {Knowledge-based multi-agent system for manufacturing problem solving process in production plants},
journal = {Journal of Manufacturing Systems},
volume = {47},
pages = {115-127},
year = {2018},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612518300414},
author = {Alvaro Camarillo and José Ríos and Klaus-Dieter Althoff},
keywords = {Manufacturing Problem Solving (MPS), Case-Based Reasoning (CBR), Process Failure Mode and Effect Analysis (PFMEA), Ontology, Product Lifecycle Management (PLM)},
abstract = {This paper proposes a novel approach to develop a production-oriented software system aimed to assist shop floor actors during a Manufacturing Problem Solving (MPS) process. The proposed system integrates the problem-solving method 8D, Process Failure Mode and Effect Analysis (PFMEA), Case-Based Reasoning (CBR), and Product Lifecycle Management (PLM). The system is based on an ontology that enhances and extends existing proposals to allow representing any type of manufacturing problem linked to production lines and reusing PFMEA analysis results. The architecture of the system is based on SEASALT (Shared Experience using an Agent-based System Architecture LayouT), which is a multi-case base domain-independent reasoning architecture for extracting, analyzing, sharing, and providing experiences. A proof of concept prototype was developed, implemented, and tested in a company. The results, which were collected in two different manufacturing plants of the company, show the feasibility of the proposed approach and validate the conceptual proposal presented in this paper.}
}
@article{MECHNINE2025,
title = {In Silico Analysis and Validation of A Disintegrin and Metalloprotease (ADAM) 17 Gene Missense Variants: Structural Bioinformatics Study},
journal = {JMIR Bioinformatics and Biotechnology},
volume = {6},
year = {2025},
issn = {2563-3570},
doi = {https://doi.org/10.2196/72133},
url = {https://www.sciencedirect.com/science/article/pii/S2563357025000121},
author = {Abdelilah Mechnine and Asmae Saih and Lahcen Wakrim and Ahmed Aarab},
keywords = {bioinformatics, in silico, COVID-19, SARS-CoV-2, molecular modeling},
abstract = {Background
The protein A disintegrin and metalloprotease (ADAM) domain containing 17, also called tumor necrosis factor alpha–converting enzyme, is mainly responsible for cleaving a specific sequence Pro-Leu-Ala-Gln-Ala-/-Val-Arg-Ser-Ser-Ser in the membrane-bound precursor of tumor necrosis factor alpha. This cleavage process has significant implications for inflammatory and immune responses, and recent research indicates that genetic variants of ADAM17 may influence susceptibility to and severity of SARS-CoV-2 infection.
Objective
The aim of the study is to identify the most deleterious missense variants of ADAM17 that impact protein stability, structure, and function and to assess specific variants potentially involved in SARS-CoV-2 infection.
Methods
A bioinformatics approach was used on 12,042 single-nucleotide polymorphisms using tools including SIFT (Sorting Intolerant From Tolerant), PolyPhen2.0, PROVEAN (Protein Variation Effect Analyzer), PANTHER (Protein Analysis Through Evolutionary Relationships), SNP&GO (Single Nucleotide Polymorphisms and Gene Ontology), PhD-SNP (Predictor of Human Deleterious Single Nucleotide Polymorphisms), Mutation Assessor, SNAP2 (Screening for Non-Acceptable Polymorphisms 2), MUpro, I-Mutant, iStable, InterPro, Sparks-x, PROCHECK (Programs to Check the Stereochemical Quality of Protein Structures), PyMol, Project HOPE (Have (y)Our Protein Explained), ConSurf, and SWISS-MODEL. Missense variants of ADAM17 were collected from the Ensembl database for analysis.
Results
In total, 7 nonsynonymous single-nucleotide polymorphisms (P556L, G550D, V483A, G479E, G349E, T339P, and D232E) were identified as high-risk pathogenic by all prediction tools, and these variants were found to potentially have deleterious effects on the stability, structure, and function of the ADAM17 protein, potentially destroying the entire cleavage process. Additionally, 4 missense variants (Q658H, D657G, D654N, and F652L) in positions related to SARS-CoV-2 infection exhibited high conservation scores and were predicted to be deleterious, suggesting that they play an important role in SARS-CoV-2 infection.
Conclusions
Specific missense variants of ADAM17 are predicted to be highly pathogenic, potentially affecting protein stability and function and contributing to SARS-CoV-2 pathogenesis. These findings provide a basis for understanding their clinical relevance, aiding in early diagnosis, risk assessment, and therapeutic development.}
}
@article{FUCHS2024102735,
title = {Intermediate representations to improve the semantic parsing of building regulations},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102735},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102735},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003835},
author = {Stefan Fuchs and Johannes Dimyadi and Michael Witbrock and Robert Amor},
keywords = {Semantic parsing, Building regulation, Transformer, Intermediate representation, Automated compliance checking},
abstract = {Recent developments show that large transformer-based language models have the capability to generate coherent text and source code in response to user prompts. This capability can be used in the construction domain to interpret building regulations and convert them into a formal representation usable for automated compliance checking. While base-size models can already be taught to perform semantic parsing with decent quality, this paper shows how Intermediate Representations (IRs) can be used to improve the semantic parsing quality. With reversible IRs, the training time was reduced to almost a quarter of the initial duration, and through adding a hierarchical parsing step, improvements of up to 6.6% on F1 scores were reached. Furthermore, intermediate representations provide a novel and interpretable method towards a human-in-the-loop approach for translating building regulations into a formal representation.}
}
@article{POMMIER20191671403,
title = {Applying FAIR Principles to Plant Phenotypic Data Management in GnpIS},
journal = {Plant Phenomics},
volume = {2019},
pages = {1671403},
year = {2019},
issn = {2643-6515},
doi = {https://doi.org/10.34133/2019/1671403},
url = {https://www.sciencedirect.com/science/article/pii/S2643651524000025},
author = {C. Pommier and C. Michotey and G. Cornut and P. Roumet and E. Duchêne and R. Flores and A. Lebreton and M. Alaux and S. Durand and E. Kimmel and T. Letellier and G. Merceron and M. Laine and C. Guerche and M. Loaec and D. Steinbach and M.A. Laporte and E. Arnaud and H. Quesneville and A.F. Adam-Blondon},
abstract = {GnpIS is a data repository for plant phenomics that stores whole field and greenhouse experimental data including environment measures. It allows long-term access to datasets following the FAIR principles: Findable, Accessible, Interoperable, and Reusable, by using a flexible and original approach. It is based on a generic and ontology driven data model and an innovative software architecture that uncouples data integration, storage, and querying. It takes advantage of international standards including the Crop Ontology, MIAPPE, and the Breeding API. GnpIS allows handling data for a wide range of species and experiment types, including multiannual perennial plants experimental network or annual plant trials with either raw data, i.e., direct measures, or computed traits. It also ensures the integration and the interoperability among phenotyping datasets and with genotyping data. This is achieved through a careful curation and annotation of the key resources conducted in close collaboration with the communities providing data. Our repository follows the Open Science data publication principles by ensuring citability of each dataset. Finally, GnpIS compliance with international standards enables its interoperability with other data repositories hence allowing data links between phenotype and other data types. GnpIS can therefore contribute to emerging international federations of information systems.}
}
@article{LI2020101857,
title = {An intelligent semantic system for real-time demand response management of a thermal grid},
journal = {Sustainable Cities and Society},
volume = {52},
pages = {101857},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2019.101857},
url = {https://www.sciencedirect.com/science/article/pii/S2210670719316634},
author = {Yu Li and Yacine Rezgui and Sylvain Kubicki},
keywords = {Thermal grid, Demand response, Energy optimization, Operation cost, Data interoperability, Semantic ontology},
abstract = {“Demand Response” energy management of thermal grids requires consideration of a wide range of factors at building and district level, supported by continuously calibrated simulation models that reflect real operation conditions. Moreover, cross-domain data interoperability between concepts used by the numerous hardware and software is essential, in terms of Terminology, Metadata, Meaning and Logic. This paper leverages domain ontology to map and align the semantic resources that underpin building and district energy management, with a focus on the optimization of a thermal grid informed by real-time energy demand. The intelligence of the system is derived from simulation-based optimization, informed by calibrated thermal models that predict the network’s energy demand to inform (near) real-time generation. The paper demonstrates that the use of semantics helps alleviate the endemic energy performance gap, as validated in a real district heating network where 36% reduction on operation cost and 43% reduction on CO2 emission were observed compared to baseline operational data.}
}
@article{CUTHBERTSON2020e94,
title = {Theory and application of research principles and philosophical underpinning for a study utilising interpretative phenomenological analysis},
journal = {Radiography},
volume = {26},
number = {2},
pages = {e94-e102},
year = {2020},
issn = {1078-8174},
doi = {https://doi.org/10.1016/j.radi.2019.11.092},
url = {https://www.sciencedirect.com/science/article/pii/S1078817419302548},
author = {L.M. Cuthbertson and Y.A. Robb and S. Blair},
keywords = {Qualitative Research, Interpretative Phenomenological Analysis, Philosophical Assumptions, Ontology, Epistemology, Methodology},
abstract = {Introduction
Qualitative research approaches have potential to provide unique and valuable insights intoperceptions, experiences and behaviours. Reports however indicate that papers often fail to sufficiently detail the underlying principles that explain the philosophical assumptions and ontological, epistemological and methodological perspectives. Primarily directed towards radiographers considering a qualitative approach for doctoral research, this paper will also be informative for other health and social care practitioners.
Method
Part 1 discusses research principles broadly and how philosophical assumptions can be used for selection of the approach and methodology to explore a particular topic. Part 2 provides a worked example applied in context for a qualitative approach utilising Interpretative Phenomenological Analysis (IPA), that explored perceptions and experiences of lived experiences of radiographers on a journey to advanced practice in skeletal trauma reporting.
Results
The paper identifies the need for understanding of the theory and application of research principles broadly. Explanation and justification of choice is expected for the selection of research approach, paradigm, philosophical underpinning, underlying assumptions and methodology, to best answer the research question and inform participant selection, data collection, data analysis and interpretation methods
Conclusion
Coherent research requires synthesis of ontology, epistemology and methodology with the choice of research design based on the most appropriate approach. Qualitative research has greatly enhanced its reputation for methodological rigour and the uptake of IPA is increasing within health and social care.
Implications for practice
Future research within the Interpretative paradigm, utilising IPA as a methodology has potential to expand the body of evidence for Radiography research.}
}
@article{WANG2021,
title = {Pathway-Driven Coordinated Telehealth System for Management of Patients With Single or Multiple Chronic Diseases in China: System Development and Retrospective Study},
journal = {JMIR Medical Informatics},
volume = {9},
number = {5},
year = {2021},
issn = {2291-9694},
doi = {https://doi.org/10.2196/27228},
url = {https://www.sciencedirect.com/science/article/pii/S2291969421001940},
author = {Zheyu Wang and Jiye An and Hui Lin and Jiaqiang Zhou and Fang Liu and Juan Chen and Huilong Duan and Ning Deng},
keywords = {chronic disease, telehealth system, integrated care, pathway, ontology},
abstract = {Background
Integrated care enhanced with information technology has emerged as a means to transform health services to meet the long-term care needs of patients with chronic diseases. However, the feasibility of applying integrated care to the emerging “three-manager” mode in China remains to be explored. Moreover, few studies have attempted to integrate multiple types of chronic diseases into a single system.
Objective
The aim of this study was to develop a coordinated telehealth system that addresses the existing challenges of the “three-manager” mode in China while supporting the management of single or multiple chronic diseases.
Methods
The system was designed based on a tailored integrated care model. The model was constructed at the individual scale, mainly focusing on specifying the involved roles and responsibilities through a universal care pathway. A custom ontology was developed to represent the knowledge contained in the model. The system consists of a service engine for data storage and decision support, as well as different forms of clients for care providers and patients. Currently, the system supports management of three single chronic diseases (hypertension, type 2 diabetes mellitus, and chronic obstructive pulmonary disease) and one type of multiple chronic conditions (hypertension with type 2 diabetes mellitus). A retrospective study was performed based on the long-term observational data extracted from the database to evaluate system usability, treatment effect, and quality of care.
Results
The retrospective analysis involved 6964 patients with chronic diseases and 249 care providers who have registered in our system since its deployment in 2015. A total of 519,598 self-monitoring records have been submitted by the patients. The engine could generate different types of records regularly based on the specific care pathway. Results of the comparison tests and causal inference showed that a part of patient outcomes improved after receiving management through the system, especially the systolic blood pressure of patients with hypertension (P<.001 in all comparison tests and an approximately 5 mmHg decrease after intervention via causal inference). A regional case study showed that the work efficiency of care providers differed among individuals.
Conclusions
Our system has potential to provide effective management support for single or multiple chronic conditions simultaneously. The tailored closed-loop care pathway was feasible and effective under the “three-manager” mode in China. One direction for future work is to introduce advanced artificial intelligence techniques to construct a more personalized care pathway.}
}
@article{ALI2025103400,
title = {Deep-CABPred: Deep learning model for predicting functional chlorophyll a-b binding proteins in trait-based plant ecology using hybrid embedding with semi-normalized temporal convolutional networks},
journal = {Ecological Informatics},
volume = {91},
pages = {103400},
year = {2025},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2025.103400},
url = {https://www.sciencedirect.com/science/article/pii/S1574954125004091},
author = {Farman Ali and Raed Alsini and Tamim Alkhalifah and Fahad Alturise and Wajdi Alghamdi and Majdi Khalid},
keywords = {Chlorophyll a-b binding proteins, Deep learning, Machine learning, Semi-normalized temporal convolutional network},
abstract = {Chlorophyll a-b binding proteins (CABs) are crucial for photosynthesis, directly influencing plant efficiency and environmental adaptation. Identifying these proteins is vital for understanding ecological function and productivity, but traditional experimental methods are laborious. To overcome this, we developed Deep-CABPred, a novel deep learning model for efficient CAB prediction. Our approach leverages a combination of advanced feature embedding techniques. We extract multi-source protein information from primary sequences, employing FastText for discriminative subword-level patterns and ProtBERT (a protein large language model) for contextualized sequential features. These independently extracted features are then fused into a comprehensive representation. This fused feature set is subsequently integrated into a Semi-Normalized Temporal Convolutional Network (SN-TCN) for model training. Deep-CABPred's performance was rigorously validated using a five-fold cross-validation strategy, achieving impressive accuracies of 88.60 % on the training dataset and 83.68 % on the testing dataset. This model offers an effective computational solution for CAB prediction and holds significant potential for advancing our understanding of plant functional traits, ultimately supporting agricultural and conservation efforts in a changing climate.}
}
@article{XU2019245,
title = {A prediction method of building seismic loss based on BIM and FEMA P-58},
journal = {Automation in Construction},
volume = {102},
pages = {245-257},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S092658051830760X},
author = {Zhen Xu and Huazhen Zhang and Xinzheng Lu and Yongjia Xu and Zongcai Zhang and Yi Li},
keywords = {Seismic loss, BIM, FEMA P-58, Component level, Ontology},
abstract = {Predicting the seismic loss of a building is critical for its resilience. A prediction method for building seismic loss based on the building information model (BIM) and FEMA P-58 is proposed in this study. First, a component-level damage prediction algorithm is designed to establish the mapping from BIM components to the performance groups (PGs) in FEMA P-58, and to predict the component damage using the BIM-based time-history analysis (THA) and the fragility curves of PGs. Subsequently, an ontology-based model considering the deduction rules in the local unit-repair-cost database is created for obtaining exact measurement data of components in a BIM. Meanwhile, a component-level loss prediction algorithm is developed using the measurement data and the unit repair costs corresponding to damage states, by which the predicted seismic losses can agree with the actual situation of the specific region. Finally, a component-level visualization algorithm is designed to display the seismic damage and loss in a virtual reality (VR) environment. A six-story office building in Beijing is used as a pilot test to demonstrate the advantages of the proposed method. The outcome of this study produces a component-level and visual loss prediction result that agrees with the actual situation of the specific region, which can be used to evaluate the post-earthquake economic resilience of different buildings.}
}
@article{ARIZA202114,
title = {IoT architecture for adaptation to transient devices},
journal = {Journal of Parallel and Distributed Computing},
volume = {148},
pages = {14-30},
year = {2021},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0743731520303737},
author = {Jairo Ariza and Kelly Garcés and Nicolás Cardozo and Juan Pablo Rodríguez Sánchez and Fernando Jiménez Vargas},
keywords = {Internet of Things, Instance matching, Transient systems, Dynamic adaptation},
abstract = {IoT environments are continuously changing. Changes may come from the service, connectivity, or physical layers of the IoT architecture. Therefore, to function appropriately, the system needs to dynamically adapt to its environment. In previous work, we posited eight challenges to foster adaptation through all architecture layers of IoT systems. In this paper, we address the challenges to manage the inclusion of new devices and devices’ transient connection, by means of dynamic adaptations incorporated into our proposed software architecture for adaptive IoT systems. To manage dynamic adaptations, we extend the reference IoT architecture with our specialized components. In particular, we use (1) ontologies and instances to represent the domain knowledge; (2) a matching algorithm to pair services and IoT devices, taking into account their functional requirements, quality attributes and sensors properties; and (3) a match update algorithm used whenever sensors become (un)available. We evaluate the effectiveness of our solution with respect to the accuracy of matching services and IoT devices, and the response to environment changes.}
}
@article{OWEN2024,
title = {AI for Analyzing Mental Health Disorders Among Social Media Users: Quarter-Century Narrative Review of Progress and Challenges},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/59225},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124007933},
author = {David Owen and Amy J Lynham and Sophie E Smart and Antonio F Pardiñas and Jose {Camacho Collados}},
keywords = {mental health, depression, anxiety, schizophrenia, social media, natural language processing, narrative review},
abstract = {Background
Mental health disorders are currently the main contributor to poor quality of life and years lived with disability. Symptoms common to many mental health disorders lead to impairments or changes in the use of language, which are observable in the routine use of social media. Detection of these linguistic cues has been explored throughout the last quarter century, but interest and methodological development have burgeoned following the COVID-19 pandemic. The next decade may see the development of reliable methods for predicting mental health status using social media data. This might have implications for clinical practice and public health policy, particularly in the context of early intervention in mental health care.
Objective
This study aims to examine the state of the art in methods for predicting mental health statuses of social media users. Our focus is the development of artificial intelligence–driven methods, particularly natural language processing, for analyzing large volumes of written text. This study details constraints affecting research in this area. These include the dearth of high-quality public datasets for methodological benchmarking and the need to adopt ethical and privacy frameworks acknowledging the stigma experienced by those with a mental illness.
Methods
A Google Scholar search yielded peer-reviewed articles dated between 1999 and 2024. We manually grouped the articles by 4 primary areas of interest: datasets on social media and mental health, methods for predicting mental health status, longitudinal analyses of mental health, and ethical aspects of the data and analysis of mental health. Selected articles from these groups formed our narrative review.
Results
Larger datasets with precise dates of participants’ diagnoses are needed to support the development of methods for predicting mental health status, particularly in severe disorders such as schizophrenia. Inviting users to donate their social media data for research purposes could help overcome widespread ethical and privacy concerns. In any event, multimodal methods for predicting mental health status appear likely to provide advancements that may not be achievable using natural language processing alone.
Conclusions
Multimodal methods for predicting mental health status from voice, image, and video-based social media data need to be further developed before they may be considered for adoption in health care, medical support, or as consumer-facing products. Such methods are likely to garner greater public confidence in their efficacy than those that rely on text alone. To achieve this, more high-quality social media datasets need to be made available and privacy concerns regarding the use of these data must be formally addressed. A social media platform feature that invites users to share their data upon publication is a possible solution. Finally, a review of literature studying the effects of social media use on a user’s depression and anxiety is merited.}
}
@article{KAYES2020307,
title = {Achieving security scalability and flexibility using Fog-Based Context-Aware Access Control},
journal = {Future Generation Computer Systems},
volume = {107},
pages = {307-323},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19323349},
author = {A.S.M. Kayes and Wenny Rahayu and Paul Watters and Mamoun Alazab and Tharam Dillon and Elizabeth Chang},
keywords = {Access control, Fog computing, Cloud computing, Security, Privacy, Cybercrime, Internet of Things},
abstract = {In the cyberspace environment, access control is one of the foremost fundamental safeguards used to prevent unauthorized access and to minimize the impact from security breaches. Fog computing preserves many benefits for the integration of both internet of things (IoT) and cloud computing platforms. Security in Fog computing environment remains a significant concern among practitioners from academia and industry. The current existing access control models, like the traditional Context-Aware Access Control (CAAC), are limited to access data from centralized sources, and not robust due to lack of semantics and cloud-based service. This major concern has not been addressed in the literature, also literature still lacks a practical solution to control fog data view from multiple sources. This paper critically reviews and investigates the limitations of current fog-based access control. It considers the trade-off between latency and processing overheads which has not been thoroughly studied before. In this paper, a new generation of Fog-Based Context-Aware Access Control (FB-CAAC) framework is proposed to enable flexible access control data from multiple sources. To fill the gap in the literature this paper introduces (i) a general data model and its associated mapping model to collate data from multiple sources. (ii) a data view model to provide an integrated result to the users, dealing with the privacy requirements of the associated stakeholders, (iii) a unified set of CAAC policies with an access controller to reduce both administrative and processing overheads, and (iv) a data ontology to represent the common classes in the relevant data sets. The applicability of FB-CAAC proposal is demonstrated via a walkthrough of the entire mechanism along with several case studies and a prototype testing. The results show the efficiency, flexibility, effectiveness, and practicality of FB-CAAC for data access control in fog computing environment.}
}
@article{WANG2022107639,
title = {Prediction of the disease causal genes based on heterogeneous network and multi-feature combination method},
journal = {Computational Biology and Chemistry},
volume = {97},
pages = {107639},
year = {2022},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2022.107639},
url = {https://www.sciencedirect.com/science/article/pii/S1476927122000196},
author = {Lexiang Wang and Mingxiao Wu and Yulin Wu and Xiaofeng Zhang and Sen Li and Ming He and Fan Zhang and Yadong Wang and Junyi Li},
keywords = {Disease-causative genes, Heterogeneous network, Link prediction, Structure features, Embedding features, Feature combination},
abstract = {At present, the prediction of disease causal genes is mainly based on heterogeneous. Research shows that heterogeneous network contains more information and have better prediction results. In this paper, we constructed a heterogeneous network including four node types of disease, gene, phenotype and gene ontology. On this basis, we use a machine learning algorithm to predict disease-causing genes. The algorithm is divided into three steps: preprocess and training sample extraction, features extraction and combination, model training and prediction. In the process of feature extraction and combination, by using network representation method, the representation vectors of nodes are generated as the embedding features of the nodes. We also extracted the structural features of each node in the network and then the embedding features and structure features are combined. The results of training and prediction show that the prediction algorithm based on all features combined together achieves the best prediction performance. Moreover, the combination of each network representation method’s embedding features and structural features has also achieved performance improvement. In the process of training samples extraction, we propose three improvement directions according to the network structure and data set distribution. Firstly, a positive sample algorithm based on network connectivity is proposed, we try to keep the connectivity of the whole heterogeneous graph in the sampling process to avoid the negative impact of embedding features’ extraction. Moreover, the influence of sample sampling ratio on experimental results was tested in the range of 0–1 with step size of 0.1. The influence of different proportion of positive and negative samples on the results was also tested. These improvements are intended to enhance the balance and robustness of the method. When the positive sample ratio is 0.1 and the proportion of negative and positive samples is 3, the model achieves the optimal result, and its AUC value and accuracy are 0.9887% and 94.55%, respectively, which are significantly higher than other models.}
}
@article{HUSSAIN20191,
title = {Automated framework for classification and selection of software design patterns},
journal = {Applied Soft Computing},
volume = {75},
pages = {1-20},
year = {2019},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.10.049},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618306173},
author = {Shahid Hussain and Jacky Keung and Muhammad Khalid Sohail and Arif Ali Khan and Manzoor Ilahi},
keywords = {Design patterns, Design problems, Unsupervised learning, Text categorization, Feature selection, Supervised learning},
abstract = {Though, Unified Modeling Language (UML), Ontology, and Text categorization approaches have been used to automate the classification and selection of design pattern(s). However, there are certain issues such as time and effort for formal specification of new patterns, system context-awareness, and lack of knowledge which needs to be addressed. We propose a framework (i.e. Three-phase method) to discuss these issues, which can aid novice developers to organize and select the correct design pattern(s) for a given design problem in a systematic way. Subsequently, we propose an evaluation model to gauge the efficacy of the proposed framework via certain unsupervised learning techniques. We performed three case studies to describe the working procedure of the proposed framework in the context of three widely used design pattern catalogs and 103 design problems. We find the significant results of Fuzzy c-means and Partition Around Medoids (PAM) as compared to other unsupervised learning techniques. The promising results encourage the applicability of the proposed framework in terms of design patterns organization and selection with respect to a given design problem.}
}
@article{CAMARDI2018356,
title = {Engineering perfection. What does it mean for a system to be perfect?},
journal = {Chaos, Solitons & Fractals},
volume = {115},
pages = {356-361},
year = {2018},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2018.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S0960077918306258},
author = {Giovanni Camardi},
keywords = {Perfection, Information, Philosophy, Ontology, Modal logic}
}
@incollection{JAHNKE202381,
title = {A public good? The commodification of Māori language and culture in higher education},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {81-88},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.06014-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305060140},
author = {Huia T. Jahnke and Te Rina Warren},
keywords = {Māori education, Indigenous education, Indigeneity in higher education, Commodification of indigenous culture, Public good, Māori language, Appropriation},
abstract = {This chapter examines the notion of public good within Māori historical experiences of colonization and their effects on indigenous institutions and social systems through perpetuating ignorance as truth, myth creation (Hetaraka, 2022; Mikaere, 2011) or as Moana Jackson refers to as “myth-takes”. The way in which tertiary institutions exert their power and privilege to appropriate and commodify indigenous languages, culture and knowledge has increased in recent years. This includes the power to justify and define Māori cultural terms and concepts in te reo Māori within institutional discourses that are superficial, cosmetic and are invariably aimed at maintaining the status quo.}
}
@incollection{GUDIVADA2018403,
title = {Chapter 12 - Natural Language Core Tasks and Applications},
editor = {Venkat N. Gudivada and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {38},
pages = {403-428},
year = {2018},
booktitle = {Computational Analysis and Understanding of Natural Languages: Principles, Methods and Applications},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2018.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169716118300257},
author = {Venkat N. Gudivada},
keywords = {Language identification, Text segmentation, Word-sense disambiguation, Language modeling, Part of Speech tagging, Parsing, Named entity recognition, Word segmentation, Question-answering systems, Machine translation, Information extraction, Natural language user interfaces, Parsing},
abstract = {This chapter describes core tasks that are routinely performed on natural language texts. Some of these tasks are often considered as preprocessing and form the foundation for developing natural language processing applications. We begin with the need for annotated language corpora and discuss the following tasks: language identification, text and word segmentation, word-sense disambiguation, language modeling, Part of Speech (PoS) tagging, parsing, named entity recognition, machine translation, information extraction, text summarization, question-answering systems, and natural language user interfaces. By design, we describe these tasks at a conceptual level and provide pointers to relevant literature.}
}
@article{ZHONG20242082,
title = {Natural Language Processing Approaches in Industrial Maintenance: A Systematic Literature Review},
journal = {Procedia Computer Science},
volume = {232},
pages = {2082-2097},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002060},
author = {Keyi Zhong and Tom Jackson and Andrew West and Georgina Cosma},
keywords = {Artificial intelligence, AI, Natural language processing, NLP, Industrial maintenance, Systematic literature review},
abstract = {Industrial maintenance plays a crucial role in manufacturing by significantly reducing machine failure time and minimizing costs, especially in the revolution of Industry 4.0. Consequently, researchers and industrial engineers have continuously focused on this area. Manufacturing companies possess extensive maintenance reports or logs containing valuable textual information, which offers a new avenue for exploring effective industrial maintenance methods. Natural Language Processing (NLP), a subfield of Artificial Intelligence, has demonstrated remarkable potential in analyzing maintenance reports and achieving promising results in various tasks. This paper presents a comprehensive systematic literature review that specifically concentrates on the applications of NLP approaches employed in the field of industrial maintenance. Additionally, this review analyzed the datasets utilized in previous studies and the evaluation measures adopted, which can serve as a valuable resource for other researchers seeking potential solutions in maintenance. Furthermore, the paper discusses the challenges encountered in applying NLP to industrial maintenance and outlines future research directions in this domain. By conducting this systematic literature review, we provide a comprehensive understanding of the current state of NLP applications in industrial maintenance, identify gaps in the existing literature, and guide future research efforts in leveraging NLP techniques for enhanced maintenance practices.}
}
@article{SAH2022101126,
title = {Unbiased but ideologically unclear: Teacher beliefs about language practices of emergent bilingual students in the U.S.},
journal = {Linguistics and Education},
volume = {72},
pages = {101126},
year = {2022},
issn = {0898-5898},
doi = {https://doi.org/10.1016/j.linged.2022.101126},
url = {https://www.sciencedirect.com/science/article/pii/S0898589822001140},
author = {Pramod K. Sah and Huseyin Uysal},
keywords = {Bilingualism, Critical teachers, Emergent bilinguals, Home language, Teacher language ideologies},
abstract = {Language ideologies are influential on teaching practices and teachers’ beliefs and attitudes toward linguistically and culturally diverse students. This multiple case study investigates the beliefs and experiences of two content-area teachers and focuses on their language ideologies regarding the home language use of emergent bilingual (EB) students in the United States. We collected data from our interviews and informal meetings with them as well as observations of their participation in a workshop. Our cross-case analysis reveals that they seemed to have positive beliefs about tapping into the home language of EB students as a resource but were simultaneously guided by normative language ideologies. Although the teachers were aware of their power of negotiating and resisting normative language ideologies, they were guided by the liberal ideology of home languages, which states that they should be used with restriction. They did not seem to demonstrate the ideological clarity of critically incorporating home languages into teaching practices. We concur that there is a need for preparing teachers to be more critical to challenge taken-for-granted assumptions about EB’s use of language.}
}
@article{GUZMAN2020145036,
title = {Analysis of hepatic transcriptome modulation exerted by γ-conglutin from lupins in a streptozotocin-induced diabetes model},
journal = {Gene},
volume = {761},
pages = {145036},
year = {2020},
issn = {0378-1119},
doi = {https://doi.org/10.1016/j.gene.2020.145036},
url = {https://www.sciencedirect.com/science/article/pii/S0378111920307058},
author = {Tereso Jovany Guzmán and Belinda Vargas-Guerrero and Pedro Macedonio García-López and Carmen Magdalena Gurrola-Díaz},
keywords = {, Legumes, Nutraceuticals, Streptozotocin-induced diabetes, Gene expression profiling, DNA microarray, Plant proteins},
abstract = {Lupinus albus γ-conglutin is proposed to positively affect glucose metabolism through inhibition of hepatic glucose production and insulin-mimetic activity; however, the action mechanism is not entirely known. Besides, most studies had focused on its effect on molecular targets directly related to glucose metabolism, and few studies have investigated how γ-conglutin may affect the liver gene expression or if it plays a role in other metabolic processes. Therefore, we investigated the influence of γ-conglutin on the liver transcriptome of streptozotocin-induced diabetic rats using DNA microarrays, ontological analyses, and quantitative PCR. Of the 22,000 genes evaluated, 803 and 173 were downregulated and upregulated, respectively. The ontological analyses of the differentially expressed genes revealed that among others, the mitochondria, microtubules, cytoskeleton, and oxidoreductase activity terms were enriched, implying a possible role of γ-conglutin on autophagy. To corroborate the microarray results, we selected and quantified, by PCR, the expression of two genes associated with autophagy (Atg7 and Snx18) and found their expression augmented two and threefold, respectively; indicating a higher autophagy activity in animals treated with γ-conglutin. Although complementary studies are required, our findings indicate for the first time that the hypoglycaemic effects of γ-conglutin may involve an autophagy induction mechanism, a pivotal process for the preservation of cell physiology and glucose homeostasis.}
}
@article{CHEN2025388,
title = {Artificial intelligence and perspective for rare genetic kidney diseases},
journal = {Kidney International},
volume = {108},
number = {3},
pages = {388-393},
year = {2025},
issn = {0085-2538},
doi = {https://doi.org/10.1016/j.kint.2025.03.033},
url = {https://www.sciencedirect.com/science/article/pii/S0085253825004855},
author = {Xiaoyi Chen and Anita Burgun and Olivia Boyer and Bertrand Knebelmann and Nicolas Garcelon},
keywords = {artificial intelligence, clinical decision support, disease characterization, hypothesis generation, rare genetic kidney disease},
abstract = {The integration of big data and artificial intelligence (AI) has revolutionized biomedicine, enhancing our understanding of diseases and health care practices. Although AI has shown remarkable success in some medical fields, its application in nephrology faces challenges because of the complex disease mechanisms and intricate physiology. These obstacles are further compounded in rare diseases, affecting <1 in 2000 people, where data scarcity and clinical complexities create additional challenges for AI in accurate disease characterization and prediction. Rare kidney diseases encompass >150 different conditions, with significant clinical and genetic heterogeneity, posing unique challenges for AI applications. Embracing AI for rare kidney diseases is essential, not only for driving the discovery of novel genes, pathways, and mechanisms relevant to both rare and common diseases, but also for shortening the diagnostic odyssey faced by patients with rare conditions, a goal regarded as the most urgent and transformative need in rare disease care. Recent reviews highlight AI applications in nephrology, focusing on big data sources, decision support systems, imaging data, multi-omics integration, and genotype-phenotype analysis. This review explores the current landscape of AI in rare genetic kidney diseases, examining key challenges and advancements in disease characterization and clinical decision support, with an emphasis on hypothesis generation using unsupervised methods and generative AI. It shows how AI can empower physicians to interpret complex data sets, identify patterns, and generate insights that can lead to improved patient outcomes and innovative medical research for rare genetic kidney conditions.}
}
@article{CAI2025106289,
title = {Construction and application of characteristic townscape knowledge graph based on space gene theory},
journal = {Cities},
volume = {167},
pages = {106289},
year = {2025},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2025.106289},
url = {https://www.sciencedirect.com/science/article/pii/S0264275125005906},
author = {Tianyi Cai and Jin Duan},
keywords = {Characteristic townscape, Knowledge graph construction, Space gene identification},
abstract = {In the context of globalization, acceleration of urbanization, emergence of new technologies, and growing openness of the information society have collectively fostered an integrated development trend. However, deviations in the practice approaches of urban construction have led to the convergence of townscape and the erosion of local features. As cities serve as the bearer of civilization, this townscape convergence fundamentally hinders the transmission of cultural inheritance and threatens the preservation of human cultural diversity. Many successful explorations have been carried out globally. But the theoretical and methodological problems of cultural inheritance have led to the inability to fundamentally address the feature convergence in the newly built areas from the root. The proposal of Space Gene Theory and the emergence of knowledge graph (KG) technologies provide new ideas to solve it. Based on Space Gene Theory, we use the principle that the interaction of multiple elements of space, nature and socio-culture forms characteristic townscapes, to construct a KG. On this basis, we use computer algorithms to identify characteristic townscape genes and their associated features. This method shifts the perspective of townscape study from “built environment” to “underlying structure.” And it proposes construction and application approach of the first multimodal fusion KG in this field. It is of great value for the inheritance of characteristic townscapes worldwide and for advancing methods of space gene identification.}
}
@article{XU2022108586,
title = {Aspect-level sentiment classification based on attention-BiLSTM model and transfer learning},
journal = {Knowledge-Based Systems},
volume = {245},
pages = {108586},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108586},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122002623},
author = {Guixian Xu and Zixin Zhang and Ting Zhang and Shaona Yu and Yueting Meng and Sijin Chen},
keywords = {Sentiment classification, BiLSTM, Attention, Transfer learning},
abstract = {Aspect-level sentiment classification, a fine-grained sentiment analysis task which provides entire and intensive results, has been a research focus in recent years. However, the performance of neural network models is largely limited by the small scale of datasets for aspect-level sentiment classification due to the challenges to label such data. In this paper, we propose an aspect-level sentiment classification model based on Attention-Bidirectional Long Short-Term Memory (Attention-BiLSTM) model and transfer learning. Based on Attention-BiLSTM model, three models including Pre-training (PRET), Multitask learning (MTL), and Pre-training & Multitask learning (PRET+MTL) are proposed to transfer the knowledge obtained from document-level training of sentiment classification to aspect-level sentiment classification. Finally, the performance of the four models is verified on four datasets. Experiments show that proposed methods make up for the shortcomings of poor training of neural network models due to the small dataset of the aspect-level sentiment classification.}
}
@incollection{PANDA202599,
title = {6 - Role of knowledge graph-based methods in human—AI systems for automated driving},
editor = {Rajesh Kumar Dhanaraj and M. Nalini and Malathy Sathyamoorthy and Manar Mohaisen},
booktitle = {Knowledge Graph-Based Methods for Automated Driving},
publisher = {Elsevier},
pages = {99-118},
year = {2025},
isbn = {978-0-443-30040-0},
doi = {https://doi.org/10.1016/B978-0-443-30040-0.00006-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300400000060},
author = {Sankarsan Panda and Kiran Shrimant Kakade and M. Nalini and Sheshang Degadwala},
keywords = {Knowledge graph, AI systems, Autonomous driving, highway traffic, Information systems, Autonomous vehicles, modular autonomy, collision avoidance, Information management, Artificial intelligence, Machine learning},
abstract = {This article discusses the knowledge-driven autonomous driving technologies that are currently under development. The limits of the currently available autonomous driving systems are found in our analysis. Some of these problems are that these systems can be affected by data bias, it can be hard to deal with long-tail cases, and the results can’t be easily understood. As an alternative, knowledge-based methods that include the skills of reasoning, generalization, and learning that lasts a lifetime seem like they could be a good way to solve these issues. The study’s goal is to look into the most important parts of knowledge-driven autonomous driving, including the dataset and standard, the surroundings, and the driver agent. Many advanced artificial intelligence techniques, such as large language models, world models, neural modeling, and others, are used by these parts to make an autonomous driving system that is more complete, flexible, and smart. The goal of this study is to give ideas and suggestions for further research and real-world applications of self-driving cars. It will also organize and evaluate previous research in this area in an organized way. As well as sharing the newest and most useful open-source materials related to the subject at hand, the newest developments in knowledge-driven autonomous driving are discussed.}
}
@article{ELMOHADAB2020561,
title = {Automatic CV processing for scientific research using data mining algorithm},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {32},
number = {5},
pages = {561-567},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2018.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S1319157818302921},
author = {Mohamed {El Mohadab} and Belaid Bouikhalene and Said Safi},
keywords = {Scientific research, Natural language processing, Data mining, Decision aid, CV processing},
abstract = {To manage and measure the performance of scientific research at the university, managers or policymakers need synthetic indicators that are cleverly grouped in several indicators which aim to offer to the leaders the necessary tools, so as to improve scientific research. The governance of information system has posed a serious challenge for the leadership of universities especially through the use of decision aid. In order to improve the information system, especially scientific research, a study of automatizing curriculum vitae for researchers of different disciplines belonging to various research laboratories is discussed in this paper. The use of natural language processing with data mining classifier Decision Tree is presented in order to predict the field of work of each researcher. The choice of Decision Tree classifier among One Rule classifier and Naive Bayes classifier is not arbitrary; it is chosen by comparing performance metrics- such as Precision, Recall, F-measure, Correctly Classify Instance, Incorrectly Classify Instance, Kappa Statistic, Root Mean Squad Error, Relative Absolute Error, and Root Relative Squad Error.}
}
@article{FEUERRIEGEL201888,
title = {Long-term stock index forecasting based on text mining of regulatory disclosures},
journal = {Decision Support Systems},
volume = {112},
pages = {88-97},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618301052},
author = {Stefan Feuerriegel and Julius Gordon},
keywords = {Text mining, Natural language processing, Financial news, Financial forecasting, Stock index, Predictive analytics},
abstract = {Share valuations are known to adjust to new information entering the market, such as regulatory disclosures. We study whether the language of such news items can improve short-term and especially long-term (24 months) forecasts of stock indices. For this purpose, this work utilizes predictive models suited to high-dimensional data and specifically compares techniques for data-driven and knowledge-driven dimensionality reduction in order to avoid overfitting. Our experiments, based on 75,927 ad hoc announcements from 1996–2016, reveal the following results: in the long run, text-based models succeed in reducing forecast errors below baseline predictions from historic lags at a statistically significant level. Our research provides implications to business applications of decision-support in financial markets, especially given the growing prevalence of index ETFs (exchange traded funds).}
}
@article{SGOURIDIS2022102497,
title = {Visions before models: The ethos of energy modeling in an era of transition},
journal = {Energy Research & Social Science},
volume = {88},
pages = {102497},
year = {2022},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2022.102497},
url = {https://www.sciencedirect.com/science/article/pii/S2214629622000056},
author = {Sgouris Sgouridis and Christian Kimmich and Jordi Solé and Martin Černý and Melf-Hinrich Ehlers and Christian Kerschner},
keywords = {Energy modeling, Energy policy, Forecasting, Backcasting, Validation, Participatory modeling},
abstract = {Energy-Economy-Environment (E3) models feature prominently in energy policy and climate mitigation planning. Nevertheless, these models have a mixed track record when assessed retrospectively and exhibit biases that can make them counterproductive for prescriptive policy during transition. We argue that in times of energy transitions it is preferable to develop a vision of the desired future energy system rather than relying on techno-economic solutions based on simple objectives (e.g. lower carbon emissions). We support this argument through reasoned inference supported by historical examples. A critical appraisal of E3 modeling exercises highlights the biases, structural or implicit, favoring existing energy system modalities. As a result, if E3 models are uncritically used to formulate long-term energy policy, there is the risk of unintended or deliberate performativity preventing a radical transition. Given the significant learning-by-doing effects in reducing technology costs, the evolution of energy systems is path-dependent and reinforced by technology policy feedbacks. This is showcased by Germany's Energiewende. Therefore, it is preferable to prioritize a clear articulation of the vision for the future desired end-state which can be shared with stakeholders a priori. Then utilize models as exploratory tools for assessing the economics and scale of corresponding interventions. These should include focused technology policy that aims to commoditize relevant technical innovations through learning-by-doing and scale economies. Ideally such models should be open, exploratory, reflexive and incorporate the dynamics of innovation.}
}
@article{WANG2024110520,
title = {Integrating machine learning and robust optimization for new product development: A consumer and expert preference-based approach},
journal = {Computers & Industrial Engineering},
volume = {197},
pages = {110520},
year = {2024},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110520},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224006417},
author = {Zheng Wang and Huiran Liu and Xiaojun Fan and Tao Zhang},
keywords = {Preference, Information Usefulness, Group Decision-Making, Machine learning, Natural Language Processing},
abstract = {This study proposes a novel hybrid decision-making approach that integrates machine learning and robust optimization for product development and improvement. The main contributions are as follows: (1) A hybrid decision-making method that combines interval tuple transformation to integrate consumer and expert preferences, leveraging the strengths of product designers while meeting market demands; (2) A new approach for extracting and weighting consumer preferences based on perceived useful information, reducing errors from irrelevant and false data; (3) Integration of consumer preference information from similar products to extract key preference features and analyze competitiveness factors, ensuring the market adaptability of the method; (4) A data-driven robust model that addresses uncertainties in expert group decision-making by determining the uncertainty set of unit adjustment costs for experts based on consumer preferences, thus reducing biases in group decision-making opinions. These contributions provide a reliable decision support system for managers and are validated through experimental analysis of over 100,000 consumer reviews, demonstrating the effectiveness of the proposed method in accurately capturing market demand and enhancing product competitiveness. This research not only advances the theoretical discussion on the integration of consumer preferences but also offers practical insights for global brands and enterprises in their product development and improvement efforts.}
}
@article{RABHI2021209,
title = {Design of an innovative IT platform for analytics knowledge management},
journal = {Future Generation Computer Systems},
volume = {116},
pages = {209-219},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2032999X},
author = {Fethi A. Rabhi and Madhushi Bandara and Kun Lu and Saif Dewan},
keywords = {Predictive analytics, Knowledge management, Knowledge base, Semantic modelling, Ontologies},
abstract = {An organisation wishing to conduct data analytics to support day-to-day decision making often needs a system to help analysts represent and maintain knowledge about research variables, datasets or analytical models, and effectively determine the best combination to use when solving the problem at hand. Often, such knowledge is not explicitly captured by the organisation. To address this problem, this paper presents the design of an innovative Information Technology (IT) platform which enables data sharing between different analytics models and provides the ability to extend or customise models or data sources without necessarily involving the analysts who created them. It can make analytics knowledge readily available and modifiable for future use and problem-solving by analysts and other stakeholders. In the context of our work, we organise analytics knowledge around the concept of a research variable, which analysts often use when defining and proving a hypothesis. By focusing on such a concept, this platform is particularly suited to develop empirical data analytics applications in any domain. This paper presents the architecture of this platform, including the knowledge base and the Application Programming Interface (API) layer. Capabilities of this platform are illustrated through a software prototype and a use case on property price prediction across Sydney, Australia.}
}
@article{LIU2022101757,
title = {A novel Data-Driven framework based on BIM and knowledge graph for automatic model auditing and Quantity Take-off},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101757},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101757},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002154},
author = {Hao Liu and Jack C.P. Cheng and Vincent J.L. Gan and Shanjing Zhou},
keywords = {Building information modeling, Quantity take-off, BIM information quality, BIM model auditing, Knowledge graph embedding},
abstract = {Model auditing is a critical step before conducting Building Information Modeling (BIM)-based Quantity Take-off (QTO) because these models may contain various human errors and mistakes, leading to insufficient semantic information and inconsistent modeling style in BIM models. The traditional object-oriented approach has difficulties in representing unstructured BIM data (e.g., interrelationships), while rule-based methods involve tremendous human efforts to develop rule sets, lacking flexibility for different requirements. Therefore, this study aims to establish a novel data-driven framework based on BIM and knowledge graph (KG) to represent unstructured BIM data for automatic inferences of auditing results of BIM model mistakes. It starts by establishing a BIM-KG data model via identifying required information for auditing purposes. Subsequently, BIM data is automatically transformed into the BIM-KG representations, the embeddings of which are trained using a knowledge graph embedding model. Automatic mechanisms are then developed to utilize the computable embeddings to effectively identify mistake BIM elements. The framework is validated using illustrative examples and the results show that 100% mistake elements can be identified successfully without human intervention.}
}
@article{HUANG2023227,
title = {Knowledge production of university-industry collaboration in academic capitalism: An analysis based on Hoffman's framework},
journal = {Asian Journal of Social Science},
volume = {51},
number = {4},
pages = {227-236},
year = {2023},
issn = {1568-4849},
doi = {https://doi.org/10.1016/j.ajss.2023.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1568484923000345},
author = {Jinghui Huang and Kui Xiong},
keywords = {Academic capitalism, Industry-academia collaboration, Knowledge production, Sociology of knowledge, The Greater Bay Area of China},
abstract = {As academic capitalism promotes the transformation of knowledge production globally, both parties in the industry-academia collaboration are repositioned as knowledge creators in the new knowledge circuit, in which the knowledge production process becomes an issue to be explored in depth. Based on the analytical framework of Hoffman and the experience of River City in Guangdong Province-Hong Kong-Macao Greater Bay Area, we analyse the ontological, epistemological, and applicative knowledge practices of both industry and academia. Both parties acknowledge the inherent uncertainties in knowledge production. Risk discourse for knowledge practices reflects influences of market/market-like logic on industry-academia collaboration. Around the three dimensions, the contradictions revealed in their interactions are multiple. Accordingly, both parties seek consensus on knowledge outcomes by suspending ontological discussions of knowledge, conceding epistemological issues, and imagining applied consensus. Nonetheless, these acts hardly achieve the desired goal of knowledge advancement and create obstacles to the knowledge circuit.}
}
@article{HUSSAIN2020105701,
title = {Acquiring guideline-enabled data driven clinical knowledge model using formally verified refined knowledge acquisition method},
journal = {Computer Methods and Programs in Biomedicine},
volume = {197},
pages = {105701},
year = {2020},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105701},
url = {https://www.sciencedirect.com/science/article/pii/S0169260720315340},
author = {Maqbool Hussain and Muhammad Afzal and Khalid M. Malik and Taqdir Ali and Wajahat {Ali Khan} and Muhammad Irfan and Arif Jamshed and Sungyoung Lee},
keywords = {Knowledge acquisition, Clinical practice guidelines, Data driven knowledge acquisition, Cancer treatment plan, Clinical decision support system, Formal verification},
abstract = {Background and Objective: Validation and verification are the critical requirements for the knowledge acquisition method of the clinical decision support system (CDSS). After acquiring the medical knowledge from diverse sources, the rigorous validation and formal verification process are required before creating the final knowledge model. Previously, we have proposed a hybrid knowledge acquisition method with the support of a rigorous validation process for acquiring medical knowledge from clinical practice guidelines (CPGs) and patient data for the treatment of oral cavity cancer. However, due to lack of formal verification process, it involves various inconsistencies in knowledge relevant to the formalism of knowledge, conformance to CPGs, quality of knowledge, and complexities of knowledge acquisition artifacts. Methods: This paper presents the refined knowledge acquisition (ReKA) method, which uses the Z formal verification process. The ReKA method adopts the verification method and explores the mechanism of theorem proving using the Z notation. It enhances a hybrid knowledge acquisition method to thwart the inconsistencies using formal verification. Results: ReKA adds a set of nine additional criteria to be used to have a final valid refined clinical knowledge model. These criteria ensure the validity of the final knowledge model concerning formalism of knowledge, conformance to GPGs, quality of the knowledge, usage of stringent conditions and treatment plans, and inconsistencies possibly resulting from the complexities. Evaluation, using four medical knowledge acquisition scenarios, shows that newly added knowledge in CDSS due to the additional criteria by the ReKA method always produces a valid knowledge model. The final knowledge model was also evaluated with 1229 oral cavity patient cases, which outperformed with an accuracy of 72.57% compared to a similar approach with an accuracy of 69.7%. Furthermore, the ReKA method identified a set of decision paths (about 47.8%) in the existing approach, which results in a final knowledge model with low quality, non-conformed from standard CPGs. Conclusion: ReKA refined the hybrid knowledge acquisition method by discovering the missing steps in the current validation process at the acquisition stage. As a formally proven method, it always yields a valid knowledge model having high quality, supporting local practices, and influenced by standard CPGs. Furthermore, the final knowledge model obtained from ReKA also preserves the performance such as the accuracy of the individual source knowledge models.}
}
@article{YUAN2024570,
title = {Analysis of international publication trends in artificial intelligence in skin cancer},
journal = {Clinics in Dermatology},
volume = {42},
number = {6},
pages = {570-584},
year = {2024},
issn = {0738-081X},
doi = {https://doi.org/10.1016/j.clindermatol.2024.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0738081X24001810},
author = {Lu Yuan and Kai Jin and An Shao and Jia Feng and Caiping Shi and Juan Ye and Andrzej Grzybowski},
abstract = {Bibliometric methods were used to analyze publications on the use of artificial intelligence (AI) in skin cancer from 2010 to 2022, aiming to explore current publication trends and future directions. A comprehensive search using four terms, “artificial intelligence,” “machine learning,” “deep learning,” and “skin cancer,” was performed in the Web of Science database for original English language publications on AI in skin cancer from 2010 to 2022. We visually analyzed publication, citation, and coupling information, focusing on authors, countries and regions, publishing journals, institutions, and core keywords. The analysis of 989 publications revealed a consistent year-on-year increase in publications from 2010 to 2022 (0.51% versus 33.57%). The United States, India, and China emerged as the leading contributors. IEEE Access was identified as the most prolific journal in this area. Key journals and influential authors were highlighted. Examination of the top 10 most cited publications highlights the significant potential of AI in oncology. Co-citation network analysis identified four primary categories of classical literature on AI in skin tumors. Keyword analysis indicated that "melanoma," "classification," and "deep learning" were the most prevalent keywords, suggesting that deep learning for melanoma diagnosis and grading is the current research focus. The term “pigmented skin lesions” showed the strongest burst and longest duration, whereas “texture” was the latest emerging keyword. AI represents a rapidly growing area of research in skin cancer with the potential to significantly improve skin cancer management. Future research will likely focus on machine learning and deep learning technologies for screening and diagnostic purposes.}
}
@article{COTSAFTIS2023101199,
title = {Designing conditions for coexistence},
journal = {Design Studies},
volume = {87},
pages = {101199},
year = {2023},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2023.101199},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X23000406},
author = {Olivier Cotsaftis and Nina Williams and Gyungju Chyon and John Sadar and Daphne Mohajer {Va Pesaran} and Samuel Wines and Sarah Naarden}
}
@article{HELLWEG2022245,
title = {Knowledge graph for manufacturing cost estimation of gear shafts - a case study on the availability of product and manufacturing information in practice},
journal = {Procedia CIRP},
volume = {109},
pages = {245-250},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.244},
url = {https://www.sciencedirect.com/science/article/pii/S221282712200693X},
author = {Fynn Hellweg and Harry Brückmann and Thomas Beul and Constantin Mandel and Albert Albers},
keywords = {Manufacturing Cost Estimation, Product Development, Gear Hobbing, Ontology, Knowledge Graph, Semantic Technologies, Reference System Elements},
abstract = {Growing cost pressure forces companies to actively manage their product costs to secure profitability. Here, manufacturing cost estimation within product development estimates manufacturing and material costs. As most products are developed in generations, needed product and manufacturing information can origin from reference system elements (RSE), for example similar components of prior product generations. Problematically, this product and manufacturing information as well as the knowledge of its interrelation is often stored in an unstructured way, document based or at least not machine-readable. This makes manufacturing cost estimation an effortful, time consuming and mainly manual activity with low traceability, where a wide manufacturing knowledge is required. Trends in production, like new manufacturing processes and production systems further increase the need for manufacturing information and knowledge. Knowledge graphs as semantic technologies can improve the findability and reusability of reference system elements and enable automatic information processing. Within this research, cost estimation of research and development of a large automotive supplier was used as research environment. Guided by the model of PGE an ontology for the manufacturing cost estimation domain was developed. Then, a knowledge graph was instantiated based on product and manufacturing information from gear shafts of electric axles. A case study was carried out to evaluate process-specific cycle time calculation as exemplary use case of the knowledge graph. Process-specific cycle times are generally effortful estimated based on detailed manufacturing information and then used together with machine hourly rates to estimate manufacturing costs. Here, the structured and machine-readable manufacturing information of identified reference system elements is extracted from the knowledge graph to reduce the effort, increase the traceability and enable future automation. The case study shows exemplary, how a knowledge graph can support manufacturing cost estimation of gear shafts where product and manufacturing information is automatically identified using reference system elements.}
}
@article{AVILAGARZON2022,
title = {An Agent-Based Social Simulation for Citizenship Competences and Conflict Resolution Styles},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.306749},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000357},
author = {Cecilia Avila-Garzon and Manuel Balaguera and Valentina Tabares-Morales},
keywords = {Agent-Based Social Simulation, Citizenship Competences, Conflict Resolution Styles, NetLogo, Ontology},
abstract = {ABSTRACT
The development of citizenship competences plays an important role in a complex system like society. Thus, to analyze how such competences impact other contexts is a great challenge because this kind of study involves the work with people and the use of variables that depend on human behaviors. In this sense, many studies have highlighted the advantage of using simulation systems and tools. In particular, the agent-based social simulation field relies upon the Semantic Web to manage knowledge representation in social scenarios. This study focuses on how citizenship competences impact conflict resolution. Moreover, a simulation model in which citizens interact to resolve conflicts by considering citizenship competences and conflict resolution styles is also introduced. It was developed in NetLogo together with an extension that connects it with the ontology of competences. Results show that the higher interactions of citizens-conflicts, the higher level of citizenship competences, and the number of conflicts solved is higher when using citizenship competences.}
}
@article{PADIA2019100497,
title = {Knowledge graph fact prediction via knowledge-enriched tensor factorization},
journal = {Journal of Web Semantics},
volume = {59},
pages = {100497},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300046},
author = {Ankur Padia and Konstantinos Kalpakis and Francis Ferraro and Tim Finin},
keywords = {Knowledge graph, Knowledge graph embedding, Tensor decomposition, Tensor factorization, Representation learning, Fact prediction},
abstract = {We present a family of novel methods for embedding knowledge graphs into real-valued tensors. These tensor-based embeddings capture the ordered relations that are typical in the knowledge graphs represented by semantic web languages like RDF. Unlike many previous models, our methods can easily use prior background knowledge provided by users or extracted automatically from existing knowledge graphs. In addition to providing more robust methods for knowledge graph embedding, we provide a provably-convergent, linear tensor factorization algorithm. We demonstrate the efficacy of our models for the task of predicting new facts across eight different knowledge graphs, achieving between 5% and 50% relative improvement over existing state-of-the-art knowledge graph embedding techniques. Our empirical evaluation shows that all of the tensor decomposition models perform well when the average degree of an entity in a graph is high, with constraint-based models doing better on graphs with a small number of highly similar relations and regularization-based models dominating for graphs with relations of varying degrees of similarity.}
}
@incollection{PHILLIPS2023181,
title = {Children's participation in local curriculum-making},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {181-187},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.03029-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305030293},
author = {Louise G. Phillips},
keywords = {Children's participation, Children's rights, Collective will formation, Curriculum-making, Deliberative democracy, Emergent curriculum, Place-based curriculum, Posthuman curriculum, Rhizomatic curriculum},
abstract = {Curricula across the world is often designed for children by adults. Anyone regardless of age is more likely to participate in an activity if they have a say in what happens. If curriculum-making is activity of learning, how might children contribute ideas, perspectives, collectively decide and co-construct curriculum. Ontologies, theories and ideas which enable children's participation in local curriculum-making are discussed, including Indigenous ontologies, UN Convention of the Rights of the Child, Reggio Emilia approach, emergent curriculum, mosaic approach, national early childhood curricula, deliberative democracy, home and community learning, place-based education and posthumanism.}
}
@article{JARVENPAA2021435,
title = {Capability matchmaking software for rapid production system design and reconfiguration planning},
journal = {Procedia CIRP},
volume = {97},
pages = {435-440},
year = {2021},
note = {8th CIRP Conference of Assembly Technology and Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.264},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120314864},
author = {Eeva Järvenpää and Niko Siltala and Otto Hylli and Minna Lanz},
keywords = {Production system design, Production system reconfiguration, Capability matchmaking, Matchmaking software, Resource modelling, Ontology},
abstract = {Traditionally, the production system design and reconfiguration planning are manual processes, which rely heavily on the designers’ expertise and tacit knowledge to find feasible system configuration solutions. Rapid responsiveness of future production systems calls for new computer-aided intelligent design and planning solutions, that would reduce the time and effort put into system design, both in brownfield and greenfield scenarios. This paper describes the implementation of a capability matchmaking software, which automatizes the matchmaking between product requirements and resource capabilities. The interaction of the matchmaking system with external design and planning tools is explained and illustrated with a case example. The matchmaking approach supports production system design and reconfiguration planning by providing automatic means for checking if the existing system already fulfills the new product requirements, and for finding alternative resources and resource combinations to specific product requirements from large search spaces, e.g. from global resource catalogues.}
}
@incollection{WINTER202335,
title = {Critical curriculum analysis framework (CCAF)},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {35-48},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.03005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305030050},
author = {Christine Winter and David Hyatt},
keywords = {Analysis, Critical, Curriculum, Deconstruction, Discourse, Framework, Policy, Politics, Power, Text},
abstract = {This article provides a clear, flexible and accessible curriculum audit framework to support educators engaging in curriculum analysis. We understand curriculum as discourse, which, circulating through language, serves as modalities of power. The framework draws on a critical discourse studies (CDS) approach to deconstruct curricula, enabling their reconstruction in more socially equitable forms. The framework has four elements: curriculum context, curriculum deconstruction, curriculum per se and curriculum knowledge. We apply the framework to a school curriculum in England. In conclusion, we argue that an intelligent, pluralistic, just society requires curricula being subject to the reflective critical inquiry facilitated here.}
}
@article{SENE201818,
title = {Data mining for decision support with uncertainty on the airplane},
journal = {Data & Knowledge Engineering},
volume = {117},
pages = {18-36},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X1730071X},
author = {A. Sene and B. Kamsu-Foguem and P. Rumeau},
keywords = {Dempster-Shafer theory, Frequent pattern mining, Semantic reasoning, Decision support system, In-flight medical incidents},
abstract = {This study describes the formalization of the medical decision-making process under uncertainty underpinned by conditional preferences, the theory of evidence and the exploitation of high-utility patterns in data mining. To assist a decision maker, the medical process (clinical pathway) was implemented using a Conditional Preferences Base (CPB). Then for knowledge engineering, a Dempster-Shafer ontology integrating uncertainty underpinned by evidence theory was built. Beliefs from different sources are established with the use of data mining. The result is recorded in an In-flight Electronic Health Records (IEHR). The IEHR contains evidential items corresponding to the variables determining the management of medical incidents. Finally, to manage tolerance to uncertainty, a belief fusion algorithm was developed. There is an inherent risk in the practice of medicine that can affect the conditions of medical activities (diagnostic or therapeutic purposes). The management of uncertainty is also an integral part of decision-making processes in the medical field. Different models of medical decisions under uncertainty have been proposed. Much of the current literature on these models pays particular attention to health economics inspired by how to manage uncertainty in economic decisions. However, these models fail to consider the purely medical aspect of the decision that always remains poorly characterized. Besides, the models achieving interesting decision outcomes are those considering the patient's health variable and other variables such as the costs associated with the care services. These models are aimed at defining health policy (health economics) without a deep consideration for the uncertainty surrounding the medical practices and associated technologies. Our approach is to integrate the management of uncertainty into clinical reasoning models such as Clinical Pathway and to exploit the relationships between the determinants of incident management using data mining tools. To this end, how healthcare professionals see and conceive uncertainty has been investigated. This allowed for the identification of the characteristics determining people under uncertainty and to understand the different forms and representations of uncertainty. Furthermore, what an in-flight medical incident is and how its management is a decision under uncertainty issues was defined. This is the first phase of common data mining that will provide an evidential transaction basis. Subsequently an evidential and ontological reasoning to manage this uncertainty has been established in order to support decision making processes on the airplane.}
}
@article{HAO2023154890,
title = {High PGAP3 expression is associated with lymph node metastasis and low CD8+T cell in patients with HER2+ breast cancer},
journal = {Pathology - Research and Practice},
volume = {251},
pages = {154890},
year = {2023},
issn = {0344-0338},
doi = {https://doi.org/10.1016/j.prp.2023.154890},
url = {https://www.sciencedirect.com/science/article/pii/S0344033823005915},
author = {Na Hao and Mingyang Li and Jiachen Wang and Yichen Song and Yuelei Zhao and Ling Zhang and Xinyu Yang and Ligang Chen and Junchi Ma and Qingge Jia and Fang Sui},
keywords = {Breast cancer, PGAP3, Pathology, Prognosis},
abstract = {Background
Breast cancer (BC) stands as the most prevalent malignancy among women and ranks as the second most frequently diagnosed cancer globally among newly identified cases. Post-GPI attachment to proteins factor 3（PGAP3）was reported to involve in lipid remodeling. However, its specific role in breast cancer remains inadequately elucidated. Consequently, the principal objective of this study was to investigate the clinical significance of PGAP3 in breast cancer.
Methods
We conducted an extensive analysis using both public databases and our own sample cohort to assess the role of PGAP3 in breast cancer. Immunohistochemistry was employed to assess PGAP3 expression, immune markers, and the co-expression of PGAP3 with key susceptibility genes. Data analysis was performed using the R programming language.
Results
Our findings revealed that PGAP3 is significantly overexpressed in breast cancer, particularly in human epidermal growth factor 2 positive (HER2 +) breast cancer cases (p < 0.001). Co-expression analyses demonstrated a significant correlation between PGAP3 and susceptibility genes associated with breast cancer, including BRCA1, BRCA2, PALB2, ATM, CHEK2, RAD51C, and RAD51D (p < 0.05). Logistic regression analysis identified PGAP3 as a significant predictor of estrogen receptor (ER), progesterone receptor (PR), HER2, and lymph node metastasis status (p < 0.01). Furthermore, higher PGAP3 expression was associated with decreased infiltration of CD8 + T cells in breast cancer samples.
Conclusion
Our study sheds light on the clinical significance of PGAP3 in breast cancer. PGAP3 is not only overexpressed in breast cancer but also correlates with key susceptibility genes, lymph node metastasis, and CD8 + T cell infiltration. These findings provide valuable insights into the potential role of PGAP3 as a biomarker in breast cancer and may contribute to our understanding of the disease's pathogenesis.}
}
@article{BAYRAM2021687,
title = {The unknown knowns: a graph-based approach for temporal COVID-19 literature mining},
journal = {Online Information Review},
volume = {45},
number = {4},
pages = {687-708},
year = {2021},
issn = {1468-4527},
doi = {https://doi.org/10.1108/OIR-12-2020-0562},
url = {https://www.sciencedirect.com/science/article/pii/S1468452721000639},
author = {Ulya Bayram and Runia Roy and Aqil Assalil and Lamia BenHiba},
keywords = {COVID-19, Semantic graphs, Natural language processing, Link prediction, Machine learning},
abstract = {Purpose
The COVID-19 pandemic has sparked a remarkable volume of research literature, and scientists are increasingly in need of intelligent tools to cut through the noise and uncover relevant research directions. As a response, the authors propose a novel framework. In this framework, the authors develop a novel weighted semantic graph model to compress the research studies efficiently. Also, the authors present two analyses on this graph to propose alternative ways to uncover additional aspects of COVID-19 research.
Design/methodology/approach
The authors construct the semantic graph using state-of-the-art natural language processing (NLP) techniques on COVID-19 publication texts (>100,000 texts). Next, the authors conduct an evolutionary analysis to capture the changes in COVID-19 research across time. Finally, the authors apply a link prediction study to detect novel COVID-19 research directions that are so far undiscovered.
Findings
Findings reveal the success of the semantic graph in capturing scientific knowledge and its evolution. Meanwhile, the prediction experiments provide 79% accuracy on returning intelligible links, showing the reliability of the methods for predicting novel connections that could help scientists discover potential new directions.
Originality/value
To the authors’ knowledge, this is the first study to propose a holistic framework that includes encoding the scientific knowledge in a semantic graph, demonstrates an evolutionary examination of past and ongoing research and offers scientists with tools to generate new hypotheses and research directions through predictive modeling and deep machine learning techniques.}
}
@article{NISHANBAEV2020e00139,
title = {A web repository for geo-located 3D digital cultural heritage models},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {16},
pages = {e00139},
year = {2020},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2020.e00139},
url = {https://www.sciencedirect.com/science/article/pii/S221205481930058X},
author = {Ikrom Nishanbaev},
keywords = {Web repositories, 3D digital cultural heritage archives, 3D digital cultural heritage models, Long-term archiving, 3D visualization on the web},
abstract = {Recent advances in 3D surveying and web technologies have made a significant contribution to the digital conservation and dissemination of cultural heritage. 3D cultural heritage models are now a critical component in the cultural heritage conservation, which are also employed for other use cases in education, research, tourism, virtual and augmented reality. The World Wide Web is used as a primary medium for dissemination of 3D cultural heritage models, while databases including databases of web repositories for long-term archiving. This article aims to report a new methodology and a web repository to integrate maps, 3D models, and geospatial data such as geolocation. It can, therefore, be utilized for long-term archiving and visualization of geo-located 3D digital cultural heritage models on the Web. Unlike many previous related projects which developed the web repositories from the ground up, the web repository built using this methodology is based on free and open-source, easy-to-implement content management system namely KeystoneJS and other associated frameworks, which are reusable and completely extendable. It can, thereby, help cultural heritage organizations and cultural heritage professionals facilitate the rapid development of web repositories for geo-located 3D digital cultural heritage models, which can also be further extended per project requirements. While this methodology is presented for the cultural heritage domain in mind, in the long term it can be employed and extended for use in a wide range of domains such as archaeology, engineering, and geographic information systems (GIS) among others.}
}
@article{BEATTIE2020140,
title = {Human Dignity and Rights in the Context of Gender and the Sacramental Priesthood},
journal = {Interdisciplinary Journal for Religion and Transformation in Contemporary Society},
volume = {6},
number = {1},
pages = {140-157},
year = {2020},
issn = {2365-3140},
doi = {https://doi.org/10.30965/23642807-00601009},
url = {https://www.sciencedirect.com/science/article/pii/S2365314020000285},
author = {Tina Beattie},
keywords = {Baptism, dignity, gender, priesthood, rights, sacraments, women},
abstract = {This paper considers the question of women’s ordination to the sacramental priesthood in the context of human dignity and rights. Differentiating between two forms of ontological or intrinsic dignity – the universal dignity of the human being made in the imago Dei, and the particular dignity of those baptised into the imago Trinitatis – it argues that the refusal of ordination to women is a violation of baptismal dignity that constitutes a refusal of women’s rights. It analyses the arguments against women’s ordination and shows them to be based on a misreading of Thomas Aquinas, on the innovative concept of sexual complementarity which has replaced the earlier hierarchical model of sexual difference, and on appeals to mystery that might be better described as mystification. It concludes that the refusal to allow women to respond to the call to ordination is based on a modern form of essentialised sexual difference that is alien to the Catholic tradition and that violates Christological orthodoxy, insofar as it suggests that women are not able to image Christ.}
}
@article{ZHANG2022333,
title = {Screening of genes related to breast cancer prognosis based on the DO-UniBIC method},
journal = {The American Journal of the Medical Sciences},
volume = {364},
number = {3},
pages = {333-342},
year = {2022},
issn = {0002-9629},
doi = {https://doi.org/10.1016/j.amjms.2022.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0002962922001872},
author = {Fan Zhang and Yawei Zhang and Tingting Hou and Fangtao Ren and Xi Liu and Runan Zhao and Xinhong Zhang},
keywords = {Breast cancer, Prognosis, Differentially expressed gene, DO-UniBIC},
abstract = {Background
Early screening is the most effective way to control breast cancer. Due to the lack of accurate biomarkers, early diagnosis of breast cancer is still very difficult. Therefore, it is necessary to discover new candidate genes of breast cancer and improve the early diagnosis and prognosis.
Methods
A DO-UniBIC gene screening method was proposed. First, Disease Ontology (DO) analysis was used to screen out breast cancer related genes from differentially expressed genes, and then the UniBIC algorithm was used to find all gene clusters with the same changing trend based on the longest common subsequence. In addition, an eight-gene prognostic model was constructed to assess the prognostic risk of breast cancer patients.
Results
The prognostic analysis of the candidate genomes based on multivariate Cox proportional regression model revealed eight genes that were significantly related to prognosis. The eight genes were ACSL1, CD24, EMP1, JPH3, CAMK4, JUN, S100B and TP53AIP1. Among them, ACSL1 was a new potential breast cancer related gene screened by the DO-UniBIC method.
Conclusions
More comprehensive cancer-related genes can be screened based on the DO-UniBIC method, which can be used as the candidate gene set for prognostic analysis.}
}
@article{HURTADO2024111230,
title = {e-Science workflow: A semantic approach for airborne pollen prediction},
journal = {Knowledge-Based Systems},
volume = {284},
pages = {111230},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.111230},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123009802},
author = {Sandro Hurtado and María Luisa Antequera-Gómez and Cristóbal Barba-González and Antonio Picornell and Ismael Navas-Delgado},
keywords = {Big data analytics, Semantics, e-Science, Pollen prediction},
abstract = {Allergic rhinitis has become a global health problem in recent decades because airborne pollen is a primary trigger of this respiratory disorder. Moreover, pollinosis can exacerbate the symptoms of asthma and favour respiratory infections. Seasonal pollen trends and climatic circumstances (such as temperature, precipitation, relative humidity, wind speed and direction, and other variables) can impact daily airborne pollen concentrations, influencing local pollen emission and dispersion. Because of that, pollen monitoring and prediction are becoming more relevant to the urban population and scientific interest is put into them. Due to such tasks’ high volume of data, scientists are starting to use computational tools like workflows to automate and speed up the process. Furthermore, using the expert scientific domain is critical for improving the analysis, allowing, among others, a better workflow configuration and data provenance. As semantic web technologies have been revealed as an essential means for knowledge representation, we implemented this workflow information as an ontology using formats like RDF(S) and OWL. Consequently, this paper provides a semantic-enhanced e-Science workflow based on the TITAN framework for pollen forecasting analysis using meteorological data. Furthermore, a catalogue of components is developed on the TITAN framework, which allows the creation of different workflow versions. Two case studies of pollen prediction were developed to test the implementation of the aforementioned methodologies. Both were elaborated with airborne pollen data obtained in the city of Málaga (Spain). Still, one was elaborated for Platanus pollen type (narrow annual main pollination period), while the other was done for Amaranthaceae pollen type (extensive annual main pollination period). The predictions have been conducted using machine and deep learning algorithms like SARIMA or CNN-LSTM that intend to optimise the pollen prediction procedure depending on its stational and seasonal profile.}
}
@article{MOURTZIS20211668,
title = {A Methodology for the Assessment of Operator 4.0 Skills based on Sentiment Analysis and Augmented Reality},
journal = {Procedia CIRP},
volume = {104},
pages = {1668-1673},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.281},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121011793},
author = {Dimitris Mourtzis and John Angelopoulos and Vasilis Siatras and Nikos Panopoulos},
keywords = {Skills, Competencies, Operator 4.0, Augmented Reality, Natural Language Processing},
abstract = {Human-Cyber-Physical Systems are the key to the successful operation of manufacturing systems. Consequently, the need for adequate assessment of human operators and tracking of their skills and competencies evolution emerges. Additionally, the advances in digital technologies encourage the development of supportive and adaptive frameworks for the operation of flexible manufacturing systems. This paper presents an Augmented Reality based methodology for the detailed evaluation of human skills and competencies based on the processing of raw textual data with a Natural Language Processing algorithm, aiming at the provision of technician guidance. The developed framework is tested and validated in an industrial environment.}
}
@article{THANISCH2019116,
title = {Detecting measurement issues in SQL arithmetic expressions and aggregations},
journal = {Data & Knowledge Engineering},
volume = {122},
pages = {116-129},
year = {2019},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X1730589X},
author = {Peter Thanisch and Tapio Niemi and Jyrki Nummenmaa and Marko Niinimäki},
keywords = {SQL, Measurement scale and unit, Aggregation, Summarizability},
abstract = {Research on user errors in retrieving information from SQL databases has focused on erroneous syntax in the query language and erroneous semantics concerning the data model. In the present paper, we investigate a third source of error, namely erroneous aggregations that break the limitations imposed by the numerical properties of the data. An erroneous aggregation might arise because of the SQL programmer’s misunderstanding concerning those numerical properties, or because of a simple mistake. We show that for database queries in the SQL language, significant classes of erroneous aggregations can be detected by non-intrusive, off-line checking, using only a simple set of metadata rules that can be supplied by the data provider. We have implemented software that performs static checks on users’ SQL queries, looking for evidence of misunderstandings concerning the measurement properties of the numerical data.}
}
@incollection{KONIG2021413,
title = {Executable Simulation Model of the Liver},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {413-422},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11682-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383116824},
author = {Matthias König},
keywords = {COMBINE, Glucose, Liver, Metabolism, Replication, Reproducibility, SBML, SED-ML},
abstract = {To address the issue of reproducibility in computational modeling we developed the concept of an executable simulation model (EXSIMO). An EXSIMO combines model, data and code with the execution environment to run the computational analysis in an automated manner using tools from software engineering. Key components are (i) models, data and code for the computational analysis; (ii) tests for models, data and code; and (iii) an automation layer to run tests and execute the analysis. An EXSIMO combines version control, model, data, units, annotations, analysis, reports, execution environment, testing, continuous integration and release. We applied the concept to perform a replication study of a computational analysis of hepatic glucose metabolism in the liver. The corresponding EXSIMO is available from https://github.com/matthiaskoenig/exsimo.}
}
@incollection{KANZA2021129,
title = {Semantic Technologies in Drug Discovery},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {129-144},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11520-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012801238311520X},
author = {Samantha Kanza and Jeremy {Graham Frey}},
keywords = {Chemicals, Data integration, Drug discovery, Drugs, Drug-targets, Genomics, Human and computer readable data, Knowledgebases, Linked data, Molecular mechanisms, Ontologies, Pharmacogenomics, Proteomics, RDF, Semantic search, Semantic web, SPARQL},
abstract = {Semantic drug discovery has gained significant traction in recent years, with researchers becoming more aware that these technologies enable them to link together and query disparate datasets for information that cannot be extracted from a single dataset. This article provides a comprehensive reference source of the current knowledge available regarding Semantic Web technologies in drug discovery. The main aspects of Semantic Web technologies are explained, detailing the different ways in which they can be used in drug discovery. Over 1000 biomedical ontologies were reviewed as part of the work undertaken for this paper and 34 of the most relevant ontologies in the drug discovery field are categorized and described, followed by details of semantic applications and successes in drug discovery. Some core standards and guidelines have been established for sharing Semantic drug discovery data, both through making well established medical taxonomies available in a Semantic format, and by creating upper-level ontologies and guidelines for creating new ontologies in the biomedical domain. This article concludes that a majority of the prevalent ontologies in drug discovery follow these standards and provides advice for researchers wishing to use Semantic Web technologies in their drug discovery research.}
}
@article{ZHU2019671,
title = {A novel approach based on Neo4j for multi-constrained flexible job shop scheduling problem},
journal = {Computers & Industrial Engineering},
volume = {130},
pages = {671-686},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2019.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0360835219301585},
author = {Zhenwei Zhu and Xionghui Zhou and Kang Shao},
keywords = {Flexible job shop scheduling, Neo4j, Semantic graph, Precedence constraint, Stock constraint, Ant colony optimization},
abstract = {To accommodate the need for scheduling complex fabricated products manufacturing, this paper studies the flexible job shop scheduling problem with additional job precedence constraints, time constraints, and stock constraints. As a powerful graph database which deals with connected data and embraces relationships in flexible graphs, Neo4j is creatively introduced to tackle this problem. This paper proposes a semantic graph model which can not only represent the scheduling problem with extended constraints but also integrate the entire lifecycle data. In the semantic graph model, diverse specific data linked with semantic relationships are stored in Neo4j while the semantics of conceptual data model are recorded in the ontology. Based on Neo4j, a scheduling application framework incorporating graph database, semantic web and knowledge capture is also developed. By means of parsing the table header’s semantics, an automatic conversion mechanism is achieved between tabular data in Excel spreadsheets and graph data in Neo4j. Inspired by the similarity between ants finding food sources along paths scattered with pheromone trails and assigning operations on resources one by one in line with the time under the guidance of accumulated knowledge, a simulation-based ant colony algorithm is carried out to acquire a feasible and nearly optimal schedule solution.}
}
@article{HARRISON2019105058,
title = {EcoHealth and One Health: A theory-focused review in response to calls for convergence},
journal = {Environment International},
volume = {132},
pages = {105058},
year = {2019},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2019.105058},
url = {https://www.sciencedirect.com/science/article/pii/S0160412019305409},
author = {Sarah Harrison and Lucy Kivuti-Bitok and Alexandra Macmillan and Patricia Priest},
keywords = {EcoHealth, One Health, Theoretical approaches, Critical realism, Systems thinking, System dynamics modelling},
abstract = {Background
EcoHealth and One Health are two major approaches broadly aimed at understanding the links between human, animal, and environment health. There have been increasing calls for convergence between the two. If convergence is desired, greater clarity regarding the underlying theoretical assumptions of both approaches is required. This would also support integrated research to effectively address complex health issues at the human, animal and environment interface. To better understand the areas of overlap and alignment, we systematically compared and contrasted the theoretical assumptions of both approaches.
Objectives
We aimed to gain a more in-depth understanding of the ontological, epistemological and methodological underpinnings of EcoHealth and One Health in order to identify areas of difference and overlap, and consider the extent to which closer convergence between the two may be possible.
Methods
We undertook a scoping review of literature about the ontological, epistemological and methodological positions of EcoHealth and One Health, and analyzed these according to Lincoln, Lynham and Guba's paradigm framework.
Results
EcoHealth and One Health are both collaborative, systems-focused approaches at the human, animal, and ecosystem health interface. EcoHealth typically leans towards constructivist-leaning assumptions. Many consider this a necessary aspiration for One Health. However, in practice One Health remains dominated by the veterinary and medical disciplines that emphasize positivist-leaning assumptions.
Discussion
The aspirations of EcoHealth and One Health appear to overlap at the conceptual level, and may well warrant closer convergence. However, further shared discussions about their epistemological and ontological assumptions are needed to reconcile important theoretical differences, and to better guide scopes of practice. Critical realism may be a crucial theoretical meeting point. Systems thinking methods (with critical realist underpinnings), such as system dynamics modelling, are potentially useful methodologies for supporting convergent practice.}
}
@article{MEIER2025102420,
title = {An efficient, non-viral arrayed CRISPR screening platform for iPSC-derived myeloid and microglia models},
journal = {Stem Cell Reports},
volume = {20},
number = {3},
pages = {102420},
year = {2025},
issn = {2213-6711},
doi = {https://doi.org/10.1016/j.stemcr.2025.102420},
url = {https://www.sciencedirect.com/science/article/pii/S2213671125000244},
author = {Sonja Meier and Anne Sofie Gry Larsen and Florian Wanke and Nicolas Mercado and Arianna Mei and Livia Takacs and Eva Suszanna Mracsko and Ludovic Collin and Martin Kampmann and Filip Roudnicky and Ravi Jagasia},
keywords = {APOE, iPSC-derived microglia, CRISPR-Cas9 gene editing, arrayed genetic screening, lipid droplet screen, lipid metabolism, lipid accumulation, mTORC1, lysosome, lipid regulation},
abstract = {Summary
Here, we developed a CRISPR-Cas9 arrayed screen to investigate lipid handling pathways in human induced pluripotent stem cell (iPSC)-derived microglia. We established a robust method for the nucleofection of CRISPR-Cas9 ribonucleoprotein complexes into iPSC-derived myeloid cells, enabling genetic perturbations. Using this approach, we performed a targeted screen to identify key regulators of lipid droplet formation dependent on Apolipoprotein E (APOE). We identify the Mammalian Target of Rapamycin Complex 1 (mTORC1) signaling pathway as a critical modulator of lipid storage in both APOE3 and APOE knockout microglia. This study is a proof of concept underscoring the utility of CRISPR-Cas9 technology in elucidating the molecular pathways of lipid dysregulation associated with Alzheimer’s disease and neuroinflammation.}
}
@article{BLANCO2019112835,
title = {Multi-label clinical document classification: Impact of label-density},
journal = {Expert Systems with Applications},
volume = {138},
pages = {112835},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.112835},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419305378},
author = {Alberto Blanco and Arantza Casillas and Alicia Pérez and Arantza {Diaz de Ilarraza}},
keywords = {Multi-label classification, Document classification, Electronic health records, ICD-10 classification},
abstract = {Objective
The goal of this work is the classification of Electronic Health Records using Natural Language Techniques. Electronic Health Records (EHRs) convey valuable clinical information, as diagnoses and patient conditions. We explore several Deep Learning classification models for assigning multiple ICD codes to clinical documents. Within the framework of data mining, the aim of multi-label classification is to associate each instance with a set of labels.
Methods
The multi-label classification is typically carried out based on multiple independent classifiers, in the so-called binary relevance learning approach. Nevertheless, diseases tend to be co-related, independent classifiers are unable to model relationships and do not guarantee the consistency of the predicted label-set. To tackle this, we investigate three Neural Network architectures. We study models that are capable of capturing and modeling label dependencies on the output layer. Moreover, learning from data with low label-density is an inherent challenge in multi-label classification. Thorough experiments were conducted to assess each architecture under different scenarios, varying the language, amount of data and label-density.
Results
The results showed that the Bi-GRU model outperform the DNN and both overcome the baseline (BLR). We observed better results with MIMIC than with Osakidetza corpus. Experimental results showed that as the label-density decreases the prediction task becomes harder. It seems that label-density is very much related to the learning ability of the neural networks and another important factor that affects the inference is the amount of training data.
Conclusions
The contributions of this work are: (a) a comparison among three classification approaches based on Neural Networks on data sets in English and Spanish to cope with the multi-label classification problem and (b) the study of the impact of label-density in prediction capabilities in the multi-label context.}
}
@article{SERRANOGUERRERO2024122340,
title = {A 2-tuple fuzzy linguistic model for recommending health care services grounded on aspect-based sentiment analysis},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122340},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122340},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423028427},
author = {Jesus Serrano-Guerrero and Mohammad Bani-Doumi and Francisco P. Romero and Jose A. Olivas},
keywords = {2-tuple fuzzy model, Multi-granular fuzzy linguistic modeling, Recommender system, Sentiment analysis, Multicriteria decision making},
abstract = {Evaluating the quality of health care systems usually entails the examination of objective variables (waiting time, patients per doctor, etc.). Nonetheless, other subjective variables can be used thanks to new Internet tools. Many online medical services let users convey their opinions on the offered services. That information is an interesting tool to measure the quality of these services. It describes sentiments about different features using a wide range of adjectives, adverbs and nouns, many times, completely different for each feature. Therefore, it is interesting to assess every feature individually using different scales. This study presents an application of a multi-granular fuzzy linguistic model to represent the opinions about the different features of health care systems with the aim of recommending hospitals according to the user preferences. To test this approach, the opinions from real hospitals have been assessed using different user preferences. The obtained results outperform other state-of-the-art approaches.}
}
@article{HAMMAL2020101165,
title = {Formal techniques for consistency checking of orchestrations of semantic Web services},
journal = {Journal of Computational Science},
volume = {44},
pages = {101165},
year = {2020},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101165},
url = {https://www.sciencedirect.com/science/article/pii/S187775032030466X},
author = {Youcef Hammal and Khadidja {Salah Mansour} and Abdelkrim Abdelli and Lynda Mokdad},
keywords = {Web service composition, Formal modeling, Compatibility checking, Consistency checking, OWL-S, WS-BPEL, WS-CDL},
abstract = {Web service orchestrations aim at offering value-added services that meet the increasing complex requirements of business processes. Since it becomes harder to manually check the correctness of such systems, we propose a formal method for their consistency checking by first translating OWL-S composite services and WS-BPEL orchestrator into communicating automata using an iterative process driven by control structures of these languages. Thereafter, we build the synchronization product of the resulting automata and proceed to its compatibility checking in order to unveil forbidden states, which depict incompatibilities in that orchestration interaction protocol. Last, this automaton consistency is checked against the behavioral model of WS-CDL choreography seen as its specification to which it has to conform.}
}
@article{RECKNAGEL2023102039,
title = {Cyberinfrastructure for sourcing and processing ecological data},
journal = {Ecological Informatics},
volume = {75},
pages = {102039},
year = {2023},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2023.102039},
url = {https://www.sciencedirect.com/science/article/pii/S1574954123000687},
author = {Friedrich Recknagel},
keywords = {Cyberinfrastructure, Adaptive management, Ecological entities, Ecological monitoring, Data management, Ecological synthesis, Ecological forecasting, Machine learning, Deep learning, Hybrid modelling},
abstract = {Cyberinfrastructure is a product of the information age that provides a framework for informing adaptive management of ecological entities under the impact of regional and global change. It supports proximity monitoring, user-friendly data management, knowledge discovery by data synthesis, and decision making by forecasting. A workflow is proposed that suits the iterative nature of adaptive management. It takes advantage of novel sensor, genomics, and communication technology for ecological monitoring, of ontologies, semantic webs and blockchain for data management, of hybrid, machine and deep learning concepts for data synthesis and forecasting. Forecasting at different time horizons is guiding decision making for adjusting management and continuing monitoring. This review aims to make researchers, decision makers and stakeholders aware of currently existing technology to make better use of ecological data and models for timely and evidence-based decisions.}
}
@article{LAMY2021102074,
title = {A data science approach to drug safety: Semantic and visual mining of adverse drug events from clinical trials of pain treatments},
journal = {Artificial Intelligence in Medicine},
volume = {115},
pages = {102074},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102074},
url = {https://www.sciencedirect.com/science/article/pii/S0933365721000671},
author = {Jean-Baptiste Lamy},
keywords = {Data mining, Ontology, Visual analytics, Glyph, Drug safety, Adverse drug events, Pain treatments, Painkillers},
abstract = {Clinical trials are the basis of Evidence-Based Medicine. Trial results are reviewed by experts and consensus panels for producing meta-analyses and clinical practice guidelines. However, reviewing these results is a long and tedious task, hence the meta-analyses and guidelines are not updated each time a new trial is published. Moreover, the independence of experts may be difficult to appraise. On the contrary, in many other domains, including medical risk analysis, the advent of data science, big data and visual analytics allowed moving from expert-based to fact-based knowledge. Since 12 years, many trial results are publicly available online in trial registries. Nevertheless, data science methods have not yet been applied widely to trial data. In this paper, we present a platform for analyzing the safety events reported during clinical trials and published in trial registries. This platform is based on an ontological model including 582 trials on pain treatments, and uses semantic web technologies for querying this dataset at various levels of granularity. It also relies on a 26-dimensional flower glyph for the visualization of the Adverse Drug Events (ADE) rates in 13 categories and 2 levels of seriousness. We illustrate the interest of this platform through several use cases and we were able to find back conclusions that were initially found during meta-analyses. The platform was presented to four experts in drug safety, and is publicly available online, with the ontology of pain treatment ADE.}
}
@article{SHIN2024421,
title = {Thalamocortical organoids enable in vitro modeling of 22q11.2 microdeletion associated with neuropsychiatric disorders},
journal = {Cell Stem Cell},
volume = {31},
number = {3},
pages = {421-432.e8},
year = {2024},
issn = {1934-5909},
doi = {https://doi.org/10.1016/j.stem.2024.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S1934590924000407},
author = {David Shin and Chang N. Kim and Jayden Ross and Kelsey M. Hennick and Sih-Rong Wu and Neha Paranjape and Rachel Leonard and Jerrick C. Wang and Matthew G. Keefe and Bryan J. Pavlovic and Kevin C. Donohue and Clara Moreau and Emilie M. Wigdor and H. Hanh Larson and Denise E. Allen and Cathryn R. Cadwell and Aparna Bhaduri and Galina Popova and Carrie E. Bearden and Alex A. Pollen and Sebastien Jacquemont and Stephan J. Sanders and David Haussler and Arun P. Wiita and Nicholas A. Frost and Vikaas S. Sohal and Tomasz J. Nowakowski},
keywords = {organoid, 22q11 microdeletion, thalamocortical, neurodevelopmental disorders, DiGeorge syndrome, velocardiofacial syndrome},
abstract = {Summary
Thalamic dysfunction has been implicated in multiple psychiatric disorders. We sought to study the mechanisms by which abnormalities emerge in the context of the 22q11.2 microdeletion, which confers significant genetic risk for psychiatric disorders. We investigated early stages of human thalamus development using human pluripotent stem cell-derived organoids and show that the 22q11.2 microdeletion underlies widespread transcriptional dysregulation associated with psychiatric disorders in thalamic neurons and glia, including elevated expression of FOXP2. Using an organoid co-culture model, we demonstrate that the 22q11.2 microdeletion mediates an overgrowth of thalamic axons in a FOXP2-dependent manner. Finally, we identify ROBO2 as a candidate molecular mediator of the effects of FOXP2 overexpression on thalamic axon overgrowth. Together, our study suggests that early steps in thalamic development are dysregulated in a model of genetic risk for schizophrenia and contribute to neural phenotypes in 22q11.2 deletion syndrome.}
}
@article{CAESAR2019307,
title = {Context-sensitive reconfiguration of collaborative manufacturing systems},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {307-312},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.194},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319311449},
author = {Birte Caesar and Michael Nieke and Aljosha Köcher and Constantin Hildebrandt and Christoph Seidl and Alexander Fay and Ina Schaefer},
keywords = {Software Product Lines, Collaborative Systems, Open, Dynamic Context, Reconfigurable Manufacturing Systems, Ontology Creation},
abstract = {To stay competitive in a highly dynamic environment, manufacturing companies have to quickly react to changing customer demands. Manufacturing systems may only serve demands that are covered by their capabilities. The manufacturability of a product can be analyzed by comparing the provided capabilities against the product’s requirements. The manufacturing capabilities depend on the current system and subsystem configuration. If a product cannot be manufactured, first, it must be analyzed whether any valid configuration exists that provides the required capabilities, and second, the system has to be reconfigured according to the new configuration. To the best of our knowledge no existing method exists that enables these previously mentioned steps. In this paper, we introduce a method for context-sensitive reconfiguration of multiple collaborating manufacturing systems that might come from different vendors to create a customized product on demand. We utilize variability models to capture the possible configuration space and describe influences of product properties on features of the variability model by providing a context-sensitive variability model. We apply our method to a demonstrator and show that we enable modeling of reconfigurable manufacturing systems that are automatically reconfigured on context change.}
}
@article{NEELY2023681,
title = {Toward an Integrated Machine Learning Model of a Proteomics Experiment},
journal = {Journal of Proteome Research},
volume = {22},
number = {3},
pages = {681-696},
year = {2023},
issn = {1535-3907},
doi = {https://doi.org/10.1021/acs.jproteome.2c00711},
url = {https://www.sciencedirect.com/science/article/pii/S1535390723001622},
author = {Benjamin A. Neely and Viktoria Dorfer and Lennart Martens and Isabell Bludau and Robbin Bouwmeester and Sven Degroeve and Eric W. Deutsch and Siegfried Gessulat and Lukas Käll and Pawel Palczynski and Samuel H. Payne and Tobias Greisager Rehfeldt and Tobias Schmidt and Veit Schwämmle and Julian Uszkoreit and Juan Antonio Vizcaíno and Mathias Wilhelm and Magnus Palmblad},
keywords = {machine learning, deep learning, artificial intelligence, synthetic data, enzymatic digestion, liquid chromatography, ion mobility, tandem mass spectrometry, research integrity},
abstract = {In recent years machine learning has made extensive progress in modeling many aspects of mass spectrometry data. We brought together proteomics data generators, repository managers, and machine learning experts in a workshop with the goals to evaluate and explore machine learning applications for realistic modeling of data from multidimensional mass spectrometry-based proteomics analysis of any sample or organism. Following this sample-to-data roadmap helped identify knowledge gaps and define needs. Being able to generate bespoke and realistic synthetic data has legitimate and important uses in system suitability, method development, and algorithm benchmarking, while also posing critical ethical questions. The interdisciplinary nature of the workshop informed discussions of what is currently possible and future opportunities and challenges. In the following perspective we summarize these discussions in the hope of conveying our excitement about the potential of machine learning in proteomics and to inspire future research.
}
}
@article{LI2023104887,
title = {An integrated multi-level analysis reveals learning-memory deficits and synaptic dysfunction in the rat model exposure to austere environment},
journal = {Journal of Proteomics},
volume = {279},
pages = {104887},
year = {2023},
issn = {1874-3919},
doi = {https://doi.org/10.1016/j.jprot.2023.104887},
url = {https://www.sciencedirect.com/science/article/pii/S1874391923000763},
author = {Nuomin Li and Yanan Gao and Yongqian Zhang and Yulin Deng},
keywords = {Austere environment, Depressive-like behavior, Synaptic plasticity, Synaptic vesicle, Learning and memory},
abstract = {Austere environment existing in tank, submarine and vessel has many risk factors including high temperature and humidity, confinement, noise, hypoxia, and high level of carbon dioxide, which may cause depression and cognitive impairment. However, the underlying mechanism is not fully understood yet. We attempt to investigate the effects of austere environment (AE) on emotion and cognitive function in a rodent model. After 21 days of AE stress, the rats exhibit depressive-like behavior and cognitive impairment. Compared with control group, the glucose metabolic level of the hippocampus is significantly decreased using whole-brain positron emission tomography (PET) imaging, and the density of dendritic spines of the hippocampus is remarkably reduced in AE group. Then, we employ a label-free quantitative proteomics strategy to investigate the differentially abundant proteins in rats' hippocampus. It is striking that the differentially abundant proteins annotated by KEGG enrich in oxidative phosphorylation pathway, synaptic vesicle cycle pathway and glutamatergic synapses pathway. The synaptic vesicle transport related proteins (Syntaxin-1A, Synaptogyrin-1 and SV-2) are down-regulated, resulting in the accumulation of intracellular glutamate. Furthermore, the concentration of hydrogen peroxide and malondialdehyde is increased while the activity of superoxide dismutase and complex I and IV of mitochondria is decreased, indicating that oxidative damage to hippocampal synapses is associated with the cognitive decline. The results of this study offer direct evidence, for the first time, that austere environment can substantially cause learning and memory deficits and synaptic dysfunction in a rodent model via behavioral assessments, PET imaging, label-free proteomics, and oxidative stress tests.
Significance
The incidence of depression and cognitive decline in military occupations (for example, tanker and submariner) is significantly higher than that of global population. In the present study, we first established novel model to simulate the coexisting risk factors in the austere environment. The results of this study offer the direct evidences, for the first time, that the austere environment can substantially cause learning and memory deficits by altering plasticity of the synaptic transmission in a rodent model via proteomic strategy, PET imaging, oxidative stress and behavioral assessments. These findings provide valuable information to better understand the mechanisms of cognitive impairment.}
}
@article{CARBONNEL20191,
title = {Modelling equivalence classes of feature models with concept lattices to assist their extraction from product descriptions},
journal = {Journal of Systems and Software},
volume = {152},
pages = {1-23},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300378},
author = {Jessie Carbonnel and Marianne Huchard and Clémentine Nebut},
keywords = {Software product lines, Reverse engineering, Formal concept analysis, Variability modelling, Feature models},
abstract = {Software product line engineering gathers a set of methods to help create, manage and maintain a collection of similar software systems. Variability modelling is a focal point of this paradigm, where feature models (FMs) are the prevalent notation. Migration from single system development to software product lines is a spreading topic in software engineering. To ease the migration, research has been done to automatically extract FMs from software descriptions, but most of these approaches are defined in a functional manner based on an ad-hoc variability analysis. In this paper, we propose a theoretical view on FM extraction from software descriptions based on Formal Concept Analysis (FCA). It is a structural framework for variability representation which allows to lay down theoretical foundation to variability extraction. We propose an original mapping between relationships expressed in FMs and the ones emphasised in FCA conceptual structures. We show that conceptual structures represent equivalence classes of FMs that steer the user choices during their synthesis, and propose a reverse engineering method based on them. We discuss its applicability and show that the combinatorial explosion of concept lattices can be avoided by the use of two sub-orders embodying the necessary information concerning variability.}
}
@article{RAUNER2025100135,
title = {The socio-technical adoption and diffusion of digital health innovations: The development of the STAD-HC model based on telemedicine in Germany},
journal = {Digital Business},
volume = {5},
number = {2},
pages = {100135},
year = {2025},
issn = {2666-9544},
doi = {https://doi.org/10.1016/j.digbus.2025.100135},
url = {https://www.sciencedirect.com/science/article/pii/S2666954425000304},
author = {Yvonne Rauner and Harald Stummer},
keywords = {Adoption theory, Diffusion theory, Social shaping of technology theory, Implementation science, Digital innovations healthcare},
abstract = {Digital and telemedicine innovations are increasingly central to healthcare transformation, yet their adoption and diffusion remain complex. While the Diffusion of Innovations (DOI) theory explains staged adoption processes, it underrepresents socio-political and institutional factors. The Social Shaping of Technology (SST) approach complements DOI by focusing on the co-construction of technology through cultural, organizational, and political dynamics. This study investigates how DOI and SST can be combined to explain the adoption and diffusion of digital health innovations, with a focus on telemedicine in the German healthcare system. Using the Decision Matrix for Theory Borrowing, DOI and SST were selected as complementary frameworks. A secondary analysis of 12 expert interviews was conducted using SST-based content analysis. Data were coded according to Rogers' five adoption stages and four SST dimensions. Findings were mapped in a DOI–SST matrix and analysed across micro-, meso-, and macro-levels. The analysis shows that innovation trajectories are shaped by interdependent socio-technical factors across all adoption stages. Key barriers include cultural skepticism, organizational inertia, fragmented governance, and economic misalignment. The Socio-Technical Adoption and Diffusion Model for Healthcare (STAD-HC) was developed to represent these dynamics, linking DOI's stage logic with SST's contextual dimensions. The STAD-HC model offers a multi-level framework to analyse how digital health technologies are adopted, contested, and institutionalized. By integrating behavioural processes with structural influences, it provides practical guidance for managing innovation and supports future research on complex digital health technologies such as AI and blockchain.}
}
@incollection{GORANSON2025145,
title = {7 - User affordances to engineer open-world enterprise dynamics},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Hesham Fouad},
booktitle = {Interdependent Human-Machine Teams},
publisher = {Academic Press},
pages = {145-174},
year = {2025},
isbn = {978-0-443-29246-0},
doi = {https://doi.org/10.1016/B978-0-443-29246-0.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292460000067},
author = {Ted Goranson and Beth Cardier and Mathew Hancock and Naso Evangelou-Oost and Benjamin J. Seligmann and Matthew Garcia and Glen Smith},
keywords = {Enterprise affordances, Human–machine enterprises, Possibilistic networks, Risk engineering},
abstract = {We address the challenge of engineering enterprise risk to prevent undesirable outcomes that have not previously occurred. This is generally understood as a problem of open-world modeling. Modeling open-world enterprises in the risk domain is complicated by numerous factors: These activities often span disciplines, institutional boundaries, and engage across both human and machine processes. Traditional modeling approaches generalize enterprise operations so that standard policies and procedures can be developed to prevent harm to workers and workplaces. However, these approaches can only prevent problems that have already happened, or which are otherwise expected. We report on studies toward graphical modeling techniques to allow a user flexible insight and control among different presentations while accessing deep open-world models. Our goal is to cross domains and contexts to modify models in a manner that guides outcomes in these open worlds. To support this strategy, we leverage a novel reasoning strategy that captures extended notions of cause, probability, influence, and possibility.}
}
@article{DEMASELLIS2022116059,
title = {Solving reachability problems on data-aware workflows},
journal = {Expert Systems with Applications},
volume = {189},
pages = {116059},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116059},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421013993},
author = {Riccardo {De Masellis} and Chiara {Di Francescomarino} and Chiara Ghidini and Sergio Tessaris},
keywords = {Data-aware business process management, Formal verification, Reachability, Planning, Model-checking, Data-aware workflow nets},
abstract = {Recent advances in the field of Business Process Management (BPM) have brought about several suites able to model data objects along with the traditional control flow perspective. Nonetheless, when it comes to formal verification there is still a lack of effective verification tools on imperative data-aware process models and executions: the data perspective is often abstracted away and verification tools are often missing. Automated Planning is one of the core areas of Artificial Intelligence where theoretical investigations and concrete and robust tools have made possible the reasoning about dynamic systems and domains. Moreover planning techniques are gaining popularity in the context of BPM. Starting from these observations, we provide here a concrete framework for formal verification of reachability properties on an expressive, yet empirically tractable class of data-aware process models, an extension of Workflow Nets. Then we provide a rigorous mapping between the semantics of such models and that of three important Automated Planning paradigms: Action Languages, Classical Planning, and Model-Checking. Finally, we perform a comprehensive assessment of the performance of three popular tools supporting the above paradigms in solving reachability problems for imperative data-aware business processes, which paves the way for a theoretically well founded and practically viable exploitation of planning-based techniques on data-aware business processes.}
}
@article{KHODAPARAST2022102580,
title = {Cloud computing security: A survey of service-based models},
journal = {Computers & Security},
volume = {114},
pages = {102580},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102580},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821003977},
author = {Fatemeh {Khoda Parast} and Chandni Sindhav and Seema Nikam and Hadiseh {Izadi Yekta} and Kenneth B. Kent and Saqib Hakak},
keywords = {Security, Cloud computing, Service-based cloud computing, IaaS, PaaS, SaaS},
abstract = {Cloud computing has recently attracted significant attention due to its economical and high-quality services. In the last decade, cloud services have inevitably entangled with businesses’ and individuals’ daily lives through products and services. On-demand, pay-per-use characteristics encourage corporations to outsource part of their businesses to accelerate their services and multiply value. The latest market tendency toward migration to cloud environments, started in 2019, indicates a flourishing trend in the next few years. Despite the numerous benefits of the cloud computing model for businesses or individuals, security issues still have been stated as the top cloud challenge in 2020. Although various factors affect security, technologies enabling cloud computing such as virtualization and multitenancy, in addition to on-demand characteristics, initiate new security entrances for malevolent activities. In this study, we surveyed service-based cloud computing security issues to establish the current state of the field. The main contribution of this paper is to analyze the state of cloud security in the last decade and provide a unified taxonomy of security issues over the three-layer model, i.e., IaaS, PaaS, and SaaS.}
}
@article{OCONNOR2024e24703,
title = {Heterozygous Nexmif female mice demonstrate mosaic NEXMIF expression, autism-like behaviors, and abnormalities in dendritic arborization and synaptogenesis},
journal = {Heliyon},
volume = {10},
number = {3},
pages = {e24703},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e24703},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024007345},
author = {Margaret O'Connor and Hui Qiao and KathrynAnn Odamah and Pedro Casariego Cerdeira and Heng-Ye Man},
abstract = {Autism spectrum disorder (ASD) is a neurodevelopmental disorder with a strong genetic basis. ASDs are commonly characterized by impairments in language, restrictive and repetitive behaviors, and deficits in social interactions. Although ASD is a highly heterogeneous disease with many different genes implicated in its etiology, many ASD-associated genes converge on common cellular defects, such as aberrant neuronal morphology and synapse dysregulation. Our previous work revealed that, in mice, complete loss of the ASD-associated X-linked gene NEXMIF results in a reduction in dendritic complexity, a decrease in spine and synapse density, altered synaptic transmission, and ASD-like behaviors. Interestingly, human females of NEXMIF haploinsufficiency have recently been reported to demonstrate autistic features; however, the cellular and molecular basis for this haploinsufficiency-caused ASD remains unclear. Here we report that in the brains of Nexmif ± female mice, NEXMIF shows a mosaic pattern in its expression in neurons. Heterozygous female mice demonstrate behavioral impairments similar to those of knockout male mice. In the mosaic mixture of neurons from Nexmif ± mice, cells that lack NEXMIF have impairments in dendritic arborization and spine development. Remarkably, the NEXMIF-expressing neurons from Nexmif ± mice also demonstrate similar defects in dendritic growth and spine formation. These findings establish a novel mouse model of NEXMIF haploinsufficiency and provide new insights into the pathogenesis of NEXMIF-dependent ASD.}
}
@article{NAIR20251356,
title = {Graph-Based Generation and Validation of Use Case Diagrams},
journal = {Procedia Computer Science},
volume = {259},
pages = {1356-1365},
year = {2025},
note = {Sixth International Conference on Futuristic Trends in Networks and Computing Technologies (FTNCT06), held in Uttarakhand, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.090},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925011913},
author = {Reshma P Nair and M G Thushara and K Somasundaram},
keywords = {NLP, Graph Theory, Use case Diagrams, Automatic Generation, Shortest-Path Algorithms},
abstract = {As we know in today’s modern software engineering, use case diagrams for representing the functionalities of the system act significantly in mediating the gap between the stakeholder and the developer. Some of the earlier techniques used in constructing such diagrams can be problematic when these diagrams are to be employed in large systems where there are many actors and activities. That is why every limitation outlined above might be resolved further with the help of NLP and developments in graphs theory. The proposed method makes improvements by reducing the human interference; it can handle increase in system size and the number of actors and their interactions; it also provides highly accurate shortest-path identification through use of validation methods. In addition, shortest-path algorithms are incorporated to improve the validation exercise, thereby minimizing errors common in presenting the system functionalities. Hence, the results indicate that the proposed method is more effective not only for generating use case diagrams with increased precision and scalability but also for constructing the necessary framework for checking system interactions’ correctness. This work provides a foundation for future studies aimed at enhancing the automation of the use case diagram especially for complex systems.}
}
@article{TCHOUANGUEMDJUEDJA2021102930,
title = {An integrated Linked Building Data system: AEC industry case},
journal = {Advances in Engineering Software},
volume = {152},
pages = {102930},
year = {2021},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2020.102930},
url = {https://www.sciencedirect.com/science/article/pii/S0965997820309765},
author = {Justine Flore {Tchouanguem Djuedja} and Fonbeyin Henry Abanda and Bernard Kamsu-Foguem and Pieter Pauwels and Camille Magniont and Mohamed Hedi Karray},
keywords = {Construction product databases, Linked Building Data (LBD), Environmental data, Building Information Modelling (BIM), Semantic web, Linked data},
abstract = {Environmental assessment is a critical activity for ensuring buildings are performing according to specified requirements, and efficient, seamless exchange of building information is crucial for environmental assessment. Therefore, all those involved in built environment issues should be able to access and share not only building information but also data about products, especially environmental assessment results for the products used in building projects. Of the several approaches that have been proposed to achieve efficient information exchange, semantic web technologies are amongst the most promising due to their capability to share data and enhance interoperability between the most heterogeneous systems. This study proposes an approach that can be used to make environmental data available in the early phases of the building lifecycle. It relies on Semantic Web techniques, especially Linked Data principles, while building on emerging Building Information Modelling (BIM) technology to propose an approach that facilitates information exchange to enhance the sustainability assessment of buildings. The paper ends with an illustration of how lifecycle inventory databases can be integrated, linked to BIM software and used in exchanging environmental building data.}
}
@article{VIRY202087,
title = {Ontologie d’Alerte Choucas : de la modélisation des connaissances à un outil support d’un raisonnement géovisuel1},
journal = {Geomatica},
volume = {74},
number = {3},
pages = {87-103},
year = {2020},
issn = {1195-1036},
doi = {https://doi.org/10.1139/geomat-2020-0005},
url = {https://www.sciencedirect.com/science/article/pii/S1195103624005433},
author = {Matthieu Viry and Marlène Villanova-Oliver},
keywords = {représentation des connaissances, raisonnement géovisuel, secours en montagne, knowledge representation, geovisual reasoning, mountain search and rescue},
abstract = {Lorsqu’une intervention de secours est nécessaire, localiser précisément et rapidement le site sur lequel envoyer les équipes est primordial. La littérature montre que des outils de géovisualisation constituent des solutions pertinentes pour supporter des processus d’analyse d’informations dans des contextes variés. Nous nous intéressons ici au raisonnement du secouriste réceptionnant un appel à l’aide et visons des solutions conceptuelles et logicielles dédiées à la tâche de détermination de la localisation de la victime, plus particulièrement dans le contexte du secours en montagne. Nous avons formalisé le raisonnement du secouriste et les informations sur lesquelles il ou elle s’appuie à l’aide d’une ontologie. L’Ontologie d’Alerte Choucas structure les concepts exploités par le secouriste qui élabore des hypothèses de localisation probable de la victime à partir d’informations (telles qu’une position relative, un temps de marche, une direction) fournies lors d’un échange verbal. Dans notre approche, l’ontologie est en outre exploitée pour dériver les composants d’interface d’un prototype de géovisualisation facilitant le raisonnement du secouriste. Ces composants sont une aide à la saisie des informations, en fournissant une restitution cartographique adaptée, et contribuent à construire et à affiner la zone de localisation. Notre approche présente une chaine de traitements originale, menant de la représentation des connaissances à la génération automatisée d’une interface fonctionnelle d’aide au raisonnement visant à localiser des victimes en montagne. When a search and rescue intervention is required, it is essential to quickly and accurately locate the site where rescuers have to be deployed. The literature shows that geovisualization tools are relevant solutions to support various information analysis processes in various contexts. In this work, we are interested in the reasoning of the rescuer receiving a call for help and we aim at conceptual and software solutions dedicated to determining the victim’s location, more particularly in the context of mountain rescue. We have formalized the rescuer’s reasoning and the information he or she relies on by using an ontology. The Choucas Alert Ontology structures the concepts used by the rescuer, who develops assumptions of probable victim location based on information (such as relative position, walking time, direction) provided during a verbal exchange. In our approach, the ontology is also used to derive interface components for a geovisualization prototype that facilitates the rescuer’s reasoning. These components assist the information capture, providing an adapted cartographic restitution, and contribute to the construction and refinement of the localization zone. Our approach presents an original processing chain, from the representation of knowledge to the automated generation of a functional interface to assist reasoning in locating victims in the mountains.}
}
@article{KAUFHOLD20181,
title = {Creating translanguaging spaces in students’ academic writing practices},
journal = {Linguistics and Education},
volume = {45},
pages = {1-9},
year = {2018},
issn = {0898-5898},
doi = {https://doi.org/10.1016/j.linged.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0898589817301729},
author = {Kathrin Kaufhold},
keywords = {Academic writing, Translanguaging space, Translanguaging, Linguistic repertoire, Linguistic ideology, Lived experience},
abstract = {Postgraduates increasingly write in multilingual contexts. Studies have focused on developing bilingual expertise or harnessing expressions of writer identity. Yet, the role of students’ linguistic ideologies and their writing experiences has so far not been problematised. Based on Busch's sociolinguistic model of linguistic repertoire (2012), this paper investigates how students develop their academic writing across language codes and registers in the multilingual contexts of a Swedish university. The qualitative, longitudinal study presents data from two students including interviews based on the students’ written text relating to their master's thesis. Findings show that students’ linguistic ideologies and their experiences can enable or restrict their capacity to draw on their varied repertoires. When enabled, students create translanguaging spaces for meaning making in collaboration with peers and institutional actors. I argue that the metaphor of translanguaging space can be fruitfully applied as a pedagogic tool.}
}
@article{XIE2025105161,
title = {Transforming intangible cultural heritage in destinations: A fashion communication perspective},
journal = {Tourism Management},
volume = {110},
pages = {105161},
year = {2025},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2025.105161},
url = {https://www.sciencedirect.com/science/article/pii/S0261517725000317},
author = {Chaowu Xie and Feifei Lai and Jiangchi Zhang and Songshan (Sam) Huang},
keywords = {Destination, Intangible cultural heritage (ICH), Fashion communication, Framework construction, Diffusion of innovation theory},
abstract = {Intangible cultural heritage (ICH) is a key attraction for the development of tourist destinations, but few studies have examined the popularization of ICH in destinations through the lens of fashion communication. This research pioneers the conceptualization of ICH fashion communication in tourist destinations. Using qualitative and quantitative methods, we identify and construct a theoretical framework of ICH fashion communication. Study 1 reveals that the fashion communication of ICH in tourist destinations follows a process framework of “fashion communication elements - fashion communication channels - fashion communication results,” involving six distinct constructs. Study 2 and Study 3 demonstrate that the fashion communication elements (fashion representation, fashion ontology, and fashion construction) significantly influence tourists’ fashion perception. Additionally, fashion communication channels (diffusion of exhibition spaces and participation of diverse groups) mediate the relationship between these elements and tourists’ fashion perception. This research enhances the theoretical understanding of ICH communication and marketing in tourism.}
}
@article{ROSSOLATOS2020100484,
title = {A brand storytelling approach to Covid-19's terrorealization: Cartographing the narrative space of a global pandemic},
journal = {Journal of Destination Marketing & Management},
volume = {18},
pages = {100484},
year = {2020},
issn = {2212-571X},
doi = {https://doi.org/10.1016/j.jdmm.2020.100484},
url = {https://www.sciencedirect.com/science/article/pii/S2212571X20301062},
author = {George Rossolatos},
keywords = {Covid-19, Place branding, Brand storytelling, Narratology, Metaphorical modeling, Semiotics},
abstract = {This paper offers a brand storytelling or narratological account of the Covid-19 pandemic's emergence phase. By adopting a fictional ontological standpoint, the virus' deploying media-storyworld is identified with a process of narrative spacing. Subsequently, the brand's personality is analyzed as a narrative place brand. The advanced narrative model aims to outline the main episodes that make up the virus' brand personality as process and structural components (actors, settings, actions, and relationships). A series of deep or ontological metaphors are identified as the core DNA of this place brand by applying metaphorical modeling to the tropic articulation of Covid-19's narrative. The virus is fundamentally identified with terror as a menacing force that wipes out existing regimes of signification due to its uncertain motives, origins, and operational mode. In this context, familiar urban spaces, cultural practices, and intersubjective communications are redefined, repurposed, and reprogrammed. This process is called terrorealization, as the desertification and metaphorical sublation of all prior territorial significations. This study contributes to the narrative sub-stream of place branding by approaching a globally relevant socio-cultural phenomenon from a brand storytelling perspective.}
}
@article{WEN2023103532,
title = {A quantitative security evaluation and analysis model for web applications based on OWASP application security verification standard},
journal = {Computers & Security},
volume = {135},
pages = {103532},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103532},
url = {https://www.sciencedirect.com/science/article/pii/S016740482300442X},
author = {Shao-Fang Wen and Basel Katt},
keywords = {Web application security, Security evaluation, Quantitative approach, Security analysis},
abstract = {In today's digital world, web applications are popular tools used by businesses. As more and more applications are deployed on the web, they are seen as increasingly attractive targets by malicious actors eager to exploit any security gaps present. Organizations are always at risk for potential vulnerabilities in their web-based software systems, which can lead to data loss, service interruption, and lack of trust. Therefore, organizations need to have an effective and efficient method for assessing and analyzing the security of acquired web-based software to ensure adequate confidence in its use. Quantitative security evaluation employs mathematical and computational techniques to express the security level that a system reaches. This research focuses on improving the quantitative analysis of web application security evaluation. We strive to unite the Open Web Application Security Project's (OWASP) Application Security Verification Standard (ASVS) into a structural and analyzable model, which aims to efficiently evaluate web application security levels while providing meaningful insights into their strengths and weaknesses.}
}
@article{MUPPAVARAPU2021101923,
title = {Knowledge extraction using semantic similarity of concepts from Web of Things knowledge bases},
journal = {Data & Knowledge Engineering},
volume = {135},
pages = {101923},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101923},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000501},
author = {Vamsee Muppavarapu and Gowtham Ramesh and Amelie Gyrard and Mahda Noura},
keywords = {Interoperability, Internet of Things, Semantic Web of Things, Popular concepts, Smart building, Smart home},
abstract = {The Internet of Things (IoT) is one of the rapidly growing technologies with the aim of establishing communication among objects, people, and processes. This rapidly growing technology faces a lot of challenges that hinder its wider adoption, specifically in developing applications that involve heterogeneous domains. Currently, developing such interoperable applications require substantial efforts by the developers to hard code the requirements to ensure the correctness of transferring knowledge. The efforts can be significantly reduced by developing an interoperable platform that ensures seamless communication between heterogeneous IoT devices. W3C Web of Things (WoT) is a significant step towards enabling interoperability between IoT devices by integrating the existing Web ecosystem with “Things”. WoT provides a unified interface over a suitable network protocol facilitating interactions between different IoT protocols. WoT Thing Descriptions (TD) enrich interoperability providing both human and machine readable metadata about a Thing. However, the WoT still falls short in providing semantic interoperability due to insufficient standard vocabularies which can describe different IoT application domains. In this paper, we propose a semantic similarity-based approach to automatically identify and extract the most common concepts from sixteen popular ontologies belonging to smart home and smart building domains. The proposed method helps the developers and researchers to develop a domain ontology with reduced effort. The extracted concepts are evaluated by the domain experts and are found to be sufficient in describing the smart home and smart building domains.}
}
@incollection{FALLETTACOWDEN202234,
title = {6.03 - Acceptance and Commitment Therapy: Applying Contextual Behavioral Science to the Therapeutic Process},
editor = {Gordon J.G. Asmundson},
booktitle = {Comprehensive Clinical Psychology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {34-57},
year = {2022},
isbn = {978-0-12-822232-4},
doi = {https://doi.org/10.1016/B978-0-12-818697-8.00173-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186978001734},
author = {Neal Falletta-Cowden and Steven C. Hayes},
keywords = {Acceptance and commitment therapy, ACT, Functional contextualism, Relational frame theory, RFT, Process-based therapy, PBT, Values, Experiential avoidance, Acceptance, Defusion, Fusion, Present-moment focus, Committed action, Perspective taking, Conceptualized self, Self as context, Psychological flexibility, Contextual behavioral science, Evolutionary science, Behavior analysis},
abstract = {Acceptance and Commitment Therapy (ACT) is a process-based approach to psychological intervention that fosters acceptance and mindfulness processes, and commitment and behavior change processes, in order to build client psychological flexibility. Psychological flexibility helps clients direct behavior toward positive habits of values based action, consciously fitting these actions to the present internal and external context, harnessing rather than avoiding or attaching oneself to historically or situationally produced private experiences such as thoughts, feelings, memories, urges, or bodily sensations that they may encounter in their daily lives. As contrasted with syndrome-focused approaches to intervention, ACT emphasizes the importance of identifying underlying processes that influence behavioral outcomes and targeting change in these processes rather than signs and symptoms of hypothesized latent disease entities. The Psychological Flexibility model upon which ACT rests is itself built upon an extensive research program on language and cognition known as Relational Frame Theory (RFT). Over 800 Randomized Controlled Trials and thousands of other basic and applied studies have now investigated the efficacy of ACT and its underlying basic and applied model, oriented toward an incredible variety of human problems and desired areas of prosperity, from mental health, to behavioral health, to social problems, to organizational issues, and high performance situations. This chapter briefly covers the underlying theory, processes, and evidence for ACT's efficacy as well as examples of how to apply ACT to human behavior change.}
}
@article{ROBACH2025103323,
title = {Certified control for train sign classification},
journal = {Science of Computer Programming},
volume = {246},
pages = {103323},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2025.103323},
url = {https://www.sciencedirect.com/science/article/pii/S0167642325000620},
author = {Jan Roßbach and Michael Leuschel},
keywords = {ATO, Artificial intelligence, Formal methods, Computer vision, Autonomous systems},
abstract = {Certified control makes it possible to use artificial intelligence for safety-critical systems. It is a runtime monitoring architecture, which requires an AI to provide certificates for its decisions; these certificates can then be checked by a separate classical system. In this article, we evaluate the practicality of certified control for providing formal guarantees about an AI-based perception system. In this case study, we implemented a certificate checker that uses classical computer vision algorithms to verify railway signs detected by an AI object detection model. We have integrated this prototype with the popular object detection model YOLO. Performance metrics on generated data are promising for the use-case, but further research is needed to generalize certified control for other tasks.}
}
@article{WANG2024104052,
title = {A product recommendation model based on online reviews: Improving PageRank algorithm considering attribute weights},
journal = {Journal of Retailing and Consumer Services},
volume = {81},
pages = {104052},
year = {2024},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2024.104052},
url = {https://www.sciencedirect.com/science/article/pii/S0969698924003485},
author = {Xiaoli Wang and Chenxi Zhang and Zeshui Xu},
keywords = {Recommendation model, Online reviews, Review usefulness, PageRank algorithm, Multi-criteria decision making (MCDM)},
abstract = {Consumer reviews play a crucial role in evaluating products on online e-commerce platforms. Unlike numerical ratings, online reviews provide valuable information and sentiment. However, existing studies often overlook the unique interrelationships between products on e-commerce platforms, and fail to adequately capture the psychological behavior of consumers during online shopping. To address these gaps, this study presents a novel product recommendation model based on online reviews that evaluates products’ multi-attribute performances. The study first identifies the product attributes that are most important to consumers by analyzing review texts. Then, this study calculates the attribute performance scores of each product by considering consumer sentiment and the usefulness of online reviews. Next, it identifies competitors for the target product using a weighted Euclidean distance function and ranks all products employing an improved PageRank algorithm. Finally, to illustrate the validity of the proposed model, the study conducts a case study using a dataset of 41,352 online reviews obtained from Best Buy, and segments the data into three categories according to price. Comparative results with traditional MCDM models show that among the three categories, our results achieved a maximum improvement of 18.3% in the Spearman correlation coefficient.}
}
@article{REPA20192144,
title = {Model Consistency as a Tool for Digital Business Architecture Verification},
journal = {Procedia Computer Science},
volume = {159},
pages = {2144-2153},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.388},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919315911},
author = {Václav Řepa and Oleg Svatoš},
keywords = {business architecture, model consistency, digital transformation},
abstract = {Digitalization of enterprises and introduction of digital products has become in recent years inevitable. In the global competition enterprises have to use all means available in order to be able to respond to competitors’ innovations or even to be able to implement innovations which would take them ahead of the competition. With the rising complexity of the business and the information technologies used, there is inevitable that there have to be methods applied, which make possible for the digitalization projects to formulate the consistent and complete business requirements in form of business model (architecture) increasing so the chance that the following implementation will be successful. In this paper we describe the consistency rules and the necessary minimum of business architecture which should be modeled so that the business requirements can be clearly defined, and their completeness and correctness verified. The method is illustrated on the generally accepted EA standards TOGAF/ArchiMate, UML and BPMN and its benefits are discussed. We also provide the reader with a metamodel which specifies how these standards in the discussed topics match together.}
}
@article{HAN2025,
title = {Leveraging Conceptual Metaphors to Boost English Vocabulary Learning Among College Students},
journal = {International Journal of Web-Based Learning and Teaching Technologies},
volume = {20},
number = {1},
year = {2025},
issn = {1548-1093},
doi = {https://doi.org/10.4018/IJWLTT.381235},
url = {https://www.sciencedirect.com/science/article/pii/S1548109325000312},
author = {Yi Han and Nazifah Hamidun and Rozilawati Mahadi},
keywords = {Cognitive Linguistics, Conceptual Metaphor Theory, EFL Learners, Learner Motivation, Vocabulary Acquisition},
abstract = {ABSTRACT
This study investigates the effectiveness of Conceptual Metaphor Theory (CMT)-based instruction in enhancing vocabulary acquisition and learner interest among Chinese university students learning English as a Foreign Language (EFL). Using a quasi-experimental design, 73 undergraduates were divided into an experimental group (n = 37) receiving CMT-based instruction and a control group (n = 36) following traditional methods. Pre- and post-tests measured vocabulary gains, while questionnaires assessed learner interest. Results show that the experimental group significantly outperformed the control group in vocabulary acquisition (mean difference: 7.24 vs. 4.00 points; Cohen’s d = 4.56) and learner interest (Cohen’s d = -2.93 vs. -2.79). Statistical analyses confirmed the superiority of CMT-based instruction (vocabulary acquisition: t(71) = 3.43, p = .001; learner motivation: t(71) = -5.91, p &lt; .001).}
}
@article{LAING2020103163,
title = {Questioning integration of verification in model-based systems engineering: an industrial perspective⋆},
journal = {Computers in Industry},
volume = {114},
pages = {103163},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.103163},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519302799},
author = {C. Laing and P David and E. Blanco and X. Dorel},
keywords = {Model Based Systems Engineering, Verification, Industrial practices, Survey},
abstract = {The increasing of complexity of industrial products makes Verification and Validation procedures challenging. Model-Based Systems Engineering (MBSE) has been proposed as an approach to manage such complexity through the formalized application of models throughout the systems development life cycle to support verification activities. The purpose of this paper is to make a first step towards developing a verification strategy by performing a qualitative survey on current industrial practices against the state of the art in MBSE. Data had been collected through workshops of 16 engineering organizations located in the Rhone-Alpes region of France. Key success criteria for integrating model-based verification with MBSE are identified by industry respondent. These criteria allow to analyse the top verification methods presented in MBSE publications and compare them to current practices. To further validate the use of these presented methods, interviews and observations were conducted to analyse the methods through different industrial perspectives and identify their feasibility for successful application. The findings of this study gives a picture of current practices in integrating verification and MBSE in industry. The paper discusses the impact of implementing such practices in companies beginning to adopt MBSE approaches. It is concluded by identifying opportunities and barriers for Model-Based Verification adoption.}
}
@article{KUMAR2023102239,
title = {An efficient and scalable SPARQL query processing framework for big data using MapReduce and hybrid optimum load balancing},
journal = {Data & Knowledge Engineering},
volume = {148},
pages = {102239},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102239},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2300099X},
author = {V. Naveen Kumar and Ashok Kumar P.S.},
keywords = {RDF data storage, SPARQL querying, Hadoop, Extended vertical partitioning, Hybrid optimum load balancing},
abstract = {The increasing RDF (Resource Description Framework) data volume requires a Hadoop platform for processing queries over large datasets. In this work, SPARQL (Simple Protocol and Rdf Query Language) queries are evaluated with Hadoop based on the objective of minimizing the number of joins through data partitioning for performing map/reduce jobs. The query evaluation time and the number of cross node joins are minimized with the proposed partitioning techniques. Extended vertical partitioning is proposed for distributed data stores based on objects’ explicit information for splitting predicates. For accessing the RDF data, hybrid monarch butterfly with beetle swarm load balancing optimization with Map-reduce (Hybrid Optimum Load Balancing) is applied. The proposed SPARQL query processing is evaluated over large RDF datasets. The proposed approach’s evaluation results are analyzed with the existing approaches, indicating the proposed framework’s efficiency. By using the proposed approach, an accuracy of 97 % is obtained.}
}
@article{BARBER2023100202,
title = {Transformative agents of change and investigative neurotechnologies: A qualitative study of psychedelic technology identities},
journal = {SSM - Qualitative Research in Health},
volume = {3},
pages = {100202},
year = {2023},
issn = {2667-3215},
doi = {https://doi.org/10.1016/j.ssmqr.2022.100202},
url = {https://www.sciencedirect.com/science/article/pii/S2667321522001640},
author = {Michaela Barber and John Gardner and Adrian Carter},
keywords = {Psychedelics, Innovation, Qualitative methodology, Mental health treatment},
abstract = {Clinical and basic science research with psychedelics is a nascent but rapidly growing field. Historically, psychedelics research has faced setbacks due to overenthusiasm, methodological challenges, and conflicting worldviews. This can in part be attributed to the dual heritages of spiritualism and science that ground western psychedelic use. Through semi-structured interviews with Australian researchers, we explore how issues of ontological conflict are playing out in contemporary research, using the analytical frame of promissory technology identities. We illustrate two identities currently at play that are invoked and negotiated by researchers: psychedelics as transformative agents of change and as investigational neurotechnologies. We argue that these identities represent differing ontological heritages in psychedelics research and may affect the design, interpretation and translation of clinical and neuroscientific psychedelics research. These identities exist in compromise, though not without tensions that may rupture or resolve as the field matures, such as priorisation of clinical research versus basic or preclinical research, Researchers appear sensitive to a need to pragmatically deploy both identities in order to engage the diverse stakeholders required for research and translation. This paper both provides a case study of promissory technology identities, and demonstraties the concept's usefulness as a framework for understanding the social factors that affect the production, communication and reception of evidence in the development of novel technologies.}
}
@article{TOYER2023100684,
title = {dalawhatyoumust: Kaaps, translingualism and linguistic citizenship in Cape Town, South Africa},
journal = {Discourse, Context & Media},
volume = {52},
pages = {100684},
year = {2023},
issn = {2211-6958},
doi = {https://doi.org/10.1016/j.dcm.2023.100684},
url = {https://www.sciencedirect.com/science/article/pii/S221169582300017X},
author = {Zaib Toyer and Amiena Peck},
keywords = {#dalawhatyoumust, Kaaps, South Africa, Linguistic citizenship, Translingual practice, Languaging},
abstract = {In 2016 Wayde Van Niekerk, a South African athlete of mixed-race heritage won an Olympic gold medal. In South Africa, his win caused hashtags such as #proudlysouthafrican, #blackexcellence and #colouredexcellence to trend online. By and large, these hashtags index the ongoing competitive discourses regarding nationalism, race and culture in Cape Town (cf. Author, 2018). Amongst these hashtags, however, was #dalawhatyoumust, a Kaaps hashtag generally meaning to “do what needs to be done”. Unlike the aforementioned hashtags, this one seems to cross the linguistic and racial divide despite its strong associations with Coloured11Coloured people are characterized as those of mixed descent and so was differentiated from Black and White during apartheid. This is a diverse group where heritage could stem from countries such as Britain, Netherlands, Malaysia, Madagascar, Ceylon, India, Java and other places in Africa. people on the Cape Flats. The seemingly effortless uptake of this hashtag by diverse South Africans suggest that it has somehow become unmoored of its ethnic and linguistic inception. We explore the use of this Kaaps hashtag as a form of translingual practice which is affect-laden and transportable across and between diverse users online and which promotes a particular “cool Capetonian” culture. Analyzing select posts from the #dalawhatyoumust thread on Facebook, we provide a nuanced look at #dalawhatyoumust as an uplifting genre which proleptically advises nameless viewers of the importance of self-actualization, determination and aspiration. Additionally, we include Goffman’s (1974) framing foundation to investigate how positivistic discourse has been rhizomatically taken up by a ‘realm’ of implicit collective users online. This research interrogates long-held ideological boundaries between Kaaps and legitimized Standard Afrikaans and standard English. We conclude with a focus on Kaaps hashtags as semiotic acts of Linguistic Citizenship (cf. Williams and Stroud, 2013) which allows for the conjoining of Kaaps with diverse audiences, complex trajectories, and an assortment of accompanying semiotics. Following Stroud (2018:3) we argue that this Kaaps hashtag has become a form of languaging that facilitates “…the building of broad affinities of speakers that cut across…divisions and borders, and that negotiate co-existence/co-habitation outside of common ground in recognition of equivocation”. In South Africa, division was the order of the day and when we explore contemporary ordinary moments posted by heterogenous users using #dalawhatyoumust (henceforth #dwym) we aim to explore the ordinariness of languaging which brings people together despite their race, linguistic background, and ethnicity, that is to say an affinity of ‘cool Capetonian’ style.}
}
@article{BADREDDINE2022103649,
title = {Logic Tensor Networks},
journal = {Artificial Intelligence},
volume = {303},
pages = {103649},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103649},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221002009},
author = {Samy Badreddine and Artur {d'Avila Garcez} and Luciano Serafini and Michael Spranger},
keywords = {Neurosymbolic AI, Deep learning and reasoning, Many-valued logics},
abstract = {Attempts at combining logic and neural networks into neurosymbolic approaches have been on the increase in recent years. In a neurosymbolic system, symbolic knowledge assists deep learning, which typically uses a sub-symbolic distributed representation, to learn and reason at a higher level of abstraction. We present Logic Tensor Networks (LTN), a neurosymbolic framework that supports querying, learning and reasoning with both rich data and abstract knowledge about the world. LTN introduces a fully differentiable logical language, called Real Logic, whereby the elements of a first-order logic signature are grounded onto data using neural computational graphs and first-order fuzzy logic semantics. We show that LTN provides a uniform language to represent and compute efficiently many of the most important AI tasks such as multi-label classification, relational learning, data clustering, semi-supervised learning, regression, embedding learning and query answering. We implement and illustrate each of the above tasks with several simple explanatory examples using TensorFlow 2. The results indicate that LTN can be a general and powerful framework for neurosymbolic AI.}
}
@article{SURAM2018374,
title = {Engineering design analysis utilizing a cloud platform},
journal = {Advances in Engineering Software},
volume = {115},
pages = {374-385},
year = {2018},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2017.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0965997817303733},
author = {Sunil Suram and Nordica A. MacCarty and Kenneth M. Bryden},
abstract = {In this paper we present a novel methodology for modeling engineered and other systems based on integrating a set of component models that are accessible as “model-as-a-service” components within a cloud platform. These component models can be combined together to form a systems model. The component models are stateless and web-enabled. The advantage of being web-enabled is that developers can use the models as API endpoints as opposed to library components, hence making the models themselves language agnostic and less restrictive in their use. These ideas are presented within the context of a previously published engineering model for the thermal analysis and preliminary design of a small biomass cookstove. In this paper the monolithic biomass cookstove model is separated into six independent, stateless component models supported by a generic model application infrastructure. Interaction between the models is orchestrated by a federated model system. Finally, the results of the cookstove from the monolithic model were compared with the distributed systems model. It was found that there was no change in the results. However, the systems model increased the time-to-solution due to network latency. However, the ability to share models and data via API endpoints, will likely offset the overall wall-clock time for model integration, since model developers do not have to make code changes. In conclusion, it is advantageous to build web-enabled component models for their easy reuse across multiple systems models.}
}
@incollection{SHA2025241,
title = {Chapter 16 - Large-scale genetic mapping for human brain asymmetry},
editor = {Costanza Papagno and Paul Corballis},
series = {Handbook of Clinical Neurology},
publisher = {Elsevier},
volume = {208},
pages = {241-254},
year = {2025},
booktitle = {Cerebral Asymmetries},
issn = {0072-9752},
doi = {https://doi.org/10.1016/B978-0-443-15646-5.00029-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443156465000294},
author = {Zhiqiang Sha and Clyde Francks},
keywords = {Brain asymmetry, Hemispheric specialization, Laterality, Genetic association, Gene expression, Neurodevelopment, Genomics, Microtubules, Chirality},
abstract = {Left–right asymmetry is an important aspect of human brain organization for functions including language and hand motor control, which can be altered in some psychiatric traits. The last 5 years have seen rapid advances in the identification of specific genes linked to variation in asymmetry of the human brain and/or handedness. These advances have been driven by a new generation of large-scale genome-wide association studies, carried out in samples ranging from roughly 16,000 to over 1.5 million participants. The implicated genes tend to be most active in the embryonic and fetal brain, consistent with early developmental patterning of brain asymmetry. Several of the genes encode components of microtubules or other microtubule-associated proteins. Microtubules are key elements of the internal cellular skeleton (cytoskeleton). A major challenge remains to understand how these genes affect, or even induce, the brain's left–right axis. Several of the implicated genes have also been associated with psychiatric or neurologic disorders, and polygenic dispositions to autism and schizophrenia have been associated with structural brain asymmetry. Knowledge of developmental mechanisms that lead to hemispheric specialization may ultimately help to define etiologic subtypes of brain disorders.}
}
@article{DIBICCARI2022101753,
title = {Building information modeling and building performance simulation interoperability: State-of-the-art and trends in current literature},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101753},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101753},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002117},
author = {Carla {Di Biccari} and Filippo Calcerano and Francesca D'Uffizi and Antonio Esposito and Massimo Campari and Elena Gigliarelli},
abstract = {The study outlines the state-of-art on interoperability between Building Information Modeling (BIM) and Building Performance Simulation (BPS). First, the paper organises the result of a Systematic Literature Review (SLR) on the topic into three main strategies to achieve interoperability: the scripting to automate the information transfer between BIM and BPS environments, the pipelines supported by current practices, and the MVD and IFC schema extension approach. Software applications, types of files involved, building features, life cycle phases and model geometry considered by the literature along with reported interoperability issues, are also analysed. Secondly, an expert review of grey literature focusing on EU funded projects, guidelines, reports, best practices and key initiatives on the field of BIM to BPS interoperability is presented. The study wraps up by reporting on six major research trends identified by the review and highlighting future developments. The results of the review seem to indicate that effective interoperability can be achieved with the definition of a commonly accepted strategy, integrating shared guidelines for modelling, a better inclusion of energy evaluations through the whole life cycle of a building and the upgrade of software application for the accurate production of information with open format exchange files.}
}
@article{HAN2021104262,
title = {Transformer based network for Open Information Extraction},
journal = {Engineering Applications of Artificial Intelligence},
volume = {102},
pages = {104262},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104262},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621001093},
author = {Jiabao Han and Hongzhi Wang},
keywords = {Open Information Extraction, Neural network, Transformer, Distant supervision},
abstract = {Research on Open Information Extraction (Open IE) has made great progress in recent years; it is the task that detects a group of structured, machine-readable statements usually represented in triple form or n-ary relation statements. Open IE is among the core areas of the territory of Natural Language Processing (NLP), and these extractions decompose grammatically complex sentences in a corpus into the relationships they represent, which can be leveraged for various downstream tasks. Even though a lot of work has been done in this direction, there are still many issues with the existing strategies. Most of the previous Open IE systems employ a group of artificially constructed patterns to detect and extract relational tuples from a sentence in a corpus, and these patterns are either automatically learned from annotated training examples or hand-crafted. Such an approach faces some issues, the first is that it requires a lot of manpower. Secondly, they used many NLP tools, therefore, error accumulation in the procedure can negatively impact the results. In this paper, we propose an Open IE approach based on the Transformer architecture. To verify our approach, we make a study using a large and public benchmark dataset, and the experimental results showed that our model achieves a better performance than many existing baselines.}
}
@article{LUONG20241416,
title = {System Architecture for Microservice-Based Data Exchange in the Manufacturing Plant Design Process},
journal = {Procedia CIRP},
volume = {130},
pages = {1416-1421},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.260},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124014173},
author = {Tommy Luong and David Hoffmann and Tobias Drees and Alfred Hypki and Bernd Kuhlenkötter},
keywords = {Data Exchange, Microservices, Engineering Process, Scalable Architectures},
abstract = {The data exchange between numerous sub-processes and involved suppliers in the manufacturing plant design process is complex due to the use of proprietary data formats from a variety of vendor-specific tools. Consequently, the current data transfer in the plant design process is characterized by data inconsistencies and increased error rates, requiring additional revisions when incorporating multidisciplinary proprietary data formats. Therefore, this publication proposes a microservice-based system architecture approach that aims to simplify data exchange between cross-manufacturer sub-processes in the manufacturing plant design process. For this purpose, the required components of the system architecture are specified, namely the microservices, an ontology database, and a backbone. The open standard AutomationML is used as the data exchange format, as it converts the information into an object-oriented data structure that summarizes the structure, topology, attributes, and roles of the objects being described. An exemplary processing with an initial set of six microservices is presented to demonstrate the functional overview for a simplified data exchange, covering basic data management functionalities and the conversion from AML to JSON. In order to validate ideas at an early stage of the development process, Python is selected for initial development. For the selection of a suitable software framework, a list of criteria is created to evaluate different solutions to build microservice architectures. A comparison of Flask, Django, and FastAPI, three well-established Python libraries, indicates that FastAPI meets the criteria to cover database integration, security and scalability with its built-in features. The resulting system architecture shows the potential to speed up the manufacturing plant design processes and indicates flexibility and scalability through the use of microservices.}
}
@article{WU2023143,
title = {A fractal-theory-based multi-agent model of the cyber physical production system for customized products},
journal = {Journal of Manufacturing Systems},
volume = {67},
pages = {143-154},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523000201},
author = {Wei Wu and Jianfeng Lu and Hao Zhang},
keywords = {Cyber physical production system, Smart factory, Fractal theory, Multi-agent system, Customized products},
abstract = {The manufacturing of customized products relies on precise control and close collaboration between distributed production processes. However, discrete manufacturing units cannot achieve consistently efficient collaboration owing to varying grades of smart manufacturing capabilities. Traditional cyber physical production systems have already achieved service- and data-driven resource allocation optimization, but the difficulty of discrete resource collaboration management remains. To address such an issue, this paper proposes a low-complexity and flexible factory operation model, the smart fractal factory (SFF). First, we propose the model definition of the cyber–physical fractal (CPF) based on fractal theory, whose information composition and evolution mechanism are used to formally describe the multi-scale self-similarity of structural characteristics and the operation mode of the smart manufacturing system. The core of the CPF is essential information element (EIEs) and typical information flow (TIFs), which are critical to upgrading manufacturing capabilities intelligently. Second, considering the realizability of the CPF pattern, the attribute configuration of EIEs and the operation mechanism of TIFs are established. Among them, self-organizing, self-adaptation, and self-learning TIFs are utilized for the construction of resource collaboration relationships, the evaluation and optimization of dynamic collaboration, and the enhancement of autonomous analysis or decision-making capabilities. Then, by leveraging agent modeling technology, we present a CPF implementation method to encapsulate distributed resources intelligently and homogeneously into smart manufacturing units that can autonomously optimize and collaborate. Thus, the multilayered architecture of traditional factories in the industrial Internet environment is transformed into an intelligent collaboration architecture based on CPF clusters in the SFF system. Finally, the usability and effectiveness of SFF were verified in a real manufacturing case of a customized product.}
}
@article{BAGHAPOUR20185,
title = {A computer-based approach for data analyzing in hospital’s health-care waste management sector by developing an index using consensus-based fuzzy multi-criteria group decision-making models},
journal = {International Journal of Medical Informatics},
volume = {118},
pages = {5-15},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618305343},
author = {Mohammad Ali Baghapour and Mohammad Reza Shooshtarian and Mohammad Reza Javaheri and Sina Dehghanifard and Razieh Sefidkar and Amir Fadaei Nobandegani},
keywords = {Process mining, Data mining, Hospital, Health-care waste, Index, Decision-making},
abstract = {Background
Proper Health-Care Waste Management (HCWM) and integrated documentation in this sector of hospitals require analyzing massive data collected by hospital’s health experts. This study presented a quantitative software-based index to assess the HCWM process performance by integrating ontology-based Multi-Criteria Group Decision-Making techniques and fuzzy modeling that were coupled with data mining. This framework represented the Complex Event Processing (CEP) and Corporate Performance Management (CPM) types of Process Mining in which a user-friendly software namely Group Fuzzy Decision-Making (GFDM) was employed for index calculation.
Findings
Assessing the governmental hospitals of Shiraz, Iran in 2016 showed that the proposed index was able to determine the waste management condition and clarify the blind spots of HCWM in the hospitals. The index values under 50 were found in some of the hospitals showing poor process performance that should be at the priority of optimization and improvement.
Conclusion
The proposed framework has distinctive features such as modeling the uncertainties (risks) in hospitals’ process assessment and flexibility enabling users to define the intended criteria, stakeholders, and number of hospitals. Having computer-aided approach for decision process also accelerates the index calculation as well as its accuracy which would contribute to more willingness of hospitals’ experts and other end-users to use the index in practice. The methodology could efficiently be employed as a tool for managing hospitals’ event logs and digital documentation in big data environment not only for the health-care waste management, but also in other administrative wards of hospitals.}
}
@article{JIANG2023101951,
title = {Multi-domain ubiquitous digital twin model for information management of complex infrastructure systems},
journal = {Advanced Engineering Informatics},
volume = {56},
pages = {101951},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101951},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000794},
author = {Yishuo Jiang and Ming Li and Wei Wu and Xiqiang Wu and Xiaoning Zhang and Xinyan Huang and Ray Y. Zhong and George G.Q. Huang},
keywords = {Digital twin, Ubiquitous model, Information management, Industrial infrastructure systems, Transdisciplinary management, Domain-Driven Design (DDD)},
abstract = {Digital twin is a comprehensive digital equivalent of an object or an activity, reflecting the semantic and geometric properties and behaviors through virtual models and data. Digital twin and related information technologies are widely used in the construction, operation and maintenance of infrastructure and facility. Transdisciplinary stakeholders are always involved in the long-term and cross-scene management of IoT and digital twin-enabled smart infrastructures and facilities. The intensive interactions among stakeholders often cause conflicts due to the variations in experience, knowledge, and interests. Moreover, with the change propagation of digital twins, cyber-physical resources can’t be efficiently and consistently established, connected, and utilized with multi-domain information through selective simplification and structured methods. This paper proposes a Ubiquitous Digital Twin model for the information management of complex infrastructure systems based on Domain-Driven Design. To achieve the unified and structured description, six domains are deployed in the proposed model with sequential or parallel tuples for shared understanding of overall system framework or specific functional modules. Three cases of one smart nuclear plant management scenario are hierarchically instantiated to evaluate the proposed model.}
}
@article{LIA2022,
title = {Development and validation of pyroptosis-related lncRNAs prediction model for bladder cancer},
journal = {Bioscience Reports},
volume = {42},
number = {1},
year = {2022},
issn = {1573-4935},
doi = {https://doi.org/10.1042/BSR20212253},
url = {https://www.sciencedirect.com/science/article/pii/S157349352200131X},
author = {Thongher Lia and Yanxiang Shao and Parbatraj Regmi and Xiang Li},
keywords = {bladder cancer, long non-coding RNA, prognostic biomarker, pyroptosis},
abstract = {Bladder cancer (BLCA) is one of the highly heterogeneous disorders accompanied by a poor prognosis. The present study aimed to construct a model based on pyroptosis-related long-stranded non-coding RNA (lncRNA) to evaluate the potential prognostic application in bladder cancer. The mRNA expression profiles of bladder cancer patients and corresponding clinical data were downloaded from the public database from The Cancer Genome Atlas (TCGA). Pyroptosis-related lncRNAs were identified by utilizing a co-expression network of pyroptosis-related genes and lncRNAs. The lncRNA was further screened by univariate Cox regression analysis. Finally, eight pyroptosis-related lncRNA markers were established using least absolute shrinkage and selection operator (Lasso) regression and multivariate Cox regression analyses. Patients were separated into high- and low-risk groups based on the performance value of the median risk score. Patients in the high-risk group had significantly poorer overall survival (OS) than those in the low-risk group (P<0.001). In multivariate Cox regression analysis, the risk score was an independent predictive factor of OS (HR > 1, P<0.01). The areas under the curve (AUCs) of the 3- and 5-year OS in the receiver operating characteristic (ROC) curve were 0.742 and 0.739, respectively. In conclusion, these eight pyroptosis-related lncRNA and their markers may be potential molecular markers and therapeutic targets for bladder cancer patients.}
}
@article{SOKOLOVA2023162,
title = {Co-producing ‘The Future(s) We Want’: How does political imagination translate into democratised knowledge-action models for sustainability transformations?},
journal = {Environmental Science & Policy},
volume = {144},
pages = {162-173},
year = {2023},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2023.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S1462901123000904},
author = {Tatiana Sokolova},
keywords = {Knowledge politics, Co-production, Science-policy interface, Sweden, Environmental governance, Democracy},
abstract = {Democratic societies face the challenge of effecting sustainability transformations, allowing for variously imagined futures and underpinned by a diversity of practices of knowledge production and action. This article investigates how political imagination of sustainable futures informs the ways knowledge and action are understood and linked in sustainability and research policy, and what potential implications this has for democratic transformative change. Empirically, the article analyses the overarching sustainability and research policies in Sweden, focusing on the central documents produced by the government and public research financiers. The analysis shows parallels between the conceptualisations of sustainability and knowledge-action, characterised by linearity, instrumentalisation of knowledge and circumscription of power-sharing spaces for knowledge creation against the background of endorsement of collaborations between academia and society. Such conceptualisations, apart from sending mixed signals to sustainability researchers and practitioners, potentially enable knowledge and action processes driven by impact, competitiveness and atomisation, precluding the exercise of the intrinsic value of democratic knowledge and action practices necessary for reflexive governance of transformations towards sustainability.}
}
@article{POMP2018249,
title = {A Web-based UI to Enable Semantic Modeling for Everyone},
journal = {Procedia Computer Science},
volume = {137},
pages = {249-254},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.024},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316296},
author = {André Pomp and Alexander Paulus and Daniel Klischies and Christian Schwier and Tobias Meisen},
keywords = {Semantic Modeling, Knowledge Graph, User Interface},
abstract = {Since companies generate and store large amounts of data daily in centralized systems such as data lakes, understanding data sets from different sources is becoming an increasingly complex task in dealing with data heterogeneity across domains. One solution for describing semantics of data sources is the use of semantic models based on an available vocabulary. However, creating detailed semantic models can be a challenging task for users who are not familiar with semantic modeling and today’s available tools. To overcome this challenge, we developed an intuitive and user-friendly interface, allowing data owners to define detailed semantic models for their data sources. The design of the user interface is based on an intensive requirement analysis gathered among several peers. It provides an intuitive mapping of semantic concepts to data attributes and the definition of relations between those concepts using drag and drop interaction. The user is given full modeling freedom as the insertion of semantic concepts and relations that are missing in the underlying vocabulary can be done on-demand and does not delay or impair the modeling process. Additionally, the refinement of the original detected data schema is supported with several operations. We built the interface into the semantic data platform ESKAPE, which already uses a flexible knowledge graph as underlying vocabulary and provides a detailed analysis of the data schema.}
}
@article{BAO2024103514,
title = {En route to becoming researcher-teachers? Chinese university EFL teachers’ boundary crossing in professional doctoral programs},
journal = {System},
volume = {127},
pages = {103514},
year = {2024},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2024.103514},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X24002963},
author = {Jie Bao and Guangwei Hu and Dezheng Feng},
keywords = {Teaching-research nexus, Chinese university EFL teachers, Professional doctoral programs, Professional development, Boundary crossing, Higher education},
abstract = {Drawing upon the theoretical framework of learning and identity work through boundary crossing, this study examined how Chinese university English-as-a-foreign-language (EFL) teachers negotiated their researcher-teacher identities through attending professional doctoral programs in education and applied linguistics. Adopting a case study design, data were collected through narrative frames and follow-up interviews with eight participants, with journal entries and other documents drawn on as additional data. Analysis of the data revealed four distinctive trajectories of identity development: from intuitive teacher to rigorous teacher, from passive follower to critical inquirer, from idealistic researcher-teacher to qualified researcher first, and from EFL teacher to EFL academic. In general, while boundary crossing in professional doctoral programs facilitated the participants' progression towards a researcher-teacher identity, the construction of the teaching-research nexus in the participants' professional work followed a non-linear route and took variegated forms. The study thus yielded insights into the complexities of teachers’ navigation of the teaching-research nexus through doctoral education. Based on the findings, research-mindset-informed practice is proposed as an alternative approach to the teaching-research nexus that complements previous approaches such as evidence-based practice and practice-based research.}
}
@article{PANZARELLA2024116522,
title = {MAATrica: a measure for assessing consistency and methods in medicinal and nutraceutical chemistry papers},
journal = {European Journal of Medicinal Chemistry},
volume = {273},
pages = {116522},
year = {2024},
issn = {0223-5234},
doi = {https://doi.org/10.1016/j.ejmech.2024.116522},
url = {https://www.sciencedirect.com/science/article/pii/S0223523424004021},
author = {Giulia Panzarella and Alessandro Gallo and Sandra Coecke and Maddalena Querci and Francesco Ortuso and Martin Hofmann-Apitius and Pierangelo Veltri and Jürgen Bajorath and Stefano Alcaro},
keywords = {Medicinal chemistry, Nutraceuticals, Information extraction, Text mining, Research metrics},
abstract = {The growing number of scientific papers and document sources underscores the need for methods capable of evaluating the quality of publications. Researchers who are looking for relevant papers for their studies need ways to assess the scientific value of these documents. One approach involves using semantic search engines that can automatically extract important knowledge from the growing body of text. In this study, we introduce a new metric called “MAATrica,” which serves as the foundation for an innovative method designed to evaluate research papers. MAATrica offers a new way to analyze and categorize text, focusing on the consistency of research documents in the life sciences, particularly in the fields of medicinal and nutraceutical chemistry. This method utilizes semantic descriptions to cover in silico experiments, as well as in vitro and in vivo essays. Created to aid in evaluation processes like peer review, MAATrica uses toolkits and semantic applications to build the proposed measure, identify scientific entities, and gather information. We have applied MAATrica to roughly 90,000 papers and present our findings here.}
}
@article{WANG2024111950,
title = {A comprehensive survey on interactive evolutionary computation in the first two decades of the 21st century},
journal = {Applied Soft Computing},
volume = {164},
pages = {111950},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111950},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624007245},
author = {Yanan Wang and Yan Pei},
keywords = {Interactive evolutionary computation, Evolutionary computation, Computational intelligence, Humanized computational intelligence, Human-machine interaction},
abstract = {Interactive evolutionary computation (IEC) has demonstrated significant success in addressing numerous real-world problems that are challenging to quantify mathematically or are inadequately evaluated using conventional computational models. This success arises from IEC’s ability to effectively amalgamate evolutionary computation (EC) algorithms with expert knowledge and user preferences. These problems encompass the creative and personalized generation of products, art, and sound; the design optimization of communication systems, environments, and pharmaceuticals; and expert support in areas such as portfolio selection and hearing aid fitting, among others. Despite significant advancements in IEC over the past two decades, no major comprehensive survey encompassing all aspects of IEC research has been conducted since 2001. This article aims to address this gap by providing a comprehensive survey and an enriched definition and scope of IEC, along with innovative ideas for future research in this field. The proposed IEC definition more clearly reflects the mechanism and current research status of the IEC. Additionally, the survey categorizes IEC research into five distinct directions from a problem-oriented perspective: interactive evolutionary computation algorithms, IEC algorithm improvements, evolutionary multi-objective optimization (EMO) with IEC, human perception studies with IEC, and IEC applications. Each direction is meticulously explored, elucidating its contents and key features, while providing a concise summary of pertinent IEC studies. Finally, the survey investigates several promising future trends in IEC, analyzing them through the lens of these five directions and considering the current perspective of computational intelligence, artificial intelligence, and human-machine interaction.}
}
@article{SIGNORELLI2021103168,
title = {Reasoning about conscious experience with axiomatic and graphical mathematics},
journal = {Consciousness and Cognition},
volume = {95},
pages = {103168},
year = {2021},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2021.103168},
url = {https://www.sciencedirect.com/science/article/pii/S1053810021000945},
author = {Camilo Miguel Signorelli and Quanlong Wang and Bob Coecke},
keywords = {Consciousness, Conscious experience, Conscious agents, Compositionality, Graphical calculi, Mathematical consciousness science, Monoidal categories, Phenomenology, Unity of consciousness},
abstract = {We cast aspects of consciousness in axiomatic mathematical terms, using the graphical calculus of general process theories (a.k.a symmetric monoidal categories and Frobenius algebras therein). This calculus exploits the ontological neutrality of process theories. A toy example using the axiomatic calculus is given to show the power of this approach, recovering other aspects of conscious experience, such as external and internal subjective distinction, privacy or unreadability of personal subjective experience, and phenomenal unity, one of the main issues for scientific studies of consciousness. In fact, these features naturally arise from the compositional nature of axiomatic calculus.}
}
@article{KAZIM2022100526,
title = {On the sui generis value capture of new digital technologies: The case of AI},
journal = {Patterns},
volume = {3},
number = {7},
pages = {100526},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100526},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922001234},
author = {Emre Kazim and Enzo Fenoglio and Airlie Hilliard and Adriano Koshiyama and Catherine Mulligan and Markus Trengove and Abigail Gilbert and Arthur Gwagwa and Denise Almeida and Phil Godsiff and Kaska Porayska-Pomsta},
keywords = {value theory, information theory, digital assets, ontology, artificial intelligence},
abstract = {Summary
Much of the academic interest surrounding the emergence of new digital technologies has focused on forwarding the engineering literature, concentrating on the potential opportunities (economic, innovation, etc.) and harms (ethics, climate, etc.), with less focus on the foundational and theoretical shifts brought about by these technologies (e.g., what are “digital things”? What is the ontological nature and state of phenomena produced by and expressed in terms of digital products? Are there distinctions between the traditional conceptions of digital and non-digital technologies?. We investigate the question of what value is being expressed by an algorithm, which we conceptualize in terms of a digital asset, defining a digital asset as a valued digital thing that is derived from a particular digital technology (in this case, an algorithmic system). Our main takeaway is to invite the reader to consider artificial intelligence as a representation of the capture of value sui generis and that this may be a step change in the capture of value vis à vis the emergence of digital technologies.}
}
@article{PALIWAL20225345,
title = {XGBRS Framework Integrated with Word2Vec Sentiment Analysis for Augmented Drug Recommendation},
journal = {Computers, Materials and Continua},
volume = {72},
number = {3},
pages = {5345-5362},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.025858},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822009511},
author = {Shweta Paliwal and Amit Kumar Mishra and Ram Krishn Mishra and Nishad Nawaz and M. Senthilkumar},
keywords = {Recommendation system, Word2Vec, XGBOOST, sentiment analysis, natural language processing (NLP), machine learning},
abstract = {Machine Learning is revolutionizing the era day by day and the scope is no more limited to computer science as the advancements are evident in the field of healthcare. Disease diagnosis, personalized medicine, and Recommendation system (RS) are among the promising applications that are using Machine Learning (ML) at a higher level. A recommendation system helps inefficient decision-making and suggests personalized recommendations accordingly. Today people share their experiences through reviews and hence designing of recommendation system based on users’ sentiments is a challenge. The recommendation system has gained significant attention in different fields but considering healthcare, little is being done from the perspective of drugs, disease, and medical recommendations. This study is engrossed in designing a recommendation system that is based on the fusion of sentiment analysis and radiant boosting. The polarity of the sentiments is analyzed through user reviews and the processed data is fed into the Extreme Gradient Boosting (XGBOOST) framework to generate the drug recommendation. To establish the applicability of the concept a comparative study is performed between the proposed approach and the existing approaches.}
}
@article{MANN2021107533,
title = {Retrosynthesis prediction using grammar-based neural machine translation: An information-theoretic approach},
journal = {Computers & Chemical Engineering},
volume = {155},
pages = {107533},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107533},
url = {https://www.sciencedirect.com/science/article/pii/S0098135421003112},
author = {Vipul Mann and Venkat Venkatasubramanian},
keywords = {Machine learning, Retrosynthetic analysis, Artificial intelligence, Synthesis Planning, Reaction prediction},
abstract = {Retrosynthetic prediction is one of the main challenges in chemical synthesis because it requires a search over the space of plausible chemical reactions that often results in complex, multi-step, branched synthesis trees for even moderately complex organic reactions. Here, we propose an approach that performs single-step retrosynthesis prediction using SMILES grammar-based representations in a neural machine translation framework. Information-theoretic analyses of such grammar-representations reveal that they are superior to SMILES representations and are better-suited for machine learning tasks due to their underlying redundancy and high information capacity. We report the top-1 prediction accuracy of 43.8% (syntactic validity 95.6%) and maximal fragment (MaxFrag) accuracy of 50.4%. Comparing our model’s performance with previous work that used character-based SMILES representations demonstrate significant reduction in grammatically invalid predictions and improved prediction accuracy. Fewer invalid predictions for both known and unknown reaction class scenarios demonstrate the model’s ability to learn the underlying SMILES grammar efficiently.}
}
@article{ZGHEIB2023160,
title = {Towards an ML-based semantic IoT for pandemic management: A survey of enabling technologies for COVID-19},
journal = {Neurocomputing},
volume = {528},
pages = {160-177},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223000139},
author = {Rita Zgheib and Ghazar Chahbandarian and Firuz Kamalov and Haythem El Messiry and Ahmed Al-Gindy},
keywords = {COVID-19, Machine learning, Ontologies, Internet of things, Cloud architecture, Survey},
abstract = {The connection between humans and digital technologies has been documented extensively in the past decades but needs to be evaluated through the current global pandemic. Artificial Intelligence(AI), with its two strands, Machine Learning (ML) and Semantic Reasoning, has proven to be a great solution to provide efficient ways to prevent, diagnose and limit the spread of COVID-19. IoT solutions have been widely proposed for COVID-19 disease monitoring, infection geolocation, and social applications. In this paper, we investigate the usage of the three technologies for handling the COVID-19 pandemic. For this purpose, we surveyed the existing ML applications and algorithms proposed during the pandemic to detect COVID-19 disease using symptom factors and image processing. The survey includes existing approaches including semantic technologies and IoT systems for COVID-19. Based on the survey result, we classified the main challenges and the solutions that could solve them. The study proposes a conceptual framework for pandemic management and discusses challenges and trends for future research.}
}
@article{KARAGIANNIS2022103631,
title = {The OMiLAB Digital Innovation environment: Agile conceptual models to bridge business value with Digital and Physical Twins for Product-Service Systems development},
journal = {Computers in Industry},
volume = {138},
pages = {103631},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103631},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000264},
author = {Dimitris Karagiannis and Robert Andrei Buchmann and Wilfrid Utz},
keywords = {Digital twin, Physical twin, Smart Product-service Systems, Agile modeling method engineering, OMiLAB, Domain-specific conceptual modeling},
abstract = {OMiLAB is a community of practice which offers a digital ecosystem bringing together open technologies to investigate and apply conceptual modeling methods for varying purposes and domains. One of the core value propositions is a dedicated Digital Innovation environment comprising several toolkits and workspaces, designed to support Product-Service Systems (PSS) prototyping – a key ingredient for PSS lifecycle management. At the core of this environment is a notion of Agile Digital Twin – a conceptual representation that can be tailored with knowledge engineering means to bridge the semantic and functional gap between a business perspective (focusing on value creation) and an engineering perspective (focusing on cyber-physical proofs-of-concept). To facilitate this bridging, the hereby proposed environment orchestrates, across three abstraction layers, methods such as Design Thinking, Agile Modeling Method Engineering and Model-driven Engineering to turn Ideation into smart Product-Service Systems experiments, in a laboratory setting. The proposed environment was built following Design Science principles. It addresses the problem of historically-disconnected skills required for Digital Innovation projects and it provides a testbed for feasibility experimentation. For design-oriented, artifact building research, a higher Technology Readiness Level can thus be achieved (compared to the level that idea development methods typically attain).}
}
@article{TOZZI202212,
title = {Colonizing the rains: Disentangling more-than-human technopolitics of drought protection in the archive},
journal = {Geoforum},
volume = {135},
pages = {12-24},
year = {2022},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2022.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0016718522001464},
author = {Arianna Tozzi and Stefan Bouzarovski and Caitlin Henry},
keywords = {More-than-human, Technopolitics, Rainfed agriculture, Ontologies, Water, India},
abstract = {Preoccupations for the widening gap between irrigated and rainfed areas are central to debates addressing the agrarian crisis in semi-arid India. Yet policies are driven by a catch-up mentality that points towards an irrigated model of agriculture, demarcating rainfed areas as spaces of rural marginality. To unpack the historical causes behind this ‘irrigation at all costs’ mindset, the paper traces how the rainfed/irrigated gap became constituted through policies of drought-protection during British colonial time. Focussing on the Bombay Deccan after the establishment of the British Raj, it frames drought-protection as a more-than-human technopolitics to explore the performative power of technopolitical practices to bring water worlds into being. Through a critical reading of the colonial archive, we trace the ontological work of drought-protection as a practice that rearranged existing human-water relations to materialize a reality of water ‘as irrigation’. Grounded on linear and predictable flows, this irrigated ontology divided the landscape along an irrigated-as-protected and rainfed-as-unprotected logic. Encountering a world that followed geographies of water ‘as precipitation’ however created sites of contestation blurring the partition envisioned by engineering plans. Rather than the imposition of hydrological power from above, colonizing the rains represents a contested project whereby certain water worlds became present and real while others discarded and less real. Contributing to scholarship shaking the ontological ground underneath water management regimes, we suggest a reflexive turn for these practices, as they must confront their power to materialize (water) realities and the possibility to enact a decolonial technopolitics beyond water’s liquid form.}
}
@article{DADZIE201851,
title = {Structuring visual exploratory analysis of skill demand},
journal = {Journal of Web Semantics},
volume = {49},
pages = {51-70},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2017.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570826817300690},
author = {A.-S. Dadzie and E.M. Sibarani and I. Novalija and S. Scerri},
keywords = {Domain modeling, Knowledge discovery, Visual exploration, Ontology-guided visual analytics, Trend identification, Demand analysis},
abstract = {The analysis of increasingly large and diverse data for meaningful interpretation and question answering is handicapped by human cognitive limitations. Consequently, semi-automatic abstraction of complex data within structured information spaces becomes increasingly important, if its knowledge content is to support intuitive, exploratory discovery. Exploration of skill demand is an area where regularly updated, multi-dimensional data may be exploited to assess capability within the workforce to manage the demands of the modern, technology- and data-driven economy. The knowledge derived may be employed by skilled practitioners in defining career pathways, to identify where, when and how to update their skillsets in line with advancing technology and changing work demands. This same knowledge may also be used to identify the combination of skills essential in recruiting for new roles. To address the challenges inherent in exploring the complex, heterogeneous, dynamic data that feeds into such applications, we investigate the use of an ontology to guide structuring of the information space, to allow individuals and institutions to interactively explore and interpret the dynamic skill demand landscape for their specific needs. As a test case we consider the relatively new and highly dynamic field of Data Science, where insightful, exploratory data analysis and knowledge discovery are critical. We employ context-driven and task-centred scenarios to explore our research questions and guide iterative design, development and formative evaluation of our ontology-driven, visual exploratory discovery and analysis approach, to measure where it adds value to users’ analytical activity. Our findings reinforce the potential in our approach, and point us to future paths to build on.}
}
@incollection{GREENBERG2025349,
title = {15 - Ethics for artificial agents: Toward commensurate capability and self-regulation},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Hesham Fouad},
booktitle = {Interdependent Human-Machine Teams},
publisher = {Academic Press},
pages = {349-371},
year = {2025},
isbn = {978-0-443-29246-0},
doi = {https://doi.org/10.1016/B978-0-443-29246-0.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292460000018},
author = {Ariel M. Greenberg},
keywords = {Advance directive, Artificial intelligence, Autonomous systems, Design policy, Duties, Machine ethics, Moral judgment and decision-making},
abstract = {In this chapter, we tackle the charge to make artificial agent self-regulation on par with their expanding capabilities. We begin by offering research and development schemes focused on installing in machines the perception, knowledge, and reasoning required to support ethics sensitivity and to produce agentic behavior compliant with the principles of nonmaleficence, beneficence, and responsibility. Next, we review evolutionary, psychological, and neuroscientific accounts of the phenomenology and emergence of moral judgment and decision-making that speak to the natural coupling of capability and self-regulation. Finally, we discuss sweeping themes of endogeneity, generality, mentalization, legibility, and duty responsiveness meant to guide how we ought to design an appropriate balance of capability and self-regulation in artificial agents.}
}
@article{HANG202529,
title = {Evolving biomaterials design from trial and error to intelligent innovation},
journal = {Acta Biomaterialia},
volume = {197},
pages = {29-47},
year = {2025},
issn = {1742-7061},
doi = {https://doi.org/10.1016/j.actbio.2025.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S174270612500176X},
author = {Ruiyue Hang and Xiaohong Yao and Long Bai and Ruiqiang Hang},
keywords = {Biomaterials design, Orthogonal experiment, High-throughput screening, Artificial intelligence, Innovation},
abstract = {The design and exploration of biomaterials plays a pivotal role in many fields, including medical and engineering. The prevailing approach to biomaterials discovery relies on orthogonal experiments, with repeated attempts to optimize experimental conditions. This method has proven invaluable in gaining experience, but it is also inefficient and challenging to predict the behavior of complex systems. The advent of high-throughput screening (HTS) techniques has led to a notable enhancement in the efficiency of biomaterials development, enabling researchers to assess a vast array of material combinations within a relatively short timeframe. Nevertheless, the emergence of artificial intelligence (AI) has been the catalyst for a new era in biomaterials design. AI has markedly accelerated the development of new materials by enabling the prediction of material properties through machine learning (ML) and deep learning models, as well as optimizing the design pipeline. This review will present a systematic overview of the development of biomaterials design technology. It will also explore the integration of AI with HTS technology and envisage the potential of AI-driven materials design in biomaterials for the future.
Statement of significance
The design and synthesis of biomaterials have undergone substantial shifts, reflecting evolving research paradigms. High-throughput screening has emerged as a broad and efficient alternative to traditional free-form combination methods in biomaterial design. The advent of artificial intelligence (AI) enables personalized biomaterial design and, as a transformative tool in biomaterial development, is poised to redefine the field and offer long-term solutions for its advancement. Building on these advancements, this review systematically summarizes the evolution of biomaterial design, offering insights into the future trajectory of the field.}
}
@article{ARBIB201883,
title = {From cybernetics to brain theory, and more: A memoir},
journal = {Cognitive Systems Research},
volume = {50},
pages = {83-145},
year = {2018},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718301360},
author = {Michael A. Arbib},
keywords = {Action-oriented perception, Ape, Architecture, Artificial intelligence, Automata theory, Basal ganglia, Brain theory, Cerebellum, Cerebral cortex, Cognitive science, Computational neuroscience, Cybernetics, Frog, Hippocampus, Human, Language evolution, Linguistics, Monkey, Rat, Robotics, Schema theory, Social implications, Systems theory, Theological implications},
abstract = {While structured as an autobiography, this memoir exemplifies ways in which classic contributions to cybernetics (e.g., by Wiener, McCulloch & Pitts, and von Neumann) have fed into a diversity of current research areas, including the mathematical theory of systems and computation, artificial intelligence and robotics, computational neuroscience, linguistics, and cognitive science. The challenges of brain theory receive special emphasis. Action-oriented perception and schema theory complement neural network modeling in analyzing cerebral cortex, cerebellum, hippocampus, and basal ganglia. Comparative studies of frog, rat, monkey, ape and human not only deepen insights into the human brain but also ground an EvoDevoSocio view of “how the brain got language.” The rapprochement between neuroscience and architecture provides a recent challenge. The essay also assesses some of the social and theological implications of this broad perspective.}
}
@article{LI2022101884,
title = {Performance benchmark on semantic web repositories for spatially explicit knowledge graph applications},
journal = {Computers, Environment and Urban Systems},
volume = {98},
pages = {101884},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2022.101884},
url = {https://www.sciencedirect.com/science/article/pii/S0198971522001284},
author = {Wenwen Li and Sizhe Wang and Sheng Wu and Zhining Gu and Yuanyuan Tian},
keywords = {Triple store, Property graph databases, Ontology, Knowledge graph, Relational database, Ontology-based Data Access (OBDA)},
abstract = {Knowledge graph has become a cutting-edge technology for linking and integrating heterogeneous, cross-domain datasets to address critical scientific questions. As big data has become prevalent in today's scientific analysis, semantic data repositories that can store and manage large knowledge graph data have become critical in successfully deploying spatially explicit knowledge graph applications. This paper provides a comprehensive evaluation of the popular semantic data repositories and their computational performance in managing and providing semantic support for spatial queries. There are three types of semantic data repositories: (1) triple store solutions (RDF4j, Fuseki, GraphDB, Virtuoso), (2) property graph databases (Neo4j), and (3) an Ontology-Based Data Access (OBDA) approach (Ontop). Experiments were conducted to compare each repository's efficiency (e.g., query response time) in handling geometric, topological, and spatial-semantic related queries. The results show that Virtuoso achieves the overall best performance in both non-spatial and spatial-semantic queries. The OBDA solution, Ontop, has the second-best query performance in spatial and complex queries and the best storage efficiency, requiring the least data-to-RDF conversion efforts. Other triple store solutions suffer from various issues that cause performance bottlenecks in handling spatial queries, such as inefficient memory management and lack of proper query optimization.}
}
@article{SILVA2022726,
title = {Rule-based Clinical Decision Support System using the OpenEHR Standard},
journal = {Procedia Computer Science},
volume = {201},
pages = {726-731},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922005117},
author = {Sarah Tifany Silva and Francini Hak and José Machado},
keywords = {clinical decision support system, decision module language, guideline, openEHR, rules},
abstract = {Clinical Decision Support Systems present diverse ramifications that ultimately help healthcare professionals in their decision-making process. These systems can manifest themselves in the form of computerized guidelines that, depending on their goal and stipulated directives, help and optimize healthcare professional’s daily tasks. However, nowadays there is a certain resistance from the healthcare community towards using these systems, with valid justifications such as: the absence of transparency in defined rules, the lack of interoperability within these systems and the difficulty in their usage as they prove themselves unintuitive and hard. With the intention of culminating these flaws, a clinical decision support system was developed based on the openEHR module in order to ensure standardization and semantic interoperability. This system will be oriented towards management and creation of clinical guidelines based on the specifications of the openEHR standard.}
}
@article{SCHNEIDER2020103402,
title = {Design of knowledge-based systems for automated deployment of building management services},
journal = {Automation in Construction},
volume = {119},
pages = {103402},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103402},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520309821},
author = {Georg F. Schneider and Georgios D. Kontes and Haonan Qiu and Filipe J. Silva and Mircea Bucur and Jakub Malanik and Zdenek Schindler and Panos Andriopolous and Pablo {de Agustin-Camacho} and Ander Romero-Amorrortu and Gunnar Grün},
keywords = {Building management services, Knowledge-based systems, Energy efficiency, Knowledge engineering, Ontology},
abstract = {Despite its high potential, the building's sector lags behind in reducing its energy demand. Tremendous savings can be achieved by deploying building management services during operation, however, the manual deployment of these services needs to be undertaken by experts and it is a tedious, time and cost consuming task. It requires detailed expert knowledge to match the diverse requirements of services with the present constellation of envelope, equipment and automation system in a target building. To enable the widespread deployment of these services, this knowledge-intensive task needs to be automated. Knowledge-based methods solve this task, however, their widespread adoption is hampered and solutions proposed in the past do not stick to basic principles of state of the art knowledge engineering methods. To fill this gap we present a novel methodological approach for the design of knowledge-based systems for the automated deployment of building management services. The approach covers the essential steps and best practices: (1) representation of terminological knowledge of a building and its systems based on well-established knowledge engineering methods; (2) representation and capturing of assertional knowledge on a real building portfolio based on open standards; and (3) use of the acquired knowledge for the automated deployment of building management services to increase the energy efficiency of buildings during operation. We validate the methodological approach by deploying it in a real-world large-scale European pilot on a diverse portfolio of buildings and a novel set of building management services. In addition, a novel ontology, which reuses and extends existing ontologies is presented.}
}
@article{JONES2018,
title = {Novel Approach to Cluster Patient-Generated Data Into Actionable Topics: Case Study of a Web-Based Breast Cancer Forum},
journal = {JMIR Medical Informatics},
volume = {6},
number = {4},
year = {2018},
issn = {2291-9694},
doi = {https://doi.org/10.2196/medinform.9162},
url = {https://www.sciencedirect.com/science/article/pii/S2291969418000674},
author = {Josette Jones and Meeta Pradhan and Masoud Hosseini and Anand Kulanthaivel and Mahmood Hosseini},
keywords = {data interpretation, natural language processing, patient-generated information, social media, statistical analysis, infodemiology},
abstract = {Background
The increasing use of social media and mHealth apps has generated new opportunities for health care consumers to share information about their health and well-being. Information shared through social media contains not only medical information but also valuable information about how the survivors manage disease and recovery in the context of daily life.
Objective
The objective of this study was to determine the feasibility of acquiring and modeling the topics of a major online breast cancer support forum. Breast cancer patient support forums were selected to discover the hidden, less obvious aspects of disease management and recovery.
Methods
First, manual topic categorization was performed using qualitative content analysis (QCA) of each individual forum board. Second, we requested permission from the Breastcancer.org Community for a more in-depth analysis of the postings. Topic modeling was then performed using open source software Machine Learning Language Toolkit, followed by multiple linear regression (MLR) analysis to detect highly correlated topics among the different website forums.
Results
QCA of the forums resulted in 20 categories of user discussion. The final topic model organized >4 million postings into 30 manageable topics. Using qualitative analysis of the topic models and statistical analysis, we grouped these 30 topics into 4 distinct clusters with similarity scores of ≥0.80; these clusters were labeled Symptoms & Diagnosis, Treatment, Financial, and Family & Friends. A clinician review confirmed the clinical significance of the topic clusters, allowing for future detection of actionable items within social media postings. To identify the most significant topics across individual forums, MLR demonstrated that 6 topics—based on the Akaike information criterion values ranging from −642.75 to −412.32—were statistically significant.
Conclusions
The developed method provides an insight into the areas of interest and concern, including those not ascertainable in the clinic. Such topics included support from lay and professional caregivers and late side effects of therapy that consumers discuss in social media and may be of interest to clinicians. The developed methods and results indicate the potential of social media to inform the clinical workflow with regards to the impact of recovery on daily life.}
}
@incollection{PSAROMMATIS2022243,
title = {Chapter 9 - The role of big data analytics in the context of modeling design and operation of manufacturing systems},
editor = {Dimitris Mourtzis},
booktitle = {Design and Operation of Production Networks for Mass Personalization in the Era of Cloud Technology},
publisher = {Elsevier},
pages = {243-275},
year = {2022},
isbn = {978-0-12-823657-4},
doi = {https://doi.org/10.1016/B978-0-12-823657-4.00012-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128236574000129},
author = {Foivos Psarommatis and Paul Arthur Dreyfus and Dimitris Kiritsis},
keywords = {Big data, Industry 4.0, Mass customization, Semantics, Zero defect manufacturing},
abstract = {Contemporary industries are generating a huge amount of data coming from a high variety of sources, such as machine controllers, corporate systems, sensors, operators, and more. Therefore, the need for the utilization of the information generated, also known as big data, has emerged. Although data importance is well known, very often in manufacturing industries data are abundant and often underexploited leading to poor performance and efficiency. Big data are part of Industry 4.0 concept and also serve as key element for the Industry 4.0 technologies. But still there is a lot of ground to be covered in the exploitation and industrialization of those technologies. Modeling, design and operation of modern manufacturing systems, where Industry 4.0 technologies are applied, are three fundamental processes in the contemporary manufacturing domain that require constant attention and continuous optimization in order for the manufacturer to stay competitive and improve the efficiency and reduce the environmental impact.}
}
@article{BAILEY2024100647,
title = {Knowing versus doing: Children's social conceptions of and behaviors toward virtual reality agents},
journal = {International Journal of Child-Computer Interaction},
volume = {40},
pages = {100647},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100647},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000151},
author = {Jakki O. Bailey and J. Isabella Schloss},
keywords = {Virtual reality, Children, Characters, Interpersonal distance, Ontological understanding, Embodied agents},
abstract = {Virtual reality (VR) can blur fantasy and reality for children by replacing their physical world with artificial stimuli. This immersive technology often includes intelligent and interactive embodied agents. In this within-participant study, we investigated 5- to 9-year-old children's (N = 25) social conceptions of and behaviors toward embodied agents in VR that represented different probabilities of existence in their daily lives (i.e., a probable child, an improbable giraffe, and an impossible Muppet). Participants rated the child and the giraffe agents significantly higher as social living beings than they rated the Muppet agent. When tasked with walking up to each embodied agent, significantly more children chose to approach the giraffe agent first rather than the child and Muppet agents. However, children stood significantly closer to the child agent, and significantly more children spontaneously reached out to try to touch the Muppet agent. Finally, children expressed strong emotions (amazement, excitement, happiness, fear, worry) toward all three embodied agents, with the giraffe evoking the most positive and the Muppet the most negative emotions. These results show that types of embodied agents in VR significantly impact children's conscious and unconscious social conceptions and behaviors differently, with implications for future interventions.}
}
@article{CAMPBELL2022109656,
title = {Expanding the biocultural benefits of species distribution modelling with Indigenous collaborators: Case study from northern Australia},
journal = {Biological Conservation},
volume = {274},
pages = {109656},
year = {2022},
issn = {0006-3207},
doi = {https://doi.org/10.1016/j.biocon.2022.109656},
url = {https://www.sciencedirect.com/science/article/pii/S0006320722002099},
author = {Bridget L. Campbell and Rachael V. Gallagher and Emilié J. Ens},
keywords = {Biocultural diversity, Indigenous knowledge, Multiple evidence base, Yolŋu, Species distribution models, Culturally significant species},
abstract = {Urgent calls for biocultural conservation highlight the need to engage with Indigenous peoples, whose lands cover some of the most biodiverse regions on Earth. In these places, Indigenous knowledge can be strong, especially for culturally significant species; however, this knowledge tends to be overlooked and deemed incompatible with dominant Western systems of conservation decision-making. The practical mechanics of how Indigenous and Western knowledge systems can be respectfully synergized to inform species conservation decision-making on a regional scale are not well documented. In collaboration with Yolŋu Indigenous rangers from northern Australia, we used a ‘multiple evidence based’ approach to synergize knowledge from senior Yolŋu custodians with Western scientific species distribution modelling (SDM) to explore distribution shifts and biocultural knowledge of two culturally significant, near threatened (critical weight range) mammals: wan'kurra, (Northern Brown Bandicoot, Isoodon macrourus) and marrŋu/rupu (Common Brushtail Possum, Trichosurus vulpecula). Historical Yolŋu species sightings and seasonal observations and the SDM revealed spatial contractions of both species to the northern and mesic regions of the Laynhapuy Indigenous Protected Area over the last 50 years. Knowledge custodians were concerned about the loss of these species and associated cultural knowledge in younger generations. Such cross-cultural knowledge gathering can guide biocultural conservation decision-making and facilitate intergenerational knowledge transmission of threatened cultural knowledge. This approach to collaborative cross-cultural fauna research can be applied globally, and we argue is imperative in contexts where: species and cultures are under threat, and where Indigenous knowledge and peoples can provide insight to inform inclusive biocultural conservation strategies.}
}
@article{KOS2018818,
title = {A speech-based distributed architecture platform for an intelligent ambience},
journal = {Computers & Electrical Engineering},
volume = {71},
pages = {818-832},
year = {2018},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2017.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617321079},
author = {Marko Kos and Matej Rojc and Andrej Žgank and Zdravko Kačič and Damjan Vlaj},
keywords = {Automatic speech recognition, Text-to-speech synthesis, Acoustic modelling, Distributed computing, Mobile robotic unit, Intelligent ambience, Supportive environment},
abstract = {In the paper, a speech-based platform for intelligent ambience and/or supportive environment applications is presented. The platform has a distributed architecture, which enables extended connectivity and support for multiple intelligent ambience services. The mobile unit Genesis is an integral part of the distributed platform, enabling interaction between several users and the environment. Furthermore, the sophisticated client/server platform's architecture incorporates robust speech recognition and text-to-speech synthesis engines for more natural human-machine interaction between users and the mobile unit Genesis. Both engines are multilingual oriented. Although the whole system is developed for the Slovenian language, it can be quickly adapted for other languages when appropriate language resources are available. With high speaker independent speech recognition accuracy and low command-to-operation delay, Genesis proves to have good manoeuvrability and it is easy to operate even by a non-experienced operator.}
}
@article{PABLE202094,
title = {Integrating linguistic relativity},
journal = {Language & Communication},
volume = {75},
pages = {94-102},
year = {2020},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2020.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0271530920300823},
author = {Adrian Pablé},
keywords = {Integrational linguistics, Languages as second-order concepts, Linguistic holism, Sapir-Whorf hypothesis, Reocentric surrogationalism, Scientific paradigms},
abstract = {This article considers linguistic relativism and the Sapir-Whorf hypothesis from the point of view of integrational linguistics (Harris, 1998). Taking Roy Harris' scattered comments on linguistic relativity as its starting-point, this article explores to what extent integrational theory adheres itself to some form of relativism. The article then considers a second major current in philosophy of language providing an explanation of how languages relate to reality, namely surrogationalism, which Harris divides into a reocentric and a psychocentric version (Harris, 1996). While linguistic relativism of a Saussurean, Whorfian or Quinean stamp provide holistic explanations, none can satisfy the onto-epistemological needs of Science, which presupposes a reocentric approach to language. Integrationism constitutes the third position, incompatible with either linguistic determinism or surrogationalism.}
}
@article{LI2024113918,
title = {Integration of Kupffer cells into human iPSC-derived liver organoids for modeling liver dysfunction in sepsis},
journal = {Cell Reports},
volume = {43},
number = {3},
pages = {113918},
year = {2024},
issn = {2211-1247},
doi = {https://doi.org/10.1016/j.celrep.2024.113918},
url = {https://www.sciencedirect.com/science/article/pii/S2211124724002468},
author = {Yang Li and Yunzhong Nie and Xia Yang and Yang Liu and Xiaoshan Deng and Yoshihito Hayashi and Riana Plummer and Qinglin Li and Na Luo and Toshiharu Kasai and Takashi Okumura and Yu Kamishibahara and Takemasa Komoto and Takuya Ohkuma and Satoshi Okamoto and Yumiko Isobe and Kiyoshi Yamaguchi and Yoichi Furukawa and Hideki Taniguchi},
keywords = {human iPSCs, liver organoids, Kupffer cells, liver inflammation, EMP, sepsis, disease model, self-recovery, TLR4, hematopoiesis},
abstract = {Summary
Maximizing the potential of human liver organoids (LOs) for modeling human septic liver requires the integration of innate immune cells, particularly resident macrophage Kupffer cells. In this study, we present a strategy to generate LOs containing Kupffer cells (KuLOs) by recapitulating fetal liver hematopoiesis using human induced pluripotent stem cell (hiPSC)-derived erythro-myeloid progenitors (EMPs), the origin of tissue-resident macrophages, and hiPSC-derived LOs. Remarkably, LOs actively promote EMP hematopoiesis toward myeloid and erythroid lineages. Moreover, supplementing with macrophage colony-stimulating factor (M-CSF) proves crucial in sustaining the hematopoietic population during the establishment of KuLOs. Exposing KuLOs to sepsis-like endotoxins leads to significant organoid dysfunction that closely resembles the pathological characteristics of the human septic liver. Furthermore, we observe a notable functional recovery in KuLOs upon endotoxin elimination, which is accelerated by using Toll-like receptor-4-directed endotoxin antagonist. Our study represents a comprehensive framework for integrating hematopoietic cells into organoids, facilitating in-depth investigations into inflammation-mediated liver pathologies.}
}
@article{ACTON201986,
title = {What is the Sargasso Sea? The problem of fixing space in a fluid ocean},
journal = {Political Geography},
volume = {68},
pages = {86-100},
year = {2019},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0962629818301240},
author = {Leslie Acton and Lisa M. Campbell and Jesse Cleary and Noella J. Gray and Patrick N. Halpin},
keywords = {Oceans, Wet ontology, Territoriality, Conservation, High seas, Materiality},
abstract = {The political boundaries used to territorialize ocean spaces are often negotiated as largely social relations, with little attention to material aspects. Material aspects of ocean spaces include physical forces, interacting life, and constant transformation. In this paper, we use Steinberg and Peters' (2015) “wet ontology” and concepts of the hydrosphere, liquidity, dynamism, and emergence to reflect on how the Sargasso Sea was located in geographic space through analysis of scientific data that revealed its complex materiality. Drawing from policy documents, white papers, presentations, and 14 semi-structured interviews with scientists, government officials, and NGO representatives, we then trace how the Sargasso Sea Alliance produced the linear boundaries that define the Sargasso Sea as an Ecologically and Biologically Significant Marine Area and, later, as an area for international collaboration on its conservation. Although the data used to locate the Sargasso Sea demonstrated its mobility and complexity, policymaking processes calling for legible boundaries produced a simplified and fixed Sargasso Sea that obscures its “wet” materiality. This ‘fixed’ Sargasso Sea was created, in part, to test the potential of existing high seas governing bodies in the lead-up to current negotiations for an international legally-binding instrument for high seas governance; this case thus demonstrates how the social relations that construct existing understandings of territory in oceans may continue to dictate policy options, even as new, more dynamic management techniques are developed. We conclude with a discussion of emerging governance possibilities that may better address and account for the entangled material and social realities of oceans.}
}
@article{QIAN2024100914,
title = {Model informed precision medicine of Chinese herbal medicines formulas–A multi-scale mechanistic intelligent model},
journal = {Journal of Pharmaceutical Analysis},
volume = {14},
number = {4},
pages = {100914},
year = {2024},
issn = {2095-1779},
doi = {https://doi.org/10.1016/j.jpha.2023.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095177923002873},
author = {Yuanyuan Qian and Xiting Wang and Lulu Cai and Jiangxue Han and Zhu Huang and Yahui Lou and Bingyue Zhang and Yanjie Wang and Xiaoning Sun and Yan Zhang and Aisong Zhu},
keywords = {Chinese herbal medicine formulas, Precision medicine, Mathematical modeling, Systems biology, Coronary heart disease, Depression, Ischemia-reperfusion, Inflammation},
abstract = {Recent trends suggest that Chinese herbal medicine formulas (CHM formulas) are promising treatments for complex diseases. To characterize the precise syndromes, precise diseases and precise targets of the precise targets between complex diseases and CHM formulas, we developed an artificial intelligence-based quantitative predictive algorithm (DeepTCM). DeepTCM has gone through multilevel model calibration and validation against a comprehensive set of herb and disease data so that it accurately captures the complex cellular signaling, molecular and theoretical levels of traditional Chinese medicine (TCM). As an example, our model simulated the optimal CHM formulas for the treatment of coronary heart disease (CHD) with depression, and through model sensitivity analysis, we calculated the balanced scoring of the formulas. Furthermore, we constructed a biological knowledge graph representing interactions by associating herb-target and gene-disease interactions. Finally, we experimentally confirmed the therapeutic effect and pharmacological mechanism of a novel model-predicted intervention in humans and mice. This novel multiscale model opened up a new avenue to combine “disease syndrome” and “macro micro” system modeling to facilitate translational research in CHM formulas.}
}
@article{GIANNAKOPOULOU2021106590,
title = {Automated formalization of structured natural language requirements},
journal = {Information and Software Technology},
volume = {137},
pages = {106590},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106590},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000707},
author = {Dimitra Giannakopoulou and Thomas Pressburger and Anastasia Mavridou and Johann Schumann},
keywords = {Requirements, Structured natural language, Temporal logic, Verification, Testing, Analysis},
abstract = {The use of structured natural languages to capture requirements provides a reasonable trade-off between ambiguous natural language and unintuitive formal notations. There are two major challenges in making structured natural language amenable to formal analysis: (1) formalizing requirements as formulas that can be processed by analysis tools and (2) ensuring that the formulas conform to the semantics of the structured natural language. fretish is a structured natural language that incorporates features from existing research and from NASA applications. Even though fretish is quite expressive, its underlying semantics is determined by the types of four fields: scope, condition, timing, and response. Each combination of field types defines a template with Real-Time Graphical Interval Logic (RTGIL) semantics. We have developed a framework that constructs temporal logic formulas for each template compositionally, from its fields. The compositional nature of our algorithms facilitates maintenance and extensibility. Our goal is to be inclusive not only in terms of language expressivity, but also in terms of requirements analysis tools that we can interface with. For this reason we generate metric-temporal logic formulas with (1) exclusively future-time operators, over both finite and infinite traces, and (2) exclusively past-time operators. To establish trust in the produced formalizations for each template, our framework: (1) extensively tests the generated formulas against the template semantics and (2) proves equivalence between its past-time and future-time formulas. Our approach is available through the open-source tool fret and has been used to capture and analyze requirements for a Lockheed Martin Cyber–Physical System challenge.}
}
@incollection{JOKINEN20199,
title = {Chapter 1 - Multimodal open-domain conversations with robotic platforms},
editor = {Xavier Alameda-Pineda and Elisa Ricci and Nicu Sebe},
booktitle = {Multimodal Behavior Analysis in the Wild},
publisher = {Academic Press},
pages = {9-26},
year = {2019},
series = {Computer Vision and Pattern Recognition},
isbn = {978-0-12-814601-9},
doi = {https://doi.org/10.1016/B978-0-12-814601-9.00025-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128146019000250},
author = {Kristiina Jokinen and Graham Wilcock},
keywords = {Multimodal communication, Constructive dialog model, Human–robot interaction, Open-domain dialogs},
abstract = {The chapter discusses how to move from closed-domain dialogs to open-domain dialogs, and from speech-based dialogs to multimodal dialogs with speech, gestures, and gaze, using robot agents. We briefly describe the Constructive Dialog Model, the foundation for our work. Management of topic shifts is one of the challenges for open-domain dialogs, and we describe how Wikipedia can be used for topic shifts as well as an open-domain knowledge source. Multimodal issues are illustrated by our multimodal WikiTalk open-domain robot dialog system. Two future research directions are discussed: the use of domain ontologies in dialog systems and the need to integrate robots with the Internet of Things.}
}
@article{ZHANG2025104558,
title = {MER-GCN: Reasoning about attacking group behaviors using industrial control system attack knowledge graphs},
journal = {Computers & Security},
volume = {157},
pages = {104558},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104558},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825002470},
author = {Xiao Zhang and Yingxu Lai and Xinrui Dong and Xinyu Xu},
keywords = {Attacking behaviors reasoning, Graph embedding, Knowledge graph, Link prediction, Industrial control system security},
abstract = {To enhance the ability of Intrusion Detection Systems (IDSs) to detect complex attacks on Industrial Control Systems (ICSs), we developed the ICS attack knowledge graph (ICS-Attack-KG). This graph focuses on learning the correlations across attack groups’ behaviors to enable cross-group threat intelligence sharing. Based on the knowledge learned, the graph can reason about potential attack behaviors more comprehensively and accurately, which is beneficial for IDS to update its rulebase and detect complex attacking behaviors. However, data sparsity caused by the difficulty in obtaining threat intelligence of advanced attack group, as well as the data complexity brought by learning correlations across attack groups’ behaviors, increases the difficulty of embedding and reasoning on a knowledge graph. To address these issues, we introduce a novel link prediction model named the Multi-Edge Relation Graph Convolutional Network (MER-GCN). This model overcomes the limitations of data sparsity by embedding global graph structure into relation vectors, enabling it to supply missing information through adjacent or related nodes. To better learn the correlations across attack groups’ behaviors, MER-GCN sets attack group as relations and involves three-dimensional convolutional computation and relational projections to capture pattern sharing and differences across relational subgraphs. Empirical evaluation results demonstrate that the model significantly improves the accuracy and completeness of reasoning about attack groups’ behaviors in ICS. On the ICS-Attack-KG dataset, the model achieves an 11.3% improvement in mean reverse rank (MRR) over the state-of-the-art MR-GCN model. Additionally, the model also improved by 6.8% on the widely recognized Reuters dataset, demonstrating the model’s good generalization ability on a common dataset.}
}
@article{YANG2025691,
title = {A Novel KIF4A-related Model for Predicting Immunotherapy Response and Prognosis in Kidney Renal Clear Cell Carcinoma},
journal = {Combinatorial Chemistry & High Throughput Screening},
volume = {28},
number = {4},
pages = {691-710},
year = {2025},
issn = {1386-2073},
doi = {https://doi.org/10.2174/0113862073296897240212114403},
url = {https://www.sciencedirect.com/science/article/pii/S1386207325000692},
author = {Guang Hua Yang and Xu Dong and Xi Feng Wei and Ran Lu Liu and Chao Wang},
keywords = {Kidney renal clear cell carcinoma, KIF4A, Pan-Cancer, prognosis, risk score, tumour mutation burden},
abstract = {Background
The efficacy of chemotherapy in treating Kidney Renal Clear Cell Carcinoma (KIRC) is limited, whereas immunotherapy has shown some promising clinical outcomes. In this context, KIF4A is considered a potential therapeutic target for various cancers. Therefore, identifying the mechanism of KIF4A that can predict the prognosis and immunotherapy response of KIRC would be of significant importance.
Methods
Based on the TCGA Pan-Cancer dataset, the prognostic significance of the KIF4A expression across 33 cancer types was analyzed by univariate Cox algorithm. Furthermore, overlapping differentially expressed genes (DEGs1) between the KIF4A high- and low-expression groups and DEGs2 between the KIRC and normal groups were also analyzed. Machine learning and Cox regression algorithms were performed to obtain biomarkers and construct a prognostic model. Finally, the role of KIF4A in KIRC was analyzed using quantitative real-time PCR, transwell assay, and EdU experiment.
Results
Our analysis revealed that KIF4A was significant for the prognosis of 13 cancer types. The highest correlation with KIF4A was found for KICH among the tumour mutation burden (TMB) indicators. Subsequently, a prognostic model developed with UBE2C, OTX1, PPP2R2C, and RFLNA was obtained and verified with the Renal Cell Cancer-EU/FR dataset. There was a positive correlation between risk score and immunotherapy. Furthermore, the experiment results indicated that KIF4A expression was considerably increased in the KIRC group. Besides, the proliferation, migration, and invasion abilities of KIRC tumor cells were significantly weakened after KIF4A was knocked out.
Conclusion
We identified four KIF4A-related biomarkers that hold potential for prognostic assessment in KIRC. Specifically, early implementation of immunotherapy targeting these biomarkers may yield improved outcomes for patients with KIRC.}
}
@article{DORE2018131,
title = {The Routledge Handbook of Language and Humor},
journal = {Journal of Pragmatics},
volume = {138},
pages = {131-134},
year = {2018},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378216618306829},
author = {Margherita Dore}
}
@article{HASSAN2024128058,
title = {Unfolding Explainable AI for Brain Tumor Segmentation},
journal = {Neurocomputing},
volume = {599},
pages = {128058},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128058},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224008294},
author = {Muhammad Hassan and Ahmed Ameen Fateh and Jieqiong Lin and Yijiang Zhuang and Guisen Lin and Hairui Xiong and Zhou You and Peiwu Qin and Hongwu Zeng},
keywords = {Segmentation, Brain Tumor, Machine Learning, Deep Learning, Explainable AI, Neuro-Symbolic Learning},
abstract = {Brain tumor segmentation (BTS) has been studied from handcrafted engineered features to conventional machine learning (ML) methods, followed by the cutting-edge deep learning approaches. Each recent approach has attempted to overcome the challenges of previous methods and brought conveniences in efficacy, throughput, computation, explainability, investigation, and interpretability. Recently, deep learning (DL) algorithms show excellent performance regarding diverse fields, including image process, computer vision, health analytics, autonomous vehicles, and natural language processes; however, ultimately impediment in making the artificial intelligence explainable and interpretable to clinicians while dealing with critical health informatics and radiomics. Besides the sophisticated deep learning models for brain tumor segmentation, notorious notions like explainability, investigation, trust, and interpretability of DL raised significant concerns for clinicians in their domains. Among many DL methods, the neuro-symbolic learning (NSL) concept has gained more attention as it can contribute to explainable and interpretable AI. In the current study, we survey the prominent approaches, from handcrafted engineering conventional ML to deep learning algorithms, highlight the challenges in DL algorithms, and propose NSL architectures for BTS. Compared to existing surveys, our study not only outlines handcrafted to DL methods for BTS but also proposed explainable and interpretable pipelines appropriate for clinical practices. Our study can better facilitate novice learners in explainable AI and propose efficient, robust, interpretable DL models to facilitate the diagnosis, prognosis, and treatment of BTS.}
}
@article{FERRIS20203874,
title = {Symposium review: Dairy Brain—Informing decisions on dairy farms using data analytics},
journal = {Journal of Dairy Science},
volume = {103},
number = {4},
pages = {3874-3881},
year = {2020},
issn = {0022-0302},
doi = {https://doi.org/10.3168/jds.2019-17199},
url = {https://www.sciencedirect.com/science/article/pii/S0022030220301454},
author = {Michael C. Ferris and Adam Christensen and Steven R. Wangen},
keywords = {data management and decision support tools, Dairy Brain, machine learning and optimization, application programming interface},
abstract = {ABSTRACT
Management decisions can be informed by near-real-time data streams to improve the economics of the farm and to positively benefit the overall health of a dairy herd or the larger environment. Decision support tools can use data management services and analytics to exploit data streams from farm and other economic, health, and agricultural sources. We will describe a decision support tool that couples data analytics tools to underlying cow, herd, and economic data with an application programming interface. This interface allows the user to interact with a collection of dairy applications without fully exposing the intricacies of the underlying system model and understand the effects of different decisions on outputs of interest. The collection of these applications will form the basis of the Dairy Brain decision support system, which will provide management suggestions to farmers at a single animal or farm level. Dairy operations data will be gathered, cleaned, organized, and disseminated through an agricultural data hub, exploiting newly developed ontologies for integration of multiple data sources. Models of feed efficiency, culling, or other dairy operations (such as large capital expenditures, outsourcing opportunities, and interactions with regulators) form the basis of analytical approaches, operationalized via tools that help secure information and control uncertainties. The applications will be independently generated to provide flexibility, and use tools and modeling approaches from the data science, simulation, machine learning, and optimization disciplines to provide specific recommendations to decision makers. The Dairy Brain is a decision support system that couples data analytics tools with a suite of applications that integrate cow, herd, and economic data to inform management, operational, and animal health improving practices. Research challenges that remain include dealing with increased variability as predictions go from herd or pen level down to individual cow level and choosing the appropriate tool or technique to deal with a specific problem.}
}
@article{BOUDAA2021,
title = {DATAtourist:},
journal = {International Journal of Decision Support System Technology},
volume = {13},
number = {2},
year = {2021},
issn = {1941-6296},
doi = {https://doi.org/10.4018/IJDSST.2021040104},
url = {https://www.sciencedirect.com/science/article/pii/S1941629621000021},
author = {Boudjemaa Boudaa and Djamila Figuir and Slimane Hammoudi and Sidi Mohamed Benslimane},
keywords = {Constraint-Based Recommender System, DATAtourisme, DATAtourist, Knowledge-Based Recommendation, Personalized Recommendation},
abstract = {ABSTRACT
Collaborative and content-based recommender systems are widely employed in several activity domains helping users in finding relevant products and services (i.e., items). However, with the increasing features of items, the users are getting more demanding in their requirements, and these recommender systems are becoming not able to be efficient for this purpose. Built on knowledge bases about users and items, constraint-based recommender systems (CBRSs) come to meet the complex user requirements. Nevertheless, this kind of recommender systems witnesses a rarity in research and remains underutilised, essentially due to difficulties in knowledge acquisition and/or in their software engineering. This paper details a generic software architecture for the CBRSs development. Accordingly, a prototype mobile application called DATAtourist has been realized using DATAtourisme ontology as a recent real-world knowledge source in tourism. The DATAtourist evaluation under varied usage scenarios has demonstrated its usability and reliability to recommend personalized touristic points of interest.}
}
@article{GRAVES2024100051,
title = {Modeling morality and spirituality in artificial chaplains},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {1},
pages = {100051},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2024.100051},
url = {https://www.sciencedirect.com/science/article/pii/S2949882124000112},
author = {Mark Graves}
}
@article{YASSAEEMEYBODI2020104081,
title = {CAMND: Comparative analysis of metabolic network decomposition based on previous and two new criteria, a web based application},
journal = {Biosystems},
volume = {189},
pages = {104081},
year = {2020},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2019.104081},
url = {https://www.sciencedirect.com/science/article/pii/S0303264719304460},
author = {Fatemeh {Yassaee Meybodi} and Akram Emdadi and Abolfazl Rezvan and Changiz Eslahchi},
keywords = {Metabolic network, Decomposition, Criteria, Package},
abstract = {Metabolic networks can model the behavior of metabolism in the cell. Since analyzing the whole metabolic networks is not easy, network modulation is an important issue to be investigated. Decomposing metabolic networks is a strategy to obtain better insight into metabolic functions. Additionally, decomposing these networks facilitates using computational methods, which are very slow when applied to the original genome-scale network. Several methods have been proposed for decomposing of the metabolic network. Therefore, it is necessary to evaluate these methods by suitable criteria. In this study, we introduce a web server package for decomposing of metabolic networks with 10 different methods, 9 datasets and the ability of computing 12 criteria, to evaluate and compare the results of different methods using ten previously defined and two new criteria which are based on Chebi ontology and Co-expression_of_Enzymes information. This package visualizes the obtained modules via “gephi” software. The ability of this package is that the user can examine whether two metabolites or reactions are in the same module or not. The functionality of the package can be easily extended to include new datasets and criteria. It also has the ability to compare the results of novel methods with the results of previously developed methods. The package is implemented in python and is available at http://eslahchilab.ir/softwares/dmn.}
}
@article{BAK2020100999,
title = {Smart Block: A visual block language and its programming environment for IoT},
journal = {Journal of Computer Languages},
volume = {60},
pages = {100999},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2020.100999},
url = {https://www.sciencedirect.com/science/article/pii/S2590118420300599},
author = {Nayeon Bak and Byeong-Mo Chang and Kwanghoon Choi},
keywords = {Visual block language, IoT, SmartThings, SmartApp ECA rule, IoTa, Blockly},
abstract = {A visual block programming language allows users to make their own programs by dragging and dropping graphic blocks rather than by writing the program. This enables users who are not proficient in programming to create programs easily. Although existing studies have applied this idea to programming Internet of things (IoT) applications, existing visual language tools have certain limitations in terms of expressiveness, extensibility, and error prevention. In this paper, we propose a visual block language called Smart Block for SmartThings home automation, together with a visual programming environment that supports the three properties. We designed the visual block language based on the Internet of things automation (IoTa) calculus, a core calculus for IoT automation that generalizes event-condition-action (ECA) rules. Each ECA rule specifies that when an event occurs, and if a condition is met, a certain action is performed. Smart Block supports writing IoT applications in the ECA style and is implemented with Google Blockly, a client-side JavaScript library for creating visual block languages. Smart Block can help users develop reliable SmartApps by checking for redundancy, inconsistency, and circularity in the ECA rules before generating the code. We demonstrate that Smart Block can build 54 out of 56 (96.4%) of the SmartApps provided by the official SmartThings IDE. Furthermore, a user study with 33 participants shows that our approach, based on the foundation of the IoTa calculus, is understandable for users.}
}
@article{LV20241,
title = {A Graph Convolution Model for Intelligent Datum Features Selection},
journal = {Procedia CIRP},
volume = {129},
pages = {1-6},
year = {2024},
note = {18th CIRP Conference on Computer Aided Tolerancing (CAT2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124011387},
author = {Wenbo Lv and Chaolong Zhang and Yuanping Xu and Chao Kong and Jin Jin and Tukun Li and Xiangqian Jiang and Dan Tang and Jian Huang and Zongzheng Zhang},
keywords = {Datum, GCN;Diagram structure representation, Geometric elements, Eigenvectors},
abstract = {The datum is a crucial component in tolerance specification, which is the foundation for the selections of geometric tolerances and tolerance principles. Currently, intelligent datum reasoning is largely based on logical rules that are mainly extracted from human experience, resulting in the high uncertainty and low efficiency. To tickle these issues, this study proposes a data selection model based on the GCN (Graph Convolutional Networks), In the devised model, the different geometric features of a workpiece are represented in a graph structure. The geometric, spatial, and assembly relationships, as well as positioning features are computed to obtain vectorized representations of the different geometric features, which serve as inputs to the constructed GCN model. Then, based on the GCN, a datum discriminant classifier has been developed on the training samples. To enhance the classifier accuracy, multiple GCN layers are employed for training, with the output of each GCN module added to a list. Ultimately, the outputs of all GCN modules are concatenated and subjected to classification prediction through fully connected layers. Datum specifications are established based on the classification of geometric features. The effectiveness and feasibility of this method are validated through case studies, e.g. rear floor crossbeams, with comparative results indicating a similarity rate of 85.19% with manually designed outcomes.}
}
@article{VODYAHO2025107823,
title = {Run time dynamic digital twins and dynamic digital twins networks},
journal = {Future Generation Computer Systems},
volume = {172},
pages = {107823},
year = {2025},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2025.107823},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X25001189},
author = {Alexander Vodyaho and Radhakrishnan Delhibabu and Dmitry I. Ignatov and Nataly Zhukova},
keywords = {Digital twin, Digital twin networks, Dynamic digital threads, Model synthesis},
abstract = {Digital twins are widely used for building various types of cyber–physical systems. There are a huge number of publications devoted to the use of digital twins in production systems. Much less attention is paid to the issues of building runtime digital twins. The article describes an approach to building complex distributed cyber–physical systems with a high level of architectural dynamics built on fog and edge computing platforms based on the use of digital twins. The issues of implementing runtime digital twins and distributed systems of runtime digital twins are considered. The requirements to runtime digital twins are defined. Typical problem statements for constructing and maintaining a runtime digital twin system are formulated. A reference architecture of a dynamic runtime digital twin is proposed, which includes a model of the observed system (or the object) and a model processor. The dynamic model of the observed and managed system is considered as a key element of the digital twin. Possible approaches to the synthesis of built-in models of runtime digital twins are discussed. Examples of using the proposed approach to solve practical problems are given. The described approach may be of interest to specialists involved in research and development of various types of information systems implemented on Internet of Things platforms, such as smart cities, smart transport, medical information systems, etc. It is proposed to conduct further research and development in the areas of creating human digital twins.}
}
@article{HOU2023,
title = {Generate Analysis-Ready Data for Real-world Evidence: Tutorial for Harnessing Electronic Health Records With Advanced Informatic Technologies},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/45662},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123003916},
author = {Jue Hou and Rachel Zhao and Jessica Gronsbell and Yucong Lin and Clara-Lea Bonzel and Qingyi Zeng and Sinian Zhang and Brett K Beaulieu-Jones and Griffin M Weber and Thomas Jemielita and Shuyan Sabrina Wan and Chuan Hong and Tianrun Cai and Jun Wen and Vidul {Ayakulangara Panickan} and Kai-Li Liaw and Katherine Liao and Tianxi Cai},
keywords = {electronic health records, real-world evidence, data curation, medical informatics, randomized controlled trials, reproducibility},
abstract = {Although randomized controlled trials (RCTs) are the gold standard for establishing the efficacy and safety of a medical treatment, real-world evidence (RWE) generated from real-world data has been vital in postapproval monitoring and is being promoted for the regulatory process of experimental therapies. An emerging source of real-world data is electronic health records (EHRs), which contain detailed information on patient care in both structured (eg, diagnosis codes) and unstructured (eg, clinical notes and images) forms. Despite the granularity of the data available in EHRs, the critical variables required to reliably assess the relationship between a treatment and clinical outcome are challenging to extract. To address this fundamental challenge and accelerate the reliable use of EHRs for RWE, we introduce an integrated data curation and modeling pipeline consisting of 4 modules that leverage recent advances in natural language processing, computational phenotyping, and causal modeling techniques with noisy data. Module 1 consists of techniques for data harmonization. We use natural language processing to recognize clinical variables from RCT design documents and map the extracted variables to EHR features with description matching and knowledge networks. Module 2 then develops techniques for cohort construction using advanced phenotyping algorithms to both identify patients with diseases of interest and define the treatment arms. Module 3 introduces methods for variable curation, including a list of existing tools to extract baseline variables from different sources (eg, codified, free text, and medical imaging) and end points of various types (eg, death, binary, temporal, and numerical). Finally, module 4 presents validation and robust modeling methods, and we propose a strategy to create gold-standard labels for EHR variables of interest to validate data curation quality and perform subsequent causal modeling for RWE. In addition to the workflow proposed in our pipeline, we also develop a reporting guideline for RWE that covers the necessary information to facilitate transparent reporting and reproducibility of results. Moreover, our pipeline is highly data driven, enhancing study data with a rich variety of publicly available information and knowledge sources. We also showcase our pipeline and provide guidance on the deployment of relevant tools by revisiting the emulation of the Clinical Outcomes of Surgical Therapy Study Group Trial on laparoscopy-assisted colectomy versus open colectomy in patients with early-stage colon cancer. We also draw on existing literature on EHR emulation of RCTs together with our own studies with the Mass General Brigham EHR.}
}
@article{KUO2024340,
title = {Artificial intelligence approach for linking competences in nuclear field},
journal = {Nuclear Engineering and Technology},
volume = {56},
number = {1},
pages = {340-356},
year = {2024},
issn = {1738-5733},
doi = {https://doi.org/10.1016/j.net.2023.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1738573323004539},
author = {Vincent Kuo and Günther H. Filz and Jussi Leveinen},
keywords = {Nuclear knowledge management, Competence management, Latent semantic analysis, Community of practice, Natural language processing, Artificial intelligence, Semantic technology},
abstract = {Bridging traditional experts’ disciplinary boundaries is important for nuclear knowledge management systems. However, expert competences are often described in unstructured texts and require substantial human effort to link related competences across disciplines. The purpose of this research is to develop and evaluate a natural language processing approach, based on Latent Semantic Analysis, to enable the automatic linking of related competences across different disciplines and communities of practice. With datasets of unstructured texts as input training data, our results show that the algorithm can readily identify nuclear domain-specific semantic links between words and concepts. We discuss how our results can be utilized to generate a quantitative network of links between competences across disciplines, thus acting as an enabler for identifying and bridging communities of practice, in nuclear and beyond.}
}
@article{MENG2023541,
title = {Solving Arithmetic Word Problems of Entailing Deep Implicit Relations by Qualia Syntax-Semantic Model},
journal = {Computers, Materials and Continua},
volume = {77},
number = {1},
pages = {541-555},
year = {2023},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.041508},
url = {https://www.sciencedirect.com/science/article/pii/S1546221823001169},
author = {Hao Meng and Xinguo Yu and Bin He and Litian Huang and Liang Xue and Zongyou Qiu},
keywords = {Arithmetic word problem, implicit quantity relations, qualia syntax-semantic model},
abstract = {Solving arithmetic word problems that entail deep implicit relations is still a challenging problem. However, significant progress has been made in solving Arithmetic Word Problems (AWP) over the past six decades. This paper proposes to discover deep implicit relations by qualia inference to solve Arithmetic Word Problems entailing Deep Implicit Relations (DIR-AWP), such as entailing commonsense or subject-domain knowledge involved in the problem-solving process. This paper proposes to take three steps to solve DIR-AWPs, in which the first three steps are used to conduct the qualia inference process. The first step uses the prepared set of qualia-quantity models to identify qualia scenes from the explicit relations extracted by the Syntax-Semantic (S2) method from the given problem. The second step adds missing entities and deep implicit relations in order using the identified qualia scenes and the qualia-quantity models, respectively. The third step distills the relations for solving the given problem by pruning the spare branches of the qualia dependency graph of all the acquired relations. The research contributes to the field by presenting a comprehensive approach combining explicit and implicit knowledge to enhance reasoning abilities. The experimental results on Math23K demonstrate hat the proposed algorithm is superior to the baseline algorithms in solving AWPs requiring deep implicit relations.}
}
@incollection{SCARCELLO2025342,
title = {Artificial Intelligence},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {342-350},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00109-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027001093},
author = {Francesco Scarcello and Simona Nisticò and Luigi Palopoli},
keywords = {Agents, Artificial intelligence, Knowledge representation, Logic, Machine learning, Natural language processing, Planning, Reasoning, Robotics, Vision},
abstract = {Artificial Intelligence is going to support every human activity, from communication to healthcare, business, entertaining, and so forth. It is a very active field of research with an uncountable spectrum of applications. A picture of the field is provided, with a brief overview of history, recent achievements, and directions to detailed information on the various facets of AI.}
}
@article{APOLINARIO2024100725,
title = {FingerCI: Writing industrial process specifications from network traffic},
journal = {International Journal of Critical Infrastructure Protection},
volume = {47},
pages = {100725},
year = {2024},
issn = {1874-5482},
doi = {https://doi.org/10.1016/j.ijcip.2024.100725},
url = {https://www.sciencedirect.com/science/article/pii/S1874548224000660},
author = {Filipe Apolinário and Nelson Escravana and Éric Hervé and Miguel L. Pardal and Miguel Correia},
keywords = {Cyber–physical systems, Network profiling, Process mining, Critical infrastructures, Specification-based intrusion detection systems},
abstract = {Critical infrastructures (CIs) are often targets of cyber-attacks, requiring accurate process specifications to identify and defend against incidents. However, discrepancies between these specifications and real-world CI conditions arise due to the costly process of manual specification by experts. This paper introduces FingerCI, a method for automatically generating CI process specifications through network traffic analysis and physical behavior modeling. By defining a Specification Language that integrates with existing systems, FingerCI extracts industrial process specifications without infrastructure changes or downtime. The specifications include a behavior model that validates physical correctness. We evaluated FingerCI on a digital twin of an airport baggage handling system, achieving 99.98% fitness to observed behavior. Our method improves cybersecurity and fault detection with high accuracy.}
}
@article{WAEGELL202039,
title = {Reformulating Bell's theorem: The search for a truly local quantum theory},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {70},
pages = {39-50},
year = {2020},
issn = {1355-2198},
doi = {https://doi.org/10.1016/j.shpsb.2020.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1355219819302175},
author = {Mordecai Waegell and Kelvin J. McQueen},
abstract = {The apparent nonlocality of quantum theory has been a persistent concern. Einstein et al. (1935) and Bell (1964) emphasized the apparent nonlocality arising from entanglement correlations. While some interpretations embrace this nonlocality, modern variations of the Everett-inspired many worlds interpretation try to circumvent it. In this paper, we review Bell's “no-go” theorem and explain how it rests on three axioms, local causality, no superdeterminism, and one world. Although Bell is often taken to have shown that local causality is ruled out by the experimentally confirmed entanglement correlations, we make clear that it is the conjunction of the three axioms that is ruled out by these correlations. We then show that by assuming local causality and no superdeterminism, we can give a direct proof of many worlds. The remainder of the paper searches for a consistent, local, formulation of many worlds. We show that prominent formulations whose ontology is given by the wave function violate local causality, and we critically evaluate claims in the literature to the contrary. We ultimately identify a local many worlds interpretation that replaces the wave function with a separable Lorentz-invariant wave-field. We conclude with discussions of the Born rule, and other interpretations of quantum mechanics.}
}
@article{BARCENAS20203,
title = {A Note on Constructive Interpolation for the Multi-Modal Logic Km},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {354},
pages = {3-16},
year = {2020},
note = {Proceedings of the Eleventh and Twelfth Latin American Workshop on Logic/Languages, Algorithms and New Methods of Reasoning (LANMR)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2020.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1571066120300785},
author = {Everardo Bárcenas and José-de-Jesús Lavalle-Martínez and Guillermo Molero-Castillo and Alejandro Velázquez-Mena},
keywords = {Craig Interpolation, Multi-modal logic , Tree-Hypersequents, Beth Definability, Robinson Joint Consistency},
abstract = {The Craig Interpolation Theorem is a well-known property in the mathematical logic curricula, with many domain applications, such as in the modularization of formal specifications and ontologies. This property states the following: given an implication, say formula ϕ implies another formula ψ, then there is a formula β, called the interpolant, in the common language of ϕ and ψ, such that ϕ also implies β, as well as β implies ψ. Although it is already known that the propositional multi-modal logic Km enjoys Craig interpolation, we are not aware of method providing an explicit construction of interpolants. We describe in this paper a constructive proof of the Craig interpolation property on the multi-modal logic Km. Interpolants can be explicitly computed from the proof. Furthermore, we also describe an upper bound for the computation of interpolants. The proof is based on the application of Maehara technique on a tree-hypersequent calculus. As a corollary of interpolation, we also show Beth definability and Robinson joint consistency.}
}
@article{MUSIC2022100251,
title = {AVA: A component-oriented abstraction layer for virtual plug&produce automation systems engineering},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100251},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100251},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000509},
author = {Goran Musić and Bernhard Heinzl and Wolfgang Kastner},
keywords = {Automation, Domain language, Architectural pattern, Framework},
abstract = {The prevailing system and software model in automation systems engineering is defined by the IEC 61131 norm. It is to date the best way we know how to express low-level logic and manipulate electrical hardware signals. However, the exponential technological growth is continuing to raise the expectations on what automation systems are supposed to be capable of doing. Fulfilling rising requirements and managing the exploding complexity requires a systematic support for high-level descriptions, structuring, and communication, which the original approach was not built to provide. This work proposes the introduction of an abstraction layer, a component-container infrastructure, defined on top of standard system and software models in automation and mirroring the world of cyber–physical systems, where independent components are interconnected to realize the systems’ purpose by using each other’s functionalities. The concept is implemented in the form of a domain-specific modeling language, applying a classical two-level Model-driven Software Engineering (MDSE) approach. By engineering distinct industrial use cases in accordance with the proposed approach, it is shown that the defined abstractions and mechanisms are capable of expressing the nuances of software design in different domains and can enable the streamlining of the automation systems engineering workflow into a virtual plug&produce process.}
}
@article{FERGUSSON2025115938,
title = {Ex vivo model of functioning human lymph node reveals role for innate lymphocytes and stroma in response to vaccine adjuvant},
journal = {Cell Reports},
volume = {44},
number = {7},
pages = {115938},
year = {2025},
issn = {2211-1247},
doi = {https://doi.org/10.1016/j.celrep.2025.115938},
url = {https://www.sciencedirect.com/science/article/pii/S2211124725007090},
author = {Joannah R. Fergusson and Jacqueline H.Y. Siu and Nitya Gupta and Edward Jenkins and Eloise Nee and Sören Reinke and Tamara Ströbel and Ananya Bhalla and Shyami M. Kandage and Thomas Courant and Sarah Hill and Moustafa Attar and Michael L. Dustin and Alex Gordon-Weeks and Mark Coles and Calliope A. Dendrou and Anita Milicic},
keywords = { human lymph node, vaccines, adjuvants, single-cell transcriptomics, mRNA-seq, fluorescent imaging},
abstract = {Summary
Immunological processes that underpin human immune responses to therapeutics and vaccine components, such as vaccine adjuvants, remain poorly defined due to a paucity of models that faithfully recapitulate immune activation in lymphoid tissues. We describe precision-cut human lymph node (LN) slices as a functioning, architecturally preserved, full-organ cross-sectional model system. Using single-cell transcriptomics and multiplexed imaging, we explore early inflammatory response to a potent, clinically relevant liposomal vaccine adjuvant containing a TLR4-agonist and QS-21 saponin. Both TLR4 and NLRP3 inflammasome activation are involved in the direct initiation of the inflammatory response to adjuvant by monocytes and macrophages (Mon./Mac.) with secretion of interleukin (IL)-1β, but not IL-18, dependent on TLR4 signaling. Innate lymphoid cells, including natural killer cells, are indirectly activated by Mon./Mac.-produced cytokines, signaling downstream to B cells via interferon-γ secretion. Resident LN stromal populations, primed both directly and indirectly by vaccine adjuvant, are instrumental in mediating inflammatory cell recruitment, particularly neutrophils.}
}
@incollection{BHAMIDIPATY2025133,
title = {Chapter 7 - Intelligent health care: applications of artificial intelligence and machine learning in computational medicine},
editor = {Tuan Anh Nguyen},
booktitle = {Blockchain and Digital Twin for Smart Hospitals},
publisher = {Elsevier},
pages = {133-169},
year = {2025},
isbn = {978-0-443-34226-4},
doi = {https://doi.org/10.1016/B978-0-443-34226-4.00008-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443342264000083},
author = {Veenadhari Bhamidipaty and Durgananda Lahari Bhamidipaty and Fayaz S.M and K.D.P. Bhamidipaty and Rajesh Botchu},
keywords = {Artificial intelligence, deep neural networks, electronic medical records, machine learning, radiomics},
abstract = {The application of artificial intelligence (AI) and machine learning (ML) to healthcare has revolutionized the fields of data analysis, disease detection, treatment planning, and comprehension of complex biological interactions. In the field of literature mining in particular, computational techniques have become indispensable instruments that enable the swift processing of large amounts of biomedical text to extract meaningful insights. AI algorithms may detect biological links that are essential to the pathophysiology and host responses of viral infections, highlighting ML's capacity to unearth intricate networks that go beyond manual curation. Furthermore, the neural concept recognizer (NCR) is a neural dictionary model using convolutional neural networks for concept detection in medical literature. The NCR surpasses rule-based baselines and displays ML's ability to generalize knowledge across multiple terminologies by utilizing ontological structures such as Systematized Nomenclature of Medicine - Clinical Terms (SNOMED-CT) and Human Phenotype Ontology (HPO). These advancements highlight the potential of AI-powered instruments in clinical decision systems and their function in providing researchers with rapid insights. However, there are issues that need to be addressed, like upholding existing ontologies, integrating different data sources seamlessly, guaranteeing algorithm openness, resolving biases in training datasets, and protecting patient privacy.}
}
@article{MARFEO2025101888,
title = {Applying NLP methods to code functional performance in electronic health records using the international classification of functioning, disability, and health},
journal = {Disability and Health Journal},
volume = {18},
number = {4},
pages = {101888},
year = {2025},
issn = {1936-6574},
doi = {https://doi.org/10.1016/j.dhjo.2025.101888},
url = {https://www.sciencedirect.com/science/article/pii/S1936657425001177},
author = {Elizabeth Marfeo and Maryanne Sacco and Jona Camacho Maldonado and Kathleen Coale and Rafael Jimenez Silva and Rebecca Parks and Elizabeth K. Rasch},
keywords = {Natural language processing, Electronic health records, Disability assessment},
abstract = {Background
Clinical records often provide information on a person's functioning (activities), reflecting their lived experience of health. Automated extraction using clinical natural language processing (cNLP) can assist providers with clinical decision-making, treatment planning, predicting health outcomes, and informing health care policy.
Objective
We aim to (1) describe the applicability of the World Health Organization's International Classification of Functioning, Disability and Health (ICF) to development of cNLP tools, (2) identify key challenges in application of the ICF, and (3) offer recommendations to improve this process.
Methods
Apply the ICF as a framework to manually annotate free-text electronic health records (EHRs) from the United States (US) Social Security Administration (SSA) and the National Institutes of Health (NIH) Clinical Center using cNLP tools for each activity domain of the ICF.
Results
Conceptual and content issues were encountered within four primary domains: Mobility, Self-Care and Domestic Life, Interpersonal Interactions and Relationships, and Communication and Cognition. Subsequent recommendations for ICF updates were provided.
Conclusion
Overall, the ICF performed well applied to a use case for which it was not originally developed (SSA disability determination), which assessed its effectiveness, and highlighted both strengths and weaknesses between ICF conceptualizations and documented real-world functioning observations. This work provides a foundation upon which to improve the ICF and integrate it with cNLP models in order to give clinicians, researchers, and policy makers robust informatics tools that quickly identify functioning information for clinical decision and policy making purposes.}
}
@article{KOHLS201853,
journal = {Journal of Second Language Writing},
volume = {39},
pages = {53-55},
year = {2018},
issn = {1060-3743},
doi = {https://doi.org/10.1016/j.jslw.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1060374317305167},
author = {Robert Kohls}
}
@article{VANLEERDAM2024108877,
title = {A predictive model for hypocalcaemia in dairy cows utilizing behavioural sensor data combined with deep learning},
journal = {Computers and Electronics in Agriculture},
volume = {220},
pages = {108877},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.108877},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924002680},
author = {Meike {van Leerdam} and Peter R. Hut and Arno Liseune and Elena Slavco and Jan Hulsen and Miel Hostens},
keywords = {Dairy cattle, Hypocalcaemia, Sensors, Deep learning, Prediction, Transition period},
abstract = {(Sub)clinical hypocalcaemia occurs frequently in the dairy industry, and is one of the earliest symptoms of an impaired transition period. Calcium deficiency is accompanied by changes in cows’ daily behavioural variables, which can be measured by sensors. The goal of this study was to construct a predictive model to identify cows at risk of hypocalcaemia in dairy cows using behavioural sensor data. For this study 133 primiparous and 476 multiparous cows from 8 commercial Dutch dairy farms were equipped with neck and leg sensors measuring daily behavioural parameters, including eating, ruminating, standing, lying, and walking behaviour of the 21 days before calving. From each cow, a blood sample was taken within 48 h after calving to measure their blood calcium concentration. Cows with a blood calcium concentration ≤2.0 mmol/L were defined as hypocalcemic. In order to create a more context based cut-off, a second way of dividing the calcium concentrations into two categories was proposed, using a linear mixed-effects model with a k-Means clustering. Three possible binary predictive models were tested; a logistic regression model, a XgBoost model and a LSTM deep learning model. The models were expanded by adding the following static features as input variables; parity (1, 2 or 3＋), calving season (summer, autumn, winter, spring), day of calcium sampling relative to calving (0, 1 or 2), body condition score and locomotion score. Of the three models, the deep learning model performed best with an area under the receiver operating characteristic curve (AUC) of 0.71 and an average precision of 0.47. This final model was constructed with the addition of the static features, since they improved the model’s tuning AUC with 0.11. The calcium label based on the cut-off categorization method proved to be easier to predict for the models compared to the categorization method with the k-means clustering. This study provides a novel approach for the prediction of hypocalcaemia, and an ameliorated version of the deep learning model proposed in this study could serve as a tool to help monitor herd calcium status and to identify animals at risk for associated transition diseases.}
}
@article{KOPKE201925,
title = {Annotation paths for matching XML-Schemas},
journal = {Data & Knowledge Engineering},
volume = {122},
pages = {25-54},
year = {2019},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300654},
author = {Julius Köpke},
keywords = {Semantic annotation, Schema matching, Schema mapping, Document transformations, SAWSDL, XML, Interoperability},
abstract = {Annotation paths are a technique for the semantic annotation of XML-Schemas. The design rationale was to develop an embedded annotation method on top of SAWSDL which is fully declarative, easily applicable and still provides the proper expressiveness for high-quality logic-based schema matching. Annotation paths capture significantly more semantics than plain model references, the declarative annotation method of the W3C standard SAWSDL. While the concept of annotation paths was introduced in earlier works, we provide a new formalization of their structure and based thereon define their semantics and introduce matching methods to derive simple and complex value correspondences. Such correspondences can be used for the generation of executable schema mappings using state of the art mapping tools. We provide a comprehensive evaluation of our annotation method and the proposed matching algorithms using real-world schemas and reference ontologies and demonstrate the feasibility of generating executable mappings using a state of the art mapping system. Our evaluations show that our annotation-based matcher achieves outstanding matching quality (avg. f-measure between 0.98 and 1.0).}
}
@article{ALPER2021103685,
title = {Making science computable: Developing code systems for statistics, study design, and risk of bias},
journal = {Journal of Biomedical Informatics},
volume = {115},
pages = {103685},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103685},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421000149},
author = {Brian S. Alper and Joanne Dehnbostel and Muhammad Afzal and Vignesh Subbian and Andrey Soares and Ilkka Kunnamo and Khalid Shahin and Robert C. McClure},
keywords = {Ontology, Code system, Terminology, Evidence-based medicine, Science communication, Research literature},
abstract = {The COVID-19 crisis led a group of scientific and informatics experts to accelerate development of an infrastructure for electronic data exchange for the identification, processing, and reporting of scientific findings. The Fast Healthcare Interoperability Resources (FHIR®) standard which is overcoming the interoperability problems in health information exchange was extended to evidence-based medicine (EBM) knowledge with the EBMonFHIR project. A 13-step Code System Development Protocol was created in September 2020 to support global development of terminologies for exchange of scientific evidence. For Step 1, we assembled expert working groups with 55 people from 26 countries by October 2020. For Step 2, we identified 23 commonly used tools and systems for which the first version of code systems will be developed. For Step 3, a total of 368 non-redundant concepts were drafted to become display terms for four code systems (Statistic Type, Statistic Model, Study Design, Risk of Bias). Steps 4 through 13 will guide ongoing development and maintenance of these terminologies for scientific exchange. When completed, the code systems will facilitate identifying, processing, and reporting research results and the reliability of those results. More efficient and detailed scientific communication will reduce cost and burden and improve health outcomes, quality of life, and patient, caregiver, and healthcare professional satisfaction. We hope the achievements reached thus far will outlive COVID-19 and provide an infrastructure to make science computable for future generations. Anyone may join the effort at https://www.gps.health/covid19_knowledge_accelerator.html.}
}
@article{RODOLPHE202311117,
title = {Employing BERT model backed by expert knowledge to extract from textual media event of interest along container shipping supply chain},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {11117-11122},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.824},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323012028},
author = {Barlogis Rodolphe and Ouedraogo Cheik and Aurélie Montarnal and Didier Gourc},
keywords = {NLP, Supply Chain management, BERT, Information Retrieval, IoT, Intermodal transport, Container shipping},
abstract = {Container is the keystone of multimodal supply chains. As container shipping involves numerous actors and because of immense volumes, associated data is teeming. IoT now enables us to see through this mist at the container level. We therefore, we propose a demonstration service to extend visibility by utilizing insights offered by IoT data inherent to containers. The location of containers serves as a starting point to gather information about higher-level circumstances. We armed the service with machine learning algorithms for detecting events of interest along the supply chain through textual exogenous data. An automated information extraction methodology based on BERT model backed by expert knowledge has been implemented. It is illustrated here on a use case to detect climatic events along tracked container route by retrieving tweets from twitter API.}
}
@article{KUMAWAT20253294,
title = {Impact Analysis of Text Representation on Biomedical Multi-Label Text Classification with Deep Learning},
journal = {Procedia Computer Science},
volume = {258},
pages = {3294-3304},
year = {2025},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.587},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925016916},
author = {Hemraj Kumawat and Aditi Sharan and Shikha Verma},
keywords = {multi-label text classification, deep learning, natural language processing, BERT},
abstract = {Multi-label text classification is a challenging task compared to single-label text classification due to the need to predict multiple, non-exclusive labels simultaneously for a single text document. Deep learning models can capture such label dependencies. However, classification performance depends on the specific deep learning architecture and how the text is represented numerically for input to deep learning models. This research examines the impact of text representation on multi-label text classification tasks using various text representation techniques, such as traditional TF-IDF, embedding-based models like GloVe, Word2Vec, fastText, and transformer-based models like BERT, BioBERT, SciBERT, and PubMedBERT. This paper utilizes two publicly available datasets for multi-label text classification. The first dataset, the Chemical Exposure Information (CEI) Corpus, consists of a collection of PubMed abstracts labeled with one or more classes using an exposure taxonomy of 32 classes. The second dataset, Hallmarks of Cancer (HoC), consists of a collection of PubMed abstracts manually labeled with one or more Hallmarks of cancer classes. The performance of these models was evaluated using micro-precision, micro-recall, micro-F1 score, 0/1 subset accuracy, hamming loss, jaccard score, coverage, ranking loss, and label ranking precision. The results reveal that model performance varies significantly, with PubMedBERT, a transformer-based model, outperforming other models with micro-precision, micro-recall, and micro-F1 score values of 0.93, 0.89, and 0.91, respectively, for the CEI dataset and with micro-precision, micro-recall, and micro-F1 score values of 0.88, 0.78, and 0.83 respectively, for HoC dataset. This study emphasizes the necessity of employing suitable text representation techniques to increase the efficiency of text classification applications such as literature indexing and information retrieval.}
}
@article{BAUDRIT2022101551,
title = {Graph based knowledge models for capitalizing, predicting and learning: A proof of concept applied to the dam systems},
journal = {Advanced Engineering Informatics},
volume = {52},
pages = {101551},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101551},
url = {https://www.sciencedirect.com/science/article/pii/S147403462200026X},
author = {Cedric Baudrit and Franck Taillandier and Corinne Curt and Q.A. Hoang and Zoubir-Mehdi Sbartaï and Denys Breysse},
keywords = {Knowledge representation, Conceptual graph, Dam failure, Forensic engineering},
abstract = {The capitalization and the analysis of historical information is nowadays a prerequisite for any effective risk management and assessment in a wide range of domains. Despite the development of mathematical models, procedures, support decision systems and databases, some engineering disciplines, such as civil engineering, remain resistant to the use of new digital technology due to the gap between the expectations of the engineers and the support that the tools may really provide. It is essential to propose a tool able to process both cross disciplinary and interdisciplinary knowledge flux and feedback from experience in a common and convenient unifying framework. The aim is to assist and to support engineering work and to make the task of knowledge modelling easier. The domain of dam systems is no exception to the rule. Dam failures are still commonplace. These failures stem from a lack of understanding about the complex relationships between three different factors: random hazards, the limit states of dam structures along with human activities and decisions. No generic and holistic approach is currently available that permits the processing of both knowledge and data, performs inferences and is easily usable for all types of users. This paper proposes the basic principles of a convenient design methodology for capitalizing, learning and predicting based on the formalism of conceptual graphs. The aim is to provide an easily usable tool able to (1) capitalise heterogeneous knowledge and store a database about dams, (2) issue alerts on current projects, (3) draw lessons from past dam failures and (4) tackle key issues in forensic civil engineering.}
}
@article{FENG202310,
title = {Detecting contradictions from IoT protocol specification documents based on neural generated knowledge graph},
journal = {ISA Transactions},
volume = {141},
pages = {10-19},
year = {2023},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2023.04.025},
url = {https://www.sciencedirect.com/science/article/pii/S0019057823001945},
author = {Xinguo Feng and Yanjun Zhang and Mark Huasong Meng and Yansong Li and Chegne Eu Joe and Zhe Wang and Guangdong Bai},
keywords = {Internet of things, Natural language processing, Web protocol, Contradiction detection, Large language models},
abstract = {Due to the boom of Internet of Things (IoT) in recent years, various IoT devices are connected to the Internet and communicate with each other through network protocols such as the Constrained Application Protocol (CoAP). These protocols are typically defined and described in specification documents, such as Request for Comments (RFC), which are written in natural or semi-formal languages. Since developers largely follow the specification documents when implementing web protocols, they have become the de facto protocol specifications. Therefore, it must be ensured that the descriptions in them are consistent to avoid technological issues, incompatibility, security risks, or even legal concerns. In this work, we propose Neural RFC Knowledge Graph (NRFCKG), a neural network-generated knowledge graph based contradictions detection tool for IoT protocol specification documents. Our approach can automatically parse the specification documents and construct knowledge graphs from them through entity extraction, relation extraction, and rule extraction with large language models. It then conducts an intra-entity and inter-entity contradiction detection over the generated knowledge graph. We implement NRFCKG and apply it to the most extensively used messaging protocols in IoT, including the main RFC (RFC7252) of CoAP, the specification document of MQTT, and the specification document of AMQP. Our evaluation shows that NRFCKG generalizes well to other specification documents and it manages to detect contradictions from these IoT protocol specification documents.}
}
@article{LUO2024116886,
title = {Lobelia chinensis Lour inhibits the progression of hepatocellular carcinoma via the regulation of the PTEN/AKT signaling pathway in vivo and in vitro},
journal = {Journal of Ethnopharmacology},
volume = {318},
pages = {116886},
year = {2024},
issn = {0378-8741},
doi = {https://doi.org/10.1016/j.jep.2023.116886},
url = {https://www.sciencedirect.com/science/article/pii/S0378874123007547},
author = {Jin Luo and Qiu-xia Chen and Pan Li and He Yu and Ling Yu and Jia-li Lu and Hong-zhi Yin and Bi-jun Huang and Shi-jun Zhang},
keywords = { Lour., Hepatocellular carcinoma, Network pharmacology, Mechanism, Signaling pathway},
abstract = {Ethnopharmacological relevance
Lobelia chinensis Lour. (LCL) is a common herb used for clearing heat and detoxifying, and it has antitumor activity. Quercetin is one of its important components, which may play an important role in the treatment of hepatocellular carcinoma (HCC).
Aim of the study
To study the active ingredients of LCL, their mechanism of action on HCC, and lay the foundations for the development of new drugs for the treatment of HCC.
Materials and methods
Network pharmacology was used to examine the probable active ingredients and mechanisms of action of LCL in HCC treatment. Based on an oral bioavailability of ≥30% and a drug-likeness index of ≥0.18, relevant compounds were selected from the Traditional Chinese Medicine Systems Pharmacology database and TCM Database@Taiwan. HCC-related targets were identified using gene cards and the Online Mendelian Inheritance in Man (OMIM) database. A Venn diagram was created to assess the relationship between the intersection of disease and medication targets by creating a protein–protein interaction network, and the hub targets were selected by topology. Gene Ontology enrichment analyses were performed using the DAVID tool. Finally, in vivo and in vitro experiments (qRT-PCR, western blotting, hematoxylin and eosin staining, transwell assays, scratch tests, and flow cytometry assays) verified that LCL demonstrated notable therapeutic effects on HCC.
Results
In total, 16 bioactive LCL compounds met the screening criteria. The 30 most important LCL therapeutic target genes were identified. Of these, AKT1 and MAPK1 were the most important target genes, and the AKT signaling pathway was identified as the key pathway. Transwell and scratch assays showed that LCL prevented cell migration, and flow cytometry tests revealed that the LCL-treated group showed a considerably higher rate of apoptosis than the control group. LCL reduced tumor formation in mice in vivo, and Western blot analysis of tumor tissues treated with LCL indicated variations in PTEN, p-MAPK and p-AKT1 levels. The results show that LCL may inhibit the progression of HCC through the PTEN/AKT signaling pathway to achieve the goal of treating HCC.
Conclusion
LCL is a broad-spectrum anticancer agent. These findings reveal potential treatment targets and strategies for preventing the spread of cancer, which could aid in screening potential traditional Chinese medicine for anticancer and clarifying their mechanisms.}
}
@article{SONG2019133,
title = {Toward any-language zero-shot topic classification of textual documents},
journal = {Artificial Intelligence},
volume = {274},
pages = {133-150},
year = {2019},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370219300414},
author = {Yangqiu Song and Shyam Upadhyay and Haoruo Peng and Stephen Mayhew and Dan Roth},
keywords = {Multilingual text classification, Cross-lingual text classification, Zero-shot text classification, Semantic Supervision},
abstract = {In this paper, we present a zero-shot classification approach to document classification in any language into topics which can be described by English keywords. This is done by embedding both labels and documents into a shared semantic space that allows one to compute meaningful semantic similarity between a document and a potential label. The embedding space can be created by either mapping into a Wikipedia-based semantic representation or learning cross-lingual embeddings. But if the Wikipedia in the target language is small or there is not enough training corpus to train a good embedding space for low-resource languages, then performance can suffer. Thus, for low-resource languages, we further use a word-level dictionary to convert documents into a high-resource language, and then perform classification based on the high-resource language. This approach can be applied to thousands of languages, which can be contrasted with machine translation, which is a supervision-heavy approach feasible for about 100 languages. We also develop a ranking algorithm that makes use of language similarity metrics to automatically select a good pivot or bridging high-resource language, and show that this significantly improves classification of low-resource language documents, performing comparably to the best bridge possible.}
}
@article{KELLY2021269,
title = {Mapping hydropower conflicts: A legal geography of dispossession in Mapuche-Williche Territory, Chile},
journal = {Geoforum},
volume = {127},
pages = {269-282},
year = {2021},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2021.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0016718521002992},
author = {Sarah H. Kelly},
keywords = {Legal geography, Dispossession, Energy transition, Hydropower, Water, Indigenous rights},
abstract = {In this article, I examine how hydropower projects in Mapuche territory both form part of internationally recognized approaches to develop renewable energy and also anchor colonial relations in rivers. In pursuit of energy development, water and ancestral cultural practices of the Mapuche Pueblo are being seized by a nexus of state laws and informal practices of private sector actors. Concurrently, Mapuche people assert their jurisdictions and experience resurgence of Indigenous lifeways through defending their waterways. Drawing on collaborative research guided by the Alianza Territorial Puelwillimapu, a Mapuche-Williche alliance convoked by ancestral leaders, I provide a methodological contribution to legal geography’s analysis of Indigenous rights. Bringing a legal geography approach to dispossession, I explain how collaborative mapmaking and systematizing the “layers of dispossession” provides a methodological approach to consider structural limitations to environmental justice on Indigenous lands. Overall, this case contributes to how we conceive of spatial justice in legal geography and in renewable energy development.}
}
@article{PANDIT2018262,
title = {Queryable Provenance Metadata For GDPR Compliance},
journal = {Procedia Computer Science},
volume = {137},
pages = {262-268},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316314},
author = {Harshvardhan J. Pandit and Declan O’Sullivan and Dave Lewis},
keywords = {GDPR, regulatory compliance, semantic web, provenance},
abstract = {Information associated with regulatory compliance is often siloed as legal documentation that is not suitable for querying or reuse. Utilising open standards and technologies to represent and query this information can facilitate interoperability between stakeholders and assist in the task of maintaining as well as demonstrating compliance. In this paper, we show how semantic web technologies can assist in representation and querying of compliance information related to the General Data Protection Regulation (GDPR), an European law governing the use of consent and personal data. We focus on the subset of obligations related to the use of consent and personal data, and represent the associated metadata using the previously published GDPRov ontology and GDPRtEXT resource. We present a proof-of-concept demonstration (available online) where information is queried to automatically populate the GDPR-readiness checklist published by Ireland’s Data Protection Commissioner.}
}
@article{HUANG2018101,
title = {Type theory based semantic verification for service composition in cloud computing environments},
journal = {Information Sciences},
volume = {469},
pages = {101-118},
year = {2018},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.08.042},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518306467},
author = {Changqin Huang and Xizhe Wang and Dianhui Wang},
keywords = {Type theory, Cloud service composition, Semantic verification, Theoretical proof, Software-as-a-service applications},
abstract = {To ensure successful running of service composition in software-as-a-service (SaaS) applications under dynamic cloud computing environments, service semantic verification becomes essentially important and meaningful. Taking advantages from both semantic expression in dependence record types (DRT) and completeness of service description in session types (ST), this paper presents a novel method for service composition verification at semantic level. Based on the characteristics of DRT, an ontology approach is proposed to enrich DRTs semantic description and establish a set of subtyping rules to characterize relationships of transmitted messages among services. As a general service composition description and semantic verification method, message DRT extended multiparty session types (MDRT-MST) are then constructed. A theoretical justification on the main patterns of service composition models is given through the extended reduction rules. As a case study, an online payment system is implemented for evaluating the system performance in practice. Experimental results demonstrate the feasibility and effectiveness of the proposed method in service reduction and verification for the real-world SaaS applications.}
}
@article{KOWSHER2021131,
title = {LSTM-ANN & BiLSTM-ANN: Hybrid deep learning models for enhanced classification accuracy},
journal = {Procedia Computer Science},
volume = {193},
pages = {131-140},
year = {2021},
note = {10th International Young Scientists Conference in Computational Science, YSC2021, 28 June – 2 July, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020548},
author = {Md. Kowsher and Anik Tahabilder and Md. Zahidul {Islam Sanjid} and Nusrat Jahan Prottasha and Md. Shihab Uddin and Md Arman Hossain and Md. Abdul {Kader Jilani}},
keywords = {BiLSTM-ANN, LSTM-ANN, Supervised machine learning, Hybrid ML model, Fusion of ML model, NLP},
abstract = {Machine learning is getting more and more advanced with the progression of state-of-the-art technologies. Since existing algorithms do not provide a palatable learning performance most often, it is necessary to carry on the trail of upgrading the current algorithms incessantly. The hybridization of two or more algorithms can potentially increase the performance of the blueprinted model. Although LSTM and BiLSTM are two excellent far and widely used algorithms in natural language processing, there still could be room for improvement in terms of accuracy via the hybridization method. Thus, the advantages of both RNN and ANN algorithms can be obtained simultaneously. This paper has illustrated the deep integration of BiLSTM-ANN (Fully Connected Neural Network) and LSTM-ANN and manifested how these integration methods are performing better than single BiLSTM, LSTM and ANN models. Undertaking Bangla content classification is challenging because of its equivocalness, intricacy, diversity, and shortage of relevant data, therefore, we have executed the whole integrated models on the Bangla content classification dataset from newspaper articles. The proposed hybrid BiLSTM-ANN model beats all the implemented models with the most noteworthy accuracy score of 93% for both validation & testing. Moreover, we have analyzed and compared the performance of the models based on the most relevant parameters.}
}
@article{TERZIYAN2022216,
title = {Explainable AI for Industry 4.0: Semantic Representation of Deep Learning Models},
journal = {Procedia Computer Science},
volume = {200},
pages = {216-226},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.220},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002290},
author = {Vagan Terziyan and Oleksandra Vitko},
keywords = {Explainable Artificial Intelligence, Industry 4.0, semantic web, deep learning, predictive maintenance},
abstract = {Artificial Intelligence is an important asset of Industry 4.0. Current discoveries within machine learning and particularly in deep learning enable qualitative change within the industrial processes, applications, systems and products. However, there is an important challenge related to explainability of (and, therefore, trust to) the decisions made by the deep learning models (aka black-boxes) and their poor capacity for being integrated with each other. Explainable artificial intelligence is needed instead but without loss of effectiveness of the deep learning models. In this paper we present the transformation technique between black-box models and explainable (as well as interoperable) classifiers on the basis of semantic rules via automatic recreation of the training datasets and retraining the decision trees (explainable models) in between. Our transformation technique results to explainable rule-based classifiers with good performance and efficient training process due to embedded incremental ignorance discovery and adversarial samples (“corner cases”) generation algorithms. We have also shown the use-case scenario for such explainable and interoperable classifiers, which is collaborative condition monitoring, diagnostics and predictive maintenance of distributed (and isolated) smart industrial assets while preserving data and knowledge privacy of the users.}
}
@article{PINQUIE2023146,
title = {Human-Centric Co-Design of Model-Based System Architecture},
journal = {Procedia CIRP},
volume = {119},
pages = {146-151},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.02.131},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004614},
author = {Romain Pinquié and Haobo Wang and Frédéric Noel},
keywords = {Model-based systems engineering, visual notation, graphical modelling, virtual reality, conceptual modelling, SysML},
abstract = {Model-Based Systems Engineers, particularly those moving from software to systems engineering, claim that SysML-like notations, which are symbolic two-dimensional diagrams made of boxes and lines, are domain-independent, thus very convenient to support the cross-functional design of a system architecture. However, the abstract diagramming syntax of MBSE notations makes their adoption difficult, especially by notational nonexperts, and using iconic graphics is one way of improvement. Very few studies attempted to replace 2D diagrams with immersive 3D visuals without objective evidence. We assume that it is due to a lack of quality criteria to compare 2D diagrams with 3D visuals. This paper will argue that human-centric 3D visuals should replace MBSE diagrams where appropriate. A new human-centric MBSE modelling environment motivates the adoption of interactive and immersive 3D visuals to facilitate communication and participation in multidisciplinary co-design activities from mission to concept definition. Our claim is not supported by measured evidence, but a functional demonstrator provides observed evidence and, compared to existing studies, we discuss our proposal based on the Physics of Notation. Future studies will concentrate on the modelling of the behavioural and structural views and the definition of criteria to validate the human-centric MBSE modelling environment.}
}
@article{LI2024105210,
title = {Digital twin for intelligent tunnel construction},
journal = {Automation in Construction},
volume = {158},
pages = {105210},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105210},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523004703},
author = {Tao Li and Xiaojun Li and Yi Rui and Jiaxin Ling and Sicheng Zhao and Hehua Zhu},
keywords = {Digital twin, Intelligent tunnel construction, Literature review, Conceptual framework, Building information modelling, Knowledge graph, Point cloud, Machine learning, Infrastructure, AEC},
abstract = {New-generation intelligent construction places higher demands on digitisation and intelligence of tunnel. Digital twin (DT) effectively supports high-fidelity modelling, virtual-real mapping, and analysis-based decision-making but with research in the initial stage. To begin with, this paper delves into the complexity and uncertainty inherent in tunnel construction, highlighting DT as a promising solution compared to exisiting technologies such as Building Information Modelling. Then, a systematic literature survey is conducted, revealing growing focus on DT research topics. To provide comprehensive insights into DT-related technologies and their application in tunnel construction, this paper clusters literature from perspectives of sensor networks, Internet of Things (IoT), computer vision-based twin data acquisition, communication, natural language processing (NLP), automatic control-based connection, and geometric, semantic, analytical integrated twin modelling. These aspects shed light on potentials and limitations of existing researh in developing a functional DT. In response to the challenges of information richness, timeliness, and analytical capabilities, an improved conceptual framework tailored for tunnel is proposed to close the information and control loop. Finally, the paper discusses the prospects and gaps of DT in theory and practice to leverage DT implementation.}
}
@article{YANG2025110285,
title = {Alzheimer's disease knowledge graph enhances knowledge discovery and disease prediction},
journal = {Computers in Biology and Medicine},
volume = {192},
pages = {110285},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110285},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525006365},
author = {Yue Yang and Kaixian Yu and Shan Gao and Sheng Yu and Di Xiong and Chuanyang Qin and Huiyuan Chen and Jiarui Tang and Niansheng Tang and Hongtu Zhu},
keywords = {Alzheimer's disease, Disease prediction, Knowledge graph construction, Link prediction},
abstract = {Objective
To construct an Alzheimer's Disease Knowledge Graph (ADKG) by extracting and integrating relationships among Alzheimer's disease (AD), genes, variants, chemicals, drugs, and other diseases from biomedical literature, aiming to identify existing treatments, potential targets, and diagnostic methods for AD.
Methods
We annotated 800 PubMed abstracts (ADERC corpus) with 20,886 entities and 4935 relationships, augmented via GPT-4. A SpERT model (SciBERT-based) trained on this data extracted relations from PubMed abstracts, supported by biomedical databases and entity linking refined via abbreviation resolution/string matching. The resulting knowledge graph trained embedding models to predict novel relationships. ADKG's utility was validated by integrating it with UK Biobank data for predictive modeling.
Results
The ADKG contained 3,199,276 entity mentions and 633,733 triplets, linking >5K unique entities and capturing complex AD-related interactions. Its graph embedding models produced evidence-supported predictions, enabling testable hypotheses. In UK Biobank predictive modeling, ADKG-enhanced models achieved higher AUROC of 0.928 comparing to 0.903 without ADKG enhancement.
Conclusion
By synthesizing literature-derived insights into a computable framework, ADKG bridges molecular mechanisms to clinical phenotypes, advancing precision medicine in Alzheimer's research. Its structured data and predictive utility underscore its potential to accelerate therapeutic discovery and risk stratification.}
}
@article{BERGER2024106003,
title = {Towards reusable building blocks for agent-based modelling and theory development},
journal = {Environmental Modelling & Software},
volume = {175},
pages = {106003},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106003},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224000641},
author = {Uta Berger and Andrew Bell and C. Michael Barton and Emile Chappin and Gunnar Dreßler and Tatiana Filatova and Thibault Fronville and Allen Lee and Emiel {van Loon} and Iris Lorscheid and Matthias Meyer and Birgit Müller and Cyril Piou and Viktoriia Radchuk and Nicholas Roxburgh and Lennart Schüler and Christian Troost and Nanda Wijermans and Tim G. Williams and Marie-Christin Wimmler and Volker Grimm},
keywords = {Individual-based modelling, Theory development, Complex adaptive systems, Software engineering, Best practices},
abstract = {Despite the increasing use of standards for documenting and testing agent-based models (ABMs) and sharing of open access code, most ABMs are still developed from scratch. This is not only inefficient, but also leads to ad hoc and often inconsistent implementations of the same theories in computational code and delays progress in the exploration of the functioning of complex social-ecological systems (SES). We argue that reusable building blocks (RBBs) known from professional software development can mitigate these issues. An RBB is a submodel that represents a particular mechanism or process that is relevant across many ABMs in an application domain, such as plant competition in vegetation models, or reinforcement learning in a behavioural model. RBBs need to be distinguished from modules, which represent entire subsystems and include more than one mechanism and process. While linking modules faces the same challenges as integrating different models in general, RBBs are “atomic” enough to be more easily re-used in different contexts. We describe and provide examples from different domains for how and why building blocks are used in software development, and the benefits of doing so for the ABM community and to individual modellers. We propose a template to guide the development and publication of RBBs and provide example RBBs that use this template. Most importantly, we propose and initiate a strategy for community-based development, sharing and use of RBBs. Individual modellers can have a much greater impact in their field with an RBB than with a single paper, while the community will benefit from increased coherence, facilitating the development of theory for both the behaviour of agents and the systems they form. We invite peers to upload and share their RBBs via our website - preferably referenced by a DOI (digital object identifier obtained e.g. via Zenodo). After a critical mass of candidate RBBs has accumulated, feedback and discussion can take place and both the template and the scope of the envisioned platform can be improved.}
}
@article{INAU2023,
title = {Initiatives, Concepts, and Implementation Practices of the Findable, Accessible, Interoperable, and Reusable Data Principles in Health Data Stewardship: Scoping Review},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/45013},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123010142},
author = {Esther Thea Inau and Jean Sack and Dagmar Waltemath and Atinkut Alamirrew Zeleke},
keywords = {data stewardship, findable, accessible, interoperable, and reusable data principles, FAIR data principles, health research, Preferred Reporting Items for Systematic Reviews and Meta-Analyses, PRISMA, qualitative analysis, scoping review, information retrieval, health information exchange},
abstract = {Background
Thorough data stewardship is a key enabler of comprehensive health research. Processes such as data collection, storage, access, sharing, and analytics require researchers to follow elaborate data management strategies properly and consistently. Studies have shown that findable, accessible, interoperable, and reusable (FAIR) data leads to improved data sharing in different scientific domains.
Objective
This scoping review identifies and discusses concepts, approaches, implementation experiences, and lessons learned in FAIR initiatives in health research data.
Methods
The Arksey and O’Malley stage-based methodological framework for scoping reviews was applied. PubMed, Web of Science, and Google Scholar were searched to access relevant publications. Articles written in English, published between 2014 and 2020, and addressing FAIR concepts or practices in the health domain were included. The 3 data sources were deduplicated using a reference management software. In total, 2 independent authors reviewed the eligibility of each article based on defined inclusion and exclusion criteria. A charting tool was used to extract information from the full-text papers. The results were reported using the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines.
Results
A total of 2.18% (34/1561) of the screened articles were included in the final review. The authors reported FAIRification approaches, which include interpolation, inclusion of comprehensive data dictionaries, repository design, semantic interoperability, ontologies, data quality, linked data, and requirement gathering for FAIRification tools. Challenges and mitigation strategies associated with FAIRification, such as high setup costs, data politics, technical and administrative issues, privacy concerns, and difficulties encountered in sharing health data despite its sensitive nature were also reported. We found various workflows, tools, and infrastructures designed by different groups worldwide to facilitate the FAIRification of health research data. We also uncovered a wide range of problems and questions that researchers are trying to address by using the different workflows, tools, and infrastructures. Although the concept of FAIR data stewardship in the health research domain is relatively new, almost all continents have been reached by at least one network trying to achieve health data FAIRness. Documented outcomes of FAIRification efforts include peer-reviewed publications, improved data sharing, facilitated data reuse, return on investment, and new treatments. Successful FAIRification of data has informed the management and prognosis of various diseases such as cancer, cardiovascular diseases, and neurological diseases. Efforts to FAIRify data on a wider variety of diseases have been ongoing since the COVID-19 pandemic.
Conclusions
This work summarises projects, tools, and workflows for the FAIRification of health research data. The comprehensive review shows that implementing the FAIR concept in health data stewardship carries the promise of improved research data management and transparency in the era of big data and open research publishing.
International Registered Report Identifier (IRRID)
RR2-10.2196/22505}
}
@article{YAHYA2025106002,
title = {Predicting adverse drug reactions in oncology: A critical review of machine learning approaches and future directions},
journal = {Results in Engineering},
volume = {27},
pages = {106002},
year = {2025},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2025.106002},
url = {https://www.sciencedirect.com/science/article/pii/S2590123025020742},
author = {Abid Yahya and Phatsimo Lobelo and Afiya Eram and Sana Althaf Hussain and Irfan Anjum Badruddin and Lory Liza D. Bulay-og and Dionel O. Albina},
keywords = {Adverse drug reactions (ADRs), Machine learning, Cancer therapy, Pharmacovigilance, Deep learning, Supervised learning, Clinical decision support, Natural language processing (NLP), Biomedical informatics, Oncology},
abstract = {Because of polypharmacy and complicated treatment protocols, adverse drug reactions (ADRs) continue to be a major problem in oncology and frequently lead to serious clinical complications. Recent developments in the use of artificial intelligence (AI) and machine learning (ML) for ADR prediction in anticancer therapy are critically assessed in this review. We go over a variety of methods for utilizing both structured and unstructured clinical data, such as supervised, unsupervised, and deep learning models in addition to natural language processing (NLP) strategies. Strong performance has been demonstrated by ensemble techniques like Random Forest and Gradient Boosting, while deep neural networks allow for sophisticated feature extraction, albeit with interpretability issues. We highlight new integrative techniques based on current literature trends, such as integrating demographic information, treatment history, and physiological signals with CNN-based models and SHAP-based.}
}
@article{KANSOU2022363,
title = {Food modelling strategies and approaches for knowledge transfer},
journal = {Trends in Food Science & Technology},
volume = {120},
pages = {363-373},
year = {2022},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2022.01.021},
url = {https://www.sciencedirect.com/science/article/pii/S0924224422000292},
author = {Kamal Kansou and Wim Laurier and Maria N. Charalambides and Guy Della-Valle and Ilija Djekic and Aberham Hailu Feyissa and Francesco Marra and Rallou Thomopoulos and Bert Bredeweg},
keywords = {Scientific software, Software re-use, Modelling, Model exchange, Collaborative modelling, Education},
abstract = {Background
Scientific software incorporates models that capture fundamental domain knowledge. This software is becoming increasingly more relevant as an instrument for food research. However, scientific software is currently hardly shared among and (re-)used by stakeholders in the food domain, which hampers effective dissemination of knowledge, i.e. knowledge transfer.
Scope and approach
This paper reviews selected approaches, best practices, hurdles and limitations regarding knowledge transfer via software and the mathematical models embedded in it to provide points of reference for the food community.
Key findings and conclusions
The paper focusses on three aspects. Firstly, the publication of digital objects on the web, which offers valorisation software as a scientific asset. Secondly, building transferrable software as way to share knowledge through collaboration with experts and stakeholders. Thirdly, developing food engineers' modelling skills through the use of food models and software in education and training.}
}
@article{CAI2025103075,
title = {Automatic identification of integrated construction elements using open-set object detection based on image and text modality fusion},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103075},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103075},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007262},
author = {Ruying Cai and Zhigang Guo and Xiangsheng Chen and Jingru Li and Yi Tan and Jingyuan Tang},
keywords = {Computer vision, Convolutional neural network, Construction safety, Unsafe behaviors, Grounding DINO, Multimodal},
abstract = {The application of object detection technology in the field of construction safety contributes significantly to on-site safety management and has already shown considerable progress. However, current research primarily focuses on detecting pre-defined classes annotated within single datasets. In-depth research in construction safety requires the detection of all influencing factors related to construction safety. The emergence of large language models offers new possibilities, and multimodal models that combine these with computer vision technology could break through the existing limitations. Therefore, this paper proposes the Grounding DINO multimodal model for the automatic detection of integrated construction elements, enhancing construction safety. First, this study reviews the literature to collect relevant datasets, summarizes their characteristics, and processes the data, including the processing of annotation files and the integration of classes. Subsequently, the Grounding DINO model is constructed, encompassing image and text feature extraction and enhancement, and a cross-modal decoder that fuses image and text features. Multiple dataset experimental strategies are designed to validate Grounding DINO’s capabilities in continuous learning, with a unified class system created based on integrated classes for model detection input text prompts. Finally, experiments involving zero-shot and fine-tuning evaluations, continuous learning validation, and effectiveness testing are conducted. The experimental results demonstrate the generalization capability and potential for continuous learning of the multimodal model.}
}
@article{GATASHEH2024108178,
title = {Identifying key genes against rutin on human colorectal cancer cells via ROS pathway by integrated bioinformatic analysis and experimental validation},
journal = {Computational Biology and Chemistry},
volume = {112},
pages = {108178},
year = {2024},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2024.108178},
url = {https://www.sciencedirect.com/science/article/pii/S147692712400166X},
author = {Mansour K. Gatasheh},
keywords = {Colorectal cancer, Rutin, Bioinformatic analysis, Molecular targets, Antioxidant properties, Pharmacological network analysis, Experimental validation},
abstract = {Colorectal cancer (CRC) poses a significant global health challenge, characterized by substantial prevalence variations across regions. This study delves into the therapeutic potential of rutin, a polyphenol abundant in fruits, for treating CRC. The primary objectives encompass identifying molecular targets and pathways influenced by rutin through an integrated approach combining bioinformatic analysis and experimental validation. Employing Gene Set Enrichment Analysis (GSEA), the study focused on identifying potential differentially expressed genes (DEGs) associated with CRC, specifically those involved in regulating reactive oxygen species, metabolic reprogramming, cell cycle regulation, and apoptosis. Utilizing diverse databases such as GEO2R, CTD, and Gene Cards, the investigation revealed a set of 16 targets. A pharmacological network analysis was subsequently conducted using STITCH and Cytoscape, pinpointing six highly upregulated genes within the rutin network, including TP53, PCNA, CDK4, CCNEB1, CDKN1A, and LDHA. Gene Ontology (GO) analysis predicted functional categories, shedding light on rutin's potential impact on antioxidant properties. KEGG pathway analysis enriched crucial pathways like metabolic and ROS signaling pathways, HIF1a, and mTOR signaling. Diagnostic assessments were performed using UALCAN and GEPIA databases, evaluating mRNA expression levels and overall survival for the identified targets. Molecular docking studies confirmed robust binding associations between rutin and biomolecules such as TP53, PCNA, CDK4, CCNEB1, CDKN1A, and LDHA. Experimental validation included inhibiting colorectal cell HT-29 growth and promoting cell growth with NAC through MTT assay. Flow cytometric analysis also observed rutin-induced G1 phase arrest and cell death in HT-29 cells. RT-PCR demonstrated reduced expression levels of target biomolecules in HT-29 cells treated with rutin. This comprehensive study underscores rutin's potential as a promising therapeutic avenue for CRC, combining computational insights with robust experimental evidence to provide a holistic understanding of its efficacy.}
}
@article{CAO2025102403,
title = {Textual data augmentation using generative approaches - Impact on named entity recognition tasks},
journal = {Data & Knowledge Engineering},
volume = {156},
pages = {102403},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102403},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24001277},
author = {Danrun Cao and Nicolas Béchet and Pierre-François Marteau and Oussama Ahmia},
keywords = {Data augmentation, Named entity recognition, Feature engineering, Word embeddings, Model robustness, Generative model, Call for tenders},
abstract = {Industrial applications of Named Entity Recognition (NER) are usually confronted with small and imbalanced corpora. This could harm the performance of trained and finetuned recognition models, especially when they encounter unknown data. In this study we develop three generation-based data enrichment approaches, in order to increase the number of examples of underrepresented entities. We compare the impact of enriched corpora on NER models, using both non-contextual (fastText) and contextual (Bert-like) embedding models to provide discriminant features to a biLSTM-CRF used as an entity classifier. The approach is evaluated on a contract renewal detection task applied to a corpus of calls for tenders. The results show that the proposed data enrichment procedure effectively improves the NER model’s effectiveness when applied on both known and unknown data.}
}
@incollection{BARNETT2020187,
title = {Deconstruction},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {187-194},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10635-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955106353},
author = {Clive Barnett},
keywords = {Analytical philosophy, Continental philosophy, Deconstruction, Derrida, Parasite, Performative, Philosophy, Politics, Spatiality, Temporality, Textuality, Theory},
abstract = {Deconstruction is as a tradition of philosophical analysis associated with the work of Jacques Derrida. Deconstruction has a presence across a number of disciplines, as well as in popular culture. Deconstruction can be understood as a form of parasitical analysis, and is just as likely to be vindicatory as disobliging. The trajectory of deconstruction is divided into a critical and an affirmative phase. The basic features of deconstruction as a way of reading are illustrated by Derrida's treatment of the distinction between speech and writing in various philosophical and theoretical traditions. Themes such as “logocentrism” and the “metaphysics of presence” indicate the spatial and temporal resonances of deconstruction. Derrida's work became more explicitly concerned with ethical and political issues from the late 1980s onward. This phase of deconstruction was in part parasitical on a reworking of concepts drawn from analytical philosophy. Deconstruction therefore illustrates the relationship between theory-formation and the movement of philosophy. Geographers have taken up deconstruction as a mode of ideology-critique, as the basis of alternative spatial ontologies, as well as in postcolonial geography, and in geographies of radical democracy. The central question raised by these usages of deconstruction in human geography is whether and how deconstructive motifs can be applied to social science research.}
}
@article{DASILVA2023103408,
title = {Protection, expertise and domination: Cyber masculinity in practice},
journal = {Computers & Security},
volume = {133},
pages = {103408},
year = {2023},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103408},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823003188},
author = {Joseph {Da Silva}},
abstract = {Masculine tropes are ingrained in cyber-security discourse, as with other forms of security. Such language perpetuates concepts and paradigms that regulate the identities of those involved in cyber-security practice. Further, it may serve to exclude those for whom such identities are not desirable. Through a qualitative study of 18 commercial businesses, utilising data gained from semi-structured interviews of CISOs and senior organisational leaders as well as analysis of company documentation, this study provides empirical data that demonstrates the use of masculine language and concepts in cyber-security practice, which is discussed and deconstructed through a lens of identity work. In so doing, this paper identifies such language as being both regulatory and exclusionary. We introduce the term cyber masculinity to refer to the dominance of these masculine concepts and propose future research directions that can challenge this dominance and achieve a de-masculinization of the field.}
}
@article{DEBAUCHE202010,
title = {A new Edge Architecture for AI-IoT services deployment},
journal = {Procedia Computer Science},
volume = {175},
pages = {10-19},
year = {2020},
note = {The 17th International Conference on Mobile Systems and Pervasive Computing (MobiSPC),The 15th International Conference on Future Networks and Communications (FNC),The 10th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920316859},
author = {Olivier Debauche and Saïd Mahmoudi and Sidi Ahmed Mahmoudi and Pierre Manneback and Frédéric Lebeau},
keywords = {Edge computing, Edge Internet of Things, Edge Artificial Intelligence, Edge A2IoT architecture, Internet of Things, Artificial Intelligence, Edge AI-IoT architecture},
abstract = {Artificial intelligence (AI) and Internet of things (IoT) have progressively emerged in all domains of our daily lives. Nowadays, both domains are combined in what is called artificial intelligence of thing (AIoT). With the increase of the amount of data produced by the myriad of connected things and the large wide of data needed to train Artificial Intelligence models, data processing and storage became a real challenge. Indeed, the amount of data to process, to transfer by network and to treat in the cloud have call into question classical data storage and processing architectures. Also, the large amount of data generated at the edge has increased the speed of data transportation that is becoming the bottleneck for the cloud-based computing paradigms. The post-cloud approaches using Edge computing allow to improve latency and jitter. These infrastructures manage mechanisms of containerization and orchestration in order to provide automatic and fast deployment and migration of services such as reasoning mechanisms, ontology, and specifically adapted artificial intelligence algorithms. In this paper we propose a new architecture used to deploy at edge level micro services and adapted artificial intelligence algorithms and models.}
}
@article{ELIXHAUSER2024103586,
title = {Interdisciplinary, but how? Anthropological Perspectives from Collaborative Research on Climate and Environmental Change},
journal = {Environmental Science & Policy},
volume = {151},
pages = {103586},
year = {2024},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2023.103586},
url = {https://www.sciencedirect.com/science/article/pii/S1462901123002356},
author = {Sophie Elixhauser and Zofia Boni and Nataša {Gregorič Bon} and Urša Kanjir and Alexandra Meyer and Frank Muttenzer and Mareike Pampus and Zdenka Sokolíčková},
keywords = {Climatic and environmental change, Anthropology, Interdisciplinarity, Knowledge co-creation, Good practices, Collaboration},
abstract = {The aim of this perspective article is to rethink how anthropology can be involved in interdisciplinary research on climate and environmental change, considering wide-spread obstacles for successful collaboration and recommending best practices. Anthropologists complement ”big data“ with “thick data“, which must not be overlooked if the global scientific goal is to have a sustainable and responsible local impact in communities facing environmental change. Anthropologists are used to working with uncertainty, qualified for shifting scales and perspectives, and, perhaps most importantly, pre-occupied with studying the human dimensions of environmental change. However, there are still many practical, ontological and epistemological challenges concerning interdisciplinary research with an environmental focus. After outlining the most recent developments and literature on interdisciplinary research, we share our experience with integrating diverse forms of environmental knowledge including local and indigenous knowledge. Using an inductive approach, we build on and illustrate our conclusions with ethnographic vignettes that stem from a variety of our interdisciplinary projects. Several key themes and suggestions emerge: a) establishment of a joint epistemological framework before the research phase; b) humility and respect for methodologies used by other disciplines, including time spent on studying these with colleagues of different disciplinary backgrounds; c) openness, creativity and flexibility to step out of one’s own disciplinary comfort zone; d) communication within the project team based on trust and without disciplinary hierarchies. Finally, we share some practical suggestions on how to set up interdisciplinary projects.}
}
@article{YANG2023102595,
title = {Meta-model-based shop-floor digital twin architecture, modeling and application},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {84},
pages = {102595},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102595},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000716},
author = {Xiaolang Yang and Xuemei Liu and Heng Zhang and Ling Fu and Yanbin Yu},
keywords = {Shop-floor digital twin, Meta-model, MBSE, RAMI 4.0, Intelligent manufacturing},
abstract = {Digital twin is regarded as the virtual counterpart of physical entities, which can mirror the physical behavior and performance. Digital twin technology provides strong support for the achievement of cyber-physical system and intelligent manufacturing. Many investigations have been carried out for the digital twin of specific products. However, there are less researches on digital twin in the shop-floor domain, and there is a lack of model-driven digital twin comprehensive architecture. The modeling approach to the full lifecycle of digital twin is not considered enough. This paper proposes a meta-model-based shop-floor digital twin construction approach and a comprehensive architecture. A meta-model based on RAMI 4.0 is constructed, which provide a novel idea for the description of manufacturing resources and their status. The proposed shop-floor digital twin architecture consists of three key implementation elements: the meta-model construction, data modeling (including data interaction between cyber-physical spaces) and constructing different integration level models of shop-floor digital twin based on iteration feedback between the demands and models. The proposed approach is validated through a case study of the fischer learning factory 4.0.}
}
@article{MAHRINGER2025e41952,
title = {The Iceberg Model of Change: A taxonomy differentiating approaches to change},
journal = {Heliyon},
volume = {11},
number = {2},
pages = {e41952},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2025.e41952},
url = {https://www.sciencedirect.com/science/article/pii/S2405844025003329},
author = {Christian A. Mahringer and Laura Schmiedle and Lisa Albicker and Simone Mayer},
keywords = {Climate change, Development, Grand challenges, Interdisciplinarity, Organizational change, Phase change, Process, Transformation},
abstract = {Change is a ubiquitous phenomenon, but different scientific communities conceptualize change differently, which hampers conceptual clarity. This conceptual paper, which is based on a review of the literature on change, addresses this problem by developing the ‘Iceberg Model of Change’. This framework distinguishes three approaches to change: objectification, distinction, and unfolding. The objectification approach treats processes of change as things with symbolic properties, which can be used to steer societal and political discourse, reveal thematic relationships across studies, and emphasize the significance of work. This approach also tends to consider change as a variable (dependent or independent) that can be used to understand antecedents and consequences. The distinction approach conceptualizes change as a series of discrete states of an entity or system at multiple points in time or as phases, enabling comparison of those states. The unfolding approach considers how change processes develop, including the complex, interrelated mechanisms underpinning change. Here, line graphs, visualizations of interaction mechanisms, and trajectories are used to capture change. This framework contributes to research, a) by enabling a comprehensive consideration of change phenomena, b) by promoting interdisciplinary collaboration when project partners differ in their assumptions about change, and c) by emphasizing the need for methodological reflexivity.}
}
@article{TRAPPEY2022100331,
title = {VR-enabled engineering consultation chatbot for integrated and intelligent manufacturing services},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100331},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100331},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X2200005X},
author = {Amy J.C. Trappey and Charles V. Trappey and Min-Hua Chao and Chun-Ting Wu},
keywords = {Chatbot, Virtual reality, Power transformer, Natural language processing, Manufacturing service, Product-service system},
abstract = {Industrial equipment manufacturing, such as electric power transformers, require highly customized design, assembly, installation and maintenance services for customers. Engineering consultation is an essential manufacturing service which requires deep domain knowledge. This research develops and implements the framework and the prototype of a Virtual Reality (VR) enabled intelligent engineering consultation chatbot system with natural language communication capabilities. The consultation chatbot case focuses on the power transformer knowledge domain. The system is constructed using a comprehensive knowledge base of Frequently Asked Questions (FAQs) and answers from a power transformer manufacturer. A total of 490,000 technical articles (with more than 1.1 billion words) from Wikipedia and a comprehensive domain-specific document set are used to train a word embedding model for retrieving questions and answers (QA) from the FAQ knowledge base. Immersive VR technology provides a highly interactive QA chatbot with realistic graphical views for informative engineering counselling. The case example demonstrates the accuracy of the proposed VR-enabled chatbot for transformer QAs exceeds 91%.}
}
@article{ROBACH2025103323,
title = {Certified control for train sign classification},
journal = {Science of Computer Programming},
volume = {246},
pages = {103323},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2025.103323},
url = {https://www.sciencedirect.com/science/article/pii/S0167642325000620},
author = {Jan Roßbach and Michael Leuschel},
keywords = {ATO, Artificial intelligence, Formal methods, Computer vision, Autonomous systems},
abstract = {Certified control makes it possible to use artificial intelligence for safety-critical systems. It is a runtime monitoring architecture, which requires an AI to provide certificates for its decisions; these certificates can then be checked by a separate classical system. In this article, we evaluate the practicality of certified control for providing formal guarantees about an AI-based perception system. In this case study, we implemented a certificate checker that uses classical computer vision algorithms to verify railway signs detected by an AI object detection model. We have integrated this prototype with the popular object detection model YOLO. Performance metrics on generated data are promising for the use-case, but further research is needed to generalize certified control for other tasks.}
}
@article{MWINLAARU2021265,
title = {Syntactic position, qualitative features and extended demonstrative functions: Dagaare distal demonstratives nὲ and lὲ in interactional discourse},
journal = {Journal of Pragmatics},
volume = {182},
pages = {265-292},
year = {2021},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2021.01.033},
url = {https://www.sciencedirect.com/science/article/pii/S0378216621000679},
author = {Isaac N. Mwinlaaru and Foong Ha Yap},
keywords = {Dagaare, Deixis, Demonstratives, Discourse markers, Indexing stance, Specificity and definiteness},
abstract = {Research has shown that demonstratives are used beyond their basic meanings to encode various extended discourse functions. Based on data from the Lobr dialect of Dagaare (Niger–Congo: Gur/Mabia), the present study hypothesises that the syntactic position of demonstratives and their ‘qualitative features’ (i.e. non-deictic semantic features) influence their discourse uses. The study examines the extended discourse uses of two distal demonstratives, nὲ, a demonstrative determiner that identifies lower order ontological entities (i.e. material phenomena, including humans), and lὲ, a demonstrative pronoun that refers to higher order ontological entities (i.e. phenomena such as locutions, ideas and actions). The study shows that the demonstrative determiner nὲ extends its uses within the nominal domain to encode speaker attitude towards referents and to express a range of stances and presuppositions that contribute to the management of interpersonal relations among interactants, such as establishing common ground and focus reinforcement. The free-standing demonstrative pronoun lὲ, on the other hand, extends its uses beyond the nominal domain to serve discourse organization functions such as a frame signalling device and textual cohesion. The study shows how demonstratives extend their domain of discourse uses from spatio-temporal deixis to textual and psycho-social deixis.}
}
@article{BALDVINSDOTTIR2021101039,
title = {The validity of management accounting language games – A pragmatic constructive perspective},
journal = {The British Accounting Review},
volume = {53},
number = {6},
pages = {101039},
year = {2021},
issn = {0890-8389},
doi = {https://doi.org/10.1016/j.bar.2021.101039},
url = {https://www.sciencedirect.com/science/article/pii/S0890838921000652},
author = {Gudrun Baldvinsdottir},
abstract = {To support the development of a pragmatic practice-based theory for management accounting, practice theory offers valuable avenues for understanding the development, role, and effects of tools and techniques used by practitioners. Scholars have identified communication as a critical dimension for analysis, as discourse can govern the production of knowledge and power structures. However, previous research has mainly focused on the role of actors within static environments; how actors justify their actions and goals, compared with other actors; and how they are aligned with or justify new practices relative to existing practices. The pragmatic constructivist framework that underpins this special issue builds on this research but recognises that previous research has paid little attention to how people—when interacting within a dynamic environment—develop and create new types of constructive causality. Importantly, the necessary conditions for people's actions to construct what they intend to remain largely unexplored. Pragmatic constructivism is founded on the recognition that any theory of management accounting must include conceptual devices representing the notion of success, as well as techniques for evaluating the truth aptness of local practices of reality construction, such as those represented by managemen accounting. This special issue aims to theoretically and empirically explore the potential in management accounting for the measurement and governance of constructed causality. The central topic is the role of management accounting in supporting individual and collective actors to effectively construct causal chains that make organisations work. The three articles in this special issue adopt different approaches to combining the pragmatic constructivist framework with additional theoretical frameworks, as well as using different research methodologies. The papers' findings point to the importance of co-authorship in the creation of functioning organisational practices, and suggest that this might be threatened by information technology. Co-authorship involves language games in the form of dialogical interaction between the accountants and other involved actors. This process develops a shared understanding anchored in a fusion of conceptual models that is tied to individual collaborators' local practice.}
}
@article{GARE2021104487,
title = {Code biology and the problem of emergence},
journal = {Biosystems},
volume = {208},
pages = {104487},
year = {2021},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2021.104487},
url = {https://www.sciencedirect.com/science/article/pii/S0303264721001349},
author = {Arran Gare},
keywords = {Code biology, Emergence, Hierarchy theory, Downward causation, Final causes, Teleology, Infodynamics, Anticipatory systems, Dissipative structures, Transduction, Individuation, Marcello barbieri, Stanley N. Salthe, Robert Rosen, Gilbert Simondon},
abstract = {It should now be recognized that codes are central to life and to understanding its more complex forms, including human culture. Recognizing the ‘conventional’ nature of codes provides solid grounds for rejecting efforts to reduce life to biochemistry and justifies according a place to semantics in life. The question I want to consider is whether this is enough. Focussing on Eigen's paradox of how a complex code could originate, I will argue that along with Barbieri's efforts to account for the origins of life based on the ribosome and then to account for the refined codes through a process of ambiguity reduction, something more is required. Barbieri has not provided an adequate account of emergence, or the basis for providing such an account. I will argue that Stanley Salthe has clarified to some extent the nature of emergence by conceptualizing it as the interpolation of new enabling constraints. Clearly, codes can be seen as enabling constraints. How this actually happens, though, is still not explained. Stuart Kauffman has grappled with this issue and shown that it radically challenges the assumptions of mainstream science going back to Newton. He has attempted to reintroduce real possibilities or potentialities into his ontology, and argued that radically new developments in nature are associated with realizing adjacent possibles. This is still not adequate. What is also involved, I will suggest, utilizing concepts developed by the French natural philosopher Gilbert Simondon, is ‘transduction’ as part of ‘ontogenesis’ of individuals in a process of ‘individuation’, that is, the emergence of ‘individuals’ from preindividual fields or milieux.}
}
@article{HEATH2022102258,
title = {The processes-to-end(s) paradox of public relations},
journal = {Public Relations Review},
volume = {48},
number = {5},
pages = {102258},
year = {2022},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2022.102258},
url = {https://www.sciencedirect.com/science/article/pii/S0363811122001138},
author = {Robert L. Heath},
keywords = {Communitarianism/constituted communities, Processual textuality as narrative continuity, Process-to-ends paradigms, Collective resource management, Responsible obligation},
abstract = {Public relations researchers, theorists, and practitioners should integrate professional practice with research topics, themes, concepts, theories, contextual intelligences, strategic processes, moral judgment, and functional practices by addressing the end(s) that public relations serves in community. Disciplinary intelligences feature outcomes that yield to specialized knowledge-based, processually achievable, intellectually justifiable, strategically valuable, and morally serviceable practices that constitute culture, society, and community. In that communitarian spirit, this paper draws on established research streams and strategic process-to-ends paradigms to argue that unique knowledge serves to collaboratively assist the constitution of community as place in search for order. Public relations’ unique knowledge, constantly refined and critically guided, adds strategic, emergent force to human’s communitarian imperative. This communitarian rationale for the constitutive paradigm reasons that as individuals, organizations and communities engage to communicate they communicate to organize through narrative continuity. Organizations, individuals, and groups engage on the winding path of agonistic, emergently epistemological and ontological cultural and societal moments to seek order in the public interest. Reputations (identities), complexes of relationships (institutionalized identification) and textuality of narrative continuity empower community members co-manage resources in service of shared interests’ license to operate.}
}
@article{CHAKRABORTY2024214,
title = {Syntactic Category based Assamese Question Pattern Extraction using N-grams},
journal = {Procedia Computer Science},
volume = {235},
pages = {214-230},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924007002},
author = {Rita Chakraborty and Manisha Deka and Shikhar Kr. Sarma},
keywords = {Syntactic categories, PoS tags, Question pattern, Unique patterns, N-grams},
abstract = {Syntactic categories are important because they play a major role in analysis of sentences. Assamese is a versatile language added with diversified linguistic features. Analysis and manipulation of such a rich language in terms of its vocabulary is really challenging. It requires incorporation of syntactic level annotations like Parts of Speech (PoS) tags into the text for the purpose of efficient and effective processing for different Natural Language Processing (NLP) tasks. Our research mainly focuses on extraction of patterns of syntactic categories in Assamese question sentences. We have also worked on extracting the Bi-grams and Tri-grams of these patterns so that analysis and processing can be done on more basic level. In this paper, we have tried to focus on extraction of PoS tag based Assamese question patterns. We also have tried to generate the Bi-grams and Tri-grams with their frequencies of occurrences throughout the patterns. The work is a noval attempt in modeling question sentence patterns with the embedded PoS patterns.}
}
@article{LI202324,
title = {Construction of a genomic instability-derived predictive prognostic signature for non-small cell lung cancer patients},
journal = {Cancer Genetics},
volume = {278-279},
pages = {24-37},
year = {2023},
issn = {2210-7762},
doi = {https://doi.org/10.1016/j.cancergen.2023.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S2210776223000431},
author = {Wei Li and Huaman Wu and Juan Xu},
keywords = {Genomic instability, Non-small cell lung cancer, Prognosis, Derived signature},
abstract = {Background
Genomic instability (GI) is an effective prognostic marker of cancer. Thus, in this work, we aimed to explore the impact of GI derived signature on prognosis in non-small cell lung cancer (NSCLC) patients using bioinformatics methods.
Methods
The data of NSCLC patients were collected from The Cancer Genome Atlas. Totally 1794 immune related genes were downloaded from immport database. The optimal prognosis related genes were identified by univariate and LASSO Cox analyses. The risk score model was built to predict the NSCLC patients’ prognosis. The immune cell infiltration was analyzed in CIBERSORT.
Results
The 951 differentially expressed genes (DEGs) between the genomic stability (GS) and GI groups were enriched in 862 Gene ontology terms and 32 Kyoto Encyclopedia of Genes and Genomes pathways. Based on the 13 optimal genes, a prognostic risk score mode for NSCLC was established, and the high-risk patients exhibited worse overall survival. Moreover, the nomogram could reliably predict the clinical outcomes. The immune cell infiltration and checkpoints were significantly differential between the two groups (high-risk and low-risk).
Conclusion
The GI related 13-gene signature (TMPRSS11E, TNNC2, HLF, FOXM1, PKMYT1, TCN1, RGS20, SYT8, CD1B, LY6K, MFSD4A, KLRG2 APCDD1L) could reliably predict the prognosis of NSCLC patients.}
}
@article{LEYS2024101563,
title = {Valuation and conflicts in the Peruvian extractive frontier: Towards a politics of value analytical framework},
journal = {The Extractive Industries and Society},
volume = {20},
pages = {101563},
year = {2024},
issn = {2214-790X},
doi = {https://doi.org/10.1016/j.exis.2024.101563},
url = {https://www.sciencedirect.com/science/article/pii/S2214790X2400159X},
author = {Peter Leys},
keywords = {Valuation, Extractivism, Conflicts, Peru, Graeber},
abstract = {This article contributes to the on-going debate about how to understand extractive conflicts. What drives conflicts in areas of extraction? Do local people mobilize to reject extraction outright, or do they mobilize to secure rents and compensation from extractive projects? Political ecologists and ecological economists argue that different incommensurable languages of valuation challenge monetary value in a contest of values. Political economists disagree and argue that the majority of conflicts are about compensation, not alternative valuations. In this article, I suggest that value does indeed play a key role in extractive conflicts, but I also recognize the criticisms presented by political economy. To explore an alternative usage of value for understanding extractive conflicts, I draw on David Graebers anthropological notion of value, through which I elaborate an analytical framework of a politics of value, illustrated by three case studies of extractive conflicts in the Peruvian Andes. By analyzing conflicts in the extractive frontier as a ‘politics of value’, this article re-thinks how we understand the complicated dynamics of value and valuation in the extractive frontier and develops an analytical framework of a politics of value, to understand how conflict dynamics shape valuation, and how valuation, in turn, shapes conflict dynamics.}
}
@article{SCHINTKE202482,
title = {Validity constraints for data analysis workflows},
journal = {Future Generation Computer Systems},
volume = {157},
pages = {82-97},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.03.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24001079},
author = {Florian Schintke and Khalid Belhajjame and Ninon {De Mecquenem} and David Frantz and Vanessa Emanuela Guarino and Marcus Hilbrich and Fabian Lehmann and Paolo Missier and Rebecca Sattler and Jan Arne Sparka and Daniel T. Speckhard and Hermann Stolte and Anh Duc Vu and Ulf Leser},
keywords = {Scientific workflow systems, Workflow specification languages, Validity constraints, Dependability, Integrity and conformance checking},
abstract = {Porting a scientific data analysis workflow (DAW) to a cluster infrastructure, a new software stack, or even only a new dataset with some notably different properties is often challenging. Despite the structured definition of the steps (tasks) and their interdependencies during a complex data analysis in the DAW specification, relevant assumptions may remain unspecified and implicit. Such hidden assumptions often lead to crashing tasks without a reasonable error message, poor performance in general, non-terminating executions, or silent wrong results of the DAW, to name only a few possible consequences. Searching for the causes of such errors and drawbacks in a distributed compute cluster managed by a complex infrastructure stack, where DAWs for large datasets typically are executed, can be tedious and time-consuming. We propose validity constraints (VCs) as a new concept for DAW languages to alleviate this situation. A VC is a constraint specifying logical conditions that must be fulfilled at certain times for DAW executions to be valid. When defined together with a DAW, VCs help to improve the portability, adaptability, and reusability of DAWs by making implicit assumptions explicit. Once specified, VCs can be controlled automatically by the DAW infrastructure, and violations can lead to meaningful error messages and graceful behavior (e.g., termination or invocation of repair mechanisms). We provide a broad list of possible VCs, classify them along multiple dimensions, and compare them to similar concepts one can find in related fields. We also provide a proof-of-concept implementation for the workflow system Nextflow.}
}
@article{CORNISH202291,
title = {Text, discourse, context: A meta-trilogy for discourse analysis},
journal = {Journal of Pragmatics},
volume = {199},
pages = {91-104},
year = {2022},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2022.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378216622001837},
author = {Francis Cornish},
keywords = {Text, Discourse, Context, Indexical reference, Mental discourse representations},
abstract = {The majority of functional models of language, which purport to account for text, do not recognize the distinct dimension corresponding to “discourse”, as conceived here. Instead, the various semantic-pragmatic aspects of the use of indexical expressions, in particular, tend to be conceived uniquely in terms of the textual environment of the markers involved. Correlatively, the invocation of “context” in such approaches tends to be limited essentially to the co-text. But according to the present study, such expressions operate in terms of the mental discourse representations which the participants are jointly as well as severally constructing as the discourse proceeds. The article's aim is to show that the discourse dimension of language use is crucial to the ways in which indexical markers function, and hence should be taken into account in their modelling. This can only be done, it is argued, by integrating their treatment within a model of the broader utterance context in which such expressions are used, thus permitting a dialectic between the system-derived properties of such expressions and those emanating from the particular uses of such resources in actual communication.}
}
@article{WANG2024105602,
title = {Proactive safety hazard identification using visual–text semantic similarity for construction safety management},
journal = {Automation in Construction},
volume = {166},
pages = {105602},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105602},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524003388},
author = {Yiheng Wang and Bo Xiao and Ahmed Bouferguene and Mohamed Al-Hussein},
keywords = {Behavior-based safety, Computer vision, Natural language processing, Image captioning, Automatic safety hazard identification},
abstract = {Automated safety management in construction can reduce injuries by identifying hazardous postures, actions, and missing personal protective equipment (PPE). However, existing computer vision (CV) methods have limitations in connecting recognition results to text-based safety rules. To address this issue, this paper presents a multi-modal framework that bridges the gap between construction image monitoring and safety knowledge. The framework includes an image processing module that utilizes CV and dense image captioning techniques, and a text processing module that employs natural language processing for semantic similarity evaluation. Experiments showed a mean average precision of 49.6% in dense captioning and an F1 score of 74.3% in hazard identification. While the proposed framework demonstrates a promising multi-modal approach towards automated safety hazard identification and reasoning, improvements in dataset size and model performance are still needed to enhance its effectiveness in real-world applications.}
}
@article{STETTER2023109,
title = {Geometric and kinetic digital twin of a body-in-white assembly system for virtual commissioning},
journal = {Procedia CIRP},
volume = {119},
pages = {109-114},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004559},
author = {Ralf Stetter and Tobias Grüble and Markus Till},
keywords = {Digital twin, graph-based design languages, digital engineering, model-based systems engineering},
abstract = {Producing companies face enormous challenges due to increasing customer expectations, global competition and sustainability considerations. In recent years, the concept of the digital twin (DT) has found rising attention, because this concept has the potential to enable, amongst others, superior monitoring, process control, diagnosis, traceability and quality control. Additionally, the concept of a DT may intensify the integration of product development, production planning and production, because it is sensible to initiate the creation of the digital twin already in product development and the potential of this concept can only be fully realized if all phases of the system lifecycle are considered. Extensive research has already covered the application of DTs during production operation, but concerning the application in the product design stage many questions remain. The research described in this paper is focused on the question how design automation concepts can contribute to an automated creation of a DT. The paper explains a novel concept for realizing a digital twin. The novel concept is employing an engineering framework based on graph-based design languages. It is applied to an automotive body-in-white assembly system and aims at supporting virtual commissioning. The digital twin includes geometric and kinetic models and simulations and its creation already starts in product development.}
}
@article{HAMMOUDEH2022104,
title = {Soccer captioning: dataset, transformer-based model, and triple-level evaluation},
journal = {Procedia Computer Science},
volume = {210},
pages = {104-111},
year = {2022},
note = {The 13th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN) / The 12th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2022) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.10.125},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922015812},
author = {Ahmad Hammoudeh and Bastien Vanderplaetse and Stéphane Dupont},
keywords = {video captioning, transformer, soccer, deep learning, multimodality},
abstract = {This work aims at generating captions for soccer videos using deep learning. The paper introduces a novel dataset, model, and triple-level evaluation. The dataset consists of 22k caption-clip pairs and three visual features (images, optical flow, inpainting) for 500 hours of SoccerNet videos. The model is divided into three parts: a transformer learns language, ConvNets learn vision, and a fusion of linguistic and visual features generates captions. The suggested evaluation criterion of captioning models covers three levels: syntax (the commonly used evaluation metrics such as BLEU-score and CIDEr), semantics (the quality of descriptions for a domain expert), and corpus (the diversity of generated captions). The paper shows that the diversity of generated captions has improved (from 0.07 reaching 0.18) with semantics-related losses that prioritize selected words. Semantics-related losses and the utilization of more visual features (optical flow, inpainting) improved the normalized captioning score by 27%.}
}
@article{YU2022101649,
title = {Distributed representation learning and intelligent retrieval of knowledge concepts for conceptual design},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101649},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101649},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001136},
author = {Hui Yu and Wu Zhao and Qian Zhao},
keywords = {Data-driven conceptual design, Knowledge representation, Engineering semantic network, Pathfinding algorithm},
abstract = {Data-driven conceptual design is rapidly emerging as a powerful approach to generate novel and meaningful ideas by leveraging external knowledge especially in the early design phase. Currently, most existing studies focus on the identification and exploration of design knowledge by either using common-sense or building specific-domain ontology databases and semantic networks. However, the overwhelming majority of engineering knowledge is published as highly unstructured and heterogeneous texts, which presents two main challenges for modern conceptual design: (a) how to capture the highly contextual and complex knowledge relationships, (b) how to efficiently retrieve of meaningful and valuable implicit knowledge associations. To this end, in this work, we propose a new data-driven conceptual design approach to represent and retrieve cross-domain knowledge concepts for enhancing design ideation. Specifically, this methodology is divided into three parts. Firstly, engineering design knowledge from the massive body of scientific literature is efficiently learned as information-dense word embeddings, which can encode complex and diverse engineering knowledge concepts into a common distributed vector space. Secondly, we develop a novel semantic association metric to effectively quantify the strength of both explicit and implicit knowledge associations, which further guides the construction of a novel large-scale design knowledge semantic network (DKSN). The resulting DKSN can structure cross-domain engineering knowledge concepts into a weighted directed graph with interconnected nodes. Thirdly, to automatically explore both explicit and implicit knowledge associations of design queries, we further establish an intelligent retrieval framework by applying pathfinding algorithms on the DKSN. Next, the validation results on three benchmarks MTURK-771, TTR and MDEH demonstrate that our constructed DKSN can represent and associate engineering knowledge concepts better than existing state-of-the-art semantic networks. Eventually, two case studies show the effectiveness and practicality of our proposed approach in the real-world engineering conceptual design.}
}
@article{ZHANG2025101499,
title = {Development of the TP53 mutation associated hypopharyngeal squamous cell carcinoma prognostic model through bulk multi-omics sequencing and single-cell sequencing},
journal = {Brazilian Journal of Otorhinolaryngology},
volume = {91},
number = {1},
pages = {101499},
year = {2025},
issn = {1808-8694},
doi = {https://doi.org/10.1016/j.bjorl.2024.101499},
url = {https://www.sciencedirect.com/science/article/pii/S1808869424001149},
author = {Ying Zhang and Yue Cui and Congfan Hao and Yingjie Li and Xinyang He and Wenhui Li and Hongyang Yu},
keywords = {Hypopharyngeal squamous cell carcinoma, Prognosis, TP53 mutation, Single-cell sequencing},
abstract = {Objective
The aim of this study was to construct a prognostic model based on the TP53 mutation to calculate prognostic risk scores of patients with HPSCC.
Methods
TP53 mutation and transcriptome data were downloaded from the TCGA databases. Gene expression data from GSE65858, GSE41613, GSE3292, GSE31056, GSE39366, and GSE227156 datasets were downloaded from the GEO database. GSEA, univariate, multivariate Cox analyses, and LASSO analysis were employed to identify key genes and construct the prognostic model. ROC curves were utilized to validate the OS and RFS results obtained from the model. The associations between risk scores with various clinicopathological characteristics and immune scores were analyzed via ggplot2, corrplot package, and GSVA, respectively. Single-cell sequencing data was analyzed via unbiased clustering and SingleR cell annotations.
Results
Initially, two key genes, POLD2 and POLR2G, were identified and utilized to construct the prognostic model. Samples were divided into different risk groups via the risk scores obtained from the model, with high-risk group samples exhibiting poorer prognosis. Furthermore, the risk score exhibited a positive correlation with lymphatic metastasis in patients and the immune scores of CD4+ T, CD8+ T, dendritic cell, macrophage, and neutrophil. The immune responses also exhibited notable disparities between the high- and low-risk groups. The results of single-cell sequencing analysis demonstrated that epithelial cells and macrophages were relatively abundant in HPSCC samples. POLD2 and POLR2G exhibited higher expressions in epithelial cells, with most of the identified pathways also enriched in epithelial cells.
Conclusion
The prognostic model exhibited a significant capacity for predicting the prognosis of HSPCC samples based on the TP53 mutation conditions and may also predict the cancer characteristics and immune infiltration scores of samples via different risk scores obtained from the model.
Level of evidence
Level 5.}
}
@incollection{DAS20231,
title = {Chapter 1 - Sentiment analysis and computational intelligence},
editor = {Dipankar Das and Anup Kumar Kolya and Abhishek Basu and Soham Sarkar},
booktitle = {Computational Intelligence Applications for Text and Sentiment Data Analysis},
publisher = {Academic Press},
pages = {1-16},
year = {2023},
series = {Hybrid Computational Intelligence for Pattern Analysis and Understanding},
isbn = {978-0-323-90535-0},
doi = {https://doi.org/10.1016/B978-0-32-390535-0.00006-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323905350000069},
author = {Dipankar Das and Anup Kumar Kolya and Soham Sarkar and Abhishek Basu},
keywords = {Sentiment analysis, Computational intelligence, Social network},
abstract = {The internet boom of this millennium and especially the explosion of social-networking platforms allow digital data to glide all around the world in real-time and evoke multi-faceted content data analysis as a challenging proposition to scientists as well as researchers. While much of the content originates from various social-media sources on different topics and in different languages, it fuels more challenges to analyze sentiments accurately while processing opinions accumulated from different channels. Emotion and polarity prediction from various social media such as Facebook, Twitter, websites, etc., is becoming an emerging field of predictive modeling. Thus Computational Intelligence (CI) techniques play important roles here to solve such inherent problems of sentiment-analysis applications by serving the key roles in development of recent technologies from academia, even in educational psychology, to industrial arenas such as remote sensing, interfacing technologies, retrieval, and even recommendation systems.}
}
@article{ROYGREGOIRE2019688,
title = {Dialogue as racism? The promotion of “Canadian dialogue” in Guatemala's extractive sector},
journal = {The Extractive Industries and Society},
volume = {6},
number = {3},
pages = {688-701},
year = {2019},
issn = {2214-790X},
doi = {https://doi.org/10.1016/j.exis.2019.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S2214790X18300534},
author = {Etienne {Roy Grégoire}},
keywords = {Dialogue, Corporate Social Responsibility, Post-conflict societies, Guatemala, Canada},
abstract = {“Dialogue” is an essential element of Canada's CSR policies. Although the notion evokes a rich tradition in political and legal philosophy, in the context of mining governance its political and normative implications have received very little attention. This article contributes to a better understanding of such implications by conducting a case study of the promotion of dialogue by the Canadian state in Guatemala. Through critical discourse analysis, I examine logical correspondences between dialogue as a normative framework and a racist discourse that emanates from the Guatemalan oligarchy. I also examine interrelationship between concrete dialogue promotion and the repression of mining opponents. I find that the promotion of dialogue by Canada enunciates political ontologies that resonate at the core of Guatemala's post-conflict politics and contributes to stigmatizing rights-based opposition to mining, hindering Indigenous collective action and undermining democratization efforts.}
}
@incollection{DEVARDA2024,
title = {Word Semantic Similarity Norms},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00176-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041001769},
author = {Andrea Gregor {de Varda} and Marco Ciapparelli},
keywords = {Semantic similarity, Human ratings, Taxonomic similarity, Thematic similarity},
abstract = {Word similarity estimates play a crucial role in the language sciences, highlighting the importance of choosing the appropriate norms for the use case at hand. This article presents an overview of the most popular resources offering human-assessed similarity scores between word pairs. The available norms are distinguished by the kind of relationship that the raters are asked to evaluate (taxonomic, thematic, perceptual) and their coverage, providing the readers with pointers to relevant resources.}
}
@article{LEBOUTEILLER2019438,
title = {A new conceptual methodology for interpretation of mass transport processes from seismic data},
journal = {Marine and Petroleum Geology},
volume = {103},
pages = {438-455},
year = {2019},
issn = {0264-8172},
doi = {https://doi.org/10.1016/j.marpetgeo.2018.12.027},
url = {https://www.sciencedirect.com/science/article/pii/S0264817218305579},
author = {Pauline {Le Bouteiller} and Sara Lafuerza and Jean Charléty and Antonio Tadeu Reis and Didier Granjeon and Florence Delprat-Jannaud and Christian Gorini},
keywords = {Mass transport deposits, Mass transport processes, Submarine slope failures, Seismic interpretation, Knowledge-based interpretation, Ontology},
abstract = {Identification and seismic mapping of mass-transport deposits (MTDs) are vital targets for marine geological studies both for a better understanding of mass wasting processes and geohazards and for economic prospects in sedimentary basins. In recent decades, refinements in the interpretation of these geobodies have benefited from increasingly good quality 3D seismic data. However, approaches to define the characteristics, rheology and mechanics of such slope failure deposits still rely mainly on inferences from case-dependent interpretations of these stratigraphic elements; what is more, features and seismic characteristics of MTDs may vary significantly from one case to another, implying the existence of many different environments and related physics. This makes the study of submarine mass movement a challenging task for a seismic interpreter. In this paper, we present a new conceptual analytical method based on an objective approach for interpreting the wide range of diverse objects related to mass wasting, in order to minimize seismic interpretation subjectivity. We propose an ontology-like methodology, based on a conceptual organization of a diversity of interpretation elements arranged in a knowledge base. MTDs are considered as objects with representative properties, each one characterized by several descriptors, which are themselves impacted by multiple physical processes in a graph-based conception. We thus propose a method to infer the most probable interpretations for one mass movement from its deposit characteristics. We applied our graph-based methodology to two MTDs delineated in 3D seismic data in the Offshore Amazon Basin, Brazil. Based on the analysis of all MTD properties and their possible causes, several candidate interpretations were provided. The interpretations yielded by the graph are in line with the known geology and instability processes of the region, thereby validating the feasibility of the approach. The next development stage is a numerical definition of the knowledge base for further sharing and operability.}
}
@article{RICHARDBOLLANS202345,
title = {Identifying and modelling polysemous senses of spatial prepositions in referring expressions},
journal = {Cognitive Systems Research},
volume = {77},
pages = {45-61},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2022.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041722000432},
author = {Adam Richard-Bollans and Lucía {Gómez Álvarez} and Anthony G. Cohn},
keywords = {Semantics, Spatial language, Polysemy, Referring expressions},
abstract = {In this paper we analyse the issue of reference using spatial language and examine how the polysemy exhibited by spatial prepositions can be incorporated into semantic models for situated dialogue. After providing a brief overview of polysemy in spatial language and a review of related work, we describe an experimental study we used to collect data on a set of relevant spatial prepositions. We then establish a semantic model in which to integrate polysemy (the Baseline Prototype Model), which we test against a Simple Relation Model and a Perceptron Model. To incorporate polysemy into the baseline model we introduce two methods of identifying polysemes in grounded settings. The first is based on ‘ideal meanings’ and a modification of the ‘principled polysemy’ framework and the second is based on ‘object-specific features’. In order to compare polysemes and aid typicality judgements we then introduce a notion of ‘polyseme hierarchy’. Finally, we test the performance of the polysemy models against the Baseline Prototype Model and Perceptron Model and discuss the improvements shown by the polysemy models.}
}
@article{ASH2022101456,
title = {Marra philosophies of stone, and the stone artefacts of Walanjiwurru 1 rockshelter, Marra Country, northern Australia},
journal = {Journal of Anthropological Archaeology},
volume = {68},
pages = {101456},
year = {2022},
issn = {0278-4165},
doi = {https://doi.org/10.1016/j.jaa.2022.101456},
url = {https://www.sciencedirect.com/science/article/pii/S0278416522000642},
author = {Jeremy Ash and John J. Bradley and Jerome Mialanes and Liam M. Brady and Shaun Evans and David Barrett and Bruno David and Daryl Wesley and Emilie Dotte-Sarout and Cassandra Rowe and Chris Urwin and Tiina Manne},
keywords = {Aboriginal archaeology, Chert, Ethnoarchaeology, Ontology, Quarries, Quartzite, Relationality, Stone artefacts},
abstract = {In archaeology, investigations into the social and cultural contexts of stone artefacts have largely focused on their typological styles, manufacturing technologies, functions, geographic distributions and the significance of the quarries they come from. Yet what is oftentimes overlooked is the deeper contemporary understandings by Indigenous groups of the stone artefacts recovered from excavations. In this paper, we analyse an assemblage of 9,642 excavated stone artefacts from the rockshelter site of Walanjiwurru 1 in Marra Country in northern Australia, in light of the cosmological significance of regional stone sources to local Aboriginal groups. Each recovered stone artefact, and the quarries of their raw materials, is laden with meanings that help reveal how Marra Aboriginal people socially and cosmologically engaged with their landscape. By combining archaeological and Marra cultural perspectives, we argue that subtle variations in the range of stones and their relational characteristics signal changing political engagements with ancestral places over the past 2300 years.}
}
@article{JEONG2025103599,
title = {Beyond ideologies of nativeness in the intelligibility principle for L2 English pronunciation: A corpus-supported review},
journal = {System},
volume = {129},
pages = {103599},
year = {2025},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2025.103599},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X25000090},
author = {Hyeseung Jeong and Stephanie Lindemann},
keywords = {Ideologies of nativeness, Intelligibility principle, English language teaching (ELT), Mutual responsibility for intelligibility, Comprehensibility, Equitable L2 pronunciation teaching and research},
abstract = {The intelligibility principle for second language (L2) pronunciation challenges the nativeness principle that suggests that L2 speakers' goal should be nativelike pronunciation. However, in this corpus-supported review of studies based on the intelligibility principle, we show that the understanding of the principle is still underpinned by what we term ideologies of nativeness, which favor speakers of privileged first language (L1) varieties and undermine other L1 and L2 World Englishes speakers. We focus on discourses surrounding the two most prominent keywords: errors, frequently used to describe proficient L2 speech and blame it for compromised intelligibility, and comprehensibility, often uncritically used to gauge listeners’ understanding although it measures subjective perception and can be closely associated with nativeness. Such discourses can obscure a more complete understanding of communication that acknowledges variation in L1 speech and the mutual nature of communication, as well as contribute toward negative perceptions of L2 speakers. Based on our review, we suggest that teachers avoid prescribed pronunciation models, instead helping learners develop their pronunciation based on a broad perceptual repertoire. We further recommend that researchers focus on how communication difficulties are successfully repaired without relying on prescribed pronunciation norms, recognizing mutual responsibility and accountability for intelligibility by all interlocutors.}
}
@article{WANG2025103755,
title = {Formalized representation of eco-design rules for additive manufacturing design advisor system},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103755},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103755},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625006482},
author = {Yanan Wang and Tao Peng and Yi Xiong and Samyeon Kim and Renzhong Tang},
keywords = {Sustainability, Design for additive manufacturing, Eco-design, Rule representation, Design advisor system},
abstract = {Sustainability has emerged as one of the most critical requirements in product design. Additive Manufacturing (AM), characterized by advantages such as design flexibility, economic low-volume customization, and reduced material waste, holds significant potential to enhance product sustainability performance from a life cycle perspective. Design for additive manufacturing (DfAM) is thereby an essential topic to leverage such potentials, which needs intelligent AM design advisor systems to assist designers in making informative decisions. While a few of system that support decisions in DfAM processes have been developed, their functions do not effectively address eco-design requirements in practical applications. The main challenge is the lack of accessible sustainability-related knowledge in the system to inform designers in various eco-design tasks such as sustainable material selection, lightweight structure design, and support material optimization. These tasks involve three different DfAM stages: conceptual, embodiment, and detail stages. Existing AM-related research has documented eco-design guidelines, either process-specific or task-specific. However, these guidelines are often presented in inconsistent formats, making them difficult for designers to interpret and for software systems to reuse directly. To address this issue, this study aims to establish a formalized representation of eco-design rules (EDRs) that enables system to reason and generate design advice related to sustainability. A three-layer framework, consisting of a raw knowledge layer, a semantic model layer, and a formalized rule layer, is then proposed to transform existing text-based eco-design guidelines with multi-dimensional product information into a formalized, human–machine equally readable EDRs representation. This formalized representation ensures that the rules are consistent, reusable, extendable, and computable. A rule-based AM eco-design advisor system is then developed to illustrate the application mechanism of the formalized EDRs within the system. Case studies involving the design of a hydraulic manifold fabricated by AM were conducted, where decision-making guidance in two eco-design tasks demonstrated the practical application of these formalized rules. The results indicate that the proposed formalized representation of EDRs enables the design system to provide valuable eco-design advice for designers in various tasks within three different DfAM stages, thereby significantly contributing to achieving sustainability in AM-fabricated products.}
}
@article{CUI2018177,
title = {Auditing SNOMED CT hierarchical relations based on lexical features of concepts in non-lattice subgraphs},
journal = {Journal of Biomedical Informatics},
volume = {78},
pages = {177-184},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417302800},
author = {Licong Cui and Olivier Bodenreider and Jay Shi and Guo-Qiang Zhang},
keywords = {Biomedical ontologies, SNOMED CT, Quality assurance, Non-lattice subgraphs, Lexical attributes},
abstract = {Objective
We introduce a structural-lexical approach for auditing SNOMED CT using a combination of non-lattice subgraphs of the underlying hierarchical relations and enriched lexical attributes of fully specified concept names. Our goal is to develop a scalable and effective approach that automatically identifies missing hierarchical IS-A relations.
Methods
Our approach involves 3 stages. In stage 1, all non-lattice subgraphs of SNOMED CT’s IS-A hierarchical relations are extracted. In stage 2, lexical attributes of fully-specified concept names in such non-lattice subgraphs are extracted. For each concept in a non-lattice subgraph, we enrich its set of attributes with attributes from its ancestor concepts within the non-lattice subgraph. In stage 3, subset inclusion relations between the lexical attribute sets of each pair of concepts in each non-lattice subgraph are compared to existing IS-A relations in SNOMED CT. For concept pairs within each non-lattice subgraph, if a subset relation is identified but an IS-A relation is not present in SNOMED CT IS-A transitive closure, then a missing IS-A relation is reported. The September 2017 release of SNOMED CT (US edition) was used in this investigation.
Results
A total of 14,380 non-lattice subgraphs were extracted, from which we suggested a total of 41,357 missing IS-A relations. For evaluation purposes, 200 non-lattice subgraphs were randomly selected from 996 smaller subgraphs (of size 4, 5, or 6) within the “Clinical Finding” and “Procedure” sub-hierarchies. Two domain experts confirmed 185 (among 223) suggested missing IS-A relations, a precision of 82.96%.
Conclusions
Our results demonstrate that analyzing the lexical features of concepts in non-lattice subgraphs is an effective approach for auditing SNOMED CT.}
}
@article{ZHANG2023e18037,
title = {Immune-dysregulated neutrophils characterized by upregulation of CXCL1 may be a potential factor in the pathogenesis of abdominal aortic aneurysm and systemic lupus erythematosus},
journal = {Heliyon},
volume = {9},
number = {7},
pages = {e18037},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e18037},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023052453},
author = {Lin Zhang and Que Li and Chenxing Zhou and Zhanman Zhang and Jiangfeng Zhang and Xiao Qin},
keywords = {Abdominal aortic aneurysm, Systemic lupus erythematosus, Neutrophils, Immune cell infiltration analysis, Bioinformatics},
abstract = {Background
The abdominal aortic aneurysm (AAA) incidence is closely related to systemic lupus erythematosus (SLE). However, the common mechanisms between AAA and SLE are still unknown. The purpose of this research was to examine the main molecules and pathways involved in the immunization process that lead to the co-occurrence of AAA and SLE through the utilization of quantitative bioinformatics analysis of publicly available RNA sequencing databases. Moreover, routine blood test information was gathered from 460 patients to validate the findings.
Materials and methods
Datasets of both AAA (GSE57691 and GSE205071) and SLE (GSE50772 and GSE154851) were downloaded from the Gene Expression Omnibus (GEO) database, and differentially expressed genes (DEGs) were analyzed using bioinformatic tools. To determine the functions of the common differentially expressed genes (DEGs), Gene Ontology (GO) and Kyoto Encyclopedia analyses were conducted. Subsequently, the hub gene was identified through cytoHubba, and its validation was carried out in GSE47472 for AAA and GSE81622 for SLE. Immune cell infiltration analysis was performed to identify the key immune cells correlated with AAA and SLE, and to evaluate the correlation between key immune cells and the hub gene. Subsequently, the routine blood test data of 460 patients were collected, and the result of the immune cell infiltration analysis was further validated by univariate and multivariate logistic regression analysis.
Results
A total of 25 common DEGs were obtained, and three genes were screened by cytoHubba algorithms. Upon validation of the datasets, CXCL1 emerged as the hub gene with strong predictive capabilities, as evidenced by an area under the curve (AUC) > 0.7 for both AAA and SLE. The infiltration of immune cells was also validated, revealing a significant upregulation of neutrophils in the AAA and SLE datasets, along with a correlation between neutrophil infiltration and CXCL1 upregulation. Clinical data analysis revealed a significant increase in neutrophils in both AAA and SLE patients (p < 0.05). Neutrophils were found to be an independent factor in the diagnosis of AAA and SLE, exhibiting good diagnostic accuracy with AUC >0.7.
Conclusion
This study elucidates CXCL1 as a hub gene for the co-occurrence of AAA and SLE. Neutrophil infiltration plays a central role in the development of AAA and SLE and may serve to be a potential diagnostic and therapeutic target.}
}
@article{CUI2025101762,
title = {Knowledge-enhanced meta-prompt for few-shot relation extraction},
journal = {Computer Speech & Language},
volume = {91},
pages = {101762},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101762},
url = {https://www.sciencedirect.com/science/article/pii/S088523082400144X},
author = {Jinman Cui and Fu Xu and Xinyang Wang and Yakun Li and Xiaolong Qu and Lei Yao and Dongmei Li},
keywords = {Prompt-tuning, Meta-learning, Relation extraction, Few-shot learning},
abstract = {Few-shot relation extraction (RE) aims to identity and extract the relation between head and tail entities in a given context by utilizing a few annotated instances. Recent studies have shown that prompt-tuning models can improve the performance of few-shot learning by bridging the gap between pre-training and downstream tasks. The core idea of prompt-tuning is to leverage prompt templates to wrap the original input text into a cloze question and map the output words to corresponding labels via a language verbalizer for predictions. However, designing an appropriate prompt template and language verbalizer for RE task is cumbersome and time-consuming. Furthermore, the rich prior knowledge and semantic information contained in the relations are easily ignored, which can be used to construct prompts. To address these issues, we propose a novel Knowledge-enhanced Meta-Prompt (Know-MP) framework, which can improve meta-learning capabilities by introducing external knowledge to construct prompts. Specifically, we first inject the entity types of head and tail entities to construct prompt templates, thereby encoding the prior knowledge contained in the relations into prompt-tuning. Then, we expand rich label words for each relation type from their relation name to construct a knowledge-enhanced soft verbalizer. Finally, we adopt the meta-learning algorithm based on the attention mechanisms to mitigate the impact of noisy data on few-shot RE to accurately predict the relation of query instances and optimize the parameters of meta-learner. Experiments on FewRel 1.0 and FewRel 2.0, two benchmark datasets of few-shot RE, demonstrate the effectiveness of Know-MP.}
}
@article{BOSCH2023100950,
title = {Elucidating the clinical and molecular spectrum of SMARCC2-associated NDD in a cohort of 65 affected individuals},
journal = {Genetics in Medicine},
volume = {25},
number = {11},
pages = {100950},
year = {2023},
issn = {1098-3600},
doi = {https://doi.org/10.1016/j.gim.2023.100950},
url = {https://www.sciencedirect.com/science/article/pii/S1098360023009632},
author = {Elisabeth Bosch and Bernt Popp and Esther Güse and Cindy Skinner and Pleuntje J. {van der Sluijs} and Isabelle Maystadt and Anna Maria Pinto and Alessandra Renieri and Lucia Pia Bruno and Stefania Granata and Carlo Marcelis and Özlem Baysal and Dewi Hartwich and Laura Holthöfer and Bertrand Isidor and Benjamin Cogne and Dagmar Wieczorek and Valeria Capra and Marcello Scala and Patrizia {De Marco} and Marzia Ognibene and Rami Abou Jamra and Konrad Platzer and Lauren B. Carter and Outi Kuismin and Arie {van Haeringen} and Reza Maroofian and Irene Valenzuela and Ivon Cuscó and Julian A. Martinez-Agosto and Ahna M. Rabani and Heather C. Mefford and Elaine M. Pereira and Charlotte Close and Kwame Anyane-Yeboa and Mallory Wagner and Mark C. Hannibal and Pia Zacher and Isabelle Thiffault and Gea Beunders and Muhammad Umair and Priya T. Bhola and Erin McGinnis and John Millichap and Jiddeke M. {van de Kamp} and Eloise J. Prijoles and Amy Dobson and Amelle Shillington and Brett H. Graham and Evan-Jacob Garcia and Maureen Kelly Galindo and Fabienne G. Ropers and Esther A.R. Nibbeling and Gail Hubbard and Catherine Karimov and Guido Goj and Renee Bend and Julie Rath and Michelle M. Morrow and Francisca Millan and Vincenzo Salpietro and Annalaura Torella and Vincenzo Nigro and Mitja Kurki and Roger E. Stevenson and Gijs W.E. Santen and Markus Zweier and Philippe M. Campeau and Mariasavina Severino and André Reis and Andrea Accogli and Georgia Vasileiou},
keywords = {BAF, BAFopathy, Coffin-Siris syndrome, NDD, SMARCC2},
abstract = {Purpose
Coffin-Siris and Nicolaides-Baraitser syndromes are recognizable neurodevelopmental disorders caused by germline variants in BAF complex subunits. The SMARCC2 BAFopathy was recently reported. Herein, we present clinical and molecular data on a large cohort.
Methods
Clinical symptoms for 41 novel and 24 previously published affected individuals were analyzed using the Human Phenotype Ontology. For genotype-phenotype correlations, molecular data were standardized and grouped into non-truncating and likely gene-disrupting (LGD) variants. Missense variant protein expression and BAF-subunit interactions were examined using 3D protein modeling, co-immunoprecipitation, and proximity-ligation assays.
Results
Neurodevelopmental delay with intellectual disability, muscular hypotonia, and behavioral disorders were the major manifestations. Clinical hallmarks of BAFopathies were rare. Clinical presentation differed significantly, with LGD variants being predominantly inherited and associated with mildly reduced or normal cognitive development, whereas non-truncating variants were mostly de novo and presented with severe developmental delay. These distinct manifestations and non-truncating variant clustering in functional domains suggest different pathomechanisms. In vitro testing showed decreased protein expression for N-terminal missense variants similar to LGD.
Conclusion
This study improved SMARCC2 variant classification and identified discernible SMARCC2-associated phenotypes for LGD and non-truncating variants, which were distinct from other BAFopathies. The pathomechanism of most non-truncating variants has yet to be investigated.}
}
@article{WU2020103786,
title = {A systematic review of educational digital storytelling},
journal = {Computers & Education},
volume = {147},
pages = {103786},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2019.103786},
url = {https://www.sciencedirect.com/science/article/pii/S0360131519303367},
author = {Jing Wu and Der-Thanq Victor Chen},
keywords = {Digital storytelling, Education},
abstract = {Although Digital Storytelling (DS) has been popular in education since the 1990s, there is a lack of systematic review to inform how it has been applied and what has been achieved in the field. Most publications are either conceptual discussions or reports of implementations. This paper presents a systematic review of 57 studies on educational DS (EDS). Our analysis identified a continuous interest within the USA, where EDS originated, and increasing adoption in Asian and European countries, with usage across primary, secondary, and higher education levels. It was used as stand-alone pedagogy or in combination with other pedagogies, usually in the humanities and social science contexts. The review categorizes five orientations (appropriative, agentive, reflective, reconstructive, and reflexive) and eight types of outcomes (affective, cognitive, conceptual, academic, technological, linguistic, ontological, and social). Based on this synthesis, we discuss three issues: a linguistic conundrum, a rosy picture of positive outcomes, and a further expansion of the optimal reconstructive orientation. Possible reasons for these issues are explored, and directions for future studies are suggested.}
}
@article{SUN2024108505,
title = {Nodule-CLIP: Lung nodule classification based on multi-modal contrastive learning},
journal = {Computers in Biology and Medicine},
volume = {175},
pages = {108505},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108505},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524005894},
author = {Lijing Sun and Mengyi Zhang and Yu Lu and Wenjun Zhu and Yang Yi and Fei Yan},
keywords = {Classification of lung nodules, Contrastive learning, Complex attributes of lung nodules, Feature alignments},
abstract = {The latest developments in deep learning have demonstrated the importance of CT medical imaging for the classification of pulmonary nodules. However, challenges remain in fully leveraging the relevant medical annotations of pulmonary nodules and distinguishing between the benign and malignant labels of adjacent nodules. Therefore, this paper proposes the Nodule-CLIP model, which deeply mines the potential relationship between CT images, complex attributes of lung nodules, and benign and malignant attributes of lung nodules through a comparative learning method, and optimizes the model in the image feature extraction network by using its similarities and differences to improve its ability to distinguish similar lung nodules. Firstly, we segment the 3D lung nodule information by U-Net to reduce the interference caused by the background of lung nodules and focus on the lung nodule images. Secondly, the image features, class features, and complex attribute features are aligned by contrastive learning and loss function in Nodule-CLIP to achieve lung nodule image optimization and improve classification ability. A series of testing and ablation experiments were conducted on the public dataset LIDC-IDRI, and the final benign and malignant classification rate was 90.6%, and the recall rate was 92.81%. The experimental results show the advantages of this method in terms of lung nodule classification as well as interpretability.}
}
@article{KOSZTYAN2025111039,
title = {Automated research methodology classification using machine learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {156},
pages = {111039},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111039},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625010395},
author = {Zsolt T. Kosztyán and Tünde Király and Tibor Csizmadia and Attila Imre Katona and Ágnes Vathy-Fogarassy},
keywords = {Machine learning, Classification, Applied research methods, Extreme Gradient Boosting, Large language models},
abstract = {Scientific papers have become the primary means for disseminating scientific research, and thus, the ability to classify research papers based on different aspects has become essential. Therefore, many works have developed classification approaches; however, they focused solely on research topic-based classification. In addition, no solution has been developed to classify papers based on the applied methodology, and finally, the accuracy of the existing paper classification methods is not satisfactory. In this study, a novel automated classification methodology using a refined Extreme Gradient boosting (XGBoost) model is presented to classify the research methods employed in scientific papers. Three article sets, including quantitative and qualitative research methods, were collected from the topics of tourism, medical science and information systems, consisting of 229, 557 and 787 papers, respectively. The classification problem was considered a binary classification task to maintain interpretability. The developed model was trained and tested on article set 1 (tourism) and 2 (medical science), and then, the proposed model was applied to article set 3, (information systems and tourism). The high accuracy achieved in different research fields (90%–95% accuracies on average) indicates that the proposed classification model is generalizable because it can be successfully applied in many disciplines. The automated classifier enables the rapid acquisition of vital information and the identification of significant differences among the applied methodologies in various research domains. A future development direction will be to increase the scalability of the proposed model to achieve efficient operations on large volumes of research papers.}
}
@article{KARVONEN2023101166,
title = {Fundamental concepts of cognitive mimetics},
journal = {Cognitive Systems Research},
volume = {82},
pages = {101166},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101166},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001006},
author = {Antero Karvonen and Tuomo Kujala and Tommi Kärkkäinen and Pertti Saariluoma},
keywords = {AI design, Cognitive mimetics, Design, Artificial intelligence},
abstract = {The rapid development and widespread adoption of Artificial Intelligence (AI) technologies have made the development of AI-specific design methods an important topic to advance. In recent decades, the centre of gravity in AI has shifted away from cognitive science and related fields like psychology. However, there is a clear need and potential for added value in returning to stronger interaction. One potential challenge for this interaction may be the lack of common conceptual grounds and design languages. In this article, we aim to contribute to the development of conceptual interfaces for human-based AI-specific design methods through the idea of cognitive mimetics. We begin by introducing basic concepts from mimetic design and interpret them in the context of this thematic area. These provide some of the basic building blocks for a design language and bring to the surface key questions. These in turn provide a ground for explicating cognitive mimetics. In the second part of this paper, we focus on specifying a key aspect in cognitive mimetics: the contents of information processes. Others engaged in this field can derive value from using or developing the basic conceptual machinery to specify their own approaches in this interdisciplinary field that is still shaping itself. Furthermore, those who resonate with the idea of cognitive mimetics, as specified here, can join in taking this particular approach further.}
}
@article{WANG2024106007,
title = {Research on conceptual graph gallery-based cognitive communication method for geographical conceptual modeling},
journal = {Environmental Modelling & Software},
volume = {176},
pages = {106007},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224000689},
author = {Jin Wang and Yuchen Lu and Xiangyun Kong and Yongning Wen and Songshan Yue and Guonian Lü and Zaiyang Ma},
keywords = {Geographic conceptual modeling, Geographic concept, Conceptual graph gallery, Cognitive communication},
abstract = {Geographic modeling is considered as an effective way to solve geographic problems. Geographic conceptual modeling is the first step of geographic modeling, in which modelers can fully exchange modeling ideas, thereby achieving a common understanding of geographic system and guiding subsequent geographic modeling process. However, due to the diverse research backgrounds of modelers, it is challenging for them to exchange modeling ideas with one another. This paper provides a visual cognitive communication method for modelers by constructing a conceptual graph gallery, and then improves the efficiency of geographic conceptual modeling. A multidimensional description method that describes modeling cognition from multiple perspectives, the conceptual graph gallery that includes concept items and conceptual graphs, and the conceptual graph gallery-based cognitive communication methods are designed in this paper. Finally, a case study involving the construction of a hydrological conceptual graph gallery is designed to illustrate the feasibility of conceptual graph gallery-based cognitive communication.}
}
@article{ABEDIAN2025105686,
title = {Application of the openEHR reference model for PGHD: A case study on the DH-Convener initiative},
journal = {International Journal of Medical Informatics},
volume = {193},
pages = {105686},
year = {2025},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105686},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624003496},
author = {Somayeh Abedian and Sten Hanke and Rada Hussein},
keywords = {Electronic Health Record (EHR), Healthcare interoperability standards, Patient-Generated Health Data (PGHD), Health Data Integration, Wearable devices},
abstract = {Objectives
Patient-Generated Health Data (PGHD) is increasingly influential in therapy and diagnostic decisions. PGHD should be integrated into electronic health records (EHR) to maximize its utility. This study evaluates the openEHR Reference Model (RM) compatibility with the DH-Convener initiative’s modules (Data Collection Module and Data Connector Module) as a potential concept for standardizing PGHD across wearable health devices, focusing on achieving interoperability.
Materials and Methods
The study analyzes various types of PGHD, assessing the data formats and structures used by wearable tools. We evaluate openEHR RM specification with our initiative, DH-Convenor, focusing on PGHD semantic interoperability challenges. We evaluated current Archetypes and Templates that are now created and exist on openEHR Clinical Knowledge Management (CKM) and mapped them to our requirements. The DH-Convener modules are examined for their compatibility in standardizing PGHD integration into openEHR clinical workflows and compared with other existing standards for flexibility, scalability, and interoperability.
Results
The findings indicate that the diversity in data formats across wearable tools and openEHR shows strong potential as unifying data models based on the DH-Convener’s modules. It supports a wide range of PGHD types in existing archetypes and aligns well with our initiative’s requirements for storing PGHD, enabling more seamless integration into EHR systems.
Discussion
Integrating PGHD into EHR is crucial for personalized healthcare, but inconsistent device formats hinder interoperability. The DH-Convener leverages openEHR to provide a strong solution, though stakeholder collaboration remains essential. Our initiative demonstrates openEHR’s ability to ensure consistency, particularly in Europe.
Conclusion
We aligned the openEHR layers with the DH-Convener modules, demonstrating openEHR’s compatibility for storing PGHD and supporting interoperability goals, such as standardized storage and seamless data transfer to Austria’s national EHR. Future efforts should prioritize promoting these models and ensuring their adaptability to emerging wearable devices.}
}
@article{BELLINI2023100834,
title = {Managing complexity of data models and performance in broker-based Internet/Web of Things architectures},
journal = {Internet of Things},
volume = {23},
pages = {100834},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100834},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523001579},
author = {Pierfrancesco Bellini and Luciano Alessandro {Ipsaro Palesi} and Alberto Giovannoni and Paolo Nesi},
keywords = {IoT (Internet of Things), Automated IoT device registration, Internal and external IoT brokers, Smart data model, Snap4City, FIWARE},
abstract = {The Internet of Things (IoT) is becoming pervasive and with each new installation of IoT platforms new and legacy brokers have to be exploited. New internal brokers are those under the control of the platform, while legacy external brokers are those in place managed by third parties. The solution proposed addressed problems of (a) interoperability to reduce set up time to cope with unknown data structures (devices, entities) distributed via brokers; (b) performance by dimensioning both front-end and back-end processes to reach high rates in a broker-based platform, while preserving full capability features of the data warehouse. Interoperability aspects have been addressed by introducing our concepts and a reasoner into an IoT Directory tool to manage Internal and External brokers, automate device discovery and registration from both standard and customized data models. Despite the managed complexity, a broker-based solution turned out to provide high performance. To this end, a specific assessment and architecture tuning have been performed and reported in the paper to give evidence and validation. The proposed integrated IoT Directory has been developed in the context of the Herit-Data Project, and it is currently used in the whole Snap4City network of 18 tenants and billions of data. Snap4City is an open-source IoT platform for Smart Cities and Industry 4.0, which is an official FIWARE platform and solution, EOSC service and libs of Node-RED.}
}
@article{SUN2025122451,
title = {TeProM: A rule-free method for extracting process from complex text with enhanced coreference handling},
journal = {Information Sciences},
volume = {719},
pages = {122451},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2025.122451},
url = {https://www.sciencedirect.com/science/article/pii/S0020025525005833},
author = {Xiaoxiao Sun and Chenying Zhao and Dongjin Yu and Yi Xu and Nana Xiao},
keywords = {Business process management, Automatic process extraction, Coreference resolution, Supervised learning},
abstract = {Extracting business process models from textual documents remains a significant challenge in enterprises. Traditional rule-based methods suffer from poor applicability due to customized rule sets while most machine-learning based methods focus on simple process documents. This paper presents Text-based Process Modeling (TeProM), a novel method for extracting business process components and their relations from textual descriptions. By adopting a rule-free design, TeProM departs from traditional rule-based systems and leverages a neural network model to address complex coreference phenomena in text, thereby ensuring the accurate mapping of process components within the model. This approach applies to various types of business process documents, particularly excelling in processing complex textual structures with long-range dependencies. Compared to previous approaches, TeProM is able to effectively address the complex logical structures and coreference issues concealed in business process documents. TeProM achieved the best performance over 10 baselines in multidimensional evaluation. Additionally, evaluations on the PET and SAP-OPC datasets for relation extraction further demonstrated the effectiveness of the proposed method. An annotated dataset consisting of 91 real business process documents is also provided, which serves as a valuable resource for future research.}
}
@article{MENDEZ201989,
title = {A new semantic-based feature selection method for spam filtering},
journal = {Applied Soft Computing},
volume = {76},
pages = {89-104},
year = {2019},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618306963},
author = {José R. Méndez and Tomás R. Cotos-Yañez and David Ruano-Ordás},
keywords = {Feature selection methods, Text mining, Spam filtering, e-mail, Classification, Machine learning},
abstract = {The Internet emerged as a powerful infrastructure for the worldwide communication and interaction of people. Some unethical uses of this technology (for instance spam or viruses) generated challenges in the development of mechanisms to guarantee an affordable and secure experience concerning its usage. This study deals with the massive delivery of unwanted content or advertising campaigns without the accordance of target users (also known as spam). Currently, words (tokens) are selected by using feature selection schemes; they are then used to create feature vectors for training different Machine Learning (ML) approaches. This study introduces a new feature selection method able to take advantage of a semantic ontology to group words into topics and use them to build feature vectors. To this end, we have compared the performance of nine well-known ML approaches in conjunction with (i) Information Gain, the most popular feature selection method in the spam-filtering domain and (ii) Latent Dirichlet Allocation, a generative statistical model that allows sets of observations to be explained by unobserved groups that describe why some parts of the data are similar, and (iii) our semantic-based feature selection proposal. Results have shown the suitability and additional benefits of topic-driven methods to develop and deploy high-performance spam filters.}
}
@article{NJ20254279,
title = {A Novel Automatic Text Document Classification Using Learning based Text Classification(LbTC) Approach},
journal = {Procedia Computer Science},
volume = {258},
pages = {4279-4290},
year = {2025},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.677},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925017818},
author = {Avinash {N J} and Krishnaraj Rao and Rama Moorthy H and Raviprakash B and Raghunadan {K R} and  Vasudeva and Venkatadri M},
keywords = {Machine learning, shallow learning, deep learning, text classification, document Classification},
abstract = {There are practical uses for text document classification. In actuality, it is crucial to categorize and parse documents with natural language. Utilizing applications like bogus news identification, query tagging, sentiment classification, and spam filtering needs this sort of study. However, because of their ambiguity, open-ended nature, and vastness, text documents provide a difficult Classification challenge. Machine learning (ML) methods became valuable alongside the growth of artificial intelligence (AI) due to their data-driven learning techniques. These methods are seen as highly effective at handling and analyzing vast data sets in depth. Topic segmentation, text categorization, entity identification, machine translation, and text summarization, to name a few, are just a few of the issues that may be resolved with ML approaches. In this study, we introduced an automatic Text Classification system (ATCF), a system that uses shallow and deep neural networks to classify text documents. To implement our system, we proposed an approach called Learning based Text Classification (LbTC). We investigated the performance of several models in comparison. Based on the training provided to ML models, the suggested framework assists in categorizing any type of document. It is compatible with practical applications where the categorization of documents is essential.}
}
@article{KRIEGEL2020172,
title = {Most specific consequences in the description logic EL},
journal = {Discrete Applied Mathematics},
volume = {273},
pages = {172-204},
year = {2020},
note = {Advances in Formal Concept Analysis: Traces of CLA 2016},
issn = {0166-218X},
doi = {https://doi.org/10.1016/j.dam.2019.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S0166218X19300642},
author = {Francesco Kriegel},
keywords = {Description logic, Formal concept analysis, Lattice theory, Most specific consequence, Concept inclusion, Terminological axiom, TBox, Ontology, Interpretation, Model, Closure operator, Axiomatization, Error tolerance, Stream, Machine learning, Inductive learning},
abstract = {The notion of a most specific consequence with respect to some terminological box is introduced, conditions for its existence in the description logic EL and its variants are provided, and means for its computation are developed. Algebraic properties of most specific consequences are explored. Furthermore, several applications that make use of this new notion are proposed and, in particular, it is shown how given terminological knowledge can be incorporated in existing approaches for the axiomatization of observations. For instance, a procedure for an incremental learning of concept inclusions from sequences of interpretations is developed.}
}
@article{HAMMAMI2019239,
title = {Towards Agile and Gamified Flipped Learning Design models: Application to the System and Data Integration Course},
journal = {Procedia Computer Science},
volume = {164},
pages = {239-244},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.178},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919322173},
author = {Jihed Hammami and Maha Khemaja},
keywords = {IMS LD, Flipped classroom, Flipped learning, Agile, Gamification, Authoring Tool, Domain Specific Language},
abstract = {Education and learning have no limits. At all ages, people are willing to be engaged in new experiences and get additional knowledge, skills and competencies. In this paper, we propose a new learning model that is learner-centered, gives values to learners’ preferences and competencies and encourages an engaging and motivating learning experience. Therefore, we discuss some of its pedagogical and technical aspects and requirements. We present the flipped classroom/learning, Agile methodology and Gamification as the basis of our proposed approach. We additionally, examine the possible support that the existing authoring tools could provide. We attempt to validate our proposal with a scenario intended to design the System and Data Integration course.}
}
@article{KAZMALI20251563,
title = {Web Scraping: Legal and Ethical Considerations in General and Local Context - A Review},
journal = {Procedia Computer Science},
volume = {259},
pages = {1563-1572},
year = {2025},
note = {Sixth International Conference on Futuristic Trends in Networks and Computing Technologies (FTNCT06), held in Uttarakhand, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.111},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925012128},
author = {Ahmet Sinan Kazmali and Ahmet Sayar},
keywords = {Etichs, Web Scraping, Web Scraping Techniques, Protection},
abstract = {In today’s fast-paced and competitive environment, there is an increasing need for data in a wide variety of different fields such as the finance industry, the manufacturing industry, the artificial intelligence industry, and academic studies. Data can be accessed from a wide variety of databases, whether printed or virtual. Data can be accessed easily and quickly from online sources. Web scraping has become an important tool in the field of data collection in the operation of online sources. Data can be automatically collected from online sources in HyperText Markup Language (HTML), Extensible Markup Language (XML), JavaScript Object Notation (JSON) format with tools coded for web scraping. In order for this study to be a guide for web scraping tool developers, web scraping techniques such as Human Copy Paste, HTML Parsing, API Scraping, XPath have been tried to be explained to developers. On the other hand, Load Balancing, Rate Limiting, IP Blocking, CAPTCHA systems can be used to prevent the negative effects of web scraping on running systems. Data scraping can result in legal or ethical violations of data collected from online sources. Because, it may cause situations such as extraction of personal data, data theft, violation of system policies, blocking of running services, access problems for users. In this research study, the methods of web scraping, methods of preventing scraping, ethical and legal consequences will be discussed as a global and local comparative study and it is aimed to create a guide for web scraper developers in order for researchers, companies and organizations that try to collect data using web scraping to pay attention in ethical and legal terms.}
}
@article{YU2023e15034,
title = {Identification of potential biomarkers and pathways for sepsis using RNA sequencing technology and bioinformatic analysis},
journal = {Heliyon},
volume = {9},
number = {4},
pages = {e15034},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e15034},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023022417},
author = {Rongjie Yu and Yingchen Wang and Qi Liang and Yuzhi Xu and Amina Elmi Yusf and Liqun Sun},
keywords = {Sepsis, Biomarkers, Bioinformatic analysis, Differentially expressed gene (DEG), Hub gene},
abstract = {Long non-coding RNAs (lncRNAs) has been proven by many to play a crucial part in the process of sepsis. To obtain a better understanding of sepsis, the molecular biomarkers associated with it, and its possible pathogenesis, we obtained data from RNA-sequencing analysis using serum from three sepsis patients and three healthy controls (HCs). Using edgeR (one of the Bioconductor software package), we identified 1118 differentially expressed mRNAs (DEmRNAs) and 1394 differentially expressed long noncoding RNAs (DElncRNAs) between sepsis patients and HCs. We identified the biological functions of these disordered genes using Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) signaling pathway analyses. The GO analysis showed that the homophilic cell adhesion via plasma membrane adhesion molecules was the most significantly enriched category. The KEGG signaling pathway analysis indicated that the differentially expressed genes (DEGs) were most significantly enriched in retrograde endocannabinoid signaling. Using STRING, a protein-protein interaction network was also created, and Cytohubba was used to determine the top 10 hub genes. To examine the relationship between the hub genes and sepsis, we examined three datasets relevant to sepsis that were found in the gene expression omnibus (GEO) database. PTEN and HIST2H2BE were recognized as hub gene in both GSE4607, GSE26378, and GSE9692 datasets. The receiver operating characteristic (ROC) curves indicate that PTEN and HIST2H2BE have good diagnostic value for sepsis. In conclusion, this two hub genes may be biomarkers for the early diagnosis of sepsis, our findings should deepen our understanding of the pathogenesis of sepsis.}
}
@article{GALER2020683,
title = {Semantic Similarity Analysis Reveals Robust Gene-Disease Relationships in Developmental and Epileptic Encephalopathies},
journal = {The American Journal of Human Genetics},
volume = {107},
number = {4},
pages = {683-697},
year = {2020},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2020.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0002929720302718},
author = {Peter D. Galer and Shiva Ganesan and David Lewis-Smith and Sarah E. McKeown and Manuela Pendziwiat and Katherine L. Helbig and Colin A. Ellis and Annika Rademacher and Lacey Smith and Annapurna Poduri and Simone Seiffert and Sarah {von Spiczak} and Hiltrud Muhle and Andreas {van Baalen} and Rhys H. Thomas and Roland Krause and Yvonne Weber and Ingo Helbig},
keywords = {electronic medical records, Human Phenotype Ontology, childhood epilepsies, neurogenetic disorders, developmental and epileptic encephalopathies, whole-exome sequencing, computational phenotypes},
abstract = {Summary
More than 100 genetic etiologies have been identified in developmental and epileptic encephalopathies (DEEs), but correlating genetic findings with clinical features at scale has remained a hurdle because of a lack of frameworks for analyzing heterogenous clinical data. Here, we analyzed 31,742 Human Phenotype Ontology (HPO) terms in 846 individuals with existing whole-exome trio data and assessed associated clinical features and phenotypic relatedness by using HPO-based semantic similarity analysis for individuals with de novo variants in the same gene. Gene-specific phenotypic signatures included associations of SCN1A with “complex febrile seizures” (HP: 0011172; p = 2.1 × 10−5) and “focal clonic seizures” (HP: 0002266; p = 8.9 × 10−6), STXBP1 with “absent speech” (HP: 0001344; p = 1.3 × 10−11), and SLC6A1 with “EEG with generalized slow activity” (HP: 0010845; p = 0.018). Of 41 genes with de novo variants in two or more individuals, 11 genes showed significant phenotypic similarity, including SCN1A (n = 16, p < 0.0001), STXBP1 (n = 14, p = 0.0021), and KCNB1 (n = 6, p = 0.011). Including genetic and phenotypic data of control subjects increased phenotypic similarity for all genetic etiologies, whereas the probability of observing de novo variants decreased, emphasizing the conceptual differences between semantic similarity analysis and approaches based on the expected number of de novo events. We demonstrate that HPO-based phenotype analysis captures unique profiles for distinct genetic etiologies, reflecting the breadth of the phenotypic spectrum in genetic epilepsies. Semantic similarity can be used to generate statistical evidence for disease causation analogous to the traditional approach of primarily defining disease entities through similar clinical features.}
}
@article{BILIDAS2021100639,
title = {Handling redundant processing in OBDA query execution over relational sources},
journal = {Journal of Web Semantics},
volume = {68},
pages = {100639},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100639},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000147},
author = {Dimitris Bilidas and Manolis Koubarakis},
keywords = {Query translation, Data integration, Ontology-based data access, Ontop},
abstract = {Redundant processing is a key problem in the translation of initial queries posed over an ontology into SQL queries, through mappings, as it is performed by ontology-based data access systems. Examples of such processing are duplicate answers obtained during query evaluation, which must finally be discarded, or common expressions evaluated multiple times from different parts of the same complex query. Many optimizations that aim to minimize this problem have been proposed and implemented, mostly based on semantic query optimization techniques, by exploiting ontological axioms and constraints defined in the database schema. However, data operations that introduce redundant processing are still generated in many practical settings, and this is a factor that impacts query execution. In this work we propose a cost-based method for query translation, which starts from an initial result and uses information about redundant processing in order to come up with an equivalent, more efficient translation. The method operates in a number of steps, by relying on certain heuristics indicating that we obtain a more efficient query in each step. Through experimental evaluation using the Ontop system for ontology-based data access, we exhibit the benefits of our method.}
}
@article{JANFADA201921,
title = {Becoming Blind},
journal = {Video Journal of Education and Pedagogy},
volume = {4},
number = {2},
pages = {21-59},
year = {2019},
issn = {2364-4583},
doi = {https://doi.org/10.1163/23644583-00401017},
url = {https://www.sciencedirect.com/science/article/pii/S2364458323000083},
author = {Mahtab Janfada},
keywords = {ontology, epistemology, integrated learning, dialogic pedagogy, critical literacy},
abstract = {Medicine has always been regarded as one of the most significant disciplines, grounded in a humanistic approach, due to its ultimate exposure and connection with people as ‘patients’ and hence a holistic understanding of the patient as ‘human’ is fundamental. In our potentially dangerous times, the instrumental, technical and fragmented ways of seeing knowledge tend to permeate most disciplines, including medicine. This may result in individuals becoming alienated with the ‘self’ as potential doctors, with the discipline and with patients through the monologic discourse of academia or clinics. This article examines this (in)visible global issue in the specific context of Iran, where bilingual medical education adds another level of complexity in dialogic ‘seeing’ of self, knowledge and patients. Grounded in Bakhtin’s theory of dialogue and critical literacy approach to language and literacy, this article explores the affordances of a pedagogical intervention at an Iranian university. This offers diverse avenues for constructing a holistic medical knowledge in the process of becoming a professional through narrative medicine, clinical scenarios, evidence-based medicine and personal experiences. Selected stories of participants’ ontological and epistemological transformations, in their process of ideological becoming, are offered to argue for the urgency of dialogic ways of ‘seeing’ in potentially dangerous times.}
}
@article{FIGUEIRA2025100752,
title = {Nesting the multi-level perspective and social-ecological systems frameworks for green taxonomy implementation: A conceptual development with a case study of Portuguese rice farmers},
journal = {Environmental and Sustainability Indicators},
volume = {27},
pages = {100752},
year = {2025},
issn = {2665-9727},
doi = {https://doi.org/10.1016/j.indic.2025.100752},
url = {https://www.sciencedirect.com/science/article/pii/S2665972725001734},
author = {Márcia Figueira and António Guerreiro de Brito and Isabel de Sousa},
keywords = {Social-ecological systems, Multi-level perspective of socio-technical transitions, Agriculture transitions, Green taxonomy, EU taxonomy, Rice},
abstract = {This paper introduces the Multi-Level Social-Ecological-Technical Systems (ML-SETS) framework to operationalize sustainability indicators from green finance tools, using the EU Environmental Taxonomy as a focal case. ML-SETS integrates the Social-Ecological Systems (SES) and Multi-Level Perspective (MLP) frameworks to bridge micro-level resource governance with macro-level institutional and technological change. A case study of 12 rice farmers in Portugal's Sado Delta illustrates the application of ML-SETS by analyzing nitrogen use efficiency and surplus under Option B of the Taxonomy. Results reveal substantial intra-group variability and a critical dependency of indicators on yield, which can distort compliance in years of climatic stress. The framework's analytical core highlights the importance of evaluating indicators within feedback-rich systems and opens pathways for alternative compliance routes, such as biodiversity-oriented options. ML-SETS provides a scalable approach for assessing green finance tools in dynamic and situated transition contexts.}
}
@incollection{TIWARI20221,
title = {Chapter 1 - Semantic modeling for healthcare applications: an introduction},
editor = {Sanju Tiwari and Fernando {Ortiz Rodriguez} and M.A. Jabbar},
booktitle = {Semantic Models in IoT and eHealth Applications},
publisher = {Academic Press},
pages = {1-17},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-91773-5},
doi = {https://doi.org/10.1016/B978-0-32-391773-5.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917735000078},
author = {Sanju Tiwari and Fernando Ortiz-Rodriguez and M.A. Jabbar},
keywords = {Semantic modeling, IoT, Semantic annotation, Semantic linking, Semantic representation},
abstract = {Healthcare is a sensitive domain for human beings and needs extra care to monitor patients and elderly people. Massive exploitation of associated devices is the primary source for collecting health data during the monitoring of patients. All connected devices such as sensors, actuators, and RFIDs produce challenging data simultaneously and sometimes lack consistency and provide incorrect information. Recently, semantic web technologies have been involved in overcoming these issues and provide a semantic and syntactic representation of data that are interpreted by machines and humans. Ontologies are presented as a backbone of the semantic web and exhibited in semantic models with explicit representation of health data and connected devices. The primary aim of semantic modeling of healthcare data is to facilitate its domain's reusability and generalized representation. In this chapter, a brief introduction of healthcare semantic models has been discussed in general-purpose and domain-specific IoT-based semantic models. It also covers semantic modeling phases such as semantic annotation, semantic linking, the construction process, and semantic representation. Various case studies and examples include all phases.}
}
@article{HUANG2025113770,
title = {ES-MRE: Evidence subgraph enhanced reasoning for multimodal relation extraction},
journal = {Knowledge-Based Systems},
volume = {325},
pages = {113770},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113770},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125008160},
author = {Wenti Huang and Jiayi Chen and Junjie Li and Yiyu Mao and Ningyi Mao},
keywords = {Multimodal reasoning, Relation extraction, Knowledge graph},
abstract = {The aim of multimodal relation extraction is to identify the semantic relations between two named entities by analyzing linguistic sequences and associated images. However, existing approaches are frequently limited due to inadequate information, which is often attributable to the brevity and ambiguity of the texts. Moreover, these models are easily affected by irrelevant objects during inter-modal alignment, leading to a problem known as error sensitivity. In this paper, we propose the Evidence Subgraph Enhanced Reasoning for Multimodal Relation Extraction (ES-MRE) framework. Specifically, the MLLM-guided Evidence Subgraph Generation (MESG) module is introduced, which leverages the image understanding capabilities of Multimodal Large Language Models (MLLMs) to extract visual and textual entities from image descriptions and original texts. It retrieves the clue paths of different entities in existing Knowledge Graphs (KGs), generating an Evidence Subgraph to provide structural features of inter-entity factual knowledge for relation reasoning. Additionally, we introduce a Multi-image Hierarchical Fusion (MHF) module that treats the generated image as a back-translation of the text, hierarchically fine-grained aligned text and generated images as well as the original image to mitigate the impact of irrelevant objects. Experimental results using the most popular dataset demonstrate the effectiveness of our approach, and ablation studies further validate the contributions of the MESG and MHF modules.}
}
@article{ZHOU2023100757,
title = {Feedback seeking by first-year Chinese international students: Understanding practices and challenges},
journal = {Assessing Writing},
volume = {57},
pages = {100757},
year = {2023},
issn = {1075-2935},
doi = {https://doi.org/10.1016/j.asw.2023.100757},
url = {https://www.sciencedirect.com/science/article/pii/S107529352300065X},
author = {Jiming Zhou and Chris Deneen and Joanna Tai and Phillip Dawson},
keywords = {Feedback seeking, Writing assessment, Chinese international students, Student feedback literacy},
abstract = {Feedback seeking is an emerging focus in higher education and writing assessment research. Feedback may be sought directly from others or through observing cues in learning contexts. Existing research suggests feedback seeking may be essential to student feedback literacy, but few studies explore this issue in relation to specific high-need populations. This study examined Chinese international students’ practices and perceptions of feedback seeking in writing assessments during their first years at an Australian university. Thirty-seven participants from three faculties participated in 14 individual interviews and seven focus groups. Data were analysed using thematic analysis protocols. Encountered challenges were grouped into epistemological, ontological, and practical dimensions. Findings indicate that students drew upon various sources for inquiry and monitoring at the stages of planning and preparation, revising before submission, and reacting after getting results. Implications include the importance of creating low-threshold opportunities for facilitating learners’ feedback seeking. Findings also suggest that educators’ enhanced understanding of students' educational identity and epistemological beliefs may improve students’ feedback-seeking practices.}
}
@article{PEREZPEREZ2021102131,
title = {A framework to extract biomedical knowledge from gluten-related tweets: The case of dietary concerns in digital era},
journal = {Artificial Intelligence in Medicine},
volume = {118},
pages = {102131},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102131},
url = {https://www.sciencedirect.com/science/article/pii/S093336572100124X},
author = {Martín Pérez-Pérez and Gilberto Igrejas and Florentino Fdez-Riverola and Anália Lourenço},
keywords = {Social media, Sociome profiling, Text mining, Graph mining, Machine learning, Health for informatics},
abstract = {Big data importance and potential are becoming more and more relevant nowadays, enhanced by the explosive growth of information volume that is being generated on the Internet in the last years. In this sense, many experts agree that social media networks are one of the internet areas with higher growth in recent years and one of the fields that are expected to have a more significant increment in the coming years. Similarly, social media sites are quickly becoming one of the most popular platforms to discuss health issues and exchange social support with others. In this context, this work presents a new methodology to process, classify, visualise and analyse the big data knowledge produced by the sociome on social media platforms. This work proposes a methodology that combines natural language processing techniques, ontology-based named entity recognition methods, machine learning algorithms and graph mining techniques to: (i) reduce the irrelevant messages by identifying and focusing the analysis only on individuals and patient experiences from the public discussion; (ii) reduce the lexical noise produced by the different ways in how users express themselves through the use of domain ontologies; (iii) infer the demographic data of the individuals through the combined analysis of textual, geographical and visual profile information; (iv) perform a community detection and evaluate the health topic study combining the semantic processing of the public discourse with knowledge graph representation techniques; and (v) gain information about the shared resources combining the social media statistics with the semantical analysis of the web contents. The practical relevance of the proposed methodology has been proven in the study of 1.1 million unique messages from >400,000 distinct users related to one of the most popular dietary fads that evolve into a multibillion-dollar industry, i.e., gluten-free food. Besides, this work analysed one of the least research fields studied on Twitter concerning public health (i.e., the allergies or immunology diseases as celiac disease), discovering a wide range of health-related conclusions.}
}
@article{RABBI2024105443,
title = {AI integration in construction safety: Current state, challenges, and future opportunities in text, vision, and audio based applications},
journal = {Automation in Construction},
volume = {164},
pages = {105443},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105443},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524001791},
author = {Ahmed Bin Kabir Rabbi and Idris Jeelani},
keywords = {Artificial intelligence, Construction safety, Safety management, Automated safety},
abstract = {High occupational injury and fatality rate in the construction industry is a serious global concern. Recognizing AI as a solution to enhance safety performance, this study reviews 153 papers to assess and categorize current AI applications in construction, focusing on text, visual, and audio data, while also identifying challenges and future research opportunities. Real-time monitoring, hazard detection, and information extraction are identified as key areas where AI is applied, with a notable reliance on deep neural networks, object recognition, and Natural Language Processing. The review highlights major challenges, including the need for high-quality data management, semantic feature representation, and occluded object detection. Additionally, it underscores the untapped potential of audio-based AI and the advancements possible with Large Language Models for text interpretation. The findings emphasize the need for integrated, multi-faceted AI systems and advocate for responsible AI deployment to mitigate safety risks on construction sites.}
}
@article{ACHICH20201141,
title = {Approach to Reasoning about Uncertain Temporal Data in OWL 2},
journal = {Procedia Computer Science},
volume = {176},
pages = {1141-1150},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.110},
url = {https://www.sciencedirect.com/science/article/pii/S187705092032010X},
author = {Nassira Achich and Fatma Ghorbel and Fayçal Hamdi and Elisabeth Métais and Faiez Gargouri},
keywords = {Certain Temporal Data, Uncertain Temporal Data, Temporal Representation, Temporal Reasoning, 4D-Fluents Approach, Allen’s Interval Algebra, Certain Ontology},
abstract = {In this paper, we propose an ontology-based approach for representing and reasoning about certain and uncertain temporal data. It handles temporal data in terms of quantitative time intervals and points and the qualitative relations between them (e.g., “before”). It includes three parts. (1) We extend the 4D-fluents approach with certain ontological components to represent the handled temporal data in OWL 2. (2) We extend the Allen’s interval algebra to reason about certain and uncertain time intervals. We adapt these relations to allow relating a time interval and a time point, and two time points. All relations can be used for temporal reasoning by means of transitivity tables. (3) The extended Allen’s algebra instantiates the 4D-fluents-based representation. Inferences are based on SWRL rules. Based on this ontology, a prototype is implemented and integrated into an ontology-based memory prosthesis for Alzheimer’s patients to handle certain and uncertain temporal data inputs.}
}
@article{SARDAO2024103412,
title = {From the universal to the plural: Imagining, building up and obliterating alternative futures},
journal = {Futures},
volume = {161},
pages = {103412},
year = {2024},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2024.103412},
url = {https://www.sciencedirect.com/science/article/pii/S0016328724000958},
author = {Maíra Sardão and Pedro Gabriel Silva},
keywords = {Desirable futures, Political ecology, Libidinal capitalism, Social transformation, Utopia, Waste humans},
abstract = {In the face of abundant signs that we are on the brink of ecological collapse, exploring alternatives to production and consumption based on wasteful socio-economic relations is imperative. This article aims to identify and discuss alternative ways of acting in the present whilst envisioning different conceptions of futures beyond those engendered within capitalism. We look at the consolidation process of an idea of a singular universal way of existing and how it hinders different worldviews and conceptions of futures. The role of power structures and agencies is also observed, considering how different theoretical traditions deal with vulnerabilities and uncertainties while imagining and building alternative futures. The analysis draws on a conceptual model in which utopia, desire and hope emerge as symbolic resources that encourage social transformation. Based on the literature, experiences from different geographical locations illustrate the role of utopia, desire, and hope in building alternatives that move away from, when not resisting, hegemonic models of living. Despite the diversity of experiences, a common feature binds them: an inclination towards commoning. This stance is crucial to reimagine future horizons capable of challenging and deconstructing hegemonic worldviews.}
}
@article{NORTON202150,
title = {Suppressing spacetime emergence},
journal = {Studies in History and Philosophy of Science},
volume = {88},
pages = {50-59},
year = {2021},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2021.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0039368121000534},
author = {Joshua Norton},
keywords = {Spacetime, Loop quantum gravity, Emergence, Scale-dependent ontology, Perspectivism},
abstract = {One of the primary tasks in building a quantum theory of gravity is discovering how to save spatiotemporal phenomena using a theory which, putatively, does not include spacetime. Some have taken this task a step further and argue for the actual emergence of spacetime from a non-spatiotemporal ontology in the low-energy regime. In this paper, it is argued that the account of spacetime emergence presented in Huggett and Wüthrich(2013) and then assumed in Baron (2019), Crowther (2016), Wüthrich (2017), and Wüthrich and Lam (2018) fails to accomplish the task to which it is set. There is a prima facie contradiction between the scale-independent ontology of spacetime in GR and the scale-dependent account of emergence proposed by this literature. One can avoid this contradiction but only at the cost of changing the target of emergence and by endorsing a perspectival theory of ontology – a view I call “ontic-perspectivism”. Though this paper explicitly addresses spacetime emergence, many of the following arguments are applicable to other accounts where objects of ontology, or their properties, are claimed to emerge in the low-energy regime.}
}
@article{KANDHARE2025100375,
title = {Artificial intelligence in pharmaceutical sciences: A comprehensive review},
journal = {Medicine in Novel Technology and Devices},
volume = {27},
pages = {100375},
year = {2025},
issn = {2590-0935},
doi = {https://doi.org/10.1016/j.medntd.2025.100375},
url = {https://www.sciencedirect.com/science/article/pii/S2590093525000268},
author = {Priyanka Kandhare and Mrunal Kurlekar and Tanvi Deshpande and Atmaram Pawar},
keywords = {Artificial intelligence, Molecular modeling, Pharmacovigilance, Deep learning, Cheminformatics, Drug discovery},
abstract = {The integration of artificial intelligence (AI) and machine learning (ML) into pharmaceutical sciences has catalyzed transformative advancements across drug discovery, clinical development, manufacturing, and post-market surveillance. This review comprehensively examines AI's role in modern pharmacotherapy, beginning with its historical evolution in life sciences and progressing to cutting-edge applications such as AlphaFold-driven protein modeling, natural language processing (NLP) for biomedical literature mining, and AI-augmented pharmacovigilance. Methodologically, we synthesize interdisciplinary insights from peer-reviewed literature (2013–2023), highlighting innovations in cheminformatics (e.g., QSAR, RDKit), predictive toxicology, and personalized medicine. Case studies illustrate AI's capacity to compress drug development timelines, as seen in COVID-19 repurposing efforts and de novo kinase inhibitor design. However, challenges persist, including algorithmic bias, regulatory ambiguities, and the “black-box” nature of deep learning models. By critically evaluating successes and limitations, this review underscores AI's potential to redefine pharmaceutical innovation while advocating for robust frameworks to ensure ethical, transparent, and clinically translatable AI deployment.}
}
@article{DENG202584,
title = {An emerging paradigm for scientific decision: the AI evaluation of space science projects},
journal = {Life Sciences in Space Research},
volume = {47},
pages = {84-94},
year = {2025},
issn = {2214-5524},
doi = {https://doi.org/10.1016/j.lssr.2025.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214552425000756},
author = {Qiwen Deng and Yanzhong Wen and Chaosen Liu and Xinyang Yue and Jianfei Sun and Yuexia Han},
keywords = {Project evaluation, Space life science, Knowledge graph, Artificial Intelligence},
abstract = {In the past six decades, the progress of spaceflight projects has won the admiration of the whole world. However, how to evaluate the values of research projects remains an esoteric and cost effective question. To improve the selections in space science projects, we utilized AI tools to provide an overall framework for broader audience. Our work conducted a three-phased study. We explored space life science research as it is one of the most intensively researched areas in space science. We learned the domain science data and constructed a space science knowledge graph. Subsequently, to better extract semantic features, we introduced SpaceBERT, a pre-trained language model fine-tuned with contrastive learning. We then developed SpaceGL, a deep learning framework tailored for predicting frontier research. Lastly, we prioritized candidate space experimental projects based on AI model and compared with the real results from the science panel judges and the “Lottery model.”}
}
@article{OYELADE2020100395,
title = {A case-based reasoning framework for early detection and diagnosis of novel coronavirus},
journal = {Informatics in Medicine Unlocked},
volume = {20},
pages = {100395},
year = {2020},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2020.100395},
url = {https://www.sciencedirect.com/science/article/pii/S2352914820303683},
author = {Olaide N. Oyelade and Absalom E. Ezugwu},
keywords = {COVID-19, Coronavirus, Case-based reasoning, Ontology, Natural language processing},
abstract = {Coronavirus, also known as COVID-19, has been declared a pandemic by the World Health Organization (WHO). At the time of conducting this study, it had recorded over 11,301,850 confirmed cases while more than 531,806 have died due to it, with these figures rising daily across the globe. The burden of this highly contagious respiratory disease is that it presents itself in both symptomatic and asymptomatic patterns in those already infected, thereby leading to an exponential rise in the number of contractions of the disease and fatalities. It is, therefore, crucial to expedite the process of early detection and diagnosis of the disease across the world. The case-based reasoning (CBR) model is a compelling paradigm that allows for the utilization of case-specific knowledge previously experienced, concrete problem situations or specific patient cases for solving new cases. This study, therefore, aims to leverage the very rich database of cases of COVID-19 to solve new cases. The approach adopted in this study employs the use of an improved CBR model for state-of-the-art reasoning task in the classification of suspected cases of COVID-19. The CBR model leverages on a novel feature selection and the semantic-based mathematical model proposed in this study for case similarity computation. An initial population of the archive was achieved from 71 (67 adults and 4 pediatrics) cases obtained from the Italian Society of Medical and Interventional Radiology (SIRM) repository. Results obtained revealed that the proposed approach in this study successfully classified suspected cases into their categories with an accuracy of 94.54%. The study found that the proposed model can support physicians to easily diagnose suspected cases of COVID-19 based on their medical records without subjecting the specimen to laboratory tests. As a result, there will be a global minimization of contagion rate occasioned by slow testing and in addition, reduced false-positive rates of diagnosed cases as observed in some parts of the globe.}
}
@article{ROSEDAVIS2022104693,
title = {Semantic knowledge modeling and evaluation of argument Theory to develop dialogue based patient education systems for chronic disease Self-Management},
journal = {International Journal of Medical Informatics},
volume = {160},
pages = {104693},
year = {2022},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2022.104693},
url = {https://www.sciencedirect.com/science/article/pii/S1386505622000077},
author = {Benjamin Rose-Davis and William {Van Woensel} and Samina {Raza Abidi} and Elizabeth Stringer and Syed {Sibte Raza Abidi}},
keywords = {Patient Education, Dialogue Systems, Argument Theory, Knowledge Modelling},
abstract = {Background
To improve understanding as well as uptake of health educational material, it should be tailored to informational needs, offer intuitive modes of interaction, and present credible evidence for health claims. Dialogue systems go some way in meeting these requirements, as they emulate interactive and intuitive person-to-person communication. However, most works do not offer a formal model nor modelling process to structure dialogue content, and do not focus on ensuring credibility.
Methods
We propose an Extended Model of Argument (EMA) dialogue model and modelling process to support educational dialogue systems. In this dialogue model, computerized arguments directly offer evidence for health claims. EMA further offers “dialogue by design”, where argument structures and interrelations are dynamically leveraged to offer dialogues, instead of relying on predefining discourse flows. We implemented an EMA-based dialogue education system for Juvenile Idiopathic Arthritis (JIA). We performed a qualitative evaluation with JIA health experts involving a Cognitive Walkthrough and Semi-Structured Interview. We applied Directed Content Analysis using categories from the O’Grady framework, and coded sub-themes within those categories using Grounded Theory.
Results
We identified 6 sub-themes within the participant feedback pertaining to Quality, Credibility, and Utility. Participants attached strong importance to credibility and found the dialogue system to be a flexible educational tool. Some participants suggested sorting educational items by importance, and presenting only salient knowledge associations to reduce dialogue complexity.
Conclusion
Overall, our qualitative evaluation confirmed the following: the ability of EMA to offer credible and appropriate dialogues; and, in general, the utility of dialogue systems to educate JIA patients and their families. In future work, we will revise the system based on evaluation feedback, and perform a more extensive evaluation with patients and caregivers.}
}
@incollection{PHILLIPS2023181,
title = {Children's participation in local curriculum-making},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {181-187},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.03029-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305030293},
author = {Louise G. Phillips},
keywords = {Children's participation, Children's rights, Collective will formation, Curriculum-making, Deliberative democracy, Emergent curriculum, Place-based curriculum, Posthuman curriculum, Rhizomatic curriculum},
abstract = {Curricula across the world is often designed for children by adults. Anyone regardless of age is more likely to participate in an activity if they have a say in what happens. If curriculum-making is activity of learning, how might children contribute ideas, perspectives, collectively decide and co-construct curriculum. Ontologies, theories and ideas which enable children's participation in local curriculum-making are discussed, including Indigenous ontologies, UN Convention of the Rights of the Child, Reggio Emilia approach, emergent curriculum, mosaic approach, national early childhood curricula, deliberative democracy, home and community learning, place-based education and posthumanism.}
}
@article{HAMMAMI2021103712,
title = {Automated classification of cancer morphology from Italian pathology reports using Natural Language Processing techniques: A rule-based approach},
journal = {Journal of Biomedical Informatics},
volume = {116},
pages = {103712},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103712},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421000411},
author = {Linda Hammami and Alessia Paglialonga and Giancarlo Pruneri and Michele Torresani and Milena Sant and Carlo Bono and Enrico Gianluca Caiani and Paolo Baili},
keywords = {Natural Language Processing, Italian language, Pathology Reports, Cancer morphology},
abstract = {Pathology reports represent a primary source of information for cancer registries. Hospitals routinely process high volumes of free-text reports, a valuable source of information regarding cancer diagnosis for improving clinical care and supporting research. Information extraction and coding of textual unstructured data is typically a manual, labour-intensive process. There is a need to develop automated approaches to extract meaningful information from such texts in a reliable and accurate way. In this scenario, Natural Language Processing (NLP) algorithms offer a unique opportunity to automatically encode the unstructured reports into structured data, thus representing a potential powerful alternative to expensive manual processing. However, notwithstanding the increasing interest in this area, there is still limited availability of NLP approaches for pathology reports in languages other than English, including Italian, to date. The aim of our work was to develop an automated algorithm based on NLP techniques, able to identify and classify the morphological content of pathology reports in the Italian language with micro-averaged performance scores higher than 95%. Specifically, a novel, domain-specific classifier that uses linguistic rules was developed and tested on 27,239 pathology reports from a single Italian oncological centre, following the International Classification of Diseases for Oncology morphology classification standard (ICD-O-M). The proposed classification algorithm achieved successful results with a micro-F1 score of 98.14% on 9594 pathology reports in the test dataset. This algorithm relies on rules defined on data from a single hospital that is specifically dedicated to cancer, but it is based on general processing steps which can be applied to different datasets. Further research will be important to demonstrate the generalizability of the proposed approach on a larger corpus from different hospitals.}
}
@article{MEHDI2025113777,
title = {2D-diffractogram analysis: Kinematic-diffraction simulator for neural-network training-data generation},
journal = {Computational Materials Science},
volume = {252},
pages = {113777},
year = {2025},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2025.113777},
url = {https://www.sciencedirect.com/science/article/pii/S092702562500120X},
author = {Redad Mehdi and Rounak Chawla and Erika I. Barcelos and Matthew A. Willard and Roger H. French and Frank Ernst},
keywords = {X-ray diffractometry, Diffractogram analysis, Diffraction simulator, Machine learning, Neural networks},
abstract = {To exploit the information contained in 2D X-ray diffractograms fully, quantitatively, automatically, and with high throughput, e.g. for analyzing video sequences from in-situ experiments, we can train deep-learning NNs (neural networks) with simulated diffractograms. Realistic models of materials microstructures require “ground truth” training datasets of high cardinality. To produce these, we developed a “kinematic-diffraction simulator,” implemented in the Wolfram Language and executed within a high-performance computing environment. The simulator can rapidly generate Fraunhofer diffractograms for diverse crystal- and microstructure models over a significant multi-dimensional space of parameters. We conclude that simulated diffractograms can enable suitable training of deep-learning NNs – in spite of not including some “real-world” features that occur in experimental diffractograms – and that high-performance computing achieves training data generation rates that support modeling of microstructures with a realistically large number of parameters.}
}
@article{JOHNSON20254711,
title = {Human interpretable grammar encodes multicellular systems biology models to democratize virtual cell laboratories},
journal = {Cell},
volume = {188},
number = {17},
pages = {4711-4733.e37},
year = {2025},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2025.06.048},
url = {https://www.sciencedirect.com/science/article/pii/S0092867425007500},
author = {Jeanette A.I. Johnson and Daniel R. Bergman and Heber L. Rocha and David L. Zhou and Eric Cramer and Ian C. Mclean and Yoseph W. Dance and Max Booth and Zachary Nicholas and Tamara Lopez-Vidal and Atul Deshpande and Randy Heiland and Elmar Bucher and Fatemeh Shojaeian and Matthew Dunworth and André Forjaz and Michael Getz and Inês Godet and Furkan Kurtoglu and Melissa Lyman and John Metzcar and Jacob T. Mitchell and Andrew Raddatz and Jacobo Solorzano and Aneequa Sundus and Yafei Wang and David G. DeNardo and Andrew J. Ewald and Daniele M. Gilkes and Luciane T. Kagohara and Ashley L. Kiemen and Elizabeth D. Thompson and Denis Wirtz and Laura D. Wood and Pei-Hsun Wu and Neeha Zaidi and Lei Zheng and Jacquelyn W. Zimmerman and Jude M. Phillip and Elizabeth M. Jaffee and Joe W. Gray and Lisa M. Coussens and Young Hwan Chang and Laura M. Heiser and Genevieve L. Stein-O’Brien and Elana J. Fertig and Paul Macklin},
keywords = {cell behavior hypothesis grammar, cell behaviors, multicellular systems, mathematical modeling, agent-based modeling, simulation, immunology, cancer biology, immunotherapy, spatial transcriptomics, cell interactions, tissue dynamics, physics of multicellular biology, multicellular systems biology, mathematical biology, modeling language, multi-omics},
abstract = {Summary
Cells interact as dynamically evolving ecosystems. While recent single-cell and spatial multi-omics technologies quantify individual cell characteristics, predicting their evolution requires mathematical modeling. We propose a conceptual framework—a cell behavior hypothesis grammar—that uses natural language statements (cell rules) to create mathematical models. This enables systematic integration of biological knowledge and multi-omics data to generate in silico models, enabling virtual “thought experiments” that test and expand our understanding of multicellular systems and generate new testable hypotheses. This paper motivates and describes the grammar, offers a reference implementation, and demonstrates its use in developing both de novo mechanistic models and those informed by multi-omics data. We show its potential through examples in cancer and its broader applicability in simulating brain development. This approach bridges biological, clinical, and systems biology research for mathematical modeling at scale, allowing the community to predict emergent multicellular behavior.}
}
@article{JAULENT201819,
title = {Semantic interoperability challenges to process large amount of data perspectives in forensic and legal medicine},
journal = {Journal of Forensic and Legal Medicine},
volume = {57},
pages = {19-23},
year = {2018},
note = {Thematic section: Big dataGuest editor: Thomas LefèvreThematic section: Health issues in police custodyGuest editors: Patrick Chariot and Steffen Heide},
issn = {1752-928X},
doi = {https://doi.org/10.1016/j.jflm.2016.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1752928X1630124X},
author = {Marie-Christine Jaulent and Damien Leprovost and Jean Charlet and Remy Choquet},
keywords = {Variety in big data, Knowledge engineering, Semantic interoperability, Ontology, Forensic science},
abstract = {This article is a position paper dealing with semantic interoperability challenges. It addresses the Variety and Veracity dimensions when integrating, sharing and reusing large amount of heterogeneous data for data analysis and decision making applications in the healthcare domain. Many issues are raised by the necessity to conform Big Data to interoperability standards. We discuss how semantics can contribute to the improvement of information sharing and address the problem of data mediation with domain ontologies. We then introduce the main steps for building domain ontologies as they could be implemented in the context of Forensic and Legal medicine. We conclude with a particular emphasis on the current limitations in standardisation and the importance of knowledge formalization.}
}
@article{SHEKHAR202035,
title = {Effective management of slums- Case study of Kalaburagi city, Karnataka, India},
journal = {Journal of Urban Management},
volume = {9},
number = {1},
pages = {35-53},
year = {2020},
issn = {2226-5856},
doi = {https://doi.org/10.1016/j.jum.2019.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2226585619300317},
author = {Sulochana Shekhar},
abstract = {As urbanization grows, we may expect that slums will tend to grow even faster. Kalaburagi is a second tier city and an important commercial hub for the Hyderabad-Karnataka region located in the north eastern part of Karnataka state. It attracts the rural folk from neighbouring districts and the city is undergoing rapid changes in terms of population growth as well as in the degree of urbanization due to which the slum population is increasing. There are 60 slums that account for 11% of city's population. This study aims to contribute some sustainable methodologies for better execution of slum development programs to advance the living conditions of slum dwellers. Slum ontology has been built to identify slums from very high-resolution satellite data that will benefit all stakeholders. The developed ontology was validated with field survey and the help of field photographs, the physical and socio-economic conditions were documented. As per the slum ontology, basic inputs for the Cellular Automata (CA) model were identified and criteria maps with scores were generated. The suitability map was created by giving appropriate weight to the factors. When the slum map prepared based on slum ontology was overlaid on the final results, the CA model output has given reliable results regarding current slum distribution and also a hint of the possible occurrence of slums in the future. Since the proposed land use plan for 2021 has been finalized by the Urban Local Body (ULB), the suitable area for developing affordable housing stock was suggested for preventing slums from arising. The Spatial Decision Support System (SDSS) was built for selecting slum development options. The design and running of the SDSS model were demonstrated with one case study – Borabai nagar slum to enable the ULB to apply the approach to other slum areas for effective implementation of slum development programs.}
}
@article{LOKALA2024,
title = {Detecting Substance Use Disorder Using Social Media Data and the Dark Web: Time- and Knowledge-Aware Study},
journal = {JMIRx Med},
volume = {5},
year = {2024},
issn = {2563-6316},
doi = {https://doi.org/10.2196/48519},
url = {https://www.sciencedirect.com/science/article/pii/S2563631624000360},
author = {Usha Lokala and Orchid Chetia Phukan and Triyasha Ghosh Dastidar and Francois Lamy and Raminta Daniulaityte and Amit Sheth},
keywords = {opioid, substance use, substance use disorder, social media, US, opioid crisis, mental health, substance misuse, crypto, dark web, users, user perception, fentanyl, synthetic opioids, United States},
abstract = {Background
Opioid and substance misuse has become a widespread problem in the United States, leading to the “opioid crisis.” The relationship between substance misuse and mental health has been extensively studied, with one possible relationship being that substance misuse causes poor mental health. However, the lack of evidence on the relationship has resulted in opioids being largely inaccessible through legal means.
Objectives
This study aims to analyze social media posts related to substance use and opioids being sold through cryptomarket listings. The study aims to use state-of-the-art deep learning models to generate sentiment and emotion from social media posts to understand users’ perceptions of social media. The study also aims to investigate questions such as which synthetic opioids people are optimistic, neutral, or negative about; what kind of drugs induced fear and sorrow; what kind of drugs people love or are thankful about; which drugs people think negatively about; and which opioids cause little to no sentimental reaction.
Methods
The study used the drug abuse ontology and state-of-the-art deep learning models, including knowledge-aware Bidirectional Encoder Representations From Transformers–based models, to generate sentiment and emotion from social media posts related to substance use and opioids being sold through cryptomarket listings. The study crawled cryptomarket data and extracted posts for fentanyl, fentanyl analogs, and other novel synthetic opioids. The study performed topic analysis associated with the generated sentiments and emotions to understand which topics correlate with people’s responses to various drugs. Additionally, the study analyzed time-aware neural models built on these features while considering historical sentiment and emotional activity of posts related to a drug.
Results
The study found that the most effective model performed well (statistically significant, with a macro–F1-score of 82.12 and recall of 83.58) in identifying substance use disorder. The study also found that there were varying levels of sentiment and emotion associated with different synthetic opioids, with some drugs eliciting more positive or negative responses than others. The study identified topics that correlated with people’s responses to various drugs, such as pain relief, addiction, and withdrawal symptoms.
Conclusions
The study provides insight into users’ perceptions of synthetic opioids based on sentiment and emotion expressed in social media posts. The study’s findings can be used to inform interventions and policies aimed at reducing substance misuse and addressing the opioid crisis. The study demonstrates the potential of deep learning models for analyzing social media data to gain insights into public health issues.}
}
@article{RUOTSALAINEN2025,
title = {A System Model and Requirements for Transformation to Human-Centric Digital Health},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/68661},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125006090},
author = {Pekka Ruotsalainen and Bernd Blobel},
keywords = {digital health, human rights, privacy, dignity, autonomy, digital economy, neoliberalism, modeling, system analysis, artificial intelligence},
abstract = {Digital transformation is widely understood as a process where technology is used to modify an organization’s products and services and to create new ones. It is rapidly advancing in all sectors of society. Researchers have shown that it is a multidimensional process determined by human decisions based on ideologies, ideas, beliefs, goals, and the ways in which technology is used. In health care and health, the end result of digital transformation is digital health. In this study, a detailed literature review covering 560 research articles published in major journals was performed, followed by an analysis of ideas, beliefs, and goals guiding digital transformation and their possible consequences for privacy, human rights, dignity, and autonomy in health care and health. Results of literature analyses demonstrated that from the point of view of privacy, dignity, and human rights, the current laws, regulations, and system architectures have major weaknesses. One possible model of digital health is based on the dominant ideas and goals of the business world related to the digital economy and neoliberalism, including privatization of health care services, monetization and commodification of health data, and personal responsibility for health. These ideas represent meaningful risks to human rights, privacy, dignity, and autonomy. In this paper, we present an alternative solution for digital health called human-centric digital health (HCDH). Using system thinking and system modeling methods, we developed a system model for HCDH. It uses 5 views (ideas, health data, principles, regulation, and organizational and technical innovations) to align with human rights and values and support dignity, privacy, and autonomy. To make HCDH future proof, extensions to human rights, the adoption of the principle of restricted informational ownership of health data, and the development of new duties, responsibilities, and laws are needed. Finally, we developed a system-oriented, architecture-centric, ontology-based, and policy-driven approach to represent and manage HCDH ecosystems.}
}
@article{CHADZYNSKI2021100106,
title = {Semantic 3D City Database — An enabler for a dynamic geospatial knowledge graph},
journal = {Energy and AI},
volume = {6},
pages = {100106},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100106},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000574},
author = {Arkadiusz Chadzynski and Nenad Krdzavac and Feroz Farazi and Mei Qi Lim and Shiying Li and Ayda Grisiute and Pieter Herthogs and Aurel {von Richthofen} and Stephen Cairns and Markus Kraft},
keywords = {CityGML, Sustainability, Digitisation, Urban planning, Semantic web, Knowledge graph, Ontology, Decision support system, Artificial intelligence, Geospatial modelling, Geospatial search},
abstract = {This paper presents a dynamic geospatial knowledge graph as part of The World Avatar project, with an underlying ontology based on CityGML 2.0 for three-dimensional geometrical city objects. We comprehensively evaluated, repaired and refined an existing CityGML ontology to produce an improved version that could pass the necessary tests and complete unit test development. A corresponding data transformation tool, originally designed to work alongside CityGML, was extended. This allowed for the transformation of original data into a form of semantic triples. We compared various scalable technologies for this semantic data storage and chose Blazegraph™ as it provided the required geospatial search functionality. We also evaluated scalable hardware data solutions and file systems using the publicly available CityGML 2.0 data of Charlottenburg in Berlin, Germany as a working example. The structural isomorphism of the CityGML schemas and the OntoCityGML Tbox allowed the data to be transformed without loss of information. Efficient geospatial search algorithms allowed us to retrieve building data from any point in a city using coordinates. The use of named graphs and namespaces for data partitioning ensured the system performance stayed well below its capacity limits. This was achieved by evaluating scalable and dedicated data storage hardware capable of hosting expansible file systems, which strengthened the architectural foundations of the target system.}
}
@article{KIM2023116265,
title = {Five urban health research traditions: A meta-narrative review},
journal = {Social Science & Medicine},
volume = {336},
pages = {116265},
year = {2023},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2023.116265},
url = {https://www.sciencedirect.com/science/article/pii/S0277953623006226},
author = {Jinhee Kim and Evelyne {de Leeuw} and Ben Harris-Roxas and Peter Sainsbury},
keywords = {Urban health, Research traditions, Paradigms, Ontology, Transdisciplinary, Meta-narrative review, Bibliometric analysis},
abstract = {Urban health scholars explore the connection between the urban space and health through ontological perspectives that are shaped by their disciplinary traditions. Without explicit recognition of the different approaches, there are barriers to collaboration. This paper maps the terrain of the urban health scholarship to identify key urban health research traditions; and to articulate the main features distinguishing these different traditions. We apply a meta-narrative review guided by a bibliometric co-citation network analysis to the body of research on urban health retrieved from the Web of Science Core Collection. Five urban health research traditions were identified: (1) sustainable urban development, (2) urban ecosystem services, (3) urban resilience, (4) healthy urban planning, and (5) urban green spaces. Each research tradition has a different conceptual and thematic perspective to addressing urban health. These include perspectives on the scale of the urban health issue of interest, and on the conceptualisation of the urban context and health. Additionally, we developed a framework to allow for better differentiation between the differing research traditions based on (1) perspectives of the urban system as complicated or complex, (2) the preferred locus of change as a function of structure and agency and (3) the geographic scale of the urban health issue that is addressed. These dimensions have even deeper implications for transdisciplinary collaboration as they are underpinned by paradigmatic differences, rather than disciplinary differences. We conclude that it is essential for urban health researchers to reflect on the different urban health approaches and seek coherence by understanding their similarities and differences. Such endeavours are required to produce and interpret transdisciplinary knowledge for the goal of improving health by transforming urban systems.}
}
@article{TAN2023103796,
title = {RNA-sequencing of peripheral whole blood of individuals at ultra-high-risk for psychosis – A longitudinal perspective},
journal = {Asian Journal of Psychiatry},
volume = {89},
pages = {103796},
year = {2023},
issn = {1876-2018},
doi = {https://doi.org/10.1016/j.ajp.2023.103796},
url = {https://www.sciencedirect.com/science/article/pii/S1876201823003520},
author = {Samuel Ming Xuan Tan and Jie Yin Yee and Sugam Budhraja and Balkaran Singh and Zohreh Doborjeh and Maryam Doborjeh and Nikola Kasabov and Edmund Lai and Alexander Sumich and Jimmy Lee and Wilson Wen Bin Goh},
abstract = {Background
The peripheral blood is an attractive source of prognostic biomarkers for psychosis conversion. There is limited research on the transcriptomic changes associated with psychosis conversion in the peripheral whole blood.
Study Design
We performed RNA-sequencing of peripheral whole blood from 65 ultra-high-risk (UHR) participants and 70 healthy control participants recruited in the Longitudinal Youth-at-Risk Study (LYRIKS) cohort. 13 UHR participants converted in the study duration. Samples were collected at 3 timepoints, at 12-months interval across a 2-year period. We examined whether the genes differential with psychosis conversion contain schizophrenia risk loci. We then examined the functional ontologies and GWAS associations of the differential genes. We also identified the overlap between differentially expressed genes across different comparisons.
Study results
Genes containing schizophrenia risk loci were not differentially expressed in the peripheral whole blood in psychosis conversion. The differentially expressed genes in psychosis conversion are enriched for ontologies associated with cellular replication. The differentially expressed genes in psychosis conversion are associated with non-neurological GWAS phenotypes reported to be perturbed in schizophrenia and psychosis but not schizophrenia and psychosis phenotypes themselves. We found minimal overlap between the genes differential with psychosis conversion and the genes that are differential between pre-conversion and non-conversion samples.
Conclusion
The associations between psychosis conversion and peripheral blood-based biomarkers are likely to be indirect. Further studies to elucidate the mechanism behind potential indirect associations are needed.}
}
@article{ALMAHMOUD2024121767,
title = {SEWAR: A corpus-based N-gram approach for extracting semantically-related words from Arabic medical corpus},
journal = {Expert Systems with Applications},
volume = {238},
pages = {121767},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121767},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423022698},
author = {Rana Husni AlMahmoud and Bassam H. Hammo},
keywords = {Semantically-related words, Multi-word term extraction, Word-embedding, Arabic medical dataset, Arabic language, FastText},
abstract = {Automatic aggregation of similar words into semantically related groups (or clusters) is of interest to many natural language processing (NLP) applications. Extracting semantically related words and quasi-synonyms from text is a relatively new research area for the under-resourced Arabic language. Previous attempts addressed the problem of single-word term extraction. However, the absence of multiword terms (MWTs) dictionary, ontology, and semantic network makes extracting and identifying high-quality MWTs for the Arabic language a challenging problem. The main goal of this study is to extract corpus-based and coherent MWTs in the form of bigram and trigram sequences as an adequate representation of syntactic and semantic clusters. Therefore, this study contributes to this problem by implementing an algorithm named SEWAR, which uses the FastText algorithm to extract high-quality MWTs from a medical health corpus in Arabic. If put into practice, SEWAR can provide a suitable and helpful solution to classify and direct medical health questions to proper medical practitioners without human intervention. In addition, SEWAR can be applied to plenty of NLP tasks, such as information retrieval, question answering, and text summarization. Three metrics were used to assess the extracted MWTs; the pointwise mutual information (PMI), the cosine similarity, and the clustering purity measure. The results were promising and encouraging to generalize and apply SEWAR to extract MWTs from any Arabic corpus.}
}
@article{RICHTER2025108442,
title = {Cosmological limits to growth, affective abundance, and Rights of Nature: Insights from Buen Vivir/sumak kawsay for the cultural politics of degrowth},
journal = {Ecological Economics},
volume = {228},
pages = {108442},
year = {2025},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2024.108442},
url = {https://www.sciencedirect.com/science/article/pii/S0921800924003392},
author = {Katharina Richter},
keywords = {Degrowth, Buen Vivir, Limits to growth, Rights of Nature, Cultural politics, Sustainability transformations},
abstract = {This article creates an inter-epistemic dialogue between degrowth and Buen Vivir/sumak kawsay based on qualitative research conducted in Ecuador. It builds on degrowth scholarship that considers cultural change an integral part of sustainability transformations. The article envisions what that change could look like by developing non-anthropocentric and de-individualised visions of sustainability transformations. It thereby advances recently reignited debates around limits to growth and artificial scarcity. Buen Vivir/sumak kawsay is an Andean-Amazonian indigenous conceptualisation of Good Living. An engagement with the reciprocal practices, behaviours and rituals of its protagonists yields three insights for the cultural politics of degrowth. First, cosmological limits to growth are normative constraints to harming the Living World and arise from relational ontologies that embed the human into the natural world. Second, the political economy of Buen Vivir/sumak kawsay produces affective abundance via reciprocity with the non-human world. This offers a de-individualised understanding of abundance for degrowth, beyond enjoyment and provision of universal basic services. Third, these ideas can be implemented in practice through Rights of Nature, put forward here as a viable policy option because of its potential to impute relational worldviews into materialist understandings of nature. These pluriverse avenues can enact cultural change towards sustainability transformations.}
}
@incollection{PREISIG20221237,
title = {Topology-Based Construction of Business-Integrated Material Modelling Workflows},
editor = {Ludovic Montastruc and Stephane Negny},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {51},
pages = {1237-1242},
year = {2022},
booktitle = {32nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-95879-0.50207-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323958790502071},
author = {Heinz A. Preisig and Peter Klein and Natalia Konchakova and Thomas F. Hagelien and Jesper Friis and Martin T. Horsch},
keywords = {Computational engineering, multi-scale modelling, applied ontology},
abstract = {Designing a new product requires information from the business and physical domains, which implies integrating business decision tools with process and material simulation processes to form an overall workflow. The integration involves coupling the business workflow management systems with analysis tools, optimisation and decision support systems, which require process simulations and an integrated data transfer service. The process simulation, in turn, will in general model multiple layers of time scales and thus is also in need of data transfer between different solvers. Here we discuss the main components in the light of a coating-design project.}
}
@article{MOONTHIYA2024104707,
title = {Identities of non-English-dominant teachers in transnational language teacher education: A systematic review},
journal = {Teaching and Teacher Education},
volume = {149},
pages = {104707},
year = {2024},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2024.104707},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X24002397},
author = {Itsaraphap Moonthiya and Marie Stevenson},
keywords = {Language teacher identity, Identity-in-practice, Identity-in-discourse, NNESTs},
abstract = {This article provides a systematic review of 46 empirical studies published between 1990 and 2022 on identities of non-English-dominant teachers undertaking transnational language teacher education in universities in English-speaking countries. The study characterizes the current state of existing research and conducts a research synthesis of existing research to identify themes in the research findings, using identity-in-practice and identity-in-discourse as a theoretical lens. The findings revealed that there is a more dominant presence of themes about identity-in-discourse than those about identity-in-practice in existing research, and that relatively little is known about the links between language teacher identity and language teacher education.}
}
@article{CATTELANI2020242,
title = {A rule-based framework for risk assessment in the health domain},
journal = {International Journal of Approximate Reasoning},
volume = {119},
pages = {242-259},
year = {2020},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2019.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X19300957},
author = {Luca Cattelani and Federico Chesani and Luca Palmerini and Pierpaolo Palumbo and Lorenzo Chiari and Stefania Bandinelli},
keywords = {Depression, Falls, Formal ontology, Logical rule-based system, Missing data, Risk assessment},
abstract = {Risk assessment is an important decision support task in many domains, including health, engineering, process management, and economy. There is a growing interest in automated methods for risk assessment. These methods should be able to process information efficiently and with little user involvement. Currently, from the scientific literature in the health domain, there is availability of evidence-based knowledge about specific risk factors. On the other hand, there is no automatic procedure to exploit this available knowledge in order to create a general risk assessment tool which can combine the available quantitative data about risk factors and their impact on the corresponding risk. We present a Framework for the Assessment of Risk of adverse Events (FARE) and its first concrete applications FRAT-up and DRAT-up, which were used for fall and depression risk assessment in older persons and validated on four and three European epidemiological datasets, respectively. FARE consists of i) a novel formal ontology called On2Risk; and ii) a logical and probabilistic rule-based model. The ontology was designed to represent qualitative and quantitative data about risks in a general, structured and machine-readable manner so that this data may be concretely exploited by risk assessment algorithms. We describe the structure of the FARE model in the form of logic and probabilistic rules. We show how when starting from machine-readable data about risk factors, like the data contained in On2Risk, an instance of the algorithm can be automatically constructed and used to estimate the risk of an adverse event.}
}
@article{HOUSTON2024107607,
title = {46 Automated Derivation of Diagnostic Criteria for Lung Cancer using Natural Language Processing on Electronic Health Records: A pilot study},
journal = {Lung Cancer},
volume = {190},
pages = {107607},
year = {2024},
note = {Poster abstracts of the 22nd Annual British Thoracic Oncology Group Conference 2024 ICC Belfast},
issn = {0169-5002},
doi = {https://doi.org/10.1016/j.lungcan.2024.107607},
url = {https://www.sciencedirect.com/science/article/pii/S0169500224001521},
author = {Andrew Houston and Sophie Williams and William Ricketts and Charles Gutteridge and Chris Tackaberry and Marcus Simon and John Conibear}
}
@article{ARROTTA2023101780,
title = {Probabilistic knowledge infusion through symbolic features for context-aware activity recognition},
journal = {Pervasive and Mobile Computing},
volume = {91},
pages = {101780},
year = {2023},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2023.101780},
url = {https://www.sciencedirect.com/science/article/pii/S157411922300038X},
author = {Luca Arrotta and Gabriele Civitarese and Claudio Bettini},
keywords = {Human activity recognition, Neuro-symbolic, Context-awareness},
abstract = {In the general machine learning domain, solutions based on the integration of deep learning models with knowledge-based approaches are emerging. Indeed, such hybrid systems have the advantage of improving the recognition rate and the model’s interpretability. At the same time, they require a significantly reduced amount of labeled data to reliably train the model. However, these techniques have been poorly explored in the sensor-based Human Activity Recognition (HAR) domain. The common-sense knowledge about activity execution can potentially improve purely data-driven approaches. While a few knowledge infusion approaches have been proposed for HAR, they rely on rigid logic formalisms that do not take into account uncertainty. In this paper, we propose P-NIMBUS, a novel knowledge infusion approach for sensor-based HAR that relies on probabilistic reasoning. A probabilistic ontology is in charge of computing symbolic features that are combined with the features automatically extracted by a CNN model from raw sensor data and high-level context data. In particular, the symbolic features encode probabilistic common-sense knowledge about the activities consistent with the user’s surrounding context. These features are infused within the model before the classification layer. We experimentally evaluated P-NIMBUS on a HAR dataset of mobile devices sensor data that includes 14 different activities performed by 25 users. Our results show that P-NIMBUS outperforms state-of-the-art neuro-symbolic approaches, with the advantage of requiring a limited amount of training data to reach satisfying recognition rates (i.e., more than 80% of F1-score with only 20% of labeled data).}
}
@article{GURDUR2018468,
title = {Knowledge Representation of Cyber-physical Systems for Monitoring Purpose},
journal = {Procedia CIRP},
volume = {72},
pages = {468-473},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118301173},
author = {Didem Gürdür and Aneta {Vulgarakis Feljan} and Jad El-khoury and Swarup {Kumar Mohalik} and Ramamurthy Badrinath and Anusha {Pradeep Mujumdar} and Elena Fersman},
keywords = {automated warehouse, knowledge representation, ontologies, cyber-physical systems, OSLC},
abstract = {Automated warehouses, as a form of cyber-physical systems (CPSs), require several components to work collaboratively to address the common business objectives of complex logistics systems. During the collaborative operations, a number of key performance indicators (KPI) can be monitored to understand the proficiency of the warehouse and control the operations and decisions. It is possible to drive and monitor these KPIs by looking at both the state of the warehouse components and the operations carried out by them. Therefore, it is necessary to represent this knowledge in an explicit and formally-specified data model and provide automated methods to derive the KPIs from the representation. In this paper, we implement a minimalistic data model for a subset of warehouse resources using linked data in order to monitor a few KPIs, namely sustainability, safety and performance. The applicability of the approach and the data model is illustrated through a use case. We demonstrate that it is possible to develop minimalistic data models through Open Services for Lifecycle Collaboration (OSLC) resource shapes which enables compatibility with the declarative and procedural knowledge of automated warehouse agents specified in Planning Domain Definition Language (PDDL).}
}
@article{BOLLA20241929,
title = {Detection of Objectionable Song Lyrics Using Weakly Supervised Learning and Natural Language Processing Techniques},
journal = {Procedia Computer Science},
volume = {235},
pages = {1929-1942},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.183},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924008597},
author = {Bharath Kumar Bolla and Soumya Ranjan Pattnaik and Sambit Patra},
keywords = {Weak Supervision, Snorkel, Class Imbalance, SMOTE, DistilBert, TF-IDF, Data Programming, Logistic Regression, Toxicity, Objectionable Content, SVM, Random Forest},
abstract = {The growth of digital and social media has led to an exponential increase in the availability of music and its lyrics. It has been observed that objectionable content is becoming a huge problem in the age of the digital era. This research aims to develop a system for automatically detecting lyrics that may contain offensive or controversial themes, such as hate speech, sexism, racism, and violence, by generating labeled data using weak supervision. In this study, we generated labels using heuristic rules using the Snorkel framework, and the performance of various machine learning classifiers, namely, Logistic Regression, Random Forest, and Support Vector Machine, were evaluated on label-generated data. The results show that DistilBert performs better than frequency-based methods (TF-IDF and CountVectorizer) by 3-5% in F1 score, and the results are even more impressive on SMOTE augmented dataset, where Logistic Regression and SVM on Distilbert vectorized data performed similarly and has exhibited 6-8% of F1 score gain over frequency based methods. In addition, it asserts that the proposed method can be used to detect offensive lyrics in other languages, given the same type of data used in this study. Our work will help music industry professionals and digital content monitoring systems for hate and offensive content to adopt the methodology and approaches used in our work.}
}
@article{BITTERMAN2023262,
title = {An End-to-End Natural Language Processing System for Automatically Extracting Radiation Therapy Events From Clinical Texts},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {117},
number = {1},
pages = {262-273},
year = {2023},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2023.03.055},
url = {https://www.sciencedirect.com/science/article/pii/S036030162300295X},
author = {Danielle S. Bitterman and Eli Goldner and Sean Finan and David Harris and Eric B. Durbin and Harry Hochheiser and Jeremy L. Warner and Raymond H. Mak and Timothy Miller and Guergana K. Savova},
abstract = {Purpose
Real-world evidence for radiation therapy (RT) is limited because it is often documented only in the clinical narrative. We developed a natural language processing system for automated extraction of detailed RT events from text to support clinical phenotyping.
Methods and Materials
A multi-institutional data set of 96 clinician notes, 129 North American Association of Central Cancer Registries cancer abstracts, and 270 RT prescriptions from HemOnc.org was used and divided into train, development, and test sets. Documents were annotated for RT events and associated properties: dose, fraction frequency, fraction number, date, treatment site, and boost. Named entity recognition models for properties were developed by fine-tuning BioClinicalBERT and RoBERTa transformer models. A multiclass RoBERTa-based relation extraction model was developed to link each dose mention with each property in the same event. Models were combined with symbolic rules to create a hybrid end-to-end pipeline for comprehensive RT event extraction.
Results
Named entity recognition models were evaluated on the held-out test set with F1 results of 0.96, 0.88, 0.94, 0.88, 0.67, and 0.94 for dose, fraction frequency, fraction number, date, treatment site, and boost, respectively. The relation model achieved an average F1 of 0.86 when the input was gold-labeled entities. The end-to-end system F1 result was 0.81. The end-to-end system performed best on North American Association of Central Cancer Registries abstracts (average F1 0.90), which are mostly copy–paste content from clinician notes.
Conclusions
We developed methods and a hybrid end-to-end system for RT event extraction, which is the first natural language processing system for this task. This system provides proof-of-concept for real-world RT data collection for research and is promising for the potential of natural language processing methods to support clinical care.}
}
@article{LACASTA201882,
title = {Agricultural recommendation system for crop protection},
journal = {Computers and Electronics in Agriculture},
volume = {152},
pages = {82-89},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.06.049},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917315260},
author = {Javier Lacasta and F. Javier Lopez-Pellicer and Borja Espejo-García and Javier Nogueras-Iso and F. Javier Zarazaga-Soria},
keywords = {Ontology creation, Ontology population, Data integration, Intelligent systems, Pest control},
abstract = {Pests in crops produce important economic loses all around the world. To deal with them without damaging people or the environment, governments have established strict legislation and norms describing the products and procedures of use. However, since these norms frequently change to reflect scientific and technological advances, it is needed to perform a frequent review of affected norms in order to update pest related information systems. This is not an easy task because they are usually human-oriented, so intensive manual labour is required. To facilitate the use of this information, this work proposes the construction of a recommendation system that facilitates the identification of pests and the selection of suitable treatments. The core of this system is an ontology that models the interactions between crops, pests and treatments.}
}
@article{WEN2020101017,
title = {Recurrent neural network language generation for spoken dialogue systems},
journal = {Computer Speech & Language},
volume = {63},
pages = {101017},
year = {2020},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2019.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0885230817300578},
author = {Tsung-Hsien Wen and Steve Young},
keywords = {Dialogue systems, Recurrent neural networks, Natural language generation, Domain adaptation, Discriminative training, Human evaluation},
abstract = {Natural Language Generation (NLG) is a critical component of spoken dialogue systems and it has a significant impact both on usability and perceived quality. Most existing NLG approaches in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. Moreover, these limitations also add significantly to development costs and make the delivery of cross-domain, cross-lingual dialogue systems especially complex and expensive. The first contribution of this paper is to present RNNLG, a Recurrent Neural Network (RNN)-based statistical natural language generator that can learn to generate utterances directly from dialogue act – utterance pairs without any predefined syntaxes or semantic alignments. The presentation includes a systematic comparison of the principal RNN-based NLG models available. The second contribution, is to test the scalability of the proposed system by adapting models from one domain to another. We show that by pairing RNN-based NLG models with a proposed data counterfeiting method and a discriminative objective function, a pre-trained model can be quickly adapted to different domains with only a few examples. All of the findings presented are supported by both corpus-based and human evaluations.}
}
@article{LI2025116885,
title = {Integrated network pharmacology and RNA sequencing analysis to reveal the mechanisms of Qici Sanling decoction in the treatment of gemcitabine resistant bladder cancer},
journal = {Journal of Pharmaceutical and Biomedical Analysis},
volume = {262},
pages = {116885},
year = {2025},
issn = {0731-7085},
doi = {https://doi.org/10.1016/j.jpba.2025.116885},
url = {https://www.sciencedirect.com/science/article/pii/S0731708525002262},
author = {Zhuolun Li and Jinpeng Wang and Wanhui Wang and Bo Geng and Wei Zhang and Weiyang Liu and Yunfeng Nan and Bosen You and Enyang Zhao and Xuedong Li},
keywords = {Gemcitabine resistance, Bladder cancer, Traditional Chinese medicine, Network pharmacology, Molecular docking},
abstract = {Bladder cancer (BCa) is the most prevalent cancer of the urinary system in adults; the prognosis is dismal for BCa treated with gemcitabine (GEM) owing to intrinsic or acquired chemoresistance. This study investigated the potential of Qici Sanling decoction (QCSL), an herbal Chinese medicine, to augment the efficacy of GEM in treating GEM-resistant BCa via network pharmacology and RNA sequencing. We screened 103 active components of QCSL and their 226 targets from the TCMSP database and identified 3985 targets of GEM-resistant BCa via transcriptome sequencing. On the basis of the 69 common targets, a proteinprotein interaction (PPI) network was constructed to identify the top 7 targets. Disease Ontology (DO), Gene Ontology (GO), and Kyoto Encyclopedia of Genes and Genomes (KEGG) functional enrichment analyses were conducted to uncover key pathways. CCK-8 assays, Western blotting, flow cytometry, colony formation, and EdU assays were used to assess the apoptosis and proliferation of GEM-resistant T24 and J82 cells treated with QCSL. The BCa gene set was among the top enriched gene sets in the DO analysis; GO analysis revealed enrichment of 2020 terms linked to GEM resistance, and KEGG analysis revealed 161 enriched signalling pathways. Molecular docking indicated that PTGS2 has high affinity for targets of QCSL components. In vitro experiments demonstrated that cells treated with both QCSL and GEM had significantly reduced viability, increased levels of apoptosis, and decreased proliferative capacity. Thus, QCSL enhances the therapeutic effects of GEM in BCa by promoting cell apoptosis and inhibiting cell proliferation. These findings have significant clinical implications, highlighting a potential combined treatment strategy for GEM-resistant BCa to improve patient outcomes.}
}
@article{TSILIONIS2022106734,
title = {A model-driven framework to support strategic agility: Value-added perspective},
journal = {Information and Software Technology},
volume = {141},
pages = {106734},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106734},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921001841},
author = {Konstantinos Tsilionis and Yves Wautelet},
keywords = {Strategic agility, IT governance, Strategic value, Model-driven development, Agile, Agility, i* framework, Scaled agile framework, SAFe, MoDrIGo, StratAMoDrIGo},
abstract = {Context:
The Covid-19 pandemic has shown the entire world that the habits of work, freedom, and consumption can change quickly and significantly for an undetermined amount of time. A dynamic environment as such, prompts organizations to move fast in order to leverage changing circumstances as sources of opportunity rather than deadly threats. Drastic changes in work organization, consumption habits, compliance, etc., may require firms to quickly adopt new technology delivering all sorts of added value.
Objective:
The development and adoption of new technology – structurally impacting the way the organization conducts its activities – requires a considerable amount of effort in a short time frame, thus rendering it a governance decision where the alignment of the technology’s adoption and use to the long term strategy needs to be evaluated. The short time frame requiring fast response implies that agility should not remain a development or management/operational concept but should also be adopted onto the strategic layer.
Method:
Design Science Research (DSR) has been applied to build-up a framework supporting strategic agility in a model-driven fashion called Strategic Agile Model Driven IT Governance (StratAMoDrIGo). The relevance, rigor and design cycles of DSR have been applied and presented.
Results:
StratAMoDrIGo is based on the identification of sources of value for the organization’s strategy, its stakeholders and the users of the implemented/adopted technology. Relevant concepts are consolidated in an ontology of which the application uses the NFR Model at strategic-level and the i* Strategic Rationale Model at management-level. The proposal is applied on the case of an hospital facing the Covid-19 pandemic.
Conclusion:
The value brought by strategic opportunities’ adoption to the organization, stakeholders and users can be evaluated ex ante through conceptual models.}
}
@article{HUGHES201843,
title = {Enabling interoperability in planetary sciences and heliophysics: The case for an information model},
journal = {Planetary and Space Science},
volume = {150},
pages = {43-49},
year = {2018},
note = {Enabling Open and Interoperable Access to Planetary Science and Heliophysics Databases and Tools},
issn = {0032-0633},
doi = {https://doi.org/10.1016/j.pss.2017.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0032063316304652},
author = {J. Steven Hughes and Daniel J. Crichton and Anne C. Raugh and Baptiste Cecconi and Edward A. Guinness and Christopher E. Isbell and Joseph N. Mafi and Mitchell K. Gordon and Sean H. Hardman and Ronald S. Joyner},
keywords = {Interoperability, Information architecture, Information model, Planetary science, Heliophysics, Semantic, Digital repository, Ontology},
abstract = {The Planetary Data System has developed the PDS4 Information Model to enable interoperability across diverse science disciplines. The Information Model is based on an integration of International Organization for Standardization (ISO) level standards for trusted digital archives, information model development, and metadata registries. Where controlled vocabularies provides a basic level of interoperability by providing a common set of terms for communication between both machines and humans the Information Model improves interoperability by means of an ontology that provides semantic information or additional related context for the terms. The information model was defined by team of computer scientists and science experts from each of the diverse disciplines in the Planetary Science community, including Atmospheres, Geosciences, Cartography and Imaging Sciences, Navigational and Ancillary Information, Planetary Plasma Interactions, Ring-Moon Systems, and Small Bodies. The model was designed to be extensible beyond the Planetary Science community, for example there are overlaps between certain PDS disciplines and the Heliophysics and Astrophysics disciplines. “Interoperability” can apply to many aspects of both the developer and the end-user experience, for example agency-to-agency, semantic level, and application level interoperability. We define these types of interoperability and focus on semantic level interoperability, the type of interoperability most directly enabled by an information model.}
}
@article{KORMILITZIN2021102086,
title = {Med7: A transferable clinical natural language processing model for electronic health records},
journal = {Artificial Intelligence in Medicine},
volume = {118},
pages = {102086},
year = {2021},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2021.102086},
url = {https://www.sciencedirect.com/science/article/pii/S0933365721000798},
author = {Andrey Kormilitzin and Nemanja Vaci and Qiang Liu and Alejo Nevado-Holgado},
keywords = {Clinical natural language processing, Neural networks, Self-supervised learning, Noisy labelling, Active learning},
abstract = {Electronic health record systems are ubiquitous and the majority of patients’ data are now being collected electronically in the form of free text. Deep learning has significantly advanced the field of natural language processing and the self-supervised representation learning and the transfer learning have become the methods of choice in particular when the high quality annotated data are limited. Identification of medical concepts and information extraction is a challenging task, yet important ingredient for parsing unstructured data into structured and tabulated format for downstream analytical tasks. In this work we introduced a named-entity recognition (NER) model for clinical natural language processing. The model is trained to recognise seven categories: drug names, route of administration, frequency, dosage, strength, form, duration. The model was first pre-trained on the task of predicting the next word, using a collection of 2 million free-text patients’ records from MIMIC-III corpora followed by fine-tuning on the named-entity recognition task. The model achieved a micro-averaged F1 score of 0.957 across all seven categories. Additionally, we evaluated the transferability of the developed model using the data from the Intensive Care Unit in the US to secondary care mental health records (CRIS) in the UK. A direct application of the trained NER model to CRIS data resulted in reduced performance of F1 = 0.762, however after fine-tuning on a small sample from CRIS, the model achieved a reasonable performance of F1 = 0.944. This demonstrated that despite a close similarity between the data sets and the NER tasks, it is essential to fine-tune the target domain data in order to achieve more accurate results. The resulting model and the pre-trained embeddings are available at https://github.com/kormilitzin/med7.}
}
@article{HASSEN20242158,
title = {Multi-dimensional Classification of Sensitive Business Process Modeling Aspects},
journal = {Procedia Computer Science},
volume = {239},
pages = {2158-2167},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.404},
url = {https://www.sciencedirect.com/science/article/pii/S187705092401648X},
author = {Mariam Ben Hassen and Faïez Gargouri},
keywords = {Knowledge management, Business process modeling, Sensitive business process, Modeling ap-proaches, languages, BPMN 2.0.2},
abstract = {This paper introduces the problematic of the sensitive business processes (SBP) modeling. First of all, we propose a characterization of SBP (which distinguishes it from classic, structured and conventional BPs). Secondly, we the different SBP modeling aspects and requirements to develop expressive, comprehensive and rigorous models. Besides, we present an in-depth study of the different modeling approaches and languages, in order to analyze their expressiveness and their ability to perfectly and explicitly represent the new specific requirements of SBP modeling. In this study, we choose the better one positioned nowadays, BPMN 2.0, as the best suited standard for SBP representation.}
}
@article{RIANO2022102343,
title = {Modelling and assessing one- and two-drug dose titrations},
journal = {Artificial Intelligence in Medicine},
volume = {131},
pages = {102343},
year = {2022},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2022.102343},
url = {https://www.sciencedirect.com/science/article/pii/S0933365722001087},
author = {David Riaño and Špela Pečnik and Josep Ramon Alonso and Aida Kamišalić},
keywords = {Drug dose titration, Medication errors, Knowledge representation, Evidence-based medicine},
abstract = {In health-care, there is a need to quantify medical errors. Among these errors, we observe wrong dose prescriptions. Drug dose titration (DT) is the process by which dosage is progressively adjusted to the patient till a steady dose is reached. Depending on the clinical disease, drug, and patient condition, dose titration can follow different procedures. Once modeled, these procedures can serve for clinical homogenization, standardization, decision support and retrospective analysis. Here, we propose a language to model dose titration procedures. The language was used to formalize one- and two-drug titration of chronic and acute cases, and to perform retrospective analysis of the drug titration processes on 253 patients diagnosed of diabetes mellitus type 2 and treated with metformin, 321 patients treated of chonic heart failure with furosemide, 155 patients with hyperuricemia treated with allopurinol as initial drug and febuxostat as alternative drug, and 187 hyperuricemia patients with primary drug allopurinol and supplementary drug probenecid, in order to identify different types of drug titration deviations from standard DT methods.}
}
@article{WANG2023101535,
title = {A knowledge graph for standard carbonate microfacies and its application in the automatical reconstruction of the relative sea-level curve},
journal = {Geoscience Frontiers},
volume = {14},
number = {5},
pages = {101535},
year = {2023},
issn = {1674-9871},
doi = {https://doi.org/10.1016/j.gsf.2023.101535},
url = {https://www.sciencedirect.com/science/article/pii/S1674987123000026},
author = {Han Wang and Hanting Zhong and Anqing Chen and Keran Li and Hang He and Zhe Qi and Dongyu Zheng and Hongyi Zhao and Mingcai Hou},
keywords = {Sea-level curve, Standard microfacies, Knowledge graph, Ediacaran, Dengying Formation},
abstract = {The reconstruction of high-resolution sea-level variation curves in deep time based on the standard carbonate microfacies knowledge graph (SMFKG) is of great scientific significance for exploring the Earth system evolution and predicting future sea-level and climate changes. In this study, the concepts, attributes, and relationships among standard carbonate microfacies (SMF) are comprehensively analyzed; an ontology layer is established and its data layer is constructed using thin-section descriptions; and finally, the SMFKG is established. Additionally, based on the knowledge graph, an application for automatically identifying SMF using identification markers and reconstructing the high-resolution relative sea-level variation curve using the SMF and facies zones is compiled. Then, all thin sections of the late Ediacaran Dengying Formation in the western margin of the Yangtze Platform are observed and described in detail, the SMF and facies zones are identified automatically, and the relative sea-level curve is reconstructed automatically using the SMFKG. The reconstruction results show that the Yangtze Platform experienced four sea-level rise and fall cycles in the late Ediacaran, of which two intense regressions led to subaerial-exposed unconformities in the interior and top of the Dengying Formation, which is highly consistent with previous research results. This shows that the high-resolution relative sea-level variation curve in deep time can be reconstructed efficiently and intelligently using the SMFKG. Additionally, in the near future, the combination of an automatic digital slide-scanning system, machine-learning techniques, and the SMFKG can achieve one-stop fully automatic SMF recognition and reconstruction of high-resolution relative sea-level variation curves in deep time, which has a high application value.}
}
@article{WARREN2024104587,
title = {Advocacy through sociodramatic scriptwriting: Figured worlds of multilingualism in online language teacher education},
journal = {Teaching and Teacher Education},
volume = {144},
pages = {104587},
year = {2024},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2024.104587},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X24001197},
author = {Amber N. Warren and Fares J. Karam},
keywords = {Advocacy, Language teacher education, Ideologies, Figured worlds, TESOL, Dialogic learning},
abstract = {Advocacy is increasingly acknowledged as a critical component of language teachers’ work. Yet, how we prepare teachers for such work, particularly in online language teacher education, is underexamined. This article contributes to investigations into the dialogic nature of teacher advocacy by exploring student-created fictional scripts and discussion of 49 teacher candidates over two semesters. Findings illustrate how participants tried out arguments for multilingualism and presented emergent counterarguments to monoglossic language ideologies, shared insights they learned from one another, yet still missed other opportunities for advocacy– with implications for supporting the development of language teacher advocacy.}
}
@article{CURSI2022104444,
title = {Linking external knowledge to heritage BIM},
journal = {Automation in Construction},
volume = {141},
pages = {104444},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104444},
url = {https://www.sciencedirect.com/science/article/pii/S092658052200317X},
author = {Stefano Cursi and Letizia Martinelli and Nicolò Paraciani and Filippo Calcerano and Elena Gigliarelli},
keywords = {BIM, HBIM, Built heritage, Semantic-enrichment, Database design, Ontologies, Interoperability},
abstract = {The application of the Building Information Modelling (BIM) process to Built Heritage (HBIM) is a growing practice in processes and activities aimed at the investigation, documentation, and conservation of architectural heritage. However, it still raises some questions, such as the compatibility between the formalisation of information in the BIM model; and what limitations are inherent in current software implementations to establish a connection with external resources. On this basis, this paper aims at reviewing which methods are currently experimented to improve the level of semantic enrichment and to extend the knowledge representation domain offered today by the most common BIM authoring tools in an HBIM process. The analysis distinguishes two approaches that differ in the role played by external databases and the objectives pursued. Conclusions were drawn highlighting common needs for overcoming technical and conceptual limitations imposed by the use of proprietary BIM authoring tools, and differences in application perspectives and effectiveness.}
}
@article{HUNG2019166,
title = {Handling probabilistic integrity constraints in pay-as-you-go reconciliation of data models},
journal = {Information Systems},
volume = {83},
pages = {166-180},
year = {2019},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2019.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S030643791830320X},
author = {Nguyen Quoc Viet Hung and Matthias Weidlich and Nguyen Thanh Tam and Zoltán Miklós and Karl Aberer and Avigdor Gal and Bela Stantic},
keywords = {Data integration, Probabilistic constraints, Model reconciliation},
abstract = {Data models capture the structure and characteristic properties of data entities, e.g., in terms of a database schema or an ontology. They are the backbone of diverse applications, reaching from information integration, through peer-to-peer systems and electronic commerce to social networking. Many of these applications involve models of diverse data sources. Effective utilisation and evolution of data models, therefore, calls for matching techniques that generate correspondences between their elements. Various such matching tools have been developed in the past. Yet, their results are often incomplete or erroneous, and thus need to be reconciled, i.e., validated by an expert. This paper analyses the reconciliation process in the presence of large collections of data models, where the network induced by generated correspondences shall meet consistency expectations in terms of integrity constraints. We specifically focus on how to handle data models that show some internal structure and potentially differ in terms of their assumed level of abstraction. We argue that such a setting calls for a probabilistic model of integrity constraints, for which satisfaction is preferred, but not required. In this work, we present a model for probabilistic constraints that enables reasoning on the correctness of individual correspondences within a network of data models, in order to guide an expert in the validation process. To support pay-as-you-go reconciliation, we also show how to construct a set of high-quality correspondences, even if an expert validates only a subset of all generated correspondences. We demonstrate the efficiency of our techniques for real-world datasets comprising database schemas and ontologies from various application domains.}
}
@article{DUNLAP202241,
title = {Is the Information-Theoretic Interpretation of Quantum Mechanics an ontic structural realist view?},
journal = {Studies in History and Philosophy of Science},
volume = {91},
pages = {41-48},
year = {2022},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2021.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0039368121001837},
author = {Lucas Dunlap},
abstract = {The Information-Theoretic Interpretation of Quantum Mechanics from (Bub & Pitowsky, 2010) has been criticized in two ways related to the ontological picture it supplies. This paper explores whether Ontic Structural Realism can supplement the metaphysics of ITIQM in a way that would satisfy its critics. The many similarities between the two views are detailed. And it is argued that the ITIQM view ca. 2010 does seem to be compatible with OSR, but as the view evolved in Bub's Bananaworld (2016), its fundamental metaphysical commitments shifted, making it a less clean fit with OSR.}
}
@article{BRASE20241923,
title = {Digital chemistry: navigating the confluence of computation and experimentation – definition, status quo, and future perspective},
journal = {Digital Discovery},
volume = {3},
number = {10},
pages = {1923-1932},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00130c},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24001530},
author = {Stefan Bräse},
abstract = {Digital chemistry represents a transformative approach integrating computational methods, digital data, and automation within the chemical sciences. It is defined by using digital toolkits and algorithms to simulate, predict, accelerate, and analyze chemical processes and properties, augmenting traditional experimental methods. The current status quo of digital chemistry is marked by rapid advancements in several key areas: high-throughput screening, machine learning models, quantum chemistry, and laboratory automation. These technologies have enabled unprecedented speeds in discovering and optimizing new molecules, materials, and reactions. Digital retrosynthesis and structure–active prediction tools have supported these endeavors. Furthermore, integrating large-language models and robotics in chemistry labs (e.g. demonstrated in self-driving labs) have begun to automate routine tasks and complex decision-making processes. Looking forward, the future of digital and digitalized chemistry is poised for significant growth, driven by the increasing accessibility of computational resources, the expansion of chemical databases, and the refinement of artificial intelligence algorithms. This evolution promises to accelerate innovation in drug discovery, materials science, and sustainable manufacturing, ultimately leading to more efficient, cost-effective, and environmentally friendly chemical research and production. The challenge lies in advancing the technology itself, fostering interdisciplinary collaboration, and ensuring the ethical use of digital tools in chemical research.}
}
@article{WEIGAND2020101437,
title = {Shared Ledger Accounting — Implementing the Economic Exchange pattern},
journal = {Information Systems},
volume = {90},
pages = {101437},
year = {2020},
note = {Advances in Information Systems Engineering Best Papers of CAiSE 2018},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2019.101437},
url = {https://www.sciencedirect.com/science/article/pii/S0306437919304892},
author = {Hans Weigand and Ivars Blums and Joost de Kruijff},
keywords = {Accounting ontology, Smart contracts, UFO ontology, Blockchain},
abstract = {Distributed Ledger Technology (DLT) suggests a new way to implement Accounting Information Systems, but an ontologically sound consensus-based design is missing to date. Against this research gap, the paper introduces a DLT-based shared ledger solution in a formal way and compliant with Financial Reporting Standards. We build on the COFRIS accounting ontology (grounded on UFO) and the blockchain ontology developed by De Kruijff & Weigand that distinguishes between a Datalogical level, an Infological and an Essential (conceptual) level. It is shown how both consensual and enterprise-specific parts of the business exchange transaction can be represented in a concise way, and how this pattern can be implemented using Smart Contracts. It is argued that the proposed Shared Ledger Accounting system increases the quality of the contents from an accounting perspective as well as the quality of the system in terms of auditability and interoperability.}
}
@article{OFER20211750,
title = {The language of proteins: NLP, machine learning & protein sequences},
journal = {Computational and Structural Biotechnology Journal},
volume = {19},
pages = {1750-1758},
year = {2021},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2021.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S2001037021000945},
author = {Dan Ofer and Nadav Brandes and Michal Linial},
keywords = {Natural language processing, Deep learning, Language models, BERT, Bag of words, Tokenization, Word embedding, Contextualized embedding, Transformer, Artificial neural networks, Word2vec, Bioinformatics},
abstract = {Natural language processing (NLP) is a field of computer science concerned with automated text and language analysis. In recent years, following a series of breakthroughs in deep and machine learning, NLP methods have shown overwhelming progress. Here, we review the success, promise and pitfalls of applying NLP algorithms to the study of proteins. Proteins, which can be represented as strings of amino-acid letters, are a natural fit to many NLP methods. We explore the conceptual similarities and differences between proteins and language, and review a range of protein-related tasks amenable to machine learning. We present methods for encoding the information of proteins as text and analyzing it with NLP methods, reviewing classic concepts such as bag-of-words, k-mers/n-grams and text search, as well as modern techniques such as word embedding, contextualized embedding, deep learning and neural language models. In particular, we focus on recent innovations such as masked language modeling, self-supervised learning and attention-based models. Finally, we discuss trends and challenges in the intersection of NLP and protein research.}
}
@article{CAGNONI2020238,
title = {Emotion-based analysis of programming languages on Stack Overflow},
journal = {ICT Express},
volume = {6},
number = {3},
pages = {238-242},
year = {2020},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2020.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405959520301077},
author = {Stefano Cagnoni and Lorenzo Cozzini and Gianfranco Lombardo and Monica Mordonini and Agostino Poggi and Michele Tomaiuolo},
keywords = {Programming languages, Emotion detection, Machine learning},
abstract = {When developing a software engineering project, selecting the most appropriate programming language is a crucial step. Most often, feeling at ease with the possible options becomes almost as relevant as the technical features of the language. Therefore, it appears to be worth analyzing the role that the emotional component plays in this process. In this article, we analyze the trend of the emotions expressed by developers in 2018 on the Stack Overflow platform in posts concerning 26 programming languages. To do so, we propose a learning model trained by distant supervision and the comparison of two different classifier architectures.}
}
@article{HUANG2024104741,
title = {HEART: Learning better representation of EHR data with a heterogeneous relation-aware transformer},
journal = {Journal of Biomedical Informatics},
volume = {159},
pages = {104741},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104741},
url = {https://www.sciencedirect.com/science/article/pii/S153204642400159X},
author = {Tinglin Huang and Syed Asad Rizvi and Rohan Krishna Thakur and Vimig Socrates and Meili Gupta and David {van Dijk} and R. Andrew Taylor and Rex Ying},
keywords = {EHR, Pretrained language model},
abstract = {Objective:
Pretrained language models have recently demonstrated their effectiveness in modeling Electronic Health Record (EHR) data by modeling the encounters of patients as sentences. However, existing methods fall short of utilizing the inherent heterogeneous correlations between medical entities—which include diagnoses, medications, procedures, and lab tests. Existing studies either focus merely on diagnosis entities or encode different entities in a homogeneous space, leading to suboptimal performance. Motivated by this, we aim to develop a foundational language model pre-trained on EHR data with explicitly incorporating the heterogeneous correlations among these entities.
Methods:
In this study, we propose HEART, a heterogeneous relation-aware transformer for EHR. Our model includes a range of heterogeneous entities within each input sequence and represents pairwise relationships between entities as a relation embedding. Such a higher-order representation allows the model to perform complex reasoning and derive attention weights in the heterogeneous context. Additionally, a multi-level attention scheme is employed to exploit the connection between different encounters while alleviating the high computational costs. For pretraining, HEART engages with two tasks, missing entity prediction and anomaly detection, which both effectively enhance the model’s performance on various downstream tasks.
Results:
Extensive experiments on two EHR datasets and five downstream tasks demonstrate HEART’s superior performance compared to four SOTA foundation models. For instance, HEART achieves improvements of 12.1% and 4.1% over Med-BERT in death and readmission prediction, respectively. Additionally, case studies show that HEART offers interpretable insights into the relationships between entities through the learned relation embeddings.
Conclusion:
We study the problem of EHR representation learning and propose HEART, a model that leverages the heterogeneous relationships between medical entities. Our approach includes a multi-level encoding scheme and two specialized pretrained objectives, designed to boost both the efficiency and effectiveness of the model. We have comprehensively evaluated HEART across five clinically significant downstream tasks using two EHR datasets. The experimental results verify the model’s great performance and validate its practical utility in healthcare applications. Code: https://github.com/Graph-and-Geometric-Learning/HEART.}
}
@article{LAJEVARDI201964,
title = {A semantic-based correlation approach for detecting hybrid and low-level APTs},
journal = {Future Generation Computer Systems},
volume = {96},
pages = {64-88},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.01.056},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18314924},
author = {Amir Mohammadzade Lajevardi and Morteza Amini},
keywords = {Advanced persistent threat, Low-level attack, Ontology, Semantic-based correlation, Malware},
abstract = {Sophisticated and targeted malwares, which today are known as Advanced Persistent Threats (APTs), use multi-step, distributed, hybrid and low-level patterns to leak and exfiltrate information, manipulate data, or prevent progression of a program or mission. Since current intrusion detection systems (IDSs) and alert correlation systems do not correlate low-level operating system events with network events and use alert correlation instead of event correlation, the intruders use low and hybrid events in order to distribute the attack vector, hide malwares behaviors, and therefore make detection difficult for such detection systems. In this paper, a new approach for detecting hybrid and low-level attacks, which are prevalent in APTs, is proposed. The proposed approach uses low-level interception and correlates operating system events with network events based on the semantic relationships that are defined between the entities in system ontology. In this scheme, malicious events, especially the events implicitly violate the security policies, are deduced and detected based on the event relations and defined security policies. Also, the proposed approach can track information flows between the existing subjects using a memory transition/manipulation model to reconstruct distributed attack vectors. Evaluation of the proposed approach on a computer network which contains many APTs scenarios shows the effectiveness of our detection approach.}
}
@article{REN2024127253,
title = {Product promotion copywriting from multimodal data: New benchmark and model},
journal = {Neurocomputing},
volume = {575},
pages = {127253},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127253},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224000249},
author = {Jinjin Ren and Liming Lin and Wei Zheng},
keywords = {Dataset, Multimodal, Multi-structured information},
abstract = {In our latest project, we devise a comprehensive corpus for product promotion text generation, named Video-Enabled Product Promotion Corpus (VPPC), which integrates multimodal and multi-structural information of products such as visual spatial details and fine structural specifics. It is crucial to highlight that this is one of the largest datasets available in the field of video captioning. Notably, conventional multimodal text generation often focuses on regular descriptions of entities and events, which doesn not suffice the real-world requirements of product promotion copywriting, as it necessitates a more lively language style and a high degree of authenticity. Regrettably, there is an evident lack of reusable evaluation frameworks and sufficient datasets at the current stage. To address these challenges, we have proposed a unique baseline approach and authenticity evaluation metric, both tailored to meet the realistic demands of our dataset. The results are promising, as our method surpasses previous approaches across all evaluation metrics.}
}
@article{AHANI2021519,
title = {Evaluating medical travelers’ satisfaction through online review analysis},
journal = {Journal of Hospitality and Tourism Management},
volume = {48},
pages = {519-537},
year = {2021},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2021.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1447677021001261},
author = {Ali Ahani and Mehrbakhsh Nilashi and Waleed Abdu Zogaan and Sarminah Samad and Nojood O. Aljehane and Ashwaq Alhargan and Saidatulakmal Mohd and Hossein Ahmadi and Louis Sanzogni},
keywords = {Medical tourism, Online reviews, Big social data, Semantic filtering, Ontology, Traveler satisfaction},
abstract = {Medical tourism is increasing quickly since it contributes to both the health and tourism sectors. The use of big social data has been effective in the development of medical tourism as a huge amount of data is produced and shared by travelers about the services through different social media platforms. Indeed, communicative information and knowledge can be mined from a large amount of information provided by travelers about medical tourism services. It is important to analyze such data to understand the customers' satisfaction level and their demands. Although several studies have been conducted to find the factors influencing customer satisfaction in medical tourism, there is a lack of studies about big social data and online behavioral analysis of medical travelers. In addition, the analysis of customers' online reviews is fairly unexplored by machine learning techniques in the context of medical tourism. Hence, this research aims to fill this gap and develop a new method to reveal travelers' choice preferences and satisfaction with medical tourism services through the analysis of the online review. Text mining and ontology approaches are used in the proposed method. The method can mine data from medical tourism websites, discover the satisfaction dimensions, and reveal the satisfaction level of medical tourists through textual reviews. We rely on the demographic information of medical tourists and ontological semantic filtering approaches to better detect the travelers’ preferences in medical tourism websites. The proposed method is evaluated through the numerical and textual reviews obtained from medical tourism websites. The results of data analysis showed that the proposed method is effective for big data analysis in the medical tourism context and may help medical tourism organizers to improve their medical tourism services to obtain a high level of medical travelers' satisfaction.}
}
@article{FATHY2025103126,
title = {A comprehensive review of ICU readmission prediction models: From statistical methods to deep learning approaches},
journal = {Artificial Intelligence in Medicine},
volume = {165},
pages = {103126},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2025.103126},
url = {https://www.sciencedirect.com/science/article/pii/S0933365725000612},
author = {Waleed Fathy and Guillaume Emeriaud and Farida Cheriet},
keywords = {Intensive care unit, Readmission, Review, Machine learning, Deep learning},
abstract = {The prediction of Intensive Care Unit (ICU) readmission has become a crucial area of research due to the increasing demand for ICU resources and the need to provide timely interventions to critically ill patients. In recent years, several studies have explored the use of statistical, machine learning (ML), and deep learning (DL) models to predict ICU readmission. This review paper presents an extensive overview of these studies and discusses the challenges associated with ICU readmission prediction. We categorize the studies based on the type of model used and evaluate their strengths and limitations. We also discuss the performance metrics used to evaluate the models and their potential clinical applications. In addition, this review explores current methodologies, data usage, and recent advances in interpretability and explainable AI for medical applications, offering insights to guide future research and development in this field. Finally, we identify gaps in the current literature and provide recommendations for future research. Recent advances like ML and DL have moderately improved the prediction of the risk of ICU readmission. However, more progress is needed to reach the precision required to build computerized decision support tools.}
}
@article{RAFID2025106198,
title = {Simulation as a decision-support tool in construction project management: Simphony-Dynamic-as-a-Service},
journal = {Automation in Construction},
volume = {175},
pages = {106198},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106198},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525002389},
author = {Muhtasim Fuad Rafid and Stephen Hague and Simaan AbouRizk and Eleni Stroulia},
keywords = {Construction simulation, Project management, Building information modeling},
abstract = {Simulation can play a key role in construction project management, enabling decision-makers to forecast the impact of real-world events, alternative scheduling, and resource-allocation decisions. Such simulation-based forecasting can substantially inform project management but has not yet been widely adopted in the construction industry. This paper describes Simphony-Dynamic-as-a-Service, an advanced simulation tool that bridges the gap between simulation software and real-world construction practices. The contributions are the following: Re-architecting Simphony Dynamicinto a set of coordinated cloud-based micro-services. Grounding the construction project data in a well-defined project-modeling language — DiCon. Offering a variety of browser-accessible user interfaces, tailored to the needs of different stakeholders. Incorporating critical-path computation and comparisons to assist in identifying critical tasks and evaluating alternative schedules. The platform provides APIs for real-time project progress tracking, enhancing monitoring capabilities, and enabling model re-simulation from any point in time for exploring what-if scenarios and facilitating informed decision-making.}
}
@article{FLOTYNSKI2021766,
title = {Knowledge-Based Management of Virtual Training Scenarios},
journal = {Procedia Computer Science},
volume = {192},
pages = {766-775},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.079},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015672},
author = {Jakub Flotyński and Krzysztof Walczak and Paweł Sobociński and Adam Gałązkiewicz},
keywords = {semantic web, knowledge representation, ontologies, training, virtual reality, 3D content},
abstract = {Virtual reality (VR) gains increasing attention as a method of implementing training systems in different domains, in particular, when real training is potentially dangerous for the trainees or the environment, or requires expensive equipment. The essential element of professional training is domain-specific knowledge, which can be represented using the semantic web approach. It enables reasoning as well as complex queries against the representation of training scenarios, which can be valuable for teaching purposes. However, the available methods and tools for creating VR training systems do not use semantic knowledge representation. Currently, the creation, modification, and management of training scenarios require skills in programming and computer graphics. Hence, they are unavailable to domain experts without expertise in IT. In this paper, we propose an ontology-based representation and a method of modeling VR training scenarios. In our approach, trainees’ activities, potential mistakes as well as equipment and its possible errors are represented using domain knowledge understandable to domain experts. We illustrate the approach by modeling VR training scenarios for electrical operators of high-voltage installations.}
}
@incollection{NAGAR2025251,
title = {13 - An integrated framework for knowledge graphs based on battery management},
editor = {Rajesh Kumar Dhanaraj and M. Nalini and Malathy Sathyamoorthy and Manar Mohaisen},
booktitle = {Knowledge Graph-Based Methods for Automated Driving},
publisher = {Elsevier},
pages = {251-272},
year = {2025},
isbn = {978-0-443-30040-0},
doi = {https://doi.org/10.1016/B978-0-443-30040-0.00013-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300400000138},
author = {Khushboo Nagar and Ayesha Mandloi and Sumit Jain},
keywords = {Ontology, Knowledge graph, Battery management, Semantic modeling, Graph-based representation, Data integration, Ontological structuring, Graph-based querying mechanisms, Information retrieval, EV battery deployment},
abstract = {The proposed framework addresses the multifaceted challenges associated with EV battery management by consolidating diverse data sources into a unified knowledge graph. Utilizing semantic modeling and graph-based representations, the framework synthesizes fragmented data encompassing battery specifications, charging profiles, usage patterns, environmental conditions, and performance histories. Through ontological structuring and graph-based querying mechanisms, the framework facilitates seamless data retrieval and inference capabilities. By amalgamating information from various sources, including battery manufacturers’ specifications, vehicle telematics, and real-time battery health monitoring systems, the knowledge graph empowers informed decision-making in EV battery deployment, charging strategies, and predictive maintenance.}
}
@article{STORK2019100462,
title = {Semantic annotation of natural history collections},
journal = {Journal of Web Semantics},
volume = {59},
pages = {100462},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300283},
author = {Lise Stork and Andreas Weber and Eulàlia {Gassó Miracle} and Fons Verbeek and Aske Plaat and Jaap {van den Herik} and Katherine Wolstencroft},
keywords = {Linked data, Biodiversity, Natural history collections, Ontologies, Semantic annotation, History of science},
abstract = {Large collections of historical biodiversity expeditions are housed in natural history museums throughout the world. Potentially they can serve as rich sources of data for cultural historical and biodiversity research. However, they exist as only partially catalogued specimen repositories and images of unstructured, non-standardised, hand-written text and drawings. Although many archival collections have been digitised, disclosing their content is challenging. They refer to historical place names and outdated taxonomic classifications and are written in multiple languages. Efforts to transcribe the hand-written text can make the content accessible, but semantically describing and interlinking the content would further facilitate research. We propose a semantic model that serves to structure the named entities in natural history archival collections. In addition, we present an approach for the semantic annotation of these collections whilst documenting their provenance. This approach serves as an initial step for an adaptive learning approach for semi-automated extraction of named entities from natural history archival collections. The applicability of the semantic model and the annotation approach is demonstrated using image scans from a collection of 8, 000 field book pages gathered by the Committee for Natural History of the Netherlands Indies between 1820 and 1850, and evaluated together with domain experts from the field of natural and cultural history.}
}
@article{XIA2022105465,
title = {PFmulDL: a novel strategy enabling multi-class and multi-label protein function annotation by integrating diverse deep learning methods},
journal = {Computers in Biology and Medicine},
volume = {145},
pages = {105465},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105465},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522002578},
author = {Weiqi Xia and Lingyan Zheng and Jiebin Fang and Fengcheng Li and Ying Zhou and Zhenyu Zeng and Bing Zhang and Zhaorong Li and Honglin Li and Feng Zhu},
keywords = {Protein function prediction, Deep learning, Gene ontology, Convolutional neural network, Recurrent neural network},
abstract = {Bioinformatic annotation of protein function is essential but extremely sophisticated, which asks for extensive efforts to develop effective prediction method. However, the existing methods tend to amplify the representativeness of the families with large number of proteins by misclassifying the proteins in the families with small number of proteins. That is to say, the ability of the existing methods to annotate proteins in the ‘rare classes’ remains limited. Herein, a new protein function annotation strategy, PFmulDL, integrating multiple deep learning methods, was thus constructed. First, the recurrent neural network was integrated, for the first time, with the convolutional neural network to facilitate the function annotation. Second, a transfer learning method was introduced to the model construction for further improving the prediction performances. Third, based on the latest data of Gene Ontology, the newly constructed model could annotate the largest number of protein families comparing with the existing methods. Finally, this newly constructed model was found capable of significantly elevating the prediction performance for the ‘rare classes’ without sacrificing that for the ‘major classes’. All in all, due to the emerging requirements on improving the prediction performance for the proteins in ‘rare classes’, this new strategy would become an essential complement to the existing methods for protein function prediction. All the models and source codes are freely available and open to all users at: https://github.com/idrblab/PFmulDL.}
}
@article{LI20221595,
title = {Research on the construction of smart care question answering system based on knowledge graph},
journal = {Procedia Computer Science},
volume = {214},
pages = {1595-1602},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.348},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922020609},
author = {Aihua Li and Qinyan Wei and Che Han and Xinzhu Xing},
keywords = {Smart care, Knowledge graph, Question answering system},
abstract = {With the deepening of aging in China, the demand of intelligent care system for the elderly is increasing day by day. The rapid development of information technology and artificial intelligence technology have gradually promoted the transformation of traditional care services from artificial to intelligent. Based on knowledge graph technology, this study built a knowledge graph model for elderly chronic disease smart care. The specific process includes ontology design, knowledge extraction, knowledge graph construction and question answering system design. The uniqueness of the knowledge graph in this paper is that it uses specific information about the elderly and data from professional care books. It is well guaranteed in practicality and reliability.With the help of the knowledge map constructed in this paper, primary care staff can effectively query high-quality and customized chronic disease care knowledge.}
}
@article{MEDEIROS20243014,
title = {Towards Public Health-Risk Detection and Analysis through Textual Data Mining},
journal = {Procedia Computer Science},
volume = {246},
pages = {3014-3023},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.370},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924024025},
author = {Gabriel H.A. Medeiros and Lina F. Soualmia and Cecilia Zanni-Merk},
keywords = {Knowledge Graphs, COVID-19, Spatiotemporal reasoning, Event-Based Surveillance, Biomedical data},
abstract = {The coronavirus disease (COVID-19) spread rampantly around the world at the beginning of 2020 before the governments of each country could prevent it by making decisions based on medical data analysis. With proper formalization, the terabytes of new textual data available online every day could have been used for the early description and detection of cases of this virus. Since then, the number of Event-Based Surveillance (EBS) applications has increased exponentially. These applications aim to mine channels of unstructured data to detect signs of possible public health events. However, one problem with such systems is the need for expert intervention to define which event will be captured, which relevant terms should be used in the search, and to analyze the events to modify the search procedure constantly. Another problem is that many of these applications do not consider both spatial and temporal characteristics. Addressing such limitations, this article presents a novel approach. We propose the biomedical domain specialization of the Core Propagation Phenomenon Ontology (PropaPhen) to capture spatiotemporal characteristics of the propagation of health-related phenomena. We also propose the Description-Detection-Framework (DDF), which leverages PropaPhen, UMLS, and OpenStreetMaps to detect new medical events automatically. Finally, we demonstrate a use case with experiments on extracts from online newspapers about COVID-19. The results show that DDF can be useful for detecting clusters of suspicious cases of possible emerging health-related phenomena.}
}
@article{BARBAGONZALEZ2019543,
title = {BIGOWL: Knowledge centered Big Data analytics},
journal = {Expert Systems with Applications},
volume = {115},
pages = {543-556},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418305347},
author = {Cristóbal Barba-González and José García-Nieto and María del Mar Roldán-García and Ismael Navas-Delgado and Antonio J. Nebro and José F. Aldana-Montes},
keywords = {Ontology, Big Data analytics, Semantics, Knowledge extraction},
abstract = {Knowledge extraction and incorporation is currently considered to be beneficial for efficient Big Data analytics. Knowledge can take part in workflow design, constraint definition, parameter selection and configuration, human interactive and decision-making strategies. This paper proposes BIGOWL, an ontology to support knowledge management in Big Data analytics. BIGOWL is designed to cover a wide vocabulary of terms concerning Big Data analytics workflows, including their components and how they are connected, from data sources to the analytics visualization. It also takes into consideration aspects such as parameters, restrictions and formats. This ontology defines not only the taxonomic relationships between the different concepts, but also instances representing specific individuals to guide the users in the design of Big Data analytics workflows. For testing purposes, two case studies are developed, which consists in: first, real-world streaming processing with Spark of traffic Open Data, for route optimization in urban environment of New York city; and second, data mining classification of an academic dataset on local/cloud platforms. The analytics workflows resulting from the BIGOWL semantic model are validated and successfully evaluated.}
}
@article{YAGHOOBIRAFI2019342,
title = {An approach to XBRL interoperability based on Ant Colony Optimization algorithm},
journal = {Knowledge-Based Systems},
volume = {163},
pages = {342-357},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.08.038},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118304349},
author = {Kamaleddin Yaghoobirafi and Eslam Nazemi},
keywords = {Extensible Business Reporting Language (XBRL), Ant Colony Optimization (ACO), Semantic, Mapping, Interoperability},
abstract = {Extensible Business Reporting Language (XBRL) is an XML-based language developed for enhancing interoperability among the entities involved in process of business reporting. Although this language is adopted by various regulators all around the world and has contributed greatly to semantic interoperability in this field, the variations between taxonomies and also between elements of instance documents, still cause many inconsistencies between elements. Although some existing approaches suppose the conversion of XBRL to ontologies and then resolve the inconsistencies by applying some mapping techniques, it does not seem practical because of low precision and incompleteness of these conversions. In this paper, a novel approach is proposed which utilizes Ant Colony Optimization (ACO) in order to detect best semantic mappings between inconsistent concepts of two XBRL documents. This approach analyzes the possible mappings with respect to various factors like concept names, all label texts, presentation and calculation hierarchies and so on. This makes the approach capable of finding mappings, which were not easily discoverable otherwise. The proposed approach is implemented and applied to actual XBRL reports. The results are measured with aid of well-known criteria (precision, recall and F-measure) and are compared with the well-known Hungarian algorithm and illustrate the better performance in accordance with these three criteria.}
}
@article{DONG2020102894,
title = {Towards a new typology of meteorological events: A study based on synchronic and diachronic data},
journal = {Lingua},
volume = {247},
pages = {102894},
year = {2020},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2020.102894},
url = {https://www.sciencedirect.com/science/article/pii/S0024384120301029},
author = {Sicong Dong and Chu-Ren Huang and He Ren},
keywords = {Weather event typology, Sinitic languages, Directionality, Fog, Dew, Frost},
abstract = {In this article, we expand the typological studies on weather expressions by bridging linguistic and meteorological ontologies. Based on our investigations into weather words of Sinitic languages from both synchronic and diachronic perspectives, we propose a new weather event typology, typology of meteorological events (TyME), with two binary features, [±Process] and [±Material]. We argue that this typology covers more weather phenomena in a systematic and ontologically transparent way and can benefit synchronic and diachronic studies on weather and language. In addition, a cross-linguistic investigation is conducted on previously less studied meteorological expressions: fog, dew and frost. The results show that fog, dew and frost can be said to fall in the majority of the languages, which seems to contradict their meteorological formation behaviours, but in fact conforms to natural laws. Based on the new weather event typology and analysed data, we discover that fog, dew and frost all correlate with precipitation in terms of directionality and encoding types. The two binary features we propose account for these formerly overlooked weather events as well as others and can provide effective assistance in analysing the mechanisms underlying those seemingly scientifically infelicitous expressions.}
}
@article{LI2024102635,
title = {Quantification as social technology: Integrating studies of quantification with philosophy of technology},
journal = {Technology in Society},
volume = {78},
pages = {102635},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102635},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X24001830},
author = {Weibo Li},
keywords = {Studies of quantification, Social constructivism, Post-phenomenology, Ethics of technology},
abstract = {Studies of quantification are an emerging academic interest due to the datafication and quantification trends in modern society. The existing literature on this interdisciplinary field involves two approaches: a history of science approach and a sociology approach. This article argues for adding perspectives from the philosophy of technology. The philosophy of technology raises otherwise neglected ontological, epistemological, and ethical questions about the technology of quantification. Quantification as social technology is non-neutral and exhibits social constructed features, which beyond the commonly perceived realist opinions. The social-constructed quantification also has the ability to influence social agents, mediating the way humans exist in and understand the life world. Ethical and political questions about quantification call for integration into an already rich ethical and political discourse surrounding technological artifacts, actions, and political agendas.}
}
@article{DIVAN2022298,
title = {Measurement project interoperability for real-time data gathering systems},
journal = {Future Generation Computer Systems},
volume = {129},
pages = {298-314},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.11.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004738},
author = {Mario José Diván and María Laura Sánchez-Reynoso and Silvio Miguel Gonnet},
keywords = {Measurement, Interoperability, Internet-of-Things, Merkle tree, Metadata-guided processing},
abstract = {The Internet-of-Things devices have allowed the increment of the Spatiotemporal resolution in the Real-time Data Gathering Systems. However, this has increased the complexity due to the heterogeneity of sensors. Thus, it is important to know how data can be understood aware of its context at the edge. Also, a transversal perspective is expected for aligning and reusing as many sensors as possible (based on an experimental design). Because of the hardware limitations, the experimental design should be communicated as simple, integrated, and consistent as possible under single content fostering the projects’ complementarity. This work describes a strategy for defining and communicating a set of measurement projects, following a conceptual hierarchy derived from a measurement ontology. Every project definition is obtained from the user requirements following a well-established strategy. BriefPD is introduced as a data interchange format that is self-contained, partially verifiable (through a Merkle tree insight), consistent, and does not require the use of tags to determine the content meaning. BriefPD generates a project definition in 6.27 ms (1.9 times quicker than JSON-equivalent), consuming 20.39% of JSON’s size. A Github library is provided with the reference implementation jointly with a ‘Code Ocean’ capsule for reproducing the simulation results.}
}
@article{GHEISARI2020106504,
title = {An Edge Computing-enhanced Internet of Things Framework for Privacy-preserving in Smart City},
journal = {Computers & Electrical Engineering},
volume = {81},
pages = {106504},
year = {2020},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2019.106504},
url = {https://www.sciencedirect.com/science/article/pii/S0045790618329082},
author = {Mehdi Gheisari and Guojun Wang and Shuhong Chen},
keywords = {Privacy-preserving, Smart city, Ontology, Edge computing, Internet of things, Owner, Privacy, IoT, Cloud computing},
abstract = {To supervise massive generated data by the Internet of Things (IoT) efficiently, we face two issues that should be addressed which are: (1) heterogeneity or satisfying diversity among IoT devices, and (2) privacy-preserving or preventing unintentional disclosure of sensitive data. Through observation, we found that existing solutions apply one common privacy-preserving rule for all devices while they address the heterogeneity issue separately that lead to unappealing performance. In this paper, we propose a framework for addressing the heterogeneity issue and privacy-preserving of IoT devices at the network edge using a novel proposed ontology data model. Besides, it leverages the proposed ontology to obtain a privacy-preserving method by frequently changing the privacy-preserving behaviors of IoT devices. Through simulation, we show that our solution overhead is less than 9 percent in the worst situation so that it is affordable to most IoT devices in one of its applications that is smart city.}
}
@article{LE201849,
title = {Risk prediction using natural language processing of electronic mental health records in an inpatient forensic psychiatry setting},
journal = {Journal of Biomedical Informatics},
volume = {86},
pages = {49-58},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S153204641830162X},
author = {Duy Van Le and James Montgomery and Kenneth C. Kirkby and Joel Scanlan},
keywords = {Text mining, Natural language processing, Electronic health record, Mental health, Psychiatry},
abstract = {Objective
Instruments rating risk of harm to self and others are widely used in inpatient forensic psychiatry settings. A potential alternate or supplementary means of risk prediction is from the automated analysis of case notes in Electronic Health Records (EHRs) using Natural Language Processing (NLP). This exploratory study rated presence or absence and frequency of words in a forensic EHR dataset, comparing four reference dictionaries. Seven machine learning algorithms and different time periods of EHR analysis were used to probe which dictionary and which time period were most predictive of risk assessment scores on validated instruments.
Materials and methods
The EHR dataset comprised de-identified forensic inpatient notes from the Wilfred Lopes Centre in Tasmania. The data comprised unstructured free-text case note entries and serial ratings of three risk assessment scales: Historical Clinical Risk Management-20 (HCR-20), Short-Term Assessment of Risk and Treatability (START) and Dynamic Appraisal of Situational Aggression (DASA). Four NLP dictionary word lists were selected: 6865 mental health symptom words from the Unified Medical Language System (UMLS), 455 DSM-IV diagnoses from UMLS repository, 6790 English positive and negative sentiment words, and 1837 high frequency words from the Corpus of Contemporary American English (COCA). Seven machine learning methods Bagging, J48, Jrip, Logistic Model Trees (LMT), Logistic Regression, Linear Regression and Support Vector Machine (SVM) were used to identify the combination of dictionaries and algorithms that best predicted risk assessment scores.
Results
The most accurate prediction was attained on the DASA dataset using the sentiment dictionary and the LMT and SVM algorithms.
Conclusions
NLP, used in conjunction with NLP dictionaries and machine learning, predicted risk ratings on the HCR-20, START, and DASA, based on EHR content. Further research is required to ascertain the utility of NLP approaches in predicting endpoints of actual self-harm, harm to others or victimisation.}
}
@article{ENGWALL2025101422,
title = {What is the “project”? A typology of approaches to the core concept in project studies},
journal = {Scandinavian Journal of Management},
volume = {41},
number = {2},
pages = {101422},
year = {2025},
issn = {0956-5221},
doi = {https://doi.org/10.1016/j.scaman.2025.101422},
url = {https://www.sciencedirect.com/science/article/pii/S0956522125000272},
author = {Mats Engwall and Maxim Miterev},
keywords = {Project concept, Open concepts, Essentially contested concept, Performative aspects, Temporary organizations, Project management, Unit of analysis, Demarcation},
abstract = {This paper critically revisits the fundamental, defining notion of “project” in project studies. It examines extant approaches to defining projects and illuminates the ontological fuzziness of the main unit of analysis in project studies. Drawing on the notions of open and essentially contested concepts, the paper emphasizes the importance of studying how the project concept has been applied in empirical research. To advance the debate in this direction, two dimensions are emphasized: (1) the “project” as a realization of an object versus the “project” as an actor’s assignment, and (2) the “project” viewed as an empirical phenomenon versus the “project” used as an analytical lens, resulting in a typology of four distinct research approaches to the project concept. By discussing the approaches’ implications for research inquiries on projects and project management, the paper calls for more prudence when applying the “project” as unit of analysis in future empirical research. Consequently, the paper takes a step towards invigorating the once vivid debate on the core, defining concept within project studies.}
}
@article{ALESHINSKAYA2020326,
title = {A cognitive model to enhance professional competence in computer science},
journal = {Procedia Computer Science},
volume = {169},
pages = {326-329},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.191},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303148},
author = {Evgeniya Aleshinskaya and Ahmad Albatsha},
keywords = {Professional communication, terminological system, cognitive approach, thesaurus, frame model},
abstract = {The paper presents the results of the cognitive modeling of the COMPUTER SCIENCE terminological system in the form of a thesaurus. The thesaurus comprises over 3000 units, which are drawn from explanatory monolingual and bilingual dictionaries of computer science terms representing the basic phenomena and processes in the professional context. Methodologically, the analysis is based on the frame model and focuses on semantic relations specific to the sphere of computer science in terms of ontological and epistemological features. The thesaurus facilitates the detailed description and effective arrangement of the terminological system characterized by a complicated hierarchical structure, and thus plays a crucial role in forming and developing professional competencies.}
}
@article{SAPEL2022243,
title = {Towards digital shadows for production planning and control in injection molding},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {38},
pages = {243-251},
year = {2022},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2022.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1755581722000803},
author = {Patrick Sapel and Aymen Gannouni and Judith Fulterer and Christian Hopmann and Mauritius Schmitz and Daniel Lütticke and Andreas Gützlaff and Günther Schuh},
keywords = {Production planning and control, Digital Shadows, Injection molding, Ontology},
abstract = {In dynamic production environments, bringing flexibility to production planning and control (PPC) against continuously changing requirements is key to fulfill production objectives. Such environments often involve various information systems that make the integration of knowledge on the current state of production more challenging. In this paper, we present a conceptual approach of Digital Shadows that allow for a holistic data view on PPC-specific tasks based on an ontology, serving as a knowledge base of semantic-enriched data and production constraints. An exemplary real-world application for PPC in injection molding demonstrates the benefits of the concept.}
}
@article{LOPEZGOPAR2022102022,
title = {Mexican student-teachers’ “English” language praxicum: Decolonizing attempts},
journal = {International Journal of Educational Research},
volume = {115},
pages = {102022},
year = {2022},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2022.102022},
url = {https://www.sciencedirect.com/science/article/pii/S0883035522001008},
author = {Mario E. López-Gopar and William M. Sughrua and Vilma Huerta Cordova and Amairani Jatziri Ríos González and Gerardo Regalado Vera},
abstract = {The teaching of English in Mexican elementary schools is emblematic of colonial practices. Counteracting this trend, this paper presents results of an “English” teaching praxicum which, allowing for decolonizing subjectivities, was conducted by two of the authors of this paper (Jatziri and Gerardo), Mexican student teachers in a low-socio-economic elementary school in Oaxaca, Mexico. Utilizing decolonizing pedagogies as a theoretical framework and critical ethnography as methodology, this paper presents the childhood life stories of Jatziri and Gerardo, and their decolonizing attempts during their language teaching praxicum through their dialogue with a student. Findings suggest these student teachers developed acute awareness of the diversity in the classroom through reflexive teaching. Hence, language education programs with decolonizing agendas should include reflexive teaching.}
}
@article{SIGNORELLI2021103168,
title = {Reasoning about conscious experience with axiomatic and graphical mathematics},
journal = {Consciousness and Cognition},
volume = {95},
pages = {103168},
year = {2021},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2021.103168},
url = {https://www.sciencedirect.com/science/article/pii/S1053810021000945},
author = {Camilo Miguel Signorelli and Quanlong Wang and Bob Coecke},
keywords = {Consciousness, Conscious experience, Conscious agents, Compositionality, Graphical calculi, Mathematical consciousness science, Monoidal categories, Phenomenology, Unity of consciousness},
abstract = {We cast aspects of consciousness in axiomatic mathematical terms, using the graphical calculus of general process theories (a.k.a symmetric monoidal categories and Frobenius algebras therein). This calculus exploits the ontological neutrality of process theories. A toy example using the axiomatic calculus is given to show the power of this approach, recovering other aspects of conscious experience, such as external and internal subjective distinction, privacy or unreadability of personal subjective experience, and phenomenal unity, one of the main issues for scientific studies of consciousness. In fact, these features naturally arise from the compositional nature of axiomatic calculus.}
}
@article{ISLAM2022e08892,
title = {Network based systems biology approach to identify diseasome and comorbidity associations of Systemic Sclerosis with cancers},
journal = {Heliyon},
volume = {8},
number = {2},
pages = {e08892},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e08892},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022001803},
author = {Md Khairul Islam and Md. Habibur Rahman and Md Rakibul Islam and Md Zahidul Islam and Md Mainul Islam Mamun and A.K.M. Azad and Mohammad Ali Moni},
keywords = {Bioinformatics, Comorbidities, Associations, Gene, Gene set enrichment analysis, Correlation, Gene ontology, Systemic Sclerosis, Pathways, Cancer},
abstract = {Systemic Sclerosis (SSc) is an autoimmune disease associated with changes in the skin's structure in which the immune system attacks the body. A recent meta-analysis has reported a high incidence of cancer prognosis including lung cancer (LC), leukemia (LK), and lymphoma (LP) in patients with SSc as comorbidity but its underlying mechanistic details are yet to be revealed. To address this research gap, bioinformatics methodologies were developed to explore the comorbidity interactions between a pair of diseases. Firstly, appropriate gene expression datasets from different repositories on SSc and its comorbidities were collected. Then the interconnection between SSc and its cancer comorbidities was identified by applying the developed pipelines. The pipeline was designed as a generic workflow to demonstrate a premise comorbid condition that integrate regarding gene expression data, tissue/organ meta-data, Gene Ontology (GO), Molecular pathways, and other online resources, and analyze them with Gene Set Enrichment Analysis (GSEA), Pathway enrichment and Semantic Similarity (SS). The pipeline was implemented in R and can be accessed through our Github repository: https://github.com/hiddenntreasure/comorbidity. Our result suggests that SSc and its cancer comorbidities share differentially expressed genes, functional terms (gene ontology), and pathways. The findings have led to a better understanding of disease pathways and our developed methodologies may be applied to any set of diseases for finding any association between them. This research may be used by physicians, researchers, biologists, and others.}
}
@article{NADEGGER2023103613,
title = {Reassembling more-than-human sustainability: Relations with snow},
journal = {Annals of Tourism Research},
volume = {101},
pages = {103613},
year = {2023},
issn = {0160-7383},
doi = {https://doi.org/10.1016/j.annals.2023.103613},
url = {https://www.sciencedirect.com/science/article/pii/S0160738323000865},
author = {Monica Nadegger},
keywords = {More-than-human sustainability, Relational ontology, Feminist new materialism, Winter tourism, Narrative methodologies, Snow},
abstract = {This article illuminates the importance of reassembling sustainability from a relational, more-than-human perspective in response to critical debates regarding sustainable development and its underlying managerial paradigms. Theoretically, the study develops the concept of more-than-human sustainability by building upon relational ontologies and feminist, new materialist work. Empirically, the study draws upon ethnographic fieldwork of shadowing snow in ski areas in the Tyrolean Alps. Integrating more-than-human relations in winter tourism through speculative storytelling, the findings uncover the hidden relations of waiting-for, connecting-to, and thriving-with snow, examining how these perspectives disrupt capitalist modes of operation and open alternative approaches to sustainability in future winter tourism. The study concludes by examining alternative approaches to sustainable world-making by critically engaging with more-than-human relations in tourism.}
}
@article{WU2019,
title = {Efficient Reuse of Natural Language Processing Models for Phenotype-Mention Identification in Free-text Electronic Medical Records: A Phenotype Embedding Approach},
journal = {JMIR Medical Informatics},
volume = {7},
number = {4},
year = {2019},
issn = {2291-9694},
doi = {https://doi.org/10.2196/14782},
url = {https://www.sciencedirect.com/science/article/pii/S2291969419001121},
author = {Honghan Wu and Karen Hodgson and Sue Dyson and Katherine I Morley and Zina M Ibrahim and Ehtesham Iqbal and Robert Stewart and Richard JB Dobson and Cathie Sudlow},
keywords = {natural language processing, text mining, phenotype, word embedding, phenotype embedding, model adaptation, electronic health records, machine learning, clustering},
abstract = {Background
Much effort has been put into the use of automated approaches, such as natural language processing (NLP), to mine or extract data from free-text medical records in order to construct comprehensive patient profiles for delivering better health care. Reusing NLP models in new settings, however, remains cumbersome, as it requires validation and retraining on new data iteratively to achieve convergent results.
Objective
The aim of this work is to minimize the effort involved in reusing NLP models on free-text medical records.
Methods
We formally define and analyze the model adaptation problem in phenotype-mention identification tasks. We identify “duplicate waste” and “imbalance waste,” which collectively impede efficient model reuse. We propose a phenotype embedding–based approach to minimize these sources of waste without the need for labelled data from new settings.
Results
We conduct experiments on data from a large mental health registry to reuse NLP models in four phenotype-mention identification tasks. The proposed approach can choose the best model for a new task, identifying up to 76% waste (duplicate waste), that is, phenotype mentions without the need for validation and model retraining and with very good performance (93%-97% accuracy). It can also provide guidance for validating and retraining the selected model for novel language patterns in new tasks, saving around 80% waste (imbalance waste), that is, the effort required in “blind” model-adaptation approaches.
Conclusions
Adapting pretrained NLP models for new tasks can be more efficient and effective if the language pattern landscapes of old settings and new settings can be made explicit and comparable. Our experiments show that the phenotype-mention embedding approach is an effective way to model language patterns for phenotype-mention identification tasks and that its use can guide efficient NLP model reuse.}
}
@article{LAURIOLA2022443,
title = {An introduction to Deep Learning in Natural Language Processing: Models, techniques, and tools},
journal = {Neurocomputing},
volume = {470},
pages = {443-456},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.05.103},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221010997},
author = {Ivano Lauriola and Alberto Lavelli and Fabio Aiolli},
keywords = {Deep Learning, Natural Language Processing, Transformer, Language Models, Software},
abstract = {Natural Language Processing (NLP) is a branch of artificial intelligence that involves the design and implementation of systems and algorithms able to interact through human language. Thanks to the recent advances of deep learning, NLP applications have received an unprecedented boost in performance. In this paper, we present a survey of the application of deep learning techniques in NLP, with a focus on the various tasks where deep learning is demonstrating stronger impact. Additionally, we explore, describe, and revise the main resources in NLP research, including software, hardware, and popular corpora. Finally, we emphasize the main limits of deep learning in NLP and current research directions.}
}
@article{BONDUEL202034,
title = {Including widespread geometry schemas into Linked Data-based BIM applied to built heritage},
journal = {Proceedings of the Institution of Civil Engineers - Smart Infrastructure and Construction},
volume = {172},
number = {1},
pages = {34-51},
year = {2020},
issn = {2397-8759},
doi = {https://doi.org/10.1680/jsmic.19.00014},
url = {https://www.sciencedirect.com/science/article/pii/S2397875920000022},
author = {Mathias Bonduel and Anna Wagner and Pieter Pauwels and Maarten Vergauwen and Ralf Klein},
keywords = {Building Information Modelling (BIM), conservation, knowledge management},
abstract = {A reliable data exchange often including geometry-related data between stakeholders is crucial in construction projects. In this regard, data exchange frameworks built on Linked Data principles are very promising for combining disparate data sets. However, existing proposals to combine geometry and Linked Data either demand dedicated applications or support only a limited number of common geometry schemas. If any existing geometry schema could be used in a Linked Data context, error-prone geometry conversions are avoided and stakeholders do not need to invest in new geometry engines. In this paper, the applicability of Resource Description Framework (RDF) literals for including a wide variety of existing geometry schemas is studied and applied in a built heritage context. The uniform linking pattern and related terminology of the Ontology for Managing Geometry are used to implement this approach. Subsequently, the File Ontology for Geometry formats and Geometry Metadata Ontology are developed to ease the reuse of linked geometry descriptions. The effectiveness of the entire data structure is demonstrated in a built heritage case study project. The receiving party is able to create successfully a coordinated view using a demo web application on shared, but disparate, RDF data sets containing geometry descriptions.}
}
@article{VJESTICA2021101053,
title = {Multi-level production process modeling language},
journal = {Journal of Computer Languages},
volume = {66},
pages = {101053},
year = {2021},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2021.101053},
url = {https://www.sciencedirect.com/science/article/pii/S2590118421000320},
author = {Marko Vještica and Vladimir Dimitrieski and Milan Pisarić and Slavica Kordić and Sonja Ristić and Ivan Luković},
keywords = {Production process modeling, Model-Driven engineering, Domain-Specific Modeling Languages, Industry 4.0, Process execution, Evaluation},
abstract = {The fourth industrial revolution introduces changes in traditional manufacturing systems and creates basis for a lot-size-one production. The complexity of production processes is significantly increased, alongside the need to enable efficient process simulation, execution, monitoring, real-time decision making and control. The main goal of our research is to define a methodological approach and a software solution in which the Model-Driven (MD) principles and Domain-Specific Modeling Languages (DSMLs) are used to create a framework for the formal description and automatic execution of production processes. In that way production process models are used as central artifacts to manage the production. In this paper, we analyze production process modeling domain and present a DSML which can be used to create production process models suitable for automatic generation of executable code. The generated code is used for automatic execution of production processes within a simulation or a shop floor. The language can be used to specify errors that may occur during the process execution and to specify error handling and corrective steps, too. The DSML is evaluated by different groups of users and the evaluation results are presented. Both the DSML and the accompanying modeling tool are still in the prototype phase, as they are created and evaluated in use cases covering just the assembly of goods. To enable wider application of the language and the tool, it is required to have additional use cases from different manufacturing domains.}
}
@article{MARKOVIC202427,
title = {Where is the past? Time in historical geography},
journal = {Journal of Historical Geography},
volume = {84},
pages = {27-36},
year = {2024},
issn = {0305-7488},
doi = {https://doi.org/10.1016/j.jhg.2024.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0305748824000264},
author = {Ivan Marković},
keywords = {Onto-epistemology, Past, Time, Specious present, Historical geography, Presentism, Practice},
abstract = {Despite human geography's sophisticated analyses and overwhelming focus on space, time in its various guises has certainly not been absent in the literature. The same cannot be said for historical geography, which is particularly interesting as its main concern is purportedly with space and place in and across other times. In response, this paper examines the ontology and epistemology of time in “modern” historical geography since the early 2000s and does so in discussion with recent developments in theory and philosophy of history, specifically the notion of ‘new presentism’. An idea which broadly posits that the past and the future do not exist as separate categories but are always projections of specific presents, they exist as the present's own immanent modes. This is achieved by adopting Robert Dodgshon's concept of the ‘specious present’ in order to (1) affirm, albeit on different epistemological grounds, the partiality, situatedness and contingency of historical geographies as well as the embodied and performative nature of archival labour; (2) offer an accessible conceptual tool in thinking about the role of time in the practice of future historical geography research; and finally (3) suggest that thinking historical geography as a practice in and through the ‘specious present’ makes questions of ethics, accountability, and politics of knowledge production both central and inevitable, as opposed to just being examples of “good practice” or worse still, being completely sidestepped by virtue of an imagined spatio-temporal distance between the bygone past and the present moment of research.}
}
@article{VYAS20252290,
title = {Enhancing Digital Forensics: Machine Learning Techniques for Social Media Investigation},
journal = {Procedia Computer Science},
volume = {258},
pages = {2290-2301},
year = {2025},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.483},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925015868},
author = {Dhairya Vyas and Milind Shah and Ankita Kothari and Jyoti Golakia and Vruti Parikh},
keywords = {Digital Forensics, Machine Learning, Social Media Investigation, Cyber Crime Investigation, Data Analysis},
abstract = {Due to the increasing growth of social media platforms, improved methods are needed to extract, examine, and interpret digital evidence. Due to the vastness and ever-changing nature of the data that is collected via social media platforms, traditional forensic procedures usually meet challenges. Specifically created for the purpose of conducting forensic analysis of material obtained from social media platforms, the purpose of this research is to build and analyze machine learning models and algorithms. Gathering information from various platforms, such as Kaggle, is the first step in the process. The succeeding processes involve data cleaning and categorization, which includes activities such as text normalization, entity recognition, and sentiment analysis. These steps come after the first data gathering has been completed. The extraction of relevant information, such as user behaviors, temporal trends, and network connections, is accomplished through the use of feature engineering. A significant portion of the investigation is centered on the application of supervised machine learning techniques for the purpose of detecting users and detecting offensive language. In order to determine whether or not the proposed approach is effective, comprehensive experiments are conducted utilizing datasets derived from social media activities. The findings indicate that there have been considerable improvements in the capabilities of forensic investigations, including enhanced accuracy, scalability, and automation. In this research we have investigated social media’s data and detected offensive language using various machine learning algorithms such as Support Vector Machine (SVM), K-Nearest Neighbours (KNN), Decision Tree, Random Forest, and Extra Tree. The support vector machine (SVM) achieved 86% accuracy where as KNN achieved only 77% accuracy which was far lower than other learning algorithms. However, the Decision Tree, Random Forest, and Extra Tree models achieved 99% accuracy with good precision, recall, and F1-scores, proving their ability to detect and categorize social media information.}
}
@article{SCHMIDTKE2018896,
title = {Logical lateration – A cognitive systems experiment towards a new approach to the grounding problem},
journal = {Cognitive Systems Research},
volume = {52},
pages = {896-908},
year = {2018},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S1389041717302760},
author = {Hedda R. Schmidtke},
keywords = {Cognitive architecture, Logical reasoning, Mental imagery},
abstract = {Human beings are able to extract linear ordering information, such as preferences, rankings, priorities, temporal ordering, and other numerical or quasi-numerical representations as well as estimations of spatial positions in mental maps or mental images from purely qualitative statements. From the perspectives of both cognitive science and logics, it is an interesting question how such imagery can arise from logical structures as provided by language. This article presents a cognitive system, for brevity called imaginer, which is built around a non-monotonic reasoning mechanism, called logical lateration. The system numerically evaluates model sets of knowledge bases. We show results from an experiment showing that the model sets by themselves, i.e., without any ontology, give rise to context-dependent coordinate representations of a knowledge base, which the system can draw. Being both a logic-based as well as an analogous cognitive system, the experiment provides a fresh perspective on the grounding problem.}
}
@article{GUTIERREZMAQUILON2024,
title = {Integrating GPT-Based AI into Virtual Patients to Facilitate Communication Training Among Medical First Responders: Usability Study of Mixed Reality Simulation},
journal = {JMIR Formative Research},
volume = {8},
year = {2024},
issn = {2561-326X},
doi = {https://doi.org/10.2196/58623},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X24007133},
author = {Rodrigo {Gutiérrez Maquilón} and Jakob Uhl and Helmut Schrom-Feiertag and Manfred Tscheligi},
keywords = {medical first responders, verbal communication skills, training, virtual patient, generative artificial intelligence, GPT, large language models, prompt engineering, mixed reality},
abstract = {Background
Training in social-verbal interactions is crucial for medical first responders (MFRs) to assess a patient’s condition and perform urgent treatment during emergency medical service administration. Integrating conversational agents (CAs) in virtual patients (VPs), that is, digital simulations, is a cost-effective alternative to resource-intensive human role-playing. There is moderate evidence that CAs improve communication skills more effectively when used with instructional interventions. However, more recent GPT-based artificial intelligence (AI) produces richer, more diverse, and more natural responses than previous CAs and has control of prosodic voice qualities like pitch and duration. These functionalities have the potential to better match the interaction expectations of MFRs regarding habitability.
Objective
We aimed to study how the integration of GPT-based AI in a mixed reality (MR)–VP could support communication training of MFRs.
Methods
We developed an MR simulation of a traffic accident with a VP. ChatGPT (OpenAI) was integrated into the VP and prompted with verified characteristics of accident victims. MFRs (N=24) were instructed on how to interact with the MR scenario. After assessing and treating the VP, the MFRs were administered the Mean Opinion Scale-Expanded, version 2, and the Subjective Assessment of Speech System Interfaces questionnaires to study their perception of the voice quality and the usability of the voice interactions, respectively. Open-ended questions were asked after completing the questionnaires. The observed and logged interactions with the VP, descriptive statistics of the questionnaires, and the output of the open-ended questions are reported.
Results
The usability assessment of the VP resulted in moderate positive ratings, especially in habitability (median 4.25, IQR 4-4.81) and likeability (median 4.50, IQR 3.97-5.91). Interactions were negatively affected by the approximately 3-second latency of the responses. MFRs acknowledged the naturalness of determining the physiological states of the VP through verbal communication, for example, with questions such as “Where does it hurt?” However, the question-answer dynamic in the verbal exchange with the VP and the lack of the VP’s ability to start the verbal exchange were noticed. Noteworthy insights highlighted the potential of domain-knowledge prompt engineering to steer the actions of MFRs for effective training.
Conclusions
Generative AI in VPs facilitates MFRs’ training but continues to rely on instructions for effective verbal interactions. Therefore, the capabilities of the GPT-VP and a training protocol need to be communicated to trainees. Future interactions should implement triggers based on keyword recognition, the VP pointing to the hurting area, conversational turn-taking techniques, and add the ability for the VP to start a verbal exchange. Furthermore, a local AI server, chunk processing, and lowering the audio resolution of the VP’s voice could ameliorate the delay in response and allay privacy concerns. Prompting could be used in future studies to create a virtual MFR capable of assisting trainees.}
}
@article{GOMAA20242312,
title = {Exploring Recommender Systems for Assisting Teachers in E-Learning Gamification},
journal = {Procedia Computer Science},
volume = {246},
pages = {2312-2321},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.556},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924026024},
author = {Yara Gomaa and Sherin Moussa and Christine Lahoud and Marie-Hélène Abel},
keywords = {teacher, e-learning, gamification, recommender systems, ontology},
abstract = {Gamification in e-learning systems implies integrating game elements in non-gaming context to attain the learner’s engagement through pre-defined engagement objectives. These objectives impact the selection of the to-be-applied game elements, which typically influences the learner’s behavior within the learning process. In turn, this behavior is expected to help the teachers in reaching their learning objectives. Therefore, involving teachers in the gamification process as they are in the learning process can ensure the coherence between the learning and engagement objectives. However, since teachers often lack familiarity with gamification, they are rarely involved with the gamification process. Accordingly, recommender systems can play a vital role in assisting teachers through gamification. Therefore, this paper investigates the existing recommender systems from teacher’s perspective to highlight the advancements proposed for assisting teachers and examines the utilization of game elements within gamified e-learning systems to identify their types and explore their practical applications. Based on our research results, this paper proposes a (1) game elements categorization based on their practical applications and (2) a layered recommendation process flow that incorporates the different game elements categories, to assist the teachers in gamifying the learning process.}
}
@incollection{ESPOSITO2018103,
title = {Chapter 5 - Smart Access Control Models in Sensor Network},
editor = {Massimo Ficco and Francesco Palmieri},
booktitle = {Security and Resilience in Intelligent Data-Centric Systems and Communication Networks},
publisher = {Academic Press},
pages = {103-122},
year = {2018},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-811373-8},
doi = {https://doi.org/10.1016/B978-0-12-811373-8.00005-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128113738000057},
author = {Christian Esposito and Jian Shen and Chang Choi},
keywords = {Sensor networks, Access control, Ontology, Ontology matching},
abstract = {The upcoming and radical evolution of sensor networks, leading to the novel concept of the Internet of Things, and their application for groundbreaking innovations, such as Smart Cities, encompass the integration of several legacy networks in order to realize large scale networks able to cover a city or even a country. Practical examples in these sense can be found in pollution monitoring, intelligent transportation systems, or smart city government. Since such sensor networks may gather and collect sensitive data on citizens’ habits or used by a malicious adversary to conduct attacks, security is a key concern for such networks, and access control represents a demanding feature in order to provide it. While the current literature is rich with several kinds of models to control access in sensor networks and a novel one is not needed, it is necessary to theorize a means to let different models coexist within a given network. In this paper, we presents such a problem and deal with it by means of a semantic approach focused on ontologies, and how matching different ontological descriptions of access control models so as to make them interoperable.}
}
@article{NELSON2025104115,
title = {From control to care: Trans-hegemonic approaches to just-sustainability transformations},
journal = {Environmental Science & Policy},
volume = {171},
pages = {104115},
year = {2025},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2025.104115},
url = {https://www.sciencedirect.com/science/article/pii/S1462901125001315},
author = {Valerie Nelson},
keywords = {Colonial modernity, Patriarchy, Sustainability transformations, Transformative change, Decoloniality, Relationality},
abstract = {Sustainability transformations are the subject of increasing academic and policy attention, but definitions and practice remain contested. This paper provides a comparative analysis of four meta-reviews of sustainability transformation theorisation to identify new insights on transformative change. The overarching analysis compares four interpretive framings of sustainability transformation theory, in terms of their features and mutual critiques, evolution of the field involving a broadening of disciplines and perspectives toward greater attention to critical and relational social sciences, overlaps and continuing tensions. This article proposes a new interpretive clustering, that foregrounds relational, more-than-human, feminist political ecology, Indigenous and decolonial theorisation in sustainability discourse, and calls for their exploration in future research and action. This is in support of unlearning and unmaking invisible common sense formations that are the underlying common causes (although differentiated in manifestations) of unsustainabilities and which prevent transformative change from occurring. The article goes on to identify principles, practices and capacities for action, especially transdisciplinary action research, offered as non-exhaustive, polythetic dimensions of trans-hegemonic sustainability transformations. The paper concludes with an exploration of justice in relation to sustainability transformations, involving the advancement of shifts from control-based imaginaries to pluriversal, care-based futures.}
}
@article{SCHNEIDER2020103402,
title = {Design of knowledge-based systems for automated deployment of building management services},
journal = {Automation in Construction},
volume = {119},
pages = {103402},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103402},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520309821},
author = {Georg F. Schneider and Georgios D. Kontes and Haonan Qiu and Filipe J. Silva and Mircea Bucur and Jakub Malanik and Zdenek Schindler and Panos Andriopolous and Pablo {de Agustin-Camacho} and Ander Romero-Amorrortu and Gunnar Grün},
keywords = {Building management services, Knowledge-based systems, Energy efficiency, Knowledge engineering, Ontology},
abstract = {Despite its high potential, the building's sector lags behind in reducing its energy demand. Tremendous savings can be achieved by deploying building management services during operation, however, the manual deployment of these services needs to be undertaken by experts and it is a tedious, time and cost consuming task. It requires detailed expert knowledge to match the diverse requirements of services with the present constellation of envelope, equipment and automation system in a target building. To enable the widespread deployment of these services, this knowledge-intensive task needs to be automated. Knowledge-based methods solve this task, however, their widespread adoption is hampered and solutions proposed in the past do not stick to basic principles of state of the art knowledge engineering methods. To fill this gap we present a novel methodological approach for the design of knowledge-based systems for the automated deployment of building management services. The approach covers the essential steps and best practices: (1) representation of terminological knowledge of a building and its systems based on well-established knowledge engineering methods; (2) representation and capturing of assertional knowledge on a real building portfolio based on open standards; and (3) use of the acquired knowledge for the automated deployment of building management services to increase the energy efficiency of buildings during operation. We validate the methodological approach by deploying it in a real-world large-scale European pilot on a diverse portfolio of buildings and a novel set of building management services. In addition, a novel ontology, which reuses and extends existing ontologies is presented.}
}
@incollection{DOJA2025,
title = {Structural models in anthropology},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-443-26629-4.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443266294000022},
author = {Albert Doja},
keywords = {Morphodynamics, Transformation, Myth, Boundary condition, Lévi-Strauss, Structural analysis, Structures, Modeling, Simulation},
abstract = {In this entry, I outline a theoretical path toward a scientific method to map some burning contemporary societal issues. It is aimed to represent an important take on social epistemological and anthropological scientific approaches, based on the anthropological and sociological methodology of the French structural tradition, which may figure out what anthropology is or may be heading today. Lévi-Strauss's procedures of canonical formalization for the structural study of myth are proposed as innovative structural models in anthropology, which may radically help revise anthropological methods and goals to confront the pressing problems of the contemporary world. The morphodynamic formalization of larger unsuspected entities can be modeled, simulated, and visualized by means of these structural models built in software mechanisms, which may allow us to identify instrumental ideologies and to anticipate new social changes as a result of the mediating logical operation of a boundary condition within sociocultural systems and institutional frameworks. In particular, the boundary condition can be thought to anticipate the discursive activation of a particular cultural ideology acting as a hidden agency of instrumental politics.}
}
@article{ZHANG2023105121,
title = {IFC-enabled LCA for carbon assessment in pumped storage hydropower (PSH) with concrete face rockfill dams},
journal = {Automation in Construction},
volume = {156},
pages = {105121},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105121},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523003813},
author = {Shihang Zhang and Sherong Zhang and Zhengqiao Wu and Xiaohua Wang and Zhiyong Jiang and Chao Wang and Guojie Zhao},
keywords = {Industry foundation classes (IFC), Life cycle assessment (LCA), Building information modeling (BIM), Carbon assessment, Pumped storage hydropower (PSH), Concrete face rockfill dam},
abstract = {The construction sector in China released 5.08 billion tons of CO2 in 2020, which accounted for 50.9% of national carbon emissions. With the establishment of a clean energy system, an increasing number of pumped storage hydropower (PSH) plants are scheduled to be constructed for energy storage. However, a gap remains in calculating carbon emissions for each specific building during design and later construction, considering the specific design parameters that affect these emissions. This paper provides a life cycle assessment (LCA) method for integrated carbon assessment for PSH design projects based on industry foundation classes (IFC), and the method is used to calculate the carbon emissions from each building directly at the design stage. PSH project carbon emissions are calculated by estimating the consumption of diesel, electricity and materials, together with the corresponding conversion factors. The proposed method is validated with data from real-world PSH projects and then used to analyze the carbon emissions of different buildings to determine the influence of various factors. This study could benefit future research related to the green evaluation of PSH design projects by improving carbon emission estimation based on IFC.}
}
@article{MALBURG2023106727,
title = {Converting semantic web services into formal planning domain descriptions to enable manufacturing process planning and scheduling in industry 4.0},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106727},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106727},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623009119},
author = {Lukas Malburg and Patrick Klein and Ralph Bergmann},
keywords = {Semantic web services, Industry 4.0, Automated planning, Planning domain definition language, Cyber-physical workflows},
abstract = {To build intelligent manufacturing systems that react flexibly in case of failures or unexpected circumstances, manufacturing capabilities of production systems must be utilized as much as possible. Artificial Intelligence (AI) and, in particular, automated planning can contribute to this by enabling flexible production processes. To efficiently leverage automated planning, an almost complete planning domain description of the real-world is necessary. However, creating such planning descriptions is a demanding and error-prone task that requires high manual efforts even for domain experts. In addition, maintaining the encoded knowledge is laborious and, thus, can lead to outdated domain descriptions. To reduce the high efforts, already existing knowledge can be reused and transformed automatically into planning descriptions to benefit from organization-wide knowledge engineering activities. This paper presents a novel approach that reduces the described efforts by reusing existing knowledge for planning and scheduling in Industry 4.0 (I4.0). For this purpose, requirements for developing a converter that transforms existing knowledge are derived from literature. Based on these requirements, the SWS2PDDL converter is developed that transforms the knowledge into formal Planning Domain Definition Language (PDDL) descriptions. The approach’s usefulness is verified by a practical evaluation with a near real-world application scenario by generating failures in a physical smart factory and evaluating the generated re-planned production processes. When comparing the resulting plan quality to those achieved by using a manually modeled planning domain by a domain expert, the automatic transformation by SWS2PDDL leads to comparable or even better results without requiring the otherwise high manual modeling efforts.}
}
@article{EBSAAD20211483,
title = {Development of a Smart Technique for Mobile Web Services Discovery},
journal = {Computers, Materials and Continua},
volume = {69},
number = {2},
pages = {1483-1501},
year = {2021},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2021.017783},
url = {https://www.sciencedirect.com/science/article/pii/S154622182100446X},
author = {Mohamed Eb-Saad and Yunyoung Nam and Hazem M. El-bakry and Samir Abdelrazek},
keywords = {Cloud, web service, web service discovery, semantic matching, mobile web service discovery},
abstract = {Web service (WS) presents a good solution to the interoperability of different types of systems that aims to reduce the overhead of high processing in a resource-limited environment. With the increasing demand for mobile WS (MWS), the WS discovery process has become a significant challenging point in the WS lifecycle that aims to identify the relevant MWSs that best match the service requests. This discovery process is a resource-consuming task that cannot be performed efficiently in a mobile computing environment due to the limitations of mobile devices. Meanwhile, a cloud computing can provide rich computing resources for mobile environments given its unlimited and easily scalable resources. This paper proposes a semantic WS discovery and invocation framework in mobile environments based on cloud and a relationship-aware matchmaking algorithm. The discovery algorithm enriches MWS and user requests semantically with the functional and non-functional properties of Ontology Web Language for Services, such as Quality of Web Service, device context, and user preferences. The WS repository is filtered based on logical reasoning and a parameter-based matching algorithm to minimize the matching space and improve runtime performance. The cosine similarity between the user request and services repository is then assessed to generate the most relevant WS. The relationships among concepts in the ontology are considered to improve the recall and precision ratio. After the WS discovery process, users can invoke and test these services in a mobile environment through a dynamic user interface. The interface of the invocation process is changed according to the WS description document. An application prototype is also developed to evaluate the framework based on a Cordova cross-mobile development framework.}
}
@article{XIE2024,
title = {Identifying Symptoms Prior to Pancreatic Ductal Adenocarcinoma Diagnosis in Real-World Care Settings: Natural Language Processing Approach},
journal = {JMIR AI},
volume = {3},
year = {2024},
issn = {2817-1705},
doi = {https://doi.org/10.2196/51240},
url = {https://www.sciencedirect.com/science/article/pii/S281717052400005X},
author = {Fagen Xie and Jenny Chang and Tiffany Luong and Bechien Wu and Eva Lustigova and Eva Shrader and Wansu Chen},
keywords = {cancer, pancreatic ductal adenocarcinoma, symptom, clinical note, electronic health record, natural language processing, computerized algorithm, pancreatic cancer, cancer death, abdominal pain, pain, validation, detection, pancreas},
abstract = {Background
Pancreatic cancer is the third leading cause of cancer deaths in the United States. Pancreatic ductal adenocarcinoma (PDAC) is the most common form of pancreatic cancer, accounting for up to 90% of all cases. Patient-reported symptoms are often the triggers of cancer diagnosis and therefore, understanding the PDAC-associated symptoms and the timing of symptom onset could facilitate early detection of PDAC.
Objective
This paper aims to develop a natural language processing (NLP) algorithm to capture symptoms associated with PDAC from clinical notes within a large integrated health care system.
Methods
We used unstructured data within 2 years prior to PDAC diagnosis between 2010 and 2019 and among matched patients without PDAC to identify 17 PDAC-related symptoms. Related terms and phrases were first compiled from publicly available resources and then recursively reviewed and enriched with input from clinicians and chart review. A computerized NLP algorithm was iteratively developed and fine-trained via multiple rounds of chart review followed by adjudication. Finally, the developed algorithm was applied to the validation data set to assess performance and to the study implementation notes.
Results
A total of 408,147 and 709,789 notes were retrieved from 2611 patients with PDAC and 10,085 matched patients without PDAC, respectively. In descending order, the symptom distribution of the study implementation notes ranged from 4.98% for abdominal or epigastric pain to 0.05% for upper extremity deep vein thrombosis in the PDAC group, and from 1.75% for back pain to 0.01% for pale stool in the non-PDAC group. Validation of the NLP algorithm against adjudicated chart review results of 1000 notes showed that precision ranged from 98.9% (jaundice) to 84% (upper extremity deep vein thrombosis), recall ranged from 98.1% (weight loss) to 82.8% (epigastric bloating), and F1-scores ranged from 0.97 (jaundice) to 0.86 (depression).
Conclusions
The developed and validated NLP algorithm could be used for the early detection of PDAC.}
}
@article{QIU2025100691,
title = {Text semantics to controllable design: A residential layout generation method based on stable diffusion model},
journal = {Developments in the Built Environment},
volume = {23},
pages = {100691},
year = {2025},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2025.100691},
url = {https://www.sciencedirect.com/science/article/pii/S2666165925000912},
author = {Zijin Qiu and Jiepeng Liu and Yi Xia and Hongtuo Qi and Pengkun Liu},
keywords = {Residential layout generation, Multimodal generative design, Natural language processing, Stable diffusion model, Knowledge graph},
abstract = {Controlling flexibility and design effectiveness is challenging in artificial intelligence-based residential layout design. Compared to the previous rule-based or graph-based generation methods, this paper proposes a controllable multimodal approach based on the Stable Diffusion model for generating residential layouts. This method incorporates natural language as design constraints and introduces ControlNet, which enables the generation of controllable layouts through two distinct pathways. A knowledge graph and natural language mapping scheme is proposed to provide an interpretable representation of the design knowledge encapsulated in the knowledge graph. The comprehensibility of natural language and the diversity of input options enable professionals and non-professionals to directly express design requirements, thereby providing a flexible and controllable design method. Finally, comparative and ablation experiments verify the controllability and diversity of the two proposed design paths under multimodal constraints.}
}
@article{LEGUILLARME2023103497,
title = {A practical approach to constructing a knowledge graph for soil ecological research},
journal = {European Journal of Soil Biology},
volume = {117},
pages = {103497},
year = {2023},
issn = {1164-5563},
doi = {https://doi.org/10.1016/j.ejsobi.2023.103497},
url = {https://www.sciencedirect.com/science/article/pii/S116455632300033X},
author = {Nicolas {Le Guillarme} and Wilfried Thuiller},
keywords = {Data integration, Knowledge graph, Ontology, Reasoning, Soil ecology},
abstract = {With the rapid accumulation of biodiversity data, data integration has emerged as a hot topic in soil ecology. Data integration has indeed the potential to advance our knowledge of global patterns in soil biodiversity by facilitating large-scale meta-analytical studies of soil ecosystems. However, ecologists are still poorly equipped when it comes to integrating disparate datasets. In recent years, knowledge graphs have emerged as a powerful tool for integrating large amounts of distributed heterogeneous data while making these data more easily interpretable by humans and computers. This paper presents a practical approach to constructing a biodiversity knowledge graph from heterogeneous and distributed (semi-)structured data sources. To illustrate our approach, we integrate several datasets on the trophic ecology of soil organisms into a trophic knowledge graph and show how both explicit and implicit information can be retrieved from the graph to support multi-trophic studies.}
}
@article{MANERO201971,
title = {Imprints of the underlying structure of physical theories},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {68},
pages = {71-89},
year = {2019},
issn = {1355-2198},
doi = {https://doi.org/10.1016/j.shpsb.2019.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1355219818300145},
author = {Jorge Manero},
keywords = {Ontic structural realism, Theory change, Surplus structure, Partial structures, Lie group theory, Symplectic group},
abstract = {In the context of scientific realism, this paper intends to provide a formal and accurate description of the structural-based ontology posited by classical mechanics, quantum mechanics and special relativity, which is preserved across the empirical domains of these theories and explain their successful predictions. Along the lines of ontic structural realism, such a description is undertaken by a particular ontological commitment: the belief in the existence of a freestanding actual structure, approximately represented by a subgroup of the Inhomogeneous Symplectic Group (up to group homomorphisms), and their corresponding state-space representations. Accordingly, the hierarchy and the complexity of this group-theoretical structure is represented by appropriate philosophical tools, namely, by the language of partial structures. Upon this approach, the lack of knowledge of some relations that hold at the boundary between mathematics and physics, and the presence of surplus structure within the structural edifice are explored and represented. The conclusive issue appeals to an interesting example of a surplus but fruitful structure, where superposition of states with different mass are suggested to be actual relativistic remnants within non-relativistic quantum mechanics, as opposed to the standard interpretation in which they are empirically meaningless.}
}
@article{DADZIE201821,
title = {Value-driven partner search for Energy from Waste projects},
journal = {Procedia Computer Science},
volume = {137},
pages = {21-32},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316077},
author = {Aba-Sah Dadzie and Victoria Uren and Tim Miller and Al-Amin Abba-Dabo},
keywords = {Value chains, Ontologies, Business networks, Triple bottom line, KPIs},
abstract = {Energy from Waste (EfW) projects require complex value chains to operate effectively. To identify business partners, plant operators need to network with organisations whose strategic objectives are aligned with their own. Supplier organisations need to work out where they fit in the value chain. Our aim is to support people in identifying potential business partners, based on their organisation’s interpretation of value. Value for an organisation should reflect its strategy and may be interpreted using key priorities and KPIs (key performance indicators). KPIs may comprise any or all of knowledge, operational, economic, social and convenience indicators. This paper presents an ontology for modelling and prioritising connections within the business environment, and in the process provides means for defining value and mapping these to corresponding KPIs. The ontology is used to guide the design of a visual representation of the environment to aid partner search.}
}
@article{DONKERS2025112433,
title = {Personal indoor comfort models through knowledge discovery in cross-domain semantic digital twins},
journal = {Building and Environment},
volume = {269},
pages = {112433},
year = {2025},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2024.112433},
url = {https://www.sciencedirect.com/science/article/pii/S0360132324012745},
author = {Alex Donkers and Dujuan Yang and Bauke {de Vries} and Nico Baken},
keywords = {Semantic web technologies, Linked data, Indoor environmental quality, Occupant feedback, Ontologies, Data mining, BIM},
abstract = {Methods to assess the performance of a building have been developed for decades, however, many buildings still do not satisfy their occupants in their indoor comfort preferences. This paper presents methods to generate insights from semantic digital twins on the perceived comfort levels of individuals to tighten the as-designed and as-perceived building performance gap. This paper first reviews existing personal indoor comfort models and shares state-of-the-art semantic web technologies in this domain. The paper then presents a generic framework to integrate heterogeneous data into knowledge graphs and use them in data mining processes. This framework is then applied to a case study in the Vertigo building in Eindhoven, The Netherlands. A wide range of information is collected, including building information models, indoor and outdoor sensor data, personal information, and feedback on indoor environmental quality. The integrated data are then used to create personal comfort models. First, multinomial logistic regression models are used to predict future dissatisfaction, after which a latent class analysis created cohorts of people with similar indoor comfort preferences. The results are stored back into the knowledge graph, after which they could be used in other applications, such as to perform occupant-centric control of systems. The methods presented in this paper are summarized in a generic framework that can be used and extended to other domains that aim to combine data integration and data mining.}
}
@article{CHEN2024105158,
title = {Augmented reality, deep learning and vision-language query system for construction worker safety},
journal = {Automation in Construction},
volume = {157},
pages = {105158},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105158},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523004181},
author = {Haosen Chen and Lei Hou and Shaoze Wu and Guomin Zhang and Yang Zou and Sungkon Moon and Muhammed Bhuiyan},
keywords = {Construction safety, Deep learning, Vision-language models, Augmented reality},
abstract = {Low situational awareness contributes to safety incidents in construction. Existing Deep Learning (DL)-based applications lack the capability to provide context-specific and interactive feedback that is essential for workers to fully understand their surrounding environments. This paper proposes the Visual Construction Safety Query (VCSQ) system. The system encompasses real-time Image Captioning (IC), safety-centric Visual Question Answering (VQA), and keyword-based Image-Text Retrieval (ITR), integrated with head-mounted Augmented Reality (AR) devices. System validation includes benchmarks and real-world images. The ITR module posted high recall rates of 0.801 and 0.835 for Recall@5 and @10. The VQA module achieved an 89.7% accuracy rate, and the IC module had a SPICE score of 0.449. Feasibility tests and surveys confirmed the system's practical advantages in different construction scenarios. This study establishes an integration roadmap adaptable to future advancements in interactive DL and immersive AR.}
}
@article{LI2025103187,
title = {Deep learning for automatic ICD coding: Review, opportunities and challenges},
journal = {Artificial Intelligence in Medicine},
volume = {168},
pages = {103187},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2025.103187},
url = {https://www.sciencedirect.com/science/article/pii/S0933365725001228},
author = {Xiaobo Li and Yijia Zhang and Xiaodi Hou and Shilong Wang and Hongfei Lin},
keywords = {Automatic ICD coding, Medical code assignment, Electronic health records, Deep learning},
abstract = {Background:
The automatic International Classification of Diseases (ICD) coding task assigns unique medical codes to diseases in clinical texts for further data statistics, quality control, billing and other tasks. The efficiency and accuracy of medical code assignment is a significant challenge affecting healthcare. However, in clinical practice, Electronic Health Records (EHRs) data are usually complex, heterogeneous, non-standard and unstructured, and the manual coding process is time-consuming, laborious and error-prone. Traditional machine learning methods struggle to extract significant semantic information from clinical texts accurately, but the latest progress in Deep Learning (DL) has shown promising results to address these issues.
Objective:
This paper comprehensively reviewed recent advancements in utilizing deep learning for automatic ICD coding, which aimed to reveal prominent challenges and emerging development trends by summarizing and analyzing the model’s year, design motivation, deep neural networks, and auxiliary data.
Methods:
This review introduced systematic literature on automatic ICD coding methods based on deep learning. We screened 5 online databases, including Web of Science, SpringerLink, PubMed, ACM, and IEEE digital library, and collected 53 published articles related to deep learning-based ICD coding from 2017 to 2023.
Results:
These deep neural network methods aimed to overcome some challenges, such as lengthy and noisy clinical text, high dimensionality and functional relationships of medical codes, and long-tail label distribution. The Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), attention mechanisms, Transformers, Pre-trained Language Models (PLMs), etc, have become popular to address prominent issues in ICD coding. Meanwhile, introducing medical ontology within the ICD coding system (code description and code hierarchy) and external knowledge (Wikipedia articles, tabular data, Clinical Classification Software (CCS), fine-tuning PLMs based on biomedical corpus, entity recognition and concept extraction) has become an emerging trend for automatic ICD coding.
Conclusion:
This paper provided a comprehensive review of recent literature on applying deep learning technology to improve medical code assignment from a unique perspective. Multiple neural network methods (CNNs, RNNs, Transformers, PLMs, especially attention mechanisms) have been successfully applied in ICD tasks and achieved excellent performance. Various medical auxiliary data has also proven valuable in enhancing model feature representation and classification performance. Our in-depth and systematic analysis suggested that the automatic ICD coding method based on deep learning has a bright future in healthcare. Finally, we discussed some major challenges and outlined future development directions.}
}
@article{FREI2023104478,
title = {Annotated dataset creation through large language models for non-english medical NLP},
journal = {Journal of Biomedical Informatics},
volume = {145},
pages = {104478},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104478},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423001995},
author = {Johann Frei and Frank Kramer},
keywords = {Natural language processing, Information extraction, Named entity recognition, Data augmentation, Knowledge distillation, Medication detection},
abstract = {Obtaining text datasets with semantic annotations is an effortful process, yet crucial for supervised training in natural language processing (NLP). In general, developing and applying new NLP pipelines in domain-specific contexts for tasks often requires custom-designed datasets to address NLP tasks in a supervised machine learning fashion. When operating in non-English languages for medical data processing, this exposes several minor and major, interconnected problems such as the lack of task-matching datasets as well as task-specific pre-trained models. In our work, we suggest to leverage pre-trained large language models for training data acquisition in order to retrieve sufficiently large datasets for training smaller and more efficient models for use-case-specific tasks. To demonstrate the effectiveness of your approach, we create a custom dataset that we use to train a medical NER model for German texts, GPTNERMED, yet our method remains language-independent in principle. Our obtained dataset as well as our pre-trained models are publicly available at https://github.com/frankkramer-lab/GPTNERMED.}
}
@article{MARTINEZCASANUEVA202498,
title = {CANDIL: A federated data fabric for network analytics},
journal = {Future Generation Computer Systems},
volume = {158},
pages = {98-109},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24001420},
author = {Ignacio D. Martinez-Casanueva and Luis Bellido and Daniel González-Sánchez and Diego Lopez},
keywords = {Data fabric, Knowledge graph, Network analytics, Ontology, Network management},
abstract = {The availability of data sources during the Big Data era provides the opportunity for new analytical applications in the networking domain, which are envisioned as one of the main enablers of the future autonomous networks. But the proliferation of heterogeneous data sources has resulted into a sea of data silos, in which finding data, understanding data, and dealing with the complexities of each data source becomes a challenge. Aiming to tackle the connection of data silos, the data fabric has appeared as a new paradigm that provides a uniform access to all the data, abstracting consumers from the underlying complexities of the data sources. In this regard, the knowledge graph has raised as a promising solution that can integrate data from heterogeneous silos based on common concepts captured in ontologies. Building upon knowledge graph standards, this paper introduces CANDIL, a federated data fabric to support the integration of data from distributed systems, mainly focused on networking domain aspects. CANDIL defines an ontology that captures network topology and interface concepts, along with a reference architecture to ingest and integrate data in a federated knowledge graph that spans across the Edge-Cloud continuum. The proposal is validated with a prototype implementation and two example use cases of network analytics.}
}
@article{DIAZHONRUBIA2024102557,
title = {PALADIN: A process-based constraint language for data validation},
journal = {Information Fusion},
volume = {112},
pages = {102557},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102557},
url = {https://www.sciencedirect.com/science/article/pii/S156625352400335X},
author = {Antonio Jesus Diaz-Honrubia and Philipp D. Rohde and Emetis Niazmand and Ernestina Menasalvas and Maria-Esther Vidal},
keywords = {Process-based data, Information fusion, Shape-based validation languages},
abstract = {In many processes, ranging from medical treatments to supply chains and employee management, there is a growing need to gather information with the objective of enhancing the efficiency of the process in question. Often, the information gathered from different stages of a process resides in disparate storage systems, necessitating an information fusion process. Post-fusion, it is common to encounter data inconsistencies that hinder an accurate analysis. Unfortunately, existing data validation languages lack the capability to model constraints across stages, making it challenging to identify inconsistencies without introducing artificial elements. This paper introduces PALADIN, a language which has been specifically designed to allow the formulation of constraints in the realm of process-based data, i.e., data points that evolve through various stages of a process with constraints that change according to the stage at which a data point is. PALADIN is data model-agnostic, which means it is not specific to any particular data model or format. This paper provides a formalization, together with implementation details of PALADIN validators, and their validation through a use case. Furthermore, PALADIN is subjected to an empirical evaluation across 20 datasets, including 18 synthetically generated ones that are openly shared with the scientific community. The experimentation involves 53 testbeds, and shows that PALADIN reduces the data validation time compared with other languages that are not tailored for process-based data—achieving a speed-up of up to five times. The results also highlight the impact of parameters such as the type of data integration system, the number of integrity constraints, and the dataset size on the validation time of PALADIN shape schemas.}
}
@article{BARANGI2025112611,
title = {HarmonyCAS: A Model-Driven Framework for Facilitating Interoperability in Context-Aware Systems},
journal = {Journal of Systems and Software},
pages = {112611},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2025.112611},
url = {https://www.sciencedirect.com/science/article/pii/S0164121225002808},
author = {Hamed Barangi and Shekoufeh Kolahdouz Rahimi and Bahman Zamani and Hossein Moradi},
keywords = {Context-aware systems, Model-driven development, Publish/subscribe middleware, Interoperability, Context sharing, Domain-specific language},
abstract = {The growing adoption of Context-Aware Systems (CAS), driven by advancements in ubiquitous computing and the Internet of Things (IoT), has heightened the need for seamless context sharing across heterogeneous domains. However, achieving interoperability remains a significant challenge, particularly in addressing syntactic (e.g., data format conflicts) and semantic (e.g., differing conceptual definitions) issues. This paper introduces HarmonyCAS, a model-driven framework designed to simplify the development of publish/subscribe middleware and enhance CAS interoperability. HarmonyCAS incorporates a Domain-Specific Language (DSL) for modeling publish/subscribe middleware components, CAS, and syntactic/semantic mappings, along with a code generation tool that automatically generates middleware code. Evaluation results demonstrate that HarmonyCAS delivers robust performance, achieving a 0% error rate at 2000 messages and maintaining scalable responsiveness with an average response time of 49 ms at 5000 messages. Additionally, usability surveys indicate high user satisfaction, with mean scores exceeding 4 out of 5. These findings confirm the framework’s effectiveness in facilitating seamless context exchange while meeting key interoperability requirements.}
}
@article{SOOD2025110307,
title = {The paradigm of hallucinations in AI-driven cybersecurity systems: Understanding taxonomy, classification outcomes, and mitigations},
journal = {Computers and Electrical Engineering},
volume = {124},
pages = {110307},
year = {2025},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2025.110307},
url = {https://www.sciencedirect.com/science/article/pii/S0045790625002502},
author = {Aditya K Sood and Sherali Zeadally and EenKee Hong},
keywords = {Artificial intelligence, Cybersecurity, Threat intelligence, Generative AI, Large language models},
abstract = {The adoption of AI to solve cybersecurity problems is occurring exponentially. However, AI-driven cybersecurity systems face significant challenges due to the impact of hallucinations in Large Language Models (LLMs). In AI-driven cybersecurity systems, hallucinations refer to instances when an AI model generates fabricated, inaccurate, and misleading information that impacts the security posture of organizations. This failure to recognize and misreport security threats identifies benign activities as malicious, invents insights not grounded to actual cyber threats, and causes real threats to go undetected due to erroneous interpretations. Hallucinations are a critical problem in AI-driven cybersecurity because they can lead to severe vulnerabilities, erode trust in automated systems, and divert resources to address non-existent threats. In cybersecurity, where real-time, accurate insights are vital, hallucinated outputs—such as mistakenly generated alerts, can cause a misallocation of time and resources. It is crucial to address hallucinations by improving LLM accuracy, grounding outputs in real-time data, and implementing human oversight mechanisms to ensure that AI-based cybersecurity systems remain trustworthy, reliable, and capable of defending against sophisticated threats. We present a taxonomy of hallucinations in LLMs for cybersecurity, including mapping LLM responses to classification outcomes (confusion matrix components). Finally, we discuss mitigation strategies to combat hallucinations.}
}
@article{MARSHOODULLA2023100763,
title = {An approach towards removal of data heterogeneity in SDN-based IoT framework},
journal = {Internet of Things},
volume = {22},
pages = {100763},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100763},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523000860},
author = {Syeda Zeenat Marshoodulla and Goutam Saha},
keywords = {Internet of Things, Software defined networks, Data heterogeneity, Semantics, Ontology, Spark, Kafka},
abstract = {IoT is an emerging technology that provides innovative solutions to our day-to-day life challenges. With billions of diverse smart objects, IoT produces gigantic amount of heterogeneous data that mostly need to be processed in real time and shared across systems and applications. For IoT data to be truly accessable and interpretable, the data heterogeneity problem needs to be resolved first. Semantic technologies is one of the most used solutions for data integration across systems. Data represented as knowledge graphs enables easier querying and matching. To address this, we propose an SDN based IoT architectural framework utilizing semantics for data integration and sharing using Kafka and Spark Streaming for real-time data handling. One main contribution of this work is that any consistent IoT ontology, defined using Web Ontology Language(OWL) which explicitly specify its classes, objects and data properties, can be used to convert the stream data into RDF format automatically using Natural Language Processing endeavor. SDN helps in dynamically locating the data producing devices such as sensors for providing flexible data routing. We have presented a testbed experimentation framework and evaluated the results with simulated virtual sensors. The homogeneous results from diverse datasets was generated in acceptable minimal time. A use-case for Air Quality Monitoring is given and SPARQL queries executed have shown satisfactory response time with over thousands of triples.}
}
@article{BARKI2024101141,
title = {Advancing radiation therapy safety in cancer-care: Leveraging AI for adverse reaction prediction},
journal = {Journal of Radiation Research and Applied Sciences},
volume = {17},
number = {4},
pages = {101141},
year = {2024},
issn = {1687-8507},
doi = {https://doi.org/10.1016/j.jrras.2024.101141},
url = {https://www.sciencedirect.com/science/article/pii/S168785072400325X},
author = {Chamseddine Barki and Sultan J. Alsufyani and Ahmad Softah and Salam Labidi and Hanene Boussi Rahmouni},
keywords = {Radiation therapy, Cancer, Artificial intelligence, Decision-support, Radiation protection, Risk mitigation, Knowledge representation, Health informatics, Ontology modeling, SNOMED CT, Machine learning},
abstract = {The development of advanced technologies for radiation cancer treatment plays a pivotal role in anticipating potential complications, safeguarding patient well-being, and elevating treatment satisfaction. Access to curated medical data coupled with specialized knowledge in radiation oncology is indispensable for equipping healthcare professionals with the insights needed to make informed treatment decisions. This study endeavors to provide systematic support to practitioners in predicting personalized side effects of radiation oncology treatments before their administration to patients. For this, a pioneering ontology-based methodology for evidence-based medicine is introduced, drawing upon information and evidence from various scientific outlets. This approach utilizes semantic modeling and rule-based reasoning to assess radiation therapy pathways, ensuring adjustments are made when necessary to mitigate health risks and enhance safety. Focused on radiation oncology, the intricacies of knowledge are simplified and validated through real reported cases obtained from scientific literature. The resulted predictive model demonstrated a notable compatibility rate of 76.7% with real reported risks, surpassing conventional methods by 4% during testing achieves 89% success with single therapies but faces challenges with complex regimens. Data enrichment may increase accuracy by 20% over five years. Current method accuracy is 92%, with potential for 97% through quantum-inspired methods. Testing reveals 80.1% compatibility, exceeding conventional approaches by 3%. These findings hold significant potential of ontology-based models for revolutionizing radiation oncology treatment strategies in the realm of radiation protection.}
}
@article{BERNARDO2023109345,
title = {A novel framework to improve motion planning of robotic systems through semantic knowledge-based reasoning},
journal = {Computers & Industrial Engineering},
volume = {182},
pages = {109345},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109345},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223003698},
author = {Rodrigo Bernardo and João M.C. Sousa and Paulo J.S. Gonçalves},
keywords = {Knowledge representation, Ontologies, Manipulation, Motion planning, Semantic maps},
abstract = {The need to improve motion planning techniques for manipulator robots, and new effective strategies to manipulate different objects to perform more complex tasks, is crucial for various real-world applications where robots cooperate with humans. This paper proposes a novel framework that aims to improve the motion planning of a robotic agent (a manipulator robot) through semantic knowledge-based reasoning. The Semantic Web Rule Language (SWRL) was used to infer new knowledge based on the known environment and the robotic system. Ontological knowledge, e.g., semantic maps, were generated through a deep neural network, trained to detect and classify objects in the environment where the robotic agent performs. Manipulation constraints were deduced, and the environment corresponding to the agent’s manipulation workspace was created so the planner could interpret it to generate a collision-free path. For reasoning with the ontology, different SPARQL queries were used. The proposed framework was implemented and validated in a real experimental setup, using the planning framework ROSPlan to perform the planning tasks. The proposed framework proved to be a promising strategy to improve motion planning of robotics systems, showing the benefits of artificial intelligence, for knowledge representation and reasoning in robotics.}
}
@article{LI2025104148,
title = {Quality-Controllable automatic construction method of Chinese knowledge graph for medical decision-making applications},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104148},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104148},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000895},
author = {Xue Li and Ye Yuan and Yang Yang and Yi Guan and Haotian Wang and Jingchi Jiang and Huaizhang Shi and Xiguang Liu},
keywords = {Medical knowledge graphs, Automatic construction of knowledge graph, Knowledge quality control, Knowledge-enhanced medical large language model, Medical decision-making application},
abstract = {Medical Knowledge Graphs (KGs) store complex medical knowledge in a structured manner, increasingly becoming the foundation of medical artificial intelligence. They provide interpretable evidence for disease diagnosis and treatment, and enhance the accuracy and interpretability of medical information in large language models (LLMs), thus mitigating the hallucination issues. However, existing medical KGs lack diverse knowledge types, sufficient coverage, fine granularity, and high quality, resulting in low utilization rates. To address these issues, this paper, under the guidance of medical professionals, proposes guidelines and automated methods for constructing a Chinese medical KG, drawing from existing experience in building KGs and the requirements of medical decision systems. The construction principles include (1) universality and personalization, (2) comprehensiveness and granularity, (3) knowledge quality control. Furthermore, the automated construction method integrates a chain of thought-based knowledge mining approach and an axiom logic-based quality control module, which improves the scalability of mining and the quality of the knowledge. Based on these, a Chinese medical KG named WiMedKG has been developed. It meets the established construction guidelines by: (1) including both commonsense and experiential medical knowledge, (2) comprehensively covering 111 departments with content ranging from clinical practice to preventive medicine and rehabilitation treatments. The granularity of the knowledge is detailed, featuring 29 entity types, 128 refined relationship types, and 40 attribute types, comprising a total of 367,108 entities, 3,176,389 relational triples, and 1,021,966 attribute triples. (3) The knowledge has been validated and completed, receiving an evaluation score of 90.66% from medical professionals, which demonstrates the reliability of the quality-controlled automatic KG construction method. Finally, we constructed medical LLM WiMedLLM enhanced by WiMedKG. Experimental results on the medical test dataset show an average performance improvement of 1.51% after KG enhancement, demonstrating the necessity of KG construction and the effectiveness of the automatic construction method. The data and system resources can be found on our page: https://github.com/lx-hit/WiMedKG.}
}
@article{MUMUNI2024101188,
title = {Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning},
journal = {Cognitive Systems Research},
volume = {84},
pages = {101188},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101188},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001225},
author = {Fuseini Mumuni and Alhassan Mumuni},
keywords = {Domain knowledge, Cognitive architecture, Brain-inspired neural network, Explainable AI, Adversarial attack, Zero-shot generalization},
abstract = {We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-shot learning. Data-driven machine learning models have achieved remarkable performance and demonstrated capabilities surpassing humans in many applications. Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions. Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. Furthermore, models are usually trained on standard datasets with a closed-world assumption. Hence, they struggle to generalize to unseen cases during inference in practical open-world environments, thus, raising the zero- or few-shot generalization problem. Although many conventional solutions exist, explicit domain knowledge, brain-inspired neural networks and cognitive architectures offer powerful new dimensions towards alleviating these problems. Prior knowledge is represented in appropriate forms like mathematical relations, logic rules, knowledge graphs, and large language models (LLMs). and incorporated in deep learning frameworks to improve performance. Brain-inspired cognition methods use computational models that mimic the human brain to enhance intelligent behavior in artificial agents and autonomous robots. Ultimately, these models achieve better explainability, higher adversarial robustness and data-efficient learning, and can, in turn, provide insights for cognitive science and neuroscience—that is, to deepen human understanding on how the brain works in general, and how it handles these problems.}
}
@article{RUBIO2024100998,
title = {Commonsense reasoning and automatic generation of IoT contextual knowledge: An Answer Set Programming approach},
journal = {Internet of Things},
volume = {25},
pages = {100998},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100998},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523003219},
author = {Ana Rubio and Rubén Cantarero and Alessandro Margara and Gianpaolo Cugola and David Villa and Juan Carlos López},
keywords = {Commonsense reasoning, Ontology population, Knowledge generation, Internet of Things, Answer Set Programming, Smart environments},
abstract = {The increasing interest in searching for intelligent, proactive, and autonomous environments leads to the necessity of accessing contextual knowledge: knowledge about changes that occur in the environment and even about the capabilities of the devices that compose a specific deployment. This knowledge would allow carrying out commonsense reasoning (exhibit human-like comprehension) according to the current situation, enabling decision making and task planning. One of the most used forms of knowledge specification is ontologies, but ontological knowledge modeling is usually a manual process, making it a costly task. In addition, ontologies can be found that have been designed for specifying knowledge about IoT-related concepts, but many are overly complex or do not have an adequate orientation that allows modeling the system’s capabilities. There are many different types of reasoners, but Answer Set Solvers are of particular interest for commonsense reasoning. This paper proposes an ontology to represent contextual knowledge about IoT device capabilities, and an Answer Set Programming-based reasoner for the automatic generation of this knowledge. The main challenge is to demonstrate that contextual knowledge can be generated through commonsense reasoning processes implemented with Answer Set Programming, and that the resulting knowledge can be used for decision making (also through commonsense reasoning). This work shows how the generated knowledge is correct through different use cases, presents an application example that demonstrates the benefits of using the generated knowledge, and analyzes the reasoner performance to demonstrate that the execution time is adequate.}
}
@incollection{HALPIN2024935,
title = {18 - Other Modeling Aspects and Trends},
editor = {Terry Halpin and Tony Morgan},
booktitle = {Information Modeling and Relational Databases (Third Edition)},
publisher = {Morgan Kaufmann},
edition = {Third Edition},
pages = {935-1005},
year = {2024},
isbn = {978-0-443-23790-4},
doi = {https://doi.org/10.1016/B978-0-443-23790-4.00010-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443237904000104},
author = {Terry Halpin and Tony Morgan},
keywords = {Data warehouse, Online analytical processing (OLAP), Conceptual query languages, Schema abstraction, Process models, Ontologies, Semantic web, OWL, Metamodeling},
abstract = {This chapter discusses other aspects and recent trends related to modeling and querying information systems. Section 18.2 provides a brief introduction to data warehousing and online analytical processing. Section 18.3 discusses some very high-level languages for querying information systems. Section 18.4 outlines some ways to perform schema abstraction, enabling modelers to focus on various aspects by hiding other details. Section 18.5 discusses further design aspects, such as coordinating data and process models, and user interface design. Section 18.6 outlines some recent work on ontologies and the semantic web, especially OWL. Section 18.7 provides an introduction to metamodeling, in which schemas themselves are treated as instances of a higher-level metaschema. Section 18.8 furnishes a summary that completes this final chapter.}
}
@article{WANG2023349,
title = {NetGO 3.0: Protein Language Model Improves Large-scale Functional Annotations},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {21},
number = {2},
pages = {349-358},
year = {2023},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2023.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1672022923000669},
author = {Shaojun Wang and Ronghui You and Yunjia Liu and Yi Xiong and Shanfeng Zhu},
keywords = {Protein function prediction, Web service, Protein language model, Learning to rank, Large-scale multi-label learning},
abstract = {As one of the state-of-the-art automated function prediction (AFP) methods, NetGO 2.0 integrates multi-source information to improve the performance. However, it mainly utilizes the proteins with experimentally supported functional annotations without leveraging valuable information from a vast number of unannotated proteins. Recently, protein language models have been proposed to learn informative representations [e.g., Evolutionary Scale Modeling (ESM)-1b embedding] from protein sequences based on self-supervision. Here, we represented each protein by ESM-1b and used logistic regression (LR) to train a new model, LR-ESM, for AFP. The experimental results showed that LR-ESM achieved comparable performance with the best-performing component of NetGO 2.0. Therefore, by incorporating LR-ESM into NetGO 2.0, we developed NetGO 3.0 to improve the performance of AFP extensively. NetGO 3.0 is freely accessible at https://dmiip.sjtu.edu.cn/ng3.0.}
}
@article{WANG2025103633,
title = {Intelligent port logistics: A spatiotemporal knowledge graph and AI-agent framework for berth allocation},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103633},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103633},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005269},
author = {Peng Wang and Qinyou Hu and Qiang Mei and Shaohua Wang and Yang Yang and Da Guo and Xiaotong Liu and Wenlong Hu and Jihong Chen},
keywords = {Spatiotemporal knowledge graph, Intelligent berth allocation, Multi-agent LLMs, Port logistics, Yangtze River Delta},
abstract = {Efficient berth allocation is crucial for optimizing port logistics, directly impacting ship loading, refueling, and sheltering operations while enhancing the overall efficiency of maritime logistics systems. However, the inherent uncertainty of ship arrival times often leads to inefficiencies, such as “ships waiting for berths” or “berths waiting for ships,” negatively affecting port operations and economic performance. Existing berth allocation models struggle to fully capture the dynamic and complex interactions between ships and berths, particularly under spatiotemporal constraints and fluctuating maritime conditions. To address these challenges, this study proposes an intelligent berth recommendation framework, STK-LLM, which integrates Spatiotemporal Knowledge Graphs and AI-Agents. This framework constructs structured spatiotemporal triplet models to represent ship-berth interactions and employs AI-driven decision-making to enhance berth allocation strategies. An empirical analysis covering 10 ports, 49 port areas, and 2,180 berths in the Yangtze River Delta reveals competitive and cooperative berth allocation dynamics. The results indicate that the STK-LLM framework reduces ship waiting times at anchorages by approximately 20% and increases berth utilization by about 15%. This study advances intelligent port logistics by providing data-driven decision support, improving berth resource efficiency. By addressing the challenges of modeling and optimizing complex, dynamic, and uncertain ship–berth interactions—difficulties that have hindered previous berth allocation models—this framework enhances the performance of maritime logistics systems.}
}
@incollection{LENCI2024,
title = {Artificial Intelligence and Language☆},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00241-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041002416},
author = {Alessandro Lenci and Andrea Vestrucci},
keywords = {Artificial intelligence (AI), Symbolic AI, Subsymbolic AI, Hybrid AI, Large language models (LLMs), Neuro-symbolic AI, Natural language inference, Machine learning (ML), Natural language reasoning, Inferential tasks, Semantic gap},
abstract = {This article explores the roles of three primary paradigms of Artificial Intelligence (AI)—symbolic, subsymbolic, and hybrid—in natural language reasoning and inferential tasks. Symbolic AI leverages explicit knowledge representations and logical rules to provide high interpretability and precision, making it suitable for tasks requiring clear and verifiable reasoning. However, its rigidity and lack of scalability limit its effectiveness in handling the nuanced nature of human language. Subsymbolic AI, driven by machine learning, deep learning, and large language models, excels in flexibility and adaptability by learning from vast datasets. While capable of generating human-like text and performing complex inferences, subsymbolic AI faces challenges such as semantic gaps, susceptibility to biases, and a lack of transparency in decision-making processes. Hybrid AI combines the strengths of both symbolic and subsymbolic approaches, aiming to create more robust and reliable systems. By integrating explicit reasoning capabilities with data-driven learning, hybrid models enhance interpretability while maintaining adaptability to diverse linguistic contexts. This article concludes by emphasizing the need for continued research to refine these integrations, address current limitations, and pave the way for more intelligent and trustworthy language reasoning systems.}
}
@article{LOPEZ2025100845,
title = {Enhancing foundation models for scientific discovery via multimodal knowledge graph representations},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100845},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100845},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000313},
author = {Vanessa Lopez and Lam Hoang and Marcos Martinez-Galindo and Raúl Fernández-Díaz and Marco Luca Sbodio and Rodrigo Ordonez-Hurtado and Mykhaylo Zayats and Natasha Mulligan and Joao Bettencourt-Silva},
keywords = {Multimodal graph learning, Multimodal knowledge graphs, Knowledge-enhanced drug discovery},
abstract = {Foundation Models (FMs) hold transformative potential to accelerate scientific discovery, yet reaching their full capacity in complex, highly multimodal domains such as genomics, drug discovery, and materials science requires a deeper consideration of the contextual nature of the scientific knowledge. We revisit the synergy between FMs and Multimodal Knowledge Graph (MKG) representation and learning, exploring their potential to enhance predictive and generative tasks in biomedical contexts like drug discovery. We seek to exploit MKGs to improve generative AI models’ ability to capture intricate domain-specific relations and facilitate multimodal fusion. This integration promises to accelerate discovery workflows by providing more meaningful multimodal knowledge-enhanced representations and contextual evidence. Despite this potential, challenges and opportunities remain, including fusing multiple sequential, structural and knowledge modalities and models leveraging the strengths of each; developing scalable architectures for multi-task multi-dataset learning; creating end-to-end workflows to enhance the trustworthiness of biomedical FMs using knowledge from heterogeneous datasets and scientific literature; the domain data bottleneck and the lack of a unified representation between natural language and chemical representations; and benchmarking, specifically the transfer learning to tasks with limited data (e.g., unseen molecules and proteins, rear diseases). Finally, fostering openness and collaboration is key to accelerate scientific breakthroughs.}
}
@article{RAZAVIPOUR2023100760,
title = {Classroom writing assessment and feedback practices: A new materialist encounter},
journal = {Assessing Writing},
volume = {57},
pages = {100760},
year = {2023},
issn = {1075-2935},
doi = {https://doi.org/10.1016/j.asw.2023.100760},
url = {https://www.sciencedirect.com/science/article/pii/S1075293523000685},
author = {Kioumars Razavipour},
keywords = {Writing assessment literacy, New materialisms, Assemblage, Feedback practices, Flat ontology},
abstract = {The mainstream approach to teacher assessment literacy seems to be founded on a (post)positivist paradigm leading to an autonomous model of literacy comprised of generic knowledge and skills. This paradigm obscures the non-cognitive, embodied, and affective dimensions of assessment practices. In this conceptual inquiry, I use the New Materialist philosophy to make sense of writing assessment literacy and feedback practices. In New Materialisms, the materiality of everything is emphasized, ontology is flat, reality is becoming, agency is relational, knowledge is entangled practice, and language is a resource in communicative assemblage. Using the noted conceptual tools, I try to provide a materialized conceptualization of writing assessment and feedback practice arguing that from a New Materialist perspective, feedback practices are an assemblage of rhetoric, IELTS, institution, materiality, art, cross-lingual resources, social relations, affect, and embodiment.}
}
@article{ZHUANG2025,
title = {Autonomous International Classification of Diseases Coding Using Pretrained Language Models and Advanced Prompt Learning Techniques: Evaluation of an Automated Analysis System Using Medical Text},
journal = {JMIR Medical Informatics},
volume = {13},
year = {2025},
issn = {2291-9694},
doi = {https://doi.org/10.2196/63020},
url = {https://www.sciencedirect.com/science/article/pii/S2291969425000031},
author = {Yan Zhuang and Junyan Zhang and Xiuxing Li and Chao Liu and Yue Yu and Wei Dong and Kunlun He},
keywords = {BERT, bidirectional encoder representations from transformers, pretrained language models, prompt learning, ICD, International Classification of Diseases, cardiovascular disease, few-shot learning, multicenter medical data},
abstract = {Background
Machine learning models can reduce the burden on doctors by converting medical records into International Classification of Diseases (ICD) codes in real time, thereby enhancing the efficiency of diagnosis and treatment. However, it faces challenges such as small datasets, diverse writing styles, unstructured records, and the need for semimanual preprocessing. Existing approaches, such as naive Bayes, Word2Vec, and convolutional neural networks, have limitations in handling missing values and understanding the context of medical texts, leading to a high error rate. We developed a fully automated pipeline based on the Key–bidirectional encoder representations from transformers (BERT) approach and large-scale medical records for continued pretraining, which effectively converts long free text into standard ICD codes. By adjusting parameter settings, such as mixed templates and soft verbalizers, the model can adapt flexibly to different requirements, enabling task-specific prompt learning.
Objective
This study aims to propose a prompt learning real-time framework based on pretrained language models that can automatically label long free-text data with ICD-10 codes for cardiovascular diseases without the need for semiautomatic preprocessing.
Methods
We integrated 4 components into our framework: a medical pretrained BERT, a keyword filtration BERT in a functional order, a fine-tuning phase, and task-specific prompt learning utilizing mixed templates and soft verbalizers. This framework was validated on a multicenter medical dataset for the automated ICD coding of 13 common cardiovascular diseases (584,969 records). Its performance was compared against robustly optimized BERT pretraining approach, extreme language network, and various BERT-based fine-tuning pipelines. Additionally, we evaluated the framework’s performance under different prompt learning and fine-tuning settings. Furthermore, few-shot learning experiments were conducted to assess the feasibility and efficacy of our framework in scenarios involving small- to mid-sized datasets.
Results
Compared with traditional pretraining and fine-tuning pipelines, our approach achieved a higher micro–F1-score of 0.838 and a macro–area under the receiver operating characteristic curve (macro-AUC) of 0.958, which is 10% higher than other methods. Among different prompt learning setups, the combination of mixed templates and soft verbalizers yielded the best performance. Few-shot experiments showed that performance stabilized and the AUC peaked at 500 shots.
Conclusions
These findings underscore the effectiveness and superior performance of prompt learning and fine-tuning for subtasks within pretrained language models in medical practice. Our real-time ICD coding pipeline efficiently converts detailed medical free text into standardized labels, offering promising applications in clinical decision-making. It can assist doctors unfamiliar with the ICD coding system in organizing medical record information, thereby accelerating the medical process and enhancing the efficiency of diagnosis and treatment.}
}
@article{DAM2023105233,
title = {Augmented intelligence facilitates concept mapping across different electronic health records},
journal = {International Journal of Medical Informatics},
volume = {179},
pages = {105233},
year = {2023},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2023.105233},
url = {https://www.sciencedirect.com/science/article/pii/S1386505623002514},
author = {Tariq A. Dam and Lucas M. Fleuren and Luca F. Roggeveen and Martijn Otten and Laurens Biesheuvel and Ameet R. Jagesar and Robbert C.A. Lalisang and Robert F.J. Kullberg and Tom Hendriks and Armand R.J. Girbes and Mark Hoogendoorn and Patrick J. Thoral and Paul W.G. Elbers},
keywords = {Concept mapping, Ontology, Ontology alignment, Augmented Intelligence, Intensive care},
abstract = {Introduction
With the advent of artificial intelligence, the secondary use of routinely collected medical data from electronic healthcare records (EHR) has become increasingly popular. However, different EHR systems typically use different names for the same medical concepts. This obviously hampers scalable model development and subsequent clinical implementation for decision support. Therefore, converting original parameter names to a so-called ontology, a standardized set of predefined concepts, is necessary but time-consuming and labor-intensive. We therefore propose an augmented intelligence approach to facilitate ontology alignment by predicting correct concepts based on parameter names from raw electronic health record data exports.
Methods
We used the manually mapped parameter names from the multicenter “Dutch ICU data warehouse against COVID-19” sourced from three types of EHR systems to train machine learning models for concept mapping. Data from 29 intensive care units on 38,824 parameters mapped to 1,679 relevant and unique concepts and 38,069 parameters labeled as irrelevant were used for model development and validation. We used the Natural Language Toolkit (NLTK) to preprocess the parameter names based on WordNet cognitive synonyms transformed by term-frequency inverse document frequency (TF-IDF), yielding numeric features. We then trained linear classifiers using stochastic gradient descent for multi-class prediction. Finally, we fine-tuned these predictions using information on distributions of the data associated with each parameter name through similarity score and skewness comparisons.
Results
The initial model, trained using data from one hospital organization for each of three EHR systems, scored an overall top 1 precision of 0.744, recall of 0.771, and F1-score of 0.737 on a total of 58,804 parameters. Leave-one-hospital-out analysis returned an average top 1 recall of 0.680 for relevant parameters, which increased to 0.905 for the top 5 predictions. When reducing the training dataset to only include relevant parameters, top 1 recall was 0.811 and top 5 recall was 0.914 for relevant parameters. Performance improvement based on similarity score or skewness comparisons affected at most 5.23% of numeric parameters.
Conclusion
Augmented intelligence is a promising method to improve concept mapping of parameter names from raw electronic health record data exports. We propose a robust method for mapping data across various domains, facilitating the integration of diverse data sources. However, recall is not perfect, and therefore manual validation of mapping remains essential.}
}
@article{MOUSAVI20191254,
title = {A Survey of Model-Based System Engineering Methods to Analyse Complex Supply Chains: A Case Study in Semiconductor Supply Chain},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {1254-1259},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.370},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319313497},
author = {Behrouz Alizadeh Mousavi and Radhia Azzouz and Cathal Heavey and Hans Ehm},
keywords = {Model-Based system Engineering, Conceptual modelling, Simulation, SysML, Ontology, BPMN, Supply chain planning},
abstract = {Model-Based System Engineering (MBSE) is an increasingly important methodology to support system engineering and has attained a high level of attentiveness in business simulation practices as a conceptual modelling approach. In this paper, we present our results related to the application of MBSE approaches in complex semiconductor manufacturing supply chain planning systems. We investigate System Modeling Language (SysML), Web Ontology Language (OWL) and Business Process Modeling Notation (BPMN) as different approaches and languages for MBSE. These approaches are surveyed and used to develop conceptual models for the simulation of the order management process inside the supply chain management. This study aims to survey and offer a number of implications for MBSE practice and seeks to stimulate and guide further research in this area.}
}
@article{GUO2024118077,
title = {A semantic reasoning-based emergency rescue assistant decision method for maritime accidents involving chemicals},
journal = {Ocean Engineering},
volume = {306},
pages = {118077},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.118077},
url = {https://www.sciencedirect.com/science/article/pii/S002980182401415X},
author = {Siqi Guo and Changshi Xiao and Hongxun Huang and Fan Zhang and Cheng Li and Chunhui Zhou},
keywords = {Maritime accident, Chemicals transportation, Emergency rescue, Ontology, Assistant decision-making},
abstract = {Maritime accidents involving chemicals (MAICs) constitute a unique category of marine emergency characterized by low frequency, high risk, and complex disposal challenges. These accidents often entail fires, explosions, and pollutant leakage, demanding effective emergency responses. The success of MAICs rescue operations relies on multiple factors, encompassing the structural features of the involved vessel, personnel, cargo, marine environment, and the availability of resources and response procedures. Proficiency in various professional fields, including ship transportation, chemistry, firefighting, and environmental science, is essential for informed decision-making. This paper addresses the need for a standardized description of knowledge in the MAICs emergency rescue decision-making process, aiming to enhance practical application frequency and decision-making efficiency. Employing ontology theory, the approach involves structurally modeling the knowledge of MAICs emergency rescue, constructing an ontology model, and establishing a decision rule base. Validating the proposed method, the emergency response to the “Longqing 1″ accident is examined as a case study, illustrating its potential for supporting MAICs emergency rescue decision-making. The application of ontology theory in this context holds both theoretical and practical significance. It facilitates the sharing, reuse, and expansion of knowledge in the emergency rescue field, providing crucial technical support for intelligent decision-making in MAICs emergencies.}
}
@article{ALDANAMARTIN2022115838,
title = {Semantic modelling of Earth Observation remote sensing},
journal = {Expert Systems with Applications},
volume = {187},
pages = {115838},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115838},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421012008},
author = {José F. Aldana-Martín and José García-Nieto and María {del Mar Roldán-García} and José F. Aldana-Montes},
keywords = {Remote sensing, Earth Observation, Semantic web, Ontology, Linked data, Reasoning},
abstract = {Earth Observation (EO) based on Remote Sensing (RS) is gaining importance nowadays, since it offers a well-grounded technological framework for the development of advanced applications in multiple domains, such as climate change, precision agriculture, smart urbanism, safety, and many others. This promotes the continuous generation of data-driven software facilities oriented to advanced processing, analysis and visualization, which often offer enhanced computing capabilities. Nevertheless, the development of knowledge-driven approaches is still an open challenge in remote sensing, besides they provide human experts with domain knowledge representation, support for data standardization and semantic integration of sources, which indeed enhance the construction of advanced on-top applications. To this end, the use of ontologies and web semantic technologies have shown high success in knowledge representation in many fields, in which the Earth Observation is not an exception. However, as argued by the research community, there is large room for improvement in the specific case of remote sensing, where ontologies that consider the special nature and structure of different satellital and airborne data products are demanded. This article addresses, in first instance, part of this need by proposing a semantic model for the consolidation, integration, reasoning and linking of data (and meta-data), in the context of satellital remote sensing products for EO. With this objective, an OWL ontology has been developed and an RDF repository has been generated to allow advanced SPARQL querying. Although the proposal has been designed to consider remote sensing data products in general, the current study is mainly focused on the Sentinel 2 satellite mission from the Copernicus Programme of the European Space Agency (ESA). Four different use cases are showcased to check potentials of the proposed semantic model in terms of ontology integration, federated querying, data analysis and reasoning.}
}
@article{QUEK2024109507,
title = {Dynamic knowledge graph applications for augmented built environments through “The World Avatar”},
journal = {Journal of Building Engineering},
volume = {91},
pages = {109507},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.109507},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224010751},
author = {Hou Yee Quek and Markus Hofmeister and Simon D. Rihm and Jingya Yan and Jiawei Lai and George Brownbridge and Michael Hillman and Sebastian Mosbach and Wilson Ang and Yi-Kai Tsai and Dan N. Tran and Soon Kang, William Tan and Markus Kraft},
keywords = {BIM, GIS, Interoperability, Built environment, Knowledge graph},
abstract = {The proliferation of digital building models in recent years has led to a corresponding rise in specialised, non-interoperable models. These models impede sustainable developments by forming data silos that hinder cross-application data exchange and knowledge discovery processes. Although Semantic Web solutions hold promise in addressing these silos, current approaches primarily focus on developing novel ontologies, yielding similar outcomes. But it is unclear how these methodologies could support broader knowledge discovery processes and application requirements. This paper addresses these research challenges by introducing a dynamic knowledge graph as implemented within The World Avatar for interoperable building models. We demonstrate its value through two distinct applications in urban energy management and laboratory automation. The dynamic knowledge graph revolves around a comprehensive structured knowledge model constructed from ontologies and agents. Ontologies semantically annotate data and represent domain knowledge and their relationships with standardised definitions. When augmented with an agent architecture, the resulting knowledge model can align stakeholder perspectives and accommodate the dynamic and scalable nature of urban data. Moreover, the dynamic knowledge graph fosters innovative human-machine interactions through visualisation interfaces to augment knowledge discovery processes in the built environment for greater efficiencies and innovation. As the knowledge model expands, users gain access to a broader spectrum of private and public data sources and technologies, while reducing integration barriers. This is especially pertinent for smaller and less influential entities like municipal and local governments with limited resources, who can realise substantial benefits at reduced costs.}
}
@article{ALMEIDA2021113538,
title = {Tun-OCM: A model-driven approach to support database tuning decision making},
journal = {Decision Support Systems},
volume = {145},
pages = {113538},
year = {2021},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2021.113538},
url = {https://www.sciencedirect.com/science/article/pii/S0167923621000488},
author = {Ana Carolina Almeida and Fernanda Baião and Sérgio Lifschitz and Daniel Schwabe and Maria Luiza M. Campos},
keywords = {Database systems, Tuning decision, Heuristics, Configuration management, Ontology pattern language},
abstract = {Database tuning is a task executed by Database Administrators (DBAs) based on their practical experience and on tuning systems, which support DBA actions towards improving the performance of a database system. It is notoriously a complex task that requires precise domain knowledge about possible database configurations. Ideally, a DBA should keep track of several Database Management Systems (DBMS) parameters, configure data structures, and must be aware about possible interferences among several database (DB) configurations. We claim that an automatic tuning system is a decision support system and DB tuning may also be seen as a configuration management task. Therefore, we may characterize it by means of a formal domain conceptualization, benefiting from existing control practices and computational support in the configuration management domain. This work presents Tun-OCM, a conceptual model represented as a well-founded ontology, that encompasses a novel characterization of the database tuning domain as a configuration management conceptualization to support decision making. We develop and represent Tun-OCM using the CM-OPL methodology and its underlying language. The benefits of Tun-OCM are discussed by instantiating it in a real scenario.}
}
@article{GOBELS2025106418,
title = {Graph-based method for extracting spatial information from semi-formal text to derive 3D bridge and damage models from legacy maintenance data},
journal = {Automation in Construction},
volume = {178},
pages = {106418},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106418},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525004583},
author = {Anne Göbels and Jakob Beetz},
keywords = {Bridge maintenance, Damage localization, Legacy data integration, Bridge management systems, Spatial knowledge graphs, Automatic model generation, Textual location information processing},
abstract = {Aging bridges require improved maintenance strategies; however, recent developments often rely on newly collected data to represent the bridge and its condition, hindering their large-scale adoption and thus significant improvements. This paper demonstrates how existing data from legacy bridge management systems (BMS) can be utilized to automatically create object-oriented knowledge graphs and three-dimensional models of bridge structures and their inspection data. It applies a relative spatial reference system to position and link components and damage, generating a bridge maintenance graph from BMS data that supports spatial queries using natural-language-based location terms. This enables the automatic localization of recorded damage through their textual location descriptions. The method successfully processed 90% of 2,348 damages from two use cases with a precision of 0.8 and a recall of 0.97. The approach bridges the gap between the needs of modern information models and legacy data structures, facilitating the widespread implementation of improved maintenance strategies.}
}
@article{LISTL20231558,
title = {Utilizing ISA-95 in an Industrial Knowledge Graph for Material Flow Simulation - Semantic Model Extensions and Efficient Data Integration},
journal = {Procedia CIRP},
volume = {120},
pages = {1558-1563},
year = {2023},
note = {56th CIRP International Conference on Manufacturing Systems 2023},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.09.214},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123009460},
author = {Franz Georg Listl and Jan Fischer and Annelie Sohr and Daniel Dittler and Nasser Jazdi and Michael Weyrich},
keywords = {Knowledge Graph, Ontology, Data Integration, Semantic Mappin, Industry 4.0, ISA-95},
abstract = {Despite access to emerging technologies for effective data representation and interoperability between different types of data, many enterprises struggle to use the information and knowledge available in manufacturing data to increase the efficiency and quality of their production. This is especially true for knowledge graphs and semantic web technologies. Nevertheless, breaking down the rigid and historical manufacturing data systems and transferring them to knowledge graphs often involves a lot of manual work for cleaning, mapping, and transforming the data. To address this issue, we have developed a methodology to efficiently utilize ISA-95 in knowledge graphs for manufacturing. The methodology is based on two main building blocks. First, we built an ontology based on the ISA-95 standard and extended it with additional semantic concepts. The second building block is an extract, transform, and load pipeline, to allow an efficient mapping of different data formats into the concepts specified by the ontology, and consequently a fast data integration into the knowledge graph. The created knowledge graph serves as basis for efficient management of material flow simulations.}
}
@article{SCHAFER2023104,
title = {Systematics for an Integrative Modelling of Product and Production System},
journal = {Procedia CIRP},
volume = {118},
pages = {104-109},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S221282712300241X},
author = {Louis Schäfer and Matthias Günther and Alex Martin and Mariella Lüpfert and Constantin Mandel and Simon Rapp and Gisela Lanza and Harald Anacker and Albert Albers and Daniel Köchling},
keywords = {Model-Based Systems Engineering, Advanced Systems Engineering, Product-Production-CoDesign},
abstract = {Due to shorter product life-cycles, manufacturing companies nowadays must maximize efficiency in development and planning of products and production systems. To achieve this, new methods and tools are required to cope with increasing product variants and system complexity. Therefore, we propose a holistic systematics to develop a consistent, model-based architecture for products, production systems and their interdependencies. For this, we give a domain-specific ontology containing model elements, relationships and attributes. Ontology and systematics are applied to a use-case from the automotive industry. In summary, the approach deals with modern-day demands in engineering by considering interdependencies between product and production systems.}
}
@article{NAJM2024108594,
title = {Integrating data and knowledge to support the selection of service plant species in agroecology},
journal = {Computers and Electronics in Agriculture},
volume = {217},
pages = {108594},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.108594},
url = {https://www.sciencedirect.com/science/article/pii/S0168169923009821},
author = {Elie Najm and Marie-Laure Mugnier and Christian Gary and Jean-François Baget and Raphael Métral and Léo Garcia},
keywords = {Functional trait, Ecosystem service, Service crop, Ontology-based data access, Rules, Reasoning},
abstract = {There is a crucial need for tools to help researchers, technicians and farmers designing sustainable agroecosystems based on agroecology Indeed, such agroecosystems are inherently complex and their design requires to integrate various data and unstabilised scientific knowledge. In this paper, we consider the issue of selecting service plant species according to their potential to provide ecosystem services. To tackle that issue, we adopt an approach based both on a formalized representation of domain knowledge, which enables reasoning, and on the exploitation of available data, collected independently of the targeted application. More specifically, we rely on the one hand on recent scientific results in agronomy linking functional traits (i.e., measurable characteristics of plant species) to ecosystem services, and on the other hand on data about functional traits collected by the research community in ecology. The architecture of our system is inspired by the ontology-based data access paradigm, which allows to combine data and knowledge in a principled way. We provide a methodology to acquire scientific knowledge in the form of diagrams linked to data sources, as well as a formalization in a logical rule-based language. Importantly, our rules are independent from specific diagrams and data, to ensure genericity and facilitate the evolution of the system. We detail the construction of a knowledge base devoted to vine grassing, i.e., installing herbaceous service plants in vineyards, and present an evaluation of the system’s results on this use case. We finally discuss the lessons learned and further challenges to be met.}
}
@article{ZHANG2025103670,
title = {Semantic enrichment of BIM models for construction cost estimation in pumped storage hydropower using industry foundation classes and interconnected data dictionaries},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103670},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103670},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005634},
author = {Shihang Zhang and Sherong Zhang and Han Liu and Chao Wang and Zhiyong Zhao and Xiaohua Wang and Lei Yan},
keywords = {Building Information Modeling (BIM), Industry Foundation Classes (IFC), Interconnected data dictionary, Ontology, Artificial Intelligence (AI), Pumped hydro energy storage},
abstract = {Accurate construction cost estimation is vital for ensuring profitable project execution for both owners and contractors. However, traditional methods often rely on disparate engineering data, leading to inaccuracies and inconsistencies among stakeholders. To address this, this paper introduces an automated system that semantically enriches Building Information Modeling (BIM) models by integrating Industry Foundation Classes (IFC) with interconnected data dictionaries. By automating the cost estimation procedure, the proposed system streamlines decision-making, demonstrating significant advantages over traditional project management frameworks in a real-world pumped-storage project during the preconstruction phase. The findings also highlight future research directions for incorporating real-time engineering quotas and fluctuating market prices to enhance construction practices.}
}
@article{ROSZKOWSKI2023102658,
title = {Modelling doctoral dissertations in Wikidata knowledge graph: Selected issues},
journal = {The Journal of Academic Librarianship},
volume = {49},
number = {1},
pages = {102658},
year = {2023},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2022.102658},
url = {https://www.sciencedirect.com/science/article/pii/S0099133322001744},
author = {Marcin Roszkowski},
keywords = {Doctoral dissertations, Wikidata, Electronic theses and dissertations, Metadata integration, Conceptual modelling},
abstract = {Academic libraries have a long tradition of collecting, preserving, and offering access to information about doctoral dissertations through the repositories of electronic theses and dissertations. In recent years libraries have turned their attention to Wikidata as a knowledge graph for publishing structured data on the Web. These initiatives include the publication of metadata about theses and dissertations. However, Wikidata is a sociotechnical infrastructure, where the responsibility for its development is in the hands of its users. This includes asynchronous collaborative work on its ontology by many non-expert users. This may cause tensions between traditional metadata control and socially developed schemas. The goal of this article is to develop a conceptual understanding of the representation patterns of doctoral dissertations in a socially constructed Wikidata knowledge graph. This study uses an interpretative approach guided by the method of the close reading of the infrastructure to develop an understanding of what a doctoral dissertation is in this socially constructed representation of reality and how doctoral dissertations are described using Wikidata ontology. The ontological status of the doctoral dissertation is revealed by interpreting the place of this concept in the taxonomic structure of the Wikidata ontology. The expressiveness of Wikidata ontology for the description of doctoral dissertations is limited to three selected properties, authorship, doctoral advisor, and the institution to which the dissertation was submitted. The results of this study show on the one hand the redundancies in modelling doctoral dissertations, and on the other inconsistencies in the descriptions. This calls for close attention from libraries to metadata curatorial practices in Wikidata sociotechnical infrastructure.}
}
@article{MAJOR2025105516,
title = {From code to archetype: Toward a unified theory of biological, neural, and artificial artifacts},
journal = {BioSystems},
volume = {254},
pages = {105516},
year = {2025},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2025.105516},
url = {https://www.sciencedirect.com/science/article/pii/S0303264725001261},
author = {João Carlos Major},
abstract = {Carl Jung's concept of archetypes as innate, universal structures of the human psyche finds surprising resonance with contemporary theories in Code Biology, neuroscience, and artificial intelligence. Archetypes, far from being metaphysical abstractions, can be reframed as codified artifacts of the mind, shaped by phylogenetic inheritance, stabilized by neural coding, and expressed through cognitive and cultural configurations. Marcello Barbieri's Code Biology demonstrates that life is driven not only by biochemical causality but also by natural conventions — systems of natural correspondence that create functional structures, or bio-artifacts, through processes of translation carried out by molecular mediators. Similarly, research in neuroscience identifies neural codes as dynamic patterns of brain activity that organize perception, emotion, and cognition. Building on this, this paper advances the hypothesis that archetypes operate as neurofunctional artifacts — internal configurations that stabilize recurring experiences across individuals and cultures, leading to expressive forms such as myths, dreams, and emotional constellations that shape human meaning-making. These artifacts are not limited to biology or psyche alone; they now appear in the digital realm. The rise of generative AI systems, such as large language models (LLMs), introduces a new category of algorithmic artifacts that replicate archetypes. These systems simulate meaning without subjective intentionality. This interdisciplinary framework brings together biology, psychology, and AI under a unified theory of codified form. Archetypes, in this perspective, are artifacts instantiated by lower-level codes that serve as mediators in higher-level interpretative processes — acting as a bridge between gene and glyph, neuron and narrative, matter and symbol … and silicon.}
}
@article{DING2023102183,
title = {Knowledge graph modeling method for product manufacturing process based on human–cyber–physical fusion},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102183},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102183},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003117},
author = {Chen Ding and Fei Qiao and Juan Liu and Dongyuan Wang},
keywords = {Knowledge graph modeling, Product manufacturing process, Human–cyber–physical, Ontology},
abstract = {The data generated in the product manufacturing process are usually distributed in different formats, triggering fragmented knowledge and disconnected information. To address this problem, we present a knowledge graph modeling method for the product manufacturing process. First, the concepts of human–cyber–physical (HCP) elements are analyzed in detail. The HCP-related classes, attributes, and relations are defined in a formalized manner in the ontology modeling process. Second, a knowledge graph model for the product manufacturing process (KGM/PMP) is constructed by three steps, including knowledge extraction, knowledge fusion, and knowledge reasoning. When constructing the KGM/PMP model, a deep learning method called BERT-D’BiGRU-CRF is presented to automatically extract knowledge from the manufacturing data. Moreover, a set of reasoning rules are designed to infer new knowledge. Finally, a case study is carried out to validate the effectiveness of the proposed method. The validity of the BERT-D’BiGRU-CRF method on knowledge extraction is verified by comparing performance with four other methods. The applicability of the knowledge graph model is demonstrated through developing a prototype system. With this system, manufacturing knowledge can be provided for the demanders rapidly and accurately.}
}
@article{SHEN2023101880,
title = {Dynamic knowledge modeling and fusion method for custom apparel production process based on knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101880},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101880},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000083},
author = {Xingwang Shen and Xinyu Li and Bin Zhou and Yanan Jiang and Jinsong Bao},
keywords = {Knowledge graph, Custom apparel, Knowledge modeling, Knowledge fusion, Knowledge generation},
abstract = {Compared with ordinary mass-produced apparel products, custom apparel products will generate more data at each stage of their life cycle. Such data is in a highly dynamic state, while the relationship between the data is more complex. However, the current use of traditional relational data to store the whole life cycle data of custom apparel products has several problems of high redundancy, weak correlation, discrete distribution, and certain limitation of storage capacity. Therefore, based on knowledge graph, a dynamic knowledge modeling and fusion method is proposed for the production process of custom apparel. Firstly, an ontology-based knowledge modeling method is designed for custom apparel, which defined three types of ontology modeling methods for the process, resources, and features. On this basis, a knowledge graph construction method based on bi-directional fusion for the custom apparel production system is proposed. With one order as a unit, a knowledge graph facet (KGF) model, as well as the derived knowledge representation, generation and fusion method, is established to realize dynamic knowledge fusion of the custom apparel production process. Finally, taking the suit production process of a custom apparel factory as an example, the corresponding knowledge graph is constructed based on the ontology knowledge model, and the effectiveness of the proposed knowledge fusion method is verified.}
}
@article{HOFMEISTER2024100376,
title = {Dynamic control of district heating networks with integrated emission modelling: A dynamic knowledge graph approach},
journal = {Energy and AI},
volume = {17},
pages = {100376},
year = {2024},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2024.100376},
url = {https://www.sciencedirect.com/science/article/pii/S2666546824000429},
author = {Markus Hofmeister and Kok Foong Lee and Yi-Kai Tsai and Magnus Müller and Karthik Nagarajan and Sebastian Mosbach and Jethro Akroyd and Markus Kraft},
keywords = {Knowledge graph, Digital twin, Interoperability, Energy modelling, Emission dispersion},
abstract = {This paper presents a knowledge graph-based approach for the dynamic control of a district heating network with integrated emission dispersion modelling. We propose an interoperable and extensible implementation to forecast the anticipated heat demand of a municipal heating network, minimise associated total generation cost based on a previously devised methodology, and couple it with dispersion simulations for induced airborne pollutants to provide automatic insights into air quality implications of various heat sourcing strategies. We create cross-domain interoperability in the nexus of energy and air quality via newly developed ontologies and semantic software agents, which can be chained together via The World Avatar dynamic knowledge graph to resemble the behaviour of complex systems. Furthermore, we integrate the City Energy Analyst into this ecosystem to provide building-level insights into energy demand and renewable generation potential to foster strategic analyses and scenario planning. Underlying calculations use building and weather data from the knowledge graph in place of inherent assumptions in the official software release, facilitating a more data-driven approach. All use cases are implemented for a mid-size town in Germany as a proof-of-concept, and a unified visualisation interface is provided, allowing for the examination of 3D buildings alongside their corresponding energy demand and supply time series, as well as emission dispersion data. With this work, we outline the potential of Semantic Web technologies to connect digital twins for holistic energy modelling in smart cities, thereby addressing the increasing complexity of interconnected energy systems.}
}
@article{YOUN2024109072,
title = {FoodAtlas: Automated knowledge extraction of food and chemicals from literature},
journal = {Computers in Biology and Medicine},
volume = {181},
pages = {109072},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109072},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524011570},
author = {Jason Youn and Fangzhou Li and Gabriel Simmons and Shanghyeon Kim and Ilias Tagkopoulos},
keywords = {Nutrition, Food chemical, Data mining, Knowledge graph, Deep learning, Large language model, Link prediction, Quality control},
abstract = {Automated generation of knowledge graphs that accurately capture published information can help with knowledge organization and access, which have the potential to accelerate discovery and innovation. Here, we present an integrated pipeline to construct a large-scale knowledge graph using large language models in an active learning setting. We apply our pipeline to the association of raw food, ingredients, and chemicals, a domain that lacks such knowledge resources. By using an iterative active learning approach of 4120 manually curated premise-hypothesis pairs as training data for ten consecutive cycles, the entailment model extracted 230,848 food-chemical composition relationships from 155,260 scientific papers, with 106,082 (46.0 %) of them never been reported in any published database. To augment the knowledge incorporated in the knowledge graph, we further incorporated information from 5 external databases and ontology sources. We then applied a link prediction model to identify putative food-chemical relationships that were not part of the constructed knowledge graph. Validation of the 443 hypotheses generated by the link prediction model resulted in 355 new food-chemical relationships, while results show that the model score correlates well (R2 = 0.70) with the probability of a novel finding. This work demonstrates how automated learning from literature at scale can accelerate discovery and support practical applications through reproducible, evidence-based capture of latent interactions of diverse entities, such as food and chemicals.}
}
@article{LUO202368,
title = {Extraction and analysis of risk factors from Chinese chemical accident reports},
journal = {Chinese Journal of Chemical Engineering},
volume = {61},
pages = {68-81},
year = {2023},
issn = {1004-9541},
doi = {https://doi.org/10.1016/j.cjche.2023.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S1004954123000848},
author = {Xi Luo and Xiayuan Feng and Xu Ji and Yagu Dang and Li Zhou and Kexin Bi and Yiyang Dai},
keywords = {Chemical processes, Chemical process safety, Natural language process, Knowledge graph, Neural networks, Algorithm},
abstract = {Accidents in chemical production usually result in fatal injury, economic loss and negative social impact. Chemical accident reports which record past accident information, contain a large amount of expert knowledge. However, manually finding out the key factors causing accidents needs reading and analyzing of numerous accident reports, which is time-consuming and labor intensive. Herein, in this paper, a semi-automatic method based on natural language process (NLP) technology is developed to construct a knowledge graph of chemical accidents. Firstly, we build a named entity recognition (NER) model using SoftLexicon (simplify the usage of lexicon) + BERT-Transformer-CRF (conditional random field) to automatically extract the accident information and risk factors. The risk factors leading to accident in chemical accident reports are divided into five categories: human, machine, material, management, and environment. Through analysis of the extraction results of different chemical industries and different accident types, corresponding accident prevention suggestions are given. Secondly, based on the definition of classes and hierarchies of information in chemical accident reports, the seven-step method developed at Stanford University is used to construct the ontology-based chemical accident knowledge description model. Finally, the ontology knowledge description model is imported into the graph database Neo4j, and the knowledge graph is constructed to realize the structured storage of chemical accident knowledge. In the case of information extraction from 290 Chinese chemical accident reports, SoftLexicon + BERT-Transformer-CRF shows the best extraction performance among nine experimental models. Demonstrating that the method developed in the current work can be a promising tool in obtaining the factors causing accidents, which contributes to intelligent accident analysis and auxiliary accident prevention.}
}
@article{ZAPPATORE20231,
title = {Semantic models for IoT sensing to infer environment–wellness relationships},
journal = {Future Generation Computer Systems},
volume = {140},
pages = {1-17},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003211},
author = {Marco Zappatore and Antonella Longo and Angelo Martella and Beniamino {Di Martino} and Antonio Esposito and Serena Angela Gracco},
keywords = {IoT interoperability, Semantic API, Environmental sensing, Mobile Crowd Sensing, Ontology patterns},
abstract = {Every time an Internet of Things (IoT) solution is deployed, every time a smartphone owner connects her/his wireless device to a wearable activity-tracker, every time groups of citizens use geo-mapping applications to move around the city, choosing the least crowded path, data are produced and information have to be exchanged appropriately via APIs. Even if novel added-value IoT-based applications appear on the market with increasing speed, true semantic interoperability is far from being achieved, thus limiting the large-scale exploitation, the scalability and the time-to-market of novel apps. Currently, connecting different data prosumers with multiple data sources is still hampered by the lack of standardized and sustainable solutions, especially due to the significant heterogeneity of IoT platforms. In such a landscape, ontologies come to the rescue, thanks to their formal semantics, knowledge representation formats, and shared vocabularies. In this paper we examine, from an ontological perspective, how to describe environmental sensing and wellness monitoring, two of the most popular application cases of Mobile Crowd Sensing (MCS) and IoT, respectively. To this purpose, an ontology of sensor-agnostic APIs is proposed, along with a set of MCS-dedicated ontology modules (and the supporting platform), leveraging on standard and reusable domain ontologies. Moreover, it will be shown how to properly combine the proposed ontologies in order to support complex functionalities based on inference rules addressing the environment–wellness relationships. Finally, specific semantic modeling patterns suitable for typical IoT and MCS scenarios will be discussed.}
}
@article{CORNISH2023101584,
title = {On the place and role of ‘discourse’ in the Functional Discourse Grammar model. The interface between language system and language use},
journal = {Language Sciences},
volume = {100},
pages = {101584},
year = {2023},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2023.101584},
url = {https://www.sciencedirect.com/science/article/pii/S0388000123000499},
author = {Francis Cornish},
keywords = {Contextualization, Discourse, Functional discourse grammar, Illocution, The lexicon, Text},
abstract = {Mackenzie (2020) is a defense of the position adopted by the architects of the standard model of Functional Discourse Grammar (FDG): namely that the model cannot (and even could never) be considered a ‘grammar of discourse’. The article examines the arguments given for rejecting the ‘discourse’ dimension from the FDG model, proposes an independent account of discourse, and suggests a means of dovetailing it within a model of the wider utterance context. On the one hand, the author's arguments are in the main valid: for ‘discourse’, as characterized in section 3, is not a formal, clearly delineated object amenable to systematic treatment within a grammatical model of a given language. Yet on the other, it is arguable that even the presence of the term ‘discourse’ in the model's name is not in fine justified. Notwithstanding, in order to include the ‘discourse dimension’ (section 3), it is argued that the Core FDG model could be integrated with a broader model of the utterance context involved. This would enable it to account more adequately, for example, for the ways in which indexical reference, the lexicon and adjectival modification operate in actual texts. In turn, it would influence certain of the other characterizations independently assigned within the Core model.}
}
@article{BONISOLI2025113386,
title = {Document-level event extraction from Italian crime news using minimal data},
journal = {Knowledge-Based Systems},
volume = {317},
pages = {113386},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113386},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125004332},
author = {Giovanni Bonisoli and David Vilares and Federica Rollo and Laura Po},
keywords = {Event extraction, Large language models, In-context prompting, Few-shot learning, Prompt tuning, Crime news, Information extraction},
abstract = {Event extraction from unstructured text is a critical task in natural language processing, often requiring substantial annotated data. This study presents an approach to document-level event extraction applied to Italian crime news, utilizing large language models (LLMs) with minimal labeled data. Our method leverages zero-shot prompting and in-context learning to effectively extract relevant event information. We address three key challenges: (1) identifying text spans corresponding to event entities, (2) associating related spans dispersed throughout the text with the same entity, and (3) formatting the extracted data into a structured JSON. The findings are promising: LLMs achieve an F1-score of approximately 60% for detecting event-related text spans, demonstrating their potential even in resource-constrained settings. This work represents a significant advancement in utilizing LLMs for tasks traditionally dependent on extensive data, showing that meaningful results are achievable with minimal data annotation. Additionally, the proposed approach outperforms several baselines, confirming its robustness and adaptability to various event extraction scenarios.}
}
@article{NASRABADI2023104016,
title = {Toward a digital materials mechanical testing lab},
journal = {Computers in Industry},
volume = {153},
pages = {104016},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.104016},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523001665},
author = {Hossein Beygi Nasrabadi and Thomas Hanke and Matthias Weber and Miriam Eisenbart and Felix Bauer and Roy Meissner and Gordian Dziwis and Ladji Tikana and Yue Chen and Birgit Skrotzki},
keywords = {Industry 4.0, Digitalization, Ontology, Mechanical testing, FAIR data},
abstract = {To accelerate the growth of Industry 4.0 technologies, the digitalization of mechanical testing laboratories as one of the main data-driven units of materials processing industries is introduced in this paper. The digital lab infrastructure consists of highly detailed and standard-compliant materials testing knowledge graphs for a wide range of mechanical testing processes, as well as some tools that enable the efficient ontology development and conversion of heterogeneous materials’ mechanical testing data to the machine-readable data of uniform and standardized structures. As a basis for designing such a digital lab, the mechanical testing ontology (MTO) was developed based on the ISO 23718 and ISO/IEC 21838-2 standards for the semantic representation of the mechanical testing experiments, quantities, artifacts, and report data. The trial digitalization of materials mechanical testing lab was successfully performed by utilizing the developed tools and knowledge graph of processes for converting the various experimental test data of heterogeneous structures, languages, and formats to standardized Resource Description Framework (RDF) data formats. The concepts of data storage and data sharing in data spaces were also introduced and SPARQL queries were utilized to evaluate how the introduced approach can result in the data retrieval and response to the competency questions. The proposed digital materials mechanical testing lab approach allows the industries to access lots of trustworthy and traceable mechanical testing data of other academic and industrial organizations, and subsequently organize various data-driven research for their faster and cheaper product development leading to a higher performance of products in engineering and ecological aspects.}
}
@article{LOMBARDO201812,
title = {Semantics–informed geological maps: Conceptual modeling and knowledge encoding},
journal = {Computers & Geosciences},
volume = {116},
pages = {12-22},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0098300416302321},
author = {Vincenzo Lombardo and Fabrizio Piana and Dario Mimmo},
keywords = {Geologic knowledge encoding, Geologic unit ontology, Geodatabase, Geological map, Conceptual modeling of geologic knowledge, Automatic reasoning},
abstract = {This paper introduces a novel, semantics-informed geologic mapping process, whose application domain is the production of a synthetic geologic map of a large administrative region. A number of approaches concerning the expression of geologic knowledge through UML schemata and ontologies have been around for more than a decade. These approaches have yielded resources that concern specific domains, such as, e.g., lithology. We develop a conceptual model that aims at building a digital encoding of several domains of geologic knowledge, in order to support the interoperability of the sources. We apply the devised terminological base to the classification of the elements of a geologic map of the Italian Western Alps and northern Apennines (Piemonte region). The digitally encoded knowledge base is a merged set of ontologies, called OntoGeonous. The encoding process identifies the objects of the semantic encoding, the geologic units, gathers the relevant information about such objects from authoritative resources, such as GeoSciML (giving priority to the application schemata reported in the INSPIRE Encoding Cookbook), and expresses the statements by means of axioms encoded in the Web Ontology Language (OWL). To support interoperability, OntoGeonous interlinks the general concepts by referring to the upper part level of ontology SWEET (developed by NASA), and imports knowledge that is already encoded in ontological format (e.g., ontology Simple Lithology). Machine-readable knowledge allows for consistency checking and for classification of the geological map data through algorithms of automatic reasoning.}
}
@article{BOBED2024123838,
title = {Praedixi, Redegi, Cogitavi: Adaptive knowledge for resource-aware semantic reasoning},
journal = {Expert Systems with Applications},
volume = {250},
pages = {123838},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123838},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424007048},
author = {Carlos Bobed and Fernando Bobillo and Ernesto Jiménez-Ruiz and Eduardo Mena and Jeff Z. Pan},
keywords = {Ontology reasoning, Semantic reasoning, Resource-constrained devices, Ontology modularisation},
abstract = {Representing knowledge with ontologies and performing reasoning with semantic reasoners is important in many intelligent applications. However, existing reasoners do not take into account the available resources of the device where they run, which can be important in many scenarios such as reasoning with very large ontologies or reasoning on resource-constrained mobile devices. In this paper, we propose a novel approach to adapt the size of knowledge managed by applications, taking into account several criteria about resources available (such as time, memory, and battery consumption), at the same time. Thus, rather than giving no answer due to the lack of resources needed to deal with a full ontology, we propose a novel architecture to compute a subontology to provide an incomplete answer at least. Our approach makes use of existing approaches to predict the performance of semantic reasoners and to compute ontology modularisation and ontology partition, but taking into account the associated resource consumption. We also propose a novel measure to estimate the semantic loss when replacing the original ontology by a subontology. Finally, we present an implementation and evaluation of the whole pipeline, showing that the semantic loss incurred in the process is acceptable.}
}
@article{ZOU2025102263,
title = {El Agente: An autonomous agent for quantum chemistry},
journal = {Matter},
volume = {8},
number = {7},
pages = {102263},
year = {2025},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2025.102263},
url = {https://www.sciencedirect.com/science/article/pii/S2590238525003066},
author = {Yunheng Zou and Austin H. Cheng and Abdulrahman Aldossary and Jiaru Bai and Shi Xuan Leong and Jorge Arturo Campos-Gonzalez-Angulo and Changhyeok Choi and Cher Tian Ser and Gary Tom and Andrew Wang and Zijian Zhang and Ilya Yakavets and Han Hao and Chris Crebolder and Varinia Bernales and Alán Aspuru-Guzik},
keywords = {agentic systems, large language models, foundation models, quantum computational chemistry, artificial intelligence, density functional theory, time-dependent density functional theory},
abstract = {Summary
Computational chemistry tools are widely used to study the behavior of chemical phenomena. Yet, the complexity of these tools can make them inaccessible to non-specialists and challenging even for experts. In this work, we introduce El Agente Q, an LLM-based multi-agent system that dynamically generates and executes quantum chemistry workflows from natural language user prompts. The system is built on a novel cognitive architecture featuring a hierarchical memory framework that enables flexible task decomposition, adaptive tool selection, post-analysis, and autonomous file handling and submission. El Agente Q is benchmarked on six university-level course exercises and two case studies, demonstrating robust problem-solving performance (averaging >87% task success) and adaptive error handling through in situ debugging. It also supports longer-term, multi-step task execution for more complex workflows, while maintaining transparency through detailed action trace logs. Together, these capabilities lay the foundation for increasingly autonomous and accessible quantum chemistry.}
}
@article{RAFALKO2025100204,
title = {On the use of natural language processing to implement the target trial framework using unstructured data from the electronic health record},
journal = {Global Epidemiology},
volume = {9},
pages = {100204},
year = {2025},
issn = {2590-1133},
doi = {https://doi.org/10.1016/j.gloepi.2025.100204},
url = {https://www.sciencedirect.com/science/article/pii/S2590113325000227},
author = {Nicole Rafalko and Milena Gianfrancesco and Neal D. Goldstein},
keywords = {Electronic health record, Natural language processing, Observational research, Target trial framework},
abstract = {The increasing availability and accessibility of electronic health record (EHR) data has made it a rich secondary source to conduct comparative effectiveness studies. To perform such studies, many researchers are turning to the target trial framework (TTF) to emulate the hypothetical randomized clinical trial. The quality of this emulation depends, in part, on the availability and accessibility of data for each component of the TTF. Yet one overarching challenge with using EHR data is that unstructured fields, such as clinical encounter notes, contain copious details on the patient yet require additional steps to extract if needed in the conduct of the study. Natural language processing (NLP) represents a spectrum of methods to assist with automating this extraction, from simpler rule-based methods to machine learning and artificial intelligence approaches that can handle complex language structures. What follows is a discussion on how NLP methods can augment information and data for researchers looking to estimate a treatment effect using EHR data via the TTF to emulate the hypothetical clinical trial. We conclude with recommendations for researchers interested in using NLP methods to obtain data stored in the free text of the EHR as well as considerations regarding the quality and validity of this data for the TTF.}
}
@article{FUELLEN2025102617,
title = {Validation requirements for AI-based intervention-evaluation in aging and longevity research and practice},
journal = {Ageing Research Reviews},
volume = {104},
pages = {102617},
year = {2025},
issn = {1568-1637},
doi = {https://doi.org/10.1016/j.arr.2024.102617},
url = {https://www.sciencedirect.com/science/article/pii/S1568163724004355},
author = {Georg Fuellen and Anton Kulaga and Sebastian Lobentanzer and Maximilian Unfried and Roberto A. Avelar and Daniel Palmer and Brian K. Kennedy},
keywords = {Longevity, Preventive Medicine, Large Language Models},
abstract = {The field of aging and longevity research is overwhelmed by vast amounts of data, calling for the use of Artificial Intelligence (AI), including Large Language Models (LLMs), for the evaluation of geroprotective interventions. Such evaluations should be correct, useful, comprehensive, explainable, and they should consider causality, interdisciplinarity, adherence to standards, longitudinal data and known aging biology. In particular, comprehensive analyses should go beyond comparing data based on canonical biomedical databases, suggesting the use of AI to interpret changes in biomarkers and outcomes. Our requirements motivate the use of LLMs with Knowledge Graphs and dedicated workflows employing, e.g., Retrieval-Augmented Generation. While naive trust in the responses of AI tools can cause harm, adding our requirements to LLM queries can improve response quality, calling for benchmarking efforts and justifying the informed use of LLMs for advice on longevity interventions.}
}
@article{KOIRALA2024100196,
title = {Digitalization of urban multi-energy systems – Advances in digital twin applications across life-cycle phases},
journal = {Advances in Applied Energy},
volume = {16},
pages = {100196},
year = {2024},
issn = {2666-7924},
doi = {https://doi.org/10.1016/j.adapen.2024.100196},
url = {https://www.sciencedirect.com/science/article/pii/S2666792424000349},
author = {B. Koirala and H. Cai and F. Khayatian and E. Munoz and J.G. An and R. Mutschler and M. Sulzer and C. {De Wolf} and K. Orehounig},
keywords = {Digital twin, Urban energy systems, Digitalization, Modeling, Data, Ontologies, Life-cycle phases},
abstract = {Urban multi-energy systems (UMES) incorporating distributed energy resources are vital to future low-carbon energy systems. These systems demand complex solutions, including increased integration of renewables, improved efficiency through electrification, and exploitation of synergies via sector coupling across multiple sectors and infrastructures. Digitalization and the Internet of Things bring new opportunities for the design-build-operate workflow of the cyber-physical urban multi-energy systems. In this context, digital twins are expected to play a crucial role in managing the intricate integration of assets, systems, and actors within urban multi-energy systems. This review explores digital twin opportunities for urban multi-energy systems by first considering the challenges of urban multi energy systems. It then reviews recent advancements in digital twin architectures, energy system data categories, semantic ontologies, and data management solutions, addressing the growing data demands and modelling complexities. Digital twins provide an objective and comprehensive information base covering the entire design, operation, decommissioning, and reuse lifecycle phases, enhancing collaborative decision-making among stakeholders. This review also highlights that future research should focus on scaling digital twins to manage the complexities of urban environments. A key challenge remains in identifying standardized ontologies for seamless data exchange and interoperability between energy systems and sectors. As the technology matures, future research is required to explore the socio-economic and regulatory implications of digital twins, ensuring that the transition to smart energy systems is both technologically sound and socially equitable. The paper concludes by making a series of recommendations on how digital twins could be implemented for urban multi energy systems.}
}
@article{SHOAIP20213531,
title = {Alzheimer’s Disease Diagnosis Based on a Semantic Rule-Based Modeling and Reasoning Approach},
journal = {Computers, Materials and Continua},
volume = {69},
number = {3},
pages = {3531-3548},
year = {2021},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2021.019069},
url = {https://www.sciencedirect.com/science/article/pii/S1546221821012339},
author = {Nora Shoaip and Amira Rezk and Shaker EL-Sappagh and Tamer Abuhmed and Sherif Barakat and Mohammed Elmogy},
keywords = {Mild cognitive impairment, Alzheimer’s disease, knowledge based, semantic web rule language, reasoning system, ADNI dataset, machine learning techniques},
abstract = {Alzheimer’s disease (AD) is a very complex disease that causes brain failure, then eventually, dementia ensues. It is a global health problem. 99% of clinical trials have failed to limit the progression of this disease. The risks and barriers to detecting AD are huge as pathological events begin decades before appearing clinical symptoms. Therapies for AD are likely to be more helpful if the diagnosis is determined early before the final stage of neurological dysfunction. In this regard, the need becomes more urgent for biomarker-based detection. A key issue in understanding AD is the need to solve complex and high-dimensional datasets and heterogeneous biomarkers, such as genetics, magnetic resonance imaging (MRI), cerebrospinal fluid (CSF), and cognitive scores. Establishing an interpretable reasoning system and performing interoperability that achieves in terms of a semantic model is potentially very useful. Thus, our aim in this work is to propose an interpretable approach to detect AD based on Alzheimer’s disease diagnosis ontology (ADDO) and the expression of semantic web rule language (SWRL). This work implements an ontology-based application that exploits three different machine learning models. These models are random forest (RF), JRip, and J48, which have been used along with the voting ensemble. ADNI dataset was used for this study. The proposed classifier’s result with the voting ensemble achieves a higher accuracy of 94.1% and precision of 94.3%. Our approach provides effective inference rules. Besides, it contributes to a real, accurate, and interpretable classifier model based on various AD biomarkers for inferring whether the subject is a normal cognitive (NC), significant memory concern (SMC), early mild cognitive impairment (EMCI), late mild cognitive impairment (LMCI), or AD.}
}
@article{DIAMANTINI2023224,
title = {Process-aware IIoT Knowledge Graph: A semantic model for Industrial IoT integration and analytics},
journal = {Future Generation Computer Systems},
volume = {139},
pages = {224-238},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2200320X},
author = {Claudia Diamantini and Alex Mircoli and Domenico Potena and Emanuele Storti},
keywords = {Industrial Internet of Things, Data integration, Business process, Semantics, Ontology, Knowledge Graph},
abstract = {The integration of the huge data streams produced by the Industrial Internet of Things (IIoT) can provide invaluable knowledge in the context of Industry 4.0, and is also an open research issue. The present paper proposes a semantic approach to this issue, centred around the notion of process as the backbone. We build an ontology describing the fundamental elements involved in IIoT and their relations, and discuss the construction of the Process-aware IIoT Knowledge Graph, where raw sensor data are enriched with information about process activities and the physical production environment. We also propose a framework for querying the Knowledge Graph, and we demonstrate its capabilities by considering the production of metal accessories as case study.}
}
@article{CHARI2023102498,
title = {Informing clinical assessment by contextualizing post-hoc explanations of risk prediction models in type-2 diabetes},
journal = {Artificial Intelligence in Medicine},
volume = {137},
pages = {102498},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102498},
url = {https://www.sciencedirect.com/science/article/pii/S093336572300012X},
author = {Shruthi Chari and Prasant Acharya and Daniel M. Gruen and Olivia Zhang and Elif K. Eyigoz and Mohamed Ghalwash and Oshani Seneviratne and Fernando Suarez Saiz and Pablo Meyer and Prithwish Chakraborty and Deborah L. McGuinness},
keywords = {User-driven, Clinical explainability, Contextual explanations, Question-answering approach, Type-2 diabetes comorbidity risk prediction},
abstract = {Medical experts may use Artificial Intelligence (AI) systems with greater trust if these are supported by ‘contextual explanations’ that let the practitioner connect system inferences to their context of use. However, their importance in improving model usage and understanding has not been extensively studied. Hence, we consider a comorbidity risk prediction scenario and focus on contexts regarding the patients’ clinical state, AI predictions about their risk of complications, and algorithmic explanations supporting the predictions. We explore how relevant information for such dimensions can be extracted from Medical guidelines to answer typical questions from clinical practitioners. We identify this as a question answering (QA) task and employ several state-of-the-art Large Language Models (LLM) to present contexts around risk prediction model inferences and evaluate their acceptability. Finally, we study the benefits of contextual explanations by building an end-to-end AI pipeline including data cohorting, AI risk modeling, post-hoc model explanations, and prototyped a visual dashboard to present the combined insights from different context dimensions and data sources, while predicting and identifying the drivers of risk of Chronic Kidney Disease (CKD) - a common type-2 diabetes (T2DM) comorbidity. All of these steps were performed in deep engagement with medical experts, including a final evaluation of the dashboard results by an expert medical panel. We show that LLMs, in particular BERT and SciBERT, can be readily deployed to extract some relevant explanations to support clinical usage. To understand the value-add of the contextual explanations, the expert panel evaluated these regarding actionable insights in the relevant clinical setting. Overall, our paper is one of the first end-to-end analyses identifying the feasibility and benefits of contextual explanations in a real-world clinical use case. Our findings can help improve clinicians’ usage of AI models.}
}
@article{RIESENER2024339,
title = {Active Learning with Pre-trained Language Models for Named Entity Recognition in Requirements Engineering},
journal = {Procedia CIRP},
volume = {128},
pages = {339-344},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.06.027},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124007078},
author = {M. Riesener and M. Kuhn and S. Schümmelfeder and D. Xiao and J. Norheim and E. Rebentisch and G. Schuh},
keywords = {Named Entity Recognition, Language Model, Active Learning, Query Strategy, Requirements Engineering},
abstract = {The ever-increasing number of textual requirements and the handling of these consumes significant time in product design. Therefore, automating requirements analysis with natural language processing is critical to increase efficiency. Pre-trained language models have proven useful for requirements analysis. However, to deploy pre-trained language models, a fully labeled dataset must first be created to finetune language models towards the requirements domain. The creation of a fully labeled dataset, especially for named entity recognition (NER) applications, poses a major challenge as it entails high manual labeling effort by requirements engineers. Furthermore, in an ever-changing requirements environment, the manual labeling process must be repeated to avoid model drift, adding additional workload. Recent advances of pre-trained language models combined with active learning can significantly reduce the manual labeling effort. Thus, we present a novel approach for leveraging active learning with pre-trained language models for NER. We demonstrate the efficacy of this combination in an experiment, where we are able to reduce the manual requirements labeling effort by 74% while even improving model performance. We argue that by reducing the manual effort, active learning can shorten the ramp-up time of language model deployment for requirements engineering and enable industrial adoption.}
}
@article{CHANG2022104118,
title = {A vector-based semantic relatedness measure using multiple relations within SNOMED CT and UMLS},
journal = {Journal of Biomedical Informatics},
volume = {131},
pages = {104118},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104118},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422001344},
author = {Eunsuk Chang},
keywords = {UMLS, SNOMED CT, Similarity, Relatedness, Word vector, Natural language processing},
abstract = {Objective
To propose a new vector-based relatedness metric that derives word vectors from the intrinsic structure of biomedical ontologies, without consulting external resources such as large-scale biomedical corpora.
Materials and Methods
SNOMED CT on the mapping layer of UMLS was used as a testbed ontology. Vectors were created for every concept at the end of all semantic relations—attribute–value relations and descendants as well as is_a relation—of the defining concept. The cosine similarity between the averages of those vectors with respect to each defining concept was computed to produce a final semantic relatedness.
Results
Two benchmark sets that include a total of 62 biomedical term pairs were used for evaluation. Spearman’s rank coefficient of the current method was 0.655, 0.744, and 0.742 with the relatedness rated by physicians, coders, and medical experts, respectively. The proposed method was comparable to a word-embedding method and outperformed path-based, information content-based, and another multiple relation-based relatedness metrics.
Discussion
The current study demonstrated that the addition of attribute relations to the is_a hierarchy of SNOMED CT better conforms to the human sense of relatedness than models based on taxonomic relations. The current approach also showed that it is robust to the design inconsistency of ontologies.
Conclusion
Unlike the previous vector-based approach, the current study exploited the intrinsic semantic structure of an ontology, precluding the need for external textual resources to obtain context information of defining terms. Future research is recommended to prove the validity of the current method with other biomedical ontologies.}
}
@article{ZENG2022102923,
title = {User-interactive innovation knowledge acquisition model based on social media},
journal = {Information Processing & Management},
volume = {59},
number = {3},
pages = {102923},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.102923},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322000474},
author = {Dalin Zeng and Jinghua Zhao and Wei Zhang and Yan Zhou},
keywords = {Interactive innovation, Ontology modeling, Social media, Latent Dirichlet allocation},
abstract = {Mainstream social media, such as Facebook, Twitter, and Weibo, provide enterprises an opportunity to innovate and develop. User-generated content on social media platforms can help determine the needs of the user and identify a target market, providing a basis for enterprise innovation. In this study, we propose a user-interactive innovation knowledge acquisition model. Accordingly, the comments data on a selected forum were first crawled using network crawler software. Subsequently, we pre-processed the data to obtain a semi-structured user corpus. We then used the Latent Dirichlet Allocation model to cluster topics and obtain the subject words that were hidden from each comment text. A user demand ontology was built based on the subject words, and with an expert's reference, the product function ontology was established. Through semantic similarity matching, we integrated two ontologies to obtain the user-interactive innovation knowledge acquisition model. Finally, the model was validated using the Volvo XC60 automobile as an example. The empirical results showed that the proposed model could assist enterprises by providing ideas for follow-up innovation and product development.}
}
@article{SUTHAR2025453,
title = {Exploring the Landscape of Natural Language Processing for Text Analytics: A comprehensive Review},
journal = {Procedia Computer Science},
volume = {259},
pages = {453-462},
year = {2025},
note = {Sixth International Conference on Futuristic Trends in Networks and Computing Technologies (FTNCT06), held in Uttarakhand, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.03.347},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925010919},
author = {Om Prakash Suthar and Ankita Mishra and Shilpa Singhal},
keywords = {NLP, LDA, NMF, text analytics, text extraction},
abstract = {The amount of textual information that can be analyzed in order to look for meaningful information has become a constraint as the amount of digital content that is being produced everyday increases. When it comes to mining large text datasets for useful information, NLP methods and models are necessary and extremely effective tools. The paper’s primary objective is to present a comprehensive review of the NLP methods and models that are utilized for text analytics, sentiment analysis, topic modelling, text summarization, and text generation. In this paper, we will discuss the trending methodologies for social media analysis, consumer opinion analysis, and content creation. In addition, we will discuss the methodologies, methods, and evaluation metrics that are utilized in these types of contexts. This analysis aims to provide context for the development of natural language processing (NLP) in the context of text analytics, both historically and prospectively. This will be accomplished by providing context for the development of natural language processing in the context of text analytics.}
}
@incollection{KUMAR2024237,
title = {Chapter Twelve - Human AI: Explainable and responsible models in computer vision},
editor = {Muskan Garg and Deepika Koundal},
booktitle = {Emotional AI and Human-AI Interactions in Social Networking},
publisher = {Academic Press},
pages = {237-254},
year = {2024},
isbn = {978-0-443-19096-4},
doi = {https://doi.org/10.1016/B978-0-443-19096-4.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190964000067},
author = {Kukatlapalli Pradeep Kumar and Michael Moses Thiruthuvanathan and Swathikiran K.K. and Duvvi Roopesh Chandra},
keywords = {Computer vision and image processing, Explainable AI, Ontology, Responsible AI, Unobtrusive student engagement analysis},
abstract = {Artificial intelligence (AI) is being used in all areas of information, research, and technology. Allied parts of AI have to be investigated for understanding the association among them. Human and explainable AI (XAI) are a few examples that can help in the development of understandable systems. Posthoc actions and operations are geared toward explainable AI, which investigates “what went wrong” in a black box setting. Responsible AI, on the other hand, seeks to avoid such blunders in the first ring. Ontology is defined as the “study of existence” and has several applications in computer science, specifically in platforms such as Resource Description Framework and Web Ontology Language. In this chapter, we examine both parts of the aforementioned AI and attempt to establish a link between ontology and explainable AI as they complement each other in terms of creating trustworthy systems. As part of the chapter, an applicable literature is also brought in, emphasizing the necessity of current understanding in explainable and responsible AI. For illustrating the lineage of input and output operations in relation to ontology characteristics and AI, a scenario of AI implementation using image processing dataset is studied. Classroom learning is an integral element of every student's daily life. Assessing the interest levels of individual pupils would help in enhancing the process of teaching and learning. This work contributes to the process of explainable AI by eliciting algorithms that can extract faces from frames, recognize emotions, conduct studies on engagement levels, and provide a session-wide analysis. Detailed descriptions of these operations, as well as specific parameters, are provided to relate the theme of work. We feel that this collaboration between ontology and explainable AI is unique in that it acts as a springboard for future study in these domains.}
}
@article{INOUE2023101582,
title = {Toward an ecological model of language: from cognitive linguistics to ecological semantics},
journal = {Language Sciences},
volume = {100},
pages = {101582},
year = {2023},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2023.101582},
url = {https://www.sciencedirect.com/science/article/pii/S0388000123000475},
author = {Takuya Inoue},
keywords = {Affordances, Cognitive Linguistics, Design, Ecological semantics, Visualization},
abstract = {The ecological perspective of language has gained prominence in linguistics over the past two decades. Since its anti-representationalist and anti-cognitivist stance, the ecological approach faces a challenge in reconciling with modern linguistic theories: While the ecological approach focuses on the dynamic aspects of language, it has been criticized for needing help to account for stable linguistic meaning. To address this issue, Cognitive Linguistics is the best candidate for giving an ecological account of static meaning. Also, I introduce the concept of design to establish the ecological model of language and demonstrate how this model can describe linguistic meaning within an ecological framework. Cognitive Linguistics develops into the ecological theory of meaning through these steps, namely ecological semantics.}
}
@article{NZETCHOU2022103575,
title = {Semantic enrichment approach for low-level CAD models managed in PLM context: Literature review and research prospect},
journal = {Computers in Industry},
volume = {135},
pages = {103575},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103575},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001822},
author = {Stéphane Nzetchou and Alexandre Durupt and Sébastien Remy and Benoit Eynard},
keywords = {Computer-aided design, Standardisation, Product lifecycle management, Knowledge management, Ontology},
abstract = {The proposed approach in the paper is dedicated to enrichment of CAD models storage in PLM (Product Lifecycle Management) systems or archive databases. The paper considers low level CAD models (i.e. frozen geometry and without CAD model tree) based on standards such as STL, IGES and STEP AP203 first edition where the CAD model tree is absent. The paper also highlights the differences in semantic richness between these standards and their degrees of industrial implementation. STEP AP242 standard is considered as a high level representation for CAD files. The paper aims to review the literature that addresses the methods for semantic enrichment of CAD models by using ontologies. The future challenges and one possible research direction are then discussed. A first application called VAQUERO for CAD enrichment using an ontology based on STEP AP242 standard is proposed.}
}
@article{YANG2024107497,
title = {Application of question answering systems for intelligent agriculture production and sustainable management: A review},
journal = {Resources, Conservation and Recycling},
volume = {204},
pages = {107497},
year = {2024},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2024.107497},
url = {https://www.sciencedirect.com/science/article/pii/S0921344924000910},
author = {Tian Yang and Yupeng Mei and Ling Xu and Huihui Yu and Yingyi Chen},
keywords = {Question answering system, Intelligent agriculture, Knowledge graphs, Large language models},
abstract = {The increasing application of artificial intelligence in agriculture production and management has generated a large amount of data, leading to a demand for processing this data. This review focuses on the knowledge storage approaches in agricultural question answering systems, namely corpora, knowledge graphs, and large language models. These systems are built on massive amounts of data and aim to process and retrieve information effectively in the context of sustainable agriculture. Corpora refer to large collections of diverse documents that serve as foundational resources for training and fine-tuning question answering systems. Knowledge graphs capture structured and interconnected knowledge by representing entities, relationships, and attributes, enabling efficient organization and querying of information. Large language models, such as GPT-4, enhance the capacity of question answering systems to provide accurate and relevant responses. By exploring these three prominent knowledge storage approaches, this review analyses the methodology and impact of agricultural question answering systems, highlighting their applications in the production process. The findings provide important implications for future research in agriculture, and potential directions for further exploration.}
}
@article{YIN2025111359,
title = {Context-aware prompt learning for test-time vision recognition with frozen vision-language model},
journal = {Pattern Recognition},
volume = {162},
pages = {111359},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2025.111359},
url = {https://www.sciencedirect.com/science/article/pii/S0031320325000196},
author = {Junhui Yin and Xinyu Zhang and Lin Wu and Xiaojie Wang},
keywords = {In-context learning, Prompt learning, Vision-language model, Vision recognition, Test-time adaptation},
abstract = {Current pre-trained vision-language models, such as CLIP, have demonstrated remarkable zero-shot generalization capabilities across various downstream tasks. However, their performance significantly degrades when test inputs exhibit different distributions. In this paper, we explore the concept of test-time prompt tuning (TTPT), which facilitates the adaptation of the CLIP model to novel downstream tasks through a one-step unsupervised optimization that involves only test samples. Inspired by in-context learning in natural language processing (NLP), we propose Context-aware Prompt Learning (CaPL) for test-time visual recognition tasks, which empowers a pre-trained vision-language model with labeled examples as context information on downstream tasks. Specifically, CaPL associates a new test sample with very few labeled examples (sometimes just one) as context information, enabling reliable label estimation for the test sample and facilitating model adaptation. To achieve this, CaPL employs an efficient language-to-vision translator to explore the textual prior information for visual prompt learning. Further, we introduce a context-aware unsupervised loss to optimize visual prompts tailored to test samples. Finally, we design a cyclic learning strategy for visual and textual prompts to ensure mutual synergy across different modalities. This enables a pre-trained, frozen CLIP model to adapt to any task using its learned adaptive prompt. Our method demonstrates superior performance and achieves state-of-the-art results across various downstream datasets. The code is available at: https://github.com/yjh576/CaPL.}
}
@article{ZHAO2025103746,
title = {A knowledge graph-driven framework of multi-stakeholder synergistic operation and maintenance for complex products: design, implementation and industrial validation},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103746},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103746},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625006391},
author = {Xin Zhao and Ruixuan Wang and Shan Ren and Geng Zhang and Yingfeng Zhang},
keywords = {Complex products, Knowledge graph, Knowledge navigation, Synergistic operation and maintenance},
abstract = {To ensure the optimal-state operation, low maintenance costs and optimized resource allocation, synergistic operation and maintenance (OM) mode of complex products (CPs) is becoming a current research hotspot. However, multiple stakeholders (e.g. user, manufacturer and service provider) involved in the synergistic OM of CPs are facing challenges in integrating heterogeneous data, resolving ambiguities in service requirements and generating explainable maintenance strategies due to inconsistent data semantics and fragmented knowledge divide. Moreover, although the promising application of knowledge graph (KG) to handle large-scale data semantically and resulting in better knowledge sharing, the research simultaneously combing multiple stakeholder synergistic OM and KG is in its infancy. To address these challenges and problems, a KG-driven framework of multi-stakeholder synergistic OM for CPs is developed is this paper. Then, a formal ontology modelling method for consistent representation of multi-stakeholder heterogeneous data and a Naive Bayes-enabled knowledge navigation model for reducing the ambiguities of multi-stakeholder OM information are proposed to provide technical support for effective implementation of the framework. Finally, an industrial application scenario of an electric multiple units (EMU) trailer bogie OM is presented to validate the feasibility and effectiveness of the framework. The result shows that the developed framework achieves 87.8% accuracy in OM knowledge classification and retrieval (e.g., fault cause identification and maintenance strategy recommendation). Therefore, it can be used to effectively facilitate the heterogeneous data using and implement the multi-source knowledge sharing among different stakeholders for generating explainable and actionable synergistic OM strategies for unfamiliar maintenance tasks.}
}
@article{NEIVA2024634,
title = {ICD-10 - ORPHA: An Interactive Complex Network Model for Brazilian Rare Diseases},
journal = {Procedia Computer Science},
volume = {239},
pages = {634-642},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.218},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924014613},
author = {Mariane Barros Neiva and Bibiana Mello {de Oliveira} and Amanda Maria Schmidt and Victória Machado Scheibe and Júlia Cordeiro Milke and Mariana Lopes {dos Santos} and Diego Bettiol Yamada and Márcio Eloi {Colombo Filho} and Giovane Thomazini Soares and Yasmin {de Araújo Ribeiro} and Odemir Martinez Bruno and Têmis Maria Félix and Domingos Alves and  {RARAS Network group}},
keywords = {Rare Disease, Complex Network, Biomedical Ontology, Graph Modeling},
abstract = {A disease is considered rare if it has a low prevalence. It is estimated that around 400 million people worldwide have a rare disease, including 15 million in Brazil. Consequently, it became a public health priority for the World Health Organization and the Brazilian Health Ministry. In 2014, the Brazilian government launched a national policy regarding the care for rare patients’, the Ordinance nº199. The national politic defines guidelines, procedures, and descriptions of rare disease codes to provide access and diagnosis in the public health system to reduce mortality and improve patient’s quality of life. Diseases are identified according to the International Classification of Diseases 10th Revision, a widely used terminology in this context. However, there are also different terminologies to codify a rare disease, such as the ORPHAcode provided by Orphanet. This paper proposes a complex network model using the terminologies’ relationship to show that the International Classification of Diseases 10th Revision may be generic for diagnosing rare Brazilian patients. Moreover, there is no perfect nomenclature to define rare diseases, but each context has a better application. So, mapping the relationship between each terminology is fundamental for creating consistent semantic relationships in biomedical ontologies, providing a functional environment for carrying out tasks involving more than one terminology.}
}
@article{DAMICO2024366,
title = {Knowledge transfer in Digital Twins: The methodology to develop Cognitive Digital Twins},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {52},
pages = {366-385},
year = {2024},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2024.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1755581724000932},
author = {Rosario Davide D’Amico and Arkopaul Sarkar and Mohamed Hedi Karray and Sri Addepalli and John Ahmet Erkoyuncu},
keywords = {Basic Formal Ontology (BFO), Common Core Ontology (CCO), Connected Digital Twins, Digital Twin, Industrial Ontology Foundry (IOF), Knowledge Graph, Semantics, Top-Level Ontology},
abstract = {In the realm of Digital Twins (DTs), industry experts have emphasised the pivotal concept of the Federation of Twins, envisioning seamless collaboration across sectors driven by shared semantics. In response to this challenge, the Cognitive Digital Twin (CDT) integrates the DT framework with formal semantics, specifically ontologies. This paper introduces a comprehensive five-step methodology for CDT development. Furthermore, it becomes possible to incorporate human expertise into the DT ecosystem by adopting an ontological approach. The CDT enhances DT services with advanced reasoning capabilities, leading to a profound semantic enrichment of the data. The presented methodology has been validated using a use case where the CDT is employed to detect malfunctions, significantly reducing manual intervention. This paper advocates for the adoption of CDTs, which represent a harmonious fusion of formal semantics and human expertise, enhancing system efficiency and operational performance.}
}
@article{BARESI2025106232,
title = {Neo-colonial intelligence: How AI risks reinforcing spatial injustices in a digitally divided world},
journal = {Cities},
volume = {166},
pages = {106232},
year = {2025},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2025.106232},
url = {https://www.sciencedirect.com/science/article/pii/S0264275125005335},
author = {Umberto Baresi},
keywords = {Artificial intelligence, Spatial injustice, Neocolonialism, Urban governance, Digital divide, Epistemic violence},
abstract = {Artificial intelligence (AI) is rapidly transforming urban planning processes, yet its deployment risks reinforcing historical patterns of spatial injustice. This Viewpoint critically examines the colonial traits embedded in AI systems applied to urban governance, arguing that algorithmic models often replicate epistemic violence, territorial exclusion, and data-driven extractivism. Drawing from political theory, ecotheology, and postcolonial urbanism, it highlights real-world cases, including smart city projects and geospatial data systems that illustrate how AI perpetuates neocolonial spatial dynamics. The piece calls for reorientation towards decolonial, participatory, and ecologically-centered AI design practices, emphasizing the need for ethical frameworks that honor diverse ontologies and spiritual connections to land. Without such critical engagement, AI risks becoming a new architecture of digital colonialism rather than a tool for just, inclusive, and sustainable urban futures.}
}
@article{GROSMAN2020101553,
title = {Eras: Improving the quality control in the annotation process for Natural Language Processing tasks},
journal = {Information Systems},
volume = {93},
pages = {101553},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101553},
url = {https://www.sciencedirect.com/science/article/pii/S0306437920300521},
author = {Jonatas S. Grosman and Pedro H.T. Furtado and Ariane M.B. Rodrigues and Guilherme G. Schardong and Simone D.J. Barbosa and Hélio C.V. Lopes},
keywords = {Annotation tool, Ontology-based annotation, Entities and relations annotation, Corpus curation, Natural Language Processing},
abstract = {The increasing amount of valuable, unstructured textual information poses a major challenge to extract value from those texts. We need to use NLP (Natural Language Processing) techniques, most of which rely on manually annotating a large corpus of text for its development and evaluation. Creating a large annotated corpus is laborious and requires suitable computational support. There are many annotation tools available, but their main weaknesses are the absence of data management features for quality control and the need for a commercial license. As the quality of the data used to train an NLP model directly affects the quality of the results, the quality control of the annotations is essential. In this paper, we introduce ERAS, a novel web-based text annotation tool developed to facilitate and manage the process of text annotation. ERAS includes not only the key features of current mainstream annotation systems but also other features necessary to improve the curation process, such as the inter-annotator agreement, self-agreement and annotation log visualization, for annotation quality control. ERAS also implements a series of features to improve the customization of the user’s annotation workflow, such as: random document selection, re-annotation stages, and warm-up annotations. We conducted two empirical studies to evaluate the tool’s support to text annotation, and the results suggest that the tool not only meets the basic needs of the annotation task but also has some important advantages over the other tools evaluated in the studies. ERAS is freely available at https://github.com/grosmanjs/eras.}
}
@article{WANG2024102848,
title = {Medical knowledge graph completion via fusion of entity description and type information},
journal = {Artificial Intelligence in Medicine},
volume = {151},
pages = {102848},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102848},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724000903},
author = {Xiaochen Wang and Runtong Zhang and Butian Zhao and Yuhan Yao and Hongmei Zhao and Xiaomin Zhu},
keywords = {Link prediction, Entity property set, Graph embeddings, Language embeddings, BioBERT, Information fusion},
abstract = {Medical Knowledge Graphs (MKGs) are vital in propelling big data technologies in healthcare and facilitating the realization of medical intelligence. However, large-scale MKGs often exhibit characteristics of data sparsity and missing facts. Following the latest advances, knowledge embedding addresses these problems by performing knowledge graph completion. Most knowledge embedding algorithms rely solely on triplet structural information, overlooking the rich information hidden within entity property sets, leading to bottlenecks in performance enhancement when dealing with the intricate relations of MKGs. Inspired by the semantic sensitivity and explicit type constraints unique to the medical domain, we propose BioBERT-based graph embedding model. This model represents an evolvable framework that integrates graph embedding, language embedding, and type information, thereby optimizing the utility of MKGs. Our study utilizes not only WordNet as a benchmark dataset but also incorporates MedicalKG to compare and corroborate the specificity of medical knowledge. Experimental results on these datasets indicate that the proposed fusion framework achieves state-of-art (SOTA) performance compared to other baselines. We believe that this incremental improvement provides promising insights for future medical knowledge graph completion endeavors.}
}
@article{PANEQUE2023118892,
title = {e-LION: Data integration semantic model to enhance predictive analytics in e-Learning},
journal = {Expert Systems with Applications},
volume = {213},
pages = {118892},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118892},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422019108},
author = {Manuel Paneque and María del Mar Roldán-García and José García-Nieto},
keywords = {E-learning, Ontology, Open data, Data analysis, Knowledge graph},
abstract = {In the last years, Learning Management systems (LMSs) are acquiring great importance in online education, since they offer flexible integration platforms for organising a vast amount of learning resources, as well as for establishing effective communication channels between teachers and learners, at any direction. These online platforms are then attracting an increasing number of users that continuously access, download/upload resources and interact each other during their teaching/learning processes, which is even accelerating by the breakout of COVID-19. In this context, academic institutions are generating large volumes of learning-related data that can be analysed for supporting teachers in lesson, course or faculty degree planning, as well as administrations in university strategic level. However, managing such amount of data, usually coming from multiple heterogeneous sources and with attributes sometimes reflecting semantic inconsistencies, constitutes an emerging challenge, so they require common definition and integration schemes to easily fuse them, with the aim of efficiently feeding machine learning models. In this regard, semantic web technologies arise as a useful framework for the semantic integration of multi-source e-learning data, allowing the consolidation, linkage and advanced querying in a systematic way. With this motivation, the e-LION (e-Learning Integration ONtology) semantic model is proposed for the first time in this work to operate as data consolidation approach of different e-learning knowledge-bases, hence leading to enrich on-top analysis. For demonstration purposes, the proposed ontological model is populated with real-world private and public data sources from different LMSs referring university courses of the Software Engineering degree of the University of Malaga (Spain) and the Open University Learning. In this regard, a set of four case studies are worked for validation, which comprise advance semantic querying of data for feeding predictive modelling and time-series forecasting of students’ interactions according to their final grades, as well as the generation of SWRL reasoning rules for student’s behaviour classification. The results are promising and lead to the possible use of e-LION as ontological mediator scheme for the integration of new future semantic models in the domain of e-learning.}
}
@article{TENG20251125,
title = {Cerebrum twin: A 6D semantic digital twin of multi-lobe digital brain functions for human-centric Industry 5.0},
journal = {Journal of Manufacturing Systems},
volume = {82},
pages = {1125-1144},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525002079},
author = {Hanwei Teng and Shuo Chen and Changping Li and Shujian Li and Rendi Kurniawan and Moran Xu and Jielin Chen and Tae Jo Ko},
keywords = {Semantic digital twin, CNC system, Information fusion, Remote/Real-time Control and Inspection, Multi-Lobe digital brain, Cerebrum twin},
abstract = {Industry 5.0 highlights the need for human-centric and adaptive intelligence in smart manufacturing. This paper proposes the Cerebrum Twin (CT), a brain-inspired, six-dimensional semantic digital twin (SDT) system that unifies five human-like senses, including listening, speaking, reading, writing, and looking, within a cohesive multi-lobe digital brain framework. CT integrates real-time physical signals from force, vibration, and vision sensors by leveraging a synergistic ensemble of advanced artificial intelligence (AI) modules, such as Extreme Gradient Boosting (XGBoost), ConvNeXt V2, Efficient Sub-Pixel Convolutional Networks (ESPCN), stacked sparse autoencoder with supervision (SSAES), large language models (LLM), and reinforcement learning (RL). Uniquely, CT establishes a closed-loop semantic feedback mechanism, enabling dynamic perception, multimodal semantic abstraction, signal-driven prediction, adaptive parameter optimization, and intuitive voice-based human interaction. This holistic integration bridges the physical, semantic, and cognitive layers of CNC machining, supporting robust, transparent, and operator-oriented decision-making. The proposed system was validated through ultrasonic vibration-assisted blade dicing (UVABD) experiments. CT reduced dicing force prediction error by 39.86 %, improved tool wear prediction accuracy by 29.59 %, and decreased edge chipping severity by 60.47 % compared to the baseline model. These results demonstrate that a semantically empowered, multisensory digital twin (DT), enabled by real-time physical–semantic–AI fusion and human-in-the-loop optimization, can significantly enhance intelligent manufacturing performance and fulfill the vision of Industry 5.0.}
}
@article{ROSENAU2024,
title = {Bridging Data Models in Health Care With a Novel Intermediate Query Format for Feasibility Queries: Mixed Methods Study},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/58541},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001418},
author = {Lorenz Rosenau and Julian Gruendner and Alexander Kiel and Thomas Köhler and Bastian Schaffer and Raphael W Majeed},
keywords = {feasibility, FHIR, CQL, eligibility criteria, clinical research, intermediate query format, healthcare interoperability, cohort definition, query, queries, interoperability, interoperable, informatics, portal, portals, implementation, develop, development, ontology, ontologies, JSON},
abstract = {Background
To advance research with clinical data, it is essential to make access to the available data as fast and easy as possible for researchers, which is especially challenging for data from different source systems within and across institutions. Over the years, many research repositories and data standards have been created. One of these is the Fast Healthcare Interoperability Resources (FHIR) standard, used by the German Medical Informatics Initiative (MII) to harmonize and standardize data across university hospitals in Germany. One of the first steps to make these data available is to allow researchers to create feasibility queries to determine the data availability for a specific research question. Given the heterogeneity of different query languages to access different data across and even within standards such as FHIR (eg, CQL and FHIR Search), creating an intermediate query syntax for feasibility queries reduces the complexity of query translation and improves interoperability across different research repositories and query languages.
Objective
This study describes the creation and implementation of an intermediate query syntax for feasibility queries and how it integrates into the federated German health research portal (Forschungsdatenportal Gesundheit) and the MII.
Methods
We analyzed the requirements for feasibility queries and the feasibility tools that are currently available in research repositories. Based on this analysis, we developed an intermediate query syntax that can be easily translated into different research repository–specific query languages.
Results
The resulting Clinical Cohort Definition Language (CCDL) for feasibility queries combines inclusion criteria in a conjunctive normal form and exclusion criteria in a disjunctive normal form, allowing for additional filters like time or numerical restrictions. The inclusion and exclusion results are combined via an expression to specify feasibility queries. We defined a JSON schema for the CCDL, generated an ontology, and demonstrated the use and translatability of the CCDL across multiple studies and real-world use cases.
Conclusions
We developed and evaluated a structured query syntax for feasibility queries and demonstrated its use in a real-world example as part of a research platform across 39 German university hospitals.}
}
@article{JING2025104353,
title = {Knowledge graph construction with meta-learning for continuously accumulated manufacturing knowledge},
journal = {Computers in Industry},
volume = {172},
pages = {104353},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2025.104353},
url = {https://www.sciencedirect.com/science/article/pii/S0166361525001186},
author = {Yanzhen Jing and Guanghui Zhou and Chao Zhang and Fengtian Chang and Jiacheng Li},
keywords = {Manufacturing knowledge graph, Meta-learning, Low resource, Knowledge extraction, Manufacturing knowledge reuse},
abstract = {The construction of manufacturing knowledge graph (MKG) has been regarded as an important technical roadmap to support designer-oriented manufacturing knowledge reuse. It can improve product manufacturability and reduce design iterations. However, manufacturing knowledge is lesson-learned texts of enterprises. Traditional deep learning-driven MKG construction requires sufficient training samples, which heavily rely on manual labeling. It is both time-consuming and labor-intensive. Meanwhile, due to the new manufacturing knowledge accumulation, an MKG also needs to be continuously updated. To bridge the gap, this paper proposes an efficient MKG construction approach with meta-learning. Based on the manufacturing knowledge ontology, a novel two-stage knowledge extraction model (TKEM) is presented to achieve low-resource entity recognition. Then, considering the newly accumulated manufacturing knowledge, a continuous knowledge fusion strategy is illustrated to complete the MKG construction and update. Finally, the experimental results show that the TKEM outperforms state-of-the-art baselines on both the manufacturing knowledge dataset and a public dataset. In addition, a prototype system provides the application of MKG-based manufacturing knowledge reuse, which can perceive explicit and implicit knowledge requirements of designers by MKG embedding learning.}
}
@incollection{HENRY2025,
title = {Performative Language},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00654-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041006542},
author = {Jacob Henry and Kira Hall},
keywords = {Austin, Butler, Discourse, Gender, Identity, Indexicality, Interaction, Performativity, Race, Class, Sexuality},
abstract = {This chapter explores the historical concept of performative language and how it has evolved within linguistic anthropological thought up to the modern day. The discussion begins with J. L. Austin's understanding of the performative utterance in the 1960s and traces its conceptual lineage into Judith Butler's understanding of performativity in the 1990s. It then examines how Butler's performativity theory has been taken up by linguistic anthropologists in thinking about performative language, especially scholars of gender, race, and class, largely in conjunction with Silverstein's work on indexicality and indexical performance.}
}
@article{BRAUN2025100006,
title = {AI as the naive intelligibility of the artificial},
journal = {TAO},
volume = {1},
number = {1},
pages = {100006},
year = {2025},
issn = {3050-5283},
doi = {https://doi.org/10.1016/j.tao.2025.100006},
url = {https://www.sciencedirect.com/science/article/pii/S3050528325000048},
author = {Robert Braun},
keywords = {Science and technology studies, Ethnomethodology, Phenomenal field, Bullshitting, Anthropocene, Political ontology},
abstract = {This paper discusses the challenges of integrating GenAI into intersubjective, object-full, language-infused embodied actions that make-up local processes of Heideggerian worlding. Addressing the GenAI challenge from a Science and Technology Studies (STS) perspective and within the context of the Anthropocene, it redefines human-GenAI interactions with the help of an ethnomethodological perspective. The paper discusses the Anthropocene in terms of a political ontology and the Anthropic condition: a representational view of the world facilitated by ontopolitical gerrymandering - introducing the cut between the world and Anthropos. A political programme, it endows Anthropos with the sovereign power of settling struggles over objectivity and truth; what and how things are in our world, what counts as real, and what counts as incoherent fiction. In this ontopolitical setting analysed with an ethnomethodological gaze, GenAI disrupts the realm of embodied actions, situated within the depth of lived space and time, filled with objects and permeated by language inferring with linguistic intersubjectivity causing incommensurability. GenAI becomes a social bullshitter concealed as sage as relations to the phenomenal field of organized objects that produce a perceived coherence become disrupted. The paper offers a radical critique of GenAI as a social bullshitter and the reduction of GenAI to naïve intelligibility by members; introducing a new analytic, that of postquantum, which challenges the foundationalist, realist, and non-relational worlding of the Anthropic condition and GenAI therein as another technological fix stabilizing the condition as the One-World world real.}
}
@article{PAEK2025,
title = {Real-World Insights Into Dementia Diagnosis Trajectory and Clinical Practice Patterns Unveiled by Natural Language Processing: Development and Usability Study},
journal = {JMIR Aging},
volume = {8},
year = {2025},
issn = {2561-7605},
doi = {https://doi.org/10.2196/65221},
url = {https://www.sciencedirect.com/science/article/pii/S2561760525000246},
author = {Hunki Paek and Richard H Fortinsky and Kyeryoung Lee and Liang-Chin Huang and Yazeed S Maghaydah and George A Kuchel and Xiaoyan Wang},
keywords = {dementia, memory loss, memory, cognitive, Alzheimer disease, natural language processing, NLP, deep learning, machine learning, real-world insights, electronic health records, EHR, cohort, diagnosis, diagnostic, trajectory, pattern, prognosis, geriatric, older adults, aging},
abstract = {Background
Understanding the dementia disease trajectory and clinical practice patterns in outpatient settings is vital for effective management. Knowledge about the path from initial memory loss complaints to dementia diagnosis remains limited.
Objective
This study aims to (1) determine the time intervals between initial memory loss complaints and dementia diagnosis in outpatient care, (2) assess the proportion of patients receiving cognition-enhancing medication prior to dementia diagnosis, and (3) identify patient and provider characteristics that influence the time between memory complaints and diagnosis and the prescription of cognition-enhancing medication.
Methods
This retrospective cohort study used a large outpatient electronic health record (EHR) database from the University of Connecticut Health Center, covering 2010‐2018, with a cohort of 581 outpatients. We used a customized deep learning–based natural language processing (NLP) pipeline to extract clinical information from EHR data, focusing on cognition-related symptoms, primary caregiver relation, and medication usage. We applied descriptive statistics, linear, and logistic regression for analysis.
Results
The NLP pipeline showed precision, recall, and F1-scores of 0.97, 0.93, and 0.95, respectively. The median time from the first memory loss complaint to dementia diagnosis was 342 (IQR 200-675) days. Factors such as the location of initial complaints and diagnosis and primary caregiver relationships significantly affected this interval. Around 25.1% (146/581) of patients were prescribed cognition-enhancing medication before diagnosis, with the number of complaints influencing medication usage.
Conclusions
Our NLP-guided analysis provided insights into the clinical pathways from memory complaints to dementia diagnosis and medication practices, which can enhance patient care and decision-making in outpatient settings.}
}
@article{WANG2025103188,
title = {HMEA: A hierarchical medical knowledge graph entity alignment model fusing multi-aspect information},
journal = {Artificial Intelligence in Medicine},
volume = {168},
pages = {103188},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2025.103188},
url = {https://www.sciencedirect.com/science/article/pii/S093336572500123X},
author = {Weiguang Wang and Lijuan Ma and Wei Cai and Haiyan Zhao and Xia Zhang},
keywords = {Medical entity alignment, Entity alignment, Knowledge graph, Medical knowledge graph, Representation learning, Knowledge fusion},
abstract = {Medical entity alignment is crucial for the integration and reasoning of medical knowledge, aiming to match semantically equivalent entities across different medical knowledge graphs. Unlike entities in general knowledge graphs, medical entities contain rich multi-aspect information, which not only includes structural and attribute information but also additional information such as ontology and descriptions. However, existing entity alignment methods overlook these additional pieces of information and lack exploration into the fusion of multi-aspect information. This leads to less-than-ideal performance in medical entity alignment. To address the aforementioned issues, in this paper, we propose a hierarchical medical knowledge graph entity alignment method, termed HMEA, which integrates multi-aspect information. Firstly, we represent the medical knowledge graph as a hierarchical heterogeneous graph to model the multi-aspect information of medical entities. Secondly, we design different representation learning methods according to the characteristics of multi-aspect information to obtain vector representations of entities in different dimensions. Subsequently, we devise a two-stage multi-aspect knowledge fusion mechanism to dynamically integrate multi-aspect information, enabling mutual complementarity. Finally, we utilize the fused entity vector representations to guide entity alignment. We compare our approach with state-of-the-art baseline models on ten different types of publicly available datasets and further conduct ablation and parameter analyses. Experimental results validate the effectiveness and robustness of the proposed model. In benchmark tests across all datasets, HMEA outperforms the current state-of-the-art methods significantly.}
}
@article{OZMEN2025105215,
title = {“Whom do we educate? Uncertainties and inexplicable ecstasy of the GenAI era in foreign language teacher education”},
journal = {Teaching and Teacher Education},
volume = {167},
pages = {105215},
year = {2025},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2025.105215},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X25002926},
author = {Kemal Sinan Özmen and Hale Ülkü Aydın},
keywords = {GenAI, English language teacher education, Teacher educators, Student teacher competences},
abstract = {This study examines the implications of Generative Artificial Intelligence (GenAI) in English language teacher education, focusing on how teacher educators perceive and respond to its integration. Framed by Schön's reflective model and the TPACK framework, it advances the conceptualization of GenAI as a core element of teacher competence rather than a peripheral technological skill. Using qualitative interviews with six ELT professors and document analysis of 22 curricula in Türkiye, the study identifies four themes: (1) a gap between student digital readiness and institutional inertia, (2) pedagogical and ethical challenges, (3) time constraints in methodology courses, and (4) a role exchange where GenAI assumes cognitively demanding tasks. Participants proposed five strategies, including curriculum-wide integration, professional development for educators, and AI detection tools. The study contributes an ethically grounded, context-sensitive perspective, urging SLTE programs to embed reflective and learner-centered approaches in preparing future teachers for the pedagogical realities of AI-mediated education.}
}
@article{SANSONE2022101967,
title = {Legal Information Retrieval systems: State-of-the-art and open issues},
journal = {Information Systems},
volume = {106},
pages = {101967},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101967},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001551},
author = {Carlo Sansone and Giancarlo Sperlí},
keywords = {Legal Information Retrieval, Artificial Intelligence, Natural Processing Language, Ontology},
abstract = {In the last years, the legal domain has been revolutionized by the use of Information and Communication Technologies, producing large amount of digital information. Legal practitioners’ needs, then, in browsing these repositories has required to investigate more efficient retrieval methods, that assume more relevance because digital information is mostly unstructured. In this paper we analyze the state-of-the-art of artificial intelligence approaches for legal domain, focusing on Legal Information Retrieval systems based on Natural Language Processing, Machine Learning and Knowledge Extraction techniques. Finally, we also discuss challenges – mainly focusing on retrieving similar cases, statutes or paragraph for supporting latest cases’ analysis – and open issues about Legal Information Retrieval systems.}
}
@article{PEREIRA2025103049,
title = {A semantics-driven framework to enable demand flexibility control applications in real buildings},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103049},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103049},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007006},
author = {Flavia de Andrade Pereira and Kyriakos Katsigarakis and Dimitrios Rovas and Marco Pritoni and Conor Shaw and Lazlo Paul and Anand Prakash and Susana Martin-Toral and Donal Finn and James O’Donnell},
keywords = {Demand flexibility controls, Semantic models, Interoperability, Modularity, Brick, SAREF},
abstract = {Decarbonising and digitalising the energy sector requires scalable and interoperable Demand Flexibility (DF) applications. Semantic models are promising technologies for achieving these goals, but existing studies focused on DF applications exhibit limitations. These include dependence on bespoke ontologies, lack of computational methods to generate semantic models, ineffective temporal data management and absence of platforms that use these models to easily develop, configure and deploy controls in real buildings. This paper introduces a semantics-driven framework to enable DF control applications in real buildings. The framework supports the generation of semantic models that adhere to Brick and SAREF while using metadata from Building Information Models (BIM) and Building Automation Systems (BAS). The work also introduces a web platform that leverages these models and an actor and microservices architecture to streamline the development, configuration and deployment of DF controls. The paper demonstrates the framework through a case study, illustrating its ability to integrate diverse data sources, execute DF actuation in a real building, and promote modularity for easy reuse, extension, and customisation of applications. The paper also discusses the alignment between Brick and SAREF, the value of leveraging BIM data sources, and the framework’s benefits over existing approaches, demonstrating a 75% reduction in effort for developing, configuring, and deploying building controls.}
}
@article{MUDAVANHU2025244,
title = {Rethinking strategy execution: an enterprise engineering approach},
journal = {Management Decision},
volume = {63},
number = {13},
pages = {244-271},
year = {2025},
issn = {0025-1747},
doi = {https://doi.org/10.1108/MD-06-2024-1227},
url = {https://www.sciencedirect.com/science/article/pii/S0025174725000163},
author = {Thabani Mudavanhu and Bruno Emwanu},
keywords = {Strategy execution, Strategy implementation, Enterprise engineering, Organisational construction flaws},
abstract = {Purpose
This study sought to understand how the success rate of strategy execution can be improved through the application of enterprise engineering principles and practices.
Design/methodology/approach
Considering the complexity/greyness of the study area and limited literature in the new domain of enterprise engineering application as it relates to implementation, a structured literature review (SLR) was used as the basis for a Delphi study. A two round Delphi study was conducted with experts in the field to determine and validate the critical dimensions in strategy execution. Thirty-one and twenty-five experts participated in Delphi Round 1 and 2 respectively.
Findings
There is a relationship between the design of an enterprise and seven specific themes that were synthesised in the study. These seven themes are aggregated to three dimensions – namely: (1) ontological essentials, (2) basic requirements and (3) consequential/resultant elements for effective strategy execution. By explaining the relationship of these dimensions, from an enterprise ontology perspective – this article demonstrates that most of the commonly cited causes of strategy execution challenges, for example performance culture, communication and the like, are in fact consequential and not ontological. Such insights position this study as a foundational piece on the application of enterprise ontology to understand and aid strategy execution.
Originality/value
This study extends existing knowledge about the influence of organisational design on strategy execution by presenting a new theoretical lens that challenges the conventional view of organisations. For practitioners and managers, this entails (1) clarity on specific elements to be considered for enhancing strategy execution frameworks and (2) visibility of consequential dimensions that can easily cloud the conceptualisation of the “to be” or desired framework.}
}
@article{ZHU20251538,
title = {ToxDL 2.0: Protein toxicity prediction using a pretrained language model and graph neural networks},
journal = {Computational and Structural Biotechnology Journal},
volume = {27},
pages = {1538-1549},
year = {2025},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2025.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2001037025001230},
author = {Lin Zhu and Yi Fang and Shuting Liu and Hong-Bin Shen and Wesley {De Neve} and Xiaoyong Pan},
keywords = {Protein toxicity, Graph Neural Network, Language models, Multi-modal deep learning},
abstract = {Motivation
Assessing the potential toxicity of proteins is crucial for both therapeutic and agricultural applications. Traditional experimental methods for protein toxicity evaluation are time-consuming, expensive, and labor-intensive, highlighting the requirement for efficient computational approaches. Recent advancements in language models and deep learning have significantly improved protein toxicity prediction, yet current models often lack the ability to integrate evolutionary and structural information, which is crucial for accurate toxicity assessment of proteins.
Results
In this study, we present ToxDL 2.0, a novel multimodal deep learning model for protein toxicity prediction that integrates both evolutionary and structural information derived from a pretrained language model and AlphaFold2. ToxDL 2.0 consists of three key modules: (1) a Graph Convolutional Network (GCN) module for generating protein graph embeddings based on AlphaFold2-predicted structures, (2) a domain embedding module for capturing protein domain representations, and (3) a dense module that combines these embeddings to predict the toxicity. After constructing a comprehensive toxicity benchmark dataset, we obtained experimental results on both an original non-redundant test set (comprising pre-2022 protein sequences) and an independent non-redundant test set (a holdout set of post-2022 protein sequences), demonstrating that ToxDL 2.0 outperforms existing state-of-the-art methods. Additionally, we utilized Integrated Gradients to discover known toxic motifs associated with protein toxicity. A web server for ToxDL 2.0 is publicly available at www.csbio.sjtu.edu.cn/bioinf/ToxDL2/.}
}
@article{GUERRA202480,
title = {Towards a digital twin architecture for the lighting industry},
journal = {Future Generation Computer Systems},
volume = {155},
pages = {80-95},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.01.028},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24000359},
author = {Victor Guerra and Benoit Hamon and Benoit Bataillou and Adwait Inamdar and Willem D. {van Driel}},
keywords = {Digital twin, Lighting, Ontology, Remaining useful lifetime, Scheduled maintenance, Predictive maintenance},
abstract = {This paper introduces an ontology-based Digital Twin (DT) architecture for the lighting industry, integrating simulation models, data analytics, and visualization to represent luminaires. The ontology standardizes luminaire components, facilitating interoperability with design tools. The calculated ontology-level metrics suggest mid-level complexity with Size Of Vocabulary (SOV) at 37, Edge-to-Node Ratio (ENR) at 0.865, Tree Impurity (TIP) at 0, and Entropy Of Graph (EOG) at 2.61. A use case explores the utility of the ontology in the design phase across two different geographical locations, assessing environmental adaptability. The ontology captures opto-thermo-electric interactions, providing insights into luminaire performance. Results from inflating the DT and conducting simulations align with existing literature, indicating a degradation of around 12% over 8 years on the radiant flux. This ontology, up to the authors’ knowledge, is the first formal definition for the lighting industry, aiming to encompass the entire luminaire lifecycle. The current focus is on design and operational phases, with potential future enhancements to include real-time monitoring for performance evaluation and predictive maintenance. This work contributes to luminaire analysis and supports the development of sustainable lighting solutions in the industry.}
}
@article{MORTENSEN2025S30,
title = {S15-02 New Approaches to Data and Model Integration of Adverse Outcome Pathway information in the EPA AOP-DB},
journal = {Toxicology Letters},
volume = {411},
pages = {S30},
year = {2025},
note = {Abstracts of the 59th Congress of the European Societies of Toxicology (EUROTOX 2025) TOXICOLOGY ADDRESSES SOCIETY'S REAL LIFE RISKS FOR SUSTAINABLE HEALTH AND WELL BEING},
issn = {0378-4274},
doi = {https://doi.org/10.1016/j.toxlet.2025.07.092},
url = {https://www.sciencedirect.com/science/article/pii/S0378427425016753},
author = {H.M. Mortensen},
abstract = {Adverse Outcome Pathways (AOPs) is a conceptual framework that describes the mechanistic progression of key biological events that result in an adverse response. How we catalog these entities and interactions enhances our ability to understand mechanistic effects and subsequently toxicological outcomes relevant to human health. Structured implementation of AOP knowledge contributes to New Approach Methodologies (NAMs) and further development of machine learning and artificial intelligence (AI) utilization for regulatory objectives. The longevity and success of database/knowledgebase and infrastructure projects have typically been hampered by inconsistent and limited funding. This has clear effects on data sustainability practices, quality of data, and data reuse policies. For AOPs, improving consistent mapping to other types of biological and toxicological data will increase utility, reuse, and interoperability. For example, integrating FAIR (findable, accessible, interoperable, and re-usable) data standards is a good approach, but is reliant on 3rd party tools. One such tool, the EPA Adverse Outcome Pathway Database (AOP-DB), integrates multiple publicly available resources, extending ontology mapping of AOPs to the mapping of molecular and mechanistic componentsincluding biomedical entities (e.g.,gene, protein, biological pathway, disease, tissue, assay, etc).Since the AOP-DB was created, additional tools have been initiated to improve automatic and systematic parsing, machine-actionability of AOP data to elucidate biological mechanism, and mapping to (meta)data. These tools are necessarybecause the AOP-Wiki, the primary repository of AOP information, does not programmatically map to this type of information. As a result, there is no consistency across AOPs in FAIR reporting standards related to biomedical entity mapping or the human/machine readability within a given AOP. Currently, no 3rd party mapping of biomedical information pertaining to an AOP feeds into the AOP-Wiki repository. Standardization/harmonization of AOP (meta)data and defining AOP biomedical data lifecycles will facilitate the machine-actionability of AOPs and improve trust, transparency and accessibility. Four independent, expert workgroups have been formed to address FAIR AOP data standards: the FAIR AOP Cluster Workgroup; the Elixer Toxicology Community; the Environmental Health Language Collaborative AOP Standards Workgroup; and the AOP Ontology Workgroup. These workgroups are currently interacting todevelop a FAIR AOP Roadmap to ensure that AOP data and related biomedical information are easily accessible and interoperable for researchers across different disciplines (e.g., AOP, toxicology, biomedical, regulatory). Here we report on the current direction of the OECD, WPHA, SAAOP, and expert workgroups to improve standards for AOP biomedical entity mapping and coordinate this information. This abstract does not reflect EPA Policy.}
}
@article{LYU2024109334,
title = {Language model and its interpretability in biomedicine: A scoping review},
journal = {iScience},
volume = {27},
number = {4},
pages = {109334},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.109334},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224005558},
author = {Daoming Lyu and Xingbo Wang and Yong Chen and Fei Wang},
keywords = {Health sciences, Natural sciences, Computer science},
abstract = {Summary
With advancements in large language models, artificial intelligence (AI) is undergoing a paradigm shift where AI models can be repurposed with minimal effort across various downstream tasks. This provides great promise in learning generally useful representations from biomedical corpora, at scale, which would empower AI solutions in healthcare and biomedical research. Nonetheless, our understanding of how they work, when they fail, and what they are capable of remains underexplored due to their emergent properties. Consequently, there is a need to comprehensively examine the use of language models in biomedicine. This review aims to summarize existing studies of language models in biomedicine and identify topics ripe for future research, along with the technical and analytical challenges w.r.t. interpretability. We expect this review to help researchers and practitioners better understand the landscape of language models in biomedicine and what methods are available to enhance the interpretability of their models.}
}
@article{ZHONG2023110071,
title = {Natural Language Processing for systems engineering: Automatic generation of Systems Modelling Language diagrams},
journal = {Knowledge-Based Systems},
volume = {259},
pages = {110071},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110071},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122011649},
author = {Shaohong Zhong and Andrea Scarinci and Alice Cicirello},
keywords = {SysML diagram, NLP, Structure diagram, Requirement diagram, Text-to-diagram, Automated diagrams from text},
abstract = {The design of complex engineering systems is an often long and articulated process that highly relies on engineers’ expertise and professional judgment. As such, the typical pitfalls of activities involving the human factor often manifest themselves in terms of lack of completeness or exhaustiveness of the analysis, inconsistencies across design choices or documentation, as well as an implicit degree of subjectivity. An approach is proposed to assist systems engineers in the automatic generation of systems diagrams from unstructured natural language text. Natural Language Processing (NLP) techniques are used to extract entities and their relationships from textual resources (e.g., specifications, manuals, technical reports, maintenance reports) available within an organisation, and convert them into Systems Modelling Language (SysML) diagrams, with particular focus on structure and requirement diagrams. The intention is to provide the users with a more standardised, comprehensive and automated starting point onto which subsequently refine and adapt the diagrams according to their needs. The proposed approach is flexible and open-domain. It consists of six steps which leverage open-access tools, and it leads to an automatic generation of SysML diagrams without intermediate modelling requirement, but through the specification of a set of parameters by the user. The applicability and benefits of the proposed approach are shown through six case studies having different textual sources as inputs, and benchmarked against manually defined diagram elements.}
}
@article{ZIVIC2025113525,
title = {Materials informatics: A review of AI and machine learning tools, platforms, data repositories, and applications to architectured porous materials},
journal = {Materials Today Communications},
volume = {48},
pages = {113525},
year = {2025},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2025.113525},
url = {https://www.sciencedirect.com/science/article/pii/S2352492825020379},
author = {Fatima Zivic and Ana Kaplarevic Malisic and Nenad Grujovic and Boban Stojanovic and Milos Ivanovic},
keywords = {Traditional computational models, Data-driven AI material models, Smart materials, Deep Tech, Structure-property-processing relationships, High-throughput screening, Electrospinning, 3D printed biomimetic porosity},
abstract = {This review presents the key aspects and development directions of materials informatics, emphasizing the role of artificial intelligence (AI) and machine learning (ML) in materials science research. The objective is to provide a comprehensive overview of materials informatics tools, workflows, and case studies, particularly aimed at experimental researchers unfamiliar with AI frameworks. Basic concepts are introduced and traditional modelling methods compared to AI/ML-assisted models. Existing material models serve as a foundation for advanced modelling and simulations aimed at reducing the time required for characterisation and discovery, with physics-based models gaining importance in the development of AI-supported surrogate models. This review also covers currently available resources, including: (i) software for solving complex mathematical equations and material modelling; (ii) web-based platforms and tools designed for both expert and non-expert users; and (iii) materials data repositories, prioritising standardisation. Case examples involving materials with architectured macro-, micro-, and nano-porosity are reviewed across three material types: metal-organic frameworks (MOFs), electrospun PVDF piezoelectrics, and 3D printed mechanical metamaterials. Traditional computational models offer interpretability and physical consistency, AI/ML excels in speed and complexity handling but may lack transparency. Hybrid models combining both approaches show excellent results in prediction, simulation, and optimisation, offering both speed and interpretability. Progress depends on modular, interoperable AI systems, standardised FAIR data, and cross-disciplinary collaboration. Addressing data quality and integration challenges will resolve issues related to metadata gaps, semantic ontologies, and data infrastructures, especially for small datasets and unlock transformative advances in fields like nanocomposites, MOFs, and adaptive materials.}
}
@article{WINDISCH2022550,
title = {Approach for model-based requirements engineering for the planning of engineering generations in the agile development of mechatronic systems},
journal = {Procedia CIRP},
volume = {109},
pages = {550-555},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.293},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122007429},
author = {Emily Windisch and Constantin Mandel and Simon Rapp and Nikola Bursac and Albert Albers},
keywords = {mbse, agility, validation, test planning},
abstract = {The crucial factor for a successful usage of modeling approaches of systems engineering is the interaction of language, method, and tool. For this, specific challenges arise for the application of MBSE in agile requirements engineering. From observations in agile development practice at a machine tool manufacturer, the challenges for model-based requirements engineering are described and each is assigned to its critical aspect of modeling: The language must formally represent the requirements data model, especially for planning engineering generations. The tool must support collaborative, interdisciplinary cooperation, and consider the dynamics of the requirements model during the development process. The method must individually support the requirements engineering activities, which are carried out several times in a sprint during the development process and must enable a target-oriented process for bundling the requirements into engineering generations. Taking these demands into account, an approach is then presented providing activity-based views in conjunction with activity steps based on a consistent ontology for the description of product requirements and verification activities. The activity steps are composed in activity patterns and support the user in making use of the views for modeling requirements for the engineering generations. The approach is implemented in the software JIRA at a machine tool manufacturer. The subsequent evaluation shows that the approach is used in development practice and offers the potential to plan engineering generation systematically and comprehensibly and to ensure a regular review of the implemented requirements.}
}
@article{UHM2025106363,
title = {Automated analysis of construction safety accident videos using a large multimodal model and graph retrieval-augmented generation},
journal = {Automation in Construction},
volume = {177},
pages = {106363},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106363},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525004030},
author = {Miyoung Uhm and Jaehee Kim and Ghang Lee},
keywords = {Construction accident video, Large multimodal model (LMM), Large language models (LLM), Graph retrieval augmented generation (Graph RAG), Automated analysis of construction accident videos (AcciVid)},
abstract = {Safety investigators are challenged by the manual task of analyzing large volumes of accident videos through the repetitive process of reviewing them frame by frame, which is both tedious and labor-intensive. This paper proposes the Accident Video Analysis framework (AcciVid), which automates this process using a large multimodal model (LMM) integrated with Graph Retrieval Augmented Generation (Graph RAG). Accident video content and regulations are converted into Resource Description Framework (RDF) triples and stored as graphs, enabling regulation-based analysis through Graph RAG. AcciVid detected 90 %p more potential safety violations than human safety investigators, achieving an F2 Score of 82.4 % compared to their F2 Score of 54.8 %. Furthermore, AcciVid required only an average of 42 s to generate a draft report, whereas human safety investigators needed an average of 4.6 h. This demonstrates AcciVid's potential as an assistant to safety investigators in reducing manual workloads while maintaining high accuracy and efficiency.}
}
@article{MOORE2025107716,
title = {Integrating human-centered AI for land use policy: Insights from agricultural interventions in international development},
journal = {Land Use Policy},
volume = {158},
pages = {107716},
year = {2025},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2025.107716},
url = {https://www.sciencedirect.com/science/article/pii/S0264837725002509},
author = {Lindsey Moore and Mindel {van de Laar} and Pui-Hang Wong and Cathal O’Donoghue},
keywords = {Human-centered artificial intelligence (AI), AI in international development, Fine-tuning AI for development, Evidence-based decision making, Machine learning, Large language models (LLMs), Agriculture interventions},
abstract = {Policymakers often struggle with information overload from vast technical documentation, hindering effective evidence-based decision-making. This study explores how a human-centered artificial intelligence model was fine-tuned to analyze agricultural interventions within international development projects, providing a methodological foundation to support the synthesis of complex evidence for more informed land use policy formulation. Engaging domain experts and incorporating human expertise, we developed a taxonomy of land use practices—such as water resource management, land use planning, and agronomic practices—that reflects the nuanced realities of local interventions. By integrating this human-centered taxonomy into the model's training, we ensured that the artificial intelligence system efficiently identified and categorized interventions in a way that upholds humanistic practices and aligns with the needs of policymakers and communities. Our findings demonstrate that this approach enhances the analysis of land use interventions. The model proved to be both scalable and cost-effective, analyzing large volumes of data more rapidly than traditional human analysis. These results underscore the potential of human-centered artificial intelligence in transforming land use policymaking by empowering stakeholders with faster and more accurate data synthesis. This methodological approach has the potential to support policymakers in synthesizing evidence more efficiently, which could ultimately lead to more informed and effective land use policies and improved outcomes in international development.}
}
@article{KHARCHENKO2025100201,
title = {Spatiotemporal scales in narrative inquiry research on language identity development during study abroad},
journal = {Research Methods in Applied Linguistics},
volume = {4},
number = {2},
pages = {100201},
year = {2025},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2025.100201},
url = {https://www.sciencedirect.com/science/article/pii/S2772766125000229},
author = {Yulia Kharchenko},
keywords = {Narrative inquiry, Spatiotemporal scales, Language identity, Study abroad},
abstract = {This paper examines how language identity of international English language students develops in study abroad settings. The methods used in this study stem from different narrative approaches that focus on aspects of identity and narrative form. The analytical focus is on various scales of time and space over which identity developments occur, from historic to the more immediate contexts of interaction. First, autobiographical narratives are constructed using thematic analysis of interview and diary data. To complement the big-picture findings, short stories of critical identity episodes are analysed in terms of content and context. Finally, the small-scale analysis of interview data provides insights into how identity work is interactively constructed. The diverse findings are unified into a rich research narrative by applying the concept of spatiotemporal scales as an overarching, unifying logic. It is argued that a combination of narrative inquiry approaches and the spatiotemporal perspective results in a more nuanced understanding of identity dynamics of language learners. The findings demonstrate a range of language-related identity developments in a single case study and encourage further narrative studies in applied linguistics.}
}
@article{XU2018,
title = {Resolving “orphaned” non-specific structures using machine learning and natural language processing methods},
journal = {Biodiversity Data Journal},
volume = {6},
year = {2018},
issn = {1314-2836},
doi = {https://doi.org/10.3897/BDJ.6.e26659},
url = {https://www.sciencedirect.com/science/article/pii/S1314283618000210},
author = {Dongfang Xu and Steven S Chong and Thomas Rodenhausen and Hong Cui},
keywords = {Information Extraction, Machine Learning, Anaphora Resolution, Ontology Application, Biodiversity Literature, Morphological Descriptions, Performance Evaluation},
abstract = {Scholarly publications of biodiversity literature contain a vast amount of information in human readable format. The detailed morphological descriptions in these publications contain rich information that can be extracted to facilitate analysis and computational biology research. However, the idiosyncrasies of morphological descriptions still pose a number of challenges to machines. In this work, we demonstrate the use of two different approaches to resolve meronym (i.e. part-of) relations between anatomical parts and their anchor organs, including a syntactic rule-based approach and a SVM-based (support vector machine) method. Both methods made use of domain ontologies. We compared the two approaches with two other baseline methods and the evaluation results show the syntactic methods (92.1% F1 score) outperformed the SVM methods (80.7% F1 score) and the part-of ontologies were valuable knowledge sources for the task. It is notable that the mistakes made by the two approaches rarely overlapped. Additional tests will be conducted on the development version of the Explorer of Taxon Concepts toolkit before we make the functionality publicly available. Meanwhile, we will further investigate and leverage the complementary nature of the two approaches to further drive down the error rate, as in practical application, even a 1% error rate could lead to hundreds of errors.}
}
@article{DESUL20231356,
title = {Semantic technology for cultural heritage: a bibliometric-based review},
journal = {Global Knowledge, Memory and Communication},
volume = {74},
number = {56},
pages = {1356-1380},
year = {2023},
issn = {2514-9342},
doi = {https://doi.org/10.1108/GKMC-04-2023-0125},
url = {https://www.sciencedirect.com/science/article/pii/S2514934223000168},
author = {Sudarsan Desul and Rabindra Kumar Mahapatra and Raj Kishore Patra and Mrutyunjay Sethy and Neha Pandey},
keywords = {Cultural heritage, Ontology, Semantic technology, Intangible cultural heritage, Digital humanities, Linked data},
abstract = {Purpose
The purpose of this study is to review the application of semantic technologies in cultural heritage (STCH) to achieve interoperability and enable advanced applications like 3D modeling and augmented reality by enhancing the understanding and appreciation of CH. The study aims to identify the trends and patterns in using STCH and provide insights for scholars and policymakers on future research directions.
Design/methodology/approach
This research paper uses a bibliometric study to analyze the articles published in Scopus and Web of Science (WoS)-indexed journals from 1999 to 2022 on STCH. A total of 580 articles were analyzed using the Biblioshiny package in RStudio.
Findings
The study reveals a substantial increase in STCH publications since 2008, with Italy leading in contributions. Key research areas such as ontologies, semantic Web, linked data and digital humanities are extensively explored, highlighting their significance and characteristics within the STCH research domain.
Research limitations/implications
This study only analyzed articles published in Scopus and WoS-indexed journals in the English language. Further research could include articles published in other languages and non-indexed journals.
Originality/value
This study extensively analyses the research published on STCH over the past 23 years, identifying the leading authors, institutions, countries and top research topics. The findings provide guidelines for future research direction and contribute to the literature on promoting, preserving and managing the CH globally.}
}
@article{HAUCK20181,
title = {Language in the Amerindian imagination: An inquiry into linguistic natures},
journal = {Language & Communication},
volume = {63},
pages = {1-8},
year = {2018},
note = {Language in the Amerindian Imagination},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2018.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0271530918300995},
author = {Jan David Hauck and Guilherme Orlandini Heurich}
}
@article{DAI202242,
title = {Toward Intelligent Callout and Semantic Interpretation of ISO Geometrical Production Specification},
journal = {Procedia CIRP},
volume = {114},
pages = {42-47},
year = {2022},
note = {17th CIRP Conference on Computer Aided Tolerancing (CAT2022)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122014615},
author = {Xiangcheng Dai and Benjun Guo and Yuanping Xu and Tukun Li and Xiangqian Jiang and Qiuyan Gai and Jia He and Jiliu Zhou and Jian Huang},
keywords = {GPS, Complete tolerance callouts, Tolerance specification, Ontology knowledge modeling, SWRL, Semantic interpretation},
abstract = {ISO Geometrical Product Specification and Verification standards (GPS) is a worldwide used technique language to control the geometrical variation of a workpiece and its digital twins. More than 150 standards documents have been developed. Normally, the call out of a symbol needs to check many standards documents. It is difficult to check the complete callouts manually and lead to an increase of the specification uncertainty. To this end, this paper presents an ontology-based semantic model to graphically and integrally represent the multi-dimensional knowledge in ISO GPS. In the knowledge modelling stage, this study focuses on integrating tolerance specification and verification knowledge seamlessly. To support automatic advanced rule extractions and reasonings, this study applies Semantic Web Rule Language (SWRL) to extract and represent sufficient multi-dimensional relationships inheriting from GPS documents. Then the constraint relations and reasoners are used to support the intelligent creations and semantic interpretations of the complete tolerance specification callouts. A case study on a typical workpiece-RV reducer is undertaken to test and evaluate the validity, usability and universality of the devised knowledge model.}
}
@article{SHI2025100044,
title = {From Machine Learning to Multimodal Models: The AI Revolution in Enzyme Engineering},
journal = {BioDesign Research},
pages = {100044},
year = {2025},
issn = {2693-1257},
doi = {https://doi.org/10.1016/j.bidere.2025.100044},
url = {https://www.sciencedirect.com/science/article/pii/S2693125725000457},
author = {Ziyan Shi and Shuping Xu and Sihan Xue and Kaiming Chen and Yifan Lu and Feiyue Wang and Siyu Long and Yannan Tian and Peng Zhang and Jianing Wang and Yanhui Gu and Junsheng Zhou and Hao Zhou and Shuaiqi Meng and Haiyang Cui},
abstract = {Protein engineering is a powerful tool for applications spanning synthetic biology, biocatalysis, and drug discovery. Recent advances in artificial intelligence (AI), from conventional machine learning algorithms to large-scale pre-trained protein models, have greatly accelerated enzyme engineering field entering a data-driven era. This review provides a guidance map of current enzyme engineering tasks and builds an integrative perspective on AI methods, model types, landmark tasks, and data resources. We begin by delineating the core modeling tasks in enzyme engineering, which include encompassing function annotation, structural modeling, and property prediction and by reviewing recent advances alongside dominant algorithmic frameworks. Next, we outlined the evolution of AI integration into enzyme engineering, which can be broadly categorized into four stages: classical machine learning approaches, deep neural networks, protein language models (pLMs), and emerging multimodal architectures. Finally, we highlight four converging trends that are redefining the landscape of AI-driven enzyme design: (i) the replacement of handcrafted features with unified, token-level embeddings; (ii) a shift from single-modal models toward multimodal, multitask systems; (iii) the emergence of intelligent agents capable of reasoning; and (iv) a movement beyond static structure prediction toward dynamic simulation of enzyme function. Together, these developments are paving the way for intelligent, generalizable, and mechanistically interpretable AI platforms poised to synthetic biology.}
}
@article{WANG2024102792,
title = {An automatic unsafe states reasoning approach towards Industry 5.0’s human-centered manufacturing via Digital Twin},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102792},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102792},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004403},
author = {Haoqi Wang and Guangwei Wang and Hao Li and Jiewu Leng and Lindong Lv and Vincent Thomson and Yuyan Zhang and Linli Li and Lucheng Chen},
keywords = {Digital Twin, Human-centered manufacturing, Industry 5.0, Unsafe state reasoning, Semantic relationship detection},
abstract = {The safety management of manufacturing workshops is crucial for ensuring human- centered Industry 5.0. To infer the unsafe state of a manufacturing workshop, an integrated Digital Twin (DT) and ontology method was used. However, two challenges arise from this approach. First, creating and labeling virtual-real mixed datasets for unsafe state detection typically requires manual effort. Secondly, recognizing semantic relations of instances of unsafe states using an ontology is time-consuming. To address these challenges, an automatic semantic reasoning framework for unsafe state in a Digital Twin Workshop (DTW) is proposed by integrating instance segmentation, relationship detection and ontology technology. For the first challenge, the Mask R-CNN algorithm is applied to generate automatically a semi-virtual dataset in a DTW, which is then mixed with the real datasets to form a virtual-real mixed dataset. This dataset detects object instances of unsafe states. For the second challenge, a relationship detection model is constructed to predict the semantic relationship of detected object instances. The predicted semantic relationship and detected instances are then mapped to unsafe states in an ontology to be used for automatic reasoning. Finally, an experiment in a welding shop demonstrates that the proposed approach can alleviate the two challenges.}
}
@article{LI2025104600,
title = {Power global multi-source heterogeneous unified metadata query method under pluggable storage framework},
journal = {Results in Engineering},
volume = {26},
pages = {104600},
year = {2025},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2025.104600},
url = {https://www.sciencedirect.com/science/article/pii/S2590123025006784},
author = {Jiwei Li and Bo Li and Shi Liu and Hongwei Lv and Fei Zheng and Qing Liu},
keywords = {Pluggable storage framework, Power overall situation, Multi-source heterogeneity, Hive metadata, Hive technology, Locally sensitive hash, Local ontology integration},
abstract = {In the efficient access and query of power users to multiple data sources, the iterative query process of multi-source heterogeneous data is too time-consuming. This paper studies the global multi-source heterogeneous unified metadata query method of power under the pluggable storage framework. Hive technology is used to build a pluggable storage framework for global multi-source heterogeneous metadata of electric power. Hive technology is used as a pure computing engine, HDFS is used as the underlying storage technology, and Hive metadata is shared through the pluggable metadata framework. Hive technology adopts local ontology integration to construct the domain ontology of global multi-source heterogeneous unified metadata of electric power, and extracts the theme concept of metadata from the constructed domain ontology. The local sensitive hash index is located layer by layer, and then the Top-k distance query is carried out to realize the global multi-source heterogeneous unified metadata query of electric power. The experimental results show that this method can effectively query the global multi-source heterogeneous unified metadata of electric power, and the acceleration ratio of metadata query is higher than 1.5, which simplifies the complexity of data management and query, improves the consistency and accuracy of data, dynamically adds or removes data storage components according to actual needs, enables users to obtain the required data faster, solves the query and management problems of multi-source heterogeneous data, and provides strong support for the digital transformation of electric power industry.}
}
@article{GALLYAMOVA2024112780,
title = {Paranormal beliefs and core knowledge confusions: A meta-analysis},
journal = {Personality and Individual Differences},
volume = {230},
pages = {112780},
year = {2024},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2024.112780},
url = {https://www.sciencedirect.com/science/article/pii/S019188692400240X},
author = {Albina Gallyamova and Elizaveta Komyaginskaya and Dmitry Grigoryev},
keywords = {Paranormal beliefs, Core knowledge confusions, Unfounded beliefs, Ontological confusion, Meta-analysis},
abstract = {This meta-analysis examines the relationship between paranormal beliefs and Core Knowledge Confusions (CKC), a concept introduced by Lindeman and Aarnio (2007) to explore the link between ontological errors and belief in the paranormal. The CKC, which uses metaphor classification to assess ontological confusion, suggests that cognitive processes organize information into ontologies, and our interpretation of literal versus metaphorical expressions reflects these categorizations. Despite advancements in education, ontological confusion persists, manifesting in various forms like mentalization of matter, physicalization of the mental, and biologization of the mental. Our analysis, encompassing 25 effect sizes with 16,129 participants from 11 countries and publications over 14 years, found a significant average effect size (r = 0.40) in the relationship between paranormal beliefs and the CKC, indicating a robust and consistent association across various populations and contexts. The study observed that contextual factors specific to data from Finland enhance the estimated magnitude of the effect size. Despite considerable heterogeneity and potential influences of unexamined moderators, the results suggest a universal cognitive pattern linking paranormal beliefs with certain types of ontological confusion. This meta-analysis underscores the need for further exploration into contextual variations in understanding the complex relationship between paranormal beliefs and ontological errors.}
}
@article{GROS2025105927,
title = {From surveys to simulations: Integrating Notre-Dame de Paris' buttressing system diagnosis with knowledge graphs},
journal = {Automation in Construction},
volume = {170},
pages = {105927},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105927},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524006630},
author = {Antoine Gros and Livio {De Luca} and Frédéric Dubois and Philippe Véron and Kévin Jacquot},
keywords = {Knowledge graph, Structural diagnosis, Maintenance, Cultural heritage buildings, Masonry buildings, Block-based simulation model, Semantic web, Interoperability, Linked building data, Ontology},
abstract = {The assessment of structural safety and a thorough understanding of buildings' structural behavior are critical to enhancing the resilience of the built environment. Cultural Heritage (CH) buildings present unique diagnosis challenges due to their diverse designs and construction techniques, often requiring attention during maintenance or disaster relief efforts. However, collaboration across CH and Architecture, Engineering, and Construction (AEC) fields is hindered by increasing information complexity and prolonged feedback loops. This paper introduces a methodological approach utilizing Knowledge Graph technologies to integrate structural diagnosis information and processes. The approach is applied to the diagnosis of the Notre-Dame de Paris buttressing system, demonstrated through a proof-of-concept knowledge system. By leveraging Knowledge Graph functionalities, insights are derived from the spatialization and provenance of mechanical phenomena, including observed or simulation-predicted cracks in mortar-bound masonry.}
}
@article{DEFILIPPIS2025104809,
title = {A systematic mapping study of semantic technologies in multi-omics data integration},
journal = {Journal of Biomedical Informatics},
volume = {165},
pages = {104809},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104809},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000383},
author = {Giovanni Maria {De Filippis} and Domenico Amalfitano and Cristiano Russo and Cristian Tommasino and Antonio Maria Rinaldi},
keywords = {Semantic technologies, Multi-omic integration, Ontologies, Knowledge graphs, Systematic mapping study, Integrative bioinformatics},
abstract = {Objective:
The integration of multi-omics data is essential for understanding complex biological systems, providing insights beyond single-omics approaches. However, challenges related to data heterogeneity, standardization, and computational scalability persist. This study explores the interdisciplinary application of semantic technologies to enhance data integration, standardization, and analysis in multi-omics research.
Methods:
We performed a systematic mapping study assessing literature from 2014 to 2024, focusing on the utilization of ontologies, knowledge graphs, and graph-based methods for multi-omics integration.
Results:
Our findings indicate a growing number of publications in this field, predominantly appearing in high-impact journals. The deployment of semantic technologies has notably improved data visualization, querying, and management, thus enhancing gene and pathway discovery, and providing deeper disease insights and more accurate predictive modeling.
Conclusion:
The study underscores the significance of semantic technologies in overcoming multi-omics integration challenges. Future research should focus on integrating diverse data types, developing advanced computational tools, and incorporating AI and machine learning to foster personalized medicine applications.}
}
@article{LUKYANENKO2022102062,
title = {System: A core conceptual modeling construct for capturing complexity},
journal = {Data & Knowledge Engineering},
volume = {141},
pages = {102062},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2022.102062},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2200057X},
author = {Roman Lukyanenko and Veda C. Storey and Oscar Pastor},
keywords = {System, Systemism, Conceptual modeling, Complexity, CESM+, Emergent properties, Ontology, Bunge Systemist Ontology (BSO), Retrospective case study, Citizen science},
abstract = {The digitalization of human society continues at a relentless rate. However, to develop modern information technologies, the increasing complexity of the real-world must be modeled, suggesting the general need to reconsider how to carry out conceptual modeling. This research proposes that the often-overlooked notion of “system” should be a separate, and core, conceptual modeling construct and argues for incorporating it and related concepts, such as emergence, into existing approaches to conceptual modeling. The work conducts a synthesis of the ontology of systems and general systems theory. These modeling foundations are then used to propose a CESM+ template for conducing systems-grounded conceptual modeling. Several new conceptual modeling notations are introduced. The systemist modeling is then applied to a case study on the development of a citizen science platform. The case demonstrates the potential contributions of the systemist approach and identifies specific implications of explicit modeling with systems for theory and practice. The paper provides recommendations for how to incorporate systems into existing projects and suggests fruitful opportunities for future conceptual modeling research.}
}
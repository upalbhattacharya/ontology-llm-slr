@INPROCEEDINGS{10286646,
  author={Sadirmekova, Zhanna and Sambetbayeva, Madina and Daiyrbayeva, Elmira and Yerimbetova, Aigerim and Altynbekova, Zhanar and Murzakhmetov, Aslanbek},
  booktitle={2023 8th International Conference on Computer Science and Engineering (UBMK)}, 
  title={Constructing the Terminological Core of NLP Ontology}, 
  year={2023},
  volume={},
  number={},
  pages={81-85},
  abstract={The basis of any intellectual resource is a knowledge base, which, based on the basic terms of the field under consideration, builds relationships between them. Therefore, the first task to building multilingual information system using Natural language processing (NLP)in scientific and educational activities will be the development of a multilingual dictionary on modern NLP methods, including terms in Kazakh, English and Russian. For its construction, linguistic models for semantic dictionaries and thesauruses will be used, as well as methods of automatic extraction of terms from the corpus of texts of a given subject area. In this paper, a system of concepts of the NLP domain will be formalized, which will form the terminological core of the NLP ontology. To systematize information and provide support for multilingualism and accessibility, we plan to apply ontological engineering methods to systematize information and build the upper levels of the NLP ontology (its terminological core) using the dictionary of terms obtained at the previous stage. The ontology developed by us can later become the conceptual basis for a multilingual information system used in scientific and educational activities using NLP. This system will provide systematization of all information, convenient navigation on it, integration into a single information space, as well as access to it.},
  keywords={Dictionaries;Navigation;Semantics;Knowledge based systems;Ontologies;Linguistics;Natural language processing;Natural language processing;ontology;conceptual model;scientific and educational information system;ontological design patterns},
  doi={10.1109/UBMK59864.2023.10286646},
  ISSN={2521-1641},
  month={Sep.},}@ARTICLE{10286838,
  author={Mohammadat, Tage},
  journal={IEEE Access}, 
  title={A Model of Design for Computing Systems: A Categorical Approach}, 
  year={2023},
  volume={11},
  number={},
  pages={116304-116347},
  abstract={This paper introduces the model of design (MoD), a framework that leverages category theory to study the design and development of computer-driven systems, to the academic and engineering communities dealing with computer systems. The model of design aims to offer a minimal framework for modelling the design and development of embedded computation across domains and abstractions, focusing on functional and extra-functional aspects as well as overarching concerns for automaticity, correctness and reuse. This nuanced approach provides insights into the theory and practice of computer systems design.},
  keywords={Computational modeling;Semantics;Symbols;Solid modeling;Ontologies;Grammar;Context modeling;Design automation;System-level design;Computing systems;computer-aided design (CAD);electronic design automation (EDA);embedded system design;architectural design;model-driven engineering;domain-specific modelling languages;model of computation;system-level design;hardware/software co-design},
  doi={10.1109/ACCESS.2023.3325349},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10275523,
  author={Dhouib, Saadia and Huang, Yining and Smaoui, Asma and Bhanja, Tapanta and Gezer, Volkan},
  booktitle={2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Papyrus4Manufacturing: A Model-Based Systems Engineering approach to AAS Digital Twins}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={As digital twins gain momentum in their usage in diverse domains, the concept of Asset Administration Shells (AAS) has become very relevant for achieving the digital twin approach, where Administration Shells are the digital representation of physical assets. Being a relatively new concept in the Industrial Internet of Things (IIoT) domain, the tools and approaches for creating and deploying AASs are likewise in infancy. This paper introduces an open-source tool, Papyrus4Manufacturing, which provides a model-based systems engineering approach to the AAS. This toolset supports the creation of AAS digital twins from modeling to automatic deployment and connection to assets using the OPC UA protocol. This paper also includes an evaluation of its usability, as it is put to test with an academic use case.},
  keywords={Protocols;Databases;Memory;Software;Digital twins;Servers;Modeling;Digital Twins;Asset Administration Shell;Model-Based System Engineering;Unified Modelling Language;UML Profiles;Generative Software Engineering;OPC UA;BaSyx;Eclipse Papyrus},
  doi={10.1109/ETFA54631.2023.10275523},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10275701,
  author={Knorr, Felix and Kastner, Wolfgang},
  booktitle={2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Towards a Uniform Exchange Format for Home and Building Automation using VDI 3814}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Exchanging technical documents in the building automation domain is a complicated process. Files are distributed either as drawings, spreadsheets, or text documents. Each stakeholder has to re-enter the data into their own system, and changes are revised manually, often even without revision control. This paper presents a uniform exchange format based on the ’graphical’ standard VDI 3814. To increase acceptance, the industry standards XSD and XML were chosen. As a result of this work, a model is provided that covers the concepts and exchange files provided in the VDI 3814 standard. Given a supporting tool, data can be entered, revised, and exchanged automatically. Based on this unified representation, it is subsequently possible to transfer the data into one of the already existing ontologies in this domain by using model transformations. Some of these ontologies are also referred to in this paper.},
  keywords={Industries;Buildings;XML;Ontologies;Data models;Stakeholders;Standards;Building Automation;Uniform Format;VDI 3813;VDI 3814},
  doi={10.1109/ETFA54631.2023.10275701},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10266116,
  author={Powell, James and Balakireva, Lyudmila},
  booktitle={2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL)}, 
  title={Measuring the Growth of Ideas in a Title Corpus}, 
  year={2023},
  volume={},
  number={},
  pages={291-292},
  abstract={Beyond bibliometrics, there is interest in characterizing the evolution of the number of ideas in scientific papers, or more generally, exploring the progress of science. We use various tokenization and phrase extraction strategies combined with lexical diversity metrics to analyze titles in our corpus. We compared four lexical diversity metrics for each corpora variants, to look for indications that new concepts might be emerging over time.},
  keywords={Measurement;Semantics;Ecosystems;Ontologies;Tokenization;Libraries;Ecology;lexical diversity;natural language processing;word embeddings;science of science},
  doi={10.1109/JCDL57899.2023.00063},
  ISSN={2575-8152},
  month={June},}@INPROCEEDINGS{10266269,
  author={Sierra-Múnera, Alejandro and Westphal, Jan and Krestel, Ralf},
  booktitle={2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL)}, 
  title={Efficient Ultrafine Typing of Named Entities}, 
  year={2023},
  volume={},
  number={},
  pages={205-214},
  abstract={Ultrafine named entity typing (UFET) refers to the assignment of predefined labels to entity mentions in a given context. In contrast to traditional named entity typing, the number of potential labels is in the thousands and one mention can have more than one assigned type. Previous approaches either depend on large training datasets, or require inefficient encoding of all input-type combinations. Therefore, there is a need for investigating the efficiency during training and prediction of entity typing models in the ultrafine-grained setting, considering its distinctively bigger search space, compared to the coarse- and fine-grained tasks. To efficiently solve UFET, we propose Decent, a lightweight model that encodes, using a pretrained language model, the input sentences separately from the type labels. Additionally, we make use of negative oversampling to speed up the training while improving the generalization of unseen types. Using an openly available UFET dataset, we evaluated the classification and runtime performance of Decent and observed that training and prediction runtime is orders of magnitude faster than the current state-of-the-art approaches, while maintaining a competitive classification performance.},
  keywords={Training;Vocabulary;Runtime;Limiting;Computational modeling;Semantics;Predictive models;ultrafine enity typing;named entity recognition},
  doi={10.1109/JCDL57899.2023.00038},
  ISSN={2575-8152},
  month={June},}@INPROCEEDINGS{10261615,
  author={Zeng, Ling and Liang, Zaoqing and Liang, Yun and Huang, Peijie},
  booktitle={2023 5th International Conference on Computer Science and Technologies in Education (CSTE)}, 
  title={Research on Key Technologies of Automated Instructional Design for Engineering Education Courses}, 
  year={2023},
  volume={},
  number={},
  pages={86-92},
  abstract={An Automated Instructional Design (AID) solution for engineering education courses instructional design is proposed in this research. By limiting the Domain of AID to engineering education courses, and limiting the course objectives to the graduate attributes and professional competence of engineering education programme, as well as limiting the instructional methods to task-and-activity-based methods, and applying the evaluation vocabulary regularly used in the field of engineering education, a complete set of instructional design vocabulary related to the instructional design domain can be abstracted. Using the Unified Modeling Language (UML) Profile mechanism, the complete set of instructional design vocabulary can be represented by a designed visual model language, which provide a complete instructional design description language, named Instructional Design Visual Model Language (IDVML), dedicated to the field of engineering education for AID tool implementation. Then the existed Model-Driven Architecture (MDA) tools can be applied to design IDVML-related Platform Independent Model (PIM) and various Platform Specific Model (PSM) meta-models, and the conversion templates from PIM to various PSM, as well as various conversion templates from PSM to executable code, database table, Web page, course Ontology, etc., so as to realize the conversion from IDVML to program code directly. Finally, using the Meta-model Object Facility (MOF), an independent AID can be implemented. This AID can help the teacher to design the teaching objectives, content, tasks and activities, evaluation of engineering education related courses which meet the requirements of engineering education accreditation. At the same time, the course Ontology can be generated to provide Ontology basis for the subsequent construction of individualized learning system.},
  keywords={Vocabulary;Visualization;Limiting;Unified modeling language;Computer architecture;Ontologies;Accreditation;automated instructional design;engineering education;instructional design visulized modleing language;instructional design meta-model},
  doi={10.1109/CSTE59648.2023.00022},
  ISSN={},
  month={April},}@INPROCEEDINGS{10223881,
  author={Ji, Wei and Cao, Qinghong and Shi, Jin and Zhu, Enyao and Xu, Tianyi and He, Hao},
  booktitle={2023 26th ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)}, 
  title={Research on Domain Knowledge Representation Techniques}, 
  year={2023},
  volume={},
  number={},
  pages={150-157},
  abstract={Knowledge is the fruit of human's knowledge of the objective world in practice and the crystallization of wisdom. A knowledge base makes a knowledge-based system (or expert system) intelligent by structuring and sorting out knowledge in a specific field and storing, organizing, managing and using it in a computer using a scientific knowledge representation. Based on the research of knowledge base construction in securities industry, this paper first summarizes and explains several representative knowledge representation models. Then, it summarizes the application scenarios of common knowledge representation techniques in the fields of product design, robot control, and natural language processing. In addition, based on the investigation of the knowledge characteristics of the securities industry, a knowledge representation model of the securities industry based on 5W1lH is proposed to organize and manage the multimodal information resources and provide high-value information for user needs, while the knowledge representation technology of the mechanical industry based on hypergraph embedding is examined and the specific processes and application scenarios are summarized.},
  keywords={Industries;Service robots;Robot control;Knowledge representation;Product design;Natural language processing;Security;knowledge representation;knowledge graph;industry knowledge base},
  doi={10.1109/SNPD-Winter57765.2023.10223881},
  ISSN={},
  month={July},}@INPROCEEDINGS{10224086,
  author={Jousselme, A-L. and de Villiers, J.P. and de Freitas, A. and Blasch, E. and Dragos, V. and Pavlin, G. and Costa, P. C. and Laskey, K. B. and Laudy, C.},
  booktitle={2023 26th International Conference on Information Fusion (FUSION)}, 
  title={Uncertain about ChatGPT: enabling the uncertainty evaluation of large language models}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={ChatGPT, OpenAI’s chatbot, has gained consider-able attention since its launch in November 2022, owing to its ability to formulate articulated responses to text queries and comments relating to seemingly any conceivable subject. As impressive as the majority of interactions with ChatGPT are, this large language model has a number of acknowledged shortcomings, which in several cases, may be directly related to how ChatGPT handles uncertainty. The objective of this paper is to pave the way to formal analysis of ChatGPT uncertainty handling. To this end, the ability of the Uncertainty Representation and Reasoning Framework (URREF) ontology is assessed, to support such analysis. Elements of structured experiments for reproducible results are identified. The dataset built varies Information Criteria of Correctness, Non-specificity, Self-confidence, Relevance and Inconsistency, and the Source Criteria of Reliability, Competency and Type. ChatGPT’s answers are analyzed along Information Criteria of Correctness, Non-specificity and Self-confidence. Both generic and singular information are sequentially provided. The outcome of this preliminary study is twofold: Firstly, we validate that the experimental setup is efficient in capturing aspects of ChatGPT uncertainty handling. Secondly, we identify possible modifications to the URREF ontology that will be discussed and eventually implemented in URREF ontology Version 4.0 under development.},
  keywords={Uncertainty;Ontologies;Media;Chatbots;Cognition;Reliability;Disruptive technologies;Uncertainty evaluation;Ontology;Information quality;Source quality;Large Language Models;NLP},
  doi={10.23919/FUSION52260.2023.10224086},
  ISSN={},
  month={June},}@INPROCEEDINGS{10208670,
  author={Srinivasan, Tejas and Ren, Xiang and Thomason, Jesse},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={Curriculum Learning for Data-Efficient Vision-Language Alignment}, 
  year={2023},
  volume={},
  number={},
  pages={5619-5624},
  abstract={Aligning image and text encoders from scratch using contrastive learning requires large amounts of paired image-text data. We alleviate this need by aligning individually pre-trained language and vision representation models using a much smaller amount of paired data with a curriculum learning algorithm to learn fine-grained vision-language alignments. TOnICS (Training with Ontology-Informed Contrastive Sampling) initially samples minibatches whose image-text pairs contain a wide variety of objects to learn object-level vision-language alignment, and progressively samples minibatches where all image-text pairs contain the same object to learn finer-grained contextual alignment. Aligning pre-trained BERT and VinVL-OD models to each other using TOnICS outperforms CLIP on downstream zero-shot image retrieval using < 1% as much training data.},
  keywords={Training;Computer vision;Conferences;Computational modeling;Image retrieval;Bit error rate;Training data},
  doi={10.1109/CVPRW59228.2023.00595},
  ISSN={2160-7516},
  month={June},}@INPROCEEDINGS{10205809,
  author={Nadhila, Fadiah and Alamsyah, Andry},
  booktitle={2023 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)}, 
  title={Mapping Personality Traits to Customer Complaints: Framework for Personalized Customer Service}, 
  year={2023},
  volume={},
  number={},
  pages={96-101},
  abstract={The study establishes utilizing the Big Five Personality framework and a Personality Measurement Platform (PMP) for personality analysis. Moreover, Customer Complaint Ontology (CCOntology) framework implements a Naive Bayes machine learning methodology to evaluate and scrutinize customer complaints. The algorithm works by calculating the probability of each complaint category. This association is measured in percentages, enabling the identification of specific personality traits related to customer complaints through identifying complaint characteristics and areas of concern. The study has found that individuals with neurotic personality traits who encounter customer complaints are often associated with problem categories such as Non-Contract, Privacy, and Contract and are more likely to express strong emotional dissatisfaction with a product or service. Linking customer complaints with their corresponding personalities can be an incredibly effective and innovative strategy for personalized customer service businesses in anticipating their needs and providing tailored recommendations that can improve the likelihood of customers making purchases. This approach involves educating employees on the importance of actively listening to customers, asking relevant questions, and anticipating their needs, ensuring that businesses can enhance customer satisfaction while building a loyal customer base.},
  keywords={Technological innovation;Privacy;Social networking (online);Customer services;Oral communication;Ontologies;Big Data;Big Five Personality;Customer Complaint Ontology (CCOntology);Naive Bayes;Personality Measurement Platform;Personalized Customer Service},
  doi={10.1109/IAICT59002.2023.10205809},
  ISSN={2834-8249},
  month={July},}@INPROCEEDINGS{10196264,
  author={Okazaki, Sho and Shirafuji, Shouhei and Yasui, Toshinori and Ota, Jun},
  booktitle={2023 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)}, 
  title={A Framework to Support Failure Cause Identification in Manufacturing Systems through Generalization of Past FMEAs}, 
  year={2023},
  volume={},
  number={},
  pages={858-865},
  abstract={This study proposes a framework for inferring the causes of failures occurring in manufacturing systems from past Failure Mode and Effect Analyses (FMEAs) conducted on other systems to assist in inspecting and maintaining the systems. Among various manufacturing systems, a framework to search past FMEAs and the corresponding causes of the failure requires solving the following problems. First, the difference in products, equipment, and wording to represent them make it difficult to search the similar failure phenomenon from FMEAs. Secondly, the causes of failure highly depend on the process flow of the system until the failure occurs. Therefore, it is also hard to find appropriate failure causes from FMEAs without reflecting on the process. The framework solves the first issue by generalizing descriptions in past FMEAs based on structured concepts of manufacturing systems in an ontology before inference of causes to address. Furthermore, the framework analyzes the correspondence of the process flows between the target manufacturing system and past FMEAs using a process order model generated by SysML diagrams to solve the second issue. The comparison between the causes inferred by the proposed framework and by skilled experts for three typical failures in the manufacturing system and the interview with them about the plausibility of the inference results showed that more than 73 % of them were valid.},
  keywords={Analytical models;Mechatronics;Ontologies;Maintenance engineering;Search problems;Cognition;Data models},
  doi={10.1109/AIM46323.2023.10196264},
  ISSN={2159-6255},
  month={June},}@ARTICLE{10198434,
  author={Jaradeh, Amer and Kurdy, Mohamad-Bassam},
  journal={IEEE Access}, 
  title={ArEmotive Bridging the Gap: Automatic Ontology Augmentation Using Zero-Shot Classification for Fine-Grained Sentiment Analysis of Arabic Text}, 
  year={2023},
  volume={11},
  number={},
  pages={81318-81330},
  abstract={Human-computer interaction remains one of the final frontiers to conquer while held in perspective with the rapid developments and technology growth over recent years. It is an arduous task to convey the true human intent to the machine in order to generate a computerized relevant decision in a certain field. In recent years, focus has shifted to cover fields of study that relate to Sentiment Analysis (SA) to improve and ease the tasks of our daily lives. We Propose ArEmotive (Arabic Emotive), a fine-grained sentiment analysis system that is human-independent which can automatically grow its source of information allowing for more precision and a greater dataset each time it is used through ontology augmentation and classification. Our proposed architecture relies on multiple data sources running through certain pipelines to generate a central online repository utilized by any mobile system to access this info-base. This system is important because many researchers in the field of automated ontology alignment and ontology mapping achieved a semi-automated approach to map new ontologies out of old ones or to extend already existing ontologies with data from new ones. ArEmotive identifies fine-grained emotions in text based on a dynamic ontology enriched through ontology alignment, mapping and machine learning assisted classification, resulting in a structure that contributes in: a centralized dataset ever growing to fit the need of the users, a sustainable structure able to allocate new data sources without the need to modify the system, ability to generate appropriate information even with the absence of “parent” sources.},
  keywords={Ontologies;Task analysis;Social networking (online);Sentiment analysis;Bridges;Blogs;Soft sensors;Emotion recognition;Arabic NLP;fine-grained emotions;ontology augmentation},
  doi={10.1109/ACCESS.2023.3300737},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10193435,
  author={Belani, Hrvoje and Šolić, Petar and Perković, Toni and Pleština, Vladimir},
  booktitle={2023 8th International Conference on Smart and Sustainable Technologies (SpliTech)}, 
  title={IoT Ontology Development Process for Well-Being, Aging and Health: Challenges and Opportunities}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Ontology development processes are not trivial, given the inherently complex nature of knowledge capturing and management, as well as the need to provide structural and methodical approach on the process methodology itself in order for it to be adopted and usable. If aiming to develop an ontology for multidimensional concepts, such as well-being, aging and health, it is certain that knowledge from multiple domains have to be included, which only extends the time needed for ontology engineering. If such environments aim to be supported by the Internet of Things, than challenges rise even more. This paper provides a scoping analysis of existing well-known ontology development methodologies, with a note on the extent of their adoption and readiness to be used in a multi-domain circumstances. The approach to IoT ontology development process tailoring has been presented and elaborated, as well as the challenges specific to IoT ontology development for well-being, aging and health. Finally, research opportunities have been presented and future directions given on providing more comprehensive, more tailored and more usable ontology development methodologies.},
  keywords={Knowledge engineering;Semantics;Ontologies;Aging;Reliability;Internet of Things;Internet of Things;ontology;well-being;e-health;development process},
  doi={10.23919/SpliTech58164.2023.10193435},
  ISSN={},
  month={June},}@INPROCEEDINGS{10184697,
  author={Geng, Yuxia and Chen, Jiaoyan and Pan, Jeff Z. and Chen, Mingyang and Jiang, Song and Zhang, Wen and Chen, Huajun},
  booktitle={2023 IEEE 39th International Conference on Data Engineering (ICDE)}, 
  title={Relational Message Passing for Fully Inductive Knowledge Graph Completion}, 
  year={2023},
  volume={},
  number={},
  pages={1221-1233},
  abstract={In knowledge graph completion (KGC), predicting triples involving emerging entities and/or relations, which are unseen when the KG embeddings are learned, has become a critical challenge. Subgraph reasoning with message passing is a promising and popular solution. Some recent methods have achieved good performance, but they (i) usually can only predict triples involving unseen entities alone, failing to address more realistic fully inductive situations with both unseen entities and unseen relations, and (ii) often conduct message passing over the entities with the relation patterns not fully utilized. In this study, we propose a new method named RMPI which uses a novel Relational Message Passing network for fully Inductive KGC. It passes messages directly between relations to make full use of the relation patterns for subgraph reasoning with new techniques on graph transformation, graph pruning, relation-aware neighborhood attention, addressing empty subgraphs, etc., and can utilize the relation semantics defined in the KG’s ontological schema. Extensive evaluation on multiple benchmarks has shown the effectiveness of RMPI’s techniques and its better performance compared with the existing methods that support fully inductive KGC. RMPI is also comparable to the state-of-the-art partially inductive KGC methods with very promising results achieved. Our codes, data and some supplementary experiment results are available at https://github.com/zjukg/RMPI.},
  keywords={Knowledge engineering;Codes;Message passing;Semantics;Knowledge graphs;Benchmark testing;Data engineering;Knowledge Graph;Inductive Knowledge Graph Completion;Message Passing;Link Prediction;Ontology},
  doi={10.1109/ICDE55515.2023.00098},
  ISSN={2375-026X},
  month={April},}@INPROCEEDINGS{10187511,
  author={Weigand, Hans and Johannesson, Paul},
  booktitle={2023 IEEE 25th Conference on Business Informatics (CBI)}, 
  title={How to Identify your Design Science Research Artifact}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  abstract={Design Science Research (DSR) is about the development and investigation of artifacts in context. However, in many articles that subscribe to a DSR approach, the artifact is not clearly classified and identified. Very little attention has been given in the DSR literature on this topic, so guidelines are lacking. Based on artifact ontology, this paper proposes guidelines for design researchers in the IS domain on how to specify both the artifact and the research objective. For validation, the guidelines have been applied to a range of DSR papers. Our results show that the artifact definition guidelines can add to the precision of the research object specification.},
  keywords={Philosophical considerations;Design methodology;Bibliographies;Ontologies;Data science;Data models;Informatics;Design Science;artifact ontology;research methodology},
  doi={10.1109/CBI58679.2023.10187511},
  ISSN={2378-1971},
  month={June},}@ARTICLE{10185050,
  author={Razzaq, Muhammad Saad and Maqbool, Fahad and Ilyas, Muhammad and Jabeen, Hajira},
  journal={IEEE Access}, 
  title={EvoRecipes: A Generative Approach for Evolving Context-Aware Recipes}, 
  year={2023},
  volume={11},
  number={},
  pages={74148-74164},
  abstract={Generative AI e.g. Large Language Models (LLMs) can be used to generate new recipes. However, LLMs struggle with more complex aspects like recipe semantics and process comprehension. Furthermore, LLMs have limited ability to account for user preferences since they are based on statistical patterns. As a result, these recipes may be invalid. Evolutionary algorithms inspired by the process of natural selection are optimization algorithms that use stochastic operators to generate new solutions. These algorithms can generate large number of solutions from the set of possible solution space. Moreover, these algorithms have the capability to incorporate user preferences in fitness function to generate novel recipes that are more aligned with the fitness objective. In this paper, we propose the  $EvoRecipes$  framework to generate novel recipes. The  $EvoRecipes$  framework utilizes both Genetic Algorithm and generative AI in addition to  $RecipeOn$  ontology, and  $RecipeKG$  knowledge graph. Genetic Algorithm explore the large solution space of encoded recipe solutions and are capable of incorporating user preferences, while LLMs are used to generate recipe text from encoded recipe solutions.  $EvoRecipes$  uses a population of context-aware recipe solutions from the  $RecipeKG$  knowledge graph.  $RecipeKG$  encodes recipes in RDF format using classes and properties as defined in the  $RecipeOn$  ontology. Moreover, to evaluate the alignment of  $EvoRecipe$  generated recipes with multiple intended objectives, we propose a fitness function that incorporates novelty, simplicity, visual appeal, and feasibility. Additionally, to evaluate the quality of the  $EvoRecipe$  generated recipes while considering the subjective nature of recipes, we conducted a survey using multi-dimensional metrics (i.e. contextual, procedural, and novelty). Results show that  $EvoRecipes$  generated recipes are novel, valid and incorporate user preferences.},
  keywords={Ontologies;Creativity;Resource description framework;Knowledge graphs;Genetic algorithms;Sociology;Semantics;Food products;Knowledge graph;ontology;computational creativity;recipe evolution;recipe;food},
  doi={10.1109/ACCESS.2023.3296144},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10145722,
  author={Bandara, H. M. R. L. and Ranathunga, L.},
  booktitle={2023 3rd International Conference on Advanced Research in Computing (ICARC)}, 
  title={Ontology Based Restaurant Recommendation Approach}, 
  year={2023},
  volume={},
  number={},
  pages={78-83},
  abstract={As the world moves forward, the restaurant industry is rapidly expanding. Customers may never physically evaluate a restaurant based on its services until that customer has practical experience with it. A better recommendation mechanism can always direct the customer to the correct location, resulting in a positive outcome. The paper discusses an approach of a context rich chatbot that can identify the customer’s mode of thinking using a restaurant ontology that suggests relevant restaurants and foods. Most importantly the defined methodology will use a hybrid version of knowledge bases along with a two-way bind to the primary knowledge base. The chatbot will proceed to find relationships in the ontology by tracing concept definitions and properties while feeding information from the database. The main components related to the proposed system are Natural Language Understanding (NLU) pipeline, dialog management module, action server, knowledge query module, and data repository (MongoDB). This mechanism was evaluated through information retrieval measures.},
  keywords={Industries;Databases;Knowledge based systems;Pipelines;Ontologies;Chatbots;Information retrieval;Rasa Framework;Ontology;Database;Knowledge Query Module;Dialog Management Module;Action Server},
  doi={10.1109/ICARC57651.2023.10145722},
  ISSN={},
  month={Feb},}@INBOOK{10144419,
  author={Godspower, Osaretin Ekuobase and Esingbemi, Princewill Ebietomere},
  booktitle={Semantic Technologies for Intelligent Industry 4.0 Applications}, 
  title={1 Semantic Search Engine in Industry 4.0}, 
  year={2023},
  volume={},
  number={},
  pages={1-24},
  abstract={As the world enters the era of big data, there is a serious need to give a semantic perspective to the data in order to find unseen patterns, derive meaningful information, and make intelligent decisions. Semantic technologies offer the richest machine-interpretable (rather than just machine-processable) and explicit semantics that are being extensively used in various domains and industries. These technologies reduce the problem of large semantic loss in the process of modelling knowledge, and provide sharable, reusable knowledge, and a common understanding of the knowledge. As a result, the interoperability and interconnectivity of the model make it priceless for addressing the issues of querying data. These technologies work with the concepts and relations that are very close to the working of the human brain. They provide a semantic representation of any data format: unstructured or semi-structured. As a consequence, data becomes real-world entity rather than a string of characters. For these reasons, semantic technologies are highly valuable tools to simplify the existing problems of the industry leading to new opportunities. However, there are some challenges that need to be addressed to make industrial applications and machines smarter. This book aims to provide a roadmap for semantic technologies and highlights the role of these technologies in industry. The book also explores the present and future prospects of these semantic technologies along with providing answers to various questions like: Are semantic technologies useful for the next era (industry 4.0)? Why are semantic technologies so popular and extensively used in the industry? Can semantic technologies make intelligent industrial applications? Which type of problem requires the immediate attention of researchers? Why are semantic technologies very helpful in people&#x2019;s future lives? This book will potentially serve as an important guide towards the latest industrial applications of semantic technologies for the upcoming generation, and thus becomes a unique resource for scholars, researchers, professionals and practitioners in the field.},
  keywords={},
  doi={},
  ISSN={},
  publisher={River Publishers},
  isbn={9788770227810},
  url={https://ieeexplore.ieee.org/document/10144419},}@INPROCEEDINGS{10131050,
  author={Dunbar, Daniel and Vierlboeck, Maximilian and Blackburn, Mark},
  booktitle={2023 IEEE International Systems Conference (SysCon)}, 
  title={Use of Natural Language Processing in Digital Engineering Context to Aid Tagging of Model}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper uses Natural Language Processing to provide augmented intelligence assistance to the resource intensive task of aligning systems engineering artifacts, namely text requirements and system models, with ontologies. Ontologies are a key enabling technology for digital, multidisciplinary interoperability. The approach presented in this paper combines the efficiency of statistical based natural language processing to process large sets of data with expert verification of output to enable accurate alignment to ontologies in a time efficient manner. It applies this approach to an example from the telecommunications domain to demonstrate the workflows and highlight key points in the process. Enabling easier, faster alignment of systems engineering artifacts with ontologies allows for a holistic view of a system under design and enables interoperability between tools and domains.},
  keywords={Measurement;Ontologies;Tagging;Natural language processing;Telecommunications;Requirements engineering;Task analysis;ontology;natural language processing;semantic web;digital engineering;authoritative source of truth;augmented intelligence},
  doi={10.1109/SysCon53073.2023.10131050},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{10131078,
  author={Chen, Rui and Wang, Guoxin and Wu, Shouxuan and Lu, Jinzhi and Yan, Yan},
  booktitle={2023 IEEE International Systems Conference (SysCon)}, 
  title={A Service-oriented Approach Supporting Model Integration in Model-based Systems Engineering}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={When using Model-Based Systems Engineering (MBSE) to develop complex systems, models using different syntax and semantics are typically implemented in a heterogeneous environment which leads to difficulties to realize data integrations across the entire lifecycle. Specifically, seamless exchanges between models of different modeling tools are needed to support system lifecycle activities such as requirement analysis, function analysis, verification and validation. This article illustrates a service-oriented approach to support model integration for model-based systems engineering, especially between architecture design and system verification. First, a set of semantic mapping rules between architecture models and simulation models based on Open Service of Lifecycle Collaboration (OSLC) are proposed to support the formalization of technical resources (models, data, APIs). Then OSLC adapters are developed to transform models, data and APIs into web-based services. The services are deployed by a service discovering plug-in within a specific modeling tool for model information exchange. The approach is illustrated by a case study on KARMA architecture model and Modelica simulation model for a six-degree-of-freedom robot (RobotR3) system. We evaluate the availability and efficiency of this method from both qualitative and quantitative perspectives. The results show that our approach is effective in model and data integration.},
  keywords={Adaptation models;Analytical models;System verification;Semantics;Data integration;Transforms;Syntactics;Model integration;Model-Based Systems Engineering;Open Service for Lifecycle Collaboration;Modelica},
  doi={10.1109/SysCon53073.2023.10131078},
  ISSN={2472-9647},
  month={April},}@ARTICLE{10129977,
  author={Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Yang, Yumeng and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
  journal={IEEE Transactions on NanoBioscience}, 
  title={Protein Function Prediction With Functional and Topological Knowledge of Gene Ontology}, 
  year={2023},
  volume={22},
  number={4},
  pages={755-762},
  abstract={Gene Ontology (GO) is a widely used bioinformatics resource for describing biological processes, molecular functions, and cellular components of proteins. It covers more than 5000 terms hierarchically organized into a directed acyclic graph and known functional annotations. Automatically annotating protein functions by using GO-based computational models has been an area of active research for a long time. However, due to the limited functional annotation information and complex topological structures of GO, existing models cannot effectively capture the knowledge representation of GO. To solve this issue, we present a method that fuses the functional and topological knowledge of GO to guide protein function prediction. This method employs a multi-view GCN model to extract a variety of GO representations from functional information, topological structure, and their combinations. To dynamically learn the significance weights of these representations, it adopts an attention mechanism to learn the final knowledge representation of GO. Furthermore, it uses a pre-trained language model (i.e., ESM-1b) to efficiently learn biological features for each protein sequence. Finally, it obtains all predicted scores by calculating the dot product of sequence features and GO representation. Our method outperforms other state-of-the-art methods, as demonstrated by the experimental results on datasets from three different species, namely Yeast, Human and Arabidopsis. Our proposed method’s code can be accessed at: https://github.com/Candyperfect/Master.},
  keywords={Proteins;Feature extraction;Amino acids;Annotations;Predictive models;Biological system modeling;Protein sequence;Convolutional neural networks;Graph neural networks;Protein function prediction;gene ontology;multi-view GCN;pre-trained language model},
  doi={10.1109/TNB.2023.3278033},
  ISSN={1558-2639},
  month={Oct},}@ARTICLE{10123130,
  author={Zelina, Petr and Halámková, Jana and Nováček, Vít},
  journal={IEEE Transactions on NanoBioscience}, 
  title={Extraction, Labeling, Clustering, and Semantic Mapping of Segments From Clinical Notes}, 
  year={2023},
  volume={22},
  number={4},
  pages={781-788},
  abstract={This work is motivated by the scarcity of tools for accurate, unsupervised information extraction from unstructured clinical notes in computationally underrepresented languages, such as Czech. We introduce a stepping stone to a broad array of downstream tasks such as summarisation or integration of individual patient records, extraction of structured information for national cancer registry reporting or building of semi-structured semantic patient representations that can be used for computing patient embeddings. More specifically, we present a method for unsupervised extraction of semantically-labeled textual segments from clinical notes and test it out on a dataset of Czech breast cancer patients, provided by Masaryk Memorial Cancer Institute (the largest Czech hospital specialising exclusively in oncology). Our goal was to extract, classify (i.e. label) and cluster segments of the free-text notes that correspond to specific clinical features (e.g., family background, comorbidities or toxicities). Finally, we propose a tool for computer-assisted semantic mapping of segment types to pre-defined ontologies and validate it on a downstream task of category-specific patient similarity. The presented results demonstrate the practical relevance of the proposed approach for building more sophisticated extraction and analytical pipelines deployed on Czech clinical notes.},
  keywords={Task analysis;Semantics;Feature extraction;Ontologies;Nanobioscience;Measurement;Clinical diagnosis;Text categorization;Information retrieval;NLP;EHR;clinical notes;information extraction;text classification},
  doi={10.1109/TNB.2023.3275195},
  ISSN={1558-2639},
  month={Oct},}@INPROCEEDINGS{10095062,
  author={Abrougui, Rim and Damnati, Géraldine and Heinecke, Johannes and Béchet, Frédéric},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Abstract Representation for Multi-Intent Spoken Language Understanding}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Current sequence tagging models based on Deep Neural Network models with pretrained language models achieve almost perfect results on many SLU benchmarks with a flat semantic annotation at the token level such as ATIS or SNIPS. When dealing with more complex human-machine interactions (multi-domain, multi-intent, dialog context), relational semantic structures are needed in order to encode the links between slots and intents within an utterance and through dialog history. We propose in this study a new way to project annotation in an abstract structure with more compositional expressive power and a model to directly generate this abstract structure. We evaluate it on the MultiWoz dataset in a contextual SLU experimental setup. We show that this projection can be used to extend the existing flat annotations towards graph-based structures.},
  keywords={Human computer interaction;Deep learning;Annotations;Semantics;Neural networks;Natural languages;Tagging;Natural Language Understanding;Spoken Language Understanding;sequence tagging;sequence-to-sequence models},
  doi={10.1109/ICASSP49357.2023.10095062},
  ISSN={2379-190X},
  month={June},}@INPROCEEDINGS{10096669,
  author={Su, Ruolin and Yang, Jingfeng and Wu, Ting-Wei and Juang, Biing-Hwang},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Choice Fusion As Knowledge For Zero-Shot Dialogue State Tracking}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={With the demanding need for deploying dialogue systems in new domains with less cost, zero-shot dialogue state tracking (DST), which tracks user’s requirements in task-oriented dialogues without training on desired domains, draws attention increasingly. Although prior works have leveraged question-answering (QA) data to reduce the need for in-domain training in DST, they fail to explicitly model knowledge transfer and fusion for tracking dialogue states. To address this issue, we propose CoFunDST, which is trained on domain-agnostic QA datasets and directly uses candidate choices of slot-values as knowledge for zero-shot dialogue-state generation, based on a T5 pre-trained language model. Specifically, CoFunDST selects highly-relevant choices to the reference context and fuses them to initialize the decoder to constrain the model outputs. Our experimental results show that our proposed model achieves outperformed joint goal accuracy compared to existing zero-shot DST approaches in most domains on the MultiWOZ 2.1. Extensive analyses demonstrate the effectiveness of our proposed approach for improving zero-shot DST learning from QA.},
  keywords={Training;Costs;Fuses;Signal processing;Data models;Decoding;Task analysis;Dialogue state tracking;zero-shot;question answering;pre-trained language model;knowledge fusion},
  doi={10.1109/ICASSP49357.2023.10096669},
  ISSN={2379-190X},
  month={June},}@INPROCEEDINGS{10112187,
  author={Saraswat, Deepak},
  booktitle={2023 6th International Conference on Information Systems and Computer Networks (ISCON)}, 
  title={Ontology Based Agriculture Data Mining using IWO and RNN}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={An ontology is a machine-interpretable formal description of domain knowledge. In current years, ontologies have risen to prominence as a key tool for demonstrating domain knowledge and a key element of several knowledge management systems, decision-support systems (DSS) and other intelligent systems including in agriculture. However, a study of the current literature on agricultural ontologies suggests that the majority of research that suggest agricultural ontologies lack a clear assessment mechanism. This is unwanted because this is impossible to assess the value of ontologies in research and practise without well-structured assessment mechanisms. Furthermore, relying on such ontologies and sharing them on the Semantic Web or amongst semantic-aware apps is problematic. This paper presents a framework for selecting appropriate assessment techniques for Ontology Based Agriculture Data Mining utilizing Invasive Weed Optimization (IWO) and Re-current Neural Network (RNN) that appears to be absent from most recent agricultural ontology research. The framework facilitates the selection of relevant evaluation techniques for a particular ontology based on its intended user.},
  keywords={Decision support systems;Semantic Web;Knowledge engineering;Recurrent neural networks;Prototypes;Ontologies;Agriculture;Data Mining;Ontology;IWO;RNN;Agriculture 4.0;Agriculture 5.0},
  doi={10.1109/ISCON57294.2023.10112187},
  ISSN={2832-143X},
  month={March},}@ARTICLE{10107403,
  author={Li, Jijie and Shuang, Kai and Guo, Jinyu and Shi, Zengyi and Wang, Hongman},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Enhancing Semantic Relation Classification With Shortest Dependency Path Reasoning}, 
  year={2023},
  volume={31},
  number={},
  pages={1550-1560},
  abstract={Relation Classification (RC) is a basic and essential task of Natural Language Processing. Existing RC methods can be classified into two categories: sequence-based methods and dependency-based methods. Sequence-based methods identify the target relation based on the overall semantics of the whole sentence, which will inevitably introduce noisy features. Dependency-based methods extract indicative word-level features from the Shortest Dependency Path (SDP) between given entities and attempt to establish a statistical association between the words and the target relations. This pattern relatively eliminates the influence of noisy features and achieves a robust performance on long sentences. Nevertheless, we observe that majority of relation classification processes involve complex semantic reasoning which is hard to be achieved based on the word-level statistical association. To solve this problem, we categorize all relations into atomic relations and composed-relations. The atomic relations are the basic relations that can be identified based on the word-level features, while the composed-relation requires to be deducted from multiple atomic relations. Correspondingly, we propose the Atomic Relation Encoding and Reasoning Model (ATERM). In the atomic relation encoding stage, ATERM groups the word-level features and encodes multiple atomic relations in parallel. In the atomic relation reasoning stage, ATERM establishes the atomic relation chain where relation-level features are extracted to identify composed-relations. Experiments show that our method achieves state-of-the-art results on the three most popular relation classification datasets – TACRED, TACRED-Revisit, and SemEval 2010 task 8 with significant improvements.},
  keywords={Semantics;Knowledge based systems;Ontologies;Feature extraction;Cognition;Encoding;Natural language processing;Information extraction;graph convolution;shortest dependency path;semantic reasoning},
  doi={10.1109/TASLP.2023.3265205},
  ISSN={2329-9304},
  month={},}@ARTICLE{10100906,
  author={Huang, Heyan and Liu, Xiao and Shi, Ge and Liu, Qian},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Event Extraction With Dynamic Prefix Tuning and Relevance Retrieval}, 
  year={2023},
  volume={35},
  number={10},
  pages={9946-9958},
  abstract={We consider event extraction in a generative manner with template-based conditional generation. Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts, these generation-based methods have several significant challenges, including using suboptimal prompts, static event type information, and the overwhelming number of irrelevant event types. In this article, we propose a generative template-based method with dynamic prefixes and a relevance retrieval framework for event extraction (GREE) by first integrating context information with type-specific prefixes to learn a context-specific prefix for each context, and then retrieving the relevant event types with an adaptive threshold. Experimental results show that our model achieves competitive results with the state-of-the-art classification-based model OneIE on ACE 2005 and achieves the best performances on ERE. Additionally, our model is proven to be portable to new types of events effectively.},
  keywords={Task analysis;Tuning;Data mining;Adaptation models;Ontologies;Decoding;Feature extraction;Conditional generation;dense retrieval;event extraction;prompt tuning},
  doi={10.1109/TKDE.2023.3266495},
  ISSN={1558-2191},
  month={Oct},}@INPROCEEDINGS{10090855,
  author={Tang, Jin and Xu, Chengxian and Zhang, Wanda},
  booktitle={2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)}, 
  title={Construction and Accurate Retrieval Method of Knowledge Graph of Automobile Engine Fault}, 
  year={2023},
  volume={},
  number={},
  pages={336-345},
  abstract={In order to improve the efficiency and accuracy of automobile engine fault maintenance, an accurate retrieval method of automobile engine fault driven by knowledge graph was proposed. Firstly, the definition and framework of knowledge graph are discussed. The entity extraction of engine fault features was carried out by multi-source neural network, and the disambiguation of fault entities was carried out by integrating entity link technologies; Secondly, fault knowledge reasoning is carried out to eliminate the wrong knowledge in the knowledge base and infer new knowledge to form a complete knowledge graph.. On this basis, the retrieval subgraph of engine fault semantics is designed. Combined with the influence of physical distance and proximity, the retrieval result evaluation model is established, and the subgraph matching was carried out based on the similarity calculation of graph structure and semantic information. Finally, four knowledge graphs including entity equipment graph, ontology graph, maintenance rule graph and history graph were constructed by selecting some automobile engine fault cases from 2017 to 2020. Finally, the process architecture of engine fault search and analysis is constructed and the effectiveness of the proposed method was verified by precision rate and recall rata, which provides a new idea for accurate and efficient engine maintenance.},
  keywords={Knowledge engineering;Visualization;Semantic search;Decision making;Knowledge graphs;Maintenance engineering;Feature extraction;engine fault;knowledge graph;entity link;Subgraph matching;semantic search},
  doi={10.1109/EEBDA56825.2023.10090855},
  ISSN={},
  month={Feb},}@ARTICLE{10091124,
  author={Tu, Yamei and Wang, Xiaoqi and Qiu, Rui and Shen, Han-Wei and Miller, Michelle and Rao, Jinmeng and Gao, Song and Huber, Patrick R. and Hollander, Allan D. and Lange, Matthew and Garcia, Christian R. and Stubbs, Joe},
  journal={IEEE Computer Graphics and Applications}, 
  title={An Interactive Knowledge and Learning Environment in Smart Foodsheds}, 
  year={2023},
  volume={43},
  number={3},
  pages={36-47},
  abstract={The Internet of Food (IoF) is an emerging field in smart foodsheds, involving the creation of a knowledge graph (KG) about the environment, agriculture, food, diet, and health. However, the heterogeneity and size of the KG present challenges for downstream tasks, such as information retrieval and interactive exploration. To address those challenges, we propose an interactive knowledge and learning environment (IKLE) that integrates three programming and modeling languages to support multiple downstream tasks in the analysis pipeline. To make IKLE easier to use, we have developed algorithms to automate the generation of each language. In addition, we collaborated with domain experts to design and develop a dataflow visualization system, which embeds the automatic language generations into components and allows users to build their analysis pipeline by dragging and connecting components of interest. We have demonstrated the effectiveness of IKLE through three real-world case studies in smart foodsheds.},
  keywords={Knowledge graphs;Ontologies;Data visualization;Food products;Smart agriculture;Internet of Things;Data models},
  doi={10.1109/MCG.2023.3263960},
  ISSN={1558-1756},
  month={May},}@ARTICLE{10052691,
  author={Jha, Kanchan and Saha, Sriparna and Karmakar, Sourav},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
  title={Prediction of Protein-Protein Interactions Using Vision Transformer and Language Model}, 
  year={2023},
  volume={20},
  number={5},
  pages={3215-3225},
  abstract={The knowledge of protein-protein interaction (PPI) helps us to understand proteins’ functions, the causes and growth of several diseases, and can aid in designing new drugs. The majority of existing PPI research has relied mainly on sequence-based approaches. With the availability of multi-omics datasets (sequence, 3D structure) and advancements in deep learning techniques, it is feasible to develop a deep multi-modal framework that fuses the features learned from different sources of information to predict PPI. In this work, we propose a multi-modal approach utilizing protein sequence and 3D structure. To extract features from the 3D structure of proteins, we use a pre-trained vision transformer model that has been fine-tuned on the structural representation of proteins. The protein sequence is encoded into a feature vector using a pre-trained language model. The feature vectors extracted from the two modalities are fused and then fed to the neural network classifier to predict the protein interactions. To showcase the effectiveness of the proposed methodology, we conduct experiments on two popular PPI datasets, namely, the human dataset and the S. cerevisiae dataset. Our approach outperforms the existing methodologies to predict PPI, including multi-modal approaches. We also evaluate the contributions of each modality by designing uni-modal baselines. We perform experiments with three modalities as well, having gene ontology as the third modality.},
  keywords={Proteins;Feature extraction;Three-dimensional displays;Protein sequence;Amino acids;Deep learning;Transformers;Language model;protein-protein interaction;vision transformer},
  doi={10.1109/TCBB.2023.3248797},
  ISSN={1557-9964},
  month={Sep.},}@ARTICLE{10044673,
  author={Kim, Han Kyul and Park, Yujin and Park, Yeju and Choi, Eunji and Kim, Sodam and You, Hahyun and Bae, Ye Seul},
  journal={IEEE Access}, 
  title={Identifying Alcohol-Related Information From Unstructured Bilingual Clinical Notes With Multilingual Transformers}, 
  year={2023},
  volume={11},
  number={},
  pages={16066-16075},
  abstract={As a key modifiable risk factor, alcohol consumption is clinically crucial information that allows medical professionals to further understand their patients’ medical conditions and suggest appropriate lifestyle modifying interventions. However, identifying alcohol-related information from unstructured free-text clinical notes is often challenging. Not only are the formats of the notes inconsistent, but they also include a massive amount of non-alcohol-related information. Furthermore, for medical institutions outside of English-speaking countries, these clinical notes contain both a mixture of English and local languages, inducing additional difficulty in the extraction. Thanks to the increasing availability of electronic medical record (EMR), several previous works explored the idea of using natural language processing (NLP) to train machine learning models that automatically identify alcohol-related information from unstructured clinical notes. However, all these previous works are limited to English clinical notes, thereby able to leverage various large-scale external ontologies during the text preprocessing. Furthermore, they rely on simple NLP techniques such as the bag-of-words models that suffer from high dimensionality and out-of-vocabulary issues. Addressing these issues, we adopt fine-tuning multilingual transformers. By leveraging their linguistically rich contextual information learned during their pre-training, we are able to extract alcohol-related information from unstructured clinical notes without preprocessing the clinical notes on any external ontologies. Furthermore, our work is the first to explore the use of transformers in bilingual clinical notes to extract alcohol-related information. Even with minimal text preprocessing, we achieve extraction accuracy of 84.70% in terms of macro F-1 score.},
  keywords={Transformers;Data mining;Hospitals;Terminology;Symbols;Ontologies;Alcoholic beverages;Bioinformatics;Natural language processing;Informatics;Clinical diagnosis;Clinical informatics;alcohol information extraction;natural language processing;information extraction from clinical notes;multilingual transformers},
  doi={10.1109/ACCESS.2023.3245523},
  ISSN={2169-3536},
  month={},}@ARTICLE{10016758,
  author={Liu, Mingyi and Tu, Zhiying and Xu, Hanchuan and Xu, Xiaofei and Wang, Zhongjie},
  journal={IEEE Transactions on Services Computing}, 
  title={DySR: A Dynamic Graph Neural Network Based Service Bundle Recommendation Model for Mashup Creation}, 
  year={2023},
  volume={16},
  number={4},
  pages={2592-2605},
  abstract={An increasing number and diversity of services are available, which results in significant challenges to effectively reuse service during mashup creation. Many works have modeled the mashup creation problem as a service recommendation task and have achieved remarkable results. However, the performance of these methods can be further improved. The main problems affecting these methods include the constraints among recommended services, the evolution of services, and the semantic gap existing in services and mashups. In this article, we model the mashup creation problem as a service bundle recommendation task that is formally defined to address the constraints among recommended services. And then, a dynamic graph neural network based model called DySR is proposed to tackle the evolution of service and the semantic gap between services and mashups. In order to quantitatively measure how significant the semantic gap between mashups and services is, a measurement method of a semantic gap is given. With it, experiments show that to what extent DySR can reduce the semantic gap in the context of mashup creation. In addition, new evaluation metrics are introduced to overcome the preference for popular services in traditional service recommendations. Extensive experiments conducted on a real-world dataset from ProgrammableWeb, and the experiment results show that DySR outperforms existing state-of-the-art methods.},
  keywords={Mashups;Semantics;Task analysis;Graph neural networks;Computational modeling;Quality of service;Ontologies;Dynamic graph neural networks;evolving service;mashup creation;semantic gap;service bundle recommendation},
  doi={10.1109/TSC.2023.3234293},
  ISSN={1939-1374},
  month={July},}@ARTICLE{10013735,
  author={Benarab, Achref and Sun, Jianguo and Rafique, Fahad and Refoufi, Allaoua},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Global Ontology Entities Embeddings}, 
  year={2023},
  volume={35},
  number={11},
  pages={11449-11460},
  abstract={Ontologies are among the most widely used types of knowledge representation formalisms. The application of deep learning techniques in the field of ontology engineering has reinforced the need to learn and generate representations of ontological data. This allows ontologies to be exploited by such models, and thus automate various ontology engineering tasks, where most of the existing tools and machine learning approaches require a numerical feature vectors associated with each concept. This paper outlines a novel approach for learning global ontology entities embeddings by exploiting the structure and the various taxonomic and semantic relationships present in ontologies, taking into account all the information present in the ontological graph and carried by the OWL/RDF triples. Thus, producing global ontology entities embeddings capturing the global ontological graph semantics and similarities enclosed in the source ontology. Three different neural network models have been proposed based on two architectures: multi-input and multi-output, trained using the contrastive estimation technique. The evaluation on OWL/RDF ontologies and word semantic similarity tasks using various graph and WordNet based similarity measures, show that our approach yields competitive results outperforming the state-of-the-art ontology and word embedding models.},
  keywords={Ontologies;Semantics;Task analysis;Adaptation models;Predictive models;Neural networks;Deep learning;Concept embeddings;feature representation;neural networks;ontology embeddings;ontology entities vector representations},
  doi={10.1109/TKDE.2023.3235779},
  ISSN={1558-2191},
  month={Nov},}@ARTICLE{9961919,
  author={Sun, Guangzhi and Zhang, Chao and Woodland, Philip C.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Minimising Biasing Word Errors for Contextual ASR With the Tree-Constrained Pointer Generator}, 
  year={2023},
  volume={31},
  number={},
  pages={345-354},
  abstract={Contextual knowledge is essential for reducing speech recognition errors on high-valued long-tail words. This paper proposes a novel tree-constrained pointer generator (TCPGen) component that enables end-to-end ASR models to bias towards a list of long-tail words obtained using external contextual information. With only a small overhead in memory use and computation cost, TCPGen can structure thousands of biasing words efficiently into a symbolic prefix-tree, and creates a neural shortcut between the tree and the final ASR output to facilitate the recognition of the biasing words. To enhance TCPGen, we further propose a novel minimum biasing word error (MBWE) loss that directly optimises biasing word errors during training, along with a biasing-word-driven language model discounting (BLMD) method during the test. All contextual ASR systems were evaluated on the public Librispeech audiobook corpus and the data from the dialogue state tracking challenges (DSTC) with the biasing lists extracted from the dialogue-system ontology. Consistent word error rate (WER) reductions were achieved with TCPGen, which were particularly significant on the biasing words with around 40% relative reductions in the recognition error rates. MBWE and BLMD further improved the effectiveness of TCPGen, and achieved more significant WER reductions on the biasing words. TCPGen also achieved zero-shot learning of words not in the audio training set with large WER reductions on the out-of-vocabulary words in the biasing list.},
  keywords={Training;Generators;Decoding;Hidden Markov models;Context modeling;Speech recognition;Error analysis;Contextual speech recognition;end-to-end;language model discounting;minimum Bayes' risk;Pointer generator},
  doi={10.1109/TASLP.2022.3224286},
  ISSN={2329-9304},
  month={},}@ARTICLE{9950327,
  author={Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Improving Protein Function Prediction by Adaptively Fusing Information From Protein Sequences and Biomedical Literature}, 
  year={2023},
  volume={27},
  number={2},
  pages={1140-1148},
  abstract={Proteins are the main undertakers of life activities, and accurately predicting their biological functions can help human better understand life mechanism and promote the development of themselves. With the rapid development of high-throughput technologies, an abundance of proteins are discovered. However, the gap between proteins and function annotations is still huge. To accelerate the process of protein function prediction, some computational methods taking advantage of multiple data have been proposed. Among these methods, the deep-learning-based methods are currently the most popular for their capability of learning information automatically from raw data. However, due to the diversity and scale difference between data, it is challenging for existing deep learning methods to capture related information from different data effectively. In this paper, we introduce a deep learning method that can adaptively learn information from protein sequences and biomedical literature, namely DeepAF. DeepAF first extracts the two kinds of information by using different extractors, which are built based on pre-trained language models and can capture rudimentary biological knowledge. Then, to integrate those information, it performs an adaptive fusion layer based on a Cross-attention mechanism that considers the knowledge of mutual interactions between two information. Finally, based on the mixed information, DeepAF utilizes logistic regression to obtain prediction scores. The experimental results on the datasets of two species (i.e., Human and Yeast) show that DeepAF outperforms other state-of-the-art approaches.},
  keywords={Proteins;Protein engineering;Data mining;Biological system modeling;Amino acids;Semantics;Predictive models;Protein function prediction;deep learning;multiple data;pre-trained language models;cross-attention mechanism},
  doi={10.1109/JBHI.2022.3221988},
  ISSN={2168-2208},
  month={Feb},}@ARTICLE{9865204,
  author={Sun, Zequn and Hu, Wei and Wang, Chengming and Wang, Yuxin and Qu, Yuzhong},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Revisiting Embedding-Based Entity Alignment: A Robust and Adaptive Method}, 
  year={2023},
  volume={35},
  number={8},
  pages={8461-8475},
  abstract={Entity alignment—the discovery of identical entities across different knowledge graphs (KGs)—is a critical task in data fusion. In this paper, we revisit existing entity alignment methods in practical and challenging scenarios. Our empirical studies show that current work has a low level of robustness to long-tail entities and the lack of entity names or relation triples. We aim to develop a robust and adaptive entity alignment method, and the availability of relations, attributes, or names is not required. Our method consists of an attribute encoder and a relation encoder, representing an entity by aggregating its attributes or relational neighbors using the attention mechanisms that can highlight the useful attributes and relations in end-to-end learning. To let the encoders complement each other and produce a coherent representation space, we propose adaptive embedding fusion via a gating mechanism. We consider four evaluation settings, i.e., the conventional setting with both relation and attribute triples, as well as three challenging settings without attributes, without relations, without both relations and names, respectively. Results show that our method can achieve state-of-the-art performance. Even in the most challenging setting without relations and names, our method can still achieve promising results while existing methods fail.},
  keywords={Robustness;Task analysis;Logic gates;Sun;Ontologies;Manuals;Convolution;Knowledge graph embedding;entity alignment;adaptive embedding fusion},
  doi={10.1109/TKDE.2022.3200981},
  ISSN={1558-2191},
  month={Aug},}@ARTICLE{9792280,
  author={Guan, Saiping and Cheng, Xueqi and Bai, Long and Zhang, Fujun and Li, Zixuan and Zeng, Yutao and Jin, Xiaolong and Guo, Jiafeng},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={What is Event Knowledge Graph: A Survey}, 
  year={2023},
  volume={35},
  number={7},
  pages={7569-7589},
  abstract={Besides entity-centric knowledge, usually organized as Knowledge Graph (KG), events are also an essential kind of knowledge in the world, which trigger the spring up of event-centric knowledge representation form like Event KG (EKG). It plays an increasingly important role in many downstream applications, such as search, question-answering, recommendation, financial quantitative investments, and text generation. This paper provides a comprehensive survey of EKG from history, ontology, instance, and application views. Specifically, to characterize EKG thoroughly, we focus on its history, definition, schema induction, acquisition, related representative graphs/systems, and applications. The development processes and trends are studied therein. We further summarize prospective directions to facilitate future research on EKG.},
  keywords={Electrocardiography;History;Ontologies;Task analysis;Semantics;Internet;Standards;Event knowledge graph;schema;event acquisition;script event prediction;temporal knowledge graph prediction},
  doi={10.1109/TKDE.2022.3180362},
  ISSN={1558-2191},
  month={July},}@INBOOK{10787484,
  author={Renes, Joseph},
  booktitle={Quantum Information Theory: Concepts and Methods}, 
  title={2 Probability theory}, 
  year={2022},
  volume={},
  number={},
  pages={17-35},
  abstract={},
  keywords={Ontologies;Context modeling;OWL;Object oriented modeling;Context-aware services;Semantics;Resource description framework;Logic;Computational modeling;Cognition},
  doi={},
  ISSN={},
  publisher={De Gruyter},
  isbn={9783110570328},
  url={https://ieeexplore.ieee.org/document/10787484},}@INBOOK{10514903,
  author={Weilkiens, Tim and Lamm, Jesko G. and Roth, Stephan and Walker, Markus},
  booktitle={Model-Based System Architecture}, 
  title={Model&#x2010;Based Requirements Engineering and Use Case Analysis}, 
  year={2022},
  volume={},
  number={},
  pages={99-118},
  abstract={Summary <p>In this chapter, the authors give a brief description of requirements engineering and use case analysis. They define the most important requirements and use case terms. The authors present the requirements and use case analysis methodology steps of the &#x201c;Systems Modeling Toolbox&#x201d; (SYSMOD). The SYSMOD methodology defines common methods and is not specific to any modeling tool. The authors cover the Storyboard Activity Modeling for Systems (SAMS) method that provides an illustrative approach to identify use case activities with stakeholders. The SAMS method is intended to integrate the use of storyboards as a means of use case analysis into model&#x2010;based system development. The authors also present the Use Case 2.0 approach that works well with agile frameworks. The Use Case 2.0 concept introduces the use case slice that represents only one or some ways of the possible use case performances that can be handled as a unit from specification to implementation within an agile approach.</p>},
  keywords={Adaptation models;Stakeholders;Computer architecture;Systems architecture;Ontologies;Business;Analytical models},
  doi={10.1002/9781119746683.ch10},
  ISSN={},
  publisher={Wiley},
  isbn={9781119746669},
  url={https://ieeexplore.ieee.org/document/10514903},}@INPROCEEDINGS{10148526,
  author={Zhang, Xiang and Yu, Bruce X.B. and Liu, Yan and Chen, Gong and Ng, George Wing-Yiu and Chia, Nam-Hung and So, Eric Hang-Kwong and So, Sze-Sze and Cheung, Victor Kai-Lam},
  booktitle={2022 IEEE International Conference on Teaching, Assessment and Learning for Engineering (TALE)}, 
  title={Conversational System for Clinical Communication Training Supporting User-defined Tasks}, 
  year={2022},
  volume={},
  number={},
  pages={396-403},
  abstract={Effective clinical communication is essential for delivering safe and high-quality patient care, especially in emergent cases. Standard communication protocols have been developed to improve communication accuracy and efficiency. However, traditional training and evaluation require substantial manpower and time, which can be infeasible during public crises when training is most needed. This research aims to facilitate autonomous, low-cost, adaptive clinical communication training via artificial intelligence (AI)-powered techniques. We propose a conversational system for clinical communication training supporting user-defined tasks. Two data augmentation (DA) methods, term replacement and context expansion, are proposed to allow non-professional users to create Al models with a small number of samples. Equipped with biomedical ontology and pre-trained language models, our system is able to simulate clinical communication scenarios, provide timely evaluation, and adapt to new tasks with minimal editing. Various experiments demonstrate that our proposed algorithms can achieve satisfactory performance using a small amount of training data. Real-world practice in local hospitals shows that our system can provide expert-level evaluation and deliver effective clinical communication training.},
  keywords={Training;Adaptation models;Protocols;Biological system modeling;Training data;Ontologies;Data models;clinical communication;human-computer interaction;autonomous communication training;conversational system},
  doi={10.1109/TALE54877.2022.00071},
  ISSN={2470-6698},
  month={Dec},}@INPROCEEDINGS{10074709,
  author={Chen, Luming and Qi, Yifan and Wu, Aiping and Deng, Lizong and Jiang, Taijiao},
  booktitle={2022 IEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)}, 
  title={Enhancing Cross-lingual Medical Concept Alignment by Leveraging Synonyms and Translations of the Unified Medical Language System}, 
  year={2022},
  volume={},
  number={},
  pages={2078-2083},
  abstract={Well-developed medical terminology systems like the Unified Medical Language System (UMLS) improve the ability of language models to handle medical entity linking tasks. However, such magnificent terminology systems are only available for few languages, such as English. For Chinese, both simplified and traditional, the lack of well-developed terminology systems remains a big challenge to unify Chinese medical terminologies by linking medical entities as concepts. In this study, we purpose a translation enhanced contrastive learning scheme which leverages translations and synonyms of UMLS to infuse knowledge into the language model, and present a cross-lingual pre-trained language model called TeaBERT that aligns cross-lingual Chinese and English medical synonyms well at semantic level. Comparing with former cross-lingual language models, TeaBERT significantly outperforms on evaluation datasets, with 93.21%, 89.89% and 76.45% of Top 5 accuracy on ICDI0-CN, CHPO and RealWorld dataset respectively, and achieves new state-of-theart performance without task specific fine-tuning. Our contrastive learning scheme can not only be used for enhancing Chinese-English medical concepts alignment, but also be applied to other languages facing the same challenges.},
  keywords={Terminology;Biological system modeling;Unified modeling language;Semantics;Knowledge representation;Task analysis;NLP;pre-trained language model;cross-lingual medical entity linking;UMLS},
  doi={10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00309},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10076731,
  author={Taru, Uma and Patil, Archana},
  booktitle={2022 International Conference on Machine Learning, Computer Systems and Security (MLCSS)}, 
  title={Building Ontology for Toxic words}, 
  year={2022},
  volume={},
  number={},
  pages={241-246},
  abstract={Many online social media platforms have particular community guidelines for comment sections. The platforms that maintain commentary sections in various posts, videos, and blogs need to adhere to these guidelines. These comment sections may have specific comments that fail to satisfy the rules and regulations to maintain societal norms of communication. These comments are classified as toxic comments. Google's Perspective API defines toxic comments as comments that are rude, offensive, and likely to make someone leave the conversation. In this paper, we have built a toxic words ontology, which is as per our knowledge, first Ontology built on toxic words. This Ontology consists of toxic words and their antonyms and synonyms in increasing order of their toxicity levels. Traversing this ontology, we can find the best-suited word with less toxicity and similar meaning. This is a dynamic ontology and new words can be added easily. Thus letting us convey messages in a civil manner. We propose to reduce toxicity in the most straightforward way. After studying several papers, we found out that the toxicity mainly occurs because of use of toxic words. We also observed that use of less toxic synonyms or no toxic synonyms has huge effects on toxicity score given by the Perspective API, and results section proves that.},
  keywords={Toxicology;Social networking (online);Oral communication;Machine learning;Ontologies;Regulation;Internet;toxicity;ontology;similarity;antonyms;synonyms},
  doi={10.1109/MLCSS57186.2022.00052},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10043275,
  author={Mendil, Ismail and Rivière, Peter and Ait-Ameur, Yamine and Singh, Neeraj Kumar and Méry, Dominique and Palanque, Philippe},
  booktitle={2022 29th Asia-Pacific Software Engineering Conference (APSEC)}, 
  title={Non-Intrusive Annotation-Based Domain-Specific Analysis to Certify Event-B Models Behaviours}, 
  year={2022},
  volume={},
  number={},
  pages={129-138},
  abstract={System engineering advocates a thorough under-standing of the engineering domain or certification standards (aeronautics, railway, medical, etc.) associated to the system under design. In this context, engineering domain knowledge plays a predominant role in system design and/or certification. Furthermore, it is a prerequisite to achieve the effectiveness and performance of the designed system. This article proposes a formal method for describing and setting up domain-specific behavioural analyses. It defines a formal verification technique for dynamic properties entailed by engineering domain knowledge where Event-B formal models are annotated and analysed in a non-intrusive way, i.e. without destructive alteration. This method is based on the formalisation of behavioural properties analyses relying on domain knowledge as an ontology on the one hand and a meta-theory for Event-B on the other hand. The proposed method is illustrated using a critical interactive system.},
  keywords={Knowledge engineering;Analytical models;Interactive systems;Ontologies;Rail transportation;Proposals;Certification;Domain knowledge;formal methods;Event-B;refinement;proof;ontology;behavioural analyses},
  doi={10.1109/APSEC57359.2022.00025},
  ISSN={2640-0715},
  month={Dec},}@INPROCEEDINGS{10020417,
  author={Mijalcheva, Viktorija and Davcheva, Ana and Gramatikov, Sasho and Jovanovik, Milos and Trajanov, Dimitar and Stojanov, Riste},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Learning Robust Food Ontology Alignment}, 
  year={2022},
  volume={},
  number={},
  pages={4097-4104},
  abstract={In today’s knowledge society, large number of information systems use many different individual schemes to represent data. Ontologies are a promising approach for formal knowledge representation and their number is growing rapidly. The semantic linking of these ontologies is a necessary prerequisite for establishing interoperability between the large number of services that structure the data with these ontologies. Consequently, the alignment of ontologies becomes a central issue when building a worldwide Semantic Web. There is a need to develop automatic or at least semi-automatic techniques to reduce the burden of manually creating and maintaining alignments. Ontologies are seen as a solution to data heterogeneity on the Web. However, the available ontologies are themselves a source of heterogeneity. On the Web, there are multiple ontologies that refer to the same domain, and with that comes the challenge of a given graph-based system using multiple ontologies whose taxonomy is different, but the semantics are the same. This can be overcome by aligning the ontologies or by finding the correspondence between their components.In this paper, we propose a method for indexing ontologies as a support to a solution for ontology alignment based on a neural network. In this process, for each semantic resource we combine the graph based representations from the RDF2vec model, together with the text representation from the BERT model in order to capture the semantic and structural features. This methodology is evaluated using the FoodOn and OntoFood ontologies, based on the Food Onto Map alignment dataset, which contains 155 unique and validly aligned resources. Using these limited resources, we managed to obtain accuracy of 74% and F1 score of 75% on the test set, which is a promising result that can be further improved in future. Furthermore, the methodology presented in this paper is both robust and ontology-agnostic. It can be applied to any ontology, regardless of the domain.},
  keywords={Training;Semantic Web;Semantics;Neural networks;Taxonomy;Ontologies;Big Data;Ontology Alignment;Natural language processing;Text representation;Embeddings;Data normalization;Data linking},
  doi={10.1109/BigData55660.2022.10020417},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10020882,
  author={Hsiao, Yi-Hao and Chuang, Chia-Yi and Huang, Megn-Chi and Yang, Chia-Lee and Wu, Jyh-Horng},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Using Contextual Text Mining and Ontology Methods to Establish a Novel Technology Trend and Associative Analysis Framework for Sustainable Energy Development in Taiwan}, 
  year={2022},
  volume={},
  number={},
  pages={4491-4494},
  abstract={In 2015, the United Nations proposed 17 Sustainable Development Goals, SDGs, as the guidelines for all countries in the world to promote sustainable development before 2030. Government Research Bulletin (GRB), the research projects and technical reports sponsored by government, which has long-term, numerous, complete research method, technology development and policy analysis information in Taiwan. Therefore, it is an important and effective way to explore SDG-related information from a large amount of GRB text. In this paper, a novel technologies trend and associative analysis framework which uses contextual text mining and ontology methods is proposed and applied to SDG 7, which "Affordable and Clean Energy". First, we integrate dictionary-based method and semantic textual similarity analysis algorithm to obtain a SDG 7 classifier which can exactly and quickly classify a large amount number of GRB text to SDG 7. Then, two major SDG 7 analysis procedures based on the classification results are implemented. One is using contextual text mining algorithm to obtain energy technologies trend information. The other is adopting ontology method to establish energy technologies associative analysis concept map. According to the analysis results mentioned above, we are able to efficiently incorporate the energy technology with long-term trend, energy technology associative information, and the most influential authors on the specify energy technology in order to generate a global strategy for continuous improvement in Taiwan.},
  keywords={Text mining;Government;Semantics;Ontologies;Big Data;Writing;Market research;Contextual text mining;Ontology;SBERTs;Sustainable Development Goals (SDGs)},
  doi={10.1109/BigData55660.2022.10020882},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10020339,
  author={Wullschleger, Pascal and Lionetti, Simone and Daly, Donnacha and Volpe, Francesca and Caro, Grégoire},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Auto-Regressive Self-Attention Models for Diagnosis Prediction on Electronic Health Records}, 
  year={2022},
  volume={},
  number={},
  pages={1950-1956},
  abstract={Insufficient data and data lacking the diversity to represent the general public is a common challenge when modelling diagnosis prediction. We consider a much larger and more diverse database of commercial Electronic Health Records than what is prevalent in the literature. We formulate a simplified version of diagnosis prediction that focuses on major developments in medical histories of patients. To this end, we leverage Auto-Regressive Self-Attention models that have seen promising applications in language modelling and extend them to incorporate ontological representations of medical codes. Additionally, we include time-intervals between diagnoses into the attention calculation. We evaluate models and baselines at different levels of diagnostic granularity and our results suggest that using very detailed clinical classifications does not significantly degrade performance, possibly allowing their use in practice. Our model outperforms all baselines and we suggest that leveraging the ontology for generating diagnosis representations is mostly helpful for rare diagnoses.},
  keywords={Codes;Databases;Predictive models;Big Data;Ontologies;Data models;History;Electronic Health Record;Transformer;Ontological Representation;Diagnosis Prediction},
  doi={10.1109/BigData55660.2022.10020339},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10020513,
  author={Chen, Xianlai and Lin, Jiamiao and An, Ying},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={DL-BERT: a time-aware double-level BERT-style model with pre-training for disease prediction}, 
  year={2022},
  volume={},
  number={},
  pages={1801-1808},
  abstract={Disease prediction based on the Electronic Health Record (EHR) is an important task in healthcare. EHR records patients’ every visit by time, and there are many kinds of medical codes within a visit, therefore, EHR has characteristics of temporal irregularity and hierarchical structure. Some recent works employ BERT-style models to process EHR data for disease prediction. However, few of these models can give consideration to capture both the interaction between medical codes and the impact of temporal irregularity. To solve this problem, we propose the Double-Level BERT-style model (DL-BERT). Considering EHR’s hierarchical structure, the model contains a code-level and a visit-level representation layer which can learn the relationship between medical codes and temporal influence respectively. In the code-level representation layer, the model achieves the representation power by employing external medical ontologies to provide multi-resolution information of medical codes and the Transformer to embed medical codes. Besides, the model adopts two pre-training tasks to enhance the ability to capture the link between different kinds of codes. In the visit-level representation layer, DL-BERT utilizes a special time-aware Transformer to model temporal information. And the model adopts a visit-level pre-training task for better learning context information. Experiments are conducted on two real-world healthcare datasets and show that our model outperforms all baselines demonstrating the effectiveness of our model.},
  keywords={Codes;Medical services;Ontologies;Predictive models;Big Data;Transformers;Data models;Electronic Health Record;BERT;Transformer;medical ontology;pre-train},
  doi={10.1109/BigData55660.2022.10020513},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10020509,
  author={Parolin, Erick Skorupa and Hu, Yibo and Khan, Latifur and Brandt, Patrick T. and Osorio, Javier and D’Orazio, Vito},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Confli-T5: An AutoPrompt Pipeline for Conflict Related Text Augmentation}, 
  year={2022},
  volume={},
  number={},
  pages={1906-1913},
  abstract={Recent advances in natural language processing (NLP) and Big Data technologies have been crucial for scientists to analyze political unrest and violence, prevent harm, and promote global conflict management. Government agencies and public security organizations have invested heavily in deep learning-based applications to study global conflicts and political violence. However, such applications involving text classification, information extraction, and other NLP-related tasks require extensive human efforts in annotating/labeling texts. While limited labeled data may drastically hurt the models’ performance (over-fitting), large demands on annotation tasks may turn real-world applications impracticable. To address this problem, we propose Confli-T5, a prompt-based method that leverages the domain knowledge from existing political science ontology to generate synthetic but realistic labeled text samples in the conflict and mediation domain. Our model allows generating textual data from the ground up and employs our novel Double Random Sampling mechanism to improve the quality (coherency and consistency) of the generated samples. We conduct experiments over six standard datasets relevant to political science studies to show the superiority of Confli-T5. Our codes are publicly available 1.},
  keywords={Text categorization;Standards organizations;Pipelines;Big Data;Natural language processing;Data models;Safety;text augmentation;generation;classification;natural language processing;conflict;coding event data;CAMEO},
  doi={10.1109/BigData55660.2022.10020509},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10013619,
  author={Kulagin, Grigory and Ermakov, Ivan and Lyadova, Lyudmila},
  booktitle={2022 IEEE 16th International Conference on Application of Information and Communication Technologies (AICT)}, 
  title={Ontology-Based Development of Domain-Specific Languages via Customizing Base Language}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={The quality of the systems depends on compliance to the domain requirements. High quality is achieved only with involving experts in the relevant fields to the system design as experts. Modern design methods are based on using professional tools and modeling languages. Using these tools are difficult for domain experts. Domain-Specific Languages (DSLs) can be considered as "user interfaces" for experts because they bridge the gap between the domain experts and the software development tools via customizing modeling languages. Usability of DSLs by domain experts is a key factor for their successful adoption. But DSL creation is challenging task. An approach to DSL customization based on using multifaceted ontology is proposed. General scheme of DSL metamodel generation based on multifaceted ontology is described. Examples of created DSLs and models illustrating the applicability of the proposed method are shown. The DSL metamodels were developed and tested in several domains. The results of experiments confirmed practical significance of the ontology-based approach to DSL creation.},
  keywords={Visualization;Prototypes;Ontologies;User interfaces;Software;DSL;Task analysis;domain-specific modeling;DSM;domain-specific language;DSL;metamodel generation;multifaceted ontology;language customization;GalileoSky;algorithm description language},
  doi={10.1109/AICT55583.2022.10013619},
  ISSN={2472-8586},
  month={Oct},}@INPROCEEDINGS{10005324,
  author={Zindel, Andreas and Feo-Arenis, Sergio and Helle, Philipp and Schramm, Gerrit and Elaasar, Maged},
  booktitle={2022 IEEE International Symposium on Systems Engineering (ISSE)}, 
  title={Building a Semantic Layer for Early Design Trade Studies in the Development of Commercial Aircraft}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={To improve the adoption of Model-based Systems Engineering (MBSE), data that is distributed across engineering disciplines needs to be made available in an open and descriptive way. This paper describes a new approach to implementing a semantic layer that allows integrating and publishing MBSE data stored in heterogeneous models in a uniform way by means of Semantic Web technologies. The tool-independent views on engineering data provided by the semantic layer enable the implementation of services for accessing, classifying, checking and reuse of federated information. We report on the creation of a common vocabulary in the Ontology Modeling Language (OML) that can be automatically instantiated from distributed models into a knowledge graph. We demonstrate the benefits of our approach using a Systems Modeling Language (SysML) based early design trade study in the aeronautics domain.},
  keywords={Semantic Web;Vocabulary;Atmospheric modeling;Unified modeling language;Semantics;Ontologies;Data models},
  doi={10.1109/ISSE54508.2022.10005324},
  ISSN={2687-8828},
  month={Oct},}@INPROCEEDINGS{9994917,
  author={Choi, Kyudam and Lee, Yurim and Kim, Cheongwon},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={GCL-GO: A novel sequence-based hierarchy-aware method for protein function prediction}, 
  year={2022},
  volume={},
  number={},
  pages={51-56},
  abstract={Experimental protein functional annotation does not cover rapidly-expanding protein sequences. Sequence-based methods, one of the computational methods, have been developed for extending functional annotations to fast-growing sequence databases. We propose a novel sequence-based hierarchy-aware method, namely GCL-GO. GCL-GO applies a protein language model to represent sequences, applies graph contrastive learning to represent GO terms, and then predicts protein functions by combining these two features. By contrasting the GO graph and semantic features of GO terms, GCL-GO has generalizability and scalability by accurately embedding the features of GO terms while relying less on training data. We also suggest GCL-GO+, which combines a sequence similarity-based method with GCLGO, to improve performance. GCL-GO+ outperforms sequence-based competing methods on both the CAFA3 and the TALE datasets. Furthermore, GCL-GO and GCL-GO+ demonstrate functional generalization and scalability potential by having the best performance on new GO terms or on GO terms annotated infrequently in the training dataset. Our code is available in https://github.com/kch38896/GCL-GO},
  keywords={Proteins;Training;Protein engineering;Annotations;Databases;Scalability;Semantics;protein function prediction;gene ontology;graph constructive learning;protein language model},
  doi={10.1109/BIBM55620.2022.9994917},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9995651,
  author={Wang, Hong and Wang, Xiaoqi and Liu, Wenjuan and Xie, Xiaolan and Peng, Shaoliang},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={deepDGA: Biomedical Heterogeneous Network-based Deep Learning Framework for Disease-Gene Association Predictions}, 
  year={2022},
  volume={},
  number={},
  pages={601-606},
  abstract={Accurate prediction of disease-gene associations is a crucial tissue in the treatment of diseases. Currently, deep learning-based methods have been proposed to determine the associations between diseases and genes. However, previous network-based models do not consider the semantic characteristics of various biomedical entities and suffer from the problems of cold-start. To this end, this study proposes a heterogeneous network-based deep learning framework (termed deepDGA) to predict disease-gene associations. First, a heterogeneous network with four kinds of biological nodes and eight kinds of edges is constructed. Second, we develop a meta path-driven deep Transformer encoder to learn node representations which contains semantic characteristics of nodes in the heterogeneous network. Finally, the inductive matrix completion algorithm that can solve problem of cold-start, is used for disease-gene association prediction. The results of 5-flod cross-validation and top-ranked predictions suggest that deepDGA is superior to other methods. In addition, we further observe that deepDGA performs the highest predictive ability for specific diseases via the literature verification, KEGG human pathway analyses, and GO enrichment analyses. In summary, deepDGA is an effective framework for predicting the diseases-gene associations.},
  keywords={Deep learning;Biological system modeling;Semantics;Pipelines;Predictive models;Transformers;Prediction algorithms;disease-gene association;heterogeneous network;deep Transformer encoder;inductive matrix completion},
  doi={10.1109/BIBM55620.2022.9995651},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9995517,
  author={Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Adaptive Multi-view Graph Convolutional Network for Gene Ontology Annotations of Proteins}, 
  year={2022},
  volume={},
  number={},
  pages={90-93},
  abstract={Gene Ontology (GO) containing a set of standard concepts (or terms) is launched to unify the functional descriptions of proteins. Developing computational models based on GO to automatically annotate protein functions has been a longstanding active research area. In this paper, we propose a novel method to adaptively fuse functional and topological information between GO Terms. Our method is composed of a pre-trained language model for encoding protein sequences and an adaptive multi-view graph convolutional network (Multi-view GCN) for representing GO terms. Particularly, the Multi-view GCN considers multiple views from functional information, topological structures, and their combinations, and extracts multiple corresponding representations of GO terms. Then, an attention mechanism is applied to adaptively learn the importance weights of these representations. Finally, the predicted scores are calculated by using a dot product between protein sequence features and GO term representations. Experimental results on the datasets of two species (i.e., Human and Yeast) show that our method outperforms other state-of-the-art methods. The code of our proposed method is available at: https://github.com/Candyperfect/Master.},
  keywords={Convolutional codes;Adaptation models;Adaptive systems;Computational modeling;RNA;Ontologies;Feature extraction;gene ontology terms;protein function prediction;deep learning;adaptive multi-view graph convolutional network},
  doi={10.1109/BIBM55620.2022.9995517},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9994899,
  author={Huang, Zhijian and Zheng, Rongtao and Deng, Lei},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={DeepFusionGO: Protein function prediction by fusing heterogeneous features through deep learning}, 
  year={2022},
  volume={},
  number={},
  pages={12-17},
  abstract={Exploring the functions of proteins is crucial for explaining cellular mechanisms, treating diseases, and developing new drugs. Due to experimental limitations, large-scale identification of protein function remains a challenging task in cell biology. Here we propose DeepFusionGo, a novel protein function prediction method that adopts a graph representation learning approach (GraphSAGE) to extract features from heterogeneous data sources. First, we generate embeddings from protein sequences using the pre-trained protein language model and InterPro domains with scaling gradient. Then we integrate these two embeddings with adaptive feature weights to the PPI graph and use GraphSAGE to generate the representation vector. Finally, we build the classification model to predict protein function based on the concatenated feature vector. The experimental results show that DeepFusionGO outperforms existing state-of-the-art methods, including sequence-based DeepGOPLUS, and PPI-based DeepGraphGO. DeepFusionGO also performs well in difficult protein function prediction. We demonstrate that selecting an appropriate protein features fusion method can improve the prediction performance, and using the PPI network and the protein representation vector obtained from the protein language model through the GraphSAGE algorithm is an effective way to mine potential functional clues. The source code and data sets are available at: https://github.com/Hhhzj-7/DeepFusionGO.},
  keywords={Proteins;Representation learning;Adaptation models;Biological system modeling;Source coding;Soft sensors;Predictive models;Protein function prediction;graph representation learning;GraphSAGE;feature fusion},
  doi={10.1109/BIBM55620.2022.9994899},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9999076,
  author={Japa, Sai Sharath and Green, Sarah},
  booktitle={2022 IEEE Eighth International Conference on Multimedia Big Data (BigMM)}, 
  title={Question Answering over Knowledge Base with Variational Auto-Encoder}, 
  year={2022},
  volume={},
  number={},
  pages={29-36},
  abstract={Knowledge Base is a semantic data repos-itory made available over the web. Knowledge bases are multi-relational graphs consisting of entities and relations representing facts about the world. These facts follow a controlled vocabulary that drives the knowledge base, often called ontology. Entities are the graph nodes, while relations are the edges connecting the nodes. While knowledge base attention prolifer-ates, extracting information from these resources is challenging. One of the promising approaches is Question Answering systems. The Question Answering over Knowledge Base(KB-QA) system aims to provide a natural language interface to pose a query to the KB. QA systems use embeddings to map the posed Question to the facts stored in the knowledge bases. QA systems can be effectively improved using large transformer models. These transformer models are computationally expensive, which prevents their use in real-world applications. To solve this, we started our study with the use-case of Knowledge Base Question Answering systems. In KB-QA systems, representing the entities and relationships play a pivotal role in extracting the answer to the posed query. Traditional KB embedding techniques represent entities/relations as vectors, mapping them in different semantic spaces and ignoring the subtle cor-relation. A pre-training language model for encoding the KB facts will preserve the semantic and contextual correlation. While learning linguistic knowledge, these models also store relational knowledge in the training data. These Language Models are often trained on a massive corpus addressing the Out-Of- Vocabulary problem. In this paper, we incorporate a co-embedding model for knowledge base embedding, which learns low-dimensional representations of entities and relations in the same semantic space. We propose a Variational Auto-Encoder, an efficient transformer architecture that represents knowledge base representations as Gaussian distributions to address the issue of neglecting uncertainty. In addition, our method has high quality and interpretability advantages compared with previous methods. Our experimental results show the effectiveness and the superiority of the VAE approach for question-answering systems on knowledge bases over other well-known pre-trained embedding methods.},
  keywords={Vocabulary;Uncertainty;Computational modeling;Knowledge based systems;Semantics;Training data;Transformers;knowledge base question answering;Bert;Language Model;KBQA;Multi-Head Attention;VAE;Encoder;Transformers},
  doi={10.1109/BigMM55396.2022.00012},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9987258,
  author={Chen, Jun},
  booktitle={2022 Sixth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)}, 
  title={Selection Method of Fuzzy Semantics in Machine Translation and the Integration of LBP Algorithm}, 
  year={2022},
  volume={},
  number={},
  pages={963-966},
  abstract={This paper studies the accuracy and rationality of machine English translation based on the LBP algorithm, and proposes a machine English translation method based on the selection of the optimal solution of fuzzy semantics. Construct an information extraction model for machine English translation, establish a fuzzy semantic topic word attribute table for machine English translation, and use phrases as the basic granularity to produce paraphrase results that are semantically consistent with the translation hypothesis set. Extract phrase paraphrase resources by using massively parallel corpus. Experimental test results show that using this method for machine English translation improves the recall performance of semantic information by 6.7%, and the feature matching degree of topic words is higher.},
  keywords={Analytical models;Semantics;Coherence;Ontologies;Information retrieval;Feature extraction;Machine translation;Fuzzy Semantics;Machine Translation;LBP Algorithm;Selection Method},
  doi={10.1109/I-SMAC55078.2022.9987258},
  ISSN={2768-0673},
  month={Nov},}@INPROCEEDINGS{9980157,
  author={Vasantharajan, Charangan and Tun, Kyaw Zin and Thi-Nga, Ho and Jain, Sparsh and Rong, Tong and Siong, Chng Eng},
  booktitle={2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)}, 
  title={MedBERT: A Pre-trained Language Model for Biomedical Named Entity Recognition}, 
  year={2022},
  volume={},
  number={},
  pages={1482-1488},
  abstract={This paper introduces MedBERT, a new pre-trained transformer-based model for biomedical named entity recognition. MedBERT is trained with 57.46M tokens collected from biomedical-related data sources, i.e. datasets acquired from N2C2, BioNLP, CRAFT challenges, and biomedical-related articles crawled from Wikipedia. We validate the effectiveness of MedBERT by comparing it with four publicly available pre-trained models on ten biomedical datasets from BioNLP and CRAFT shared tasks. Our experimental results show that models fine-tuned on MedBERT achieve state-of-the-art performance in nine datasets that predict Protein, Gene, Chemical, Cellular/Component, Gene Ontology, and Taxonomy entities. Specifically, the model achieved an average of 84.04% F1-micro score on ten test sets from BioNLP and CRAFT challenges with an improvement of 3.7% and 7.83% as compared to models that were fine-tuned on BioBERT and Bio_ClinicalBERT, respectively.},
  keywords={Proteins;Protein engineering;Biological system modeling;Taxonomy;Predictive models;Ontologies;Transformers},
  doi={10.23919/APSIPAASC55919.2022.9980157},
  ISSN={2640-0103},
  month={Nov},}@INPROCEEDINGS{9973695,
  author={Filgueira, Rosa},
  booktitle={2022 IEEE 18th International Conference on e-Science (e-Science)}, 
  title={frances: A Deep Learning NLP and Text Mining Web Tool to Unlock Historical Digital Collections: A Case Study on the Encyclopaedia Britannica}, 
  year={2022},
  volume={},
  number={},
  pages={246-255},
  abstract={This work presents frances, an integrated text mining tool that combines information extraction, knowledge graphs, NLP, deep learning, parallel processing and Semantic Web techniques to unlock the full value of historical digital textual collections, offering new capabilities for researchers to use powerful analysis methods without being distracted by the technology and middleware details. To demonstrate these capabilities, we use the first eight editions of the Encyclopaedia Britannica offered by the National Library of Scotland (NLS) as an example digital collection to mine and analyse. We have developed novel parallel heuristics to extract terms from the original collection (alongside metadata), which provides a mix of unstructured and semi-structured input data, and populated a new knowledge graph with this information. Our Natural Language Processing models enable frances to perform advanced analyses that go significantly beyond simple search using the information stored in the knowledge graph. Furthermore, frances also allows for creating and running complex text mining analyses at scale. Our results show that the novel computational techniques developed within frances provide a vehicle for researchers to formalize and connect findings and insights derived from the analysis of large-scale digital corpora such as the Encyclopaedia Britannica.},
  keywords={Text mining;Deep learning;Semantic Web;Knowledge engineering;Parallel processing;Metadata;Information retrieval;information extraction;knowledge graphs;deep transfer learning;natural language processing;text mining;web tools;semantic web;parallel computing;digital tools;historical digital textual collections},
  doi={10.1109/eScience55777.2022.00038},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9970920,
  author={Cahyaningsih, Elin and Silalahi, Natascha Lestari Eunike and Rohajawati, Siti and Avianti, Yuliza Maulina},
  booktitle={2022 International Conference on Information Technology Systems and Innovation (ICITSI)}, 
  title={COMMONKADS for Knowledge Based System Development: A Literature Study}, 
  year={2022},
  volume={},
  number={},
  pages={213-218},
  abstract={COMMONKADS is a method for developing knowledge-based system. This method describes foundation, technique, modeling language and document structure for develop the knowledge-based system. COMMONKADS is people-oriented system development methodology, and this methodology is often used for developing organizational knowledge management system. COMMONKADS approach is divided based on context (organizational model, task model, agent model), concept (knowledge model) and artifact (design model). COMMONKADS have been used widely for knowledge-based system in several fields, such as COMMONKADS that integrated in tourism knowledge-based system, COMMONKADS for irrigation expert system, expertise model using COMMONKADS in manufactured company, COMMONKADS in energy management system and many more. Generally, there are eight strengths of COMMONKADS methodology for develop knowledge-based system. Its strength is flexible to use in any scope, represent knowledge (organizational, domain, task and inference knowledge), complete (representation, model, and form), powerful, accurate, comprehensive, represent KM process, systematic and effective. While the weakness of COMMONKADS methodology only three, there are don't have validation process and difficult to acquisition knowledge and use semi formal language, large data storage. Nevertheless, COMMONKADS is recommended methodology for develop knowledge-based system.},
  keywords={Technological innovation;Systematics;Knowledge based systems;Government;Memory;Software;Regulation;COMMONKADS;strength of COMMONKADS;knowledge-based system;weakness of COMMONKADS;methodology;knowledge-based engineering},
  doi={10.1109/ICITSI56531.2022.9970920},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9961280,
  author={Li, Jianli and Yilahun, Hankiz and Hamdulla, Askar},
  booktitle={2022 International Conference on Asian Language Processing (IALP)}, 
  title={Petroleum Exploration and Development Text Triplet Extraction Based on Deep Learning}, 
  year={2022},
  volume={},
  number={},
  pages={225-230},
  abstract={The petroleum exploration and development industry is moving from digital to intelligent. Under the guidance of AI, machine reading and automatic extraction of petroleum exploration and development knowledge are needed for unstructured datas. There are a large number of long entities and complex nested entities in petroleum exploration and development text, which increase the challenge of petroleum exploration and development triplet extraction task. To solve the above problems, (1) The ALBERT-BiLSTM-Attention-CRF method based on large-scale pre-trained Chinese language model is used to extract text triples for Petroleum exploration and development. (2) The ALBERT-BiGRU-Attention method is used to carry out text dichotomies to judge whether triplet extraction is effective. By collecting the datas of Petroleum exploration and development, the corpus of Petroleum exploration and development is established. Secondly, knowledge composition is analyzed and corpus is annotated under the guidance of Petroleum exploration and development theory. Finally, the deep learning training of Petroleum exploration and development descriptive corpus knowledge extraction is carried out. The experimental results show that the accuracy of SPO triplet named entity recognition is 85.49%. The recognition accuracy of extracted triples is 95.90%, which can achieve good recognition effect in small-scale Petroleum exploration and development corpus.},
  keywords={Deep learning;Training;Industries;Text recognition;Geology;Semantics;Ontologies;Deep learning;ALBERT-BiLSTM-Attention-CRF;Triplet extraction;ALBERT-BiGRU-Attention;Petroleum Exploration and Development},
  doi={10.1109/IALP57159.2022.9961280},
  ISSN={},
  month={Oct},}@ARTICLE{9940925,
  author={Borrego, Agustín and Dessì, Danilo and Hernández, Inma and Osborne, Francesco and Reforgiato Recupero, Diego and Ruiz, David and Buscaldi, Davide and Motta, Enrico},
  journal={IEEE Access}, 
  title={Completing Scientific Facts in Knowledge Graphs of Research Concepts}, 
  year={2022},
  volume={10},
  number={},
  pages={125867-125880},
  abstract={In the last few years, we have witnessed the emergence of several knowledge graphs that explicitly describe research knowledge with the aim of enabling intelligent systems for supporting and accelerating the scientific process. These resources typically characterize a set of entities in this space (e.g., tasks, methods, evaluation techniques, proteins, chemicals), their relations, and the relevant actors (e.g., researchers, organizations) and documents (e.g., articles, books). However, they are usually very partial representations of the actual research knowledge and may miss several relevant facts. In this paper, we introduce SciCheck, a new triple classification approach for completing scientific statements in knowledge graphs. SciCheck was evaluated against other state-of-the-art approaches on seven benchmarks, yielding excellent results. Finally, we provide a real-world use case and applied SciCheck to the Artificial Intelligence Knowledge Graph (AI-KG), a large-scale automatically-generated open knowledge graph including 1.2M statements extracted from the 333K most cited articles in the field of Artificial Intelligence, and generated a new version of this knowledge graph with 300K additional triples.},
  keywords={Machine learning;Feature extraction;Semantic Web;Task analysis;Context modeling;Computational modeling;Benchmark testing;Knowledge based systems;Knowledge graphs;science of science;knowledge graph completion;triple classification;machine learning;semantic web},
  doi={10.1109/ACCESS.2022.3220241},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9926710,
  author={Ji, Fan and Ocker, Felix and Zou, Minjie and Vogel-Heuser, Birgit and Oligschläger, Marius},
  booktitle={2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)}, 
  title={Identifying Inconsistencies in the Design of Large-scale Casting Systems – An Ontology-based Approach}, 
  year={2022},
  volume={},
  number={},
  pages={319-325},
  abstract={The development of modern automated production systems requires the close cooperation of engineers from different domains. Due to the large amount of domain-specific documents and heterogeneous data they create during the multidisciplinary engineering activities, ensuring the consistency of information is always challenging. Since most of these documents are texted-based and lack a standardized structure, extracting required information from these files is oftentimes problematic. This issue is particularly critical in the development of large-scale production plants due to the high complexity of the systems and the diversity of disciplines involved. To help engineers efficiently utilize unstructured data sources as well as identify potential information contradictions, we propose an ontology-based inconsistency management approach for large-scale production systems that generates the knowledge base from unstructured engineering data and (semi-) automatically detects multiple types of inconsistencies. In addition, the presented framework also supports the tracking of information changes during the system design process.},
  keywords={Production systems;Casting;Computer aided software engineering;Automation;Soft sensors;Knowledge based systems;Complexity theory},
  doi={10.1109/CASE49997.2022.9926710},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{9921564,
  author={Köcher, Aljosha and Da Silva, Luis Miguel Vieira and Fay, Alexander},
  booktitle={2022 IEEE 27th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Modeling and Executing Production Processes with Capabilities and Skills using Ontologies and BPMN}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Current challenges of the manufacturing industry require modular and changeable manufacturing systems that can be adapted to variable conditions with little effort. At the same time, production recipes typically represent important company know-how that should not be directly tied to changing plant configurations. Thus, there is a need to model general production recipes independent of specific plant layouts. For execution of such a recipe however, a binding to then available production resources needs to be made. In this contribution, we select a suitable modeling language to model and execute such recipes. Furthermore, we present an approach to solve the issue of recipe modeling and execution in modular plants using semantically modeled capabilities and skills as well as BPMN. We make use of BPMN to model production recipes using capability processes, i.e. production processes referencing abstract descriptions of resource functions. These capability processes are not bound to a certain plant layout, as there can be multiple resources fulfilling the same capability. For execution, every capability in a capability process is replaced by a skill realizing it, effectively creating a skill process consisting of various skill invocations. The presented solution is capable of orchestrating and executing complex processes that integrate production steps with typical IT functionalities such as error handling, user interactions and notifications. Benefits of the approach are demonstrated using a flexible manufacturing system.},
  keywords={Manufacturing industries;Adaptation models;Layout;Production;Companies;Ontologies;Flexible manufacturing systems;Capabilities;Skills;Skill-Based Production;Orchestration;BPMN;Ontologies;Semantic Web},
  doi={10.1109/ETFA52439.2022.9921564},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9909441,
  author={Rybiński, Kamil and Śmiałek, Michał},
  booktitle={2022 17th Conference on Computer Science and Intelligence Systems (FedCSIS)}, 
  title={Beyond Low-Code Development: Marrying Requirements Models and Knowledge Representations}, 
  year={2022},
  volume={},
  number={},
  pages={919-928},
  abstract={Typical Low-Code Development platforms enable model-driven generation of web applications from high-level visual notations. They normally express the UI and the application logic, which allows generating the frontend and basic CRUD operations. However, more complex domain logic (data processing) operations still necessitate the use of traditional programming. This paper presents a visual language, called RSL-DL, to represent domain knowledge with complex domain rules aligned with requirements models. The language synthesises and extends approaches found in knowledge representation (ontologies) and software modelling language engineering. Its purpose is to enable a fully automatic generation of domain logic code by reasoning over and reusing domain knowledge. The language’s abstract syntax is defined using a meta-model expressed in MOF. Its semantics is expressed with several translational rules that map RSL-DL models onto typical programming language constructs. The rules are explained informally in natural language and formalised using a graphical transformation notation. It is also supported by introducing an inference engine that enables processing queries to domain models and selecting appropriate invocations to generated code. The presented language was implemented by building a dedicated model editor and transformation engine. It was also initially validated through usability studies. Based on these results, we conclude that declarative knowledge representations can be successfully used to produce imperative back-end code with non-trivial logic.},
  keywords={Visualization;Codes;Computational modeling;Semantics;Syntactics;Programming;Ontologies},
  doi={10.15439/2022F129},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9905090,
  author={Pradeepani, M. K. T. and Jayawardena, C. and Rajapaksha, U. U. S.},
  booktitle={2022 International Research Conference on Smart Computing and Systems Engineering (SCSE)}, 
  title={Adding Commonsense to Robotic Application Using Ontology-Based Model Retraining}, 
  year={2022},
  volume={5},
  number={},
  pages={157-164},
  abstract={In terms of the level of technological capability in the world today, the use of automated robotics is common in various fields. There are large projects going on in many industries that collaborate between robots and other robots, as well as humans and robots. In hospital environments, care for people with medical needs and their needs and used to make appropriate suggestions to their problems. Robots can also be found in certain areas that can respond quickly as an emergency rescue agent. Furthermore, robots, which can be seen in the hotel industry as waiters and as farm assistants in agriculture, have a great tendency to be used as multi-tasking agents in many fields. In each of these areas, robots must co-operate with humans. In that situation, the importance of the exchange of mutual knowledge between robots-robots and between humans-robots comes into the picture. What matters here is not only the quantitative vastness of knowledge but also the ability to understand each other in the same medium. Although the common sense that people need in their day-to-day work is completely obvious to humans, the commonsense knowledge domain needs to be implanted in robots. Whatever concept is defined for adding commonsense to robotics, it should be a consistent concept that can be logically constructed so that it can be understood by a machine. As will be discussed later in the paper, different methods have been used in various related works to add a different kind of domain knowledge to robotics. The objective of this paper is to provide an improved retrained model for robotics in order to give them the ability to act more human-like when performing tasks. By using the proposed model robots are able to answer the incomplete command or inquiries related to a given context. One of the objectives of this work is to use the ontology-based, commonsense-support existing knowledge base as a mechanism to retrain and build a new model.},
  keywords={Training;Adaptation models;Service robots;Knowledge based systems;Robot sensing systems;Hardware;Sensors;BERT;commonsense;robotics;transfer learning},
  doi={10.1109/SCSE56529.2022.9905090},
  ISSN={2613-8662},
  month={Sep.},}@INPROCEEDINGS{9892075,
  author={Lin, RuiMing and Cheng, LiangLun and Wang, Tao and Deng, Jianfeng},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Trans-SBLGCN: A Transfer Learning Model for Event Logic Knowledge Graph Construction of Fault Diagnosis}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Taking fault diagnosis corpus as the research object, an event logic knowledge graph construction method is proposed in this paper. Firstly, we propose a data labeling strategy based on a constructed event logic ontology model, then collect large-scale robot transmission system fault diagnosis corpus, and label part of the data according to the strategy. Secondly, we propose a transfer learning model called Trans-SBLGCN for event argument entity and event argument relation joint extraction. A language model is trained based on large-scale unlabeled fault diagnosis corpus and transferred to a model based on stacked bidirectional long short term memory (BiLSTM) and bidirectional graph convolutional network (BiGCN). Experimental results show that the method is superior to other methods. Finally, an event logic knowledge graph of robot transmission system fault diagnosis is constructed to provide decision support for autonomous robot transmission system fault diagnosis.},
  keywords={Fault diagnosis;Knowledge engineering;Transfer learning;Neural networks;Ontologies;Data models;Labeling;Event Logic Knowledge Graph;Fault Diagnosis;Knowledge Joint Extraction;BiGCN},
  doi={10.1109/IJCNN55064.2022.9892075},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9896311,
  author={Rogachev, Aleksey and Melikhova, Elena},
  booktitle={2022 International Russian Automation Conference (RusAutoCon)}, 
  title={Automation of Architecture Justification and Parameters Selection of Artificial Neural Networks for Intelligent Detection of Cyber-Physical Threats}, 
  year={2022},
  volume={},
  number={},
  pages={908-912},
  abstract={The problems of improving the quality of training of deep artificial neural networks (ANN) for various applied tasks require automatization of the selection of hyperparameters of neural networks. The KerasTuner software toolkit can be used to automate the search for optimal values of ANN hyperparameters. It includes random search methods, Bayesian optimization, etc. The formation of training text samples for neural network identification of cyber-physical threats is a separate scientific and methodological task. The complexity of the problem is due to the diversity of the ontology of the key terms of the cyberphysical thesaurus, the variety of styles of lexicological content, as well as the partial intersection of the content of previously identified ontological categories. In the process of experimental study of hyperparameters of deep ANNs being developed, models of “embedding”, “bag of words” and dense vector representation in Python were compared. On the basis of a systematic approach, an information-morphological matrix of thematic blocks is constructed. In the conducted experiments, the values of parameters such as the number of convolutional blocks, the number of their filters, the type of activation functions, the parameters of the “dropout” layers, etc. were changed. The studied tools provided optimization of hyperparameters of the convolutional network, while the calculation time on the Colaboratory platform for the studied ANN architectures using GPU graphics accelerators was 5…9 o’clock. The developed modified algorithm for computer detection of cyberphysical threats in electronic resources allowed to substantiate alternative architectures and optimize the main hyperparameters of ANN.},
  keywords={Training;Automation;Graphics processing units;Artificial neural networks;Computer architecture;Ontologies;Cyber-physical systems;cyber-physical threat;artificial neural network;hyperparameter;intelligent detection;Automation},
  doi={10.1109/RusAutoCon54946.2022.9896311},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9887476,
  author={Dudija, Nidya and Natalia, Lezia and Alamsyah, Andry and Romadhony, Ade},
  booktitle={2022 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)}, 
  title={Identification of Extraversion and Neuroticism Personality Dimensions Using IndoBERT’s Deep Learning Model}, 
  year={2022},
  volume={},
  number={},
  pages={155-159},
  abstract={Human resources are essential for the business organization to adapt to change. Identifying the personality dimensions of new talent could help recruiters conduct the selection process of matching skilled talent to the organization’s needs. The objective of this study is to identify the personality dimensions corresponding to the job need, which correlates with extraversion and neuroticism. The legacy methodology to determine personality dimensions is through interviews or questionnaire surveys, but this process is costly and takes longer time to complete. This paper proposes a work on a person personality identification based on social media text as a complementary methodology. We utilize the textual data to support identifying new talent personality dimensions. In this study, we use IndoBERT model to capture person personality dimension based on their post on Twitter social media. As a result, our model achieves 96% accuracy in identifying extraversion and neuroticism personality dimensions. We also compare our result with the previous work based on the ontology model.},
  keywords={Deep learning;Costs;Social networking (online);Blogs;Ontologies;Communications technology;Fourth Industrial Revolution;Human Resource;Talent Selection;Personality Identification Dimension;Deep Learning;IndoBERT},
  doi={10.1109/IAICT55358.2022.9887476},
  ISSN={},
  month={July},}@INPROCEEDINGS{9874511,
  author={Faramarzi, Noushin Salek and Dara, Akanksha and Banerjee, Ritwik},
  booktitle={2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)}, 
  title={Combining Attention-based Models with the MeSH Ontology for Semantic Textual Similarity in Clinical Notes}, 
  year={2022},
  volume={},
  number={},
  pages={74-83},
  abstract={In this study, we present several transformer-based models as well as traditional machine learning methods to detect semantic textual similarity (STS) in clinical notes. We investigate transformer models pretrained on general English as well as clinical notes, and use generic English STS datasets as a supplemental corpus to clinical notes data. Our work is based on the 2019 National NLP Clinical Challenge (n2c2). We identify and annotate six types of sentences in the clinical notes corpus, and report an ensemble method that combines attention-based contextualized embeddings with a similarity score based on the MeSH ontology obtained by computing least common ancestors of clinical terms. Our approach does not need additional clinical data for model training, while still achieving comparable Pearson's correlation coefficient of 0.901.},
  keywords={Training;Drugs;Vocabulary;Computational modeling;Semantics;Machine learning;Ontologies;Electronic Health Records;Natural Language Processing;Clinical Semantic Textual Similarity;Transformers;MeSH},
  doi={10.1109/ICHI54592.2022.00023},
  ISSN={2575-2634},
  month={June},}@INPROCEEDINGS{9874698,
  author={Zheng, Can and Wang, Yanshan and Jia, Xiaowei},
  booktitle={2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)}, 
  title={Graph-Augmented Cyclic Learning Framework for Similarity Estimation of Medical Clinical Notes}, 
  year={2022},
  volume={},
  number={},
  pages={97-103},
  abstract={Semantic textual similarity (STS) in the clinical domain helps improve diagnostic efficiency and produce concise texts for downstream data mining tasks. However, given the high degree of domain knowledge involved in clinic text, it remains challenging for general language models to infer implicit medical relationships behind clinical sentences and output similarities correctly. In this paper, we present a graph-augmented cyclic learning framework for similarity estimation in the clinical domain. The framework can be conveniently implemented on a state-of-art backbone language model, and improve its performance by leveraging domain knowledge through co-training with an auxiliary graph convolution network (GCN) based network. We report the success of introducing domain knowledge in GCN and the co-training framework by improving the Bio-clinical BERT baseline by 16.3% and 27.9%, respectively.},
  keywords={Knowledge engineering;Biological system modeling;Semantics;Estimation;Medical services;Manuals;Ontologies;clinical notes;graph neural networks;BERT},
  doi={10.1109/ICHI54592.2022.00026},
  ISSN={2575-2634},
  month={June},}@INPROCEEDINGS{9865330,
  author={Dordiuk, Vladislav and Demicheva, Ekaterina and Espino, Fernando Polanco and Ushenin, Konstantin},
  booktitle={2022 Ural-Siberian Conference on Computational Technologies in Cognitive Science, Genomics and Biomedicine (CSGB)}, 
  title={Natural language processing for clusterization of genes according to their functions}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={There are hundreds of methods for analysis of data obtained in mRNA-sequencing. The most of them are focused on small number of genes. In this study, we propose an approach that reduces the analysis of several thousand genes to analysis of several clusters. The list of genes is enriched with information from open databases. Then, the descriptions are encoded as vectors using the pretrained language model (BERT) and some text processing approaches. The encoded gene function pass through the dimensionality reduction and clusterization. Aiming to find the most efficient pipeline, 180 cases of pipeline with different methods in the major pipeline steps were analyzed. The performance was evaluated with clusterization indexes and expert review of the results.},
  keywords={Dimensionality reduction;Databases;Pipelines;Bit error rate;Genomics;Natural language processing;Cognitive science;natural language processing;BERT;semantic analysis;differential gene expression analysis;gene ontology;gene expression;clusterization},
  doi={10.1109/CSGB56354.2022.9865330},
  ISSN={},
  month={July},}@INPROCEEDINGS{9863051,
  author={Ataei, Sima and Butler, Gregory},
  booktitle={2022 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)}, 
  title={Predicting the specific substrate for transmembrane transport proteins using BERT language model}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Transmembrane transport proteins play a vital role in cells' metabolism by the selective passage of substrates through the cell membrane. Metabolic network reconstruction requires transport reactions that describe the specific substrate transported as well as the metabolic reactions of enzyme catalysis. In this paper, we apply BERT (Bidirectional Encoder Representations from Transformers) language model for protein sequences to predict one of 12 specific substrates. Our UniProt-ICAT-100 dataset is automatically constructed from UniProt using the ChEBI and GO ontologies to identify 4,112 proteins transporting 12 inorganic anion or cation substrates. We classified this dataset using three different models including Logistic Regression with an MCC of 0.81 and accuracy of 97.5%; Feed-forward Neural Networks classifier with an MCC of 0.88 and accuracy of 98.5%. Our third model utilizes a Fine-tuned BERT language model to predict the specific substrate with an MCC of 0.95 and accuracy of 99.3% on an independent test set.},
  keywords={Proteins;Biological system modeling;Computational modeling;Bit error rate;Neural networks;Cells (biology);Predictive models;Classification;BERT model;Transport protein;Specific substrate Prediction;ChEBI ontology;Gene Ontology},
  doi={10.1109/CIBCB55180.2022.9863051},
  ISSN={},
  month={Aug},}@ARTICLE{9866769,
  author={Silega, Nemury and Noguera, Manuel and Rogozov, Yuri I. and Lapshin, Vyacheslav S. and González, Tahumara},
  journal={IEEE Access}, 
  title={Transformation From CIM to PIM: a Systematic Mapping}, 
  year={2022},
  volume={10},
  number={},
  pages={90857-90872},
  abstract={Model Driven Architecture (MDA) is the most prominent and accepted methodology based on the Model Driven Development (MDD) principles. MDA includes three abstraction levels: Computer Independent Models (CIM), Platform Independent models (PIM) and Platform specific models (PSM). MDA encourages the automatic transformation of models as a means to increase the speed of the software development process and to prevent human errors. There are plenty of solutions to transform PIMs to PSMs, however the CIM to PIM transformation does not receive a similar attention. In that sense, this paper aims to describe a systematic mapping to analyze the main characteristics of the approaches that deal with the CIM to PIM transformation as well as to discuss research directions stemming out from our analysis. The results of this mapping study could be a valuable information source for the scientific community in order to know the real advances in this topic and to avoid unnecessary effort dealing with problems that have already been addressed. For example, this study yielded the models at the CIM level that have already been transformed into models at the PIM level. Hence, with this information, the researchers could focus their attention on finding solutions to transform those models at CIM level that have not been transformed into models at PIM level. Likewise, this mapping study provides information regarding the technological support of this type of transformation. This information could be useful for those software projects interested to adopt MDA.},
  keywords={Computational modeling;Software;Systematics;Business;Software engineering;Mathematical models;Internet;Model driven architecture (MDA);computer independent models (CIM);platform independent models (PIM);systematic mapping},
  doi={10.1109/ACCESS.2022.3201556},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9861086,
  author={Morine, Melissa J. and Priami, Corrado and Coronado, Edith and Haber, Juliana and Kaput, Jim},
  booktitle={2022 IEEE International Conference on Digital Health (ICDH)}, 
  title={A Comprehensive and Holistic Health Database}, 
  year={2022},
  volume={},
  number={},
  pages={202-207},
  abstract={Health and the initiation, progression, and outcome of disease are the result of multiple environmental factors interacting with individual genetic makeups. Collectively, results from primary clinical research on health and disease represent the most compendious and reliable source of actionable knowledge on strategies to optimize health. However, the dispersal of this information as unstructured data, distributed across millions of documents, is a substantial challenge in bridging the gap between primary research and concrete recommendations for improving health. Described here is the development and implementation of a machine reading pipeline that builds a knowledge graph of causal relationships between a broad range of predictive/modifiable diet and lifestyle factors and health outcomes, extracted from the vast biomedical corpus in the National Library of Medicine.},
  keywords={Text mining;Systematics;Pipelines;Semantics;Genetics;Libraries;Environmental factors;Healthware;knowledge graphs;natural language processing},
  doi={10.1109/ICDH55609.2022.00039},
  ISSN={},
  month={July},}@INPROCEEDINGS{9856059,
  author={Rudwan, Mohammed Suleiman Mohammed and Fonou-Dombeu, Jean Vincent},
  booktitle={2022 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)}, 
  title={Ontology Reuse: Neural Network-Based Measurement of Concepts Representations and Similarities in Ontology Corpus}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Ontologies are the heart of the semantic web. They are designed to be reused in web applications. This paper aims to discover a given concept's representation against existing ontologies in a corpus, and if the concept is represented, other similar concepts and terms to it are extracted. A corpus formed of several ontologies in the agricultural domain was constructed. SPARQL queries were used to extract the required data from existing ontologies. And a machine learning technique, the Word2Vec, was employed for ontology reuse process to measure concepts similarity against the existing ontologies. The experimental results showed that the proposed methodology successfully detected previously seen vocabularies during the training on the data in the ontology corpus, and retrieved other similar concepts from the ontologies as well as their degree of similarity (Cosin similarity). Furthermore, the proposed model could process over two million terms in around one minute, reflecting its effectiveness in this context. The proposed method would be useful to ontology and knowledge engineers to conduct a preliminary investigation about which existing ontologies are suitable for reuse in the process of developing new ontologies. Other applications of the proposed method may include ontology alignment to measure the degree of similarity between existing ontologies.},
  keywords={Training;Semantic Web;Knowledge engineering;Heart;Vocabulary;Machine learning;Ontologies;automated ontology reuse;NLP;Word2Vec;artificial neural networks},
  doi={10.1109/icABCD54961.2022.9856059},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9845845,
  author={Yiming, Liu and Li, Duan},
  booktitle={2022 7th International Conference on Computer and Communication Systems (ICCCS)}, 
  title={Research on the Construction of Maritime Legal Knowledge Graph}, 
  year={2022},
  volume={},
  number={},
  pages={903-908},
  abstract={As the marine industry booms, the maritime legal documents are of great importance to the maneuver on the sea. However, the traditional way of consulting the text can not meet the demand of maritime operation nowadays. This paper aims to explore a way to extract and strengthen data from maritime legal texts to better support legal question answering. To mine knowledge from unstructured maritime laws and regulations, this paper proposes a method to build the maritime legal knowledge graph. To extract information from unstructured texts, BERT+BiLSTM+CRF is used for named entity recognition. DeepKE toolkit is used for relation extraction. And to strengthen the logics between entities, heterogeneous nodes are introduced to enhance the semantic associations in the maritime legal knowledge graph. The document-enhanced knowledge graph expanded in scale, so it can better support subsequent intelligent applications.},
  keywords={Law;Text recognition;Semantics;Pipelines;Ontologies;Information retrieval;Regulation;knowledge graph;maritime law;named entity recognition;heterogeneous entities},
  doi={10.1109/ICCCS55155.2022.9845845},
  ISSN={},
  month={April},}@INPROCEEDINGS{9843341,
  author={Mordecai, Yaniv and Markina–Khusid, Aleksandra and Quinn, Greg and Crawley, Edward F.},
  booktitle={2022 IEEE Aerospace Conference (AERO)}, 
  title={Applying Model-Based Ontology Coverage Analysis to Mission Architectures}, 
  year={2022},
  volume={},
  number={},
  pages={01-18},
  abstract={This paper introduces a method for Model-based Ontology Coverage Analysis (MOCA) and applies it to SysML models of mission architectures. An ontology is a set of concepts that constitute a common language, standard terminology, and consistent pattern reference across multiple models within an organization, industry, or domain. The purpose of MOCA is to assess the overlap between a system architecture model and a given ontology, and thereby the architecture model's compliance with the ontology and the ontology's utilization by the architecture. We demonstrate MOCA on a SysML model of a humanitarian airlift mission, using a conceptual mission architecting SysML profile model that serves as the ontology. MOCA automates and simplifies reasoning over models, and creates digital model-based artifacts that support stakeholders in concept validation, decision making, and system/mission design. Thus, MOCA enhances digital systems engineering.},
  keywords={Analytical models;Vocabulary;Visualization;Digital systems;Atmospheric modeling;Unified modeling language;Semantics;Digital Engineering;Model-Based Systems Engineering;MBSE;Mission Architecture;Mission Engineering;Ontology;Ontological Analysis},
  doi={10.1109/AERO53065.2022.9843341},
  ISSN={1095-323X},
  month={March},}@INPROCEEDINGS{9816191,
  author={Andreadis, Stelios and Elias, Mirette and Mavropoulos, Thanassis and Papadopoulos, Charis and Pantelidis, Nick and Gialampoukidis, Ilias and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
  booktitle={2022 IEEE 14th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP)}, 
  title={SPARQL querying for validating the usage of automatically georeferenced social media data as human sensors for air quality}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={The problem of air pollution is one of the countless topics discussed on social media on an everyday basis. This rich, crowdsourced information can be exploited to assess the air quality of urban areas, using humans as sensors. Nevertheless, the majority of social media data are falsely geotagged or completely lack geoinformation, which is an essential attribute, while the reliability of the air pollution events reported by online citizens has to be proven. The scope of this work is to present a framework that collects Twitter messages in German that refer to the atmosphere, automatically georeferences them, and finally validates them through semantic representation and SPARQL queries in order to associate them with real measurements of air quality sensors. The georeferencing models are evaluated against state-of-the-art works and the proposed framework is validated in a near-six-month scenario in Germany.},
  keywords={Multidimensional signal processing;Social networking (online);Atmospheric measurements;Atmospheric modeling;Urban areas;Semantics;Sensor phenomena and characterization;air quality;social media;georeferencing;semantic representation;SPARQL querying},
  doi={10.1109/IVMSP54334.2022.9816191},
  ISSN={},
  month={June},}@BOOK{9785669,
  author={Quamar, Abdul and Efthymiou, Vasilis and Lei, Chuan and Özcan, Fatma},
  booktitle={Natural Language Interfaces to Data},
  year={2022},
  volume={},
  number={},
  pages={},
  abstract={Natural language interfaces provide an easy way to query and interact with data and enable non-technical users to investigate data sets without the need to know a query language. Recent advances in natural language understanding and processing have resulted in a renewed interest in natural language interfaces to data. The main challenges in natural language querying are identifying the entities involved in the user utterance, connecting the different entities in a meaningful way over the underlying data source to interpret user intents, and generating a structured query. There are two main approaches in the literature for interpreting a user’s natural language query. The first are rule-based systems that make use of semantic indices, ontologies, and knowledge graphs to identify the entities in the query, understand the intended relationships between those entities, and utilize grammars to generate the target queries. Second are hybrid approaches that utilize both rule-based techniques as well as deep learning models. Conversational interfaces are the next natural step to one-shot natural language querying by exploiting query context between multiple turns of conversation for disambiguation. In this monograph, the authors review the rule-based and hybrid technologies that are used in natural language interfaces and survey the different approaches to natural language querying. They also describe conversational interfaces for data analytics and discuss several benchmarks used for natural language querying research and evaluation. The monograph concludes with discussion on challenges that need to be addressed before these systems can be widely adopted.},
  keywords={},
  doi={},
  ISSN={},
  publisher={now},
  isbn={9781638280293},
  url={https://ieeexplore.ieee.org/document/9785669},}@INPROCEEDINGS{9776306,
  author={Ma, Ke and Qin, Bo and Wang, Hongwei},
  booktitle={2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
  title={A Profiling and Query Platform for Research Management Based on Knowledge Graph}, 
  year={2022},
  volume={},
  number={},
  pages={822-827},
  abstract={Researching is a process whereby a large amount of new and unstructured knowledge is created and accumulated. In this context, the capture of complex knowledge about detailed work and decision-making issues throughout a research project is very challenging for modern researchers. Knowledge graph technology can help machines better understand complicated relationships between entities, has great potential for helping researchers with organizing and automating such kinds of repetitive works, and even uncovering and providing new insights into related topics.This paper introduces a way to construct a research management platform by providing a profiling and query system visualized as a knowledge graph. With the scope of this platform being restricted to the research field, typical ontologies are proposed on different levels. For better and more meaningful visualization, specific modifications and improvements to the traditional knowledge graph structure are discussed. A prototype system that is under construction is then described based on the above work, with the extensive applications discussed.},
  keywords={Visualization;Conferences;Decision making;Prototypes;Ontologies;Collaborative work;knowledge graph;research management;ontology},
  doi={10.1109/CSCWD54268.2022.9776306},
  ISSN={},
  month={May},}@ARTICLE{9744572,
  author={Hsu, Hao-Hsuan and Huang, Nen-Fu},
  journal={IEEE Transactions on Learning Technologies}, 
  title={Xiao-Shih: A Self-Enriched Question Answering Bot With Machine Learning on Chinese-Based MOOCs}, 
  year={2022},
  volume={15},
  number={2},
  pages={223-237},
  abstract={This article introduces Xiao-Shih, the first intelligent question answering bot on Chinese-based massive open online courses (MOOCs). Question answering is critical for solving individual problems. However, instructors on MOOCs must respond to many questions, and learners must wait a long time for answers. To address this issue, Xiao-Shih integrates many novel natural language processing and machine learning approaches to achieve state-of-the-art performance. Furthermore, Xiao-Shih has a built-in self-enriched mechanism for expanding the knowledge base through open community-based question answering. This article proposes a novel approach, known as spreading question similarity (SQS), which iterates similar keywords on our keyword networks to find duplicate questions. Compared with BERT, an advanced neural language model, the results showed that SQS outperforms BERT on recall and accuracy above a prediction probability threshold of 0.8. After training, Xiao-Shih achieved a perfect correct rate. Furthermore, Xiao-Shih outperforms Jill Watson 1.0, which is a noted question answering bot, on answer rate with the self-enriched mechanism.},
  keywords={Chatbots;Standards;Electronic learning;Data science;Computer aided instruction;Bit error rate;Machine learning;Answer selection;machine learning (ML);massive open online courses (MOOCs);natural language processing (NLP);ontologies;question answering bot;question retrieval},
  doi={10.1109/TLT.2022.3162572},
  ISSN={1939-1382},
  month={April},}@INPROCEEDINGS{9736481,
  author={Lim, Chae-Gyun and Lee, Dongkun and Lee, Young-Jun and Choi, Ho-Jin},
  booktitle={2022 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Knowledge Management Approach for Memory Components Based on User-friendly Conversational System}, 
  year={2022},
  volume={},
  number={},
  pages={401-403},
  abstract={Due to the recent technological development and the growth of computing resources, there are various studies that apply large-scale language models in the field of natural language processing such as conversational systems. Also, there are researches that attempt to maintain a conversation flow and naturally lead a dialogue by treating the contextual information exchanged with users from the perspective of knowledge. In this paper, we propose a method for managing various contextual information such as chat history, situations, and preferred topics based on a knowledge base and generating a conversation customized for the specific user. We design a schema of memory components to deal with the user's contextual information so that implement a Web-based conversational system, which is friendly come to those users. It is expected that these systems using the user-specific memory components will be helpful in such domains like education or customer consultation.},
  keywords={Conferences;Computational modeling;Memory management;Knowledge based systems;Education;Lead;Big Data;memory component;ontology-based approach;conversation history;conversational system},
  doi={10.1109/BigComp54360.2022.00091},
  ISSN={2375-9356},
  month={Jan},}@INPROCEEDINGS{9736309,
  author={Burgdorf, Andreas and Paulus, Alexander and Pomp, André and Meisen, Tobias},
  booktitle={2022 IEEE 16th International Conference on Semantic Computing (ICSC)}, 
  title={DocSemMap: Leveraging Textual Data Documentations for Mapping Structured Data Sets into Knowledge Graphs}, 
  year={2022},
  volume={},
  number={},
  pages={209-216},
  abstract={Today, knowledge graphs have been proven to enable the efficient integration of heterogeneous data sets. An important step in creating such knowledge graphs is the mapping of the attributes of a data set to the knowledge graph's ontology. So far, numerous methods have been developed to support this mapping process by using both the schema information as well as the actual data values from a data set in conjunction with external knowledge bases or machine learning approaches. A third source of information, namely textual data documentations, has not yet been considered. In this paper, we present DocSemMap, a novel approach that utilizes textual data documentations of data sets as an additional source for the creation of semantic mappings. We train custom embeddings on the textual data documentations. Further, we utilize pre-trained embeddings that allow us to identify similarities between excerpts of the textual data documentations and descriptions of ontological concepts. Based on this, we build candidate sets of the best suitable concepts for mapping and finally use weighted similarity scores to identify the best fitting concept for each attribute of a data set. The evaluation of our approach outperforms existing approaches for semantic mapping but still has potential for improvement.},
  keywords={Semantics;Natural languages;Knowledge based systems;Fitting;Documentation;Machine learning;Syntactics;semantic mapping;knowledge graph construction;natural language processing;textual data documentation},
  doi={10.1109/ICSC52841.2022.00042},
  ISSN={2325-6516},
  month={Jan},}@INPROCEEDINGS{9721762,
  author={Sajid, Hira and Kanwal, Javeria and Bhatti, Saeed Ur Rehman and Qureshi, Saad Ali and Basharat, Amna and Hussain, Shujaat and Khan, Kifayat Ullah},
  booktitle={2022 16th International Conference on Ubiquitous Information Management and Communication (IMCOM)}, 
  title={Resume Parsing Framework for E-recruitment}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Modern approaches to improve networking and communication have given ways to the advancement of recruitment process through the development of e-recruitment recommender systems. The increasing expansion of internet- based recruiting has resulted in a large number of resumes being stored in recruitment systems. Most resumes are prepared in a variety of styles to attract the attention of recruiters, including different font sizes, font colors, and table formats. However, data mining operations such as resume information extraction, automatic profile matching, and applicant ranking are immensely affected by the variety of formats. Rule-based methods, supervised methods and semantics-based methods have been introduced to extract facts from resume accurately, however, these methods heavily depend on large amounts of annotated data that is usually difficult to collect Furthermore, these techniques are time-intensive and bear knowledge incompleteness that strongly affect the accuracy of resume parser. In this paper, we present a resume parsing framework that handles the limitations faced in the previous techniques. At first, the raw text is extracted from resumes and blocks are separated using text block classification. Furthermore, the entities are extracted using named entity recognition and enriched using ontology. The proposed resume parser accurately extracts information from resumes that directly contributes towards the selection of best candidate.},
  keywords={Training;Resumes;Layout;Ontologies;Information retrieval;Feature extraction;Information management;Resume parsing;Data Enrichment;Text Extraction;Ontology;Boolean Naive Bayes},
  doi={10.1109/IMCOM53663.2022.9721762},
  ISSN={},
  month={Jan},}@ARTICLE{9556591,
  author={Marschall, Benedikt and Ochsenkuehn, Daniel and Voigt, Tobias},
  journal={IEEE Journal of Emerging and Selected Topics in Industrial Electronics}, 
  title={Design and Implementation of a Smart, Product-Led Production Control Using Industrial Agents}, 
  year={2022},
  volume={3},
  number={1},
  pages={48-56},
  abstract={In theory, the design of modern production systems in the form of a cyber–physical production system (CPPS) allows more flexibility, simple expandability, quick adaptability, and intelligent production control by the product. Multiagent systems (MASs) are thereby recommended as control solution because of their autonomy and dynamic decentralized architecture. Although their potential use and technical excellence have been proven, the costs of implementation and maintenance still outweigh their supposed advantages. This results in low acceptance and usage of operational MAS in the industry. This article describes topics that need to be considered when designing and implementing an MAS as production control for a CPPS in the context of customized mass production. Generally developed approaches are presented, which were implemented in a commercially developed agent framework and validated on the basis of an industrial use case for a product-led filling process in lot size one. All implemented concepts aim to be reusable in comparable applications across industries. In combination with the MAS-internal testing approach also presented, this should contribute to faster, more cost-effective implementation of reliable MAS solutions and ultimately increase their technical maturity.},
  keywords={Production systems;Production control;XML;Radiofrequency identification;Costs;Companies;Testing;Industrial agents (IAs);cyber–physical product ion system (CPPS);multiagent system (MAS)},
  doi={10.1109/JESTIE.2021.3117121},
  ISSN={2687-9743},
  month={Jan},}@ARTICLE{9534721,
  author={Lu, Jinzhi and Ma, Junda and Zheng, Xiaochen and Wang, Guoxin and Li, Han and Kiritsis, Dimitris},
  journal={IEEE Systems Journal}, 
  title={Design Ontology Supporting Model-Based Systems Engineering Formalisms}, 
  year={2022},
  volume={16},
  number={4},
  pages={5465-5476},
  abstract={Model-based systems engineering (MBSE) provides an important capability for managing the complexities of system development. MBSE empowers the formalism of system architectures for supporting model-based requirement elicitation, specification, design, development, testing, fielding, etc. However, the modeling languages and techniques are heterogeneous, even within the same enterprise system, which leads to difficulties for data interoperability. The discrepancies among data structures and language syntaxes make information exchange among MBSE models more difficult, resulting in considerable information deviations when connecting data flows across the enterprise. Therefore, this article presents an ontology based upon graphs, objects, points, properties, roles, and relationships with extensions (GOPPRRE), providing metamodels that support the various MBSE formalisms across lifecycle stages. In particular, knowledge graph models are developed to support unified model representations to further implement ontological data integration based on GOPPRRE throughout the entire lifecycle. The applicability of the MBSE formalism is verified using quantitative and qualitative approaches. Moreover, the GOPPRRE ontologies are used to create the MBSE formalisms in a domain-specific modeling tool, MetaGraph, for evaluating its availability. The results demonstrate that the proposed ontology supports the formal structures and descriptive logic of the systems engineering lifecycle.},
  keywords={Modeling;Ontologies;Unified modeling language;Tools;Systems engineering and theory;Semantics;Data models;Formalism;interoperability;knowledge graph;model-based systems engineering;ontology},
  doi={10.1109/JSYST.2021.3106195},
  ISSN={1937-9234},
  month={Dec},}@ARTICLE{9525274,
  author={Nourani, Esmaeil and Asgari, Ehsaneddin and McHardy, Alice C. and Mofrad, Mohammad R.K.},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
  title={TripletProt: Deep Representation Learning of Proteins Based On Siamese Networks}, 
  year={2022},
  volume={19},
  number={6},
  pages={3744-3753},
  abstract={Pretrained representations have recently gained attention in various machine learning applications. Nonetheless, the high computational costs associated with training these models have motivated alternative approaches for representation learning. Herein we introduce TripletProt, a new approach for protein representation learning based on the Siamese neural networks. Representation learning of biological entities which capture essential features can alleviate many of the challenges associated with supervised learning in bioinformatics. The most important distinction of our proposed method is relying on the protein-protein interaction (PPI) network. The computational cost of the generated representations for any potential application is significantly lower than comparable methods since the length of the representations is significantly smaller than that in other approaches. TripletProt offers great potentials for the protein informatics tasks and can be widely applied to similar tasks. We evaluate TripletProt comprehensively in protein functional annotation tasks including sub-cellular localization (14 categories) and gene ontology prediction (more than 2000 classes), which are both challenging multi-class, multi-label classification machine learning problems. We compare the performance of TripletProt with the state-of-the-art approaches including a recurrent language model-based approach (i.e., UniRep), as well as a protein-protein interaction (PPI) network and sequence-based method (i.e., DeepGO). Our TripletProt showed an overall improvement of F1 score in the above mentioned comprehensive functional annotation tasks, solely relying on the PPI network. Availability: The source code and datasets are available at https://github.com/EsmaeilNourani/TripletProt.},
  keywords={Proteins;Task analysis;Computational modeling;Training;Protein engineering;Feature extraction;Computational efficiency;Protein representation learning;triplet loss;siamese networks},
  doi={10.1109/TCBB.2021.3108718},
  ISSN={1557-9964},
  month={Nov},}@ARTICLE{9237126,
  author={Eckhart, Matthias and Ekelhart, Andreas and Weippl, Edgar},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={Automated Security Risk Identification Using AutomationML-Based Engineering Data}, 
  year={2022},
  volume={19},
  number={3},
  pages={1655-1672},
  abstract={Systems integrators and vendors of industrial components need to establish a security-by-design approach, which includes the assessment and subsequent treatment of security risks. However, conducting security risk assessments along the engineering process is a costly and labor-intensive endeavor due to the complexity of the system(s) under consideration and the lack of automated methods. This, in turn, hampers the ability of security analysts to assess risks pertaining to cyber-physical systems (CPSs) in an efficient manner. In this work, we propose a method that automatically identifies security risks based on the CPS's data representation, which exists within engineering artifacts. To lay the foundation for our method, we present security-focused semantics for the engineering data exchange format AutomationML (AML). These semantics enable the reuse of security-relevant know-how in AML artifacts by means of a formal knowledge representation, modeled with a security-enriched ontology. Our method is capable of automating the identification of security risk sources and potential consequences in order to construct cyber-physical attack graphs that capture the paths adversaries may take. We demonstrate the benefits of the proposed method through a case study and an open-source prototypical implementation. Finally, we prove that our solution is scalable by conducting a rigorous performance evaluation.},
  keywords={Security;IEC Standards;Risk management;Topology;Semantics;Data models;Knowledge engineering;Cyber-physical systems;information security;AutomationML;security modeling;security risk assessment;industrial control systems;IEC 62443},
  doi={10.1109/TDSC.2020.3033150},
  ISSN={1941-0018},
  month={May},}@INPROCEEDINGS{9742010,
  author={Yang, Wansheng and Deng, Fei and Ma, Siyou and Wu, Linbo and Sun, Zhe and Hu, Chi},
  booktitle={2021 IEEE 21st International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
  title={Test Case Reuse Based on Software Testing Knowledge Graph and Collaborative Filtering Recommendation Algorithm}, 
  year={2021},
  volume={},
  number={},
  pages={67-76},
  abstract={As an important role of software test, the reuse of test cases is essential in terms of finding software defects and locating the causes of them. However, the existing related approaches are insufficient to establish an internal relationship between test cases and defects and their abilities to find or diagnose errors are limited. In this paper, an ontology model based on the software testing process is applied to establish a software testing knowledge graph, which serves as the foundation to build an recommendation system. Specifically, the recommendation system takes the functions of software under test as the “user”, and the defect-occurrence-chain which establishes the correlation between test cases and defects in the knowledge graph as the “item”. Both of them provide the evidence to build collaborative filtering recommendation algorithm based on the user-item scoring matrix. It aims to assist testers in recommending reusable test cases to identify software errors effectively. Against this background, the BERT+Bi-LSTM-CRF model is selected to extract the latent test requirements of the software under test, and an overt variable factorization model is built so as to iteratively optimize the user-item scoring matrix. Further, an empirical study has been conducted, and it is found that the recommended test cases can significantly help testers find software defects faster in a more efficient way, and locate defects more accurately.},
  keywords={Software testing;Collaborative filtering;Software algorithms;Semantics;Software quality;Ontologies;Software;software testing knowledge graph;collaborative filtering;BERT+Bi-LSTM-CRF;defect-occurrence-chain;overt variable factorization model},
  doi={10.1109/QRS-C55045.2021.00020},
  ISSN={2693-9371},
  month={Dec},}@INPROCEEDINGS{9688274,
  author={Kim, Seokhwan and Liu, Yang and Jin, Di and Papangelis, Alexandros and Gopalakrishnan, Karthik and Hedayatnia, Behnam and Hakkani-Tür, Dilek},
  booktitle={2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)}, 
  title={“How Robust R U?”: Evaluating Task-Oriented Dialogue Systems on Spoken Conversations}, 
  year={2021},
  volume={},
  number={},
  pages={1147-1154},
  abstract={Most prior work in dialogue modeling has been on written conversations mostly because of existing data sets. However, written dialogues are not sufficient to fully capture the nature of spoken conversations as well as the potential speech recognition errors in practical spoken dialogue systems. This work presents a new benchmark on spoken task-oriented conversations, which is intended to study multi-domain dialogue state tracking and knowledge-grounded dialogue modeling. We report that the existing state-of-the-art models trained on written conversations are not performing well on our spoken data, as expected. Furthermore, we observe improvements in task performances when leveraging $n$-best speech recognition hypotheses such as by combining predictions based on individual hypotheses. Our data set enables speech-based benchmarking of task-oriented dialogue systems.},
  keywords={Conferences;Benchmark testing;Data models;Robustness;Task analysis;Automatic speech recognition;spoken dialogue systems;dialogue state tracking;knowledge-grounded dialogue generation},
  doi={10.1109/ASRU51503.2021.9688274},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9686335,
  author={Palchunov, Dmitry and Tregubov, A.S.},
  booktitle={2021 International Symposium on Knowledge, Ontology, and Theory (KNOTH)}, 
  title={Semantic methods of intelligent assistant developing}, 
  year={2021},
  volume={},
  number={},
  pages={30-35},
  abstract={Human-computer interaction with people whose visual perception is limited is possible only with tactile and voice interfaces, the latter are being used more and more recently. The aim of the work is to create an intelligent assistant for an indoor navigation system designed for blind and visually impaired people. The development of an intelligent assistant is based on a semantic user model and a four-level ontological model of the subject domain. To build a dialogue between an intelligent assistant and a user, we use the theory of speech acts, argumentation theory and case-based reasoning. The developed software system is aimed at identifying the desires and user needs and proposing possible user actions aimed at achieving them. The system allows for the decomposition of user tasks and the formation of a sequence of their execution based on semantic models of the user and the subject domain.},
  keywords={Knowledge engineering;Uncertainty;Web services;Semantics;Speech recognition;Ontologies;Software systems;intelligent assistant;ontology;machine learning;natural language processing;intent recognition;argumentation theory;case-based reasoning},
  doi={10.1109/KNOTH54462.2021.9686335},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9679873,
  author={Sakhrani, Harsh and Parekh, Saloni and Ratadiya, Pratik},
  booktitle={2021 International Conference on Data Mining Workshops (ICDMW)}, 
  title={Transformer-based Hierarchical Encoder for Document Classification}, 
  year={2021},
  volume={},
  number={},
  pages={852-858},
  abstract={Document Classification has a wide range of applications in various domains like Ontology Mapping, Sentiment Analysis, Topic Categorization and Document Clustering, to mention a few. Unlike Text Classification, Document Classification works with longer sequences that typically contain multiple paragraphs. Previous approaches for this task have achieved promising results, but have often relied on complex recurrence mechanisms that are expensive and time-consuming in nature. Recently, self-attention based models like Transformers and BERT have achieved state-of-the-art performance on several Natural Language Understanding (NLU) tasks, but owing to the quadratic computational complexity of the self-attention mechanism with respect to the input sequence length, these approaches are generally applied to shorter text sequences. In this paper, we address this issue, by proposing a new Transformer-based Hierarchical Encoder approach for the Document Classification task. The hierarchical framework we adopt helps us extend the self-attention mechanism to long-form text modelling thereby reducing the complexity considerably. We use the Bidirectional Transformer Encoder (BTE) at the sentence-level to generate a fixed-size sentence embedding for each sentence in the document. A document-level Transformer Encoder is then used to model the global document context and learn the inter-sentence dependencies. We also carry out experiments with the BTE in a feature-extraction and a fine-tuning setup, allowing us to evaluate the trade-off between computation power and accuracy. Furthermore, we also conduct ablation experiments, and evaluate the impact of different pre-training strategies on the overall performance. Experimental results demonstrate that our proposed model achieves state-of-the-art performance on two standard benchmark datasets.},
  keywords={Training;Sentiment analysis;Computational modeling;Transfer learning;Text categorization;Natural languages;Ontologies;Transformer;Self-attention;Document Classification},
  doi={10.1109/ICDMW53433.2021.00109},
  ISSN={2375-9259},
  month={Dec},}@INPROCEEDINGS{9673037,
  author={Amini, M. Mohammad and Aldanondo, M. and Vareilles, E. and Coudert, T.},
  booktitle={2021 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)}, 
  title={Twenty Years of Configuration Knowledge Modeling Research. Main Works, What To Do Next?}, 
  year={2021},
  volume={},
  number={},
  pages={1328-1332},
  abstract={A configuration software (configurator) associates a knowledge base (KB) with a knowledge processing unit (PU). The KB describes all possible combinations of components while the PU overlays this knowledge with the customer requirements. Our work deals with the KB and the approaches, models, or tools for modeling configuration knowledge. Our goal is to present a small quantitative literature survey highlighting two work streams: the first one gathers modeling works dealing with constraint-based approaches while the second deals with ontologies, description logic, or object-oriented modeling approach. We will also consider hybrid approaches. We will present a quantitative analysis of published materials in Web of science over the last twenty years. The keywords occurrence versus time will also be studied in detail to identify tendencies in configuration knowledge modeling.},
  keywords={Knowledge engineering;Statistical analysis;Object oriented modeling;Engineering management;Knowledge based systems;Ontologies;Maintenance engineering;Configuration knowledge modeling;constraints satisfaction problem;ontology;UML;OWL;rules},
  doi={10.1109/IEEM50564.2021.9673037},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9669634,
  author={An, Ying and Zhang, Haojia and Sheng, Yu and Wang, Jianxin and Chen, Xianlai},
  booktitle={2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={MAIN: Multimodal Attention-based Fusion Networks for Diagnosis Prediction}, 
  year={2021},
  volume={},
  number={},
  pages={809-816},
  abstract={Predicting the future diagnoses from patients’ historical Electronic Health Records (EHR) is a significant task in healthcare. EHR consist of multiple modal data, each modality has different features and contains a wealth of information of patients. However, most of the existing EHR-based prediction methods either only use unimodal data, or fail to fully explore the correlation between different modalities when fusing multimodal data. To address these challenges, we propose a Multimodal Attention-based fusIon Networks (MAIN) for diagnosis prediction. In this model, we first design different feature extraction modules for each modality. Then, an inter-modal correlation module which contains two layers is applied to capture the intermodal correlation. Finally, a multimodal fusion module based on weighted averaging is utilized to integrate the representations derived from different modalities and their correlation to obtain the patient representation for diagnosis prediction. We evaluate our proposed model on two medical datasets, and the experimental results demonstrate the effectiveness of MAIN.},
  keywords={Correlation;Conferences;Medical services;Prediction methods;Ontologies;Feature extraction;Task analysis;Electronic Health Records;diagnosis prediction;multimodal fusion;attention mechanism},
  doi={10.1109/BIBM52615.2021.9669634},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9672080,
  author={Parolin, Erick Skorupa and Hu, Yibo and Khan, Latifur and Osorio, Javier and Brandt, Patrick T. and D’Orazio, Vito},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)}, 
  title={CoMe-KE: A New Transformers Based Approach for Knowledge Extraction in Conflict and Mediation Domain}, 
  year={2021},
  volume={},
  number={},
  pages={1449-1459},
  abstract={Knowledge discovery and extraction approaches attract special attention across industries and areas moving toward the 5V Era. In the political and social sciences, scholars and governments dedicate considerable resources to develop intelligent systems for monitoring, analyzing and predicting conflicts and affairs involving political entities across the globe. Such systems rely on background knowledge from external knowledge bases, that conflict experts commonly maintain manually. The high costs and extensive human efforts associated with updating and extending these repositories often compromise their correctness of. Here we introduce CoMe-KE (Conflict and Mediation Knowledge Extractor) to extend automatically knowledge bases about conflict and mediation events. We explore state-of-the-art natural language models to discover new political entities, their roles and status from news. We propose a distant supervised method and propose an innovative zero-shot approach based on a dynamic hypothesis procedure. Our methods leverage pre-trained models through transfer learning techniques to obtain excellent results with no need for a labeled data. Finally, we demonstrate the superiority of our method through a comprehensive set of experiments involving two study cases in the social sciences domain. CoMe-KE significantly outperforms the existing baseline, with (on average) double of the performance retrieving new political entities.},
  keywords={Knowledge based systems;Social sciences;Transfer learning;Natural languages;Big Data;Transformers;Knowledge discovery;knowledge base construction;knowledge extraction;ontologies;link and graph mining;transfer-learning;natural language processing;web search and mining;semantic-based data mining;CAMEO},
  doi={10.1109/BigData52589.2021.9672080},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9671503,
  author={Zhao, Xintong and Greenberg, Jane and McClellan, Scott and Hu, Yong-Jie and Lopez, Steven and Saikin, Semion K. and Hu, Xiaohua and An, Yuan},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)}, 
  title={Knowledge Graph-Empowered Materials Discovery}, 
  year={2021},
  volume={},
  number={},
  pages={4628-4632},
  abstract={In this position paper, we describe research on knowledge graph-empowered materials science prediction and discovery. The research consists of several key components including ontology mapping, materials data annotation, and information extraction from unstructured scholarly articles. We argue that although big data generated by simulations and experiments have motivated and accelerated the data-driven science, the distribution and heterogeneity of materials science-related big data hinders major advancements in the field. Knowledge graphs, as semantic hubs, integrate disparate data and provide a feasible solution to addressing this challenge. We design a knowledge-graph based approach for data discovery, extraction, and integration in materials science.},
  keywords={Materials science and technology;Vocabulary;Technological innovation;Semantics;Prototypes;Transforms;Big Data;Knowledge Graph;Materials Discovery;Information Extraction;Ontology;Natural Language Processing},
  doi={10.1109/BigData52589.2021.9671503},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9671788,
  author={Eggleston, Chloe and Abramson, Jeremy},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)}, 
  title={Woolery: Extending Frame Semantics to Structured Documents}, 
  year={2021},
  volume={},
  number={},
  pages={5597-5601},
  abstract={This paper presents Woolery, a system for semantic annotation and mapping of structured documents (such as JSON key-value pairs) to FrameNet. Implemented as a graphical interface, Woolery provides an annotator with a guided means to map keys in a JSON document to FrameNet elements, without the need for extensive knowledge of FrameNet's semantic structures. Candidate frame elements are identified via a search across FrameNet's internal representations, or via mapping keys to their potential WordNet synsets. Final element selection is automated via a pretrained language model. Initial results are promising, with the model giving an overall accuracy of 77.8% when labeling frames across a diverse corpus of JSON document schemas.},
  keywords={Annotations;Conferences;Semantics;Big Data;Labeling;FrameNet;natural language processing;annotation;lexical databases;JSON;ontology alignment;computational semantics},
  doi={10.1109/BigData52589.2021.9671788},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9643789,
  author={Jeusfeld, Manfred A. and Frank, Ulrich},
  booktitle={2021 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Unifying multi-level modeling: A position paper}, 
  year={2021},
  volume={},
  number={},
  pages={536-540},
  abstract={Multi-level modeling (MLM) as part of object-oriented modeling aims at fully utilizing the expressive power of multiple abstraction levels. While these levels where initially used to define domain-specific modeling languages, i.e. for linguistic purposes, the MLM community has long argued that there is much more to gain by tapping into ontological abstraction levels. While MLM is a rather specialized research field, there are now quite a number of different proposals. There is thus an opportunity to develop a uniform core of MLM that then possibly can become part of a standard and be taken up by the larger modeling community.},
  keywords={Limiting;Object oriented modeling;Education;Linguistics;Solids;Reflection;Model driven engineering;multi-level modeling;conceptual modeling;research agenda},
  doi={10.1109/MODELS-C53483.2021.00083},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9641021,
  author={K C, Hitha and V K, Kiran},
  booktitle={2021 Fifth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)}, 
  title={Topic Recognition and Correlation Analysis of Articles in Computer Science}, 
  year={2021},
  volume={},
  number={},
  pages={1115-1118},
  abstract={Topic identification and similarity detection are two related essential task in data mining, information retrieval, and bibliometric data analysis, which aims to identify significant topics and to find similarity between text collections.It is an essential activity to identify research papers according to their research topics to enhance their retrievability, help create smart analytics, and promote a range of approaches to evaluating the research environment and making sense of it.The proposed frame work deals with three main steps: text extraction, topic identification, and similarity detection.The PyPDF2 module is used to extract text from pdf file. CSO classifier is used for topic identification and similarity between documents is calculated using different models, such as Tf-Idf, Bert, Glove, Word2vec, and Doc2vec.and compared these models with respect to cosine similarity and Eucleadian distance obtained from these models.},
  keywords={Semantic search;Computational modeling;Manuals;Euclidean distance;Ontologies;Syntactics;Portable document format;PyPDF2 module;CSO Classifier;Tf-Idf;Bert;Glove;Word2Vec;Doc2Vec},
  doi={10.1109/I-SMAC52330.2021.9641021},
  ISSN={2768-0673},
  month={Nov},}

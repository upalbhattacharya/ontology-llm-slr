@ARTICLE{11151739,
  author={Garimella, Ritvik and Yip, Hong Yung and Venkataramanan, Revathy and Sheth, Amit P.},
  journal={IEEE Internet Computing}, 
  title={Building Multimodal Knowledge Graphs: Automation for Enterprise Integration}, 
  year={2025},
  volume={29},
  number={3},
  pages={76-84},
  abstract={As enterprises increasingly aim to incorporate artificial intelligence into their workflows to tackle complex, multimodal tasks, the demand for intelligent, robust, and trustworthy systems is paramount. While multimodal large language models offer initial capabilities for processing diverse data streams, their dependence on embedding-based representations limit their effectiveness in delivering semantically grounded explanations and reasoning, as well as qualities essential for enterprise-grade applications. Neurosymbolic approaches provide a promising alternative by enabling traceable, context-aware decision making. However, constructing enterprise-level multimodal knowledge graphs (MMKGs) that enable neurosymbolic approaches remains largely impractical. Although prior efforts have explored MMKG construction, they fall short in addressing the scalability, modularity, and integration that are necessary for any enterprise-grade application. We present a fully automated MMKG construction framework tailored to real-world enterprise environments. Our system features a modular, self-refining lifecycle with a support for human-in-the-loop feedback, enabling scalable, cost-effective, and task-aligned MMKG generation. We demonstrate the practical value of our framework through a real-world case study, showcasing its ability to transform unstructured multimodal data into actionable, semantically grounded knowledge assets for enterprise use.},
  keywords={Scalability;Large language models;Decision making;Semantics;Knowledge graphs;Transforms;Human in the loop;Cognition;Internet;Iterative methods;Artificial intelligence;Multimodal sensors},
  doi={10.1109/MIC.2025.3588546},
  ISSN={1941-0131},
  month={May},}@INPROCEEDINGS{11131908,
  author={Đurić, B. Okreša and Carrascosa, C.},
  booktitle={2025 MIPRO 48th ICT and Electronics Convention}, 
  title={A Framework for Ontology-Driven Multiagent System Generation}, 
  year={2025},
  volume={},
  number={},
  pages={198-203},
  abstract={This paper presents a novel framework for developing multiagent systems using ontology-based modelling consisting of an ontology and an executable module to translate ontology to implementation templates. It systematically maps concepts and relationships to agent roles, behaviours, and interactions by capturing domain knowledge in an ontology. Furthermore, it automatically generates implementation templates for the modelled multiagent system. This way, the need for extensive manual coding may be somewhat reduced, ensuring that conceptual integrity is maintained throughout development. A case study showcases how the proposed framework may speed up the design cycle, reduce coding errors, and enhance system extensibility. Using ontology-driven generation ensures consistency and interoperability, as each agent artefact follows the established domain semantics. This approach may be especially beneficial in complex, knowledge-intensive environments that require collaborative and distributed solutions. Ultimately, it may lead to a flexible and scalable process from domain modelling to multiagent implementation. Our framework provides a way that might help bridge the gap between conceptual design and agent-based execution, streamlining multiagent system development and ensuring robust alignment with domainspecific ontologies.},
  keywords={Semantics;Collaboration;Manuals;Ontologies;Encoding;Software;Extensibility;Artificial intelligence;Interoperability;Multi-agent systems;multiagent system;ontology engineering;software frameworks;knowledge representation;artificial intelligence},
  doi={10.1109/MIPRO65660.2025.11131908},
  ISSN={1847-3938},
  month={June},}@INPROCEEDINGS{11137761,
  author={Paik, Incheon},
  booktitle={2025 International Technical Conference on Circuits/Systems, Computers, and Communications (ITC-CSCC)}, 
  title={Integrating Ontology Rules with Large Language Models for Enhanced Reasoning}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Ontology-based reasoning enables structured inference and logical consistency in specialized domains. However, traditional ontology reasoning frameworks rely heavily on predefined inference engines and XML-based structured data, limiting their integration with Large Language Models (LLMs). Despite their strong general reasoning capabilities, LLMs struggle with structured ontological knowledge due to their reliance on unstructured text.This study proposes a novel approach to integrating ontology-based reasoning into LLMs by transforming ontological concepts and rules into structured natural language. By systematically defining ontology components—such as classes, properties, and rules—using a natural language-friendly format, we enable LLMs to process and infer knowledge from domain-specific ontologies without dedicated inference engines. The feasibility of this transformation is demonstrated through experiments on a domain-specific cocktail ontology. Our findings highlight the potential of natural language-driven ontology representation in bridging the gap between rule-based inference and flexible LLM reasoning.},
  keywords={Computers;Limiting;Large language models;Scalability;Natural languages;Computer architecture;Ontologies;Cognition;Engines;Optimization;Ontology;Large Language Model;Ontology Reasoning;Ontology in Natural Language;RAG},
  doi={10.1109/ITC-CSCC66376.2025.11137761},
  ISSN={2997-741X},
  month={July},}@INPROCEEDINGS{11127920,
  author={Saucedo, Mario A.V. and Viswanathan, Vignesh Kottayam and Kanellakis, Christoforos and Nikolakopoulos, George},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Estimating Commonsense Scene Composition on Belief Scene Graphs}, 
  year={2025},
  volume={},
  number={},
  pages={2861-2867},
  abstract={This work establishes the concept of commonsense scene composition, with a focus on extending Belief Scene Graphs by estimating the spatial distribution of unseen objects. Specifically, the commonsense scene composition capability refers to the understanding of the spatial relationships among related objects in the scene, which in this article is modeled as a joint probability distribution for all possible locations of the semantic object class. The proposed framework includes two variants of a Correlation Information (CECI) model for learning probability distributions: (i) a baseline approach based on a Graph Convolutional Network, and (ii) a neuro-symbolic extension that integrates a spatial ontology based on Large Language Models (LLMs). Furthermore, this article provides a detailed description of the dataset generation process for such tasks. Finally, the framework has been validated through multiple runs on simulated data, as well as in a real-world indoor environment, demonstrating its ability to spatially interpret scenes across different room types. For a video of the article, showcasing the experimental demonstration, please refer to the following link: https://youtu.be/f0tqtPVFZ2A},
  keywords={Graphical models;Graph convolutional networks;Large language models;Semantics;Ontologies;Probability distribution;Indoor environment;Robotics and automation;Videos;Distribution functions},
  doi={10.1109/ICRA55743.2025.11127920},
  ISSN={},
  month={May},}@INPROCEEDINGS{11139975,
  author={Han, Qichang and Zhang, Juan and Bi, Lie and Lu, Xi},
  booktitle={2025 4th International Symposium on Robotics, Artificial Intelligence and Information Engineering (RAIIE)}, 
  title={A Reasoning Method for Micro Assembly Sequence Based on Knowledge Graph and Large Language Model}, 
  year={2025},
  volume={},
  number={},
  pages={216-224},
  abstract={For the assembly of multi-species laser inertial confinement fusion microtargets, micro-assembly robots cannot adaptively respond to changes in the structure of the microtargets and generate assembly sequences autonomously, and manual intervention is needed to reset the assembly process, so there are the problems of autonomous configuration capability and low rapid response capability of the robot system for the assembly of new microtargets and the problem of low response capability, which can not satisfy the needs of small batch and multi-species assembly and rapid response. For this reason, this paper introduces Large-scale Language Model (LLM) and Knowledge Graph (KG) in the assembly sequence generation of micro-assembly robots, and puts forward the method of "Thinking with Knowledge Graph" (TwKG). By integrating external knowledge such as manual assembly experience, a priori information of parts, and robot information, the method enables the micro-assembly robot to generate assembly sequences autonomously, optimize the sequences based on assembly experience, and update the sequences autonomously in case of environmental changes. Since the sequences directly generated by this method cannot be directly used for device control, a Reactive Behavioral Trees method (RBT) based on dynamic behavioral trees is designed based on the analysis of rationality relationships among assembly sequences for automatically creating and updating assembly sequences to cope with abnormal situations. Finally, the effectiveness of the proposed method in improving assembly efficiency and sequence rationality is verified by comprehensive tests in the equipment platform.},
  keywords={Knowledge engineering;Training;Visualization;Large language models;Knowledge graphs;Manuals;Cognition;Planning;Assembly;Robots;assembly sequence planning;information fusion;knowledge graph;large language modeling;micro-target},
  doi={10.1109/RAIIE65740.2025.11139975},
  ISSN={},
  month={June},}@INPROCEEDINGS{11129570,
  author={Sridharan, Hayagriv and Sarkar, Saurabh and Paul, Parag and Cheng, Eugene},
  booktitle={2025 IEEE 11th International Conference on Big Data Computing Service and Machine Learning Applications (BigDataService)}, 
  title={Toward a Hybrid Ontology Framework for Semantic Data Understanding in AI-Augmented Data Warehousing}, 
  year={2025},
  volume={},
  number={},
  pages={174-175},
  abstract={The growing complexity of hybrid data infrastructures, combining structured, semi-structured, and unstructured sources, poses significant challenges for achieving semantic data understanding in enterprise environments. While Large Language Models (LLMs) offer promising capabilities for data exploration, their effectiveness is limited without a unified semantic layer that bridges business meaning and technical metadata. We present a hybrid ontology framework that integrates deterministic schema metadata (such as tables, keys, and ER diagrams) with flexible semantic enrichment derived from documentation, ETL scripts, and business glossaries. This framework enables the construction of a layered knowledge graph aligned with metadata catalogs like OpenMetadata and DataHub, supporting both metadata ingestion and semantic feedback loops. Although named an ontology, our prototype is a Neo4j like property graph; conversion to RDF/OWL is deferred because SHACL (Shapes Constraint Language) normalization is nontrivial, our graph uses rich property types, while OWL/RDF requires explicit classes/relations. Our implementation demonstrates improvements in entity disambiguation, catalog enrichment, and cross-system understanding.},
  keywords={Shape;Semantics;Warehousing;Prototypes;Knowledge graphs;Machine learning;Ontologies;Metadata;Big Data;Business;Ontology;Knowledge Graph;Hybrid Infrastructure;Big Data;Data Warehouse;Semantic Retrieval Techniques;Semantic Data Understanding;Metadata Catalog},
  doi={10.1109/BigDataService65758.2025.00033},
  ISSN={2690-828X},
  month={July},}@INPROCEEDINGS{11139242,
  author={Ospan, Assel and Mussa, Aman and Mansurova, Madina and Sarsembayeva, Talshyn},
  booktitle={2025 IEEE 5th International Conference on Smart Information Systems and Technologies (SIST)}, 
  title={LLM Agents for Enhanced Tabular Data Interpretation: A Perspective}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The task of interpreting tabular data semantically is central to domains ranging from biomedical research to finance, where structured tables must be linked to rich domain knowledge. Ontology-driven methods, such as the iterative approaches could provide a structured, interpretable framework for entity recognition and semantic enrichment. However, these methods often struggle with handling ambiguous terms, evolving taxonomies, and rapid domain shifts. Simultaneously, recent advances in Large Language Models (LLMs) have demonstrated remarkable flexibility and context-aware reasoning capabilities, making them attractive complementary tools. In this perspective paper, we review the ontology-driven semantic analysis landscape, highlight its limitations, and discuss how LLM-based agents can address these challenges. We then outline a conceptual framework for integrating LLM reasoning with ontology-driven pipelines, enabling dynamic ontology extension, robust entity disambiguation, and scalable cross-domain adaptation. By bridging symbolic knowledge structures with the adaptive intelligence of LLMs, we propose a pathway to more flexible, accurate, and future-proof semantic interpretation of tabular data.},
  keywords={Accuracy;Terminology;Large language models;Semantics;Taxonomy;Ontologies;Cognition;Stability analysis;Tuning;Thermal stability;semantic analysis;OWL ontology;table interpretation;large language model;deep learning},
  doi={10.1109/SIST61657.2025.11139242},
  ISSN={},
  month={May},}@INPROCEEDINGS{11126792,
  author={Yu, Muran and Wang, Jie and Chen, Yirong and Lepech, Micheal D. and Liu, Ying and Law, Kincho H.},
  booktitle={2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Ontology-based Adaptive Knowledge System (OAKS): Adaptive and Consistent Knowledge Acquisition through LLMs for Diverse User Backgrounds}, 
  year={2025},
  volume={},
  number={},
  pages={593-599},
  abstract={Although most of the research on large language models (LLMs) focuses on their development and validation against datasets, significant gaps remain in their application to real-world knowledge-intensive tasks. This research addresses key challenges in using LLMs for extracting and synthesizing knowledge from unstructured sources, focusing on applications where the validity and consistency of the results are critical. We propose Ontology-based Adaptive Knowledge System (OAKS) as a holistic approach to manage the complexities of acquiring unstructured knowledge, varying user expertise, and dynamic query formulation. This research provides practical value for enabling the domain user community to leverage their technical documentation and expertise and accelerate ongoing working projects through improved literature review and cross-disciplinary insight discovery. Validated through empirical studies, our findings offer insight into best practices for the deployment of OAKS, bridging the gaps between AI capabilities and real-world needs in knowledge acquisition and research.},
  keywords={Adaptive systems;Knowledge acquisition;Knowledge based systems;Semantics;Ontologies;Software;Real-time systems;Reliability;Problem-solving;Systematic literature review;Human-LLM interaction;knowledge acquisition;nonmonotonic reasoning;ontology-based dynamic query},
  doi={10.1109/COMPSAC65507.2025.00081},
  ISSN={2836-3795},
  month={July},}@INPROCEEDINGS{11126748,
  author={Chowdhury, Suparno Roy and Desai, Ayush and Kapure, Mrunal and Shikalgar, Mustakim and Venugopal, Ramchander and Bansal, Srividya},
  booktitle={2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Enhanced tracking and reporting of missing persons using Knowledge Graph and Ontology Engineering}, 
  year={2025},
  volume={},
  number={},
  pages={619-627},
  abstract={The issue of missing persons remains a significant concern in the United States, with thousands of cases reported annually. Addressing this challenge requires innovative methods to integrate and analyze disparate data sources to uncover patterns in disappearances. This research explores the potential of Ontology Engineering and Knowledge Graphs to provide a structured and interconnected perspective on missing person data. Ontologies enable the representation and linking of real-world entities within a unified framework, facilitating more meaningful data relationships.In this study, a knowledge graph is constructed to capture key details about missing persons, including the circumstances of their disappearance, personal characteristics, and last known locations. Additionally, a Large Language Model (LLM) is integrated to summarize query results, enhancing data interpretation. This knowledge graph serves as a foundation for advanced data analysis, enabling real-time insights, supporting proactive interventions, and aiding in case resolution.},
  keywords={Semantic Web;Data analysis;Databases;Large language models;Soft sensors;Data integration;Knowledge graphs;Ontologies;Real-time systems;Open data;Semantic web;Ontologies;Knowledge Graphs;Data Integration;Missing people;Risk factors;NamUS;LLM},
  doi={10.1109/COMPSAC65507.2025.00084},
  ISSN={2836-3795},
  month={July},}@INPROCEEDINGS{11126672,
  author={Busany, Nimrod and Hadar, Ethan and Hadad, Hananel and Rosenblum, Gil and Maszlanka, Zofia and Akhigbe, Okhaide and Amyot, Daniel},
  booktitle={2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Automating Business Intelligence Requirements with Generative AI and Semantic Search}, 
  year={2025},
  volume={},
  number={},
  pages={1891-1898},
  abstract={Eliciting Business Intelligence (BI) requirements is challenging, especially in dynamic business environments. This paper introduces AutoBIR, an AI-driven system that uses semantic search and Large Language Models (LLMs) to automate BI specification and prototyping. Through a conversational interface, it translates user inputs into analytic code, descriptions, and data dependencies while generating test-case reports with optional visuals. AutoBIR refines BI reporting via feedback, accelerating data-driven decision-making. We also explore the broader potential of generative AI in transforming BI development, illustrating its role in enhancing data engineering practice for large-scale, evolving systems.},
  keywords={Visualization;Translation;Codes;Semantic search;Generative AI;Large language models;Decision making;Data engineering;Software;Business intelligence;Business Intelligence;Generative AI;Large Language Models;AI-Driven Data Engineering;Requirements;Semantic Search;Ontology-Based Query Generation;Prototyping;Text-to-SQL},
  doi={10.1109/COMPSAC65507.2025.00260},
  ISSN={2836-3795},
  month={July},}@INPROCEEDINGS{11123952,
  author={de Villiers, J.P. and de Freitas, A. and Jousselme, A-L. and Kaplan, L. and Blasch, E. and Laudy, C. and Costa, P. C.},
  booktitle={2025 28th International Conference on Information Fusion (FUSION)}, 
  title={Evaluation of LLM Reasoning Under Uncertainty: An Atomic Comparison to Normative Approaches}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Evaluating uncertainty in large language model (LLM) reasoning is challenging due to their vast parameter space, abstract knowledge representation, and limited transparency regarding training data. While normative formalisms, such as deductive logic, clearly define sound reasoning in the absence of uncertainty, reasoning under uncertainty admits multiple approaches, including probabilistic reasoning (e.g. Bayesian), belief function reasoning (e.g. Dempster-Shafer), or fuzzy logic, to name a few. This paper examines how LLMs handle uncertainty by analyzing outcomes based on an atomic fusion and reasoning problem. We establish a point of reference using the simplest of fusion topologies to facilitate transparency and understanding of how LLMs align with established theories. The reasoning approaches of different LLMs with varying complexities are compared to established normative frameworks, providing insights into which formalism best aligns with LLM reasoning and assessing its soundness and consistency. A deviation function for assessment is developed, and the results indicate that the tested LLMs' reasoning under uncertainty does not consistently align with established theories, even for the simplest information fusion topologies. These preliminary results form the basis for further investigations and LLM refinements.},
  keywords={Measurement;Uncertainty;Large language models;Training data;Reliability theory;Ontologies;Probabilistic logic;Cognition;Topology;Standards;Large language models;reasoning under uncertainty;URREF;evaluation;evaluation criteria;reasoning models},
  doi={10.23919/FUSION65864.2025.11123952},
  ISSN={},
  month={July},}@INPROCEEDINGS{11129447,
  author={Cheng, Yutong and Bajaber, Osama and Tsegai, Saimon Amanuel and Song, Dawn and Gao, Peng},
  booktitle={2025 IEEE 10th European Symposium on Security and Privacy (EuroS&P)}, 
  title={CTINexus: Automatic Cyber Threat Intelligence Knowledge Graph Construction Using Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={923-938},
  abstract={Textual descriptions in cyber threat intelligence (CTI) reports, such as security articles and news, are rich sources of knowledge about cyber threats, crucial for organizations to stay informed about the rapidly evolving threat landscape. However, current CTI knowledge extraction methods lack flexibility and generalizability, often resulting in inaccurate and incomplete knowledge extraction. Syntax parsing relies on fixed rules and dictionaries, while model fine-tuning requires large annotated datasets, making both paradigms challenging to adapt to new threats and ontologies. To bridge the gap, we propose CTINexus, a novel framework leveraging optimized in-context learning (ICL) of large language models (LLMs) for data-efficient CTI knowledge extraction and high-quality cybersecurity knowledge graph (CSKG) construction. Unlike existing methods, CTINexus requires neither extensive data nor parameter tuning and can adapt to various ontologies with minimal annotated examples. This is achieved through: (1) a carefully designed automatic prompt construction strategy with optimal demonstration retrieval for extracting a wide range of cybersecurity entities and relations; (2) a hierarchical entity alignment technique that canonicalizes the extracted knowledge and removes redundancy; (3) an long-distance relation prediction technique to further complete the CSKG with missing links. Our extensive evaluations using 150 real-world CTI reports collected from 10 platforms demonstrate that CTINexus significantly outperforms existing methods in constructing accurate and complete CSKG, highlighting its potential to transform CTI analysis with an efficient and adaptable solution for the dynamic threat landscape.},
  keywords={Large language models;Redundancy;Knowledge graphs;Transforms;Organizations;Ontologies;Syntactics;Cyber threat intelligence;Computer security;Tuning;Cyber Threat Intelligence;Large Language Model;In-Context Learning;Cybersecurity Knowledge Graph},
  doi={10.1109/EuroSP63326.2025.00057},
  ISSN={2995-1356},
  month={June},}@INPROCEEDINGS{11120386,
  author={AlShomar, Ahmad and Supakkul, Sam and Bao Pham, To Kim and Hill, Tom and Chung, Lawrence},
  booktitle={2025 IEEE International Conference on Software Services Engineering (SSE)}, 
  title={Teaching LLMs Non-Functional Requirements Modeling: A Grammar and RAG Approach}, 
  year={2025},
  volume={},
  number={},
  pages={57-66},
  abstract={A picture is worth a thousand words. Non-Functional Requirements (NFRs), such as security and usability, are modeled using Softgoal Interdependency Graphs (SIGs) to capture potential conflicts and synergies. However, the practice of NFR modeling remains limited, partly due to unfamiliarity with modeling languages like SIG and insufficient understanding of relevant NFRs. Large Language Models (LLMs) show some knowledge of NFRs and SIG concepts, such as goal decomposition and operationalization, but often lack precise knowledge of formal SIG syntax. We introduce SIG-GPT, a GPT-4-based LLM augmented with SIG knowledge using text-based grammar supplied and Retrieval Augmented Generation (RAG). RAG enhancs LLM responses by retrieving relevant external knowledge, while the grammar enforces correct syntax, guiding the LLM to generate SIGs align with formal notation. To help practitioners better understand SIG modeling, reduce time and effort, and enhance NFR proficiency, we apply textual grammar to SIG-GPT, ensuring it is ready for seamless integration with visual modeling tools like RE- Tool, enabling the LLM to generate correct SIG structures without requiring a large dataset of SIG examples. Results show that SIG-GPT with grammar and RAG achieves 100% syntactic accuracy, 95% semantic accuracy, and 98% cohesion (CCR) while aligning with Bloom's Taxonomy to enhance structured reasoning in SIG modeling.},
  keywords={Visualization;Accuracy;Large language models;Retrieval augmented generation;Taxonomy;Semantics;Syntactics;Software;Grammar;Usability;Non-Functional Requirements (NFRs);Softgoal Interdependency Graph (SIG);Retrieval Augmented Generation (RAG);Large Language Models (LLMs);Visual Modeling Languages;Goal Modeling and Goal-oriented Requirement Language},
  doi={10.1109/SSE67621.2025.00016},
  ISSN={},
  month={July},}@INPROCEEDINGS{11107463,
  author={Goel, Mansi and Andres, Frederic},
  booktitle={2025 IEEE 41st International Conference on Data Engineering Workshops (ICDEW)}, 
  title={A Multilingual Ontology and Knowledge Graph for Recipes}, 
  year={2025},
  volume={},
  number={},
  pages={282-288},
  abstract={In recent years, the fusion of artificial intelligence and semantic web technologies has paved the way for innovative approaches to managing and utilizing information. With the growing demand for structured gastronomical data, there is a need for well-defined ontologies that facilitate recipe organization, ingredient classification, nutritional insights, and personalized diet recommendations. This article presents a multilingual recipe ontology and knowledge graph, capturing critical relationships between ingredients, nutrition, cooking actions, and recipe planning. Our ontology supports interoperability across different languages (English, Hindi, and Japanese), facilitating AI applications such as ingredient substitution and personalized recommendations. The proposed knowledge graph leverages semantic web technologies to enhance structured data accessibility and machine-readable representations, ultimately contributing to computational gastronomy and food informatics.},
  keywords={Semantic Web;Redundancy;Knowledge graphs;Ontologies;Cognition;Real-time systems;Multilingual;Planning;Informatics;Interoperability;Ontology Construction;Recipe Ontology;Knowledge Graph;Multilingual Data;Food Informatics},
  doi={10.1109/ICDEW67478.2025.00040},
  ISSN={2473-3490},
  month={May},}@INPROCEEDINGS{11118904,
  author={Sriharsha, A V and Ganesh, C Sai and Teja, Gudi and Sumanth, K and Sindhuja, G and Ajay, C},
  booktitle={2025 International Conference on Computing Technologies (ICOCT)}, 
  title={Image-to-Label-to-Answer: A Simplified LLM Framework Medical Visual Question Answering}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Medical Visual Question Answering (MVQA) has emerged as a promising approach for assisting clinicians in interpreting complex medical images by providing accurate and context-aware answers to specific visual queries. In this project we introduce an innovative framework, "Image to Label to Answer" (ILA) designed to enhance the efficiency and accuracy of clinical applications in MVQA. The proposed project will involve a setup of integrated image recognition techniques along with advanced natural language processing models that systematically convert the medical images into interpretable labels, which in turn generate the correct answers to clinical questions. The ILA framework will operate in three stages: image labelling, context generation, and answer synthesis. First, the system uses deep learning-based image segmentation and classification models to extract relevant features and assign diagnostic labels to medical images. These labels provide the basis for generating contextual understanding, which is crucial for the subsequent stage. In the context generation phase, the framework uses ontology-based reasoning in combination with language models to interpret the labelled data within the clinical context. As seen, the synthesis module can build full and accurate replies after the contextual understanding created over the input queries.},
  keywords={Visualization;Image segmentation;Accuracy;Image recognition;Optical character recognition;Feature extraction;Question answering (information retrieval);Labeling;Medical diagnostic imaging;Context modeling;Medical Visual Question Answering;Convolutional Neural Networks;Optical Character Recognition;Medical Images;Image Segmentation;Context Generation},
  doi={10.1109/ICOCT64433.2025.11118904},
  ISSN={},
  month={June},}@ARTICLE{11123856,
  author={Gaddafi, Ibtisam El and Zakaria Rashad, Magdi and Abou Eleneen, Amal},
  journal={IEEE Access}, 
  title={A Survey on Modeling of Blockchain Oriented Software Systems and Smart Contracts}, 
  year={2025},
  volume={13},
  number={},
  pages={142397-142415},
  abstract={Modeling is the process of developing models representing software systems from different perspectives using a modeling language that includes graphical notation. This process assists software developers in understanding the system functionality, evaluating design proposals, and documenting the software to be implemented. Blockchain is a popular, decentralized, efficient, and secure technology that enables transparent and tamper-resistant transactions across distributed networks. However, blockchain systems lack a dedicated modeling language to represent Blockchain-Based Applications (BBAs) and related Smart Contracts (SCs). Recent research has introduced adaptations of well-known graphical notations to meet the unique modeling needs of blockchain systems. This paper proposes a structured review of graphical models, techniques, and languages to enhance Blockchain-Oriented Software Engineering (BOSE) modeling. A detailed analysis of 36 studies published between 2018 and 2023 highlights the trends and developments in blockchain modeling, and a classification of modified modeling techniques and languages for BBAs and SCs is presented. A modeling framework that guides blockchain developers in describing, designing, and documenting BBAs and related SCs is developed, and an example is provided to demonstrate how this framework is used. These insights can help software engineers select suitable modeling strategies to improve the reliability and quality of BBAs.},
  keywords={Unified modeling language;Blockchains;Adaptation models;Surveys;Smart contracts;Decentralized applications;Software systems;Ontologies;Computational modeling;Supply chain management;Blockchain;modeling languages;modeling;dApp design},
  doi={10.1109/ACCESS.2025.3598627},
  ISSN={2169-3536},
  month={},}@ARTICLE{11124350,
  author={Triantafyllopoulos, Andreas and Tsangko, Iosif and Gebhard, Alexander and Mesaros, Annamaria and Virtanen, Tuomas and Schuller, Björn W.},
  journal={Proceedings of the IEEE}, 
  title={Computer Audition: From Task-Specific Machine Learning to Foundation Models}, 
  year={2025},
  volume={113},
  number={4},
  pages={317-343},
  abstract={Foundation models (FMs) are increasingly spearheading recent advances on a variety of tasks that fall under the purview of computer audition—i.e., the use of machines to understand sounds. They feature several advantages over traditional pipelines: among others, the ability to consolidate multiple tasks in a single model, the option to leverage knowledge from other modalities, and the readily available interaction with human users. Naturally, these promises have created substantial excitement in the audio community and have led to a wave of early attempts to build new, generalpurpose FMs for audio. In the present contribution, we give an overview of computational audio analysis as it transitions from traditional pipelines toward auditory FMs. Our work highlights the key operating principles that underpin those models and showcases how they can accommodate multiple tasks that the audio community previously tackled separately.},
  keywords={Frequency modulation;Acoustics;Machine learning;Automobiles;Foundation models;Computational modeling;Training;Tagging;Ontologies;Market research;Acoustic scene classification;artificial intelligence (AI);audio captioning (AC);computational audio analysis;computer audition;foundation models (FMs);large audio models;machine listening;sound event detection (SED)},
  doi={10.1109/JPROC.2025.3593952},
  ISSN={1558-2256},
  month={April},}@INPROCEEDINGS{11106622,
  author={Alex, Gabriel},
  booktitle={2025 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)}, 
  title={Leveraging Large Language Models for Automated XR Instructional Content Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={This paper presents a study in which authors examine the potential of leveraging large language models to generate instructional content for eXtended Reality environments. Considering the IEEE ARLEM standard as a framework for structuring data, it could be integrated and interpreted by existing authoring tools. In terms of methods, authors have adopted an exploratory approach in testing various strategies. A case study focusing on the use of an eXtended Reality authoring tool for teaching operating procedures is presented. Finally, this exploratory work shows that while simple prompts can produce scenarios with satisfactory quality, imposing a structured schema through more complex prompts leads to less reliable outcomes.},
  keywords={Technological innovation;Authoring systems;Extended reality;Large language models;Focusing;Ontologies;Reliability;Engineering education;Standards;Testing;extended reality;Artificial Intelligence;Large Language Model;ontology;authoring tool;engineering education},
  doi={10.1109/ICE/ITMC65658.2025.11106622},
  ISSN={2693-8855},
  month={June},}@INPROCEEDINGS{11106597,
  author={Wan, Yuwei and Liu, Ying and Zammit, Joseph Paul and Chen, Zheyuan and Li, Li and Francalanza, Emmanuel},
  booktitle={2025 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)}, 
  title={Facilitating Design for Additive Manufacturing with KG-based Retrieval-Augmented Generation}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Additive manufacturing (AM), or 3D printing, enables the production of complex geometries and highly customized components. However, design for AM (DfAM) requires specialised and comprehensive knowledge of process constraints, material behaviours, and performance parameters. While knowledge graphs (KGs) have been utilized to organize and integrate DfAM knowledge, they do not understand natural language and have limited reasoning capabilities, which make them less accessible to non-experts and ineffective in handling complex and context-dependent design queries. Large language models (LLMs) offer powerful language processing and generalizability but suffer from hallucinations when handling specialized domains. To address these limitations, this study proposes a KG-based retrieval-augmented generation (RAG) approach to develop a domain-specific question-answering (Q&A) in DfAM. By integrating structured knowledge from a DfAM KG with the strengths of LLMs, the proposed approach improves response accuracy and relevance. Comparative experiments evaluated LLMs with non-RAG and KG-based RAG using generic and domain-specific metrics. Results demonstrated that KG-based RAG enhances information retrieval and response quality, reduces hallucinations, and ensures alignment with domain knowledge.},
  keywords={Technological innovation;Accuracy;Large language models;Retrieval augmented generation;Pipelines;Knowledge graphs;Production;Three-dimensional printing;Stakeholders;Prompt engineering;Design for additive manufacturing;Additive manufacturing;Retrieval-augmented generation;Large language model;Knowledge graph},
  doi={10.1109/ICE/ITMC65658.2025.11106597},
  ISSN={2693-8855},
  month={June},}@INPROCEEDINGS{11106608,
  author={Büschelberger, Matthias and Tsitseklis, Konstantinos and Morand, Lukas and Zafeiropoulos, Anastasios and Nahshon, Yoav and Papavassiliou, Symeon and Helm, Dirk},
  booktitle={2025 IEEE International Conference on Engineering, Technology, and Innovation (ICE/ITMC)}, 
  title={Digital Products Based on Large Language Models for the Exploration of Graph-Databases in Materials Science and Manufacturing}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={Semantic technologies are gaining traction in materials science and manufacturing. Specifically, the integration of graph databases with ontologies facilitates the harmonization of typically heterogeneous materials and process data, as well as the representation of complex workflows in the field (e.g., processing experimental and simulation data or transferring and tracking data along process chains). This approach enables previously inaccessible data for scientists and engineers to be made available in a FAIR (findable, accessible, interoperable, reusable) manner. On this basis, both science and industry, anticipate a significant boost in materials and process innovation, leading to a more resilient and sustainable production. Nevertheless, one of the main challenges in making semantic technologies usable for engineers is enabling navigation and exploration of the typically complex and flexible graph-based data structures. This work presents two approaches for data exploration in graph-databases using large language models (LLMs), namely LLM-CypherGen and SPARQL-Agent, and their application in two digital products developed within the EU research project DiMAT demonstrated across different use cases in materials science and manufacturing.},
  keywords={Materials science and technology;Ciphers;Technological innovation;Large language models;Knowledge graphs;Production;Ontologies;Syntactics;Manufacturing;Semantic technology;AI;LLM;Knowledge Graph;Material Science;SPARQL;Cypher;Ontology},
  doi={10.1109/ICE/ITMC65658.2025.11106608},
  ISSN={2693-8855},
  month={June},}@INPROCEEDINGS{11094726,
  author={Li, Bozheng and Wu, Yongliang and Lu, Yi and Yu, Jiashuo and Tang, Licheng and Cao, Jiawang and Zhu, Wenqing and Sun, Yuyang and Wu, Jay and Zhu, Wenbo},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={VEU-Bench: Towards Comprehensive Understanding of Video Editing}, 
  year={2025},
  volume={},
  number={},
  pages={13671-13680},
  abstract={Widely shared videos on the internet are often edited. Recently, although Video Large Language Models (Vid-LLMs) have made great progress in general video understanding tasks, their capabilities in video editing understanding (VEU) tasks remain unexplored. To address this gap, in this paper, we introduce VEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark that categorizes video editing components across various dimensions, from intra-frame features like shot size to inter-shot attributes such as cut types and transitions. Unlike previous video editing understanding benchmarks that focus mainly on editing element classification, VEU-Bench encompasses 19 fine-grained tasks across three stages: recognition, reasoning, and judging. To enhance the annotation of VEU automatically, we built an annotation pipeline integrated with an ontology-based knowledge base. Through extensive experiments with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs face significant challenges in VEU tasks, with some performing worse than random choice. To alleviate this issue, we develop Oscars1, a VEU expert model fine-tuned on the curated VEU-Bench dataset. It outperforms existing open-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves performance comparable to commercial models like GPT-4o. We also demonstrate that incorporating VEU data significantly enhances the performance of Vid-LLMs on general video understanding benchmarks, with an average improvement of 8.3% across nine reasoning tasks. The code and data are available at project page},
  keywords={Annotations;Soft sensors;Large language models;Face recognition;Pipelines;Knowledge based systems;Benchmark testing;Performance gain;Cognition;Videos;video editing;video understanding;videollm;video benchmark},
  doi={10.1109/CVPR52734.2025.01276},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{11096639,
  author={Teslya, Nikolay},
  booktitle={2025 IEEE 26th International Conference of Young Professionals in Electron Devices and Materials (EDM)}, 
  title={Using NLP Tools for Linking Materials Within the “Pushkin Digital” Resource}, 
  year={2025},
  volume={},
  number={},
  pages={1540-1543},
  abstract={The development of the scientific and educational resource “Pushkin Digital” requires processing a substantial volume of documents to create an ontology of A. S. Pushkin's literary heritage. The works of A. S. Pushkin and related texts contain mentions of entities, such as historical figure names, geographical locations, dates, and references biographical sources. All these entities are the source of links in the ontology. The paper presents a description of the natural language processing techniques used in processing the materials featured on the Pushkin Digital resource in order to create links between them. The proposed system system utilizes state-of-the-art NLP techniques including BERT for robust named entity recognition, identifying and classifying key entities within the text, SBERT for enabling the system to discern relationships and connections between entities even when expressed with different wording, and LLMs for complex text analysis. Regular expressions are employed for identifying and processing structured text elements, such as dates and bibliographic references, ensuring data consistency and accuracy. This combination of techniques allows for the automated construction of a rich and interconnected ontology, facilitating indepth exploration of Pushkin's literary heritage and its broader cultural significance.},
  keywords={Training;Text mining;Adaptation models;Text analysis;Large language models;Semantics;Named entity recognition;Ontologies;Cultural differences;Information systems;natural language processing;ontology;document analysis;text mining;information extraction},
  doi={10.1109/EDM65517.2025.11096639},
  ISSN={2325-419X},
  month={June},}@INPROCEEDINGS{11096818,
  author={Kotenko, Igor and Abramenko, Georgii},
  booktitle={2025 IEEE 26th International Conference of Young Professionals in Electron Devices and Materials (EDM)}, 
  title={Detecting and Analysing Cyber Attacks Based on Graph Neural Networks, Ontologies and Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={1460-1464},
  abstract={This paper presents an intelligent system to automate the process of detecting and analysing cyber-attacks using Suricata logs, graph neural networks (GNNs), and large language models (LLMs). The proposed approach is based on several key components: collecting and preprocessing network events from Suricata, building an ontological model of attacks using MITRE ATT&CK, applying graph neural networks to identify relationships between events, and finally integrating a language model for dialogue interaction with the operator and generating attack hypotheses. Experimental results demonstrate high accuracy in detecting anomalous network patterns and operator friendliness and indicate the potential for further development of the system for use in high-load and distributed infrastructures.},
  keywords={Accuracy;Large language models;Buildings;Ontologies;Graph neural networks;Computational efficiency;Intelligent systems;Electron devices;Cyberattack;Optimization;LLM;GNN;RAG;process automation;anomaly detection;ontologies;Suricata},
  doi={10.1109/EDM65517.2025.11096818},
  ISSN={2325-419X},
  month={June},}@INPROCEEDINGS{11091706,
  author={Tomasović, Željka and Tomić, Marijana},
  booktitle={2025 10th International Conference on Smart and Sustainable Technologies (SpliTech)}, 
  title={Semantic Usability of Digital Images -Ontology-Based Readability Assessment}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper introduces an ontology-driven method for assessing the semantic usability of digitized images, with a focus on readability and layout readiness, emphasizing practical metrics relevant for text extraction and annotation workflows: Optical Character Recognition (OCR) performance, line segmentation quality, script and annotation compatibility. While previous work has addressed blind Image Quality Assessment (IQA) using perceptual and statistical methods, such approaches often overlook a critical dimension - the semantic usability of images. The method assesses digitized images using a combination of objective metrics (e.g., resolution, noise, compression) and semantic tests (e.g. OCR accuracy, layout analysis) with future goal of implementing metadata completeness, Linked Open Data and International Image Interoperability Framework (LOD/IIIF) compatibility in order to provide a composite usability score. The proposed model is grounded in the Image Authenticity and Quality Ontology (IAQO), a lightweight ontology designed to represent assessment outputs in a standardized, machine-readable format. A dataset of 800 public-domain manuscript images was analyzed, yielding average semantic usability scores ranging from 73.8 to 85.4 (out of 100) across repositories. Results demonstrate that high visual fidelity does not always imply high semantic usability but also the framework’s potential to improve digitizing practices. This approach supports FAIR (Findable, Accessible, Interoperable, Reusable)-compliant image assessment.},
  keywords={Image quality;Image resolution;Annotations;Digital images;Semantics;Optical character recognition;Layout;Ontologies;Cultural differences;Usability;Semantic Usability;Digital Image Quality Assessment;Ontology Modeling;Cultural Heritage Repositories;Digital Humanities;JSON-LD},
  doi={10.23919/SpliTech65624.2025.11091706},
  ISSN={},
  month={June},}@INPROCEEDINGS{11087870,
  author={Heppner, Sebastian and Miny, Torben and Kleinert, Tobias and Zholdybayev, Ayan and Ristin, Marko and van de Venn, Hans Wernher},
  booktitle={2025 IEEE 8th International Conference on Industrial Cyber-Physical Systems (ICPS)}, 
  title={Refining NLP Semantic Matches Through Dialogue with Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Advancements in digitization are fostering innovation within Industrie 4.0 (I4.0) ecosystems. The 2030 vision for I4.0 prioritizes sovereignty, sustainability, and interoperability to transform value chains into dynamic networks. While many physical and syntactical interoperability challenges have been addressed, there are still open questions regarding semantic interoperability. Natural Language Processing (NLP)-based ranking by semantic similarity offers a straightforward approach for matching semantically heterogeneous data but often lacks informative rationale and usability, especially when relevant matches are buried deep in the rankings. In this paper, we explore how we can further improve the ranking to obtain more precise semantical matches by leveraging Large Language Model (LLM), in particular Retrieval-Augmented Generation (RAG). Since comprehensibility is crucial for the end user when selecting the matches, we employ the LLM to additionally explain the rankings and enable follow-up queries. In a series of experiments, we show that this not only improves the usability, but also ensures that users can navigate complex semantic matches effectively.},
  keywords={Industries;Databases;Large language models;Biological system modeling;Semantics;Retrieval augmented generation;Transforms;Natural language processing;Usability;Interoperability;Industrie 4.0;Semantic Matching;Large Language Model;Retrieval-Augmented Generation},
  doi={10.1109/ICPS65515.2025.11087870},
  ISSN={2769-3899},
  month={May},}@BOOK{11099035,
  author={Raieli, Salvatore and Iuculano, Gabriele},
  booktitle={Building AI Agents with LLMs, RAG, and Knowledge Graphs: A practical guide to autonomous and modern AI agents},
  year={2025},
  volume={},
  number={},
  pages={},
  abstract={Master LLM fundamentals to advanced techniques like RAG, reinforcement learning, and knowledge graphs to build, deploy, and scale intelligent AI agents that reason, retrieve, and act autonomouslyKey FeaturesImplement RAG and knowledge graphs for advanced problem-solvingLeverage innovative approaches like LangChain to create real-world intelligent systemsIntegrate large language models, graph databases, and tool use for next-gen AI solutionsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionThis AI agents book addresses the challenge of building AI that not only generates text but also grounds its responses in real data and takes action. Authored by AI specialists with deep expertise in drug discovery and systems optimization, this guide empowers you to leverage retrieval-augmented generation (RAG), knowledge graphs, and agent-based architectures to engineer truly intelligent behavior. By combining large language models (LLMs) with up-to-date information retrieval and structured knowledge, you'll create AI agents capable of deeper reasoning and more reliable problem-solving. Inside, you'll find a practical roadmap from concept to implementation. You’ll discover how to connect language models with external data via RAG pipelines for increasing factual accuracy and incorporate knowledge graphs for context-rich reasoning. The chapters will help you build and orchestrate autonomous agents that combine planning, tool use, and knowledge retrieval to achieve complex goals. Concrete Python examples built on popular libraries, along with real-world case studies, reinforce each concept and show you how these techniques come together. By the end of this book, you’ll be well-equipped to build intelligent AI agents that reason, retrieve, and interact dynamically, empowering you to deploy powerful AI solutions across industries.What you will learnLearn how LLMs work, their structure, uses, and limits, and design RAG pipelines to link them to external dataBuild and query knowledge graphs for structured context and factual groundingDevelop AI agents that plan, reason, and use tools to complete tasksIntegrate LLMs with external APIs and databases to incorporate live dataApply techniques to minimize hallucinations and ensure accurate outputsOrchestrate multiple agents to solve complex, multi-step problemsOptimize prompts, memory, and context handling for long-running tasksDeploy and monitor AI agents in production environmentsWho this book is forIf you are a data scientist or researcher who wants to learn how to create and deploy an AI agent to solve limitless tasks, this book is for you. To get the most out of this book, you should have basic knowledge of Python and Gen AI. This book is also excellent for experienced data scientists who want to explore state-of-the-art developments in LLM and LLM-based applications.},
  keywords={},
  doi={},
  ISSN={},
  publisher={Packt Publishing},
  isbn={9781835080382},
  url={https://ieeexplore.ieee.org/document/11099035},}@INPROCEEDINGS{11091995,
  author={Peng, Yuying and Zhao, Siqi and Luo, Ximeng and Ning, Yongzhi},
  booktitle={2025 7th International Conference on Computer Science and Technologies in Education (CSTE)}, 
  title={Large Language Models and Knowledge Graphs Synergistically Enhancing Personalized Learning}, 
  year={2025},
  volume={},
  number={},
  pages={1115-1119},
  abstract={The rapid development of artificial intelligence technology has promoted the intelligence of the education field. The collaborative work of large language models (LLMs) and knowledge graphs (KGs) can effectively develop personalized learning paths. Personalized learning in programming courses is crucial for meeting students' different needs and proficiency levels. This article proposes a method for personalized teaching of programming courses using LLMs and KGs. The LLMs is used to analyze students' learning status, provide real-time feedback, and generate customized learning materials, while the KGs provides a structured knowledge base to plan essential skills and learning progress, highly integrating the advantages of the LLMs and the logical connections of knowledge from various disciplines. The application results show that designing a knowledge system ontology through disciplinary knowledge analysis, retraining and fine-tuning the LLMs, forms the final personalized knowledge system, improves the accuracy of learning path recommendations and the relevance of generated exercises, and can be easily promoted to other disciplinary fields.},
  keywords={Accuracy;Large language models;Atmospheric modeling;Education;Knowledge based systems;Semantics;Knowledge graphs;Learning (artificial intelligence);Real-time systems;Programming profession;large language models;knowledge graphs;personalized learning;artificial intelligence technology;programming courses},
  doi={10.1109/CSTE64638.2025.11091995},
  ISSN={},
  month={April},}@INPROCEEDINGS{11090152,
  author={Yang, Ye and Duan, Ran and Yi, Xinyang and Ma, Qicheng and Liu, Jie and Hu, Zhongxu and Hu, Youmin},
  booktitle={15th Prognostics and System Health Management Conference (PHM 2025)}, 
  title={Knowledge graph-based dual-modal collaborative QA framework for hydropower operation and maintenance}, 
  year={2025},
  volume={2025},
  number={},
  pages={152-157},
  abstract={Maintenance of hydropower equipment is essential for ensuring a reliable supply of renewable energy. However, in practical maintenance operations, technicians rely heavily on personal experience and extensive domain manuals, which can reduce the accuracy and efficiency of decision-making processes. This paper proposes a knowledge graph-based dual-modal collaborative QA (KGD-QA) framework for hydropower operation and maintenance. Firstly, a multi-granularity segmentation strategy is used to divide domain manuals into thematically grouped text blocks and path-labeled sentence units. Secondly, a LoRA-fine-tuned large language model (LLM), guided by domain ontologies and chain-of-thought prompts, extracts entity-relation triples. Finally, a dual-modal QA mechanism is designed to integrate knowledge graph subgraph queries and text vector retrieval, enabling information acquisition in both detailed and global modes. Experimental results show that, compared with baseline model, this approach improves the accuracy of knowledge extraction while the dual-modal collaborative QA retrieval method provides more traceable and evidence-backed responses to maintenance-related queries. This framework offers a novel paradigm for advancing intelligent operation and maintenance technologies for hydropower equipment.},
  keywords={},
  doi={10.1049/icp.2025.2348},
  ISSN={},
  month={June},}@INPROCEEDINGS{11081553,
  author={He, Bofan and Cheng, Jerry Q. and Gu, Huanying},
  booktitle={2025 IEEE 13th International Conference on Healthcare Informatics (ICHI)}, 
  title={Static and Dynamic Embedding Approaches to Identify Is-A Relations in SNOMED CT}, 
  year={2025},
  volume={},
  number={},
  pages={369-378},
  abstract={Systematized Nomenclature of Medicine - Clinical Terms (SNOMED CT) is a comprehensive clinical terminology with over 350,000 unique concepts and 1 million relationships. It serves as an essential resource for both clinical practice and medical research. Approximately 40% of these relationships are of the is-a type, which is key to classifying concepts as subtypes or subclasses within broader categories and is fundamental for decision support systems and automated reasoning in clinical environments. Identifying is-a relationships in SNOMED CT is crucial for enhancing the accuracy of patient cohort queries, which form the backbone of clinical and research applications. Our study aims to use deep learning-based neural networks to determine whether a relationship between two concepts falls under the is-a or non-is-a categories. This approach can help detect misclassified or undefined is-a concept pairs, ultimately improving the quality of SNOMED CT ontology. We construct a node-edge-node structure for concept pairs and their relationships, which we then split into two categories: is-a and non-is-a classes. In this study, we propose two classification approaches. In the first approach using embeddingBag, the data are mapped into vector representations and used as input for a fully connected neural network to learn the patterns of the is-a relationship. During model training, 80% of the data is used for training and the remaining 20% for testing.We achieved a precision of 0.903, recall of 0.894, and F1 Score of 0.898 in predicting the is-a relation among the associated medical concepts. The second approach involved transformer-based models like BERT(Bidirectional Encoder Representations from Transformers), introduced by Devlin et al. [1], which dramatically advanced various natural language processing (NLP) tasks by offering deeply contextualized word embeddings. And RoBERTa(Robustly optimized BERT approach)[2], both BERT and RoBERTa models have significantly advanced the field of NLP. This work uses them as encoders connected to a classifier head to predict is-a or non-is-a categories. We fine-tune our models using a historical SNOMED CT dataset from 2017, while evaluation is carried out on new data introduced after 2023 in the 2024 release. With a recall of 0.933, precision of 0.933, F1 score of 0.932, accuracy of 0.933, and ROC of 0.972, RoBERTa outperforms the other baseline models (Support Vector Machine(SVM), K-Nearest Neighbors(KNN), Naive Bayes, and Multilayer Perceptron(MLP)) across all metrics. This work demonstrates that our implementation can effectively identify unknown is-a relations in future datasets.},
  keywords={Training;Accuracy;Neural networks;Bidirectional control;Ontologies;Transformers;Natural language processing;Encoding;Vectors;Data models;Classification of Multi Sentence;Deep Learning;Information Retrieval;Natural Language Processing;SNOMED-CT;Large Language Model;BERT},
  doi={10.1109/ICHI64645.2025.00050},
  ISSN={2575-2634},
  month={June},}@INPROCEEDINGS{11081548,
  author={Cox, Kyle and Qu, Gang and Hsu, Chi-Yang and Xu, Jiawei and Zhou, Yingtong and Tan, Zhen and Hu, Mengzhou and Chen, Tianlong and Hu, Ziniu and Zhao, Zhongming and Ding, Ying},
  booktitle={2025 IEEE 13th International Conference on Healthcare Informatics (ICHI)}, 
  title={Thought Graph: Balancing Specificity and Uncertainty in LLM-Based Gene Set Annotation}, 
  year={2025},
  volume={},
  number={},
  pages={462-470},
  abstract={Accurate predictive reasoning is a cornerstone of biomedical decision-making, particularly in precision oncology, where elucidating the intricate relationships between disease-risk genes and biological processes is critical. This study presents a novel "Thought Graph" methodology, an advancement of the Tree of Thoughts framework, to systematically generate and refine biological process representations derived from gene sets while addressing the trade-off between specificity and uncertainty. Balancing these factors is essential for robust and interpretable gene set analyses, as it accounts for the complexity, variability, and overlapping functions of biological pathways. Furthermore, we introduce a quantitative metric that integrates specificity and uncertainty, thereby enhancing the rigor and transparency of the inference process. Using a subset of the Gene Ontology database, we evaluate the effectiveness of our system in generating biologically meaningful terms that accurately describe the underlying biological processes of gene sets. We compare its performance against a domain-specific tool (GSEA) and five LLM baselines across multiple metrics. Our system achieves the highest cosine similarity (64.00%) and specificity percentile (96.40%), highlighting its capacity to generate terms closely aligned with human annotations while maintaining a balance between specificity and accuracy. By advancing the artificial intelligence driven analyses, this work facilitates more informed decision-making in biomedical research, precision oncology, and related fields.},
  keywords={Measurement;Uncertainty;Accuracy;Large language models;Decision making;Biological processes;Oncology;Genetics;Cognition;Cancer;Gene set analysis;Large language model;Medical reasoning;Precision oncology;Thought graph},
  doi={10.1109/ICHI64645.2025.00060},
  ISSN={2575-2634},
  month={June},}@INPROCEEDINGS{11082040,
  author={Khalov, Andrey and Ataeva, Olga},
  booktitle={2025 8th International Conference on Artificial Intelligence and Big Data (ICAIBD)}, 
  title={Automatic Mapping of Upper-Level Ontology Classes (DOLCE) and Domain-Specific Ontology ITSMO}, 
  year={2025},
  volume={},
  number={},
  pages={795-802},
  abstract={This paper proposes a method for extending the top-level ontology DOLCE (DOLCE-lite version, referred to as TLO) to the domain of IT services without expert involvement. The main challenge addressed is the automatic mapping of classes in conditions of a small number of objects (<100) and the absence of annotated data. A review of existing approaches is conducted, their limitations are identified, and novel mapping methods are proposed, integrating embeddings and large language models. The suggested method achieved an 82.35% mapping accuracy when integrating DOLCE and ITSMO ontologies. As a result, the ITO-seed ontology was developed, containing linked classes from DOLCE and ITSMO, which can be utilized in further research and in building knowledge graphs for IT Service Management (ITSM) systems.},
  keywords={Reviews;Large language models;OWL;Knowledge graphs;Ontologies;Transformers;Resource description framework;Knowledge management;Graph neural networks;Prompt engineering;Ontology;RDF;OWL;mapping;rdf2vec;owl2vec;BERT;GPT;prompt engineering;clustering;ITIL;DOLCE},
  doi={10.1109/ICAIBD64986.2025.11082040},
  ISSN={2769-3554},
  month={May},}@INPROCEEDINGS{11075342,
  author={Wang, Xi and Shen, Tian and Chen, Xi and Zhang, Jundong},
  booktitle={2025 16th International Conference on E-Education, E-Business, E-Management and E-Learning (IC4e)}, 
  title={Knowledge Graphs Combined with ChatGPT in the Field of Acupuncture: A Case Study on the Treatment of Depression 1}, 
  year={2025},
  volume={},
  number={},
  pages={216-220},
  abstract={This study integrates knowledge graphs (KGs) and large language models (LLMs) to enhance acupuncture-based depression treatment through structured knowledge sharing. We developed a depression-specific KG using traditional acupuncture principles and modern IT, leveraging LLMs to optimize knowledge integration and global accessibility for researchers and clinicians.},
  keywords={Electronic learning;Large language models;Knowledge graphs;Depression;Chatbots;Acupuncture;Acupuncture;Knowledge graph;Depression},
  doi={10.1109/IC4e65071.2025.11075342},
  ISSN={},
  month={April},}@INPROCEEDINGS{11073642,
  author={Neupane, Roshan Lal and Pusapati, Vamsi and Edara, Lakshmi Srinivas and Cheng, Xiyao and Neupane, Kiran and Chintapatla, Harshavardhan and Mitra, Reshmi and Korkali, Mert and Suk Na, Hyeong and Srinivas, Sharan and Calyam, Prasad},
  booktitle={NOMS 2025-2025 IEEE Network Operations and Management Symposium}, 
  title={Securing Inverter-Based Resources via Knowledge-Driven Threat Modeling, Analysis, and Mitigation}, 
  year={2025},
  volume={},
  number={},
  pages={1-9},
  abstract={Inverter-based Resources (IBRs) present unique cybersecurity challenges due to their digital control systems and connection to the electric grid. They rely on digital communications and control systems, making them vulnerable to cyberattacks. These attacks can disrupt grid operations and stability, compromise data, or cause physical damage to equipment. To address these challenges, it is essential to establish robust cybersecurity measures that meet and exceed existing industry standards. In this paper, we describe a comprehensive strategy to bolster the cybersecurity of IBRs through cutting-edge applications and technologies via a cybersecurity framework called “CIBR-Fort”, a knowledge-driven, interoperable, scalable, and manageable framework for modeling, analysis, and mitigation of cyber threats disrupting different components of IBR systems. Our knowledge-driven analysis consists of a fusion of knowledge graphs (KGs) in cybersecurity and the electric grid, achieved through link prediction leveraging Large Language Models (LLMs) and cosine similarity, attributed towards informed decision-making for threat mitigation. The evaluation results show how we can automate LLM-driven link prediction based on the fusion of two distantly separated ontologies, generating a dataset that can be used for scaling via graph learning that can be utilized for further security analyses of IBR systems. In addition, we show our knowledge-driven threat analysis can predict different attacks with 91.88% maximum accuracy. Lastly, we show how we can achieve real-time end-to-end threat mitigation with an average of 40 ms per traffic flow.},
  keywords={Threat modeling;Prevention and mitigation;Large language models;Stability criteria;Retrieval augmented generation;Knowledge graphs;Power system stability;Real-time systems;Computer security;Standards;knowledge graph;cybersecurity;inverter-based resources;large language models;retrieval augmented generation},
  doi={10.1109/NOMS57970.2025.11073642},
  ISSN={2374-9709},
  month={May},}@INPROCEEDINGS{11079470,
  author={Blasch, Erik and Insaurralde, Carlos C.},
  booktitle={2025 IEEE Conference on Cognitive and Computational Aspects of Situation Management (CogSIMA)}, 
  title={Space and Air Traffic Management Situation Awareness with Notices}, 
  year={2025},
  volume={},
  number={},
  pages={192-199},
  abstract={Air traffic management has long been associated with situation awareness, especially supporting pilots in assessment and response to challenging situations. For multi-domain air and space operations, artificial intelligence and recently large language models (LLMs) can increase semantic understanding. In this paper, we focus on LLM aerospace analysis of Notice to Airman (NOTAM) and Notice to Space Operators (NOTSO). The notices afford a communication of the situation that can be extracted as an ontology to support human operators. Results show that LLM-based clustering can facilitate cognitive situation awareness.},
  keywords={Space vehicles;Uncertainty;Large language models;Semantics;Data integration;Ontologies;Orbits;Digital twins;Air traffic control;Data Fusion;Air traffic Management;Space Traffic Management;Uncertainty Ontology},
  doi={10.1109/CogSIMA64436.2025.11079470},
  ISSN={2379-1675},
  month={June},}@INPROCEEDINGS{11063905,
  author={Li, Yi and Lu, Wenxin and Xiang, Xiuzhen},
  booktitle={2025 5th International Conference on Neural Networks, Information and Communication Engineering (NNICE)}, 
  title={OntoCons: A Method for Intelligent Construction of Domain Ontology Models for Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={706-712},
  abstract={With the development of large language models technology, its application in the field of education has been steadily expanding. However, due to limitations in understanding specialized literature in vertical fields, large language models face serious issues of “hallucination” and prior bias when responding to teachers' questions about specialized domains. To address this, this paper proposes an intelligent construction framework for ontology models based on knowledge tuple extraction, called OntoCons, which introduces entity extraction and relation extraction methods into the field of ontology model construction. OntoCons transforms the ontology model construction problem into a knowledge tuple extraction problem, enhancing the transparency and interpretability of ontology model construction methods, and introduces subgraph encoding strategies to improve the accuracy of relation extraction. This research provides a new method for the intelligent construction of ontology models in vertical fields, improving the understanding and response quality of large language models in specialized domains by combining machine learning and manual analysis.},
  keywords={Knowledge engineering;Accuracy;Large language models;Manuals;Machine learning;Transforms;Ontologies;Reliability theory;Encoding;Data mining;large language model;ontology model;knowledge extraction;entity extraction;relation extraction},
  doi={10.1109/NNICE64954.2025.11063905},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{11070747,
  author={Zhou, Liyun},
  booktitle={2025 6th International Conference on Computing, Networks and Internet of Things (CNIOT)}, 
  title={Design and Implementation of a Modern Chinese History QA System Based on Knowledge Graphs}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={To address the challenge of quickly obtaining accurate answers while exploring modern historical knowledge, this study develops an intelligent question-answering system based on knowledge graphs, focusing on modern Chinese history. The system leverages the Robustly Optimized BERT Approach (RoBERTa) model for word embedding construction and integrates an adversarial training mechanism to enhance robustness. BiLSTM is employed to capture the contextual features of input text, while Conditional Random Fields (CRF) are used to generate the optimal prediction sequence. Experimental results demonstrate that the model achieves an accuracy of 92.2%, a recall of 94.2%, and an F1-score of 93.0% on the modern history dataset, significantly outperforming other approaches.},
  keywords={Training;Accuracy;Text recognition;Knowledge graphs;Ontologies;Market research;Conditional random fields;Robustness;Question answering (information retrieval);History;question answering system;knowledge graph;domain ontology;modern Chinese history},
  doi={10.1109/CNIOT65435.2025.11070747},
  ISSN={},
  month={May},}@INPROCEEDINGS{11070706,
  author={Pokkuluri, Kiran Sree and jain, Kirti and Rajya Lakshmi, V Sravani and Pastariya, Rishab and Lathigara, Amit and Navanitha, Dubbaka},
  booktitle={2025 4th OPJU International Technology Conference (OTCON) on Smart Computing for Innovation and Advancement in Industry 5.0}, 
  title={A Unified Knowledge Base for Drug Label Analysis Using Learning Models, NLP, and IoT Tasks}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Linguistic transmission advanced humanity. Voice, text, and pictures affect knowledge transmission. Techniques for interpolating data may retain knowledge by determining semantic equivalents based on its context, the lesson, and connections without sacrificing data integrity. Knowledge has affected the security and privacy of data, showing vulnerability on every side of privacy and security violations with numerous legislation and standards. linguistic models convert voice impulses to digital data to examine phonological, prosodic, phonotactic, and lexical features. In contrast, embedding text in pictures and voice patterns modify word meaning via resolution, simplicity, and pitch adjustment. NLP uses vectors to detect word similarity, ensuring anonymity for ontology-based understanding with compatibility while deciphering control features. Using text-based semantic similarities analysis, visual text is classed by arrangement and topic. Every individual's linguistic dialect is distinct, with a tendency toward the local dialect. Due of rapid modifications to the environment, methods' acoustic qualities are difficult to transmit. Voice translating syllables might or might not communicate word feeling according to phonetical modification. A learnt variables taxonomy with prosody properties helps knowledge bases find relevant material without intermediary stages. For successful information transmission, integration, and edge manufacturing, edge learning requires context. An ontology of learnt connections from text, speech, and pictures shows information resemblance usefulness, and interest. The suggested XTI-CNN technique excels in all four metrics and has 93.2% accuracy, making it ideal for hybrid analysis of data in related to drugs data extraction applications.},
  keywords={Drugs;Data privacy;Visualization;Data integrity;Semantics;Knowledge based systems;Ontologies;Linguistics;Data models;Security;Text Extraction;Context Learning;Semantic Similarity;Privateness;Semantic Reasoning;Voice cloning;Interoperability;Speech recognition;Voice activity detection;Language models;Ontology},
  doi={10.1109/OTCON65728.2025.11070706},
  ISSN={},
  month={April},}@INPROCEEDINGS{11068551,
  author={Johnson, Hamilton and SureshKumar, Mayuranath and Thomas, L. Dale and Kannan, Hanumanthrao},
  booktitle={2025 IEEE Aerospace Conference}, 
  title={Ontological Methods of Functional Analysis for Aerospace Concepts}, 
  year={2025},
  volume={},
  number={},
  pages={1-13},
  abstract={Functional analysis and decomposition are crucial steps in the development of complex aerospace systems, involving the translation of high-level system requirements into high-level functions, their breakdown into lower-level functions, and linking these functions to system elements. Traditionally, these processes are conducted informally, relying on the engineering judgment of developers, which may limit consistency and reduce the potential for automated assistance. To formalize these processes, we propose using an ontology-a structured conceptualization of terms and relations within a domain. This paper presents a function sub-ontology, part of an ongoing ontology development effort in association with the Advanced Concepts Office at Marshall Space Flight Center, aimed at modeling early stage space system concepts. The function sub-ontology defines types of functions, their interactions, and decomposition rules using the Ontological Modeling Language (OML). This structured approach supports automated reasoning, facilitating more consistent and traceable functional decomposition. The ontology's reasoning capabilities are demonstrated through several examples, showcasing its utility in space system concept development. These examples highlight how the ontology can help ensure consistency between functions, determine equivalence of functions, suggest functional decompositions, and recommend necessary system elements. While the ontology shows promise in enhancing functional analysis in aerospace systems, it is a work in progress. Future work will focus on refining the formal theory underpinning the ontology and further expanding its capabilities.},
  keywords={Translation;Observatories;Subject matter experts;Refining;Ontologies;Propulsion;Functional analysis;Cognition;Modeling;Aerospace engineering},
  doi={10.1109/AERO63441.2025.11068551},
  ISSN={2996-2358},
  month={March},}@INPROCEEDINGS{11064138,
  author={Sharma, Devanshi and Das, Asmita},
  booktitle={2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)}, 
  title={LYNX-RNA: A Scalable Nextflow Workflow for RNA-Seq Analysis with Integrated Large Language Models for Comprehensive Result Interpretation}, 
  year={2025},
  volume={3},
  number={},
  pages={334-342},
  abstract={RNA sequencing has become an essential technique in the life sciences, providing a comprehensive methodology. RNA sequencing has become an essential method in the life sciences, providing a comprehensive means to quantitatively evaluate RNA levels in tissues and cells. From alignment to subsequent pathway analysis, the increasing usage of RNA-seq has inspired continuous creation of creative tools for all phases of study. For non-experts especially, it is difficult to use these analytical approaches in a scalable, repeatable, and easily available manner. We have established an intuitive, rapid, efficient, and thorough process for RNA-seq analysis using the workflow management system “Nextflow.” LYNX-RNA (Language-augmented Yield for Nextflow-based RNA eXpression analysis) enhances RNA-seq analysis involves the processing of raw sequencing data through alignment, quality control, and subsequent differential expression and pathway analysis. LYNX-RNA incorporates a distinctive integration with a Large Language Model (LLM) that autonomously produces concise, comprehensible summaries of results, thereby rendering RNA-seq data analysis accessible to a diverse audience, including clinicians, biomedical researchers, pharmaceutical scientists, public health professionals, educators, and students. This enhanced accessibility enables other sectors to extract relevant insights from RNA-seq data without necessitating substantial bioinformatics knowledge. The pipeline was verified with patient-derived xenografts from leukemia and lymphoma, identifying differentially expressed genes and enriched pathways pertinent to disease development and therapy response. LYNX-RNA's applications encompass oncology, virology, immunology, personalized medicine, neuroscience, and agriculture, facilitating biomarker discovery, therapeutic target identification, and sustainable agricultural innovations, thereby rendering RNA-seq analysis both accessible and significant across various fields. Looking forward, LYNX-RNA aims to incorporate multi-omics integration, single-cell and spatial transcriptomics support, and machine learning-based predictive analytics. Planned enhancements include cloud deployment, improved visualization, compatibility with long-read sequencing, and clinical application readiness. These advancements will broaden LYNX-RNA's utility in biomarker discovery, personalized medicine, and agricultural innovations, solidifying its role as a comprehensive RNA-seq analysis tool across various fields.},
  keywords={Sequential analysis;Technological innovation;RNA;Precision medicine;Large language models;Process control;Rendering (computer graphics);Life sciences;Workflow management software;Virology;RNA sequencing (RNA-seq);Peline;Nextflow;LYNX-RNA;large language models (LLM);biomarker discovery;personalized medicine},
  doi={10.1109/ICCSAI64074.2025.11064138},
  ISSN={},
  month={April},}@INPROCEEDINGS{11071598,
  author={Molina, Victor and Ruiz-Celada, Oriol and Suarez, Raul and Rosell, Jan and Zaplana, Isiah},
  booktitle={2025 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)}, 
  title={Robot Situation and Task Awareness Using Large Language Models and Ontologies}, 
  year={2025},
  volume={},
  number={},
  pages={96-103},
  abstract={Robot situation and task awareness requires a deep understanding of the environment, the domain knowledge, and task planning. We present a novel framework that integrates ontologies, Large Language Models (LLMs), and the Planning Domain Definition Language (PDDL) to enhance the comprehension capabilities of robotic systems. The framework employs an LLM to extract structured knowledge from natural language descriptions provided by a human user, populating an OWL ontology that captures relevant objects, properties, and relations. This populated ontology is then used to parse a PDDL Domain file and generate a corresponding PDDL Problem file to solve particular planning problems. This research contributes to the intersection of knowledge representation, natural language processing, and automated planning, providing a solution for intuitive human-robot interaction through LLMs.},
  keywords={Large language models;Conferences;OWL;Human-robot interaction;Ontologies;Natural language processing;Planning;Robots;robotic manipulation;task planning;ontologies;Large Language Models},
  doi={10.1109/DSN-W65791.2025.00045},
  ISSN={2325-6664},
  month={June},}@ARTICLE{11078835,
  author={Niu, Guanglin and Li, Bo and Feng, Siling},
  journal={IEEE Transactions on Big Data}, 
  title={A Pluggable Common Sense-Enhanced Framework for Knowledge Graph Completion}, 
  year={2025},
  volume={},
  number={},
  pages={1-18},
  abstract={Knowledge graph completion (KGC) tasks aim to infer missing facts in a knowledge graph (KG) for many knowledgeintensive applications. However, existing embedding-based KGC approaches primarily rely on factual triples, potentially leading to outcomes inconsistent with common sense. Besides, generating explicit common sense is often impractical or costly for a KG. To address these challenges, we propose a pluggable common sense-enhanced KGC framework that incorporates both fact and common sense for KGC. This framework is adaptable to different KGs based on their entity concept richness and has the capability to automatically generate explicit or implicit common sense from factual triples. Furthermore, we introduce common senseguided negative sampling and a coarse-to-fine inference approach for KGs with rich entity concepts. For KGs without concepts, we propose a dual scoring scheme involving a relation-aware concept embedding mechanism. Importantly, our approach can be integrated as a pluggable module for many knowledge graph embedding (KGE) models, facilitating joint common sense and fact-driven training and inference. The experiments illustrate that our framework exhibits good scalability and outperforms existing models across various KGC tasks.},
  keywords={Training;Ontologies;Knowledge graphs;Tensors;Internet;Uncertainty;Translation;Large language models;Data models;Visualization;Knowledge graph completion;pluggable framework;common sense;entity concepts;negative sampling},
  doi={10.1109/TBDATA.2025.3588081},
  ISSN={2332-7790},
  month={},}@ARTICLE{11075757,
  author={Sheth, Amit P. and Roy, Kaushik and Venkataramanan, Revathy and Nadimuthu, Venkatesan and Shyalika, Chathurangi},
  journal={IEEE Internet Computing}, 
  title={Composite AI With Custom, Compact, Neurosymbolic Models: The Emergent Enterprise Artificial Intelligence Paradigm}, 
  year={2025},
  volume={29},
  number={2},
  pages={37-49},
  abstract={Artificial-intelligence architectures are shifting from monolithic, internet-scale models toward multi-component composite systems that must perform reliably in high-stakes settings. We introduce Custom, Compact and Composite AI with a Neurosymbolic approach (C3AN), a fourth-generation framework that integrates data, domain knowledge and human expertise through 14 foundation elements spanning reliability, grounding and safety. Custom focuses on right-sized, domain-specific data and workflows; Compact emphasizes resource-conscious models that can run on edge or mobile hardware; Composite unifies neural, symbolic and feedback modules for end-to-end reasoning. Three pilot systems illustrate the paradigm: Nourich (diabetes-aware dietary guidance) outperforms nine LLM baselines on recipe suitability; MAIC (K–12 mental-health triage) matches PHQ-9 gold-standard accuracy while halving compute; and SmartPilot (edge manufacturing copilot) achieves 93% anomaly accuracy and 21% forecasting gain over LSTM baselines. C3AN demonstrates that domain-aligned, neurosymbolic design can deliver transparent, trustworthy enterprise AI without extreme scale.},
  keywords={Computational modeling;Mission critical systems;Cognition;Safety;Manufacturing;Reliability;Artificial intelligence;Long short term memory;Neural engineering},
  doi={10.1109/MIC.2025.3570554},
  ISSN={1941-0131},
  month={March},}@ARTICLE{11072468,
  author={Vereno, Dominik and Neureiter, Christian and Eschlberger, Simon and Millaku, Mergim and Kuchenbuch, René and Uslar, Mathias},
  journal={IEEE Access}, 
  title={SGAM Toolbox Revisited: A Standards-Based Domain-Specific Modeling Language and Toolset}, 
  year={2025},
  volume={13},
  number={},
  pages={119243-119261},
  abstract={The SGAM Toolbox has established itself as a valuable modeling tool in the energy sector, particularly for interdisciplinary system-of-systems use cases. Built on a domain-specific modeling language that is anchored in European smart grid standardization, the toolbox has been adopted across academia and industry. Drawing on this extensive experience, we introduce the new and improved SGAM Toolbox. First, we review published applications of the tool; it has been widely used for high-level architecture modeling, often alongside other software in areas such as security, privacy, and e-mobility integration. Based on our findings, the key updates for the new toolbox include a strong formal foundation, a clear definition on how the tool should interface with requirements engineering, comprehensive semantics, and a viewpoint structure that segregates logical from technical aspects; these improvements enhance usability and real-world applicability. Second, the updated toolbox is presented in accordance with the TILO language-engineering stack; the specification includes the underlying ontology, a MOF-conformant metamodel, a UML-based implementation, and a modeling add-in for Enterprise Architect for advanced features. Third, we offer a practical demonstration of the toolbox, showing use case–driven architecture modeling. The SGAM Toolbox aims to strengthen its role as a platform for collaborating on system-of-systems use cases, bringing together the diverse stakeholders in smart grid projects.},
  keywords={Unified modeling language;Modeling;Smart grids;DSL;Stakeholders;Ontologies;Computer architecture;Object oriented modeling;Systems engineering and theory;Syntactics;Smart grid architecture model;systems engineering;model-based systems engineering;use case;domain-specific language;smart grid;system of systems;architecture},
  doi={10.1109/ACCESS.2025.3586722},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11050458,
  author={Shyalika, Chathurangi and Prasad, Renjith and Al Ghazo, Alaa and Eswaramoorthi, Darssan and Kaur, Harleen and Muthuselvam, Sara Shree and Sheth, Amit},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={SmartPilot: A Multiagent CoPilot for Adaptive and Intelligent Manufacturing}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={In the dynamic landscape of Industry 4.0, achieving efficiency, precision, and adaptability is essential to optimize manufacturing operations. Industries suffer due to supply chain disruptions caused by anomalies, which are being detected by current AI models but leaving domain experts uncertain without deeper insights into these anomalies. Additionally, operational inefficiencies persist due to inaccurate production forecasts and the limited effectiveness of traditional AI models for processing complex sensor data. Despite these advancements, existing systems lack the seamless integration of these capabilities needed to create a truly unified solution for enhancing production and decision-making. We propose SmartPilot, a neurosymbolic, multiagent CoPilot designed for advanced reasoning and contextual decision-making to address these challenges. SmartPilot processes multimodal sensor data and is compact to deploy on edge devices. It focuses on three key tasks: anomaly prediction, production forecasting, and domain-specific question answering. By bridging the gap between AI capabilities and real-world industrial needs, SmartPilot empowers industries with intelligent decision-making and drives transformative innovation in manufacturing. The demonstration video, datasets, and supplementary materials are available at https://github.com/ChathurangiShyalika/SmartPilot.},
  keywords={Industries;Technological innovation;Multimodal sensors;Decision making;Supply chains;Production;Predictive models;Question answering (information retrieval);Artificial intelligence;Smart manufacturing;Smart Manufacturing;Multimodal data;CoPilot;Multiagent;Neurosymbolic AI},
  doi={10.1109/CAI64502.2025.00007},
  ISSN={},
  month={May},}@INPROCEEDINGS{11050553,
  author={Tong, Richard J. and Cortês, Marina and DeFalco, Jeanine A. and Underwood, Mark and Zalewski, Janusz},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={A First-Principles Based Risk Assessment Framework and the IEEE P3396 Standard}, 
  year={2025},
  volume={},
  number={},
  pages={1588-1595},
  abstract={Generative Artificial Intelligence (AI) is enabling unprecedented automation in content creation and decision support, but it also raises novel risks. This paper presents a first-principles risk assessment framework underlying the IEEE P3396 Recommended Practice for AI Risk, Safety, Trustworthiness, and Responsibility. We distinguish between process risks (risks arising from how AI systems are built or operated) and outcome risks (risks manifest in the AI system's outputs and their real-world effects), arguing that generative AI governance should prioritize outcome risks. Central to our approach is an information-centric ontology that classifies AI-generated outputs into four fundamen-tal categories: (1) Perception-level information, (2) Knowledge-level information, (3) Decision/Action plan information, and (4) Control tokens (access or resource directives). This classification allows systematic identification of harms and more precise attribution of responsibility to stakeholders (developers, deployers, users, regulators) based on the nature of the information produced. We illustrate how each information type entails distinct outcome risks (e.g, deception, misinformation, unsafe recommendations, security breaches) and requires tailored risk metrics and mitigations. By grounding the framework in the essence of information, human agency, and cognition, we align risk evaluation with how AI outputs influence human understanding and action. The result is a principled approach to AI risk that supports clear accountability and targeted safeguards, in contrast to broad application-based risk categorizations. We include example tables mapping information types to risks and responsibilities. This work aims to inform the IEEE P3396 Recommended Practice and broader AI governance with a rigorous, first-principles foundation for assessing generative AI risks while enabling responsible innovation.},
  keywords={Ethics;Technological innovation;Regulators;Generative AI;Standards organizations;Organizations;Cognition;Stakeholders;Security;Fake news;Risk;Information Categorization;AI Agency;Human Agency;GenAl},
  doi={10.1109/CAI64502.2025.00237},
  ISSN={},
  month={May},}@INPROCEEDINGS{11050462,
  author={Xu, Bin and Tong, Richard J and Li, Yanyan and Chen, Penghe and Li, Hanming and Liang, Joleen and Fan, Xing and Tong, Jessie},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={An Architectural Framework for Educational Knowledge Graphs (IEEE P2807.6): Ontology Design, Llm Integration, and Adaptive Learning Applications}, 
  year={2025},
  volume={},
  number={},
  pages={1610-1616},
  abstract={The IEEE P2807.6 Education Knowledge Graph (EduKG) standard defines a semantic infrastructure to represent educational knowledge, resources, and pedagogy in a unified graph format. This paper expands on the core EduKG architecture, detailing its ontology design and key entities-Learning Points, Resource Items, and Pedagogical Rules-that collectively model the domain, content, and instructional strategies of learning systems. We further explore how EduKG can be integrated with advanced AI technologies, including large language models (LLMs) and retrieval-augmented generation (Graph-RAG) via embedding databases, to enable intelligent behavior such as semantic search, question answering, and dynamic content generation. These integrations position EduKG as a central component in next-generation smart education systems, wherein knowledge graphs work in concert with intelligent agents and adaptive instructional systems to deliver fully automated, personalized, and interactive learning experiences. By leveraging the standardized graph-structured representation and semantic reasoning capabilities of EduKG, such systems can achieve interoperability across platforms and support complex AI-driven tutoring and training scenarios. This work provides a comprehensive overview of the EduKG framework and highlights its role in empowering adaptive, cognitive, and collaborative learning solutions for the future of digital education.},
  keywords={Learning systems;Adaptive systems;Large language models;Semantics;Retrieval augmented generation;Knowledge graphs;Ontologies;Artificial intelligence;Standards;Interoperability;Education Knowledge Graph;Ontology;Large Language Models;Retrieval-Augmented Generation;Adaptive Instructional Systems;Intelligent Tutoring;Interoperability},
  doi={10.1109/CAI64502.2025.00249},
  ISSN={},
  month={May},}@INPROCEEDINGS{11050705,
  author={Do, Thanh Son and Hier, Daniel B. and Obafemi-Ajayi, Tayo},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Mapping Biomedical Ontology Terms to Ids: Effect of Domain Prevalence on Prediction Accuracy}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This study evaluates the ability of large language models (LLMs) to map biomedical ontology terms to their corresponding ontology IDs across the Human Phenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies. Using counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate for their prevalence in the biomedical literature, we examined the relationship between ontology ID prevalence and mapping accuracy. Results indicate that ontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO IDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers. Higher prevalence of ontology IDs in the biomedical literature correlated with higher mapping accuracy. Predictive models based on receiver operating characteristic (ROC) curves confirmed this relationship. In contrast, this pattern did not apply to mapping protein names to Human Genome Organisation's (HUGO) gene symbols. GPT-4 achieved a high baseline performance (95 %) in mapping protein names to HUGO gene symbols, with mapping accuracy unaffected by prevalence. We propose that the high prevalence of HUGO gene symbols in the literature has caused these symbols to become lexicalized, enabling GPT-4 to map protein names to HUGO gene symbols with high accuracy. These findings highlight the limitations of LLMs in mapping ontology terms to low-prevalence ontology IDs and underscore the importance of incorporating ontology ID prevalence into the training and evaluation of LLMs for biomedical applications.},
  keywords={Proteins;Training;Protein engineering;Accuracy;Phenotypes;Terminology;Large language models;Symbols;Training data;Ontologies;Ontology mapping;machine codes;large language models;Zipf's Law;Gene Ontology;Human Phenotype Ontology;lexicalization;UniProt KB},
  doi={10.1109/CAI64502.2025.00101},
  ISSN={},
  month={May},}@INPROCEEDINGS{11050457,
  author={Al Khatib, Hassan S. and Mittal, Sudip and Rahimi, Shahram and Marhamati, Nina and Bozorgzad, Sean},
  booktitle={2025 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={From Patient Consultations to Graphs: Leveraging LLMs for Patient Journey Knowledge Graph Construction}, 
  year={2025},
  volume={},
  number={},
  pages={410-415},
  abstract={The shift toward patient-centric healthcare requires understanding comprehensive patient journeys. Current healthcare data systems often fail to provide holistic representations, hindering coordinated care. Patient Journey Knowledge Graphs (PJKGs) solve this by integrating diverse patient information into unified, structured formats. This paper presents a methodology for constructing PJKGs using Large Language Models (LLMs) to process both clinical documentation and patient-provider conversations. These graphs capture temporal and causal relationships between clinical events, enabling advanced reasoning and personalized insights. Our evaluation of four LLMs (Claude 3.5, Mistral, Llama 3.1, ChatGPT4o) shows all achieved perfect structural compliance but varied in medical entity processing, computational efficiency, and semantic accuracy. This work advances patient-centric healthcare through actionable knowledge graphs (KGs) that enhance care coordination and outcome prediction.},
  keywords={Accuracy;Large language models;Semantics;Medical services;Knowledge graphs;Oral communication;Documentation;Cognition;Data systems;Computational efficiency;Healthcare Journey Mapping;Large Language Model (LLM);Temporal and Causal Reasoning;Patient-Centric Healthcare;Knowledge Graph},
  doi={10.1109/CAI64502.2025.00075},
  ISSN={},
  month={May},}@INPROCEEDINGS{11058583,
  author={Menad, Safaa and Medeiros, Gabriel H. A. and Soualmia, Lina F.},
  booktitle={2025 IEEE 38th International Symposium on Computer-Based Medical Systems (CBMS)}, 
  title={Enhancing the Description-Detection Framework with Semantic Clustering Using Biostransformers}, 
  year={2025},
  volume={},
  number={},
  pages={654-659},
  abstract={Event-Based Surveillance Systems (EBS) are crucial for detecting emerging public health threats. However, these systems face significant challenges, including overreliance on manual expert intervention, limited handling of heterogeneous textual data, etc. The Description-Detection Framework (DDF) addresses some of these limitations by leveraging PropaPhen (Core Propagation Phenomenon Ontology), UMLS, and OpenStreetMaps to detect suspicious health-related cases using spatiotemporal and textual data. However, DDF is restricted to detection and lacks the ability to classify the detected observations into meaningful categories. To adress this limitation, we propose to enhance DDF by incorporating a clustering-based classification process. This enhancement employs BioSTransformers, a pretrained biomedical language model built on Sentence Transformers trained on PubMed data, to compute semantic similarity between observations. By capturing domain-specific semantic relationships, BioSTransformers enables clustering that integrates biological semantics with spatiotemporal context, outperforming traditional methods from the literature in observation classification. Our proposed approach reduces the dependency on manual expert effort, improves the system's ability to process heterogeneous data, and enhances the accuracy and contextual relevance of case classification. The results demonstrate the potential of this method to advance EBS systems, providing a scalable and automated solution to public health surveillance challenges.},
  keywords={Biological system modeling;Computational modeling;Surveillance;Unified modeling language;Semantics;Ontologies;Transformers;Spatiotemporal phenomena;Public healthcare;Context modeling;Public Health Surveillance;Biomedical Language Models;Spatiotemporal Reasoning;Description Detection Framework;Core Propagation Phenomenon Ontology},
  doi={10.1109/CBMS65348.2025.00135},
  ISSN={2372-9198},
  month={June},}@INPROCEEDINGS{11058739,
  author={Menad, Safaa and Abdeddaïm, Saïd and Soualmia, Lina F.},
  booktitle={2025 IEEE 38th International Symposium on Computer-Based Medical Systems (CBMS)}, 
  title={Predicting Similarities Between Biomedical Ontologies : The UMLs Use-Case}, 
  year={2025},
  volume={},
  number={},
  pages={648-653},
  abstract={Biomedical ontologies are crucial for organizing domain-specific knowledge, yet traditional alignment methods relying on lexical matching often fail to capture complex semantic relationships. To address this limitation, we propose a novel approach leveraging siamese neural networks and transformerbased models to enhance ontology alignment within the biomedical domain. Our method applies self-supervised contrastive learning to biomedical literature, optimizing the prediction of semantic similarities between concepts in the UMLS Metathesaurus. The results demonstrate that this approach surpasses lexical-based techniques by identifying contextual relationships and uncovering new interconnections among UMLS terminologies. This highlights the potential of our models in improving ontology alignment and enriching biomedical knowledge integration.},
  keywords={Vocabulary;Terminology;Biological system modeling;Unified modeling language;Semantics;Neural networks;Contrastive learning;Ontologies;Transformers;Vectors;Ontology;Alignment;Matching;UMLS Metathesaurus;Biomedical Ontology;Transformers;Siamese Neural Network;Semantic Similarity;Sentence Embeddings},
  doi={10.1109/CBMS65348.2025.00134},
  ISSN={2372-9198},
  month={June},}@INPROCEEDINGS{11049984,
  author={Zlobin, O. N. and Litvinov, V. L. and Filippov, F. V.},
  booktitle={2025 VI International Conference on Neural Networks and Neurotechnologies (NeuroNT)}, 
  title={Domain-Specific Language Models for Continuous Learning}, 
  year={2025},
  volume={},
  number={},
  pages={3-5},
  abstract={The paper examines the issue of building domain-specific (domain-oriented) language models and the possibility of their continuous learning. A concept is proposed that provides for the introduction of additional cognitive memory with domain administration, designed for the gradual accumulation of new information obtained after the termination of learning the language model. The issue of transition from the inference mode to a new model learning is considered.},
  keywords={Training;Knowledge engineering;Buildings;Ontologies;Search problems;Data mining;Tuning;Neurotechnology;Domain specific languages;Context modeling;artificial intelligence;language models;verbal memory;cognitive memory;update context;embeddings;domains;ontology concepts},
  doi={10.1109/NeuroNT66873.2025.11049984},
  ISSN={},
  month={June},}@INPROCEEDINGS{11059459,
  author={Gueddes, Abdelweheb and Fathallah, Wyssem and Mahjoub, Mohamed Ali},
  booktitle={2025 International Wireless Communications and Mobile Computing (IWCMC)}, 
  title={BERT-Based Knowledge Graph Construction from Social Media}, 
  year={2025},
  volume={},
  number={},
  pages={461-466},
  abstract={Social media platforms serve as massive repositories of textual data, reflecting diverse human interactions and preferences. However, the unstructured nature of this content poses significant challenges for extracting semantically rich insights. This paper introduces a novel methodology for the automated construction of knowledge graphs (KGs) from social media discourse, specifically focusing on Twitter tweets. Our approach synergistically integrates large language models (LLMs), specifically a fine-tuned BERT model, with an ontology-driven framework. First, we define a detailed ontology of online communication concepts. The pre-trained BERT model is then fine-tuned using a multi-task learning approach on a curated dataset of anonymized and segmented Twitter discussions, thereby aligning its semantic representations with the predefined ontology. The fine-tuned LLM is leveraged for several critical tasks including entity and relation extraction, sentiment analysis, intention classification and the inference of contextual information and discussion styles. Furthermore, a mechanism is introduced to infer inter-user relationships and shared interests using graph neural networks (GNNs), analyzing patterns in interaction and language use. This multi-faceted extracted and inferred data is subsequently employed to build a knowledge graph, stored and queried via the Neo4j graph database management system. This study presents several contributions such as the integration of a ontology with an LLM method and the innovative user relationship and shared interest extraction using graph neural networks. The proposed methodology was rigorously evaluated using a real-world dataset of Twitter discussions, showcasing its ability to capture semantic content, and elucidate inter-user relationships effectively and revealing shared interests of the users involved. Furthermore, an ablation study is included which further demonstrates each of the method component contribution and demonstrates the importance of such integrations. Our findings highlight the potential for various downstream applications such as in community structure analysis and sentiment analysis to improve information management within online social networks.},
  keywords={Sentiment analysis;Ethics;Social networking (online);Scalability;Large language models;Blogs;Semantics;Knowledge graphs;Ontologies;Graph neural networks;Knowledge Graph;Social Media Analysis;Large Language Models (LLMs);Ontology-Driven Methodology;BERT},
  doi={10.1109/IWCMC65282.2025.11059459},
  ISSN={2376-6506},
  month={May},}@INPROCEEDINGS{11052053,
  author={Shukla, Varsha and Pradhan, Rahul},
  booktitle={2025 International Conference on Intelligent and Cloud Computing (ICoICC)}, 
  title={Protein Matching for Function Prediction using Siamese Neural Network}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Protein similarity lies at the heart of many critical tasks in bio-informatics, from predicting protein function to drug discovery and evolutionary studies. The discipline has deployed traditional approaches such as sequence alignment and structural comparison to serve the field for decades, their limitations in scalability, sensitivity to remote homologs, and reliance on handcrafted features have become increasingly apparent in the era of high-throughput data. In this study, one of the concept of deep learning, specifically Siamese neural networks for similarity matching, is explored for protein matching. By leveraging pretrained protein sequence embeddings and biologically informed functional annotations, a framework is developed that can learn nuanced relationships between protein pairs.},
  keywords={Heart;Deep learning;Cloud computing;Sensitivity;Scalability;Graph neural networks;Protein sequence;Biology;Drug discovery;Convolutional neural networks;Siamese neural network (SNN);Convolutional neural network (CNN);Graph neural network (GNN)},
  doi={10.1109/ICoICC64033.2025.11052053},
  ISSN={},
  month={May},}@INPROCEEDINGS{11047951,
  author={Loevenich, Johannes F. and Adler, Erik and Hürten, Tobias and Spelter, Florian and Roncevic, Damian and Lopes, Roberto Rigolin F.},
  booktitle={2025 International Conference on Military Communication and Information Systems (ICMCIS)}, 
  title={Automating Cyber Threat Intelligence and Attack Chain Generation using Cyber Security Knowledge Graphs and Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={Modern cyberattacks are increasingly complex, using sophisticated tactics, techniques and procedures (TTPs) to evade detection and compromise systems. Effective cyber defence relies on real-time and accurate Cyber Threat Intelligence (CTI), which is often challenged by data quality, completeness and accessibility. While traditional methods and manually maintained knowledge bases provide valuable insights, they struggle to adapt to the rapidly evolving threat landscape. To address these challenges, we propose an architecture that uses Large Language Models (LLMs) for automated annotation of CTI reports and construction of Cybersecurity Knowledge Graphs (CSKG) to build sophisticated attack chains. Building on our previous research, we extend the capabilities of Autonomous Cyber Defence (ACD) agents to improve situational awareness and defence mechanisms in dynamic environments. Experimental results demonstrate the effectiveness of our approach in improving CTI accessibility, accuracy, and integration into defence strategies. Our experimental results highlight the potential of combining LLM, knowledge graphs and automated planning to improve proactive cyber defence and attack simulation methodologies.},
  keywords={Military communication;Accuracy;Annotations;Large language models;Soft sensors;Knowledge based systems;Knowledge graphs;Real-time systems;Cyber threat intelligence;Planning;Autonomous Cyber Defence;Knowledge Graphs;Cybersecurity;Large Language Model},
  doi={10.1109/ICMCIS64378.2025.11047951},
  ISSN={2993-4974},
  month={May},}@ARTICLE{11053835,
  author={Nagata, Yoshiteru and Kohama, Daiki and Watanabe, Yoshiki and Katayama, Shin and Urano, Kenta and Yonezawa, Takuro and Kawaguchi, Nobuo},
  journal={IEEE Access}, 
  title={SemantiPack: An Efficient Real-World Data Compressor Using Structural and Semantic Metadata}, 
  year={2025},
  volume={13},
  number={},
  pages={114159-114178},
  abstract={The exponential growth of Real-World Data (RWD), primarily collected from IoT sensors and spanning domains such as mobility, environment, and energy consumption, presents critical challenges due to its scale, heterogeneity, and structural variability. Traditional compression methods often fail to adapt efficiently to these complexities, leading to sub-optimal storage and analysis performance. Additionally, while several metadata schemas exist to enhance the availability of RWD, smaller organizations often lack the resources to create and manage metadata effectively. This paper introduces RWD Profile, an automatically generatable metadata schema for RWD, and SemantiPack, which applies tailored compression to data fields in the individual RWD based on RWD Profile. RWD Profile has two kinds of metadata, Structural Profile and Semantic Profile. Structural Profile is generated through rule-based systems, while Semantic Profile is generated using large language models (LLMs) to capture semantic data properties. On the other hand, SemantiPack performs compression at the data field level in RWD using RWD Profile to achieve higher compression ratios. Experimental results demonstrate up to 23.2% improved compression rate for JSON and 19.3% for CSV compared to conventional methods while maintaining a faster or equivalent processing speed compared to conventional methods. Furthermore, SemantiPack supports lossy compression for applications prioritizing storage over precision. This research not only improves compression efficiency but also establishes a scalable solution for automated analysis and sustainable data utilization, paving the way for advancements in RWD management.},
  keywords={Metadata;Semantics;Data compression;Image coding;Sensor phenomena and characterization;Resource description framework;Ontologies;Standards organizations;Wireless sensor networks;Transform coding;Data compression;real-world data;linked data;semantic web},
  doi={10.1109/ACCESS.2025.3583829},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11042856,
  author={S, Jenny Kalaiarasi. and K, Nimala.},
  booktitle={2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)}, 
  title={Advancements in Context-Aware Recommender Systems: An Exhaustive Exploration of Ontology and LLM in improving ranking accuracy}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={Recommendation systems have become a ubiquitous presence across various domains such as e-commerce, entertainment (movies and music), tourism, news, advertising, stock markets and social networks. The integration of Ontology and Large Language Models (LLMs) presents a transformative approach for enhancing recommender systems by improving contextual intelligence, personalization, and explainability. Traditional recommendation algorithms often suffer from data sparsity, cold-start problems, and limited semantic understanding, which hinder their ability to deliver highly relevant and interpretable suggestions. This research provides an exhaustive exploration of how ontology-driven knowledge representation can be seamlessly combined with LLMs to refine user-item interactions, enrich embeddings, and improve reasoning in recommendation pipelines. We propose a hybrid framework where ontological structures encode domain-specific relationships while LLMs process, infer, and enhance recommendations through contextual embeddings.This synergy enables semantic-aware recommendations, explainable item suggestions, and adaptive learning mechanisms that adjust based on user behavior and external context. Furthermore, we investigate novel ontology injection techniques, including knowledge graph-based reinforcement, to improve long-tail item exposure and enhance recommendation diversity. Through rigorous evaluation on real-world datasets, we demonstrated how ontology-LLM fusion significantly outperforms baseline models in accuracy, novelty, and interpretability. This study establishes a foundation for the next generation of context-aware, knowledge-enhanced recommender systems that leverage LLMs for reasoning and ontologies for structured knowledge representation, paving the way for more intelligent and user-centric recommendations.},
  keywords={Accuracy;Heavily-tailed distribution;Social networking (online);Semantics;Diversity reception;Ontologies;Cognition;Graph neural networks;Stock markets;Recommender systems;Context-Aware Recommendation Systems;Computational intelligence;LLM;Ontology;semantic-aware},
  doi={10.1109/RMKMATE64874.2025.11042856},
  ISSN={},
  month={May},}@INPROCEEDINGS{11042435,
  author={Shi, Ruhui and Chen, Tianran and Lu, Chunyu and Shang, Duo and Luo, Jun and Hui, Xin and Li, Haoran and He, Huihong},
  booktitle={2025 8th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)}, 
  title={Dynamic Assessment and Early Warning of Cross-Border Pipeline Risks Based on Knowledge Graph}, 
  year={2025},
  volume={},
  number={},
  pages={374-377},
  abstract={This paper introduces a novel framework for the dynamic assessment and early warning of cross-border pipeline risks, leveraging the zero-shot learning capabilities of the GPT-4 large language model (LLM) to construct and utilize a domain-specific knowledge graph. The approach circumvents the need for extensive pre-training or fine-tuning, thereby enabling rapid development and deployment in a domain characterized by data scarcity. The methodology begins with the construction of a knowledge graph, which involves a comprehensive ontology design, followed by knowledge extraction from unstructured text sources, and culminates in knowledge representation and storage in a structured format. Subsequently, the framework facilitates dynamic risk understanding and analysis, encompassing risk factor identification, risk event inference, and the analysis of complex relationships between these elements, all supported by the structured knowledge base. Finally, the system provides knowledge-based risk interpretation and decision-making support, generating explanations for risk events, assessing their potential impacts, and offering actionable recommendations to stakeholders. By employing carefully crafted prompts and the inherent natural language processing capabilities of GPT-4, the framework facilitates the extraction of entities and relationships from unstructured textual data, effectively populating the knowledge graph. This knowledge graph, representing entities, attributes, and relationships pertinent to cross-border pipeline risks, serves as a foundation for dynamic risk assessment. This method presents a scalable, data-efficient approach tailored to the specialized domain of cross-border pipeline risk management, particularly advantageous in contexts with limited resources. The proposed framework demonstrates the potential of LLMs, specifically GPT-4, in conjunction with knowledge graph methodologies, to advance risk management practices in complex, data-sparse domains.},
  keywords={Large language models;Pipelines;Knowledge based systems;Zero shot learning;Knowledge graphs;Ontologies;Safety;Risk management;Stakeholders;Software engineering;LLM;China-Russia pipeline;knowledge graph},
  doi={10.1109/AEMCSE65292.2025.11042435},
  ISSN={},
  month={May},}@INPROCEEDINGS{11041826,
  author={Wang, Shenghui and Li, Mengjiao and Miao, Yuxin and Liu, Nan and Sun, Zihao},
  booktitle={2025 7th International Conference on Information Science, Electrical and Automation Engineering (ISEAE)}, 
  title={Construction and application of power transformer fault knowledge graph based on Sentence-BERT}, 
  year={2025},
  volume={},
  number={},
  pages={841-846},
  abstract={With the rapid development of smart grid, the relevance of multidimensional information in power grid is weak, and the decision-making generation efficiency in operation and maintenance process is low. At present, knowledge graph has innovative applications in various fields, which improves the efficiency of knowledge query in related professional fields. This paper proposes a power transformer fault diagnosis system driven by knowledge graph, which aims to solve the problems of weak information relevance and low decision-making efficiency in transformer operation and maintenance; The system is composed of three modules: data acquisition, data analysis and data service. By constructing the knowledge graph and model of fault diagnosis, the intelligent query and diagnosis of transformer fault are realized; The construction of knowledge graph is based on expert knowledge and fault data, and the entity extraction is carried out by using the Sentence-BERT-BiLSTM-CRF model, which improves the extraction accuracy of transformer fault information; The three tuple data is stored in the neo4j graph database, and the cypher language is used for efficient query, providing accurate fault diagnosis and maintenance strategies for operation and maintenance personnel. Experimental results show that the proposed method can effectively improve the accuracy and efficiency of fault diagnosis.},
  keywords={Fault diagnosis;Accuracy;Decision making;Knowledge graphs;Ontologies;Data models;Maintenance;Power transformers;Data mining;Visual databases;Knowledge graph;Knowledge extraction;Fault diagnosis;Power transformer;Bert model},
  doi={10.1109/ISEAE64934.2025.11041826},
  ISSN={},
  month={April},}@INPROCEEDINGS{11035002,
  author={Zhao, Wei and Bai, Wei and Hu, Zhi-Yuan and He, Chao-Xin},
  booktitle={2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={DMM-GPT: A Dual Modeling Mechanism-Based Generic Protocol Translation Framework}, 
  year={2025},
  volume={},
  number={},
  pages={1-12},
  abstract={With the increasing complexity of heterogeneous communication environments, the problem of data sharing caused by multiple heterogeneous protocols has become the core challenge restricting interoperability. Aiming at the problems of poor scalability and automation ability of existing protocol translation methods, this study proposes a Dual Model Mechanism-based Generic Protocol Translation (DMM-GPT) framework. The framework creatively combines ontology-based semantic modeling with state-machine-based behavioral modeling, enabling automated identification of protocol translation bridging points and autonomous construction of a unified protocol state transition model. Firstly, a protocol ontology model is constructed to model the semantic associations between protocol entities. Based on the ontology, semantic information is extracted. Semantic similarities from message level to field level are computed, thereby sequentially identifying message mapping and field mapping relationships. Secondly, modeling protocol behavior through state machine models. Based on the identified mapping relationships, the heterogeneous protocol bridge points are constructed, and a unified state transition model is synthesized automatically. Thus, interoperable behavior between heterogeneous protocols is implemented. Finally, the complete closed loop of protocol interoperability is implemented by applying semantic alignment results to the state transition model. To validate the effectiveness of DMM-GPT, experiments were conducted in two representative scenarios: service discovery (SLP-Bonjour) and IoT communication (HTTP-CoAP). The results demonstrate its effectiveness in enabling interoperability between heterogeneous protocols.},
  keywords={Seminars;Protocols;Translation;Computational modeling;Scalability;Semantics;Ontologies;Data mining;Information technology;Interoperability;protocol translation;semantic similarity;ontology modeling;state machine},
  doi={10.1109/AINIT65432.2025.11035002},
  ISSN={},
  month={April},}@INPROCEEDINGS{11033426,
  author={Matta, Nada and Pfundstein, Patrick and Larde, Camille and Ismedon, Baptiste},
  booktitle={2025 28th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
  title={Answering Application of Generative AI in Industry: Integration of Semantic Representation}, 
  year={2025},
  volume={},
  number={},
  pages={825-828},
  abstract={Generative AI acts as an effective guide in decision-making process. This paper is designed to outline the main challenges in applying this technique within businesses and for specific activities. Semantic representation as ontology can be one solution for these challenges. Our first work to link ontology to LLM algorithms is illustrated in financial institutions},
  keywords={Industries;Machine learning algorithms;Generative AI;Text recognition;Federated learning;Semantics;Retrieval augmented generation;Finance;Companies;Ontologies;Generative AI;ChatGPT;Bard;LLM;Ontology;finance},
  doi={10.1109/CSCWD64889.2025.11033426},
  ISSN={2768-1904},
  month={May},}@INPROCEEDINGS{11036147,
  author={Onozuka, Soichi and Ohnishi, Takaaki},
  booktitle={2025 19th International Conference on Semantic Computing (ICSC)}, 
  title={Analysis of LLMs for RDF Triple Generation: Semantic and Syntactic Evaluation Using WebNLG}, 
  year={2025},
  volume={},
  number={},
  pages={136-143},
  abstract={Using the WebNLG dataset as ground truth, we evaluate the capability of large language models (LLMs) to generate resource description framework triples from natural language input. The proposed method employs two complementary evaluation metrics: cosine similarity for assessing semantic proximity and graph edit distance for comparing structural aspects of triple sets. The analysis demonstrates that these metrics provide distinct yet complementary perspectives on semantic evaluation, enabling a comprehensive assessment of the natural language understanding capabilities of LLMs. Through this approach, we demonstrate that modern LLMs exhibit sophisticated abilities in integrated syntax and semantics processing by utilizing distributed representations where both types of information coexist within high-dimensional vector spaces. This integration suggests that understanding of language structure of LLMs transcends simple pattern recognition to achieve meaningful semantic comprehension.},
  keywords={Measurement;Large language models;Semantics;Syntactics;Ontologies;Resource description framework;Vectors;Natural language processing;Pattern recognition;Ontology;RDF;LLMs;Similarity},
  doi={10.1109/ICSC64641.2025.00025},
  ISSN={2472-9671},
  month={Feb},}@INPROCEEDINGS{11036150,
  author={Johnson, Matthew and Rashid, Sabbir M. and McGuinness, Deborah L.},
  booktitle={2025 19th International Conference on Semantic Computing (ICSC)}, 
  title={Improving Tabular Reusability Through Data Dictionary Descriptions}, 
  year={2025},
  volume={},
  number={},
  pages={209-216},
  abstract={Tables have become a ubiquitous standard for capturing, storing, and sharing data on the web. This is primarily due to the semi-structured nature of tables, where relationships between data are often ambiguously encoded using locality. While this format can be easy for humans to interpret in simple cases, as table complexity increases, so does the difficulty in interpretability. To bridge this context gap, many data publishers provide a data dictionary to capture schema elements' meaning through text descriptions. Existing work compounds the need for data dictionaries to improve tabular interoperability, but few provide detailed requirements for data dictionary descriptions. This paper identifies and defines three common types of data dictionary descriptions in the biomedical domain. We then compare the effectiveness of each description type by normalizing data dictionary descriptions to a single type using large language models and measuring their performance using a semantic tabular interpretation algorithm. Our experiments show that intensional descriptions, which describe the general properties a column member should have, are most effective for tabular alignment and improve the reusability of data dictionaries.},
  keywords={Dictionaries;Reviews;Large language models;Semantics;Information sharing;Complexity theory;Compounds;Interoperability;Standards;Lenses;tabular data;data dictionary;semantic data dictionary;semantic annotation;large language model},
  doi={10.1109/ICSC64641.2025.00035},
  ISSN={2472-9671},
  month={Feb},}@INPROCEEDINGS{11036145,
  author={Jiang, Longquan and Huang, Junbo and Möller, Cedric and Usbeck, Ricardo},
  booktitle={2025 19th International Conference on Semantic Computing (ICSC)}, 
  title={Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge Graph Question Answering}, 
  year={2025},
  volume={},
  number={},
  pages={28-35},
  abstract={Most existing Knowledge Graph Question Answering (KGQA) approaches are designed for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the heterogeneity of the underlying graph schema, topology and assertions, most KGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without resource-intensive training data. We present OntoSCPrompt, a novel Large Language Model (LLM)-based KGQA approach with a two-stage architecture that separates semantic parsing from KG-dependent interactions. OntoSCPrompt first generates a SPARQL query structure (including SPARQL keywords such as SELECT, ASK, WHERE and placeholders for missing tokens) and then fills them with KG-specific information. To enhance the understanding of the underlying KG, we present an ontology-guided, hybrid prompt learning strategy that integrates KG ontology into the learning process of hybrid prompts (e.g., discrete and continuous vectors). We also present several task-specific decoding strategies to ensure the correctness and executability of generated SPARQL queries in both stages. Experimental results demonstrate that OntoSCPrompt performs as well as SOTA approaches without retraining on a number of KGQA datasets such as CWQ, WebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well to unseen domain-specific KGs like DBLP-QuAD and CoyPu KG 11Code: https://github.com/LongquanJiang/OntoSCPrompt.},
  keywords={Large language models;Semantics;Training data;Knowledge graphs;Ontologies;Predictive models;Question answering (information retrieval);Vectors;Decoding;Topology;QA;KGQA;LLM;Generalization},
  doi={10.1109/ICSC64641.2025.00010},
  ISSN={2472-9671},
  month={Feb},}@INPROCEEDINGS{11034402,
  author={Robert, Kuo-Chung Lin and Jessica, Wen-Jie Lin},
  booktitle={2025 4th International Conference on Artificial Intelligence, Internet and Digital Economy (ICAID)}, 
  title={Case Study: Apply Ontology and AIGC for Task Appoint in Smart Building Control Centers}, 
  year={2025},
  volume={},
  number={},
  pages={22-26},
  abstract={Since the amazing effect of ChatGPT has been recognized by the industry, towards integrating with employees’ workflow to create more business applications. At the same time, many artificial intelligence scientists are actively looking for methods and platforms that are highly efficient and can quickly embed LLM and gain business value quickly. This study proposes two important innovations. First, it uses historical operating data to quickly and automatically generate LLMs in the company’s exclusive domain. Second, to quickly embed those models in ontology maps that to generate thinking and decision-making documents (as AIGC). In the paper, first step is use auto-insight engine to transfer input data from operational datasets. Secondly, to adjust parameters the training in deep learning framework to output models. Finally, integrate more API that they like conversation tools or other services. It can automatically generate and output high-precision event determination results in real time and can automatic notification and complete the processing flow that integrate daily report for frontline partners. In future, it will be able to develop more follow-up applications, such as artificial intelligence assistant for the security control center partner, which can determine the high level of daily incidents and automatically complete high-complexity daily operation procedures in a closed-loop security environment, and moderately connect high-level voice assistants to achieve important indicators such as health care, efficiency improvement and cost reduction.},
  keywords={Industries;Training;Smart buildings;Costs;Large language models;Biological system modeling;Ontologies;Predictive models;Data models;Security;Ontology;Smart Building;LLMs;AIGC},
  doi={10.1109/ICAID65275.2025.11034402},
  ISSN={},
  month={April},}@INPROCEEDINGS{11021058,
  author={Lee, Sam Yu-Te and Hung, Cheng-Wei and Yuan, Mei-Hua and Ma, Kwan-Liu},
  booktitle={2025 IEEE 18th Pacific Visualization Conference (PacificVis)}, 
  title={Visual Text Mining with Progressive Taxonomy Construction for Environmental Studies}, 
  year={2025},
  volume={},
  number={},
  pages={307-317},
  abstract={Environmental experts have developed the DPSIR (Driver, Pressure, State, Impact, Response) framework to systematically study and communicate key relationships between society and the environment. Using this framework requires experts to construct a DPSIR taxonomy from a corpus, annotate the documents, and identify DPSIR variables and relationships, which is laborious and inflexible. Automating it with conventional text mining faces technical challenges, primarily because the taxonomy often begins with abstract definitions, which experts progressively refine and contextualize as they annotate the corpus. In response, we develop GreenMine, a system that supports interactive text mining with prompt engineering. The system implements a prompting pipeline consisting of three simple and evaluable subtasks. In each subtask, the DPSIR taxonomy can be defined in natural language and iteratively refined as experts analyze the corpus. To support users evaluate the taxonomy, we introduce an uncertainty score based on response consistency. Then, we design a radial uncertainty chart that visualizes uncertainties and corpus topics, which supports interleaved evaluation and exploration. Using the system, experts can progressively construct the DPSIR taxonomy and annotate the corpus with LLMs. Using real-world interview transcripts, we present a case study to demonstrate the capability of the system in supporting interactive mining of DPSIR relationships, and an expert review in the form of collaborative discussion to understand the potential and limitations of the system. We discuss the lessons learned from developing the system and future opportunities for supporting interactive text mining in knowledge-intensive tasks for other application scenarios.},
  keywords={Text mining;Visualization;Uncertainty;Reviews;Taxonomy;Pipelines;Green products;Data visualization;Systems support;Information systems;Information systems;Information systems applications;Data Mining;Human-centered computing;Visualization;Visualization systems and tools},
  doi={10.1109/PacificVis64226.2025.00037},
  ISSN={2165-8773},
  month={April},}@INPROCEEDINGS{11028174,
  author={Guryanov, A. V. and Moshkin, V. S. and Dyrnochkin, A. A.},
  booktitle={2025 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)}, 
  title={An Approach to Automated Ontology Extraction From Technological Documentation Using NLP and LLM}, 
  year={2025},
  volume={},
  number={},
  pages={1011-1016},
  abstract={The paper presents an approach to constructing a formal description of a subject area using automated analysis of technological documentation. The first stage of the proposed approach is extracting data (terms, objects of the subject area) from technical documentation on the subject area. Then, a natural language query is formed for a large language model (LLM), which helps to extract relations between the extracted terms. At the final stage, an OWL ontology is formed by postprocessing the obtained data. Evaluation experiments were conducted on 16 documents (ISO) on the topic of “Space industry”. Experiments were also conducted to compare algorithms for extracting terms and relations using LLM and the C-value algorithm. The proposed approach has proven its effectiveness on strictly formalized texts.},
  keywords={Industries;Large language models;ISO Standards;OWL;Prototypes;Documentation;Ontologies;Industrial engineering;Natural language processing;Manufacturing;ontology;large language model;natural language processing;design documentation;technical documentation;term},
  doi={10.1109/ICIEAM65163.2025.11028174},
  ISSN={2993-4060},
  month={May},}@INPROCEEDINGS{11025595,
  author={Ahmed, Nafisa and Kwok, Hin Chi and Hamdaqa, Mohammad and Assunção, Wesley K. G.},
  booktitle={2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR)}, 
  title={SMATCH-M-LLM: Semantic Similarity in Metamodel Matching With Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={199-210},
  abstract={Metamodel matching plays a crucial role in defining transformation rules in model-driven engineering by identifying correspondences between different metamodels, forming the foundation for effective transformations. Current techniques face significant challenges due to syntactical and structural heterogeneity. To address this, matching techniques often employ semantic similarity to identify correspondences. Traditional semantic matchers, however, rely on ontology matching tools or lexical databases, which often struggle when metamodels use different terminologies or hierarchical structures. Inspired by the contextual understanding capabilities of Large Language Models (LLMs), this paper explores the capability of GPT-4 potentials as a semantic matcher and alternative to existing methods for metamodel matching. However, metamodels can be large, which can overwhelm LLMs if provided in a single prompt, leading to reduced accuracy. Therefore, we propose prompting LLMs with fragments of the source and target metamodels, identifying correspondences through an iterative process. The fragments to be provided in the prompt are identified based on an initial mapping derived from their elements’ definitions. Through experiments with 10 metamodels, our results show that our LLMbased approach improves the accuracy of metamodel matching, achieving an average F-measure of $\approx 91 \%$, outperforming both the baseline and hybrid approaches, which have a maximum average F-measure of $\approx \mathbf{2 9 \%}$ and $\approx \mathbf{7 4 \%}$, respectively. Moreover, our approach surpasses single-prompt LLM-based matching, which has an average $\mathbf{F}$-measure of $\mathbf{8 0 \%}$, by approximately $\mathbf{1 1 \%}$.},
  keywords={Accuracy;Costs;Terminology;Databases;Large language models;Semantics;Ontologies;Model driven engineering;Software;Iterative methods;Model-driven Engineering;Metamodel Matching;Domain-Specific Languages;Model Migration},
  doi={10.1109/MSR66628.2025.00040},
  ISSN={2574-3864},
  month={April},}@INPROCEEDINGS{11016516,
  author={Pruski, Cédric and Gallais, Marie and Da Silveira, Marcos},
  booktitle={2025 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Enhancing ESCO with Generative AI: A Dynamic Approach to Supporting 21st Century Education}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={In the rapidly evolving landscape of engineering education, upskilling and lifelong learning have become critical to maintaining competitiveness and fostering innovation. The use of ontologies, such as the European Skills, Competences, Qualifications, and Occupations (ESCO), plays a crucial role in organizing and managing the skills required for modern engineering roles. However, the slow pace of ontology updates and the lack of contextual adaptability present significant challenges, leading to outdated and irrelevant information for educators, learners, and industry professionals. This paper explores the potential of integrating Large Language Models (LLMs) with knowledge engineering to accelerate the process of updating ontologies like ESCO. By dynamically analyzing data and incor-porating contextual information, LLMs offer promising avenues for enhancing the evolution and precision of these ontologies. We discuss the potential impact of this approach in engineering education, particularly in aligning ups killing and reskilling efforts with the demands of emerging technologies such as AI -driven automation and digital engineering. This paper aims to highlight how LLMs can support the creation of more responsive, context-aware learning frameworks, ultimately sustaining educational ex-cellence and fostering critical thinking in engineering education.},
  keywords={Industries;Knowledge engineering;Technological innovation;Large language models;Soft sensors;Ontologies;Robustness;Multilingual;Engineering education;Qualifications;Ontology evolution;Contextualization;LLM;Upskilling;lifelong learning},
  doi={10.1109/EDUCON62633.2025.11016516},
  ISSN={2165-9567},
  month={April},}@INPROCEEDINGS{11016377,
  author={Abu-Rasheed, Hasan and Jumbo, Constance and Al Amin, Rashed and Weber, Christian and Wiese, Veit and Obermaisser, Roman and Fathi, Madjid},
  booktitle={2025 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={LLM-Assisted Knowledge Graph Completion for Curriculum and Domain Modelling in Personalized Higher Education Recommendations}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={While learning personalization offers great potential for learners, modern practices in higher education require a deeper consideration of domain models and learning contexts, to develop effective personalization algorithms. This paper introduces an innovative approach to higher education curriculum modelling that utilizes large language models (LLMs) for knowledge graph (KG) completion, with the goal of creating personalized learning-path recommendations. Our research focuses on modelling university subjects and linking their topics to corresponding domain models, enabling the integration of learning modules from different faculties and institutions in the student's learning path. Central to our approach is a collaborative process, where LLMs assist human experts in extracting high-quality, fine-grained topics from lecture materials. We develop a domain, curriculum, and user models for university modules and stakeholders. We implement this model to create the KG from two study modules: Embedded Systems and Development of Embedded Systems Using FPGA. The resulting KG structures the curriculum and links it to the domain models. We evaluate our approach through qualitative expert feedback and quantitative graph quality metrics. Domain experts validated the relevance and accuracy of the model, while the graph quality metrics measured the structural properties of our KG. Our results show that the LLM-assisted graph completion approach enhances the ability to connect related courses across disciplines to personalize the learning experience. Expert feedback also showed high acceptance of the proposed collaborative approach for concept extraction and classification.},
  keywords={Measurement;Embedded systems;Large language models;Pipelines;Collaboration;Knowledge graphs;Ontologies;Stakeholders;Field programmable gate arrays;Recommender systems;Higher education;Knowledge graph (KG);Large language models (LLM);curriculum model;domain model;Recommender systems},
  doi={10.1109/EDUCON62633.2025.11016377},
  ISSN={2165-9567},
  month={April},}@INPROCEEDINGS{11011971,
  author={Garzo, Grazia and Palumbo, Alessandro},
  booktitle={2025 13th International Symposium on Digital Forensics and Security (ISDFS)}, 
  title={Human-in-the-Loop: Legal Knowledge Formalization in Attempto Controlled English}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Automating legal knowledge through the use of computational languages presents a series of challenges, particularly in translating complex normative texts into formalized representations that are semantically faithful and computationally valid. A salient concern is reconciling the ambiguity inherent in natural legal language with the syntactic constraints of controlled languages. This work proposes a human-in-the-loop methodology for formalizing normative texts in the controlled language Attempto Controlled English (ACE), centered on the interaction between legal experts and computational constraints. Specifically, key legal provisions and fundamental case-law principles were translated into ACE. The findings reveal that even ostensibly straightforward legal provisions necessitate a substantial decomposition, abstraction, and adaptation process. The analysis also identified recurrent linguistic patterns (modal, temporal, and conditional) and reusable translation strategies.},
  keywords={Translation;Systematics;Law;Semantics;Europe;Syntactics;Ontologies;Human in the loop;Cognition;Security;Computable Law;Human-Machine Interaction;Controlled Language;Attempto Controlled English},
  doi={10.1109/ISDFS65363.2025.11011971},
  ISSN={2768-1831},
  month={April},}@INPROCEEDINGS{11014969,
  author={Friedenberger, Dirk and Pirl, Lukas and Boockmeyer, Arne and Schmid, Robert and Polze, Andreas},
  booktitle={2025 IEEE 22nd International Conference on Software Architecture Companion (ICSA-C)}, 
  title={A Train Dispatcher in the Cloud Generated from RDF Models}, 
  year={2025},
  volume={},
  number={},
  pages={111-119},
  abstract={In the context of fast systems and software development, combining model-based systems engineering with automated code generation is essential for managing complexity while enhancing efficiency and adaptability. This paper presents a model-based approach, where a composite RDF model is created, which serves as a flexible basis for the subsequent generation of various artifacts. The artifacts include not only source code but also technical configuration files (e.g. Dockerfiles, Kubernetes objects), CI/CD pipeline configurations, and documentation. The approach has been successfully applied to the Train Dispatcher in the Cloud (ZLiC), a cloud-based approach to digitalize the German Zugleitbetrieb. An iterative development process enabled continuous system expansion and adaptation to specific project requirements. The generated prototype has been validated through simulations and field tests, confirming the robustness and practical applicability of the approach.},
  keywords={Adaptation models;Codes;Source coding;Pipelines;Prototypes;Documentation;Computer architecture;Resource description framework;Iterative methods;Modeling;Model-based systems engineering;ontology-based systems engineering;code generation;railway},
  doi={10.1109/ICSA-C65153.2025.00023},
  ISSN={2768-4288},
  month={March},}@INPROCEEDINGS{11014868,
  author={Davila-Andino, Arturo J. and Huang, Edward and Zaidi, Abbas K.},
  booktitle={2025 IEEE International systems Conference (SysCon)}, 
  title={Identifying Vagueness in Model-Based Systems Engineering Theory Through a Materialist Ontology}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={Model-Based Systems Engineering (MBSE) has had inconsistent implementation across industry and academia. This inconsistency is caused by different definitions of the term “system” have led to theoretical vagueness and contradictions that undermine the effectiveness of MBSE practices including metamodeling. By grounding MBSE in materialist philosophy, we aim to analyze vagueness in current definitions of “system.” We analyze the current definitions by using semantic analysis of decomposing their reference classes and intensions. Where reference classes determine the set of objects that a theory refers to, and intensions are the logical decomposition of a statement as it is written. Our findings reveal that the vagueness in current systems engineering theories is mostly due to the inability to resolve the ontological status of information. That is, current definitions do not clearly state how information interacts in the material world. Moreover, we have found internal inconsistencies and contradictions in current foundational theories, particularly with regard to emergence, interactions, and properties in both real and conceptual systems. We conclude that current systems engineering theories are not suitable for scientific practice. Moreover, we argue that adopting a materialist stance provides a precise ontological basis for defining systems, thereby reducing inconsistencies and enhancing the rigor of MBSE theories.},
  keywords={Industries;Philosophical considerations;Grounding;Semantics;Metamodeling;Ontologies;Systems engineering and theory;Mathematical models;Modeling;System analysis and design;Model-Based Systems Engineering (MBSE);Systems Engineering;Systems Theory;Ontology;Semantic Analysis},
  doi={10.1109/SysCon64521.2025.11014868},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{11014844,
  author={Xu, Tianxiao and Moalla, Néjib and Bentaha, Mohand-Lounes and Aktekin, Hazal and Agostinelli, Claudia},
  booktitle={2025 IEEE International systems Conference (SysCon)}, 
  title={A MBSE-Enhanced Semantically Integration Method for Populating MDAO Design Processes}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={With the development of the automotive market, the complexity of product portfolios is significantly increasing, while the frequency of market requirement changes is also rising. In this context, automotive OEMs face significant challenges in exploring feasible solutions for the next generation of vehicle development. Model-Based Systems Engineering (MBSE) effectively addresses these challenges by managing product complexity and establishing connections between requirements and solutions. However, an alternative solution often necessitates early verification & validation (V&V) and trade-off analysis across multiple disciplines. Multidisciplinary Design Analysis and Optimization (MDAO) has already been applied in solving such multidisciplinary challenges. MBSE is an effective approach for demonstrating multidisciplinary coupling relationships needed to meet specific requirements. By integrating MBSE with MDAO, it is possible to realize traceability from MDAO elements and results to the corresponding system elements and requirements, and to keep data consistency. This paper discusses a method of semantically linking MBSE and MDAO by establishing a MDAO domainspecific metamodel as a SysML profile. This approach positions MDAO as a viewpoint for vehicle development. MBSE serves as the overarching framework throughout different lifecycle stages, ensuring the consistency of MDAO specification. An instantiation of MDAO metamodel has been established to present a coupled multidisciplinary problem structure.},
  keywords={Hands;Couplings;Analytical models;Semantics;Data models;Complexity theory;Modeling;Optimization;Next generation networking;Automotive engineering;Model-Based Systems Engineering (MBSE);Multidisciplinary Design Analysis and Optimization (MDAO);Metamodel;Early verification & validation (V&V);SysML},
  doi={10.1109/SysCon64521.2025.11014844},
  ISSN={2472-9647},
  month={April},}@ARTICLE{11008609,
  author={Donald, Andy and Galanopoulos, Apostolos and Curry, Edward and Muñoz, Emir and Ullah, Ihsan and Waskow, M. A. and Kalra, Manan and Saxena, Sagar and Iqbal, Talha},
  journal={IEEE Access}, 
  title={A Semantic Approach for Linked Model, Data, and Dataspace Cards}, 
  year={2025},
  volume={13},
  number={},
  pages={110194-110207},
  abstract={In artificial intelligence, the significance of thorough documentation of models and datasets for publication is underestimated. However, due to the rising trend in the explainability and fairness of AI models, frameworks like Model Cards, Service Cards and Data Cards have emerged to facilitate understanding and reusing those models and datasets. Moreover, the Dataspace concept integrates these resources into Dataspace Cards, a comprehensive framework that systematically captures and organises crucial information to guide model and data selection for a specific application. This paper advocates a Semantic Web approach for transforming Model/Data Cards into Linked Data or knowledge graphs within a Dataspace, rendering them machine-readable and interoperable. A significant contribution is the development of a vocabulary that unifies Data, Model and Dataspace Card ontologies, enhancing consistent documentation and understanding of the Dataspace design. The paper further demonstrates the applicability of the proposed schema in various use cases, including bias detection in BERT-base-uncased and Large Language Models. Additionally, we propose a conceptual semantic approach, examined in-depth for sentiment and emotion analysis, to highlight how extended Dataspace Cards can improve applicability and outcomes. We found that this unified, ontology-driven approach results in more consistent metadata linking and more fine-grained bias detection in BERT-based-uncased than standalone documentation tools relying solely on Model or Data Cards. Furthermore, compared to existing frameworks, the richer interlinking capabilities of our proposed Dataspace Cards also facilitated easier traceability of performance outcomes, thereby ultimately fostering higher trustworthiness and reusability of AI resources.},
  keywords={Data models;Artificial intelligence;Documentation;Semantics;Adaptation models;Linked data;Europe;Training;Semantic Web;Vocabulary;Semantic web;AI documentation;data cards;model cards;service cards;dataspace cards},
  doi={10.1109/ACCESS.2025.3572211},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11004934,
  author={Almoqren, Nuha and Alrashoud, Mubarak},
  booktitle={2024 International Conference on IT Innovation and Knowledge Discovery (ITIKD)}, 
  title={A Smart Framework for Optimizing User Feedback Prioritization in Application Development}, 
  year={2025},
  volume={},
  number={},
  pages={1-8},
  abstract={In mobile app development, user reviews are a significant source of requirements. Users frequently report bugs, request new features, or suggest enhancements. Mobile app vendors aim to maximize user satisfaction by addressing these continuous comments and requests as early as possible. Typically, they prioritize delivering the most promising features, extracted from user reviews, in early releases while deferring fewer promising ones to later stages. However, due to the massive volume of reviews, redundancy, and conflicts among them, manually extracting requirements is inefficient and often challenging, making requirement prioritization even more difficult. Therefore, automating this process is essential. This paper presents a conceptual framework for the requirements prioritization process's automation, continuity, and scalability. The proposed framework follows a hybrid approach that integrates multiple advanced techniques: generative artificial intelligence, active learning, ontologies, and optimization algorithms. Generative artificial intelligence enables the identification of important patterns and the automatic extraction of requirements and their properties, which aids in assessing properties to determine requirement priorities. The generative artificial intelligence framework integrates with an active learning system to improve annotation efficiency. Ontologies help comprehend relationships, properties, and dependencies among requirements, aligning them with domain-specific knowledge. Optimization methods playa crucial role in the requirements prioritization process by computing the weights of various properties to identify the most effective combination of requirements and determine the optimal order for implementation. Consequently, this research presents a smart theoretical framework for enhancing user-driven maintenance and development of mobile applications. Researchers are tackling several critical challenges that remain unresolved in the field, with future directions focusing on evaluating its applicability and effectiveness in real-world scenarios.},
  keywords={Technological innovation;Reviews;Generative AI;Active learning;Ontologies;Feature extraction;Software;Mobile applications;Software measurement;Software development management;Software Requirements;App Reviews;Generative AI;Optimization;Mobile Applications;Smart Framework},
  doi={10.1109/ITIKD63574.2025.11004934},
  ISSN={},
  month={April},}@ARTICLE{10981743,
  author={Ouriques, Leandro and Barbosa, Carlos Eduardo and Kritz, Joshua and Xexéo, Geraldo},
  journal={IEEE Access}, 
  title={Toward an Ontology of Wargame Design}, 
  year={2025},
  volume={13},
  number={},
  pages={78928-78958},
  abstract={Governments rely on military power to address conflicts, manage crises, and adapt to the evolving nature of modern warfare, which is often characterized by uncertainty and disorder. Wargames, traditionally military tools for simulating conflicts and decision-making, have gained prominence in civilian applications, including business, cybersecurity, disaster management, and critical infrastructure protection. Despite their utility, designing wargames is a time-intensive process with significant challenges, such as scenario creation and decision modeling, necessitating structured and systematic approaches. This research formalizes wargame design through ontology-driven conceptual modeling, structuring its key concepts, characteristics, and elements. Ontologies provide a structured representation of knowledge, facilitating communication, knowledge management, and collaborative design. As a result, we developed core ontologies for wargame design based on the Unified Foundational Ontology (UFO) and implemented them using OntoUML. Our innovation lies in analyzing wargame design processes across various countries and military organizations to develop a comprehensive reference model for wargame design. Additionally, we are the first to apply UFO for conceptual modeling of the wargame domain. These ontologies enhance wargame design by fostering standardization, adaptability, and support for intelligent systems, enabling dynamic and responsive scenarios. These contributions enhance wargame design efficiency and effectiveness, applicable to military and civilian contexts.},
  keywords={Ontologies;Games;Decision making;Adaptation models;Uncertainty;Training;Systematics;Planning;Personnel;Object recognition;Ontology;ontology-driven conceptual modeling (ODCM);OntoUML;unified foundational ontology (UFO);wargame;wargame design;wargaming},
  doi={10.1109/ACCESS.2025.3566249},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10974109,
  author={Cooper, Sara and Ros, Raquel and Lemaignan, Séverin and Gebellí, Ferran and Ferrini, Lorenzo and Juričić, Luka},
  booktitle={2025 20th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
  title={Demonstration of an Open-Source ROS 2 Framework and Simulator for Situated Interactive Social Robots}, 
  year={2025},
  volume={},
  number={},
  pages={1770-1772},
  abstract={We introduce an open-source ROS 2 architecture for situated social robots, along with a simulator that allows mixed-reality development and interactions. The architecture is a hybrid symbolic/subsymbolic system that integrates explicit ontology semantics for perception, reasoning, and execution, with LLMs. It features multimodal social perception by leveraging the open source ROS4HRI framework; LLMs (both edge- and cloud-based) to facilitate natural language interaction between the user and system; KnowledgeCore, an open-source knowledge base, to reason about facts in the world; and an intent-based controller to supervise the execution of parallel/sequential tasks and skills. We demonstrate our system architecture with a social robot running the mixed-reality system.},
  keywords={Social robots;Semantics;Natural languages;Knowledge based systems;Systems architecture;Virtual reality;Ontologies;Cognition;Situated social robots;ROS 2 framework;mixed-reality simulator},
  doi={10.1109/HRI61500.2025.10974109},
  ISSN={},
  month={March},}@ARTICLE{10971367,
  author={Kim, Seungyeon and Kim, Donghyun and Hwang, Seokju and Lee, Kyong-Ho and Lee, Kyunghwa},
  journal={IEEE Access}, 
  title={LLM-Assisted Ontology Restriction Verification With Clustering-Based Description Generation}, 
  year={2025},
  volume={13},
  number={},
  pages={73603-73618},
  abstract={An ontology is a scheme for structuring relationships between concepts in a domain, promoting data interoperability and system integration. However, poorly designed ontologies can lead to errors and performance issues. While systems engineering has standardized evaluation guidelines (e.g., ISO/IEC), ontology engineering lacks such standards, leading to various independent evaluation methods. One frequent issue among novice developers is the misuse of ontology restrictions, particularly ‘allValuesFrom’ and ‘someValuesFrom’, which can significantly impact the correctness and reliability of ontologies. However, existing studies have not adequately addressed effective methods for detecting such errors. To address this gap, we propose a context-aware verification framework utilizing large language models to detect and correct misuse in ontology restrictions. Unlike conventional methods, our framework integrates contextual descriptions derived from ontological axioms, enabling more accurate verification. Additionally, we introduce a clustering-based description generation method that systematically organizes contextual information, further enhancing verification accuracy. Experimental evaluation conducted on diverse ontology datasets suggests that contextual integration improves verification performance. Moreover, the clustering-based description generation improves restriction misuse detection and correction compared to traditional approaches. By automating ontology restriction verification, this study contributes significantly to enhancing the reliability of ontology evaluation and provides a foundation for developing more scalable and standardized verification techniques.},
  keywords={Ontologies;Accuracy;Reliability;ISO Standards;IEC Standards;Translation;Software;Semantics;Scalability;Quality assessment;Ontology evaluation;ontology restriction verification;text generation;clustering},
  doi={10.1109/ACCESS.2025.3562560},
  ISSN={2169-3536},
  month={},}@ARTICLE{10962554,
  author={Chen, Jiaoyan and Mashkova, Olga and Zhapa-Camacho, Fernando and Hoehndorf, Robert and He, Yuan and Horrocks, Ian},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Ontology Embedding: A Survey of Methods, Applications and Resources}, 
  year={2025},
  volume={37},
  number={7},
  pages={4193-4212},
  abstract={Ontologies are widely used for representing domain knowledge and meta data, playing an increasingly important role in Information Systems, the Semantic Web, Bioinformatics and many other domains. However, logical reasoning that ontologies can directly support are quite limited in learning, approximation and prediction. One straightforward solution is to integrate statistical analysis and machine learning. To this end, automatically learning vector representation for knowledge of an ontology i.e., ontology embedding has been widely investigated. Numerous papers have been published on ontology embedding, but a lack of systematic reviews hinders researchers from gaining a comprehensive understanding of this field. To bridge this gap, we write this survey paper, which first introduces different kinds of semantics of ontologies and formally defines ontology embedding as well as its property of faithfulness. Based on this, it systematically categorizes and analyses a relatively complete set of over 80 papers, according to the ontologies they aim at and their technical solutions including geometric modeling, sequence modeling and graph propagation. This survey also introduces the applications of ontology embedding in ontology engineering, machine learning augmentation and life sciences, presents a new library mOWL and discusses the challenges and future directions.},
  keywords={Ontologies;Resource description framework;OWL;Semantics;Vectors;Machine learning;Surveys;Knowledge engineering;Logic;Cognition;Ontology;ontology embedding;web ontology language;knowledge graph;representation learning},
  doi={10.1109/TKDE.2025.3559023},
  ISSN={1558-2191},
  month={July},}@ARTICLE{10962207,
  author={Mishra, Pratham and Narayanasamy, Senthil Kumar and Srinivasan, Kathiravan},
  journal={IEEE Access}, 
  title={Context-Aware Embedded Language Transformers for Evaluating Climate Change-Based Sustainable Development Goals}, 
  year={2025},
  volume={13},
  number={},
  pages={65757-65775},
  abstract={This research work addresses the pressing issue of climate change and the urgent need for a comprehensive and reliable dataset that can assist stakeholders, policymakers and researchers in making informed and data-driven decisions. We propose the process for leveraging publicly available raw data such as news articles and social media posts and help to create meaningful ontologies. The study demonstrates the use of advanced Natural Language Processing and Deep Learning techniques to transform raw data into potential insights. Additionally, we present a methodology for utilizing these generated ontologies to anticipate the impacts of decisions on climate change. The primary objective of this research is to present a Context-Aware Embedded Language Transformers model that can be easily integrated into various pipelines by generating meaningful ontologies to support more informed decision-making. These ontologies will serve as knowledge graphs and contribute to a large dataset for future research in the field and address the current lack of comprehensive resources.},
  keywords={Climate change;Meteorology;Sustainable development;Ontologies;Transformers;Social networking (online);Decision making;Data models;Databases;Surveys;Climate change;graph convolution networks (GCNs);knowledge graphs;natural language processing (NLP);sustainable development goals;transformers},
  doi={10.1109/ACCESS.2025.3559548},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10947388,
  author={Coffelt, Jeremy Paul and Kampmann, Peter and Beetz, Michael},
  booktitle={2025 IEEE Underwater Technology (UT)}, 
  title={Implementation and Application of a Knowledge Service for AUV Mission Explainability}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={This paper presents a knowledge service aimed at enhancing mission explainability in subsea robotics operations. The proposed system consists of an AI agent that chains together specialized large language models (LLMs) and a graph database to enable natural language querying and interactive visualization. The graph database models entities and relationships relevant to subsea inspection and maintenance, such as clients, industries, sites, vehicles, sensors, and underwater scene elements, including pipeline components and seafloor characteristics. The browser-based GUI aims to allow stakeholders—including field teams, robot developers, industry clients, and regulatory agencies—to intuitively interact with mission data, supporting post-mission analysis and explainability. Using pipeline inspections as a case study, we illustrate the potential of this approach and discuss future developments needed to advance this framework toward a practical solution for subsea robotics.},
  keywords={Industries;Service robots;Large language models;Pipelines;Natural languages;Sensor phenomena and characterization;Inspection;Robot sensing systems;Visual databases;Underwater technology;AI agent;graph database;subsea robotics;knowledge representation and reasoning;mission explainability},
  doi={10.1109/UT61067.2025.10947388},
  ISSN={},
  month={March},}@INPROCEEDINGS{10943742,
  author={Singh, Chandan Kumar and Kumar, Devesh and Sanap, Vipul and Sinha, Rajesh},
  booktitle={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={LLM-RSPF: Large Language Model-Based Robotic System Planning Framework for Domain Specific Use-cases}, 
  year={2025},
  volume={},
  number={},
  pages={7277-7286},
  abstract={The employment of large language models (LLMs) for task planning and reasoning has emerged as a focal point of interest within the robotics research community. However, directly applying LLMs, even with large token-sized prompts, does not achieve the task planning performance required for an industrial-grade domain-specific use-case (DSU). This work aims to overcome the obstacles of a robotic task planner for DSUs by introducing a novel planning framework, LLM-RSPF (Large Language Model-based Robotic System Planning Framework). Central to the LLM-RSPF is a novel robotic system ontology that organizes the components of the robotic system in a coherent and a systematic manner. The ontology empowers the LLM-RSP F to efficiently capture a contextual representation of the DSU using the LLMs. Subsequently, the research introduces a LLM-tuning regimen referred as chain of hierarchical thought (CoHT), specifically crafted to complement the proposed system ontology. Integrating these two components, the LLM-RSPF aims to enhance the accuracy, robustness, and throughput of a robotic system in a cost-effective manner. In addition, the research presents an empirical methodology to generate the LLM-tuning dataset size for a guaranteed performance. The LLM-RSPF is validated on a retail order-fulfillment use-case thereby, illustrating the efficacy of the framework. Through rigorous evaluation, the LLM-RSPF demonstrates exceptional performance on the generated dataset, effectively meeting the DSU objectives.},
  keywords={Solid modeling;Accuracy;Three-dimensional displays;Systematics;Service robots;Ontologies;Throughput;Robustness;Planning;Robots;coht;domain-specific use-case;large language model;robotic system ontology;task planning},
  doi={10.1109/WACV61041.2025.00707},
  ISSN={2642-9381},
  month={Feb},}@INPROCEEDINGS{10935139,
  author={Abolhasani, Mohammad Sadeq and Pan, Rong},
  booktitle={2025 Annual Reliability and Maintainability Symposium (RAMS)}, 
  title={OntoKGen: A Genuine Ontology and Knowledge Graph Generator Using Large Language Model}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Extracting relevant and structured knowledge from large, complex technical documents within the Reliability and Maintainability (RAM) domain is labor-intensive and prone to errors. Our work addresses this challenge by presenting OntoKGen, a Genuine pipeline for Ontology extraction and Knowledge Graph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through an interactive user interface guided by our adaptive iterative Chain of Thought (CoT) algorithm to ensure that the ontology extraction process and, thus, KG generation align with user-specific requirements. Although KG generation follows a clear, structured path based on the confirmed ontology, there is no universally correct ontology as it is inherently based on the user's preferences. OntoKGen recommends an ontology grounded in best practices, minimizing user effort and providing valuable insights that may have been overlooked, all while giving the user complete control over the final ontology. Having generated the KG based on the confirmed ontology, OntoKGen enables seamless integration into schemeless, non-relational databases like Neo4j. This integration allows for flexible storage and retrieval of knowledge from diverse, unstructured sources, facilitating advanced querying, analysis, and decision-making. Moreover, the generated KG serves as a robust foundation for future integration into Retrieval-Augmented Generation (RAG) systems, offering enhanced capabilities for developing domain-specific intelligent applications.},
  keywords={Large language models;Retrieval augmented generation;Pipelines;Random access memory;Knowledge graphs;Ontologies;User interfaces;Reliability engineering;Generators;Prompt engineering;Large Language Model;Ontology and Knowledge Graph Generator;Prompt Engineering;Neo4J},
  doi={10.1109/RAMS48127.2025.10935139},
  ISSN={2577-0993},
  month={Jan},}@INPROCEEDINGS{10937875,
  author={Khalil, Ibrahim and Ahmad, Israr and Rasheed, Uzair and Butt, Wasi Haider and Anwaar, Zaeem},
  booktitle={2025 6th International Conference on Advancements in Computational Sciences (ICACS)}, 
  title={Detecting Cross-Domain Ambiguity In Requirements Through Natural Language Processing, A Systematic Literature Review}, 
  year={2025},
  volume={},
  number={},
  pages={1-7},
  abstract={A strong foundation for a project is the first step to measuring or estimating the success parameters of a project. Requirement engineers gather and elicit the requirements of the project in detail after the feasibility study. A project’s success can be measured/estimated if and only if the initially collected requirements are clear, unambiguous, and well-understood. Similarly, ambiguous or not understandable requirements can lead to failure or closure of the project in disastrous form. An initial step in requirement elicitation is usually gathering requirements in natural language. This study analyzes different tools, techniques, and approaches in different research articles used for detecting ambiguities in requirements in the natural language. We identified 33 research articles, published during 2012-24 through a Systematic Literature Review. For automated ambiguities detection in requirements engineering, we identified 17 tools & techniques and 12 NLP approaches. We analyzed the better approach for cross-domain ambiguity detection. SGNS (Skipgram Negative Sampling) variant of Word2Vec for the cross-domain ambiguity detection in the review of the study, resulted to be the most efficient technique implemented with the usage view and easily scalable for multiple requirements and domains.},
  keywords={Scalability;Software algorithms;Natural language processing;Software;Requirements engineering;Systematic literature review;requirement’s engineering;natural language processing (nlp);ambiguity detection in natural language requirements;cross-domain ambiguity},
  doi={10.1109/ICACS64902.2025.10937875},
  ISSN={2616-3330},
  month={Feb},}@INPROCEEDINGS{10932144,
  author={Naik, Anushka and Naik, Akanksha and Deshmukh, Pratiksha},
  booktitle={2025 1st International Conference on AIML-Applications for Engineering & Technology (ICAET)}, 
  title={ProtGAT: Refining Antimicrobial Resistance Detection with Graph-Based Deep Learning*}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={The rise of antimicrobial resistance (AMR) is a critical threat to global health, necessitating urgent advancements in diagnostic capabilities. Current methods for detecting AMR are hampered by slow processing times and a lack of precision, making early and accurate detection challenging. This paper introduces ProtGAT, a novel computational framework that combines ProteinBERT and Graph Attention Networks (GAT) to enhance the detection of AMR. Our model leverages deep learning to analyze complex protein sequences and their interactions, improving the accuracy of AMR predictions. The integration of ProteinBERT provides a robust feature extraction from protein sequences, while GAT focuses on identifying key relational features within these sequences. Early testing of ProtGAT demonstrates its superior performance over traditional models, particularly in identifying novel AMR sequences that evade conventional detection methods. This study highlights the potential of advanced computational approaches to revolutionize AMR diagnostics, offering faster and more reliable tools for healthcare providers in combating this growing public health concern.},
  keywords={Deep learning;Accuracy;Computational modeling;Refining;Feature extraction;Protein sequence;Reliability;Public healthcare;Immune system;Testing;ProteinBERT;Graph Attention Networks;Antimicrobial Resistance;Protein Sequence Analysis;Computational Biology},
  doi={10.1109/ICAET63349.2025.10932144},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10932270,
  author={Desai, Arun and Kulkarni, Anagha},
  booktitle={2025 1st International Conference on AIML-Applications for Engineering & Technology (ICAET)}, 
  title={Unleashing the Potential of Ontology in Skill Extraction}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Research reveals ontology's ability to extract information about skills from online job sites by giving a structured and semantically rich representation of skills. The study talks about more accurate and thorough skill profiling by systematically building ontological models that allow for an indepth knowledge of the complex links between skills, abilities, and domains. In this research, we study papers from different methods like supervised learning, unsupervised learning, and LLMs. The paper begins by providing not only an overview what is new in ontology generation but also its application in the context of skill extraction. It then delves into the challenges and opportunities associated with ontology-based skill extraction, highlighting the ways of processing natural language in bridging the gap between unstructured text and the formal representation of skills and competencies. Furthermore, the paper presents a comprehensive framework for ontology-driven skill extraction, emphasizing the importance of contextual awareness and the identification of implicit skills that may not be explicitly stated in the source text. The potential implications of this approach are manifold, as it could significantly impact various aspects of the talent management ecosystem.},
  keywords={Manifolds;Accuracy;Biological system modeling;Supervised learning;Natural languages;Ecosystems;Machine learning;Ontologies;Data mining;Unsupervised learning;Ontology;skill extraction;skills;Jobs;Machine Learning},
  doi={10.1109/ICAET63349.2025.10932270},
  ISSN={},
  month={Jan},}@ARTICLE{10922135,
  author={Xiahou, Xiaer and Chen, Gaotong and Li, Zirui and Xu, Xin and Li, Qiming},
  journal={IEEE Transactions on Engineering Management}, 
  title={Knowledge Management in Construction Quality Management: Current State, Challenges, and Future Directions}, 
  year={2025},
  volume={72},
  number={},
  pages={1069-1088},
  abstract={Construction quality management (CQM), as one of the major activities in construction project management, relies heavily on knowledge. Unfortunately, the knowledge of CQM is diverse in format and scattered in different stakeholders within the whole construction processes. Therefore, knowledge management (KM) of CQM is underinvestigated. To offering a comprehensive view of KM in CQM, this article employed a mixed review method to critically review 87 related articles. The results indicate 1) building information modeling, ontology, and natural language processing are identified as critical technologies in KM, 2) expert system and decision support, structural health monitoring, and project management are the major application domains. This article conducts an in-depth analysis of the literature based on the three phases of quality control: pre-construction, in-construction, and post-construction. The results are discussed to critically assess the critical technologies in KM. A framework is proposed to guide the effective implementation of KM in CQM, alongside a discussion of the current challenges and opportunities. The article further identifies potential development directions for KM in CQM, including total quality management, digital twins, development of large language models, construction of “No-cost” KM platforms, uniform evaluation and standardization mechanisms, tacit knowledge capture, and confidentiality and security. A novel paradigm for knowledge-driven quality management decision-making is first introduced. This article offers a comprehensive perspective on the application of KM in CQM, which will significantly enhance the effectiveness of CQM implementation in the future.},
  keywords={Quality management;Reviews;Industries;Engineering management;Training;Data mining;Costs;Stakeholders;Complexity theory;Total quality management;Construction quality management (CQM);knowledge management (KM);knowledge-driven quality management;mixed review;quality control},
  doi={10.1109/TEM.2025.3550354},
  ISSN={1558-0040},
  month={},}@ARTICLE{10904297,
  author={Hosseini, Ali M. and Kastner, Wolfgang and Sauter, Thilo},
  journal={IEEE Open Journal of the Industrial Electronics Society}, 
  title={Leveraging LLMs and Knowledge Graphs to Design Secure Automation Systems}, 
  year={2025},
  volume={6},
  number={},
  pages={380-395},
  abstract={The digital transformation of Industrial Control Systems (ICSs) within the Industry 4.0 paradigm is essential for industrial organizations to remain competitive, while cybersecurity is an enabler. However, security measures, often implemented late in the engineering process, lead to costly and complicated implementations. Thus, this article is concerned with the “security by design” principle in ICSs and facilitates compliance with ICS security standards, which can be legally mandated for some critical systems or adopted by asset owners to protect their assets. Current methods for compliance demand manual efforts from security experts, making the compliance process time-consuming and costly. To address this, we propose a framework for leveraging large language models (LLMs) combined with knowledge graphs to automate the interpretation of security requirements and system architecture as two main elements of the design phase. Our knowledge graph-augmented LLM framework converts system architectures into human natural language, enhancing the automation of various security analyses, especially those that need to handle textual requirements. The framework enables validating applicable security requirements provided by IEC 62443-3-3 (a widely-used ICS security standard) concerning system designs through a question-and-answer interface. To evaluate the framework, various questions with reference responses from human experts were prepared in the context of a use case, and the quality of the LLMs' responses was measured across various metrics. Moreover, we compared the framework with a baseline approach based on formal queries. The results show that the proposed framework effectively automates security tasks and offers a user-friendly interface accessible to nonexperts.},
  keywords={Security;Ontologies;Knowledge graphs;Computer security;IEC Standards;Systems architecture;Cyberattack;Cognition;Natural languages;Large language models;Industrial control system (ICS);security by design;knowledge graph;large language model (LLM);ontology},
  doi={10.1109/OJIES.2025.3545811},
  ISSN={2644-1284},
  month={},}@ARTICLE{10891880,
  author={Yang, Yang and Shen, Wei and Shu, Junfeng and Liu, Yinan and Curry, Edward and Li, Guoliang},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={CMVC+: A Multi-View Clustering Framework for Open Knowledge Base Canonicalization Via Contrastive Learning}, 
  year={2025},
  volume={37},
  number={5},
  pages={2296-2310},
  abstract={Open information extraction (OIE) methods extract plenty of OIE triples $< $<noun phrase, relation phrase, noun phrase$> $> from unstructured text, which compose large open knowledge bases (OKBs). Noun phrases and relation phrases in such OKBs are not canonicalized, which leads to scattered and redundant facts. It is found that two views of knowledge (i.e., a fact view based on the fact triple and a context view based on the fact triple's source context) provide complementary information that is vital to the task of OKB canonicalization, which clusters synonymous noun phrases and relation phrases into the same group and assigns them unique identifiers. In order to leverage these two views of knowledge jointly, we propose CMVC+, a novel unsupervised framework for canonicalizing OKBs without the need for manually annotated labels. Specifically, we propose a multi-view CHF K-Means clustering algorithm to mutually reinforce the clustering of view-specific embeddings learned from each view by considering the clustering quality in a fine-grained manner. Furthermore, we propose a novel contrastive learning module to refine the learned view-specific embeddings and further enhance the canonicalization performance. We demonstrate the superiority of our framework through extensive experiments on multiple real-world OKB data sets against state-of-the-art methods.},
  keywords={Contrastive learning;Clustering algorithms;Knowledge based systems;Organizations;Electronic mail;Data mining;Ontologies;Information retrieval;Indexes;Training;Open knowledge base canonicalization;multi-view clustering;contrastive learning},
  doi={10.1109/TKDE.2025.3543423},
  ISSN={1558-2191},
  month={May},}@ARTICLE{10887205,
  author={Huang, Wenhao and Zhang, Jiahao and Li, Xin and Zhou, Xiao and Qi, Deyu and Xi, Jianqing and Liu, Wenjun},
  journal={IEEE Access}, 
  title={A Semantic and Intelligent Focused Crawler based on BERT Semantic Vector Space Model and Hybrid Algorithm (October 2024)}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={The goal of a focused crawler is to selectively fetch pages that are relevant to a given topic. Previous crawlers use text content to determine text topic relevance and manually determined weighting factors to predict the priority of unvisited URLs. However, there are still some problems in the above focused crawler methods, the calculation formula of semantic similarity between words is flawed. The weighting factor for the priority of unvisited URLs is determined arbitrarily. In order to solve the above problems, this paper proposes a semantic and intelligent focused crawler based on BERT semantic vector space model and hybrid algorithm. This method used BERT semantic vector space model to calculate the topic relevance of documents, and used a hybrid algorithm to optimize the weighting factor of unvisited URL priority. The experimental results show that the proposed BSVSM-HA crawler can obtain better evaluation indicators compared with the other three crawlers including Word2vec crawler, ELMO crawler and BSVSM crawler. In conclusion, the semantic and intelligent crawler proposed in this paper makes the semantic similarity between terms more accurate, and improves the topic relevance of the text, and the optimized weighting factor makes the priority evaluation of unvisited URLs more accurate.},
  keywords={Crawlers;Semantics;Vectors;Web pages;Uniform resource locators;Hypertext systems;Accuracy;Ontologies;Training;Search problems;Focused Crawler;Semantic Vector Space Model;Hybrid Algorithm},
  doi={10.1109/ACCESS.2025.3542064},
  ISSN={2169-3536},
  month={},}@ARTICLE{10870246,
  author={Kim, Han Kyul and Park, Yujin and Kim, Yoon Ji and Yi, Seungah and Park, Yeju and So, Sujin and Lee, Hyeon-Ji and Bae, Ye Seul},
  journal={IEEE Access}, 
  title={EILEEN: A Multi-Modal Framework for Extracting Alcohol Consumption Patterns From Bilingual Clinical Notes}, 
  year={2025},
  volume={13},
  number={},
  pages={25741-25751},
  abstract={In this work, we introduce EILEEN (Efficient Inference for Language-based Extraction of EHR Notes), a novel multi-modal natural language processing (NLP) framework designed to extract various alcohol consumption patterns from unstructured clinical notes, particularly in bilingual and non-English contexts. Recent advances in NLP have significantly improved information extraction capability across various domains. However, identifying patterns of alcohol consumption in medical documents remains underexplored, with existing approaches heavily relying on traditional NLP methods such as bag-of-words models that require extensive text preprocessing. These methods are often limited to English-language clinical settings, where robust medical ontologies and NLP toolkits are available to support preprocessing tasks. Therefore, this limitation hinders their use in multilingual healthcare settings and in environments lacking robust NLP toolkits to facilitate preprocessing. Motivated by the need for a more generalizable and accurate approach, this paper investigates the impact of large language models (LLMs) in advancing alcohol consumption pattern extraction from clinical notes. By reducing the need for manual preprocessing and improving adaptability to multilingual clinical notes, this work aims to enable broader, more practical applications of NLP models in extracting alcohol consumption patterns from clinical notes. By fine-tuning multilingual language models along with additional data sources, EILEEN effectively analyzes unstructured electronic health records (EHR) without relying on traditional concept normalization or extensive text preprocessing resources. Furthermore, the multi-modal component of EILEEN enables it to integrate and leverage diverse types of alcohol-related information, such as various types and amounts of alcohol consumed by a patient, thereby improving its pattern extraction accuracy. Our experiments, conducted in two different medical institutions in Korea, demonstrate that EILEEN significantly outperforms existing NLP methods in accurately identifying clinically relevant alcohol consumption patterns. By providing accurate, detailed, and clinically useful alcohol consumption patterns from unstructured clinical notes, EILEEN empowers healthcare practitioners with actionable insights essential for informed clinical decision-making.},
  keywords={Natural language processing;Multilingual;Accuracy;Data mining;Ontologies;Hospitals;Vectors;Unified modeling language;Transformers;Feature extraction;Clinical informatics;alcohol information extraction;natural language processing;multimodal learning;multilingual transformers},
  doi={10.1109/ACCESS.2025.3538803},
  ISSN={2169-3536},
  month={},}@ARTICLE{10858700,
  author={Yhdego, Tsegai O. and Wang, Hui},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Automated Ontology Generation for Zero-shot Defect Identification in Manufacturing}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={A lack of labeled data presents a significant challenge to automatic defect identification in manufacturing, which is a crucial step in process control and certification during process development. State-of-the-art transfer learning is incapable of handling such zero-shot learning (ZSL) when defect labels are absent in training datasets. The latest research on ZSL leverages natural language processing (NLP) based on large language models (LLM) and shows promise by supplementing information to generate labels. However, its performance is hampered by the supporting LLMs pre-trained on generic vocabulary that failed to characterize manufacturing defects accurately. This paper establishes a methodology to automatically extract multi-level attributes from literature to improve defect representation, thereby facilitating ZSL. The extracted attributes contribute to a hierarchical knowledge graph, called defect ontology, to characterize multiple aspects of manufacturing defects. The proposed algorithm takes the defect images and associated text from the literature as input and develops an unsupervised method to identify the hierarchical relationships among the tokenized information extracted from the input text-feature corpora. The hierarchical graph is refined to retain the most relevant information by a pruning algorithm based on a minimum path search. A walk algorithm, along with NLP, parsed the generated ontology to create embedding of defects to enable zero-shot attribute learning to identify defects. The proposed method advances the ZSL methodology by automatically creating a hierarchical knowledge representation from literature and images to replace generic vocabulary in LLM adopted by ZSL algorithms, thus improving defect representation. The case studies are among the earlier attempts to demonstrate the feasibility of using literature data from public sources to extract attributes automatically to identify defects in a real additive manufacturing process based on direct-ink-writing.},
  keywords={Manufacturing;Zero shot learning;Ontologies;Automation;Data mining;Vocabulary;Accuracy;Transfer learning;Certification;Process control;Zero-shot learning;defect identification;manufacturing automation;self-supervised learning},
  doi={10.1109/TASE.2025.3537463},
  ISSN={1558-3783},
  month={},}@ARTICLE{10841380,
  author={Wang, Wenkang and Shuai, Yunyan and Li, Yiming and Zeng, Min and Li, Min},
  journal={IEEE Transactions on Computational Biology and Bioinformatics}, 
  title={Enhancing Protein Function Prediction Through the Fusion of Multi-Type Biological Knowledge With Protein Language Model and Graph Neural Network}, 
  year={2025},
  volume={22},
  number={2},
  pages={581-590},
  abstract={Proteins play crucial roles in diverse biological functions. Accurately annotating their functions is essential for understanding cellular mechanisms and developing therapies for complex diseases. Computational methods have been proposed as alternatives to labor-intensive and expensive experimental approaches. Existing computational methods have demonstrated that protein evolution information and Protein-Protein Interactions (PPIs) are essential for protein function prediction. However, traditional computational approaches for generating evolution information are time-consuming. On the other hand, proteins lacking interactions are ignored in previous studies. To address these limitations, we propose a novel deep learning framework, named DeepFMB, which incorporates multi-type biological knowledge. DeepFMB leverages a pre-trained protein language model to extract evolution information. Moreover, DeepFMB generates PPI-related features and orthology-related features using graph neural networks on the constructed PPI and orthology networks. Then, these multi-type features are fused adaptively for protein function prediction. Compared to eight state-of-the-art methods, DeepFMB outperforms all of them in terms of F-max and AUPR. Additionally, with the combination of sequence similarity-based inference, our predicted model predicts protein functions more accurately. Experimental results also validate the superior performance of our methods in predicting low-frequency GO terms. Ablation studies demonstrate that the multi-type biological knowledge we use is highly relevant to protein functions.},
  keywords={Proteins;Feature extraction;Databases;Biological system modeling;Biological information theory;Predictive models;Protein sequence;Evolution (biology);Aggregates;Knowledge engineering;Protein function prediction;protein-protein interactions;orthology relations;pre-trained protein language model;graph neural network},
  doi={10.1109/TCBBIO.2025.3529301},
  ISSN={2998-4165},
  month={March},}@ARTICLE{10819318,
  author={Wu, Yanting and Sun, Yicheng and Wen, Xiaojian and Liu, Xiaoqiang and Bao, Jinsong and Wang, Sen},
  journal={IEEE Internet Computing}, 
  title={A Generative Modeling Method for Digital Twin Shop Floor}, 
  year={2025},
  volume={29},
  number={1},
  pages={24-31},
  abstract={Digital twin (DT) as a key enabling technology for achieving digitization, flexibility, and customization in shop floors has attracted significant attention. However, the shop floor involves diverse assets across multiple dimensions, scales, and interdisciplinary fields, making the modeling process complex. To address this issue, this article analyzes the construction process of ontology-based information models and proposes a generative modeling method for digital twin shop floor driven by large language models (LLMs). First, LLMs are utilized to analyze user intentions, acquiring the hierarchical object structure of DT models. Second, by combining an analysis–retrieval method to extract domain knowledge and generate dynamic prompts, LLMs are guided to realize the creation and fusion of objects and construct structured and semantically enriched DT models. Finally, the effectiveness of the proposed method is validated through examples of shop floor resource scheduling.},
  keywords={Data models;Object oriented modeling;Digital twins;Analytical models;Ontologies;Semantics;Context modeling;Computational modeling;Internet;Natural languages;Industrial facilities;Job shop scheduling},
  doi={10.1109/MIC.2024.3522301},
  ISSN={1941-0131},
  month={Jan},}@INBOOK{10766917,
  author={Cady, Field},
  booktitle={The Data Science Handbook}, 
  title={Traditional Natural Language Processing}, 
  year={2025},
  volume={},
  number={},
  pages={197-208},
  abstract={Summary <p>This chapter discusses techniques for natural language process, especially the ones that pre&#x2010;date large language models. It starts with bag&#x2010;of&#x2010;words &#x2013; the central concept for turning text into a numerical vector &#x2013; and covers increasingly sophisticated ways of using linguistic concepts (lemmatization, synsets, etc.) to improve the vectorization process. Classic problems like sentiment analysis and topic modeling, which you can do once the data is vectorized, are covered along with more advanced topics like syntax trees and ontologies.</p>},
  keywords={Natural language processing;Vectors;Training;Web pages;Training data;Tokenization;Libraries;Internet;Computer hacking;Turning},
  doi={10.1002/9781394234523.ch16},
  ISSN={},
  publisher={Wiley},
  isbn={9781394234516},
  url={https://ieeexplore.ieee.org/document/10766917},}@ARTICLE{10721368,
  author={Dalal, Sumit and Tilwani, Deepa and Gaur, Manas and Jain, Sarika and Shalin, Valerie L. and Sheth, Amit P.},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={A Cross Attention Approach to Diagnostic Explainability Using Clinical Practice Guidelines for Depression}, 
  year={2025},
  volume={29},
  number={2},
  pages={1333-1342},
  abstract={The lack of explainability in using relevant clinical knowledge hinders the adoption of artificial intelligence-powered analysis of unstructured clinical dialogue. A wealth of relevant, untapped Mental Health (MH) data is available in online communities, providing the opportunity to address the explainability problem with substantial potential impact as a screening tool for both online and offline applications. Inspired by how clinicians rely on their expertise when interacting with patients, we leverage relevant clinical knowledge to classify and explain depression-related data, reducing manual review time and engendering trust. We developed a method to enhance attention in contemporary transformer models and generate explanations for classifications that are understandable by mental health practitioners (MHPs) by incorporating external clinical knowledge. We propose a domain-general architecture called ProcesS knowledge-infused cross ATtention (PSAT) that incorporates clinical practice guidelines (CPG) when computing attention. We transform a CPG resource focused on depression, such as the Patient Health Questionnaire (e.g. PHQ-9) and related questions, into a machine-readable ontology using SNOMED-CT. With this resource, PSAT enhances the ability of models like GPT-3.5 to generate application-relevant explanations. Evaluation of four expert-curated datasets related to depression demonstrates PSAT’s application-relevant explanations. PSAT surpasses the performance of twelve baseline models and can provide explanations where other baselines fall short.},
  keywords={Depression;Ontologies;Computer architecture;Guidelines;Mental health;Unified modeling language;Medical treatment;Manuals;Knowledge based systems;Closed box;Cross attention;depression;explainable;language models;mental health;PHQ-9},
  doi={10.1109/JBHI.2024.3483577},
  ISSN={2168-2208},
  month={Feb},}@ARTICLE{10684379,
  author={Yang, Jian and Shu, Liqi and Duan, Huilong and Li, Haomin},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={RDguru: A Conversational Intelligent Agent for Rare Diseases}, 
  year={2025},
  volume={29},
  number={9},
  pages={6366-6378},
  abstract={Large language models (LLMs) hold significant promise in clinical practice, yet their real-world adoption is constrained by their propensity to produce erroneous and occasionally harmful outputs, particularly in the intricate domain of rare diseases (RDs). This study introduces RDguru, a conversational intelligent agent leveraging the LangChain framework and powered by GPT-3.5-turbo. RDguru offers a comprehensive suite of functionalities, encompassing evidence-traceable knowledge Q&A and professional medical consultations for differential diagnosis (DDX), integrating authoritative knowledge sources and reliable tools. A novel multi-source fusion diagnostic model, rooted in deep Q-network, amalgamates three diagnostic recommendation strategies (GPT-4, PheLR, and phenotype matching) to enhance diagnostic recall during medical consultations. Through tailored tools and advanced algorithms for retrieval-augmented generation, RDguru excels in knowledge Q&A, automated phenotype annotation, and RD DDX. A multi-aspect Q&A analysis demonstrates RDguru outperforms ChatGPT in generating descriptions aligned with authoritative knowledge, quantified by ROUGE scores, GPT-4-based automatic rating, and RAGAs evaluation metrics. Testing on 238 published RD cases reveals that RDguru's top 5 multi-source fusion diagnoses recapture 63.87% of actual diagnoses, marking a 5.47% improvement over the state-of-the-art diagnostic method PheLR. Furthermore, RDguru's consultation strategy proves effective in eliciting diagnostically beneficial phenotypes and refining the prioritization of genuine diagnoses through multi-round phenotype-orient questioning. Evaluations against established benchmarks and real-world patient data demonstrate RDguru's efficacy and reliability, highlighting its potential to enhance clinical decision-making in the realm of RDs.},
  keywords={Medical diagnostic imaging;Diseases;Phenotypes;Cognition;Intelligent agents;Bioinformatics;Knowledge engineering;Conversational AI;deep Q-network;knowledge Q&A;large language model;medical consultation;rare diseases},
  doi={10.1109/JBHI.2024.3464555},
  ISSN={2168-2208},
  month={Sep.},}@ARTICLE{10637955,
  author={Rezayi, Saed and Liu, Zhengliang and Wu, Zihao and Dhakal, Chandra and Ge, Bao and Dai, Haixing and Mai, Gengchen and Liu, Ninghao and Zhen, Chen and Liu, Tianming and Li, Sheng},
  journal={IEEE Transactions on Big Data}, 
  title={Exploring New Frontiers in Agricultural NLP: Investigating the Potential of Large Language Models for Food Applications}, 
  year={2025},
  volume={11},
  number={3},
  pages={1235-1246},
  abstract={This paper explores new frontiers in agricultural natural language processing (NLP) by investigating the effectiveness of food-related text corpora for pretraining transformer-based language models. Specifically, we focus on semantic matching, establishing mappings between food descriptions and nutrition data through fine-tuning AgriBERT with the FoodOn ontology. Our work introduces an expanded comparison with state-of-the-art language models such as GPT-4, Mistral-large, Claude 3 Sonnet, and Gemini 1.0 Ultra. This exploratory investigation, rather than a direct comparison, aims to understand how AgriBERT, a domain-specific, fine-tuned, open-source model, complements the broad knowledge and generative abilities of these advanced LLMs in addressing the unique challenges of the agricultural sector. We also experiment with other applications, such as cuisine prediction from ingredients, expanding our research to include various NLP tasks beyond semantic matching. Overall, this paper underscores the potential of integrating domain-specific models like AgriBERT with advanced LLMs to enhance the performance and applicability of agricultural NLP applications.},
  keywords={Data models;Task analysis;Semantics;Training;Context modeling;Biological system modeling;Natural language processing;ChatGPT;food applications;language models;natural language processing;semantic matching},
  doi={10.1109/TBDATA.2024.3442542},
  ISSN={2332-7790},
  month={June},}@ARTICLE{10632866,
  author={Gratius, Nicolas and Bergés, Mario and Akinci, Burcu},
  journal={IEEE Transactions on Aerospace and Electronic Systems}, 
  title={A Parameter Ontology for Simulation Models of Deep Space Habitats}, 
  year={2025},
  volume={61},
  number={1},
  pages={61-75},
  abstract={Deep space habitat operations will depend less on ground support than in low Earth orbit because of increasing communication delays. For the crew to be more autonomous, some of the resources dedicated to anomaly response mechanisms must be transferred from the ground to the habitat. Simulation models replicating the behavior of the system are examples of such resources as they can inform decisions to select mitigation strategies. Multiple models are typically developed for a single spacecraft as many domains of expertise and stakeholders are involved. Enabling simulation onboard a habitat will therefore require integration efforts to provide system-wide capabilities. Specifically, model parameters are model components that must be regularly calibrated to accurately represent the system state during operation. Depending on the type of simulation, these can differ greatly in their nature, thereby making difficult the tasks of creating, reading, updating, and deleting parameters. There is no standard specifying how to represent simulation parameters in a multimodel setting. Model ontologies have been shown to support integration, but their representation of model parameters is usually limited, i.e., the required type of parameter attributes and their hierarchical relationships are unknown. For example, quantifying the uncertainties in parameter values is essential in deep space operations but it is unclear how such information should be formally represented. In this article, we propose a parameter ontology extending existing standards with attributes enabling probabilistic parameter identification. This article is validated by comparing related work with the responses of the proposed ontology to competency questions and by evaluating its consistency, conciseness, completeness, and expandability.},
  keywords={Calibration;Task analysis;Unified modeling language;Space technology;Computational modeling;Buildings;Ontologies;Autonomous system;calibration;knowledge base;ontology;parameter;simulation models;space habitat;vocabulary},
  doi={10.1109/TAES.2024.3440967},
  ISSN={1557-9603},
  month={Feb},}@ARTICLE{10454094,
  author={Ji, Fan and Vogel-Heuser, Birgit and Schypula, Rafael and Wünnenberg, Maximilian and Goedicke, Michael and Fottner, Johannes},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Ontology Versioning for Managing Inconsistencies in Engineering Models Arising From Model Changes in the Design of Intralogistics Systems}, 
  year={2025},
  volume={22},
  number={},
  pages={1249-1261},
  abstract={The interdisciplinary design of intralogistics systems (ILS) involves engineers from various disciplines, resulting in the generation of discipline-specific model files with overlapping information. For instance, a conveyor system can be represented from various perspectives, such as 3D-CAD models that capture its geometric information and discrete-event simulation models that depict the system’s dynamic material flow performance. The growing demands for flexible reconfigurability and adaptability in intralogistics systems necessitate frequent updates to engineering models. However, these updates often result in potential model inconsistencies due to insufficient stakeholder communication. Detecting the impact of model changes and related inconsistencies is challenging in practice due to data heterogeneity and complex inter-model relations. To address these challenges, we propose an ontology-versioning approach that automates the identification of inconsistencies resulting from model changes. Our approach facilitates the integration of heterogeneous model data, enables database versioning, detects inconsistencies caused by model updates, and provides traceability for identified issues. The concept is evaluated utilizing models from a prototypical implementation on a lab-sized demonstrator. Note to Practitioners—In the industry, the current development of intralogistics systems often lacks automated synchronization of overlapping model information and consistent model interfaces, frequently leading to contradictions among the models. This has been identified as a significant source of errors in the design of both industrial and academic intralogistics systems, as revealed by a study involving intralogistics experts from different technical disciplines. Effectively managing model inconsistencies is crucial for project success, particularly when frequent model changes occur. A promising approach to tackle this issue is to systematically link model data from different disciplines, through which model inconsistencies caused by inadequate communication among engineers can be identified and prevented. However, in many cases, changes in different model versions and their resulting inconsistencies are not adequately considered. To address this issue, we propose a concept based on ontology versioning that allows for the generation, comparison, and analysis of different versions of an ontological model database. This concept automatically identifies model changes, assesses their impacts on other models, and provides information to assist engineers in problem-solving. The effectiveness of our approach is assessed through an evaluation of three representative change scenarios, simplified from real-world use cases. In future research, we plan to extend the approach to general production systems and incorporate industrial-scale models from the broad range of disciplines involved in the design process.},
  keywords={Ontologies;Solid modeling;Unified modeling language;Modeling;Data models;Analytical models;Adaptation models;Intralogistics;inconsistency management;model change management},
  doi={10.1109/TASE.2024.3362599},
  ISSN={1558-3783},
  month={},}

@ARTICLE{10336875,
  author={Fu, Chengcheng and Pan, Xueli and Wu, Jieyu and Cai, Junkai and Huang, Zhisheng and van Harmelen, Frank and Zhao, Weizhong and Jiang, Xingpeng and He, Tingting},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={KG4NH: A Comprehensive Knowledge Graph for Question Answering in Dietary Nutrition and Human Health}, 
  year={2025},
  volume={29},
  number={3},
  pages={1793-1804},
  abstract={It is commonly known that food nutrition is closely related to human health. The complex interactions between food nutrients and diseases, influenced by gut microbial metabolism, present challenges in systematizing and practically applying knowledge. To address this, we propose a method for extracting triples from a vast amount of literature, which is used to construct a comprehensive knowledge graph on nutrition and human health. Concurrently, we develop a query-based question answering system over our knowledge graph, proficiently addressing three types of questions. The results show that our proposed model outperforms other state-of-art methods, achieving a precision of 0.92, a recall of 0.81, and an F1 score of 0.86 in the nutrition and disease relation extraction task. Meanwhile, our question answering system achieves an accuracy of 0.68 and an F1 score of 0.61 on our benchmark dataset, showcasing competitiveness in practical scenarios. Furthermore, we design five independent experiments to assess the quality of the data structure in the knowledge graph, ensuring results characterized by high accuracy and interpretability. In conclusion, the construction of our knowledge graph shows significant promise in facilitating diet recommendations, enhancing patient care applications, and informing decision-making in clinical research.},
  keywords={Diseases;Knowledge graphs;Ontologies;Question answering (information retrieval);Semantics;Text mining;Task analysis;Knowledge graph;text mining;question answering;nutrition;human diseases},
  doi={10.1109/JBHI.2023.3338356},
  ISSN={2168-2208},
  month={March},}@ARTICLE{10056133,
  author={Gao, Su and Cao, Yue and Li, Yinqiao and Chen, Yujun and Jin, Song and Xie, Shikun and Liu, Jihong},
  journal={IEEE Access}, 
  title={Reusability, Reconfigurability and Efficiency Optimization of Satellite Network Modeling and Simulation}, 
  year={2025},
  volume={13},
  number={},
  pages={122035-122058},
  abstract={Model-based system engineering (MBSE) with reusable mechanisms can serve as an effective way for complex system architecture design. Stakeholder needs should be satisfied while product and architecture design need to be consistent with user requirements in all stages during the whole product lifecycle. In this paper, satellite network as an example of complex system is modeled in a reusable, reconfigurable and efficient manner using the system modeling language (SysML) together with pattern viewpoints and simulation constructs. Based upon abstract syntax described using metamodels and a set of profiles, concept reusability is established for the specific domain. Additionally a reusable modeling framework is developed with tailored design patterns and multiple viewpoints. Analysis metamodel, profile and interface are further presented to preserve reusability during iterations among multiple optimization rounds. A novel satellite network simulation model is formulated and multi-objective optimization is solved by transformation under practical application scenarios. A set of metrics are designed to assess and validate the models. Results show that the proposed reusable model has viewpoint coverage of more than 80 percent compared to a half for the baseline OOSEM model. The proposed model thus covers the pattern viewpoints and ontologies in a wider and more frequent way and is more efficient. Design choices made based on the model can be incorporated into this mechanism which is extensible along the system lifespan.},
  keywords={Object oriented modeling;Satellites;Unified modeling language;Modeling;Software;Optimization;Computer architecture;Domain specific languages;Reusability;satellite network;domain specific model;pattern viewpoint;simulation;multi-objective optimization},
  doi={10.1109/ACCESS.2023.3250426},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{11093511,
  author={Shah-Mohammadi, Fatemeh and Finkelstein, Joseph},
  booktitle={2024 IEEE 15th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)}, 
  title={Comparative Analysis of Rule-Based and Large Language Model-Based Approaches in Addressing Variability in Clinical Outcome Reporting}, 
  year={2024},
  volume={},
  number={},
  pages={039-045},
  abstract={In clinical trials, varied terminologies and definitions often obscure the clarity and consistency needed to interpret results effectively. The ability to standardize clinical outcome reports and align semantically similar outcomes is crucial in healthcare and research, as inconsistencies can impede the comparability of trial results, complicating metaanalyses and informed decision-making. This research focuses on minimizing variability in the reporting of outcome measures through a comparative analysis of rule-based and advanced language modeling techniques. The rule-based method employs established ontologies, while the language model-based approach utilizes large language models. Findings indicate a low linkage of outcomes to traditional rule-based ontology, particularly for three-word outcomes, and underscore large language models’ efficacy in recognizing semantically similar outcomes across varying word counts. This supports the critical role of large language models in harmonizing outcome data, reducing redundancies, and improving data interoperability in clinical research contexts.},
  keywords={Analytical models;Terminology;Large language models;Semantics;Redundancy;Pipelines;Clinical trials;Ontologies;Mobile communication;Interoperability;Clinical trial;Semantic variability;Large language model;GPT;SBERT;Outcome alignment},
  doi={10.1109/IEMCON62851.2024.11093511},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11063495,
  author={Racharak, Teeradaj and Wang, Tongyu and Jearanaiwongkul, Watanee},
  booktitle={2024 16th International Conference on Knowledge and System Engineering (KSE)}, 
  title={An Automated Medical Rdf Knowledge Graph Construction From Text Using in-Context Learning}, 
  year={2024},
  volume={},
  number={},
  pages={465-471},
  abstract={The parameterized knowledge within large language models (LLMs), like ChatGPT, offers a significant opportunity for modelling domain knowledge base from text. However, LLMs' context sensitivity can hinder obtaining precise and taskaligned outcomes, thus requiring a suitable design for leveraging prompt engineering. This study explores the efficacy of different prompting methods for RDF knowledge graph construction from medical documents as our preliminary investigation, aiming to develop an efficient pipeline for a large-scale automatic knowledge graph construction according to semantic web standards and technologies. The results show that leveraging in-context learning within LLMs is capable of extracting an array of precise RDF triples from text. We perform a qualitative analysis of the extracted triples with different prompt templates, giving insights that could guide potential development in the research field.},
  keywords={Knowledge engineering;Sensitivity;Large language models;Pipelines;Knowledge graphs;Ontologies;Systems engineering and theory;Resource description framework;Prompt engineering;Standards;Generative Knowledge Graph Extraction;Resource Description Framework;OpenAI;Prompting;Few-shot},
  doi={10.1109/KSE63888.2024.11063495},
  ISSN={2694-4804},
  month={Nov},}@INPROCEEDINGS{11037900,
  author={Haw, Su-Cheng and Ng, Kok-Why and J, Jayapradha and Naveen, Palanichamy},
  booktitle={2024 7th Asia Conference on Cognitive Engineering and Intelligent lnteraction (CEII)}, 
  title={OD-SIF: An Ontology-Driven Schema Integration Framework for e-Commerce Platform}, 
  year={2024},
  volume={},
  number={},
  pages={196-201},
  abstract={Automated data mapping is crucial in modern e-commerce ensuring seamless integration of diverse and heterogeneous datasets when migration from legacy systems to advanced platforms without any data loss or corruption. OD-SIF addresses these challenges by leveraging ontologies to unify data semantics, ensuring accurate and consistent schema integration. This framework excellently enhances the e-commerce operation by harmonizing product attributes such as brand, category, and specifications with a high precision of 93% and recall of 88%, completely surpassing baseline methods by 15-20% in the Fl-score. Due to OD-SIF's semantic matching capabilities, it can easily manage noisy and incomplete data with less dependency on manual intervention. On the other hand, special ontologies may further refine the limits in handling niche domains. While improving accuracy, OD-SIF allows real-time data enrichment and ensures cross-platform interoperability that supports core functionalities like inventory management, customer data processing, and transaction across diverse systems. All these advantages make OD-SIF a key enabler for digital transformation in e-commerce bridging various platforms with payment processors, logistics providers, and marketing tools into a unified efficient ecosystem.},
  keywords={Accuracy;Program processors;Semantics;Manuals;Ontologies;Inventory management;Real-time systems;Electronic commerce;Noise measurement;Logistics;automated data mapping;schema-based;ontology;mapping;e-Commerce},
  doi={10.1109/CEII65291.2024.00046},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11025395,
  author={Wang, Xuefeng and Hu, Xiaoqing and Li, Dongsheng and Luo, Xuan},
  booktitle={2024 5th International Conference on Information Science and Education (ICISE-IE)}, 
  title={Research on the Construction of Knowledge Graph for Emergency Management of Social Security Events}, 
  year={2024},
  volume={},
  number={},
  pages={616-619},
  abstract={Constructing a knowledge graph for social security event emergency management promotes the development of social security event emergency management towards an intelligent model that relies on a vast amount of knowledge and data. Relevant content is collected from multiple information sources, and the UIE model, fine-tuned with a small amount of annotated data, is utilized to achieve joint entity-relation extraction. The results are stored in a Neo4j graph database, laying the foundation for application and analysis. The research concludes that utilizing LLM-KG fine-tuning provides a feasible means for constructing large-scale knowledge graphs in vertical domains with low resources, The knowledge graph obtained from this research can satisfy application scenarios in many situations.},
  keywords={Information science;Standards organizations;Semantics;Knowledge graphs;Organizations;Emergency services;Data models;Public security;Planning;Security;social security events;emergency management;knowledge graph},
  doi={10.1109/ICISE-IE64355.2024.11025395},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10992171,
  author={Xie, Jiangcun and Li, Ren and Yang, Jianxi and Xiao, Qiao},
  booktitle={2024 4th International Conference on Digital Society and Intelligent Systems (DSInS)}, 
  title={Ontology Embeddings for Subsumption Prediction Based on Graph Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={133-137},
  abstract={With the growing importance of knowledge graphs in artificial intelligence, accurately modeling hierarchical relationships in ontologies has become a critical issue in knowledge representation learning. To address this, this paper proposes an ontology embedding framework based on graph language models, named GLMSubs, aimed at enhancing the prediction of subclass relationships. The GLMSubs framework adopts a two-stage strategy of “multi-semantic view partitioning” and “advanced training of graph language models”. Initially, it deconstructs the ontology's concepts, attributes, and instance information into five types of semantic views, such as class hierarchy view and class-attribute relationship view, through a multi-view partitioning mechanism, comprehensively capturing information from different semantic dimensions. Subsequently, the framework employs graph language models for joint training on the multi-view data to obtain embeddings that integrate both semantic and structural information. Experiments on datasets such as FoodOn and GO validate the effectiveness of GLMSubs, demonstrating that its performance in class hierarchy relationship prediction tasks significantly surpasses existing methods.},
  keywords={Training;Proteins;Biological system modeling;Semantics;OWL;Knowledge graphs;Medical services;Ontologies;Predictive models;Resource description framework;knowledge representation learning;graph language models;OWL ontology;knowledge graph},
  doi={10.1109/DSInS64146.2024.10992171},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10990107,
  author={Zaeifi, Mehrnoosh and Mosallanezhad, Ahmadreza and Bansal, Srividya},
  booktitle={2024 International Conference on AI x Data and Knowledge Engineering (AIxDKE)}, 
  title={Deeper and Deeper: A Lightweight Semi-Supervised Deep Reinforcement Adaptive Learning-Based Ontology Alignment}, 
  year={2024},
  volume={},
  number={},
  pages={28-35},
  abstract={Ontology alignment, also known as ontology matching, is pivotal for addressing semantic heterogeneity on the Semantic Web. Essentially, it entails linking entities across different ontologies or knowledge graphs in order to resolve ambiguity and enhance the interoperability of data. While various techniques exist, many still rely on rule-based or logic-based approaches, often requiring human intervention and domain specificity. Despite these challenges, ontology alignment remains crucial for seamlessly integrating disparate knowledge sources and facilitating effective data integration. In this paper, we tackle the limitations of current ontology alignment models by introducing a novel, lightweight, semi-supervised deep reinforcement learning model called Deep Reinforcement Adaptive Learning for Ontology Alignment (DRAL-OA). The DRAL-OA method incorporates both syntactic and structural information into the training phase. In addition, this approach is semi-supervised, utilizing a portion of the training data and automatically generating the rest, which reduces the need for human intervention. Moreover, DRAL-OA uses non-domain-specific language models to ensure broad applicability and reduce the need for extensive domain expertise. We evaluate our proposed approach using two datasets from the Ontology Alignment Evaluation Initiative (OAEI). In our experiments, we have shown that the proposed model can achieve high-quality alignments with F-measures on par with other state-of-the-art systems, all while maintaining a very short runtime and a compact model size.},
  keywords={Semantic Web;Training;Knowledge engineering;Adaptation models;Runtime;Semantics;Training data;Knowledge graphs;Ontologies;Syntactics;ontology alignment;reinforcement learning;knowledge graph;semantic web},
  doi={10.1109/AIxDKE63520.2024.00012},
  ISSN={2831-7203},
  month={Dec},}@INPROCEEDINGS{10990088,
  author={Soularidis, Andreas and Kotis, Konstantinos and Lamolle, Myriam and Mejdoul, Zakaria and Lortal, Gaëlle and Vouros, George},
  booktitle={2024 International Conference on AI x Data and Knowledge Engineering (AIxDKE)}, 
  title={LLM-Assisted Generation of SWRL Rules from Natural Language}, 
  year={2024},
  volume={},
  number={},
  pages={7-12},
  abstract={Recently, Large Language Models (LLMs) have attracted great attention due to their remarkable performance in human-like text generation and reasoning skills (although their memory and hallucination problems still remain key issues to tackle more efficiently). LLMs have been applied to various application domains, including Knowledge Graph (KG) generation, question and answering over KGs and text-to-SPARQL translation. In this work, we investigate the capabilities of LLMs in text-to-SWRL translation, i.e., translation of Natural Language (NL) rules into Semantic Web Rule Language (SWRL) rules, put in the context of an industrial Ontology Engineering (OE) environment called GLUON, presenting our first experimental results. The aim of this work is to identify the level of automation that is adequate for the LLM to generate well-formed SWRL rules, towards the development of an LLM-based framework, as a plugin to the GLUON OE environment. In this direction we leverage and combine the reasoning capabilities of GPT-4o model, the Retrieval-Augmented Generation (RAG) technology, and prompt engineering. We employ quantitative and qualitative metrics to evaluate the generated SWRL rules, focusing on the correct syntax and the level of human intervention.},
  keywords={Semantic Web;Translation;Automation;Large language models;Retrieval augmented generation;Memory management;Ontologies;Syntactics;Cognition;Prompt engineering;SWRL;Large Language Models (LLM);Retrieval-Augmented Generation (RAG);Ontology Engineering},
  doi={10.1109/AIxDKE63520.2024.00008},
  ISSN={2831-7203},
  month={Dec},}@INPROCEEDINGS{10990753,
  author={N, Sushma Rani and CH, Dhawaleswar Rao and P, Srinivasa Rao},
  booktitle={2024 2nd International Conference on Signal Processing, Communication, Power and Embedded System (SCOPES)}, 
  title={Enhanced Named Entity Recognition in Medical Texts Using Transformer-Based Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={The increase of digital medical text data which includes electronic health records (EHRs), clinical notes, and medical literature, gives an invaluable resource for advancing healthcare. As the data is unstructured, it is challenging to extract valuable insights from the data. To extract these insights there are models such as Rule-based and dictionary-based. These existing models face the drawback of handling ambiguity of words, out-of-vocabulary words. The proposed approach leverages advanced Natural Language Processing techniques, specifically state-of-the-art transformer-based models like BioBERT and ClinicalBERT, to perform Named Entity Recognition in the medical domain. By integrating additional domain-specific resources, including comprehensive medical terminologies and ontologies, we enhance the performance of entity recognition. The objective is to accurately identify and classify key medical entities from diverse medical text sources. The system has been evaluated using the metric accuracy, precision, recall and F1 score. The achieved F1 score is 91.5. The resulting structured information can be utilized in numerous applications like clinical decision support, patient data management and medical research.},
  keywords={Measurement;Terminology;Biological system modeling;Face recognition;Named entity recognition;Medical services;Signal processing;Ontologies;Transformers;Object recognition;Named Entity Recognition;medical records;transformers},
  doi={10.1109/SCOPES64467.2024.10990753},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10974777,
  author={Balaadich, Youness and Jakimi, Abdeslam},
  booktitle={2024 34th International Conference on Computer Theory and Applications (ICCTA)}, 
  title={Toward an Advanced Rural Tourism Ontology for Enhancing Visitor Experiences in Morocco’s Draa-Tafilalet Region}, 
  year={2024},
  volume={},
  number={},
  pages={74-79},
  abstract={Integrating ontologies in tourism applications represents a transformative step toward achieving a structured digital transformation to enhance the competitive edge of tourism destinations. In Morocco’s tourism sector, particularly in the culturally and naturally rich Draa-Tafilalet region, implementing an ontology is essential to organize diverse tourism-related data and provide a more personalized and accessible visitor experience. Despite Morocco’s significant efforts, the tourism industry continues to face persistent challenges in delivering customized and accessible information aligned with tourists' preferences and needs. To address this issue, this study develops comprehensive ontology using a semi-automated methodology that combines natural language processing, language models, and expert validation. The ontology encapsulates attractions, services, and visitor preferences specific to the region. The main objective is to modernize and enhance the informational structure of the tourism sector, facilitating better navigation for tourists and supporting the strategic promotion of the region’s assets.},
  keywords={Navigation;Digital transformation;Large language models;Tourism industry;Machine learning;Ontologies;Chatbots;Mobile applications;Stakeholders;Faces;Digital Transformation;Rural Tourism;Ontology;Personalized Experiences},
  doi={10.1109/ICCTA64612.2024.10974777},
  ISSN={2770-6575},
  month={Dec},}@INBOOK{10952605,
  author={Vayadande, Kuldeep and Bohri, Mustansir and Chawala, Mohit and Kulkarni, Ashutosh M. and Mursal, Asif},
  booktitle={How Machine Learning is Innovating Today's World: A Concise Technical Guide}, 
  title={The Rise of AI&#x2010;Generated News Videos}, 
  year={2024},
  volume={},
  number={},
  pages={423-451},
  abstract={Summary <p>The rapid advancements in Artificial Intelligence (AI) have given rise to the possibility of automating news video creation. AI&#x2010;powered news videos will offer a fresh and dynamic perspective on the day's top stories, delivering the content people need in a way that is easy to consume. AI&#x2010;generated news videos are the next evolution in journalism, providing a fast and accurate way to consume news content that is both informative and visually stunning. In this review paper, we explore the process of converting news articles into AI&#x2010;generated videos that can be published on platforms like YouTube. The process involves web scraping of text and images, news authentication, image searching, voice&#x2010;over creation, video generation, thumbnail creation, and YouTube video upload. We have reviewed several research papers related to each of these steps and highlighted their applications in news video creation. We have identified the challenges involved in each step, such as the authenticity of news articles, relevance of images, and the need for high&#x2010;quality voice&#x2010;overs. We have discussed proposed solutions for these challenges and the potential of AI&#x2010;generated news videos in revolutionizing the news industry. Our review paper also highlights the research gaps in this field, such as the need for more advanced image and voice recognition technology and the potential ethical concerns of using AI&#x2010;generated content.</p>},
  keywords={Videos;Artificial intelligence;Media;Reviews;Face recognition;Data mining;Visualization;Text recognition;Ontologies;Industries},
  doi={10.1002/9781394214167.ch25},
  ISSN={},
  publisher={Wiley},
  isbn={9781394214150},
  url={https://ieeexplore.ieee.org/document/10952605},}@INPROCEEDINGS{10947604,
  author={Baidya, Anushuya and Do, Tuyen and Gnimpieba, Etienne Z.},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Toward Fine-Tuning Large Language Models in Ontology of Microbial Phenotypes Construction}, 
  year={2024},
  volume={},
  number={},
  pages={6913-6920},
  abstract={Ontologies are crucial for organizing domainspecific knowledge in biomedical fields, but their manual construction is time-consuming. This study explores the automation of ontology learning using large language models (LLMs) like BERT, RoBERTa, and DistilBERT, focusing on the Ontology of Microbial Phenotypes (OMP). We investigate three key tasks: (1) entity extraction, (2) relation extraction between entities, and (3) ontology verification. These tasks align with broader applications in biomedical annotation and named entity recognition (NER) by enabling the identification and structuring of key terms and relationships within microbial phenotypes. We evaluate LLMs in two scenarios: baseline performance using pre-trained models and fine-tuned performance after training on OMP-specific data. Our approach integrates spaCy for entity extraction, Llama 2 for relation identification, and LLMs for ontology verification. Experiments reveal that fine-tuned models significantly improve accuracy, precision, recall, and F1 scores, particularly for ontology verification. This research highlights the potential of LLMs to enhance ontology learning and support related biomedical applications like biofilm analysis, annotation, and NER, while emphasizing the value of expert curation.},
  keywords={Training;Phenotypes;Annotations;Large language models;Biological system modeling;Focusing;Named entity recognition;Manuals;Ontologies;Data models;Ontology Learning;Large Language Models;Fine-tuning;Microbial Phenotypes;Ontology Verification},
  doi={10.1109/BIBM62325.2024.10947604},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10936732,
  author={Ge, Xing and Liu, Yafei and Yang, Pei and Sun, Xin and Qiao, Junfeng and Qu, Luyao and Qiu, Jingyi},
  booktitle={2024 International Conference on Information Technology, Comunication Ecosystem and Management (ITCEM)}, 
  title={Research and Application of Electronic Data Retrieval in Material Supply Chain Enhanced by Large Language Models and Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={156-160},
  abstract={In response to the new goals of building green and modern smart supply chains, the electric power equipment supply chain is experiencing a shift toward digital intelligence and low-carbon, environmentally friendly development [1]. However, traditional search platforms based on relational databases face challenges in handling vast amounts of multimodal electronic data. These platforms often suffer from low search accuracy, limited cross-dimensional correlation analysis capabilities, and inefficiencies, making them inadequate for constructing comprehensive big data platforms that integrate and share information across the entire green, modern, smart supply chain. This paper introduces an innovative electronic data retrieval method designed to meet the data retrieval needs of material supply chains. By integrating large language models with knowledge graph technology, it proposes an electronic data retrieval system that leverages vector database technology for text embedding of multimodal data. Additionally, artificial intelligence is used to enable knowledge retrieval and augmented generation, significantly enhancing data retrieval capabilities within the specialized domain of material supply chains.},
  keywords={Green buildings;Large language models;Supply chains;Knowledge graphs;Relational databases;Data retrieval;Vectors;Power systems;Information technology;Faces;large language model;knowledge graph;material supply chain;data retrieval},
  doi={10.1109/ITCEM65710.2024.00037},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10928264,
  author={Ying, Zhao and Weiyu, Chen and Longlong, Liao and Jie, Liu},
  booktitle={2024 IEEE International Conference on Computing (ICOCO)}, 
  title={Masked Theme-Specific Named Entity Recognition Assisted with Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={457-462},
  abstract={Existing Named Entity Recognition (NER) methods are required to label relevant samples and train the concrete NER models. Due to the specification of theme-specific documents, these NER models are considerably hard to identify potential theme-specific entities. To address this challenge, we propose an effective two-stage approach of masked theme-specific NER associated with Large Language Models (LLMs), which uses the unsupervised mechanism rather than the supervised one. The approach involves theme-specific entity ontology construction and masked NER in heterogeneous documents. The first stage is associated with LLMs and Wikipedia category pages, and the second one is implemented with the masked NER based on the created ontology in the first stage. Extensive experimental results suggest that the proposed masked NER can precisely locate the known entities in the theme-specific entity ontology while improving the accuracy of NER in the remaining text. Compared to the mainstream NER frameworks such as spaCy 3, the masked NER can identify more valid entities in the input Markdown text and continuously use the newly detected unknown entities to update the created ontology.},
  keywords={Accuracy;Filtering;Large language models;Computational modeling;Named entity recognition;Encyclopedias;Ontologies;Libraries;Internet;Online services;Natural Language Processing;Named Entity Recognition;Large Language Model;Heterogeneous Documents;Theme-specific Entity Ontology},
  doi={10.1109/ICOCO62848.2024.10928264},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10913712,
  author={Hier, Daniel B. and Munzir, S. Ilyas and Stahlfeld, Anne and Obafemi-Ajayi, Tayo and Carrithers, Michael D.},
  booktitle={2024 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI)}, 
  title={High-Throughput Phenotyping of Clinical Text Using Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={High-throughput phenotyping automates the mapping of patient signs to standardized concepts, such as those in Human Phenotype Ontology (HPO), a process critical to precision medicine. We evaluated the automated phenotyping of clinical summaries from the Online Mendelian Inheritance in Man (OMIM) database using a large language model. Various APIs were used to automate text retrieval, sign identification, categorization, and normalization. GPT-4 outperformed GPT-3.5Turbo in identifying, categorizing, and normalizing signs, achieving concordance with manual annotators comparable to concordance between manual annotators. While GPT-4 demonstrates high accuracy in sign identification and categorization, limitations remain in sign normalization, particularly in retrieving the correct HPO ID for a normalized term. Methods such as retrieval-augmented generation, changes in pre-training, and additional fine-tuning may help address these limitations. The combination of APIs with large language models presents a promising approach for high-throughput phenotyping of free text.},
  keywords={Neurology;Phenotypes;Databases;Large language models;Precision medicine;Retrieval augmented generation;Manuals;Ontologies;Natural language processing;Bioinformatics;phenotype;large language model;natural language processing;high-throughput;OMIM;neurology;HPO;GPT-4},
  doi={10.1109/BHI62660.2024.10913712},
  ISSN={2641-3604},
  month={Nov},}@INPROCEEDINGS{10913373,
  author={Zhou, Peiyao and Hu, Zhikui and Zi, Kangli and Zhang, Dawei},
  booktitle={2024 6th International Conference on Frontier Technologies of Information and Computer (ICFTIC)}, 
  title={Leveraging Knowledge Distillation for Improved Event Extraction in QA Models}, 
  year={2024},
  volume={},
  number={},
  pages={703-708},
  abstract={Event extraction is an important task in natural language processing, and it is widely utilized in intelligence domains such as business and military for information extraction. Recently, many works have successfully transformed document-level event extraction into Question-answering (QA) tasks with remarkable results. This approach embeds event argument information within the questions, introducing prior knowledge into the process. Jin et al. highlighted the importance of high-quality QA pairs for practical QA tasks, emphasizing that constructing these pairs remains a key challenge[1]. In this study, we propose a Prompt-based question generation method to automatically generate questions containing event arguments, which converts event extraction into a QA task. We introduce an event ontology-based information retrieval module to enhance answer accuracy and select the most relevant document segments as input text. Additionally, we employ knowledge distillation to build the QA model, transferring knowledge from a large pre-trained model to a more compact and efficient one, improving the student model's performance. Experiments show that our model performs strongly in mainstream benchmarks, with 4.8 improvements on WikiEvents.},
  keywords={Accuracy;Benchmark testing;Information retrieval;Question generation;Question answering (information retrieval);Data mining;Business;component;document-level event extraction;Question Answering;knowledge distillation},
  doi={10.1109/ICFTIC64248.2024.10913373},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10912181,
  author={Sharma, Simple and Panda, Supriya P.},
  booktitle={2024 2nd International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)}, 
  title={Performance Metrics Analysis for Deep Learning Models}, 
  year={2024},
  volume={1},
  number={},
  pages={970-976},
  abstract={Evaluating an Information Retrieval (IR) model is a multi-faceted process that requires selecting the right quantitative metrics to assess performance. The selection of evaluation metrics in IR depends on specific tasks, relevant standards, and desired characteristics. To thoroughly assess an IR system’s performance, a combination of metrics is necessary. This study explores the effectiveness of deep learning (DL) models in semantic and personalized information retrieval (SIR), focusing on BERT and other large language models (LLMs) that provide context-sensitive embeddings and advanced language comprehension capabilities. Through a detailed analysis, this paper demonstrates how DL models can significantly enhance accuracy and relevance in IR, using evaluation metrics critical for assessing model performance. Metrics like recall, precision, and F1-score are key in capturing model accuracy and coverage; for example, a recall of $\mathbf{1. 0}$ indicates complete retrieval of relevant instances, while a precision of 0.6 reflects a $\mathbf{6 0 \%}$ accuracy in positive predictions, balancing these measures at an F1-score of 46.15%. Ranking and prioritization metrics, such as Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG), evaluated at 91.67% and 95.1%, respectively highlight the models’ capabilities in prioritizing relevant results effectively. In personalized and semantic search, selecting the right evaluation metrics is essential to maximize DL model potential and improve the user experience. Recent LLM advancements, like GPT-4, are instrumental in capturing nuanced meanings and understanding complex user queries, which enhances the accuracy of search engines. Continuous optimization of these models through tailored metrics can improve IR systems’ contextual relevance and accuracy, fostering greater user satisfaction.},
  keywords={Measurement;Deep learning;Analytical models;Accuracy;Semantic search;Computational modeling;Transformers;User experience;Standards;Context modeling;BERT;Deep Learning Models;Evaluation;Metrics;Personalized IR;Semantic IR;Transformer Models;Ontology;User Profiling},
  doi={10.1109/ICAICCIT64383.2024.10912181},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10910646,
  author={Kalaiarasi, S. Jenny and Nimala, K.},
  booktitle={2024 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)}, 
  title={Enhancing E-Commerce Product Recommendations Using LLMs and Transformer-Based Deep Learning Architectures}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={The integration of large language models with deep learning architectures brought an evolutionary revolution in the context of product recommendation systems because of several limitations of traditional methods, such as collaborative and content-based filtering. In this paper, a novel framework for product recommendation is proposed by integrating large language models jointly with deep learning. The contribution of large language models is that it adds semantic understanding capability to the predictive power provided through neural networks. Domain ontologies will be used in this hybrid model to enhance the accuracy and personalization of recommendations, considering complex user preferences and product attributes in e-commerce platforms. It takes a state-of-the-art pre-trained LLM, such as Llama-3, as input and generates personalized embeddings of users based on history, item descriptions, and contextual information. In this work, the Transformer architecture has been used in re-fining and ranking the products for relevance using attention mechanisms that select the most important features in each recommendation task. Besides, knowledge distillation will be used to conduct the small and efficient student model training process. The distilled model receives soft predictions that involve the teacher LLM, which greatly reduces computational overhead but preserves high recommendation accuracy. Eventually, the framework will be evaluated on a real-world e-commerce dataset to explore how it increases the click-through rate, purchase rate, and user's engagement compared to the traditional systems.},
  keywords={Training;Accuracy;Filtering;Computational modeling;Large language models;Collaboration;Computer architecture;Ontologies;Transformers;Electronic commerce;Pre-trained LLM;Llama-3;Domain ontologies;collaborative and content-based filtering},
  doi={10.1109/ICSES63760.2024.10910646},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10908257,
  author={Huynh, Trung-Tru and Nguyen, The-Bao and Ho-Dac, Hung},
  booktitle={2024 International Conference on Advanced Technologies for Communications (ATC)}, 
  title={A Practical Approach Applying Deep Learning and Ontology to Identify Aspects in Opinions}, 
  year={2024},
  volume={},
  number={},
  pages={969-973},
  abstract={Aspect-based sentiment analysis is a very interesting problem in opinion mining. Accurately determining the evaluated aspect in an opinion contributes to improving the performance of the sentiment analysis problem. This study proposes an approach to identify aspects that is not based on keywords but on the semantics of opinions. To determine the evaluated aspects in opinions, the proposed approach uses the method of embedding knowledge from the ontology into the corpus to train deep learning algorithms. The structure of the ontology used in this study is based on the relationship between aspect words and emotion words in the field of car evaluation. The corpus is labeled with aspects not only based on keywords indicating aspects but also based on the semantics of the sentence. The high accuracy test results show the prominent application of the proposed approach.},
  keywords={Deep learning;Sentiment analysis;Accuracy;Semantics;Ontologies;Data models;Automobiles;Aspect;Corpus;Deep Learning;Ontology},
  doi={10.1109/ATC63255.2024.10908257},
  ISSN={2162-1039},
  month={Oct},}@INPROCEEDINGS{10903241,
  author={Barron, Ryan C. and Grantcharov, Vesselin and Wanna, Selma and Eren, Maksim E. and Bhattarai, Manish and Solovyev, Nicholas and Tompkins, George and Nicholas, Charles and Rasmussen, Kim Ø. and Matuszek, Cynthia and Alexandrov, Boian S.},
  booktitle={2024 International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization}, 
  year={2024},
  volume={},
  number={},
  pages={1669-1676},
  abstract={Large Language Models (LLMs) are pre-trained on large-scale corpora and excel in numerous general natural language processing (NLP) tasks, such as question answering (QA). Despite their advanced language capabilities, when it comes to domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations, knowledge cut-offs, and lack of knowledge attributions. Additionally, fine tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and time consuming process. The retrieval-augmented generation (RAG) process has recently emerged as a method capable of optimization of LLM responses, by referencing them to a predetermined ontology. It was shown that using a Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into account relevant sub-graphs that preserve the information in a structured manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM framework, that integrates RAG with KG and a vector store (VS) that store factual domain specific information. Importantly, to avoid hallucinations in the KG, we build these highly domain-specific KGs and VSs without the use of LLMs, but via NLP, data mining, and nonnegative tensor factorization with automatic model selection. Pairing our RAG with a domain-specific: (i) KG (containing structured information), and (ii) VS (containing unstructured information) enables the development of domain-specific chat-bots that attribute the source of information, mitigate hallucinations, lessen the need for fine-tuning, and excel in highly domain-specific question answering tasks. We pair SMART-SLIC with chain-of-thought prompting agents. The framework is designed to be generalizable to adapt to any specific or specialized domain. In this paper, we demonstrate the question answering capabilities of our framework on a corpus of scientific publications on malware analysis and anomaly detection.},
  keywords={Tensors;Accuracy;Retrieval augmented generation;Knowledge graphs;Ontologies;Question answering (information retrieval);Vectors;Malware;Reliability;Tuning;Artificial Intelligence;Retrieval Augmented Generation;Knowledge Graph;Natural Language Processing;Non-Negative Tensor Factorization;Topic Modeling;Agents},
  doi={10.1109/ICMLA61862.2024.00258},
  ISSN={1946-0759},
  month={Dec},}@INPROCEEDINGS{10895371,
  author={R, Menaha and R, Abilaash and N, Mohanram P and Unnikrishnan, Akash and S, Sukumar},
  booktitle={2024 International Conference on Emerging Research in Computational Science (ICERCS)}, 
  title={Drug Pills Identification System using Google Gemini LLM: A Generative AI approach}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Generative AI is emerging as a disruptive force in the healthcare industry, bringing novel solutions ranging from drug development and clinical decision support to personalized patient care. This study is focused on drug discovery using the Generative AI model. In this paper, a system is proposed for providing drug descriptions from drug pill images. The system is implemented by utilizing Large Language Models (LLMs) in combination with computer vision to detect and provide detailed information about drugs from pill images. In the proposed system, the identification process begins by taking the medicinal drug pills and their cover images. Then, the image is converted into binary values using a standard built-in function. In addition, the target language for providing audio descriptions about the drugs is also used. Then, the Google Gemini LLM model is customized by using binary values of the image, target language, and ontology-based prompt engineering. As a result, the LLM model provides drug descriptions in text. Then, the textual description of the drug is converted into the target language audio format by using the Google Text to Speech Converter. The system is experimented by using 807 medicinal drug images which are collected from web resources. The performance of the system is measured by using accuracy. The system achieved an accuracy of 95.04% which is a little higher when compared with the current state-of-the-art model.},
  keywords={Drugs;Accuracy;Text analysis;Computational modeling;System performance;Medical services;Streaming media;Internet;Text to speech;Biomedical imaging;Drug Pill Identification;Generative AI;Large Language Models;Pharmaceutical Image Analysis;Multimodal Learning;Medical Text Analysis},
  doi={10.1109/ICERCS63125.2024.10895371},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10892858,
  author={Couder, Juan Ortiz and Pate, William C. and Machado, Daniel A. and Ochoa, Omar},
  booktitle={2024 IEEE Frontiers in Education Conference (FIE)}, 
  title={Incorporating AI in the Teaching of Requirements Tracing Within Software Engineering}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={During the Software Development Lifecycle (SDLC), the first stage entails the Requirement Engineering phase. In this phase, engineers gather, analyze, and specify the requirements for a software system. Requirements playa crucial role in the SDLC as they establish the foundation for the entire system by defining the expected behaviors of the software system to be built. The resulting specifications are captured in a Software Requirement Specification (SRS) document. As part of the validation process, requirement specifications are traced. Requirement tracing involves linking the requirement to the artifacts where the customer requested the high-level requirement. Teaching proper requirements tracing can be challenging in a traditional classroom setting. It is essential to educate future software engineers on the proper process of developing an SRS document and of tracing requirements back to the originating artifact, which is also challenging due to the complexity and large scope of applying the complete requirements engineering process. Understanding how changes in customer needs can impact requirements is an imperative learning opportunity. In this work, we aim to incorporate the use of AI in the teaching of requirements tracing using Large Language Models. In this experiment, both GPT -3.5 and GPT -4 are provided the transcript of an interview between the customer and the engineering team, as well as the subsequent requirements elicited from that meeting and other customer provided artifacts. The GPTs are then instructed to determine which requirements can be traced back to the interview transcript. At the same time, the students (the requirements engineering team) conduct their own effort to trace requirements back to the original interview. The experiment was taken one step further to assess students' and the GPTs abilities to address requirements modifications. After another interview with the customer, where some needs were changed, some requirements were modified, and students, and GPTs were asked to trace the modified requirements to the new interview. The results proved that students are better than both GPT versions at tracing modified requirements, yet GPTs again identified requirements that students didn't trace back. The findings, illustrate that AI can help in the teaching of requirement tracing; these results suggest that while no AI model is currently capable of replacing real requirement engineers as they don't outperform students, it can be used as a tool to test the completeness of the requirement tracing process. We posit that GPT can be a tool for students to self-assess the degree to which their own requirements tracing is exhaustive.},
  keywords={Training;Visualization;Atmospheric modeling;Prototypes;Software systems;Requirements engineering;Interviews;Artificial intelligence;Software engineering;Software development management;AI;Requirement Tracing;Education;Software Requirement Specification;Large Language Models},
  doi={10.1109/FIE61694.2024.10892858},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{10885332,
  author={Hou, Lijuan and Qin, Hanyan and Zhang, Xiankun and Zhang, Yiying},
  booktitle={2024 IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA)}, 
  title={Protein Function Prediction Based on the Pretrained Language Model ESM2 and Graph Convolutional Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1776-1781},
  abstract={Understanding protein function is crucial for comprehending life at the molecular level. Currently, less than 0.1% of proteins having experimental GO annotations. Traditional experimental methods are time-consuming and expensive. To narrow this gap, employing accurate and efficient computational methods can fill the void in automated protein function prediction (AFP). We have developed a new method for predicting protein function using sequence and predicted structural information. We use the large-scale pretrained language model ESM2 to pretrain protein sequences and an encoder to capture contextual information. Using the 3D structural data of proteins generated by AlphaFold2, combined with the sequence, as input for the Graph Convolutional Neural Network, to infer the probabilities of Gene Ontology (GO) annotations for the proteins. Compared to earlier methods, our model achieves better performance. Evaluations on the human dataset show AUPR improvements of 9%, 9.4%, and 20.7% in the BP, MF, and CC branches, respectively, demonstrating that our model is an effective tool for predicting protein function.},
  keywords={Three-dimensional displays;Accuracy;Annotations;Predictive models;Ontologies;Logic gates;Feature extraction;Protein sequence;Data models;Context modeling;Gene Ontology;protein function prediction;ESM2;AlphaFold2;graph pooling},
  doi={10.1109/ISPA63168.2024.00242},
  ISSN={2158-9208},
  month={Oct},}@INPROCEEDINGS{10882566,
  author={Singh, Navjot and Bathla, Gaurav},
  booktitle={2024 13th International Conference on System Modeling & Advancement in Research Trends (SMART)}, 
  title={A Review on Opinion Mining Approaches: Using Deep Learning for Large Scale Data}, 
  year={2024},
  volume={},
  number={},
  pages={756-762},
  abstract={Opinion mining, another name for sentiment analysis (SA), is a branch of computer science. that focuses on analyzing the opinions and feelings expressed in text, audio and video-Sentiment analysis involves determining the sentiment expressed by person. The objective of the opinion-mining field is to conduct subjectivity analysis, indicating whether a document is subjective or objective. Subjectivity implies the presence of sentiment, while objectivity signifies content devoid of sentiment. Currently, an abundance of information about a specific product is available, with a single product often garnering hundreds of reviews across various webpages. Numerous websites, such as imdb.com, amazon.com, idlebrain.com, among others, aggregate user information and expert opinions to publish reviews. Experts meticulously analyse reviews, extract opinions, and generate ratings related to the dataset provided by the requesting agencies. However, handling the vast amount of data is a labour intensive task for experts. The continuously growing volume of web data poses challenges in extracting precise opinions from content. Hence, there is a need to design a system that can efficiently perform these tasks with human-like accuracy. This paper analyses various approaches which are capable of handling and analysing large amounts of reviews and generate the aspect level opinion mining (AOM).},
  keywords={Sentiment analysis;Accuracy;Text analysis;Reviews;Social networking (online);Terrorism;Soft sensors;Media;Monitoring;Text processing;Opining Mining;Review;Text Data;Subjectivity},
  doi={10.1109/SMART63812.2024.10882566},
  ISSN={2767-7362},
  month={Dec},}@INPROCEEDINGS{10884260,
  author={Lee, Sejin and Kim, Dongha and Song, Min},
  booktitle={2024 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={Beyond Ontology in Dialogue State Tracking for Goal-Oriented Chatbot}, 
  year={2024},
  volume={},
  number={},
  pages={177-185},
  abstract={Goal-oriented chatbots are essential for automating user tasks, such as booking flights or making restaurant reservations. A key component of these systems is Dialogue State Tracking (DST), which interprets user intent and maintains the dialogue state. However, existing DST methods often rely on fixed ontologies and manually compiled slot values, limiting their adaptability to open-domain dialogues. We propose a novel approach that leverages instruction tuning and advanced prompt strategies to enhance DST performance, without relying on any predefined ontologies. Our method enables Large Language Model (LLM) to infer dialogue states through carefully designed prompts and includes an anti-hallucination mechanism to ensure accurate tracking in diverse conversation contexts. Additionally, we employ a Variational Graph Auto-Encoder (VGAE) to model and predict subsequent user intent. Our approach achieved state-of-the-art with a JGA of 42.57% outperforming existing ontologyless DST models, and performed well in open-domain real-world conversations. This work presents a significant advancement in creating more adaptive and accurate goal-oriented chatbots. 1},
  keywords={Adaptation models;Accuracy;Limiting;Large language models;Neural networks;Oral communication;Ontologies;Predictive models;Chatbots;Tuning;Goal-oriented Dialogue;Dialogue State Tracking;Chatbot;Prompt engineering;Graph Neural Network},
  doi={10.1109/ICKG63256.2024.00030},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10884217,
  author={Chen, Gui and Liu, Xianhui},
  booktitle={2024 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={A Multi-Agent Collaborative Framework for Constructing Knowledge Graphs from Text}, 
  year={2024},
  volume={},
  number={},
  pages={9-16},
  abstract={Recent advancements in large language models (LLMs) have significantly improved natural language understanding and generation, making them valuable tools for knowledge graph construction. However, a single LLM often struggles with the complexity of this task, leading to suboptimal results. To address this challenge, we propose a robust multi-agent collaborative framework for constructing knowledge graphs from text. This framework leverages dynamic interactions among specialized agents, including knowledge graph experts, knowledge extraction experts, data processing experts, and domain-specific experts, to effectively build accurate knowledge graphs from text. Additionally, we introduce a novel prompt construction method tailored for knowledge extraction and a revision mechanism to revise preliminary knowledge graphs. These innovations address common issues in knowledge extraction and enhance the quality of model-generated content. Experimental results on four datasets across two tasks (Named Entity Recognition and Relation Extraction) demonstrate that our approach achieves superior performance in the F1 score compared to baseline methods, highlighting its effectiveness and robustness.},
  keywords={Knowledge engineering;Technological innovation;Large language models;Collaboration;Knowledge graphs;Named entity recognition;Ontologies;Data processing;Robustness;Data mining;knowledge graph construction;multi-agent;large language model;knowledge extraction;prompt engineering},
  doi={10.1109/ICKG63256.2024.00010},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10884275,
  author={Zhou, Xiaofa and Shi, Jianyong and Dong, Lei and Zhang, You and Pan, Jin and Huang, Hao},
  booktitle={2024 International Conference on New Power System and Power Electronics (NPSPE)}, 
  title={Construction of a Multimodal Knowledge Graph for Power Grid Construction Safety Based on Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={21-28},
  abstract={In response to the difficulties of safety management and the complexity of information at power grid construction sites, a multimodal knowledge graph construction method based on large language models is proposed. Data from the construction site is collected and filtered, and an ontology for safety management at the construction site is constructed. The ontology is then used as retrieval augmented generation(RAG) for assistance, enabling multimodal large model image extraction, resulting in structured data in the power grid safety field. Finally, the extracted results are displayed using a graph database, completing the construction of the multimodal knowledge graph. The constructed knowledge graph includes multimodal data from the construction site, allowing for quick querying of on-site safety incidents, providing safety managers with a valuable tool for site management.},
  keywords={Accuracy;Large language models;Safety management;Knowledge graphs;Ontologies;Information filters;Power grids;Data models;Power electronics;Data mining;Ontology;Large language model;Safety management;Knowledge extraction;Knowledge graph},
  doi={10.1109/NPSPE62515.2024.00013},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10881842,
  author={Li, Yingna and Ding, Zhiguo and Yan, Zheng and Li, Zhuahua and Shao, Hang},
  booktitle={2024 7th International Conference on Data Science and Information Technology (DSIT)}, 
  title={Insider Threat Detection based on Knowledge Graph and Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={This article proposes an insider threat detection method based on a combination of knowledge graph and large language model (LLM); first, the internal systems, users, IPs, access behaviors, etc. are modeled through the knowledge graph ontology; then, a few-shot learning information extraction method based on LLMs is used to extract knowledge from the behavior logs to complete the threat detection knowledge graph. Finally, the representation learning method based on the knowledge graph and the embedding based on the LLM are used to extract feature vectors, which are used as input of the insider threat detection model training based on Deep SVDD. The experimental results show that this method can automatically detect abnormal threat behaviors from massive logs at high accuracy, and has the ability to detect deeply hidden abnormal threat behaviors.},
  keywords={Training;Representation learning;Large language models;Semantics;Knowledge graphs;Feature extraction;Threat assessment;Vectors;Data models;Cognition;insider threat detection;knowledge graph;large language model;deep-SVDD},
  doi={10.1109/DSIT61374.2024.10881842},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10880799,
  author={Mejia, Jose M. Ruiz and Rawat, Danda B.},
  booktitle={2024 IEEE International Conference on E-health Networking, Application & Services (HealthCom)}, 
  title={ClinicalGraph: An Applied Approach in Clinical EHR Knowledge Graph Generation for Optimized Clinical Decision Support System}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Electronic Health Records (EHR) are well-known for their extensive capabilities in storing pertinent patient information across various specializations. However, EHRs are part of a broader ecosystem of applications that support the medical treatment process. This ecosystem, although somewhat centralized, does not have the interoperability that many assume. The main issue is that current triage models are not personalized to each patient. Context matters in a medical emergency situation especially when resources are low. In this research, we propose a possible solution by applying state of the art techniques in order to develop a clinical knowledge graph that contains relevant data used for triage optimization. We utilize a fine-tuned named entity recognition model (NER) to extract 41 label entity categories from previous medical records. Additionally, we employed prompt engineering utilizing a large language text generation model with medical knowledge to generate relationships. This resulted in a sum of 1,429 relationship type categories and approximately 999 entity nodes were created with 2,387 relationships.},
  keywords={Decision support systems;Reviews;Biological system modeling;Large language models;Ecosystems;Retrieval augmented generation;Medical treatment;Knowledge graphs;Prompt engineering;Medical diagnostic imaging;Rapid Triage;E-triage;Generative Artificial Intelligence;Knowledge Graph Generation;Patient Centered Nodes;Knowledge Graph;Clinical Knowledge Graph},
  doi={10.1109/HealthCom60970.2024.10880799},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10858998,
  author={Mohsenzadegan, Kabeh and Tavakkoli, Vahid and Kambale, Witesyavwirwa Vianney and Kyamakya, Kyandoghere},
  booktitle={2024 19th International Workshop on Semantic and Social Media Adaptation & Personalization (SMAP)}, 
  title={Towards Seamless Data Translation Based on Data Models: A Hybrid AI Framework for Smart Transportation and Manufacturing}, 
  year={2024},
  volume={},
  number={},
  pages={68-73},
  abstract={Interoperability between different data standards is essential for advancing digital technologies in smart manufacturing and transportation. This paper presents a hybrid AI framework that integrates Ontology Learning (OL), Knowledge Graphs (KGs), and Large Language Models (LLMs) to improve data translation across these standards. Focusing on IEEE 1451, ISO 15926, and IEC 61499, we address the challenges posed by these data models' unique structures and semantics. Our comparative analysis evaluates the strengths and limitations of OL, KGs, and LLMs across key metrics like accuracy, scalability, efficiency, robustness, and flexibility. The proposed framework leverages OL for systematic structuring, KGs for relational modeling, and LLMs for linguistic processing, enhancing translation accuracy and adaptability. However, integrating these approaches introduces scalability and processing efficiency trade-offs, particularly in resource-constrained environments. This study contributes to developing more sophisticated and scalable data translation models tailored for heterogeneous data environments, with practical implications for smart manufacturing and transportation.},
  keywords={Adaptation models;Translation;Accuracy;Systematics;Scalability;Semantics;Data models;Smart transportation;Standards;Smart manufacturing;Data Model Translation;Ontology Learning (OL);Knowledge Graphs (KGs);Large Language Models (LLMs);Smart Manufacturing;Data Interoperability;AI in Transportation;Hybrid AI Framework;Semantic Mapping;Cross-Standard Data Integration},
  doi={10.1109/SMAP63474.2024.00022},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10859032,
  author={He, Lei and Zhang, Wenbo and Shi, Tao and Zhao, Yichen and Wang, Xing and Xing, Lumin and Li, Yongmeng},
  booktitle={2024 IEEE 9th International Conference on Data Science in Cyberspace (DSC)}, 
  title={Knowledge Graph Construction in the Context of Traditional Chinese Medicine: A review}, 
  year={2024},
  volume={},
  number={},
  pages={287-293},
  abstract={Under the background of traditional Chinese medicine(TCM) informatization, knowledge graph as a tool of the information construction solves the complex and difficult problem of TCM knowledge structure. With the continuous optimization and iteration of artificial intelligence such as deep learning, the construction technology of knowledge graph in the field of TCM is also constantly updated. In the text, it summarizes the different methods and steps of knowledge graph construction, and on this basis, introduces the most important knowledge extraction technologies in the construction of TCM knowledge graph. This summarizes the knowledge graph technology applied in the scene of TCM from four fields: Chinese medical books, TCM medical records, Chinese medicine formulae and TCM health maintenance. Finally, it will provide construction ideas for the knowledge graph of TCM prevention and control of plague.},
  keywords={Deep learning;Reviews;Prevention and mitigation;Cyberspace;Knowledge graphs;Data science;Maintenance;Data mining;Optimization;Engines;Knowledge graph;Traditional Chinese medicine;Identity of named entity},
  doi={10.1109/DSC63484.2024.00045},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10858988,
  author={Fu, Yibin and Ding, Zhaoyun and Xu, Xiaojie},
  booktitle={2024 IEEE 9th International Conference on Data Science in Cyberspace (DSC)}, 
  title={LLM & Bagging for 1-shot Joint IE}, 
  year={2024},
  volume={},
  number={},
  pages={204-208},
  abstract={Domain-specific few-shot information extraction (IE) has always been the difficulty of domain knowledge graph construction, and there is a new solution in this direction after the emergence of large language models (LLM). In this paper, based on previous research, we propose LLM-based 1-shot relation-entity joint IE scheme, and the bagging enhance LLM IE method is proposed to take advantage of the randomness of the LLM output. Against the background of the concept of Internet of Things (IoT) which has received wide attention globally, we selects the IoT interconnective communication as a domain-specific example, crawls the text of the device pages of L3Harris, RockwellCollins as our corpus, selects large language models that differ in the number of parameters and invocation methods to test the proposed joint IE method in relations given by the IoT interconnective communication ontology. The bagging method is tested based on the IE results of GPT-4 Turbo, and there is an improvement of 1-3%, which shows the effectiveness of traditional machine learning methods in LLM. Finally, the results and shortcomings of this study are analyzed.},
  keywords={Training;Large language models;Knowledge graphs;Ontologies;Data science;Information retrieval;Internet of Things;Bagging;Standards;Random forests;Large Language Model (LLM);Information Extraction (IE);bagging;1-shot;Communication},
  doi={10.1109/DSC63484.2024.00034},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10852450,
  author={Mou, Yongli and Chen, Hanbin and Lode, Gwendolyn Isabella and Truhn, Daniel and Sowe, Sulayman and Decker, Stefan},
  booktitle={2024 2nd International Conference on Foundation and Large Language Models (FLLM)}, 
  title={RadLink: Linking Clinical Entities from Radiology Reports}, 
  year={2024},
  volume={},
  number={},
  pages={443-449},
  abstract={Radiology reports are a critical source of information for patient diagnosis and treatment in the medical domain. However, the vast amount of data contained in these reports is often unstructured, making it challenging to extract and normalize relevant clinical entities. Named Entity Normalization (NEN) is essential for mapping these entities to a standard ontology, facilitating better data integration, retrieval, and analysis. In this paper, we introduce RadLink, a benchmark for NEN in radiology. RadLink builds upon 425 expert-annotated radiology reports from the RadGraph dataset, extending it for NEN by mapping entities to the Unified Medical Language System (UMLS) ontology. We employ a combination of morphological and semantic matching approaches to generate normalization annotations, followed by human review for validation. We aim to set a standard with our benchmark for evaluating NEN methods in the radiology domain, that facilitate interoperability across healthcare systems and accelerate medical research by providing structured, standardized data.},
  keywords={Accuracy;Reviews;Large language models;Unified modeling language;Semantics;Radiology;Ontologies;Benchmark testing;Medical diagnostic imaging;Standards;named entity normalization;large language models;radiology reports},
  doi={10.1109/FLLM63129.2024.10852450},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10852465,
  author={Sorokoletova, Olga and Antonioni, Emanuele and Colò, Giordano},
  booktitle={2024 2nd International Conference on Foundation and Large Language Models (FLLM)}, 
  title={Towards a scalable AI-driven framework for data-independent Cyber Threat Intelligence Information Extraction}, 
  year={2024},
  volume={},
  number={},
  pages={398-406},
  abstract={Cyber Threat Intelligence (CTI) is critical for mitigating threats to organizations, governments, and institutions, yet the necessary data are often dispersed across diverse formats. AI-driven solutions for CTI Information Extraction (IE) typically depend on high-quality, annotated data, which are not always available. This paper introduces 0-CTI, a scalable AI-based framework designed for efficient CTI Information Extraction. Leveraging advanced Natural Language Processing (NLP) techniques, particularly Transformer-based architectures, the proposed system processes complete text sequences of CTI reports to extract a cyber ontology of named entities and their relationships.Our contribution is the development of 0-CTI, the first modular framework for CTI Information Extraction that supports both supervised and zero-shot learning. Unlike existing state-of-the-art models that rely heavily on annotated datasets, our system enables fully dataless operation through zero-shot methods for both Entity and Relation Extraction, making it adaptable to various data availability scenarios. Additionally, our supervised Entity Extractor surpasses current state-of-the-art performance in cyber Entity Extraction, highlighting the dual strength of the framework in both low-resource and data-rich environments.By aligning the system’s outputs with the Structured Threat Information Expression (STIX) format, a standard for information exchange in the cybersecurity domain, 0-CTI standardizes extracted knowledge, enhancing communication and collaboration in cybersecurity operations.},
  keywords={Large language models;Zero shot learning;Standards organizations;Ontologies;Information retrieval;Transformers;Natural language processing;Cyber threat intelligence;Data mining;Computer security;Cyber Threat Intelligence;Natural Language Processing;Structured Threat Information Expression;Named Entity Recognition;Relation Extraction},
  doi={10.1109/FLLM63129.2024.10852465},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10849428,
  author={Adjei-Frempah, Douglas and Chen, Lisa and LePendu, Paea},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={KB2Bench: Toward a Benchmark Framework for Large Language Models on Medical Knowledge}, 
  year={2024},
  volume={},
  number={},
  pages={485-493},
  abstract={While Large Language Models (LLMs) have trans-formed question answering tasks, their propensity for hallucinations continues to drive an area of active research. Efforts toward creating benchmarks to test LLMs' performance on queries, in particular for the field of medicine, have led to a few reputable benchmarks, but these are limited in scope because of the amount of human annotation required. Our framework addresses this issue by leveraging existing, large knowledge bases for medicine to generate vast query and answer sets dynamically, which are less likely to be memorized by LLMs. The framework rests on designing a few key knowledge patterns, which can then generate millions (potentially billions) of queries. This offers a more efficient, cost-effective, and scalable alternative to human-curated annotations used in medical question-and-answer benchmarks. Applying our framework to a small sample of five drug related ontologies, we are already capable of more than 100,000 unique drug related queries, which is 10 to 1000 times larger than existing various human annotation efforts. This paper introduces the KB2Bench framework.},
  keywords={Drugs;Annotations;Large language models;Knowledge based systems;Benchmark testing;Ontologies;Question answering (information retrieval);biomedical informatics;knowledge bases;large language models;ontologies;vocabularies;knowledge representation and reasoning;benchmarking},
  doi={10.1109/ICTAI62512.2024.00075},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{10849427,
  author={Armary, Pauline and El-Vaigh, Cheikh-Brahim and Spicher, Antoine and Narsis, Ouassila Labbani and Nicolle, Christophe},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Identifying Logical Patterns in Text for Reasoning}, 
  year={2024},
  volume={},
  number={},
  pages={837-844},
  abstract={Translating unstructured text into logical format is a key challenge for building ontologies automatically and addressing deductive inference. Most of the approaches have tackled the identification of concepts and relations in text, but few of them have addressed the most complex axioms like class expression subsumption. This work proposes DeLIR, a neuro-symbolic approach to identify complex logical patterns in text by combining a grammatical translation of dependency parsing trees and a fine-tuned Large language Model (LLM). DeLIR combines the strength of the parsing accuracy provided by a grammatical approach and pattern flexibility provided by a finetuned LLM. We evaluated our approach on FOLIO dataset for both translation capacity and inference capability. Our grammatical approach has a perfect parsing accuracy and combining the grammatical approach with LLMs improves the LLMS translation capacity: tinyLlama, T5-small-text2logic, Llama-7B and Mistral-7B. We also evaluate the inference capacity of the different LLMs. Mistral-7B, while being smaller than the state-of-the-art approach using GPT-4, presents similar results to predict the correct inference labels.},
  keywords={Hands;Translation;Accuracy;Large language models;Zero shot learning;Buildings;Ontologies;Syntactics;Cognition;Ontology Learning;Translation to Logic;Natural Language Inference},
  doi={10.1109/ICTAI62512.2024.00122},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{10851643,
  author={Nedelchev, Iliya and Tabakova-Komsalova, Veneta and Stoyanov, Ivan and Stoyanov, Stanimir and Ivanova, Vanya and Kazashka, Tsvetomira},
  booktitle={2024 International Conference Automatics and Informatics (ICAI)}, 
  title={Supporting Digitization of a Cultural and Historical Heritage Platform}, 
  year={2024},
  volume={},
  number={},
  pages={490-493},
  abstract={The article introduces a platform designed for the storage and retrieval of digitized cultural and historical objects from Bulgaria. To enhance the platform’s accessibility for tourists, a dedicated tourist guide has been created and implemented as a personalized assistant. The architecture, along with its distinct components, is thoroughly elucidated. The article provides an overview of the current state of the platform, details the experiments conducted with the realized prototype, and outlines its future development, with a focus on integrating cultural and historical object ontologies. Additionally, a brief background of the platform is provided for context.},
  keywords={Electronic learning;Prototypes;Ontologies;Cultural differences;Informatics;cultural-historical heritage platform;personal assistants;tourist guide;digitalization of cultural-historical objects;ontologies},
  doi={10.1109/ICAI63388.2024.10851643},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10849391,
  author={Saini, Anmol and Ethier, Jeffrey G. and Shimizu, Cogan},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={An Ontology for Conversations with Virtual Research Assistants}, 
  year={2024},
  volume={},
  number={},
  pages={181-186},
  abstract={Conversational artificial intelligence has expanded rapidly in recent years, especially with the growth of large language models (LLMs). Its incorporation in scientific research in the form of research assistants has also become more common-place but remains limited in some capacities, such as in the realm of polymer science. The limitations of LLMs, especially in terms of domain knowledge, warrant the need for other tools, such as knowledge graphs (KGs), to better guide conversations. While such conversational models have been developed in the past, they are generally restricted to particular domains and lack the ability to integrate semantics from various kinds of conversations. Thus, we make progress toward the construction of a universal conversational model that has a focus on the materials domain by combining aspects of existing models. We aim to implement it in such a way that renders it amenable to modifications and usable in a variety of situations. We posit that this model will be adopted and extended by others seeking to accomplish a similar goal in the future.},
  keywords={Adaptation models;Limiting;Conversational artificial intelligence;Large language models;Semantics;Natural languages;Oral communication;Knowledge graphs;Ontologies;Polymers;artificial intelligence;conversational model;knowledge graph;large language model;ontology;ontology design pattern;polymer science},
  doi={10.1109/ICTAI62512.2024.00034},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{10830995,
  author={Li, Jinghong and Phan, Huy and Gu, Wen and Ota, Koichi and Hasegawa, Shinobu},
  booktitle={2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Fish-Bone Diagram of Research Issue: Gain a Bird's-Eye View on a Specific Research Topic}, 
  year={2024},
  volume={},
  number={},
  pages={4936-4941},
  abstract={Novice researchers often face difficulties in understanding a multitude of academic papers and grasping the fundamentals of a new research field. To solve such problems, the knowledge graph supporting research survey is gradually being developed. Existing keyword-based knowledge graphs make it difficult for researchers to deeply understand abstract concepts. Meanwhile, novice researchers may find it difficult to use ChatGPT effectively for research surveys due to their limited understanding of the research field. Without the ability to ask proficient questions that align with key concepts, obtaining desired and accurate answers from this large language model (LLM) could be inefficient. This study aims to help novice researchers by providing a fish-bone diagram that includes causal relationships, offering an overview of the research topic. The diagram is constructed using the issue ontology from academic papers, and it offers a broad, highly generalized perspective of the research field, based on relevance and logical factors. Furthermore, we evaluate the strengths and improvable points of the fish-bone diagram derived from this study's development pattern, emphasizing its potential as a viable tool for supporting research survey.},
  keywords={Surveys;Training;Reviews;Knowledge graphs;Machine learning;Ontologies;User interfaces;Information retrieval;Prompt engineering;Sustainable development},
  doi={10.1109/SMC54092.2024.10830995},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10838760,
  author={Dimitrakopoulos, George and Ehm, Hans and Tsaousi, Eleni},
  booktitle={2024 Winter Simulation Conference (WSC)}, 
  title={Enhanced Ontology Extraction: Integrating GPT AI with Human Knowledge on the Example of EU Standards Related to Semiconductor Supply Chains}, 
  year={2024},
  volume={},
  number={},
  pages={1955-1965},
  abstract={This paper addresses challenges in creating ontologies for the semiconductor supply chain. Ontologies are crucial for seamless data exchange within the semantic web, enabling initiatives like GAIA-X and CatenA-X. Traditionally, ontology creation is complex. Here, we propose a novel AI-assisted method using large language models (LLMs) like ChatGPT 4 Turbo to support human experts. This collaboration aims to expedite ontology generation while maintaining quality. While initial tests show promise, refining the human-AI interface for clear content generation remains a focus. By improving this collaboration, we expect to create more accurate and complete ontologies, fostering efficient information sharing and strengthening the meaningfulness of standards within the semiconductor supply chain.},
  keywords={Semantic Web;Accuracy;Supply chains;Collaboration;Information sharing;Knowledge graphs;Ontologies;Chatbots;Reliability;Standards},
  doi={10.1109/WSC63780.2024.10838760},
  ISSN={1558-4305},
  month={Dec},}@INPROCEEDINGS{10842699,
  author={Mankari, Sagar and Sanghavi, Abhishek},
  booktitle={2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)}, 
  title={Enhancing Vector based Retrieval Augmented Generation with Contextual Knowledge Graph Construction}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The proliferation of unstructured text data necessitates efficient information retrieval systems. Traditional vector-based Retrieval Augmented Generation (RAG) models often fail to capture complex relationships and contextual nuances, limiting effectiveness in knowledge-intensive tasks. We introduce Contextual Knowledge Graph Construction (CKGC), a novel approach enhancing vector-based RAG by dynamically building a knowledge graph that reflects inherent data structures and connections.CKGC leverages text chunking, large language models (LLMs), and ontology mapping. By segmenting text and using LLMs to identify key entities and relationships, CKGC constructs a contextualized knowledge graph enriching information representation. This bridges the gap between semantic similarity and deeper contextual understanding, enabling more accurate and nuanced retrieval.Experiments on 2,000 lease agreements demonstrate that CKGC significantly improves vector-based RAG in information retrieval and question answering tasks, with substantial gains in Mean Reciprocal Rank (MRR) and Top-k Accuracy. CKGC’s adaptability across domains positions it as a valuable tool for enhancing performance and understanding of complex textual data. Our findings underscore CKGC’s transformative potential in unlocking insights from vast text corpora, paving the way for more intelligent and context-aware information retrieval systems.},
  keywords={Accuracy;Large language models;Retrieval augmented generation;Semantics;Knowledge graphs;Organizations;Information retrieval;Question answering (information retrieval);Vectors;Time factors;Retrieval Augmented Generation;Knowledge Graph Construction;Information Retrieval;Ontology Mapping},
  doi={10.1109/IDICAIEI61867.2024.10842699},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10837665,
  author={Okuhara, Fumika and Egami, Shusaku and Sei, Yuichi and Tahara, Yasuyuki and Ohsuga, Akihiko},
  booktitle={2024 21st International Conference on Information Technology Based Higher Education and Training (ITHET)}, 
  title={Automatic Question Generation with Knowledge Graph for Panoramic Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={In recent years, the global social landscape has become increasingly complex, requiring the ability to think from a wide range of diverse perspectives for effective problem-solving. In the field of education, panoramic learning, which implements interdisciplinary and comprehensive education, has become essential. Also, there has been recent research on various aspects of automatic question generation (AGQ), with some studies focusing on generating panoramic questions, which provide a comprehensive understanding, across different genres using knowledge graph (KG). KG is a knowledge base that uses a graph-structured data model and consists of entities and relationships between entities. On the other hand, research on generating panoramic questions for specific subjects with educational purposes has been limited, and this study aims to address that. In this work, we specifically targeted the field of history for question generation and used complemented entities to enhance the inclusion of panoramic knowledge in the field of history. The approach involves enhancing subgraphs with link prediction, which complements missing relationships in KGs, particularly in historical contexts requiring temporal and spatial insights. Through evaluation, it was validated that the proposed method could generate questions containing more panoramic knowledge compared to existing methods.},
  keywords={Training;Hands;Accuracy;Knowledge based systems;Focusing;Knowledge graphs;Question generation;History;Problem-solving;Information technology;Panoramic Learning;Automatic Question Gen-eration;Knowledge Graph;Linked Data},
  doi={10.1109/ITHET61869.2024.10837665},
  ISSN={2473-2060},
  month={Nov},}@INPROCEEDINGS{10825070,
  author={Li, Tangrui and Zhou, Jun and Wang, Hongzheng},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Aligning Knowledge Graphs Provided by Humans and Generated by Neural Networks}, 
  year={2024},
  volume={},
  number={},
  pages={3441-3447},
  abstract={In this paper, an approach that extracts knowledge graphs (KGs) from neural networks (NNs) and aligns the generated KGs with human-provided ones is proposed for network optimization or transparency enhancement, which is achieved by leveraging Vector Symbolic Architectures (VSAs). The approach identifies entities and relations of NN’s knowledge along with the training process, which makes it a plug-and-play solution. Experiments on synthetic data showed that the matching method works on middle and small-size KGs, and tests on MNIST demonstrated that the aligned NN-generated KG could be very close to the human-provided ones. Further tests on Text2KGBench showed that the method could produce KGs from embedding generated by backbone large language models (LLM) that aligned well with human-provided labels as well.},
  keywords={Training;Codes;Large language models;Artificial neural networks;Knowledge graphs;Ontologies;Vectors;Optimization;Synthetic data;Software development management;Vector Symbolic Architecture;Knowledge Graph;Knowledge Graph Alignment},
  doi={10.1109/BigData62323.2024.10825070},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10825567,
  author={Berger, Armin and Berghaus, David and Bashir, Ali Hamza and Grigull, Lorenz and Fendrich, Lara and Lagones, Tom Anglim and Högl, Henriette and Ernst, Gundula and Schmidt, Ralf and Bascom, David and Deußer, Tobias and Bell, Thiago and Lübbering, Max and Sifa, Rafet},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Advancing Personalized Medicine: A Scalable LLM-based Recommender System for Patient Matching}, 
  year={2024},
  volume={},
  number={},
  pages={5876-5883},
  abstract={This study explores efficient algorithms to enhance user matching in Unrare.me, a novel social networking platform designed to connect individuals affected by rare diseases. Our primary objective is to develop a recommender system that identifies and suggests users with similar medical conditions, facilitating meaningful connections within these unique communities. Utilizing textual user profile data, we train sentence embedder models to generate similar embeddings for users that have rated each other high. We investigate various fine-tuning strategies, as well as a hybrid approach between a dense embedder and sparse SPLADE embeddings. Furthermore, we investigate the efficacy of various clustering algorithms, such as TopicBERT for thematic analysis, K-Means for centroid-based grouping, and Latent Dirichlet Allocation (LDA) for probabilistic topic modeling, to reduce the matching complexity and enable better scalability of the platform.},
  keywords={Measurement;Sparse approximation;Social networking (online);Biological system modeling;Computational modeling;Clustering algorithms;Performance gain;Data models;Recommender systems;Diseases;Large Language Models;Text Matching;Rare Diseases;Recommender Systems;Text Embeddings},
  doi={10.1109/BigData62323.2024.10825567},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10825608,
  author={Ho, Duy H. and Das, Udiptaman and Ho, Regina and Lee, Yugyung},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Leveraging Multi-Agent Systems and Large Language Models for Diabetes Knowledge Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={3401-3410},
  abstract={This paper presents a novel framework for constructing a diabetes-specific knowledge graph (KG) using a streamlined multi-agent system powered by Gemini-based Large Language Models (LLMs). Leveraging insights from the 2016 National Diabetes Survey (NNDS) conducted by the National Diabetes Education Program (NDEP), the framework extracts critical variables related to diagnosis, risk perception, medical advice, and self-management practices across diverse U.S. populations. By processing data from the NNDS’s extensive 94-question survey, the methodology performs adaptive ontology mapping using APIs for six major medical standards (e.g., SNOMED CT, ICD-11), ensuring semantic interoperability. Relationships between variables are identified and structured using RDF, RDFS, and OWL standards. The integration of LLMs with ontology tools like Protégé enhances automation and scalability. Results demonstrate the framework’s effectiveness in generating contextually rich and clinically relevant knowledge graphs, providing a robust foundation for advancing healthcare informatics and personalized diabetes management.},
  keywords={Surveys;Scalability;Knowledge graphs;Medical services;Ontologies;Resource description framework;Diabetes;Standards;Medical diagnostic imaging;Multi-agent systems;Knowledge Graph;Multi-Agent System;Large Language Models;Diabetes;Ontology Mapping;RDF;OWL;Healthcare Informatics;AI},
  doi={10.1109/BigData62323.2024.10825608},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10825160,
  author={Kim, Edward and Shrestha, Manil and Foty, Richard and DeLay, Tom and Seyfert-Margolis, Vicki},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Structured Extraction of Real World Medical Knowledge using LLMs for Summarization and Search}, 
  year={2024},
  volume={},
  number={},
  pages={3421-3430},
  abstract={Creation and curation of knowledge graphs at scale can be used to exponentially accelerate the discovery, matching, and analysis of diseases in real-world data. While disease ontologies are useful for annotation, integration, and analysis of biological data, codified disease and procedure categories e.g. SNOMED-CT, ICD10, CPT, etc. rarely capture all of the nuances in a patient condition or, in the case of rare disease, may not even exist. Furthermore, there are multiple disease definitions used in data sources and publications, each having its own structure and hierarchy. Mapping between ontologies, finding disease clusters, and building a representation of the chosen disease area are resource-intensive, often requiring significant human capital. We propose the creation and curation of a patient knowledge graph utilizing large language model extraction techniques. In order to expand in volume and scale, knowledge graphs with generalized language capability allow for data to be extracted using natural language rather than being constrained by the exact terminology or hierarchy of existing ontologies. We develop a method of mapping back to existing ontologies such as MeSH, SNOMED-CT, RxNORM, HPO, etc. to ground the extracted entities to known entities in the medical community.We have access to one of the largest ambulatory care EHR databases in the country. To demonstrate the effectiveness of our method, we benchmark our extraction in a test set with over 33.6M unique patients, in the area of patient search. In this case study, we perform a patient search for a rare disease: Dravet syndrome. Dravet syndrome was codified as an ICD10 recognizable disease in October 2020. In the following research, we describe our method of the construction of patient-specific knowledge graphs and subsequent searches for patients who exhibit symptoms of a particular disease. Using patients with confirmed ICD10 codes for Dravet syndrome as our ground truth, we utilize our LLM-based entity extraction techniques and formalize an algorithmic way of characterizing patients in a grounded ontology to assist in mapping patients to specific diseases. Finally, we present the results of a real-world discovery method on Beta-propeller protein-associated neurodegeneration (BPAN), identifying patients with a rare disease, where no ground truth currently exists.},
  keywords={Proteins;Translation;Annotations;Terminology;Large language models;Knowledge graphs;Ontologies;Benchmark testing;Data mining;Diseases;Large Language Models;Knowledge Graphs;Ontology Mapping;Structured Extraction;Dravet Syndrome;Beta-propeller protein-associated neurodegeneration (BPAN)},
  doi={10.1109/BigData62323.2024.10825160},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10825755,
  author={Oranekwu, Ikechukwu and Elluri, Lavanya and Batra, Gunjan},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Automated Knowledge Framework for IoT Cybersecurity Compliance}, 
  year={2024},
  volume={},
  number={},
  pages={6336-6345},
  abstract={Rapid expansion in the manufacture and use of Internet of Things (IoT) devices has introduced significant challenges in ensuring compliance with cybersecurity standards. To protect user data and privacy, all organizations providing IoT devices must adhere to complex guidelines such as the National Institute of Standards and Technology Inter agency Report (NIST IR) 8259, which defines essential cybersecurity guidelines for IoT manufacturers. However, interpreting and applying these rules from these guidelines and the privacy policies remains a significant challenge for companies. Thus, this project presents a novel approach to extract knowledge from NIST 8259 for creating semantically rich ontology mappings. Our ontology captures key compliance rules, which are stored in a knowledge graph (KG) that allows organizations to crosscheck and update privacy policy documents with ease. The KG also enables real-time querying using SPARQL and offers a transparent view of regulatory adherence for IoT manufacturers and users. By automating the process of verifying cybersecurity compliance, the framework ensures that companies remain aligned with NIST standards, eliminating manual checks and reducing the risk of non-compliance. We also demonstrate that compared to the baseline Large Language Models (LLMs), our proposed framework has more compliance accuracy, and is more efficient and scalable.},
  keywords={Privacy;Companies;Manuals;NIST;Ontologies;NIST Standards;Real-time systems;Internet of Things;Computer security;Guidelines;IoT;Cybersecurity;NIST 8259 standards;KGs;regulatory compliance;automated compliance;LLMs;privacy policies;SPARQL},
  doi={10.1109/BigData62323.2024.10825755},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10825984,
  author={Linxen, Andrea and Schmidt, Vera-Maria and Klinke, Harald and Beecks, Christian},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Ontology-driven knowledge base for digital humanities: Restructuring knowledge organization at the library of the Folkwang University of the Arts}, 
  year={2024},
  volume={},
  number={},
  pages={2449-2455},
  abstract={Academic libraries are increasingly challenged by the need to efficiently manage and analyse vast collections of data and knowledge. The divers formats and organisation methods of these collections, ranging from traditional print media to digital archives and multimedia assets, can hinder researchers’ ability to easily access and retrieve relevant information. This paper introduces an ontology-driven knowledge base to address this issue by enabling the efficient access to knowledge in the application domain and enhancing the semantic search capabilities in the field of Digital Humanities. Our approach focuses on the development of an ontology-drive knowledge base for semantic search in academic libraries by the example of the library of the Folkwang University of Arts that captures the knowledge concepts present in the library’s archival collections. The resulting ontology framework provides a structured representation of domain knowledge, facilitating the integration of diverse data sources, including structured, semi-structured, and unstructured data from the application domain into a triple store knowledge base. By leveraging SPARQL queries generated from Large Language Model (LLM) prompts, we aim to facilitate more intuitive and effective knowledge retrieval. This approach allows users to express their information needs in a more natural and flexible way, leading to more accurate and relevant search results. We evaluate the proposed ontology-driven knowledge base in terms of its integrity, consistency, flexibility, relevance, and scalability. Our evaluation methodology includes a combination of verification and validation techniques, including automated reasoners and query results based on competence questions. Our findings demonstrate the potential of ontology engineering to enhance complex information retrieval in academic libraries. However, we also identify limitations related to processing speed for complex queries and the quality of search results. This research contributes to the field of computational archival science by providing a novel approach to semantic search in academic libraries. By enabling more precise and efficient access to knowledge, our ontology-driven knowledge base has the potential to enrich the academic and Digital Humanities landscape, empowering researchers to delve deeper into the vast resources available within these institutions.},
  keywords={Knowledge engineering;Art;Semantic search;Soft sensors;Scalability;Knowledge based systems;Organizations;Ontologies;Media;Libraries;knowledge engineering;ontology framework;digital humanities;knowledge base;semantic search},
  doi={10.1109/BigData62323.2024.10825984},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10825404,
  author={Berger, Armin and Lagones, Tom Anglim and Grigull, Lorenz and Fendrich, Lara and Bell, Thiago and Högl, Henriette and Ernst, Gundula and Schmidt, Ralf and Bascom, David and Sifa, Rafet and Lübbering, Max},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Tackling Data Sparsity and Combinatorial Challenges in Rare Disease Matching with Medical Informed Machine Learning}, 
  year={2024},
  volume={},
  number={},
  pages={6430-6438},
  abstract={With over 7,000 known rare diseases and a prevalence of less than one in a thousand, rare diseases pose substantial challenges to advanced medical support networks. This study investigates the efficacy of Unrare.me, a novel social networking platform designed for individuals affected by rare diseases, including patients, their family members, and medical professionals, addressing data sparsity and combinatorial complexities in user matching. We demonstrate that simple matching heuristics already serve as a decent basis for collecting user feedback on match quality. Leveraging over 10,000 user matching feedback scores from more than 2,000 active users, we evaluate algorithms including collaborative filtering and user embedding similarity with state-of-the-art Large Language Models (LLMs). With a top-10 and top-5 hit-rate of 55% and 37%, respectively, we show that a combination of medical data augmentation and embeddings significantly enhances performance beyond the initial heuristic baseline.},
  keywords={Social networking (online);Large language models;Collaborative filtering;Machine learning;Big Data;Data augmentation;Data models;Complexity theory;Diseases;Large Language Models;Text Matching;Rare Diseases;Recommender Systems},
  doi={10.1109/BigData62323.2024.10825404},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10825705,
  author={Purohit, Sumit and Chin, George and Mackey, Patrick S and Cottam, Joseph A},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={GraphAide: Advanced Graph-Assisted Query and Reasoning System}, 
  year={2024},
  volume={},
  number={},
  pages={3485-3493},
  abstract={Curating knowledge from multiple siloed sources that contain both structured and unstructured data is a major challenge in many real-world applications. Pattern matching and querying represent fundamental tasks in modern data analytics that leverage this curated knowledge. The development of such applications necessitates overcoming several research challenges, including data extraction, named entity recognition, data modeling, and designing query interfaces. Moreover, the explainability of these functionalities is critical for their broader adoption.The emergence of Large Language Models (LLMs) has accelerated the development lifecycle of new capabilities. Nonetheless, there is an ongoing need for domain-specific tools tailored to user activities. The creation of such digital assistants has gained considerable traction in recent years, with LLMs offering a promising avenue to develop such assistants utilizing domain-specific knowledge and assumptions.In this context, we introduce an advanced query and reasoning system, GraphAide, which constructs a knowledge graph (KG) from diverse sources and allows to query and reason over the resulting KG. GraphAide harnesses both the KG and LLMs to rapidly develop domain-specific digital assistants. It integrates design patterns from retrieval augmented generation (RAG) and the semantic web to create an agentic LLM application. GraphAide underscores the potential for streamlined and efficient development of specialized digital assistants, thereby enhancing their applicability across various domains.},
  keywords={Semantic Web;Accuracy;Large language models;Scalability;Retrieval augmented generation;Semantics;Knowledge graphs;Cognition;Usability;Pattern matching},
  doi={10.1109/BigData62323.2024.10825705},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10837958,
  author={Okuhara, Fumika and Egami, Shusaku and Sei, Yuichi and Tahara, Yasuyuki and Ohsuga, Akihiko},
  booktitle={2024 IEEE 7th International Conference on Computer and Communication Engineering Technology (CCET)}, 
  title={Enhancing Panoramic Competency Through Link Prediction in Question Knowledge Graphs using a Language Representation Model}, 
  year={2024},
  volume={},
  number={},
  pages={267-272},
  abstract={Recently, panoramic knowledge has been required. On the other hand, multiple choice questions are suitable for efficient self-learning. Therefore, the purpose of this study is to create multiple choice questions that can reinforce learners' panoramic knowledge. Specifically, we proposed a method for automatically generating multiple choice questions that use Linked Data to present relevant information to give respondents an overall picture of relevant knowledge. There is some research on the methods that generated questions by extracting small subgraphs from the knowledge graphs consisting of entities(words) and relations(links) between the entities and hiding target words (correct answer words). In this study, our goal is to enhance the panoramic of the subgraphs of a specified size by using the link prediction method to complement edges and represent relationships not present in the knowledge graph when generating questions targeted at specific fields. The method of complementing edges involves first inputting two words as subject and object in Knowledge Graph to calculate the cosine similarity using a pretrained language model based on Wikipedia and Wikidata, then predicting the links as a predicate that should be complemented, and finally generating subgraphs by using the Graph Database added the complemented edges. For this study, we generated questions in the field of history, and since history requires temporal and spatial panoramic knowledge, words related to these aspects were focused on and complemented the relationships between them. As a result, 2,746 relationships were complemented by the proposed method in the subgraphs, and the subgraphs contained more words to learn (words found in textbooks that need to be learned) in a specific field compared to those generated using existing methods.},
  keywords={Hands;Databases;Linked data;Computational modeling;Knowledge graphs;Encyclopedias;Predictive models;History;Online services;Panoramic Knowledge;Linked Data;Knowledge Graph;Automatic Generate Question;Educational Application},
  doi={10.1109/CCET62233.2024.10837958},
  ISSN={2836-5992},
  month={Aug},}@INPROCEEDINGS{10822222,
  author={Yang, Hang and Xiao, Liang and Zhu, Rujun and Liu, Ziji and Chen, Jianxia},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={An LLM supported approach to ontology and knowledge graph construction}, 
  year={2024},
  volume={},
  number={},
  pages={5240-5246},
  abstract={The continuous development in the medical field faces multiple challenges in managing a large amount of literature and research results using traditional ontology and knowledge graph construction methods. These challenges include high labor costs, limited coverage, and poor dynamism of traditional ontology and knowledge graph construction methods. Large language models (LLMs) can solve various natural language processing tasks and can understand and generate human-like natural language, which makes automated construction of ontology expansion and knowledge graphs (KGs) possible. This paper proposes an ontology expansion method based on LLMs, using LLMs to formulate competency questions (CQs) to extend the initial ontology, and then constructing the knowledge graph based on the extended ontology. We demonstrated the feasibility of the method by creating a knowledge graph for breast cancer treatment. The combination of LLMs-based medical ontology and knowledge graph can achieve more efficient medical knowledge management and application, promoting the informatization and intelligent development of the medical field.},
  keywords={Semantic Web;Large language models;Refining;Knowledge graphs;Medical services;Ontologies;Natural language processing;Iterative methods;Reliability;Usability;Ontology;Knowledge graph;LLM;Breast cancer treatment},
  doi={10.1109/BIBM62325.2024.10822222},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10822279,
  author={Chen, Zhijian and Hu, Chuan and Wu, Min and Long, Qingqing and Wang, Xuezhi and Zhou, Yuanchun and Xiao, Meng},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={GeneSum: Large Language Model-based Gene Summary Extraction}, 
  year={2024},
  volume={},
  number={},
  pages={1438-1443},
  abstract={Emerging topics in biomedical research are continuously expanding, providing a wealth of information about genes and their function. This rapid proliferation of knowledge presents unprecedented opportunities for scientific discovery and formidable challenges for researchers striving to keep abreast of the latest advancements. One significant challenge is navigating the vast corpus of literature to extract vital gene-related information, a time-consuming and cumbersome task. To enhance the efficiency of this process, it is crucial to address several key challenges: (1) the overwhelming volume of literature, (2) the complexity of gene functions, and (3) the automated integration and generation. In response, we propose GeneSum, a two-stage automated gene summary extractor utilizing a large language model (LLM). Our approach retrieves and eliminates redundancy of target gene literature and then fine-tunes the LLM to refine and streamline the summarization process. We conducted extensive experiments to validate the efficacy of our proposed framework. The results demonstrate that LLM significantly enhances the integration of gene-specific information, allowing more efficient decision-making in ongoing research.},
  keywords={Navigation;Biological system modeling;Large language models;Redundancy;Decision making;Complexity theory;Data mining;Bioinformatics;Gene summary;prompt learning;large language model},
  doi={10.1109/BIBM62325.2024.10822279},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10822688,
  author={Chen, Dehua and Shen, Zijian and Wang, Mei and Dong, Na and Pan, Qiao and Su, Jianwen},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Extracting Structure Information from Narrative Medical Reports based on LLMs}, 
  year={2024},
  volume={},
  number={},
  pages={5616-5623},
  abstract={Extracting structured information and key details from medical report narratives is crucial to support healthcare data management, analysis and decision-making. However, the specialized nature of the reports, the complexity of the contents, and the high accuracy requirements of the results pose significant challenges to the structuring task. In this paper, we develop an LLM-based method to extract structure information from medical report narratives. Defining the structuring problem as mapping the narrative reports to the domain ontology, we design a framework to develop specialized LLMs that automatically learn and establish the mappings. At the core of this framework are report partitioning and interactive training data generation modules are. By separating complete reports into logically independent segments and training the LLMs on these segments independently, the trained LLMs can accurately capture the semantic relationships within each segment. Additionally, we explore different LLMs and formulate a simplistic scoring method to compare their accuracy, enabling us to select the best-performing model. Experimental evaluation on a real-world breast ultrasound report dataset demonstrates that our method achieves high accuracy with a small training dataset (400 samples). Specifically, the accuracy of structural information extraction and the attribute-value matching accuracy both exceed 96%.},
  keywords={Training;Accuracy;Ultrasonic imaging;Semantics;Training data;Medical services;Ontologies;Information retrieval;Data mining;Standards;Medical examination reports;Large language models;Report structuring},
  doi={10.1109/BIBM62325.2024.10822688},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10822556,
  author={Qi, Jiewei and Luo, Ling and Yang, Zhihao and Wang, Jian and Zhou, Huiwei and Lin, Hongfei},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={An Improved Method for Phenotype Concept Recognition Using Rich HPO Information}, 
  year={2024},
  volume={},
  number={},
  pages={1135-1140},
  abstract={Automatically identifying human phenotype ontology (HPO) concepts from text is important for disease analysis. Existing ontology-driven methods for phenotype concept recognition mainly rely on concept names and synonym information from the ontology, without fully exploiting the rich ontology information. In this paper, we present an improved phenotype concept recognition method by incorporating rich HPO information. We first design prompts with HPO information and use a cutting-edge large language model GPT-4 to generate synonym augmentation for expanding distant supervised training data. We then propose an ontology vector-enhanced phenotype concept classification model to efficiently integrate the taxonomic hierarchical structure of HPO. Additionally, we employ noisy data augmentation to improve the model’s recognition ability in noisy texts and implement a negation detection function. Experimental results on three standard corpora and two typo corpora show our method compares favorably to previous methods and achieves a significant improvement in noisy texts. The source code and data are freely available at https://github.com/DUTIR-BioNLP/PhenoTagger-Updates.},
  keywords={Phenotypes;Source coding;Large language models;Training data;Ontologies;Data augmentation;Noise measurement;Reliability;Standards;Diseases;Phenotype Concept Recognition;Human Phenotype Ontology;Ontology Information Enhancement},
  doi={10.1109/BIBM62325.2024.10822556},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10817901,
  author={Sugioka, Koki and Kamei, Sayaka and Morimoto, Yasuhiko},
  booktitle={2024 Twelfth International Symposium on Computing and Networking Workshops (CANDARW)}, 
  title={BERT Pre-training for Cooking Time Prediction from Cooking Recipes}, 
  year={2024},
  volume={},
  number={},
  pages={157-162},
  abstract={Recently, websites that allow ordinary users to share and search for recipes have become popular. Typically, each recipe contains various information such as a title, a list of ingredients, and descriptions of the cooking process through text and photos. The estimated cooking time is another valuable piece of information when selecting a recipe. However, it can be difficult to accurately describe cooking time because it depends on the cooking environment and conditions, such as heat level and quantity. Therefore, some recipes do not include cooking time information. In this study, we consider the prediction of cooking time in general cases from the list of ingredients and the textual description of the cooking process of each recipe by BERT, a natural language processing model. For this purpose, we propose an additional pre-training method that weights words related to cooking time from a cooking ontology. Our experimental results show that our methods outperform a fine-tuned BERT model with additional pre-training by a standard method.},
  keywords={Heating systems;Conferences;Computational modeling;Ontologies;Predictive models;Natural language processing;Standards;Cooking recipes;BERT;domain-adaptive pre-training},
  doi={10.1109/CANDARW64572.2024.00032},
  ISSN={2832-1324},
  month={Nov},}@INPROCEEDINGS{10818303,
  author={Anaguchi, Fumikatsu and Chakraborty, Sudesna and Morita, Takeshi and Egami, Shusaku and Ugai, Takanori and Fukuda, Ken},
  booktitle={2024 Twelfth International Symposium on Computing and Networking (CANDAR)}, 
  title={Reasoning and Justification System for Domestic Hazardous Behaviors Based on Knowledge Graph of Daily Activities and Retrieval-Augmented Generation}, 
  year={2024},
  volume={},
  number={},
  pages={11-20},
  abstract={Accidents among people over 65 years of age predominantly occur within residential settings, making the maintenance of a safe home environment a crucial social issue. To address this issue, previous research has developed systems that construct Knowledge Graphs (KG) based on simulations of daily household activities, and studies have been conducted on detecting hazardous behaviors using such KG analysis. In this current study, we propose a system capable of presenting the reason and justification for the detected domestic hazardous behaviors. Our system will first generates the reason for the detected behavior using a Large Language Model (LLM). To ensure the accuracy, reliability and reproducibility of the LLM output, the system will provides reliable sources to support the output. We employed Retrieval-Augmented Generation (RAG) to search for sentences similar to the reason generated by the LLM within reliable, authoritative documents describing domestic accident cases and their causes and these will be presented as the evidence alongside the search engine results to the users. Consequently, a knowledge graph (KG) of domestic hazardous behavior is developed based on evidence ontology. Finally, to evaluate the ability of our proposed system in appropriately generating reasons for domestic hazardous behaviors and the adequacy of the justifications provided, the output was rated using LLMs and human volunteers. The rating results showed a significant correlation between LLMs and human evaluation, indicating that the proposed system can provide sufficient reasons and justifications for domestic hazardous behaviors at residential setting.},
  keywords={Correlation;Retrieval augmented generation;Knowledge graphs;Search engines;Ontologies;Reproducibility of results;Behavioral sciences;Sensors;Optimization;Accidents;Retrieval-Augmented Generation;Large Language Model;Explainable AI;Knowledge Graph},
  doi={10.1109/CANDAR64496.2024.00010},
  ISSN={2379-1896},
  month={Nov},}@INPROCEEDINGS{10814501,
  author={Sadlek, Lukáš and Husák, Martin and Čeleda, Pavel},
  booktitle={2024 20th International Conference on Network and Service Management (CNSM)}, 
  title={Hierarchical Modeling of Cyber Assets in Kill Chain Attack Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Cyber threat modeling is a proactive method for identifying possible cyber attacks on network infrastructure that has a wide range of applications in security assessment, risk analysis, and threat exposure management. Popular modeling methods are kill chains and attack graphs. Kill chains divide attacks into phases, and attack graphs depict attack paths. A difficult issue is how to hierarchically model categories of cyber assets that should be used in threat models due to the variety of cyber systems in the current networks. This task should be addressed to provide automation of realistic threat modeling and interoperability with public knowledge bases, such as MITRE ATT&CK. In this paper, we propose a hierarchical modeling methodology for representing cyber assets in kill chain attack graphs. We illustrate its practical application on MITRE D3FEND’s Digital Artifact Ontology. Moreover, we define how cyber assets with related attack techniques should be transformed into logical facts and attack rules. We implemented proof-of-concept software modules that can process data obtained from network and host-based monitoring together with attack rules to generate attack graphs. We evaluated the approach with data from a cyber exercise captured in a network of a digital twin organization. The results show that the approach is applicable in real-world networks and can reveal ground-truth attacks.},
  keywords={Threat modeling;Automation;Large language models;Knowledge based systems;Organizations;Ontologies;Software;Security;Risk analysis;Monitoring;attack graph;kill chain;cyber threat scenario;MITRE ATT&CK;MITRE D3FEND},
  doi={10.23919/CNSM62983.2024.10814501},
  ISSN={2165-963X},
  month={Oct},}@INPROCEEDINGS{10810521,
  author={Sophaken, Chotanansub and Vongpanich, Kantapong and Intaphan, Wachirawit and Utasri, Tharathon and Deepho, Chutamas and Takhom, Akkharawoot},
  booktitle={2024 8th International Conference on Information Technology (InCIT)}, 
  title={Leveraging Graph-RAG for Enhanced Diagnostic and Treatment Strategies in Dentistry}, 
  year={2024},
  volume={},
  number={},
  pages={606-611},
  abstract={This paper presents a method for extracting and interpreting information from diverse, unstructured dental literature using advanced AI techniques. By integrating information extraction, ontologies, and knowledge graphs, the approach enhances the efficiency and accuracy of dental data analysis. Named Entity Recognition (NER) and a Large Language Model (LLM) are employed to extract relevant entities and relationships, which are then structured into triples and integrated with a dental ontology to ensure contextual relevance. This enriched ontology supports Retrieval-Augmented Generation (RAG) applications, enabling advanced querying and analysis. The methodology improves the identification and categorization of dental conditions, treatments, and anatomical terms, providing a structured representation of dental knowledge. Knowledge graphs facilitate the representation and analysis of relationships between entities, fostering insightful interpretations and supporting hypothesis generation, thereby enhancing the accessibility and usability of dental knowledge. Experimental results demonstrate the effectiveness of this approach in managing complex dental information, showcasing the benefits of combining Knowledge Representation (KR) with Machine Learning (ML). This research contributes to dental studies by offering a robust framework for extracting and utilizing knowledge from diverse and extensive datasets.},
  keywords={Large language models;Retrieval augmented generation;Knowledge graphs;Named entity recognition;Machine learning;Ontologies;Dentistry;Data mining;Usability;Information technology;Information Extraction;Dental Literature;Large Language Models;Ontologies;Knowledge Graphs;Oral Health;Dentistry},
  doi={10.1109/InCIT63192.2024.10810521},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10810102,
  author={Henry, Matthew Martianus and Heryanto, Nur Adhianti and Isnan, Mahmud and Nugroho, Kuncahyo Setyo and Pardamean, Bens},
  booktitle={2024 9th International Conference on Information Technology and Digital Applications (ICITDA)}, 
  title={Automatic Multiple Choice Question Generation: A Systematic Literature Review}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Despite their drawbacks, multiple-choice questions (MCQ) have been widely used to assess the students' understanding of lectures through examinations. The development of automatic MCQ generation is beneficial, especially for educators. As a starting point for further development, a Systematic Literature Review (SLR) is conducted to uncover current trends, future challenges, and opportunities in automatic MCQ generation. Previously, an SLR was conducted, but it lacks coverage of the utilization of transformer-based models. This SLR covers the development of automatic MCQ generation using either traditional or advanced approaches such as Transformers. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses framework was used to gather the data from Scopus, IEEE Xplore, SpringerLink, arXiv, and Semantic Scholar. The included articles must be open-access computer science conference papers or journal articles and written in English less than five years ago. Four independent reviewers analyzed the research workflow, evaluation metric, and dataset used in each study. There are 18 included studies, where 17% (n = 3) studies are from 2024, 33% (n = 6) studies are from 2023, 22% (n = 4) studies are from 2022, 11% (n = 2) studies are from 2021, and 17% (n = 3) studies are from 2020. There are 33% (n = 6) of the studies used the traditional feature-based engineering approach, 39% (n = 7) of the studies used the Transformer-based model fine-tuning approach, and the remaining used novel approaches. The study found that BERT variants are the most utilized Transformer-based model in automatic MCQ. The research notes some challenges, but also open various opportunities for further research, including Large Language Model (LLM) utilization for automatic MCQ generation, the utilization BERT-based models for standardized machine-learned evaluation metrics, and the initiative for the creation of an MCQ dataset benchmark.},
  keywords={Measurement;Semantics;Reinforcement learning;Benchmark testing;Ontologies;Transformers;Question generation;Prompt engineering;Standards;Systematic literature review;Multiple choice question;question generation;systematic literature review;Transformer model},
  doi={10.1109/ICITDA64560.2024.10810102},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10801853,
  author={Cornelio, Cristina and Diab, Mohammed},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery}, 
  year={2024},
  volume={},
  number={},
  pages={12435-12442},
  abstract={Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor’s logical rules accurately detect all failures in the analyzed tasks, and that Recover considerably outperforms, for both failure detection and recovery, a baseline method reliant solely on LLMs. Supplementary material, including the OntoThor ontology, is available at: https://recover-ontothor.github.io.},
  keywords={Costs;Large language models;Ontologies;Intelligent robots},
  doi={10.1109/IROS58592.2024.10801853},
  ISSN={2153-0866},
  month={Oct},}@INPROCEEDINGS{10802273,
  author={Nakajima, Haru and Miura, Jun},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Combining Ontological Knowledge and Large Language Model for User-Friendly Service Robots}, 
  year={2024},
  volume={},
  number={},
  pages={4755-4762},
  abstract={Lifestyle support through robotics is an increasingly promising field, with expectations for robots to take over or assist with chores like floor cleaning, table setting and clearing, and fetching items. The growth of AI, particularly foundation models, such as large language models (LLMs) and visual language models (VLMs), is significantly shaping this sector. LLMs, by facilitating natural interactions and providing vast general knowledge, are proving invaluable for robotic tasks. This paper focuses on the benefits of LLMs for "bring-me" tasks, where robots fetch specific items for users, often based on ambiguous instructions. Our previous efforts utilized an ontology extended to handle environmental data to resolve such ambiguities, but faced limitations when unresolvable ambiguities required user intervention for clarity. Here, we enhance our approach by integrating LLMs for providing additional commonsense knowledge, pairing it with ontological data to mitigate the issue of hallucinations and reduce the need for user queries, thus improving system usability. We present a system that merges these knowledge bases and assess its efficacy on "bring-me" tasks, aiming to provide a more seamless and efficient robotic assistance experience.},
  keywords={Visualization;Service robots;Foundation models;Large language models;Knowledge based systems;Ontologies;Usability;Intelligent robots;Floors;Commonsense reasoning},
  doi={10.1109/IROS58592.2024.10802273},
  ISSN={2153-0866},
  month={Oct},}@INPROCEEDINGS{10803481,
  author={Li, Haotian and Xia, Congmin and Hou, Youjuan and Hu, Sile and Quan, Jiang and Liu, Yanjun},
  booktitle={2024 IEEE International Conference on Medical Artificial Intelligence (MedAI)}, 
  title={TCMRD-KG: Design and Development of a Rheumatism Knowledge Graph Based on Ancient Chinese Literature}, 
  year={2024},
  volume={},
  number={},
  pages={588-593},
  abstract={The use of Traditional Chinese medicineTCM in rheumatic diseases dates back to thousands of years ago. Compared with standardized treatment, TCM has the advantages of low cost, low side effects, and flexible medication. Ancient books of traditional Chinese medicine play an important role in clinical and scientific research. This study takes the content related to rheumatism in ancient books of traditional Chinese medicine as the research object, integrates the ontology theory and technology in the knowledge graph, realizes the reconstruction of traditional Chinese medicine information knowledge, and provides basic data structure for data mining and knowledge discovery. This study is the first rheumatism-specific knowledge graph constructed based on ancient books of traditional Chinese medicine; it has tried the construction method of knowledge graph of ancient books of traditional Chinese medicine by combining automatic labeling of mainstream large language models with manual review; and according to the knowledge characteristics of ancient books of traditional Chinese medicine, the existing word segmentation technology is difficult to accurately reproduce the accurate meaning of the original text of ancient books, a new type of entity extraction method is given.},
  keywords={Reviews;Large language models;Knowledge graphs;Manuals;Ontologies;Knowledge discovery;Data structures;Labeling;Data mining;Diseases;Traditional Chinese Medicine;Rheumatic Diseases;Knowledge Graph},
  doi={10.1109/MedAI62885.2024.00083},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10800417,
  author={Chow, Sabrina and Guo, Lilian and Chow, Jonathan and Chia, Chelsea and Li, Sarah and Huang, Dong-Yan},
  booktitle={2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP)}, 
  title={Semantic Search Using LLM-Aided Topic Generation on Knowledge Graphs for Paper Discovery}, 
  year={2024},
  volume={},
  number={},
  pages={353-357},
  abstract={The exponential growth of academic papers presents a huge challenge for researchers, exacerbating the already tedious literature review process. Current tools like Google Scholar and Connected Papers offer solutions for text-based and citation-based searches but fail to address the need for finding semantically similar yet terminologically different papers efficiently. This paper proposes an innovative approach to paper discovery using semantic search to create a knowledge graph of topics and papers. By generating a tree of topics using ChatGPT 4o and calculating semantic similarity with SciBERT, this method aims to uncover relevant papers overlooked by traditional citation-based searches. The solution, validated through quantitative evaluation, demonstrates the potential to improve the efficiency and comprehensiveness of paper discovery.},
  keywords={Semantic search;Navigation;Bibliographies;Focusing;Knowledge graphs;Chatbots;Rendering (computer graphics);Internet;Semantic Search;Knowledge Graphs;Literature Review;Natural Language Processing (NLP);SciBERT},
  doi={10.1109/ISCSLP63861.2024.10800417},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10800180,
  author={Wang, Changlong and Sang, Xiujuan and Wang, Xijie and Gao, Yuan and Liu, Yi},
  booktitle={2024 7th International Conference on Machine Learning and Natural Language Processing (MLNLP)}, 
  title={Research on Knowledge Graph Extraction Methods for Chinese STEM Curriculum}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={STEM education, as an innovative teaching model, has gained widespread attention in recent years. However, the lack of relevant textbooks and learning resources has made its implementation challenging. Developing interdisciplinary knowledge graphs tailored for STEM education has become an urgent issue. To address this, a knowledge extraction framework named Llms4edu is proposed, which utilizes a series of effective prompts to guide large language models in knowledge extraction. Specifically, the knowledge extraction task is transformed into multiple rounds of question-and-answer interactions with the LLM, gradually identifying entity-relation triplets from subject data. Through experiments, an F1-score of 89.4% was achieved on the named entity recognition task in the chemistry subject, and an F1-score of 66.7% on the relation extraction task. Finally, a subject ontology model was built for subject text, and a subject data set was constructed using Llms4edu, which includes three subjects of junior high school mathematics, physics, and chemistry, a total of 2,511 entities, 2,010 relationship triples, and cross-disciplinary knowledge is linked to construct a cross-disciplinary knowledge graph.},
  keywords={Knowledge engineering;Training;Chemistry;Annotations;Large language models;Knowledge graphs;Named entity recognition;Ontologies;Data mining;Physics;interdisciplinary knowledge graph;large language model;prompt engineering},
  doi={10.1109/MLNLP63328.2024.10800180},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10800729,
  author={Lee, Chang-Shing and Wang, Mei-Hui and Tseng, Guan-Ying and Yue, Chao-Cyuan and Hsieh, Hao-Chun and Reformat, Marek},
  booktitle={2024 27th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)}, 
  title={Cao Robot for Taiwanese/English Knowledge Graph Application}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper proposes a Content Attention Ontology (CAO) robot for constructing Taiwanese/English Knowledge Graphs (KGs) by prompting audio or texts to Large Language Models (LLMs), including TAIDE, Zephyr, and Llama 3.1. The collected data includes lecture videos from the IEEE WCCI 2024 in Japan and the 2024 National Language Development Forum in Taiwan, along with students' learning data from the 2024 Summer School on Taiwanese/English Human and Robot Co-Learning at Rende Elementary School (RDES). In addition, the fundamental concepts of Computational Intelligence (CI) and Quantum CI (QCI) learning were incorporated into the study. The generative KGs highlight important concepts, relations, and communities within the collected teaching and learning data. Additionally, we utilized data from subjects wearing braincomputer interface (BCI) devices while speaking Taiwanese/English to generate KGs. We also compared the differences in these KGs and analyzed the similarities between the transcribed texts of lectures and learners. In the future, we plan to expand the CAO robot to more validation fields across Taiwan, aiming to engage young students in speaking Taiwanese while concurrently enhancing their English language skills through interaction with the robot.},
  keywords={Measurement;Quantum computing;Statistical analysis;Large language models;Knowledge graphs;Speech enhancement;Ontologies;Physiology;Robots;Videos;CAO Robot;Knowledge Graph;Taiwanese/English Language Co-learning;Large Language Model;Llama 3.1;TAIDE},
  doi={10.1109/O-COCOSDA64382.2024.10800729},
  ISSN={2472-7695},
  month={Oct},}@INPROCEEDINGS{10801278,
  author={Krouwel, Marien R. and Mulder, Mark A.T.},
  booktitle={2024 26th International Conference on Business Informatics (CBI)}, 
  title={Revising the DEMO method: modelling wait links}, 
  year={2024},
  volume={},
  number={},
  pages={188-197},
  abstract={The DEMO method and modelling language for enterprises has evolved over the past 30 years. Extensive work has been done to specify the modelling language to create DEMO based modelling tools and code generators. However, many issues have been identified regarding the adoption, readability, and completeness of DEMO specification.In this paper, we focus on the Process Model that has several issues regarding its relation with the Action Model and wait links. The proposed solution encompasses adjusting the visualisation of the fact kinds in the Process Model so that they can directly be linked to the event part of the Action Rules Specifications in the Action Model. Some other suggestions for improving the comprehensibility of the Process Model and the impact on DEMO are included. The proposed solution is illustrated with three cases and evaluated against the expected benefits. Preliminary results show an improved readability and easier creation of the Action Model that comply with the Process Model. More research is needed to validate the proposed solution with experts and to solve other issues regarding DEMO.},
  keywords={Visualization;Codes;Symbols;Generators;Informatics;Business;DEMO;Action Model;Process Model;Enterprise Ontology;Enterprise Implementation;Enterprise Design},
  doi={10.1109/CBI62504.2024.00030},
  ISSN={2378-1971},
  month={Sep.},}@INPROCEEDINGS{10782681,
  author={Ye, Bowei and Liang, Jie},
  booktitle={2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
  title={Predicting Functional Surface Topographies Combining Topological Data Analysis and Deep Learning Across the Human Protein Universe}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Characterizing geometric and topological properties of protein structures encompassing surface pockets, interior cavities, and cross channels is important for understanding their functions. Our knowledge of protein structures has been greatly advanced by AI-powered structure prediction tools, with AlphaFold2 (AF2) providing accurate 3D structure predictions for most protein sequences. Nonetheless, there is a substantial lack of function annotations and corresponding functional surface topographical information. We develop a method to predict functional pockets, along with their associated Gene Ontology (GO) terms and Enzyme Commission (EC) numbers, for a set of 65,013 AF2-predicted human non-singleton representative structures, which can be mapped to 186,095 "non-fragment" AF2-predicted human protein structures. The identification of functional pockets, along with their respective GO terms and EC numbers, is achieved by combining topological data analysis and the deep learning method of DeepFRI. All predicted functional pockets for these 65,013 AF2-predicted human representative structures are accessible at: https://cfold.bme.uic.edu/castpfold.},
  keywords={Proteins;Deep learning;Knowledge engineering;Enzymes;Data analysis;Three-dimensional displays;Databases;Annotations;Ontologies;Surface topography},
  doi={10.1109/EMBC53108.2024.10782681},
  ISSN={2694-0604},
  month={July},}@INPROCEEDINGS{10782119,
  author={Munzir, Syed I. and Hier, Daniel B. and Carrithers, Michael D.},
  booktitle={2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
  title={High Throughput Phenotyping of Physician Notes with Large Language and Hybrid NLP Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Deep phenotyping is the detailed description of patient signs and symptoms using concepts from an ontology. The deep phenotyping of the numerous physician notes in electronic health records requires high throughput methods. Over the past 30 years, progress toward making high-throughput phenotyping feasible. In this study, we demonstrate that a large language model and a hybrid NLP model (combining word vectors with a machine learning classifier) can perform high throughput phenotyping on physician notes with high accuracy. Large language models will likely emerge as the preferred method for high throughput deep phenotyping physician notes.Clinical relevance: Large language models will likely emerge as the dominant method for the high throughput phenotyping of signs and symptoms in physician notes},
  keywords={Accuracy;Large language models;Biological system modeling;Medical services;Machine learning;Ontologies;Assistive technologies;Throughput;Vectors;Electronic medical records},
  doi={10.1109/EMBC53108.2024.10782119},
  ISSN={2694-0604},
  month={July},}@INPROCEEDINGS{10773797,
  author={Piazza, Nancirose and Upadhayay, Bibek and Scarpa, Ronald and Behzadan, Vahid},
  booktitle={MILCOM 2024 - 2024 IEEE Military Communications Conference (MILCOM)}, 
  title={Large Language Models for Automatic Standardization of Cyber Deception Plans based on the Adversary Engagement Ontology}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Adversary Engagement Ontology (AEO) is a candidate ontology for the Unified Cyber Ontology (UCO), a community effort aimed at ontological standardization of cyber domain concepts and objects under a unifying framework. It forms a part of the Cyber Domain Ontology (CDO). In the past, community efforts and development have always been labor-intensive with regards to changes in ontology, example generation for adopters, and documentation generation. Large Language Models (LLMs), such as Claude-3.5-Sonnet and GPT4, have been proven capable of automating many tasks and aiding in human expert decision-making. Additionally, LLMs have been used in code interpretation, generation, and evaluation with efficiency and accuracy comparable to that of humans. This emergent capability of LLMs has led to the advantage of using LLMs to streamline the process of ontology development. Motivated by the aforementioned-approaches, we aim to demonstrate how these foundational LLMs can assist in ontology example generation and development, as well as be utilized to automate structured, albeit tedious tasks.},
  keywords={Military communication;Codes;Accuracy;Large language models;Decision making;Standardization;Documentation;Ontologies;Natural language processing;Adversary Engagement;Ontology},
  doi={10.1109/MILCOM61039.2024.10773797},
  ISSN={2155-7586},
  month={Oct},}@INPROCEEDINGS{10773919,
  author={Mohsenzadegan, Kabeh and Tavakkoli, Vahid and Kambale, Witesyavwirwa Vianney and Kyamakya, Kyandoghere},
  booktitle={2024 Sensor Data Fusion: Trends, Solutions, Applications (SDF)}, 
  title={A Hybrid AI Framework Integrating Ontology Learning, Knowledge Graphs, and Large Language Models for Improved Data Model Translation in Smart Manufacturing and Transportation}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Interoperability among diverse data standards is crucial for advancing digital technologies in smart manufacturing and transportation. This paper studies and presents a hybrid AI framework that integrates Ontology Learning (OL), Knowledge Graphs (KGs), and Large Language Models (LLMs) to enhance data translation across different standards. Focusing on IEEE 1451, ISO 15926, and IEC 61499, which exemplify the challenges of translating between distinct data models, we evaluate the performance of OL, KGs, and LLMs in terms of accuracy, scalability, efficiency, robustness, and flexibility. The findings indicate that the hybrid framework effectively leverages OL for semantic structuring, KGs for relational modeling, and LLMs for linguistic and contextual processing. This integration significantly improves the accuracy and adaptability of data translations, offering a comprehensive solution tailored to the complex environments of smart manufacturing and transportation, thereby advancing cross-standard data interoperability.},
  keywords={Accuracy;Large language models;Transportation;Data integration;Knowledge graphs;Ontologies;Data models;Standards;Interoperability;Smart manufacturing;Data Model Translation;Ontology Learning (OL);Knowledge Graphs (KGs);Large Language Models (LLMs);Smart Manufacturing;Data Interoperability;AI in Transportation;Hybrid AI Framework;Semantic Mapping;Cross-Standard Data Integration},
  doi={10.1109/SDF63218.2024.10773919},
  ISSN={2473-7666},
  month={Nov},}@INPROCEEDINGS{10772514,
  author={Li, Yi and Tian, Liwei and Yi, Chengyi and Li, Jingjing and Qin, Xiaodong and He, Yuxuan and Su, Huai},
  booktitle={2024 6th International Conference on System Reliability and Safety Engineering (SRSE)}, 
  title={A Large Language Model Based Knowledge Mining Method for Improving the Reliability of Fire Water Systems}, 
  year={2024},
  volume={},
  number={},
  pages={410-413},
  abstract={The fire water system plays a critical role in protecting both infrastructure and human lives. An essential aspect of enhancing the reliability of this system is fault diagnosis. However, the current fault diagnosis methods primarily rely on data-driven approaches, which often result in a high threshold for application due to their lack of interpretability. To tackle this challenge, this paper introduces a novel approach based on large language models for knowledge mining from textual data to extract fault information related to the fire water system, thereby enhancing the interpretability of data-driven fault diagnosis methods. The methodology followed in this paper consists of two main steps: firstly, analyzing the characteristics and principles of fire water system faults to develop a fault ontology, and secondly, creating a knowledge mining model using a large language model guided by the established fault ontology. Experimental findings indicate that the proposed model achieves an F1 score of 0.944, meeting the necessary criteria for effective knowledge mining in fire water system fault analysis. Furthermore, a comparative experiment was conducted to evaluate the performance of various encoder models, including GRU, BiGRU, LSTM, BiLSTM, and pre-trained large language model BERT. The results revealed a significant improvement in performance with the BERT encoder, showing increases in F1 scores of $22.12 \%$, $2.27 \%, 17.41 \%$, and $3.16 \%$ compared to the other models, respectively. This study provides valuable interpretative insights that can enhance the engineering applicability and reliability of data-driven fault diagnosis methods in fire water system.},
  keywords={Fault diagnosis;Water;Analytical models;Accuracy;Large language models;Bidirectional control;Ontologies;Reliability engineering;Encoding;Data mining;large language model;system reliability;fire water system;safety engineering;knowledge mining},
  doi={10.1109/SRSE63568.2024.10772514},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10758538,
  author={Chusova, Alina and Artemieva, Irina and Chusov, Andrey},
  booktitle={2024 IEEE International Multi-Conference on Engineering, Computer and Information Sciences (SIBIRCON)}, 
  title={A Hybrid Approach to Extraction of Knowledge From Scientific Texts Based on Large Language Models and Domain Dictionaries}, 
  year={2024},
  volume={},
  number={},
  pages={266-271},
  abstract={The vast information landscape of the Internet constitute a significant challenge for extracting valuable content. The lack of standardized data models and structures necessitates ad hoc solutions, often requiring expert knowledge that developers may lack. While Large Language Models (LLMs) hold promise for addressing this challenge, their susceptibility to AI hallucinations and inaccuracies necessitates ongoing research. This paper introduces a hybrid approach for extracting information on computational methods that combines domain-specific dictionaries with LLMs in order to improve the accuracy of method categorization. Our system incorporates explainability, allowing users to understand the reasoning behind method assignments. Furthermore, user-driven training is facilitated by allowing users to select theories and highlight relevant keywords, enhancing learning capabilities of the system. Its implementation demonstrates the effectiveness of this approach, achieving an impressive Fl-score of up to 90.3 %. This research contributes to the ongoing effort to develop robust and accurate knowledge extraction systems for navigating the ever-expanding landscape of online information.},
  keywords={Training;Accuracy;Dictionaries;Navigation;Large language models;Computational modeling;Ontologies;Acoustics;Software;Data mining;Large Language Model;Dictionary;Classilication;Acoustic models},
  doi={10.1109/SIBIRCON63777.2024.10758538},
  ISSN={2995-0996},
  month={Sep.},}@INPROCEEDINGS{10754778,
  author={Govindharajan, Hariharan and Vijayakumar, Senthilkumar},
  booktitle={2024 IEEE 15th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)}, 
  title={A Framework for automated selective Fine-Tuning of Domain-Specific Large Language Models Using Graph-Based Retrieval Augmented Generation}, 
  year={2024},
  volume={},
  number={},
  pages={431-439},
  abstract={Graph based retrieval augmented generation technique in Large Language Model (LLM) brings in major advantages by providing deep context to LLMs through relational knowledge graph for text generation, classification, question and answering use cases. However, maintaining vast data volume of domain specific data in a knowledge graph with complex relationships and querying from it every time a prompt is being posted to LLM, is a time consuming and expensive process. This paper presents a novel framework for selectively fine-tuning domain-specific large language models (LLMs) using a multi-stage Knowledge Graph (KG) based Retrieval Augmented Generation (RAG) pipeline and an Automated Incremental Fine-tuning System (AIFS). The proposed system aims to enhance the accuracy and relevance of LLM responses for text generation and Question Answering use cases by finetuning the LLM incrementally based on highly sought and highly relevant information in knowledge graph identified by leveraging page rank algorithm in KG. The framework comprises three major subsystems: Knowledge Graph Generation, Automated Incremental fine-tuning system (AIFS), and Domain Based Information Retrieval (DBIR). The effectiveness of the system is demonstrated through its ability to incrementally fine-tune LLMs based on selected highly relevant nodes within the KG, thereby improving the model’s domain-specific knowledge, response accuracy by 90% and reduce cost by 71.8%.},
  keywords={Accuracy;Costs;Large language models;Pipelines;Knowledge graphs;Information retrieval;Mobile communication;Question answering (information retrieval);Artificial Intelligence(AI);Knowledge Graph(KG);Large Language Models(LLM);LLM Fine-tuning;Contextual Information Extraction &Retrieval Systems},
  doi={10.1109/UEMCON62879.2024.10754778},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10749735,
  author={Casalicchio, Emiliano and Cotumaccio, Alberto},
  booktitle={2024 IEEE International Conference on Cloud Engineering (IC2E)}, 
  title={AI-CRAS: AI-driven Cloud Service Requirement Analysis and Specification}, 
  year={2024},
  volume={},
  number={},
  pages={11-21},
  abstract={Automated analysis and specification of software requirements expressed in natural language is a challenge addressed by the research community and is becoming a reality thanks to the advances in Artificial Intelligence (AI) and Natural Language Processing (NLP) techniques. While the research community focuses mainly on generic software requirements or specialized solutions for security requirements, we find a gap in the automation of analysis and specification for requirements in the cloud computing domain and the automatic mapping of requirements on actual products offered in the cloud service market. In this research work, we propose AI-CRAS an AI-driven cloud service requirement analysis and specification methodology. The proposed method, which leverages state-of-the-art transformer-based large language model, has been implemented and validated in a real case. Experimental results demonstrate that the model performed well in binary and multi-label classification of requirements (achieving recall/F1-score of $0.96 / 0.92$ and $0.86 / 0.76$, respectively) and mapping requirements into actual cloud services.},
  keywords={Training;Cloud computing;Accuracy;Feature extraction;Transformers;Natural language processing;Software;Vectors;Stakeholders;Testing;cloud computing;cloud migration;cloud services;requirement engineering;artificial intelligence;natural language processing;cloud service broker},
  doi={10.1109/IC2E61754.2024.00009},
  ISSN={2694-0825},
  month={Sep.},}@INPROCEEDINGS{10743881,
  author={Zhang, Kaiwen and Su, Feiyu and Huang, Yixiang and Li, Yanming and Wu, Fengqi and Mao, Yuhan},
  booktitle={2024 9th International Conference on Intelligent Computing and Signal Processing (ICSP)}, 
  title={The Application of Fine-Tuning on Pretrained Language Model in Information Extraction for Fault Knowledge Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={469-473},
  abstract={Constructing fault knowledge graphs holds significant importance for achieving intelligent maintenance and diagnosis in high-end equipment manufacturing. Effective information extraction and knowledge graph construction have proven challenging due to the lack of standardized representation of semantically complex unstructured text in the industrial domain. Therefore, in this study, we performed fine-tuning on the pre-trained language model (ChatGLM2-6B) with specific prompts to achieve information extraction from fault-related texts, ultimately leading to the construction of a fault knowledge graph. Experimental results demonstrate that the proposed method not only supports fine-tuning with limited data but also exhibits enhanced capability in understanding complex semantics related to fault symptoms and causes.},
  keywords={Computational modeling;Semantics;Knowledge graphs;Signal processing;Ontologies;Information retrieval;Stability analysis;Data models;Manufacturing;Maintenance;Pretrained language model;Parameter-efficient fine-tuning;Information extraction;Fault knowledge graph},
  doi={10.1109/ICSP62122.2024.10743881},
  ISSN={},
  month={April},}@INPROCEEDINGS{10740201,
  author={Batten, Christopher and Pinckney, Nathaniel and Liu, Mingjie and Ren, Haoxing and Khailany, Brucek},
  booktitle={2024 ACM/IEEE 6th Symposium on Machine Learning for CAD (MLCAD)}, 
  title={PyHDL-Eval: An LLM Evaluation Framework for Hardware Design Using Python-Embedded DSLs}, 
  year={2024},
  volume={},
  number={},
  pages={1-17},
  abstract={Embedding hardware design frameworks within Python is a promising technique to improve the productivity of hardware engineers. At the same time, there is significant interest in using large-language models (LLMs) to improve key chip design tasks. This paper describes PyHDL-Eval, a new framework for evaluating LLMs on specification-to-RTL tasks in the context of Python-embedded domainspecific languages (DSLs). The framework includes 168 problems, Verilog reference solutions, Verilog test benches, Python test scripts, and workflow orchestration scripts. We use the framework to conduct a detailed case study comparing five LLMs (CodeGemma 7B, Llama3 8B/70B, GPT4, and GPT4 Turbo) targeting Verilog and five Python-embedded DSLs (PyMTL3, PyRTL, MyHDL, Migen, and Amaranth). Our results demonstrate the promise of in-context learning when applied to smaller models (e.g.,pass rate for CodeGemma 7B improves from 14.9% to 32.7% on Verilog) and Python-embedded DSLs (e.g., pass rate for LLama3 70B improves from 0.6% to 33.0% on PyMTL3). We find LLMs perform better when targeting Verilog as compared to Python-embedded DSLs (e.g., pass rate for GPT4 Turbo is 72.2% on Verilog and 29.8–62.0% on the Python-embedded DSLs) despite using a popular general-purpose host language. PyHDL-Eval will serve as a useful framework for future research at the intersection of Python-embedded DSLs and LLMs. CCS Concepts • Hardware → Hardware description languages and compilation; • Computing methodologies → Machine learning.},
  keywords={Productivity;Solid modeling;Machine learning;Hardware;DSL;Hardware design languages;Chip scale packaging;hardware description languages;Python-embedded domain-specific languages;large language models},
  doi={10.1109/MLCAD62225.2024.10740201},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10731381,
  author={Grassi, Lucrezia and Recchiuto, Carmine Tommaso and Sgorbissa, Antonio},
  booktitle={2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)}, 
  title={Enhancing LLM-Based Human-Robot Interaction with Nuances for Diversity Awareness}, 
  year={2024},
  volume={},
  number={},
  pages={2287-2294},
  abstract={This paper presents a system for diversity-aware autonomous conversation leveraging the capabilities of large language models (LLMs). The system adapts to diverse populations and individuals, considering factors like background, personality, age, gender, and culture. The conversation flow is guided by the structure of the system’s pre-established knowledge base, while LLMs are tasked with various functions, including generating diversity-aware sentences. Achieving diversity-awareness involves providing carefully crafted prompts to the models, incorporating comprehensive information about users, conversation history, contextual details, and specific guidelines. To assess the system’s performance, we conducted both controlled and real-world experiments, measuring a wide range of performance indicators.},
  keywords={Large language models;Knowledge based systems;Human-robot interaction;Oral communication;Ontologies;Hybrid power systems;Time factors;Noise measurement;History;Robots},
  doi={10.1109/RO-MAN60168.2024.10731381},
  ISSN={1944-9437},
  month={Aug},}@INPROCEEDINGS{10731499,
  author={Wu, Shanglin and Yao, Xiaodong and Liu, Shu and Liang, Haitao and Liu, Zhenyuan},
  booktitle={2024 16th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)}, 
  title={Domain Knowledge Graph Construction Methods of Construction Schedule in Steel Structure Projects}, 
  year={2024},
  volume={},
  number={},
  pages={39-44},
  abstract={In the domain of engineering construction, steel structure engineering construction has accumulated a large amount of data, and the development of knowledge graph construction technology to structure this data can provide effective support for high-quality construction organization. This paper analyzes the knowledge sources of construction schedule for steel structure projects, constructs a domain ontology of construction schedule for assembled steel structure projects by using a seven-step method and points out the current problems of knowledge extraction for construction organization design for assembled steel structure projects. The semi-structured construction schedule data is subject to event extraction, and the unstructured knowledge of construction schedule is discussed from the perspective of few-shot methods and large language modeling for the knowledge extraction of construction schedule for steel structure projects.},
  keywords={Schedules;Human-machine systems;Knowledge graphs;Organizations;Ontologies;Data models;Steel;Data mining;Cybernetics;domain knowledge graphs;construction schedule;ontology construction;event extraction},
  doi={10.1109/IHMSC62065.2024.00017},
  ISSN={2157-8982},
  month={Aug},}@ARTICLE{10734210,
  author={Xu, Jun and Zhang, Hao and Zhang, Haijing and Lu, Jiawei and Xiao, Gang},
  journal={IEEE Access}, 
  title={ChatTf: A Knowledge Graph-Enhanced Intelligent Q&A System for Mitigating Factuality Hallucinations in Traditional Folklore}, 
  year={2024},
  volume={12},
  number={},
  pages={162638-162650},
  abstract={Large language models are rapidly advancing the field of artificial intelligence, with current research focusing primarily on traditional natural language understanding tasks, such as question answering and information extraction. However, in knowledge-intensive domains, such as intangible cultural heritage, hallucination problems due to insufficient domain knowledge persist. To address this, we present ChatTf, a knowledge graph-enhanced intelligent Q&A system, exemplified by Chinese traditional folklore, aimed at reducing factuality hallucinations in this domain. Specifically, we constructed the Traditional Folklore Ontology (TFOnto) and proposed the Zero-shot Traditional Folklore Triplet Extraction (ZFTE) framework. Driven by TFOnto, ZFTE builds a Traditional Folklore Knowledge Graph (TFKG). We then proposed a dual-stage Retrieval-Augmented Generation framework (TFKG-RAG) based on TFKG to provide traditional folklore knowledge to large language models, mitigating factuality hallucinations in folklore Q&A tasks. In the experimental phase, ChatTf achieved an accuracy of 96.7% on a self-built TFCQD test set, outperforming several state-of-the-art baseline methods. This demonstrates the accuracy and reliability of folklore domain question answering.},
  keywords={Ontologies;Knowledge graphs;Cultural differences;Large language models;Knowledge engineering;Training;Cognition;Accuracy;Semantics;Reliability;Knowledge graph;large language model;question answering;retrieval-augmented generation;traditional folklore},
  doi={10.1109/ACCESS.2024.3485877},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10722791,
  author={Haque, Mohd Ariful and Kamal, Marufa and George, Roy and Gupta, Kishor Datta},
  booktitle={2024 IEEE 11th International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={Utilizing Structural Metrics from Knowledge Graphs to Enhance the Robustness Quantification of Large Language Models (Extended Abstract)}, 
  year={2024},
  volume={},
  number={},
  pages={1-2},
  abstract={The goal of this study is to determine whether large language models (LLMs) like CodeLlama, Mistral, and Vicuna can be used to build knowledge graphs (KGs) from textual data. We create class descriptions for well-known KGs such as DBpedia, YAGO, and Google Knowledge Graph, from which we extract RDF triples and enhance these graphs using different preprocessing methods. Six structural quality measures are used in the study to compare the constructed and existing KGs. Our results demonstrate how important LLMs are to improving KG construction and provide insightful information for KG construction researchers. Moreover, an in-depth analysis of popular open-source LLM models enables researchers to identify the most efficient model for various tasks, ensuring optimal performance in specific applications.},
  keywords={Measurement;Analytical models;Large language models;Knowledge graphs;Ontologies;Data science;Resource description framework;Robustness;Internet},
  doi={10.1109/DSAA61799.2024.10722791},
  ISSN={2766-4112},
  month={Oct},}@INPROCEEDINGS{10711793,
  author={Pan, Xinyu and Gong, Jie and Wen, Sijie and Zhuang, Weibin and Li, Xinyu},
  booktitle={2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)}, 
  title={Mining User Requirement Scenarios and Generating Design Solutions for Rehabilitation Aids Based on Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={3155-3161},
  abstract={The development of the rehabilitation aids industry for the disabled has been pivotal in recent years, particularly in the personalized design of lower limb rehabilitation aids. Facing challenges in meeting individualized demands in design practice and the information gap between medical professionals and users, we propose a design Knowledge Graph (KG) method based on the Function-Behavior-Structure (FBS) model. This approach utilizes open-source large language models (LLMs) and fine-tunes them with instruction data generated by self-instructions to improve the accuracy of user requirements mining. The method aims to enhance the personalization and innovation of rehabilitation aids design through the integration of KG and LLM, effectively narrowing the cognitive gap between service providers and users. The anticipated results of the study are expected to promote efficient innovation in rehabilitation aids design, better meeting the needs of the disabled community.},
  keywords={Industries;Technological innovation;Computer aided software engineering;Automation;Accuracy;Large language models;Conferences;Knowledge graphs;Data mining},
  doi={10.1109/CASE59546.2024.10711793},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{10711329,
  author={Höfgen, Josua and Vogel-Heuser, Birgit and Vicaria, Alejandra and Pouzolz, François and Kurzhals, Christian},
  booktitle={2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)}, 
  title={Enhancing Model-Based System Architecting Through Formalized Decision Management}, 
  year={2024},
  volume={},
  number={},
  pages={1053-1060},
  abstract={System architecture decisions are typically informally captured in design documents. This practice leads to a loss of knowledge that impedes later activities like design changes, impact analysis, and reuse. Model-Based Systems Engineering (MBSE) frameworks support the development of increasingly complex systems but must address the problem of capitalization on architectural knowledge. To this end the "Decision Ontology for System Architectures (DOSA)" is developed to provide a formalized data model to capture system architecture decisions. DOSA is developed through a synthesis of decisions observed while developing an architecture model for a preliminary study of a novel satellite navigation system at Airbus Defence and Space. The approach is integrated into an MBSE framework enabling engineers to capture decisions that influence the architecture’s characteristics while developing the system model and imminently trace decision to artifacts of the system architecture. Subsequent visual inspection and formal querying of the decision graph facilitates the analysis of made decisions, and their interrelations.},
  keywords={Knowledge engineering;Visualization;Computer aided software engineering;Atmospheric modeling;Systems architecture;Ontologies;Inspection;Satellite navigation systems;Data models;Complex systems;MBSE;System Architecture;Decision Management;Ontology},
  doi={10.1109/CASE59546.2024.10711329},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{10708354,
  author={Kawther, Dridi and Wahiba, Ben Abdessalem Karaa},
  booktitle={2024 10th International Conference on Control, Decision and Information Technologies (CoDIT)}, 
  title={Comparative Analysis of Multilingual Text Classification Techniques: A Review of Current Approaches and Emerging}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The widespread availability of electronic documents and the exponential growth of the World Wide Web have made the automatic categorization of documents a critical method for organizing information and facilitating knowledge discovery, and information retrieval. This paper aims to examine the key techniques and methodologies utilized in multilingual document classification, while also bringing attention to some of the complex challenges that still need to be addressed. In particular, the paper presents a thorough review of the literature concerning the theory and methods of multilingual document representation and classification.},
  keywords={Dimensionality reduction;Reviews;Text categorization;Process control;Ontologies;Knowledge discovery;Vectors;Web sites;Information technology;Indexing},
  doi={10.1109/CoDIT62066.2024.10708354},
  ISSN={2576-3555},
  month={July},}@INPROCEEDINGS{10711065,
  author={Reif, Jonathan and Jeleniewski, Tom and Gill, Milapji Singh and Gehlhoff, Felix and Fay, Alexander},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Chatbot-Based Ontology Interaction Using Large Language Models and Domain-Specific Standards}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={The following contribution introduces a concept that employs Large Language Models (LLMs) and a chatbot interface to enhance SPARQL query generation for ontologies, thereby facilitating intuitive access to formalized knowledge. Utilizing natural language inputs, the system converts user inquiries into accurate SPARQL queries that strictly query the factual content of the ontology, effectively preventing misinformation or fabrication by the LLM. To enhance the quality and precision of outcomes, additional textual information from established domain-specific standards is integrated into the ontology for precise descriptions of its concepts and relationships. An experimental study assesses the accuracy of generated SPARQL queries, revealing significant benefits of using LLMs for querying ontologies and highlighting areas for future research.},
  keywords={Fabrication;Accuracy;Large language models;Semantics;Natural languages;Ontologies;Chatbots;Fake news;Standards;Manufacturing automation;Semantic Web;Ontologies;Large Language Models;Cyber-Physical Systems;Industry 4.0},
  doi={10.1109/ETFA61755.2024.10711065},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10710775,
  author={Vieira da Silva, Luis Miguel and Kocher, Aljosha and Gehlhoff, Felix and Fay, Alexander},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={On the Use of Large Language Models to Generate Capability Ontologies}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Capability ontologies are increasingly used to model functionalities of systems or machines. The creation of such onto-logical models with all properties and constraints of capabilities is very complex and can only be done by ontology experts. However, Large Language Models (LLMs) have shown that they can generate machine-interpretable models from natural language text input and thus support engineers / ontology experts. Therefore, this paper investigates how LLMs can be used to create capability ontologies. We present a study with a series of experiments in which capabilities with varying complexities are generated using different prompting techniques and with different LLMs. Errors in the generated ontologies are recorded and compared. To analyze the quality of the generated ontologies, a semi-automated approach based on RDF syntax checking, OWL reasoning, and SHACL constraints is used. The results of this study are very promising because even for complex capabilities, the generated ontologies are almost free of errors.},
  keywords={Shape;Large language models;Natural languages;OWL;Ontologies;Syntactics;Cognition;Resource description framework;Complexity theory;Testing;Large Language Models;LLMs;Capabilities;Skills;Ontologies;Semantic Web;Model-Generation},
  doi={10.1109/ETFA61755.2024.10710775},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10711054,
  author={Knollmeyer, Simon and Akmal, Muhammad Uzair and Koval, Leonid and Asif, Saara and Mathias, Selvine G. and Groβmann, Daniel},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Document Knowledge Graph to Enhance Question Answering with Retrieval Augmented Generation}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Reusing and managing existing knowledge from available documents is crucial for success in the factory planning domain. By leveraging Artificial Intelligence (AI) and Question Answering (QA) systems, users can query a document corpus through a chat-based application and receive precise answers. The recent advancements in Large Language Models (LLMs) and their linguistic capabilities present new opportunities for such applications. Utilizing the methodology of Retrieval Augmented Generation (RAG), document sections are provided to the LLM based on user queries. However, existing RAG implementations that use vector databases as document repositories face limitations when answering questions that extend beyond the text content of the documents. To address this issue, this paper proposes a concept to enhance RAG systems by integrating a Knowledge Graph (KG) constructed from the document structures.},
  keywords={Databases;Large language models;Knowledge graphs;Linguistics;Question answering (information retrieval);Vectors;Production facilities;Planning;Manufacturing automation;Faces;Information management;Retrieval Augmented Generation;Knowledge Graph;Large Language Models},
  doi={10.1109/ETFA61755.2024.10711054},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10710783,
  author={Vieira da Silva, Luis Miguel and Kocher, Aljosha and Gehlhoff, Felix and Fay, Alexander},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Toward a Method to Generate Capability Ontologies from Natural Language Descriptions}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={To achieve a flexible and adaptable system, capabil-ity ontologies are increasingly leveraged to describe functions in a machine-interpretable way. However, modeling such complex ontological descriptions is still a manual and error-prone task that requires a significant amount of effort and ontology expertise. This contribution presents an innovative method to automate capability ontology modeling using Large Language Models (LLMs), which have proven to be well suited for such tasks. Our approach requires only a natural language description of a capability, which is then automatically inserted into a predefined prompt using a few-shot prompting technique. After prompting an LLM, the resulting capability ontology is automatically verified through various steps in a loop with the LLM to check the overall correctness of the capability ontology. First, a syntax check is performed, then a check for contradictions, and finally a check for hallucinations and missing ontology elements. Our method greatly reduces manual effort, as only the initial natural language description and a final human review and possible correction are necessary, thereby streamlining the capability ontology generation process.},
  keywords={Adaptation models;Costs;Reviews;Large language models;Natural languages;Manuals;Ontologies;Syntactics;Manufacturing automation;Testing;Large Language Models;LLMs;Capabilities;Skills;Ontologies;Semantic Web;Model-Generation},
  doi={10.1109/ETFA61755.2024.10710783},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10710806,
  author={Meyer, Frederic and Freitag, Lennart and Hinrichsen, Sven and Niggemann, Oliver},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Potentials of Large Language Models for Generating Assembly Instructions}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={With the increasing complexity in manual assembly and a demographic decline in skilled workforce, the importance of well-documented processes through assembly instructions has grown. Creating these instructions is a time-consuming and knowledge-intensive task that typically relies on experienced employees. Although various automation solutions have been proposed to assist in generating assembly instructions, they often fall short in providing detailed textual guidance. With the rise of generative artificial intelligence (AI), new potentials arise in this domain. Therefore, this paper explores these potentials by employing various large language models (LLMs), prompting techniques and input data in an experimental setup for generating detailed assembly instructions, including the planning of assembly sequences as well as textual guidance on tools, assembly activities, and quality assurance measures. The findings reveal promising opportunities in leveraging LLMs but also substantial challenges, particularly in assembly sequence planning. To improve the reliability of generating assembly instructions, we propose a multi-agent concept that decomposes the complex task into simpler subtasks, each managed by specialized agents.},
  keywords={Quality assurance;Generative AI;Large language models;Decision making;Manuals;Planning;Complexity theory;Reliability;Assembly;Manufacturing automation;agent;assembly instruction;experiment;GPT;large language model;LLM;prompt},
  doi={10.1109/ETFA61755.2024.10710806},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10710789,
  author={Schoch, Nicolai and Hoernicke, Mario and Strem, Nika and Stark, Katharina},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Engineering Data Funnel (WIP) – An Ontology-Enhanced LLM-Based Agent and MoE System for Engineering Data Processing}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Automation Engineering of a process automation system is still a very manual effort due to limited support for the interpretation and processing of process design specification documents. Even though standards for digital data exchange between process and automation engineering do exist, those formats are rarely used and consequently the immense automation potential in automation engineering cannot be lifted. This contribution presents an AI -based approach and prototype - using an ontology-enhanced LLM -based agent and a mixture-of-experts system - to structure and formalize multimodal unstructured process design information as in PDF, Excel, and Word formats and make it available for state-of-the-art engineering tools for the long-known “Automation of Automation”.},
  keywords={Process design;Automation;Prototypes;Manuals;Portable document format;Data processing;Artificial intelligence;Standards;Manufacturing automation;engineering design specification;engineering data processing;LLM-based agent;mixture of experts;ontology-driven information processing;automation of automation},
  doi={10.1109/ETFA61755.2024.10710789},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10706469,
  author={Incitti, Francesca and Salfinger, Andrea and Snidaro, Lauro and Challapalli, Sri},
  booktitle={2024 27th International Conference on Information Fusion (FUSION)}, 
  title={Leveraging LLMs for Knowledge Engineering from Technical Manuals: A Case Study in the Medical Prosthesis Manufacturing Domain}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Ontologies are nowadays widely used to organize information across specific domains, being effective due to their hierarchical structure and the ability to explicitly represent relationships between concepts. Knowledge engineering, like compiling companies’ vast bodies of knowledge into these structures, however, still represents a time-consuming, largely manually performed process, esp. with significant amounts of knowledge often only recorded within unstructured text documents. Since the recently introduced Large Language Models (LLMs) excel on text summarization, this raises the question whether these could be exploited within dedicated knowledge fusion architectures to assist human knowledge engineers by automatically suggesting relevant classes, instances and relations extracted from textual corpora. We therefore propose a novel approach that leverages the taxonomic structure of a partially defined ontology to prompt LLMs for hierarchical knowledge organization. Unlike conventional methods that rely solely on static ontologies, our methodology dynamically generates prompts based on the ontology’s existing class taxonomy, prompting the LLM to generate responses that extract supplementary information from unstructured documents. It thus introduces the concept of using ontologies as scaffolds for guiding LLMs, in order to realize a mutual interplay between structured ontological knowledge and the soft fusion capabilities of LLMs. We evaluate our proposed algorithm on a real-world case study, performing a knowledge fusion task on heterogeneous technical documentation from a medical prosthesis manufacturer.},
  keywords={Knowledge engineering;Large language models;Taxonomy;Text summarization;Organizations;Manuals;Documentation;Ontologies;Manufacturing;Prosthetics;Large Language Models;Knowledge Engineering;Ontology Population;Soft Fusion;Natural Language Processing},
  doi={10.23919/FUSION59988.2024.10706469},
  ISSN={},
  month={July},}@ARTICLE{10713368,
  author={Lee, Jinkyu and Kim, Jihie},
  journal={IEEE Access}, 
  title={Improving Commonsense Bias Classification by Mitigating the Influence of Demographic Terms}, 
  year={2024},
  volume={12},
  number={},
  pages={161480-161489},
  abstract={Understanding commonsense knowledge is crucial in the field of Natural Language Processing (NLP). However, the presence of demographic terms in commonsense knowledge poses a potential risk of compromising the performance of NLP models. This study aims to investigate and propose methods for enhancing the performance and effectiveness of a commonsense polarization classifier by mitigating the influence of demographic terms. Three methods are introduced in this paper: (1) hierarchical generalization of demographic terms (2) threshold-based augmentation and (3) integration of hierarchical generalization and threshold-based augmentation methods(IHTA). The first method involves replacing demographic terms with more general ones based on a term hierarchy ontology, aiming to mitigate the influence of specific terms. To address the limited bias-related information, the second method measures the polarization of demographic terms by comparing the changes in the model’s predictions when these terms are masked versus unmasked. This method augments commonsense sentences containing terms with high polarization values by replacing their predicates with synonyms generated by ChatGPT. The third method combines the two approaches, starting with threshold-based augmentation followed by hierarchical generalization. The experiments show that the first method increases the accuracy over the baseline by 2.33%, and the second one by 0.96% over standard augmentation methods. The IHTA techniques yielded an 8.82% and 9.96% higher accuracy than threshold-based and standard augmentation methods, respectively.},
  keywords={Accuracy;Predictive models;Commonsense reasoning;Standards;Ontologies;Chatbots;Training data;Systematics;Semantics;Prevention and mitigation;Demography;Natural language processing;Classification algorithms;Commonsense bias;demographic term;bias mitigation;hierarchical generalization;threshold-based augmentation},
  doi={10.1109/ACCESS.2024.3477599},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10705189,
  author={Baghdadi, Sarra and Saleh, Majd and Paquelet, Stéphane},
  booktitle={2024 IEEE 12th International Conference on Intelligent Systems (IS)}, 
  title={TocBERT: Medical Document Structure Extraction Using Bidirectional Transformers}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Text segmentation holds paramount importance in the field of Natural Language Processing (NLP). It plays an important role in several NLP downstream tasks like information retrieval and document summarization. In this work, we propose a new solution, namely TocBERT, for segmenting texts using bidirectional transformers. TocBERT represents a supervised solution trained on the detection of titles and sub-titles from their semantic representations. This task was formulated as a named entity recognition (NER) problem. The solution has been applied on a medical text segmentation use-case where the Bio-ClinicalBERT model is fine-tuned to segment discharge summaries of the MIMIC-III dataset. The performance of TocBERT has been evaluated on a human-labeled ground truth corpus of 250 notes. It achieved an F1-score of 84.6% when evaluated on a linear text segmentation problem and 72.8 % on a hierarchical text segmentation problem. It outperformed a carefully designed rule-based solution, particularly in distinguishing titles from subtitles.},
  keywords={Biological system modeling;Semantics;MIMICs;Named entity recognition;Ontologies;Transformers;Information retrieval;Cleaning;Intelligent systems;Title detection;text segmentation;NLP;language models;transformers;BERT;information retrieval;medical text cleaning},
  doi={10.1109/IS61756.2024.10705189},
  ISSN={2767-9802},
  month={Aug},}@INPROCEEDINGS{10705235,
  author={Baddour, Moussa and Paquelet, Stéphane and Rollier, Paul and De Tayrac, Marie and Dameron, Olivier and Labbe, Thomas},
  booktitle={2024 IEEE 12th International Conference on Intelligent Systems (IS)}, 
  title={Phenotypes Extraction from Text: Analysis and Perspective in the LLM Era}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Collecting the relevant list of patient phenotypes, known as deep phenotyping, can significantly improve the final diagnosis. As textual clinical reports are the richest source of phenotypes information, their automatic extraction is a critical task. The main challenges of this Information Extraction (IE) task are to identify precisely the text spans related to a phenotype and to link them unequivocally to referenced entities from a source such as the Human Phenotype Ontology (HPO). Recently, Language Models (LMs) have been the most suc-cessful approach for extracting phenotypes from clinical reports. Solutions such as PhenoBERT, relying on BERT or GPT, have shown promising results when applied to datasets built on the hypothesis that most phenotypes are explicitly mentioned in the text. However, this assumption is not always true in medical genetics. Hence, although the LMs carry powerful semantic abilities, their contributions are not clear compared to syntactic string-matching steps that are used within the current pipelines. The goal of this study is to improve phenotype extraction from clinical notes related to genetic diseases. Our contributions are threefold: First, we provide a clear definition of the phenotype extraction task from free text, along with a high-level overview of the involved functions. Second, we conduct an in-depth analysis of PhenoBERT, one of the best existing solutions, to evaluate the proportion of phenotypes predicted with simple string-matching. Third, we demonstrate how utilizing and incorporating large language models (LLMs) for span detection step can improve performance especially with implicit phenotypes. In addition, this experiment revealed that the annotations of existing dataset are not exhaustive, and that LLM can identify relevant spans missed by human labelers.},
  keywords={Phenotypes;Annotations;Large language models;Semantics;Detectors;Syntactics;Ontologies;Data mining;Intelligent systems;Medical diagnostic imaging;phenotype;genetic;entity linking;phenoBERT;LLM;embed dings},
  doi={10.1109/IS61756.2024.10705235},
  ISSN={2767-9802},
  month={Aug},}@INPROCEEDINGS{10698993,
  author={Kumar, Amala Rashmi and Kumari, S. Meena and Rao, Tanvi and Shetty, Tavishi S},
  booktitle={2024 Second International Conference on Networks, Multimedia and Information Technology (NMITCON)}, 
  title={ReidLM: Fine-Tuning LLaMA3 using Evol-Instruct for Enhanced Contextual Accuracy in Rare Disease Research}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This study introduces ReidLM, a fine-tuned large language model (LLM) optimized for the rare disease domain. By generating a diverse and complex question-and-answer dataset using the EvolInstruct methodology, Meta’s LLaMA-3-8B-Instruct model was enhanced to deliver better performance across multiple evaluation metrics. ReidLM demonstrates significant improvements in generating contextually accurate responses for rare diseases, highlighting the potential of Evol-Instruct in specialized medical applications. Specifically, ReidLM achieved the highest ROUGE-1 (0.3281) and GEval (0.87) scores among the evaluated models, along with strong performances in METEOR (0.3662) and BERTScore (0.8782), indicating its effectiveness in producing semantically sound and relevant responses. These results offer promising advancements in medical research and patient care, with future work aimed at expanding datasets and validating clinical utility.},
  keywords={Measurement;Accuracy;Terminology;Multimedia systems;Large language models;Medical services;Meteors;Information technology;Medical diagnostic imaging;Diseases;fine-tuning;LLaMA 3;evol-instruct;rare diseases},
  doi={10.1109/NMITCON62075.2024.10698993},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10695412,
  author={Luo, Junjun and Zhu, Zhongyan and Zhu, Haijiang and Dong, Xiaohui},
  booktitle={2024 7th International Conference on Computer Information Science and Application Technology (CISAT)}, 
  title={Research on knowledge graph construction method for mine hoist fault field}, 
  year={2024},
  volume={},
  number={},
  pages={342-346},
  abstract={Mine hoists are integral to mining hoisting systems, with their safe and reliable operation being critical for ensuring the safety of mining operations. The consequences of hoist failure are severe, particularly when the root cause of the malfunction is not promptly identified and addressed, potentially compromising the overall safety of mining activities. The complexity of mine hoist systems stems from the interdependent and restrictive relationships among their components, each of which generates unique operational state information. This information, when aggregated and processed, can be distilled into various fault characteristic parameters. This paper introduces a novel approach to fault diagnosis within mine hoist systems by constructing a fault knowledge graph based on ontological principles. The proposed method harnesses the power of knowledge graphs to systematically represent and analyze the complex interplay of components within the hoist system. By doing so, it enhances the diagnostic capabilities and the preemptive identification of potential faults. The research focuses on the mine hoist as the subject of study and proposes the development of an ontologically-based fault knowledge graph. This approach is not only of significant importance to the coal mining industry but also offers innovative insights for knowledge graph construction across various domains. The implications of this study extend beyond the mining sector, providing a foundation for more robust and intelligent fault diagnosis systems in complex mechanical systems.},
  keywords={Fault diagnosis;Information science;Instruments;Knowledge graphs;Named entity recognition;Information retrieval;Safety;Complexity theory;Mechanical systems;Lifting equipment;artificial intelligence;entity recognition;fault knowledge graph;mine hoist;relation extraction},
  doi={10.1109/CISAT62382.2024.10695412},
  ISSN={},
  month={July},}@INPROCEEDINGS{10679346,
  author={Kougioumtzidou, Anna and Papoutsis, Angelos and Kavallieros, Dimitrios and Mavropoulos, Thanassis and Tsikrika, Theodora and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
  booktitle={2024 IEEE International Conference on Cyber Security and Resilience (CSR)}, 
  title={An End-to-End Framework for Cybersecurity Taxonomy and Ontology Generation and Updating}, 
  year={2024},
  volume={},
  number={},
  pages={247-254},
  abstract={Effective cyber-defense practices often require the use of structured knowledge representations, such as taxonomies and ontologies, to organise vast amounts of data and facili-tate knowledge representation and reasoning. To this end, we present an Artificial Intelligence (AI)-assisted framework for the construction and update of cybersecurity taxonomies and ontologies. The proposed framework can be divided into three main phases: Taxonomy Construction, Ontology Construction, and Taxonomy/Ontology Update, each phase consisting of both information extraction and semantic knowledge representation components. For information extraction, we employ a variety of techniques originating from Natural Language Processing (NLP), particularly Transformer Neural Networks. For constructing ontologies, we propose a conceptual ontology schema based on the STIX 2.1 standard for modeling information related to attacks, threats, and vulnerabilities, and use the Owlready2 Python library. Overall, our framework effectively builds cybersecurity taxonomies and ontologies and updates existing knowledge of both the generated and open-source taxonomies and ontologies.},
  keywords={Filtering;Semantic search;Taxonomy;Ontologies;Transformers;Natural language processing;Smart grids;ontologies;taxonomies;cybersecurity;attacks;vulnerabilities;dynamic update;large language models;natural language processing;artificial intelligence},
  doi={10.1109/CSR61664.2024.10679346},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10679399,
  author={Karalka, Christina and Meditskos, Georgios and Papoutsoglou, Maria and Bassiliades, Nick},
  booktitle={2024 IEEE International Conference on Cyber Security and Resilience (CSR)}, 
  title={Towards a Generic Knowledge Graph Construction Framework for Privacy Awareness}, 
  year={2024},
  volume={},
  number={},
  pages={700-705},
  abstract={Knowledge graphs (KGs) organize data from multi-ple sources, capturing information about entities of interest in a given domain or task, such as people, places, or events, and forge connections between them. In this paper, we introduce a generic framework for building knowledge graphs designed to enhance data privacy through semantic interpretation. We demonstrate the effectiveness of our framework by applying it to the healthcare sector, where it helps organize and analyze com-plex information, support data analysis, improve decision-making processes, and uncover hidden relationships between entities. Our approach leverages domain-specific ontologies like SNOMED CT and integrates vector databases to assess and mitigate privacy risks. By using semantic techniques, we enhance the robustness of data against reidentification attacks and suggest appropriate de-identification methods. The integration of SNOMED with vector databases allows for efficient storage, retrieval, and analysis of high-dimensional healthcare data, facilitating advanced data an-alytics and knowledge discovery while maintaining data privacy. Through this framework, we aim to provide sufficient insights for identifying privacy vulnerabilities and ensuring the security and usability of sensitive health information.},
  keywords={Data privacy;Privacy;Databases;Semantics;Knowledge graphs;Medical services;Vectors},
  doi={10.1109/CSR61664.2024.10679399},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10677287,
  author={Saini, Shashwat and Vrindavanam, Jayavrinda and Mondal, Subhash},
  booktitle={2024 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)}, 
  title={Methodological Insights Into Protein Clustering Using BERT & RoBERTa}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Proteins are present in all living organisms, and understanding their processes is vital. Protein databases such as SWISS-PROT include curated information on only 570,000 protein sequences, representing a fraction of the 250 million known evidential and predicted sequences; it becomes crucial to cluster proteins into similar groups. This research explores the application of two transformer architectures, BERT and RoBERTa in clustering proteins in the supervised prediction of Gene Ontology (GO) annotations. The detailed methodology for both the pre-training and fine-tuning processes, as well as results that showcase RoBERTa outperforming BERT in the context of protein clustering, on performance metrics of accuracy and loss. Operating under constrained computational resources, the deployed model exhibits strong performance and highlight the robustness of methodology in protein clustering within resource constraints. This study not only contributes to the understanding of protein clustering but also signifies the potential of transformer models to handle biological data.},
  keywords={Proteins;Training;Analytical models;Annotations;Biological system modeling;Computational modeling;Ontologies;Protein Clustering;BERT;RoBERTa;Natural Language Processing;Transformers;Masked Language Modelling},
  doi={10.1109/CONECCT62155.2024.10677287},
  ISSN={2766-2101},
  month={July},}

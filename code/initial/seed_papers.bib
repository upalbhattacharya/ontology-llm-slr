@Comment{
ebib-main-file: /home/upal/References/bibliography.bib
}


@InProceedings{zeginis2024ApplyingOntologyAware,
	keywords = {Prompt Format},
	file = {References/pdf/zeginis2024ApplyingOntologyAware.pdf},
	author = {Dimitris Zeginis and Evangelos Kalampokis and
                  Konstantinos A. Tarabanis},
	title = {Applying an ontology-aware zero-shot {LLM} prompting
                  approach for information extraction in Greek: the
                  case of {DIAVGEIA} gov gr},
	year = 2024,
	booktitle = {Proceedings of the 28th Pan-Hellenic Conference on
                  Progress in Computing and Informatics, {PCI} 2024,
                  AthensGreece, December 13-15, 2024},
	pages = {324-330},
	doi = {10.1145/3716554.3716603},
	url = {https://doi.org/10.1145/3716554.3716603},
	crossref = {DBLP:conf/pci/2024},
	timestamp = {Wed, 11 Jun 2025 21:00:16 +0200},
	biburl = {https://dblp.org/rec/conf/pci/ZeginisKT24.bib},
	bibsource = {dblp computer science bibliography,
                  https://dblp.org}
}

@InProceedings{tsaneva2024LlmDrivenOntology,
	file = {References/pdf/tsaneva2024LlmDrivenOntology.pdf},
	author = {Stefani Tsaneva and Stefan Vasic and Marta Sabou},
	title = {LLM-driven Ontology Evaluation: Verifying Ontology
                  Restrictions with ChatGPT},
	year = 2024,
	booktitle = {Joint proceedings of the 3rd International workshop
                  on knowledge graph generation from text {(TEXT2KG)}
                  and Data Quality meets Machine Learning and
                  Knowledge Graphs {(DQMLKG)} co-located with the
                  Extended Semantic Web Conference {(} {ESWC} 2024),
                  Hersonissos, Greece, May 26-30, 2024},
	pages = 15,
	url = {https://ceur-ws.org/Vol-3747/dqmlkg\_paper3.pdf},
	crossref = {DBLP:conf/text2kg/2024},
	timestamp = {Thu, 31 Oct 2024 17:18:55 +0100},
	biburl = {https://dblp.org/rec/conf/text2kg/TsanevaVS24.bib},
	bibsource = {dblp computer science bibliography,
                  https://dblp.org}
}

@InProceedings{jain2022DistillingHypernymyRelations,
	keywords = {taxonomy discovery},
	file = {References/pdf/jain2022DistillingHypernymyRelations.pdf},
	author = {Devansh Jain and Luis Espinosa Anke},
	title = {Distilling Hypernymy Relations from Language Models:
                  On the Effectiveness of Zero-Shot Taxonomy
                  Induction},
	year = 2022,
	booktitle = {Proceedings of the 11th Joint Conference on Lexical
                  and Computational Semantics, *SEM@NAACL-HLT 2022,
                  Seattle, WA, USA, July 14-15, 2022},
	pages = {151-156},
	doi = {10.18653/V1/2022.STARSEM-1.13},
	url = {https://doi.org/10.18653/v1/2022.starsem-1.13},
	crossref = {DBLP:conf/starsem/2022},
	timestamp = {Thu, 22 Aug 2024 07:28:05 +0200},
	biburl = {https://dblp.org/rec/conf/starsem/JainA22.bib},
	bibsource = {dblp computer science bibliography,
                  https://dblp.org}
}

@InProceedings{garijo2024LlmsOntologyEngineering,
	keywords = {review, ontology enrichment},
	file = {References/pdf/garijo2024LlmsOntologyEngineering.pdf},
	author = {Daniel Garijo and Mar{\'{\i}}a
                  Poveda{-}Villal{\'{o}}n and Elvira
                  Amador{-}Dom{\'{\i}}nguez and Ziyuan Wang and
                  Ra{\'{u}}l Garc{\'{\i}}a{-}Castro and {\'{O}}scar
                  Corcho},
	title = {LLMs for Ontology Engineering: {A} landscape of
                  Tasks and Benchmarking challenges},
	year = 2024,
	booktitle = {Proceedings of the Special Session on Harmonising
                  Generative {AI} and Semantic Web Technologies
                  {(HGAIS} 2024) co-located with the 23rd
                  International Semantic Web Conference {(ISWC} 2024),
                  Baltimore, Maryland, November 13, 2024},
	url = {https://ceur-ws.org/Vol-3953/364.pdf},
	crossref = {DBLP:conf/semweb/2024hgais},
	timestamp = {Thu, 12 Jun 2025 16:38:15 +0200},
	biburl = {https://dblp.org/rec/conf/semweb/GarijoPAWGC24.bib},
	bibsource = {dblp computer science bibliography,
                  https://dblp.org}
}

@InProceedings{fathallah2024NeonGptLarge,
	file = {References/pdf/fathallah2024NeonGptLarge.pdf},
	author = {Nadeen Fathallah and Arunav Das and Stefano De
                  Giorgis and Andrea Poltronieri and Peter Haase and
                  Liubov Kovriguina},
	title = {NeOn-GPT: {A} Large Language Model-Powered Pipeline
                  for Ontology Learning},
	year = 2024,
	booktitle = {The Semantic Web: {ESWC} 2024 Satellite Events -
                  Hersonissos, Crete, Greece, May 26-30, 2024,
                  Proceedings, Part {I}},
	pages = {36-50},
	doi = {10.1007/978-3-031-78952-6\_4},
	url = {https://doi.org/10.1007/978-3-031-78952-6\_4},
	crossref = {DBLP:conf/esws/2024s-1},
	timestamp = {Wed, 19 Feb 2025 12:54:38 +0100},
	biburl = {https://dblp.org/rec/conf/esws/FathallahDGPHK24.bib},
	bibsource = {dblp computer science bibliography,
                  https://dblp.org}
}

@InProceedings{bakker2024OntologyLearningText,
	file = {References/pdf/bakker2024OntologyLearningText.pdf},
	author = {Roos M. Bakker and Daan L. Di Scala and Maaike
                  H. T. de Boer},
	title = {Ontology Learning from Text: an Analysis on {LLM}
                  Performance},
	year = 2024,
	booktitle = {Proceedings of the 3rd International Workshop on
                  Natural Language Processing for Knowledge Graph
                  Creation co-located with 20th International
                  Conference on Semantic Systems (SEMANTiCS 2024),
                  Amsterdam, The Netherlands, September 17, 2024},
	pages = {70-87},
	url = {https://ceur-ws.org/Vol-3874/paper5.pdf},
	crossref = {DBLP:conf/nlp4kgc/2024},
	timestamp = {Mon, 06 Jan 2025 16:59:41 +0100},
	biburl = {https://dblp.org/rec/conf/nlp4kgc/BakkerSB24.bib},
	bibsource = {dblp computer science bibliography,
                  https://dblp.org}
}

@InProceedings{babaei2023Llms4olLargeLanguage,
	title = {{{LLMs4OL}}: {{Large Language Models}} for {{Ontology Learning}}},
	shorttitle = {{{LLMs4OL}}},
	booktitle = {The {{Semantic Web}} – {{ISWC}} 2023},
	author = {Babaei Giglou, Hamed and D'Souza, Jennifer and Auer, Sören},
	editor = {Payne, Terry R. and Presutti, Valentina and Qi, Guilin and Poveda-Villalón, María and Stoilos, Giorgos and Hollink, Laura and Kaoudi, Zoi and Cheng, Gong and Li, Juanzi},
	date = {2023},
	pages = {408--427},
	publisher = {Springer Nature Switzerland},
	location = {Cham},
	doi = {10.1007/978-3-031-47240-4_22},
	abstract = {We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text? To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.},
	isbn = {978-3-031-47240-4},
	langid = {english},
	keywords = {Important,Large Language Models,LLMs,Ontologies,Ontology Learning,Prompt-based Learning,Prompting, taxonomy discovery},
	timestamp = {2024-09-12T12:01:26Z},
	file = {References/pdf/babaei2023Llms4olLargeLanguage.pdf}
}

@InBook{babaei2025Llms4omMatchingOntologies,
	file = {References/pdf/babaei2025Llms4omMatchingOntologies.pdf},
	title = {LLMs4OM: Matching Ontologies with Large Language
                  Models},
	year = 2025,
	author = {Babaei Giglou, Hamed and D’Souza, Jennifer and
                  Engel, Felix and Auer, Sören},
	booktitle = {The Semantic Web: ESWC 2024 Satellite Events},
	publisher = {Springer Nature Switzerland},
	isbn = 9783031789526,
	pages = {25–35},
	doi = {10.1007/978-3-031-78952-6_3},
	url = {http://dx.doi.org/10.1007/978-3-031-78952-6_3},
	ISSN = {1611-3349}
}

@Article{chen2023ContextualSemanticEmbeddings,
	file = {References/pdf/chen2023ContextualSemanticEmbeddings.pdf},
	title = {Contextual Semantic Embeddings for Ontology Subsumption Prediction},
	author = {Chen, Jiaoyan and He, Yuan and Geng, Yuxia and Jiménez-Ruiz, Ernesto and Dong, Hang and Horrocks, Ian},
	date = {2023-09},
	journaltitle = {World Wide Web-internet and Web Information Systems},
	shortjournal = {World Wide Web},
	volume = {26},
	number = {5},
	pages = {2569--2591},
	issn = {1573-1413},
	doi = {10.1007/s11280-023-01169-9},
	abstract = {Automating ontology construction and curation is an important but challenging task in knowledge engineering and artificial intelligence. Prediction by machine learning techniques such as contextual semantic embedding is a promising direction, but the relevant research is still preliminary especially for expressive ontologies in Web Ontology Language (OWL). In this paper, we present a new subsumption prediction method named BERTSubs for classes of OWL ontology. It exploits the pre-trained language model BERT to compute contextual embeddings of a class, where customized templates are proposed to incorporate the class context (e.g., neighbouring classes) and the logical existential restriction. BERTSubs is able to predict multiple kinds of subsumers including named classes from the same ontology or another ontology, and existential restrictions from the same ontology. Extensive evaluation on five real-world ontologies for three different subsumption tasks has shown the effectiveness of the templates and that BERTSubs can dramatically outperform the baselines that use (literal-aware) knowledge graph embeddings, non-contextual word embeddings and the state-of-the-art OWL ontology embeddings.},
	langid = {english},
	keywords = {Artificial Intelligence,BERT,Ontology alignment,Ontology embedding,OWL,Pre-trained language model,Subsumption prediction},
	timestamp = {2024-08-15T09:49:42Z}
}

@Online{chen2023PromptingOrFine,
	keywords = {taxonomy discovery, review},
	file = {References/pdf/chen2023PromptingOrFine.pdf},
	author = {Boqi Chen AND Fandi Yi AND Dániel Varró},
	title = {{Prompting or Fine-tuning? A Comparative Study of
                  Large Language Models for Taxonomy Construction}},
	year = 2023,
	eprint = {2309.01715v1},
	primaryclass = {cs.CL},
	archiveprefix = {arXiv}
}

@InProceedings{dong2024LanguageModelBased,
	file = {References/pdf/dong2024LanguageModelBased.pdf},
	title = {A {{Language Model Based Framework}} for {{New Concept Placement}} in {{Ontologies}}},
	booktitle = {The {{Semantic Web}}},
	author = {Dong, Hang and Chen, Jiaoyan and He, Yuan and Gao, Yongsheng and Horrocks, Ian},
	editor = {Meroño Peñuela, Albert and Dimou, Anastasia and Troncy, Raphaël and Hartig, Olaf and Acosta, Maribel and Alam, Mehwish and Paulheim, Heiko and Lisena, Pasquale},
	date = {2024},
	pages = {79--99},
	publisher = {Springer Nature Switzerland},
	location = {Cham},
	doi = {10.1007/978-3-031-60626-7_5},
	abstract = {We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark. The best settings in our framework use fine-tuned PLM for search and a multi-label Cross-encoder for selection. Zero-shot prompting of LLMs is still not adequate for the task, and we propose explainable instruction tuning of LLMs for improved performance. Our study shows the advantages of PLMs and highlights the encouraging performance of LLMs that motivates future studies.},
	isbn = {978-3-031-60626-7},
	langid = {english},
	keywords = {Concept Placement,Large Language Models,Ontology Enrichment,Pre-trained Language Models,SNOMED CT},
	timestamp = {2024-08-15T09:20:01Z}
}

@InProceedings{doumanas2024IntegratingLlmsIn,
	file = {References/pdf/doumanas2024IntegratingLlmsIn.pdf},
	title = {Integrating {{LLMs}} in the {{Engineering}} of a {{SAR Ontology}}},
	booktitle = {Artificial {{Intelligence Applications}} and {{Innovations}}},
	author = {Doumanas, Dimitrios and Soularidis, Andreas and Kotis, Konstantinos and Vouros, George},
	editor = {Maglogiannis, Ilias and Iliadis, Lazaros and Macintyre, John and Avlonitis, Markos and Papaleonidas, Antonios},
	date = {2024},
	pages = {360--374},
	publisher = {Springer Nature Switzerland},
	location = {Cham},
	doi = {10.1007/978-3-031-63223-5_27},
	abstract = {In Search and Rescue (SAR) missions, the integration of multiple sources of information may enhance operational efficiency and increase responsiveness significantly, improving situation awareness and aiding decision-making to save lives and mitigate incident impact. Ontologies are crucial for integrating and reasoning with data from diverse sources. Engineering a domain ontology for SAR can be better supported from an agile, collaborative, and iterative ontology engineering methodology (OEM), incorporating the interests of several stakeholders. Large Language Models (LLMs) can play a significant role in completing OEM processes. The goal of this work is to identify how ontology engineering (OE) tasks can be completed with the collaboration of LLMs and humans. The objectives of this paper are, a) to present preliminary exploration of LLMs to generate domain ontologies for the modeling of SAR missions in wildfire incidents b) to propose and evaluate an LLM-enhanced OE approach. In overall, the main contribution of the work presented in this paper is the analysis of LLMs capabilities to ontology engineering, and the evaluation of the synergy between humans and machines to efficiently represent knowledge, with specific focus in the SAR domain.},
	isbn = {978-3-031-63223-5},
	langid = {english},
	keywords = {Large Language Models,Ontology Engineering,Search and Rescue},
	timestamp = {2024-09-12T09:47:07Z}
}

@Online{du2024ShortReviewOntology,
	keywords = {ontology, llm, review},
	file = {References/pdf/du2024ShortReviewOntology.pdf},
	author = {Rick Du AND Huilong An AND Keyu Wang AND Weidong
                  Liu},
	title = {{A Short Review for Ontology Learning: Stride to
                  Large Language Models Trend}},
	year = 2024,
	eprint = {2404.14991v2},
	primaryclass = {cs.IR},
	archiveprefix = {arXiv}
}

@InProceedings{funk2023TowardsOntologyConstruction,
	file = {References/pdf/funk2023TowardsOntologyConstruction.pdf},
	title = {Towards {{Ontology Construction}} with {{Language Models}}},
	booktitle = {{{KBC-LM}} @ {{ISWC}} 2023},
	author = {Funk, Maurice and Hosemann, Simon and Jung, Jean Christoph and Lutz, Carsten},
	date = {2023},
	doi = {10.48550/arXiv.2309.09898},
	abstract = {An academic search engine that utilizes artificial intelligence methods to provide highly relevant results and novel tools to filter them with ease.},
	langid = {english},
	timestamp = {2024-07-25T09:10:54Z}
}

@InProceedings{he2023ExploringLargeLanguage,
	file = {References/pdf/he2023ExploringLargeLanguage.pdf},
	title = {Exploring Large Language Models for Ontology Alignment},
	booktitle = {Proceedings of the {{ISWC}} 2023 Posters, Demos and Industry Tracks: {{From}} Novel Ideas to Industrial Practice Co-Located with 22nd International Semantic Web Conference ({{ISWC}} 2023), Athens, Greece, November 6-10, 2023},
	author = {He, Yuan and Chen, Jiaoyan and Dong, Hang and Horrocks, Ian},
	editor = {Fundulaki, Irini and Kozaki, Kouji and Garijo, Daniel and Gómez-Pérez, José Manuél},
	date = {2023},
	series = {{{CEUR}} Workshop Proceedings},
	volume = {3632},
	publisher = {CEUR-WS.org},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	timestamp = {Mon, 03 Jun 2024 15:23:14 +0200}
}

@InProceedings{he2023LanguageModelAnalysis,
	file = {References/pdf/he2023LanguageModelAnalysis.pdf},
	title = {Language {{Model Analysis}} for {{Ontology Subsumption Inference}}},
	booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2023},
	author = {He, Yuan and Chen, Jiaoyan and Jimenez-Ruiz, Ernesto and Dong, Hang and Horrocks, Ian},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	date = {2023-07},
	pages = {3439--3453},
	publisher = {Association for Computational Linguistics},
	location = {Toronto, Canada},
	doi = {10.18653/v1/2023.findings-acl.213},
	abstract = {Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets.},
	timestamp = {2024-08-15T09:50:24Z}
}

@Online{kommineni2024HumanExpertsMachines,
	file = {References/pdf/kommineni2024HumanExpertsMachines.pdf},
	author = {Vamsi Krishna Kommineni AND Birgitta König-Ries AND
                  Sheeba Samuel},
	title = {{From human experts to machines: An LLM supported
                  approach to ontology and knowledge graph
                  construction}},
	year = 2024,
	eprint = {2403.08345v1},
	primaryclass = {cs.CL},
	archiveprefix = {arXiv}
}

@Article{li2025ExploringImpactTemperature,
	keywords = {temperature variation},
	file = {References/pdf/li2025ExploringImpactTemperature.pdf},
	author = {Lujun Li and Lama Sleem and Niccolo Gentile and
                  Geoffrey Nichil and Radu State},
	title = {Exploring the Impact of Temperature on Large
                  Language Models:Hot or Cold?},
	journal = {CoRR},
	year = 2025,
	volume = {abs/2506.07295},
	doi = {10.48550/ARXIV.2506.07295},
	eprint = {2506.07295},
	eprinttype = {arXiv},
	url = {https://doi.org/10.48550/arXiv.2506.07295},
	timestamp = {Mon, 07 Jul 2025 21:56:42 +0200},
	biburl = {https://dblp.org/rec/journals/corr/abs-2506-07295.bib},
	bibsource = {dblp computer science bibliography,
                  https://dblp.org}
}

@Online{lippolis2025OntologyGenerationUsing,
	keywords = {Ontology Generation, Ontology Learning, Ontology Engineering, Prompting},
	file = {References/pdf/lippolis2025OntologyGenerationUsing.pdf},
	author = {Anna Sofia Lippolis AND Mohammad Javad Saeedizade
                  AND Robin Keskisärkkä AND Sara Zuppiroli AND Miguel
                  Ceriani AND Aldo Gangemi AND Eva Blomqvist AND
                  Andrea Giovanni Nuzzolese},
	title = {{Ontology Generation using Large Language Models}},
	year = 2025,
	eprint = {2503.05388v1},
	primaryclass = {cs.AI},
	archiveprefix = {arXiv}
}

@Online{lo2024EndEndOntology,
	keywords = {Ontology Learning},
	file = {References/pdf/lo2024EndEndOntology.pdf},
	author = {Andy Lo AND Albert Q. Jiang AND Wenda Li AND Mateja
                  Jamnik},
	title = {{End-to-End Ontology Learning with Large Language
                  Models}},
	year = 2024,
	eprint = {2410.23584v1},
	primaryclass = {cs.LG},
	archiveprefix = {arXiv}
}

@InBook{mai2024DoLlmsReally,
	file = {References/pdf/mai2024DoLlmsReally.pdf},
	title = {Do LLMs Really Adapt to Domains? An Ontology
                  Learning Perspective},
	year = 2024,
	author = {Mai, Huu Tan and Chu, Cuong Xuan and Paulheim,
                  Heiko},
	booktitle = {The Semantic Web – ISWC 2024},
	publisher = {Springer Nature Switzerland},
	isbn = 9783031778445,
	pages = {126–143},
	doi = {10.1007/978-3-031-77844-5_7},
	url = {http://dx.doi.org/10.1007/978-3-031-77844-5_7},
	ISSN = {1611-3349},
	month = nov
}

@Article{mateiu2023OntologyEngineeringWith,
	file = {References/pdf/mateiu2023OntologyEngineeringWith.pdf},
	title = {Ontology Engineering with {{Large Language Models}}},
	author = {Mateiu, Patricia and Groza, Adrian},
	date = {2023-09},
	journaltitle = {2023 25th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)},
	pages = {226--229},
	publisher = {IEEE},
	location = {Nancy, France},
	doi = {10.1109/SYNASC61333.2023.00038},
	abstract = {We tackle the task of enriching ontologies by automatically translating natural language (NL) into Description Logic (DL). Since Large Language Models (LLMs) are the best tools for translations, we fine-tuned a GPT-3 model to convert NL into OWL Functional Syntax. For fine-tuning, we designed pairs of sentences in NL and the corresponding translations. This training pairs cover various aspects from ontology engineering: instances, class subsumption, domain and range of relations, object properties relationships, disjoint classes, complements, or cardinality restrictions. The resulted axioms are used to enrich an ontology, in a human supervised manner. The developed tool is publicly provided as a Protégé plugin.},
	isbn = {9798350394122},
	timestamp = {2024-08-15T09:04:33Z}
}

@Online{norouzi2024OntologyPopulationUsing,
	file = {References/pdf/norouzi2024OntologyPopulationUsing.pdf},
	author = {Sanaz Saki Norouzi AND Adrita Barua AND Antrea
                  Christou AND Nikita Gautam AND Andrew Eells AND
                  Pascal Hitzler AND Cogan Shimizu},
	title = {{Ontology Population using LLMs}},
	year = 2024,
	eprint = {2411.01612v1},
	primaryclass = {cs.AI},
	archiveprefix = {arXiv}
}

@Article{snijder2024AdvancingOntologyAlignment,
	file = {References/pdf/snijder2024AdvancingOntologyAlignment.pdf},
	title = {Advancing {{Ontology Alignment}} in the {{Labor Market}}: {{Combining Large Language Models}} with {{Domain Knowledge}}},
	shorttitle = {Advancing {{Ontology Alignment}} in the {{Labor Market}}},
	author = {Snijder, Lucas L. and Smit, Quirine T. S. and family=Boer, given=Maaike H. T., prefix=de, useprefix=true},
	date = {2024-05},
	journaltitle = {Proceedings of the AAAI Symposium Series},
	volume = {3},
	number = {1},
	pages = {253--262},
	issn = {2994-4317},
	doi = {10.1609/aaaiss.v3i1.31208},
	abstract = {One of the approaches to help the demand and supply problem in the labor market domain is to change from degree-based hiring to skill-based hiring. The link between occupations, degrees and skills is captured in domain ontologies such as ESCO in Europe and O*NET in the US. Several countries are also building or extending these ontologies. The alignment of the ontologies is important, as it should be clear how they all relate. Aligning two ontologies by creating a mapping between them is a tedious task to do manually, and with the rise of generative large language models like GPT-4, we explore how language models and domain knowledge can be combined in the matching of the instances in the ontologies and in finding the specific relation between the instances (mapping refinement). We specifically focus on the process of updating a mapping, but the methods could also be used to create a first-time mapping. We compare the performance of several state-of-the-art methods such as GPT-4 and fine-tuned BERT models on the mapping between ESCO and O*NET and ESCO and CompetentNL (the Dutch variant) for both ontology matching and mapping refinement. Our findings indicate that: 1) Match-BERT-GPT, an integration of BERT and GPT, performs best in ontology matching, while 2) TaSeR outperforms GPT-4, albeit marginally, in the task of mapping refinement. These results show that domain knowledge is still important in ontology alignment, especially in the updating of a mapping in our use cases in the labor domain.},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	langid = {english},
	keywords = {O*NET},
	timestamp = {2024-09-16T11:59:33Z}
}

@InProceedings{yang2024LlmSupportedApproach,
	file = {References/pdf/yang2024LlmSupportedApproach.pdf},
	author = {Yang, Hang and Xiao, Liang and Zhu, Rujun and Liu,
                  Ziji and Chen, Jianxia},
	title = {An LLM supported approach to ontology and knowledge
                  graph construction},
	year = 2024,
	booktitle = {2024 IEEE International Conference on Bioinformatics
                  and Biomedicine (BIBM)},
	publisher = {IEEE},
	month = dec,
	pages = {5240–5246},
	doi = {10.1109/bibm62325.2024.10822222},
	url = {http://dx.doi.org/10.1109/BIBM62325.2024.10822222}
}


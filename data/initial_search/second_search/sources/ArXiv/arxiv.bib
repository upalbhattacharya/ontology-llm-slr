@article{2411.09601v1,
Author        = {Cogan Shimizu and Pascal Hitzler},
Title         = {Accelerating Knowledge Graph and Ontology Engineering with Large
  Language Models},
Eprint        = {2411.09601v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models bear the promise of significant acceleration of key
Knowledge Graph and Ontology Engineering tasks, including ontology modeling,
extension, modification, population, alignment, as well as entity
disambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering
as a new and coming area of research, and argue that modular approaches to
ontologies will be of central importance.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.09601v1},
File          = {2411.09601v1.pdf}
}
@article{2407.17657v2,
Author        = {Carter Benson and Alec Sculley and Austin Liebers and John Beverley},
Title         = {My Ontologist: Evaluating BFO-Based AI for Definition Support},
Eprint        = {2407.17657v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Generative artificial intelligence (AI), exemplified by the release of
GPT-3.5 in 2022, has significantly advanced the potential applications of large
language models (LLMs), including in the realms of ontology development and
knowledge graph creation. Ontologies, which are structured frameworks for
organizing information, and knowledge graphs, which combine ontologies with
actual data, are essential for enabling interoperability and automated
reasoning. However, current research has largely overlooked the generation of
ontologies extending from established upper-level frameworks like the Basic
Formal Ontology (BFO), risking the creation of non-integrable ontology silos.
This study explores the extent to which LLMs, particularly GPT-4, can support
ontologists trained in BFO. Through iterative development of a specialized GPT
model named "My Ontologist," we aimed to generate BFO-conformant ontologies.
Initial versions faced challenges in maintaining definition conventions and
leveraging foundational texts effectively. My Ontologist 3.0 showed promise by
adhering to structured rules and modular ontology suites, yet the release of
GPT-4o disrupted this progress by altering the model's behavior. Our findings
underscore the importance of aligning LLM-generated ontologies with top-level
standards and highlight the complexities of integrating evolving AI
capabilities in ontology engineering.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.17657v2},
File          = {2407.17657v2.pdf}
}
@article{2312.00353v1,
Author        = {Pei-Chi Lo and Yi-Hang Tsai and Ee-Peng Lim and San-Yih Hwang},
Title         = {On Exploring the Reasoning Capability of Large Language Models with
  Knowledge Graphs},
Eprint        = {2312.00353v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper examines the capacity of LLMs to reason with knowledge graphs
using their internal knowledge graph, i.e., the knowledge graph they learned
during pre-training. Two research questions are formulated to investigate the
accuracy of LLMs in recalling information from pre-training knowledge graphs
and their ability to infer knowledge graph relations from context. To address
these questions, we employ LLMs to perform four distinct knowledge graph
reasoning tasks. Furthermore, we identify two types of hallucinations that may
occur during knowledge reasoning with LLMs: content and ontology hallucination.
Our experimental results demonstrate that LLMs can successfully tackle both
simple and complex knowledge graph reasoning tasks from their own memory, as
well as infer from input context.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.00353v1},
File          = {2312.00353v1.pdf}
}
@article{2412.15256v1,
Author        = {Edward Kim and Manil Shrestha and Richard Foty and Tom DeLay and Vicki Seyfert-Margolis},
Title         = {Structured Extraction of Real World Medical Knowledge using LLMs for
  Summarization and Search},
Eprint        = {2412.15256v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Creation and curation of knowledge graphs can accelerate disease discovery
and analysis in real-world data. While disease ontologies aid in biological
data annotation, codified categories (SNOMED-CT, ICD10, CPT) may not capture
patient condition nuances or rare diseases. Multiple disease definitions across
data sources complicate ontology mapping and disease clustering. We propose
creating patient knowledge graphs using large language model extraction
techniques, allowing data extraction via natural language rather than rigid
ontological hierarchies. Our method maps to existing ontologies (MeSH,
SNOMED-CT, RxNORM, HPO) to ground extracted entities.
  Using a large ambulatory care EHR database with 33.6M patients, we
demonstrate our method through the patient search for Dravet syndrome, which
received ICD10 recognition in October 2020. We describe our construction of
patient-specific knowledge graphs and symptom-based patient searches. Using
confirmed Dravet syndrome ICD10 codes as ground truth, we employ LLM-based
entity extraction to characterize patients in grounded ontologies. We then
apply this method to identify Beta-propeller protein-associated
neurodegeneration (BPAN) patients, demonstrating real-world discovery where no
ground truth exists.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.15256v1},
File          = {2412.15256v1.pdf}
}
@article{2405.19877v1,
Author        = {Arto Bendiken},
Title         = {KNOW: A Real-World Ontology for Knowledge Capture with Large Language
  Models},
Eprint        = {2405.19877v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We present KNOW--the Knowledge Navigator Ontology for the World--the first
ontology designed to capture everyday knowledge to augment large language
models (LLMs) in real-world generative AI use cases such as personal AI
assistants. Our domain is human life, both its everyday concerns and its major
milestones. We have limited the initial scope of the modeled concepts to only
established human universals: spacetime (places, events) plus social (people,
groups, organizations). The inclusion criteria for modeled concepts are
pragmatic, beginning with universality and utility. We compare and contrast
previous work such as Schema.org and Cyc--as well as attempts at a synthesis of
knowledge graphs and language models--noting how LLMs already encode internally
much of the commonsense tacit knowledge that took decades to capture in the Cyc
project. We also make available code-generated software libraries for the 12
most popular programming languages, enabling the direct use of ontology
concepts in software engineering. We emphasize simplicity and developer
experience in promoting AI interoperability.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.19877v1},
File          = {2405.19877v1.pdf}
}
@article{2311.07509v1,
Author        = {Juan Sequeda and Dean Allemang and Bryon Jacob},
Title         = {A Benchmark to Understand the Role of Knowledge Graphs on Large Language
  Model's Accuracy for Question Answering on Enterprise SQL Databases},
Eprint        = {2311.07509v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Enterprise applications of Large Language Models (LLMs) hold promise for
question answering on enterprise SQL databases. However, the extent to which
LLMs can accurately respond to enterprise questions in such databases remains
unclear, given the absence of suitable Text-to-SQL benchmarks tailored to
enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to
enhance LLM-based question answering by providing business context is not well
understood. This study aims to evaluate the accuracy of LLM-powered question
answering systems in the context of enterprise questions and SQL databases,
while also exploring the role of knowledge graphs in improving accuracy. To
achieve this, we introduce a benchmark comprising an enterprise SQL schema in
the insurance domain, a range of enterprise queries encompassing reporting to
metrics, and a contextual layer incorporating an ontology and mappings that
define a knowledge graph. Our primary finding reveals that question answering
using GPT-4, with zero-shot prompts directly on SQL databases, achieves an
accuracy of 16%. Notably, this accuracy increases to 54% when questions are
posed over a Knowledge Graph representation of the enterprise SQL database.
Therefore, investing in Knowledge Graph provides higher accuracy for LLM
powered question answering systems.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.07509v1},
File          = {2311.07509v1.pdf}
}
@article{2405.11706v1,
Author        = {Dean Allemang and Juan Sequeda},
Title         = {Increasing the LLM Accuracy for Question Answering: Ontologies to the
  Rescue!},
Eprint        = {2405.11706v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {There is increasing evidence that question-answering (QA) systems with Large
Language Models (LLMs), which employ a knowledge graph/semantic representation
of an enterprise SQL database (i.e. Text-to-SPARQL), achieve higher accuracy
compared to systems that answer questions directly on SQL databases (i.e.
Text-to-SQL). Our previous benchmark research showed that by using a knowledge
graph, the accuracy improved from 16% to 54%. The question remains: how can we
further improve the accuracy and reduce the error rate? Building on the
observations of our previous research where the inaccurate LLM-generated SPARQL
queries followed incorrect paths, we present an approach that consists of 1)
Ontology-based Query Check (OBQC): detects errors by leveraging the ontology of
the knowledge graph to check if the LLM-generated SPARQL query matches the
semantic of ontology and 2) LLM Repair: use the error explanations with an LLM
to repair the SPARQL query. Using the chat with the data benchmark, our primary
finding is that our approach increases the overall accuracy to 72% including an
additional 8% of "I don't know" unknown results. Thus, the overall error rate
is 20%. These results provide further evidence that investing knowledge graphs,
namely the ontology, provides higher accuracy for LLM powered question
answering systems.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.11706v1},
File          = {2405.11706v1.pdf}
}
@article{2501.14579v1,
Author        = {Alexander V. Belikov and Sacha Raoult},
Title         = {Knowledge Graphs Construction from Criminal Court Appeals: Insights from
  the French Cassation Court},
Eprint        = {2501.14579v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Despite growing interest, accurately and reliably representing unstructured
data, such as court decisions, in a structured form, remains a challenge.
Recent advancements in generative AI applied to language modeling enabled the
transformation of text into knowledge graphs, unlocking new opportunities for
analysis and modeling. This paper presents a framework for constructing
knowledge graphs from appeals to the French Cassation Court. The framework
includes a domain-specific ontology and a derived dataset, offering a
foundation for structured legal data representation and analysis.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.14579v1},
File          = {2501.14579v1.pdf}
}
@article{2411.11090v1,
Author        = {Jingyun Sun and Zhongze Luo},
Title         = {ForPKG-1.0: A Framework for Constructing Forestry Policy Knowledge Graph
  and Application Analysis},
Eprint        = {2411.11090v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {A policy knowledge graph can provide decision support for tasks such as
project compliance, policy analysis, and intelligent question answering, and
can also serve as an external knowledge base to assist the reasoning process of
related large language models. Although there have been many related works on
knowledge graphs, there is currently a lack of research on the construction
methods of policy knowledge graphs. This paper, focusing on the forestry field,
designs a complete policy knowledge graph construction framework, including:
firstly, proposing a fine-grained forestry policy domain ontology; then,
proposing an unsupervised policy information extraction method, and finally,
constructing a complete forestry policy knowledge graph. The experimental
results show that the proposed ontology has good expressiveness and
extensibility, and the policy information extraction method proposed in this
paper achieves better results than other unsupervised methods. Furthermore, by
analyzing the application of the knowledge graph in the
retrieval-augmented-generation task of the large language models, the practical
application value of the knowledge graph in the era of large language models is
confirmed. The knowledge graph resource will be released on an open-source
platform and can serve as the basic knowledge base for forestry policy-related
intelligent systems. It can also be used for academic research. In addition,
this study can provide reference and guidance for the construction of policy
knowledge graphs in other fields.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.11090v1},
File          = {2411.11090v1.pdf}
}
@article{2412.08742v1,
Author        = {Udari Madhushani Sehwag and Kassiani Papasotiriou and Jared Vann and Sumitra Ganesh},
Title         = {In-Context Learning with Topological Information for Knowledge Graph
  Completion},
Eprint        = {2412.08742v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs (KGs) are crucial for representing and reasoning over
structured information, supporting a wide range of applications such as
information retrieval, question answering, and decision-making. However, their
effectiveness is often hindered by incompleteness, limiting their potential for
real-world impact. While knowledge graph completion (KGC) has been extensively
studied in the literature, recent advances in generative AI models,
particularly large language models (LLMs), have introduced new opportunities
for innovation. In-context learning has recently emerged as a promising
approach for leveraging pretrained knowledge of LLMs across a range of natural
language processing tasks and has been widely adopted in both academia and
industry. However, how to utilize in-context learning for effective KGC remains
relatively underexplored. We develop a novel method that incorporates
topological information through in-context learning to enhance KGC performance.
By integrating ontological knowledge and graph structure into the context of
LLMs, our approach achieves strong performance in the transductive setting
i.e., nodes in the test graph dataset are present in the training graph
dataset. Furthermore, we apply our approach to KGC in the more challenging
inductive setting, i.e., nodes in the training graph dataset and test graph
dataset are disjoint, leveraging the ontology to infer useful information about
missing nodes which serve as contextual cues for the LLM during inference. Our
method demonstrates superior performance compared to baselines on the
ILPC-small and ILPC-large datasets.},
Year          = {2024},
Month         = {Dec},
Note          = {Proceedings of the ICML 2024 Workshop on Structured Probabilistic
  Inference & Generative Modeling},
Url           = {http://arxiv.org/abs/2412.08742v1},
File          = {2412.08742v1.pdf}
}
@article{2405.14012v1,
Author        = {Tolga Çöplü and Arto Bendiken and Andrii Skomorokhov and Eduard Bateiko and Stephen Cobb},
Title         = {Prompt-Time Ontology-Driven Symbolic Knowledge Capture with Large
  Language Models},
Eprint        = {2405.14012v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In applications such as personal assistants, large language models (LLMs)
must consider the user's personal information and preferences. However, LLMs
lack the inherent ability to learn from user interactions. This paper explores
capturing personal information from user prompts using ontology and
knowledge-graph approaches. We use a subset of the KNOW ontology, which models
personal information, to train the language model on these concepts. We then
evaluate the success of knowledge capture using a specially constructed
dataset. Our code and datasets are publicly available at
https://github.com/HaltiaAI/paper-PTODSKC},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.14012v1},
File          = {2405.14012v1.pdf}
}
@article{2412.20942v1,
Author        = {Xiaohan Feng and Xixin Wu and Helen Meng},
Title         = {Ontology-grounded Automatic Knowledge Graph Construction by LLM under
  Wikidata schema},
Eprint        = {2412.20942v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We propose an ontology-grounded approach to Knowledge Graph (KG) construction
using Large Language Models (LLMs) on a knowledge base. An ontology is authored
by generating Competency Questions (CQ) on knowledge base to discover knowledge
scope, extracting relations from CQs, and attempt to replace equivalent
relations by their counterpart in Wikidata. To ensure consistency and
interpretability in the resulting KG, we ground generation of KG with the
authored ontology based on extracted relations. Evaluation on benchmark
datasets demonstrates competitive performance in knowledge graph construction
task. Our work presents a promising direction for scalable KG construction
pipeline with minimal human intervention, that yields high quality and
human-interpretable KGs, which are interoperable with Wikidata semantics for
potential knowledge base expansion.},
Year          = {2024},
Month         = {Dec},
Note          = {CEUR Workshop Proceedings 3841 (2024) 117-135},
Url           = {http://arxiv.org/abs/2412.20942v1},
File          = {2412.20942v1.pdf}
}
@article{2308.02357v1,
Author        = {Nandana Mihindukulasooriya and Sanju Tiwari and Carlos F. Enguix and Kusum Lata},
Title         = {Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation
  from Text},
Eprint        = {2308.02357v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The recent advances in large language models (LLM) and foundation models with
emergent capabilities have been shown to improve the performance of many NLP
tasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMs
can be used for KG construction or completion while existing KGs can be used
for different tasks such as making LLM outputs explainable or fact-checking in
Neuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to
evaluate the capabilities of language models to generate KGs from natural
language text guided by an ontology. Given an input ontology and a set of
sentences, the task is to extract facts from the text while complying with the
given ontology (concepts, relations, domain/range constraints) and being
faithful to the input sentences. We provide two datasets (i) Wikidata-TekGen
with 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19
ontologies and 4,860 sentences. We define seven evaluation metrics to measure
fact extraction performance, ontology conformance, and hallucinations by LLMs.
Furthermore, we provide results for two baseline models, Vicuna-13B and
Alpaca-LoRA-13B using automatic prompt generation from test cases. The baseline
results show that there is room for improvement using both Semantic Web and
Natural Language Processing techniques.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.02357v1},
File          = {2308.02357v1.pdf}
}
@article{2409.07088v1,
Author        = {Daehee Kim and Deokhyung Kang and Sangwon Ryu and Gary Geunbae Lee},
Title         = {Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset
  Synthesis using Large Language Model},
Eprint        = {2409.07088v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graph-to-Text (G2T) generation involves verbalizing structured
knowledge graphs into natural language text. Recent advancements in Pretrained
Language Models (PLMs) have improved G2T performance, but their effectiveness
depends on datasets with precise graph-text alignment. However, the scarcity of
high-quality, general-domain G2T generation datasets restricts progress in the
general-domain G2T generation research. To address this issue, we introduce
Wikipedia Ontology-Free Graph-text dataset (WikiOFGraph), a new large-scale G2T
dataset generated using a novel method that leverages Large Language Model
(LLM) and Data-QuestEval. Our new dataset, which contains 5.85M general-domain
graph-text pairs, offers high graph-text consistency without relying on
external ontologies. Experimental results demonstrate that PLM fine-tuned on
WikiOFGraph outperforms those trained on other datasets across various
evaluation metrics. Our method proves to be a scalable and effective solution
for generating high-quality G2T data, significantly advancing the field of G2T
generation.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.07088v1},
File          = {2409.07088v1.pdf}
}
@article{2412.00608v3,
Author        = {Mohammad Sadeq Abolhasani and Rong Pan},
Title         = {Leveraging LLM for Automated Ontology Extraction and Knowledge Graph
  Generation},
Eprint        = {2412.00608v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Extracting relevant and structured knowledge from large, complex technical
documents within the Reliability and Maintainability (RAM) domain is
labor-intensive and prone to errors. Our work addresses this challenge by
presenting OntoKGen, a genuine pipeline for ontology extraction and Knowledge
Graph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through
an interactive user interface guided by our adaptive iterative Chain of Thought
(CoT) algorithm to ensure that the ontology extraction process and, thus, KG
generation align with user-specific requirements. Although KG generation
follows a clear, structured path based on the confirmed ontology, there is no
universally correct ontology as it is inherently based on the user's
preferences. OntoKGen recommends an ontology grounded in best practices,
minimizing user effort and providing valuable insights that may have been
overlooked, all while giving the user complete control over the final ontology.
Having generated the KG based on the confirmed ontology, OntoKGen enables
seamless integration into schemeless, non-relational databases like Neo4j. This
integration allows for flexible storage and retrieval of knowledge from
diverse, unstructured sources, facilitating advanced querying, analysis, and
decision-making. Moreover, the generated KG serves as a robust foundation for
future integration into Retrieval Augmented Generation (RAG) systems, offering
enhanced capabilities for developing domain-specific intelligent applications.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2412.00608v3},
File          = {2412.00608v3.pdf}
}
@article{2407.21708v1,
Author        = {Stefan Langer and Fabian Neuhaus and Andreas Nürnberger},
Title         = {CEAR: Automatic construction of a knowledge graph of chemical entities
  and roles from scientific literature},
Eprint        = {2407.21708v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontologies are formal representations of knowledge in specific domains that
provide a structured framework for organizing and understanding complex
information. Creating ontologies, however, is a complex and time-consuming
endeavor. ChEBI is a well-known ontology in the field of chemistry, which
provides a comprehensive resource for defining chemical entities and their
properties. However, it covers only a small fraction of the rapidly growing
knowledge in chemistry and does not provide references to the scientific
literature. To address this, we propose a methodology that involves augmenting
existing annotated text corpora with knowledge from Chebi and fine-tuning a
large language model (LLM) to recognize chemical entities and their roles in
scientific text. Our experiments demonstrate the effectiveness of our approach.
By combining ontological knowledge and the language understanding capabilities
of LLMs, we achieve high precision and recall rates in identifying both the
chemical entities and roles in scientific literature. Furthermore, we extract
them from a set of 8,000 ChemRxiv articles, and apply a second LLM to create a
knowledge graph (KG) of chemical entities and roles (CEAR), which provides
complementary information to ChEBI, and can help to extend it.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.21708v1},
File          = {2407.21708v1.pdf}
}
@article{2410.03235v2,
Author        = {Elias Crum and Antonio De Santis and Manon Ovide and Jiaxin Pan and Alessia Pisu and Nicolas Lazzari and Sebastian Rudolph},
Title         = {Enriching Ontologies with Disjointness Axioms using Large Language
  Models},
Eprint        = {2410.03235v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontologies often lack explicit disjointness declarations between classes,
despite their usefulness for sophisticated reasoning and consistency checking
in Knowledge Graphs. In this study, we explore the potential of Large Language
Models (LLMs) to enrich ontologies by identifying and asserting class
disjointness axioms. Our approach aims at leveraging the implicit knowledge
embedded in LLMs, using prompt engineering to elicit this knowledge for
classifying ontological disjointness. We validate our methodology on the
DBpedia ontology, focusing on open-source LLMs. Our findings suggest that LLMs,
when guided by effective prompt strategies, can reliably identify disjoint
class relationships, thus streamlining the process of ontology completion
without extensive manual input. For comprehensive disjointness enrichment, we
propose a process that takes logical relationships between disjointness and
subclass statements into account in order to maintain satisfiability and reduce
the number of calls to the LLM. This work provides a foundation for future
applications of LLMs in automated ontology enhancement and offers insights into
optimizing LLM performance through strategic prompt design. Our code is
publicly available on GitHub at https://github.com/n28div/llm-disjointness.},
Year          = {2024},
Month         = {Oct},
Note          = {Proc. of 2nd Workshop on Knowledge Base Construction from
  Pre-Trained Language Models (KBC-LM 2024) co-located with ISWC 2024,
  Baltimore, USA, November 12, 2024},
Url           = {http://arxiv.org/abs/2410.03235v2},
File          = {2410.03235v2.pdf}
}
@article{2201.11332v1,
Author        = {Hongbin Ye and Ningyu Zhang and Shumin Deng and Xiang Chen and Hui Chen and Feiyu Xiong and Xi Chen and Huajun Chen},
Title         = {Ontology-enhanced Prompt-tuning for Few-shot Learning},
Eprint        = {2201.11332v1},
DOI           = {10.1145/3485447.3511921},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Few-shot Learning (FSL) is aimed to make predictions based on a limited
number of samples. Structured data such as knowledge graphs and ontology
libraries has been leveraged to benefit the few-shot setting in various tasks.
However, the priors adopted by the existing methods suffer from challenging
knowledge missing, knowledge noise, and knowledge heterogeneity, which hinder
the performance for few-shot learning. In this study, we explore knowledge
injection for FSL with pre-trained language models and propose
ontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the
ontology transformation based on the external knowledge graph to address the
knowledge missing issue, which fulfills and converts structure knowledge to
text. We further introduce span-sensitive knowledge injection via a visible
matrix to select informative knowledge to handle the knowledge noise issue. To
bridge the gap between knowledge and text, we propose a collective training
algorithm to optimize representations jointly. We evaluate our proposed
OntoPrompt in three tasks, including relation extraction, event extraction, and
knowledge graph completion, with eight datasets. Experimental results
demonstrate that our approach can obtain better few-shot performance than
baselines.},
Year          = {2022},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2201.11332v1},
File          = {2201.11332v1.pdf}
}
@article{2305.04676v1,
Author        = {Milena Trajanoska and Riste Stojanov and Dimitar Trajanov},
Title         = {Enhancing Knowledge Graph Construction Using Large Language Models},
Eprint        = {2305.04676v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The growing trend of Large Language Models (LLM) development has attracted
significant attention, with models for various applications emerging
consistently. However, the combined application of Large Language Models with
semantic technologies for reasoning and inference is still a challenging task.
This paper analyzes how the current advances in foundational LLM, like ChatGPT,
can be compared with the specialized pretrained models, like REBEL, for joint
entity and relation extraction. To evaluate this approach, we conducted several
experiments using sustainability-related text as our use case. We created
pipelines for the automatic creation of Knowledge Graphs from raw texts, and
our findings indicate that using advanced LLM models can improve the accuracy
of the process of creating these graphs from unstructured text. Furthermore, we
explored the potential of automatic ontology creation using foundation LLM
models, which resulted in even more relevant and accurate knowledge graphs.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.04676v1},
File          = {2305.04676v1.pdf}
}
@article{2409.00830v1,
Author        = {Saransh Kumar Gupta and Lipika Dey and Partha Pratim Das and Ramesh Jain},
Title         = {Building FKG.in: a Knowledge Graph for Indian Food},
Eprint        = {2409.00830v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper presents an ontology design along with knowledge engineering, and
multilingual semantic reasoning techniques to build an automated system for
assimilating culinary information for Indian food in the form of a knowledge
graph. The main focus is on designing intelligent methods to derive ontology
designs and capture all-encompassing knowledge about food, recipes,
ingredients, cooking characteristics, and most importantly, nutrition, at
scale. We present our ongoing work in this workshop paper, describe in some
detail the relevant challenges in curating knowledge of Indian food, and
propose our high-level ontology design. We also present a novel workflow that
uses AI, LLM, and language technology to curate information from recipe blog
sites in the public domain to build knowledge graphs for Indian food. The
methods for knowledge curation proposed in this paper are generic and can be
replicated for any domain. The design is application-agnostic and can be used
for AI-driven smart analysis, building recommendation systems for Personalized
Digital Health, and complementing the knowledge graph for Indian food with
contextual information such as user information, food biochemistry, geographic
information, agricultural information, etc.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.00830v1},
File          = {2409.00830v1.pdf}
}
@article{2301.03980v1,
Author        = {Kunal Suri and Atul Singh and Prakhar Mishra and Swapna Sourav Rout and Rajesh Sabapathy},
Title         = {Language Models sounds the Death Knell of Knowledge Graphs},
Eprint        = {2301.03980v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Healthcare domain generates a lot of unstructured and semi-structured text.
Natural Language processing (NLP) has been used extensively to process this
data. Deep Learning based NLP especially Large Language Models (LLMs) such as
BERT have found broad acceptance and are used extensively for many
applications. A Language Model is a probability distribution over a word
sequence. Self-supervised Learning on a large corpus of data automatically
generates deep learning-based language models. BioBERT and Med-BERT are
language models pre-trained for the healthcare domain. Healthcare uses typical
NLP tasks such as question answering, information extraction, named entity
recognition, and search to simplify and improve processes. However, to ensure
robust application of the results, NLP practitioners need to normalize and
standardize them. One of the main ways of achieving normalization and
standardization is the use of Knowledge Graphs. A Knowledge Graph captures
concepts and their relationships for a specific domain, but their creation is
time-consuming and requires manual intervention from domain experts, which can
prove expensive. SNOMED CT (Systematized Nomenclature of Medicine -- Clinical
Terms), Unified Medical Language System (UMLS), and Gene Ontology (GO) are
popular ontologies from the healthcare domain. SNOMED CT and UMLS capture
concepts such as disease, symptoms and diagnosis and GO is the world's largest
source of information on the functions of genes. Healthcare has been dealing
with an explosion in information about different types of drugs, diseases, and
procedures. This paper argues that using Knowledge Graphs is not the best
solution for solving problems in this domain. We present experiments using LLMs
for the healthcare domain to demonstrate that language models provide the same
functionality as knowledge graphs, thereby making knowledge graphs redundant.},
Year          = {2023},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2301.03980v1},
File          = {2301.03980v1.pdf}
}
@article{2403.08345v1,
Author        = {Vamsi Krishna Kommineni and Birgitta König-Ries and Sheeba Samuel},
Title         = {From human experts to machines: An LLM supported approach to ontology
  and knowledge graph construction},
Eprint        = {2403.08345v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The conventional process of building Ontologies and Knowledge Graphs (KGs)
heavily relies on human domain experts to define entities and relationship
types, establish hierarchies, maintain relevance to the domain, fill the ABox
(or populate with instances), and ensure data quality (including amongst others
accuracy and completeness). On the other hand, Large Language Models (LLMs)
have recently gained popularity for their ability to understand and generate
human-like natural language, offering promising ways to automate aspects of
this process. This work explores the (semi-)automatic construction of KGs
facilitated by open-source LLMs. Our pipeline involves formulating competency
questions (CQs), developing an ontology (TBox) based on these CQs, constructing
KGs using the developed ontology, and evaluating the resultant KG with minimal
to no involvement of human experts. We showcase the feasibility of our
semi-automated pipeline by creating a KG on deep learning methodologies by
exploiting scholarly publications. To evaluate the answers generated via
Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically
extracted using LLMs, we design a judge LLM, which rates the generated content
based on ground truth. Our findings suggest that employing LLMs could
potentially reduce the human effort involved in the construction of KGs,
although a human-in-the-loop approach is recommended to evaluate automatically
generated KGs.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.08345v1},
File          = {2403.08345v1.pdf}
}
@article{2404.19146v1,
Author        = {Linyi Ding and Sizhe Zhou and Jinfeng Xiao and Jiawei Han},
Title         = {Automated Construction of Theme-specific Knowledge Graphs},
Eprint        = {2404.19146v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Despite widespread applications of knowledge graphs (KGs) in various tasks
such as question answering and intelligent conversational systems, existing KGs
face two major challenges: information granularity and deficiency in
timeliness. These hinder considerably the retrieval and analysis of in-context,
fine-grained, and up-to-date knowledge from KGs, particularly in highly
specialized themes (e.g., specialized scientific research) and rapidly evolving
contexts (e.g., breaking news or disaster tracking). To tackle such challenges,
we propose a theme-specific knowledge graph (i.e., ThemeKG), a KG constructed
from a theme-specific corpus, and design an unsupervised framework for ThemeKG
construction (named TKGCon). The framework takes raw theme-specific corpus and
generates a high-quality KG that includes salient entities and relations under
the theme. Specifically, we start with an entity ontology of the theme from
Wikipedia, based on which we then generate candidate relations by Large
Language Models (LLMs) to construct a relation ontology. To parse the documents
from the theme corpus, we first map the extracted entity pairs to the ontology
and retrieve the candidate relations. Finally, we incorporate the context and
ontology to consolidate the relations for entity pairs. We observe that
directly prompting GPT-4 for theme-specific KG leads to inaccurate entities
(such as "two main types" as one entity in the query result) and unclear (such
as "is", "has") or wrong relations (such as "have due to", "to start"). In
contrast, by constructing the theme-specific KG step by step, our model
outperforms GPT-4 and could consistently identify accurate entities and
relations. Experimental results also show that our framework excels in
evaluations compared with various KG construction baselines.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.19146v1},
File          = {2404.19146v1.pdf}
}
@article{2405.20163v1,
Author        = {Rosario Uceda-Sosa and Karthikeyan Natesan Ramamurthy and Maria Chang and Moninder Singh},
Title         = {Reasoning about concepts with LLMs: Inconsistencies abound},
Eprint        = {2405.20163v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The ability to summarize and organize knowledge into abstract concepts is key
to learning and reasoning. Many industrial applications rely on the consistent
and systematic use of concepts, especially when dealing with decision-critical
knowledge. However, we demonstrate that, when methodically questioned, large
language models (LLMs) often display and demonstrate significant
inconsistencies in their knowledge. Computationally, the basic aspects of the
conceptualization of a given domain can be represented as Is-A hierarchies in a
knowledge graph (KG) or ontology, together with a few properties or axioms that
enable straightforward reasoning. We show that even simple ontologies can be
used to reveal conceptual inconsistencies across several LLMs. We also propose
strategies that domain experts can use to evaluate and improve the coverage of
key domain concepts in LLMs of various sizes. In particular, we have been able
to significantly enhance the performance of LLMs of various sizes with openly
available weights using simple knowledge-graph (KG) based prompting strategies.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.20163v1},
File          = {2405.20163v1.pdf}
}
@article{2410.07454v1,
Author        = {Suqi Liu and Tianxi Cai and Xiaoou Li},
Title         = {Representation-Enhanced Neural Knowledge Integration with Application to
  Large-Scale Medical Ontology Learning},
Eprint        = {2410.07454v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ME},
Abstract      = {A large-scale knowledge graph enhances reproducibility in biomedical data
discovery by providing a standardized, integrated framework that ensures
consistent interpretation across diverse datasets. It improves generalizability
by connecting data from various sources, enabling broader applicability of
findings across different populations and conditions. Generating reliable
knowledge graph, leveraging multi-source information from existing literature,
however, is challenging especially with a large number of node sizes and
heterogeneous relations. In this paper, we propose a general theoretically
guaranteed statistical framework, called RENKI, to enable simultaneous learning
of multiple relation types. RENKI generalizes various network models widely
used in statistics and computer science. The proposed framework incorporates
representation learning output into initial entity embedding of a neural
network that approximates the score function for the knowledge graph and
continuously trains the model to fit observed facts. We prove nonasymptotic
bounds for in-sample and out-of-sample weighted MSEs in relation to the
pseudo-dimension of the knowledge graph function class. Additionally, we
provide pseudo-dimensions for score functions based on multilayer neural
networks with ReLU activation function, in the scenarios when the embedding
parameters either fixed or trainable. Finally, we complement our theoretical
results with numerical studies and apply the method to learn a comprehensive
medical knowledge graph combining a pretrained language model representation
with knowledge graph links observed in several medical ontologies. The
experiments justify our theoretical findings and demonstrate the effect of
weighting in the presence of heterogeneous relations and the benefit of
incorporating representation learning in nonparametric models.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.07454v1},
File          = {2410.07454v1.pdf}
}
@article{2306.10723v2,
Author        = {Teodoro Baldazzi and Luigi Bellomarini and Stefano Ceri and Andrea Colombo and Andrea Gentili and Emanuel Sallinger},
Title         = {Fine-tuning Large Enterprise Language Models via Ontological Reasoning},
Eprint        = {2306.10723v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to
diverse goals, thanks to task-specific training data. Task specificity should
go hand in hand with domain orientation, that is, the specialization of an LLM
to accurately address the tasks of a given realm of interest. However, models
are usually fine-tuned over publicly available data or, at most, over ground
data from databases, ignoring business-level definitions and domain experience.
On the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and
augment such domain knowledge via ontological reasoning. With the goal of
combining LLM flexibility with the domain orientation of EKGs, we propose a
novel neurosymbolic architecture that leverages the power of ontological
reasoning to build task- and domain-specific corpora for LLM fine-tuning.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.10723v2},
File          = {2306.10723v2.pdf}
}
@article{2202.09791v4,
Author        = {Jiaoyan Chen and Yuan He and Yuxia Geng and Ernesto Jimenez-Ruiz and Hang Dong and Ian Horrocks},
Title         = {Contextual Semantic Embeddings for Ontology Subsumption Prediction},
Eprint        = {2202.09791v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Automating ontology construction and curation is an important but challenging
task in knowledge engineering and artificial intelligence. Prediction by
machine learning techniques such as contextual semantic embedding is a
promising direction, but the relevant research is still preliminary especially
for expressive ontologies in Web Ontology Language (OWL). In this paper, we
present a new subsumption prediction method named BERTSubs for classes of OWL
ontology. It exploits the pre-trained language model BERT to compute contextual
embeddings of a class, where customized templates are proposed to incorporate
the class context (e.g., neighbouring classes) and the logical existential
restriction. BERTSubs is able to predict multiple kinds of subsumers including
named classes from the same ontology or another ontology, and existential
restrictions from the same ontology. Extensive evaluation on five real-world
ontologies for three different subsumption tasks has shown the effectiveness of
the templates and that BERTSubs can dramatically outperform the baselines that
use (literal-aware) knowledge graph embeddings, non-contextual word embeddings
and the state-of-the-art OWL ontology embeddings.},
Year          = {2022},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2202.09791v4},
File          = {2202.09791v4.pdf}
}
@article{2312.08036v1,
Author        = {Huaiyuan Ying and Zhengyun Zhao and Yang Zhao and Sihang Zeng and Sheng Yu},
Title         = {CoRTEx: Contrastive Learning for Representing Terms via Explanations
  with Applications on Constructing Biomedical Knowledge Graphs},
Eprint        = {2312.08036v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Objective: Biomedical Knowledge Graphs play a pivotal role in various
biomedical research domains. Concurrently, term clustering emerges as a crucial
step in constructing these knowledge graphs, aiming to identify synonymous
terms. Due to a lack of knowledge, previous contrastive learning models trained
with Unified Medical Language System (UMLS) synonyms struggle at clustering
difficult terms and do not generalize well beyond UMLS terms. In this work, we
leverage the world knowledge from Large Language Models (LLMs) and propose
Contrastive Learning for Representing Terms via Explanations (CoRTEx) to
enhance term representation and significantly improves term clustering.
Materials and Methods: The model training involves generating explanations for
a cleaned subset of UMLS terms using ChatGPT. We employ contrastive learning,
considering term and explanation embeddings simultaneously, and progressively
introduce hard negative samples. Additionally, a ChatGPT-assisted BIRCH
algorithm is designed for efficient clustering of a new ontology. Results: We
established a clustering test set and a hard negative test set, where our model
consistently achieves the highest F1 score. With CoRTEx embeddings and the
modified BIRCH algorithm, we grouped 35,580,932 terms from the Biomedical
Informatics Ontology System (BIOS) into 22,104,559 clusters with O(N) queries
to ChatGPT. Case studies highlight the model's efficacy in handling challenging
samples, aided by information from explanations. Conclusion: By aligning terms
to their explanations, CoRTEx demonstrates superior accuracy over benchmark
models and robustness beyond its training set, and it is suitable for
clustering terms for large-scale biomedical ontologies.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.08036v1},
File          = {2312.08036v1.pdf}
}
@article{2502.03992v1,
Author        = {Longquan Jiang and Junbo Huang and Cedric Möller and Ricardo Usbeck},
Title         = {Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge
  Graph Question Answering},
Eprint        = {2502.03992v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Most existing Knowledge Graph Question Answering (KGQA) approaches are
designed for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the
heterogeneity of the underlying graph schema, topology and assertions, most
KGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without
resource-intensive training data. We present OntoSCPrompt, a novel Large
Language Model (LLM)-based KGQA approach with a two-stage architecture that
separates semantic parsing from KG-dependent interactions. OntoSCPrompt first
generates a SPARQL query structure (including SPARQL keywords such as SELECT,
ASK, WHERE and placeholders for missing tokens) and then fills them with
KG-specific information. To enhance the understanding of the underlying KG, we
present an ontology-guided, hybrid prompt learning strategy that integrates KG
ontology into the learning process of hybrid prompts (e.g., discrete and
continuous vectors). We also present several task-specific decoding strategies
to ensure the correctness and executability of generated SPARQL queries in both
stages. Experimental results demonstrate that OntoSCPrompt performs as well as
SOTA approaches without retraining on a number of KGQA datasets such as CWQ,
WebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well
to unseen domain-specific KGs like DBLP-QuAD and CoyPu KG Code:
\href{https://github.com/LongquanJiang/OntoSCPrompt}{https://github.com/LongquanJiang/OntoSCPrompt}},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.03992v1},
File          = {2502.03992v1.pdf}
}
@article{2109.07401v1,
Author        = {Sven Hertling and Jan Portisch and Heiko Paulheim},
Title         = {Matching with Transformers in MELT},
Eprint        = {2109.07401v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {One of the strongest signals for automated matching of ontologies and
knowledge graphs are the textual descriptions of the concepts. The methods that
are typically applied (such as character- or token-based comparisons) are
relatively simple, and therefore do not capture the actual meaning of the
texts. With the rise of transformer-based language models, text comparison
based on meaning (rather than lexical features) is possible. In this paper, we
model the ontology matching task as classification problem and present
approaches based on transformer models. We further provide an easy to use
implementation in the MELT framework which is suited for ontology and knowledge
graph matching. We show that a transformer-based filter helps to choose the
correct correspondences given a high-recall alignment and already achieves a
good result with simple alignment post-processing methods.},
Year          = {2021},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2109.07401v1},
File          = {2109.07401v1.pdf}
}
@article{2411.01612v1,
Author        = {Sanaz Saki Norouzi and Adrita Barua and Antrea Christou and Nikita Gautam and Andrew Eells and Pascal Hitzler and Cogan Shimizu},
Title         = {Ontology Population using LLMs},
Eprint        = {2411.01612v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs (KGs) are increasingly utilized for data integration,
representation, and visualization. While KG population is critical, it is often
costly, especially when data must be extracted from unstructured text in
natural language, which presents challenges, such as ambiguity and complex
interpretations. Large Language Models (LLMs) offer promising capabilities for
such tasks, excelling in natural language understanding and content generation.
However, their tendency to ``hallucinate'' can produce inaccurate outputs.
Despite these limitations, LLMs offer rapid and scalable processing of natural
language data, and with prompt engineering and fine-tuning, they can
approximate human-level performance in extracting and structuring data for KGs.
This study investigates LLM effectiveness for the KG population, focusing on
the Enslaved.org Hub Ontology. In this paper, we report that compared to the
ground truth, LLM's can extract ~90% of triples, when provided a modular
ontology as guidance in the prompts.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.01612v1},
File          = {2411.01612v1.pdf}
}
@article{2408.04661v1,
Author        = {Ali Riza Durmaz and Akhil Thomas and Lokesh Mishra and Rachana Niranjan Murthy and Thomas Straub},
Title         = {MaterioMiner -- An ontology-based text mining dataset for extraction of
  process-structure-property entities},
Eprint        = {2408.04661v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {While large language models learn sound statistical representations of the
language and information therein, ontologies are symbolic knowledge
representations that can complement the former ideally. Research at this
critical intersection relies on datasets that intertwine ontologies and text
corpora to enable training and comprehensive benchmarking of neurosymbolic
models. We present the MaterioMiner dataset and the linked materials mechanics
ontology where ontological concepts from the mechanics of materials domain are
associated with textual entities within the literature corpus. Another
distinctive feature of the dataset is its eminently fine-granular annotation.
Specifically, 179 distinct classes are manually annotated by three raters
within four publications, amounting to a total of 2191 entities that were
annotated and curated. Conceptual work is presented for the symbolic
representation of causal composition-process-microstructure-property
relationships. We explore the annotation consistency between the three raters
and perform fine-tuning of pre-trained models to showcase the feasibility of
named-entity recognition model training. Reusing the dataset can foster
training and benchmarking of materials language models, automated ontology
construction, and knowledge graph generation from textual data.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.04661v1},
File          = {2408.04661v1.pdf}
}
@article{2409.08820v2,
Author        = {Xueli Pan and Jacco van Ossenbruggen and Victor de Boer and Zhisheng Huang},
Title         = {A RAG Approach for Generating Competency Questions in Ontology
  Engineering},
Eprint        = {2409.08820v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Competency question (CQ) formulation is central to several ontology
development and evaluation methodologies. Traditionally, the task of crafting
these competency questions heavily relies on the effort of domain experts and
knowledge engineers which is often time-consuming and labor-intensive. With the
emergence of Large Language Models (LLMs), there arises the possibility to
automate and enhance this process. Unlike other similar works which use
existing ontologies or knowledge graphs as input to LLMs, we present a
retrieval-augmented generation (RAG) approach that uses LLMs for the automatic
generation of CQs given a set of scientific papers considered to be a domain
knowledge base. We investigate its performance and specifically, we study the
impact of different number of papers to the RAG and different temperature
setting of the LLM. We conduct experiments using GPT-4 on two domain ontology
engineering tasks and compare results against ground-truth CQs constructed by
domain experts. Empirical assessments on the results, utilizing evaluation
metrics (precision and consistency), reveal that compared to zero-shot
prompting, adding relevant domain knowledge to the RAG improves the performance
of LLMs on generating CQs for concrete ontology engineering tasks.},
Year          = {2024},
Month         = {Sep},
Note          = {18th International Conference on Metadata and Semantics Research
  (MTSR2024)},
Url           = {http://arxiv.org/abs/2409.08820v2},
File          = {2409.08820v2.pdf}
}
@article{2405.19255v3,
Author        = {Jose Tupayachi and Haowen Xu and Olufemi A. Omitaomu and Mustafa Can Camur and Aliza Sharmin and Xueping Li},
Title         = {Towards Next-Generation Urban Decision Support Systems through
  AI-Powered Construction of Scientific Ontology using Large Language Models --
  A Case in Optimizing Intermodal Freight Transportation},
Eprint        = {2405.19255v3},
DOI           = {10.3390/smartcities7050094},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The incorporation of Artificial Intelligence (AI) models into various
optimization systems is on the rise. Yet, addressing complex urban and
environmental management problems normally requires in-depth domain science and
informatics expertise. This expertise is essential for deriving data and
simulation-driven for informed decision support. In this context, we
investigate the potential of leveraging the pre-trained Large Language Models
(LLMs). By adopting ChatGPT API as the reasoning core, we outline an integrated
workflow that encompasses natural language processing, methontology-based
prompt tuning, and transformers. This workflow automates the creation of
scenario-based ontology using existing research articles and technical manuals
of urban datasets and simulations. The outcomes of our methodology are
knowledge graphs in widely adopted ontology languages (e.g., OWL, RDF, SPARQL).
These facilitate the development of urban decision support systems by enhancing
the data and metadata modeling, the integration of complex datasets, the
coupling of multi-domain simulation models, and the formulation of
decision-making metrics and workflow. The feasibility of our methodology is
evaluated through a comparative analysis that juxtaposes our AI-generated
ontology with the well-known Pizza Ontology employed in tutorials for popular
ontology software (e.g., prot\'eg\'e). We close with a real-world case study of
optimizing the complex urban system of multi-modal freight transportation by
generating anthologies of various domain data and simulations to support
informed decision-making.},
Year          = {2024},
Month         = {May},
Note          = {Smart Cities, 2024, 7(5), 2392-2421},
Url           = {http://arxiv.org/abs/2405.19255v3},
File          = {2405.19255v3.pdf}
}
@article{2406.08223v2,
Author        = {Hanieh Khorashadizadeh and Fatima Zahra Amara and Morteza Ezzabady and Frédéric Ieng and Sanju Tiwari and Nandana Mihindukulasooriya and Jinghua Groppe and Soror Sahri and Farah Benamara and Sven Groppe},
Title         = {Research Trends for the Interplay between Large Language Models and
  Knowledge Graphs},
Eprint        = {2406.08223v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This survey investigates the synergistic relationship between Large Language
Models (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's
capabilities in understanding, reasoning, and language processing. It aims to
address gaps in current research by exploring areas such as KG Question
Answering, ontology generation, KG validation, and the enhancement of KG
accuracy and consistency through LLMs. The paper further examines the roles of
LLMs in generating descriptive texts and natural language queries for KGs.
Through a structured analysis that includes categorizing LLM-KG interactions,
examining methodologies, and investigating collaborative uses and potential
biases, this study seeks to provide new insights into the combined potential of
LLMs and KGs. It highlights the importance of their interaction for improving
AI applications and outlines future research directions.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.08223v2},
File          = {2406.08223v2.pdf}
}
@article{2407.20007v1,
Author        = {Lars Vogt and Marcel Konrad and Kheir Eddine Farfar and Manuel Prinz and Allard Oelen},
Title         = {Rosetta Statements: Lowering the Barrier for Semantic Parsing and
  Increasing the Cognitive Interoperability of Knowledge Graphs},
Eprint        = {2407.20007v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Machines need data and metadata to be machine-actionable and FAIR (findable,
accessible, interoperable, reusable) to manage increasing data volumes.
Knowledge graphs and ontologies are key to this, but their use is hampered by
high access barriers due to required prior knowledge in semantics and data
modelling. The Rosetta Statement approach proposes modeling English natural
language statements instead of a mind-independent reality. We propose a
metamodel for creating semantic schema patterns for simple statement types. The
approach supports versioning of statements and provides a detailed editing
history. Each Rosetta Statement pattern has a dynamic label for displaying
statements as natural language sentences. Implemented in the Open Research
Knowledge Graph (ORKG) as a use case, this approach allows domain experts to
define data schema patterns without needing semantic knowledge. Future plans
include combining Rosetta Statements with semantic units to organize ORKG into
meaningful subgraphs, improving usability. A search interface for querying
statements without needing SPARQL or Cypher knowledge is also planned, along
with tools for data entry and display using Large Language Models and NLP. The
Rosetta Statement metamodel supports a two-step knowledge graph construction
procedure. Domain experts can model semantic content without support from
ontology engineers, lowering entry barriers and increasing cognitive
interoperability. The second level involves developing semantic graph patterns
for reasoning, requiring collaboration with ontology engineers.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.20007v1},
File          = {2407.20007v1.pdf}
}
@article{2111.00276v2,
Author        = {Anthony Colas and Ali Sadeghian and Yue Wang and Daisy Zhe Wang},
Title         = {EventNarrative: A large-scale Event-centric Dataset for Knowledge
  Graph-to-Text Generation},
Eprint        = {2111.00276v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce EventNarrative, a knowledge graph-to-text dataset from publicly
available open-world knowledge graphs. Given the recent advances in
event-driven Information Extraction (IE), and that prior research on
graph-to-text only focused on entity-driven KGs, this paper focuses on
event-centric data. However, our data generation system can still be adapted to
other other types of KG data. Existing large-scale datasets in the
graph-to-text area are non-parallel, meaning there is a large disconnect
between the KGs and text. The datasets that have a paired KG and text, are
small scale and manually generated or generated without a rich ontology, making
the corresponding graphs sparse. Furthermore, these datasets contain many
unlinked entities between their KG and text pairs. EventNarrative consists of
approximately 230,000 graphs and their corresponding natural language text, 6
times larger than the current largest parallel dataset. It makes use of a rich
ontology, all of the KGs entities are linked to the text, and our manual
annotations confirm a high data quality. Our aim is two-fold: help break new
ground in event-centric research where data is lacking, and to give researchers
a well-defined, large-scale dataset in order to better evaluate existing and
future knowledge graph-to-text models. We also evaluate two types of baseline
on EventNarrative: a graph-to-text specific model and two state-of-the-art
language models, which previous work has shown to be adaptable to the knowledge
graph-to-text domain.},
Year          = {2021},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2111.00276v2},
File          = {2111.00276v2.pdf}
}
@article{2410.22767v1,
Author        = {Sejin Lee and Dongha Kim and Min Song},
Title         = {Beyond Ontology in Dialogue State Tracking for Goal-Oriented Chatbot},
Eprint        = {2410.22767v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Goal-oriented chatbots are essential for automating user tasks, such as
booking flights or making restaurant reservations. A key component of these
systems is Dialogue State Tracking (DST), which interprets user intent and
maintains the dialogue state. However, existing DST methods often rely on fixed
ontologies and manually compiled slot values, limiting their adaptability to
open-domain dialogues. We propose a novel approach that leverages instruction
tuning and advanced prompt strategies to enhance DST performance, without
relying on any predefined ontologies. Our method enables Large Language Model
(LLM) to infer dialogue states through carefully designed prompts and includes
an anti-hallucination mechanism to ensure accurate tracking in diverse
conversation contexts. Additionally, we employ a Variational Graph Auto-Encoder
(VGAE) to model and predict subsequent user intent. Our approach achieved
state-of-the-art with a JGA of 42.57% outperforming existing ontology-less DST
models, and performed well in open-domain real-world conversations. This work
presents a significant advancement in creating more adaptive and accurate
goal-oriented chatbots.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.22767v1},
File          = {2410.22767v1.pdf}
}
@article{2409.05556v1,
Author        = {Alireza Ghafarollahi and Markus J. Buehler},
Title         = {SciAgents: Automating scientific discovery through multi-agent
  intelligent graph reasoning},
Eprint        = {2409.05556v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {A key challenge in artificial intelligence is the creation of systems capable
of autonomously advancing scientific understanding by exploring novel domains,
identifying complex patterns, and uncovering previously unseen connections in
vast scientific data. In this work, we present SciAgents, an approach that
leverages three core concepts: (1) the use of large-scale ontological knowledge
graphs to organize and interconnect diverse scientific concepts, (2) a suite of
large language models (LLMs) and data retrieval tools, and (3) multi-agent
systems with in-situ learning capabilities. Applied to biologically inspired
materials, SciAgents reveals hidden interdisciplinary relationships that were
previously considered unrelated, achieving a scale, precision, and exploratory
power that surpasses traditional human-driven research methods. The framework
autonomously generates and refines research hypotheses, elucidating underlying
mechanisms, design principles, and unexpected material properties. By
integrating these capabilities in a modular fashion, the intelligent system
yields material discoveries, critique and improve existing hypotheses, retrieve
up-to-date data about existing research, and highlights their strengths and
limitations. Our case studies demonstrate scalable capabilities to combine
generative AI, ontological representations, and multi-agent modeling,
harnessing a `swarm of intelligence' similar to biological systems. This
provides new avenues for materials discovery and accelerates the development of
advanced materials by unlocking Nature's design principles.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.05556v1},
File          = {2409.05556v1.pdf}
}
@article{2412.03589v1,
Author        = {Valentina Anita Carriero and Antonia Azzini and Ilaria Baroni and Mario Scrocca and Irene Celino},
Title         = {Human Evaluation of Procedural Knowledge Graph Extraction from Text with
  Large Language Models},
Eprint        = {2412.03589v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Procedural Knowledge is the know-how expressed in the form of sequences of
steps needed to perform some tasks. Procedures are usually described by means
of natural language texts, such as recipes or maintenance manuals, possibly
spread across different documents and systems, and their interpretation and
subsequent execution is often left to the reader. Representing such procedures
in a Knowledge Graph (KG) can be the basis to build digital tools to support
those users who need to apply or execute them. In this paper, we leverage Large
Language Model (LLM) capabilities and propose a prompt engineering approach to
extract steps, actions, objects, equipment and temporal information from a
textual procedure, in order to populate a Procedural KG according to a
pre-defined ontology. We evaluate the KG extraction results by means of a user
study, in order to qualitatively and quantitatively assess the perceived
quality and usefulness of the LLM-extracted procedural knowledge. We show that
LLMs can produce outputs of acceptable quality and we assess the subjective
perception of AI by human evaluators.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2412.03589v1},
File          = {2412.03589v1.pdf}
}
@article{2309.12731v1,
Author        = {Dave Raggett},
Title         = {Defeasible Reasoning with Knowledge Graphs},
Eprint        = {2309.12731v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Human knowledge is subject to uncertainties, imprecision, incompleteness and
inconsistencies. Moreover, the meaning of many everyday terms is dependent on
the context. That poses a huge challenge for the Semantic Web. This paper
introduces work on an intuitive notation and model for defeasible reasoning
with imperfect knowledge, and relates it to previous work on argumentation
theory. PKN is to N3 as defeasible reasoning is to deductive logic. Further
work is needed on an intuitive syntax for describing reasoning strategies and
tactics in declarative terms, drawing upon the AIF ontology for inspiration.
The paper closes with observations on symbolic approaches in the era of large
language models.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.12731v1},
File          = {2309.12731v1.pdf}
}
@article{2310.03840v1,
Author        = {Zhu Wang},
Title         = {Contextualized Structural Self-supervised Learning for Ontology Matching},
Eprint        = {2310.03840v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Ontology matching (OM) entails the identification of semantic relationships
between concepts within two or more knowledge graphs (KGs) and serves as a
critical step in integrating KGs from various sources. Recent advancements in
deep OM models have harnessed the power of transformer-based language models
and the advantages of knowledge graph embedding. Nevertheless, these OM models
still face persistent challenges, such as a lack of reference alignments,
runtime latency, and unexplored different graph structures within an end-to-end
framework. In this study, we introduce a novel self-supervised learning OM
framework with input ontologies, called LaKERMap. This framework capitalizes on
the contextual and structural information of concepts by integrating implicit
knowledge into transformers. Specifically, we aim to capture multiple
structural contexts, encompassing both local and global interactions, by
employing distinct training objectives. To assess our methods, we utilize the
Bio-ML datasets and tasks. The findings from our innovative approach reveal
that LaKERMap surpasses state-of-the-art systems in terms of alignment quality
and inference time. Our models and codes are available here:
https://github.com/ellenzhuwang/lakermap.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.03840v1},
File          = {2310.03840v1.pdf}
}
@article{2406.18114v2,
Author        = {Lukas Bahr and Christoph Wehner and Judith Wewerka and José Bittencourt and Ute Schmid and Rüdiger Daub},
Title         = {Knowledge Graph Enhanced Retrieval-Augmented Generation for Failure Mode
  and Effects Analysis},
Eprint        = {2406.18114v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Failure mode and effects analysis (FMEA) is a critical tool for mitigating
potential failures, particular during ramp-up phases of new products. However,
its effectiveness is often limited by the missing reasoning capabilities of the
FMEA tools, which are usually tabular structured. Meanwhile, large language
models (LLMs) offer novel prospects for fine-tuning on custom datasets for
reasoning within FMEA contexts. However, LLMs face challenges in tasks that
require factual knowledge, a gap that retrieval-augmented generation (RAG)
approaches aim to fill. RAG retrieves information from a non-parametric data
store and uses a language model to generate responses. Building on this idea,
we propose to advance the non-parametric data store with a knowledge graph
(KG). By enhancing the RAG framework with a KG, our objective is to leverage
analytical and semantic question-answering capabilities on FMEA data. This
paper contributes by presenting a new ontology for FMEA observations, an
algorithm for creating vector embeddings from the FMEA KG, and a KG enhanced
RAG framework. Our approach is validated through a human study and we measure
the performance of the context retrieval recall and precision.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.18114v2},
File          = {2406.18114v2.pdf}
}
@article{2309.11791v2,
Author        = {Zhaoyi Wang and Zhenyang Zhang and Jiaxin Qin and Mizuho Iwaihara},
Title         = {SLHCat: Mapping Wikipedia Categories and Lists to DBpedia by Leveraging
  Semantic, Lexical, and Hierarchical Features},
Eprint        = {2309.11791v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Wikipedia articles are hierarchically organized through categories and lists,
providing one of the most comprehensive and universal taxonomy, but its open
creation is causing redundancies and inconsistencies. Assigning DBPedia classes
to Wikipedia categories and lists can alleviate the problem, realizing a large
knowledge graph which is essential for categorizing digital contents through
entity linking and typing. However, the existing approach of CaLiGraph is
producing incomplete and non-fine grained mappings. In this paper, we tackle
the problem as ontology alignment, where structural information of knowledge
graphs and lexical and semantic features of ontology class names are utilized
to discover confident mappings, which are in turn utilized for finetuing
pretrained language models in a distant supervision fashion. Our method SLHCat
consists of two main parts: 1) Automatically generating training data by
leveraging knowledge graph structure, semantic similarities, and named entity
typing. 2) Finetuning and prompt-tuning of the pre-trained language model BERT
are carried out over the training data, to capture semantic and syntactic
properties of class names. Our model SLHCat is evaluated over a benchmark
dataset constructed by annotating 3000 fine-grained CaLiGraph-DBpedia mapping
pairs. SLHCat is outperforming the baseline model by a large margin of 25% in
accuracy, offering a practical solution for large-scale ontology mapping.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.11791v2},
File          = {2309.11791v2.pdf}
}
@article{2408.01700v1,
Author        = {Antonio De Santis and Marco Balduini and Federico De Santis and Andrea Proia and Arsenio Leo and Marco Brambilla and Emanuele Della Valle},
Title         = {Integrating Large Language Models and Knowledge Graphs for Extraction
  and Validation of Textual Test Data},
Eprint        = {2408.01700v1},
DOI           = {10.1007/978-3-031-77847-6_17},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Aerospace manufacturing companies, such as Thales Alenia Space, design,
develop, integrate, verify, and validate products characterized by high
complexity and low volume. They carefully document all phases for each product
but analyses across products are challenging due to the heterogeneity and
unstructured nature of the data in documents. In this paper, we propose a
hybrid methodology that leverages Knowledge Graphs (KGs) in conjunction with
Large Language Models (LLMs) to extract and validate data contained in these
documents. We consider a case study focused on test data related to electronic
boards for satellites. To do so, we extend the Semantic Sensor Network
ontology. We store the metadata of the reports in a KG, while the actual test
results are stored in parquet accessible via a Virtual Knowledge Graph. The
validation process is managed using an LLM-based approach. We also conduct a
benchmarking study to evaluate the performance of state-of-the-art LLMs in
executing this task. Finally, we analyze the costs and benefits of automating
preexisting processes of manual data extraction and validation for subsequent
cross-report analyses.},
Year          = {2024},
Month         = {Aug},
Note          = {ISWC 2024},
Url           = {http://arxiv.org/abs/2408.01700v1},
File          = {2408.01700v1.pdf}
}
@article{2410.22996v1,
Author        = {Deperias Kerre and Anne Laurent and Kenneth Maussang and Dickson Owuor},
Title         = {Semantic Enrichment of the Quantum Cascade Laser Properties in Text- A
  Knowledge Graph Generation Approach},
Eprint        = {2410.22996v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {A well structured collection of the various Quantum Cascade Laser (QCL)
design and working properties data provides a platform to analyze and
understand the relationships between these properties. By analyzing these
relationships, we can gain insights into how different design features impact
laser performance properties such as the working temperature. Most of these QCL
properties are captured in scientific text. There is therefore need for
efficient methodologies that can be utilized to extract QCL properties from
text and generate a semantically enriched and interlinked platform where the
properties can be analyzed to uncover hidden relations. There is also the need
to maintain provenance and reference information on which these properties are
based. Semantic Web technologies such as Ontologies and Knowledge Graphs have
proven capability in providing interlinked data platforms for knowledge
representation in various domains. In this paper, we propose an approach for
generating a QCL properties Knowledge Graph (KG) from text for semantic
enrichment of the properties. The approach is based on the QCL ontology and a
Retrieval Augmented Generation (RAG) enabled information extraction pipeline
based on GPT 4-Turbo language model. The properties of interest include:
working temperature, laser design type, lasing frequency, laser optical power
and the heterostructure. The experimental results demonstrate the feasibility
and effectiveness of this approach for efficiently extracting QCL properties
from unstructured text and generating a QCL properties Knowledge Graph, which
has potential applications in semantic enrichment and analysis of QCL data.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.22996v1},
File          = {2410.22996v1.pdf}
}
@article{2412.14191v1,
Author        = {Chengshuai Zhao and Garima Agrawal and Tharindu Kumarage and Zhen Tan and Yuli Deng and Ying-Chih Chen and Huan Liu},
Title         = {Ontology-Aware RAG for Improved Question-Answering in Cybersecurity
  Education},
Eprint        = {2412.14191v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {Integrating AI into education has the potential to transform the teaching of
science and technology courses, particularly in the field of cybersecurity.
AI-driven question-answering (QA) systems can actively manage uncertainty in
cybersecurity problem-solving, offering interactive, inquiry-based learning
experiences. Large language models (LLMs) have gained prominence in AI-driven
QA systems, offering advanced language understanding and user engagement.
However, they face challenges like hallucinations and limited domain-specific
knowledge, which reduce their reliability in educational settings. To address
these challenges, we propose CyberRAG, an ontology-aware retrieval-augmented
generation (RAG) approach for developing a reliable and safe QA system in
cybersecurity education. CyberRAG employs a two-step approach: first, it
augments the domain-specific knowledge by retrieving validated cybersecurity
documents from a knowledge base to enhance the relevance and accuracy of the
response. Second, it mitigates hallucinations and misuse by integrating a
knowledge graph ontology to validate the final answer. Experiments on publicly
available cybersecurity datasets show that CyberRAG delivers accurate, reliable
responses aligned with domain knowledge, demonstrating the potential of AI
tools to enhance education.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.14191v1},
File          = {2412.14191v1.pdf}
}
@article{2410.16597v1,
Author        = {Prafulla Kumar Choubey and Xin Su and Man Luo and Xiangyu Peng and Caiming Xiong and Tiep Le and Shachar Rosenman and Vasudev Lal and Phil Mui and Ricky Ho and Phillip Howard and Chien-Sheng Wu},
Title         = {Distill-SynthKG: Distilling Knowledge Graph Synthesis Workflow for
  Improved Coverage and Efficiency},
Eprint        = {2410.16597v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs (KGs) generated by large language models (LLMs) are becoming
increasingly valuable for Retrieval-Augmented Generation (RAG) applications
that require knowledge-intensive reasoning. However, existing KG extraction
methods predominantly rely on prompt-based approaches, which are inefficient
for processing large-scale corpora. These approaches often suffer from
information loss, particularly with long documents, due to the lack of
specialized design for KG construction. Additionally, there is a gap in
evaluation datasets and methodologies for ontology-free KG construction. To
overcome these limitations, we propose SynthKG, a multi-step, document-level
ontology-free KG synthesis workflow based on LLMs. By fine-tuning a smaller LLM
on the synthesized document-KG pairs, we streamline the multi-step process into
a single-step KG generation approach called Distill-SynthKG, substantially
reducing the number of LLM inference calls. Furthermore, we re-purpose existing
question-answering datasets to establish KG evaluation datasets and introduce
new evaluation metrics. Using KGs produced by Distill-SynthKG, we also design a
novel graph-based retrieval framework for RAG. Experimental results demonstrate
that Distill-SynthKG not only surpasses all baseline models in KG quality --
including models up to eight times larger -- but also consistently excels in
retrieval and question-answering tasks. Our proposed graph retrieval framework
also outperforms all KG-retrieval methods across multiple benchmark datasets.
We release the SynthKG dataset and Distill-SynthKG model publicly to support
further research and development.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.16597v1},
File          = {2410.16597v1.pdf}
}
@article{2502.02896v1,
Author        = {Bradley P. Allen and Paul T. Groth},
Title         = {A Benchmark for the Detection of Metalinguistic Disagreements between
  LLMs and Knowledge Graphs},
Eprint        = {2502.02896v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Evaluating large language models (LLMs) for tasks like fact extraction in
support of knowledge graph construction frequently involves computing accuracy
metrics using a ground truth benchmark based on a knowledge graph (KG). These
evaluations assume that errors represent factual disagreements. However, human
discourse frequently features metalinguistic disagreement, where agents differ
not on facts but on the meaning of the language used to express them. Given the
complexity of natural language processing and generation using LLMs, we ask: do
metalinguistic disagreements occur between LLMs and KGs? Based on an
investigation using the T-REx knowledge alignment dataset, we hypothesize that
metalinguistic disagreement does in fact occur between LLMs and KGs, with
potential relevance for the practice of knowledge graph engineering. We propose
a benchmark for evaluating the detection of factual and metalinguistic
disagreements between LLMs and KGs. An initial proof of concept of such a
benchmark is available on Github.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.02896v1},
File          = {2502.02896v1.pdf}
}
@article{2404.06571v1,
Author        = {Yunqing Li and Binil Starly},
Title         = {Building A Knowledge Graph to Enrich ChatGPT Responses in Manufacturing
  Service Discovery},
Eprint        = {2404.06571v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Sourcing and identification of new manufacturing partners is crucial for
manufacturing system integrators to enhance agility and reduce risk through
supply chain diversification in the global economy. The advent of advanced
large language models has captured significant interest, due to their ability
to generate comprehensive and articulate responses across a wide range of
knowledge domains. However, the system often falls short in accuracy and
completeness when responding to domain-specific inquiries, particularly in
areas like manufacturing service discovery. This research explores the
potential of leveraging Knowledge Graphs in conjunction with ChatGPT to
streamline the process for prospective clients in identifying small
manufacturing enterprises. In this study, we propose a method that integrates
bottom-up ontology with advanced machine learning models to develop a
Manufacturing Service Knowledge Graph from an array of structured and
unstructured data sources, including the digital footprints of small-scale
manufacturers throughout North America. The Knowledge Graph and the learned
graph embedding vectors are leveraged to tackle intricate queries within the
digital supply chain network, responding with enhanced reliability and greater
interpretability. The approach highlighted is scalable to millions of entities
that can be distributed to form a global Manufacturing Service Knowledge
Network Graph that can potentially interconnect multiple types of Knowledge
Graphs that span industry sectors, geopolitical boundaries, and business
domains. The dataset developed for this study, now publicly accessible,
encompasses more than 13,000 manufacturers' weblinks, manufacturing services,
certifications, and location entity types.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.06571v1},
File          = {2404.06571v1.pdf}
}
@article{2410.21060v1,
Author        = {Yutong Cheng and Osama Bajaber and Saimon Amanuel Tsegai and Dawn Song and Peng Gao},
Title         = {CTINEXUS: Leveraging Optimized LLM In-Context Learning for Constructing
  Cybersecurity Knowledge Graphs Under Data Scarcity},
Eprint        = {2410.21060v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Textual descriptions in cyber threat intelligence (CTI) reports, such as
security articles and news, are rich sources of knowledge about cyber threats,
crucial for organizations to stay informed about the rapidly evolving threat
landscape. However, current CTI extraction methods lack flexibility and
generalizability, often resulting in inaccurate and incomplete knowledge
extraction. Syntax parsing relies on fixed rules and dictionaries, while model
fine-tuning requires large annotated datasets, making both paradigms
challenging to adapt to new threats and ontologies. To bridge the gap, we
propose CTINexus, a novel framework leveraging optimized in-context learning
(ICL) of large language models (LLMs) for data-efficient CTI knowledge
extraction and high-quality cybersecurity knowledge graph (CSKG) construction.
Unlike existing methods, CTINexus requires neither extensive data nor parameter
tuning and can adapt to various ontologies with minimal annotated examples.
This is achieved through (1) a carefully designed automatic prompt construction
strategy with optimal demonstration retrieval for extracting a wide range of
cybersecurity entities and relations; (2) a hierarchical entity alignment
technique that canonicalizes the extracted knowledge and removes redundancy;
(3) an ICL-enhanced long-distance relation prediction technique to further
complete the CKSG with missing links. Our extensive evaluations using 150
real-world CTI reports collected from 10 platforms demonstrate that CTINexus
significantly outperforms existing methods in constructing accurate and
complete CSKGs, highlighting its potential to transform CTI analysis with an
efficient and adaptable solution for the dynamic threat landscape.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.21060v1},
File          = {2410.21060v1.pdf}
}
@article{2309.16248v2,
Author        = {Catherine Kosten and Philippe Cudré-Mauroux and Kurt Stockinger},
Title         = {Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph
  Question Answering Systems},
Eprint        = {2309.16248v2},
DOI           = {10.1109/BigData59044.2023.10386182},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {With the recent spike in the number and availability of Large Language Models
(LLMs), it has become increasingly important to provide large and realistic
benchmarks for evaluating Knowledge Graph Question Answering (KGQA) systems. So
far the majority of benchmarks rely on pattern-based SPARQL query generation
approaches. The subsequent natural language (NL) question generation is
conducted through crowdsourcing or other automated methods, such as rule-based
paraphrasing or NL question templates. Although some of these datasets are of
considerable size, their pitfall lies in their pattern-based generation
approaches, which do not always generalize well to the vague and linguistically
diverse questions asked by humans in real-world contexts. In this paper, we
introduce Spider4SPARQL - a new SPARQL benchmark dataset featuring 9,693
previously existing manually generated NL questions and 4,721 unique, novel,
and complex SPARQL queries of varying complexity. In addition to the NL/SPARQL
pairs, we also provide their corresponding 166 knowledge graphs and ontologies,
which cover 138 different domains. Our complex benchmark enables novel ways of
evaluating the strengths and weaknesses of modern KGQA systems. We evaluate the
system with state-of-the-art KGQA systems as well as LLMs, which achieve only
up to 45\% execution accuracy, demonstrating that Spider4SPARQL is a
challenging benchmark for future research.},
Year          = {2023},
Month         = {Sep},
Note          = {IEEE International Conference on Big Data 2023},
Url           = {http://arxiv.org/abs/2309.16248v2},
File          = {2309.16248v2.pdf}
}
@article{2407.01406v3,
Author        = {Daniil Gurgurov and Mareike Hartmann and Simon Ostermann},
Title         = {Adapting Multilingual LLMs to Low-Resource Languages with Knowledge
  Graphs via Adapters},
Eprint        = {2407.01406v3},
DOI           = {10.18653/v1/2024.kallm-1.7},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper explores the integration of graph knowledge from linguistic
ontologies into multilingual Large Language Models (LLMs) using adapters to
improve performance for low-resource languages (LRLs) in sentiment analysis
(SA) and named entity recognition (NER). Building upon successful
parameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we
propose a similar approach for incorporating knowledge from multilingual
graphs, connecting concepts in various languages with each other through
linguistic relationships, into multilingual LLMs for LRLs. Specifically, we
focus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,
Uyghur, Tibetan, and Sinhala -- and employ language-specific adapters
fine-tuned on data extracted from the language-specific section of ConceptNet,
aiming to enable knowledge transfer across the languages covered by the
knowledge graph. We compare various fine-tuning objectives, including standard
Masked Language Modeling (MLM), MLM with full-word masking, and MLM with
targeted masking, to analyse their effectiveness in learning and integrating
the extracted graph data. Through empirical evaluation on language-specific
tasks, we assess how structured graph knowledge affects the performance of
multilingual LLMs for LRLs in SA and NER, providing insights into the potential
benefits of adapting language models for low-resource scenarios.},
Year          = {2024},
Month         = {Jul},
Note          = {2024.kallm-1.7},
Url           = {http://arxiv.org/abs/2407.01406v3},
File          = {2407.01406v3.pdf}
}
@article{2407.12830v2,
Author        = {Sai Sathiesh Rajan and Ezekiel Soremekun and Sudipta Chattopadhyay},
Title         = {Knowledge-based Consistency Testing of Large Language Models},
Eprint        = {2407.12830v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this work, we systematically expose and measure the inconsistency and
knowledge gaps of Large Language Models (LLMs). Specifically, we propose an
automated testing framework (called KonTest) which leverages a knowledge graph
to construct test cases. KonTest probes and measures the inconsistencies in the
LLM's knowledge of the world via a combination of semantically-equivalent
queries and test oracles (metamorphic or ontological oracle). KonTest further
mitigates knowledge gaps via a weighted LLM model ensemble. Using four
state-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that
KonTest generates 19.2% error inducing inputs (1917 errors from 9979 test
inputs). It also reveals a 16.5% knowledge gap across all tested LLMs. A
mitigation method informed by KonTest's test suite reduces LLM knowledge gap by
32.48%. Our ablation study further shows that GPT3.5 is not suitable for
knowledge-based consistency testing because it is only 60%-68% effective in
knowledge construction.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.12830v2},
File          = {2407.12830v2.pdf}
}
@article{2201.11147v6,
Author        = {Ningyu Zhang and Zhen Bi and Xiaozhuan Liang and Siyuan Cheng and Haosen Hong and Shumin Deng and Jiazhang Lian and Qiang Zhang and Huajun Chen},
Title         = {OntoProtein: Protein Pretraining With Gene Ontology Embedding},
Eprint        = {2201.11147v6},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.BM},
Abstract      = {Self-supervised protein language models have proved their effectiveness in
learning the proteins representations. With the increasing computational power,
current protein language models pre-trained with millions of diverse sequences
can advance the parameter scale from million-level to billion-level and achieve
remarkable improvement. However, those prevailing approaches rarely consider
incorporating knowledge graphs (KGs), which can provide rich structured
knowledge facts for better protein representations. We argue that informative
biology knowledge in KGs can enhance protein representation with external
knowledge. In this work, we propose OntoProtein, the first general framework
that makes use of structure in GO (Gene Ontology) into protein pre-training
models. We construct a novel large-scale knowledge graph that consists of GO
and its related proteins, and gene annotation texts or protein sequences
describe all nodes in the graph. We propose novel contrastive learning with
knowledge-aware negative sampling to jointly optimize the knowledge graph and
protein embedding during pre-training. Experimental results show that
OntoProtein can surpass state-of-the-art methods with pre-trained protein
language models in TAPE benchmark and yield better performance compared with
baselines in protein-protein interaction and protein function prediction. Code
and datasets are available in https://github.com/zjunlp/OntoProtein.},
Year          = {2022},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2201.11147v6},
File          = {2201.11147v6.pdf}
}
@article{2407.01553v2,
Author        = {JingHong Li and Huy Phan and Wen Gu and Koichi Ota and Shinobu Hasegawa},
Title         = {Fish-bone diagram of research issue: Gain a bird's-eye view on a
  specific research topic},
Eprint        = {2407.01553v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Novice researchers often face difficulties in understanding a multitude of
academic papers and grasping the fundamentals of a new research field. To solve
such problems, the knowledge graph supporting research survey is gradually
being developed. Existing keyword-based knowledge graphs make it difficult for
researchers to deeply understand abstract concepts. Meanwhile, novice
researchers may find it difficult to use ChatGPT effectively for research
surveys due to their limited understanding of the research field. Without the
ability to ask proficient questions that align with key concepts, obtaining
desired and accurate answers from this large language model (LLM) could be
inefficient. This study aims to help novice researchers by providing a
fish-bone diagram that includes causal relationships, offering an overview of
the research topic. The diagram is constructed using the issue ontology from
academic papers, and it offers a broad, highly generalized perspective of the
research field, based on relevance and logical factors. Furthermore, we
evaluate the strengths and improvable points of the fish-bone diagram derived
from this study's development pattern, emphasizing its potential as a viable
tool for supporting research survey.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2407.01553v2},
File          = {2407.01553v2.pdf}
}
@article{2308.09729v5,
Author        = {Yilin Wen and Zifeng Wang and Jimeng Sun},
Title         = {MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large
  Language Models},
Eprint        = {2308.09729v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large language models (LLMs) have achieved remarkable performance in natural
language understanding and generation tasks. However, they often suffer from
limitations such as difficulty in incorporating new knowledge, generating
hallucinations, and explaining their reasoning process. To address these
challenges, we propose a novel prompting pipeline, named \method, that
leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency.
Our method enables LLMs to comprehend KG inputs and infer with a combination of
implicit and external knowledge. Moreover, our method elicits the mind map of
LLMs, which reveals their reasoning pathways based on the ontology of
knowledge. We evaluate our method on diverse question \& answering tasks,
especially in medical domains, and show significant improvements over
baselines. We also introduce a new hallucination evaluation benchmark and
analyze the effects of different components of our method. Our results
demonstrate the effectiveness and robustness of our method in merging knowledge
from LLMs and KGs for combined inference. To reproduce our results and extend
the framework further, we make our codebase available at
https://github.com/wyl-willing/MindMap.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.09729v5},
File          = {2308.09729v5.pdf}
}
@article{2311.05662v1,
Author        = {Reham Alharbi and Valentina Tamma and Floriana Grasso and Terry Payne},
Title         = {An Experiment in Retrofitting Competency Questions for Existing
  Ontologies},
Eprint        = {2311.05662v1},
DOI           = {10.1145/3605098.3636053},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Competency Questions (CQs) are a form of ontology functional requirements
expressed as natural language questions. Inspecting CQs together with the
axioms in an ontology provides critical insights into the intended scope and
applicability of the ontology. CQs also underpin a number of tasks in the
development of ontologies e.g. ontology reuse, ontology testing, requirement
specification, and the definition of patterns that implement such requirements.
Although CQs are integral to the majority of ontology engineering
methodologies, the practice of publishing CQs alongside the ontological
artefacts is not widely observed by the community. In this context, we present
an experiment in retrofitting CQs from existing ontologies. We propose
RETROFIT-CQs, a method to extract candidate CQs directly from ontologies using
Generative AI. In the paper we present the pipeline that facilitates the
extraction of CQs by leveraging Large Language Models (LLMs) and we discuss its
application to a number of existing ontologies.},
Year          = {2023},
Month         = {Nov},
Note          = {2024},
Url           = {http://arxiv.org/abs/2311.05662v1},
File          = {2311.05662v1.pdf}
}
@article{2204.13931v1,
Author        = {Sven Hertling and Jan Portisch and Heiko Paulheim},
Title         = {KERMIT -- A Transformer-Based Approach for Knowledge Graph Matching},
Eprint        = {2204.13931v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {One of the strongest signals for automated matching of knowledge graphs and
ontologies are textual concept descriptions. With the rise of transformer-based
language models, text comparison based on meaning (rather than lexical
features) is available to researchers. However, performing pairwise comparisons
of all textual descriptions of concepts in two knowledge graphs is expensive
and scales quadratically (or even worse if concepts have more than one
description). To overcome this problem, we follow a two-step approach: we first
generate matching candidates using a pre-trained sentence transformer (so
called bi-encoder). In a second step, we use fine-tuned transformer
cross-encoders to generate the best candidates. We evaluate our approach on
multiple datasets and show that it is feasible and produces competitive
results.},
Year          = {2022},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2204.13931v1},
File          = {2204.13931v1.pdf}
}
@article{2409.20010v1,
Author        = {Frank Wawrzik and Matthias Plaue and Savan Vekariya and Christoph Grimm},
Title         = {Customized Information and Domain-centric Knowledge Graph Construction
  with Large Language Models},
Eprint        = {2409.20010v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In this paper we propose a novel approach based on knowledge graphs to
provide timely access to structured information, to enable actionable
technology intelligence, and improve cyber-physical systems planning. Our
framework encompasses a text mining process, which includes information
retrieval, keyphrase extraction, semantic network creation, and topic map
visualization. Following this data exploration process, we employ a selective
knowledge graph construction (KGC) approach supported by an electronics and
innovation ontology-backed pipeline for multi-objective decision-making with a
focus on cyber-physical systems. We apply our methodology to the domain of
automotive electrical systems to demonstrate the approach, which is scalable.
Our results demonstrate that our construction process outperforms GraphGPT as
well as our bi-LSTM and transformer REBEL with a pre-defined dataset by several
times in terms of class recognition, relationship construction and correct
"sublass of" categorization. Additionally, we outline reasoning applications
and provide a comparison with Wikidata to show the differences and advantages
of the approach.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.20010v1},
File          = {2409.20010v1.pdf}
}
@article{2312.06355v3,
Author        = {L. Siddharth and Jianxi Luo},
Title         = {Linguistic and Structural Basis of Engineering Design Knowledge},
Eprint        = {2312.06355v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Natural language artefact descriptions are primary carriers of engineering
design knowledge, whose retrieval, representation, and reuse are fundamental to
supporting knowledge-intensive tasks in the design process. In this paper, we
explicate design knowledge from patented artefact descriptions as knowledge
graphs and examine these to understand the linguistic and structural basis. The
purpose of our work is to advance the traditional and ontological perspectives
of design knowledge and to guide Large-Language Models (LLMs) on how to
articulate natural language responses that reflect knowledge that is valuable
in a design environment. We populate 33,881 knowledge graphs from a sample of
patents stratified according to technology classes. For linguistic basis, we
conduct Zipf distribution analyses on the frequencies of unique entities and
relationships to identify 64 and 37 generalisable linguistic syntaxes
respectively. The relationships largely represent attributes ('of'), structure
('in', 'with'), purpose ('to', 'for'), hierarchy ('include'), exemplification
('such as'), and behaviour ('to', 'from'). For structural basis, we draw
inspiration from various studies on biological/ecological networks and discover
motifs from patent knowledge graphs. We identify four 3-node and four 4-node
subgraph patterns that could be converged and simplified into sequence
[->...->], aggregation [->...<-], and hierarchy [<-...->]. Based on these
results, we suggest concretisation strategies for entities and relationships
and explicating hierarchical structures, potentially aiding the construction
and modularisation of design knowledge.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.06355v3},
File          = {2312.06355v3.pdf}
}
@article{2311.03837v1,
Author        = {Sven Hertling and Heiko Paulheim},
Title         = {OLaLa: Ontology Matching with Large Language Models},
Eprint        = {2311.03837v1},
DOI           = {10.1145/3587259.3627571},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Ontology (and more generally: Knowledge Graph) Matching is a challenging task
where information in natural language is one of the most important signals to
process. With the rise of Large Language Models, it is possible to incorporate
this knowledge in a better way into the matching pipeline. A number of
decisions still need to be taken, e.g., how to generate a prompt that is useful
to the model, how information in the KG can be formulated in prompts, which
Large Language Model to choose, how to provide existing correspondences to the
model, how to generate candidates, etc. In this paper, we present a prototype
that explores these questions by applying zero-shot and few-shot prompting with
multiple open Large Language Models to different tasks of the Ontology
Alignment Evaluation Initiative (OAEI). We show that with only a handful of
examples and a well-designed prompt, it is possible to achieve results that are
en par with supervised matching systems which use a much larger portion of the
ground truth.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.03837v1},
File          = {2311.03837v1.pdf}
}
@article{2410.19955v1,
Author        = {Pengfei Hu and Chang Lu and Fei Wang and Yue Ning},
Title         = {DualMAR: Medical-Augmented Representation from Dual-Expertise
  Perspectives},
Eprint        = {2410.19955v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Electronic Health Records (EHR) has revolutionized healthcare data management
and prediction in the field of AI and machine learning. Accurate predictions of
diagnosis and medications significantly mitigate health risks and provide
guidance for preventive care. However, EHR driven models often have limited
scope on understanding medical-domain knowledge and mostly rely on
simple-and-sole ontologies. In addition, due to the missing features and
incomplete disease coverage of EHR, most studies only focus on basic analysis
on conditions and medication. We propose DualMAR, a framework that enhances EHR
prediction tasks through both individual observation data and public knowledge
bases. First, we construct a bi-hierarchical Diagnosis Knowledge Graph (KG)
using verified public clinical ontologies and augment this KG via Large
Language Models (LLMs); Second, we design a new proxy-task learning on lab
results in EHR for pretraining, which further enhance KG representation and
patient embeddings. By retrieving radial and angular coordinates upon polar
space, DualMAR enables accurate predictions based on rich hierarchical and
semantic embeddings from KG. Experiments also demonstrate that DualMAR
outperforms state-of-the-art models, validating its effectiveness in EHR
prediction and KG integration in medical domains.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.19955v1},
File          = {2410.19955v1.pdf}
}
@article{2302.02231v2,
Author        = {Kian Ahrabian and Xinwei Du and Richard Delwin Myloth and Arun Baalaaji Sankar Ananthan and Jay Pujara},
Title         = {PubGraph: A Large-Scale Scientific Knowledge Graph},
Eprint        = {2302.02231v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Research publications are the primary vehicle for sharing scientific progress
in the form of new discoveries, methods, techniques, and insights.
Unfortunately, the lack of a large-scale, comprehensive, and easy-to-use
resource capturing the myriad relationships between publications, their
authors, and venues presents a barrier to applications for gaining a deeper
understanding of science. In this paper, we present PubGraph, a new resource
for studying scientific progress that takes the form of a large-scale knowledge
graph (KG) with more than 385M entities, 13B main edges, and 1.5B qualifier
edges. PubGraph is comprehensive and unifies data from various sources,
including Wikidata, OpenAlex, and Semantic Scholar, using the Wikidata
ontology. Beyond the metadata available from these sources, PubGraph includes
outputs from auxiliary community detection algorithms and large language
models. To further support studies on reasoning over scientific networks, we
create several large-scale benchmarks extracted from PubGraph for the core task
of knowledge graph completion (KGC). These benchmarks present many challenges
for knowledge graph embedding models, including an adversarial community-based
KGC evaluation setting, zero-shot inductive learning, and large-scale learning.
All of the aforementioned resources are accessible at https://pubgraph.isi.edu/
and released under the CC-BY-SA license. We plan to update PubGraph quarterly
to accommodate the release of new publications.},
Year          = {2023},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2302.02231v2},
File          = {2302.02231v2.pdf}
}
@article{2310.19998v1,
Author        = {Markus J. Buehler},
Title         = {Generative retrieval-augmented ontologic graph and multi-agent
  strategies for interpretive large language model-based materials design},
Eprint        = {2310.19998v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Transformer neural networks show promising capabilities, in particular for
uses in materials analysis, design and manufacturing, including their capacity
to work effectively with both human language, symbols, code, and numerical
data. Here we explore the use of large language models (LLMs) as a tool that
can support engineering analysis of materials, applied to retrieving key
information about subject areas, developing research hypotheses, discovery of
mechanistic relationships across disparate areas of knowledge, and writing and
executing simulation codes for active knowledge generation based on physical
ground truths. When used as sets of AI agents with specific features,
capabilities, and instructions, LLMs can provide powerful problem solution
strategies for applications in analysis and design problems. Our experiments
focus on using a fine-tuned model, MechGPT, developed based on training data in
the mechanics of materials domain. We first affirm how finetuning endows LLMs
with reasonable understanding of domain knowledge. However, when queried
outside the context of learned matter, LLMs can have difficulty to recall
correct information. We show how this can be addressed using
retrieval-augmented Ontological Knowledge Graph strategies that discern how the
model understands what concepts are important and how they are related.
Illustrated for a use case of relating distinct areas of knowledge - here,
music and proteins - such strategies can also provide an interpretable graph
structure with rich information at the node, edge and subgraph level. We
discuss nonlinear sampling strategies and agent-based modeling applied to
complex question answering, code generation and execution in the context of
automated force field development from actively learned Density Functional
Theory (DFT) modeling, and data analysis.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.19998v1},
File          = {2310.19998v1.pdf}
}
@article{2003.01163v1,
Author        = {Chen Jiang and Masood Dehghan and Martin Jagersand},
Title         = {Understanding Contexts Inside Robot and Human Manipulation Tasks through
  a Vision-Language Model and Ontology System in a Video Stream},
Eprint        = {2003.01163v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Manipulation tasks in daily life, such as pouring water, unfold intentionally
under specialized manipulation contexts. Being able to process contextual
knowledge in these Activities of Daily Living (ADLs) over time can help us
understand manipulation intentions, which are essential for an intelligent
robot to transition smoothly between various manipulation actions. In this
paper, to model the intended concepts of manipulation, we present a vision
dataset under a strictly constrained knowledge domain for both robot and human
manipulations, where manipulation concepts and relations are stored by an
ontology system in a taxonomic manner. Furthermore, we propose a scheme to
generate a combination of visual attentions and an evolving knowledge graph
filled with commonsense knowledge. Our scheme works with real-world camera
streams and fuses an attention-based Vision-Language model with the ontology
system. The experimental results demonstrate that the proposed scheme can
successfully represent the evolution of an intended object manipulation
procedure for both robots and humans. The proposed scheme allows the robot to
mimic human-like intentional behaviors by watching real-time videos. We aim to
develop this scheme further for real-world robot intelligence in Human-Robot
Interaction.},
Year          = {2020},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2003.01163v1},
File          = {2003.01163v1.pdf}
}
@article{2403.00953v4,
Author        = {Lang Cao and Jimeng Sun and Adam Cross},
Title         = {AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge
  Graph Construction Based on Ontologies-enhanced Large Language Models},
Eprint        = {2403.00953v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Rare diseases affect millions worldwide but often face limited research focus
due to their low prevalence. This results in prolonged diagnoses and a lack of
approved therapies. Recent advancements in Large Language Models (LLMs) have
shown promise in automating the extraction of medical information, offering
potential to improve medical diagnosis and management. However, most LLMs lack
professional medical knowledge, especially concerning rare diseases, and
struggle to handle the latest rare disease information. They also cannot
effectively manage rare disease data and are not directly suitable for
diagnosis and management tasks. Our objective is to create an end-to-end system
called AutoRD, which automates the extraction of information from medical texts
about rare diseases, focusing on entities and their relations. AutoRD
integrates up-to-date structured knowledge and demonstrates superior
performance in rare disease extraction tasks. We conduct various experiments to
evaluate AutoRD's performance, aiming to surpass common LLMs and traditional
methods.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.00953v4},
File          = {2403.00953v4.pdf}
}
@article{2410.02721v1,
Author        = {Ryan C. Barron and Ves Grantcharov and Selma Wanna and Maksim E. Eren and Manish Bhattarai and Nicholas Solovyev and George Tompkins and Charles Nicholas and Kim Ø. Rasmussen and Cynthia Matuszek and Boian S. Alexandrov},
Title         = {Domain-Specific Retrieval-Augmented Generation Using Vector Stores,
  Knowledge Graphs, and Tensor Factorization},
Eprint        = {2410.02721v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) are pre-trained on large-scale corpora and excel
in numerous general natural language processing (NLP) tasks, such as question
answering (QA). Despite their advanced language capabilities, when it comes to
domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,
knowledge cut-offs, and lack of knowledge attributions. Additionally, fine
tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and
time consuming process. The retrieval-augmented generation (RAG) process has
recently emerged as a method capable of optimization of LLM responses, by
referencing them to a predetermined ontology. It was shown that using a
Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into
account relevant sub-graphs that preserve the information in a structured
manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM
framework, that integrates RAG with KG and a vector store (VS) that store
factual domain specific information. Importantly, to avoid hallucinations in
the KG, we build these highly domain-specific KGs and VSs without the use of
LLMs, but via NLP, data mining, and nonnegative tensor factorization with
automatic model selection. Pairing our RAG with a domain-specific: (i) KG
(containing structured information), and (ii) VS (containing unstructured
information) enables the development of domain-specific chat-bots that
attribute the source of information, mitigate hallucinations, lessen the need
for fine-tuning, and excel in highly domain-specific question answering tasks.
We pair SMART-SLIC with chain-of-thought prompting agents. The framework is
designed to be generalizable to adapt to any specific or specialized domain. In
this paper, we demonstrate the question answering capabilities of our framework
on a corpus of scientific publications on malware analysis and anomaly
detection.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.02721v1},
File          = {2410.02721v1.pdf}
}
@article{2309.12132v1,
Author        = {Chunmo Zheng and Saika Wong and Xing Su and Yinqiu Tang},
Title         = {A knowledge representation approach for construction contract knowledge
  modeling},
Eprint        = {2309.12132v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The emergence of large language models (LLMs) presents an unprecedented
opportunity to automate construction contract management, reducing human errors
and saving significant time and costs. However, LLMs may produce convincing yet
inaccurate and misleading content due to a lack of domain expertise. To address
this issue, expert-driven contract knowledge can be represented in a structured
manner to constrain the automatic contract management process. This paper
introduces the Nested Contract Knowledge Graph (NCKG), a knowledge
representation approach that captures the complexity of contract knowledge
using a nested structure. It includes a nested knowledge representation
framework, a NCKG ontology built on the framework, and an implementation
method. Furthermore, we present the LLM-assisted contract review pipeline
enhanced with external knowledge in NCKG. Our pipeline achieves a promising
performance in contract risk reviewing, shedding light on the combination of
LLM and KG towards more reliable and interpretable contract management.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.12132v1},
File          = {2309.12132v1.pdf}
}
@article{2312.13881v1,
Author        = {Juraj Vladika and Alexander Fichtl and Florian Matthes},
Title         = {Diversifying Knowledge Enhancement of Biomedical Language Models using
  Adapter Modules and Knowledge Graphs},
Eprint        = {2312.13881v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent advances in natural language processing (NLP) owe their success to
pre-training language models on large amounts of unstructured data. Still,
there is an increasing effort to combine the unstructured nature of LMs with
structured knowledge and reasoning. Particularly in the rapidly evolving field
of biomedical NLP, knowledge-enhanced language models (KELMs) have emerged as
promising tools to bridge the gap between large language models and
domain-specific knowledge, considering the available biomedical knowledge
graphs (KGs) curated by experts over the decades. In this paper, we develop an
approach that uses lightweight adapter modules to inject structured biomedical
knowledge into pre-trained language models (PLMs). We use two large KGs, the
biomedical knowledge system UMLS and the novel biochemical ontology OntoChem,
with two prominent biomedical PLMs, PubMedBERT and BioLinkBERT. The approach
includes partitioning knowledge graphs into smaller subgraphs, fine-tuning
adapter modules for each subgraph, and combining the knowledge in a fusion
layer. We test the performance on three downstream tasks: document
classification,question answering, and natural language inference. We show that
our methodology leads to performance improvements in several instances while
keeping requirements in computing power low. Finally, we provide a detailed
interpretation of the results and report valuable insights for future work.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.13881v1},
File          = {2312.13881v1.pdf}
}
@article{2404.15923v1,
Author        = {Jack Boylan and Shashank Mangla and Dominic Thorn and Demian Gholipour Ghalandari and Parsa Ghaffari and Chris Hokamp},
Title         = {KGValidator: A Framework for Automatic Validation of Knowledge Graph
  Construction},
Eprint        = {2404.15923v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This study explores the use of Large Language Models (LLMs) for automatic
evaluation of knowledge graph (KG) completion models. Historically, validating
information in KGs has been a challenging task, requiring large-scale human
annotation at prohibitive cost. With the emergence of general-purpose
generative AI and LLMs, it is now plausible that human-in-the-loop validation
could be replaced by a generative agent. We introduce a framework for
consistency and validation when using generative models to validate knowledge
graphs. Our framework is based upon recent open-source developments for
structural and semantic validation of LLM outputs, and upon flexible approaches
to fact checking and verification, supported by the capacity to reference
external knowledge sources of any kind. The design is easy to adapt and extend,
and can be used to verify any kind of graph-structured data through a
combination of model-intrinsic knowledge, user-supplied context, and agents
capable of external knowledge retrieval.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.15923v1},
File          = {2404.15923v1.pdf}
}
@article{2407.02528v1,
Author        = {Romy Fieblinger and Md Tanvirul Alam and Nidhi Rastogi},
Title         = {Actionable Cyber Threat Intelligence using Knowledge Graphs and Large
  Language Models},
Eprint        = {2407.02528v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Cyber threats are constantly evolving. Extracting actionable insights from
unstructured Cyber Threat Intelligence (CTI) data is essential to guide
cybersecurity decisions. Increasingly, organizations like Microsoft, Trend
Micro, and CrowdStrike are using generative AI to facilitate CTI extraction.
This paper addresses the challenge of automating the extraction of actionable
CTI using advancements in Large Language Models (LLMs) and Knowledge Graphs
(KGs). We explore the application of state-of-the-art open-source LLMs,
including the Llama 2 series, Mistral 7B Instruct, and Zephyr for extracting
meaningful triples from CTI texts. Our methodology evaluates techniques such as
prompt engineering, the guidance framework, and fine-tuning to optimize
information extraction and structuring. The extracted data is then utilized to
construct a KG, offering a structured and queryable representation of threat
intelligence. Experimental results demonstrate the effectiveness of our
approach in extracting relevant information, with guidance and fine-tuning
showing superior performance over prompt engineering. However, while our
methods prove effective in small-scale tests, applying LLMs to large-scale data
for KG construction and Link Prediction presents ongoing challenges.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2407.02528v1},
File          = {2407.02528v1.pdf}
}
@article{2408.13366v1,
Author        = {Ekaterina Trofimova and Emil Sataev and Abhijit Singh Jowhari},
Title         = {CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations
  of Research Papers},
Eprint        = {2408.13366v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper presents CodeRefine, a novel framework for automatically
transforming research paper methodologies into functional code using Large
Language Models (LLMs). Our multi-step approach first extracts and summarizes
key text chunks from papers, analyzes their code relevance, and creates a
knowledge graph using a predefined ontology. Code is then generated from this
structured representation and enhanced through a proposed retrospective
retrieval-augmented generation approach. CodeRefine addresses the challenge of
bridging theoretical research and practical implementation, offering a more
accurate alternative to LLM zero-shot prompting. Evaluations on diverse
scientific papers demonstrate CodeRefine's ability to improve code
implementation from the paper, potentially accelerating the adoption of
cutting-edge algorithms in real-world applications.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.13366v1},
File          = {2408.13366v1.pdf}
}
@article{2404.03080v3,
Author        = {Yanpeng Ye and Jie Ren and Shaozhou Wang and Yuwei Wan and Haofen Wang and Imran Razzak and Bram Hoex and Tong Xie and Wenjie Zhang},
Title         = {Construction and Application of Materials Knowledge Graph in
  Multidisciplinary Materials Science via Large Language Model},
Eprint        = {2404.03080v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge in materials science is widely dispersed across extensive
scientific literature, posing significant challenges for efficient discovery
and integration of new materials. Traditional methods, often reliant on costly
and time-consuming experimental approaches, further complicate rapid
innovation. Addressing these challenges, the integration of artificial
intelligence with materials science has opened avenues for accelerating the
discovery process, though it also demands precise annotation, data extraction,
and traceability of information. To tackle these issues, this article
introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural
language processing techniques, integrated with large language models to
extract and systematically organize a decade's worth of high-quality research
into structured triples, contains 162,605 nodes and 731,772 edges. MKG
categorizes information into comprehensive labels such as Name, Formula, and
Application, structured around a meticulously designed ontology, thus enhancing
data usability and integration. By implementing network-based algorithms, MKG
not only facilitates efficient link prediction but also significantly reduces
reliance on traditional experimental methods. This structured approach not only
streamlines materials research but also lays the groundwork for more
sophisticated science knowledge graphs.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.03080v3},
File          = {2404.03080v3.pdf}
}
@article{2309.11669v1,
Author        = {Ali Mousavi and Xin Zhan and He Bai and Peng Shi and Theo Rekatsinas and Benjamin Han and Yunyao Li and Jeff Pound and Josh Susskind and Natalie Schluter and Ihab Ilyas and Navdeep Jaitly},
Title         = {Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic
  Evaluation},
Eprint        = {2309.11669v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used
to train forward and reverse neural models that generate text from KG and vice
versa. However models trained on datasets where KG and text pairs are not
equivalent can suffer from more hallucination and poorer recall. In this paper,
we verify this empirically by generating datasets with different levels of
noise and find that noisier datasets do indeed lead to more hallucination. We
argue that the ability of forward and reverse models trained on a dataset to
cyclically regenerate source KG or text is a proxy for the equivalence between
the KG and the text in the dataset. Using cyclic evaluation we find that
manually created WebNLG is much better than automatically created TeKGen and
T-REx. Guided by these observations, we construct a new, improved dataset
called LAGRANGE using heuristics meant to improve equivalence between KG and
text and show the impact of each of the heuristics on cyclic evaluation. We
also construct two synthetic datasets using large language models (LLMs), and
observe that these are conducive to models that perform significantly well on
cyclic generation of text, but less so on cyclic generation of KGs, probably
because of a lack of a consistent underlying ontology.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.11669v1},
File          = {2309.11669v1.pdf}
}
@article{2412.18419v1,
Author        = {Zihan Zhou and Ziyi Zeng and Wenhao Jiang and Yihui Zhu and Jiaxin Mao and Yonggui Yuan and Min Xia and Shubin Zhao and Mengyu Yao and Yunqian Chen},
Title         = {Research on the Proximity Relationships of Psychosomatic Disease
  Knowledge Graph Modules Extracted by Large Language Models},
Eprint        = {2412.18419v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {As social changes accelerate, the incidence of psychosomatic disorders has
significantly increased, becoming a major challenge in global health issues.
This necessitates an innovative knowledge system and analytical methods to aid
in diagnosis and treatment. Here, we establish the ontology model and entity
types, using the BERT model and LoRA-tuned LLM for named entity recognition,
constructing the knowledge graph with 9668 triples. Next, by analyzing the
network distances between disease, symptom, and drug modules, it was found that
closer network distances among diseases can predict greater similarities in
their clinical manifestations, treatment approaches, and psychological
mechanisms, and closer distances between symptoms indicate that they are more
likely to co-occur. Lastly, by comparing the proximity d and proximity z score,
it was shown that symptom-disease pairs in primary diagnostic relationships
have a stronger association and are of higher referential value than those in
diagnostic relationships. The research results revealed the potential
connections between diseases, co-occurring symptoms, and similarities in
treatment strategies, providing new perspectives for the diagnosis and
treatment of psychosomatic disorders and valuable information for future mental
health research and practice.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.18419v1},
File          = {2412.18419v1.pdf}
}
@article{1909.07459v2,
Author        = {Chen Jiang and Martin Jagersand},
Title         = {Bridging Visual Perception with Contextual Semantics for Understanding
  Robot Manipulation Tasks},
Eprint        = {1909.07459v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Understanding manipulation scenarios allows intelligent robots to plan for
appropriate actions to complete a manipulation task successfully. It is
essential for intelligent robots to semantically interpret manipulation
knowledge by describing entities, relations and attributes in a structural
manner. In this paper, we propose an implementing framework to generate
high-level conceptual dynamic knowledge graphs from video clips. A combination
of a Vision-Language model and an ontology system, in correspondence with
visual perception and contextual semantics, is used to represent robot
manipulation knowledge with Entity-Relation-Entity (E-R-E) and
Entity-Attribute-Value (E-A-V) tuples. The proposed method is flexible and
well-versed. Using the framework, we present a case study where robot performs
manipulation actions in a kitchen environment, bridging visual perception with
contextual semantics using the generated dynamic knowledge graphs.},
Year          = {2019},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1909.07459v2},
File          = {1909.07459v2.pdf}
}
@article{2210.17340v1,
Author        = {Vineeth Venugopal and Sumit Pai and Elsa Olivetti},
Title         = {MatKG: The Largest Knowledge Graph in Materials Science -- Entities,
  Relations, and Link Prediction through Graph Representation Learning},
Eprint        = {2210.17340v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cond-mat.mtrl-sci},
Abstract      = {This paper introduces MatKG, a novel graph database of key concepts in
material science spanning the traditional
material-structure-property-processing paradigm. MatKG is autonomously
generated through transformer-based, large language models and generates pseudo
ontological schema through statistical co-occurrence mapping. At present, MatKG
contains over 2 million unique relationship triples derived from 80,000
entities. This allows the curated analysis, querying, and visualization of
materials knowledge at unique resolution and scale. Further, Knowledge Graph
Embedding models are used to learn embedding representations of nodes in the
graph which are used for downstream tasks such as link prediction and entity
disambiguation. MatKG allows the rapid dissemination and assimilation of data
when used as a knowledge base, while enabling the discovery of new relations
when trained as an embedding model.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.17340v1},
File          = {2210.17340v1.pdf}
}
@article{2310.08365v2,
Author        = {Md. Rezaul Karim and Lina Molinas Comet and Md Shajalal and Oya Deniz Beyan and Dietrich Rebholz-Schuhmann and Stefan Decker},
Title         = {From Large Language Models to Knowledge Graphs for Biomarker Discovery
  in Cancer},
Eprint        = {2310.08365v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Domain experts often rely on most recent knowledge for apprehending and
disseminating specific biological processes that help them design strategies
for developing prevention and therapeutic decision-making in various disease
scenarios. A challenging scenarios for artificial intelligence (AI) is using
biomedical data (e.g., texts, imaging, omics, and clinical) to provide
diagnosis and treatment recommendations for cancerous conditions.~Data and
knowledge about biomedical entities like cancer, drugs, genes, proteins, and
their mechanism is spread across structured (knowledge bases (KBs)) and
unstructured (e.g., scientific articles) sources. A large-scale knowledge graph
(KG) can be constructed by integrating and extracting facts about semantically
interrelated entities and relations. Such a KG not only allows exploration and
question answering (QA) but also enables domain experts to deduce new
knowledge. However, exploring and querying large-scale KGs is tedious for
non-domain users due to their lack of understanding of the data assets and
semantic technologies. In this paper, we develop a domain KG to leverage
cancer-specific biomarker discovery and interactive QA. For this, we
constructed a domain ontology called OncoNet Ontology (ONO), which enables
semantic reasoning for validating gene-disease (different types of cancer)
relations. The KG is further enriched by harmonizing the ONO, metadata,
controlled vocabularies, and biomedical concepts from scientific articles by
employing BioBERT- and SciBERT-based information extractors. Further, since the
biomedical domain is evolving, where new findings often replace old ones,
without having access to up-to-date scientific findings, there is a high chance
an AI system exhibits concept drift while providing diagnosis and treatment.
Therefore, we fine-tune the KG using large language models (LLMs) based on more
recent articles and KBs.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.08365v2},
File          = {2310.08365v2.pdf}
}
@article{2410.14763v1,
Author        = {Hamed Fayyaz and Raphael Poulain and Rahmatollah Beheshti},
Title         = {Enabling Scalable Evaluation of Bias Patterns in Medical LLMs},
Eprint        = {2410.14763v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have shown impressive potential in helping with
numerous medical challenges. Deploying LLMs in high-stakes applications such as
medicine, however, brings in many concerns. One major area of concern relates
to biased behaviors of LLMs in medical applications, leading to unfair
treatment of individuals. To pave the way for the responsible and impactful
deployment of Med LLMs, rigorous evaluation is a key prerequisite. Due to the
huge complexity and variability of different medical scenarios, existing work
in this domain has primarily relied on using manually crafted datasets for bias
evaluation. In this study, we present a new method to scale up such bias
evaluations by automatically generating test cases based on rigorous medical
evidence. We specifically target the challenges of a) domain-specificity of
bias characterization, b) hallucinating while generating the test cases, and c)
various dependencies between the health outcomes and sensitive attributes. To
that end, we offer new methods to address these challenges integrated with our
generative pipeline, using medical knowledge graphs, medical ontologies, and
customized general LLM evaluation frameworks in our method. Through a series of
extensive experiments, we show that the test cases generated by our proposed
method can effectively reveal bias patterns in Med LLMs at larger and more
flexible scales than human-crafted datasets. We publish a large bias evaluation
dataset using our pipeline, which is dedicated to a few medical case studies. A
live demo of our application for vignette generation is available at
https://vignette.streamlit.app. Our code is also available at
https://github.com/healthylaife/autofair.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.14763v1},
File          = {2410.14763v1.pdf}
}
@article{2302.03905v1,
Author        = {Chengyue Jiang and Yong Jiang and Weiqi Wu and Yuting Zheng and Pengjun Xie and Kewei Tu},
Title         = {COMBO: A Complete Benchmark for Open KG Canonicalization},
Eprint        = {2302.03905v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Open knowledge graph (KG) consists of (subject, relation, object) triples
extracted from millions of raw text. The subject and object noun phrases and
the relation in open KG have severe redundancy and ambiguity and need to be
canonicalized. Existing datasets for open KG canonicalization only provide gold
entity-level canonicalization for noun phrases. In this paper, we present
COMBO, a Complete Benchmark for Open KG canonicalization. Compared with
existing datasets, we additionally provide gold canonicalization for relation
phrases, gold ontology-level canonicalization for noun phrases, as well as
source sentences from which triples are extracted. We also propose metrics for
evaluating each type of canonicalization. On the COMBO dataset, we empirically
compare previously proposed canonicalization methods as well as a few simple
baseline methods based on pretrained language models. We find that properly
encoding the phrases in a triple using pretrained language models results in
better relation canonicalization and ontology-level canonicalization of the
noun phrase. We release our dataset, baselines, and evaluation scripts at
https://github.com/jeffchy/COMBO/tree/main.},
Year          = {2023},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2302.03905v1},
File          = {2302.03905v1.pdf}
}
@article{2403.03008v1,
Author        = {Hasan Abu-Rasheed and Christian Weber and Madjid Fathi},
Title         = {Knowledge Graphs as Context Sources for LLM-Based Explanations of
  Learning Recommendations},
Eprint        = {2403.03008v1},
DOI           = {10.1109/EDUCON60312.2024.10578654},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In the era of personalized education, the provision of comprehensible
explanations for learning recommendations is of a great value to enhance the
learner's understanding and engagement with the recommended learning content.
Large language models (LLMs) and generative AI in general have recently opened
new doors for generating human-like explanations, for and along learning
recommendations. However, their precision is still far away from acceptable in
a sensitive field like education. To harness the abilities of LLMs, while still
ensuring a high level of precision towards the intent of the learners, this
paper proposes an approach to utilize knowledge graphs (KG) as a source of
factual context, for LLM prompts, reducing the risk of model hallucinations,
and safeguarding against wrong or imprecise information, while maintaining an
application-intended learning context. We utilize the semantic relations in the
knowledge graph to offer curated knowledge about learning recommendations. With
domain-experts in the loop, we design the explanation as a textual template,
which is filled and completed by the LLM. Domain experts were integrated in the
prompt engineering phase as part of a study, to ensure that explanations
include information that is relevant to the learner. We evaluate our approach
quantitatively using Rouge-N and Rouge-L measures, as well as qualitatively
with experts and learners. Our results show an enhanced recall and precision of
the generated explanations compared to those generated solely by the GPT model,
with a greatly reduced risk of generating imprecise information in the final
learning explanation.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.03008v1},
File          = {2403.03008v1.pdf}
}
@article{2308.04445v1,
Author        = {Doug Lenat and Gary Marcus},
Title         = {Getting from Generative AI to Trustworthy AI: What LLMs might learn from
  Cyc},
Eprint        = {2308.04445v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Generative AI, the most popular current approach to AI, consists of large
language models (LLMs) that are trained to produce outputs that are plausible,
but not necessarily correct. Although their abilities are often uncanny, they
are lacking in aspects of reasoning, leading LLMs to be less than completely
trustworthy. Furthermore, their results tend to be both unpredictable and
uninterpretable.
  We lay out 16 desiderata for future AI, and discuss an alternative approach
to AI which could theoretically address many of the limitations associated with
current approaches: AI educated with curated pieces of explicit knowledge and
rules of thumb, enabling an inference engine to automatically deduce the
logical entailments of all that knowledge. Even long arguments produced this
way can be both trustworthy and interpretable, since the full step-by-step line
of reasoning is always available, and for each step the provenance of the
knowledge used can be documented and audited. There is however a catch: if the
logical language is expressive enough to fully represent the meaning of
anything we can say in English, then the inference engine runs much too slowly.
That's why symbolic AI systems typically settle for some fast but much less
expressive logic, such as knowledge graphs. We describe how one AI system, Cyc,
has developed ways to overcome that tradeoff and is able to reason in higher
order logic in real time.
  We suggest that any trustworthy general AI will need to hybridize the
approaches, the LLM approach and more formal approach, and lay out a path to
realizing that dream.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2308.04445v1},
File          = {2308.04445v1.pdf}
}
@article{2406.00008v2,
Author        = {Shinnosuke Tanaka and James Barry and Vishnudev Kuruvanthodi and Movina Moses and Maxwell J. Giammona and Nathan Herr and Mohab Elkaref and Geeth De Mel},
Title         = {KnowledgeHub: An end-to-end Tool for Assisted Scientific Discovery},
Eprint        = {2406.00008v2},
DOI           = {10.24963/ijcai.2024/1039},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {This paper describes the KnowledgeHub tool, a scientific literature
Information Extraction (IE) and Question Answering (QA) pipeline. This is
achieved by supporting the ingestion of PDF documents that are converted to
text and structured representations. An ontology can then be constructed where
a user defines the types of entities and relationships they want to capture. A
browser-based annotation tool enables annotating the contents of the PDF
documents according to the ontology. Named Entity Recognition (NER) and
Relation Classification (RC) models can be trained on the resulting annotations
and can be used to annotate the unannotated portion of the documents. A
knowledge graph is constructed from these entity and relation triples which can
be queried to obtain insights from the data. Furthermore, we integrate a suite
of Large Language Models (LLMs) that can be used for QA and summarisation that
is grounded in the included documents via a retrieval component. KnowledgeHub
is a unique tool that supports annotation, IE and QA, which gives the user full
insight into the knowledge discovery pipeline.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2406.00008v2},
File          = {2406.00008v2.pdf}
}
@article{2310.10445v1,
Author        = {Markus J. Buehler},
Title         = {MechGPT, a language-based strategy for mechanics and materials modeling
  that connects knowledge across scales, disciplines and modalities},
Eprint        = {2310.10445v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {For centuries, researchers have sought out ways to connect disparate areas of
knowledge. While early scholars (Galileo, da Vinci, etc.) were experts across
fields, specialization has taken hold later. With the advent of Artificial
Intelligence, we can now explore relationships across areas (e.g.,
mechanics-biology) or disparate domains (e.g., failure mechanics-art). To
achieve this, we use a fine-tuned Large Language Model (LLM), here for a subset
of knowledge in multiscale materials failure. The approach includes the use of
a general-purpose LLM to distill question-answer pairs from raw sources
followed by LLM fine-tuning. The resulting MechGPT LLM foundation model is used
in a series of computational experiments to explore its capacity for knowledge
retrieval, various language tasks, hypothesis generation, and connecting
knowledge across disparate areas. While the model has some ability to recall
knowledge from training, we find that LLMs are particularly useful to extract
structural insights through Ontological Knowledge Graphs. These interpretable
graph structures provide explanatory insights, frameworks for new research
questions, and visual representations of knowledge that also can be used in
retrieval-augmented generation. Three versions of MechGPT are discussed,
featuring different sizes from 13 billion to 70 billion parameters, and
reaching context lengths of more than 10,000 tokens. This provides ample
capacity for sophisticated retrieval augmented strategies, as well as
agent-based modeling where multiple LLMs interact collaboratively and/or
adversarially, the incorporation of new data from the literature or web
searches, as well as multimodality.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.10445v1},
File          = {2310.10445v1.pdf}
}
@article{2404.18542v1,
Author        = {Sven Hertling and Ebrahim Norouzi and Harald Sack},
Title         = {OAEI Machine Learning Dataset for Online Model Generation},
Eprint        = {2404.18542v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Ontology and knowledge graph matching systems are evaluated annually by the
Ontology Alignment Evaluation Initiative (OAEI). More and more systems use
machine learning-based approaches, including large language models. The
training and validation datasets are usually determined by the system developer
and often a subset of the reference alignments are used. This sampling is
against the OAEI rules and makes a fair comparison impossible. Furthermore,
those models are trained offline (a trained and optimized model is packaged
into the matcher) and therefore the systems are specifically trained for those
tasks. In this paper, we introduce a dataset that contains training,
validation, and test sets for most of the OAEI tracks. Thus, online model
learning (the systems must adapt to the given input alignment without human
intervention) is made possible to enable a fair comparison for ML-based
systems. We showcase the usefulness of the dataset by fine-tuning the
confidence thresholds of popular systems.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.18542v1},
File          = {2404.18542v1.pdf}
}
@article{2307.02243v1,
Author        = {Garrett Allen and Gaole He and Ujwal Gadiraju},
Title         = {Power-up! What Can Generative Models Do for Human Computation Workflows?},
Eprint        = {2307.02243v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {We are amidst an explosion of artificial intelligence research, particularly
around large language models (LLMs). These models have a range of applications
across domains like medicine, finance, commonsense knowledge graphs, and
crowdsourcing. Investigation into LLMs as part of crowdsourcing workflows
remains an under-explored space. The crowdsourcing research community has
produced a body of work investigating workflows and methods for managing
complex tasks using hybrid human-AI methods. Within crowdsourcing, the role of
LLMs can be envisioned as akin to a cog in a larger wheel of workflows. From an
empirical standpoint, little is currently understood about how LLMs can improve
the effectiveness of crowdsourcing workflows and how such workflows can be
evaluated. In this work, we present a vision for exploring this gap from the
perspectives of various stakeholders involved in the crowdsourcing paradigm --
the task requesters, crowd workers, platforms, and end-users. We identify
junctures in typical crowdsourcing workflows at which the introduction of LLMs
can play a beneficial role and propose means to augment existing design
patterns for crowd work.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.02243v1},
File          = {2307.02243v1.pdf}
}
@article{2405.01581v1,
Author        = {Nele Köhler and Fabian Neuhaus},
Title         = {The Mercurial Top-Level Ontology of Large Language Models},
Eprint        = {2405.01581v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In our work, we systematize and analyze implicit ontological commitments in
the responses generated by large language models (LLMs), focusing on ChatGPT
3.5 as a case study. We investigate how LLMs, despite having no explicit
ontology, exhibit implicit ontological categorizations that are reflected in
the texts they generate. The paper proposes an approach to understanding the
ontological commitments of LLMs by defining ontology as a theory that provides
a systematic account of the ontological commitments of some text. We
investigate the ontological assumptions of ChatGPT and present a systematized
account, i.e., GPT's top-level ontology. This includes a taxonomy, which is
available as an OWL file, as well as a discussion about ontological assumptions
(e.g., about its mereology or presentism). We show that in some aspects GPT's
top-level ontology is quite similar to existing top-level ontologies. However,
there are significant challenges arising from the flexible nature of
LLM-generated texts, including ontological overload, ambiguity, and
inconsistency.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2405.01581v1},
File          = {2405.01581v1.pdf}
}
@article{2502.05478v1,
Author        = {Zhiqiang Liu and Chengtao Gan and Junjie Wang and Yichi Zhang and Zhongpu Bo and Mengshu Sun and Huajun Chen and Wen Zhang},
Title         = {OntoTune: Ontology-Driven Self-training for Aligning Large Language
  Models},
Eprint        = {2502.05478v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Existing domain-specific Large Language Models (LLMs) are typically developed
by fine-tuning general-purposed LLMs with large-scale domain-specific corpora.
However, training on large-scale corpora often fails to effectively organize
domain knowledge of LLMs, leading to fragmented understanding. Inspired by how
humans connect concepts and organize knowledge through mind maps, we aim to
emulate this approach by using ontology with hierarchical conceptual knowledge
to reorganize LLM's domain knowledge. From this perspective, we propose an
ontology-driven self-training framework called OntoTune, which aims to align
LLMs with ontology through in-context learning, enabling the generation of
responses guided by the ontology. We leverage in-context learning to identify
whether the LLM has acquired the specific concept's ontology knowledge, and
select the entries not yet mastered by LLM as the training set to further align
the LLM with ontology. Compared to existing domain LLMs based on newly
collected large-scale domain-specific corpora, our OntoTune, which relies on
the existing, long-term developed ontology and LLM itself, significantly
reduces data maintenance costs and offers improved generalization ability. We
conduct our study in the medical domain to evaluate the effectiveness of
OntoTune, utilizing a standardized medical ontology, SNOMED CT as our ontology
source. Experimental results demonstrate that OntoTune achieves
state-of-the-art performance in both in-ontology task hypernym discovery and
out-of-ontology task medical domain QA. Moreover, compared to the latest direct
ontology injection method TaxoLLaMA, our OntoTune better preserves original
knowledge of LLM. The code and data are available at
https://github.com/zjukg/OntoTune.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.05478v1},
File          = {2502.05478v1.pdf}
}
@article{2404.19729v1,
Author        = {Steph Buongiorno and Corey Clark},
Title         = {A Framework for Leveraging Human Computation Gaming to Enhance Knowledge
  Graphs for Accuracy Critical Generative AI Applications},
Eprint        = {2404.19729v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {External knowledge graphs (KGs) can be used to augment large language models
(LLMs), while simultaneously providing an explainable knowledge base of facts
that can be inspected by a human. This approach may be particularly valuable in
domains where explainability is critical, like human trafficking data analysis.
However, creating KGs can pose challenges. KGs parsed from documents may
comprise explicit connections (those directly stated by a document) but miss
implicit connections (those obvious to a human although not directly stated).
To address these challenges, this preliminary research introduces the GAME-KG
framework, standing for "Gaming for Augmenting Metadata and Enhancing Knowledge
Graphs." GAME-KG is a federated approach to modifying explicit as well as
implicit connections in KGs by using crowdsourced feedback collected through
video games. GAME-KG is shown through two demonstrations: a Unity test scenario
from Dark Shadows, a video game that collects feedback on KGs parsed from US
Department of Justice (DOJ) Press Releases on human trafficking, and a
following experiment where OpenAI's GPT-4 is prompted to answer questions based
on a modified and unmodified KG. Initial results suggest that GAME-KG can be an
effective framework for enhancing KGs, while simultaneously providing an
explainable set of structured facts verified by humans.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.19729v1},
File          = {2404.19729v1.pdf}
}
@article{2306.11489v2,
Author        = {Linyao Yang and Hongyang Chen and Zhao Li and Xiao Ding and Xindong Wu},
Title         = {Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs
  for Fact-aware Language Modeling},
Eprint        = {2306.11489v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recently, ChatGPT, a representative large language model (LLM), has gained
considerable attention due to its powerful emergent abilities. Some researchers
suggest that LLMs could potentially replace structured knowledge bases like
knowledge graphs (KGs) and function as parameterized knowledge bases. However,
while LLMs are proficient at learning probabilistic language patterns based on
large corpus and engaging in conversations with humans, they, like previous
smaller pre-trained language models (PLMs), still have difficulty in recalling
facts while generating knowledge-grounded contents. To overcome these
limitations, researchers have proposed enhancing data-driven PLMs with
knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus
improving their performance to generate texts requiring factual knowledge and
providing more informed responses to user queries. This paper reviews the
studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced
pre-trained language models (KGPLMs) as well as their applications. Inspired by
existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by
developing knowledge graph-enhanced large language models (KGLLMs). KGLLM
provides a solution to enhance LLMs' factual reasoning ability, opening up new
avenues for LLM research.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.11489v2},
File          = {2306.11489v2.pdf}
}
@article{2412.04948v1,
Author        = {Peng Yu and Cheng Deng and Beiya Dai and Xinbing Wang and Ying Wen},
Title         = {KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view
  Knowledge Graph Contrastive Learning},
Eprint        = {2412.04948v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Autoregressive large language models (LLMs) pre-trained by next token
prediction are inherently proficient in generative tasks. However, their
performance on knowledge-driven tasks such as factual knowledge querying
remains unsatisfactory. Knowledge graphs (KGs), as high-quality structured
knowledge bases, can provide reliable knowledge for LLMs, potentially
compensating for their knowledge deficiencies. Aligning LLMs with explicit,
structured knowledge from KGs has been a challenge; previous attempts either
failed to effectively align knowledge representations or compromised the
generative capabilities of LLMs, leading to less-than-optimal outcomes. This
paper proposes \textbf{KaLM}, a \textit{Knowledge-aligned Language Modeling}
approach, which fine-tunes autoregressive LLMs to align with KG knowledge via
the joint objective of explicit knowledge alignment and implicit knowledge
alignment. The explicit knowledge alignment objective aims to directly optimize
the knowledge representation of LLMs through dual-view knowledge graph
contrastive learning. The implicit knowledge alignment objective focuses on
incorporating textual patterns of knowledge into LLMs through triple completion
language modeling. Notably, our method achieves a significant performance boost
in evaluations of knowledge-driven tasks, specifically embedding-based
knowledge graph completion and generation-based knowledge graph question
answering.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.04948v1},
File          = {2412.04948v1.pdf}
}
@article{2401.08517v3,
Author        = {Hasan Abu-Rasheed and Mohamad Hussam Abdulsalam and Christian Weber and Madjid Fathi},
Title         = {Supporting Student Decisions on Learning Recommendations: An LLM-Based
  Chatbot with Knowledge Graph Contextualization for Conversational
  Explainability and Mentoring},
Eprint        = {2401.08517v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Student commitment towards a learning recommendation is not separable from
their understanding of the reasons it was recommended to them; and their
ability to modify it based on that understanding. Among explainability
approaches, chatbots offer the potential to engage the student in a
conversation, similar to a discussion with a peer or a mentor. The capabilities
of chatbots, however, are still not sufficient to replace a human mentor,
despite the advancements of generative AI (GenAI) and large language models
(LLM). Therefore, we propose an approach to utilize chatbots as mediators of
the conversation and sources of limited and controlled generation of
explanations, to harvest the potential of LLMs while reducing their potential
risks at the same time. The proposed LLM-based chatbot supports students in
understanding learning-paths recommendations. We use a knowledge graph (KG) as
a human-curated source of information, to regulate the LLM's output through
defining its prompt's context. A group chat approach is developed to connect
students with human mentors, either on demand or in cases that exceed the
chatbot's pre-defined tasks. We evaluate the chatbot with a user study, to
provide a proof-of-concept and highlight the potential requirements and
limitations of utilizing chatbots in conversational explainability.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.08517v3},
File          = {2401.08517v3.pdf}
}
@article{2410.21237v1,
Author        = {Zhantao Yang and Han Zhang and Fangyi Chen and Anudeepsekhar Bolimera and Marios Savvides},
Title         = {Hierarchical Knowledge Graph Construction from Images for Scalable
  E-Commerce},
Eprint        = {2410.21237v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge Graph (KG) is playing an increasingly important role in various AI
systems. For e-commerce, an efficient and low-cost automated knowledge graph
construction method is the foundation of enabling various successful downstream
applications. In this paper, we propose a novel method for constructing
structured product knowledge graphs from raw product images. The method
cooperatively leverages recent advances in the vision-language model (VLM) and
large language model (LLM), fully automating the process and allowing timely
graph updates. We also present a human-annotated e-commerce product dataset for
benchmarking product property extraction in knowledge graph construction. Our
method outperforms our baseline in all metrics and evaluated properties,
demonstrating its effectiveness and bright usage potential.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.21237v1},
File          = {2410.21237v1.pdf}
}
@article{2404.04264v5,
Author        = {Lihui Liu and Zihao Wang and Ruizhong Qiu and Yikun Ban and Eunice Chan and Yangqiu Song and Jingrui He and Hanghang Tong},
Title         = {Logic Query of Thoughts: Guiding Large Language Models to Answer Complex
  Logic Queries with Knowledge Graphs},
Eprint        = {2404.04264v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Despite the superb performance in many tasks, large language models (LLMs)
bear the risk of generating hallucination or even wrong answers when confronted
with tasks that demand the accuracy of knowledge. The issue becomes even more
noticeable when addressing logic queries that require multiple logic reasoning
steps. On the other hand, knowledge graph (KG) based question answering methods
are capable of accurately identifying the correct answers with the help of
knowledge graph, yet its accuracy could quickly deteriorate when the knowledge
graph itself is sparse and incomplete. It remains a critical challenge on how
to integrate knowledge graph reasoning with LLMs in a mutually beneficial way
so as to mitigate both the hallucination problem of LLMs as well as the
incompleteness issue of knowledge graphs. In this paper, we propose
'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs
with knowledge graph based logic query reasoning. LGOT seamlessly combines
knowledge graph reasoning and LLMs, effectively breaking down complex logic
queries into easy to answer subquestions. Through the utilization of both
knowledge graph reasoning and LLMs, it successfully derives answers for each
subquestion. By aggregating these results and selecting the highest quality
candidate answers for each step, LGOT achieves accurate results to complex
questions. Our experimental findings demonstrate substantial performance
enhancements, with up to 20% improvement over ChatGPT.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2404.04264v5},
File          = {2404.04264v5.pdf}
}
@article{2412.02035v1,
Author        = {Nadeen Fathallah and Steffen Staab and Alsayed Algergawy},
Title         = {LLMs4Life: Large Language Models for Ontology Learning in Life Sciences},
Eprint        = {2412.02035v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology learning in complex domains, such as life sciences, poses
significant challenges for current Large Language Models (LLMs). Existing LLMs
struggle to generate ontologies with multiple hierarchical levels, rich
interconnections, and comprehensive class coverage due to constraints on the
number of tokens they can generate and inadequate domain adaptation. To address
these issues, we extend the NeOn-GPT pipeline for ontology learning using LLMs
with advanced prompt engineering techniques and ontology reuse to enhance the
generated ontologies' domain-specific reasoning and structural depth. Our work
evaluates the capabilities of LLMs in ontology learning in the context of
highly specialized and complex domains such as life science domains. To assess
the logical consistency, completeness, and scalability of the generated
ontologies, we use the AquaDiva ontology developed and used in the
collaborative research center AquaDiva as a case study. Our evaluation shows
the viability of LLMs for ontology learning in specialized domains, providing
solutions to longstanding limitations in model performance and scalability.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.02035v1},
File          = {2412.02035v1.pdf}
}
@article{2406.14757v1,
Author        = {Syed I. Munzir and Daniel B. Hier and Chelsea Oommen and Michael D. Carrithers},
Title         = {A Large Language Model Outperforms Other Computational Approaches to the
  High-Throughput Phenotyping of Physician Notes},
Eprint        = {2406.14757v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {High-throughput phenotyping, the automated mapping of patient signs and
symptoms to standardized ontology concepts, is essential to gaining value from
electronic health records (EHR) in the support of precision medicine. Despite
technological advances, high-throughput phenotyping remains a challenge. This
study compares three computational approaches to high-throughput phenotyping: a
Large Language Model (LLM) incorporating generative AI, a Natural Language
Processing (NLP) approach utilizing deep learning for span categorization, and
a hybrid approach combining word vectors with machine learning. The approach
that implemented GPT-4 (a Large Language Model) demonstrated superior
performance, suggesting that Large Language Models are poised to be the
preferred method for high-throughput phenotyping of physician notes.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.14757v1},
File          = {2406.14757v1.pdf}
}
@article{2408.11975v1,
Author        = {Camila Díaz and Jocelyn Dunstan and Lorena Etcheverry and Antonia Fonck and Alejandro Grez and Domingo Mery and Juan Reutter and Hugo Rojas},
Title         = {Automatic knowledge-graph creation from historical documents: The
  Chilean dictatorship as a case study},
Eprint        = {2408.11975v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {We present our results regarding the automatic construction of a knowledge
graph from historical documents related to the Chilean dictatorship period
(1973-1990). Our approach consists on using LLMs to automatically recognize
entities and relations between these entities, and also to perform resolution
between these sets of values. In order to prevent hallucination, the
interaction with the LLM is grounded in a simple ontology with 4 types of
entities and 7 types of relations. To evaluate our architecture, we use a gold
standard graph constructed using a small subset of the documents, and compare
this to the graph obtained from our approach when processing the same set of
documents. Results show that the automatic construction manages to recognize a
good portion of all the entities in the gold standard, and that those not
recognized are mostly explained by the level of granularity in which the
information is structured in the graph, and not because the automatic approach
misses an important entity in the graph. Looking forward, we expect this report
will encourage work on other similar projects focused on enhancing research in
humanities and social science, but we remark that better evaluation metrics are
needed in order to accurately fine-tune these types of architectures.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.11975v1},
File          = {2408.11975v1.pdf}
}
@article{2410.09244v1,
Author        = {C. Civili and E. Sherkhonov and R. E. K. Stirewalt},
Title         = {Using off-the-shelf LLMs to query enterprise data by progressively
  revealing ontologies},
Eprint        = {2410.09244v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Ontologies are known to improve the accuracy of Large Language Models (LLMs)
when translating natural language queries into a formal query language like SQL
or SPARQL. There are two ways to leverage ontologies when working with LLMs.
One is to fine-tune the model, i.e., to enhance it with specific domain
knowledge. Another is the zero-shot prompting approach, where the ontology is
provided as part of the input question. Unfortunately, modern enterprises
typically have ontologies that are too large to fit in a prompt due to LLM's
token size limitations. We present a solution that incrementally reveals "just
enough" of an ontology that is needed to answer a given question.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.09244v1},
File          = {2410.09244v1.pdf}
}
@article{2406.17231v1,
Author        = {Tong Zhou and Yubo Chen and Kang Liu and Jun Zhao},
Title         = {CogMG: Collaborative Augmentation Between Large Language Model and
  Knowledge Graph},
Eprint        = {2406.17231v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models have become integral to question-answering applications
despite their propensity for generating hallucinations and factually inaccurate
content. Querying knowledge graphs to reduce hallucinations in LLM meets the
challenge of incomplete knowledge coverage in knowledge graphs. On the other
hand, updating knowledge graphs by information extraction and knowledge graph
completion faces the knowledge update misalignment issue. In this work, we
introduce a collaborative augmentation framework, CogMG, leveraging knowledge
graphs to address the limitations of LLMs in QA scenarios, explicitly targeting
the problems of incomplete knowledge coverage and knowledge update
misalignment. The LLMs identify and decompose required knowledge triples that
are not present in the KG, enriching them and aligning updates with real-world
demands. We demonstrate the efficacy of this approach through a supervised
fine-tuned LLM within an agent framework, showing significant improvements in
reducing hallucinations and enhancing factual accuracy in QA responses. Our
code and video are publicly available.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.17231v1},
File          = {2406.17231v1.pdf}
}
@article{2405.17249v2,
Author        = {Vasile Ionut Remus Iga and Gheorghe Cosmin Silaghi},
Title         = {Assessing LLMs Suitability for Knowledge Graph Completion},
Eprint        = {2405.17249v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent work has shown the capability of Large Language Models (LLMs) to solve
tasks related to Knowledge Graphs, such as Knowledge Graph Completion, even in
Zero- or Few-Shot paradigms. However, they are known to hallucinate answers, or
output results in a non-deterministic manner, thus leading to wrongly reasoned
responses, even if they satisfy the user's demands. To highlight opportunities
and challenges in knowledge graphs-related tasks, we experiment with three
distinguished LLMs, namely Mixtral-8x7b-Instruct-v0.1, GPT-3.5-Turbo-0125 and
GPT-4o, on Knowledge Graph Completion for static knowledge graphs, using
prompts constructed following the TELeR taxonomy, in Zero- and One-Shot
contexts, on a Task-Oriented Dialogue system use case. When evaluated using
both strict and flexible metrics measurement manners, our results show that
LLMs could be fit for such a task if prompts encapsulate sufficient information
and relevant examples.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.17249v2},
File          = {2405.17249v2.pdf}
}
@article{2403.11996v3,
Author        = {Markus J. Buehler},
Title         = {Accelerating Scientific Discovery with Generative Knowledge Extraction,
  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning},
Eprint        = {2403.11996v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Leveraging generative Artificial Intelligence (AI), we have transformed a
dataset comprising 1,000 scientific papers into an ontological knowledge graph.
Through an in-depth structural analysis, we have calculated node degrees,
identified communities and connectivities, and evaluated clustering
coefficients and betweenness centrality of pivotal nodes, uncovering
fascinating knowledge architectures. The graph has an inherently scale-free
nature, is highly connected, and can be used for graph reasoning by taking
advantage of transitive and isomorphic properties that reveal unprecedented
interdisciplinary relationships that can be used to answer queries, identify
gaps in knowledge, propose never-before-seen material designs, and predict
material behaviors. We compute deep node embeddings for combinatorial node
similarity ranking for use in a path sampling strategy links dissimilar
concepts that have previously not been related. One comparison revealed
structural parallels between biological materials and Beethoven's 9th Symphony,
highlighting shared patterns of complexity through isomorphic mapping. In
another example, the algorithm proposed a hierarchical mycelium-based composite
based on integrating path sampling with principles extracted from Kandinsky's
'Composition VII' painting. The resulting material integrates an innovative set
of concepts that include a balance of chaos/order, adjustable porosity,
mechanical strength, and complex patterned chemical functionalization. We
uncover other isomorphisms across science, technology and art, revealing a
nuanced ontology of immanence that reveal a context-dependent heterarchical
interplay of constituents. Graph-based generative AI achieves a far higher
degree of novelty, explorative capacity, and technical detail, than
conventional approaches and establishes a widely useful framework for
innovation by revealing hidden connections.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.11996v3},
File          = {2403.11996v3.pdf}
}
@article{2403.15864v1,
Author        = {Yihang Zhao and Neil Vetter and Kaveh Aryan},
Title         = {Using Large Language Models for OntoClean-based Ontology Refinement},
Eprint        = {2403.15864v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper explores the integration of Large Language Models (LLMs) such as
GPT-3.5 and GPT-4 into the ontology refinement process, specifically focusing
on the OntoClean methodology. OntoClean, critical for assessing the
metaphysical quality of ontologies, involves a two-step process of assigning
meta-properties to classes and verifying a set of constraints. Manually
conducting the first step proves difficult in practice, due to the need for
philosophical expertise and lack of consensus among ontologists. By employing
LLMs with two prompting strategies, the study demonstrates that high accuracy
in the labelling process can be achieved. The findings suggest the potential
for LLMs to enhance ontology refinement, proposing the development of plugin
software for ontology tools to facilitate this integration.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.15864v1},
File          = {2403.15864v1.pdf}
}
@article{2403.07311v8,
Author        = {Dong Shu and Tianle Chen and Mingyu Jin and Chong Zhang and Mengnan Du and Yongfeng Zhang},
Title         = {Knowledge Graph Large Language Model (KG-LLM) for Link Prediction},
Eprint        = {2403.07311v8},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The task of multi-hop link prediction within knowledge graphs (KGs) stands as
a challenge in the field of knowledge graph analysis, as it requires the model
to reason through and understand all intermediate connections before making a
prediction. In this paper, we introduce the Knowledge Graph Large Language
Model (KG-LLM), a novel framework that leverages large language models (LLMs)
for knowledge graph tasks. We first convert structured knowledge graph data
into natural language and then use these natural language prompts to fine-tune
LLMs to enhance multi-hop link prediction in KGs. By converting the KG to
natural language prompts, our framework is designed to learn the latent
representations of entities and their interrelations. To show the efficacy of
the KG-LLM Framework, we fine-tune three leading LLMs within this framework,
including Flan-T5, LLaMa2 and Gemma. Further, we explore the framework's
potential to provide LLMs with zero-shot capabilities for handling previously
unseen prompts. Experimental results show that KG-LLM significantly improves
the models' generalization capabilities, leading to more accurate predictions
in unfamiliar scenarios.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.07311v8},
File          = {2403.07311v8.pdf}
}
@article{2312.05209v2,
Author        = {Navapat Nananukul and Mayank Kejriwal},
Title         = {HALO: An Ontology for Representing and Categorizing Hallucinations in
  Large Language Models},
Eprint        = {2312.05209v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Recent progress in generative AI, including large language models (LLMs) like
ChatGPT, has opened up significant opportunities in fields ranging from natural
language processing to knowledge discovery and data mining. However, there is
also a growing awareness that the models can be prone to problems such as
making information up or `hallucinations', and faulty reasoning on seemingly
simple problems. Because of the popularity of models like ChatGPT, both
academic scholars and citizen scientists have documented hallucinations of
several different types and severity. Despite this body of work, a formal model
for describing and representing these hallucinations (with relevant meta-data)
at a fine-grained level, is still lacking. In this paper, we address this gap
by presenting the Hallucination Ontology or HALO, a formal, extensible ontology
written in OWL that currently offers support for six different types of
hallucinations known to arise in LLMs, along with support for provenance and
experimental metadata. We also collect and publish a dataset containing
hallucinations that we inductively gathered across multiple independent Web
sources, and show that HALO can be successfully used to model this dataset and
answer competency questions.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.05209v2},
File          = {2312.05209v2.pdf}
}
@article{2306.04136v1,
Author        = {Jinheon Baek and Alham Fikri Aji and Amir Saffari},
Title         = {Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge
  Graph Question Answering},
Eprint        = {2306.04136v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) are capable of performing zero-shot closed-book
question answering tasks, based on their internal knowledge stored in
parameters during pre-training. However, such internalized knowledge might be
insufficient and incorrect, which could lead LLMs to generate factually wrong
answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive.
To this end, we propose to augment the knowledge directly in the input of LLMs.
Specifically, we first retrieve the relevant facts to the input question from
the knowledge graph based on semantic similarities between the question and its
associated facts. After that, we prepend the retrieved facts to the input
question in the form of the prompt, which is then forwarded to LLMs to generate
the answer. Our framework, Knowledge-Augmented language model PromptING
(KAPING), requires no model training, thus completely zero-shot. We validate
the performance of our KAPING framework on the knowledge graph question
answering task, that aims to answer the user's question based on facts over a
knowledge graph, on which ours outperforms relevant zero-shot baselines by up
to 48% in average, across multiple LLMs of various sizes.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.04136v1},
File          = {2306.04136v1.pdf}
}
@article{2411.00046v1,
Author        = {Harry Caufield and Carlo Kroll and Shawn T O'Neil and Justin T Reese and Marcin P Joachimiak and Harshad Hegde and Nomi L Harris and Madan Krishnamurthy and James A McLaughlin and Damian Smedley and Melissa A Haendel and Peter N Robinson and Christopher J Mungall},
Title         = {CurateGPT: A flexible language-model assisted biocuration tool},
Eprint        = {2411.00046v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Effective data-driven biomedical discovery requires data curation: a
time-consuming process of finding, organizing, distilling, integrating,
interpreting, annotating, and validating diverse information into a structured
form suitable for databases and knowledge bases. Accurate and efficient
curation of these digital assets is critical to ensuring that they are FAIR,
trustworthy, and sustainable. Unfortunately, expert curators face significant
time and resource constraints. The rapid pace of new information being
published daily is exceeding their capacity for curation. Generative AI,
exemplified by instruction-tuned large language models (LLMs), has opened up
new possibilities for assisting human-driven curation. The design philosophy of
agents combines the emerging abilities of generative AI with more precise
methods. A curator's tasks can be aided by agents for performing reasoning,
searching ontologies, and integrating knowledge across external sources, all
efforts otherwise requiring extensive manual effort. Our LLM-driven annotation
tool, CurateGPT, melds the power of generative AI together with trusted
knowledge bases and literature sources. CurateGPT streamlines the curation
process, enhancing collaboration and efficiency in common workflows. Compared
to direct interaction with an LLM, CurateGPT's agents enable access to
information beyond that in the LLM's training data and they provide direct
links to the data supporting each claim. This helps curators, researchers, and
engineers scale up curation efforts to keep pace with the ever-increasing
volume of scientific data.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2411.00046v1},
File          = {2411.00046v1.pdf}
}
@article{2404.14991v2,
Author        = {Rick Du and Huilong An and Keyu Wang and Weidong Liu},
Title         = {A Short Review for Ontology Learning: Stride to Large Language Models
  Trend},
Eprint        = {2404.14991v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Ontologies provide formal representation of knowledge shared within Semantic
Web applications. Ontology learning involves the construction of ontologies
from a given corpus. In the past years, ontology learning has traversed through
shallow learning and deep learning methodologies, each offering distinct
advantages and limitations in the quest for knowledge extraction and
representation. A new trend of these approaches is relying on large language
models (LLMs) to enhance ontology learning. This paper gives a review in
approaches and challenges of ontology learning. It analyzes the methodologies
and limitations of shallow-learning-based and deep-learning-based techniques
for ontology learning, and provides comprehensive knowledge for the frontier
work of using LLMs to enhance ontology learning. In addition, it proposes
several noteworthy future directions for further exploration into the
integration of LLMs with ontology learning tasks.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.14991v2},
File          = {2404.14991v2.pdf}
}
@article{2308.13916v4,
Author        = {Liang Yao and Jiazhen Peng and Chengsheng Mao and Yuan Luo},
Title         = {Exploring Large Language Models for Knowledge Graph Completion},
Eprint        = {2308.13916v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs play a vital role in numerous artificial intelligence tasks,
yet they frequently face the issue of incompleteness. In this study, we explore
utilizing Large Language Models (LLM) for knowledge graph completion. We
consider triples in knowledge graphs as text sequences and introduce an
innovative framework called Knowledge Graph LLM (KG-LLM) to model these
triples. Our technique employs entity and relation descriptions of a triple as
prompts and utilizes the response for predictions. Experiments on various
benchmark knowledge graphs demonstrate that our method attains state-of-the-art
performance in tasks such as triple classification and relation prediction. We
also find that fine-tuning relatively smaller models (e.g., LLaMA-7B,
ChatGLM-6B) outperforms recent ChatGPT and GPT-4.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.13916v4},
File          = {2308.13916v4.pdf}
}
@article{2405.20527v1,
Author        = {Francesco Ronzano and Jay Nanavati},
Title         = {Towards Ontology-Enhanced Representation Learning for Large Language
  Models},
Eprint        = {2405.20527v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Taking advantage of the widespread use of ontologies to organise and
harmonize knowledge across several distinct domains, this paper proposes a
novel approach to improve an embedding-Large Language Model (embedding-LLM) of
interest by infusing the knowledge formalized by a reference ontology:
ontological knowledge infusion aims at boosting the ability of the considered
LLM to effectively model the knowledge domain described by the infused
ontology. The linguistic information (i.e. concept synonyms and descriptions)
and structural information (i.e. is-a relations) formalized by the ontology are
utilized to compile a comprehensive set of concept definitions, with the
assistance of a powerful generative LLM (i.e. GPT-3.5-turbo). These concept
definitions are then employed to fine-tune the target embedding-LLM using a
contrastive learning framework. To demonstrate and evaluate the proposed
approach, we utilize the biomedical disease ontology MONDO. The results show
that embedding-LLMs enhanced by ontological disease knowledge exhibit an
improved capability to effectively evaluate the similarity of in-domain
sentences from biomedical documents mentioning diseases, without compromising
their out-of-domain performance.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.20527v1},
File          = {2405.20527v1.pdf}
}
@article{2311.13314v1,
Author        = {Xinyan Guan and Yanjiang Liu and Hongyu Lin and Yaojie Lu and Ben He and Xianpei Han and Le Sun},
Title         = {Mitigating Large Language Model Hallucinations via Autonomous Knowledge
  Graph-based Retrofitting},
Eprint        = {2311.13314v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Incorporating factual knowledge in knowledge graph is regarded as a promising
approach for mitigating the hallucination of large language models (LLMs).
Existing methods usually only use the user's input to query the knowledge
graph, thus failing to address the factual hallucination generated by LLMs
during its reasoning process. To address this problem, this paper proposes
Knowledge Graph-based Retrofitting (KGR), a new framework that incorporates
LLMs with KGs to mitigate factual hallucination during the reasoning process by
retrofitting the initial draft responses of LLMs based on the factual knowledge
stored in KGs. Specifically, KGR leverages LLMs to extract, select, validate,
and retrofit factual statements within the model-generated responses, which
enables an autonomous knowledge verifying and refining procedure without any
additional manual efforts. Experiments show that KGR can significantly improve
the performance of LLMs on factual QA benchmarks especially when involving
complex reasoning processes, which demonstrates the necessity and effectiveness
of KGR in mitigating hallucination and enhancing the reliability of LLMs.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.13314v1},
File          = {2311.13314v1.pdf}
}
@article{2406.03746v1,
Author        = {Zhouyu Jiang and Ling Zhong and Mengshu Sun and Jun Xu and Rui Sun and Hui Cai and Shuhan Luo and Zhiqiang Zhang},
Title         = {Efficient Knowledge Infusion via KG-LLM Alignment},
Eprint        = {2406.03746v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {To tackle the problem of domain-specific knowledge scarcity within large
language models (LLMs), knowledge graph-retrievalaugmented method has been
proven to be an effective and efficient technique for knowledge infusion.
However, existing approaches face two primary challenges: knowledge mismatch
between public available knowledge graphs and the specific domain of the task
at hand, and poor information compliance of LLMs with knowledge graphs. In this
paper, we leverage a small set of labeled samples and a large-scale corpus to
efficiently construct domain-specific knowledge graphs by an LLM, addressing
the issue of knowledge mismatch. Additionally, we propose a three-stage KG-LLM
alignment strategyto enhance the LLM's capability to utilize information from
knowledge graphs. We conduct experiments with a limited-sample setting on two
biomedical question-answering datasets, and the results demonstrate that our
approach outperforms existing baselines.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.03746v1},
File          = {2406.03746v1.pdf}
}
@article{2411.08696v1,
Author        = {Nandana Mihindukulasooriya and Sanju Tiwari and Daniil Dobriy and Finn Årup Nielsen and Tek Raj Chhetri and Axel Polleres},
Title         = {Scholarly Wikidata: Population and Exploration of Conference Data in
  Wikidata using LLMs},
Eprint        = {2411.08696v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Several initiatives have been undertaken to conceptually model the domain of
scholarly data using ontologies and to create respective Knowledge Graphs. Yet,
the full potential seems unleashed, as automated means for automatic population
of said ontologies are lacking, and respective initiatives from the Semantic
Web community are not necessarily connected: we propose to make scholarly data
more sustainably accessible by leveraging Wikidata's infrastructure and
automating its population in a sustainable manner through LLMs by tapping into
unstructured sources like conference Web sites and proceedings texts as well as
already existing structured conference datasets. While an initial analysis
shows that Semantic Web conferences are only minimally represented in Wikidata,
we argue that our methodology can help to populate, evolve and maintain
scholarly data as a community within Wikidata. Our main contributions include
(a) an analysis of ontologies for representing scholarly data to identify gaps
and relevant entities/properties in Wikidata, (b) semi-automated extraction --
requiring (minimal) manual validation -- of conference metadata (e.g.,
acceptance rates, organizer roles, programme committee members, best paper
awards, keynotes, and sponsors) from websites and proceedings texts using LLMs.
Finally, we discuss (c) extensions to visualization tools in the Wikidata
context for data exploration of the generated scholarly data. Our study focuses
on data from 105 Semantic Web-related conferences and extends/adds more than
6000 entities in Wikidata. It is important to note that the method can be more
generally applicable beyond Semantic Web-related conferences for enhancing
Wikidata's utility as a comprehensive scholarly resource.
  Source Repository: https://github.com/scholarly-wikidata/
  DOI: https://doi.org/10.5281/zenodo.10989709
  License: Creative Commons CC0 (Data), MIT (Code)},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.08696v1},
File          = {2411.08696v1.pdf}
}
@article{2501.09909v1,
Author        = {Jiawei Xu and Zhandos Sembay and Swathi Thaker and Pamela Payne-Foster and Jake Yue Chen and Ying Ding},
Title         = {Demo: Interactive Visualization of Semantic Relationships in a
  Biomedical Project's Talent Knowledge Graph},
Eprint        = {2501.09909v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SI},
Abstract      = {We present an interactive visualization of the Cell Map for AI Talent
Knowledge Graph (CM4AI TKG), a detailed semantic space comprising approximately
28,000 experts and 1,000 datasets focused on the biomedical field. Our tool
leverages transformer-based embeddings, WebGL visualization techniques, and
generative AI, specifically Large Language Models (LLMs), to provide a
responsive and user-friendly interface. This visualization supports the
exploration of around 29,000 nodes, assisting users in identifying potential
collaborators and dataset users within the health and biomedical research
fields. Our solution transcends the limitations of conventional graph
visualization tools like Gephi, particularly in handling large-scale
interactive graphs. We utilize GPT-4o to furnish detailed justifications for
recommended collaborators and dataset users, promoting informed
decision-making. Key functionalities include responsive search and exploration,
as well as GenAI-driven recommendations, all contributing to a nuanced
representation of the convergence between biomedical and AI research
landscapes. In addition to benefiting the Bridge2AI and CM4AI communities, this
adaptable visualization framework can be extended to other biomedical knowledge
graphs, fostering advancements in medical AI and healthcare innovation through
improved user interaction and data exploration. The demonstration is available
at: https://jiawei-alpha.vercel.app/.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.09909v1},
File          = {2501.09909v1.pdf}
}
@article{2412.10390v1,
Author        = {Lihui Liu and Zihao Wang and Hanghang Tong},
Title         = {Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query
  Perspective},
Eprint        = {2412.10390v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graph reasoning is pivotal in various domains such as data mining,
artificial intelligence, the Web, and social sciences. These knowledge graphs
function as comprehensive repositories of human knowledge, facilitating the
inference of new information. Traditional symbolic reasoning, despite its
strengths, struggles with the challenges posed by incomplete and noisy data
within these graphs. In contrast, the rise of Neural Symbolic AI marks a
significant advancement, merging the robustness of deep learning with the
precision of symbolic reasoning. This integration aims to develop AI systems
that are not only highly interpretable and explainable but also versatile,
effectively bridging the gap between symbolic and neural methodologies.
Additionally, the advent of large language models (LLMs) has opened new
frontiers in knowledge graph reasoning, enabling the extraction and synthesis
of knowledge in unprecedented ways. This survey offers a thorough review of
knowledge graph reasoning, focusing on various query types and the
classification of neural symbolic reasoning. Furthermore, it explores the
innovative integration of knowledge graph reasoning with large language models,
highlighting the potential for groundbreaking advancements. This comprehensive
overview is designed to support researchers and practitioners across multiple
fields, including data mining, AI, the Web, and social sciences, by providing a
detailed understanding of the current landscape and future directions in
knowledge graph reasoning.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2412.10390v1},
File          = {2412.10390v1.pdf}
}
@article{2501.04008v1,
Author        = {Mayukh Bagchi},
Title         = {A Generative AI-driven Metadata Modelling Approach},
Eprint        = {2501.04008v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Since decades, the modelling of metadata has been core to the functioning of
any academic library. Its importance has only enhanced with the increasing
pervasiveness of Generative Artificial Intelligence (AI)-driven information
activities and services which constitute a library's outreach. However, with
the rising importance of metadata, there arose several outstanding problems
with the process of designing a library metadata model impacting its
reusability, crosswalk and interoperability with other metadata models. This
paper posits that the above problems stem from an underlying thesis that there
should only be a few core metadata models which would be necessary and
sufficient for any information service using them, irrespective of the
heterogeneity of intra-domain or inter-domain settings. To that end, this paper
advances a contrary view of the above thesis and substantiates its argument in
three key steps. First, it introduces a novel way of thinking about a library
metadata model as an ontology-driven composition of five functionally
interlinked representation levels from perception to its intensional definition
via properties. Second, it introduces the representational manifoldness
implicit in each of the five levels which cumulatively contributes to a
conceptually entangled library metadata model. Finally, and most importantly,
it proposes a Generative AI-driven Human-Large Language Model (LLM)
collaboration based metadata modelling approach to disentangle the entanglement
inherent in each representation level leading to the generation of a
conceptually disentangled metadata model. Throughout the paper, the arguments
are exemplified by motivating scenarios and examples from representative
libraries handling cancer information.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2501.04008v1},
File          = {2501.04008v1.pdf}
}
@article{2501.12697v1,
Author        = {Qian Tao and Xiaoyang Fan and Yong Xu and Xingquan Zhu and Yufei Tang},
Title         = {Combining Knowledge Graph and LLMs for Enhanced Zero-shot Visual
  Question Answering},
Eprint        = {2501.12697v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Zero-shot visual question answering (ZS-VQA), an emerged critical research
area, intends to answer visual questions without providing training samples.
Existing research in ZS-VQA has proposed to leverage knowledge graphs or large
language models (LLMs), respectively, as external information sources to help
VQA model comprehend images and questions. However, LLMs often struggle in
accurately interpreting specific question meanings. Meanwhile, although
knowledge graph has rich entity relationships, it is challenging to effectively
connect entities to individual image content for visual question answers. In
this paper, we propose a novel design to combine knowledge graph and LLMs for
zero-shot visual question answer. Our approach uses LLMs' powerful
understanding capabilities to accurately interpret image content through a
strategic question search mechanism. Meanwhile, the knowledge graph is used to
expand and connect users' queries to the image content for better visual
question answering. An optimization algorithm is further used to determine the
optimal weights for the loss functions derived from different information
sources, towards a globally optimal set of candidate answers. Experimental
results on two benchmark datasets demonstrate that our model achieves
state-of-the-art (SOTA) performance. Both source code and benchmark data will
be released for public access.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.12697v1},
File          = {2501.12697v1.pdf}
}
@article{2405.10745v1,
Author        = {Albert Sawczyn and Jakub Binkowski and Piotr Bielak and Tomasz Kajdanowicz},
Title         = {Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging
  General-Purpose Knowledge Graphs for Enriched Embeddings},
Eprint        = {2405.10745v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Knowledge-intensive tasks pose a significant challenge for Machine Learning
(ML) techniques. Commonly adopted methods, such as Large Language Models
(LLMs), often exhibit limitations when applied to such tasks. Nevertheless,
there have been notable endeavours to mitigate these challenges, with a
significant emphasis on augmenting LLMs through Knowledge Graphs (KGs). While
KGs provide many advantages for representing knowledge, their development costs
can deter extensive research and applications. Addressing this limitation, we
introduce a framework for enriching embeddings of small-scale domain-specific
Knowledge Graphs with well-established general-purpose KGs. Adopting our
method, a modest domain-specific KG can benefit from a performance boost in
downstream tasks when linked to a substantial general-purpose KG. Experimental
evaluations demonstrate a notable enhancement, with up to a 44% increase
observed in the Hits@10 metric. This relatively unexplored research direction
can catalyze more frequent incorporation of KGs in knowledge-intensive tasks,
resulting in more robust, reliable ML implementations, which hallucinates less
than prevalent LLM solutions.
  Keywords: knowledge graph, knowledge graph completion, entity alignment,
representation learning, machine learning},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.10745v1},
File          = {2405.10745v1.pdf}
}
@article{2410.08985v2,
Author        = {Bo Ni and Yu Wang and Lu Cheng and Erik Blasch and Tyler Derr},
Title         = {Towards Trustworthy Knowledge Graph Reasoning: An Uncertainty Aware
  Perspective},
Eprint        = {2410.08985v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Recently, Knowledge Graphs (KGs) have been successfully coupled with Large
Language Models (LLMs) to mitigate their hallucinations and enhance their
reasoning capability, such as in KG-based retrieval-augmented frameworks.
However, current KG-LLM frameworks lack rigorous uncertainty estimation,
limiting their reliable deployment in high-stakes applications. Directly
incorporating uncertainty quantification into KG-LLM frameworks presents
challenges due to their complex architectures and the intricate interactions
between the knowledge graph and language model components. To address this gap,
we propose a new trustworthy KG-LLM framework, Uncertainty Aware
Knowledge-Graph Reasoning (UAG), which incorporates uncertainty quantification
into the KG-LLM framework. We design an uncertainty-aware multi-step reasoning
framework that leverages conformal prediction to provide a theoretical
guarantee on the prediction set. To manage the error rate of the multi-step
process, we additionally introduce an error rate control module to adjust the
error rate within the individual components. Extensive experiments show that
our proposed UAG can achieve any pre-defined coverage rate while reducing the
prediction set/interval size by 40% on average over the baselines.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.08985v2},
File          = {2410.08985v2.pdf}
}
@article{2406.17532v2,
Author        = {Keyu Wang and Guilin Qi and Jiaqi Li and Songlin Zhai},
Title         = {Can Large Language Models Understand DL-Lite Ontologies? An Empirical
  Study},
Eprint        = {2406.17532v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large language models (LLMs) have shown significant achievements in solving a
wide range of tasks. Recently, LLMs' capability to store, retrieve and infer
with symbolic knowledge has drawn a great deal of attention, showing their
potential to understand structured information. However, it is not yet known
whether LLMs can understand Description Logic (DL) ontologies. In this work, we
empirically analyze the LLMs' capability of understanding DL-Lite ontologies
covering 6 representative tasks from syntactic and semantic aspects. With
extensive experiments, we demonstrate both the effectiveness and limitations of
LLMs in understanding DL-Lite ontologies. We find that LLMs can understand
formal syntax and model-theoretic semantics of concepts and roles. However,
LLMs struggle with understanding TBox NI transitivity and handling ontologies
with large ABoxes. We hope that our experiments and analyses provide more
insights into LLMs and inspire to build more faithful knowledge engineering
solutions.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.17532v2},
File          = {2406.17532v2.pdf}
}
@article{2402.07148v2,
Author        = {Eric L. Buehler and Markus J. Buehler},
Title         = {X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for
  Large Language Models with Applications in Protein Mechanics and Molecular
  Design},
Eprint        = {2402.07148v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cond-mat.soft},
Abstract      = {We report a mixture of expert strategy to create fine-tuned large language
models using a deep layer-wise token-level approach based on low-rank
adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, our gating
strategy uses the hidden states to dynamically mix adapted layers, allowing the
resulting X-LoRA model to draw upon different capabilities and create
never-before-used deep layer-wise combinations to solve tasks. The design is
inspired by the biological principles of universality and diversity, where
neural network building blocks are reused in different hierarchical
manifestations. Hence, the X-LoRA model can be easily implemented for any
existing large language model (LLM) without a need for modifications of the
underlying structure. We develop a tailored X-LoRA model that offers scientific
capabilities including forward/inverse analysis tasks and enhanced reasoning
capability, focused on biomaterial analysis, protein mechanics and design. The
impact of this work include access to readily expandable and adaptable models
with strong domain knowledge and the capability to integrate across areas of
knowledge. Featuring experts in biology, mathematics, reasoning, bio-inspired
materials, mechanics and materials, chemistry, protein biophysics, mechanics
and quantum-mechanics based molecular properties, we conduct a series of
physics-focused case studies. We examine knowledge recall, protein mechanics
forward/inverse tasks, protein design, adversarial agentic modeling including
ontological knowledge graph construction, as well as molecular design. The
model is capable not only of making quantitative predictions of nanomechanical
properties of proteins or quantum mechanical molecular properties, but also
reasons over the results and correctly predicts likely mechanisms that explain
distinct molecular behaviors.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.07148v2},
File          = {2402.07148v2.pdf}
}
@article{2309.07172v1,
Author        = {Yuan He and Jiaoyan Chen and Hang Dong and Ian Horrocks},
Title         = {Exploring Large Language Models for Ontology Alignment},
Eprint        = {2309.07172v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This work investigates the applicability of recent generative Large Language
Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for
identifying concept equivalence mappings across ontologies. To test the
zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging
subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking
into account concept labels and structural contexts. Preliminary findings
suggest that LLMs have the potential to outperform existing ontology alignment
systems like BERTMap, given careful framework and prompt design.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.07172v1},
File          = {2309.07172v1.pdf}
}
@article{2408.08088v1,
Author        = {Zongzong Wu and Fengxiao Tang and Ming Zhao and Yufeng Li},
Title         = {KGV: Integrating Large Language Models with Knowledge Graphs for Cyber
  Threat Intelligence Credibility Assessment},
Eprint        = {2408.08088v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Cyber threat intelligence is a critical tool that many organizations and
individuals use to protect themselves from sophisticated, organized,
persistent, and weaponized cyber attacks. However, few studies have focused on
the quality assessment of threat intelligence provided by intelligence
platforms, and this work still requires manual analysis by cybersecurity
experts. In this paper, we propose a knowledge graph-based verifier, a novel
Cyber Threat Intelligence (CTI) quality assessment framework that combines
knowledge graphs and Large Language Models (LLMs). Our approach introduces LLMs
to automatically extract OSCTI key claims to be verified and utilizes a
knowledge graph consisting of paragraphs for fact-checking. This method differs
from the traditional way of constructing complex knowledge graphs with entities
as nodes. By constructing knowledge graphs with paragraphs as nodes and
semantic similarity as edges, it effectively enhances the semantic
understanding ability of the model and simplifies labeling requirements.
Additionally, to fill the gap in the research field, we created and made public
the first dataset for threat intelligence assessment from heterogeneous
sources. To the best of our knowledge, this work is the first to create a
dataset on threat intelligence reliability verification, providing a reference
for future research. Experimental results show that KGV (Knowledge Graph
Verifier) significantly improves the performance of LLMs in intelligence
quality assessment. Compared with traditional methods, we reduce a large amount
of data annotation while the model still exhibits strong reasoning
capabilities. Finally, our method can achieve XXX accuracy in network threat
assessment.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.08088v1},
File          = {2408.08088v1.pdf}
}
@article{2412.04690v1,
Author        = {Xuan Chen and Tong Lu and Zhichun Wang},
Title         = {LLM-Align: Utilizing Large Language Models for Entity Alignment in
  Knowledge Graphs},
Eprint        = {2412.04690v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Entity Alignment (EA) seeks to identify and match corresponding entities
across different Knowledge Graphs (KGs), playing a crucial role in knowledge
fusion and integration. Embedding-based entity alignment (EA) has recently
gained considerable attention, resulting in the emergence of many innovative
approaches. Initially, these approaches concentrated on learning entity
embeddings based on the structural features of knowledge graphs (KGs) as
defined by relation triples. Subsequent methods have integrated entities' names
and attributes as supplementary information to improve the embeddings used for
EA. However, existing methods lack a deep semantic understanding of entity
attributes and relations. In this paper, we propose a Large Language Model
(LLM) based Entity Alignment method, LLM-Align, which explores the
instruction-following and zero-shot capabilities of Large Language Models to
infer alignments of entities. LLM-Align uses heuristic methods to select
important attributes and relations of entities, and then feeds the selected
triples of entities to an LLM to infer the alignment results. To guarantee the
quality of alignment results, we design a multi-round voting mechanism to
mitigate the hallucination and positional bias issues that occur with LLMs.
Experiments on three EA datasets, demonstrating that our approach achieves
state-of-the-art performance compared to existing EA methods.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.04690v1},
File          = {2412.04690v1.pdf}
}
@article{2412.18702v1,
Author        = {Yanlin Feng and Simone Papicchio and Sajjadur Rahman},
Title         = {CypherBench: Towards Precise Retrieval over Full-scale Modern Knowledge
  Graphs in the LLM Era},
Eprint        = {2412.18702v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Retrieval from graph data is crucial for augmenting large language models
(LLM) with both open-domain knowledge and private enterprise data, and it is
also a key component in the recent GraphRAG system (edge et al., 2024). Despite
decades of research on knowledge graphs and knowledge base question answering,
leading LLM frameworks (e.g. Langchain and LlamaIndex) have only minimal
support for retrieval from modern encyclopedic knowledge graphs like Wikidata.
In this paper, we analyze the root cause and suggest that modern RDF knowledge
graphs (e.g. Wikidata, Freebase) are less efficient for LLMs due to overly
large schemas that far exceed the typical LLM context window, use of resource
identifiers, overlapping relation types and lack of normalization. As a
solution, we propose property graph views on top of the underlying RDF graph
that can be efficiently queried by LLMs using Cypher. We instantiated this idea
on Wikidata and introduced CypherBench, the first benchmark with 11
large-scale, multi-domain property graphs with 7.8 million entities and over
10,000 questions. To achieve this, we tackled several key challenges, including
developing an RDF-to-property graph conversion engine, creating a systematic
pipeline for text-to-Cypher task generation, and designing new evaluation
metrics.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.18702v1},
File          = {2412.18702v1.pdf}
}
@article{2404.10329v2,
Author        = {Reihaneh Amini and Sanaz Saki Norouzi and Pascal Hitzler and Reza Amini},
Title         = {Towards Complex Ontology Alignment using Large Language Models},
Eprint        = {2404.10329v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology alignment, a critical process in the Semantic Web for detecting
relationships between different ontologies, has traditionally focused on
identifying so-called "simple" 1-to-1 relationships through class labels and
properties comparison. The more practically useful exploration of more complex
alignments remains a hard problem to automate, and as such is largely
underexplored, i.e. in application practice it is usually done manually by
ontology and domain experts. Recently, the surge in Natural Language Processing
(NLP) capabilities, driven by advancements in Large Language Models (LLMs),
presents new opportunities for enhancing ontology engineering practices,
including ontology alignment tasks. This paper investigates the application of
LLM technologies to tackle the complex ontology alignment challenge. Leveraging
a prompt-based approach and integrating rich ontology content so-called modules
our work constitutes a significant advance towards automating the complex
alignment task.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.10329v2},
File          = {2404.10329v2.pdf}
}
@article{2402.13593v1,
Author        = {Mengqi Zhang and Xiaotian Ye and Qiang Liu and Pengjie Ren and Shu Wu and Zhumin Chen},
Title         = {Knowledge Graph Enhanced Large Language Model Editing},
Eprint        = {2402.13593v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) are pivotal in advancing natural language
processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and
outdated knowledge. Model editing emerges as a promising solution to address
these challenges. However, existing editing methods struggle to track and
incorporate changes in knowledge associated with edits, which limits the
generalization ability of postedit LLMs in processing edited knowledge. To
tackle these problems, we propose a novel model editing method that leverages
knowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we
first utilize a knowledge graph augmentation module to uncover associated
knowledge that has changed due to editing, obtaining its internal
representations within LLMs. This approach allows knowledge alterations within
LLMs to be reflected through an external graph structure. Subsequently, we
design a graph-based knowledge edit module to integrate structured knowledge
into the model editing. This ensures that the updated parameters reflect not
only the modifications of the edited knowledge but also the changes in other
associated knowledge resulting from the editing process. Comprehensive
experiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME
significantly improves the generalization capabilities of post-edit LLMs in
employing edited knowledge.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.13593v1},
File          = {2402.13593v1.pdf}
}
@article{2502.03715v1,
Author        = {Rui Cai and Chao Wang and Qianyi Cai and Dazhong Shen and Hui Xiong},
Title         = {Boosting Knowledge Graph-based Recommendations through Confidence-Aware
  Augmentation with Large Language Models},
Eprint        = {2502.03715v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Knowledge Graph-based recommendations have gained significant attention due
to their ability to leverage rich semantic relationships. However, constructing
and maintaining Knowledge Graphs (KGs) is resource-intensive, and the accuracy
of KGs can suffer from noisy, outdated, or irrelevant triplets. Recent
advancements in Large Language Models (LLMs) offer a promising way to improve
the quality and relevance of KGs for recommendation tasks. Despite this,
integrating LLMs into KG-based systems presents challenges, such as efficiently
augmenting KGs, addressing hallucinations, and developing effective joint
learning methods. In this paper, we propose the Confidence-aware KG-based
Recommendation Framework with LLM Augmentation (CKG-LLMA), a novel framework
that combines KGs and LLMs for recommendation task. The framework includes: (1)
an LLM-based subgraph augmenter for enriching KGs with high-quality
information, (2) a confidence-aware message propagation mechanism to filter
noisy triplets, and (3) a dual-view contrastive learning method to integrate
user-item interactions and KG data. Additionally, we employ a confidence-aware
explanation generation process to guide LLMs in producing realistic
explanations for recommendations. Finally, extensive experiments demonstrate
the effectiveness of CKG-LLMA across multiple public datasets.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.03715v1},
File          = {2502.03715v1.pdf}
}
@article{2308.10168v2,
Author        = {Kai Sun and Yifan Ethan Xu and Hanwen Zha and Yue Liu and Xin Luna Dong},
Title         = {Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A.
  Will LLMs Replace Knowledge Graphs?},
Eprint        = {2308.10168v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Since the recent prosperity of Large Language Models (LLMs), there have been
interleaved discussions regarding how to reduce hallucinations from LLM
responses, how to increase the factuality of LLMs, and whether Knowledge Graphs
(KGs), which store the world knowledge in a symbolic form, will be replaced
with LLMs. In this paper, we try to answer these questions from a new angle:
How knowledgeable are LLMs?
  To answer this question, we constructed Head-to-Tail, a benchmark that
consists of 18K question-answer (QA) pairs regarding head, torso, and tail
facts in terms of popularity. We designed an automated evaluation method and a
set of metrics that closely approximate the knowledge an LLM confidently
internalizes. Through a comprehensive evaluation of 16 publicly available LLMs,
we show that existing LLMs are still far from being perfect in terms of their
grasp of factual knowledge, especially for facts of torso-to-tail entities.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.10168v2},
File          = {2308.10168v2.pdf}
}
@article{1909.03193v2,
Author        = {Liang Yao and Chengsheng Mao and Yuan Luo},
Title         = {KG-BERT: BERT for Knowledge Graph Completion},
Eprint        = {1909.03193v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs are important resources for many artificial intelligence
tasks but often suffer from incompleteness. In this work, we propose to use
pre-trained language models for knowledge graph completion. We treat triples in
knowledge graphs as textual sequences and propose a novel framework named
Knowledge Graph Bidirectional Encoder Representations from Transformer
(KG-BERT) to model these triples. Our method takes entity and relation
descriptions of a triple as input and computes scoring function of the triple
with the KG-BERT language model. Experimental results on multiple benchmark
knowledge graphs show that our method can achieve state-of-the-art performance
in triple classification, link prediction and relation prediction tasks.},
Year          = {2019},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1909.03193v2},
File          = {1909.03193v2.pdf}
}
@article{2405.02738v1,
Author        = {Sakher Khalil Alqaaidi and Krzysztof Kochut},
Title         = {Relations Prediction for Knowledge Graph Completion using Large Language
  Models},
Eprint        = {2405.02738v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graphs have been widely used to represent facts in a structured
format. Due to their large scale applications, knowledge graphs suffer from
being incomplete. The relation prediction task obtains knowledge graph
completion by assigning one or more possible relations to each pair of nodes.
In this work, we make use of the knowledge graph node names to fine-tune a
large language model for the relation prediction task. By utilizing the node
names only we enable our model to operate sufficiently in the inductive
settings. Our experiments show that we accomplish new scores on a widely used
knowledge graph benchmark.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.02738v1},
File          = {2405.02738v1.pdf}
}
@article{2402.02389v2,
Author        = {Yanbin Wei and Qiushi Huang and James T. Kwok and Yu Zhang},
Title         = {KICGPT: Large Language Model with Knowledge in Context for Knowledge
  Graph Completion},
Eprint        = {2402.02389v2},
DOI           = {10.18653/v1/2023.findings-emnlp.580},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph
incompleteness and supporting downstream applications. Many models have been
proposed for KGC. They can be categorized into two main classes: triple-based
and text-based approaches. Triple-based methods struggle with long-tail
entities due to limited structural information and imbalanced entity
distributions. Text-based methods alleviate this issue but require costly
training for language models and specific finetuning for knowledge graphs,
which limits their efficiency. To alleviate these limitations, in this paper,
we propose KICGPT, a framework that integrates a large language model (LLM) and
a triple-based KGC retriever. It alleviates the long-tail problem without
incurring additional training overhead. KICGPT uses an in-context learning
strategy called Knowledge Prompt, which encodes structural knowledge into
demonstrations to guide the LLM. Empirical results on benchmark datasets
demonstrate the effectiveness of KICGPT with smaller training overhead and no
finetuning.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.02389v2},
File          = {2402.02389v2.pdf}
}
@article{2406.11400v1,
Author        = {Golnaz Shapurian},
Title         = {Large Language Models and Knowledge Graphs for Astronomical Entity
  Disambiguation},
Eprint        = {2406.11400v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper presents an experiment conducted during a hackathon, focusing on
using large language models (LLMs) and knowledge graph clustering to extract
entities and relationships from astronomical text. The study demonstrates an
approach to disambiguate entities that can appear in various contexts within
the astronomical domain. By collecting excerpts around specific entities and
leveraging the GPT-4 language model, relevant entities and relationships are
extracted. The extracted information is then used to construct a knowledge
graph, which is clustered using the Leiden algorithm. The resulting Leiden
communities are utilized to identify the percentage of association of unknown
excerpts to each community, thereby enabling disambiguation. The experiment
showcases the potential of combining LLMs and knowledge graph clustering
techniques for information extraction in astronomical research. The results
highlight the effectiveness of the approach in identifying and disambiguating
entities, as well as grouping them into meaningful clusters based on their
relationships.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.11400v1},
File          = {2406.11400v1.pdf}
}
@article{2406.02030v2,
Author        = {Junlin Lee and Yequan Wang and Jing Li and Min Zhang},
Title         = {Multimodal Reasoning with Multimodal Knowledge Graph},
Eprint        = {2406.02030v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Multimodal reasoning with large language models (LLMs) often suffers from
hallucinations and the presence of deficient or outdated knowledge within LLMs.
Some approaches have sought to mitigate these issues by employing textual
knowledge graphs, but their singular modality of knowledge limits comprehensive
cross-modal understanding. In this paper, we propose the Multimodal Reasoning
with Multimodal Knowledge Graph (MR-MKG) method, which leverages multimodal
knowledge graphs (MMKGs) to learn rich and semantic knowledge across
modalities, significantly enhancing the multimodal reasoning capabilities of
LLMs. In particular, a relation graph attention network is utilized for
encoding MMKGs and a cross-modal alignment module is designed for optimizing
image-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with
initial expertise in multimodal reasoning through pretraining. Remarkably,
MR-MKG achieves superior performance while training on only a small fraction of
parameters, approximately 2.25% of the LLM's parameter size. Experimental
results on multimodal question answering and multimodal analogy reasoning tasks
demonstrate that our MR-MKG method outperforms previous state-of-the-art
models.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.02030v2},
File          = {2406.02030v2.pdf}
}
@article{2406.10802v1,
Author        = {Aihua Pei and Zehua Yang and Shunan Zhu and Ruoxi Cheng and Ju Jia and Lina Wang},
Title         = {KGPA: Robustness Evaluation for Large Language Models via Cross-Domain
  Knowledge Graphs},
Eprint        = {2406.10802v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Existing frameworks for assessing robustness of large language models (LLMs)
overly depend on specific benchmarks, increasing costs and failing to evaluate
performance of LLMs in professional domains due to dataset limitations. This
paper proposes a framework that systematically evaluates the robustness of LLMs
under adversarial attack scenarios by leveraging knowledge graphs (KGs). Our
framework generates original prompts from the triplets of knowledge graphs and
creates adversarial prompts by poisoning, assessing the robustness of LLMs
through the results of these adversarial attacks. We systematically evaluate
the effectiveness of this framework and its modules. Experiments show that
adversarial robustness of the ChatGPT family ranks as GPT-4-turbo > GPT-4o >
GPT-3.5-turbo, and the robustness of large language models is influenced by the
professional domains in which they operate.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.10802v1},
File          = {2406.10802v1.pdf}
}
@article{2402.06764v3,
Author        = {Stefan Dernbach and Khushbu Agarwal and Alejandro Zuniga and Michael Henry and Sutanay Choudhury},
Title         = {GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph
  Alignment via Neighborhood Partitioning and Generative Subgraph Encoding},
Eprint        = {2402.06764v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Integrating large language models (LLMs) with knowledge graphs derived from
domain-specific data represents an important advancement towards more powerful
and factual reasoning. As these models grow more capable, it is crucial to
enable them to perform multi-step inferences over real-world knowledge graphs
while minimizing hallucination. While large language models excel at
conversation and text generation, their ability to reason over
domain-specialized graphs of interconnected entities remains limited. For
example, can we query a LLM to identify the optimal contact in a professional
network for a specific goal, based on relationships and attributes in a private
database? The answer is no--such capabilities lie beyond current methods.
However, this question underscores a critical technical gap that must be
addressed. Many high-value applications in areas such as science, security, and
e-commerce rely on proprietary knowledge graphs encoding unique structures,
relationships, and logical constraints. We introduce a fine-tuning framework
for developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledge
graph into an alternate text representation with labeled question-answer pairs.
We demonstrate that grounding the models in specific graph-based knowledge
expands the models' capacity for structure-based reasoning. Our methodology
leverages the large-language model's generative capabilities to create the
dataset and proposes an efficient alternate to retrieval-augmented generation
styled methods.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.06764v3},
File          = {2402.06764v3.pdf}
}
@article{2405.04756v1,
Author        = {Chu Fei Luo and Ahmad Ghawanmeh and Xiaodan Zhu and Faiza Khan Khattak},
Title         = {BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language
  Models},
Eprint        = {2405.04756v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Modern large language models (LLMs) have a significant amount of world
knowledge, which enables strong performance in commonsense reasoning and
knowledge-intensive tasks when harnessed properly. The language model can also
learn social biases, which has a significant potential for societal harm. There
have been many mitigation strategies proposed for LLM safety, but it is unclear
how effective they are for eliminating social biases. In this work, we propose
a new methodology for attacking language models with knowledge graph augmented
generation. We refactor natural language stereotypes into a knowledge graph,
and use adversarial attacking strategies to induce biased responses from
several open- and closed-source language models. We find our method increases
bias in all models, even those trained with safety guardrails. This
demonstrates the need for further research in AI safety, and further work in
this new adversarial space.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.04756v1},
File          = {2405.04756v1.pdf}
}
@article{2402.16568v2,
Author        = {Yifu Gao and Linbo Qiao and Zhigang Kan and Zhihua Wen and Yongquan He and Dongsheng Li},
Title         = {Two-stage Generative Question Answering on Temporal Knowledge Graph
  Using Large Language Models},
Eprint        = {2402.16568v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Temporal knowledge graph question answering (TKGQA) poses a significant
challenge task, due to the temporal constraints hidden in questions and the
answers sought from dynamic structured knowledge. Although large language
models (LLMs) have made considerable progress in their reasoning ability over
structured data, their application to the TKGQA task is a relatively unexplored
area. This paper first proposes a novel generative temporal knowledge graph
question answering framework, GenTKGQA, which guides LLMs to answer temporal
questions through two phases: Subgraph Retrieval and Answer Generation. First,
we exploit LLM's intrinsic knowledge to mine temporal constraints and
structural links in the questions without extra training, thus narrowing down
the subgraph search space in both temporal and structural dimensions. Next, we
design virtual knowledge indicators to fuse the graph neural network signals of
the subgraph and the text representations of the LLM in a non-shallow way,
which helps the open-source LLM deeply understand the temporal order and
structural dependencies among the retrieved facts through instruction tuning.
Experimental results on two widely used datasets demonstrate the superiority of
our model.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.16568v2},
File          = {2402.16568v2.pdf}
}
@article{2205.11100v2,
Author        = {Jiangmeng Li and Wenyi Mo and Wenwen Qiang and Bing Su and Changwen Zheng and Hui Xiong and Ji-Rong Wen},
Title         = {Supporting Vision-Language Model Inference with Confounder-pruning
  Knowledge Prompt},
Eprint        = {2205.11100v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Vision-language models are pre-trained by aligning image-text pairs in a
common space to deal with open-set visual concepts. To boost the
transferability of the pre-trained models, recent works adopt fixed or
learnable prompts, i.e., classification weights are synthesized from natural
language describing task-relevant categories, to reduce the gap between tasks
in the training and test phases. However, how and what prompts can improve
inference performance remains unclear. In this paper, we explicitly clarify the
importance of including semantic information in prompts, while existing
prompting methods generate prompts without exploring the semantic information
of textual labels. Manually constructing prompts with rich semantics requires
domain expertise and is extremely time-consuming. To cope with this issue, we
propose a semantic-aware prompt learning method, namely CPKP, which retrieves
an ontological knowledge graph by treating the textual label as a query to
extract task-relevant semantic information. CPKP further introduces a
double-tier confounder-pruning procedure to refine the derived semantic
information. The graph-tier confounders are gradually identified and phased
out, inspired by the principle of Granger causality. The feature-tier
confounders are demolished by following the maximum entropy principle in
information theory. Empirically, the evaluations demonstrate the effectiveness
of CPKP, e.g., with two shots, CPKP outperforms the manual-prompt method by
4.64% and the learnable-prompt method by 1.09% on average, and the superiority
of CPKP in domain generalization compared to benchmark approaches. Our
implementation is available at https://github.com/Mowenyii/CPKP.},
Year          = {2022},
Month         = {May},
Url           = {http://arxiv.org/abs/2205.11100v2},
File          = {2205.11100v2.pdf}
}
@article{2412.12464v1,
Author        = {Keigo Sakurai and Ren Togo and Takahiro Ogawa and Miki Haseyama},
Title         = {LLM is Knowledge Graph Reasoner: LLM's Intuition-aware Knowledge Graph
  Reasoning for Cold-start Sequential Recommendation},
Eprint        = {2412.12464v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Knowledge Graphs (KGs) represent relationships between entities in a graph
structure and have been widely studied as promising tools for realizing
recommendations that consider the accurate content information of items.
However, traditional KG-based recommendation methods face fundamental
challenges: insufficient consideration of temporal information and poor
performance in cold-start scenarios. On the other hand, Large Language Models
(LLMs) can be considered databases with a wealth of knowledge learned from the
web data, and they have recently gained attention due to their potential
application as recommendation systems. Although approaches that treat LLMs as
recommendation systems can leverage LLMs' high recommendation literacy, their
input token limitations make it impractical to consider the entire
recommendation domain dataset and result in scalability issues. To address
these challenges, we propose a LLM's Intuition-aware Knowledge graph Reasoning
model (LIKR). Our main idea is to treat LLMs as reasoners that output intuitive
exploration strategies for KGs. To integrate the knowledge of LLMs and KGs, we
trained a recommendation agent through reinforcement learning using a reward
function that integrates different recommendation strategies, including LLM's
intuition and KG embeddings. By incorporating temporal awareness through prompt
engineering and generating textual representations of user preferences from
limited interactions, LIKR can improve recommendation performance in cold-start
scenarios. Furthermore, LIKR can avoid scalability issues by using KGs to
represent recommendation domain datasets and limiting the LLM's output to KG
exploration strategies. Experiments on real-world datasets demonstrate that our
model outperforms state-of-the-art recommendation methods in cold-start
sequential recommendation scenarios.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.12464v1},
File          = {2412.12464v1.pdf}
}
@article{2501.11911v1,
Author        = {He Chang and Jie Wu and Zhulin Tao and Yunshan Ma and Xianglin Huang and Tat-Seng Chua},
Title         = {Integrate Temporal Graph Learning into LLM-based Temporal Knowledge
  Graph Model},
Eprint        = {2501.11911v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Temporal Knowledge Graph Forecasting (TKGF) aims to predict future events
based on the observed events in history. Recently, Large Language Models (LLMs)
have exhibited remarkable capabilities, generating significant research
interest in their application for reasoning over temporal knowledge graphs
(TKGs). Existing LLM-based methods have integrated retrieved historical facts
or static graph representations into LLMs. Despite the notable performance of
LLM-based methods, they are limited by the insufficient modeling of temporal
patterns and ineffective cross-modal alignment between graph and language,
hindering the ability of LLMs to fully grasp the temporal and structural
information in TKGs. To tackle these issues, we propose a novel framework
TGL-LLM to integrate temporal graph learning into LLM-based temporal knowledge
graph model. Specifically, we introduce temporal graph learning to capture the
temporal and relational patterns and obtain the historical graph embedding.
Furthermore, we design a hybrid graph tokenization to sufficiently model the
temporal patterns within LLMs. To achieve better alignment between graph and
language, we employ a two-stage training paradigm to finetune LLMs on
high-quality and diverse data, thereby resulting in better performance.
Extensive experiments on three real-world datasets show that our approach
outperforms a range of state-of-the-art (SOTA) methods.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.11911v1},
File          = {2501.11911v1.pdf}
}
@article{2411.19064v1,
Author        = {Yutong Zhang and Lixing Chen and Shenghong Li and Nan Cao and Yang Shi and Jiaxin Ding and Zhe Qu and Pan Zhou and Yang Bai},
Title         = {Way to Specialist: Closing Loop Between Specialized LLM and Evolving
  Domain Knowledge Graph},
Eprint        = {2411.19064v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have demonstrated exceptional performance across
a wide variety of domains. Nonetheless, generalist LLMs continue to fall short
in reasoning tasks necessitating specialized knowledge. Prior investigations
into specialized LLMs focused on domain-specific training, which entails
substantial efforts in domain data acquisition and model parameter fine-tuning.
To address these challenges, this paper proposes the Way-to-Specialist (WTS)
framework, which synergizes retrieval-augmented generation with knowledge
graphs (KGs) to enhance the specialized capability of LLMs in the absence of
specialized training. In distinction to existing paradigms that merely utilize
external knowledge from general KGs or static domain KGs to prompt LLM for
enhanced domain-specific reasoning, WTS proposes an innovative
"LLM$\circlearrowright$KG" paradigm, which achieves bidirectional enhancement
between specialized LLM and domain knowledge graph (DKG). The proposed paradigm
encompasses two closely coupled components: the DKG-Augmented LLM and the
LLM-Assisted DKG Evolution. The former retrieves question-relevant domain
knowledge from DKG and uses it to prompt LLM to enhance the reasoning
capability for domain-specific tasks; the latter leverages LLM to generate new
domain knowledge from processed tasks and use it to evolve DKG. WTS closes the
loop between DKG-Augmented LLM and LLM-Assisted DKG Evolution, enabling
continuous improvement in the domain specialization as it progressively answers
and learns from domain-specific questions. We validate the performance of WTS
on 6 datasets spanning 5 domains. The experimental results show that WTS
surpasses the previous SOTA in 4 specialized domains and achieves a maximum
performance improvement of 11.3%.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.19064v1},
File          = {2411.19064v1.pdf}
}
@article{2312.15880v2,
Author        = {Tiezheng Guo and Qingwen Yang and Chen Wang and Yanyi Liu and Pan Li and Jiawei Tang and Dapeng Li and Yingyou Wen},
Title         = {KnowledgeNavigator: Leveraging Large Language Models for Enhanced
  Reasoning over Knowledge Graph},
Eprint        = {2312.15880v2},
DOI           = {10.1007/s40747-024-01527-8},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language model (LLM) has achieved outstanding performance on various
downstream tasks with its powerful natural language understanding and zero-shot
capability, but LLM still suffers from knowledge limitation. Especially in
scenarios that require long logical chains or complex reasoning, the
hallucination and knowledge limitation of LLM limit its performance in question
answering (QA). In this paper, we propose a novel framework KnowledgeNavigator
to address these challenges by efficiently and accurately retrieving external
knowledge from knowledge graph and using it as a key factor to enhance LLM
reasoning. Specifically, KnowledgeNavigator first mines and enhances the
potential constraints of the given question to guide the reasoning. Then it
retrieves and filters external knowledge that supports answering through
iterative reasoning on knowledge graph with the guidance of LLM and the
question. Finally, KnowledgeNavigator constructs the structured knowledge into
effective prompts that are friendly to LLM to help its reasoning. We evaluate
KnowledgeNavigator on multiple public KGQA benchmarks, the experiments show the
framework has great effectiveness and generalization, outperforming previous
knowledge graph enhanced LLM methods and is comparable to the fully supervised
models.},
Year          = {2023},
Month         = {Dec},
Note          = {Complex & Intelligent Systems (2024): 1-14},
Url           = {http://arxiv.org/abs/2312.15880v2},
File          = {2312.15880v2.pdf}
}
@article{2401.14931v2,
Author        = {Marco Bombieri and Paolo Fiorini and Simone Paolo Ponzetto and Marco Rospocher},
Title         = {Do LLMs Dream of Ontologies?},
Eprint        = {2401.14931v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have demonstrated remarkable performance across
diverse natural language processing tasks, yet their ability to memorize
structured knowledge remains underexplored. In this paper, we investigate the
extent to which general-purpose pre-trained LLMs retain and correctly reproduce
concept identifier (ID)-label associations from publicly available ontologies.
We conduct a systematic evaluation across multiple ontological resources,
including the Gene Ontology, Uberon, Wikidata, and ICD-10, using LLMs such as
Pythia-12B, Gemini-1.5-Flash, GPT-3.5, and GPT-4. Our findings reveal that only
a small fraction of ontological concepts is accurately memorized, with GPT-4
demonstrating the highest performance. To understand why certain concepts are
memorized more effectively than others, we analyze the relationship between
memorization accuracy and concept popularity on the Web. Our results indicate a
strong correlation between the frequency of a concept's occurrence online and
the likelihood of accurately retrieving its ID from the label. This suggests
that LLMs primarily acquire such knowledge through indirect textual exposure
rather than directly from structured ontological resources. Furthermore, we
introduce new metrics to quantify prediction invariance, demonstrating that the
stability of model responses across variations in prompt language and
temperature settings can serve as a proxy for estimating memorization
robustness.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.14931v2},
File          = {2401.14931v2.pdf}
}
@article{2411.05521v2,
Author        = {Sithursan Sivasubramaniam and Cedric Osei-Akoto and Yi Zhang and Kurt Stockinger and Jonathan Fuerst},
Title         = {SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark},
Eprint        = {2411.05521v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Electronic health records (EHRs) are stored in various database systems with
different database models on heterogeneous storage architectures, such as
relational databases, document stores, or graph databases. These different
database models have a big impact on query complexity and performance. While
this has been a known fact in database research, its implications for the
growing number of Text-to-Query systems have surprisingly not been investigated
so far. In this paper, we present SM3-Text-to-Query, the first multi-model
medical Text-to-Query benchmark based on synthetic patient data from Synthea,
following the SNOMED-CT taxonomy -- a widely used knowledge graph ontology
covering medical terminology. SM3-Text-to-Query provides data representations
for relational databases (PostgreSQL), document stores (MongoDB), and graph
databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four
popular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically
and manually develop 408 template questions, which we augment to construct a
benchmark of 10K diverse natural language question/query pairs for these four
query languages (40K pairs overall). On our dataset, we evaluate several common
in-context-learning (ICL) approaches for a set of representative closed and
open-source LLMs. Our evaluation sheds light on the trade-offs between database
models and query languages for different ICL strategies and LLMs. Last,
SM3-Text-to-Query is easily extendable to additional query languages or real,
standard-based patient databases.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.05521v2},
File          = {2411.05521v2.pdf}
}
@article{2404.00942v1,
Author        = {Xiaoze Liu and Feijie Wu and Tianyang Xu and Zhuo Chen and Yichi Zhang and Xiaoqian Wang and Jing Gao},
Title         = {Evaluating the Factuality of Large Language Models using Large-Scale
  Knowledge Graphs},
Eprint        = {2404.00942v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The advent of Large Language Models (LLMs) has significantly transformed the
AI landscape, enhancing machine learning and AI capabilities. Factuality issue
is a critical concern for LLMs, as they may generate factually incorrect
responses. In this paper, we propose GraphEval to evaluate an LLM's performance
using a substantially large test dataset. Specifically, the test dataset is
retrieved from a large knowledge graph with more than 10 million facts without
expensive human efforts. Unlike conventional methods that evaluate LLMs based
on generated responses, GraphEval streamlines the evaluation process by
creating a judge model to estimate the correctness of the answers given by the
LLM. Our experiments demonstrate that the judge model's factuality assessment
aligns closely with the correctness of the LLM's generated outputs, while also
substantially reducing evaluation costs. Besides, our findings offer valuable
insights into LLM performance across different metrics and highlight the
potential for future improvements in ensuring the factual integrity of LLM
outputs. The code is publicly available at https://github.com/xz-liu/GraphEval.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.00942v1},
File          = {2404.00942v1.pdf}
}
@article{2407.18752v3,
Author        = {Yuni Susanti and Michael Färber},
Title         = {Knowledge Graph Structure as Prompt: Improving Small Language Models
  Capabilities for Knowledge-based Causal Discovery},
Eprint        = {2407.18752v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Causal discovery aims to estimate causal structures among variables based on
observational data. Large Language Models (LLMs) offer a fresh perspective to
tackle the causal discovery problem by reasoning on the metadata associated
with variables rather than their actual data values, an approach referred to as
knowledge-based causal discovery. In this paper, we investigate the
capabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1
billion parameters) with prompt-based learning for knowledge-based causal
discovery. Specifically, we present KG Structure as Prompt, a novel approach
for integrating structural information from a knowledge graph, such as common
neighbor nodes and metapaths, into prompt-based learning to enhance the
capabilities of SLMs. Experimental results on three types of biomedical and
open-domain datasets under few-shot settings demonstrate the effectiveness of
our approach, surpassing most baselines and even conventional fine-tuning
approaches trained on full datasets. Our findings further highlight the strong
capabilities of SLMs: in combination with knowledge graphs and prompt-based
learning, SLMs demonstrate the potential to surpass LLMs with larger number of
parameters. Our code and datasets are available on GitHub.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.18752v3},
File          = {2407.18752v3.pdf}
}
@article{2405.19686v1,
Author        = {Jingwei Sun and Zhixu Du and Yiran Chen},
Title         = {Knowledge Graph Tuning: Real-time Large Language Model Personalization
  based on Human Feedback},
Eprint        = {2405.19686v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large language models (LLMs) have demonstrated remarkable proficiency in a
range of natural language processing tasks. Once deployed, LLMs encounter users
with personalized factual knowledge, and such personalized knowledge is
consistently reflected through users' interactions with the LLMs. To enhance
user experience, real-time model personalization is essential, allowing LLMs to
adapt user-specific knowledge based on user feedback during human-LLM
interactions. Existing methods mostly require back-propagation to finetune the
model parameters, which incurs high computational and memory costs. In
addition, these methods suffer from low interpretability, which will cause
unforeseen impacts on model performance during long-term use, where the user's
personalized knowledge is accumulated extensively.To address these challenges,
we propose Knowledge Graph Tuning (KGT), a novel approach that leverages
knowledge graphs (KGs) to personalize LLMs. KGT extracts personalized factual
knowledge triples from users' queries and feedback and optimizes KGs without
modifying the LLM parameters. Our method improves computational and memory
efficiency by avoiding back-propagation and ensures interpretability by making
the KG adjustments comprehensible to humans.Experiments with state-of-the-art
LLMs, including GPT-2, Llama2, and Llama3, show that KGT significantly improves
personalization performance while reducing latency and GPU memory costs.
Ultimately, KGT offers a promising solution of effective, efficient, and
interpretable real-time LLM personalization during user interactions with the
LLMs.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.19686v1},
File          = {2405.19686v1.pdf}
}
@article{2404.10317v2,
Author        = {Hamed Babaei Giglou and Jennifer D'Souza and Felix Engel and Sören Auer},
Title         = {LLMs4OM: Matching Ontologies with Large Language Models},
Eprint        = {2404.10317v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology Matching (OM), is a critical task in knowledge integration, where
aligning heterogeneous ontologies facilitates data interoperability and
knowledge sharing. Traditional OM systems often rely on expert knowledge or
predictive models, with limited exploration of the potential of Large Language
Models (LLMs). We present the LLMs4OM framework, a novel approach to evaluate
the effectiveness of LLMs in OM tasks. This framework utilizes two modules for
retrieval and matching, respectively, enhanced by zero-shot prompting across
three ontology representations: concept, concept-parent, and concept-children.
Through comprehensive evaluations using 20 OM datasets from various domains, we
demonstrate that LLMs, under the LLMs4OM framework, can match and even surpass
the performance of traditional OM systems, particularly in complex matching
scenarios. Our results highlight the potential of LLMs to significantly
contribute to the field of OM.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.10317v2},
File          = {2404.10317v2.pdf}
}
@article{2205.08184v1,
Author        = {Fedor Moiseev and Zhe Dong and Enrique Alfonseca and Martin Jaggi},
Title         = {SKILL: Structured Knowledge Infusion for Large Language Models},
Eprint        = {2205.08184v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have demonstrated human-level performance on a
vast spectrum of natural language tasks. However, it is largely unexplored
whether they can better internalize knowledge from a structured data, such as a
knowledge graph, or from text. In this work, we propose a method to infuse
structured knowledge into LLMs, by directly training T5 models on factual
triples of knowledge graphs (KGs). We show that models pre-trained on Wikidata
KG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as
well as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The
models pre-trained on factual triples compare competitively with the ones on
natural language sentences that contain the same knowledge. Trained on a
smaller size KG, WikiMovies, we saw 3x improvement of exact match score on
MetaQA task compared to T5 baseline. The proposed method has an advantage that
no alignment between the knowledge graph and text corpus is required in
curating training data. This makes our method particularly useful when working
with industry-scale knowledge graphs.},
Year          = {2022},
Month         = {May},
Url           = {http://arxiv.org/abs/2205.08184v1},
File          = {2205.08184v1.pdf}
}
@article{2407.10909v2,
Author        = {Xiaohui Victor Li and Francesco Sanna Passino},
Title         = {FinDKG: Dynamic Knowledge Graphs with Large Language Models for
  Detecting Global Trends in Financial Markets},
Eprint        = {2407.10909v2},
DOI           = {10.1145/3677052.3698603},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-fin.CP},
Abstract      = {Dynamic knowledge graphs (DKGs) are popular structures to express different
types of connections between objects over time. They can also serve as an
efficient mathematical tool to represent information extracted from complex
unstructured data sources, such as text or images. Within financial
applications, DKGs could be used to detect trends for strategic thematic
investing, based on information obtained from financial news articles. In this
work, we explore the properties of large language models (LLMs) as dynamic
knowledge graph generators, proposing a novel open-source fine-tuned LLM for
this purpose, called the Integrated Contextual Knowledge Graph Generator
(ICKG). We use ICKG to produce a novel open-source DKG from a corpus of
financial news articles, called FinDKG, and we propose an attention-based GNN
architecture for analysing it, called KGTransformer. We test the performance of
the proposed model on benchmark datasets and FinDKG, demonstrating superior
performance on link prediction tasks. Additionally, we evaluate the performance
of the KGTransformer on FinDKG for thematic investing, showing it can
outperform existing thematic ETFs.},
Year          = {2024},
Month         = {Jul},
Note          = {ICAIF '24: Proceedings of the 5th ACM International Conference on
  AI in Finance, 573-581 (2024)},
Url           = {http://arxiv.org/abs/2407.10909v2},
File          = {2407.10909v2.pdf}
}
@article{2409.12853v2,
Author        = {Hakan T. Otal and Stephen V. Faraone and M. Abdullah Canbaz},
Title         = {A New Perspective on ADHD Research: Knowledge Graph Construction with
  LLMs and Network Based Insights},
Eprint        = {2409.12853v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SI},
Abstract      = {Attention-Deficit/Hyperactivity Disorder (ADHD) is a challenging disorder to
study due to its complex symptomatology and diverse contributing factors. To
explore how we can gain deeper insights on this topic, we performed a network
analysis on a comprehensive knowledge graph (KG) of ADHD, constructed by
integrating scientific literature and clinical data with the help of
cutting-edge large language models. The analysis, including k-core techniques,
identified critical nodes and relationships that are central to understanding
the disorder. Building on these findings, we curated a knowledge graph that is
usable in a context-aware chatbot (Graph-RAG) with Large Language Models
(LLMs), enabling accurate and informed interactions. Our knowledge graph not
only advances the understanding of ADHD but also provides a powerful tool for
research and clinical applications.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.12853v2},
File          = {2409.12853v2.pdf}
}
@article{2412.20163v3,
Author        = {Minhye Jeon and Seokho Ahn and Young-Duk Seo},
Title         = {Topic-Aware Knowledge Graph with Large Language Models for
  Interoperability in Recommender Systems},
Eprint        = {2412.20163v3},
DOI           = {10.1145/3672608.3707958},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The use of knowledge graphs in recommender systems has become one of the
common approaches to addressing data sparsity and cold start problems. Recent
advances in large language models (LLMs) offer new possibilities for processing
side and context information within knowledge graphs. However, consistent
integration across various systems remains challenging due to the need for
domain expert intervention and differences in system characteristics. To
address these issues, we propose a consistent approach that extracts both
general and specific topics from both side and context information using LLMs.
First, general topics are iteratively extracted and updated from side
information. Then, specific topics are extracted using context information.
Finally, to address synonymous topics generated during the specific topic
extraction process, a refining algorithm processes and resolves these issues
effectively. This approach allows general topics to capture broad knowledge
across diverse item characteristics, while specific topics emphasize detailed
attributes, providing a more comprehensive understanding of the semantic
features of items and the preferences of users. Experimental results
demonstrate significant improvements in recommendation performance across
diverse knowledge graphs.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.20163v3},
File          = {2412.20163v3.pdf}
}
@article{2211.02744v2,
Author        = {Jason Youn and Ilias Tagkopoulos},
Title         = {KGLM: Integrating Knowledge Graph Structure in Language Models for Link
  Prediction},
Eprint        = {2211.02744v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The ability of knowledge graphs to represent complex relationships at scale
has led to their adoption for various needs including knowledge representation,
question-answering, and recommendation systems. Knowledge graphs are often
incomplete in the information they represent, necessitating the need for
knowledge graph completion tasks. Pre-trained and fine-tuned language models
have shown promise in these tasks although these models ignore the intrinsic
information encoded in the knowledge graph, namely the entity and relation
types. In this work, we propose the Knowledge Graph Language Model (KGLM)
architecture, where we introduce a new entity/relation embedding layer that
learns to differentiate distinctive entity and relation types, therefore
allowing the model to learn the structure of the knowledge graph. In this work,
we show that further pre-training the language models with this additional
embedding layer using the triples extracted from the knowledge graph, followed
by the standard fine-tuning phase sets a new state-of-the-art performance for
the link prediction task on the benchmark datasets.},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.02744v2},
File          = {2211.02744v2.pdf}
}
@article{2404.17524v4,
Author        = {Luis Miguel Vieira da Silva and Aljosha Köcher and Felix Gehlhoff and Alexander Fay},
Title         = {On the Use of Large Language Models to Generate Capability Ontologies},
Eprint        = {2404.17524v4},
DOI           = {10.1109/ETFA61755.2024.10710775},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Capability ontologies are increasingly used to model functionalities of
systems or machines. The creation of such ontological models with all
properties and constraints of capabilities is very complex and can only be done
by ontology experts. However, Large Language Models (LLMs) have shown that they
can generate machine-interpretable models from natural language text input and
thus support engineers / ontology experts. Therefore, this paper investigates
how LLMs can be used to create capability ontologies. We present a study with a
series of experiments in which capabilities with varying complexities are
generated using different prompting techniques and with different LLMs. Errors
in the generated ontologies are recorded and compared. To analyze the quality
of the generated ontologies, a semi-automated approach based on RDF syntax
checking, OWL reasoning, and SHACL constraints is used. The results of this
study are very promising because even for complex capabilities, the generated
ontologies are almost free of errors.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.17524v4},
File          = {2404.17524v4.pdf}
}
@article{2501.06465v2,
Author        = {Ye Chen and Dongdong Huang and Haoyun Xu and Cong Fu and Lin Sheng and Qingli Zhou and Yuqiang Shen and Kai Wang},
Title         = {MedCT: A Clinical Terminology Graph for Generative AI Applications in
  Healthcare},
Eprint        = {2501.06465v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce the world's first clinical terminology for the Chinese
healthcare community, namely MedCT, accompanied by a clinical foundation model
MedBERT and an entity linking model MedLink. The MedCT system enables
standardized and programmable representation of Chinese clinical data,
successively stimulating the development of new medicines, treatment pathways,
and better patient outcomes for the populous Chinese community. Moreover, the
MedCT knowledge graph provides a principled mechanism to minimize the
hallucination problem of large language models (LLMs), therefore achieving
significant levels of accuracy and safety in LLM-based clinical applications.
By leveraging the LLMs' emergent capabilities of generativeness and
expressiveness, we were able to rapidly built a production-quality terminology
system and deployed to real-world clinical field within three months, while
classical terminologies like SNOMED CT have gone through more than twenty years
development. Our experiments show that the MedCT system achieves
state-of-the-art (SOTA) performance in semantic matching and entity linking
tasks, not only for Chinese but also for English. We also conducted a
longitudinal field experiment by applying MedCT and LLMs in a representative
spectrum of clinical tasks, including electronic health record (EHR)
auto-generation and medical document search for diagnostic decision making. Our
study shows a multitude of values of MedCT for clinical workflows and patient
outcomes, especially in the new genre of clinical LLM applications. We present
our approach in sufficient engineering detail, such that implementing a
clinical terminology for other non-English societies should be readily
reproducible. We openly release our terminology, models and algorithms, along
with real-world clinical datasets for the development.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.06465v2},
File          = {2501.06465v2.pdf}
}
@article{2308.16622v1,
Author        = {Lars-Peter Meyer and Johannes Frey and Kurt Junghanns and Felix Brei and Kirill Bulert and Sabine Gründer-Fahrer and Michael Martin},
Title         = {Developing a Scalable Benchmark for Assessing Large Language Models in
  Knowledge Graph Engineering},
Eprint        = {2308.16622v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {As the field of Large Language Models (LLMs) evolves at an accelerated pace,
the critical need to assess and monitor their performance emerges. We introduce
a benchmarking framework focused on knowledge graph engineering (KGE)
accompanied by three challenges addressing syntax and error correction, facts
extraction and dataset generation. We show that while being a useful tool, LLMs
are yet unfit to assist in knowledge graph generation with zero-shot prompting.
Consequently, our LLM-KG-Bench framework provides automatic evaluation and
storage of LLM responses as well as statistical data and visualization tools to
support tracking of prompt engineering and model performance.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.16622v1},
File          = {2308.16622v1.pdf}
}
@article{2311.08732v1,
Author        = {Minze Chen and Zhenxiang Tao and Weitong Tang and Tingxin Qin and Rui Yang and Chunli Zhu},
Title         = {Enhancing Emergency Decision-making with Knowledge Graphs and Large
  Language Models},
Eprint        = {2311.08732v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Emergency management urgently requires comprehensive knowledge while having a
high possibility to go beyond individuals' cognitive scope. Therefore,
artificial intelligence(AI) supported decision-making under that circumstance
is of vital importance. Recent emerging large language models (LLM) provide a
new direction for enhancing targeted machine intelligence. However, the
utilization of LLM directly would inevitably introduce unreliable output for
its inherent issue of hallucination and poor reasoning skills. In this work, we
develop a system called Enhancing Emergency decision-making with Knowledge
Graph and LLM (E-KELL), which provides evidence-based decision-making in
various emergency stages. The study constructs a structured emergency knowledge
graph and guides LLMs to reason over it via a prompt chain. In real-world
evaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in
comprehensibility, accuracy, conciseness, and instructiveness from a group of
emergency commanders and firefighters, demonstrating a significant improvement
across various situations compared to baseline models. This work introduces a
novel approach to providing reliable emergency decision support.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.08732v1},
File          = {2311.08732v1.pdf}
}
@article{2402.11441v2,
Author        = {Fali Wang and Runxue Bao and Suhang Wang and Wenchao Yu and Yanchi Liu and Wei Cheng and Haifeng Chen},
Title         = {InfuserKI: Enhancing Large Language Models with Knowledge Graphs via
  Infuser-Guided Knowledge Integration},
Eprint        = {2402.11441v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have achieved exceptional capabilities in open
generation across various domains, yet they encounter difficulties with tasks
that require intensive knowledge. To address these challenges, methods for
integrating knowledge have been developed, which augment LLMs with
domain-specific knowledge graphs through external modules. These approaches,
however, face data inefficiency issues as they necessitate the processing of
both known and unknown knowledge for fine-tuning. Thus, our research focuses on
a novel problem: efficiently integrating unknown knowledge into LLMs without
unnecessary overlap of known knowledge. A risk of introducing new knowledge is
the potential forgetting of existing knowledge. To mitigate this risk, we
propose the innovative {\method} framework. This framework employs transformer
internal states to determine when to enrich LLM outputs with additional
information, effectively preventing knowledge forgetting. Performance
evaluations using the UMLS-2.5k and MetaQA domain knowledge graphs reveal that
{\method} not only successfully integrates new knowledge but also outperforms
state-of-the-art baselines, reducing knowledge forgetting by 9\% and 6\%,
respectively.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.11441v2},
File          = {2402.11441v2.pdf}
}
@article{2404.17000v1,
Author        = {Bradley P. Allen and Paul T. Groth},
Title         = {Evaluating Class Membership Relations in Knowledge Graphs using Large
  Language Models},
Eprint        = {2404.17000v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {A backbone of knowledge graphs are their class membership relations, which
assign entities to a given class. As part of the knowledge engineering process,
we propose a new method for evaluating the quality of these relations by
processing descriptions of a given entity and class using a zero-shot
chain-of-thought classifier that uses a natural language intensional definition
of a class. We evaluate the method using two publicly available knowledge
graphs, Wikidata and CaLiGraph, and 7 large language models. Using the
gpt-4-0125-preview large language model, the method's classification
performance achieves a macro-averaged F1-score of 0.830 on data from Wikidata
and 0.893 on data from CaLiGraph. Moreover, a manual analysis of the
classification errors shows that 40.9% of errors were due to the knowledge
graphs, with 16.0% due to missing relations and 24.9% due to incorrectly
asserted relations. These results show how large language models can assist
knowledge engineers in the process of knowledge graph refinement. The code and
data are available on Github.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.17000v1},
File          = {2404.17000v1.pdf}
}
@article{2309.17122v1,
Author        = {Johannes Frey and Lars-Peter Meyer and Natanael Arndt and Felix Brei and Kirill Bulert},
Title         = {Benchmarking the Abilities of Large Language Models for RDF Knowledge
  Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?},
Eprint        = {2309.17122v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models (LLMs) are advancing at a rapid pace, with significant
improvements at natural language processing and coding tasks. Yet, their
ability to work with formal languages representing data, specifically within
the realm of knowledge graph engineering, remains under-investigated. To
evaluate the proficiency of various LLMs, we created a set of five tasks that
probe their ability to parse, understand, analyze, and create knowledge graphs
serialized in Turtle syntax. These tasks, each embodying distinct degrees of
complexity and being able to scale with the size of the problem, have been
integrated into our automated evaluation system, the LLM-KG-Bench. The
evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4,
Claude 1.3, and Claude 2.0, as well as two freely accessible offline models,
GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth
understanding of the strengths and shortcomings of LLMs in relation to their
application within RDF knowledge graph engineering workflows utilizing Turtle
representation. While our findings show that the latest commercial models
outperform their forerunners in terms of proficiency with the Turtle language,
they also reveal an apparent weakness. These models fall short when it comes to
adhering strictly to the output formatting constraints, a crucial requirement
in this context.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.17122v1},
File          = {2309.17122v1.pdf}
}
@article{2404.09296v2,
Author        = {Tuan Bui and Oanh Tran and Phuong Nguyen and Bao Ho and Long Nguyen and Thang Bui and Tho Quan},
Title         = {Cross-Data Knowledge Graph Construction for LLM-enabled Educational
  Question-Answering System: A Case Study at HCMUT},
Eprint        = {2404.09296v2},
DOI           = {10.1145/3643479.3662055},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In today's rapidly evolving landscape of Artificial Intelligence, large
language models (LLMs) have emerged as a vibrant research topic. LLMs find
applications in various fields and contribute significantly. Despite their
powerful language capabilities, similar to pre-trained language models (PLMs),
LLMs still face challenges in remembering events, incorporating new
information, and addressing domain-specific issues or hallucinations. To
overcome these limitations, researchers have proposed Retrieval-Augmented
Generation (RAG) techniques, some others have proposed the integration of LLMs
with Knowledge Graphs (KGs) to provide factual context, thereby improving
performance and delivering more accurate feedback to user queries.
  Education plays a crucial role in human development and progress. With the
technology transformation, traditional education is being replaced by digital
or blended education. Therefore, educational data in the digital environment is
increasing day by day. Data in higher education institutions are diverse,
comprising various sources such as unstructured/structured text, relational
databases, web/app-based API access, etc. Constructing a Knowledge Graph from
these cross-data sources is not a simple task. This article proposes a method
for automatically constructing a Knowledge Graph from multiple data sources and
discusses some initial applications (experimental trials) of KG in conjunction
with LLMs for question-answering tasks.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.09296v2},
File          = {2404.09296v2.pdf}
}
@article{2409.03155v1,
Author        = {Jie Ma and Zhitao Gao and Qi Chai and Wangchun Sun and Pinghui Wang and Hongbin Pei and Jing Tao and Lingyun Song and Jun Liu and Chen Zhang and Lizhen Cui},
Title         = {Debate on Graph: a Flexible and Reliable Reasoning Framework for Large
  Language Models},
Eprint        = {2409.03155v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) may suffer from hallucinations in real-world
applications due to the lack of relevant knowledge. In contrast, knowledge
graphs encompass extensive, multi-relational structures that store a vast array
of symbolic facts. Consequently, integrating LLMs with knowledge graphs has
been extensively explored, with Knowledge Graph Question Answering (KGQA)
serving as a critical touchstone for the integration. This task requires LLMs
to answer natural language questions by retrieving relevant triples from
knowledge graphs. However, existing methods face two significant challenges:
\textit{excessively long reasoning paths distracting from the answer
generation}, and \textit{false-positive relations hindering the path
refinement}. In this paper, we propose an iterative interactive KGQA framework
that leverages the interactive learning capabilities of LLMs to perform
reasoning and Debating over Graphs (DoG). Specifically, DoG employs a
subgraph-focusing mechanism, allowing LLMs to perform answer trying after each
reasoning step, thereby mitigating the impact of lengthy reasoning paths. On
the other hand, DoG utilizes a multi-role debate team to gradually simplify
complex questions, reducing the influence of false-positive relations. This
debate mechanism ensures the reliability of the reasoning process. Experimental
results on five public datasets demonstrate the effectiveness and superiority
of our architecture. Notably, DoG outperforms the state-of-the-art method ToG
by 23.7\% and 9.1\% in accuracy on WebQuestions and GrailQA, respectively.
Furthermore, the integration experiments with various LLMs on the mentioned
datasets highlight the flexibility of DoG. Code is available at
\url{https://github.com/reml-group/DoG}.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.03155v1},
File          = {2409.03155v1.pdf}
}
@article{2412.20995v1,
Author        = {Siyuan Fang and Kaijing Ma and Tianyu Zheng and Xinrun Du and Ningxuan Lu and Ge Zhang and Qingkun Tang},
Title         = {KARPA: A Training-free Method of Adapting Knowledge Graph as References
  for Large Language Model's Reasoning Path Aggregation},
Eprint        = {2412.20995v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) demonstrate exceptional performance across a
variety of tasks, yet they are often affected by hallucinations and the
timeliness of knowledge. Leveraging knowledge graphs (KGs) as external
knowledge sources has emerged as a viable solution, but existing methods for
LLM-based knowledge graph question answering (KGQA) are often limited by
step-by-step decision-making on KGs, restricting the global planning and
reasoning capabilities of LLMs, or they require fine-tuning or pre-training on
specific KGs. To address these challenges, we propose Knowledge graph Assisted
Reasoning Path Aggregation (KARPA), a novel framework that harnesses the global
planning abilities of LLMs for efficient and accurate KG reasoning. KARPA
operates in three steps: pre-planning relation paths using the LLM's global
planning capabilities, matching semantically relevant paths via an embedding
model, and reasoning over these paths to generate answers. Unlike existing KGQA
methods, KARPA avoids stepwise traversal, requires no additional training, and
is adaptable to various LLM architectures. Extensive experimental results show
that KARPA achieves state-of-the-art performance in KGQA tasks, delivering both
high efficiency and accuracy. Our code will be available on Github.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.20995v1},
File          = {2412.20995v1.pdf}
}
@article{2406.01145v1,
Author        = {Guangyi Liu and Yongqi Zhang and Yong Li and Quanming Yao},
Title         = {Explore then Determine: A GNN-LLM Synergy Framework for Reasoning over
  Knowledge Graph},
Eprint        = {2406.01145v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The task of reasoning over Knowledge Graphs (KGs) poses a significant
challenge for Large Language Models (LLMs) due to the complex structure and
large amounts of irrelevant information. Existing LLM reasoning methods
overlook the importance of compositional learning on KG to supply with precise
knowledge. Besides, the fine-tuning and frequent interaction with LLMs incur
substantial time and resource costs. This paper focuses on the Question
Answering over Knowledge Graph (KGQA) task and proposes an
Explore-then-Determine (EtD) framework that synergizes LLMs with graph neural
networks (GNNs) for reasoning over KGs. The Explore stage employs a lightweight
GNN to explore promising candidates and relevant fine-grained knowledge to the
questions, while the Determine stage utilizes the explored information to
construct a knowledge-enhanced multiple-choice prompt, guiding a frozen LLM to
determine the final answer. Extensive experiments on three benchmark KGQA
datasets demonstrate that EtD achieves state-of-the-art performance and
generates faithful reasoning results.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.01145v1},
File          = {2406.01145v1.pdf}
}
@article{2406.06621v2,
Author        = {Harry Li and Gabriel Appleby and Ashley Suh},
Title         = {LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph
  Question-Answering},
Eprint        = {2406.06621v2},
DOI           = {10.1109/VIS55277.2024.00031},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present LinkQ, a system that leverages a large language model (LLM) to
facilitate knowledge graph (KG) query construction through natural language
question-answering. Traditional approaches often require detailed knowledge of
a graph querying language, limiting the ability for users -- even experts -- to
acquire valuable insights from KGs. LinkQ simplifies this process by
implementing a multistep protocol in which the LLM interprets a user's
question, then systematically converts it into a well-formed query. LinkQ helps
users iteratively refine any open-ended questions into precise ones, supporting
both targeted and exploratory analysis. Further, LinkQ guards against the LLM
hallucinating outputs by ensuring users' questions are only ever answered from
ground truth KG data. We demonstrate the efficacy of LinkQ through a
qualitative study with five KG practitioners. Our results indicate that
practitioners find LinkQ effective for KG question-answering, and desire future
LLM-assisted exploratory data analysis systems.},
Year          = {2024},
Month         = {Jun},
Note          = {H. Li, G. Appleby and A. Suh, "LinkQ: An LLM-Assisted Visual
  Interface for Knowledge Graph Question-Answering," 2024 IEEE Visualization
  and Visual Analytics (VIS)},
Url           = {http://arxiv.org/abs/2406.06621v2},
File          = {2406.06621v2.pdf}
}
@article{2309.11206v2,
Author        = {Yike Wu and Nan Hu and Sheng Bi and Guilin Qi and Jie Ren and Anhuan Xie and Wei Song},
Title         = {Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for
  Knowledge Graph Question Answering},
Eprint        = {2309.11206v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Despite their competitive performance on knowledge-intensive tasks, large
language models (LLMs) still have limitations in memorizing all world knowledge
especially long tail knowledge. In this paper, we study the KG-augmented
language model approach for solving the knowledge graph question answering
(KGQA) task that requires rich world knowledge. Existing work has shown that
retrieving KG knowledge to enhance LLMs prompting can significantly improve
LLMs performance in KGQA. However, their approaches lack a well-formed
verbalization of KG knowledge, i.e., they ignore the gap between KG
representations and textual representations. To this end, we propose an
answer-sensitive KG-to-Text approach that can transform KG knowledge into
well-textualized statements most informative for KGQA. Based on this approach,
we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task.
Experiments on several KGQA benchmarks show that the proposed KG-to-Text
augmented LLMs approach outperforms previous KG-augmented LLMs approaches
regarding answer accuracy and usefulness of knowledge statements.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.11206v2},
File          = {2309.11206v2.pdf}
}
@article{2409.05370v1,
Author        = {Yingshu Li and Zhanyu Wang and Yunyi Liu and Lei Wang and Lingqiao Liu and Luping Zhou},
Title         = {KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using
  Large Language Models},
Eprint        = {2409.05370v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Harnessing the robust capabilities of Large Language Models (LLMs) for
narrative generation, logical reasoning, and common-sense knowledge
integration, this study delves into utilizing LLMs to enhance automated
radiology report generation (R2Gen). Despite the wealth of knowledge within
LLMs, efficiently triggering relevant knowledge within these large models for
specific tasks like R2Gen poses a critical research challenge. This paper
presents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration
framework based on LLMs. Utilizing a frozen LLM to generate reports, the
framework integrates a knowledge graph to unlock chest disease-related
knowledge within the LLM to enhance the clinical utility of generated reports.
This is achieved by leveraging the knowledge graph to distill disease-related
features in a designed way. Since a radiology report encompasses both normal
and disease-related findings, the extracted graph-enhanced disease-related
features are integrated with regional image features, attending to both
aspects. We explore two fusion methods to automatically prioritize and select
the most relevant features. The fused features are employed by LLM to generate
reports that are more sensitive to diseases and of improved quality. Our
approach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.05370v1},
File          = {2409.05370v1.pdf}
}
@article{2409.05925v1,
Author        = {Lars-Peter Meyer and Johannes Frey and Felix Brei and Natanael Arndt},
Title         = {Assessing SPARQL capabilities of Large Language Models},
Eprint        = {2409.05925v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)
offers significant synergistic potential for knowledge-driven applications. One
possible integration is the interpretation and generation of formal languages,
such as those used in the Semantic Web, with SPARQL being a core technology for
accessing KGs. In this paper, we focus on measuring out-of-the box capabilities
of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries
applying a quantitative approach.
  We implemented various benchmarking tasks in the LLM-KG-Bench framework for
automated execution and evaluation with several LLMs. The tasks assess
capabilities along the dimensions of syntax, semantic read, semantic create,
and the role of knowledge graph prompt inclusion.
  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,
and Claude models. Our findings indicate that working with SPARQL SELECT
queries is still challenging for LLMs and heavily depends on the specific LLM
as well as the complexity of the task. While fixing basic syntax errors seems
to pose no problems for the best of the current LLMs evaluated, creating
semantically correct SPARQL SELECT queries is difficult in several cases.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.05925v1},
File          = {2409.05925v1.pdf}
}
@article{2410.13080v1,
Author        = {Linhao Luo and Zicheng Zhao and Chen Gong and Gholamreza Haffari and Shirui Pan},
Title         = {Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with
  Large Language Models},
Eprint        = {2410.13080v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have demonstrated impressive reasoning
abilities, but they still struggle with faithful reasoning due to knowledge
gaps and hallucinations. To address these issues, knowledge graphs (KGs) have
been utilized to enhance LLM reasoning through their structured knowledge.
However, existing KG-enhanced methods, either retrieval-based or agent-based,
encounter difficulties in accurately retrieving knowledge and efficiently
traversing KGs at scale. In this work, we introduce graph-constrained reasoning
(GCR), a novel framework that bridges structured knowledge in KGs with
unstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures
faithful KG-grounded reasoning by integrating KG structure into the LLM
decoding process through KG-Trie, a trie-based index that encodes KG reasoning
paths. KG-Trie constrains the decoding process, allowing LLMs to directly
reason on graphs and generate faithful reasoning paths grounded in KGs.
Additionally, GCR leverages a lightweight KG-specialized LLM for
graph-constrained reasoning alongside a powerful general LLM for inductive
reasoning over multiple reasoning paths, resulting in accurate reasoning with
zero reasoning hallucination. Extensive experiments on several KGQA benchmarks
demonstrate that GCR achieves state-of-the-art performance and exhibits strong
zero-shot generalizability to unseen KGs without additional training.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.13080v1},
File          = {2410.13080v1.pdf}
}
@article{2404.14741v3,
Author        = {Yao Xu and Shizhu He and Jiabei Chen and Zihao Wang and Yangqiu Song and Hanghang Tong and Guang Liu and Kang Liu and Jun Zhao},
Title         = {Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete
  Knowledge Graph Question Answering},
Eprint        = {2404.14741v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {To address the issues of insufficient knowledge and hallucination in Large
Language Models (LLMs), numerous studies have explored integrating LLMs with
Knowledge Graphs (KGs). However, these methods are typically evaluated on
conventional Knowledge Graph Question Answering (KGQA) with complete KGs, where
all factual triples required for each question are entirely covered by the
given KG. In such cases, LLMs primarily act as an agent to find answer entities
within the KG, rather than effectively integrating the internal knowledge of
LLMs and external knowledge sources such as KGs. In fact, KGs are often
incomplete to cover all the knowledge required to answer questions. To simulate
these real-world scenarios and evaluate the ability of LLMs to integrate
internal and external knowledge, we propose leveraging LLMs for QA under
Incomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the
factual triples for each question, and construct corresponding datasets. To
handle IKGQA, we propose a training-free method called Generate-on-Graph (GoG),
which can generate new factual triples while exploring KGs. Specifically, GoG
performs reasoning through a Thinking-Searching-Generating framework, which
treats LLM as both Agent and KG in IKGQA. Experimental results on two datasets
demonstrate that our GoG outperforms all previous methods.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.14741v3},
File          = {2404.14741v3.pdf}
}
@article{2412.13782v2,
Author        = {Yifan Lu and Yigeng Zhou and Jing Li and Yequan Wang and Xuebo Liu and Daojing He and Fangming Liu and Min Zhang},
Title         = {Knowledge Editing with Dynamic Knowledge Graphs for Multi-Hop Question
  Answering},
Eprint        = {2412.13782v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Multi-hop question answering (MHQA) poses a significant challenge for large
language models (LLMs) due to the extensive knowledge demands involved.
Knowledge editing, which aims to precisely modify the LLMs to incorporate
specific knowledge without negatively impacting other unrelated knowledge,
offers a potential solution for addressing MHQA challenges with LLMs. However,
current solutions struggle to effectively resolve issues of knowledge
conflicts. Most parameter-preserving editing methods are hindered by inaccurate
retrieval and overlook secondary editing issues, which can introduce noise into
the reasoning process of LLMs. In this paper, we introduce KEDKG, a novel
knowledge editing method that leverages a dynamic knowledge graph for MHQA,
designed to ensure the reliability of answers. KEDKG involves two primary
steps: dynamic knowledge graph construction and knowledge graph augmented
generation. Initially, KEDKG autonomously constructs a dynamic knowledge graph
to store revised information while resolving potential knowledge conflicts.
Subsequently, it employs a fine-grained retrieval strategy coupled with an
entity and relation detector to enhance the accuracy of graph retrieval for LLM
generation. Experimental results on benchmarks show that KEDKG surpasses
previous state-of-the-art models, delivering more accurate and reliable answers
in environments with dynamic information.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.13782v2},
File          = {2412.13782v2.pdf}
}
@article{2404.03044v1,
Author        = {Marcin P. Joachimiak and Mark A. Miller and J. Harry Caufield and Ryan Ly and Nomi L. Harris and Andrew Tritt and Christopher J. Mungall and Kristofer E. Bouchard},
Title         = {The Artificial Intelligence Ontology: LLM-assisted construction of AI
  concept hierarchies},
Eprint        = {2404.03044v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The Artificial Intelligence Ontology (AIO) is a systematization of artificial
intelligence (AI) concepts, methodologies, and their interrelations. Developed
via manual curation, with the additional assistance of large language models
(LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a
comprehensive framework that encompasses both technical and ethical aspects of
AI technologies. The primary audience for AIO includes AI researchers,
developers, and educators seeking standardized terminology and concepts within
the AI domain. The ontology is structured around six top-level branches:
Networks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to
support the modular composition of AI methods and facilitate a deeper
understanding of deep learning architectures and ethical considerations in AI.
  AIO's development utilized the Ontology Development Kit (ODK) for its
creation and maintenance, with its content being dynamically updated through
AI-driven curation support. This approach not only ensures the ontology's
relevance amidst the fast-paced advancements in AI but also significantly
enhances its utility for researchers, developers, and educators by simplifying
the integration of new AI concepts and methodologies.
  The ontology's utility is demonstrated through the annotation of AI methods
data in a catalog of AI research publications and the integration into the
BioPortal ontology resource, highlighting its potential for cross-disciplinary
research. The AIO ontology is open source and is available on GitHub
(https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal
(https://bioportal.bioontology.org/ontologies/AIO).},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.03044v1},
File          = {2404.03044v1.pdf}
}
@article{2404.01425v1,
Author        = {Harry Li and Gabriel Appleby and Ashley Suh},
Title         = {A Preliminary Roadmap for LLMs as Assistants in Exploring, Analyzing,
  and Visualizing Knowledge Graphs},
Eprint        = {2404.01425v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {We present a mixed-methods study to explore how large language models (LLMs)
can assist users in the visual exploration and analysis of knowledge graphs
(KGs). We surveyed and interviewed 20 professionals from industry, government
laboratories, and academia who regularly work with KGs and LLMs, either
collaboratively or concurrently. Our findings show that participants
overwhelmingly want an LLM to facilitate data retrieval from KGs through joint
query construction, to identify interesting relationships in the KG through
multi-turn conversation, and to create on-demand visualizations from the KG
that enhance their trust in the LLM's outputs. To interact with an LLM,
participants strongly prefer a chat-based 'widget,' built on top of their
regular analysis workflows, with the ability to guide the LLM using their
interactions with a visualization. When viewing an LLM's outputs, participants
similarly prefer a combination of annotated visuals (e.g., subgraphs or tables
extracted from the KG) alongside summarizing text. However, participants also
expressed concerns about an LLM's ability to maintain semantic intent when
translating natural language questions into KG queries, the risk of an LLM
'hallucinating' false data from the KG, and the difficulties of engineering a
'perfect prompt.' From the analysis of our interviews, we contribute a
preliminary roadmap for the design of LLM-driven knowledge graph exploration
systems and outline future opportunities in this emergent design space.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.01425v1},
File          = {2404.01425v1.pdf}
}
@article{2310.07170v1,
Author        = {Tatsuya Ide and Eiki Murata and Daisuke Kawahara and Takato Yamazaki and Shengzhe Li and Kenta Shinzato and Toshinori Sato},
Title         = {PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a
  Language Model},
Eprint        = {2310.07170v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Despite the remarkable progress in natural language understanding with
pretrained Transformers, neural language models often do not handle commonsense
knowledge well. Toward commonsense-aware models, there have been attempts to
obtain knowledge, ranging from automatic acquisition to crowdsourcing. However,
it is difficult to obtain a high-quality knowledge base at a low cost,
especially from scratch. In this paper, we propose PHALM, a method of building
a knowledge graph from scratch, by prompting both crowdworkers and a large
language model (LLM). We used this method to build a Japanese event knowledge
graph and trained Japanese commonsense generation models. Experimental results
revealed the acceptability of the built graph and inferences generated by the
trained models. We also report the difference in prompting humans and an LLM.
Our code, data, and models are available at
github.com/nlp-waseda/comet-atomic-ja.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.07170v1},
File          = {2310.07170v1.pdf}
}
@article{2403.01972v1,
Author        = {Derong Xu and Ziheng Zhang and Zhenxi Lin and Xian Wu and Zhihong Zhu and Tong Xu and Xiangyu Zhao and Yefeng Zheng and Enhong Chen},
Title         = {Multi-perspective Improvement of Knowledge Graph Completion with Large
  Language Models},
Eprint        = {2403.01972v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graph completion (KGC) is a widely used method to tackle
incompleteness in knowledge graphs (KGs) by making predictions for missing
links. Description-based KGC leverages pre-trained language models to learn
entity and relation representations with their names or descriptions, which
shows promising results. However, the performance of description-based KGC is
still limited by the quality of text and the incomplete structure, as it lacks
sufficient entity descriptions and relies solely on relation names, leading to
sub-optimal results. To address this issue, we propose MPIKGC, a general
framework to compensate for the deficiency of contextualized knowledge and
improve KGC by querying large language models (LLMs) from various perspectives,
which involves leveraging the reasoning, explanation, and summarization
capabilities of LLMs to expand entity descriptions, understand relations, and
extract structures, respectively. We conducted extensive evaluation of the
effectiveness and improvement of our framework based on four description-based
KGC models and four datasets, for both link prediction and triplet
classification tasks.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.01972v1},
File          = {2403.01972v1.pdf}
}
@article{2404.09077v1,
Author        = {Zukang Yang and Zixuan Zhu},
Title         = {CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge
  Graph Prompting},
Eprint        = {2404.09077v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In the field of Question Answering (QA), unifying large language models
(LLMs) with external databases has shown great success. However, these methods
often fall short in providing the advanced reasoning needed for complex QA
tasks. To address these issues, we improve over a novel approach called
Knowledge Graph Prompting (KGP), which combines knowledge graphs with a
LLM-based agent to improve reasoning and search accuracy. Nevertheless, the
original KGP framework necessitates costly fine-tuning with large datasets yet
still suffers from LLM hallucination. Therefore, we propose a reasoning-infused
LLM agent to enhance this framework. This agent mimics human curiosity to ask
follow-up questions to more efficiently navigate the search. This simple
modification significantly boosts the LLM performance in QA tasks without the
high costs and latency associated with the initial KGP framework. Our ultimate
goal is to further develop this approach, leading to more accurate, faster, and
cost-effective solutions in the QA domain.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.09077v1},
File          = {2404.09077v1.pdf}
}
@article{2409.13252v1,
Author        = {Andrea Colombo},
Title         = {Leveraging Knowledge Graphs and LLMs to Support and Monitor Legislative
  Systems},
Eprint        = {2409.13252v1},
DOI           = {10.1145/3627673.3680268},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Knowledge Graphs (KGs) have been used to organize large datasets into
structured, interconnected information, enhancing data analytics across various
fields. In the legislative context, one potential natural application of KGs is
modeling the intricate set of interconnections that link laws and their
articles with each other and the broader legislative context.
  At the same time, the rise of large language models (LLMs) such as GPT has
opened new opportunities in legal applications, such as text generation and
document drafting. Despite their potential, the use of LLMs in legislative
contexts is critical since it requires the absence of hallucinations and
reliance on up-to-date information, as new laws are published on a daily basis.
  This work investigates how Legislative Knowledge Graphs and LLMs can
synergize and support legislative processes. We address three key questions:
the benefits of using KGs for legislative systems, how LLM can support
legislative activities by ensuring an accurate output, and how we can allow
non-technical users to use such technologies in their activities. To this aim,
we develop Legis AI Platform, an interactive platform focused on Italian
legislation that enhances the possibility of conducting legislative analysis
and that aims to support lawmaking activities.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.13252v1},
File          = {2409.13252v1.pdf}
}
@article{2412.03856v1,
Author        = {Patrick Ocheja and Brendan Flanagan and Yiling Dai and Hiroaki Ogata},
Title         = {How Good is ChatGPT in Giving Adaptive Guidance Using Knowledge Graphs
  in E-Learning Environments?},
Eprint        = {2412.03856v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {E-learning environments are increasingly harnessing large language models
(LLMs) like GPT-3.5 and GPT-4 for tailored educational support. This study
introduces an approach that integrates dynamic knowledge graphs with LLMs to
offer nuanced student assistance. By evaluating past and ongoing student
interactions, the system identifies and appends the most salient learning
context to prompts directed at the LLM. Central to this method is the knowledge
graph's role in assessing a student's comprehension of topic prerequisites.
Depending on the categorized understanding (good, average, or poor), the LLM
adjusts its guidance, offering advanced assistance, foundational reviews, or
in-depth prerequisite explanations, respectively. Preliminary findings suggest
students could benefit from this tiered support, achieving enhanced
comprehension and improved task outcomes. However, several issues related to
potential errors arising from LLMs were identified, which can potentially
mislead students. This highlights the need for human intervention to mitigate
these risks. This research aims to advance AI-driven personalized learning
while acknowledging the limitations and potential pitfalls, thus guiding future
research in technology and data-driven education.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.03856v1},
File          = {2412.03856v1.pdf}
}
@article{2408.00800v2,
Author        = {Jonathan Reif and Tom Jeleniewski and Milapji Singh Gill and Felix Gehlhoff and Alexander Fay},
Title         = {Chatbot-Based Ontology Interaction Using Large Language Models and
  Domain-Specific Standards},
Eprint        = {2408.00800v2},
DOI           = {10.1109/ETFA61755.2024.10711065},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The following contribution introduces a concept that employs Large Language
Models (LLMs) and a chatbot interface to enhance SPARQL query generation for
ontologies, thereby facilitating intuitive access to formalized knowledge.
Utilizing natural language inputs, the system converts user inquiries into
accurate SPARQL queries that strictly query the factual content of the
ontology, effectively preventing misinformation or fabrication by the LLM. To
enhance the quality and precision of outcomes, additional textual information
from established domain-specific standards is integrated into the ontology for
precise descriptions of its concepts and relationships. An experimental study
assesses the accuracy of generated SPARQL queries, revealing significant
benefits of using LLMs for querying ontologies and highlighting areas for
future research.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2408.00800v2},
File          = {2408.00800v2.pdf}
}
@article{2410.23584v1,
Author        = {Andy Lo and Albert Q. Jiang and Wenda Li and Mateja Jamnik},
Title         = {End-to-End Ontology Learning with Large Language Models},
Eprint        = {2410.23584v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Ontologies are useful for automatic machine processing of domain knowledge as
they represent it in a structured format. Yet, constructing ontologies requires
substantial manual effort. To automate part of this process, large language
models (LLMs) have been applied to solve various subtasks of ontology learning.
However, this partial ontology learning does not capture the interactions
between subtasks. We address this gap by introducing OLLM, a general and
scalable method for building the taxonomic backbone of an ontology from
scratch. Rather than focusing on subtasks, like individual relations between
entities, we model entire subcomponents of the target ontology by finetuning an
LLM with a custom regulariser that reduces overfitting on high-frequency
concepts. We introduce a novel suite of metrics for evaluating the quality of
the generated ontology by measuring its semantic and structural similarity to
the ground truth. In contrast to standard metrics, our metrics use deep
learning techniques to define more robust distance measures between graphs.
Both our quantitative and qualitative results on Wikipedia show that OLLM
outperforms subtask composition methods, producing more semantically accurate
ontologies while maintaining structural integrity. We further demonstrate that
our model can be effectively adapted to new domains, like arXiv, needing only a
small number of training examples. Our source code and datasets are available
at https://github.com/andylolu2/ollm.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.23584v1},
File          = {2410.23584v1.pdf}
}
@article{2405.17846v1,
Author        = {Yong Qi and Gabriel Kyebambo and Siyuan Xie and Wei Shen and Shenghui Wang and Bitao Xie and Bin He and Zhipeng Wang and Shuo Jiang},
Title         = {Safety Control of Service Robots with LLMs and Embodied Knowledge Graphs},
Eprint        = {2405.17846v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Safety limitations in service robotics across various industries have raised
significant concerns about the need for robust mechanisms ensuring that robots
adhere to safe practices, thereby preventing actions that might harm humans or
cause property damage. Despite advances, including the integration of Knowledge
Graphs (KGs) with Large Language Models (LLMs), challenges in ensuring
consistent safety in autonomous robot actions persist. In this paper, we
propose a novel integration of Large Language Models with Embodied Robotic
Control Prompts (ERCPs) and Embodied Knowledge Graphs (EKGs) to enhance the
safety framework for service robots. ERCPs are designed as predefined
instructions that ensure LLMs generate safe and precise responses. These
responses are subsequently validated by EKGs, which provide a comprehensive
knowledge base ensuring that the actions of the robot are continuously aligned
with safety protocols, thereby promoting safer operational practices in varied
contexts. Our experimental setup involved diverse real-world tasks, where
robots equipped with our framework demonstrated significantly higher compliance
with safety standards compared to traditional methods. This integration fosters
secure human-robot interactions and positions our methodology at the forefront
of AI-driven safety innovations in service robotics.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.17846v1},
File          = {2405.17846v1.pdf}
}
@article{2405.02105v1,
Author        = {Vladyslav Nechakhin and Jennifer D'Souza and Steffen Eger},
Title         = {Evaluating Large Language Models for Structured Science Summarization in
  the Open Research Knowledge Graph},
Eprint        = {2405.02105v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Structured science summaries or research contributions using properties or
dimensions beyond traditional keywords enhances science findability. Current
methods, such as those used by the Open Research Knowledge Graph (ORKG),
involve manually curating properties to describe research papers' contributions
in a structured manner, but this is labor-intensive and inconsistent between
the domain expert human curators. We propose using Large Language Models (LLMs)
to automatically suggest these properties. However, it's essential to assess
the readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task before
application. Our study performs a comprehensive comparative analysis between
ORKG's manually curated properties and those generated by the aforementioned
state-of-the-art LLMs. We evaluate LLM performance through four unique
perspectives: semantic alignment and deviation with ORKG properties,
fine-grained properties mapping accuracy, SciNCL embeddings-based cosine
similarity, and expert surveys comparing manual annotations with LLM outputs.
These evaluations occur within a multidisciplinary science setting. Overall,
LLMs show potential as recommendation systems for structuring science, but
further finetuning is recommended to improve their alignment with scientific
tasks and mimicry of human expertise.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.02105v1},
File          = {2405.02105v1.pdf}
}
@article{2406.14282v3,
Author        = {Junjie Wang and Mingyang Chen and Binbin Hu and Dan Yang and Ziqi Liu and Yue Shen and Peng Wei and Zhiqiang Zhang and Jinjie Gu and Jun Zhou and Jeff Z. Pan and Wen Zhang and Huajun Chen},
Title         = {Learning to Plan for Retrieval-Augmented Large Language Models from
  Knowledge Graphs},
Eprint        = {2406.14282v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Improving the performance of large language models (LLMs) in complex
question-answering (QA) scenarios has always been a research focal point.
Recent studies have attempted to enhance LLMs' performance by combining
step-wise planning with external retrieval. While effective for advanced models
like GPT-3.5, smaller LLMs face challenges in decomposing complex questions,
necessitating supervised fine-tuning. Previous work has relied on manual
annotation and knowledge distillation from teacher LLMs, which are
time-consuming and not accurate enough. In this paper, we introduce a novel
framework for enhancing LLMs' planning capabilities by using planning data
derived from knowledge graphs (KGs). LLMs fine-tuned with this data have
improved planning capabilities, better equipping them to handle complex QA
tasks that involve retrieval. Evaluations on multiple datasets, including our
newly proposed benchmark, highlight the effectiveness of our framework and the
benefits of KG-derived planning data.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.14282v3},
File          = {2406.14282v3.pdf}
}
@article{2410.06062v4,
Author        = {Vincent Emonet and Jerven Bolleman and Severine Duvaud and Tarcisio Mendes de Farias and Ana Claudia Sima},
Title         = {LLM-based SPARQL Query Generation from Natural Language over Federated
  Knowledge Graphs},
Eprint        = {2410.06062v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {We introduce a Retrieval-Augmented Generation (RAG) system for translating
user questions into accurate federated SPARQL queries over bioinformatics
knowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance
accuracy and reduce hallucinations in query generation, our system utilises
metadata from the KGs, including query examples and schema information, and
incorporates a validation step to correct generated queries. The system is
available online at chat.expasy.org.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.06062v4},
File          = {2410.06062v4.pdf}
}
@article{2309.08594v1,
Author        = {Cheng Qian and Xinran Zhao and Sherry Tongshuang Wu},
Title         = {"Merge Conflicts!" Exploring the Impacts of External Distractors to
  Parametric Knowledge Graphs},
Eprint        = {2309.08594v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) acquire extensive knowledge during pre-training,
known as their parametric knowledge. However, in order to remain up-to-date and
align with human instructions, LLMs inevitably require external knowledge
during their interactions with users. This raises a crucial question: How will
LLMs respond when external knowledge interferes with their parametric
knowledge? To investigate this question, we propose a framework that
systematically elicits LLM parametric knowledge and introduces external
knowledge. Specifically, we uncover the impacts by constructing a parametric
knowledge graph to reveal the different knowledge structures of LLMs, and
introduce external knowledge through distractors of varying degrees, methods,
positions, and formats. Our experiments on both black-box and open-source
models demonstrate that LLMs tend to produce responses that deviate from their
parametric knowledge, particularly when they encounter direct conflicts or
confounding changes of information within detailed contexts. We also find that
while LLMs are sensitive to the veracity of external knowledge, they can still
be distracted by unrelated information. These findings highlight the risk of
hallucination when integrating external knowledge, even indirectly, during
interactions with current LLMs. All the data and results are publicly
available.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.08594v1},
File          = {2309.08594v1.pdf}
}
@article{2402.11163v1,
Author        = {Jinhao Jiang and Kun Zhou and Wayne Xin Zhao and Yang Song and Chen Zhu and Hengshu Zhu and Ji-Rong Wen},
Title         = {KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning
  over Knowledge Graph},
Eprint        = {2402.11163v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this paper, we aim to improve the reasoning ability of large language
models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired
by existing methods that design the interaction strategy between LLMs and KG,
we propose an autonomous LLM-based agent framework, called KG-Agent, which
enables a small LLM to actively make decisions until finishing the reasoning
process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox,
KG-based executor, and knowledge memory, and develop an iteration mechanism
that autonomously selects the tool then updates the memory for reasoning over
KG. To guarantee the effectiveness, we leverage program language to formulate
the multi-hop reasoning process over the KG, and synthesize a code-based
instruction dataset to fine-tune the base LLM. Extensive experiments
demonstrate that only using 10K samples for tuning LLaMA-7B can outperform
state-of-the-art methods using larger LLMs or more data, on both in-domain and
out-domain datasets. Our code and data will be publicly released.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.11163v1},
File          = {2402.11163v1.pdf}
}
@article{2412.10654v1,
Author        = {Xue Wu and Kostas Tsioutsiouliklis},
Title         = {Thinking with Knowledge Graphs: Enhancing LLM Reasoning Through
  Structured Data},
Eprint        = {2412.10654v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation. However, they often struggle
with complex reasoning tasks and are prone to hallucination. Recent research
has shown promising results in leveraging knowledge graphs (KGs) to enhance LLM
performance. KGs provide a structured representation of entities and their
relationships, offering a rich source of information that can enhance the
reasoning capabilities of LLMs. For this work, we have developed different
techniques that tightly integrate KG structures and semantics into LLM
representations. Our results show that we are able to significantly improve the
performance of LLMs in complex reasoning scenarios, and ground the reasoning
process with KGs. We are the first to represent KGs with programming language
and fine-tune pretrained LLMs with KGs. This integration facilitates more
accurate and interpretable reasoning processes, paving the way for more
advanced reasoning capabilities of LLMs.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.10654v1},
File          = {2412.10654v1.pdf}
}
@article{2312.05276v1,
Author        = {Chunjing Gan and Dan Yang and Binbin Hu and Ziqi Liu and Yue Shen and Zhiqiang Zhang and Jinjie Gu and Jun Zhou and Guannan Zhang},
Title         = {Making Large Language Models Better Knowledge Miners for Online
  Marketing with Progressive Prompting Augmentation},
Eprint        = {2312.05276v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Nowadays, the rapid development of mobile economy has promoted the
flourishing of online marketing campaigns, whose success greatly hinges on the
efficient matching between user preferences and desired marketing campaigns
where a well-established Marketing-oriented Knowledge Graph (dubbed as MoKG)
could serve as the critical "bridge" for preference propagation. In this paper,
we seek to carefully prompt a Large Language Model (LLM) with domain-level
knowledge as a better marketing-oriented knowledge miner for marketing-oriented
knowledge graph construction, which is however non-trivial, suffering from
several inevitable issues in real-world marketing scenarios, i.e.,
uncontrollable relation generation of LLMs,insufficient prompting ability of a
single prompt, the unaffordable deployment cost of LLMs. To this end, we
propose PAIR, a novel Progressive prompting Augmented mIning fRamework for
harvesting marketing-oriented knowledge graph with LLMs. In particular, we
reduce the pure relation generation to an LLM based adaptive relation filtering
process through the knowledge-empowered prompting technique. Next, we steer
LLMs for entity expansion with progressive prompting augmentation,followed by a
reliable aggregation with comprehensive consideration of both self-consistency
and semantic relatedness. In terms of online serving, we specialize in a small
and white-box PAIR (i.e.,LightPAIR),which is fine-tuned with a high-quality
corpus provided by a strong teacher-LLM. Extensive experiments and practical
applications in audience targeting verify the effectiveness of the proposed
(Light)PAIR.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.05276v1},
File          = {2312.05276v1.pdf}
}
@article{2312.06185v5,
Author        = {Qinggang Zhang and Junnan Dong and Hao Chen and Daochen Zha and Zailiang Yu and Xiao Huang},
Title         = {KnowGPT: Knowledge Graph based Prompting for Large Language Models},
Eprint        = {2312.06185v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have demonstrated remarkable capabilities in
many real-world applications. Nonetheless, LLMs are often criticized for their
tendency to produce hallucinations, wherein the models fabricate incorrect
statements on tasks beyond their knowledge and perception. To alleviate this
issue, researchers have explored leveraging the factual knowledge in knowledge
graphs (KGs) to ground the LLM's responses in established facts and principles.
However, most state-of-the-art LLMs are closed-source, making it challenging to
develop a prompting framework that can efficiently and effectively integrate
KGs into LLMs with hard prompts only. Generally, existing KG-enhanced LLMs
usually suffer from three critical issues, including huge search space, high
API costs, and laborious prompt engineering, that impede their widespread
application in practice. To this end, we introduce a novel Knowledge Graph
based PrompTing framework, namely KnowGPT, to enhance LLMs with domain
knowledge. KnowGPT contains a knowledge extraction module to extract the most
informative knowledge from KGs, and a context-aware prompt construction module
to automatically convert extracted knowledge into effective prompts.
Experiments on three benchmarks demonstrate that KnowGPT significantly
outperforms all competitors. Notably, KnowGPT achieves a 92.6% accuracy on
OpenbookQA leaderboard, comparable to human-level performance.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.06185v5},
File          = {2312.06185v5.pdf}
}
@article{2408.10819v2,
Author        = {Rui Yang and Jiahao Zhu and Jianping Man and Hongze Liu and Li Fang and Yi Zhou},
Title         = {GS-KGC: A Generative Subgraph-based Framework for Knowledge Graph
  Completion with Large Language Models},
Eprint        = {2408.10819v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graph completion (KGC) focuses on identifying missing triples in a
knowledge graph (KG) , which is crucial for many downstream applications. Given
the rapid development of large language models (LLMs), some LLM-based methods
are proposed for KGC task. However, most of them focus on prompt engineering
while overlooking the fact that finer-grained subgraph information can aid LLMs
in generating more accurate answers. In this paper, we propose a novel
completion framework called \textbf{G}enerative \textbf{S}ubgraph-based KGC
(GS-KGC), which utilizes subgraph information as contextual reasoning and
employs a QA approach to achieve the KGC task. This framework primarily
includes a subgraph partitioning algorithm designed to generate negatives and
neighbors. Specifically, negatives can encourage LLMs to generate a broader
range of answers, while neighbors provide additional contextual insights for
LLM reasoning. Furthermore, we found that GS-KGC can discover potential triples
within the KGs and new facts beyond the KGs. Experiments conducted on four
common KGC datasets highlight the advantages of the proposed GS-KGC, e.g., it
shows a 5.6\% increase in Hits@3 compared to the LLM-based model CP-KGC on the
FB15k-237N, and a 9.3\% increase over the LLM-based model TECHS on the ICEWS14.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.10819v2},
File          = {2408.10819v2.pdf}
}
@article{2409.06433v1,
Author        = {Gollam Rabby and Sören Auer and Jennifer D'Souza and Allard Oelen},
Title         = {Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for
  Scholarly Knowledge Organization},
Eprint        = {2409.06433v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {The increasing amount of published scholarly articles, exceeding 2.5 million
yearly, raises the challenge for researchers in following scientific progress.
Integrating the contributions from scholarly articles into a novel type of
cognitive knowledge graph (CKG) will be a crucial element for accessing and
organizing scholarly knowledge, surpassing the insights provided by titles and
abstracts. This research focuses on effectively conveying structured scholarly
knowledge by utilizing large language models (LLMs) to categorize scholarly
articles and describe their contributions in a structured and comparable
manner. While previous studies explored language models within specific
research domains, the extensive domain-independent knowledge captured by LLMs
offers a substantial opportunity for generating structured contribution
descriptions as CKGs. Additionally, LLMs offer customizable pathways through
prompt engineering or fine-tuning, thus facilitating to leveraging of smaller
LLMs known for their efficiency, cost-effectiveness, and environmental
considerations. Our methodology involves harnessing LLM knowledge, and
complementing it with domain expert-verified scholarly data sourced from a CKG.
This strategic fusion significantly enhances LLM performance, especially in
tasks like scholarly article categorization and predicate recommendation. Our
method involves fine-tuning LLMs with CKG knowledge and additionally injecting
knowledge from a CKG with a novel prompting technique significantly increasing
the accuracy of scholarly knowledge extraction. We integrated our approach in
the Open Research Knowledge Graph (ORKG), thus enabling precise access to
organized scholarly knowledge, crucially benefiting domain-independent
scholarly knowledge exchange and dissemination among policymakers, industrial
practitioners, and the general public.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.06433v1},
File          = {2409.06433v1.pdf}
}
@article{2406.07962v2,
Author        = {Luis Miguel Vieira da Silva and Aljosha Köcher and Felix Gehlhoff and Alexander Fay},
Title         = {Toward a Method to Generate Capability Ontologies from Natural Language
  Descriptions},
Eprint        = {2406.07962v2},
DOI           = {10.1109/ETFA61755.2024.10710783},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {To achieve a flexible and adaptable system, capability ontologies are
increasingly leveraged to describe functions in a machine-interpretable way.
However, modeling such complex ontological descriptions is still a manual and
error-prone task that requires a significant amount of effort and ontology
expertise. This contribution presents an innovative method to automate
capability ontology modeling using Large Language Models (LLMs), which have
proven to be well suited for such tasks. Our approach requires only a natural
language description of a capability, which is then automatically inserted into
a predefined prompt using a few-shot prompting technique. After prompting an
LLM, the resulting capability ontology is automatically verified through
various steps in a loop with the LLM to check the overall correctness of the
capability ontology. First, a syntax check is performed, then a check for
contradictions, and finally a check for hallucinations and missing ontology
elements. Our method greatly reduces manual effort, as only the initial natural
language description and a final human review and possible correction are
necessary, thereby streamlining the capability ontology generation process.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.07962v2},
File          = {2406.07962v2.pdf}
}
@article{2405.14170v3,
Author        = {Jiapu Wang and Kai Sun and Linhao Luo and Wei Wei and Yongli Hu and Alan Wee-Chung Liew and Shirui Pan and Baocai Yin},
Title         = {Large Language Models-guided Dynamic Adaptation for Temporal Knowledge
  Graph Reasoning},
Eprint        = {2405.14170v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing
temporal information to capture complex relations within a Temporal Knowledge
Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically
depend on deep learning algorithms or temporal logical rules. However, deep
learning-based TKGRs often lack interpretability, whereas rule-based TKGRs
struggle to effectively learn temporal rules that capture temporal patterns.
Recently, Large Language Models (LLMs) have demonstrated extensive knowledge
and remarkable proficiency in temporal reasoning. Consequently, the employment
of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing
interest among researchers. Nonetheless, LLMs are known to function as black
boxes, making it challenging to comprehend their reasoning process.
Additionally, due to the resource-intensive nature of fine-tuning, promptly
updating LLMs to integrate evolving knowledge within TKGs for reasoning is
impractical. To address these challenges, in this paper, we propose a Large
Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on
TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze
historical data and extract temporal logical rules. These rules unveil temporal
patterns and facilitate interpretable reasoning. To account for the evolving
nature of TKGs, a dynamic adaptation strategy is proposed to update the
LLM-generated rules with the latest events. This ensures that the extracted
rules always incorporate the most recent knowledge and better generalize to the
predictions on future events. Experimental results show that without the need
of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over
several common datasets, providing a robust framework for TKGR tasks.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.14170v3},
File          = {2405.14170v3.pdf}
}
@article{2411.08359v1,
Author        = {Jian Wang and Tiantian Zhu and Chunlin Xiong and Yan Chen},
Title         = {MultiKG: Multi-Source Threat Intelligence Aggregation for High-Quality
  Knowledge Graph Representation of Attack Techniques},
Eprint        = {2411.08359v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {The construction of attack technique knowledge graphs aims to transform
various types of attack knowledge into structured representations for more
effective attack procedure modeling. Existing methods typically rely on textual
data, such as Cyber Threat Intelligence (CTI) reports, which are often
coarse-grained and unstructured, resulting in incomplete and inaccurate
knowledge graphs. To address these issues, we expand attack knowledge sources
by incorporating audit logs and static code analysis alongside CTI reports,
providing finer-grained data for constructing attack technique knowledge
graphs.
  We propose MultiKG, a fully automated framework that integrates multiple
threat knowledge sources. MultiKG processes data from CTI reports, dynamic
logs, and static code separately, then merges them into a unified attack
knowledge graph. Through system design and the utilization of the Large
Language Model (LLM), MultiKG automates the analysis, construction, and merging
of attack graphs across these sources, producing a fine-grained, multi-source
attack knowledge graph.
  We implemented MultiKG and evaluated it using 1,015 real attack techniques
and 9,006 attack intelligence entries from CTI reports. Results show that
MultiKG effectively extracts attack knowledge graphs from diverse sources and
aggregates them into accurate, comprehensive representations. Through case
studies, we demonstrate that our approach directly benefits security tasks such
as attack reconstruction and detection.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.08359v1},
File          = {2411.08359v1.pdf}
}
@article{2404.10384v1,
Author        = {Yuqi Wang and Boran Jiang and Yi Luo and Dawei He and Peng Cheng and Liangcai Gao},
Title         = {Reasoning on Efficient Knowledge Paths:Knowledge Graph Guides Large
  Language Model for Domain Question Answering},
Eprint        = {2404.10384v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs), such as GPT3.5, GPT4 and LLAMA2 perform
surprisingly well and outperform human experts on many tasks. However, in many
domain-specific evaluations, these LLMs often suffer from hallucination
problems due to insufficient training of relevant corpus. Furthermore,
fine-tuning large models may face problems such as the LLMs are not open source
or the construction of high-quality domain instruction is difficult. Therefore,
structured knowledge databases such as knowledge graph can better provide
domain background knowledge for LLMs and make full use of the reasoning and
analysis capabilities of LLMs. In some previous works, LLM was called multiple
times to determine whether the current triplet was suitable for inclusion in
the subgraph when retrieving subgraphs through a question. Especially for the
question that require a multi-hop reasoning path, frequent calls to LLM will
consume a lot of computing power. Moreover, when choosing the reasoning path,
LLM will be called once for each step, and if one of the steps is selected
incorrectly, it will lead to the accumulation of errors in the following steps.
In this paper, we integrated and optimized a pipeline for selecting reasoning
paths from KG based on LLM, which can reduce the dependency on LLM. In
addition, we propose a simple and effective subgraph retrieval method based on
chain of thought (CoT) and page rank which can returns the paths most likely to
contain the answer. We conduct experiments on three datasets: GenMedGPT-5k
[14], WebQuestions [2], and CMCQA [21]. Finally, RoK can demonstrate that using
fewer LLM calls can achieve the same results as previous SOTAs models.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.10384v1},
File          = {2404.10384v1.pdf}
}
@article{2404.14809v1,
Author        = {Wenbo Shang and Xin Huang},
Title         = {A Survey of Large Language Models on Generative Graph Analytics: Query,
  Learning, and Applications},
Eprint        = {2404.14809v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {A graph is a fundamental data model to represent various entities and their
complex relationships in society and nature, such as social networks,
transportation networks, financial networks, and biomedical systems. Recently,
large language models (LLMs) have showcased a strong generalization ability to
handle various NLP and multi-mode tasks to answer users' arbitrary questions
and specific-domain content generation. Compared with graph learning models,
LLMs enjoy superior advantages in addressing the challenges of generalizing
graph tasks by eliminating the need for training graph learning models and
reducing the cost of manual annotation. In this survey, we conduct a
comprehensive investigation of existing LLM studies on graph data, which
summarizes the relevant graph analytics tasks solved by advanced LLM models and
points out the existing remaining challenges and future directions.
Specifically, we study the key problems of LLM-based generative graph analytics
(LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP),
LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based
applications. LLM-GQP focuses on an integration of graph analytics techniques
and LLM prompts, including graph understanding and knowledge graph (KG) based
augmented retrieval, while LLM-GIL focuses on learning and reasoning over
graphs, including graph learning, graph-formed reasoning and graph
representation. We summarize the useful prompts incorporated into LLM to handle
different graph downstream tasks. Moreover, we give a summary of LLM model
evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM
models. We also explore open problems and future directions in this exciting
interdisciplinary research area of LLMs and graph analytics.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.14809v1},
File          = {2404.14809v1.pdf}
}
@article{2309.05918v3,
Author        = {Walid S. Saba},
Title         = {Stochastic LLMs do not Understand Language: Towards Symbolic,
  Explainable and Ontologically Based LLMs},
Eprint        = {2309.05918v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In our opinion the exuberance surrounding the relative success of data-driven
large language models (LLMs) is slightly misguided and for several reasons (i)
LLMs cannot be relied upon for factual information since for LLMs all ingested
text (factual or non-factual) was created equal; (ii) due to their subsymbolic
na-ture, whatever 'knowledge' these models acquire about language will always
be buried in billions of microfeatures (weights), none of which is meaningful
on its own; and (iii) LLMs will often fail to make the correct inferences in
several linguistic contexts (e.g., nominal compounds, copredication, quantifier
scope ambi-guities, intensional contexts. Since we believe the relative success
of data-driven large language models (LLMs) is not a reflection on the symbolic
vs. subsymbol-ic debate but a reflection on applying the successful strategy of
a bottom-up reverse engineering of language at scale, we suggest in this paper
applying the effective bottom-up strategy in a symbolic setting resulting in
symbolic, explainable, and ontologically grounded language models.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.05918v3},
File          = {2309.05918v3.pdf}
}
@article{2211.08316v2,
Author        = {Changlong Yu and Weiqi Wang and Xin Liu and Jiaxin Bai and Yangqiu Song and Zheng Li and Yifan Gao and Tianyu Cao and Bing Yin},
Title         = {FolkScope: Intention Knowledge Graph Construction for E-commerce
  Commonsense Discovery},
Eprint        = {2211.08316v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Understanding users' intentions in e-commerce platforms requires commonsense
knowledge. In this paper, we present FolkScope, an intention knowledge graph
construction framework to reveal the structure of humans' minds about
purchasing items. As commonsense knowledge is usually ineffable and not
expressed explicitly, it is challenging to perform information extraction.
Thus, we propose a new approach that leverages the generation power of large
language models~(LLMs) and human-in-the-loop annotation to semi-automatically
construct the knowledge graph. LLMs first generate intention assertions via
e-commerce-specific prompts to explain shopping behaviors, where the intention
can be an open reason or a predicate falling into one of 18 categories aligning
with ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility
and typicality labels of sampled intentions as training data in order to
populate human judgments to all automatic generations. Last, to structurize the
assertions, we propose pattern mining and conceptualization to form more
condensed and abstract knowledge. Extensive evaluations and studies demonstrate
that our constructed knowledge graph can well model e-commerce knowledge and
have many potential applications.},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.08316v2},
File          = {2211.08316v2.pdf}
}
@article{2412.05453v2,
Author        = {Krishnasai Addala and Kabir Dev Paul Baghel and Dhruv Jain and Chhavi Kirtani and Avinash Anand and Rajiv Ratn Shah},
Title         = {Knowledge Graphs are all you need: Leveraging KGs in Physics Question
  Answering},
Eprint        = {2412.05453v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This study explores the effectiveness of using knowledge graphs generated by
large language models to decompose high school-level physics questions into
sub-questions. We introduce a pipeline aimed at enhancing model response
quality for Question Answering tasks. By employing LLMs to construct knowledge
graphs that capture the internal logic of the questions, these graphs then
guide the generation of subquestions. We hypothesize that this method yields
sub-questions that are more logically consistent with the original questions
compared to traditional decomposition techniques. Our results show that
sub-questions derived from knowledge graphs exhibit significantly improved
fidelity to the original question's logic. This approach not only enhances the
learning experience by providing clearer and more contextually appropriate
sub-questions but also highlights the potential of LLMs to transform
educational methodologies. The findings indicate a promising direction for
applying AI to improve the quality and effectiveness of educational content.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.05453v2},
File          = {2412.05453v2.pdf}
}
@article{2309.09898v1,
Author        = {Maurice Funk and Simon Hosemann and Jean Christoph Jung and Carsten Lutz},
Title         = {Towards Ontology Construction with Language Models},
Eprint        = {2309.09898v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We present a method for automatically constructing a concept hierarchy for a
given domain by querying a large language model. We apply this method to
various domains using OpenAI's GPT 3.5. Our experiments indicate that LLMs can
be of considerable help for constructing concept hierarchies.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.09898v1},
File          = {2309.09898v1.pdf}
}
@article{2307.01128v1,
Author        = {Salvatore Carta and Alessandro Giuliani and Leonardo Piano and Alessandro Sebastian Podda and Livio Pompianu and Sandro Gabriele Tiddia},
Title         = {Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction},
Eprint        = {2307.01128v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In the current digitalization era, capturing and effectively representing
knowledge is crucial in most real-world scenarios. In this context, knowledge
graphs represent a potent tool for retrieving and organizing a vast amount of
information in a properly interconnected and interpretable structure. However,
their generation is still challenging and often requires considerable human
effort and domain expertise, hampering the scalability and flexibility across
different application fields. This paper proposes an innovative knowledge graph
generation approach that leverages the potential of the latest generative large
language models, such as GPT-3.5, that can address all the main critical issues
in knowledge graph building. The approach is conveyed in a pipeline that
comprises novel iterative zero-shot and external knowledge-agnostic strategies
in the main stages of the generation process. Our unique manifold approach may
encompass significant benefits to the scientific community. In particular, the
main contribution can be summarized by: (i) an innovative strategy for
iteratively prompting large language models to extract relevant components of
the final graph; (ii) a zero-shot strategy for each prompt, meaning that there
is no need for providing examples for "guiding" the prompt result; (iii) a
scalable solution, as the adoption of LLMs avoids the need for any external
resources or human expertise. To assess the effectiveness of our proposed
model, we performed experiments on a dataset that covered a specific domain. We
claim that our proposal is a suitable solution for scalable and versatile
knowledge graph construction and may be applied to different and novel
contexts.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.01128v1},
File          = {2307.01128v1.pdf}
}
@article{2307.16699v1,
Author        = {Patricia Mateiu and Adrian Groza},
Title         = {Ontology engineering with Large Language Models},
Eprint        = {2307.16699v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We tackle the task of enriching ontologies by automatically translating
natural language sentences into Description Logic. Since Large Language Models
(LLMs) are the best tools for translations, we fine-tuned a GPT-3 model to
convert Natural Language sentences into OWL Functional Syntax. We employ
objective and concise examples to fine-tune the model regarding: instances,
class subsumption, domain and range of relations, object properties
relationships, disjoint classes, complements, cardinality restrictions. The
resulted axioms are used to enrich an ontology, in a human supervised manner.
The developed tool is publicly provided as a Protge plugin.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.16699v1},
File          = {2307.16699v1.pdf}
}
@article{2406.14326v2,
Author        = {Mingyi Jia and Junwen Duan and Yan Song and Jianxin Wang},
Title         = {medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced
  Clinical Diagnosis on EMRs},
Eprint        = {2406.14326v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Electronic Medical Records (EMRs), while integral to modern healthcare,
present challenges for clinical reasoning and diagnosis due to their complexity
and information redundancy. To address this, we proposed medIKAL (Integrating
Knowledge Graphs as Assistants of LLMs), a framework that combines Large
Language Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic
capabilities. medIKAL assigns weighted importance to entities in medical
records based on their type, enabling precise localization of candidate
diseases within KGs. It innovatively employs a residual network-like approach,
allowing initial diagnosis by the LLM to be merged into KG search results.
Through a path-based reranking algorithm and a fill-in-the-blank style prompt
template, it further refined the diagnostic process. We validated medIKAL's
effectiveness through extensive experiments on a newly introduced open-sourced
Chinese EMR dataset, demonstrating its potential to improve clinical diagnosis
in real-world settings.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.14326v2},
File          = {2406.14326v2.pdf}
}
@article{2412.15268v2,
Author        = {Yibo Zhao and Jiapeng Zhu and Can Xu and Xiang Li},
Title         = {Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic
  Knowledge Graph},
Eprint        = {2412.15268v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The rapid growth of social media platforms has raised significant concerns
regarding online content toxicity. When Large Language Models (LLMs) are used
for toxicity detection, two key challenges emerge: 1) the absence of
domain-specific toxic knowledge leads to false negatives; 2) the excessive
sensitivity of LLMs to toxic speech results in false positives, limiting
freedom of speech. To address these issues, we propose a novel method called
MetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance
hatred and toxicity detection. First, we construct a comprehensive meta-toxic
knowledge graph by utilizing LLMs to extract toxic information through a
three-step pipeline, with toxic benchmark datasets serving as corpora. Second,
we query the graph via retrieval and ranking processes to supplement accurate,
relevant toxic knowledge. Extensive experiments and in-depth case studies
across multiple datasets demonstrate that our MetaTox significantly decreases
the false positive rate while boosting overall toxicity detection performance.
Our code will be available soon.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.15268v2},
File          = {2412.15268v2.pdf}
}
@article{2501.06628v1,
Author        = {Mohammed Maree},
Title         = {Quantifying Relational Exploration in Cultural Heritage Knowledge Graphs
  with LLMs: A Neuro-Symbolic Approach},
Eprint        = {2501.06628v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper introduces a neuro-symbolic approach for relational exploration in
cultural heritage knowledge graphs, leveraging Large Language Models (LLMs) for
explanation generation and a novel mathematical framework to quantify the
interestingness of relationships. We demonstrate the importance of
interestingness measure using a quantitative analysis, by highlighting its
impact on the overall performance of our proposed system, particularly in terms
of precision, recall, and F1-score. Using the Wikidata Cultural Heritage Linked
Open Data (WCH-LOD) dataset, our approach yields a precision of 0.70, recall of
0.68, and an F1-score of 0.69, representing an improvement compared to
graph-based (precision: 0.28, recall: 0.25, F1-score: 0.26) and knowledge-based
baselines (precision: 0.45, recall: 0.42, F1-score: 0.43). Furthermore, our
LLM-powered explanations exhibit better quality, reflected in BLEU (0.52),
ROUGE-L (0.58), and METEOR (0.63) scores, all higher than the baseline
approaches. We show a strong correlation (0.65) between interestingness measure
and the quality of generated explanations, validating its effectiveness. The
findings highlight the importance of LLMs and a mathematical formalization for
interestingness in enhancing the effectiveness of relational exploration in
cultural heritage knowledge graphs, with results that are measurable and
testable. We further show that the system enables more effective exploration
compared to purely knowledge-based and graph-based methods.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.06628v1},
File          = {2501.06628v1.pdf}
}
@article{2406.13862v1,
Author        = {Haochen Liu and Song Wang and Yaochen Zhu and Yushun Dong and Jundong Li},
Title         = {Knowledge Graph-Enhanced Large Language Models via Path Selection},
Eprint        = {2406.13862v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have shown unprecedented performance in various
real-world applications. However, they are known to generate factually
inaccurate outputs, a.k.a. the hallucination problem. In recent years,
incorporating external knowledge extracted from Knowledge Graphs (KGs) has
become a promising strategy to improve the factual accuracy of LLM-generated
outputs. Nevertheless, most existing explorations rely on LLMs themselves to
perform KG knowledge extraction, which is highly inflexible as LLMs can only
provide binary judgment on whether a certain knowledge (e.g., a knowledge path
in KG) should be used. In addition, LLMs tend to pick only knowledge with
direct semantic relationship with the input text, while potentially useful
knowledge with indirect semantics can be ignored. In this work, we propose a
principled framework KELP with three stages to handle the above problems.
Specifically, KELP is able to achieve finer granularity of flexible knowledge
extraction by generating scores for knowledge paths with input texts via latent
semantic matching. Meanwhile, knowledge paths with indirect semantic
relationships with the input text can also be considered via trained encoding
between the selected paths in KG and the input text. Experiments on real-world
datasets validate the effectiveness of KELP.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.13862v1},
File          = {2406.13862v1.pdf}
}
@article{2407.16127v1,
Author        = {Yang Liu and Xiaobin Tian and Zequn Sun and Wei Hu},
Title         = {Finetuning Generative Large Language Models with Discrimination
  Instructions for Knowledge Graph Completion},
Eprint        = {2407.16127v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Traditional knowledge graph (KG) completion models learn embeddings to
predict missing facts. Recent works attempt to complete KGs in a
text-generation manner with large language models (LLMs). However, they need to
ground the output of LLMs to KG entities, which inevitably brings errors. In
this paper, we present a finetuning framework, DIFT, aiming to unleash the KG
completion ability of LLMs and avoid grounding errors. Given an incomplete
fact, DIFT employs a lightweight model to obtain candidate entities and
finetunes an LLM with discrimination instructions to select the correct one
from the given candidates. To improve performance while reducing instruction
data, DIFT uses a truncated sampling method to select useful facts for
finetuning and injects KG embeddings into the LLM. Extensive experiments on
benchmark datasets demonstrate the effectiveness of our proposed framework.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.16127v1},
File          = {2407.16127v1.pdf}
}
@article{2407.21358v1,
Author        = {Elan Markowitz and Anil Ramakrishna and Jwala Dhamala and Ninareh Mehrabi and Charith Peris and Rahul Gupta and Kai-Wei Chang and Aram Galstyan},
Title         = {Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting
  Black-box Language Models with Knowledge Graphs},
Eprint        = {2407.21358v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing
reliable, structured, domain-specific, and up-to-date external knowledge.
However, KGs and LLMs are often developed separately and must be integrated
after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning
algorithm that enables augmentation of black-box LLMs with one or more KGs. The
algorithm equips a LLM with actions for interfacing a KG and enables the LLM to
perform tree search over possible thoughts and actions to find high confidence
reasoning paths. We evaluate on two popular benchmark datasets. Our results
show that Tree-of-Traversals significantly improves performance on question
answering and KG question answering tasks. Code is available at
\url{https://github.com/amazon-science/tree-of-traversals}},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.21358v1},
File          = {2407.21358v1.pdf}
}
@article{2411.14258v1,
Author        = {Ernests Lavrinovics and Russa Biswas and Johannes Bjerva and Katja Hose},
Title         = {Knowledge Graphs, Large Language Models, and Hallucinations: An NLP
  Perspective},
Eprint        = {2411.14258v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) based applications including automated text generation, question
answering, chatbots, and others. However, they face a significant challenge:
hallucinations, where models produce plausible-sounding but factually incorrect
responses. This undermines trust and limits the applicability of LLMs in
different domains. Knowledge Graphs (KGs), on the other hand, provide a
structured collection of interconnected facts represented as entities (nodes)
and their relationships (edges). In recent research, KGs have been leveraged to
provide context that can fill gaps in an LLM understanding of certain topics
offering a promising approach to mitigate hallucinations in LLMs, enhancing
their reliability and accuracy while benefiting from their wide applicability.
Nonetheless, it is still a very active area of research with various unresolved
open problems. In this paper, we discuss these open challenges covering
state-of-the-art datasets and benchmarks as well as methods for knowledge
integration and evaluating hallucinations. In our discussion, we consider the
current use of KGs in LLM systems and identify future directions within each of
these challenges.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.14258v1},
File          = {2411.14258v1.pdf}
}
@article{2410.16804v1,
Author        = {Haru Nakajima and Jun Miura},
Title         = {Combining Ontological Knowledge and Large Language Model for
  User-Friendly Service Robots},
Eprint        = {2410.16804v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Lifestyle support through robotics is an increasingly promising field, with
expectations for robots to take over or assist with chores like floor cleaning,
table setting and clearing, and fetching items. The growth of AI, particularly
foundation models, such as large language models (LLMs) and visual language
models (VLMs), is significantly shaping this sector. LLMs, by facilitating
natural interactions and providing vast general knowledge, are proving
invaluable for robotic tasks. This paper zeroes in on the benefits of LLMs for
"bring-me" tasks, where robots fetch specific items for users, often based on
vague instructions. Our previous efforts utilized an ontology extended to
handle environmental data to decipher such vagueness, but faced limitations
when unresolvable ambiguities required user intervention for clarity. Here, we
enhance our approach by integrating LLMs for providing additional commonsense
knowledge, pairing it with ontological data to mitigate the issue of
hallucinations and reduce the need for user queries, thus improving system
usability. We present a system that merges these knowledge bases and assess its
efficacy on "bring-me" tasks, aiming to provide a more seamless and efficient
robotic assistance experience.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.16804v1},
File          = {2410.16804v1.pdf}
}
@article{2402.04978v2,
Author        = {Yihao Li and Ru Zhang and Jianyi Liu},
Title         = {An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge
  Graph-Integrated Collaboration},
Eprint        = {2402.04978v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {While Large Language Models (LLMs) demonstrate exceptional performance in a
multitude of Natural Language Processing (NLP) tasks, they encounter challenges
in practical applications, including issues with hallucinations, inadequate
knowledge updating, and limited transparency in the reasoning process. To
overcome these limitations, this study innovatively proposes a collaborative
training-free reasoning scheme involving tight cooperation between Knowledge
Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively
explore KG, selectively retrieving a task-relevant knowledge subgraph to
support reasoning. The LLMs are then guided to further combine inherent
implicit knowledge to reason on the subgraph while explicitly elucidating the
reasoning process. Through such a cooperative approach, our scheme achieves
more reliable knowledge-based reasoning and facilitates the tracing of the
reasoning results. Experimental results show that our scheme significantly
progressed across multiple datasets, notably achieving over a 10% improvement
on the QALD10 dataset compared to the best baseline and the fine-tuned
state-of-the-art (SOTA) work. Building on this success, this study hopes to
offer a valuable reference for future research in the fusion of KG and LLMs,
thereby enhancing LLMs' proficiency in solving complex issues.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.04978v2},
File          = {2402.04978v2.pdf}
}
@article{2402.11199v2,
Author        = {Minh-Vuong Nguyen and Linhao Luo and Fatemeh Shiri and Dinh Phung and Yuan-Fang Li and Thuy-Trang Vu and Gholamreza Haffari},
Title         = {Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with
  Knowledge Graphs},
Eprint        = {2402.11199v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) demonstrate strong reasoning abilities when
prompted to generate chain-of-thought (CoT) explanations alongside answers.
However, previous research on evaluating LLMs has solely focused on answer
accuracy, neglecting the correctness of the generated CoT. In this paper, we
delve deeper into the CoT reasoning capabilities of LLMs in multi-hop question
answering by utilizing knowledge graphs (KGs). We propose a novel
discriminative and generative CoT evaluation paradigm to assess LLMs' knowledge
of reasoning and the accuracy of the generated CoT. Through experiments
conducted on 5 different families of LLMs across 2 multi-hop question-answering
datasets, we find that LLMs possess sufficient knowledge to perform reasoning.
However, there exists a significant disparity between answer accuracy and
faithfulness of the CoT reasoning generated by LLMs, indicating that they often
arrive at correct answers through incorrect reasoning.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.11199v2},
File          = {2402.11199v2.pdf}
}
@article{2409.14556v2,
Author        = {Lindsey Linxi Wei and Guorui Xiao and Magdalena Balazinska},
Title         = {RACOON: An LLM-based Framework for Retrieval-Augmented Column Type
  Annotation with a Knowledge Graph},
Eprint        = {2409.14556v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {As an important component of data exploration and integration, Column Type
Annotation (CTA) aims to label columns of a table with one or more semantic
types. With the recent development of Large Language Models (LLMs), researchers
have started to explore the possibility of using LLMs for CTA, leveraging their
strong zero-shot capabilities. In this paper, we build on this promising work
and improve on LLM-based methods for CTA by showing how to use a Knowledge
Graph (KG) to augment the context information provided to the LLM. Our
approach, called RACOON, combines both pre-trained parametric and
non-parametric knowledge during generation to improve LLMs' performance on CTA.
Our experiments show that RACOON achieves up to a 0.21 micro F-1 improvement
compared against vanilla LLM inference.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.14556v2},
File          = {2409.14556v2.pdf}
}
@article{2412.01837v1,
Author        = {Menghan Wang and Yuchen Guo and Duanfeng Zhang and Jianian Jin and Minnie Li and Dan Schonfeld and Shawn Zhou},
Title         = {Enabling Explainable Recommendation in E-commerce with LLM-powered
  Product Knowledge Graph},
Eprint        = {2412.01837v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {How to leverage large language model's superior capability in e-commerce
recommendation has been a hot topic. In this paper, we propose LLM-PKG, an
efficient approach that distills the knowledge of LLMs into product knowledge
graph (PKG) and then applies PKG to provide explainable recommendations.
Specifically, we first build PKG by feeding curated prompts to LLM, and then
map LLM response to real enterprise products. To mitigate the risks associated
with LLM hallucination, we employ rigorous evaluation and pruning methods to
ensure the reliability and availability of the KG. Through an A/B test
conducted on an e-commerce website, we demonstrate the effectiveness of LLM-PKG
in driving user engagements and transactions significantly.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2412.01837v1},
File          = {2412.01837v1.pdf}
}
@article{2412.12643v1,
Author        = {Mufan Xu and Kehai Chen and Xuefeng Bai and Muyun Yang and Tiejun Zhao and Min Zhang},
Title         = {LLM-based Discriminative Reasoning for Knowledge Graph Question
  Answering},
Eprint        = {2412.12643v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) based on generative pre-trained Transformer have
achieved remarkable performance on knowledge graph question-answering (KGQA)
tasks. However, LLMs often produce ungrounded subgraph planning or reasoning
results in KGQA due to the hallucinatory behavior brought by the generative
paradigm, which may hinder the advancement of the LLM-based KGQA model. To deal
with the issue, we propose a novel LLM-based Discriminative Reasoning (LDR)
method to explicitly model the subgraph retrieval and answer inference process.
By adopting discriminative strategies, the proposed LDR method not only
enhances the capability of LLMs to retrieve question-related subgraphs but also
alleviates the issue of ungrounded reasoning brought by the generative paradigm
of LLMs. Experimental results show that the proposed approach outperforms
multiple strong comparison methods, along with achieving state-of-the-art
performance on two widely used WebQSP and CWQ benchmarks.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.12643v1},
File          = {2412.12643v1.pdf}
}
@article{2402.11541v4,
Author        = {Xinbang Dai and Yuncheng Hua and Tongtong Wu and Yang Sheng and Qiu Ji and Guilin Qi},
Title         = {Large Language Models Can Better Understand Knowledge Graphs Than We
  Thought},
Eprint        = {2402.11541v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {When we integrate factual knowledge from knowledge graphs (KGs) into large
language models (LLMs) to enhance their performance, the cost of injection
through training increases with the scale of the models. Consequently, there is
significant interest in developing prompt strategies that effectively
incorporate KG information into LLMs. However, the community has not yet
comprehensively understood how LLMs process and interpret KG information in
different input formats and organizations within prompts, and researchers often
rely on trial and error. To address this gap, we design extensive experiments
to empirically study LLMs' comprehension of different KG prompts. At the
literal level, we reveal LLMs' preferences for various input formats (from
linearized triples to fluent natural language text). At the attention
distribution level, we discuss the underlying mechanisms driving these
preferences. We then investigate how the organization of structured knowledge
impacts LLMs and evaluate LLMs' robustness in processing and utilizing KG
information in practical scenarios. Our experiments show that (1) linearized
triples are more effective than fluent NL text in helping LLMs understand KG
information and answer fact-intensive questions; (2) Different LLMs exhibit
varying preferences for different organizational formats of triples; (3) LLMs
with larger scales are more susceptible to noisy, incomplete subgraphs.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.11541v4},
File          = {2402.11541v4.pdf}
}
@article{2311.06858v1,
Author        = {Antonio Zaitoun and Tomer Sagi and Szymon Wilk and Mor Peleg},
Title         = {Can Large Language Models Augment a Biomedical Ontology with missing
  Concepts and Relations?},
Eprint        = {2311.06858v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Ontologies play a crucial role in organizing and representing knowledge.
However, even current ontologies do not encompass all relevant concepts and
relationships. Here, we explore the potential of large language models (LLM) to
expand an existing ontology in a semi-automated fashion. We demonstrate our
approach on the biomedical ontology SNOMED-CT utilizing semantic relation types
from the widely used UMLS semantic network. We propose a method that uses
conversational interactions with an LLM to analyze clinical practice guidelines
(CPGs) and detect the relationships among the new medical concepts that are not
present in SNOMED-CT. Our initial experimentation with the conversational
prompts yielded promising preliminary results given a manually generated gold
standard, directing our future potential improvements.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.06858v1},
File          = {2311.06858v1.pdf}
}
@article{2402.04627v1,
Author        = {Julio C. Rangel and Tarcisio Mendes de Farias and Ana Claudia Sima and Norio Kobayashi},
Title         = {SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question
  Answering over a Life Science Knowledge Graph},
Eprint        = {2402.04627v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The recent success of Large Language Models (LLM) in a wide range of Natural
Language Processing applications opens the path towards novel Question
Answering Systems over Knowledge Graphs leveraging LLMs. However, one of the
main obstacles preventing their implementation is the scarcity of training data
for the task of translating questions into corresponding SPARQL queries,
particularly in the case of domain-specific KGs. To overcome this challenge, in
this study, we evaluate several strategies for fine-tuning the OpenLlama LLM
for question answering over life science knowledge graphs. In particular, we
propose an end-to-end data augmentation approach for extending a set of
existing queries over a given knowledge graph towards a larger dataset of
semantically enriched question-to-SPARQL query pairs, enabling fine-tuning even
for datasets where these pairs are scarce. In this context, we also investigate
the role of semantic "clues" in the queries, such as meaningful variable names
and inline comments. Finally, we evaluate our approach over the real-world Bgee
gene expression knowledge graph and we show that semantic clues can improve
model performance by up to 33% compared to a baseline with random variable
names and no comments included.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.04627v1},
File          = {2402.04627v1.pdf}
}
@article{2411.19539v1,
Author        = {Yuta Ojima and Hiroki Sakaji and Tadashi Nakamura and Hiroaki Sakata and Kazuya Seki and Yuu Teshigawara and Masami Yamashita and Kazuhiro Aoyama},
Title         = {Knowledge Management for Automobile Failure Analysis Using Graph RAG},
Eprint        = {2411.19539v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper presents a knowledge management system for automobile failure
analysis using retrieval-augmented generation (RAG) with large language models
(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a
growing demand for knowledge transfer of failure analysis from experienced
engineers to young engineers. However, failure events are phenomena that occur
in a chain reaction, making them difficult for beginners to analyze them. While
knowledge graphs, which can describe semantic relationships and structure
information is effective in representing failure events, due to their
capability of representing the relationships between components, there is much
information in KGs, so it is challenging for young engineers to extract and
understand sub-graphs from the KG. On the other hand, there is increasing
interest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for
knowledge management. However, when using the current Graph RAG framework with
an existing knowledge graph for automobile failures, several issues arise
because it is difficult to generate executable queries for a knowledge graph
database which is not constructed by LLMs. To address this, we focused on
optimizing the Graph RAG pipeline for existing knowledge graphs. Using an
original Q&A dataset, the ROUGE F1 score of the sentences generated by the
proposed method showed an average improvement of 157.6% compared to the current
method. This highlights the effectiveness of the proposed method for automobile
failure analysis.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.19539v1},
File          = {2411.19539v1.pdf}
}
@article{2403.01481v1,
Author        = {Kinshuk Vasisht and Balaji Ganesan and Vikas Kumar and Vasudha Bhatnagar},
Title         = {Infusing Knowledge into Large Language Models with Contextual Prompts},
Eprint        = {2403.01481v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge infusion is a promising method for enhancing Large Language Models
for domain-specific NLP tasks rather than pre-training models over large data
from scratch. These augmented LLMs typically depend on additional pre-training
or knowledge prompts from an existing knowledge graph, which is impractical in
many applications. In contrast, knowledge infusion directly from relevant
documents is more generalisable and alleviates the need for structured
knowledge graphs while also being useful for entities that are usually not
found in any knowledge graph. With this motivation, we propose a simple yet
generalisable approach for knowledge infusion by generating prompts from the
context in the input text. Our experiments show the effectiveness of our
approach which we evaluate by probing the fine-tuned LLMs.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.01481v1},
File          = {2403.01481v1.pdf}
}
@article{2410.04949v1,
Author        = {Yongming Chen and Miner Chen and Ye Zhu and Juan Pei and Siyu Chen and Yu Zhou and Yi Wang and Yifan Zhou and Hao Li and Songan Zhang},
Title         = {Leverage Knowledge Graph and Large Language Model for Law Article
  Recommendation: A Case Study of Chinese Criminal Law},
Eprint        = {2410.04949v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Court efficiency is vital for social stability. However, in most countries
around the world, the grassroots courts face case backlogs, with decisions
relying heavily on judicial personnel's cognitive labor, lacking intelligent
tools to improve efficiency. To address this issue, we propose an efficient law
article recommendation approach utilizing a Knowledge Graph (KG) and a Large
Language Model (LLM). Firstly, we propose a Case-Enhanced Law Article Knowledge
Graph (CLAKG) as a database to store current law statutes, historical case
information, and correspondence between law articles and historical cases.
Additionally, we introduce an automated CLAKG construction method based on LLM.
On this basis, we propose a closed-loop law article recommendation method.
Finally, through a series of experiments using judgment documents from the
website "China Judgements Online", we have improved the accuracy of law article
recommendation in cases from 0.549 to 0.694, demonstrating that our proposed
method significantly outperforms baseline approaches.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.04949v1},
File          = {2410.04949v1.pdf}
}
@article{2408.07611v2,
Author        = {Weijian Xie and Xuefeng Liang and Yuhui Liu and Kaihua Ni and Hong Cheng and Zetian Hu},
Title         = {WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation
  Integrating Web Search and Knowledge Graphs},
Eprint        = {2408.07611v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have greatly contributed to the development of
adaptive intelligent agents and are positioned as an important way to achieve
Artificial General Intelligence (AGI). However, LLMs are prone to produce
factually incorrect information and often produce "phantom" content that
undermines their reliability, which poses a serious challenge for their
deployment in real-world scenarios. Enhancing LLMs by combining external
databases and information retrieval mechanisms is an effective path. To address
the above challenges, we propose a new approach called WeKnow-RAG, which
integrates Web search and Knowledge Graphs into a "Retrieval-Augmented
Generation (RAG)" system. First, the accuracy and reliability of LLM responses
are improved by combining the structured representation of Knowledge Graphs
with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes
domain-specific knowledge graphs to satisfy a variety of queries and domains,
thereby improving performance on factual information and complex reasoning
tasks by employing multi-stage web page retrieval techniques using both sparse
and dense retrieval methods. Our approach effectively balances the efficiency
and accuracy of information retrieval, thus improving the overall retrieval
process. Finally, we also integrate a self-assessment mechanism for the LLM to
evaluate the trustworthiness of the answers it generates. Our approach proves
its outstanding effectiveness in a wide range of offline experiments and online
submissions.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.07611v2},
File          = {2408.07611v2.pdf}
}
@article{2502.00010v1,
Author        = {Changyong Qi and Linzhao Jia and Yuang Wei and Yuan-Hao Jiang and Xiaoqing Gu},
Title         = {IntelliChain: An Integrated Framework for Enhanced Socratic Method
  Dialogue with LLMs and Knowledge Graphs},
Eprint        = {2502.00010v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {With the continuous advancement of educational technology, the demand for
Large Language Models (LLMs) as intelligent educational agents in providing
personalized learning experiences is rapidly increasing. This study aims to
explore how to optimize the design and collaboration of a multi-agent system
tailored for Socratic teaching through the integration of LLMs and knowledge
graphs in a chain-of-thought dialogue approach, thereby enhancing the accuracy
and reliability of educational applications. By incorporating knowledge graphs,
this research has bolstered the capability of LLMs to handle specific
educational content, ensuring the accuracy and relevance of the information
provided. Concurrently, we have focused on developing an effective multi-agent
collaboration mechanism to facilitate efficient information exchange and chain
dialogues among intelligent agents, significantly improving the quality of
educational interaction and learning outcomes. In empirical research within the
domain of mathematics education, this framework has demonstrated notable
advantages in enhancing the accuracy and credibility of educational
interactions. This study not only showcases the potential application of LLMs
and knowledge graphs in mathematics teaching but also provides valuable
insights and methodologies for the development of future AI-driven educational
solutions.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2502.00010v1},
File          = {2502.00010v1.pdf}
}
@article{2401.06072v2,
Author        = {Ruilin Luo and Tianle Gu and Haoling Li and Junzhe Li and Zicheng Lin and Jiayi Li and Yujiu Yang},
Title         = {Chain of History: Learning and Forecasting with LLMs for Temporal
  Knowledge Graph Completion},
Eprint        = {2401.06072v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Temporal Knowledge Graph Completion (TKGC) is a complex task involving the
prediction of missing event links at future timestamps by leveraging
established temporal structural knowledge. This paper aims to provide a
comprehensive perspective on harnessing the advantages of Large Language Models
(LLMs) for reasoning in temporal knowledge graphs, presenting an easily
transferable pipeline. In terms of graph modality, we underscore the LLMs'
prowess in discerning the structural information of pivotal nodes within the
historical chain. As for the generation mode of the LLMs utilized for
inference, we conduct an exhaustive exploration into the variances induced by a
range of inherent factors in LLMs, with particular attention to the challenges
in comprehending reverse logic. We adopt a parameter-efficient fine-tuning
strategy to harmonize the LLMs with the task requirements, facilitating the
learning of the key knowledge highlighted earlier. Comprehensive experiments
are undertaken on several widely recognized datasets, revealing that our
framework exceeds or parallels existing methods across numerous popular
metrics. Additionally, we execute a substantial range of ablation experiments
and draw comparisons with several advanced commercial LLMs, to investigate the
crucial factors influencing LLMs' performance in structured temporal knowledge
inference tasks.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.06072v2},
File          = {2401.06072v2.pdf}
}
@article{2408.06787v2,
Author        = {Bo Xue and Yi Xu and Yunchong Song and Yiming Pang and Yuyang Ren and Jiaxin Ding and Luoyi Fu and Xinbing Wang},
Title         = {Unlock the Power of Frozen LLMs in Knowledge Graph Completion},
Eprint        = {2408.06787v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Traditional knowledge graph completion (KGC) methods rely solely on
structural information, struggling with the inherent sparsity of knowledge
graphs (KGs). Large Language Models (LLMs) learn extensive knowledge from large
corpora with powerful context modeling, making them promising for mitigating
the limitations of previous methods. Directly fine-tuning LLMs offers great
capability but comes at the cost of huge time and memory consumption, while
utilizing frozen LLMs yields suboptimal results.In this work, we aim to
leverage LLMs for KGC effectively and efficiently. We capture the context-aware
hidden states of knowledge triples by employing prompts to stimulate the
intermediate layers of LLMs. We then train a data-efficient classifier on these
hidden states to harness the inherent capabilities of frozen LLMs in KGC.
Additionally, to reduce ambiguity and enrich knowledge representation, we
generate detailed entity descriptions through subgraph sampling on KGs.
Extensive experiments on standard benchmarks demonstrate the efficiency and
effectiveness of our approach. We outperform traditional KGC methods across
most datasets and, notably, achieve classification performance comparable to
fine-tuned LLMs while enhancing GPU memory efficiency by $188\times$ and
accelerating training and inference by $13.48\times$.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.06787v2},
File          = {2408.06787v2.pdf}
}
@article{2412.03390v1,
Author        = {Ge Zheng and Alexandra Brintrup},
Title         = {Enhancing Supply Chain Visibility with Generative AI: An Exploratory
  Case Study on Relationship Prediction in Knowledge Graphs},
Eprint        = {2412.03390v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CE},
Abstract      = {A key stumbling block in effective supply chain risk management for companies
and policymakers is a lack of visibility on interdependent supply network
relationships. Relationship prediction, also called link prediction is an
emergent area of supply chain surveillance research that aims to increase the
visibility of supply chains using data-driven techniques. Existing methods have
been successful for predicting relationships but struggle to extract the
context in which these relationships are embedded - such as the products being
supplied or locations they are supplied from. Lack of context prevents
practitioners from distinguishing transactional relations from established
supply chain relations, hindering accurate estimations of risk. In this work,
we develop a new Generative Artificial Intelligence (Gen AI) enhanced machine
learning framework that leverages pre-trained language models as embedding
models combined with machine learning models to predict supply chain
relationships within knowledge graphs. By integrating Generative AI techniques,
our approach captures the nuanced semantic relationships between entities,
thereby improving supply chain visibility and facilitating more precise risk
management. Using data from a real case study, we show that GenAI-enhanced link
prediction surpasses all benchmarks, and demonstrate how GenAI models can be
explored and effectively used in supply chain risk management.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.03390v1},
File          = {2412.03390v1.pdf}
}
@article{2409.14038v5,
Author        = {Zhangcheng Qiang and Kerry Taylor and Weiqing Wang and Jing Jiang},
Title         = {OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model
  Hallucinations in Ontology Matching},
Eprint        = {2409.14038v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Hallucinations of large language models (LLMs) commonly occur in
domain-specific downstream tasks, with no exception in ontology matching (OM).
The prevalence of using LLMs for OM raises the need for benchmarks to better
understand LLM hallucinations. The OAEI-LLM dataset is an extended version of
the Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate
LLM-specific hallucinations in OM tasks. We outline the methodology used in
dataset construction and schema extension, and provide examples of potential
use cases.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.14038v5},
File          = {2409.14038v5.pdf}
}
@article{2010.05732v1,
Author        = {Rajat Patel and Francis Ferraro},
Title         = {On the Complementary Nature of Knowledge Graph Embedding, Fine Grain
  Entity Types, and Language Modeling},
Eprint        = {2010.05732v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We demonstrate the complementary natures of neural knowledge graph embedding,
fine-grain entity type prediction, and neural language modeling. We show that a
language model-inspired knowledge graph embedding approach yields both improved
knowledge graph embeddings and fine-grain entity type representations. Our work
also shows that jointly modeling both structured knowledge tuples and language
improves both.},
Year          = {2020},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2010.05732v1},
File          = {2010.05732v1.pdf}
}
@article{2405.12656v1,
Author        = {Yu-Hsiang Lin and Huang-Ting Shieh and Chih-Yu Liu and Kuang-Ting Lee and Hsiao-Cheng Chang and Jing-Lun Yang and Yu-Sheng Lin},
Title         = {Retrieval-Augmented Language Model for Extreme Multi-Label Knowledge
  Graph Link Prediction},
Eprint        = {2405.12656v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Extrapolation in Large language models (LLMs) for open-ended inquiry
encounters two pivotal issues: (1) hallucination and (2) expensive training
costs. These issues present challenges for LLMs in specialized domains and
personalized data, requiring truthful responses and low fine-tuning costs.
Existing works attempt to tackle the problem by augmenting the input of a
smaller language model with information from a knowledge graph (KG). However,
they have two limitations: (1) failing to extract relevant information from a
large one-hop neighborhood in KG and (2) applying the same augmentation
strategy for KGs with different characteristics that may result in low
performance. Moreover, open-ended inquiry typically yields multiple responses,
further complicating extrapolation. We propose a new task, the extreme
multi-label KG link prediction task, to enable a model to perform extrapolation
with multiple responses using structured real-world knowledge. Our retriever
identifies relevant one-hop neighbors by considering entity, relation, and
textual data together. Our experiments demonstrate that (1) KGs with different
characteristics require different augmenting strategies, and (2) augmenting the
language model's input with textual data improves task performance
significantly. By incorporating the retrieval-augmented framework with KG, our
framework, with a small parameter size, is able to extrapolate based on a given
KG. The code can be obtained on GitHub:
https://github.com/exiled1143/Retrieval-Augmented-Language-Model-for-Multi-Label-Knowledge-Graph-Link-Prediction.git},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.12656v1},
File          = {2405.12656v1.pdf}
}
@article{2405.04753v1,
Author        = {Yongheng Zhang and Tingwen Du and Yunshan Ma and Xiang Wang and Yi Xie and Guozheng Yang and Yuliang Lu and Ee-Chien Chang},
Title         = {AttacKG+:Boosting Attack Knowledge Graph Construction with Large
  Language Models},
Eprint        = {2405.04753v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Attack knowledge graph construction seeks to convert textual cyber threat
intelligence (CTI) reports into structured representations, portraying the
evolutionary traces of cyber attacks. Even though previous research has
proposed various methods to construct attack knowledge graphs, they generally
suffer from limited generalization capability to diverse knowledge types as
well as requirement of expertise in model design and tuning. Addressing these
limitations, we seek to utilize Large Language Models (LLMs), which have
achieved enormous success in a broad range of tasks given exceptional
capabilities in both language understanding and zero-shot task fulfillment.
Thus, we propose a fully automatic LLM-based framework to construct attack
knowledge graphs named: AttacKG+. Our framework consists of four consecutive
modules: rewriter, parser, identifier, and summarizer, each of which is
implemented by instruction prompting and in-context learning empowered by LLMs.
Furthermore, we upgrade the existing attack knowledge schema and propose a
comprehensive version. We represent a cyber attack as a temporally unfolding
event, each temporal step of which encapsulates three layers of representation,
including behavior graph, MITRE TTP labels, and state summary. Extensive
evaluation demonstrates that: 1) our formulation seamlessly satisfies the
information needs in threat event analysis, 2) our construction framework is
effective in faithfully and accurately extracting the information defined by
AttacKG+, and 3) our attack graph directly benefits downstream security
practices such as attack reconstruction. All the code and datasets will be
released upon acceptance.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.04753v1},
File          = {2405.04753v1.pdf}
}
@article{2310.18378v2,
Author        = {Qiu Ji and Guilin Qi and Yuxin Ye and Jiaye Li and Site Li and Jianjie Ren and Songtao Lu},
Title         = {Ontology Revision based on Pre-trained Language Models},
Eprint        = {2310.18378v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology revision aims to seamlessly incorporate a new ontology into an
existing ontology and plays a crucial role in tasks such as ontology evolution,
ontology maintenance, and ontology alignment. Similar to repair single
ontologies, resolving logical incoherence in the task of ontology revision is
also important and meaningful, because incoherence is a main potential factor
to cause inconsistency and reasoning with an inconsistent ontology will obtain
meaningless answers.To deal with this problem, various ontology revision
approaches have been proposed to define revision operators and design ranking
strategies for axioms in an ontology. However, they rarely consider axiom
semantics which provides important information to differentiate axioms. In
addition, pre-trained models can be utilized to encode axiom semantics, and
have been widely applied in many natural language processing tasks and
ontology-related ones in recent years.Therefore, in this paper, we study how to
apply pre-trained models to revise ontologies. We first define four scoring
functions to rank axioms based on a pre-trained model by considering various
information from an ontology. Based on the functions, an ontology revision
algorithm is then proposed to deal with unsatisfiable concepts at once. To
improve efficiency, an adapted revision algorithm is designed to deal with
unsatisfiable concepts group by group. We conduct experiments over 19 ontology
pairs and compare our algorithms and scoring functions with existing ones.
According to the experiments, our algorithms could achieve promising
performance.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.18378v2},
File          = {2310.18378v2.pdf}
}
@article{2312.11539v3,
Author        = {Shangshang Zheng and He Bai and Yizhe Zhang and Yi Su and Xiaochuan Niu and Navdeep Jaitly},
Title         = {KGLens: Towards Efficient and Effective Knowledge Probing of Large
  Language Models with Knowledge Graphs},
Eprint        = {2312.11539v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models (LLMs) might hallucinate facts, while curated Knowledge
Graph (KGs) are typically factually reliable especially with domain-specific
knowledge. Measuring the alignment between KGs and LLMs can effectively probe
the factualness and identify the knowledge blind spots of LLMs. However,
verifying the LLMs over extensive KGs can be expensive. In this paper, we
present KGLens, a Thompson-sampling-inspired framework aimed at effectively and
efficiently measuring the alignment between KGs and LLMs. KGLens features a
graph-guided question generator for converting KGs into natural language, along
with a carefully designed importance sampling strategy based on parameterized
KG structure to expedite KG traversal. Our simulation experiment compares the
brute force method with KGLens under six different sampling methods,
demonstrating that our approach achieves superior probing efficiency.
Leveraging KGLens, we conducted in-depth analyses of the factual accuracy of
ten LLMs across three large domain-specific KGs from Wikidata, composing over
19K edges, 700 relations, and 21K entities. Human evaluation results indicate
that KGLens can assess LLMs with a level of accuracy nearly equivalent to that
of human annotators, achieving 95.7% of the accuracy rate.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.11539v3},
File          = {2312.11539v3.pdf}
}
@article{2406.11131v2,
Author        = {Yushi Sun and Hao Xin and Kai Sun and Yifan Ethan Xu and Xiao Yang and Xin Luna Dong and Nan Tang and Lei Chen},
Title         = {Are Large Language Models a Good Replacement of Taxonomies?},
Eprint        = {2406.11131v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) demonstrate an impressive ability to internalize
knowledge and answer natural language questions. Although previous studies
validate that LLMs perform well on general knowledge while presenting poor
performance on long-tail nuanced knowledge, the community is still doubtful
about whether the traditional knowledge graphs should be replaced by LLMs. In
this paper, we ask if the schema of knowledge graph (i.e., taxonomy) is made
obsolete by LLMs. Intuitively, LLMs should perform well on common taxonomies
and at taxonomy levels that are common to people. Unfortunately, there lacks a
comprehensive benchmark that evaluates the LLMs over a wide range of taxonomies
from common to specialized domains and at levels from root to leaf so that we
can draw a confident conclusion. To narrow the research gap, we constructed a
novel taxonomy hierarchical structure discovery benchmark named TaxoGlimpse to
evaluate the performance of LLMs over taxonomies. TaxoGlimpse covers ten
representative taxonomies from common to specialized domains with in-depth
experiments of different levels of entities in this taxonomy from root to leaf.
Our comprehensive experiments of eighteen state-of-the-art LLMs under three
prompting settings validate that LLMs can still not well capture the knowledge
of specialized taxonomies and leaf-level entities.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.11131v2},
File          = {2406.11131v2.pdf}
}
@article{2402.17897v2,
Author        = {Hang Dong and Jiaoyan Chen and Yuan He and Yongsheng Gao and Ian Horrocks},
Title         = {A Language Model based Framework for New Concept Placement in Ontologies},
Eprint        = {2402.17897v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We investigate the task of inserting new concepts extracted from texts into
an ontology using language models. We explore an approach with three steps:
edge search which is to find a set of candidate locations to insert (i.e.,
subsumptions between concepts), edge formation and enrichment which leverages
the ontological structure to produce and enhance the edge candidates, and edge
selection which eventually locates the edge to be placed into. In all steps, we
propose to leverage neural methods, where we apply embedding-based methods and
contrastive learning with Pre-trained Language Models (PLMs) such as BERT for
edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder,
and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for
edge selection. We evaluate the methods on recent datasets created using the
SNOMED CT ontology and the MedMentions entity linking benchmark. The best
settings in our framework use fine-tuned PLM for search and a multi-label
Cross-encoder for selection. Zero-shot prompting of LLMs is still not adequate
for the task, and we propose explainable instruction tuning of LLMs for
improved performance. Our study shows the advantages of PLMs and highlights the
encouraging performance of LLMs that motivates future studies.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.17897v2},
File          = {2402.17897v2.pdf}
}
@article{2310.11220v1,
Author        = {Jiho Kim and Yeonsu Kwon and Yohan Jo and Edward Choi},
Title         = {KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using
  Large Language Models},
Eprint        = {2310.11220v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {While large language models (LLMs) have made considerable advancements in
understanding and generating unstructured text, their application in structured
data remains underexplored. Particularly, using LLMs for complex reasoning
tasks on knowledge graphs (KGs) remains largely untouched. To address this, we
propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing
KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and
Inference, each aimed at partitioning sentences, retrieving relevant graph
components, and deriving logical conclusions, respectively. We evaluate KG-GPT
using KG-based fact verification and KGQA benchmarks, with the model showing
competitive and robust performance, even outperforming several fully-supervised
models. Our work, therefore, marks a significant step in unifying structured
and unstructured data processing within the realm of LLMs.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.11220v1},
File          = {2310.11220v1.pdf}
}
@article{2311.14740v1,
Author        = {Bohan Chen and Andrea L. Bertozzi},
Title         = {AutoKG: Efficient Automated Knowledge Graph Generation for Language
  Models},
Eprint        = {2311.14740v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Traditional methods of linking large language models (LLMs) to knowledge
bases via the semantic similarity search often fall short of capturing complex
relational dynamics. To address these limitations, we introduce AutoKG, a
lightweight and efficient approach for automated knowledge graph (KG)
construction. For a given knowledge base consisting of text blocks, AutoKG
first extracts keywords using a LLM and then evaluates the relationship weight
between each pair of keywords using graph Laplace learning. We employ a hybrid
search scheme combining vector similarity and graph-based associations to
enrich LLM responses. Preliminary experiments demonstrate that AutoKG offers a
more comprehensive and interconnected knowledge retrieval mechanism compared to
the semantic similarity search, thereby enhancing the capabilities of LLMs in
generating more insightful and relevant outputs.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.14740v1},
File          = {2311.14740v1.pdf}
}
@article{2403.12151v3,
Author        = {Filippos Gouidis and Katerina Papantoniou and Konstantinos Papoutsakis and Theodore Patkos and Antonis Argyros and Dimitris Plexousakis},
Title         = {Fusing Domain-Specific Content from Large Language Models into Knowledge
  Graphs for Enhanced Zero Shot Object State Classification},
Eprint        = {2403.12151v3},
DOI           = {10.1609/aaaiss.v3i1.31190},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Domain-specific knowledge can significantly contribute to addressing a wide
variety of vision tasks. However, the generation of such knowledge entails
considerable human labor and time costs. This study investigates the potential
of Large Language Models (LLMs) in generating and providing domain-specific
information through semantic embeddings. To achieve this, an LLM is integrated
into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors
in the context of the Vision-based Zero-shot Object State Classification task.
We thoroughly examine the behavior of the LLM through an extensive ablation
study. Our findings reveal that the integration of LLM-based embeddings, in
combination with general-purpose pre-trained embeddings, leads to substantial
performance improvements. Drawing insights from this ablation study, we conduct
a comparative analysis against competing models, thereby highlighting the
state-of-the-art performance achieved by the proposed approach.},
Year          = {2024},
Month         = {Mar},
Note          = {Proceedings of the AAAI Spring Symposium, 2024, pages 115-124},
Url           = {http://arxiv.org/abs/2403.12151v3},
File          = {2403.12151v3.pdf}
}
@article{2404.19234v1,
Author        = {Abir Chakraborty},
Title         = {Multi-hop Question Answering over Knowledge Graphs using Large Language
  Models},
Eprint        = {2404.19234v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs (KGs) are large datasets with specific structures
representing large knowledge bases (KB) where each node represents a key entity
and relations amongst them are typed edges. Natural language queries formed to
extract information from a KB entail starting from specific nodes and reasoning
over multiple edges of the corresponding KG to arrive at the correct set of
answer nodes. Traditional approaches of question answering on KG are based on
(a) semantic parsing (SP), where a logical form (e.g., S-expression, SPARQL
query, etc.) is generated using node and edge embeddings and then reasoning
over these representations or tuning language models to generate the final
answer directly, or (b) information-retrieval based that works by extracting
entities and relations sequentially. In this work, we evaluate the capability
of (LLMs) to answer questions over KG that involve multiple hops. We show that
depending upon the size and nature of the KG we need different approaches to
extract and feed the relevant information to an LLM since every LLM comes with
a fixed context window. We evaluate our approach on six KGs with and without
the availability of example-specific sub-graphs and show that both the IR and
SP-based methods can be adopted by LLMs resulting in an extremely competitive
performance.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.19234v1},
File          = {2404.19234v1.pdf}
}
@article{2407.18470v1,
Author        = {DaiFeng Li and Fan Xu},
Title         = {Synergizing Knowledge Graphs with Large Language Models: A Comprehensive
  Review and Future Prospects},
Eprint        = {2407.18470v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Recent advancements have witnessed the ascension of Large Language Models
(LLMs), endowed with prodigious linguistic capabilities, albeit marred by
shortcomings including factual inconsistencies and opacity. Conversely,
Knowledge Graphs (KGs) harbor verifiable knowledge and symbolic reasoning
prowess, thereby complementing LLMs' deficiencies. Against this backdrop, the
synergy between KGs and LLMs emerges as a pivotal research direction. Our
contribution in this paper is a comprehensive dissection of the latest
developments in integrating KGs with LLMs. Through meticulous analysis of their
confluence points and methodologies, we introduce a unifying framework designed
to elucidate and stimulate further exploration among scholars engaged in
cognate disciplines. This framework serves a dual purpose: it consolidates
extant knowledge while simultaneously delineating novel avenues for real-world
deployment, thereby amplifying the translational impact of academic research.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.18470v1},
File          = {2407.18470v1.pdf}
}
@article{2408.15903v2,
Author        = {Ruirui Chen and Weifeng Jiang and Chengwei Qin and Ishaan Singh Rawal and Cheston Tan and Dongkyu Choi and Bo Xiong and Bo Ai},
Title         = {LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration
  in Evolving Environments},
Eprint        = {2408.15903v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The important challenge of keeping knowledge in Large Language Models (LLMs)
up-to-date has led to the development of various methods for incorporating new
facts. However, existing methods for such knowledge editing still face
difficulties with multi-hop questions that require accurate fact identification
and sequential logical reasoning, particularly among numerous fact updates. To
tackle these challenges, this paper introduces Graph Memory-based Editing for
Large Language Models (GMeLLo), a straightforward and effective method that
merges the explicit knowledge representation of Knowledge Graphs (KGs) with the
linguistic flexibility of LLMs. Beyond merely leveraging LLMs for question
answering, GMeLLo employs these models to convert free-form language into
structured queries and fact triples, facilitating seamless interaction with KGs
for rapid updates and precise multi-hop reasoning. Our results show that GMeLLo
significantly surpasses current state-of-the-art (SOTA) knowledge editing
methods in the multi-hop question answering benchmark, MQuAKE, especially in
scenarios with extensive knowledge edits.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.15903v2},
File          = {2408.15903v2.pdf}
}
@article{2410.11588v1,
Author        = {Yejin Kim and Eojin Kang and Juae Kim and H. Howie Huang},
Title         = {Causal Reasoning in Large Language Models: A Knowledge Graph Approach},
Eprint        = {2410.11588v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) typically improve performance by either
retrieving semantically similar information, or enhancing reasoning abilities
through structured prompts like chain-of-thought. While both strategies are
considered crucial, it remains unclear which has a greater impact on model
performance or whether a combination of both is necessary. This paper answers
this question by proposing a knowledge graph (KG)-based random-walk reasoning
approach that leverages causal relationships. We conduct experiments on the
commonsense question answering task that is based on a KG. The KG inherently
provides both relevant information, such as related entity keywords, and a
reasoning structure through the connections between nodes. Experimental results
show that the proposed KG-based random-walk reasoning method improves the
reasoning ability and performance of LLMs. Interestingly, incorporating three
seemingly irrelevant sentences into the query using KG-based random-walk
reasoning enhances LLM performance, contrary to conventional wisdom. These
findings suggest that integrating causal structures into prompts can
significantly improve reasoning capabilities, providing new insights into the
role of causality in optimizing LLM performance.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.11588v1},
File          = {2410.11588v1.pdf}
}
@article{2410.12298v2,
Author        = {Lei Sun and Xinchen Wang and Youdi Li},
Title         = {Pyramid-Driven Alignment: Pyramid Principle Guided Integration of Large
  Language Models and Knowledge Graphs},
Eprint        = {2410.12298v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) possess impressive reasoning abilities but are
prone to generating incorrect information, often referred to as hallucinations.
While incorporating external Knowledge Graphs (KGs) can partially mitigate this
issue, existing methods primarily treat KGs as static knowledge repositories,
overlooking the critical disparity between KG and LLM knowledge, and failing to
fully exploit the reasoning capabilities inherent in KGs. To address these
limitations, we propose Pyramid-Driven Alignment (PDA), a novel framework for
seamlessly integrating LLMs with KGs. PDA utilizes Pyramid Principle analysis
to construct a hierarchical pyramid structure. This structure is designed to
reflect the input question and generate more validated deductive knowledge,
thereby enhancing the alignment of LLMs and KGs and ensuring more cohesive
integration. Furthermore, PDA employs a recursive mechanism to harness the
underlying reasoning abilities of KGs, resulting in more accurate knowledge
retrieval for question-answering tasks. Our experimental results reveal a
substantial performance advantage of PDA over state-of-the-art baselines, with
improvements reaching 26.70% and 26.78%.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.12298v2},
File          = {2410.12298v2.pdf}
}
@article{2410.15064v1,
Author        = {George Hannah and Rita T. Sousa and Ioannis Dasoulas and Claudia d'Amato},
Title         = {A Prompt Engineering Approach and a Knowledge Graph based Framework for
  Tackling Legal Implications of Large Language Model Answers},
Eprint        = {2410.15064v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {With the recent surge in popularity of Large Language Models (LLMs), there is
the rising risk of users blindly trusting the information in the response, even
in cases where the LLM recommends actions that have potential legal
implications and this may put the user in danger. We provide an empirical
analysis on multiple existing LLMs showing the urgency of the problem. Hence,
we propose a short-term solution consisting in an approach for isolating these
legal issues through prompt re-engineering. We further analyse the outcomes but
also the limitations of the prompt engineering based approach and we highlight
the need of additional resources for fully solving the problem We also propose
a framework powered by a legal knowledge graph (KG) to generate legal citations
for these legal issues, enriching the response of the LLM.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.15064v1},
File          = {2410.15064v1.pdf}
}
@article{2410.23875v1,
Author        = {Liyi Chen and Panrong Tong and Zhongming Jin and Ying Sun and Jieping Ye and Hui Xiong},
Title         = {Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model
  on Knowledge Graphs},
Eprint        = {2410.23875v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models (LLMs) have shown remarkable reasoning capabilities on
complex tasks, but they still suffer from out-of-date knowledge,
hallucinations, and opaque decision-making. In contrast, Knowledge Graphs (KGs)
can provide explicit and editable knowledge for LLMs to alleviate these issues.
Existing paradigm of KG-augmented LLM manually predefines the breadth of
exploration space and requires flawless navigation in KGs. However, this
paradigm cannot adaptively explore reasoning paths in KGs based on the question
semantics and self-correct erroneous reasoning paths, resulting in a bottleneck
in efficiency and effect. To address these limitations, we propose a novel
self-correcting adaptive planning paradigm for KG-augmented LLM named
Plan-on-Graph (PoG), which first decomposes the question into several
sub-objectives and then repeats the process of adaptively exploring reasoning
paths, updating memory, and reflecting on the need to self-correct erroneous
reasoning paths until arriving at the answer. Specifically, three important
mechanisms of Guidance, Memory, and Reflection are designed to work together,
to guarantee the adaptive breadth of self-correcting planning for graph
reasoning. Finally, extensive experiments on three real-world datasets
demonstrate the effectiveness and efficiency of PoG.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.23875v1},
File          = {2410.23875v1.pdf}
}
@article{2501.02711v1,
Author        = {Zaiyi Zheng and Yushun Dong and Song Wang and Haochen Liu and Qi Wang and Jundong Li},
Title         = {KG-CF: Knowledge Graph Completion with Context Filtering under the
  Guidance of Large Language Models},
Eprint        = {2501.02711v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models (LLMs) have shown impressive performance in various
tasks, including knowledge graph completion (KGC). However, current studies
mostly apply LLMs to classification tasks, like identifying missing triplets,
rather than ranking-based tasks, where the model ranks candidate entities based
on plausibility. This focus limits the practical use of LLMs in KGC, as
real-world applications prioritize highly plausible triplets. Additionally,
while graph paths can help infer the existence of missing triplets and improve
completion accuracy, they often contain redundant information. To address these
issues, we propose KG-CF, a framework tailored for ranking-based KGC tasks.
KG-CF leverages LLMs' reasoning abilities to filter out irrelevant contexts,
achieving superior results on real-world datasets. The code and datasets are
available at \url{https://anonymous.4open.science/r/KG-CF}.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.02711v1},
File          = {2501.02711v1.pdf}
}
@article{2501.03566v1,
Author        = {Benedikt Reitemeyer and Hans-Georg Fill},
Title         = {Applying Large Language Models in Knowledge Graph-based Enterprise
  Modeling: Challenges and Opportunities},
Eprint        = {2501.03566v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.MA},
Abstract      = {The role of large language models (LLMs) in enterprise modeling has recently
started to shift from academic research to that of industrial applications.
Thereby, LLMs represent a further building block for the machine-supported
generation of enterprise models. In this paper we employ a knowledge
graph-based approach for enterprise modeling and investigate the potential
benefits of LLMs in this context. In addition, the findings of an expert survey
and ChatGPT-4o-based experiments demonstrate that LLM-based model generations
exhibit minimal variability, yet remain constrained to specific tasks, with
reliability declining for more intricate tasks. The survey results further
suggest that the supervision and intervention of human modeling experts are
essential to ensure the accuracy and integrity of the generated models.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.03566v1},
File          = {2501.03566v1.pdf}
}
@article{2501.08897v1,
Author        = {Qinyu Ma and Yuhao Zhou and Jianfeng Li},
Title         = {Leveraging Large Language Models as Knowledge-Driven Agents for Reliable
  Retrosynthesis Planning},
Eprint        = {2501.08897v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Identifying reliable synthesis pathways in materials chemistry is a complex
task, particularly in polymer science, due to the intricate and often
non-unique nomenclature of macromolecules. To address this challenge, we
propose an agent system that integrates large language models (LLMs) and
knowledge graphs (KGs). By leveraging LLMs' powerful capabilities for
extracting and recognizing chemical substance names, and storing the extracted
data in a structured knowledge graph, our system fully automates the retrieval
of relevant literatures, extraction of reaction data, database querying,
construction of retrosynthetic pathway trees, further expansion through the
retrieval of additional literature and recommendation of optimal reaction
pathways. A novel Multi-branched Reaction Pathway Search (MBRPS) algorithm
enables the exploration of all pathways, with a particular focus on
multi-branched ones, helping LLMs overcome weak reasoning in multi-branched
paths. This work represents the first attempt to develop a fully automated
retrosynthesis planning agent tailored specially for macromolecules powered by
LLMs. Applied to polyimide synthesis, our new approach constructs a
retrosynthetic pathway tree with hundreds of pathways and recommends optimized
routes, including both known and novel pathways, demonstrating its
effectiveness and potential for broader applications.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.08897v1},
File          = {2501.08897v1.pdf}
}
@article{2501.08686v1,
Author        = {Chuangtao Ma and Sriom Chakrabarti and Arijit Khan and Bálint Molnár},
Title         = {Knowledge Graph-based Retrieval-Augmented Generation for Schema Matching},
Eprint        = {2501.08686v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Traditional similarity-based schema matching methods are incapable of
resolving semantic ambiguities and conflicts in domain-specific complex mapping
scenarios due to missing commonsense and domain-specific knowledge. The
hallucination problem of large language models (LLMs) also makes it challenging
for LLM-based schema matching to address the above issues. Therefore, we
propose a Knowledge Graph-based Retrieval-Augmented Generation model for Schema
Matching, referred to as the KG-RAG4SM. In particular, KG-RAG4SM introduces
novel vector-based, graph traversal-based, and query-based graph retrievals, as
well as a hybrid approach and ranking schemes that identify the most relevant
subgraphs from external large knowledge graphs (KGs). We showcase that KG-based
retrieval-augmented LLMs are capable of generating more accurate results for
complex matching cases without any re-training. Our experimental results show
that KG-RAG4SM outperforms the LLM-based state-of-the-art (SOTA) methods (e.g.,
Jellyfish-8B) by 35.89% and 30.50% in terms of precision and F1 score on the
MIMIC dataset, respectively; KG-RAG4SM with GPT-4o-mini outperforms the
pre-trained language model (PLM)-based SOTA methods (e.g., SMAT) by 69.20% and
21.97% in terms of precision and F1 score on the Synthea dataset, respectively.
The results also demonstrate that our approach is more efficient in end-to-end
schema matching, and scales to retrieve from large KGs. Our case studies on the
dataset from the real-world schema matching scenario exhibit that the
hallucination problem of LLMs for schema matching is well mitigated by our
solution.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.08686v1},
File          = {2501.08686v1.pdf}
}
@article{2405.12035v1,
Author        = {Diego Sanmartin},
Title         = {KG-RAG: Bridging the Gap Between Knowledge and Creativity},
Eprint        = {2405.12035v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ensuring factual accuracy while maintaining the creative capabilities of
Large Language Model Agents (LMAs) poses significant challenges in the
development of intelligent agent systems. LMAs face prevalent issues such as
information hallucinations, catastrophic forgetting, and limitations in
processing long contexts when dealing with knowledge-intensive tasks. This
paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)
pipeline, a novel framework designed to enhance the knowledge capabilities of
LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities
of LLMs, thereby significantly reducing the reliance on the latent knowledge of
LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then
performs information retrieval over the newly created graph to perform KGQA
(Knowledge Graph Question Answering). The retrieval methodology leverages a
novel algorithm called Chain of Explorations (CoE) which benefits from LLMs
reasoning to explore nodes and relationships within the KG sequentially.
Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable
improvements in the reduction of hallucinated content and suggest a promising
path toward developing intelligent systems adept at handling
knowledge-intensive tasks.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.12035v1},
File          = {2405.12035v1.pdf}
}
@article{2407.05868v2,
Author        = {Yanxu Zhu and Jinlin Xiao and Yuhang Wang and Jitao Sang},
Title         = {KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge
  Graph-based False Premise Questions},
Eprint        = {2407.05868v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent studies have demonstrated that large language models (LLMs) are
susceptible to being misled by false premise questions (FPQs), leading to
errors in factual knowledge, know as factuality hallucination. Existing
benchmarks that assess this vulnerability primarily rely on manual
construction, resulting in limited scale and lack of scalability. In this work,
we introduce an automated, scalable pipeline to create FPQs based on knowledge
graphs (KGs). The first step is modifying true triplets extracted from KGs to
create false premises. Subsequently, utilizing the state-of-the-art
capabilities of GPTs, we generate semantically rich FPQs. Based on the proposed
method, we present a comprehensive benchmark, the Knowledge Graph-based False
Premise Questions (KG-FPQ), which contains approximately 178k FPQs across three
knowledge domains, at six levels of confusability, and in two task formats.
Using KG-FPQ, we conduct extensive evaluations on several representative LLMs
and provide valuable insights. The KG-FPQ dataset and code are available
at~https://github.com/yanxuzhu/KG-FPQ.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.05868v2},
File          = {2407.05868v2.pdf}
}
@article{2410.00005v1,
Author        = {Yikuan Xia and Jiazun Chen and Jun Gao},
Title         = {Winning Solution For Meta KDD Cup' 24},
Eprint        = {2410.00005v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {This paper describes the winning solutions of all tasks in Meta KDD Cup 24
from db3 team. The challenge is to build a RAG system from web sources and
knowledge graphs. We are given multiple sources for each query to help us
answer the question. The CRAG challenge involves three tasks: (1) condensing
information from web pages into accurate answers, (2) integrating structured
data from mock knowledge graphs, and (3) selecting and integrating critical
data from extensive web pages and APIs to reflect real-world retrieval
challenges. Our solution for Task #1 is a framework of web or open-data
retrieval and answering. The large language model (LLM) is tuned for better RAG
performance and less hallucination. Task #2 and Task #3 solutions are based on
a regularized API set for domain questions and the API generation method using
tuned LLM. Our knowledge graph API interface extracts directly relevant
information to help LLMs answer correctly. Our solution achieves 1st place on
all three tasks, achieving a score of 28.4%, 42.7%, and 47.8%, respectively.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2410.00005v1},
File          = {2410.00005v1.pdf}
}
@article{2412.16833v2,
Author        = {Kaiwen Zuo and Yirui Jiang and Fan Mo and Pietro Lio},
Title         = {KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge
  Graph Enhancement for Medical Diagnosis},
Eprint        = {2412.16833v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Integrating Large Language Models (LLMs) in healthcare diagnosis demands
systematic frameworks that can handle complex medical scenarios while
maintaining specialized expertise. We present KG4Diagnosis, a novel
hierarchical multi-agent framework that combines LLMs with automated knowledge
graph construction, encompassing 362 common diseases across medical
specialties. Our framework mirrors real-world medical systems through a
two-tier architecture: a general practitioner (GP) agent for initial assessment
and triage, coordinating with specialized agents for in-depth diagnosis in
specific domains. The core innovation lies in our end-to-end knowledge graph
generation methodology, incorporating: (1) semantic-driven entity and relation
extraction optimized for medical terminology, (2) multi-dimensional decision
relationship reconstruction from unstructured medical texts, and (3)
human-guided reasoning for knowledge expansion. KG4Diagnosis serves as an
extensible foundation for specialized medical diagnosis systems, with
capabilities to incorporate new diseases and medical knowledge. The framework's
modular design enables seamless integration of domain-specific enhancements,
making it valuable for developing targeted medical diagnosis systems. We
provide architectural guidelines and protocols to facilitate adoption across
medical contexts.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.16833v2},
File          = {2412.16833v2.pdf}
}
@article{2502.06472v1,
Author        = {Yuxing Lu and Jinzhuo Wang},
Title         = {KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph
  Enrichment},
Eprint        = {2502.06472v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical
for modern AI systems, but manual curation struggles to scale with the rapid
growth of scientific literature. This paper presents KARMA, a novel framework
employing multi-agent large language models (LLMs) to automate KG enrichment
through structured analysis of unstructured text. Our approach employs nine
collaborative agents, spanning entity discovery, relation extraction, schema
alignment, and conflict resolution that iteratively parse documents, verify
extracted knowledge, and integrate it into existing graph structures while
adhering to domain-specific schema. Experiments on 1,200 PubMed articles from
three different domains demonstrate the effectiveness of KARMA in knowledge
graph enrichment, with the identification of up to 38,230 new entities while
achieving 83.1\% LLM-verified correctness and reducing conflict edges by 18.6\%
through multi-layer assessments.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.06472v1},
File          = {2502.06472v1.pdf}
}
@article{2402.18715v1,
Author        = {Andrew Eells and Brandon Dave and Pascal Hitzler and Cogan Shimizu},
Title         = {Commonsense Ontology Micropatterns},
Eprint        = {2402.18715v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The previously introduced Modular Ontology Modeling methodology (MOMo)
attempts to mimic the human analogical process by using modular patterns to
assemble more complex concepts. To support this, MOMo organizes organizes
ontology design patterns into design libraries, which are programmatically
queryable, to support accelerated ontology development, for both human and
automated processes. However, a major bottleneck to large-scale deployment of
MOMo is the (to-date) limited availability of ready-to-use ontology design
patterns. At the same time, Large Language Models have quickly become a source
of common knowledge and, in some cases, replacing search engines for questions.
In this paper, we thus present a collection of 104 ontology design patterns
representing often occurring nouns, curated from the common-sense knowledge
available in LLMs, organized into a fully-annotated modular ontology design
library ready for use with MOMo.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.18715v1},
File          = {2402.18715v1.pdf}
}
@article{2401.00426v1,
Author        = {Chaojie Wang and Yishi Xu and Zhong Peng and Chenxi Zhang and Bo Chen and Xinrun Wang and Lei Feng and Bo An},
Title         = {keqing: knowledge-based question answering is a nature chain-of-thought
  mentor of LLM},
Eprint        = {2401.00426v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have exhibited remarkable performance on various
natural language processing (NLP) tasks, especially for question answering.
However, in the face of problems beyond the scope of knowledge, these LLMs tend
to talk nonsense with a straight face, where the potential solution could be
incorporating an Information Retrieval (IR) module and generating response
based on these retrieved knowledge. In this paper, we present a novel framework
to assist LLMs, such as ChatGPT, to retrieve question-related structured
information on the knowledge graph, and demonstrate that Knowledge-based
question answering (Keqing) could be a nature Chain-of-Thought (CoT) mentor to
guide the LLM to sequentially find the answer entities of a complex question
through interpretable logical chains. Specifically, the workflow of Keqing will
execute decomposing a complex question according to predefined templates,
retrieving candidate entities on knowledge graph, reasoning answers of
sub-questions, and finally generating response with reasoning paths, which
greatly improves the reliability of LLM's response. The experimental results on
KBQA datasets show that Keqing can achieve competitive performance and
illustrate the logic of answering each question.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2401.00426v1},
File          = {2401.00426v1.pdf}
}
@article{2402.15929v2,
Author        = {Isha Chaudhary and Vedaant V. Jain and Gagandeep Singh},
Title         = {Decoding Intelligence: A Framework for Certifying Knowledge
  Comprehension in LLMs},
Eprint        = {2402.15929v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge comprehension capability is an important aspect of human
intelligence. As Large Language Models (LLMs) are being envisioned as
superhuman agents, it is crucial for them to be proficient at knowledge
comprehension. However, existing benchmarking studies do not provide
consistent, generalizable, and formal guarantees on the knowledge comprehension
capabilities of LLMs. In this work, we propose the first framework to certify
knowledge comprehension in LLMs with formal probabilistic guarantees. Our
certificates are quantitative -- they consist of high-confidence, tight bounds
on the probability that a target LLM gives the correct answer on any knowledge
comprehension prompt sampled from a distribution. We design and certify novel
specifications that precisely represent distributions of knowledge
comprehension prompts leveraging knowledge graphs. We certify SOTA LLMs for
specifications over the Wikidata5m knowledge graph. We find that the knowledge
comprehension capability improves significantly with scaling the size of the
models.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.15929v2},
File          = {2402.15929v2.pdf}
}
@article{2412.00765v1,
Author        = {Aihua Pei and Zehua Yang and Shunan Zhu and Ruoxi Cheng and Ju Jia},
Title         = {SelfPrompt: Autonomously Evaluating LLM Robustness via
  Domain-Constrained Knowledge Guidelines and Refined Adversarial Prompts},
Eprint        = {2412.00765v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Traditional methods for evaluating the robustness of large language models
(LLMs) often rely on standardized benchmarks, which can escalate costs and
limit evaluations across varied domains. This paper introduces a novel
framework designed to autonomously evaluate the robustness of LLMs by
incorporating refined adversarial prompts and domain-constrained knowledge
guidelines in the form of knowledge graphs. Our method systematically generates
descriptive sentences from domain-constrained knowledge graph triplets to
formulate adversarial prompts, enhancing the relevance and challenge of the
evaluation. These prompts, generated by the LLM itself and tailored to evaluate
its own robustness, undergo a rigorous filtering and refinement process,
ensuring that only those with high textual fluency and semantic fidelity are
used. This self-evaluation mechanism allows the LLM to evaluate its robustness
without the need for external benchmarks. We assess the effectiveness of our
framework through extensive testing on both proprietary models like ChatGPT and
open-source models such as Llama-3.1, Phi-3, and Mistral. Results confirm that
our approach not only reduces dependency on conventional data but also provides
a targeted and efficient means of evaluating LLM robustness in constrained
domains.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.00765v1},
File          = {2412.00765v1.pdf}
}
@article{2501.02226v1,
Author        = {Shijie Wang and Wenqi Fan and Yue Feng and Xinyu Ma and Shuaiqiang Wang and Dawei Yin},
Title         = {Knowledge Graph Retrieval-Augmented Generation for LLM-based
  Recommendation},
Eprint        = {2501.02226v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Recommender systems have become increasingly vital in our daily lives,
helping to alleviate the problem of information overload across various
user-oriented online services. The emergence of Large Language Models (LLMs)
has yielded remarkable achievements, demonstrating their potential for the
development of next-generation recommender systems. Despite these advancements,
LLM-based recommender systems face inherent limitations stemming from their LLM
backbones, particularly issues of hallucinations and the lack of up-to-date and
domain-specific knowledge. Recently, Retrieval-Augmented Generation (RAG) has
garnered significant attention for addressing these limitations by leveraging
external knowledge sources to enhance the understanding and generation of LLMs.
However, vanilla RAG methods often introduce noise and neglect structural
relationships in knowledge, limiting their effectiveness in LLM-based
recommendations. To address these limitations, we propose to retrieve
high-quality and up-to-date structure information from the knowledge graph (KG)
to augment recommendations. Specifically, our approach develops a
retrieval-augmented framework, termed K-RagRec, that facilitates the
recommendation generation process by incorporating structure information from
the external KG. Extensive experiments have been conducted to demonstrate the
effectiveness of our proposed method.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.02226v1},
File          = {2501.02226v1.pdf}
}
@article{2309.15427v2,
Author        = {Yijun Tian and Huan Song and Zichen Wang and Haozhu Wang and Ziqing Hu and Fang Wang and Nitesh V. Chawla and Panpan Xu},
Title         = {Graph Neural Prompting with Large Language Models},
Eprint        = {2309.15427v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have shown remarkable generalization capability
with exceptional performance in various language modeling tasks. However, they
still exhibit inherent limitations in precisely capturing and returning
grounded knowledge. While existing work has explored utilizing knowledge graphs
(KGs) to enhance language modeling via joint training and customized model
architectures, applying this to LLMs is problematic owing to their large number
of parameters and high computational cost. Therefore, how to enhance
pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented
generation, remains an open question. In this work, we propose Graph Neural
Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in
learning beneficial knowledge from KGs. GNP encompasses various designs,
including a standard graph neural network encoder, a cross-modality pooling
module, a domain projector, and a self-supervised link prediction objective.
Extensive experiments on multiple datasets demonstrate the superiority of GNP
on both commonsense and biomedical reasoning tasks across different LLM sizes
and settings. Code is available at https://github.com/meettyj/GNP.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.15427v2},
File          = {2309.15427v2.pdf}
}
@article{2312.15883v2,
Author        = {Xinke Jiang and Ruizhe Zhang and Yongxin Xu and Rihong Qiu and Yue Fang and Zhiyuan Wang and Jinyi Tang and Hongxin Ding and Xu Chu and Junfeng Zhao and Yasha Wang},
Title         = {HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and
  Reliable Medical LLMs Responses},
Eprint        = {2312.15883v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this paper, we investigate the retrieval-augmented generation (RAG) based
on Knowledge Graphs (KGs) to improve the accuracy and reliability of Large
Language Models (LLMs). Recent approaches suffer from insufficient and
repetitive knowledge retrieval, tedious and time-consuming query parsing, and
monotonous knowledge utilization. To this end, we develop a Hypothesis
Knowledge Graph Enhanced (HyKGE) framework, which leverages LLMs' powerful
reasoning capacity to compensate for the incompleteness of user queries,
optimizes the interaction process with LLMs, and provides diverse retrieved
knowledge. Specifically, HyKGE explores the zero-shot capability and the rich
knowledge of LLMs with Hypothesis Outputs to extend feasible exploration
directions in the KGs, as well as the carefully curated prompt to enhance the
density and efficiency of LLMs' responses. Furthermore, we introduce the HO
Fragment Granularity-aware Rerank Module to filter out noise while ensuring the
balance between diversity and relevance in retrieved knowledge. Experiments on
two Chinese medical multiple-choice question datasets and one Chinese
open-domain medical Q&A dataset with two LLM turbos demonstrate the superiority
of HyKGE in terms of accuracy and explainability.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.15883v2},
File          = {2312.15883v2.pdf}
}
@article{2407.02659v2,
Author        = {Devam Mondal and Carlo Lipizzi},
Title         = {LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model
  Training Data Through Knowledge Graph Comparison},
Eprint        = {2407.02659v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In light of recent legal allegations brought by publishers, newspapers, and
other creators of copyrighted corpora against large language model developers
who use their copyrighted materials for training or fine-tuning purposes, we
propose a novel system, a variant of a plagiarism detection system, that
assesses whether a knowledge source has been used in the training or
fine-tuning of a large language model. Unlike current methods, we utilize an
approach that uses Resource Description Framework (RDF) triples to create
knowledge graphs from both a source document and an LLM continuation of that
document. These graphs are then analyzed with respect to content using cosine
similarity and with respect to structure using a normalized version of graph
edit distance that shows the degree of isomorphism. Unlike traditional
plagiarism systems that focus on content matching and keyword identification
between a source and a target corpus, our approach enables a broader and more
accurate evaluation of similarity between a source document and LLM
continuation by focusing on relationships between ideas and their organization
with regards to others. Additionally, our approach does not require access to
LLM metrics like perplexity that may be unavailable in closed large language
model "black-box" systems, as well as the training corpus. We thus assess
whether an LLM has "plagiarized" a corpus in its continuation through
similarity measures. A prototype of our system will be found on a hyperlinked
GitHub repository.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.02659v2},
File          = {2407.02659v2.pdf}
}
@article{2311.09366v1,
Author        = {Jamie McCusker},
Title         = {LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph
  Construction},
Eprint        = {2311.09366v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {While the potential of Open Information Extraction (Open IE) for Knowledge
Graph Construction (KGC) may seem promising, we find that the alignment of Open
IE extraction results with existing knowledge graphs to be inadequate. The
advent of Large Language Models (LLMs), especially the commercially available
OpenAI models, have reset expectations for what is possible with deep learning
models and have created a new field called prompt engineering. We investigate
the use of GPT models and prompt engineering for knowledge graph construction
with the Wikidata knowledge graph to address a similar problem to Open IE,
which we call Open Knowledge Extraction (OKE) using an approach we call the
Linked Open Knowledge Extractor (LOKE, pronounced like "Loki"). We consider the
entity linking task essential to construction of real world knowledge graphs.
We merge the CaRB benchmark scoring approach with data from the TekGen dataset
for the LOKE task. We then show that a well engineered prompt, paired with a
naive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's
OpenIE 4 implementation on the OKE task, although it over-generates triples
compared to the reference set due to overall triple scarcity in the TekGen set.
Through an analysis of entity linkability in the CaRB dataset, as well as
outputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the "silver"
TekGen triples show that the task is significantly different in content from
OIE, if not structure. Through this analysis and a qualitative analysis of
sentence extractions via all methods, we found that LOKE-GPT extractions are of
high utility for the KGC task and suitable for use in semi-automated extraction
settings.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.09366v1},
File          = {2311.09366v1.pdf}
}
@article{2402.06861v2,
Author        = {Yansong Ning and Hao Liu},
Title         = {UrbanKGent: A Unified Large Language Model Agent Framework for Urban
  Knowledge Graph Construction},
Eprint        = {2402.06861v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Urban knowledge graph has recently worked as an emerging building block to
distill critical knowledge from multi-sourced urban data for diverse urban
application scenarios. Despite its promising benefits, urban knowledge graph
construction (UrbanKGC) still heavily relies on manual effort, hindering its
potential advancement. This paper presents UrbanKGent, a unified large language
model agent framework, for urban knowledge graph construction. Specifically, we
first construct the knowledgeable instruction set for UrbanKGC tasks (such as
relational triplet extraction and knowledge graph completion) via
heterogeneity-aware and geospatial-infused instruction generation. Moreover, we
propose a tool-augmented iterative trajectory refinement module to enhance and
refine the trajectories distilled from GPT-4. Through hybrid instruction
fine-tuning with augmented trajectories on Llama 2 and Llama 3 family, we
obtain UrbanKGC agent family, consisting of UrbanKGent-7/8/13B version. We
perform a comprehensive evaluation on two real-world datasets using both human
and GPT-4 self-evaluation. The experimental results demonstrate that UrbanKGent
family can not only significantly outperform 31 baselines in UrbanKGC tasks,
but also surpass the state-of-the-art LLM, GPT-4, by more than 10% with
approximately 20 times lower cost. Compared with the existing benchmark, the
UrbanKGent family could help construct an UrbanKG with hundreds of times richer
relationships using only one-fifth of the data. Our data and code are available
at https://github.com/usail-hkust/UrbanKGent.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.06861v2},
File          = {2402.06861v2.pdf}
}
@article{2408.13521v1,
Author        = {Azmine Toushik Wasi},
Title         = {HRGraph: Leveraging LLMs for HR Data Knowledge Graphs with Information
  Propagation-based Job Recommendation},
Eprint        = {2408.13521v1},
DOI           = {10.18653/v1/2024.kallm-1.6},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graphs (KGs) serving as semantic networks, prove highly effective
in managing complex interconnected data in different domains, by offering a
unified, contextualized, and structured representation with flexibility that
allows for easy adaptation to evolving knowledge. Processing complex Human
Resources (HR) data, KGs can help in different HR functions like recruitment,
job matching, identifying learning gaps, and enhancing employee retention.
Despite their potential, limited efforts have been made to implement practical
HR knowledge graphs. This study addresses this gap by presenting a framework
for effectively developing HR knowledge graphs from documents using Large
Language Models. The resulting KG can be used for a variety of downstream
tasks, including job matching, identifying employee skill gaps, and many more.
In this work, we showcase instances where HR KGs prove instrumental in precise
job matching, yielding advantages for both employers and employees. Empirical
evidence from experiments with information propagation in KGs and Graph Neural
Nets, along with case studies underscores the effectiveness of KGs in tasks
such as job and employee recommendations and job area classification. Code and
data are available at : https://github.com/azminewasi/HRGraph},
Year          = {2024},
Month         = {Aug},
Note          = {Proceedings of the 1st Workshop on Knowledge Graphs and Large
  Language Models (KaLLM 2024), Association for Computational Linguistics 2024},
Url           = {http://arxiv.org/abs/2408.13521v1},
File          = {2408.13521v1.pdf}
}
@article{2310.06671v2,
Author        = {Yichi Zhang and Zhuo Chen and Lingbing Guo and Yajing Xu and Wen Zhang and Huajun Chen},
Title         = {Making Large Language Models Perform Better in Knowledge Graph
  Completion},
Eprint        = {2310.06671v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language model (LLM) based knowledge graph completion (KGC) aims to
predict the missing triples in the KGs with LLMs. However, research about
LLM-based KGC fails to sufficiently harness LLMs' inference proficiencies,
overlooking critical structural information integral to KGs. In this paper, we
explore methods to incorporate structural information into the LLMs, with the
overarching goal of facilitating structure-aware reasoning. We first discuss on
the existing LLM paradigms like in-context learning and instruction tuning,
proposing basic structural information injection approaches. Then we propose a
Knowledge Prefix Adapter (KoPA) to fulfill this stated goal. The KoPA uses a
structural pre-training phase to comprehend the intricate entities and
relations within KGs, representing them as structural embeddings. Then KoPA
communicates such cross-modal structural information understanding to the LLMs
through a knowledge prefix adapter which projects the structural embeddings
into the textual space and obtains virtual knowledge tokens positioned as a
prefix of the input prompt. We conduct comprehensive experiments and provide
incisive analysis concerning how the introduction of cross-modal structural
information would be better for LLM's factual knowledge reasoning ability. Our
code and data are available at https://github.com/zjukg/KoPA .},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.06671v2},
File          = {2310.06671v2.pdf}
}
@article{2410.08475v2,
Author        = {Jiashu He and Mingyu Derek Ma and Jinxuan Fan and Dan Roth and Wei Wang and Alejandro Ribeiro},
Title         = {GIVE: Structured Reasoning of Large Language Models with Knowledge Graph
  Inspired Veracity Extrapolation},
Eprint        = {2410.08475v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Existing approaches based on context prompting or reinforcement learning (RL)
to improve the reasoning capacities of large language models (LLMs) depend on
the LLMs' internal knowledge to produce reliable Chain-Of-Thought (CoT).
However, no matter the size of LLMs, certain problems cannot be resolved in a
single forward pass. Meanwhile, agent-based reasoning systems require access to
a comprehensive nonparametric knowledge base, which is often costly or not
feasible for use in scientific and niche domains. We present Graph Inspired
Veracity Extrapolation (GIVE), a novel reasoning method that merges parametric
and non-parametric memories to improve accurate reasoning with minimal external
input. GIVE guides the LLM agent to select the most pertinent expert data
(observe), engage in query-specific divergent thinking (reflect), and then
synthesize this information to produce the final output (speak). Extensive
experiments demonstrated the following benefits of our framework: (1) GIVE
boosts the performance of LLMs across various sizes. (2) In some scenarios,
GIVE allows smaller LLMs to surpass larger, more sophisticated ones in
scientific tasks (GPT3.5T + GIVE > GPT4). (3) GIVE is effective on scientific
and open-domain assessments. (4) GIVE is a training-free method that enables
LLMs to tackle new problems that extend beyond their training data (up to 43.5%
-> 88.2%} accuracy improvement). (5) GIVE allows LLM agents to reason using
both restricted (very small) and noisy (very large) knowledge sources,
accommodating knowledge graphs (KG) ranging from 135 to more than 840k nodes.
(6) The reasoning process involved in GIVE is fully interpretable.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.08475v2},
File          = {2410.08475v2.pdf}
}
@article{2312.03022v3,
Author        = {Hongbin Ye and Honghao Gui and Aijia Zhang and Tong Liu and Weiqiang Jia},
Title         = {Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph
  Construction},
Eprint        = {2312.03022v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper introduces CooperKGC, a novel framework challenging the
conventional solitary approach of large language models (LLMs) in knowledge
graph construction (KGC). CooperKGC establishes a collaborative processing
network, assembling a team capable of concurrently addressing entity, relation,
and event extraction tasks. Experimentation demonstrates that fostering
collaboration within CooperKGC enhances knowledge selection, correction, and
aggregation capabilities across multiple rounds of interactions.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.03022v3},
File          = {2312.03022v3.pdf}
}
@article{2412.15443v1,
Author        = {Aakash Mahalingam and Vinesh Kumar Gande and Aman Chadha and Vinija Jain and Divya Chaudhary},
Title         = {SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic
  Retrieval},
Eprint        = {2412.15443v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Retrieval-Augmented Generation (RAG) systems have become pivotal in
leveraging vast corpora to generate informed and contextually relevant
responses, notably reducing hallucinations in Large Language Models. Despite
significant advancements, these systems struggle to efficiently process and
retrieve information from large datasets while maintaining a comprehensive
understanding of the context. This paper introduces SKETCH, a novel methodology
that enhances the RAG retrieval process by integrating semantic text retrieval
with knowledge graphs, thereby merging structured and unstructured data for a
more holistic comprehension. SKETCH, demonstrates substantial improvements in
retrieval performance and maintains superior context integrity compared to
traditional methods. Evaluated across four diverse datasets: QuALITY, QASPER,
NarrativeQA, and Italian Cuisine-SKETCH consistently outperforms baseline
approaches on key RAGAS metrics such as answer_relevancy, faithfulness,
context_precision and context_recall. Notably, on the Italian Cuisine dataset,
SKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99,
representing the highest performance across all evaluated metrics. These
results highlight SKETCH's capability in delivering more accurate and
contextually relevant responses, setting new benchmarks for future retrieval
systems.},
Year          = {2024},
Month         = {Dec},
Note          = {Workshop on Generative AI and Knowledge Graphs (GenAIK) at The
  31st International Conference on Computational Linguistics (COLING 2025)},
Url           = {http://arxiv.org/abs/2412.15443v1},
File          = {2412.15443v1.pdf}
}
@article{2309.01538v3,
Author        = {Linhao Luo and Jiaxin Ju and Bo Xiong and Yuan-Fang Li and Gholamreza Haffari and Shirui Pan},
Title         = {ChatRule: Mining Logical Rules with Large Language Models for Knowledge
  Graph Reasoning},
Eprint        = {2309.01538v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Logical rules are essential for uncovering the logical connections between
relations, which could improve reasoning performance and provide interpretable
results on knowledge graphs (KGs). Although there have been many efforts to
mine meaningful logical rules over KGs, existing methods suffer from
computationally intensive searches over the rule space and a lack of
scalability for large-scale KGs. Besides, they often ignore the semantics of
relations which is crucial for uncovering logical connections. Recently, large
language models (LLMs) have shown impressive performance in the field of
natural language processing and various applications, owing to their emergent
ability and generalizability. In this paper, we propose a novel framework,
ChatRule, unleashing the power of large language models for mining logical
rules over knowledge graphs. Specifically, the framework is initiated with an
LLM-based rule generator, leveraging both the semantic and structural
information of KGs to prompt LLMs to generate logical rules. To refine the
generated rules, a rule ranking module estimates the rule quality by
incorporating facts from existing KGs. Last, the ranked rules can be used to
conduct reasoning over KGs. ChatRule is evaluated on four large-scale KGs,
w.r.t. different rule quality metrics and downstream tasks, showing the
effectiveness and scalability of our method.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.01538v3},
File          = {2309.01538v3.pdf}
}
@article{2411.02435v1,
Author        = {Xinyi Leng and Jason Liang and Jack Mauro and Xu Wang and Andrea L. Bertozzi and James Chapman and Junyuan Lin and Bohan Chen and Chenchen Ye and Temple Daniel and P. Jeffrey Brantingham},
Title         = {Narrative Analysis of True Crime Podcasts With Knowledge Graph-Augmented
  Large Language Models},
Eprint        = {2411.02435v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Narrative data spans all disciplines and provides a coherent model of the
world to the reader or viewer. Recent advancement in machine learning and Large
Language Models (LLMs) have enable great strides in analyzing natural language.
However, Large language models (LLMs) still struggle with complex narrative
arcs as well as narratives containing conflicting information. Recent work
indicates LLMs augmented with external knowledge bases can improve the accuracy
and interpretability of the resulting models. In this work, we analyze the
effectiveness of applying knowledge graphs (KGs) in understanding true-crime
podcast data from both classical Natural Language Processing (NLP) and LLM
approaches. We directly compare KG-augmented LLMs (KGLLMs) with classical
methods for KG construction, topic modeling, and sentiment analysis.
Additionally, the KGLLM allows us to query the knowledge base in natural
language and test its ability to factually answer questions. We examine the
robustness of the model to adversarial prompting in order to test the model's
ability to deal with conflicting information. Finally, we apply classical
methods to understand more subtle aspects of the text such as the use of
hearsay and sentiment in narrative construction and propose future directions.
Our results indicate that KGLLMs outperform LLMs on a variety of metrics, are
more robust to adversarial prompts, and are more capable of summarizing the
text into topics.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.02435v1},
File          = {2411.02435v1.pdf}
}
@article{2309.14770v2,
Author        = {Haotian Li and Bin Yu and Yuliang Wei and Kai Wang and Richard Yi Da Xu and Bailing Wang},
Title         = {KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with
  Inverse Transformation},
Eprint        = {2309.14770v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graph completion (KGC) revolves around populating missing triples
in a knowledge graph using available information. Text-based methods, which
depend on textual descriptions of triples, often encounter difficulties when
these descriptions lack sufficient information for accurate prediction-an issue
inherent to the datasets and not easily resolved through modeling alone. To
address this and ensure data consistency, we first use large language models
(LLMs) to generate coherent descriptions, bridging the semantic gap between
queries and answers. Secondly, we utilize inverse relations to create a
symmetric graph, thereby providing augmented training samples for KGC.
Additionally, we employ the label information inherent in knowledge graphs
(KGs) to enhance the existing contrastive framework, making it fully
supervised. These efforts have led to significant performance improvements on
the WN18RR and FB15k-237 datasets. According to standard evaluation metrics,
our approach achieves a 4.2% improvement in Hit@1 on WN18RR and a 3.4%
improvement in Hit@3 on FB15k-237, demonstrating superior performance.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.14770v2},
File          = {2309.14770v2.pdf}
}
@article{2310.04835v2,
Author        = {Xuhui Jiang and Chengjin Xu and Yinghan Shen and Xun Sun and Lumingyuan Tang and Saizhuo Wang and Zhongwu Chen and Yuanzhuo Wang and Jian Guo},
Title         = {On the Evolution of Knowledge Graphs: A Survey and Perspective},
Eprint        = {2310.04835v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs (KGs) are structured representations of diversified
knowledge. They are widely used in various intelligent applications. In this
article, we provide a comprehensive survey on the evolution of various types of
knowledge graphs (i.e., static KGs, dynamic KGs, temporal KGs, and event KGs)
and techniques for knowledge extraction and reasoning. Furthermore, we
introduce the practical applications of different types of KGs, including a
case study in financial analysis. Finally, we propose our perspective on the
future directions of knowledge engineering, including the potential of
combining the power of knowledge graphs and large language models (LLMs), and
the evolution of knowledge extraction, reasoning, and representation.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.04835v2},
File          = {2310.04835v2.pdf}
}
@article{2405.03734v1,
Author        = {Silan Hu and Xiaoning Wang},
Title         = {FOKE: A Personalized and Explainable Education Framework Integrating
  Foundation Models, Knowledge Graphs, and Prompt Engineering},
Eprint        = {2405.03734v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {Integrating large language models (LLMs) and knowledge graphs (KGs) holds
great promise for revolutionizing intelligent education, but challenges remain
in achieving personalization, interactivity, and explainability. We propose
FOKE, a Forest Of Knowledge and Education framework that synergizes foundation
models, knowledge graphs, and prompt engineering to address these challenges.
FOKE introduces three key innovations: (1) a hierarchical knowledge forest for
structured domain knowledge representation; (2) a multi-dimensional user
profiling mechanism for comprehensive learner modeling; and (3) an interactive
prompt engineering scheme for generating precise and tailored learning
guidance.
  We showcase FOKE's application in programming education, homework assessment,
and learning path planning, demonstrating its effectiveness and practicality.
Additionally, we implement Scholar Hero, a real-world instantiation of FOKE.
Our research highlights the potential of integrating foundation models,
knowledge graphs, and prompt engineering to revolutionize intelligent education
practices, ultimately benefiting learners worldwide. FOKE provides a principled
and unified approach to harnessing cutting-edge AI technologies for
personalized, interactive, and explainable educational services, paving the way
for further research and development in this critical direction.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.03734v1},
File          = {2405.03734v1.pdf}
}
@article{2501.16393v1,
Author        = {Lili Zhang and Quanyan Zhu and Herman Ray and Ying Xie},
Title         = {Improving Network Threat Detection by Knowledge Graph, Large Language
  Model, and Imbalanced Learning},
Eprint        = {2501.16393v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Network threat detection has been challenging due to the complexities of
attack activities and the limitation of historical threat data to learn from.
To help enhance the existing practices of using analytics, machine learning,
and artificial intelligence methods to detect the network threats, we propose
an integrated modelling framework, where Knowledge Graph is used to analyze the
users' activity patterns, Imbalanced Learning techniques are used to prune and
weigh Knowledge Graph, and LLM is used to retrieve and interpret the users'
activities from Knowledge Graph. The proposed framework is applied to Agile
Threat Detection through Online Sequential Learning. The preliminary results
show the improved threat capture rate by 3%-4% and the increased
interpretabilities of risk predictions based on the users' activities.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.16393v1},
File          = {2501.16393v1.pdf}
}
@article{2404.04108v2,
Author        = {Giovanni Ciatto and Andrea Agiollo and Matteo Magnini and Andrea Omicini},
Title         = {Large language models as oracles for instantiating ontologies with
  domain-specific knowledge},
Eprint        = {2404.04108v2},
DOI           = {10.1016/j.knosys.2024.112940},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Background. Endowing intelligent systems with semantic data commonly requires
designing and instantiating ontologies with domain-specific knowledge.
Especially in the early phases, those activities are typically performed
manually by human experts possibly leveraging on their own experience. The
resulting process is therefore time-consuming, error-prone, and often biased by
the personal background of the ontology designer. Objective. To mitigate that
issue, we propose a novel domain-independent approach to automatically
instantiate ontologies with domain-specific knowledge, by leveraging on large
language models (LLMs) as oracles. Method. Starting from (i) an initial schema
composed by inter-related classes and properties and (ii) a set of query
templates, our method queries the LLM multiple times, and generates instances
for both classes and properties from its replies. Thus, the ontology is
automatically filled with domain-specific knowledge, compliant to the initial
schema. As a result, the ontology is quickly and automatically enriched with
manifold instances, which experts may consider to keep, adjust, discard, or
complement according to their own needs and expertise. Contribution. We
formalise our method in general way and instantiate it over various LLMs, as
well as on a concrete case study. We report experiments rooted in the
nutritional domain where an ontology of food meals and their ingredients is
automatically instantiated from scratch, starting from a categorisation of
meals and their relationships. There, we analyse the quality of the generated
ontologies and compare ontologies attained by exploiting different LLMs.
Experimentally, our approach achieves a quality metric that is up to five times
higher than the state-of-the-art, while reducing erroneous entities and
relations by up to ten times. Finally, we provide a SWOT analysis of the
proposed method.},
Year          = {2024},
Month         = {Apr},
Note          = {Knowledge-Based Systems 310 (2025) 112940},
Url           = {http://arxiv.org/abs/2404.04108v2},
File          = {2404.04108v2.pdf}
}
@article{2311.15781v1,
Author        = {Simone Conia and Min Li and Daniel Lee and Umar Farooq Minhas and Ihab Ilyas and Yunyao Li},
Title         = {Increasing Coverage and Precision of Textual Information in Multilingual
  Knowledge Graphs},
Eprint        = {2311.15781v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Recent work in Natural Language Processing and Computer Vision has been using
textual information -- e.g., entity names and descriptions -- available in
knowledge graphs to ground neural models to high-quality structured data.
However, when it comes to non-English languages, the quantity and quality of
textual information are comparatively scarce. To address this issue, we
introduce the novel task of automatic Knowledge Graph Enhancement (KGE) and
perform a thorough investigation on bridging the gap in both the quantity and
quality of textual information between English and non-English languages. More
specifically, we: i) bring to light the problem of increasing multilingual
coverage and precision of entity names and descriptions in Wikidata; ii)
demonstrate that state-of-the-art methods, namely, Machine Translation (MT),
Web Search (WS), and Large Language Models (LLMs), struggle with this task;
iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and
LLMs to generate high-quality textual information; and, iv) study the impact of
increasing multilingual coverage and precision of non-English textual
information in Entity Linking, Knowledge Graph Completion, and Question
Answering. As part of our effort towards better multilingual knowledge graphs,
we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE
approaches in 10 languages across 7 language families.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.15781v1},
File          = {2311.15781v1.pdf}
}
@article{2307.16648v2,
Author        = {Hamed Babaei Giglou and Jennifer D'Souza and Sören Auer},
Title         = {LLMs4OL: Large Language Models for Ontology Learning},
Eprint        = {2307.16648v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs)
for Ontology Learning (OL). LLMs have shown significant advancements in natural
language processing, demonstrating their ability to capture complex language
patterns in different knowledge domains. Our LLMs4OL paradigm investigates the
following hypothesis: \textit{Can LLMs effectively apply their language pattern
capturing capability to OL, which involves automatically extracting and
structuring knowledge from natural language text?} To test this hypothesis, we
conduct a comprehensive evaluation using the zero-shot prompting method. We
evaluate nine different LLM model families for three main OL tasks: term
typing, taxonomy discovery, and extraction of non-taxonomic relations.
Additionally, the evaluations encompass diverse genres of ontological
knowledge, including lexicosemantic knowledge in WordNet, geographical
knowledge in GeoNames, and medical knowledge in UMLS.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.16648v2},
File          = {2307.16648v2.pdf}
}
@article{2311.08412v1,
Author        = {Felix Ocker and Jörg Deigmöller and Julian Eggert},
Title         = {Exploring Large Language Models as a Source of Common-Sense Knowledge
  for Robots},
Eprint        = {2311.08412v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Service robots need common-sense knowledge to help humans in everyday
situations as it enables them to understand the context of their actions.
However, approaches that use ontologies face a challenge because common-sense
knowledge is often implicit, i.e., it is obvious to humans but not explicitly
stated. This paper investigates if Large Language Models (LLMs) can fill this
gap. Our experiments reveal limited effectiveness in the selective extraction
of contextual action knowledge, suggesting that LLMs may not be sufficient on
their own. However, the large-scale extraction of general, actionable knowledge
shows potential, indicating that LLMs can be a suitable tool for efficiently
creating ontologies for robots. This paper shows that the technique used for
knowledge extraction can be applied to populate a minimalist ontology,
showcasing the potential of LLMs in synergy with formal knowledge
representation.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2311.08412v1},
File          = {2311.08412v1.pdf}
}
@article{2305.01157v3,
Author        = {Nurendra Choudhary and Chandan K. Reddy},
Title         = {Complex Logical Reasoning over Knowledge Graphs using Large Language
  Models},
Eprint        = {2305.01157v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LO},
Abstract      = {Reasoning over knowledge graphs (KGs) is a challenging task that requires a
deep understanding of the complex relationships between entities and the
underlying logic of their relations. Current approaches rely on learning
geometries to embed entities in vector space for logical query operations, but
they suffer from subpar performance on complex queries and dataset-specific
representations. In this paper, we propose a novel decoupled approach,
Language-guided Abstract Reasoning over Knowledge graphs (LARK), that
formulates complex KG reasoning as a combination of contextual KG search and
logical query reasoning, to leverage the strengths of graph extraction
algorithms and large language models (LLM), respectively. Our experiments
demonstrate that the proposed approach outperforms state-of-the-art KG
reasoning methods on standard benchmark datasets across several logical query
constructs, with significant performance gain for queries of higher complexity.
Furthermore, we show that the performance of our approach improves
proportionally to the increase in size of the underlying LLM, enabling the
integration of the latest advancements in LLMs for logical reasoning over KGs.
Our work presents a new direction for addressing the challenges of complex KG
reasoning and paves the way for future research in this area.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.01157v3},
File          = {2305.01157v3.pdf}
}
@article{2410.13987v1,
Author        = {Jiatan Huang and Mingchen Li and Zonghai Yao and Zhichao Yang and Yongkang Xiao and Feiyun Ouyang and Xiaohan Li and Shuo Han and Hong Yu},
Title         = {RiTeK: A Dataset for Large Language Models Complex Reasoning over
  Textual Knowledge Graphs},
Eprint        = {2410.13987v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Answering complex real-world questions often requires accurate retrieval from
textual knowledge graphs (TKGs). The scarcity of annotated data, along with
intricate topological structures, makes this task particularly challenging. As
the nature of relational path information could enhance the inference ability
of Large Language Models (LLMs), efficiently retrieving more complex relational
path information from TKGs presents another key challenge. To tackle these
challenges, we first develop a Dataset for LLMs Complex Reasoning over Textual
Knowledge Graphs (RiTeK) with a broad topological structure coverage.We
synthesize realistic user queries that integrate diverse topological
structures, relational information, and complex textual descriptions. We
conduct rigorous expert evaluation to validate the quality of our synthesized
queries. And then, we introduce an enhanced Monte Carlo Tree Search (MCTS)
method, Relational MCTS, to automatically extract relational path information
from textual graphs for specific queries. Our dataset mainly covers the medical
domain as the relation types and entity are complex and publicly available.
Experimental results indicate that RiTeK poses significant challenges for
current retrieval and LLM systems, while the proposed Relational MCTS method
enhances LLM inference ability and achieves state-of-the-art performance on
RiTeK.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.13987v1},
File          = {2410.13987v1.pdf}
}
@article{2407.13598v1,
Author        = {Youfu Yan and Yu Hou and Yongkang Xiao and Rui Zhang and Qianwen Wang},
Title         = {KNOWNET: Guided Health Information Seeking from LLMs via Knowledge Graph
  Integration},
Eprint        = {2407.13598v1},
DOI           = {10.1109/TVCG.2024.3456364},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {The increasing reliance on Large Language Models (LLMs) for health
information seeking can pose severe risks due to the potential for
misinformation and the complexity of these topics. This paper introduces
KNOWNET a visualization system that integrates LLMs with Knowledge Graphs (KG)
to provide enhanced accuracy and structured exploration. Specifically, for
enhanced accuracy, KNOWNET extracts triples (e.g., entities and their
relations) from LLM outputs and maps them into the validated information and
supported evidence in external KGs. For structured exploration, KNOWNET
provides next-step recommendations based on the neighborhood of the currently
explored entities in KGs, aiming to guide a comprehensive understanding without
overlooking critical aspects. To enable reasoning with both the structured data
in KGs and the unstructured outputs from LLMs, KNOWNET conceptualizes the
understanding of a subject as the gradual construction of graph visualization.
A progressive graph visualization is introduced to monitor past inquiries, and
bridge the current query with the exploration history and next-step
recommendations. We demonstrate the effectiveness of our system via use cases
and expert interviews.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.13598v1},
File          = {2407.13598v1.pdf}
}
@article{2501.00223v1,
Author        = {Michael Gubanov and Anna Pyayt and Aleksandra Karolak},
Title         = {CancerKG.ORG A Web-scale, Interactive, Verifiable Knowledge Graph-LLM
  Hybrid for Assisting with Optimal Cancer Treatment and Care},
Eprint        = {2501.00223v1},
DOI           = {10.1145/3627673.3680094},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Here, we describe one of the first Web-scale hybrid Knowledge Graph
(KG)-Large Language Model (LLM), populated with the latest peer-reviewed
medical knowledge on colorectal Cancer. It is currently being evaluated to
assist with both medical research and clinical information retrieval tasks at
Moffitt Cancer Center, which is one of the top Cancer centers in the U.S. and
in the world. Our hybrid is remarkable as it serves the user needs better than
just an LLM, KG or a search-engine in isolation. LLMs as is are known to
exhibit hallucinations and catastrophic forgetting as well as are trained on
outdated corpora. The state of the art KGs, such as PrimeKG, cBioPortal,
ChEMBL, NCBI, and other require manual curation, hence are quickly getting
stale. CancerKG is unsupervised and is capable of automatically ingesting and
organizing the latest medical findings. To alleviate the LLMs shortcomings, the
verified KG serves as a Retrieval Augmented Generation (RAG) guardrail.
CancerKG exhibits 5 different advanced user interfaces, each tailored to serve
different data modalities better and more convenient for the user.},
Year          = {2024},
Month         = {Dec},
Note          = {CIKM 2024, pp. 4497 - 4505},
Url           = {http://arxiv.org/abs/2501.00223v1},
File          = {2501.00223v1.pdf}
}
@article{2305.13168v4,
Author        = {Yuqi Zhu and Xiaohan Wang and Jing Chen and Shuofei Qiao and Yixin Ou and Yunzhi Yao and Shumin Deng and Huajun Chen and Ningyu Zhang},
Title         = {LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities
  and Future Opportunities},
Eprint        = {2305.13168v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper presents an exhaustive quantitative and qualitative evaluation of
Large Language Models (LLMs) for Knowledge Graph (KG) construction and
reasoning. We engage in experiments across eight diverse datasets, focusing on
four representative tasks encompassing entity and relation extraction, event
extraction, link prediction, and question-answering, thereby thoroughly
exploring LLMs' performance in the domain of construction and inference.
Empirically, our findings suggest that LLMs, represented by GPT-4, are more
suited as inference assistants rather than few-shot information extractors.
Specifically, while GPT-4 exhibits good performance in tasks related to KG
construction, it excels further in reasoning tasks, surpassing fine-tuned
models in certain cases. Moreover, our investigation extends to the potential
generalization ability of LLMs for information extraction, leading to the
proposition of a Virtual Knowledge Extraction task and the development of the
corresponding VINE dataset. Based on these empirical findings, we further
propose AutoKG, a multi-agent-based approach employing LLMs and external
sources for KG construction and reasoning. We anticipate that this research can
provide invaluable insights for future undertakings in the field of knowledge
graphs. The code and datasets are in https://github.com/zjunlp/AutoKG.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.13168v4},
File          = {2305.13168v4.pdf}
}
@article{2308.11730v3,
Author        = {Yu Wang and Nedim Lipka and Ryan A. Rossi and Alexa Siu and Ruiyi Zhang and Tyler Derr},
Title         = {Knowledge Graph Prompting for Multi-Document Question Answering},
Eprint        = {2308.11730v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The `pre-train, prompt, predict' paradigm of large language models (LLMs) has
achieved remarkable success in open-domain question answering (OD-QA). However,
few works explore this paradigm in the scenario of multi-document question
answering (MD-QA), a task demanding a thorough understanding of the logical
associations among the contents and structures of different documents. To fill
this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to
formulate the right context in prompting LLMs for MD-QA, which consists of a
graph construction module and a graph traversal module. For graph construction,
we create a knowledge graph (KG) over multiple documents with nodes symbolizing
passages or document structures (e.g., pages/tables), and edges denoting the
semantic/lexical similarity between passages or intra-document structural
relations. For graph traversal, we design an LLM-based graph traversal agent
that navigates across nodes and gathers supporting passages assisting LLMs in
MD-QA. The constructed graph serves as the global ruler that regulates the
transitional space among passages and reduces retrieval latency. Concurrently,
the graph traversal agent acts as a local navigator that gathers pertinent
context to progressively approach the question and guarantee retrieval quality.
Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the
potential of leveraging graphs in enhancing the prompt design for LLMs. Our
code: https://github.com/YuWVandy/KG-LLM-MDQA.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.11730v3},
File          = {2308.11730v3.pdf}
}
@article{2411.17388v1,
Author        = {Haoyu Huang and Chong Chen and Conghui He and Yang Li and Jiawei Jiang and Wentao Zhang},
Title         = {Can LLMs be Good Graph Judger for Knowledge Graph Construction?},
Eprint        = {2411.17388v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In real-world scenarios, most of the data obtained from information retrieval
(IR) system is unstructured. Converting natural language sentences into
structured Knowledge Graphs (KGs) remains a critical challenge. The quality of
constructed KGs may also impact the performance of some KG-dependent domains
like GraphRAG systems and recommendation systems. Recently, Large Language
Models (LLMs) have demonstrated impressive capabilities in addressing a wide
range of natural language processing tasks. However, there are still challenges
when utilizing LLMs to address the task of generating structured KGs. And we
have identified three limitations with respect to existing KG construction
methods. (1)There is a large amount of information and excessive noise in
real-world documents, which could result in extracting messy information.
(2)Native LLMs struggle to effectively extract accuracy knowledge from some
domain-specific documents. (3)Hallucinations phenomenon cannot be overlooked
when utilizing LLMs directly as an unsupervised method for constructing KGs.
  In this paper, we propose GraphJudger, a knowledge graph construction
framework to address the aforementioned challenges. We introduce three
innovative modules in our method, which are entity-centric iterative text
denoising, knowledge aware instruction tuning and graph judgement,
respectively. We seek to utilize the capacity of LLMs to function as a graph
judger, a capability superior to their role only as a predictor for KG
construction problems. Experiments conducted on two general text-graph pair
datasets and one domain-specific text-graph pair dataset show superior
performances compared to baseline methods. The code of our proposed method is
available at https://github.com/hhy-huang/GraphJudger.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.17388v1},
File          = {2411.17388v1.pdf}
}
@article{2410.01401v1,
Author        = {Yu Zhang and Kehai Chen and Xuefeng Bai and zhao kang and Quanjiang Guo and Min Zhang},
Title         = {Question-guided Knowledge Graph Re-scoring and Injection for Knowledge
  Graph Question Answering},
Eprint        = {2410.01401v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graph question answering (KGQA) involves answering natural language
questions by leveraging structured information stored in a knowledge graph.
Typically, KGQA initially retrieve a targeted subgraph from a large-scale
knowledge graph, which serves as the basis for reasoning models to address
queries. However, the retrieved subgraph inevitably brings distraction
information for knowledge utilization, impeding the model's ability to perform
accurate reasoning. To address this issue, we propose a Question-guided
Knowledge Graph Re-scoring method (Q-KGR) to eliminate noisy pathways for the
input question, thereby focusing specifically on pertinent factual knowledge.
Moreover, we introduce Knowformer, a parameter-efficient method for injecting
the re-scored knowledge graph into large language models to enhance their
ability to perform factual reasoning. Extensive experiments on multiple KGQA
benchmarks demonstrate the superiority of our method over existing systems.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.01401v1},
File          = {2410.01401v1.pdf}
}
@article{2304.09048v2,
Author        = {Zhen Bi and Jing Chen and Yinuo Jiang and Feiyu Xiong and Wei Guo and Huajun Chen and Ningyu Zhang},
Title         = {CodeKGC: Code Language Model for Generative Knowledge Graph Construction},
Eprint        = {2304.09048v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Current generative knowledge graph construction approaches usually fail to
capture structural knowledge by simply flattening natural language into
serialized texts or a specification language. However, large generative
language model trained on structured data such as code has demonstrated
impressive capability in understanding natural language for structural
prediction and reasoning tasks. Intuitively, we address the task of generative
knowledge graph construction with code language model: given a code-format
natural language input, the target is to generate triples which can be
represented as code completion tasks. Specifically, we develop schema-aware
prompts that effectively utilize the semantic structure within the knowledge
graph. As code inherently possesses structure, such as class and function
definitions, it serves as a useful model for prior semantic structural
knowledge. Furthermore, we employ a rationale-enhanced generation method to
boost the performance. Rationales provide intermediate steps, thereby improving
knowledge extraction abilities. Experimental results indicate that the proposed
approach can obtain better performance on benchmark datasets compared with
baselines. Code and datasets are available in
https://github.com/zjunlp/DeepKE/tree/main/example/llm.},
Year          = {2023},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2304.09048v2},
File          = {2304.09048v2.pdf}
}
@article{2402.09911v1,
Author        = {Jiaxiang Liu and Tong Zhou and Yubo Chen and Kang Liu and Jun Zhao},
Title         = {Enhancing Large Language Models with Pseudo- and Multisource- Knowledge
  Graphs for Open-ended Question Answering},
Eprint        = {2402.09911v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Mitigating the hallucinations of Large Language Models (LLMs) and enhancing
them is a crucial task. Although some existing methods employ model
self-enhancement techniques, they fall short of effectively addressing unknown
factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails
to address the generalization across different KG sources and the enhancement
of open-ended answer questions simultaneously. To tackle these limitations,
there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge
Verification proposed. The enhancement of LLM using KG in an open-ended
question-answering setting is implemented by leveraging the Pseudo-Graph
Generation. Atomic Knowledge Verification utilizes atomic-level knowledge
querying and verification to achieve generalizability under different KG
sources. Compared to the baseline, this approach yields a minimum improvement
of 11.5 in the ROUGE-L score for open-ended questions. For precise questions,
we observe a minimum accuracy improvement of 7.5. Moreover, there is also
demonstration that this framework exhibits generalizability across different KG
sources. In summary, our results pave the way for enhancing LLMs by
incorporating Pseudo- and Multisource-KGs, particularly in the context of
open-ended questions.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.09911v1},
File          = {2402.09911v1.pdf}
}
@article{2405.06545v1,
Author        = {Mengjia Niu and Hao Li and Jie Shi and Hamed Haddadi and Fan Mo},
Title         = {Mitigating Hallucinations in Large Language Models via
  Self-Refinement-Enhanced Knowledge Retrieval},
Eprint        = {2405.06545v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have demonstrated remarkable capabilities across
various domains, although their susceptibility to hallucination poses
significant challenges for their deployment in critical areas such as
healthcare. To address this issue, retrieving relevant facts from knowledge
graphs (KGs) is considered a promising method. Existing KG-augmented approaches
tend to be resource-intensive, requiring multiple rounds of retrieval and
verification for each factoid, which impedes their application in real-world
scenarios.
  In this study, we propose Self-Refinement-Enhanced Knowledge Graph Retrieval
(Re-KGR) to augment the factuality of LLMs' responses with less retrieval
efforts in the medical field. Our approach leverages the attribution of
next-token predictive probability distributions across different tokens, and
various model layers to primarily identify tokens with a high potential for
hallucination, reducing verification rounds by refining knowledge triples
associated with these tokens. Moreover, we rectify inaccurate content using
retrieved knowledge in the post-processing stage, which improves the
truthfulness of generated responses. Experimental results on a medical dataset
demonstrate that our approach can enhance the factual capability of LLMs across
various foundational models as evidenced by the highest scores on truthfulness.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.06545v1},
File          = {2405.06545v1.pdf}
}
@article{2407.00653v1,
Author        = {Yifei Zhang and Xintao Wang and Jiaqing Liang and Sirui Xia and Lida Chen and Yanghua Xiao},
Title         = {Chain-of-Knowledge: Integrating Knowledge Reasoning into Large Language
  Models by Learning from Knowledge Graphs},
Eprint        = {2407.00653v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have exhibited impressive proficiency in various
natural language processing (NLP) tasks, which involve increasingly complex
reasoning. Knowledge reasoning, a primary type of reasoning, aims at deriving
new knowledge from existing one.While it has been widely studied in the context
of knowledge graphs (KGs), knowledge reasoning in LLMs remains underexplored.
In this paper, we introduce Chain-of-Knowledge, a comprehensive framework for
knowledge reasoning, including methodologies for both dataset construction and
model learning. For dataset construction, we create KnowReason via rule mining
on KGs. For model learning, we observe rule overfitting induced by naive
training. Hence, we enhance CoK with a trial-and-error mechanism that simulates
the human process of internal knowledge exploration. We conduct extensive
experiments with KnowReason. Our results show the effectiveness of CoK in
refining LLMs in not only knowledge reasoning, but also general reasoning
benchmarkms.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2407.00653v1},
File          = {2407.00653v1.pdf}
}
@article{2501.14300v1,
Author        = {Xujian Liang and Zhaoquan Gu},
Title         = {Fast Think-on-Graph: Wider, Deeper and Faster Reasoning of Large
  Language Model on Knowledge Graph},
Eprint        = {2501.14300v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Graph Retrieval Augmented Generation (GRAG) is a novel paradigm that takes
the naive RAG system a step further by integrating graph information, such as
knowledge graph (KGs), into large-scale language models (LLMs) to mitigate
hallucination. However, existing GRAG still encounter limitations: 1) simple
paradigms usually fail with the complex problems due to the narrow and shallow
correlations capture from KGs 2) methods of strong coupling with KGs tend to be
high computation cost and time consuming if the graph is dense. In this paper,
we propose the Fast Think-on-Graph (FastToG), an innovative paradigm for
enabling LLMs to think ``community by community" within KGs. To do this,
FastToG employs community detection for deeper correlation capture and two
stages community pruning - coarse and fine pruning for faster retrieval.
Furthermore, we also develop two Community-to-Text methods to convert the graph
structure of communities into textual form for better understanding by LLMs.
Experimental results demonstrate the effectiveness of FastToG, showcasing
higher accuracy, faster reasoning, and better explainability compared to the
previous works.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.14300v1},
File          = {2501.14300v1.pdf}
}
@article{2501.14892v1,
Author        = {Hang Luo and Jian Zhang and Chujun Li},
Title         = {Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in
  Graph-Augmented LLMs},
Eprint        = {2501.14892v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In knowledge-intensive tasks, especially in high-stakes domains like medicine
and law, it is critical not only to retrieve relevant information but also to
provide causal reasoning and explainability. Large language models (LLMs) have
achieved remarkable performance in natural language understanding and
generation tasks. However, they often suffer from limitations such as
difficulty in incorporating new knowledge, generating hallucinations, and
explaining their reasoning process. To address these challenges, integrating
knowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has
emerged as an effective solution. Traditional Graph RAG methods often rely on
simple graph traversal or semantic similarity, which do not capture causal
relationships or align well with the model's internal reasoning steps. This
paper proposes a novel pipeline that filters large knowledge graphs to
emphasize cause-effect edges, aligns the retrieval process with the model's
chain-of-thought (CoT), and enhances reasoning through multi-stage path
improvements. Experiments on medical question-answering tasks show consistent
gains, with up to a 10\% absolute improvement across multiple large language
models (LLMs). This approach demonstrates the value of combining causal
reasoning with stepwise retrieval, leading to more interpretable and logically
grounded solutions for complex queries.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.14892v1},
File          = {2501.14892v1.pdf}
}
@article{2501.18119v1,
Author        = {Qika Lin and Tianzhe Zhao and Kai He and Zhen Peng and Fangzhi Xu and Ling Huang and Jingying Ma and Mengling Feng},
Title         = {Self-supervised Quantized Representation for Seamlessly Integrating
  Knowledge Graphs with Large Language Models},
Eprint        = {2501.18119v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Due to the presence of the natural gap between Knowledge Graph (KG)
structures and the natural language, the effective integration of holistic
structural information of KGs with Large Language Models (LLMs) has emerged as
a significant question. To this end, we propose a two-stage framework to learn
and apply quantized codes for each entity, aiming for the seamless integration
of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR)
method is proposed to compress both KG structural and semantic knowledge into
discrete codes (\ie, tokens) that align the format of language sentences. We
further design KG instruction-following data by viewing these learned codes as
features to directly input to LLMs, thereby achieving seamless integration. The
experiment results demonstrate that SSQR outperforms existing unsupervised
quantized methods, producing more distinguishable codes. Further, the
fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link
prediction and triple classification tasks, utilizing only 16 tokens per entity
instead of thousands in conventional prompting methods.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.18119v1},
File          = {2501.18119v1.pdf}
}
@article{2410.06121v1,
Author        = {Wenyu Huang and Guancheng Zhou and Hongru Wang and Pavlos Vougiouklis and Mirella Lapata and Jeff Z. Pan},
Title         = {Less is More: Making Smaller Language Models Competent Subgraph
  Retrievers for Multi-hop KGQA},
Eprint        = {2410.06121v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Retrieval-Augmented Generation (RAG) is widely used to inject external
non-parametric knowledge into large language models (LLMs). Recent works
suggest that Knowledge Graphs (KGs) contain valuable external knowledge for
LLMs. Retrieving information from KGs differs from extracting it from document
sets. Most existing approaches seek to directly retrieve relevant subgraphs,
thereby eliminating the need for extensive SPARQL annotations, traditionally
required by semantic parsing methods. In this paper, we model the subgraph
retrieval task as a conditional generation task handled by small language
models. Specifically, we define a subgraph identifier as a sequence of
relations, each represented as a special token stored in the language models.
Our base generative subgraph retrieval model, consisting of only 220M
parameters, achieves competitive retrieval performance compared to
state-of-the-art models relying on 7B parameters, demonstrating that small
language models are capable of performing the subgraph retrieval task.
Furthermore, our largest 3B model, when plugged with an LLM reader, sets new
SOTA end-to-end performance on both the WebQSP and CWQ benchmarks. Our model
and data will be made available online: https://github.com/hwy9855/GSR.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.06121v1},
File          = {2410.06121v1.pdf}
}
@article{2412.05547v1,
Author        = {Weijie Chen and Ting Bai and Jinbo Su and Jian Luan and Wei Liu and Chuan Shi},
Title         = {KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large
  Language Models},
Eprint        = {2412.05547v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Large language models with retrieval-augmented generation encounter a pivotal
challenge in intricate retrieval tasks, e.g., multi-hop question answering,
which requires the model to navigate across multiple documents and generate
comprehensive responses based on fragmented information. To tackle this
challenge, we introduce a novel Knowledge Graph-based RAG framework with a
hierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing
in KG-Retriever is constructed on a hierarchical index graph that consists of a
knowledge graph layer and a collaborative document layer. The associative
nature of graph structures is fully utilized to strengthen intra-document and
inter-document connectivity, thereby fundamentally alleviating the information
fragmentation problem and meanwhile improving the retrieval efficiency in
cross-document retrieval of LLMs. With the coarse-grained collaborative
information from neighboring documents and concise information from the
knowledge graph, KG-Retriever achieves marked improvements on five public QA
datasets, showing the effectiveness and efficiency of our proposed RAG
framework.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.05547v1},
File          = {2412.05547v1.pdf}
}
@article{2312.11282v3,
Author        = {Yuxuan Huang},
Title         = {Evaluating and Enhancing Large Language Models for Conversational
  Reasoning on Knowledge Graphs},
Eprint        = {2312.11282v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The development of large language models (LLMs) has been catalyzed by
advancements in pre-training techniques. These models have demonstrated robust
reasoning capabilities through manually designed prompts. In this work, we
evaluate the conversational reasoning capabilities of the current
state-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the
performance of LLMs is constrained due to a lack of KG environment awareness
and the difficulties in developing effective optimization mechanisms for
intermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG
reasoning agent designed to deliver precise and adaptable predictions on KG
paths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate
state information within each reasoning step. We reframe the challenge of
multi-hop reasoning on the KG as a sequential decision-making task. Utilizing
the Proximal Policy Optimization (PPO) online policy gradient reinforcement
learning algorithm, our model is optimized to learn from rich reward signals.
Additionally, we conduct an evaluation of our model and GPT-4 on the OpenDialKG
dataset. The experimental results reveal that LLaMA-2-7B-ARK outperforms the
current state-of-the-art model by 5.28 percentage points, with a performance
rate of 36.39% on the target@1 evaluation metric. Meanwhile, GPT-4 scored
14.91%, further demonstrating the effectiveness of our method. Our code is
available on GitHub (https://github.com/Aipura/LLM-ARK) for further access.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.11282v3},
File          = {2312.11282v3.pdf}
}
@article{2405.06524v1,
Author        = {Wenyu Huang and Guancheng Zhou and Mirella Lapata and Pavlos Vougiouklis and Sebastien Montella and Jeff Z. Pan},
Title         = {Prompting Large Language Models with Knowledge Graphs for Question
  Answering Involving Long-tail Facts},
Eprint        = {2405.06524v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Although Large Language Models (LLMs) are effective in performing various NLP
tasks, they still struggle to handle tasks that require extensive, real-world
knowledge, especially when dealing with long-tail facts (facts related to
long-tail entities). This limitation highlights the need to supplement LLMs
with non-parametric knowledge. To address this issue, we analysed the effects
of different types of non-parametric knowledge, including textual passage and
knowledge graphs (KGs). Since LLMs have probably seen the majority of factual
question-answering datasets already, to facilitate our analysis, we proposed a
fully automatic pipeline for creating a benchmark that requires knowledge of
long-tail facts for answering the involved questions. Using this pipeline, we
introduce the LTGen benchmark. We evaluate state-of-the-art LLMs in different
knowledge settings using the proposed benchmark. Our experiments show that LLMs
alone struggle with answering these questions, especially when the long-tail
level is high or rich knowledge is required. Nonetheless, the performance of
the same models improved significantly when they were prompted with
non-parametric knowledge. We observed that, in most cases, prompting LLMs with
KG triples surpasses passage-based prompting using a state-of-the-art
retriever. In addition, while prompting LLMs with both KG triples and documents
does not consistently improve knowledge coverage, it can dramatically reduce
hallucinations in the generated content.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.06524v1},
File          = {2405.06524v1.pdf}
}
@article{2407.06564v1,
Author        = {Amanda Kau and Xuzeng He and Aishwarya Nambissan and Aland Astudillo and Hui Yin and Amir Aryani},
Title         = {Combining Knowledge Graphs and Large Language Models},
Eprint        = {2407.06564v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In recent years, Natural Language Processing (NLP) has played a significant
role in various Artificial Intelligence (AI) applications such as chatbots,
text generation, and language translation. The emergence of large language
models (LLMs) has greatly improved the performance of these applications,
showing astonishing results in language understanding and generation. However,
they still show some disadvantages, such as hallucinations and lack of
domain-specific knowledge, that affect their performance in real-world tasks.
These issues can be effectively mitigated by incorporating knowledge graphs
(KGs), which organise information in structured formats that capture
relationships between entities in a versatile and interpretable fashion.
Likewise, the construction and validation of KGs present challenges that LLMs
can help resolve. The complementary relationship between LLMs and KGs has led
to a trend that combines these technologies to achieve trustworthy results.
This work collected 28 papers outlining methods for KG-powered LLMs, LLM-based
KGs, and LLM-KG hybrid approaches. We systematically analysed and compared
these approaches to provide a comprehensive overview highlighting key trends,
innovative techniques, and common challenges. This synthesis will benefit
researchers new to the field and those seeking to deepen their understanding of
how KGs and LLMs can be effectively combined to enhance AI applications
capabilities.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.06564v1},
File          = {2407.06564v1.pdf}
}
@article{2407.12522v1,
Author        = {Xiaoyu Tan and Haoyu Wang and Xihe Qiu and Yuan Cheng and Yinghui Xu and Wei Chu and Yuan Qi},
Title         = {Struct-X: Enhancing Large Language Models Reasoning with Structured Data},
Eprint        = {2407.12522v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Structured data, rich in logical and relational information, has the
potential to enhance the reasoning abilities of large language models (LLMs).
Still, its integration poses a challenge due to the risk of overwhelming LLMs
with excessive tokens and irrelevant context information. To address this, we
propose Struct-X, a novel framework that operates through five key phases:
``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize
structured data. It begins by encoding structured data into a topological space
using graph embeddings, followed by filling in missing entity information with
knowledge retrieval modules, and filtering out irrelevant tokens via a
self-supervised module. The final phase involves constructing a topological
network with selected tokens to further reduce the total token length for more
effective LLM inference. Additionally, Struct-X includes an Auxiliary Module
trained to generate prompts, aiding LLMs in analyzing structured data.
Extensive experiments on benchmarks, including the knowledge graph
question-answer task and the long document reading comprehension task, show
that Struct-X notably improves LLM reasoning, demonstrating the effectiveness
of structured data augmentation in improving LLM inference with complex input
context.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.12522v1},
File          = {2407.12522v1.pdf}
}
@article{2411.08449v2,
Author        = {Siraj Munir and Alessandro Aldini},
Title         = {Towards Evaluating Large Language Models for Graph Query Generation},
Eprint        = {2411.08449v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.ET},
Abstract      = {Large Language Models (LLMs) are revolutionizing the landscape of Generative
Artificial Intelligence (GenAI), with innovative LLM-backed solutions emerging
rapidly. However, when applied to database technologies, specifically query
generation for graph databases and Knowledge Graphs (KGs), LLMs still face
significant challenges. While research on LLM-driven query generation for
Structured Query Language (SQL) exists, similar systems for graph databases
remain underdeveloped. This paper presents a comparative study addressing the
challenge of generating Cypher queries a powerful language for interacting with
graph databases using open-access LLMs. We rigorously evaluate several LLM
agents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a
locally deployed Llama 3.1 8B) using a designed few-shot learning prompt and
Retrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT)
reasoning. Our empirical analysis of query generation accuracy reveals that
Claude Sonnet 3.5 outperforms its counterparts in this specific domain.
Further, we highlight promising future research directions to address the
identified limitations and advance LLM-driven query generation for graph
databases.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.08449v2},
File          = {2411.08449v2.pdf}
}
@article{2401.16960v1,
Author        = {Linyao Yang and Hongyang Chen and Xiao Wang and Jing Yang and Fei-Yue Wang and Han Liu},
Title         = {Two Heads Are Better Than One: Integrating Knowledge from Knowledge
  Graphs and Large Language Models for Entity Alignment},
Eprint        = {2401.16960v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Entity alignment, which is a prerequisite for creating a more comprehensive
Knowledge Graph (KG), involves pinpointing equivalent entities across disparate
KGs. Contemporary methods for entity alignment have predominantly utilized
knowledge embedding models to procure entity embeddings that encapsulate
various similarities-structural, relational, and attributive. These embeddings
are then integrated through attention-based information fusion mechanisms.
Despite this progress, effectively harnessing multifaceted information remains
challenging due to inherent heterogeneity. Moreover, while Large Language
Models (LLMs) have exhibited exceptional performance across diverse downstream
tasks by implicitly capturing entity semantics, this implicit knowledge has yet
to be exploited for entity alignment. In this study, we propose a Large
Language Model-enhanced Entity Alignment framework (LLMEA), integrating
structural knowledge from KGs with semantic knowledge from LLMs to enhance
entity alignment. Specifically, LLMEA identifies candidate alignments for a
given entity by considering both embedding similarities between entities across
KGs and edit distances to a virtual equivalent entity. It then engages an LLM
iteratively, posing multiple multi-choice questions to draw upon the LLM's
inference capability. The final prediction of the equivalent entity is derived
from the LLM's output. Experiments conducted on three public datasets reveal
that LLMEA surpasses leading baseline models. Additional ablation studies
underscore the efficacy of our proposed framework.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.16960v1},
File          = {2401.16960v1.pdf}
}
@article{2307.06917v1,
Author        = {Lars-Peter Meyer and Claus Stadler and Johannes Frey and Norman Radtke and Kurt Junghanns and Roy Meissner and Gordian Dziwis and Kirill Bulert and Michael Martin},
Title         = {LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT},
Eprint        = {2307.06917v1},
DOI           = {10.1007/978-3-658-43705-3_8},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge Graphs (KG) provide us with a structured, flexible, transparent,
cross-system, and collaborative way of organizing our knowledge and data across
various domains in society and industrial as well as scientific disciplines.
KGs surpass any other form of representation in terms of effectiveness.
However, Knowledge Graph Engineering (KGE) requires in-depth experiences of
graph structures, web technologies, existing models and vocabularies, rule
sets, logic, as well as best practices. It also demands a significant amount of
work. Considering the advancements in large language models (LLMs) and their
interfaces and applications in recent years, we have conducted comprehensive
experiments with ChatGPT to explore its potential in supporting KGE. In this
paper, we present a selection of these experiments and their results to
demonstrate how ChatGPT can assist us in the development and management of KGs.},
Year          = {2023},
Month         = {Jul},
Note          = {Informatik aktuell. First Working Conference on Artificial
  Intelligence Development for a Resilient and Sustainable Tomorrow 2023.
  AIDRST 2023. p. 103-115},
Url           = {http://arxiv.org/abs/2307.06917v1},
File          = {2307.06917v1.pdf}
}
@article{2407.19338v1,
Author        = {Nour Hello and Paolo Di Lorenzo and Emilio Calvanese Strinati},
Title         = {Semantic Communication Enhanced by Knowledge Graph Representation
  Learning},
Eprint        = {2407.19338v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper investigates the advantages of representing and processing
semantic knowledge extracted into graphs within the emerging paradigm of
semantic communications. The proposed approach leverages semantic and pragmatic
aspects, incorporating recent advances on large language models (LLMs) to
achieve compact representations of knowledge to be processed and exchanged
between intelligent agents. This is accomplished by using the cascade of LLMs
and graph neural networks (GNNs) as semantic encoders, where information to be
shared is selected to be meaningful at the receiver. The embedding vectors
produced by the proposed semantic encoder represent information in the form of
triplets: nodes (semantic concepts entities), edges(relations between
concepts), nodes. Thus, semantic information is associated with the
representation of relationships among elements in the space of semantic concept
abstractions. In this paper, we investigate the potential of achieving high
compression rates in communication by incorporating relations that link
elements within graph embeddings. We propose sending semantic symbols solely
equivalent to node embeddings through the wireless channel and inferring the
complete knowledge graph at the receiver. Numerical simulations illustrate the
effectiveness of leveraging knowledge graphs to semantically compress and
transmit information.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.19338v1},
File          = {2407.19338v1.pdf}
}
@article{2410.09080v1,
Author        = {Tianqi Shang and Shu Yang and Weiqing He and Tianhua Zhai and Dawei Li and Bojian Hou and Tianlong Chen and Jason H. Moore and Marylyn D. Ritchie and Li Shen},
Title         = {Leveraging Social Determinants of Health in Alzheimer's Research Using
  LLM-Augmented Literature Mining and Knowledge Graphs},
Eprint        = {2410.09080v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Growing evidence suggests that social determinants of health (SDoH), a set of
nonmedical factors, affect individuals' risks of developing Alzheimer's disease
(AD) and related dementias. Nevertheless, the etiological mechanisms underlying
such relationships remain largely unclear, mainly due to difficulties in
collecting relevant information. This study presents a novel, automated
framework that leverages recent advancements of large language model (LLM) and
natural language processing techniques to mine SDoH knowledge from extensive
literature and integrate it with AD-related biological entities extracted from
the general-purpose knowledge graph PrimeKG. Utilizing graph neural networks,
we performed link prediction tasks to evaluate the resultant SDoH-augmented
knowledge graph. Our framework shows promise for enhancing knowledge discovery
in AD and can be generalized to other SDoH-related research areas, offering a
new tool for exploring the impact of social determinants on health outcomes.
Our code is available at: https://github.com/hwq0726/SDoHenPKG},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.09080v1},
File          = {2410.09080v1.pdf}
}
@article{2412.10064v1,
Author        = {Makbule Gulcin Ozsoy and Leila Messallem and Jon Besga and Gianandrea Minneci},
Title         = {Text2Cypher: Bridging Natural Language and Graph Databases},
Eprint        = {2412.10064v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Knowledge graphs use nodes, relationships, and properties to represent
arbitrarily complex data. When stored in a graph database, the Cypher query
language enables efficient modeling and querying of knowledge graphs. However,
using Cypher requires specialized knowledge, which can present a challenge for
non-expert users. Our work Text2Cypher aims to bridge this gap by translating
natural language queries into Cypher query language and extending the utility
of knowledge graphs to non-technical expert users.
  While large language models (LLMs) can be used for this purpose, they often
struggle to capture complex nuances, resulting in incomplete or incorrect
outputs. Fine-tuning LLMs on domain-specific datasets has proven to be a more
promising approach, but the limited availability of high-quality, publicly
available Text2Cypher datasets makes this challenging. In this work, we show
how we combined, cleaned and organized several publicly available datasets into
a total of 44,387 instances, enabling effective fine-tuning and evaluation.
Models fine-tuned on this dataset showed significant performance gains, with
improvements in Google-BLEU and Exact Match scores over baseline models,
highlighting the importance of high-quality datasets and fine-tuning in
improving Text2Cypher performance.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.10064v1},
File          = {2412.10064v1.pdf}
}
@article{2501.14101v1,
Author        = {Murugan Sankaradas and Ravi K. Rajendran and Srimat T. Chakradhar},
Title         = {StreamingRAG: Real-time Contextual Retrieval and Generation Framework},
Eprint        = {2501.14101v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Extracting real-time insights from multi-modal data streams from various
domains such as healthcare, intelligent transportation, and satellite remote
sensing remains a challenge. High computational demands and limited knowledge
scope restrict the applicability of Multi-Modal Large Language Models (MM-LLMs)
on these data streams. Traditional Retrieval-Augmented Generation (RAG) systems
address knowledge limitations of these models, but suffer from slow
preprocessing, making them unsuitable for real-time analysis. We propose
StreamingRAG, a novel RAG framework designed for streaming data. StreamingRAG
constructs evolving knowledge graphs capturing scene-object-entity
relationships in real-time. The knowledge graph achieves temporal-aware scene
representations using MM-LLMs and enables timely responses for specific events
or user queries. StreamingRAG addresses limitations in existing methods,
achieving significant improvements in real-time analysis (5-6x faster
throughput), contextual accuracy (through a temporal knowledge graph), and
reduced resource consumption (using lightweight models by 2-3x).},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.14101v1},
File          = {2501.14101v1.pdf}
}
@article{2403.07118v1,
Author        = {Atharva Phatak and Vijay K. Mago and Ameeta Agrawal and Aravind Inbasekaran and Philippe J. Giabbanelli},
Title         = {Narrating Causal Graphs with Large Language Models},
Eprint        = {2403.07118v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The use of generative AI to create text descriptions from graphs has mostly
focused on knowledge graphs, which connect concepts using facts. In this work
we explore the capability of large pretrained language models to generate text
from causal graphs, where salient concepts are represented as nodes and
causality is represented via directed, typed edges. The causal reasoning
encoded in these graphs can support applications as diverse as healthcare or
marketing. Using two publicly available causal graph datasets, we empirically
investigate the performance of four GPT-3 models under various settings. Our
results indicate that while causal text descriptions improve with training
data, compared to fact-based graphs, they are harder to generate under
zero-shot settings. Results further suggest that users of generative AI can
deploy future applications faster since similar performances are obtained when
training a model with only a few examples as compared to fine-tuning via a
large curated dataset.},
Year          = {2024},
Month         = {Mar},
Note          = {Proceedings of the 57th Hawaii International Conference on System
  Sciences 2024},
Url           = {http://arxiv.org/abs/2403.07118v1},
File          = {2403.07118v1.pdf}
}
@article{2409.10146v1,
Author        = {Hamed Babaei Giglou and Jennifer D'Souza and Sören Auer},
Title         = {LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology
  Learning Challenge},
Eprint        = {2409.10146v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper outlines the LLMs4OL 2024, the first edition of the Large Language
Models for Ontology Learning Challenge. LLMs4OL is a community development
initiative collocated with the 23rd International Semantic Web Conference
(ISWC) to explore the potential of Large Language Models (LLMs) in Ontology
Learning (OL), a vital process for enhancing the web with structured knowledge
to improve interoperability. By leveraging LLMs, the challenge aims to advance
understanding and innovation in OL, aligning with the goals of the Semantic Web
to create a more intelligent and user-friendly web. In this paper, we give an
overview of the 2024 edition of the LLMs4OL challenge and summarize the
contributions.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.10146v1},
File          = {2409.10146v1.pdf}
}
@article{2412.16100v1,
Author        = {Bishwamittra Ghosh and Sarah Hasan and Naheed Anjum Arafat and Arijit Khan},
Title         = {Logical Consistency of Large Language Models in Fact-checking},
Eprint        = {2412.16100v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In recent years, large language models (LLMs) have demonstrated significant
success in performing varied natural language tasks such as language
translation, question-answering, summarizing, fact-checking, etc. Despite LLMs'
impressive ability to generate human-like texts, LLMs are infamous for their
inconsistent responses -- a meaning-preserving change in the input query
results in an inconsistent response and attributes to vulnerabilities of LLMs
such as hallucination, jailbreaking, etc. Consequently, existing research
focuses on simple paraphrasing-based consistency assessment of LLMs, and
ignores complex queries that necessitates an even better understanding of
logical reasoning by an LLM. Our work therefore addresses the logical
inconsistency of LLMs under complex logical queries with primitive logical
operators, e.g., negation, conjunction, and disjunction. As a test bed, we
consider retrieval-augmented LLMs on a fact-checking task involving
propositional logic queries from real-world knowledge graphs (KGs). Our
contributions are three-fold. Benchmark: We introduce three logical
fact-checking datasets over KGs for community development towards logically
consistent LLMs. Assessment: We propose consistency measures of LLMs on
propositional logic queries as input and demonstrate that existing LLMs lack
logical consistency, specially on complex queries. Improvement: We employ
supervised fine-tuning to improve the logical consistency of LLMs on the
complex fact-checking task with KG contexts.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.16100v1},
File          = {2412.16100v1.pdf}
}
@article{2312.00326v8,
Author        = {Zhangcheng Qiang and Weiqing Wang and Kerry Taylor},
Title         = {Agent-OM: Leveraging LLM Agents for Ontology Matching},
Eprint        = {2312.00326v8},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology matching (OM) enables semantic interoperability between different
ontologies and resolves their conceptual heterogeneity by aligning related
entities. OM systems currently have two prevailing design paradigms:
conventional knowledge-based expert systems and newer machine learning-based
predictive systems. While large language models (LLMs) and LLM agents have
revolutionised data engineering and have been applied creatively in many
domains, their potential for OM remains underexplored. This study introduces a
novel agent-powered LLM-based design paradigm for OM systems. With
consideration of several specific challenges in leveraging LLM agents for OM,
we propose a generic framework, namely Agent-OM (Agent for Ontology Matching),
consisting of two Siamese agents for retrieval and matching, with a set of OM
tools. Our framework is implemented in a proof-of-concept system. Evaluations
of three Ontology Alignment Evaluation Initiative (OAEI) tracks over
state-of-the-art OM systems show that our system can achieve results very close
to the long-standing best performance on simple OM tasks and can significantly
improve the performance on complex and few-shot OM tasks.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.00326v8},
File          = {2312.00326v8.pdf}
}
@article{2403.01395v1,
Author        = {Willis Guo and Armin Toroghi and Scott Sanner},
Title         = {CR-LT-KGQA: A Knowledge Graph Question Answering Dataset Requiring
  Commonsense Reasoning and Long-Tail Knowledge},
Eprint        = {2403.01395v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graph question answering (KGQA) is a well-established field that
seeks to provide factual answers to natural language (NL) questions by
leveraging knowledge graphs (KGs). However, existing KGQA datasets suffer from
two significant limitations: (1) no existing KGQA dataset requires commonsense
reasoning to arrive at an answer and (2) existing KGQA datasets focus on
popular entities for which large language models (LLMs) can directly answer
without hallucinating and without leveraging the KG. In this work, we seek a
novel KGQA dataset that supports commonsense reasoning and focuses on long-tail
entities (e.g., non-mainstream and recent entities) where LLMs frequently
hallucinate, and thus create the need for novel methodologies that leverage the
KG for factual and attributable commonsense inference. We create a novel
Commonsense Reasoning (CR) and Long-Tail (LT) KGQA dataset with two subtasks --
question answering and claim verification -- that address both limitations (1)
and (2). We construct CR-LT-KGQA by building extensions to existing reasoning
datasets StrategyQA and CREAK over Wikidata. While existing KGQA methods are
not applicable due to their lack of commonsense inference support, baseline
evaluation of LLMs on CR-LT KGQA demonstrate a high rate of hallucination.
Thus, CR-LT KGQA poses significant challenges for hallucination-prone LLMs,
hence paving the way for future commonsense KGQA research to provide accurate
and factual answers for long-tail entities in the era of LLMs.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.01395v1},
File          = {2403.01395v1.pdf}
}
@article{2107.04771v2,
Author        = {Jaspreet Singh Dhani and Ruchika Bhatt and Balaji Ganesan and Parikshet Sirohi and Vasudha Bhatnagar},
Title         = {Similar Cases Recommendation using Legal Knowledge Graphs},
Eprint        = {2107.04771v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {A legal knowledge graph constructed from court cases, judgments, laws and
other legal documents can enable a number of applications like question
answering, document similarity, and search. While the use of knowledge graphs
for distant supervision in NLP tasks is well researched, using knowledge graphs
for applications like case similarity presents challenges. In this work, we
describe our solution for predicting similar cases in Indian court judgements.
We present our results and also discuss the impact of large language models on
this task.},
Year          = {2021},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2107.04771v2},
File          = {2107.04771v2.pdf}
}
@article{2412.14387v1,
Author        = {Berkan Çakır},
Title         = {Clinical Trials Ontology Engineering with Large Language Models},
Eprint        = {2412.14387v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Managing clinical trial information is currently a significant challenge for
the medical industry, as traditional methods are both time-consuming and
costly. This paper proposes a simple yet effective methodology to extract and
integrate clinical trial data in a cost-effective and time-efficient manner.
Allowing the medical industry to stay up-to-date with medical developments.
Comparing time, cost, and quality of the ontologies created by humans, GPT3.5,
GPT4, and Llama3 (8b & 70b). Findings suggest that large language models (LLM)
are a viable option to automate this process both from a cost and time
perspective. This study underscores significant implications for medical
research where real-time data integration from clinical trials could become the
norm.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.14387v1},
File          = {2412.14387v1.pdf}
}
@article{2401.13444v3,
Author        = {Dehao Tao and Congqi Wang and Feng Huang and Junhao Chen and Yongfeng Huang and Minghu Jiang},
Title         = {Fine-Grained Stateful Knowledge Exploration: A Novel Paradigm for
  Integrating Knowledge Graphs with Large Language Models},
Eprint        = {2401.13444v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have shown impressive capabilities, yet updating
their knowledge remains a significant challenge, often leading to outdated or
inaccurate responses. A proposed solution is the integration of external
knowledge bases, such as knowledge graphs, with LLMs. Most existing methods use
a paradigm that treats the question as the objective, with relevant knowledge
being incrementally retrieved from the knowledge graph. However, this strategy
frequently experiences an mismatch in the granularity of knowledge between the
target question and the entities and relations being retrieved. As a result,
the information in the question cannot precisely correspond to the retrieved
knowledge. This may cause redundant exploration or omission of vital knowledge,
thereby leading to enhanced computational consumption and reduced retrieval
accuracy. In this paper, we propose a novel paradigm of fine-grained stateful
knowledge exploration, which addresses the `information granularity mismatch'
issue. We extract fine-grained information from questions and explore the
semantic mapping between this information and the knowledge in graph. By
dynamically updating the mapping records, we avoid redundant exploration and
ensure no pertinent information is overlooked, thereby reducing computational
overhead and improving the accuracy of knowledge exploration. The use of
fine-grained information also eliminates the need for a priori knowledge, a
common requirement in existing methods. Experiments on multiple datasets
revealed that our paradigm surpasses current advanced methods in knowledge
retrieval while significantly reducing the average number of LLM invocations.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.13444v3},
File          = {2401.13444v3.pdf}
}
@article{2403.01390v2,
Author        = {Armin Toroghi and Willis Guo and Mohammad Mahdi Abdollah Pour and Scott Sanner},
Title         = {Right for Right Reasons: Large Language Models for Verifiable
  Commonsense Knowledge Graph Question Answering},
Eprint        = {2403.01390v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graph Question Answering (KGQA) methods seek to answer Natural
Language questions using the relational information stored in Knowledge Graphs
(KGs). With the recent advancements of Large Language Models (LLMs) and their
remarkable reasoning abilities, there is a growing trend to leverage them for
KGQA. However, existing methodologies have only focused on answering factual
questions, e.g., "In which city was Silvio Berlusconi's first wife born?",
leaving questions involving commonsense reasoning that real-world users may
pose more often, e.g., "Do I need separate visas to see the Venus of Willendorf
and attend the Olympics this summer?" unaddressed. In this work, we first
observe that existing LLM-based methods for KGQA struggle with hallucination on
such questions, especially on queries targeting long-tail entities (e.g.,
non-mainstream and recent entities), thus hindering their applicability in
real-world applications especially since their reasoning processes are not
easily verifiable. In response, we propose Right for Right Reasons (R3), a
commonsense KGQA methodology that allows for a verifiable reasoning procedure
by axiomatically surfacing intrinsic commonsense knowledge of LLMs and
grounding every factual reasoning step on KG triples. Through experimental
evaluations across three different tasks--question answering, claim
verification, and preference matching--our findings showcase R3 as a superior
approach, outperforming existing methodologies and notably reducing instances
of hallucination and reasoning errors.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.01390v2},
File          = {2403.01390v2.pdf}
}
@article{2405.16412v3,
Author        = {Pengcheng Jiang and Lang Cao and Cao Xiao and Parminder Bhatia and Jimeng Sun and Jiawei Han},
Title         = {KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge},
Eprint        = {2405.16412v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graph Embedding (KGE) techniques are crucial in learning compact
representations of entities and relations within a knowledge graph,
facilitating efficient reasoning and knowledge discovery. While existing
methods typically focus either on training KGE models solely based on graph
structure or fine-tuning pre-trained language models with classification data
in KG, KG-FIT leverages LLM-guided refinement to construct a semantically
coherent hierarchical structure of entity clusters. By incorporating this
hierarchical knowledge along with textual information during the fine-tuning
process, KG-FIT effectively captures both global semantics from the LLM and
local semantics from the KG. Extensive experiments on the benchmark datasets
FB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over
state-of-the-art pre-trained language model-based methods, achieving
improvements of 14.4%, 13.5%, and 11.9% in the Hits@10 metric for the link
prediction task, respectively. Furthermore, KG-FIT yields substantial
performance gains of 12.6%, 6.7%, and 17.7% compared to the structure-based
base models upon which it is built. These results highlight the effectiveness
of KG-FIT in incorporating open-world knowledge from LLMs to significantly
enhance the expressiveness and informativeness of KG embeddings.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.16412v3},
File          = {2405.16412v3.pdf}
}
@article{2401.00761v1,
Author        = {Wenxuan Wang and Juluan Shi and Zhaopeng Tu and Youliang Yuan and Jen-tse Huang and Wenxiang Jiao and Michael R. Lyu},
Title         = {The Earth is Flat? Unveiling Factual Errors in Large Language Models},
Eprint        = {2401.00761v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Large Language Models (LLMs) like ChatGPT are foundational in various
applications due to their extensive knowledge from pre-training and
fine-tuning. Despite this, they are prone to generating factual and commonsense
errors, raising concerns in critical areas like healthcare, journalism, and
education to mislead users. Current methods for evaluating LLMs' veracity are
limited by test data leakage or the need for extensive human labor, hindering
efficient and accurate error detection. To tackle this problem, we introduce a
novel, automatic testing framework, FactChecker, aimed at uncovering factual
inaccuracies in LLMs. This framework involves three main steps: First, it
constructs a factual knowledge graph by retrieving fact triplets from a
large-scale knowledge database. Then, leveraging the knowledge graph,
FactChecker employs a rule-based approach to generates three types of questions
(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and
multi-hop relations, along with correct answers. Lastly, it assesses the LLMs'
responses for accuracy using tailored matching strategies for each question
type. Our extensive tests on six prominent LLMs, including text-davinci-002,
text-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal
that FactChecker can trigger factual errors in up to 45\% of questions in these
models. Moreover, we demonstrate that FactChecker's test cases can improve
LLMs' factual accuracy through in-context learning and fine-tuning (e.g.,
llama-2-13b-chat's accuracy increase from 35.3\% to 68.5\%). We are making all
code, data, and results available for future research endeavors.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.00761v1},
File          = {2401.00761v1.pdf}
}
@article{2402.14424v3,
Author        = {Song Tong and Kai Mao and Zhen Huang and Yukun Zhao and Kaiping Peng},
Title         = {Automating psychological hypothesis generation with AI: when large
  language models meet causal graph},
Eprint        = {2402.14424v3},
DOI           = {10.1057/s41599-024-03407-5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Leveraging the synergy between causal knowledge graphs and a large language
model (LLM), our study introduces a groundbreaking approach for computational
hypothesis generation in psychology. We analyzed 43,312 psychology articles
using a LLM to extract causal relation pairs. This analysis produced a
specialized causal graph for psychology. Applying link prediction algorithms,
we generated 130 potential psychological hypotheses focusing on `well-being',
then compared them against research ideas conceived by doctoral scholars and
those produced solely by the LLM. Interestingly, our combined approach of a LLM
and causal graphs mirrored the expert-level insights in terms of novelty,
clearly surpassing the LLM-only hypotheses (t(59) = 3.34, p=0.007 and t(59) =
4.32, p<0.001, respectively). This alignment was further corroborated using
deep semantic analysis. Our results show that combining LLM with machine
learning techniques such as causal knowledge graphs can revolutionize automated
discovery in psychology, extracting novel insights from the extensive
literature. This work stands at the crossroads of psychology and artificial
intelligence, championing a new enriched paradigm for data-driven hypothesis
generation in psychological research.},
Year          = {2024},
Month         = {Feb},
Note          = {Humanities and Social Sciences Communications, (2024) 11:896},
Url           = {http://arxiv.org/abs/2402.14424v3},
File          = {2402.14424v3.pdf}
}
@article{2410.08085v2,
Author        = {Yuan Sui and Yufei He and Zifeng Ding and Bryan Hooi},
Title         = {Can Knowledge Graphs Make Large Language Models More Trustworthy? An
  Empirical Study over Open-ended Question Answering},
Eprint        = {2410.08085v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent works integrating Knowledge Graphs (KGs) have led to promising
improvements in enhancing reasoning accuracy of Large Language Models (LLMs).
However, current benchmarks focus mainly on closed-ended tasks, leaving a gap
in the assessment of more complex real-world scenarios. This gap has also
obscured the evaluation of KGs' potential to mitigate the problem of
hallucination in LLMs. To fill the gap, we introduce OKGQA, a new benchmark
specifically designed to assess LLMs enhanced with KGs under open-ended,
real-world question answering scenarios. OKGQA is designed to closely reflect
the complexities of practical applications using questions from different
types, and incorporates specific metrics to measure both the reduction in
hallucinations and the enhancement in reasoning capabilities. To consider the
scenario in which KGs may have varying levels of mistakes, we propose another
benchmark variant OKGQA-P to assess model performance when the semantics and
structure of KGs are deliberately perturbed and contaminated. OKGQA aims to (1)
explore whether KGs can make LLMs more trustworthy in an open-ended setting,
and (2) conduct a comparative analysis to shed light on methods and future
directions for leveraging KGs to reduce LLMs' hallucination. We believe that
this study can facilitate a more complete performance comparison and encourage
continuous improvement in integrating KGs with LLMs.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.08085v2},
File          = {2410.08085v2.pdf}
}
@article{2410.11550v1,
Author        = {Tengfei Ma and Xuan Lin and Tianle Li and Chaoyi Li and Long Chen and Peng Zhou and Xibao Cai and Xinyu Yang and Daojian Zeng and Dongsheng Cao and Xiangxiang Zeng},
Title         = {Y-Mol: A Multiscale Biomedical Knowledge-Guided Large Language Model for
  Drug Development},
Eprint        = {2410.11550v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models (LLMs) have recently demonstrated remarkable
performance in general tasks across various fields. However, their
effectiveness within specific domains such as drug development remains
challenges. To solve these challenges, we introduce \textbf{Y-Mol}, forming a
well-established LLM paradigm for the flow of drug development. Y-Mol is a
multiscale biomedical knowledge-guided LLM designed to accomplish tasks across
lead compound discovery, pre-clinic, and clinic prediction. By integrating
millions of multiscale biomedical knowledge and using LLaMA2 as the base LLM,
Y-Mol augments the reasoning capability in the biomedical domain by learning
from a corpus of publications, knowledge graphs, and expert-designed synthetic
data. The capability is further enriched with three types of drug-oriented
instructions: description-based prompts from processed publications,
semantic-based prompts for extracting associations from knowledge graphs, and
template-based prompts for understanding expert knowledge from biomedical
tools. Besides, Y-Mol offers a set of LLM paradigms that can autonomously
execute the downstream tasks across the entire process of drug development,
including virtual screening, drug design, pharmacological properties
prediction, and drug-related interaction prediction. Our extensive evaluations
of various biomedical sources demonstrate that Y-Mol significantly outperforms
general-purpose LLMs in discovering lead compounds, predicting molecular
properties, and identifying drug interaction events.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.11550v1},
File          = {2410.11550v1.pdf}
}
@article{2410.20724v4,
Author        = {Mufei Li and Siqi Miao and Pan Li},
Title         = {Simple Is Effective: The Roles of Graphs and Large Language Models in
  Knowledge-Graph-Based Retrieval-Augmented Generation},
Eprint        = {2410.20724v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) demonstrate strong reasoning abilities but face
limitations such as hallucinations and outdated knowledge. Knowledge Graph
(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by
grounding LLM outputs in structured external knowledge from KGs. However,
current KG-based RAG frameworks still struggle to optimize the trade-off
between retrieval effectiveness and efficiency in identifying a suitable amount
of relevant graph information for the LLM to digest. We introduce SubgraphRAG,
extending the KG-based RAG framework that retrieves subgraphs and leverages
LLMs for reasoning and answer prediction. Our approach innovatively integrates
a lightweight multilayer perceptron with a parallel triple-scoring mechanism
for efficient and flexible subgraph retrieval while encoding directional
structural distances to enhance retrieval effectiveness. The size of retrieved
subgraphs can be flexibly adjusted to match the query's need and the downstream
LLM's capabilities. This design strikes a balance between model complexity and
reasoning power, enabling scalable and generalizable retrieval processes.
Notably, based on our retrieved subgraphs, smaller LLMs like
Llama3.1-8B-Instruct deliver competitive results with explainable reasoning,
while larger models like GPT-4o achieve state-of-the-art accuracy compared with
previous baselines -- all without fine-tuning. Extensive evaluations on the
WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,
accuracy, and reliability by reducing hallucinations and improving response
grounding.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.20724v4},
File          = {2410.20724v4.pdf}
}
@article{2412.09094v3,
Author        = {Ben Liu and Jihai Zhang and Fangquan Lin and Cheng Yang and Min Peng},
Title         = {Filter-then-Generate: Large Language Models with Structure-Text Adapter
  for Knowledge Graph Completion},
Eprint        = {2412.09094v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) present massive inherent knowledge and superior
semantic comprehension capability, which have revolutionized various tasks in
natural language processing. Despite their success, a critical gap remains in
enabling LLMs to perform knowledge graph completion (KGC). Empirical evidence
suggests that LLMs consistently perform worse than conventional KGC approaches,
even through sophisticated prompt design or tailored instruction-tuning.
Fundamentally, applying LLMs on KGC introduces several critical challenges,
including a vast set of entity candidates, hallucination issue of LLMs, and
under-exploitation of the graph structure. To address these challenges, we
propose a novel instruction-tuning-based method, namely FtG. Specifically, we
present a filter-then-generate paradigm and formulate the KGC task into a
multiple-choice question format. In this way, we can harness the capability of
LLMs while mitigating the issue casused by hallucinations. Moreover, we devise
a flexible ego-graph serialization prompt and employ a structure-text adapter
to couple structure and text information in a contextualized manner.
Experimental results demonstrate that FtG achieves substantial performance gain
compared to existing state-of-the-art methods. The instruction dataset and code
are available at https://github.com/LB0828/FtG.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.09094v3},
File          = {2412.09094v3.pdf}
}
@article{2403.14950v1,
Author        = {Xindi Luo and Zequn Sun and Jing Zhao and Zhe Zhao and Wei Hu},
Title         = {KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable
  Adaptation},
Eprint        = {2403.14950v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Parameter-efficient finetuning (PEFT) is a key technique for adapting large
language models (LLMs) to downstream tasks. In this paper, we study leveraging
knowledge graph embeddings to improve the effectiveness of PEFT. We propose a
knowledgeable adaptation method called KnowLA. It inserts an adaptation layer
into an LLM to integrate the embeddings of entities appearing in the input
text. The adaptation layer is trained in combination with LoRA on instruction
data. Experiments on six benchmarks with two popular LLMs and three knowledge
graphs demonstrate the effectiveness and robustness of KnowLA. We show that
\modelname can help activate the relevant parameterized knowledge in an LLM to
answer a question without changing its parameters or input prompts.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.14950v1},
File          = {2403.14950v1.pdf}
}
@article{2404.07677v2,
Author        = {Lei Sun and Zhengwei Tao and Youdi Li and Hiroshi Arakawa},
Title         = {ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs},
Eprint        = {2404.07677v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The integration of Large Language Models (LLMs) and knowledge graphs (KGs)
has achieved remarkable success in various natural language processing tasks.
However, existing methodologies that integrate LLMs and KGs often navigate the
task-solving process solely based on the LLM's analysis of the question,
overlooking the rich cognitive potential inherent in the vast knowledge
encapsulated in KGs. To address this, we introduce Observation-Driven Agent
(ODA), a novel AI agent framework tailored for tasks involving KGs. ODA
incorporates KG reasoning abilities via global observation, which enhances
reasoning capabilities through a cyclical paradigm of observation, action, and
reflection. Confronting the exponential explosion of knowledge during
observation, we innovatively design a recursive observation mechanism.
Subsequently, we integrate the observed knowledge into the action and
reflection modules. Through extensive experiments, ODA demonstrates
state-of-the-art performance on several datasets, notably achieving accuracy
improvements of 12.87% and 8.9%.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.07677v2},
File          = {2404.07677v2.pdf}
}
@article{2407.04363v2,
Author        = {Petr Anokhin and Nikita Semenov and Artyom Sorokin and Dmitry Evseev and Mikhail Burtsev and Evgeny Burnaev},
Title         = {AriGraph: Learning Knowledge Graph World Models with Episodic Memory for
  LLM Agents},
Eprint        = {2407.04363v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Advancements in the capabilities of Large Language Models (LLMs) have created
a promising foundation for developing autonomous agents. With the right tools,
these agents could learn to solve tasks in new environments by accumulating and
updating their knowledge. Current LLM-based agents process past experiences
using a full history of observations, summarization, retrieval augmentation.
However, these unstructured memory representations do not facilitate the
reasoning and planning essential for complex decision-making. In our study, we
introduce AriGraph, a novel method wherein the agent constructs and updates a
memory graph that integrates semantic and episodic memories while exploring the
environment. We demonstrate that our Ariadne LLM agent, consisting of the
proposed memory architecture augmented with planning and decision-making,
effectively handles complex tasks within interactive text game environments
difficult even for human players. Results show that our approach markedly
outperforms other established memory methods and strong RL baselines in a range
of problems of varying complexity. Additionally, AriGraph demonstrates
competitive performance compared to dedicated knowledge graph-based methods in
static multi-hop question-answering.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.04363v2},
File          = {2407.04363v2.pdf}
}
@article{2408.07494v1,
Author        = {Jan Luca Scheerer and Anton Lykov and Moe Kayali and Ilias Fountalis and Dan Olteanu and Nikolaos Vasiloglou and Dan Suciu},
Title         = {QirK: Question Answering via Intermediate Representation on Knowledge
  Graphs},
Eprint        = {2408.07494v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {We demonstrate QirK, a system for answering natural language questions on
Knowledge Graphs (KG). QirK can answer structurally complex questions that are
still beyond the reach of emerging Large Language Models (LLMs). It does so
using a unique combination of database technology, LLMs, and semantic search
over vector embeddings. The glue for these components is an intermediate
representation (IR). The input question is mapped to IR using LLMs, which is
then repaired into a valid relational database query with the aid of a semantic
search on vector embeddings. This allows a practical synthesis of LLM
capabilities and KG reliability.
  A short video demonstrating QirK is available at
https://youtu.be/6c81BLmOZ0U.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.07494v1},
File          = {2408.07494v1.pdf}
}
@article{2409.19753v2,
Author        = {Yike Wu and Yi Huang and Nan Hu and Yuncheng Hua and Guilin Qi and Jiaoyan Chen and Jeff Z. Pan},
Title         = {CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex
  Knowledge Graph Question Answering},
Eprint        = {2409.19753v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent studies have explored the use of Large Language Models (LLMs) with
Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering
(KGQA). They typically require rewriting retrieved subgraphs into natural
language formats comprehensible to LLMs. However, when tackling complex
questions, the knowledge rewritten by existing methods may include irrelevant
information, omit crucial details, or fail to align with the question's
semantics. To address them, we propose a novel rewriting method CoTKR,
Chain-of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces
and corresponding knowledge in an interleaved manner, thereby mitigating the
limitations of single-step knowledge rewriting. Additionally, to bridge the
preference gap between the knowledge rewriter and the question answering (QA)
model, we propose a training strategy PAQAF, Preference Alignment from Question
Answering Feedback, for leveraging feedback from the QA model to further
optimize the knowledge rewriter. We conduct experiments using various LLMs
across several KGQA benchmarks. Experimental results demonstrate that, compared
with previous knowledge rewriting methods, CoTKR generates the most beneficial
knowledge representation for QA models, which significantly improves the
performance of LLMs in KGQA.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.19753v2},
File          = {2409.19753v2.pdf}
}
@article{2411.14012v2,
Author        = {Aldo Gangemi and Andrea Giovanni Nuzzolese},
Title         = {Logic Augmented Generation},
Eprint        = {2411.14012v2},
DOI           = {10.1016/j.websem.2024.100859},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Semantic Knowledge Graphs (SKG) face challenges with scalability,
flexibility, contextual understanding, and handling unstructured or ambiguous
information. However, they offer formal and structured knowledge enabling
highly interpretable and reliable results by means of reasoning and querying.
Large Language Models (LLMs) overcome those limitations making them suitable in
open-ended tasks and unstructured environments. Nevertheless, LLMs are neither
interpretable nor reliable. To solve the dichotomy between LLMs and SKGs we
envision Logic Augmented Generation (LAG) that combines the benefits of the two
worlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate
potentially infinite relations and tacit knowledge on-demand. SKGs are key for
injecting a discrete heuristic dimension with clear logical and factual
boundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,
medical diagnostics and climate projections. Understanding the properties and
limitations of LAG, which are still mostly unknown, is of utmost importance for
enabling a variety of tasks involving tacit knowledge in order to provide
interpretable and effective results.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.14012v2},
File          = {2411.14012v2.pdf}
}
@article{2412.11387v1,
Author        = {Abdulrahman Althobaiti and Angel Ayala and JingYing Gao and Ali Almutairi and Mohammad Deghat and Imran Razzak and Francisco Cruz},
Title         = {How Can LLMs and Knowledge Graphs Contribute to Robot Safety? A Few-Shot
  Learning Approach},
Eprint        = {2412.11387v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Large Language Models (LLMs) are transforming the robotics domain by enabling
robots to comprehend and execute natural language instructions. The cornerstone
benefits of LLM include processing textual data from technical manuals,
instructions, academic papers, and user queries based on the knowledge
provided. However, deploying LLM-generated code in robotic systems without
safety verification poses significant risks. This paper outlines a safety layer
that verifies the code generated by ChatGPT before executing it to control a
drone in a simulated environment. The safety layer consists of a fine-tuned
GPT-4o model using Few-Shot learning, supported by knowledge graph prompting
(KGP). Our approach improves the safety and compliance of robotic actions,
ensuring that they adhere to the regulations of drone operations.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.11387v1},
File          = {2412.11387v1.pdf}
}
@article{2411.11532v3,
Author        = {Hanxiang Xu and Wei Ma and Ting Zhou and Yanjie Zhao and Kai Chen and Qiang Hu and Yang Liu and Haoyu Wang},
Title         = {CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge
  Graph},
Eprint        = {2411.11532v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {In recent years, the programming capabilities of large language models (LLMs)
have garnered significant attention. Fuzz testing, a highly effective
technique, plays a key role in enhancing software reliability and detecting
vulnerabilities. However, traditional fuzz testing tools rely on manually
crafted fuzz drivers, which can limit both testing efficiency and
effectiveness. To address this challenge, we propose an automated fuzz testing
method driven by a code knowledge graph and powered by an LLM-based intelligent
agent system, referred to as CKGFuzzer. We approach fuzz driver creation as a
code generation task, leveraging the knowledge graph of the code repository to
automate the generation process within the fuzzing loop, while continuously
refining both the fuzz driver and input seeds. The code knowledge graph is
constructed through interprocedural program analysis, where each node in the
graph represents a code entity, such as a function or a file. The knowledge
graph-enhanced CKGFuzzer not only effectively resolves compilation errors in
fuzz drivers and generates input seeds tailored to specific API usage
scenarios, but also analyzes fuzz driver crash reports, assisting developers in
improving code quality. By querying the knowledge graph of the code repository
and learning from API usage scenarios, we can better identify testing targets
and understand the specific purpose of each fuzz driver. We evaluated our
approach using eight open-source software projects. The experimental results
indicate that CKGFuzzer achieved an average improvement of 8.73% in code
coverage compared to state-of-the-art techniques. Additionally, CKGFuzzer
reduced the manual review workload in crash case analysis by 84.4% and
successfully detected 11 real bugs (including nine previously unreported bugs)
across the tested libraries.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.11532v3},
File          = {2411.11532v3.pdf}
}
@article{2306.08302v3,
Author        = {Shirui Pan and Linhao Luo and Yufei Wang and Chen Chen and Jiapu Wang and Xindong Wu},
Title         = {Unifying Large Language Models and Knowledge Graphs: A Roadmap},
Eprint        = {2306.08302v3},
DOI           = {10.1109/TKDE.2024.3352100},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs), such as ChatGPT and GPT4, are making new waves
in the field of natural language processing and artificial intelligence, due to
their emergent ability and generalizability. However, LLMs are black-box
models, which often fall short of capturing and accessing factual knowledge. In
contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are
structured knowledge models that explicitly store rich factual knowledge. KGs
can enhance LLMs by providing external knowledge for inference and
interpretability. Meanwhile, KGs are difficult to construct and evolving by
nature, which challenges the existing methods in KGs to generate new facts and
represent unseen knowledge. Therefore, it is complementary to unify LLMs and
KGs together and simultaneously leverage their advantages. In this article, we
present a forward-looking roadmap for the unification of LLMs and KGs. Our
roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,
which incorporate KGs during the pre-training and inference phases of LLMs, or
for the purpose of enhancing understanding of the knowledge learned by LLMs; 2)
LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,
completion, construction, graph-to-text generation, and question answering; and
3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a
mutually beneficial way to enhance both LLMs and KGs for bidirectional
reasoning driven by both data and knowledge. We review and summarize existing
efforts within these three frameworks in our roadmap and pinpoint their future
research directions.},
Year          = {2023},
Month         = {Jun},
Note          = {IEEE Transactions on Knowledge and Data Engineering (TKDE) 2024},
Url           = {http://arxiv.org/abs/2306.08302v3},
File          = {2306.08302v3.pdf}
}
@article{2312.03633v3,
Author        = {Da Wu and Jingye Yang and Kai Wang},
Title         = {Exploring the Reversal Curse and Other Deductive Logical Reasoning in
  BERT and GPT-Based Large Language Models},
Eprint        = {2312.03633v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The term "Reversal Curse" refers to the scenario where auto-regressive
decoder large language models (LLMs), such as ChatGPT, trained on "A is B" fail
to learn "B is A," assuming that B and A are distinct and can be uniquely
identified from each other, demonstrating a basic failure of logical deduction.
This raises a red flag in the use of GPT models for certain general tasks such
as constructing knowledge graphs, considering their adherence to this symmetric
principle. In our study, we examined a bidirectional LLM, BERT, and found that
it is immune to the reversal curse. Driven by ongoing efforts to construct
biomedical knowledge graphs with LLMs, we also embarked on evaluating more
complex but essential deductive reasoning capabilities. This process included
first training encoder and decoder language models to master the intersection
and union operations on two sets and then moving on to assess their capability
to infer different combinations of union and intersection operations on three
newly created sets. The findings showed that while both encoder and decoder
language models, trained for tasks involving two sets (union/intersection),
were proficient in such scenarios, they encountered difficulties when dealing
with operations that included three sets (various combinations of union and
intersection). Our research highlights the distinct characteristics of encoder
and decoder models in simple and complex logical reasoning. In practice, the
choice between BERT and GPT should be guided by the specific requirements and
nature of the task at hand, leveraging their respective strengths in
bidirectional context comprehension and sequence prediction.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.03633v3},
File          = {2312.03633v3.pdf}
}
@article{2402.13750v1,
Author        = {Qian Zhao and Hao Qian and Ziqi Liu and Gong-Duo Zhang and Lihong Gu},
Title         = {Breaking the Barrier: Utilizing Large Language Models for Industrial
  Recommendation Systems through an Inferential Knowledge Graph},
Eprint        = {2402.13750v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Recommendation systems are widely used in e-commerce websites and online
platforms to address information overload. However, existing systems primarily
rely on historical data and user feedback, making it difficult to capture user
intent transitions. Recently, Knowledge Base (KB)-based models are proposed to
incorporate expert knowledge, but it struggle to adapt to new items and the
evolving e-commerce environment. To address these challenges, we propose a
novel Large Language Model based Complementary Knowledge Enhanced
Recommendation System (LLM-KERec). It introduces an entity extractor that
extracts unified concept terms from item and user information. To provide
cost-effective and reliable prior knowledge, entity pairs are generated based
on entity popularity and specific strategies. The large language model
determines complementary relationships in each entity pair, constructing a
complementary knowledge graph. Furthermore, a new complementary recall module
and an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of
the ranking model using real complementary exposure-click samples. Extensive
experiments conducted on three industry datasets demonstrate the significant
performance improvement of our model compared to existing approaches.
Additionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm
for consumption by recommending complementary items. In summary, LLM-KERec
addresses the limitations of traditional recommendation systems by
incorporating complementary knowledge and utilizing a large language model to
capture user intent transitions, adapt to new items, and enhance recommendation
efficiency in the evolving e-commerce landscape.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.13750v1},
File          = {2402.13750v1.pdf}
}
@article{2411.11531v2,
Author        = {Viktoriia Chekalina and Anton Razzhigaev and Elizaveta Goncharova and Andrey Kuznetsov},
Title         = {Addressing Hallucinations in Language Models with Knowledge Graph
  Embeddings as an Additional Modality},
Eprint        = {2411.11531v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this paper we present an approach to reduce hallucinations in Large
Language Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional
modality. Our method involves transforming input text into a set of KG
embeddings and using an adapter to integrate these embeddings into the language
model space, without relying on external retrieval processes.
  To facilitate this, we created WikiEntities, a dataset containing over 3
million Wikipedia texts annotated with entities from Wikidata and their
corresponding embeddings from PyTorch-BigGraph. This dataset serves as a
valuable resource for training Entity Linking models and adapting the described
method to various LLMs using specialized adapters.
  Our method does not require fine-tuning of the language models themselves;
instead, we only train the adapter. This ensures that the model's performance
on other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA
2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and
demonstrated that our approach improves performance on the HaluEval, True-False
benchmarks and FEVER dataset. The results indicate that incorporating KGs as a
new modality can effectively reduce hallucinations and improve the factual
accuracy of language models, all without the need for external retrieval.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.11531v2},
File          = {2411.11531v2.pdf}
}
@article{2309.03118v1,
Author        = {Chao Feng and Xinyu Zhang and Zichu Fei},
Title         = {Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from
  Knowledge Graphs},
Eprint        = {2309.03118v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs), such as ChatGPT and GPT-4, are versatile and
can solve different tasks due to their emergent ability and generalizability.
However, LLMs sometimes lack domain-specific knowledge to perform tasks, which
would also cause hallucination during inference. In some previous works,
additional modules like graph neural networks (GNNs) are trained on retrieved
knowledge from external knowledge bases, aiming to mitigate the problem of
lacking domain-specific knowledge. However, incorporating additional modules:
1) would need retraining additional modules when encountering novel domains; 2)
would become a bottleneck since LLMs' strong abilities are not fully utilized
for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver
(KSL), to teach LLMs to search for essential knowledge from external knowledge
bases by harnessing their own strong generalizability. Specifically, we design
a simple yet effective prompt to transform retrieval into a multi-hop decision
sequence, which empowers LLMs with searching knowledge ability in zero-shot
manner. Additionally, KSL is able to provide complete retrieval paths and
therefore increase explainability of LLMs' reasoning processes. We conduct
experiments on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, and
found that our approach improves LLM baseline performance by a relatively large
margin.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.03118v1},
File          = {2309.03118v1.pdf}
}
@article{2311.08614v2,
Author        = {Zichen Chen and Jianda Chen and Ambuj Singh and Misha Sra},
Title         = {XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded
  Explanations in LLMs},
Eprint        = {2311.08614v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have achieved remarkable success in natural
language tasks, yet understanding their reasoning processes remains a
significant challenge. We address this by introducing XplainLLM, a dataset
accompanying an explanation framework designed to enhance LLM transparency and
reliability. Our dataset comprises 24,204 instances where each instance
interprets the LLM's reasoning behavior using knowledge graphs (KGs) and graph
attention networks (GAT), and includes explanations of LLMs such as the
decoder-only Llama-3 and the encoder-only RoBERTa. XplainLLM also features a
framework for generating grounded explanations and the debugger-scores for
multidimensional quality analysis. Our explanations include why-choose and
why-not-choose components, reason-elements, and debugger-scores that
collectively illuminate the LLM's reasoning behavior. Our evaluations
demonstrate XplainLLM's potential to reduce hallucinations and improve grounded
explanation generation in LLMs. XplainLLM is a resource for researchers and
practitioners to build trust and verify the reliability of LLM outputs.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.08614v2},
File          = {2311.08614v2.pdf}
}
@article{2409.13732v2,
Author        = {HuangChao Xu and Baohua Zhang and Zhong Jin and Tiannian Zhu and Quansheng Wu and Hongming Weng},
Title         = {Enhancing Large Language Models with Domain-Specific Knowledge: The Case
  in Topological Materials},
Eprint        = {2409.13732v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs), such as ChatGPT, have demonstrated impressive
performance in the text generation task, showing the ability to understand and
respond to complex instructions. However, the performance of naive LLMs in
speciffc domains is limited due to the scarcity of domain-speciffc corpora and
specialized training. Moreover, training a specialized large-scale model
necessitates signiffcant hardware resources, which restricts researchers from
leveraging such models to drive advances. Hence, it is crucial to further
improve and optimize LLMs to meet speciffc domain demands and enhance their
scalability. Based on the condensed matter data center, we establish a material
knowledge graph (MaterialsKG) and integrate it with literature. Using large
language models and prompt learning, we develop a specialized dialogue system
for topological materials called TopoChat. Compared to naive LLMs, TopoChat
exhibits superior performance in structural and property querying, material
recommendation, and complex relational reasoning. This system enables efffcient
and precise retrieval of information and facilitates knowledge interaction,
thereby encouraging the advancement on the ffeld of condensed matter materials.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.13732v2},
File          = {2409.13732v2.pdf}
}
@article{2308.06374v1,
Author        = {Jeff Z. Pan and Simon Razniewski and Jan-Christoph Kalo and Sneha Singhania and Jiaoyan Chen and Stefan Dietze and Hajira Jabeen and Janna Omeliyanenko and Wen Zhang and Matteo Lissandrini and Russa Biswas and Gerard de Melo and Angela Bonifati and Edlira Vakaj and Mauro Dragoni and Damien Graux},
Title         = {Large Language Models and Knowledge Graphs: Opportunities and Challenges},
Eprint        = {2308.06374v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models (LLMs) have taken Knowledge Representation -- and the
world -- by storm. This inflection point marks a shift from explicit knowledge
representation to a renewed focus on the hybrid representation of both explicit
knowledge and parametric knowledge. In this position paper, we will discuss
some of the common debate points within the community on LLMs (parametric
knowledge) and Knowledge Graphs (explicit knowledge) and speculate on
opportunities and visions that the renewed focus brings, as well as related
research topics and challenges.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.06374v1},
File          = {2308.06374v1.pdf}
}
@article{2401.14640v1,
Author        = {Nan Hu and Jiaoyan Chen and Yike Wu and Guilin Qi and Sheng Bi and Tongtong Wu and Jeff Z. Pan},
Title         = {Benchmarking Large Language Models in Complex Question Answering
  Attribution using Knowledge Graphs},
Eprint        = {2401.14640v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The attribution of question answering is to provide citations for supporting
generated statements, and has attracted wide research attention. The current
methods for automatically evaluating the attribution, which are often based on
Large Language Models (LLMs), are still inadequate, particularly in recognizing
subtle differences between attributions, and complex relationships between
citations and statements. To compare these attribution evaluation methods and
develop new ones, we introduce a set of fine-grained categories (i.e.,
supportive, insufficient, contradictory and irrelevant) for measuring the
attribution, and develop a Complex Attributed Question Answering (CAQA)
benchmark by leveraging knowledge graphs (KGs) for automatically generating
attributions of different categories to question-answer pairs. Our analysis
reveals that existing evaluators perform poorly under fine-grained attribution
settings and exhibit weaknesses in complex citation-statement reasoning. Our
CAQA benchmark, validated with human annotations, emerges as a promising tool
for selecting and developing LLM attribution evaluators.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.14640v1},
File          = {2401.14640v1.pdf}
}
@article{2408.07705v1,
Author        = {Sara AlMahri and Liming Xu and Alexandra Brintrup},
Title         = {Enhancing Supply Chain Visibility with Knowledge Graphs and Large
  Language Models},
Eprint        = {2408.07705v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {In today's globalized economy, comprehensive supply chain visibility is
crucial for effective risk management. Achieving visibility remains a
significant challenge due to limited information sharing among supply chain
partners. This paper presents a novel framework leveraging Knowledge Graphs
(KGs) and Large Language Models (LLMs) to enhance supply chain visibility
without relying on direct stakeholder information sharing. Our zero-shot,
LLM-driven approach automates the extraction of supply chain information from
diverse public sources and constructs KGs to capture complex interdependencies
between supply chain entities. We employ zero-shot prompting for Named Entity
Recognition (NER) and Relation Extraction (RE) tasks, eliminating the need for
extensive domain-specific training. We validate the framework with a case study
on electric vehicle supply chains, focusing on tracking critical minerals for
battery manufacturing. Results show significant improvements in supply chain
mapping, extending visibility beyond tier-2 suppliers. The framework reveals
critical dependencies and alternative sourcing options, enhancing risk
management and strategic planning. With high accuracy in NER and RE tasks, it
provides an effective tool for understanding complex, multi-tiered supply
networks. This research offers a scalable, flexible method for constructing
domain-specific supply chain KGs, addressing longstanding challenges in
visibility and paving the way for advancements in digital supply chain
surveillance.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.07705v1},
File          = {2408.07705v1.pdf}
}
@article{2412.07412v1,
Author        = {Ahan Bhatt and Nandan Vaghela and Kush Dudhia},
Title         = {Generating Knowledge Graphs from Large Language Models: A Comparative
  Study of GPT-4, LLaMA 2, and BERT},
Eprint        = {2412.07412v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a
form of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks
requiring structured reasoning and semantic understanding. However, creating
KGs for GraphRAGs remains a significant challenge due to accuracy and
scalability limitations of traditional methods. This paper introduces a novel
approach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and
BERT to generate KGs directly from unstructured data, bypassing traditional
pipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit
Distance, and Semantic Similarity, we evaluate the models' ability to generate
high-quality KGs. Results demonstrate that GPT-4 achieves superior semantic
fidelity and structural accuracy, LLaMA 2 excels in lightweight,
domain-specific graphs, and BERT provides insights into challenges in
entity-relationship modeling. This study underscores the potential of LLMs to
streamline KG creation and enhance GraphRAG accessibility for real-world
applications, while setting a foundation for future advancements.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.07412v1},
File          = {2412.07412v1.pdf}
}
@article{2501.18542v1,
Author        = {Raia Abu Ahmad and Reham Alharbi and Roberto Barile and Martin Böckling and Francisco Bolanos and Sara Bonfitto and Oleksandra Bruns and Irene Celino and Yashrajsinh Chudasama and Martin Critelli and Claudia d'Amato and Giada D'Ippolito and Ioannis Dasoulas and Stefano De Giorgis and Vincenzo De Leo and Chiara Di Bonaventura and Marco Di Panfilo and Daniil Dobriy and John Domingue and Xuemin Duan and Michel Dumontier and Sefika Efeoglu and Ruben Eschauzier and Fakih Ginwa and Nicolas Ferranti and Arianna Graciotti and Philipp Hanisch and George Hannah and Golsa Heidari and Aidan Hogan and Hassan Hussein and Alexane Jouglar and Jan-Christoph Kalo and Manoé Kieffer and Antonis Klironomos and Inês Koch and Weronika Lajewska and Nicolas Lazzari and Mikael Lindekrans and Anna Sofia Lippolis and Majlinda Llugiqi and Eleonora Mancini and Eleonora Marzi and Laura Menotti and Daniela Milon Flores and Soulakshmee Nagowah and Kerstin Neubert and Emetis Niazmand and Ebrahim Norouzi and Beatriz Olarte Martinez and Anouk Michelle Oudshoorn and Andrea Poltronieri and Valentina Presutti and Disha Purohit and Ensiyeh Raoufi and Celian Ringwald and Johanna Rockstroh and Sebastian Rudolph and Harald Sack and Zafar Saeed and Mohammad Javad Saeedizade and Aya Sahbi and Cristian Santini and Aleksandra Simic and Dennis Sommer and Rita Sousa and Mary Ann Tan and Vidyashree Tarikere and Tabea Tietz and Liam Tirpitz and Arnaldo Tomasino and Frank van Harmelen and Joao Vissoci and Caitlin Woods and Bohui Zhang and Xinyue Zhang and Heng Zheng},
Title         = {Semantic Web and Creative AI -- A Technical Report from ISWS 2023},
Eprint        = {2501.18542v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The International Semantic Web Research School (ISWS) is a week-long
intensive program designed to immerse participants in the field. This document
reports a collaborative effort performed by ten teams of students, each guided
by a senior researcher as their mentor, attending ISWS 2023. Each team provided
a different perspective to the topic of creative AI, substantiated by a set of
research questions as the main subject of their investigation. The 2023 edition
of ISWS focuses on the intersection of Semantic Web technologies and Creative
AI. ISWS 2023 explored various intersections between Semantic Web technologies
and creative AI. A key area of focus was the potential of LLMs as support tools
for knowledge engineering. Participants also delved into the multifaceted
applications of LLMs, including legal aspects of creative content production,
humans in the loop, decentralised approaches to multimodal generative AI
models, nanopublications and AI for personal scientific knowledge graphs,
commonsense knowledge in automatic story and narrative completion, generative
AI for art critique, prompt engineering, automatic music composition,
commonsense prototyping and conceptual blending, and elicitation of tacit
knowledge. As Large Language Models and semantic technologies continue to
evolve, new exciting prospects are emerging: a future where the boundaries
between creative expression and factual knowledge become increasingly permeable
and porous, leading to a world of knowledge that is both informative and
inspiring.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.18542v1},
File          = {2501.18542v1.pdf}
}
@article{2310.00299v2,
Author        = {Asahi Ushio and Jose Camacho-Collados and Steven Schockaert},
Title         = {RelBERT: Embedding Relations with Language Models},
Eprint        = {2310.00299v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Many applications need access to background knowledge about how different
concepts and entities are related. Although Knowledge Graphs (KG) and Large
Language Models (LLM) can address this need to some extent, KGs are inevitably
incomplete and their relational schema is often too coarse-grained, while LLMs
are inefficient and difficult to control. As an alternative, we propose to
extract relation embeddings from relatively small language models. In
particular, we show that masked language models such as RoBERTa can be
straightforwardly fine-tuned for this purpose, using only a small amount of
training data. The resulting model, which we call RelBERT, captures relational
similarity in a surprisingly fine-grained way, allowing us to set a new
state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of
modelling relations that go well beyond what the model has seen during
training. For instance, we obtained strong results on relations between named
entities with a model that was only trained on lexical relations between
concepts, and we observed that RelBERT can recognise morphological analogies
despite not being trained on such examples. Overall, we find that RelBERT
significantly outperforms strategies based on prompting language models that
are several orders of magnitude larger, including recent GPT-based models and
open source models.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2310.00299v2},
File          = {2310.00299v2.pdf}
}
@article{2408.08972v1,
Author        = {Debashis Gupta and Aditi Golder and Luis Fernendez and Miles Silman and Greg Lersen and Fan Yang and Bob Plemmons and Sarra Alqahtani and Paul Victor Pauca},
Title         = {ASGM-KG: Unveiling Alluvial Gold Mining Through Knowledge Graphs},
Eprint        = {2408.08972v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Artisanal and Small-Scale Gold Mining (ASGM) is a low-cost yet highly
destructive mining practice, leading to environmental disasters across the
world's tropical watersheds. The topic of ASGM spans multiple domains of
research and information, including natural and social systems, and knowledge
is often atomized across a diversity of media and documents. We therefore
introduce a knowledge graph (ASGM-KG) that consolidates and provides crucial
information about ASGM practices and their environmental effects. The current
version of ASGM-KG consists of 1,899 triples extracted using a large language
model (LLM) from documents and reports published by both non-governmental and
governmental organizations. These documents were carefully selected by a group
of tropical ecologists with expertise in ASGM. This knowledge graph was
validated using two methods. First, a small team of ASGM experts reviewed and
labeled triples as factual or non-factual. Second, we devised and applied an
automated factual reduction framework that relies on a search engine and an LLM
for labeling triples. Our framework performs as well as five baselines on a
publicly available knowledge graph and achieves over 90 accuracy on our ASGM-KG
validated by domain experts. ASGM-KG demonstrates an advancement in knowledge
aggregation and representation for complex, interdisciplinary environmental
crises such as ASGM.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.08972v1},
File          = {2408.08972v1.pdf}
}
@article{2501.13746v1,
Author        = {Yuhui Yun and Huilong Ye and Xinru Li and Ruojia Li and Jingfeng Deng and Li Li and Haoyi Xiong},
Title         = {EICopilot: Search and Explore Enterprise Information over Large-scale
  Knowledge Graphs with LLM-driven Agents},
Eprint        = {2501.13746v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The paper introduces EICopilot, an novel agent-based solution enhancing
search and exploration of enterprise registration data within extensive online
knowledge graphs like those detailing legal entities, registered capital, and
major shareholders. Traditional methods necessitate text-based queries and
manual subgraph explorations, often resulting in time-consuming processes.
EICopilot, deployed as a chatbot via Baidu Enterprise Search, improves this
landscape by utilizing Large Language Models (LLMs) to interpret natural
language queries. This solution automatically generates and executes Gremlin
scripts, providing efficient summaries of complex enterprise relationships.
Distinct feature a data pre-processing pipeline that compiles and annotates
representative queries into a vector database of examples for In-context
learning (ICL), a comprehensive reasoning pipeline combining Chain-of-Thought
with ICL to enhance Gremlin script generation for knowledge graph search and
exploration, and a novel query masking strategy that improves intent
recognition for heightened script accuracy. Empirical evaluations demonstrate
the superior performance of EICopilot, including speed and accuracy, over
baseline methods, with the \emph{Full Mask} variant achieving a syntax error
rate reduction to as low as 10.00% and an execution correctness of up to
82.14%. These components collectively contribute to superior querying
capabilities and summarization of intricate datasets, positioning EICopilot as
a groundbreaking tool in the exploration and exploitation of large-scale
knowledge graphs for enterprise information search.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.13746v1},
File          = {2501.13746v1.pdf}
}
@article{2501.15791v1,
Author        = {Yu Li and Yi Huang and Guilin Qi and Junlan Feng and Nan Hu and Songlin Zhai and Haohan Xue and Yongrui Chen and Ruoyan Shen and Tongtong Wu},
Title         = {Harnessing Diverse Perspectives: A Multi-Agent Framework for Enhanced
  Error Detection in Knowledge Graphs},
Eprint        = {2501.15791v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs are widely used in industrial applications, making error
detection crucial for ensuring the reliability of downstream applications.
Existing error detection methods often fail to effectively leverage
fine-grained subgraph information and rely solely on fixed graph structures,
while also lacking transparency in their decision-making processes, which
results in suboptimal detection performance. In this paper, we propose a novel
Multi-Agent framework for Knowledge Graph Error Detection (MAKGED) that
utilizes multiple large language models (LLMs) in a collaborative setting. By
concatenating fine-grained, bidirectional subgraph embeddings with LLM-based
query embeddings during training, our framework integrates these
representations to produce four specialized agents. These agents utilize
subgraph information from different dimensions to engage in multi-round
discussions, thereby improving error detection accuracy and ensuring a
transparent decision-making process. Extensive experiments on FB15K and WN18RR
demonstrate that MAKGED outperforms state-of-the-art methods, enhancing the
accuracy and robustness of KG evaluation. For specific industrial scenarios,
our framework can facilitate the training of specialized agents using
domain-specific knowledge graphs for error detection, which highlights the
potential industrial application value of our framework. Our code and datasets
are available at https://github.com/kse-ElEvEn/MAKGED.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.15791v1},
File          = {2501.15791v1.pdf}
}
@article{2412.15235v1,
Author        = {Kartik Sharma and Peeyush Kumar and Yunqing Li},
Title         = {OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large
  Language Models},
Eprint        = {2412.15235v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper presents OG-RAG, an Ontology-Grounded Retrieval Augmented
Generation method designed to enhance LLM-generated responses by anchoring
retrieval processes in domain-specific ontologies. While LLMs are widely used
for tasks like question answering and search, they struggle to adapt to
specialized knowledge, such as industrial workflows or knowledge work, without
expensive fine-tuning or sub-optimal retrieval methods. Existing
retrieval-augmented models, such as RAG, offer improvements but fail to account
for structured domain knowledge, leading to suboptimal context generation.
Ontologies, which conceptually organize domain knowledge by defining entities
and their interrelationships, offer a structured representation to address this
gap. OG-RAG constructs a hypergraph representation of domain documents, where
each hyperedge encapsulates clusters of factual knowledge grounded using
domain-specific ontology. An optimization algorithm then retrieves the minimal
set of hyperedges that constructs a precise, conceptually grounded context for
the LLM. This method enables efficient retrieval while preserving the complex
relationships between entities. OG-RAG applies to domains where fact-based
reasoning is essential, particularly in tasks that require workflows or
decision-making steps to follow predefined rules and procedures. These include
industrial workflows in healthcare, legal, and agricultural sectors, as well as
knowledge-driven tasks such as news journalism, investigative research,
consulting and more. Our evaluations demonstrate that OG-RAG increases the
recall of accurate facts by 55% and improves response correctness by 40% across
four different LLMs. Additionally, OG-RAG enables 30% faster attribution of
responses to context and boosts fact-based reasoning accuracy by 27% compared
to baseline methods.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.15235v1},
File          = {2412.15235v1.pdf}
}
@article{2406.06610v1,
Author        = {Walid S. Saba},
Title         = {Reinterpreting 'the Company a Word Keeps': Towards Explainable and
  Ontologically Grounded Language Models},
Eprint        = {2406.06610v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We argue that the relative success of large language models (LLMs) is not a
reflection on the symbolic vs. subsymbolic debate but a reflection on employing
a successful bottom-up strategy of a reverse engineering of language at scale.
However, and due to their subsymbolic nature whatever knowledge these systems
acquire about language will always be buried in millions of weights none of
which is meaningful on its own, rendering such systems utterly unexplainable.
Furthermore, and due to their stochastic nature, LLMs will often fail in making
the correct inferences in various linguistic contexts that require reasoning in
intensional, temporal, or modal contexts. To remedy these shortcomings we
suggest employing the same successful bottom-up strategy employed in LLMs but
in a symbolic setting, resulting in explainable, language-agnostic, and
ontologically grounded language models.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.06610v1},
File          = {2406.06610v1.pdf}
}
@article{2409.04181v2,
Author        = {Larissa Pusch and Tim O. F. Conrad},
Title         = {Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question
  Answering},
Eprint        = {2409.04181v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Advancements in natural language processing have revolutionized the way we
can interact with digital information systems, such as databases, making them
more accessible. However, challenges persist, especially when accuracy is
critical, as in the biomedical domain. A key issue is the hallucination
problem, where models generate information unsupported by the underlying data,
potentially leading to dangerous misinformation. This paper presents a novel
approach designed to bridge this gap by combining Large Language Models (LLM)
and Knowledge Graphs (KG) to improve the accuracy and reliability of
question-answering systems, on the example of a biomedical KG. Built on the
LangChain framework, our method incorporates a query checker that ensures the
syntactical and semantic validity of LLM-generated queries, which are then used
to extract information from a Knowledge Graph, substantially reducing errors
like hallucinations. We evaluated the overall performance using a new benchmark
dataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo
and llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other
models in generating accurate queries, open-source models like llama3:70b show
promise with appropriate prompt engineering. To make this approach accessible,
a user-friendly web-based interface has been developed, allowing users to input
natural language queries, view generated and corrected Cypher queries, and
verify the resulting paths for accuracy. Overall, this hybrid approach
effectively addresses common issues such as data gaps and hallucinations,
offering a reliable and intuitive solution for question answering systems. The
source code for generating the results of this paper and for the user-interface
can be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.04181v2},
File          = {2409.04181v2.pdf}
}
@article{2402.01729v3,
Author        = {Dawei Li and Zhen Tan and Tianlong Chen and Huan Liu},
Title         = {Contextualization Distillation from Large Language Model for Knowledge
  Graph Completion},
Eprint        = {2402.01729v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {While textual information significantly enhances the performance of
pre-trained language models (PLMs) in knowledge graph completion (KGC), the
static and noisy nature of existing corpora collected from Wikipedia articles
or synsets definitions often limits the potential of PLM-based KGC models. To
surmount these challenges, we introduce the Contextualization Distillation
strategy, a versatile plug-in-and-play approach compatible with both
discriminative and generative KGC frameworks. Our method begins by instructing
large language models (LLMs) to transform compact, structural triplets into
context-rich segments. Subsequently, we introduce two tailored auxiliary tasks,
reconstruction and contextualization, allowing smaller KGC models to assimilate
insights from these enriched triplets. Comprehensive evaluations across diverse
datasets and KGC techniques highlight the efficacy and adaptability of our
approach, revealing consistent performance enhancements irrespective of
underlying pipelines or architectures. Moreover, our analysis makes our method
more explainable and provides insight into generating path selection, as well
as the choosing of suitable distillation tasks. All the code and data in this
work will be released at
https://github.com/David-Li0406/Contextulization-Distillation},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2402.01729v3},
File          = {2402.01729v3.pdf}
}
@article{2301.12473v2,
Author        = {Vahan Arsenyan and Spartak Bughdaryan and Fadi Shaya and Kent Small and Davit Shahnazaryan},
Title         = {Large Language Models for Biomedical Knowledge Graph Construction:
  Information extraction from EMR notes},
Eprint        = {2301.12473v2},
DOI           = {10.18653/v1/2024.bionlp-1.23},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The automatic construction of knowledge graphs (KGs) is an important research
area in medicine, with far-reaching applications spanning drug discovery and
clinical trial design. These applications hinge on the accurate identification
of interactions among medical and biological entities. In this study, we
propose an end-to-end machine learning solution based on large language models
(LLMs) that utilize electronic medical record notes to construct KGs. The
entities used in the KG construction process are diseases, factors, treatments,
as well as manifestations that coexist with the patient while experiencing the
disease. Given the critical need for high-quality performance in medical
applications, we embark on a comprehensive assessment of 12 LLMs of various
architectures, evaluating their performance and safety attributes. To gauge the
quantitative efficacy of our approach by assessing both precision and recall,
we manually annotate a dataset provided by the Macula and Retina Institute. We
also assess the qualitative performance of LLMs, such as the ability to
generate structured outputs or the tendency to hallucinate. The results
illustrate that in contrast to encoder-only and encoder-decoder, decoder-only
LLMs require further investigation. Additionally, we provide guided prompt
design to utilize such LLMs. The application of the proposed methodology is
demonstrated on age-related macular degeneration.},
Year          = {2023},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2301.12473v2},
File          = {2301.12473v2.pdf}
}
@article{2310.07793v5,
Author        = {Ruotong Liao and Xu Jia and Yangzhe Li and Yunpu Ma and Volker Tresp},
Title         = {GenTKG: Generative Forecasting on Temporal Knowledge Graph with Large
  Language Models},
Eprint        = {2310.07793v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The rapid advancements in large language models (LLMs) have ignited interest
in the temporal knowledge graph (tKG) domain, where conventional
embedding-based and rule-based methods dominate. The question remains open of
whether pre-trained LLMs can understand structured temporal relational data and
replace them as the foundation model for temporal relational forecasting.
Therefore, we bring temporal knowledge forecasting into the generative setting.
However, challenges occur in the huge chasms between complex temporal graph
data structure and sequential natural expressions LLMs can handle, and between
the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.
To address these challenges, we propose a novel retrieval-augmented generation
framework named GenTKG combining a temporal logical rule-based retrieval
strategy and few-shot parameter-efficient instruction tuning to solve the above
challenges, respectively. Extensive experiments have shown that GenTKG
outperforms conventional methods of temporal relational forecasting with low
computation resources using extremely limited training data as few as 16
samples. GenTKG also highlights remarkable cross-domain generalizability with
outperforming performance on unseen datasets without re-training, and in-domain
generalizability regardless of time split in the same dataset. Our work reveals
the huge potential of LLMs in the tKG domain and opens a new frontier for
generative forecasting on tKGs. Code and data are released here:
https://github.com/mayhugotong/GenTKG.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.07793v5},
File          = {2310.07793v5.pdf}
}
@article{2408.03010v1,
Author        = {Daniel Steinigen and Roman Teucher and Timm Heine Ruland and Max Rudat and Nicolas Flores-Herr and Peter Fischer and Nikola Milosevic and Christopher Schymura and Angelo Ziletti},
Title         = {Fact Finder -- Enhancing Domain Expertise of Large Language Models by
  Incorporating Knowledge Graphs},
Eprint        = {2408.03010v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent advancements in Large Language Models (LLMs) have showcased their
proficiency in answering natural language queries. However, their effectiveness
is hindered by limited domain-specific knowledge, raising concerns about the
reliability of their responses. We introduce a hybrid system that augments LLMs
with domain-specific knowledge graphs (KGs), thereby aiming to enhance factual
correctness using a KG-based retrieval approach. We focus on a medical KG to
demonstrate our methodology, which includes (1) pre-processing, (2) Cypher
query generation, (3) Cypher query processing, (4) KG retrieval, and (5)
LLM-enhanced response generation. We evaluate our system on a curated dataset
of 69 samples, achieving a precision of 78\% in retrieving correct KG nodes.
Our findings indicate that the hybrid system surpasses a standalone LLM in
accuracy and completeness, as verified by an LLM-as-a-Judge evaluation method.
This positions the system as a promising tool for applications that demand
factual correctness and completeness, such as target identification -- a
critical process in pinpointing biological entities for disease treatment or
crop enhancement. Moreover, its intuitive search interface and ability to
provide accurate responses within seconds make it well-suited for
time-sensitive, precision-focused research contexts. We publish the source code
together with the dataset and the prompt templates used.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.03010v1},
File          = {2408.03010v1.pdf}
}
@article{2412.18537v2,
Author        = {Derong Xu and Xinhang Li and Ziheng Zhang and Zhenxi Lin and Zhihong Zhu and Zhi Zheng and Xian Wu and Xiangyu Zhao and Tong Xu and Enhong Chen},
Title         = {Harnessing Large Language Models for Knowledge Graph Question Answering
  via Adaptive Multi-Aspect Retrieval-Augmentation},
Eprint        = {2412.18537v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) demonstrate remarkable capabilities, yet
struggle with hallucination and outdated knowledge when tasked with complex
knowledge reasoning, resulting in factually incorrect outputs. Previous studies
have attempted to mitigate it by retrieving factual knowledge from large-scale
knowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of
answers. However, this kind of approach often introduces noise and irrelevant
data, especially in situations with extensive context from multiple knowledge
aspects. In this way, LLM attention can be potentially mislead from question
and relevant information. In our study, we introduce an Adaptive Multi-Aspect
Retrieval-augmented over KGs (Amar) framework. This method retrieves knowledge
including entities, relations, and subgraphs, and converts each piece of
retrieved text into prompt embeddings. The Amar framework comprises two key
sub-components: 1) a self-alignment module that aligns commonalities among
entities, relations, and subgraphs to enhance retrieved text, thereby reducing
noise interference; 2) a relevance gating module that employs a soft gate to
learn the relevance score between question and multi-aspect retrieved data, to
determine which information should be used to enhance LLMs' output, or even
filtered altogether. Our method has achieved state-of-the-art performance on
two common datasets, WebQSP and CWQ, showing a 1.9\% improvement in accuracy
over its best competitor and a 6.6\% improvement in logical form generation
over a method that directly uses retrieved text as context prompts. These
results demonstrate the effectiveness of Amar in improving the reasoning of
LLMs.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.18537v2},
File          = {2412.18537v2.pdf}
}
@article{2409.17011v3,
Author        = {Shengwei Tian and Lifeng Han and Goran Nenadic},
Title         = {AutoLLM-CARD: Towards a Description and Landscape of Large Language
  Models},
Eprint        = {2409.17011v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {With the rapid growth of the Natural Language Processing (NLP) field, a vast
variety of Large Language Models (LLMs) continue to emerge for diverse NLP
tasks. As more papers are published, researchers and developers face the
challenge of information overload. Thus, developing a system that can
automatically extract and organise key information about LLMs from academic
papers is particularly important. The standard format for documenting
information about LLMs is the LLM model card (\textbf{LLM-Card}). We propose a
method for automatically generating LLM model cards from scientific
publications. We use Named Entity Recognition (\textbf{NER}) and Relation
Extraction (\textbf{RE}) methods that automatically extract key information
about LLMs from the papers, helping researchers to access information about
LLMs efficiently. These features include model \textit{licence}, model
\textit{name}, and model \textit{application}. With these features, we can form
a model card for each paper. We processed 106 academic papers by defining three
dictionaries -- LLM's name, licence, and application. 11,051 sentences were
extracted through dictionary lookup, and the dataset was constructed through
manual review of the final selection of 129 sentences with a link between the
name and the \textit{licence}, and 106 sentences with a link between the model
name and the \textit{application}. The resulting resource is relevant for LLM
card illustrations using relational knowledge graphs. Our code and findings can
contribute to automatic LLM card generation. Data and code in
\textsc{autoLLM-Card} will be shared and freely available at
\url{https://github.com/shengwei-tian/dependency-parser-visualization}},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.17011v3},
File          = {2409.17011v3.pdf}
}
@article{2501.06699v1,
Author        = {Aidan Hogan and Xin Luna Dong and Denny Vrandečić and Gerhard Weikum},
Title         = {Large Language Models, Knowledge Graphs and Search Engines: A Crossroads
  for Answering Users' Questions},
Eprint        = {2501.06699v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Much has been discussed about how Large Language Models, Knowledge Graphs and
Search Engines can be combined in a synergistic manner. A dimension largely
absent from current academic discourse is the user perspective. In particular,
there remain many open questions regarding how best to address the diverse
information needs of users, incorporating varying facets and levels of
difficulty. This paper introduces a taxonomy of user information needs, which
guides us to study the pros, cons and possible synergies of Large Language
Models, Knowledge Graphs and Search Engines. From this study, we derive a
roadmap for future research.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.06699v1},
File          = {2501.06699v1.pdf}
}
@article{2404.08020v1,
Author        = {Sanat Sharma and Mayank Poddar and Jayant Kumar and Kosta Blank and Tracy King},
Title         = {Augmenting Knowledge Graph Hierarchies Using Neural Transformers},
Eprint        = {2404.08020v1},
DOI           = {10.1007/978-3-031-56069-9_35},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs are useful tools to organize, recommend and sort data.
Hierarchies in knowledge graphs provide significant benefit in improving
understanding and compartmentalization of the data within a knowledge graph.
This work leverages large language models to generate and augment hierarchies
in an existing knowledge graph. For small (<100,000 node) domain-specific KGs,
we find that a combination of few-shot prompting with one-shot generation works
well, while larger KG may require cyclical generation. We present techniques
for augmenting hierarchies, which led to coverage increase by 98% for intents
and 99% for colors in our knowledge graph.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.08020v1},
File          = {2404.08020v1.pdf}
}
@article{2409.00331v1,
Author        = {Oktie Hassanzadeh},
Title         = {WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph
  Construction},
Eprint        = {2409.00331v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recently, there has been an increasing interest in the construction of
general-domain and domain-specific causal knowledge graphs. Such knowledge
graphs enable reasoning for causal analysis and event prediction, and so have a
range of applications across different domains. While great progress has been
made toward automated construction of causal knowledge graphs, the evaluation
of such solutions has either focused on low-level tasks (e.g., cause-effect
phrase extraction) or on ad hoc evaluation data and small manual evaluations.
In this paper, we present a corpus, task, and evaluation framework for causal
knowledge graph construction. Our corpus consists of Wikipedia articles for a
collection of event-related concepts in Wikidata. The task is to extract causal
relations between event concepts from the corpus. The evaluation is performed
in part using existing causal relations in Wikidata to measure recall, and in
part using Large Language Models to avoid the need for manual or crowd-sourced
evaluation. We evaluate a pipeline for causal knowledge graph construction that
relies on neural models for question answering and concept linking, and show
how the corpus and the evaluation framework allow us to effectively find the
right model for each task. The corpus and the evaluation framework are publicly
available.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2409.00331v1},
File          = {2409.00331v1.pdf}
}
@article{2410.18415v1,
Author        = {Kun Li and Tianhua Zhang and Xixin Wu and Hongyin Luo and James Glass and Helen Meng},
Title         = {Decoding on Graphs: Faithful and Sound Reasoning on Knowledge Graphs
  through Generation of Well-Formed Chains},
Eprint        = {2410.18415v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graphs (KGs) can serve as reliable knowledge sources for question
answering (QA) due to their structured representation of knowledge. Existing
research on the utilization of KG for large language models (LLMs) prevalently
relies on subgraph retriever or iterative prompting, overlooking the potential
synergy of LLMs' step-wise reasoning capabilities and KGs' structural nature.
In this paper, we present DoG (Decoding on Graphs), a novel framework that
facilitates a deep synergy between LLMs and KGs. We first define a concept,
well-formed chain, which consists of a sequence of interrelated fact triplets
on the KGs, starting from question entities and leading to answers. We argue
that this concept can serve as a principle for making faithful and sound
reasoning for KGQA. To enable LLMs to generate well-formed chains, we propose
graph-aware constrained decoding, in which a constraint derived from the
topology of the KG regulates the decoding process of the LLMs. This constrained
decoding method ensures the generation of well-formed chains while making full
use of the step-wise reasoning capabilities of LLMs. Based on the above, DoG, a
training-free approach, is able to provide faithful and sound reasoning
trajectories grounded on the KGs. Experiments across various KGQA tasks with
different background KGs demonstrate that DoG achieves superior and robust
performance. DoG also shows general applicability with various open-source
LLMs.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.18415v1},
File          = {2410.18415v1.pdf}
}
@article{2411.14459v1,
Author        = {Zhangchi Qiu and Linhao Luo and Shirui Pan and Alan Wee-Chung Liew},
Title         = {Unveiling User Preferences: A Knowledge Graph and LLM-Driven Approach
  for Conversational Recommendation},
Eprint        = {2411.14459v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Conversational Recommender Systems (CRSs) aim to provide personalized
recommendations through dynamically capturing user preferences in interactive
conversations. Conventional CRSs often extract user preferences as hidden
representations, which are criticized for their lack of interpretability. This
diminishes the transparency and trustworthiness of the recommendation process.
Recent works have explored combining the impressive capabilities of Large
Language Models (LLMs) with the domain-specific knowledge of Knowledge Graphs
(KGs) to generate human-understandable recommendation explanations. Despite
these efforts, the integration of LLMs and KGs for CRSs remains challenging due
to the modality gap between unstructured dialogues and structured KGs.
Moreover, LLMs pre-trained on large-scale corpora may not be well-suited for
analyzing user preferences, which require domain-specific knowledge. In this
paper, we propose COMPASS, a plug-and-play framework that synergizes LLMs and
KGs to unveil user preferences, enhancing the performance and explainability of
existing CRSs. To address integration challenges, COMPASS employs a two-stage
training approach: first, it bridges the gap between the structured KG and
natural language through an innovative graph entity captioning pre-training
mechanism. This enables the LLM to transform KG entities into concise natural
language descriptions, allowing them to comprehend domain-specific knowledge.
Following, COMPASS optimizes user preference modeling via knowledge-aware
instruction fine-tuning, where the LLM learns to reason and summarize user
preferences from both dialogue histories and KG-augmented context. This enables
COMPASS to perform knowledge-aware reasoning and generate comprehensive and
interpretable user preferences that can seamlessly integrate with existing CRS
models for improving recommendation performance and explainability.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.14459v1},
File          = {2411.14459v1.pdf}
}
@article{2412.00478v1,
Author        = {Xinyu Lin and Tianyu Zhang and Chengbin Hou and Jinbao Wang and Jianye Xue and Hairong Lv},
Title         = {Node Importance Estimation Leveraging LLMs for Semantic Augmentation in
  Knowledge Graphs},
Eprint        = {2412.00478v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Node Importance Estimation (NIE) is a task that quantifies the importance of
node in a graph. Recent research has investigated to exploit various
information from Knowledge Graphs (KGs) to estimate node importance scores.
However, the semantic information in KGs could be insufficient, missing, and
inaccurate, which would limit the performance of existing NIE models. To
address these issues, we leverage Large Language Models (LLMs) for semantic
augmentation thanks to the LLMs' extra knowledge and ability of integrating
knowledge from both LLMs and KGs. To this end, we propose the LLMs Empowered
Node Importance Estimation (LENIE) method to enhance the semantic information
in KGs for better supporting NIE tasks. To our best knowledge, this is the
first work incorporating LLMs into NIE. Specifically, LENIE employs a novel
clustering-based triplet sampling strategy to extract diverse knowledge of a
node sampled from the given KG. After that, LENIE adopts the node-specific
adaptive prompts to integrate the sampled triplets and the original node
descriptions, which are then fed into LLMs for generating richer and more
precise augmented node descriptions. These augmented descriptions finally
initialize node embeddings for boosting the downstream NIE model performance.
Extensive experiments demonstrate LENIE's effectiveness in addressing semantic
deficiencies in KGs, enabling more informative semantic augmentation and
enhancing existing NIE models to achieve the state-of-the-art performance. The
source code of LENIE is freely available at
\url{https://github.com/XinyuLin-FZ/LENIE}.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2412.00478v1},
File          = {2412.00478v1.pdf}
}
@article{2310.11638v3,
Author        = {Linhao Luo and Thuy-Trang Vu and Dinh Phung and Gholamreza Haffari},
Title         = {Systematic Assessment of Factual Knowledge in Large Language Models},
Eprint        = {2310.11638v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Previous studies have relied on existing question-answering benchmarks to
evaluate the knowledge stored in large language models (LLMs). However, this
approach has limitations regarding factual knowledge coverage, as it mostly
focuses on generic domains which may overlap with the pretraining data. This
paper proposes a framework to systematically assess the factual knowledge of
LLMs by leveraging knowledge graphs (KGs). Our framework automatically
generates a set of questions and expected answers from the facts stored in a
given KG, and then evaluates the accuracy of LLMs in answering these questions.
We systematically evaluate the state-of-the-art LLMs with KGs in generic and
specific domains. The experiment shows that ChatGPT is consistently the top
performer across all domains. We also find that LLMs performance depends on the
instruction finetuning, domain and question complexity and is prone to
adversarial context.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.11638v3},
File          = {2310.11638v3.pdf}
}
@article{2402.15048v2,
Author        = {Xuhui Jiang and Yinghan Shen and Zhichao Shi and Chengjin Xu and Wei Li and Zixuan Li and Jian Guo and Huawei Shen and Yuanzhuo Wang},
Title         = {Unlocking the Power of Large Language Models for Entity Alignment},
Eprint        = {2402.15048v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG)
data, playing a crucial role in data-driven AI applications. Traditional EA
methods primarily rely on comparing entity embeddings, but their effectiveness
is constrained by the limited input KG data and the capabilities of the
representation learning techniques. Against this backdrop, we introduce ChatEA,
an innovative framework that incorporates large language models (LLMs) to
improve EA. To address the constraints of limited input KG data, ChatEA
introduces a KG-code translation module that translates KG structures into a
format understandable by LLMs, thereby allowing LLMs to utilize their extensive
background knowledge to improve EA accuracy. To overcome the over-reliance on
entity embedding comparisons, ChatEA implements a two-stage EA strategy that
capitalizes on LLMs' capability for multi-step reasoning in a dialogue format,
thereby enhancing accuracy while preserving efficiency. Our experimental
results verify ChatEA's superior performance, highlighting LLMs' potential in
facilitating EA tasks.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.15048v2},
File          = {2402.15048v2.pdf}
}
@article{2407.00379v1,
Author        = {Jianheng Tang and Qifan Zhang and Yuhan Li and Jia Li},
Title         = {GraphArena: Benchmarking Large Language Models on Graph Computational
  Problems},
Eprint        = {2407.00379v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The "arms race" of Large Language Models (LLMs) demands novel, challenging,
and diverse benchmarks to faithfully examine their progresses. We introduce
GraphArena, a benchmarking tool designed to evaluate LLMs on graph
computational problems using million-scale real-world graphs from diverse
scenarios such as knowledge graphs, social networks, and molecular structures.
GraphArena offers a suite of 10 computational tasks, encompassing four
polynomial-time (e.g., Shortest Distance) and six NP-complete challenges (e.g.,
Travelling Salesman Problem). It features a rigorous evaluation framework that
classifies LLM outputs as correct, suboptimal (feasible but not optimal), or
hallucinatory (properly formatted but infeasible). Evaluation of 10 leading
LLMs, including GPT-4o and LLaMA3-70B-Instruct, reveals that even
top-performing models struggle with larger, more complex graph problems and
exhibit hallucination issues. Despite the application of strategies such as
chain-of-thought prompting, these issues remain unresolved. GraphArena
contributes a valuable supplement to the existing LLM benchmarks and is
open-sourced at https://github.com/squareRoot3/GraphArena.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2407.00379v1},
File          = {2407.00379v1.pdf}
}
@article{2407.20564v1,
Author        = {Tianshi Zheng and Jiaxin Bai and Yicheng Wang and Tianqing Fang and Yue Guo and Yauwai Yim and Yangqiu Song},
Title         = {CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large
  Language Models over Factual Knowledge},
Eprint        = {2407.20564v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {While large language models (LLMs) have demonstrated impressive capabilities
across various natural language processing tasks by acquiring rich factual
knowledge from their broad training data, their ability to synthesize and
logically reason with this knowledge in complex ways remains underexplored. In
this work, we present a systematic evaluation of state-of-the-art LLMs' complex
logical reasoning abilities through a novel benchmark of automatically
generated complex reasoning questions over general domain and biomedical
knowledge graphs. Our extensive experiments, employing diverse in-context
learning techniques, reveal that LLMs excel at reasoning over general world
knowledge but face significant challenges with specialized domain-specific
knowledge. We find that prompting with explicit Chain-of-Thought demonstrations
can substantially improve LLM performance on complex logical reasoning tasks
with diverse logical operations. Interestingly, our controlled evaluations
uncover an asymmetry where LLMs display proficiency at set union operations,
but struggle considerably with set intersections - a key building block of
logical reasoning. To foster further work, we will publicly release our
evaluation benchmark and code.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.20564v1},
File          = {2407.20564v1.pdf}
}
@article{2408.09529v2,
Author        = {Xinnan Dai and Qihao Wen and Yifei Shen and Hongzhi Wen and Dongsheng Li and Jiliang Tang and Caihua Shan},
Title         = {Revisiting the Graph Reasoning Ability of Large Language Models: Case
  Studies in Translation, Connectivity and Shortest Path},
Eprint        = {2408.09529v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have achieved great success in various reasoning
tasks. In this work, we focus on the graph reasoning ability of LLMs. Although
theoretical studies proved that LLMs are capable of handling graph reasoning
tasks, empirical evaluations reveal numerous failures. To deepen our
understanding on this discrepancy, we revisit the ability of LLMs on three
fundamental graph tasks: graph description translation, graph connectivity, and
the shortest-path problem. Our findings suggest that LLMs can fail to
understand graph structures through text descriptions and exhibit varying
performance for all these three fundamental tasks. Meanwhile, we perform a
real-world investigation on knowledge graphs and make consistent observations
with our findings. The codes and datasets are available.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.09529v2},
File          = {2408.09529v2.pdf}
}
@article{2411.03883v2,
Author        = {Laura Cabello and Carmen Martin-Turrero and Uchenna Akujuobi and Anders Søgaard and Carlos Bobed},
Title         = {MEG: Medical Knowledge-Augmented Large Language Models for Question
  Answering},
Eprint        = {2411.03883v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Question answering is a natural language understanding task that involves
reasoning over both explicit context and unstated, relevant domain knowledge.
Large language models (LLMs), which underpin most contemporary question
answering systems, struggle to induce how concepts relate in specialized
domains such as medicine. Existing medical LLMs are also costly to train. In
this work, we present MEG, a parameter-efficient approach for medical
knowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate
graph embeddings into the LLM, enabling it to leverage external knowledge in a
cost-effective way. We evaluate our method on four popular medical
multiple-choice datasets and show that LLMs greatly benefit from the factual
grounding provided by knowledge graph embeddings. MEG attains an average of
+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized
models like BioMistral. We also show results based on Llama-3. Finally, we show
that MEG's performance remains robust to the choice of graph encoder.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.03883v2},
File          = {2411.03883v2.pdf}
}
@article{2412.11499v1,
Author        = {Wonje Choi and Woo Kyung Kim and Minjong Yoo and Honguk Woo},
Title         = {Embodied CoT Distillation From LLM To Off-the-shelf Agents},
Eprint        = {2412.11499v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We address the challenge of utilizing large language models (LLMs) for
complex embodied tasks, in the environment where decision-making systems
operate timely on capacity-limited, off-the-shelf devices. We present DeDer, a
framework for decomposing and distilling the embodied reasoning capabilities
from LLMs to efficient, small language model (sLM)-based policies. In DeDer,
the decision-making process of LLM-based strategies is restructured into a
hierarchy with a reasoning-policy and planning-policy. The reasoning-policy is
distilled from the data that is generated through the embodied in-context
learning and self-verification of an LLM, so it can produce effective
rationales. The planning-policy, guided by the rationales, can render optimized
plans efficiently. In turn, DeDer allows for adopting sLMs for both policies,
deployed on off-the-shelf devices. Furthermore, to enhance the quality of
intermediate rationales, specific to embodied tasks, we devise the embodied
knowledge graph, and to generate multiple rationales timely through a single
inference, we also use the contrastively prompted attention model. Our
experiments with the ALFRED benchmark demonstrate that DeDer surpasses leading
language planning and distillation approaches, indicating the applicability and
efficiency of sLM-based embodied policies derived through DeDer.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.11499v1},
File          = {2412.11499v1.pdf}
}
@article{2307.07697v6,
Author        = {Jiashuo Sun and Chengjin Xu and Lumingyuan Tang and Saizhuo Wang and Chen Lin and Yeyun Gong and Lionel M. Ni and Heung-Yeung Shum and Jian Guo},
Title         = {Think-on-Graph: Deep and Responsible Reasoning of Large Language Model
  on Knowledge Graph},
Eprint        = {2307.07697v6},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Although large language models (LLMs) have achieved significant success in
various tasks, they often struggle with hallucination problems, especially in
scenarios requiring deep and responsible reasoning. These issues could be
partially addressed by introducing external knowledge graphs (KG) in LLM
reasoning. In this paper, we propose a new LLM-KG integrating paradigm
``$\hbox{LLM}\otimes\hbox{KG}$'' which treats the LLM as an agent to
interactively explore related entities and relations on KGs and perform
reasoning based on the retrieved knowledge. We further implement this paradigm
by introducing a new approach called Think-on-Graph (ToG), in which the LLM
agent iteratively executes beam search on KG, discovers the most promising
reasoning paths, and returns the most likely reasoning results. We use a number
of well-designed experiments to examine and illustrate the following advantages
of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has
the ability of knowledge traceability and knowledge correctability by
leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible
plug-and-play framework for different LLMs, KGs and prompting strategies
without any additional training cost; 4) the performance of ToG with small LLM
models could exceed large LLM such as GPT-4 in certain scenarios and this
reduces the cost of LLM deployment and application. As a training-free method
with lower computational cost and better generality, ToG achieves overall SOTA
in 6 out of 9 datasets where most previous SOTAs rely on additional training.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.07697v6},
File          = {2307.07697v6.pdf}
}
@article{2412.08258v1,
Author        = {Tanay Aggarwal and Angelo Salatino and Francesco Osborne and Enrico Motta},
Title         = {Large Language Models for Scholarly Ontology Generation: An Extensive
  Analysis in the Engineering Field},
Eprint        = {2412.08258v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Ontologies of research topics are crucial for structuring scientific
knowledge, enabling scientists to navigate vast amounts of research, and
forming the backbone of intelligent systems such as search engines and
recommendation systems. However, manual creation of these ontologies is
expensive, slow, and often results in outdated and overly general
representations. As a solution, researchers have been investigating ways to
automate or semi-automate the process of generating these ontologies. This
paper offers a comprehensive analysis of the ability of large language models
(LLMs) to identify semantic relationships between different research topics,
which is a critical step in the development of such ontologies. To this end, we
developed a gold standard based on the IEEE Thesaurus to evaluate the task of
identifying four types of relationships between pairs of topics: broader,
narrower, same-as, and other. Our study evaluates the performance of seventeen
LLMs, which differ in scale, accessibility (open vs. proprietary), and model
type (full vs. quantised), while also assessing four zero-shot reasoning
strategies. Several models have achieved outstanding results, including
Mixtral-8x7B, Dolphin-Mistral-7B, and Claude 3 Sonnet, with F1-scores of 0.847,
0.920, and 0.967, respectively. Furthermore, our findings demonstrate that
smaller, quantised models, when optimised through prompt engineering, can
deliver performance comparable to much larger proprietary models, while
requiring significantly fewer computational resources.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.08258v1},
File          = {2412.08258v1.pdf}
}
@article{2311.10112v2,
Author        = {Zifeng Ding and Heling Cai and Jingpei Wu and Yunpu Ma and Ruotong Liao and Bo Xiong and Volker Tresp},
Title         = {zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with
  Large Language Models},
Eprint        = {2311.10112v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Modeling evolving knowledge over temporal knowledge graphs (TKGs) has become
a heated topic. Various methods have been proposed to forecast links on TKGs.
Most of them are embedding-based, where hidden representations are learned to
represent knowledge graph (KG) entities and relations based on the observed
graph contexts. Although these methods show strong performance on traditional
TKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the
unseen zero-shot relations that have no prior graph context. In this paper, we
try to mitigate this problem as follows. We first input the text descriptions
of KG relations into large language models (LLMs) for generating relation
representations, and then introduce them into embedding-based TKGF methods.
LLM-empowered representations can capture the semantic information in the
relation descriptions. This makes the relations, whether seen or unseen, with
similar semantic meanings stay close in the embedding space, enabling TKGF
models to recognize zero-shot relations even without any observed graph
context. Experimental results show that our approach helps TKGF models to
achieve much better performance in forecasting the facts with previously unseen
relations, while still maintaining their ability in link forecasting regarding
seen relations.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.10112v2},
File          = {2311.10112v2.pdf}
}
@article{2404.09763v1,
Author        = {Avinash Anand and Mohit Gupta and Kritarth Prasad and Ujjwal Goel and Naman Lal and Astha Verma and Rajiv Ratn Shah},
Title         = {KG-CTG: Citation Generation through Knowledge Graph-guided Large
  Language Models},
Eprint        = {2404.09763v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Citation Text Generation (CTG) is a task in natural language processing (NLP)
that aims to produce text that accurately cites or references a cited document
within a source document. In CTG, the generated text draws upon contextual cues
from both the source document and the cited paper, ensuring accurate and
relevant citation information is provided. Previous work in the field of
citation generation is mainly based on the text summarization of documents.
Following this, this paper presents a framework, and a comparative study to
demonstrate the use of Large Language Models (LLMs) for the task of citation
generation. Also, we have shown the improvement in the results of citation
generation by incorporating the knowledge graph relations of the papers in the
prompt for the LLM to better learn the relationship between the papers. To
assess how well our model is performing, we have used a subset of standard
S2ORC dataset, which only consists of computer science academic research papers
in the English Language. Vicuna performs best for this task with 14.15 Meteor,
12.88 Rouge-1, 1.52 Rouge-2, and 10.94 Rouge-L. Also, Alpaca performs best, and
improves the performance by 36.98% in Rouge-1, and 33.14% in Meteor by
including knowledge graphs.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.09763v1},
File          = {2404.09763v1.pdf}
}
@article{2406.02110v1,
Author        = {Zhuoyang Li and Liran Deng and Hui Liu and Qiaoqiao Liu and Junzhao Du},
Title         = {UniOQA: A Unified Framework for Knowledge Graph Question Answering with
  Large Language Models},
Eprint        = {2406.02110v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {OwnThink stands as the most extensive Chinese open-domain knowledge graph
introduced in recent times. Despite prior attempts in question answering over
OwnThink (OQA), existing studies have faced limitations in model representation
capabilities, posing challenges in further enhancing overall accuracy in
question answering. In this paper, we introduce UniOQA, a unified framework
that integrates two complementary parallel workflows. Unlike conventional
approaches, UniOQA harnesses large language models (LLMs) for precise question
answering and incorporates a direct-answer-prediction process as a
cost-effective complement. Initially, to bolster representation capacity, we
fine-tune an LLM to translate questions into the Cypher query language (CQL),
tackling issues associated with restricted semantic understanding and
hallucinations. Subsequently, we introduce the Entity and Relation Replacement
algorithm to ensure the executability of the generated CQL. Concurrently, to
augment overall accuracy in question answering, we further adapt the
Retrieval-Augmented Generation (RAG) process to the knowledge graph.
Ultimately, we optimize answer accuracy through a dynamic decision algorithm.
Experimental findings illustrate that UniOQA notably advances SpCQL Logical
Accuracy to 21.2% and Execution Accuracy to 54.9%, achieving the new
state-of-the-art results on this benchmark. Through ablation experiments, we
delve into the superior representation capacity of UniOQA and quantify its
performance breakthrough.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.02110v1},
File          = {2406.02110v1.pdf}
}
@article{2410.02811v1,
Author        = {Hanzhu Chen and Xu Shen and Qitan Lv and Jie Wang and Xiaoqi Ni and Jieping Ye},
Title         = {SAC-KG: Exploiting Large Language Models as Skilled Automatic
  Constructors for Domain Knowledge Graphs},
Eprint        = {2410.02811v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs (KGs) play a pivotal role in knowledge-intensive tasks
across specialized domains, where the acquisition of precise and dependable
knowledge is crucial. However, existing KG construction methods heavily rely on
human intervention to attain qualified KGs, which severely hinders the
practical applicability in real-world scenarios. To address this challenge, we
propose a general KG construction framework, named SAC-KG, to exploit large
language models (LLMs) as Skilled Automatic Constructors for domain Knowledge
Graph. SAC-KG effectively involves LLMs as domain experts to generate
specialized and precise multi-level KGs. Specifically, SAC-KG consists of three
components: Generator, Verifier, and Pruner. For a given entity, Generator
produces its relations and tails from raw domain corpora, to construct a
specialized single-level KG. Verifier and Pruner then work together to ensure
precision by correcting generation errors and determining whether newly
produced tails require further iteration for the next-level KG.Experiments
demonstrate that SAC-KG automatically constructs a domain KG at the scale of
over one million nodes and achieves a precision of 89.32%, leading to a
superior performance with over 20% increase in precision rate compared to
existing state-of-the-art methods for the KG construction task.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2410.02811v1},
File          = {2410.02811v1.pdf}
}
@article{2308.00081v4,
Author        = {Mehwish Alam and Frank van Harmelen and Maribel Acosta},
Title         = {Towards Semantically Enriched Embeddings for Knowledge Graph Completion},
Eprint        = {2308.00081v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Embedding based Knowledge Graph (KG) Completion has gained much attention
over the past few years. Most of the current algorithms consider a KG as a
multidirectional labeled graph and lack the ability to capture the semantics
underlying the schematic information. In a separate development, a vast amount
of information has been captured within the Large Language Models (LLMs) which
has revolutionized the field of Artificial Intelligence. KGs could benefit from
these LLMs and vice versa. This vision paper discusses the existing algorithms
for KG completion based on the variations for generating KG embeddings. It
starts with discussing various KG completion algorithms such as transductive
and inductive link prediction and entity type prediction algorithms. It then
moves on to the algorithms utilizing type information within the KGs, LLMs, and
finally to algorithms capturing the semantics represented in different
description logic axioms. We conclude the paper with a critical reflection on
the current state of work in the community and give recommendations for future
directions.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2308.00081v4},
File          = {2308.00081v4.pdf}
}
@article{2308.13534v1,
Author        = {Ahtsham Zafar and Venkatesh Balavadhani Parthasarathy and Chan Le Van and Saad Shahid and Aafaq Iqbal khan and Arsalan Shahid},
Title         = {Building Trust in Conversational AI: A Comprehensive Review and Solution
  Architecture for Explainable, Privacy-Aware Systems using LLMs and Knowledge
  Graph},
Eprint        = {2308.13534v1},
DOI           = {10.3390/bdcc8060070},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Conversational AI systems have emerged as key enablers of human-like
interactions across diverse sectors. Nevertheless, the balance between
linguistic nuance and factual accuracy has proven elusive. In this paper, we
first introduce LLMXplorer, a comprehensive tool that provides an in-depth
review of over 150 Large Language Models (LLMs), elucidating their myriad
implications ranging from social and ethical to regulatory, as well as their
applicability across industries. Building on this foundation, we propose a
novel functional architecture that seamlessly integrates the structured
dynamics of Knowledge Graphs with the linguistic capabilities of LLMs.
Validated using real-world AI news data, our architecture adeptly blends
linguistic sophistication with factual rigour and further strengthens data
security through Role-Based Access Control. This research provides insights
into the evolving landscape of conversational AI, emphasizing the imperative
for systems that are efficient, transparent, and trustworthy.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.13534v1},
File          = {2308.13534v1.pdf}
}
@article{2311.09841v1,
Author        = {Tilahun Abedissa Taffa and Ricardo Usbeck},
Title         = {Leveraging LLMs in Scholarly Knowledge Graph Question Answering},
Eprint        = {2311.09841v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper presents a scholarly Knowledge Graph Question Answering (KGQA)
that answers bibliographic natural language questions by leveraging a large
language model (LLM) in a few-shot manner. The model initially identifies the
top-n similar training questions related to a given test question via a
BERT-based sentence encoder and retrieves their corresponding SPARQL. Using the
top-n similar question-SPARQL pairs as an example and the test question creates
a prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs
the SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and
returns an answer. Our system achieves an F1 score of 99.0%, on SciQA - one of
the Scholarly-QALD-23 challenge benchmarks.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.09841v1},
File          = {2311.09841v1.pdf}
}
@article{2407.12216v2,
Author        = {Garima Agrawal and Tharindu Kumarage and Zeyad Alghamdi and Huan Liu},
Title         = {Mindful-RAG: A Study of Points of Failure in Retrieval Augmented
  Generation},
Eprint        = {2407.12216v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Large Language Models (LLMs) are proficient at generating coherent and
contextually relevant text but face challenges when addressing
knowledge-intensive queries in domain-specific and factual question-answering
tasks. Retrieval-augmented generation (RAG) systems mitigate this by
incorporating external knowledge sources, such as structured knowledge graphs
(KGs). However, LLMs often struggle to produce accurate answers despite access
to KG-extracted information containing necessary facts. Our study investigates
this dilemma by analyzing error patterns in existing KG-based RAG methods and
identifying eight critical failure points. We observed that these errors
predominantly occur due to insufficient focus on discerning the question's
intent and adequately gathering relevant context from the knowledge graph
facts. Drawing on this analysis, we propose the Mindful-RAG approach, a
framework designed for intent-based and contextually aligned knowledge
retrieval. This method explicitly targets the identified failures and offers
improvements in the correctness and relevance of responses provided by LLMs,
representing a significant step forward from existing methods.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.12216v2},
File          = {2407.12216v2.pdf}
}
@article{2409.09010v1,
Author        = {Kanchan Shivashankar and Nadine Steinmetz},
Title         = {Contri(e)ve: Context + Retrieve for Scholarly Question Answering},
Eprint        = {2409.09010v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Scholarly communication is a rapid growing field containing a wealth of
knowledge. However, due to its unstructured and document format, it is
challenging to extract useful information from them through conventional
document retrieval methods. Scholarly knowledge graphs solve this problem, by
representing the documents in a semantic network, providing, hidden insights,
summaries and ease of accessibility through queries. Naturally, question
answering for scholarly graphs expands the accessibility to a wider audience.
But some of the knowledge in this domain is still presented as unstructured
text, thus requiring a hybrid solution for question answering systems. In this
paper, we present a two step solution using open source Large Language
Model(LLM): Llama3.1 for Scholarly-QALD dataset. Firstly, we extract the
context pertaining to the question from different structured and unstructured
data sources: DBLP, SemOpenAlex knowledge graphs and Wikipedia text. Secondly,
we implement prompt engineering to improve the information retrieval
performance of the LLM. Our approach achieved an F1 score of 40% and also
observed some anomalous responses from the LLM, that are discussed in the final
part of the paper.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.09010v1},
File          = {2409.09010v1.pdf}
}
@article{2408.04174v1,
Author        = {Khai Le-Duc and Quy-Anh Dang and Tan-Hanh Pham and Truong-Son Hy},
Title         = {wav2graph: A Framework for Supervised Learning Knowledge Graph from
  Speech},
Eprint        = {2408.04174v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs (KGs) enhance the performance of large language models
(LLMs) and search engines by providing structured, interconnected data that
improves reasoning and context-awareness. However, KGs only focus on text data,
thereby neglecting other modalities such as speech. In this work, we introduce
wav2graph, the first framework for supervised learning knowledge graph from
speech data. Our pipeline are straightforward: (1) constructing a KG based on
transcribed spoken utterances and a named entity database, (2) converting KG
into embedding vectors, and (3) training graph neural networks (GNNs) for node
classification and link prediction tasks. Through extensive experiments
conducted in inductive and transductive learning contexts using
state-of-the-art GNN models, we provide baseline results and error analysis for
node classification and link prediction tasks on human transcripts and
automatic speech recognition (ASR) transcripts, including evaluations using
both encoder-based and decoder-based node embeddings, as well as monolingual
and multilingual acoustic pre-trained models. All related code, data, and
models are published online.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.04174v1},
File          = {2408.04174v1.pdf}
}
@article{2408.08535v1,
Author        = {Rong-Ching Chang and Jiawei Zhang},
Title         = {CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for
  Advanced Retrieval-Augmented Generation in Fact-Checking},
Eprint        = {2408.08535v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG) systems, their effectiveness is often hindered by a lack of
integration with entity relationships and community structures, limiting their
ability to provide contextually rich and accurate information retrieval for
fact-checking. We introduce CommunityKG-RAG (Community Knowledge
Graph-Retrieval Augmented Generation), a novel zero-shot framework that
integrates community structures within Knowledge Graphs (KGs) with RAG systems
to enhance the fact-checking process. Capable of adapting to new domains and
queries without additional training, CommunityKG-RAG utilizes the multi-hop
nature of community structures within KGs to significantly improve the accuracy
and relevance of information retrieval. Our experimental results demonstrate
that CommunityKG-RAG outperforms traditional methods, representing a
significant advancement in fact-checking by offering a robust, scalable, and
efficient solution.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.08535v1},
File          = {2408.08535v1.pdf}
}
@article{2502.06075v1,
Author        = {Han Meng and Renwen Zhang and Ganyi Wang and Yitian Yang and Peinuan Qin and Jungup Lee and Yi-Chieh Lee},
Title         = {Deconstructing Depression Stigma: Integrating AI-driven Data Collection
  and Analysis with Causal Knowledge Graphs},
Eprint        = {2502.06075v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {Mental-illness stigma is a persistent social problem, hampering both
treatment-seeking and recovery. Accordingly, there is a pressing need to
understand it more clearly, but analyzing the relevant data is highly
labor-intensive. Therefore, we designed a chatbot to engage participants in
conversations; coded those conversations qualitatively with AI assistance; and,
based on those coding results, built causal knowledge graphs to decode stigma.
The results we obtained from 1,002 participants demonstrate that conversation
with our chatbot can elicit rich information about people's attitudes toward
depression, while our AI-assisted coding was strongly consistent with
human-expert coding. Our novel approach combining large language models (LLMs)
and causal knowledge graphs uncovered patterns in individual responses and
illustrated the interrelationships of psychological constructs in the dataset
as a whole. The paper also discusses these findings' implications for HCI
researchers in developing digital interventions, decomposing human
psychological constructs, and fostering inclusive attitudes.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.06075v1},
File          = {2502.06075v1.pdf}
}
@article{2502.06864v1,
Author        = {Xiangrong Zhu and Yuexiang Xie and Yi Liu and Yaliang Li and Wei Hu},
Title         = {Knowledge Graph-Guided Retrieval Augmented Generation},
Eprint        = {2502.06864v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Retrieval-augmented generation (RAG) has emerged as a promising technology
for addressing hallucination issues in the responses generated by large
language models (LLMs). Existing studies on RAG primarily focus on applying
semantic-based approaches to retrieve isolated relevant chunks, which ignore
their intrinsic relationships. In this paper, we propose a novel Knowledge
Graph-Guided Retrieval Augmented Generation (KG$^2$RAG) framework that utilizes
knowledge graphs (KGs) to provide fact-level relationships between chunks,
improving the diversity and coherence of the retrieved results. Specifically,
after performing a semantic-based retrieval to provide seed chunks, KG$^2$RAG
employs a KG-guided chunk expansion process and a KG-based chunk organization
process to deliver relevant and important knowledge in well-organized
paragraphs. Extensive experiments conducted on the HotpotQA dataset and its
variants demonstrate the advantages of KG$^2$RAG compared to existing RAG-based
approaches, in terms of both response quality and retrieval quality.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.06864v1},
File          = {2502.06864v1.pdf}
}
@article{2305.12307v3,
Author        = {Tanay Komarlu and Minhao Jiang and Xuan Wang and Jiawei Han},
Title         = {OntoType: Ontology-Guided and Pre-Trained Language Model Assisted
  Fine-Grained Entity Typing},
Eprint        = {2305.12307v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Fine-grained entity typing (FET), which assigns entities in text with
context-sensitive, fine-grained semantic types, is a basic but important task
for knowledge extraction from unstructured text. FET has been studied
extensively in natural language processing and typically relies on
human-annotated corpora for training, which is costly and difficult to scale.
Recent studies explore the utilization of pre-trained language models (PLMs) as
a knowledge base to generate rich and context-aware weak supervision for FET.
However, a PLM still requires direction and guidance to serve as a knowledge
base as they often generate a mixture of rough and fine-grained types, or
tokens unsuitable for typing. In this study, we vision that an ontology
provides a semantics-rich, hierarchical structure, which will help select the
best results generated by multiple PLM models and head words. Specifically, we
propose a novel annotation-free, ontology-guided FET method, OntoType, which
follows a type ontological structure, from coarse to fine, ensembles multiple
PLM prompting results to generate a set of type candidates, and refines its
type resolution, under the local context with a natural language inference
model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their
associated ontological structures demonstrate that our method outperforms the
state-of-the-art zero-shot fine-grained entity typing methods as well as a
typical LLM method, ChatGPT. Our error analysis shows that refinement of the
existing ontology structures will further improve fine-grained entity typing.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.12307v3},
File          = {2305.12307v3.pdf}
}
@article{2310.05163v3,
Author        = {Chengwen Qi and Bowen Li and Binyuan Hui and Bailin Wang and Jinyang Li and Jinwang Wu and Yuanjun Laili},
Title         = {An Investigation of LLMs' Inefficacy in Understanding Converse Relations},
Eprint        = {2310.05163v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have achieved remarkable success in many formal
language oriented tasks, such as structural data-to-text and semantic parsing.
However current benchmarks mostly follow the data distribution of the
pre-training data of LLMs. Therefore, a natural question rises that do LLMs
really understand the structured semantics of formal languages. In this paper,
we investigate this problem on a special case, converse binary relation. We
introduce a new benchmark ConvRe focusing on converse relations, which contains
17 relations and 1240 triples extracted from popular knowledge graph completion
datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are
formulated as multi-choice question answering to evaluate LLMs' ability to
determine the matching between relations and associated text. For the
evaluation protocol, apart from different prompting methods, we further
introduce variants to the test text and few-shot example text. We conduct
experiments on three popular LLM families and have observed various scaling
trends. The results suggest that LLMs often resort to shortcut learning and
still face challenges on our proposed benchmark.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.05163v3},
File          = {2310.05163v3.pdf}
}
@article{2308.08204v1,
Author        = {Jiabang He and Liu Jia and Lei Wang and Xiyao Li and Xing Xu},
Title         = {MoCoSA: Momentum Contrast for Knowledge Graph Completion with
  Structure-Augmented Pre-trained Language Models},
Eprint        = {2308.08204v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graph Completion (KGC) aims to conduct reasoning on the facts
within knowledge graphs and automatically infer missing links. Existing methods
can mainly be categorized into structure-based or description-based. On the one
hand, structure-based methods effectively represent relational facts in
knowledge graphs using entity embeddings. However, they struggle with
semantically rich real-world entities due to limited structural information and
fail to generalize to unseen entities. On the other hand, description-based
methods leverage pre-trained language models (PLMs) to understand textual
information. They exhibit strong robustness towards unseen entities. However,
they have difficulty with larger negative sampling and often lag behind
structure-based methods. To address these issues, in this paper, we propose
Momentum Contrast for knowledge graph completion with Structure-Augmented
pre-trained language models (MoCoSA), which allows the PLM to perceive the
structural information by the adaptable structure encoder. To improve learning
efficiency, we proposed momentum hard negative and intra-relation negative
sampling. Experimental results demonstrate that our approach achieves
state-of-the-art performance in terms of mean reciprocal rank (MRR), with
improvements of 2.5% on WN18RR and 21% on OpenBG500.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.08204v1},
File          = {2308.08204v1.pdf}
}
@article{2403.17532v1,
Author        = {Yilin Wang and Minghao Hu and Zhen Huang and Dongsheng Li and Dong Yang and Xicheng Lu},
Title         = {KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on
  Large Language Models for Knowledge Graph Completion},
Eprint        = {2403.17532v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The goal of knowledge graph completion (KGC) is to predict missing facts
among entities. Previous methods for KGC re-ranking are mostly built on
non-generative language models to obtain the probability of each candidate.
Recently, generative large language models (LLMs) have shown outstanding
performance on several tasks such as information extraction and dialog systems.
Leveraging them for KGC re-ranking is beneficial for leveraging the extensive
pre-trained knowledge and powerful generative capabilities. However, it may
encounter new problems when accomplishing the task, namely mismatch,
misordering and omission. To this end, we introduce KC-GenRe, a
knowledge-constrained generative re-ranking method based on LLMs for KGC. To
overcome the mismatch issue, we formulate the KGC re-ranking task as a
candidate identifier sorting generation problem implemented by generative LLMs.
To tackle the misordering issue, we develop a knowledge-guided interactive
training method that enhances the identification and ranking of candidates. To
address the omission issue, we design a knowledge-augmented constrained
inference method that enables contextual prompting and controlled generation,
so as to obtain valid rankings. Experimental results show that KG-GenRe
achieves state-of-the-art performance on four datasets, with gains of up to
6.7% and 7.7% in the MRR and Hits@1 metric compared to previous methods, and
9.0% and 11.1% compared to that without re-ranking. Extensive analysis
demonstrates the effectiveness of components in KG-GenRe.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.17532v1},
File          = {2403.17532v1.pdf}
}
@article{2501.11441v1,
Author        = {Maria Taboada and Diego Martinez and Mohammed Arideh and Rosa Mosquera},
Title         = {Ontology Matching with Large Language Models and Prioritized Depth-First
  Search},
Eprint        = {2501.11441v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Ontology matching (OM) plays a key role in enabling data interoperability and
knowledge sharing, but it remains challenging due to the need for large
training datasets and limited vocabulary processing in machine learning
approaches. Recently, methods based on Large Language Model (LLMs) have shown
great promise in OM, particularly through the use of a retrieve-then-prompt
pipeline. In this approach, relevant target entities are first retrieved and
then used to prompt the LLM to predict the final matches. Despite their
potential, these systems still present limited performance and high
computational overhead. To address these issues, we introduce MILA, a novel
approach that embeds a retrieve-identify-prompt pipeline within a prioritized
depth-first search (PDFS) strategy. This approach efficiently identifies a
large number of semantic correspondences with high accuracy, limiting LLM
requests to only the most borderline cases. We evaluated MILA using the
biomedical challenge proposed in the 2023 and 2024 editions of the Ontology
Alignment Evaluation Initiative. Our method achieved the highest F-Measure in
four of the five unsupervised tasks, outperforming state-of-the-art OM systems
by up to 17%. It also performed better than or comparable to the leading
supervised OM systems. MILA further exhibited task-agnostic performance,
remaining stable across all tasks and settings, while significantly reducing
LLM requests. These findings highlight that high-performance LLM-based OM can
be achieved through a combination of programmed (PDFS), learned (embedding
vectors), and prompting-based heuristics, without the need of domain-specific
heuristics or fine-tuning.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.11441v1},
File          = {2501.11441v1.pdf}
}
@article{1906.07241v2,
Author        = {Robert L. Logan IV and Nelson F. Liu and Matthew E. Peters and Matt Gardner and Sameer Singh},
Title         = {Barack's Wife Hillary: Using Knowledge-Graphs for Fact-Aware Language
  Modeling},
Eprint        = {1906.07241v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Modeling human language requires the ability to not only generate fluent text
but also encode factual knowledge. However, traditional language models are
only capable of remembering facts seen at training time, and often have
difficulty recalling them. To address this, we introduce the knowledge graph
language model (KGLM), a neural language model with mechanisms for selecting
and copying facts from a knowledge graph that are relevant to the context.
These mechanisms enable the model to render information it has never seen
before, as well as generate out-of-vocabulary tokens. We also introduce the
Linked WikiText-2 dataset, a corpus of annotated text aligned to the Wikidata
knowledge graph whose contents (roughly) match the popular WikiText-2
benchmark. In experiments, we demonstrate that the KGLM achieves significantly
better performance than a strong baseline language model. We additionally
compare different language model's ability to complete sentences requiring
factual knowledge, showing that the KGLM outperforms even very large language
models in generating facts.},
Year          = {2019},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1906.07241v2},
File          = {1906.07241v2.pdf}
}
@article{2410.01978v1,
Author        = {Arijit Khan and Tianxing Wu and Xi Chen},
Title         = {LLM+KG@VLDB'24 Workshop Summary},
Eprint        = {2410.01978v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {The unification of large language models (LLMs) and knowledge graphs (KGs)
has emerged as a hot topic. At the LLM+KG'24 workshop, held in conjunction with
VLDB 2024 in Guangzhou, China, one of the key themes explored was important
data management challenges and opportunities due to the effective interaction
between LLMs and KGs. This report outlines the major directions and approaches
presented by various speakers during the LLM+KG'24 workshop.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.01978v1},
File          = {2410.01978v1.pdf}
}
@article{2403.05266v3,
Author        = {Jio Oh and Soyeon Kim and Junseok Seo and Jindong Wang and Ruochen Xu and Xing Xie and Steven Euijong Whang},
Title         = {ERBench: An Entity-Relationship based Automatically Verifiable
  Hallucination Benchmark for Large Language Models},
Eprint        = {2403.05266v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have achieved unprecedented performances in
various applications, yet evaluating them is still challenging. Existing
benchmarks are either manually constructed or are automatic, but lack the
ability to evaluate the thought process of LLMs with arbitrary complexity. We
contend that utilizing existing relational databases based on the
entity-relationship (ER) model is a promising approach for constructing
benchmarks as they contain structured knowledge that can be used to question
LLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational
databases have integrity constraints that can be used to better construct
complex in-depth questions and verify answers: (1) functional dependencies can
be used to pinpoint critical keywords that an LLM must know to properly answer
a given question containing certain attribute values; and (2) foreign key
constraints can be used to join relations and construct multi-hop questions,
which can be arbitrarily long and used to debug intermediate answers. We thus
propose ERBench, which uses these integrity constraints to convert any database
into an LLM benchmark. ERBench supports continuous evaluation as databases
change, multimodal questions, and various prompt engineering techniques. In our
experiments, we construct LLM benchmarks using databases of multiple domains
and make an extensive comparison of contemporary LLMs. We show how ERBench can
properly evaluate any LLM by not only checking for answer correctness, but also
effectively verifying the rationales by looking for the right keywords.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.05266v3},
File          = {2403.05266v3.pdf}
}
@article{2402.03299v4,
Author        = {Haibo Jin and Ruoxi Chen and Andy Zhou and Yang Zhang and Haohan Wang},
Title         = {GUARD: Role-playing to Generate Natural-language Jailbreakings to Test
  Guideline Adherence of Large Language Models},
Eprint        = {2402.03299v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The discovery of "jailbreaks" to bypass safety filters of Large Language
Models (LLMs) and harmful responses have encouraged the community to implement
safety measures. One major safety measure is to proactively test the LLMs with
jailbreaks prior to the release. Therefore, such testing will require a method
that can generate jailbreaks massively and efficiently. In this paper, we
follow a novel yet intuitive strategy to generate jailbreaks in the style of
the human generation. We propose a role-playing system that assigns four
different roles to the user LLMs to collaborate on new jailbreaks. Furthermore,
we collect existing jailbreaks and split them into different independent
characteristics using clustering frequency and semantic patterns sentence by
sentence. We organize these characteristics into a knowledge graph, making them
more accessible and easier to retrieve. Our system of different roles will
leverage this knowledge graph to generate new jailbreaks, which have proved
effective in inducing LLMs to generate unethical or guideline-violating
responses. In addition, we also pioneer a setting in our system that will
automatically follow the government-issued guidelines to generate jailbreaks to
test whether LLMs follow the guidelines accordingly. We refer to our system as
GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have
empirically validated the effectiveness of GUARD on three cutting-edge
open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a
widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the
realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing
GUARD's versatility and contributing valuable insights for the development of
safer, more reliable LLM-based applications across diverse modalities.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.03299v4},
File          = {2402.03299v4.pdf}
}
@article{2311.17330v2,
Author        = {Karthik Soman and Peter W Rose and John H Morris and Rabia E Akbas and Brett Smith and Braian Peetoom and Catalina Villouta-Reyes and Gabriel Cerono and Yongmei Shi and Angela Rizk-Jackson and Sharat Israni and Charlotte A Nelson and Sui Huang and Sergio E Baranzini},
Title         = {Biomedical knowledge graph-optimized prompt generation for large
  language models},
Eprint        = {2311.17330v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) are being adopted at an unprecedented rate, yet
still face challenges in knowledge-intensive domains like biomedicine.
Solutions such as pre-training and domain-specific fine-tuning add substantial
computational overhead, requiring further domain expertise. Here, we introduce
a token-optimized and robust Knowledge Graph-based Retrieval Augmented
Generation (KG-RAG) framework by leveraging a massive biomedical KG (SPOKE)
with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to generate meaningful
biomedical text rooted in established knowledge. Compared to the existing RAG
technique for Knowledge Graphs, the proposed method utilizes minimal graph
schema for context extraction and uses embedding methods for context pruning.
This optimization in context extraction results in more than 50% reduction in
token consumption without compromising the accuracy, making a cost-effective
and robust RAG implementation on proprietary LLMs. KG-RAG consistently enhanced
the performance of LLMs across diverse biomedical prompts by generating
responses rooted in established knowledge, accompanied by accurate provenance
and statistical evidence (if available) to substantiate the claims. Further
benchmarking on human curated datasets, such as biomedical true/false and
multiple-choice questions (MCQ), showed a remarkable 71% boost in the
performance of the Llama-2 model on the challenging MCQ dataset, demonstrating
the framework's capacity to empower open-source models with fewer parameters
for domain specific questions. Furthermore, KG-RAG enhanced the performance of
proprietary GPT models, such as GPT-3.5 and GPT-4. In summary, the proposed
framework combines explicit and implicit knowledge of KG and LLM in a token
optimized fashion, thus enhancing the adaptability of general-purpose LLMs to
tackle domain-specific questions in a cost-effective fashion.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.17330v2},
File          = {2311.17330v2.pdf}
}
@article{2312.10904v2,
Author        = {Sabrina Toro and Anna V Anagnostopoulos and Sue Bello and Kai Blumberg and Rhiannon Cameron and Leigh Carmody and Alexander D Diehl and Damion Dooley and William Duncan and Petra Fey and Pascale Gaudet and Nomi L Harris and Marcin Joachimiak and Leila Kiani and Tiago Lubiana and Monica C Munoz-Torres and Shawn O'Neil and David Osumi-Sutherland and Aleix Puig and Justin P Reese and Leonore Reiser and Sofia Robb and Troy Ruemping and James Seager and Eric Sid and Ray Stefancsik and Magalie Weber and Valerie Wood and Melissa A Haendel and Christopher J Mungall},
Title         = {Dynamic Retrieval Augmented Generation of Ontologies using Artificial
  Intelligence (DRAGON-AI)},
Eprint        = {2312.10904v2},
DOI           = {10.1186/s13326-024-00320-3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Background: Ontologies are fundamental components of informatics
infrastructure in domains such as biomedical, environmental, and food sciences,
representing consensus knowledge in an accurate and computable form. However,
their construction and maintenance demand substantial resources and necessitate
substantial collaboration between domain experts, curators, and ontology
experts. We present Dynamic Retrieval Augmented Generation of Ontologies using
AI (DRAGON-AI), an ontology generation method employing Large Language Models
(LLMs) and Retrieval Augmented Generation (RAG). DRAGON-AI can generate textual
and logical ontology components, drawing from existing knowledge in multiple
ontologies and unstructured text sources.
  Results: We assessed performance of DRAGON-AI on de novo term construction
across ten diverse ontologies, making use of extensive manual evaluation of
results. Our method has high precision for relationship generation, but has
slightly lower precision than from logic-based reasoning. Our method is also
able to generate definitions deemed acceptable by expert evaluators, but these
scored worse than human-authored definitions. Notably, evaluators with the
highest level of confidence in a domain were better able to discern flaws in
AI-generated definitions. We also demonstrated the ability of DRAGON-AI to
incorporate natural language instructions in the form of GitHub issues.
  Conclusions: These findings suggest DRAGON-AI's potential to substantially
aid the manual ontology construction process. However, our results also
underscore the importance of having expert curators and ontology editors drive
the ontology generation process.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.10904v2},
File          = {2312.10904v2.pdf}
}
@article{2312.09948v1,
Author        = {Kaushik Roy and Vedant Khandelwal and Harshul Surana and Valerie Vera and Amit Sheth and Heather Heckman},
Title         = {GEAR-Up: Generative AI and External Knowledge-based Retrieval Upgrading
  Scholarly Article Searches for Systematic Reviews},
Eprint        = {2312.09948v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Systematic reviews (SRs) - the librarian-assisted literature survey of
scholarly articles takes time and requires significant human resources. Given
the ever-increasing volume of published studies, applying existing computing
and informatics technology can decrease this time and resource burden. Due to
the revolutionary advances in (1) Generative AI such as ChatGPT, and (2)
External knowledge-augmented information extraction efforts such as
Retrieval-Augmented Generation, In this work, we explore the use of techniques
from (1) and (2) for SR. We demonstrate a system that takes user queries,
performs query expansion to obtain enriched context (includes additional terms
and definitions by querying language models and knowledge graphs), and uses
this context to search for articles on scholarly databases to retrieve
articles. We perform qualitative evaluations of our system through comparison
against sentinel (ground truth) articles provided by an in-house librarian. The
demo can be found at: https://youtu.be/zMdP56GJ9mU.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.09948v1},
File          = {2312.09948v1.pdf}
}
@article{2310.05634v2,
Author        = {Xinze Li and Yixin Cao and Liangming Pan and Yubo Ma and Aixin Sun},
Title         = {Towards Verifiable Generation: A Benchmark for Knowledge-aware Language
  Model Attribution},
Eprint        = {2310.05634v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Although achieving great success, Large Language Models (LLMs) usually suffer
from unreliable hallucinations. Although language attribution can be a
potential solution, there are no suitable benchmarks and evaluation metrics to
attribute LLMs to structured knowledge. In this paper, we define a new task of
Knowledge-aware Language Model Attribution (KaLMA) that improves upon three
core concerns with conventional attributed LMs. First, we extend attribution
source from unstructured texts to Knowledge Graph (KG), whose rich structures
benefit both the attribution performance and working scenarios. Second, we
propose a new ``Conscious Incompetence" setting considering the incomplete
knowledge repository, where the model identifies the need for supporting
knowledge beyond the provided KG. Third, we propose a comprehensive automatic
evaluation metric encompassing text quality, citation quality, and text
citation alignment. To implement the above innovations, we build a dataset in
biography domain BioKaLMA via evolutionary question generation strategy, to
control the question complexity and necessary knowledge to the answer. For
evaluation, we develop a baseline solution and demonstrate the room for
improvement in LLMs' citation generation, emphasizing the importance of
incorporating the "Conscious Incompetence" setting, and the critical role of
retrieval accuracy.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.05634v2},
File          = {2310.05634v2.pdf}
}
@article{2409.13537v1,
Author        = {Shuting Yang and Zehui Liu and Wolfgang Mayer},
Title         = {ShizishanGPT: An Agricultural Large Language Model Integrating Tools and
  Resources},
Eprint        = {2409.13537v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent developments in large language models (LLMs) have led to significant
improvements in intelligent dialogue systems'ability to handle complex
inquiries. However, current LLMs still exhibit limitations in specialized
domain knowledge, particularly in technical fields such as agriculture. To
address this problem, we propose ShizishanGPT, an intelligent question
answering system for agriculture based on the Retrieval Augmented Generation
(RAG) framework and agent architecture. ShizishanGPT consists of five key
modules: including a generic GPT-4 based module for answering general
questions; a search engine module that compensates for the problem that the
large language model's own knowledge cannot be updated in a timely manner; an
agricultural knowledge graph module for providing domain facts; a retrieval
module which uses RAG to supplement domain knowledge; and an agricultural agent
module, which invokes specialized models for crop phenotype prediction, gene
expression analysis, and so on. We evaluated the ShizishanGPT using a dataset
containing 100 agricultural questions specially designed for this study. The
experimental results show that the tool significantly outperforms general LLMs
as it provides more accurate and detailed answers due to its modular design and
integration of different domain knowledge sources. Our source code, dataset,
and model weights are publicly available at https://github.com/Zaiwen/CropGPT.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.13537v1},
File          = {2409.13537v1.pdf}
}
@article{2410.09541v1,
Author        = {Jiachun Li and Pengfei Cao and Chenhao Wang and Zhuoran Jin and Yubo Chen and Kang Liu and Xiaojian Jiang and Jiexin Xu and Jun Zhao},
Title         = {LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language
  Model for Commonsense Reasoning},
Eprint        = {2410.09541v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) sometimes demonstrate poor performance on
knowledge-intensive tasks, commonsense reasoning is one of them. Researchers
typically address these issues by retrieving related knowledge from knowledge
graphs or employing self-enhancement methods to elicit knowledge in LLMs.
However, noisy knowledge and invalid reasoning issues hamper their ability to
answer questions accurately. To this end, we propose a novel method named
eliciting, filtering and integrating knowledge in large language model
(LINKED). In it, we design a reward model to filter out the noisy knowledge and
take the marginal consistent reasoning module to reduce invalid reasoning. With
our comprehensive experiments on two complex commonsense reasoning benchmarks,
our method outperforms SOTA baselines (up to 9.0% improvement of accuracy).
Besides, to measure the positive and negative impact of the injected knowledge,
we propose a new metric called effectiveness-preservation score for the
knowledge enhancement works. Finally, through extensive experiments, we conduct
an in-depth analysis and find many meaningful conclusions about LLMs in
commonsense reasoning tasks.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.09541v1},
File          = {2410.09541v1.pdf}
}
@article{2310.08279v3,
Author        = {Rui Yang and Jiahao Zhu and Jianping Man and Li Fang and Yi Zhou},
Title         = {Enhancing Text-based Knowledge Graph Completion with Zero-Shot Large
  Language Models: A Focus on Semantic Enhancement},
Eprint        = {2310.08279v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The design and development of text-based knowledge graph completion (KGC)
methods leveraging textual entity descriptions are at the forefront of
research. These methods involve advanced optimization techniques such as soft
prompts and contrastive learning to enhance KGC models. The effectiveness of
text-based methods largely hinges on the quality and richness of the training
data. Large language models (LLMs) can utilize straightforward prompts to alter
text data, thereby enabling data augmentation for KGC. Nevertheless, LLMs
typically demand substantial computational resources. To address these issues,
we introduce a framework termed constrained prompts for KGC (CP-KGC). This
CP-KGC framework designs prompts that adapt to different datasets to enhance
semantic richness. Additionally, CP-KGC employs a context constraint strategy
to effectively identify polysemous entities within KGC datasets. Through
extensive experimentation, we have verified the effectiveness of this
framework. Even after quantization, the LLM (Qwen-7B-Chat-int4) still enhances
the performance of text-based KGC methods \footnote{Code and datasets are
available at
\href{https://github.com/sjlmg/CP-KGC}{https://github.com/sjlmg/CP-KGC}}. This
study extends the performance limits of existing models and promotes further
integration of KGC with LLMs.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.08279v3},
File          = {2310.08279v3.pdf}
}
@article{2403.05881v3,
Author        = {Rui Yang and Haoran Liu and Edison Marrese-Taylor and Qingcheng Zeng and Yu He Ke and Wanxin Li and Lechao Cheng and Qingyu Chen and James Caverlee and Yutaka Matsuo and Irene Li},
Title         = {KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge
  Graphs and Ranking Techniques},
Eprint        = {2403.05881v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have demonstrated impressive generative
capabilities with the potential to innovate in medicine. However, the
application of LLMs in real clinical settings remains challenging due to the
lack of factual consistency in the generated content. In this work, we develop
an augmented LLM framework, KG-Rank, which leverages a medical knowledge graph
(KG) along with ranking and re-ranking techniques, to improve the factuality of
long-form question answering (QA) in the medical domain. Specifically, when
receiving a question, KG-Rank automatically identifies medical entities within
the question and retrieves the related triples from the medical KG to gather
factual information. Subsequently, KG-Rank innovatively applies multiple
ranking techniques to refine the ordering of these triples, providing more
relevant and precise information for LLM inference. To the best of our
knowledge, KG-Rank is the first application of KG combined with ranking models
in medical QA specifically for generating long answers. Evaluation on four
selected medical QA datasets demonstrates that KG-Rank achieves an improvement
of over 18% in ROUGE-L score. Additionally, we extend KG-Rank to open domains,
including law, business, music, and history, where it realizes a 14%
improvement in ROUGE-L score, indicating the effectiveness and great potential
of KG-Rank.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.05881v3},
File          = {2403.05881v3.pdf}
}
@article{2405.12442v1,
Author        = {Qingyao Li and Wei Xia and Kounianhua Du and Qiji Zhang and Weinan Zhang and Ruiming Tang and Yong Yu},
Title         = {Learning Structure and Knowledge Aware Representation with Large
  Language Models for Concept Recommendation},
Eprint        = {2405.12442v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Concept recommendation aims to suggest the next concept for learners to study
based on their knowledge states and the human knowledge system. While knowledge
states can be predicted using knowledge tracing models, previous approaches
have not effectively integrated the human knowledge system into the process of
designing these educational models. In the era of rapidly evolving Large
Language Models (LLMs), many fields have begun using LLMs to generate and
encode text, introducing external knowledge. However, integrating LLMs into
concept recommendation presents two urgent challenges: 1) How to construct text
for concepts that effectively incorporate the human knowledge system? 2) How to
adapt non-smooth, anisotropic text encodings effectively for concept
recommendation? In this paper, we propose a novel Structure and Knowledge Aware
Representation learning framework for concept Recommendation (SKarREC). We
leverage factual knowledge from LLMs as well as the precedence and succession
relationships between concepts obtained from the knowledge graph to construct
textual representations of concepts. Furthermore, we propose a graph-based
adapter to adapt anisotropic text embeddings to the concept recommendation
task. This adapter is pre-trained through contrastive learning on the knowledge
graph to get a smooth and structure-aware concept representation. Then, it's
fine-tuned through the recommendation task, forming a
text-to-knowledge-to-recommendation adaptation pipeline, which effectively
constructs a structure and knowledge-aware concept representation. Our method
does a better job than previous adapters in transforming text encodings for
application in concept recommendation. Extensive experiments on real-world
datasets demonstrate the effectiveness of the proposed approach.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.12442v1},
File          = {2405.12442v1.pdf}
}
@article{2412.13544v1,
Author        = {Zheng Hu and Zhe Li and Ziyun Jiao and Satoshi Nakagawa and Jiawen Deng and Shimin Cai and Tao Zhou and Fuji Ren},
Title         = {Bridging the User-side Knowledge Gap in Knowledge-aware Recommendations
  with Large Language Models},
Eprint        = {2412.13544v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {In recent years, knowledge graphs have been integrated into recommender
systems as item-side auxiliary information, enhancing recommendation accuracy.
However, constructing and integrating structural user-side knowledge remains a
significant challenge due to the improper granularity and inherent scarcity of
user-side features. Recent advancements in Large Language Models (LLMs) offer
the potential to bridge this gap by leveraging their human behavior
understanding and extensive real-world knowledge. Nevertheless, integrating
LLM-generated information into recommender systems presents challenges,
including the risk of noisy information and the need for additional knowledge
transfer. In this paper, we propose an LLM-based user-side knowledge inference
method alongside a carefully designed recommendation framework to address these
challenges. Our approach employs LLMs to infer user interests based on
historical behaviors, integrating this user-side information with item-side and
collaborative data to construct a hybrid structure: the Collaborative Interest
Knowledge Graph (CIKG). Furthermore, we propose a CIKG-based recommendation
framework that includes a user interest reconstruction module and a
cross-domain contrastive learning module to mitigate potential noise and
facilitate knowledge transfer. We conduct extensive experiments on three
real-world datasets to validate the effectiveness of our method. Our approach
achieves state-of-the-art performance compared to competitive baselines,
particularly for users with sparse interactions.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.13544v1},
File          = {2412.13544v1.pdf}
}
@article{2501.07766v1,
Author        = {Bingchen Liu and Xin Li},
Title         = {Large Language Models for Knowledge Graph Embedding Techniques, Methods,
  and Challenges: A Survey},
Eprint        = {2501.07766v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have attracted a lot of attention in various
fields due to their superior performance, aiming to train hundreds of millions
or more parameters on large amounts of text data to understand and generate
natural language. As the superior performance of LLMs becomes apparent, they
are increasingly being applied to knowledge graph embedding (KGE) related tasks
to improve the processing results. As a deep learning model in the field of
Natural Language Processing (NLP), it learns a large amount of textual data to
predict the next word or generate content related to a given text. However,
LLMs have recently been invoked to varying degrees in different types of KGE
related scenarios such as multi-modal KGE and open KGE according to their task
characteristics. In this paper, we investigate a wide range of approaches for
performing LLMs-related tasks in different types of KGE scenarios. To better
compare the various approaches, we summarize each KGE scenario in a
classification. In addition to the categorization methods, we provide a tabular
overview of the methods and their source code links for a more direct
comparison. In the article we also discuss the applications in which the
methods are mainly used and suggest several forward-looking directions for the
development of this new research area.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.07766v1},
File          = {2501.07766v1.pdf}
}
@article{2410.11531v1,
Author        = {Xinjie Zhao and Moritz Blum and Rui Yang and Boming Yang and Luis Márquez Carpintero and Mónica Pina-Navarro and Tony Wang and Xin Li and Huitao Li and Yanran Fu and Rongrong Wang and Juntao Zhang and Irene Li},
Title         = {AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based
  Chatbots Utilizing Private Data},
Eprint        = {2410.11531v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models~(LLMs) have demonstrated capabilities across various
applications but face challenges such as hallucination, limited reasoning
abilities, and factual inconsistencies, especially when tackling complex,
domain-specific tasks like question answering~(QA). While Knowledge
Graphs~(KGs) have been shown to help mitigate these issues, research on the
integration of LLMs with background KGs remains limited. In particular, user
accessibility and the flexibility of the underlying KG have not been thoroughly
explored. We introduce AGENTiGraph (Adaptive Generative ENgine for Task-based
Interaction and Graphical Representation), a platform for knowledge management
through natural language interaction. It integrates knowledge extraction,
integration, and real-time visualization. AGENTiGraph employs a multi-agent
architecture to dynamically interpret user intents, manage tasks, and integrate
new knowledge, ensuring adaptability to evolving user requirements and data
contexts. Our approach demonstrates superior performance in knowledge graph
interactions, particularly for complex domain-specific tasks. Experimental
results on a dataset of 3,500 test cases show AGENTiGraph significantly
outperforms state-of-the-art zero-shot baselines, achieving 95.12\% accuracy in
task classification and 90.45\% success rate in task execution. User studies
corroborate its effectiveness in real-world scenarios. To showcase versatility,
we extended AGENTiGraph to legislation and healthcare domains, constructing
specialized KGs capable of answering complex queries in legal and medical
contexts.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.11531v1},
File          = {2410.11531v1.pdf}
}
@article{2501.07078v1,
Author        = {Jiayang Wu and Wensheng Gan and Jiahao Zhang and Philip S. Yu},
Title         = {ADKGD: Anomaly Detection in Knowledge Graphs with Dual-Channel Training},
Eprint        = {2501.07078v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In the current development of large language models (LLMs), it is important
to ensure the accuracy and reliability of the underlying data sources. LLMs are
critical for various applications, but they often suffer from hallucinations
and inaccuracies due to knowledge gaps in the training data. Knowledge graphs
(KGs), as a powerful structural tool, could serve as a vital external
information source to mitigate the aforementioned issues. By providing a
structured and comprehensive understanding of real-world data, KGs enhance the
performance and reliability of LLMs. However, it is common that errors exist in
KGs while extracting triplets from unstructured data to construct KGs. This
could lead to degraded performance in downstream tasks such as
question-answering and recommender systems. Therefore, anomaly detection in KGs
is essential to identify and correct these errors. This paper presents an
anomaly detection algorithm in knowledge graphs with dual-channel learning
(ADKGD). ADKGD leverages a dual-channel learning approach to enhance
representation learning from both the entity-view and triplet-view
perspectives. Furthermore, using a cross-layer approach, our framework
integrates internal information aggregation and context information
aggregation. We introduce a kullback-leibler (KL)-loss component to improve the
accuracy of the scoring function between the dual channels. To evaluate ADKGD's
performance, we conduct empirical studies on three real-world KGs: WN18RR,
FB15K, and NELL-995. Experimental results demonstrate that ADKGD outperforms
the state-of-the-art anomaly detection algorithms. The source code and datasets
are publicly available at https://github.com/csjywu1/ADKGD.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.07078v1},
File          = {2501.07078v1.pdf}
}
@article{2502.01298v1,
Author        = {Marco Arazzi and Davide Ligari and Serena Nicolazzo and Antonino Nocera},
Title         = {Augmented Knowledge Graph Querying leveraging LLMs},
Eprint        = {2502.01298v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Adopting Knowledge Graphs (KGs) as a structured, semantic-oriented, data
representation model has significantly improved data integration, reasoning,
and querying capabilities across different domains. This is especially true in
modern scenarios such as Industry 5.0, in which the integration of data
produced by humans, smart devices, and production processes plays a crucial
role. However, the management, retrieval, and visualization of data from a KG
using formal query languages can be difficult for non-expert users due to their
technical complexity, thus limiting their usage inside industrial environments.
For this reason, we introduce SparqLLM, a framework that utilizes a
Retrieval-Augmented Generation (RAG) solution, to enhance the querying of
Knowledge Graphs (KGs). SparqLLM executes the Extract, Transform, and Load
(ETL) pipeline to construct KGs from raw data. It also features a natural
language interface powered by Large Language Models (LLMs) to enable automatic
SPARQL query generation. By integrating template-based methods as
retrieved-context for the LLM, SparqLLM enhances query reliability and reduces
semantic errors, ensuring more accurate and efficient KG interactions.
Moreover, to improve usability, the system incorporates a dynamic visualization
dashboard that adapts to the structure of the retrieved data, presenting the
query results in an intuitive format. Rigorous experimental evaluations
demonstrate that SparqLLM achieves high query accuracy, improved robustness,
and user-friendly interaction with KGs, establishing it as a scalable solution
to access semantic data.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.01298v1},
File          = {2502.01298v1.pdf}
}
@article{2502.05453v1,
Author        = {Hanqing Yang and Jingdi Chen and Marie Siew and Tania Lorido-Botran and Carlee Joe-Wong},
Title         = {LLM-Powered Decentralized Generative Agents with Adaptive Hierarchical
  Knowledge Graph for Cooperative Planning},
Eprint        = {2502.05453v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Developing intelligent agents for long-term cooperation in dynamic open-world
scenarios is a major challenge in multi-agent systems. Traditional Multi-agent
Reinforcement Learning (MARL) frameworks like centralized training
decentralized execution (CTDE) struggle with scalability and flexibility. They
require centralized long-term planning, which is difficult without custom
reward functions, and face challenges in processing multi-modal data. CTDE
approaches also assume fixed cooperation strategies, making them impractical in
dynamic environments where agents need to adapt and plan independently. To
address decentralized multi-agent cooperation, we propose Decentralized
Adaptive Knowledge Graph Memory and Structured Communication System (DAMCS) in
a novel Multi-agent Crafter environment. Our generative agents, powered by
Large Language Models (LLMs), are more scalable than traditional MARL agents by
leveraging external knowledge and language for long-term planning and
reasoning. Instead of fully sharing information from all past experiences,
DAMCS introduces a multi-modal memory system organized as a hierarchical
knowledge graph and a structured communication protocol to optimize agent
cooperation. This allows agents to reason from past interactions and share
relevant information efficiently. Experiments on novel multi-agent open-world
tasks show that DAMCS outperforms both MARL and LLM baselines in task
efficiency and collaboration. Compared to single-agent scenarios, the two-agent
scenario achieves the same goal with 63% fewer steps, and the six-agent
scenario with 74% fewer steps, highlighting the importance of adaptive memory
and structured communication in achieving long-term goals. We publicly release
our project at: https://happyeureka.github.io/damcs.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.05453v1},
File          = {2502.05453v1.pdf}
}
@article{2309.16134v1,
Author        = {Qing Huang and Zhenyu Wan and Zhenchang Xing and Changjing Wang and Jieshan Chen and Xiwei Xu and Qinghua Lu},
Title         = {Let's Chat to Find the APIs: Connecting Human, LLM and Knowledge Graph
  through AI Chain},
Eprint        = {2309.16134v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {API recommendation methods have evolved from literal and semantic keyword
matching to query expansion and query clarification. The latest query
clarification method is knowledge graph (KG)-based, but limitations include
out-of-vocabulary (OOV) failures and rigid question templates. To address these
limitations, we propose a novel knowledge-guided query clarification approach
for API recommendation that leverages a large language model (LLM) guided by
KG. We utilize the LLM as a neural knowledge base to overcome OOV failures,
generating fluent and appropriate clarification questions and options. We also
leverage the structured API knowledge and entity relationships stored in the KG
to filter out noise, and transfer the optimal clarification path from KG to the
LLM, increasing the efficiency of the clarification process. Our approach is
designed as an AI chain that consists of five steps, each handled by a separate
LLM call, to improve accuracy, efficiency, and fluency for query clarification
in API recommendation. We verify the usefulness of each unit in our AI chain,
which all received high scores close to a perfect 5. When compared to the
baselines, our approach shows a significant improvement in MRR, with a maximum
increase of 63.9% higher when the query statement is covered in KG and 37.2%
when it is not. Ablation experiments reveal that the guidance of knowledge in
the KG and the knowledge-guided pathfinding strategy are crucial for our
approach's performance, resulting in a 19.0% and 22.2% increase in MAP,
respectively. Our approach demonstrates a way to bridge the gap between KG and
LLM, effectively compensating for the strengths and weaknesses of both.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.16134v1},
File          = {2309.16134v1.pdf}
}
@article{2406.01238v3,
Author        = {Zixuan Dong and Baoyun Peng and Yufei Wang and Jia Fu and Xiaodong Wang and Yongxue Shan and Xin Zhou},
Title         = {EffiQA: Efficient Question-Answering with Strategic Multi-Model
  Collaboration on Knowledge Graphs},
Eprint        = {2406.01238v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {While large language models (LLMs) have shown remarkable capabilities in
natural language processing, they struggle with complex, multi-step reasoning
tasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs
and KGs either underutilize the reasoning abilities of LLMs or suffer from
prohibitive computational costs due to tight coupling. To address these
limitations, we propose a novel collaborative framework named EffiQA that can
strike a balance between performance and efficiency via an iterative paradigm.
EffiQA consists of three stages: global planning, efficient KG exploration, and
self-reflection. Specifically, EffiQA leverages the commonsense capability of
LLMs to explore potential reasoning pathways through global planning. Then, it
offloads semantic pruning to a small plug-in model for efficient KG
exploration. Finally, the exploration results are fed to LLMs for
self-reflection to further improve the global planning and efficient KG
exploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA's
effectiveness, achieving an optimal balance between reasoning accuracy and
computational costs. We hope the proposed new framework will pave the way for
efficient, knowledge-intensive querying by redefining the integration of LLMs
and KGs, fostering future research on knowledge-based question answering.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.01238v3},
File          = {2406.01238v3.pdf}
}
@article{2405.11346v2,
Author        = {Ritesh Chandra and Shashi Shekhar Kumar and Rushil Patra and Sonali Agarwal},
Title         = {Decision support system for Forest fire management using Ontology with
  Big Data and LLMs},
Eprint        = {2405.11346v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Forests are crucial for ecological balance, but wildfires, a major cause of
forest loss, pose significant risks. Fire weather indices, which assess
wildfire risk and predict resource demands, are vital. With the rise of sensor
networks in fields like healthcare and environmental monitoring, semantic
sensor networks are increasingly used to gather climatic data such as wind
speed, temperature, and humidity. However, processing these data streams to
determine fire weather indices presents challenges, underscoring the growing
importance of effective forest fire detection. This paper discusses using
Apache Spark for early forest fire detection, enhancing fire risk prediction
with meteorological and geographical data. Building on our previous development
of Semantic Sensor Network (SSN) ontologies and Semantic Web Rules Language
(SWRL) for managing forest fires in Monesterial Natural Park, we expanded SWRL
to improve a Decision Support System (DSS) using a Large Language Models (LLMs)
and Spark framework. We implemented real-time alerts with Spark streaming,
tailored to various fire scenarios, and validated our approach using ontology
metrics, query-based evaluations, LLMs score precision, F1 score, and recall
measures.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.11346v2},
File          = {2405.11346v2.pdf}
}
@article{2311.00287v2,
Author        = {Ran Xu and Hejie Cui and Yue Yu and Xuan Kan and Wenqi Shi and Yuchen Zhuang and Wei Jin and Joyce Ho and Carl Yang},
Title         = {Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data
  Generation with Large Language Models},
Eprint        = {2311.00287v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Clinical natural language processing requires methods that can address
domain-specific challenges, such as complex medical terminology and clinical
contexts. Recently, large language models (LLMs) have shown promise in this
domain. Yet, their direct deployment can lead to privacy issues and are
constrained by resources. To address this challenge, we delve into synthetic
clinical text generation using LLMs for clinical NLP tasks. We propose an
innovative, resource-efficient approach, ClinGen, which infuses knowledge into
the process. Our model involves clinical knowledge extraction and
context-informed LLM prompting. Both clinical topics and writing styles are
drawn from external domain-specific knowledge graphs and LLMs to guide data
generation. Our extensive empirical study across 7 clinical NLP tasks and 16
datasets reveals that ClinGen consistently enhances performance across various
tasks, effectively aligning the distribution of real datasets and significantly
enriching the diversity of generated training instances. Our code is available
at \url{https://github.com/ritaranx/ClinGen}.},
Year          = {2023},
Month         = {Nov},
Note          = {ACL 2024},
Url           = {http://arxiv.org/abs/2311.00287v2},
File          = {2311.00287v2.pdf}
}
@article{2312.01954v1,
Author        = {Andrea Papaluca and Daniel Krefl and Sergio Mendez Rodriguez and Artem Lensky and Hanna Suominen},
Title         = {Zero- and Few-Shots Knowledge Graph Triplet Extraction with Large
  Language Models},
Eprint        = {2312.01954v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this work, we tested the Triplet Extraction (TE) capabilities of a variety
of Large Language Models (LLMs) of different sizes in the Zero- and Few-Shots
settings. In detail, we proposed a pipeline that dynamically gathers contextual
information from a Knowledge Base (KB), both in the form of context triplets
and of (sentence, triplets) pairs as examples, and provides it to the LLM
through a prompt. The additional context allowed the LLMs to be competitive
with all the older fully trained baselines based on the Bidirectional Long
Short-Term Memory (BiLSTM) Network architecture. We further conducted a
detailed analysis of the quality of the gathered KB context, finding it to be
strongly correlated with the final TE performance of the model. In contrast,
the size of the model appeared to only logarithmically improve the TE
capabilities of the LLMs.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.01954v1},
File          = {2312.01954v1.pdf}
}
@article{2402.13055v2,
Author        = {Jie Ren and Qipeng Guo and Hang Yan and Dongrui Liu and Quanshi Zhang and Xipeng Qiu and Dahua Lin},
Title         = {Identifying Semantic Induction Heads to Understand In-Context Learning},
Eprint        = {2402.13055v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Although large language models (LLMs) have demonstrated remarkable
performance, the lack of transparency in their inference logic raises concerns
about their trustworthiness. To gain a better understanding of LLMs, we conduct
a detailed analysis of the operations of attention heads and aim to better
understand the in-context learning of LLMs. Specifically, we investigate
whether attention heads encode two types of relationships between tokens
present in natural languages: the syntactic dependency parsed from sentences
and the relation within knowledge graphs. We find that certain attention heads
exhibit a pattern where, when attending to head tokens, they recall tail tokens
and increase the output logits of those tail tokens. More crucially, the
formulation of such semantic induction heads has a close correlation with the
emergence of the in-context learning ability of language models. The study of
semantic attention heads advances our understanding of the intricate operations
of attention heads in transformers, and further provides new insights into the
in-context learning of LLMs.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.13055v2},
File          = {2402.13055v2.pdf}
}
@article{2406.11162v2,
Author        = {Dawulie Jinensibieke and Mieradilijiang Maimaiti and Wentao Xiao and Yuanhang Zheng and Xiaobo Wang},
Title         = {How Good are LLMs at Relation Extraction under Low-Resource Scenario?
  Comprehensive Evaluation},
Eprint        = {2406.11162v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Relation Extraction (RE) serves as a crucial technology for transforming
unstructured text into structured information, especially within the framework
of Knowledge Graph development. Its importance is emphasized by its essential
role in various downstream tasks. Besides the conventional RE methods which are
based on neural networks and pre-trained language models, large language models
(LLMs) are also utilized in the research field of RE. However, on low-resource
languages (LRLs), both conventional RE methods and LLM-based methods perform
poorly on RE due to the data scarcity issues. To this end, this paper
constructs low-resource relation extraction datasets in 10 LRLs in three
regions (Central Asia, Southeast Asia and Middle East). The corpora are
constructed by translating the original publicly available English RE datasets
(NYT10, FewRel and CrossRE) using an effective multilingual machine
translation. Then, we use the language perplexity (PPL) to filter out the
low-quality data from the translated datasets. Finally, we conduct an empirical
study and validate the performance of several open-source LLMs on these
generated LRL RE datasets.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.11162v2},
File          = {2406.11162v2.pdf}
}
@article{2408.01933v4,
Author        = {Bowen Wang and Jiuyang Chang and Yiming Qian and Guoxin Chen and Junhao Chen and Zhouqiang Jiang and Jiahao Zhang and Yuta Nakashima and Hajime Nagahara},
Title         = {DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language
  Models},
Eprint        = {2408.01933v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have recently showcased remarkable capabilities,
spanning a wide range of tasks and applications, including those in the medical
domain. Models like GPT-4 excel in medical question answering but may face
challenges in the lack of interpretability when handling complex tasks in real
clinical settings. We thus introduce the diagnostic reasoning dataset for
clinical notes (DiReCT), aiming at evaluating the reasoning ability and
interpretability of LLMs compared to human doctors. It contains 511 clinical
notes, each meticulously annotated by physicians, detailing the diagnostic
reasoning process from observations in a clinical note to the final diagnosis.
Additionally, a diagnostic knowledge graph is provided to offer essential
knowledge for reasoning, which may not be covered in the training data of
existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant
gap between their reasoning ability and that of human doctors, highlighting the
critical need for models that can reason effectively in real-world clinical
scenarios.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.01933v4},
File          = {2408.01933v4.pdf}
}
@article{2410.10853v1,
Author        = {Abdul Muqtadir and Hafiz Syed Muhammad Bilal and Ayesha Yousaf and Hafiz Farooq Ahmed and Jamil Hussain},
Title         = {Mitigating Hallucinations Using Ensemble of Knowledge Graph and Vector
  Store in Large Language Models to Enhance Mental Health Support},
Eprint        = {2410.10853v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This research work delves into the manifestation of hallucination within
Large Language Models (LLMs) and its consequential impacts on applications
within the domain of mental health. The primary objective is to discern
effective strategies for curtailing hallucinatory occurrences, thereby
bolstering the dependability and security of LLMs in facilitating mental health
interventions such as therapy, counseling, and the dissemination of pertinent
information. Through rigorous investigation and analysis, this study seeks to
elucidate the underlying mechanisms precipitating hallucinations in LLMs and
subsequently propose targeted interventions to alleviate their occurrence. By
addressing this critical issue, the research endeavors to foster a more robust
framework for the utilization of LLMs within mental health contexts, ensuring
their efficacy and reliability in aiding therapeutic processes and delivering
accurate information to individuals seeking mental health support.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.10853v1},
File          = {2410.10853v1.pdf}
}
@article{2410.12959v1,
Author        = {Hannah YoungEun An and Lenhart K. Schubert},
Title         = {Large Language Models as a Tool for Mining Object Knowledge},
Eprint        = {2410.12959v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Commonsense knowledge is essential for machines to reason about the world.
Large language models (LLMs) have demonstrated their ability to perform almost
human-like text generation. Despite this success, they fall short as
trustworthy intelligent systems, due to the opacity of the basis for their
answers and a tendency to confabulate facts when questioned about obscure
entities or technical domains. We hypothesize, however, that their general
knowledge about objects in the everyday world is largely sound. Based on that
hypothesis, this paper investigates LLMs' ability to formulate explicit
knowledge about common physical artifacts, focusing on their parts and
materials. Our work distinguishes between the substances that comprise an
entire object and those that constitute its parts$\unicode{x2014}$a previously
underexplored distinction in knowledge base construction. Using few-shot with
five in-context examples and zero-shot multi-step prompting, we produce a
repository of data on the parts and materials of about 2,300 objects and their
subtypes. Our evaluation demonstrates LLMs' coverage and soundness in
extracting knowledge. This contribution to knowledge mining should prove useful
to AI research on reasoning about object structure and composition and serve as
an explicit knowledge source (analogous to knowledge graphs) for LLMs
performing multi-hop question answering.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.12959v1},
File          = {2410.12959v1.pdf}
}
@article{2501.12221v1,
Author        = {Allard Oelen and Sören Auer},
Title         = {Leveraging Large Language Models for Realizing Truly Intelligent User
  Interfaces},
Eprint        = {2501.12221v1},
DOI           = {10.1145/3613905.3650949},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {The number of published scholarly articles is growing at a significant rate,
making scholarly knowledge organization increasingly important. Various
approaches have been proposed to organize scholarly information, including
describing scholarly knowledge semantically leveraging knowledge graphs.
Transforming unstructured knowledge, presented within articles, to structured
and semantically represented knowledge generally requires human intelligence
and labor since natural language processing methods alone typically do not
render sufficient precision and recall for many applications. With the recent
developments of Large Language Models (LLMs), it becomes increasingly possible
to provide truly intelligent user interfaces guiding humans in the
transformation process. We present an approach to integrate non-intrusive LLMs
guidance into existing user interfaces. More specifically, we integrate
LLM-supported user interface components into an existing scholarly knowledge
infrastructure. Additionally, we provide our experiences with LLM integration,
detailing best practices and obstacles. Finally, we evaluate the approach using
a small-scale user evaluation with domain experts.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.12221v1},
File          = {2501.12221v1.pdf}
}
@article{2010.00796v1,
Author        = {Donghan Yu and Chenguang Zhu and Yiming Yang and Michael Zeng},
Title         = {JAKET: Joint Pre-training of Knowledge Graph and Language Understanding},
Eprint        = {2010.00796v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs (KGs) contain rich information about world knowledge,
entities and relations. Thus, they can be great supplements to existing
pre-trained language models. However, it remains a challenge to efficiently
integrate information from KG into language modeling. And the understanding of
a knowledge graph requires related context. We propose a novel joint
pre-training framework, JAKET, to model both the knowledge graph and language.
The knowledge module and language module provide essential information to
mutually assist each other: the knowledge module produces embeddings for
entities in text while the language module generates context-aware initial
embeddings for entities and relations in the graph. Our design enables the
pre-trained model to easily adapt to unseen knowledge graphs in new domains.
Experimental results on several knowledge-aware NLP tasks show that our
proposed framework achieves superior performance by effectively leveraging
knowledge in language understanding.},
Year          = {2020},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2010.00796v1},
File          = {2010.00796v1.pdf}
}
@article{2209.08721v1,
Author        = {Jianhao Shen and Chenguang Wang and Linyuan Gong and Dawn Song},
Title         = {Joint Language Semantic and Structure Embedding for Knowledge Graph
  Completion},
Eprint        = {2209.08721v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The task of completing knowledge triplets has broad downstream applications.
Both structural and semantic information plays an important role in knowledge
graph completion. Unlike previous approaches that rely on either the structures
or semantics of the knowledge graphs, we propose to jointly embed the semantics
in the natural language description of the knowledge triplets with their
structure information. Our method embeds knowledge graphs for the completion
task via fine-tuning pre-trained language models with respect to a
probabilistic structured loss, where the forward pass of the language models
captures semantics and the loss reconstructs structures. Our extensive
experiments on a variety of knowledge graph benchmarks have demonstrated the
state-of-the-art performance of our method. We also show that our method can
significantly improve the performance in a low-resource regime, thanks to the
better use of semantics. The code and datasets are available at
https://github.com/pkusjh/LASS.},
Year          = {2022},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2209.08721v1},
File          = {2209.08721v1.pdf}
}
@article{2402.18312v2,
Author        = {Subhabrata Dutta and Joykirat Singh and Soumen Chakrabarti and Tanmoy Chakraborty},
Title         = {How to think step-by-step: A mechanistic understanding of
  chain-of-thought reasoning},
Eprint        = {2402.18312v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Despite superior reasoning prowess demonstrated by Large Language Models
(LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails
around the internal mechanisms of the models that facilitate CoT generation.
This work investigates the neural sub-structures within LLMs that manifest CoT
reasoning from a mechanistic point of view. From an analysis of Llama-2 7B
applied to multistep reasoning over fictional ontologies, we demonstrate that
LLMs deploy multiple parallel pathways of answer generation for step-by-step
reasoning. These parallel pathways provide sequential answers from the input
question context as well as the generated CoT. We observe a functional rift in
the middle layers of the LLM. Token representations in the initial half remain
strongly biased towards the pretraining prior, with the in-context prior taking
over in the later half. This internal phase shift manifests in different
functional components: attention heads that write the answer token appear in
the later half, attention heads that move information along ontological
relationships appear in the initial half, and so on. To the best of our
knowledge, this is the first attempt towards mechanistic investigation of CoT
reasoning in LLMs.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.18312v2},
File          = {2402.18312v2.pdf}
}
@article{2410.14211v3,
Author        = {Xingyu Tan and Xiaoyang Wang and Qing Liu and Xiwei Xu and Xin Yuan and Wenjie Zhang},
Title         = {Paths-over-Graph: Knowledge Graph Empowered Large Language Model
  Reasoning},
Eprint        = {2410.14211v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have achieved impressive results in various
tasks but struggle with hallucination problems and lack of relevant knowledge,
especially in deep complex reasoning and knowledge-intensive tasks. Knowledge
Graphs (KGs), which capture vast amounts of facts in a structured format, offer
a reliable source of knowledge for reasoning. However, existing KG-based LLM
reasoning methods face challenges like handling multi-hop reasoning,
multi-entity questions, and effectively utilizing graph structures. To address
these issues, we propose Paths-over-Graph (PoG), a novel method that enhances
LLM reasoning by integrating knowledge reasoning paths from KGs, improving the
interpretability and faithfulness of LLM outputs. PoG tackles multi-hop and
multi-entity questions through a three-phase dynamic multi-hop path
exploration, which combines the inherent knowledge of LLMs with factual
knowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant
information from the graph exploration first and introduces efficient
three-step pruning techniques that incorporate graph structures, LLM prompting,
and a pre-trained language model (e.g., SBERT) to effectively narrow down the
explored candidate paths. This ensures all reasoning paths contain highly
relevant information captured from KGs, making the reasoning faithful and
interpretable in problem-solving. PoG innovatively utilizes graph structure to
prune the irrelevant noise and represents the first method to implement
multi-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive
experiments on five benchmark KGQA datasets demonstrate PoG outperforms the
state-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an
average accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo
surpasses ToG with GPT-4 by up to 23.9%.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.14211v3},
File          = {2410.14211v3.pdf}
}
@article{2311.08117v1,
Author        = {Alessandro Bruno and Pier Luigi Mazzeo and Aladine Chetouani and Marouane Tliba and Mohamed Amine Kerkouri},
Title         = {Insights into Classifying and Mitigating LLMs' Hallucinations},
Eprint        = {2311.08117v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The widespread adoption of large language models (LLMs) across diverse AI
applications is proof of the outstanding achievements obtained in several
tasks, such as text mining, text generation, and question answering. However,
LLMs are not exempt from drawbacks. One of the most concerning aspects regards
the emerging problematic phenomena known as "Hallucinations". They manifest in
text generation systems, particularly in question-answering systems reliant on
LLMs, potentially resulting in false or misleading information propagation.
This paper delves into the underlying causes of AI hallucination and elucidates
its significance in artificial intelligence. In particular, Hallucination
classification is tackled over several tasks (Machine Translation, Question and
Answer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and
Visual Question Answer). Additionally, we explore potential strategies to
mitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our
research addresses this critical issue within the HeReFaNMi (Health-Related
Fake News Mitigation) project, generously supported by NGI Search, dedicated to
combating Health-Related Fake News dissemination on the Internet. This
endeavour represents a concerted effort to safeguard the integrity of
information dissemination in an age of evolving AI technologies.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.08117v1},
File          = {2311.08117v1.pdf}
}
@article{2407.13909v1,
Author        = {Rahul Ravi and Gouri Ginde and Jon Rokne},
Title         = {PRAGyan -- Connecting the Dots in Tweets},
Eprint        = {2407.13909v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {As social media platforms grow, understanding the underlying reasons behind
events and statements becomes crucial for businesses, policymakers, and
researchers. This research explores the integration of Knowledge Graphs (KGs)
with Large Language Models (LLMs) to perform causal analysis of tweets dataset.
The LLM aided analysis techniques often lack depth in uncovering the causes
driving observed effects. By leveraging KGs and LLMs, which encode rich
semantic relationships and temporal information, this study aims to uncover the
complex interplay of factors influencing causal dynamics and compare the
results obtained using GPT-3.5 Turbo. We employ a Retrieval-Augmented
Generation (RAG) model, utilizing a KG stored in a Neo4j (a.k.a PRAGyan) data
format, to retrieve relevant context for causal reasoning. Our approach
demonstrates that the KG-enhanced LLM RAG can provide improved results when
compared to the baseline LLM (GPT-3.5 Turbo) model as the source corpus
increases in size. Our qualitative analysis highlights the advantages of
combining KGs with LLMs for improved interpretability and actionable insights,
facilitating informed decision-making across various domains. Whereas,
quantitative analysis using metrics such as BLEU and cosine similarity show
that our approach outperforms the baseline by 10\%.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.13909v1},
File          = {2407.13909v1.pdf}
}
@article{2409.03440v1,
Author        = {Phuc Phan Van and Dat Nguyen Minh and An Dinh Ngoc and Huy Phan Thanh},
Title         = {Rx Strategist: Prescription Verification using LLM Agents System},
Eprint        = {2409.03440v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {To protect patient safety, modern pharmaceutical complexity demands strict
prescription verification. We offer a new approach - Rx Strategist - that makes
use of knowledge graphs and different search strategies to enhance the power of
Large Language Models (LLMs) inside an agentic framework. This multifaceted
technique allows for a multi-stage LLM pipeline and reliable information
retrieval from a custom-built active ingredient database. Different facets of
prescription verification, such as indication, dose, and possible drug
interactions, are covered in each stage of the pipeline. We alleviate the
drawbacks of monolithic LLM techniques by spreading reasoning over these
stages, improving correctness and reliability while reducing memory demands.
Our findings demonstrate that Rx Strategist surpasses many current LLMs,
achieving performance comparable to that of a highly experienced clinical
pharmacist. In the complicated world of modern medications, this combination of
LLMs with organized knowledge and sophisticated search methods presents a
viable avenue for reducing prescription errors and enhancing patient outcomes.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.03440v1},
File          = {2409.03440v1.pdf}
}
@article{2409.19979v3,
Author        = {Xinfeng Wang and Jin Cui and Fumiyo Fukumoto and Yoshimi Suzuki},
Title         = {Enhancing High-order Interaction Awareness in LLM-based Recommender
  Model},
Eprint        = {2409.19979v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Large language models (LLMs) have demonstrated prominent reasoning
capabilities in recommendation tasks by transforming them into text-generation
tasks. However, existing approaches either disregard or ineffectively model the
user-item high-order interactions. To this end, this paper presents an enhanced
LLM-based recommender (ELMRec). We enhance whole-word embeddings to
substantially enhance LLMs' interpretation of graph-constructed interactions
for recommendations, without requiring graph pre-training. This finding may
inspire endeavors to incorporate rich knowledge graphs into LLM-based
recommenders via whole-word embedding. We also found that LLMs often recommend
items based on users' earlier interactions rather than recent ones, and present
a reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods in
both direct and sequential recommendations.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.19979v3},
File          = {2409.19979v3.pdf}
}
@article{2411.09978v1,
Author        = {Yifan Zeng},
Title         = {HistoLens: An LLM-Powered Framework for Multi-Layered Analysis of
  Historical Texts -- A Case Application of Yantie Lun},
Eprint        = {2411.09978v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper proposes HistoLens, a multi-layered analysis framework for
historical texts based on Large Language Models (LLMs). Using the important
Western Han dynasty text "Yantie Lun" as a case study, we demonstrate the
framework's potential applications in historical research and education.
HistoLens integrates NLP technology (especially LLMs), including named entity
recognition, knowledge graph construction, and geographic information
visualization. The paper showcases how HistoLens explores Western Han culture
in "Yantie Lun" through multi-dimensional, visual, and quantitative methods,
focusing particularly on the influence of Confucian and Legalist thoughts on
political, economic, military, and ethnic. We also demonstrate how HistoLens
constructs a machine teaching scenario using LLMs for explainable analysis,
based on a dataset of Confucian and Legalist ideas extracted with LLM
assistance. This approach offers novel and diverse perspectives for studying
historical texts like "Yantie Lun" and provides new auxiliary tools for history
education. The framework aims to equip historians and learners with
LLM-assisted tools to facilitate in-depth, multi-layered analysis of historical
texts and foster innovation in historical education.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.09978v1},
File          = {2411.09978v1.pdf}
}
@article{2412.07189v1,
Author        = {Yang Xiong and Ruichen Zhang and Yinqiu Liu and Dusit Niyato and Zehui Xiong and Ying-Chang Liang and Shiwen Mao},
Title         = {When Graph Meets Retrieval Augmented Generation for Wireless Networks: A
  Tutorial and Case Study},
Eprint        = {2412.07189v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NI},
Abstract      = {The rapid development of next-generation networking technologies underscores
their transformative role in revolutionizing modern communication systems,
enabling faster, more reliable, and highly interconnected solutions. However,
such development has also brought challenges to network optimizations. Thanks
to the emergence of Large Language Models (LLMs) in recent years, tools
including Retrieval Augmented Generation (RAG) have been developed and applied
in various fields including networking, and have shown their effectiveness.
Taking one step further, the integration of knowledge graphs into RAG
frameworks further enhanced the performance of RAG in networking applications
such as Intent-Driven Networks (IDNs) and spectrum knowledge maps by providing
more contextually relevant responses through more accurate retrieval of related
network information. This paper introduces the RAG framework that integrates
knowledge graphs in its database and explores such framework's application in
networking. We begin by exploring RAG's applications in networking and the
limitations of conventional RAG and present the advantages that knowledge
graphs' structured knowledge representation brings to the retrieval and
generation processes. Next, we propose a detailed GraphRAG-based framework for
networking, including a step-by-step tutorial on its construction. Our
evaluation through a case study on channel gain prediction demonstrates
GraphRAG's enhanced capability in generating accurate, contextually rich
responses, surpassing traditional RAG models. Finally, we discuss key future
directions for applying knowledge-graphs-empowered RAG frameworks in
networking, including robust updates, mitigation of hallucination, and enhanced
security measures for networking applications.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.07189v1},
File          = {2412.07189v1.pdf}
}
@article{2307.05082v1,
Author        = {Oleksandr Palagin and Vladislav Kaverinskiy and Anna Litvin and Kyrylo Malakhov},
Title         = {OntoChatGPT Information System: Ontology-Driven Structured Prompts for
  ChatGPT Meta-Learning},
Eprint        = {2307.05082v1},
DOI           = {10.47839/ijc.22.2.3086},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This research presents a comprehensive methodology for utilizing an
ontology-driven structured prompts system in interplay with ChatGPT, a widely
used large language model (LLM). The study develops formal models, both
information and functional, and establishes the methodological foundations for
integrating ontology-driven prompts with ChatGPT's meta-learning capabilities.
The resulting productive triad comprises the methodological foundations,
advanced information technology, and the OntoChatGPT system, which collectively
enhance the effectiveness and performance of chatbot systems. The
implementation of this technology is demonstrated using the Ukrainian language
within the domain of rehabilitation. By applying the proposed methodology, the
OntoChatGPT system effectively extracts entities from contexts, classifies
them, and generates relevant responses. The study highlights the versatility of
the methodology, emphasizing its applicability not only to ChatGPT but also to
other chatbot systems based on LLMs, such as Google's Bard utilizing the PaLM 2
LLM. The underlying principles of meta-learning, structured prompts, and
ontology-driven information retrieval form the core of the proposed
methodology, enabling their adaptation and utilization in various LLM-based
systems. This versatile approach opens up new possibilities for NLP and
dialogue systems, empowering developers to enhance the performance and
functionality of chatbot systems across different domains and languages.},
Year          = {2023},
Month         = {Jul},
Note          = {International Journal of Computing (2023), 22(2), 170-183},
Url           = {http://arxiv.org/abs/2307.05082v1},
File          = {2307.05082v1.pdf}
}
@article{2409.11449v1,
Author        = {Yannis Vasilakis and Rachel Bittner and Johan Pauwels},
Title         = {Evaluation of pretrained language models on music understanding},
Eprint        = {2409.11449v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Music-text multimodal systems have enabled new approaches to Music
Information Research (MIR) applications such as audio-to-text and text-to-audio
retrieval, text-based song generation, and music captioning. Despite the
reported success, little effort has been put into evaluating the musical
knowledge of Large Language Models (LLM). In this paper, we demonstrate that
LLMs suffer from 1) prompt sensitivity, 2) inability to model negation (e.g.
'rock song without guitar'), and 3) sensitivity towards the presence of
specific words. We quantified these properties as a triplet-based accuracy,
evaluating the ability to model the relative similarity of labels in a
hierarchical ontology. We leveraged the Audioset ontology to generate triplets
consisting of an anchor, a positive (relevant) label, and a negative (less
relevant) label for the genre and instruments sub-tree. We evaluated the
triplet-based musical knowledge for six general-purpose Transformer-based
models. The triplets obtained through this methodology required filtering, as
some were difficult to judge and therefore relatively uninformative for
evaluation purposes. Despite the relatively high accuracy reported,
inconsistencies are evident in all six models, suggesting that off-the-shelf
LLMs need adaptation to music before use.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.11449v1},
File          = {2409.11449v1.pdf}
}
@article{2409.13746v1,
Author        = {Daniel B. Hier and Thanh Son Do and Tayo Obafemi-Ajayi},
Title         = {When Less Is Not More: Large Language Models Normalize Less-Frequent
  Terms with Lower Accuracy},
Eprint        = {2409.13746v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Term normalization is the process of mapping a term from free text to a
standardized concept and its machine-readable code in an ontology. Accurate
normalization of terms that capture phenotypic differences between patients and
diseases is critical to the success of precision medicine initiatives. A large
language model (LLM), such as GPT-4o, can normalize terms to the Human
Phenotype Ontology (HPO), but it may retrieve incorrect HPO IDs. Reported
accuracy rates for LLMs on these tasks may be inflated due to imbalanced test
datasets skewed towards high-frequency terms. In our study, using a
comprehensive dataset of 268,776 phenotype annotations for 12,655 diseases from
the HPO, GPT-4o achieved an accuracy of 13.1% in normalizing 11,225 unique
terms. However, the accuracy was unevenly distributed, with higher-frequency
and shorter terms normalized more accurately than lower-frequency and longer
terms. Feature importance analysis, using SHAP and permutation methods,
identified low-term frequency as the most significant predictor of
normalization errors. These findings suggest that training and evaluation
datasets for LLM-based term normalization should balance low- and
high-frequency terms to improve model performance, particularly for infrequent
terms critical to precision medicine.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.13746v1},
File          = {2409.13746v1.pdf}
}
@article{2405.15374v1,
Author        = {Runsong Jia and Bowen Zhang and Sergio J. Rodríguez Méndez and Pouya G. Omran},
Title         = {Leveraging Large Language Models for Semantic Query Processing in a
  Scholarly Knowledge Graph},
Eprint        = {2405.15374v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The proposed research aims to develop an innovative semantic query processing
system that enables users to obtain comprehensive information about research
works produced by Computer Science (CS) researchers at the Australian National
University (ANU). The system integrates Large Language Models (LLMs) with the
ANU Scholarly Knowledge Graph (ASKG), a structured repository of all
research-related artifacts produced at ANU in the CS field. Each artifact and
its parts are represented as textual nodes stored in a Knowledge Graph (KG).
  To address the limitations of traditional scholarly KG construction and
utilization methods, which often fail to capture fine-grained details, we
propose a novel framework that integrates the Deep Document Model (DDM) for
comprehensive document representation and the KG-enhanced Query Processing
(KGQP) for optimized complex query handling. DDM enables a fine-grained
representation of the hierarchical structure and semantic relationships within
academic papers, while KGQP leverages the KG structure to improve query
accuracy and efficiency with LLMs.
  By combining the ASKG with LLMs, our approach enhances knowledge utilization
and natural language understanding capabilities. The proposed system employs an
automatic LLM-SPARQL fusion to retrieve relevant facts and textual nodes from
the ASKG. Initial experiments demonstrate that our framework is superior to
baseline methods in terms of accuracy retrieval and query efficiency.
  We showcase the practical application of our framework in academic research
scenarios, highlighting its potential to revolutionize scholarly knowledge
management and discovery. This work empowers researchers to acquire and utilize
knowledge from documents more effectively and provides a foundation for
developing precise and reliable interactions with LLMs.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.15374v1},
File          = {2405.15374v1.pdf}
}
@article{2406.14307v2,
Author        = {Chao Hui Huang},
Title         = {QuST-LLM: Integrating Large Language Models for Comprehensive Spatial
  Transcriptomics Analysis},
Eprint        = {2406.14307v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.GN},
Abstract      = {In this paper, we introduce QuST-LLM, an innovative extension of QuPath that
utilizes the capabilities of large language models (LLMs) to analyze and
interpret spatial transcriptomics (ST) data. In addition to simplifying the
intricate and high-dimensional nature of ST data by offering a comprehensive
workflow that includes data loading, region selection, gene expression
analysis, and functional annotation, QuST-LLM employs LLMs to transform complex
ST data into understandable and detailed biological narratives based on gene
ontology annotations, thereby significantly improving the interpretability of
ST data. Consequently, users can interact with their own ST data using natural
language. Hence, QuST-LLM provides researchers with a potent functionality to
unravel the spatial and functional complexities of tissues, fostering novel
insights and advancements in biomedical research. QuST-LLM is a part of QuST
project. The source code is hosted on GitHub and documentation is available at
(https://github.com/huangch/qust).},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.14307v2},
File          = {2406.14307v2.pdf}
}
@article{2401.09395v6,
Author        = {Pengfei Hong and Navonil Majumder and Deepanway Ghosal and Somak Aditya and Rada Mihalcea and Soujanya Poria},
Title         = {Evaluating LLMs' Mathematical and Coding Competency through
  Ontology-guided Interventions},
Eprint        = {2401.09395v6},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent advancements in Large Language Models (LLMs) have showcased striking
results on existing logical reasoning benchmarks, with some models even
surpassing human performance. However, the true depth of their competencies and
robustness in reasoning tasks remains an open question. To this end, in this
paper, we focus on two popular reasoning tasks: arithmetic reasoning and code
generation. Particularly, we introduce (i) a general ontology of perturbations
for math and coding questions, (ii) a semi-automatic method to apply these
perturbations, and (iii) two datasets, GSMORE and HUMANEVAL-CORE, respectively,
of perturbed math and coding problems to probe LLM capabilities in numeric
reasoning and coding tasks. Through comprehensive evaluations of both
closed-source and open-source LLMs, we show a significant performance drop
across all the models against the perturbed questions, suggesting that the
current LLMs lack robust problem solving skills and structured reasoning
abilities in many areas, as defined by our ontology. We open-source the
datasets and source codes at: https://github.com/declare-lab/LLM-ReasoningTest.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.09395v6},
File          = {2401.09395v6.pdf}
}
@article{2409.00054v1,
Author        = {Yuting Hu and Dancheng Liu and Qingyun Wang and Charles Yu and Heng Ji and Jinjun Xiong},
Title         = {Automating Knowledge Discovery from Scientific Literature via LLMs: A
  Dual-Agent Approach with Progressive Ontology Prompting},
Eprint        = {2409.00054v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {To address the challenge of automating knowledge discovery from a vast volume
of literature, in this paper, we introduce a novel framework based on large
language models (LLMs) that combines a progressive ontology prompting (POP)
algorithm with a dual-agent system, named LLM-Duo, designed to enhance the
automation of knowledge extraction from scientific articles. The POP algorithm
utilizes a prioritized breadth-first search (BFS) across a predefined ontology
to generate structured prompt templates and action orders, thereby guiding LLMs
to discover knowledge in an automatic manner. Additionally, our LLM-Duo employs
two specialized LLM agents: an explorer and an evaluator. These two agents work
collaboratively and adversarially to enhance the reliability of the discovery
and annotation processes. Experiments demonstrate that our method outperforms
advanced baselines, enabling more accurate and complete annotations. To
validate the effectiveness of our method in real-world scenarios, we employ our
method in a case study of speech-language intervention discovery. Our method
identifies 2,421 interventions from 64,177 research articles in the
speech-language therapy domain. We curate these findings into a publicly
accessible intervention knowledge base that holds significant potential to
benefit the speech-language therapy community.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2409.00054v1},
File          = {2409.00054v1.pdf}
}
@article{2306.16092v2,
Author        = {Jiaxi Cui and Munan Ning and Zongjian Li and Bohua Chen and Yang Yan and Hao Li and Bin Ling and Yonghong Tian and Li Yuan},
Title         = {Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge
  Graph Enhanced Mixture-of-Experts Large Language Model},
Eprint        = {2306.16092v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {AI legal assistants based on Large Language Models (LLMs) can provide
accessible legal consulting services, but the hallucination problem poses
potential legal risks. This paper presents Chatlaw, an innovative legal
assistant utilizing a Mixture-of-Experts (MoE) model and a multi-agent system
to enhance the reliability and accuracy of AI-driven legal services. By
integrating knowledge graphs with artificial screening, we construct a
high-quality legal dataset to train the MoE model. This model utilizes
different experts to address various legal issues, optimizing the accuracy of
legal responses. Additionally, Standardized Operating Procedures (SOP), modeled
after real law firm workflows, significantly reduce errors and hallucinations
in legal services. Our MoE model outperforms GPT-4 in the Lawbench and Unified
Qualification Exam for Legal Professionals by 7.73% in accuracy and 11 points,
respectively, and also surpasses other models in multiple dimensions during
real-case consultations, demonstrating our robust capability for legal
consultation.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.16092v2},
File          = {2306.16092v2.pdf}
}
@article{2307.07312v2,
Author        = {Agnes Axelsson and Gabriel Skantze},
Title         = {Using Large Language Models for Zero-Shot Natural Language Generation
  from Knowledge Graphs},
Eprint        = {2307.07312v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In any system that uses structured knowledge graph (KG) data as its
underlying knowledge representation, KG-to-text generation is a useful tool for
turning parts of the graph data into text that can be understood by humans.
Recent work has shown that models that make use of pretraining on large amounts
of text data can perform well on the KG-to-text task even with relatively small
sets of training data on the specific graph-to-text task. In this paper, we
build on this concept by using large language models to perform zero-shot
generation based on nothing but the model's understanding of the triple
structure from what it can read. We show that ChatGPT achieves near
state-of-the-art performance on some measures of the WebNLG 2020 challenge, but
falls behind on others. Additionally, we compare factual, counter-factual and
fictional statements, and show that there is a significant connection between
what the LLM already knows about the data it is parsing and the quality of the
output text.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.07312v2},
File          = {2307.07312v2.pdf}
}
@article{2402.11323v1,
Author        = {Deepak Prasad and Mayur Pimpude and Alankar Alankar},
Title         = {Towards Development of Automated Knowledge Maps and Databases for
  Materials Engineering using Large Language Models},
Eprint        = {2402.11323v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {In this work a Large Language Model (LLM) based workflow is presented that
utilizes OpenAI ChatGPT model GPT-3.5-turbo-1106 and Google Gemini Pro model to
create summary of text, data and images from research articles. It is
demonstrated that by using a series of processing, the key information can be
arranged in tabular form and knowledge graphs to capture underlying concepts.
Our method offers efficiency and comprehension, enabling researchers to extract
insights more effectively. Evaluation based on a diverse Scientific Paper
Collection demonstrates our approach in facilitating discovery of knowledge.
This work contributes to accelerated material design by smart literature
review. The method has been tested based on various qualitative and
quantitative measures of gathered information. The ChatGPT model achieved an F1
score of 0.40 for an exact match (ROUGE-1, ROUGE-2) but an impressive 0.479 for
a relaxed match (ROUGE-L, ROUGE-Lsum) structural data format in performance
evaluation. The Google Gemini Pro outperforms ChatGPT with an F1 score of 0.50
for an exact match and 0.63 for a relaxed match. This method facilitates
high-throughput development of a database relevant to materials informatics.
For demonstration, an example of data extraction and knowledge graph formation
based on a manuscript about a titanium alloy is discussed.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.11323v1},
File          = {2402.11323v1.pdf}
}
@article{2404.00209v2,
Author        = {Cheng Jiayang and Lin Qiu and Chunkit Chan and Xin Liu and Yangqiu Song and Zheng Zhang},
Title         = {EventGround: Narrative Reasoning by Grounding to Eventuality-centric
  Knowledge Graphs},
Eprint        = {2404.00209v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Narrative reasoning relies on the understanding of eventualities in story
contexts, which requires a wealth of background world knowledge. To help
machines leverage such knowledge, existing solutions can be categorized into
two groups. Some focus on implicitly modeling eventuality knowledge by
pretraining language models (LMs) with eventuality-aware objectives. However,
this approach breaks down knowledge structures and lacks interpretability.
Others explicitly collect world knowledge of eventualities into structured
eventuality-centric knowledge graphs (KGs). However, existing research on
leveraging these knowledge sources for free-texts is limited. In this work, we
propose an initial comprehensive framework called EventGround, which aims to
tackle the problem of grounding free-texts to eventuality-centric KGs for
contextualized narrative reasoning. We identify two critical problems in this
direction: the event representation and sparsity problems. We provide simple
yet effective parsing and partial information extraction methods to tackle
these problems. Experimental results demonstrate that our approach consistently
outperforms baseline models when combined with graph neural network (GNN) or
large language model (LLM) based graph reasoning models. Our framework,
incorporating grounded knowledge, achieves state-of-the-art performance while
providing interpretable evidence.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2404.00209v2},
File          = {2404.00209v2.pdf}
}
@article{2407.19643v2,
Author        = {Yunsheng Wang and Songhao Chen and Kevin Jin},
Title         = {Prometheus Chatbot: Knowledge Graph Collaborative Large Language Model
  for Computer Components Recommendation},
Eprint        = {2407.19643v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graphs (KGs) are essential in applications such as network
alignment, question-answering, and recommender systems (RSs) since they offer
structured relational data that facilitate the inference of indirect
relationships. However, the development of KG-based RSs capable of processing
user inputs in natural language faces significant challenges. Firstly, natural
language processing units must effectively handle the ambiguity and variability
in human language to interpret user intents accurately. Secondly, the system
must precisely identify and link entities, like product names, to their
corresponding nodes in KGs. To overcome these challenges, supported by Lenovo,
we developed a novel chatbot called "Prometheus," which integrates a KG with a
large language model (LLM), specifically designed for recommending computer
components. This chatbot can accurately decode user requests and deliver
personalized recommendations derived from KGs, ensuring precise comprehension
and response to their computer setup needs.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.19643v2},
File          = {2407.19643v2.pdf}
}
@article{2407.19998v1,
Author        = {Huu Tan Mai and Cuong Xuan Chu and Heiko Paulheim},
Title         = {Do LLMs Really Adapt to Domains? An Ontology Learning Perspective},
Eprint        = {2407.19998v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have demonstrated unprecedented prowess across
various natural language processing tasks in various application domains.
Recent studies show that LLMs can be leveraged to perform lexical semantic
tasks, such as Knowledge Base Completion (KBC) or Ontology Learning (OL).
However, it has not effectively been verified whether their success is due to
their ability to reason over unstructured or semi-structured data, or their
effective learning of linguistic patterns and senses alone. This unresolved
question is particularly crucial when dealing with domain-specific data, where
the lexical senses and their meaning can completely differ from what a LLM has
learned during its training stage. This paper investigates the following
question: Do LLMs really adapt to domains and remain consistent in the
extraction of structured knowledge, or do they only learn lexical senses
instead of reasoning? To answer this question and, we devise a controlled
experiment setup that uses WordNet to synthesize parallel corpora, with English
and gibberish terms. We examine the differences in the outputs of LLMs for each
corpus in two OL tasks: relation extraction and taxonomy discovery. Empirical
results show that, while adapting to the gibberish corpora, off-the-shelf LLMs
do not consistently reason over semantic relationships between concepts, and
instead leverage senses and their frame. However, fine-tuning improves the
performance of LLMs on lexical semantic tasks even when the domain-specific
terms are arbitrary and unseen during pre-training, hinting at the
applicability of pre-trained LLMs for OL.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.19998v1},
File          = {2407.19998v1.pdf}
}
@article{2406.04989v1,
Author        = {Sondre Wold and Étienne Simon and Lucas Georges Gabriel Charpentier and Egor V. Kostylev and Erik Velldal and Lilja Øvrelid},
Title         = {Compositional Generalization with Grounded Language Models},
Eprint        = {2406.04989v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Grounded language models use external sources of information, such as
knowledge graphs, to meet some of the general challenges associated with
pre-training. By extending previous work on compositional generalization in
semantic parsing, we allow for a controlled evaluation of the degree to which
these models learn and generalize from patterns in knowledge graphs. We develop
a procedure for generating natural language questions paired with knowledge
graphs that targets different aspects of compositionality and further avoids
grounding the language models in information already encoded implicitly in
their weights. We evaluate existing methods for combining language models with
knowledge graphs and find them to struggle with generalization to sequences of
unseen lengths and to novel combinations of seen base components. While our
experimental results provide some insight into the expressive power of these
models, we hope our work and released datasets motivate future research on how
to better combine language models with structured knowledge representations.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.04989v1},
File          = {2406.04989v1.pdf}
}
@article{2410.05306v1,
Author        = {Tomas Bueno Momcilovic and Beat Buesser and Giulio Zizzo and Mark Purcell and Dian Balta},
Title         = {Towards Assuring EU AI Act Compliance and Adversarial Robustness of LLMs},
Eprint        = {2410.05306v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Large language models are prone to misuse and vulnerable to security threats,
raising significant safety and security concerns. The European Union's
Artificial Intelligence Act seeks to enforce AI robustness in certain contexts,
but faces implementation challenges due to the lack of standards, complexity of
LLMs and emerging security vulnerabilities. Our research introduces a framework
using ontologies, assurance cases, and factsheets to support engineers and
stakeholders in understanding and documenting AI system compliance and security
regarding adversarial robustness. This approach aims to ensure that LLMs adhere
to regulatory standards and are equipped to counter potential threats.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.05306v1},
File          = {2410.05306v1.pdf}
}
@article{2411.00028v2,
Author        = {Zhilun Zhou and Jingyang Fan and Yu Liu and Fengli Xu and Depeng Jin and Yong Li},
Title         = {Synergizing LLM Agents and Knowledge Graph for Socioeconomic Prediction
  in LBSN},
Eprint        = {2411.00028v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The fast development of location-based social networks (LBSNs) has led to
significant changes in society, resulting in popular studies of using LBSN data
for socioeconomic prediction, e.g., regional population and commercial activity
estimation. Existing studies design various graphs to model heterogeneous LBSN
data, and further apply graph representation learning methods for socioeconomic
prediction. However, these approaches heavily rely on heuristic ideas and
expertise to extract task-relevant knowledge from diverse data, which may not
be optimal for specific tasks. Additionally, they tend to overlook the inherent
relationships between different indicators, limiting the prediction accuracy.
Motivated by the remarkable abilities of large language models (LLMs) in
commonsense reasoning, embedding, and multi-agent collaboration, in this work,
we synergize LLM agents and knowledge graph for socioeconomic prediction. We
first construct a location-based knowledge graph (LBKG) to integrate
multi-sourced LBSN data. Then we leverage the reasoning power of LLM agent to
identify relevant meta-paths in the LBKG for each type of socioeconomic
prediction task, and design a semantic-guided attention module for knowledge
fusion with meta-paths. Moreover, we introduce a cross-task communication
mechanism to further enhance performance by enabling knowledge sharing across
tasks at both LLM agent and KG levels. On the one hand, the LLM agents for
different tasks collaborate to generate more diverse and comprehensive
meta-paths. On the other hand, the embeddings from different tasks are
adaptively merged for better socioeconomic prediction. Experiments on two
datasets demonstrate the effectiveness of the synergistic design between LLM
and KG, providing insights for information sharing across socioeconomic
prediction tasks.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2411.00028v2},
File          = {2411.00028v2.pdf}
}
@article{2305.09858v1,
Author        = {Jiao Chen and Luyi Ma and Xiaohan Li and Nikhil Thakurdesai and Jianpeng Xu and Jason H. D. Cho and Kaushiki Nag and Evren Korpeoglu and Sushant Kumar and Kannan Achan},
Title         = {Knowledge Graph Completion Models are Few-shot Learners: An Empirical
  Study of Relation Labeling in E-commerce with LLMs},
Eprint        = {2305.09858v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system
performance by providing structured information about entities and their
relationships, such as complementary or substitutable relations between
products or product types, which can be utilized in recommender systems.
However, relation labeling in KGs remains a challenging task due to the dynamic
nature of e-commerce domains and the associated cost of human labor. Recently,
breakthroughs in Large Language Models (LLMs) have shown surprising results in
numerous natural language processing tasks. In this paper, we conduct an
empirical study of LLMs for relation labeling in e-commerce KGs, investigating
their powerful learning capabilities in natural language and effectiveness in
predicting relations between product types with limited labeled data. We
evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets,
demonstrating their ability to achieve competitive performance compared to
humans on relation labeling tasks using just 1 to 5 labeled examples per
relation. Additionally, we experiment with different prompt engineering
techniques to examine their impact on model performance. Our results show that
LLMs significantly outperform existing KG completion models in relation
labeling for e-commerce KGs and exhibit performance strong enough to replace
human labeling.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.09858v1},
File          = {2305.09858v1.pdf}
}
@article{2305.10613v3,
Author        = {Dong-Ho Lee and Kian Ahrabian and Woojeong Jin and Fred Morstatter and Jay Pujara},
Title         = {Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context
  Learning},
Eprint        = {2305.10613v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Temporal knowledge graph (TKG) forecasting benchmarks challenge models to
predict future facts using knowledge of past facts. In this paper, we apply
large language models (LLMs) to these benchmarks using in-context learning
(ICL). We investigate whether and to what extent LLMs can be used for TKG
forecasting, especially without any fine-tuning or explicit modules for
capturing structural and temporal information. For our experiments, we present
a framework that converts relevant historical facts into prompts and generates
ranked predictions using token probabilities. Surprisingly, we observe that
LLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully
designed and trained for TKG forecasting. Our extensive evaluation presents
performances across several models and datasets with different characteristics,
compares alternative heuristics for preparing contextual information, and
contrasts to prominent TKG methods and simple frequency and recency baselines.
We also discover that using numerical indices instead of entity/relation names,
i.e., hiding semantic information, does not significantly affect the
performance ($\pm$0.4\% Hit@1). This shows that prior semantic knowledge is
unnecessary; instead, LLMs can leverage the existing patterns in the context to
achieve such performance. Our analysis also reveals that ICL enables LLMs to
learn irregular patterns from the historical context, going beyond simple
predictions based on common or recent information.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.10613v3},
File          = {2305.10613v3.pdf}
}
@article{2404.03623v2,
Author        = {Marco Bronzini and Carlo Nicolini and Bruno Lepri and Jacopo Staiano and Andrea Passerini},
Title         = {Unveiling LLMs: The Evolution of Latent Representations in a Dynamic
  Knowledge Graph},
Eprint        = {2404.03623v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) demonstrate an impressive capacity to recall a
vast range of factual knowledge. However, understanding their underlying
reasoning and internal mechanisms in exploiting this knowledge remains a key
research area. This work unveils the factual information an LLM represents
internally for sentence-level claim verification. We propose an end-to-end
framework to decode factual knowledge embedded in token representations from a
vector space to a set of ground predicates, showing its layer-wise evolution
using a dynamic knowledge graph. Our framework employs activation patching, a
vector-level technique that alters a token representation during inference, to
extract encoded knowledge. Accordingly, we neither rely on training nor
external models. Using factual and common-sense claims from two claim
verification datasets, we showcase interpretability analyses at local and
global levels. The local analysis highlights entity centrality in LLM
reasoning, from claim-related information and multi-hop reasoning to
representation errors causing erroneous evaluation. On the other hand, the
global reveals trends in the underlying evolution, such as word-based knowledge
evolving into claim-related facts. By interpreting semantics from LLM latent
representations and enabling graph-related analyses, this work enhances the
understanding of the factual knowledge resolution process.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.03623v2},
File          = {2404.03623v2.pdf}
}
@article{2404.03868v2,
Author        = {Bowen Zhang and Harold Soh},
Title         = {Extract, Define, Canonicalize: An LLM-based Framework for Knowledge
  Graph Construction},
Eprint        = {2404.03868v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this work, we are interested in automated methods for knowledge graph
creation (KGC) from input text. Progress on large language models (LLMs) has
prompted a series of recent works applying them to KGC, e.g., via zero/few-shot
prompting. Despite successes on small domain-specific datasets, these models
face difficulties scaling up to text common in many real-world applications. A
principal issue is that, in prior methods, the KG schema has to be included in
the LLM prompt to generate valid triplets; larger and more complex schemas
easily exceed the LLMs' context window length. Furthermore, there are scenarios
where a fixed pre-defined schema is not available and we would like the method
to construct a high-quality KG with a succinct self-generated schema. To
address these problems, we propose a three-phase framework named
Extract-Define-Canonicalize (EDC): open information extraction followed by
schema definition and post-hoc canonicalization. EDC is flexible in that it can
be applied to settings where a pre-defined target schema is available and when
it is not; in the latter case, it constructs a schema automatically and applies
self-canonicalization. To further improve performance, we introduce a trained
component that retrieves schema elements relevant to the input text; this
improves the LLMs' extraction performance in a retrieval-augmented
generation-like manner. We demonstrate on three KGC benchmarks that EDC is able
to extract high-quality triplets without any parameter tuning and with
significantly larger schemas compared to prior works. Code for EDC is available
at https://github.com/clear-nus/edc.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.03868v2},
File          = {2404.03868v2.pdf}
}
@article{2406.06027v1,
Author        = {Pranoy Panda and Ankush Agarwal and Chaitanya Devaguptapu and Manohar Kaul and Prathosh A P},
Title         = {HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question
  Answering using LLMs},
Eprint        = {2406.06027v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Given unstructured text, Large Language Models (LLMs) are adept at answering
simple (single-hop) questions. However, as the complexity of the questions
increase, the performance of LLMs degrade. We believe this is due to the
overhead associated with understanding the complex question followed by
filtering and aggregating unstructured information in the raw text. Recent
methods try to reduce this burden by integrating structured knowledge triples
into the raw text, aiming to provide a structured overview that simplifies
information processing. However, this simplistic approach is query-agnostic and
the extracted facts are ambiguous as they lack context. To address these
drawbacks and to enable LLMs to answer complex (multi-hop) questions with ease,
we propose to use a knowledge graph (KG) that is context-aware and is distilled
to contain query-relevant information. The use of our compressed distilled KG
as input to the LLM results in our method utilizing up to $67\%$ fewer tokens
to represent the query relevant information present in the supporting
documents, compared to the state-of-the-art (SoTA) method. Our experiments show
consistent improvements over the SoTA across several metrics (EM, F1,
BERTScore, and Human Eval) on two popular benchmark datasets (HotpotQA and
MuSiQue).},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.06027v1},
File          = {2406.06027v1.pdf}
}
@article{2410.04585v1,
Author        = {Pengcheng Jiang and Cao Xiao and Minhao Jiang and Parminder Bhatia and Taha Kass-Hout and Jimeng Sun and Jiawei Han},
Title         = {Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community
  Retrieval},
Eprint        = {2410.04585v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have demonstrated significant potential in
clinical decision support. Yet LLMs still suffer from hallucinations and lack
fine-grained contextual medical knowledge, limiting their high-stake healthcare
applications such as clinical diagnosis. Traditional retrieval-augmented
generation (RAG) methods attempt to address these limitations but frequently
retrieve sparse or irrelevant information, undermining prediction accuracy. We
introduce KARE, a novel framework that integrates knowledge graph (KG)
community-level retrieval with LLM reasoning to enhance healthcare predictions.
KARE constructs a comprehensive multi-source KG by integrating biomedical
databases, clinical literature, and LLM-generated insights, and organizes it
using hierarchical graph community detection and summarization for precise and
contextually relevant information retrieval. Our key innovations include: (1) a
dense medical knowledge structuring approach enabling accurate retrieval of
relevant information; (2) a dynamic knowledge retrieval mechanism that enriches
patient contexts with focused, multi-faceted medical insights; and (3) a
reasoning-enhanced prediction framework that leverages these enriched contexts
to produce both accurate and interpretable clinical predictions. Extensive
experiments demonstrate that KARE outperforms leading models by up to
10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and
readmission predictions. In addition to its impressive prediction accuracy, our
framework leverages the reasoning capabilities of LLMs, enhancing the
trustworthiness of clinical predictions.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.04585v1},
File          = {2410.04585v1.pdf}
}
@article{2502.02067v1,
Author        = {Shivam Singh and Karthik Swaminathan and Nabanita Dash and Ramandeep Singh and Snehasis Banerjee and Mohan Sridharan and Madhava Krishna},
Title         = {AdaptBot: Combining LLM with Knowledge Graphs and Human Input for
  Generic-to-Specific Task Decomposition and Knowledge Refinement},
Eprint        = {2502.02067v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Embodied agents assisting humans are often asked to complete a new task in a
new scenario. An agent preparing a particular dish in the kitchen based on a
known recipe may be asked to prepare a new dish or to perform cleaning tasks in
the storeroom. There may not be sufficient resources, e.g., time or labeled
examples, to train the agent for these new situations. Large Language Models
(LLMs) trained on considerable knowledge across many domains are able to
predict a sequence of abstract actions for such new tasks and scenarios,
although it may not be possible for the agent to execute this action sequence
due to task-, agent-, or domain-specific constraints. Our framework addresses
these challenges by leveraging the generic predictions provided by LLM and the
prior domain-specific knowledge encoded in a Knowledge Graph (KG), enabling an
agent to quickly adapt to new tasks and scenarios. The robot also solicits and
uses human input as needed to refine its existing knowledge. Based on
experimental evaluation over cooking and cleaning tasks in simulation domains,
we demonstrate that the interplay between LLM, KG, and human input leads to
substantial performance gains compared with just using the LLM output.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.02067v1},
File          = {2502.02067v1.pdf}
}
@article{2502.03283v1,
Author        = {Ben Liu and Jihai Zhang and Fangquan Lin and Cheng Yang and Min Peng and Wotao Yin},
Title         = {SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex
  Reasoning over Knowledge Graphs},
Eprint        = {2502.03283v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Recent advancements have highlighted that Large Language Models (LLMs) are
prone to hallucinations when solving complex reasoning problems, leading to
erroneous results. To tackle this issue, researchers incorporate Knowledge
Graphs (KGs) to improve the reasoning ability of LLMs. However, existing
methods face two limitations: 1) they typically assume that all answers to the
questions are contained in KGs, neglecting the incompleteness issue of KGs, and
2) they treat the KG as a static repository and overlook the implicit logical
reasoning structures inherent in KGs. In this paper, we introduce SymAgent, an
innovative neural-symbolic agent framework that achieves collaborative
augmentation between KGs and LLMs. We conceptualize KGs as dynamic environments
and transform complex reasoning tasks into a multi-step interactive process,
enabling KGs to participate deeply in the reasoning process. SymAgent consists
of two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages
LLM's inductive reasoning capability to extract symbolic rules from KGs,
guiding efficient question decomposition. The Agent-Executor autonomously
invokes predefined action tools to integrate information from KGs and external
documents, addressing the issues of KG incompleteness. Furthermore, we design a
self-learning framework comprising online exploration and offline iterative
policy updating phases, enabling the agent to automatically synthesize
reasoning trajectories and improve performance. Experimental results
demonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields
better or comparable performance compared to various strong baselines. Further
analysis reveals that our agent can identify missing triples, facilitating
automatic KG updates.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.03283v1},
File          = {2502.03283v1.pdf}
}
@article{2310.16421v1,
Author        = {Qinyong Wang and Zhenxiang Gao and Rong Xu},
Title         = {Graph Agent: Explicit Reasoning Agent for Graphs},
Eprint        = {2310.16421v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Graph embedding methods such as Graph Neural Networks (GNNs) and Graph
Transformers have contributed to the development of graph reasoning algorithms
for various tasks on knowledge graphs. However, the lack of interpretability
and explainability of graph embedding methods has limited their applicability
in scenarios requiring explicit reasoning. In this paper, we introduce the
Graph Agent (GA), an intelligent agent methodology of leveraging large language
models (LLMs), inductive-deductive reasoning modules, and long-term memory for
knowledge graph reasoning tasks. GA integrates aspects of symbolic reasoning
and existing graph embedding methods to provide an innovative approach for
complex graph reasoning tasks. By converting graph structures into textual
data, GA enables LLMs to process, reason, and provide predictions alongside
human-interpretable explanations. The effectiveness of the GA was evaluated on
node classification and link prediction tasks. Results showed that GA reached
state-of-the-art performance, demonstrating accuracy of 90.65%, 95.48%, and
89.32% on Cora, PubMed, and PrimeKG datasets, respectively. Compared to
existing GNN and transformer models, GA offered advantages of explicit
reasoning ability, free-of-training, easy adaption to various graph reasoning
tasks},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.16421v1},
File          = {2310.16421v1.pdf}
}
@article{2312.17269v2,
Author        = {Lihui Liu and Blaine Hill and Boxin Du and Fei Wang and Hanghang Tong},
Title         = {Conversational Question Answering with Reformulations over Knowledge
  Graph},
Eprint        = {2312.17269v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Conversational question answering (convQA) over knowledge graphs (KGs)
involves answering multi-turn natural language questions about information
contained in a KG. State-of-the-art methods of ConvQA often struggle with
inexplicit question-answer pairs. These inputs are easy for human beings to
understand given a conversation history, but hard for a machine to interpret,
which can degrade ConvQA performance. To address this problem, we propose a
reinforcement learning (RL) based model, CornNet, which utilizes question
reformulations generated by large language models (LLMs) to improve ConvQA
performance. CornNet adopts a teacher-student architecture where a teacher
model learns question representations using human writing reformulations, and a
student model to mimic the teacher model's output via reformulations generated
by LLMs. The learned question representation is then used by an RL model to
locate the correct answer in a KG. Extensive experimental results show that
CornNet outperforms state-of-the-art convQA models.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.17269v2},
File          = {2312.17269v2.pdf}
}
@article{2405.01649v3,
Author        = {Tianle Xia and Liang Ding and Guojia Wan and Yibing Zhan and Bo Du and Dacheng Tao},
Title         = {Improving Complex Reasoning over Knowledge Graph with Logic-Aware
  Curriculum Tuning},
Eprint        = {2405.01649v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Answering complex queries over incomplete knowledge graphs (KGs) is a
challenging job. Most previous works have focused on learning entity/relation
embeddings and simulating first-order logic operators with various neural
networks. However, they are bottlenecked by the inability to share world
knowledge to improve logical reasoning, thus resulting in suboptimal
performance. In this paper, we propose a complex reasoning schema over KG upon
large language models (LLMs), containing a curriculum-based logical-aware
instruction tuning framework, named LACT. Specifically, we augment the
arbitrary first-order logical queries via binary tree decomposition, to
stimulate the reasoning capability of LLMs. To address the difficulty gap among
different types of complex queries, we design a simple and flexible logic-aware
curriculum learning framework. Experiments across widely used datasets
demonstrate that LACT has substantial improvements~(brings an average +5.5% MRR
score) over advanced methods, achieving the new state-of-the-art. Our code and
model will be released at GitHub and huggingface soon.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.01649v3},
File          = {2405.01649v3.pdf}
}
@article{2406.01311v1,
Author        = {Sushant Gautam},
Title         = {FactGenius: Combining Zero-Shot Prompting and Fuzzy Relation Mining to
  Improve Fact Verification with Knowledge Graphs},
Eprint        = {2406.01311v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Fact-checking is a crucial natural language processing (NLP) task that
verifies the truthfulness of claims by considering reliable evidence.
Traditional methods are often limited by labour-intensive data curation and
rule-based approaches. In this paper, we present FactGenius, a novel method
that enhances fact-checking by combining zero-shot prompting of large language
models (LLMs) with fuzzy text matching on knowledge graphs (KGs). Leveraging
DBpedia, a structured linked data dataset derived from Wikipedia, FactGenius
refines LLM-generated connections using similarity measures to ensure accuracy.
The evaluation of FactGenius on the FactKG, a benchmark dataset for fact
verification, demonstrates that it significantly outperforms existing
baselines, particularly when fine-tuning RoBERTa as a classifier. The two-stage
approach of filtering and validating connections proves crucial, achieving
superior performance across various reasoning types and establishing FactGenius
as a promising tool for robust fact-checking. The code and materials are
available at https://github.com/SushantGautam/FactGenius.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.01311v1},
File          = {2406.01311v1.pdf}
}
@article{2406.07080v1,
Author        = {Haishuo Fang and Xiaodan Zhu and Iryna Gurevych},
Title         = {DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for
  Question Answering over Knowledge Graphs},
Eprint        = {2406.07080v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Answering Questions over Knowledge Graphs (KGQA) is key to well-functioning
autonomous language agents in various real-life applications. To improve the
neural-symbolic reasoning capabilities of language agents powered by Large
Language Models (LLMs) in KGQA, we propose the DecompositionAlignment-Reasoning
Agent (DARA) framework. DARA effectively parses questions into formal queries
through a dual mechanism: high-level iterative task decomposition and low-level
task grounding. Importantly, DARA can be efficiently trained with a small
number of high-quality reasoning trajectories. Our experimental results
demonstrate that DARA fine-tuned on LLMs (e.g. Llama-2-7B, Mistral) outperforms
both in-context learning-based agents with GPT-4 and alternative fine-tuned
agents, across different benchmarks in zero-shot evaluation, making such models
more accessible for real-life applications. We also show that DARA attains
performance comparable to state-of-the-art enumerating-and-ranking-based
methods for KGQA.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.07080v1},
File          = {2406.07080v1.pdf}
}
@article{2410.18251v1,
Author        = {Iman Saberi and Fatemeh Fard},
Title         = {Context-Augmented Code Generation Using Programming Knowledge Graphs},
Eprint        = {2410.18251v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly
improved code generation, but, they frequently face difficulties when dealing
with challenging and complex problems. Retrieval-Augmented Generation (RAG)
addresses this issue by retrieving and integrating external knowledge at the
inference time. However, retrieval models often fail to find most relevant
context, and generation models, with limited context capacity, can hallucinate
when given irrelevant data. We present a novel framework that leverages a
Programming Knowledge Graph (PKG) to semantically represent and retrieve code.
This approach enables fine-grained code retrieval by focusing on the most
relevant segments while reducing irrelevant context through a tree-pruning
technique. PKG is coupled with a re-ranking mechanism to reduce even more
hallucinations by selectively integrating non-RAG solutions. We propose two
retrieval approaches-block-wise and function-wise-based on the PKG, optimizing
context granularity. Evaluations on the HumanEval and MBPP benchmarks show our
method improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art
models by up to 34% on MBPP. Our contributions include PKG-based retrieval,
tree pruning to enhance retrieval precision, a re-ranking method for robust
solution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic
code augmentation with relevant comments and docstrings.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.18251v1},
File          = {2410.18251v1.pdf}
}
@article{2412.15272v1,
Author        = {Yuzheng Cai and Zhenyue Guo and Yiwen Pei and Wanrui Bian and Weiguo Zheng},
Title         = {SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven
  Retrieval-Augmented Generation},
Eprint        = {2412.15272v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent advancements in large language models (LLMs) have shown impressive
versatility across various tasks. To eliminate its hallucinations,
retrieval-augmented generation (RAG) has emerged as a powerful approach,
leveraging external knowledge sources like knowledge graphs (KGs). In this
paper, we study the task of KG-driven RAG and propose a novel Similar Graph
Enhanced Retrieval-Augmented Generation (SimGRAG) method. It effectively
addresses the challenge of aligning query texts and KG structures through a
two-stage process: (1) query-to-pattern, which uses an LLM to transform queries
into a desired graph pattern, and (2) pattern-to-subgraph, which quantifies the
alignment between the pattern and candidate subgraphs using a graph semantic
distance (GSD) metric. We also develop an optimized retrieval algorithm that
efficiently identifies the top-$k$ subgraphs within 1-second latency on a
10-million-scale KG. Extensive experiments show that SimGRAG outperforms
state-of-the-art KG-driven RAG methods in both question answering and fact
verification, offering superior plug-and-play usability and scalability.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.15272v1},
File          = {2412.15272v1.pdf}
}
@article{2501.15378v1,
Author        = {Manzong Huang and Chenyang Bu and Yi He and Xindong Wu},
Title         = {How to Mitigate Information Loss in Knowledge Graphs for GraphRAG:
  Leveraging Triple Context Restoration and Query-Driven Feedback},
Eprint        = {2501.15378v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge Graph (KG)-augmented Large Language Models (LLMs) have recently
propelled significant advances in complex reasoning tasks, thanks to their
broad domain knowledge and contextual awareness. Unfortunately, current methods
often assume KGs to be complete, which is impractical given the inherent
limitations of KG construction and the potential loss of contextual cues when
converting unstructured text into entity-relation triples. In response, this
paper proposes the Triple Context Restoration and Query-driven Feedback
(TCR-QF) framework, which reconstructs the textual context underlying each
triple to mitigate information loss, while dynamically refining the KG
structure by iteratively incorporating query-relevant missing knowledge.
Experiments on five benchmark question-answering datasets substantiate the
effectiveness of TCR-QF in KG and LLM integration, where itachieves a 29.1%
improvement in Exact Match and a 15.5% improvement in F1 over its
state-of-the-art GraphRAG competitors.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.15378v1},
File          = {2501.15378v1.pdf}
}
@article{2305.08804v1,
Author        = {Hanieh Khorashadizadeh and Nandana Mihindukulasooriya and Sanju Tiwari and Jinghua Groppe and Sven Groppe},
Title         = {Exploring In-Context Learning Capabilities of Foundation Models for
  Generating Knowledge Graphs from Text},
Eprint        = {2305.08804v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs can represent information about the real-world using
entities and their relations in a structured and semantically rich manner and
they enable a variety of downstream applications such as question-answering,
recommendation systems, semantic search, and advanced analytics. However, at
the moment, building a knowledge graph involves a lot of manual effort and thus
hinders their application in some situations and the automation of this process
might benefit especially for small organizations. Automatically generating
structured knowledge graphs from a large volume of natural language is still a
challenging task and the research on sub-tasks such as named entity extraction,
relation extraction, entity and relation linking, and knowledge graph
construction aims to improve the state of the art of automatic construction and
completion of knowledge graphs from text. The recent advancement of foundation
models with billions of parameters trained in a self-supervised manner with
large volumes of training data that can be adapted to a variety of downstream
tasks has helped to demonstrate high performance on a large range of Natural
Language Processing (NLP) tasks. In this context, one emerging paradigm is
in-context learning where a language model is used as it is with a prompt that
provides instructions and some examples to perform a task without changing the
parameters of the model using traditional approaches such as fine-tuning. This
way, no computing resources are needed for re-training/fine-tuning the models
and the engineering effort is minimal. Thus, it would be beneficial to utilize
such capabilities for generating knowledge graphs from text.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.08804v1},
File          = {2305.08804v1.pdf}
}
@article{2412.18443v1,
Author        = {Yuan Yuan and Yajing Xu and Wen Zhang},
Title         = {Is Large Language Model Good at Triple Set Prediction? An Empirical
  Study},
Eprint        = {2412.18443v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The core of the Knowledge Graph Completion (KGC) task is to predict and
complete the missing relations or nodes in a KG. Common KGC tasks are mostly
about inferring unknown elements with one or two elements being known in a
triple. In comparison, the Triple Set Prediction (TSP) task is a more realistic
knowledge graph completion task. It aims to predict all elements of unknown
triples based on the information from known triples. In recent years, large
language models (LLMs) have exhibited significant advancements in language
comprehension, demonstrating considerable potential for KGC tasks. However, the
potential of LLM on the TSP task has not yet to be investigated. Thus in this
paper we proposed a new framework to explore the strengths and limitations of
LLM in the TSP task. Specifically, the framework consists of LLM-based rule
mining and LLM-based triple set prediction. The relation list of KG embedded
within rich semantic information is first leveraged to prompt LLM in the
generation of rules. This process is both efficient and independent of
statistical information, making it easier to mine effective and realistic
rules. For each subgraph, the specified rule is applied in conjunction with the
relevant triples within that subgraph to guide the LLM in predicting the
missing triples. Subsequently, the predictions from all subgraphs are
consolidated to derive the complete set of predicted triples on KG. Finally,
the method is evaluated on the relatively complete CFamily dataset. The
experimental results indicate that when LLMs are required to adhere to a large
amount of factual knowledge to predict missing triples, significant
hallucinations occurs, leading to a noticeable decline in performance. To
further explore the causes of this phenomenon, this paper presents a
comprehensive analysis supported by a detailed case study.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.18443v1},
File          = {2412.18443v1.pdf}
}
@article{2403.10822v3,
Author        = {Simon A. Lee and Timothy Lindsey},
Title         = {Can Large Language Models abstract Medical Coded Language?},
Eprint        = {2403.10822v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have become a pivotal research area, potentially
making beneficial contributions in fields like healthcare where they can
streamline automated billing and decision support. However, the frequent use of
specialized coded languages like ICD-10, which are regularly updated and
deviate from natural language formats, presents potential challenges for LLMs
in creating accurate and meaningful latent representations. This raises
concerns among healthcare professionals about potential inaccuracies or
``hallucinations" that could result in the direct impact of a patient.
Therefore, this study evaluates whether large language models (LLMs) are aware
of medical code ontologies and can accurately generate names from these codes.
We assess the capabilities and limitations of both general and
biomedical-specific generative models, such as GPT, LLaMA-2, and Meditron,
focusing on their proficiency with domain-specific terminologies. While the
results indicate that LLMs struggle with coded language, we offer insights on
how to adapt these models to reason more effectively.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.10822v3},
File          = {2403.10822v3.pdf}
}
@article{2308.14321v1,
Author        = {Yanjun Gao and Ruizhe Li and John Caskey and Dmitriy Dligach and Timothy Miller and Matthew M. Churpek and Majid Afshar},
Title         = {Leveraging A Medical Knowledge Graph into Large Language Models for
  Diagnosis Prediction},
Eprint        = {2308.14321v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Electronic Health Records (EHRs) and routine documentation practices play a
vital role in patients' daily care, providing a holistic record of health,
diagnoses, and treatment. However, complex and verbose EHR narratives overload
healthcare providers, risking diagnostic inaccuracies. While Large Language
Models (LLMs) have showcased their potential in diverse language tasks, their
application in the healthcare arena needs to ensure the minimization of
diagnostic errors and the prevention of patient harm. In this paper, we outline
an innovative approach for augmenting the proficiency of LLMs in the realm of
automated diagnosis generation, achieved through the incorporation of a medical
knowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the
clinical diagnostic reasoning process. We derive the KG from the National
Library of Medicine's Unified Medical Language System (UMLS), a robust
repository of biomedical knowledge. Our method negates the need for
pre-training and instead leverages the KG as an auxiliary instrument aiding in
the interpretation and summarization of complex medical concepts. Using
real-world hospital datasets, our experimental results demonstrate that the
proposed approach of combining LLMs with KG has the potential to improve the
accuracy of automated diagnosis generation. More importantly, our approach
offers an explainable diagnostic pathway, edging us closer to the realization
of AI-augmented diagnostic decision support systems.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.14321v1},
File          = {2308.14321v1.pdf}
}
@article{2310.18951v1,
Author        = {Zhihang Yu and Shu Wang and Yunqiang Zhu and Zhiqiang Zou},
Title         = {A Multimodal Ecological Civilization Pattern Recommendation Method Based
  on Large Language Models and Knowledge Graph},
Eprint        = {2310.18951v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The Ecological Civilization Pattern Recommendation System (ECPRS) aims to
recommend suitable ecological civilization patterns for target regions,
promoting sustainable development and reducing regional disparities. However,
the current representative recommendation methods are not suitable for
recommending ecological civilization patterns in a geographical context. There
are two reasons for this. Firstly, regions have spatial heterogeneity, and the
(ECPRS)needs to consider factors like climate, topography, vegetation, etc., to
recommend civilization patterns adapted to specific ecological environments,
ensuring the feasibility and practicality of the recommendations. Secondly, the
abstract features of the ecological civilization patterns in the real world
have not been fully utilized., resulting in poor richness in their embedding
representations and consequently, lower performance of the recommendation
system. Considering these limitations, we propose the ECPR-MML method.
Initially, based on the novel method UGPIG, we construct a knowledge graph to
extract regional representations incorporating spatial heterogeneity features.
Following that, inspired by the significant progress made by Large Language
Models (LLMs) in the field of Natural Language Processing (NLP), we employ
Large LLMs to generate multimodal features for ecological civilization patterns
in the form of text and images. We extract and integrate these multimodal
features to obtain semantically rich representations of ecological
civilization. Through extensive experiments, we validate the performance of our
ECPR-MML model. Our results show that F1@5 is 2.11% higher compared to
state-of-the-art models, 2.02% higher than NGCF, and 1.16% higher than UGPIG.
Furthermore, multimodal data can indeed enhance recommendation performance.
However, the data generated by LLM is not as effective as real data to a
certain extent.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.18951v1},
File          = {2310.18951v1.pdf}
}
@article{2408.05948v1,
Author        = {Ronak Pradeep and Daniel Lee and Ali Mousavi and Jeff Pound and Yisi Sang and Jimmy Lin and Ihab Ilyas and Saloni Potdar and Mostafa Arefiyan and Yunyao Li},
Title         = {ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge
  Graph QA datasets with Large Language Models},
Eprint        = {2408.05948v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The rapid advancement of Large Language Models (LLMs) and conversational
assistants necessitates dynamic, scalable, and configurable conversational
datasets for training and evaluation. These datasets must accommodate diverse
user interaction modes, including text and voice, each presenting unique
modeling challenges. Knowledge Graphs (KGs), with their structured and evolving
nature, offer an ideal foundation for current and precise knowledge. Although
human-curated KG-based conversational datasets exist, they struggle to keep
pace with the rapidly changing user information needs. We present ConvKGYarn, a
scalable method for generating up-to-date and configurable conversational KGQA
datasets. Qualitative psychometric analyses confirm our method can generate
high-quality datasets rivaling a popular conversational KGQA dataset while
offering it at scale and covering a wide range of human-interaction
configurations. We showcase its utility by testing LLMs on diverse
conversations - exploring model behavior on conversational KGQA sets with
different configurations grounded in the same KG fact set. Our results
highlight the ability of ConvKGYarn to improve KGQA foundations and evaluate
parametric knowledge of LLMs, thus offering a robust solution to the constantly
evolving landscape of conversational assistants.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.05948v1},
File          = {2408.05948v1.pdf}
}
@article{2410.12229v2,
Author        = {Ziqiang Cui and Yunpeng Weng and Xing Tang and Fuyuan Lyu and Dugang Liu and Xiuqiang He and Chen Ma},
Title         = {Comprehending Knowledge Graphs with Large Language Models for
  Recommender Systems},
Eprint        = {2410.12229v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {In recent years, the introduction of knowledge graphs (KGs) has significantly
advanced recommender systems by facilitating the discovery of potential
associations between items. However, existing methods still face several
limitations. First, most KGs suffer from missing facts or limited scopes.
Second, existing methods convert textual information in KGs into IDs, resulting
in the loss of natural semantic connections between different items. Third,
existing methods struggle to capture high-order connections in the global KG.
To address these limitations, we propose a novel method called CoLaKG, which
leverages large language models (LLMs) to improve KG-based recommendations. The
extensive world knowledge and remarkable reasoning capabilities of LLMs enable
our method to supplement missing facts in KGs. Additionally, their powerful
text understanding abilities allow for better utilization of semantic
information. Specifically, CoLaKG extracts useful information from the KG at
both local and global levels. By employing item-centered subgraph extraction
and prompt engineering, it accurately captures the local KG. Subsequently,
through retrieval-based neighbor enhancement, it supplements the current item
by capturing related items from the entire KG, thereby effectively utilizing
global information. The local and global information extracted by the LLM are
effectively integrated into the recommendation model through a representation
fusion module and a retrieval-augmented representation learning module,
respectively, thereby improving recommendation performance. Extensive
experiments on four real-world datasets demonstrate the superiority of our
method.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.12229v2},
File          = {2410.12229v2.pdf}
}
@article{2312.10967v3,
Author        = {Zhangchi Qiu and Ye Tao and Shirui Pan and Alan Wee-Chung Liew},
Title         = {Knowledge Graphs and Pre-trained Language Models enhanced Representation
  Learning for Conversational Recommender Systems},
Eprint        = {2312.10967v3},
DOI           = {10.1109/TNNLS.2024.3395334},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Conversational recommender systems (CRS) utilize natural language
interactions and dialogue history to infer user preferences and provide
accurate recommendations. Due to the limited conversation context and
background knowledge, existing CRSs rely on external sources such as knowledge
graphs to enrich the context and model entities based on their inter-relations.
However, these methods ignore the rich intrinsic information within entities.
To address this, we introduce the Knowledge-Enhanced Entity Representation
Learning (KERL) framework, which leverages both the knowledge graph and a
pre-trained language model to improve the semantic understanding of entities
for CRS. In our KERL framework, entity textual descriptions are encoded via a
pre-trained language model, while a knowledge graph helps reinforce the
representation of these entities. We also employ positional encoding to
effectively capture the temporal information of entities in a conversation. The
enhanced entity representation is then used to develop a recommender component
that fuses both entity and contextual representations for more informed
recommendations, as well as a dialogue component that generates informative
entity-related information in the response text. A high-quality knowledge graph
with aligned entity descriptions is constructed to facilitate our study, namely
the Wiki Movie Knowledge Graph (WikiMKG). The experimental results show that
KERL achieves state-of-the-art results in both recommendation and response
generation tasks.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.10967v3},
File          = {2312.10967v3.pdf}
}
@article{2404.17809v1,
Author        = {Guozheng Li and Peng Wang and Wenjun Ke and Yikai Guo and Ke Ji and Ziyu Shang and Jiajun Liu and Zijie Xu},
Title         = {Recall, Retrieve and Reason: Towards Better In-Context Relation
  Extraction},
Eprint        = {2404.17809v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Relation extraction (RE) aims to identify relations between entities
mentioned in texts. Although large language models (LLMs) have demonstrated
impressive in-context learning (ICL) abilities in various tasks, they still
suffer from poor performances compared to most supervised fine-tuned RE
methods. Utilizing ICL for RE with LLMs encounters two challenges: (1)
retrieving good demonstrations from training examples, and (2) enabling LLMs
exhibit strong ICL abilities in RE. On the one hand, retrieving good
demonstrations is a non-trivial process in RE, which easily results in low
relevance regarding entities and relations. On the other hand, ICL with an LLM
achieves poor performance in RE while RE is different from language modeling in
nature or the LLM is not large enough. In this work, we propose a novel
recall-retrieve-reason RE framework that synergizes LLMs with retrieval corpora
(training examples) to enable relevant retrieving and reliable in-context
reasoning. Specifically, we distill the consistently ontological knowledge from
training datasets to let LLMs generate relevant entity pairs grounded by
retrieval corpora as valid queries. These entity pairs are then used to
retrieve relevant training examples from the retrieval corpora as
demonstrations for LLMs to conduct better ICL via instruction tuning. Extensive
experiments on different LLMs and RE datasets demonstrate that our method
generates relevant and valid entity pairs and boosts ICL abilities of LLMs,
achieving competitive or new state-of-the-art performance on sentence-level RE
compared to previous supervised fine-tuning methods and ICL-based methods.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.17809v1},
File          = {2404.17809v1.pdf}
}
@article{2311.02956v1,
Author        = {Yunlong Chen and Yaming Zhang and Jianfei Yu and Li Yang and Rui Xia},
Title         = {In-Context Learning for Knowledge Base Question Answering for Unmanned
  Systems based on Large Language Models},
Eprint        = {2311.02956v1},
DOI           = {10.1007/978-981-99-7224-1_26},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Base Question Answering (KBQA) aims to answer factoid questions
based on knowledge bases. However, generating the most appropriate knowledge
base query code based on Natural Language Questions (NLQ) poses a significant
challenge in KBQA. In this work, we focus on the CCKS2023 Competition of
Question Answering with Knowledge Graph Inference for Unmanned Systems.
Inspired by the recent success of large language models (LLMs) like ChatGPT and
GPT-3 in many QA tasks, we propose a ChatGPT-based Cypher Query Language (CQL)
generation framework to generate the most appropriate CQL based on the given
NLQ. Our generative framework contains six parts: an auxiliary model predicting
the syntax-related information of CQL based on the given NLQ, a proper noun
matcher extracting proper nouns from the given NLQ, a demonstration example
selector retrieving similar examples of the input sample, a prompt constructor
designing the input template of ChatGPT, a ChatGPT-based generation model
generating the CQL, and an ensemble model to obtain the final answers from
diversified outputs. With our ChatGPT-based CQL generation framework, we
achieved the second place in the CCKS 2023 Question Answering with Knowledge
Graph Inference for Unmanned Systems competition, achieving an F1-score of
0.92676.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.02956v1},
File          = {2311.02956v1.pdf}
}
@article{2311.16075v1,
Author        = {François Remy and Kris Demuynck and Thomas Demeester},
Title         = {BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical
  Knowledge Graph Insights},
Eprint        = {2311.16075v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this study, we investigate the potential of Large Language Models to
complement biomedical knowledge graphs in the training of semantic models for
the biomedical and clinical domains. Drawing on the wealth of the UMLS
knowledge graph and harnessing cutting-edge Large Language Models, we propose a
new state-of-the-art approach for obtaining high-fidelity representations of
biomedical concepts and sentences, consisting of three steps: an improved
contrastive learning phase, a novel self-distillation phase, and a weight
averaging phase. Through rigorous evaluations via the extensive BioLORD testing
suite and diverse downstream tasks, we demonstrate consistent and substantial
performance improvements over the previous state of the art (e.g. +2pts on
MedSTS, +2.5pts on MedNLI-S, +6.1pts on EHR-Rel-B). Besides our new
state-of-the-art biomedical model for English, we also distill and release a
multilingual model compatible with 50+ languages and finetuned on 7 European
languages. Many clinical pipelines can benefit from our latest models. Our new
multilingual model enables a range of languages to benefit from our
advancements in biomedical semantic representation learning, opening a new
avenue for bioinformatics researchers around the world. As a result, we hope to
see BioLORD-2023 becoming a precious tool for future biomedical applications.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.16075v1},
File          = {2311.16075v1.pdf}
}
@article{2407.10794v1,
Author        = {Rui Yang and Boming Yang and Sixun Ouyang and Tianwei She and Aosong Feng and Yuang Jiang and Freddy Lecue and Jinghui Lu and Irene Li},
Title         = {Graphusion: Leveraging Large Language Models for Scientific Knowledge
  Graph Fusion and Construction in NLP Education},
Eprint        = {2407.10794v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs (KGs) are crucial in the field of artificial intelligence
and are widely applied in downstream tasks, such as enhancing Question
Answering (QA) systems. The construction of KGs typically requires significant
effort from domain experts. Recently, Large Language Models (LLMs) have been
used for knowledge graph construction (KGC), however, most existing approaches
focus on a local perspective, extracting knowledge triplets from individual
sentences or documents. In this work, we introduce Graphusion, a zero-shot KGC
framework from free text. The core fusion module provides a global view of
triplets, incorporating entity merging, conflict resolution, and novel triplet
discovery. We showcase how Graphusion could be applied to the natural language
processing (NLP) domain and validate it in the educational scenario.
Specifically, we introduce TutorQA, a new expert-verified benchmark for graph
reasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our
evaluation demonstrates that Graphusion surpasses supervised baselines by up to
10% in accuracy on link prediction. Additionally, it achieves average scores of
2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and
relation recognition, respectively.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.10794v1},
File          = {2407.10794v1.pdf}
}
@article{2409.20053v2,
Author        = {Sheng Ouyang and Yulan Hu and Ge Chen and Yong Liu},
Title         = {GUNDAM: Aligning Large Language Models with Graph Understanding},
Eprint        = {2409.20053v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models (LLMs) have achieved impressive results in processing
text data, which has sparked interest in applying these models beyond textual
data, such as graphs. In the field of graph learning, there is a growing
interest in harnessing LLMs to comprehend and manipulate graph-structured data.
Existing research predominantly focuses on graphs with rich textual features,
such as knowledge graphs or text attribute graphs, leveraging LLMs' ability to
process text but inadequately addressing graph structure. This work
specifically aims to assess and enhance LLMs' abilities to comprehend and
utilize the structural knowledge inherent in graph data itself, rather than
focusing solely on graphs rich in textual content. To achieve this, we
introduce the \textbf{G}raph \textbf{U}nderstanding for \textbf{N}atural
Language \textbf{D}riven \textbf{A}nalytical \textbf{M}odel (\model). This
model adapts LLMs to better understand and engage with the structure of graph
data, enabling them to perform complex reasoning tasks by leveraging the
graph's structure itself. Our experimental evaluations on graph reasoning
benchmarks not only substantiate that \model~ outperforms the SOTA baselines
for comparisons. But also reveals key factors affecting the graph reasoning
capabilities of LLMs. Moreover, we provide a theoretical analysis illustrating
how reasoning paths can enhance LLMs' reasoning capabilities.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.20053v2},
File          = {2409.20053v2.pdf}
}
@article{2404.01720v1,
Author        = {Zhuo Chen and Zhao Zhang and Zixuan Li and Fei Wang and Yutao Zeng and Xiaolong Jin and Yongjun Xu},
Title         = {Self-Improvement Programming for Temporal Knowledge Graph Question
  Answering},
Eprint        = {2404.01720v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Temporal Knowledge Graph Question Answering (TKGQA) aims to answer questions
with temporal intent over Temporal Knowledge Graphs (TKGs). The core challenge
of this task lies in understanding the complex semantic information regarding
multiple types of time constraints (e.g., before, first) in questions. Existing
end-to-end methods implicitly model the time constraints by learning time-aware
embeddings of questions and candidate answers, which is far from understanding
the question comprehensively. Motivated by semantic-parsing-based approaches
that explicitly model constraints in questions by generating logical forms with
symbolic operators, we design fundamental temporal operators for time
constraints and introduce a novel self-improvement Programming method for TKGQA
(Prog-TQA). Specifically, Prog-TQA leverages the in-context learning ability of
Large Language Models (LLMs) to understand the combinatory time constraints in
the questions and generate corresponding program drafts with a few examples
given. Then, it aligns these drafts to TKGs with the linking module and
subsequently executes them to generate the answers. To enhance the ability to
understand questions, Prog-TQA is further equipped with a self-improvement
strategy to effectively bootstrap LLMs using high-quality self-generated
drafts. Extensive experiments demonstrate the superiority of the proposed
Prog-TQA on MultiTQ and CronQuestions datasets, especially in the Hits@1
metric.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.01720v1},
File          = {2404.01720v1.pdf}
}
@article{2410.17600v2,
Author        = {Rui Yang and Boming Yang and Aosong Feng and Sixun Ouyang and Moritz Blum and Tianwei She and Yuang Jiang and Freddy Lecue and Jinghui Lu and Irene Li},
Title         = {Graphusion: A RAG Framework for Knowledge Graph Construction with a
  Global Perspective},
Eprint        = {2410.17600v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graphs (KGs) are crucial in the field of artificial intelligence
and are widely used in downstream tasks, such as question-answering (QA). The
construction of KGs typically requires significant effort from domain experts.
Large Language Models (LLMs) have recently been used for Knowledge Graph
Construction (KGC). However, most existing approaches focus on a local
perspective, extracting knowledge triplets from individual sentences or
documents, missing a fusion process to combine the knowledge in a global KG.
This work introduces Graphusion, a zero-shot KGC framework from free text. It
contains three steps: in Step 1, we extract a list of seed entities using topic
modeling to guide the final KG includes the most relevant entities; in Step 2,
we conduct candidate triplet extraction using LLMs; in Step 3, we design the
novel fusion module that provides a global view of the extracted knowledge,
incorporating entity merging, conflict resolution, and novel triplet discovery.
Results show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for
entity extraction and relation recognition, respectively. Moreover, we showcase
how Graphusion could be applied to the Natural Language Processing (NLP) domain
and validate it in an educational scenario. Specifically, we introduce TutorQA,
a new expert-verified benchmark for QA, comprising six tasks and a total of
1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant
improvement on the benchmark, for example, a 9.2% accuracy improvement on
sub-graph completion.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.17600v2},
File          = {2410.17600v2.pdf}
}
@article{2004.14224v1,
Author        = {Tao Shen and Yi Mao and Pengcheng He and Guodong Long and Adam Trischler and Weizhu Chen},
Title         = {Exploiting Structured Knowledge in Text via Graph-Guided Representation
  Learning},
Eprint        = {2004.14224v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this work, we aim at equipping pre-trained language models with structured
knowledge. We present two self-supervised tasks learning over raw text with the
guidance from knowledge graphs. Building upon entity-level masked language
models, our first contribution is an entity masking scheme that exploits
relational knowledge underlying the text. This is fulfilled by using a linked
knowledge graph to select informative entities and then masking their mentions.
In addition we use knowledge graphs to obtain distractors for the masked
entities, and propose a novel distractor-suppressed ranking objective which is
optimized jointly with masked language model. In contrast to existing
paradigms, our approach uses knowledge graphs implicitly, only during
pre-training, to inject language models with structured knowledge via learning
from raw text. It is more efficient than retrieval-based methods that perform
entity linking and integration during finetuning and inference, and generalizes
more effectively than the methods that directly learn from concatenated graph
triples. Experiments show that our proposed model achieves improved performance
on five benchmark datasets, including question answering and knowledge base
completion tasks.},
Year          = {2020},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2004.14224v1},
File          = {2004.14224v1.pdf}
}
@article{2401.04507v1,
Author        = {Jiaqi Wang and Yuying Chang and Zhong Li and Ning An and Qi Ma and Lei Hei and Haibo Luo and Yifei Lu and Feiliang Ren},
Title         = {TechGPT-2.0: A large language model project to solve the task of
  knowledge graph construction},
Eprint        = {2401.04507v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models have exhibited robust performance across diverse
natural language processing tasks. This report introduces TechGPT-2.0, a
project designed to enhance the capabilities of large language models
specifically in knowledge graph construction tasks, including named entity
recognition (NER) and relationship triple extraction (RTE) tasks in NLP
applications. Additionally, it serves as a LLM accessible for research within
the Chinese open-source model community. We offer two 7B large language model
weights and a QLoRA weight specialized for processing lengthy texts.Notably,
TechGPT-2.0 is trained on Huawei's Ascend server. Inheriting all
functionalities from TechGPT-1.0, it exhibits robust text processing
capabilities, particularly in the domains of medicine and law. Furthermore, we
introduce new capabilities to the model, enabling it to process texts in
various domains such as geographical areas, transportation, organizations,
literary works, biology, natural sciences, astronomical objects, and
architecture. These enhancements also fortified the model's adeptness in
handling hallucinations, unanswerable queries, and lengthy texts. This report
provides a comprehensive and detailed introduction to the full fine-tuning
process on Huawei's Ascend servers, encompassing experiences in Ascend server
debugging, instruction fine-tuning data processing, and model training. Our
code is available at https://github.com/neukg/TechGPT-2.0},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.04507v1},
File          = {2401.04507v1.pdf}
}
@article{2406.14745v2,
Author        = {Sefika Efeoglu and Adrian Paschke},
Title         = {Relation Extraction with Fine-Tuned Large Language Models in Retrieval
  Augmented Generation Frameworks},
Eprint        = {2406.14745v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Information Extraction (IE) is crucial for converting unstructured data into
structured formats like Knowledge Graphs (KGs). A key task within IE is
Relation Extraction (RE), which identifies relationships between entities in
text. Various RE methods exist, including supervised, unsupervised, weakly
supervised, and rule-based approaches. Recent studies leveraging pre-trained
language models (PLMs) have shown significant success in this area. In the
current era dominated by Large Language Models (LLMs), fine-tuning these models
can overcome limitations associated with zero-shot LLM prompting-based RE
methods, especially regarding domain adaptation challenges and identifying
implicit relations between entities in sentences. These implicit relations,
which cannot be easily extracted from a sentence's dependency tree, require
logical inference for accurate identification. This work explores the
performance of fine-tuned LLMs and their integration into the Retrieval
Augmented-based (RAG) RE approach to address the challenges of identifying
implicit relations at the sentence level, particularly when LLMs act as
generators within the RAG framework. Empirical evaluations on the TACRED,
TACRED-Revisited (TACREV), Re-TACRED, and SemEVAL datasets show significant
performance improvements with fine-tuned LLMs, including Llama2-7B, Mistral-7B,
and T5 (Large). Notably, our approach achieves substantial gains on SemEVAL,
where implicit relations are common, surpassing previous results on this
dataset. Additionally, our method outperforms previous works on TACRED, TACREV,
and Re-TACRED, demonstrating exceptional performance across diverse evaluation
scenarios.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.14745v2},
File          = {2406.14745v2.pdf}
}
@article{2410.09699v1,
Author        = {Xinxi Chen and Li Wang and Wei Wu and Qi Tang and Yiyao Liu},
Title         = {Honest AI: Fine-Tuning "Small" Language Models to Say "I Don't Know",
  and Reducing Hallucination in RAG},
Eprint        = {2410.09699v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Hallucination is a key roadblock for applications of Large Language Models
(LLMs), particularly for enterprise applications that are sensitive to
information accuracy. To address this issue, two general approaches have been
explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated
information as context, and fine-tuning the LLMs with new information and
desired output styles. In this paper, we propose Honest AI: a novel strategy to
fine-tune "small" language models to say "I don't know" to reduce
hallucination, along with several alternative RAG approaches. The solution
ranked 1st in Task 2 for the false premise question. The alternative approaches
include using RAG with search engine and knowledge graph results, fine-tuning
base LLMs with new information and combinations of both approaches. Although
all approaches improve the performance of the LLMs, RAG alone does not
significantly improve the performance and fine-tuning is needed for better
results. Finally, the hybrid approach achieved the highest score in the CRAG
benchmark. In addition, our approach emphasizes the use of relatively small
models with fewer than 10 billion parameters, promoting resource efficiency.},
Year          = {2024},
Month         = {Oct},
Note          = {2024 KDD Cup Workshop for Retrieval Augmented Generation at the
  30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
Url           = {http://arxiv.org/abs/2410.09699v1},
File          = {2410.09699v1.pdf}
}
@article{2305.05994v2,
Author        = {Siyu Yuan and Jiangjie Chen and Changzhi Sun and Jiaqing Liang and Yanghua Xiao and Deqing Yang},
Title         = {ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A
  Million-scale Knowledge Base},
Eprint        = {2305.05994v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Analogical reasoning is a fundamental cognitive ability of humans. However,
current language models (LMs) still struggle to achieve human-like performance
in analogical reasoning tasks due to a lack of resources for model training. In
this work, we address this gap by proposing ANALOGYKB, a million-scale analogy
knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB
identifies two types of analogies from the KGs: 1) analogies of the same
relations, which can be directly extracted from the KGs, and 2) analogies of
analogous relations, which are identified with a selection and filtering
pipeline enabled by large language models (LLMs), followed by minor human
efforts for data quality control. Evaluations on a series of datasets of two
analogical reasoning tasks (analogy recognition and generation) demonstrate
that ANALOGYKB successfully enables both smaller LMs and LLMs to gain better
analogical reasoning capabilities.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.05994v2},
File          = {2305.05994v2.pdf}
}
@article{2308.14346v1,
Author        = {Zhijie Bao and Wei Chen and Shengze Xiao and Kuang Ren and Jiaao Wu and Cheng Zhong and Jiajie Peng and Xuanjing Huang and Zhongyu Wei},
Title         = {DISC-MedLLM: Bridging General Large Language Models and Real-World
  Medical Consultation},
Eprint        = {2308.14346v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We propose DISC-MedLLM, a comprehensive solution that leverages Large
Language Models (LLMs) to provide accurate and truthful medical response in
end-to-end conversational healthcare services. To construct high-quality
Supervised Fine-Tuning (SFT) datasets, we employ three strategies: utilizing
medical knowledge-graphs, reconstructing real-world dialogues, and
incorporating human-guided preference rephrasing. These datasets are
instrumental in training DISC-MedLLM, surpassing existing medical LLMs in both
single-turn and multi-turn consultation scenarios. Extensive experimental
results demonstrate the effectiveness of the proposed model in bridging the gap
between general language models and real-world medical consultation.
Additionally, we release the constructed dataset and model weights to further
contribute to research and development. Further details and resources can be
found at https://github.com/FudanDISC/DISC-MedLLM},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.14346v1},
File          = {2308.14346v1.pdf}
}
@article{2406.06596v1,
Author        = {Sylvio Barbon Junior and Paolo Ceravolo and Sven Groppe and Mustafa Jarrar and Samira Maghool and Florence Sèdes and Soror Sahri and Maurice Van Keulen},
Title         = {Are Large Language Models the New Interface for Data Pipelines?},
Eprint        = {2406.06596v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {A Language Model is a term that encompasses various types of models designed
to understand and generate human communication. Large Language Models (LLMs)
have gained significant attention due to their ability to process text with
human-like fluency and coherence, making them valuable for a wide range of
data-related tasks fashioned as pipelines. The capabilities of LLMs in natural
language understanding and generation, combined with their scalability,
versatility, and state-of-the-art performance, enable innovative applications
across various AI-related fields, including eXplainable Artificial Intelligence
(XAI), Automated Machine Learning (AutoML), and Knowledge Graphs (KG).
Furthermore, we believe these models can extract valuable insights and make
data-driven decisions at scale, a practice commonly referred to as Big Data
Analytics (BDA). In this position paper, we provide some discussions in the
direction of unlocking synergies among these technologies, which can lead to
more powerful and intelligent AI solutions, driving improvements in data
pipelines across a wide range of applications and domains integrating humans,
computers, and knowledge.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.06596v1},
File          = {2406.06596v1.pdf}
}
@article{2308.14429v1,
Author        = {Xi Yan and Cedric Möller and Ricardo Usbeck},
Title         = {Biomedical Entity Linking with Triple-aware Pre-Training},
Eprint        = {2308.14429v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Linking biomedical entities is an essential aspect in biomedical natural
language processing tasks, such as text mining and question answering. However,
a difficulty of linking the biomedical entities using current large language
models (LLM) trained on a general corpus is that biomedical entities are
scarcely distributed in texts and therefore have been rarely seen during
training by the LLM. At the same time, those LLMs are not aware of high level
semantic connection between different biomedical entities, which are useful in
identifying similar concepts in different textual contexts. To cope with
aforementioned problems, some recent works focused on injecting knowledge graph
information into LLMs. However, former methods either ignore the relational
knowledge of the entities or lead to catastrophic forgetting. Therefore, we
propose a novel framework to pre-train the powerful generative LLM by a corpus
synthesized from a KG. In the evaluations we are unable to confirm the benefit
of including synonym, description or relational information.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.14429v1},
File          = {2308.14429v1.pdf}
}
@article{2410.07526v1,
Author        = {Lingbing Guo and Zhongpu Bo and Zhuo Chen and Yichi Zhang and Jiaoyan Chen and Yarong Lan and Mengshu Sun and Zhiqiang Zhang and Yangyifei Luo and Qian Li and Qiang Zhang and Wen Zhang and Huajun Chen},
Title         = {MKGL: Mastery of a Three-Word Language},
Eprint        = {2410.07526v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have significantly advanced performance across a
spectrum of natural language processing (NLP) tasks. Yet, their application to
knowledge graphs (KGs), which describe facts in the form of triplets and allow
minimal hallucinations, remains an underexplored frontier. In this paper, we
investigate the integration of LLMs with KGs by introducing a specialized KG
Language (KGL), where a sentence precisely consists of an entity noun, a
relation verb, and ends with another entity noun. Despite KGL's unfamiliar
vocabulary to the LLM, we facilitate its learning through a tailored dictionary
and illustrative sentences, and enhance context understanding via real-time KG
context retrieval and KGL token embedding augmentation. Our results reveal that
LLMs can achieve fluency in KGL, drastically reducing errors compared to
conventional KG embedding methods on KG completion. Furthermore, our enhanced
LLM shows exceptional competence in generating accurate three-word sentences
from an initial entity and interpreting new unseen terms out of KGs.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.07526v1},
File          = {2410.07526v1.pdf}
}
@article{2110.10457v2,
Author        = {Boshko Koloski and Timen Stepišnik-Perdih and Marko Robnik-Šikonja and Senja Pollak and Blaž Škrlj},
Title         = {Knowledge Graph informed Fake News Classification via Heterogeneous
  Representation Ensembles},
Eprint        = {2110.10457v2},
DOI           = {10.1016/j.neucom.2022.01.096},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Increasing amounts of freely available data both in textual and relational
form offers exploration of richer document representations, potentially
improving the model performance and robustness. An emerging problem in the
modern era is fake news detection -- many easily available pieces of
information are not necessarily factually correct, and can lead to wrong
conclusions or are used for manipulation. In this work we explore how different
document representations, ranging from simple symbolic bag-of-words, to
contextual, neural language model-based ones can be used for efficient fake
news identification. One of the key contributions is a set of novel document
representation learning methods based solely on knowledge graphs, i.e.
extensive collections of (grounded) subject-predicate-object triplets. We
demonstrate that knowledge graph-based representations already achieve
competitive performance to conventionally accepted representation learners.
Furthermore, when combined with existing, contextual representations, knowledge
graph-based document representations can achieve state-of-the-art performance.
To our knowledge this is the first larger-scale evaluation of how knowledge
graph-based representations can be systematically incorporated into the process
of fake news classification.},
Year          = {2021},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2110.10457v2},
File          = {2110.10457v2.pdf}
}
@article{2205.06895v1,
Author        = {Anthi Papadopoulou and Pierre Lison and Lilja Øvrelid and Ildikó Pilán},
Title         = {Bootstrapping Text Anonymization Models with Distant Supervision},
Eprint        = {2205.06895v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We propose a novel method to bootstrap text anonymization models based on
distant supervision. Instead of requiring manually labeled training data, the
approach relies on a knowledge graph expressing the background information
assumed to be publicly available about various individuals. This knowledge
graph is employed to automatically annotate text documents including personal
data about a subset of those individuals. More precisely, the method determines
which text spans ought to be masked in order to guarantee $k$-anonymity,
assuming an adversary with access to both the text documents and the background
information expressed in the knowledge graph. The resulting collection of
labeled documents is then used as training data to fine-tune a pre-trained
language model for text anonymization. We illustrate this approach using a
knowledge graph extracted from Wikidata and short biographical texts from
Wikipedia. Evaluation results with a RoBERTa-based model and a manually
annotated collection of 553 summaries showcase the potential of the approach,
but also unveil a number of issues that may arise if the knowledge graph is
noisy or incomplete. The results also illustrate that, contrary to most
sequence labeling problems, the text anonymization task may admit several
alternative solutions.},
Year          = {2022},
Month         = {May},
Url           = {http://arxiv.org/abs/2205.06895v1},
File          = {2205.06895v1.pdf}
}
@article{2211.10460v1,
Author        = {Armita Khajeh Nassiri and Nathalie Pernelle and Fatiha Sais and Gianluca Quercini},
Title         = {Knowledge Graph Refinement based on Triplet BERT-Networks},
Eprint        = {2211.10460v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graph embedding techniques are widely used for knowledge graph
refinement tasks such as graph completion and triple classification. These
techniques aim at embedding the entities and relations of a Knowledge Graph
(KG) in a low dimensional continuous feature space. This paper adopts a
transformer-based triplet network creating an embedding space that clusters the
information about an entity or relation in the KG. It creates textual sequences
from facts and fine-tunes a triplet network of pre-trained transformer-based
language models. It adheres to an evaluation paradigm that relies on an
efficient spatial semantic search technique. We show that this evaluation
protocol is more adapted to a few-shot setting for the relation prediction
task. Our proposed GilBERT method is evaluated on triplet classification and
relation prediction tasks on multiple well-known benchmark knowledge graphs
such as FB13, WN11, and FB15K. We show that GilBERT achieves better or
comparable results to the state-of-the-art performance on these two refinement
tasks.},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.10460v1},
File          = {2211.10460v1.pdf}
}
@article{2409.12865v2,
Author        = {Junnan Liu and Qianren Mao and Weifeng Jiang and Jianxin Li},
Title         = {KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning},
Eprint        = {2409.12865v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graph reasoning plays a vital role in various applications and has
garnered considerable attention. Recently, path-based methods have achieved
impressive performance. However, they may face limitations stemming from
constraints in message-passing neural networks, such as missing paths and
information over-squashing. In this paper, we revisit the application of
transformers for knowledge graph reasoning to address the constraints faced by
path-based methods and propose a novel method KnowFormer. KnowFormer utilizes a
transformer architecture to perform reasoning on knowledge graphs from the
message-passing perspective, rather than reasoning by textual information like
previous pretrained language model based methods. Specifically, we define the
attention computation based on the query prototype of knowledge graph
reasoning, facilitating convenient construction and efficient optimization. To
incorporate structural information into the self-attention mechanism, we
introduce structure-aware modules to calculate query, key, and value
respectively. Additionally, we present an efficient attention computation
method for better scalability. Experimental results demonstrate the superior
performance of KnowFormer compared to prominent baseline methods on both
transductive and inductive benchmarks.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.12865v2},
File          = {2409.12865v2.pdf}
}
@article{2502.05239v1,
Author        = {Hussam Ghanem and Christophe Cruz},
Title         = {Enhancing Knowledge Graph Construction: Evaluating with Emphasis on
  Hallucination, Omission, and Graph Similarity Metrics},
Eprint        = {2502.05239v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent advancements in large language models have demonstrated significant
potential in the automated construction of knowledge graphs from unstructured
text. This paper builds upon our previous work [16], which evaluated various
models using metrics like precision, recall, F1 score, triple matching, and
graph matching, and introduces a refined approach to address the critical
issues of hallucination and omission. We propose an enhanced evaluation
framework incorporating BERTScore for graph similarity, setting a practical
threshold of 95% for graph matching. Our experiments focus on the Mistral
model, comparing its original and fine-tuned versions in zero-shot and few-shot
settings. We further extend our experiments using examples from the KELM-sub
training dataset, illustrating that the fine-tuned model significantly improves
knowledge graph construction accuracy while reducing the exact hallucination
and omission. However, our findings also reveal that the fine-tuned models
perform worse in generalization tasks on the KELM-sub dataset. This study
underscores the importance of comprehensive evaluation metrics in advancing the
state-of-the-art in knowledge graph construction from textual data.},
Year          = {2025},
Month         = {Feb},
Note          = {Sixth International Knowledge Graph and Semantic Web Conference
  (KGSWC 2024), Dec 2024, Paris, France},
Url           = {http://arxiv.org/abs/2502.05239v1},
File          = {2502.05239v1.pdf}
}
@article{2308.09217v1,
Author        = {Sanaz Saki Norouzi and Mohammad Saeid Mahdavinejad and Pascal Hitzler},
Title         = {Conversational Ontology Alignment with ChatGPT},
Eprint        = {2308.09217v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This study evaluates the applicability and efficiency of ChatGPT for ontology
alignment using a naive approach. ChatGPT's output is compared to the results
of the Ontology Alignment Evaluation Initiative 2022 campaign using conference
track ontologies. This comparison is intended to provide insights into the
capabilities of a conversational large language model when used in a naive way
for ontology matching, and to investigate the potential advantages and
disadvantages of this approach.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.09217v1},
File          = {2308.09217v1.pdf}
}
@article{2305.14898v1,
Author        = {Keming Lu and Xiaoman Pan and Kaiqiang Song and Hongming Zhang and Dong Yu and Jianshu Chen},
Title         = {PIVOINE: Instruction Tuning for Open-world Information Extraction},
Eprint        = {2305.14898v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We consider the problem of Open-world Information Extraction (Open-world IE),
which extracts comprehensive entity profiles from unstructured texts. Different
from the conventional closed-world setting of Information Extraction (IE),
Open-world IE considers a more general situation where entities and relations
could be beyond a predefined ontology. More importantly, we seek to develop a
large language model (LLM) that is able to perform Open-world IE to extract
desirable entity profiles characterized by (possibly fine-grained) natural
language instructions. We achieve this by finetuning LLMs using instruction
tuning. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction
tuning dataset for Open-world IE enriched with a comprehensive corpus,
extensive annotations, and diverse instructions. We finetune the pretrained
BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world IE
with strong instruction-following capabilities. Our experiments demonstrate
that PIVOINE significantly outperforms traditional closed-world methods and
other LLM baselines, displaying impressive generalization capabilities on both
unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as
a promising solution to tackle the open-world challenge in IE effectively.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.14898v1},
File          = {2305.14898v1.pdf}
}
@article{2404.00756v1,
Author        = {Cristina Cornelio and Mohammed Diab},
Title         = {Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery},
Eprint        = {2404.00756v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Recognizing failures during task execution and implementing recovery
procedures is challenging in robotics. Traditional approaches rely on the
availability of extensive data or a tight set of constraints, while more recent
approaches leverage large language models (LLMs) to verify task steps and
replan accordingly. However, these methods often operate offline, necessitating
scene resets and incurring in high costs. This paper introduces Recover, a
neuro-symbolic framework for online failure identification and recovery. By
integrating ontologies, logical rules, and LLM-based planners, Recover exploits
symbolic information to enhance the ability of LLMs to generate recovery plans
and also to decrease the associated costs. In order to demonstrate the
capabilities of our method in a simulated kitchen environment, we introduce
OntoThor, an ontology describing the AI2Thor simulator setting. Empirical
evaluation shows that OntoThor's logical rules accurately detect all failures
in the analyzed tasks, and that Recover considerably outperforms, for both
failure detection and recovery, a baseline method reliant solely on LLMs.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2404.00756v1},
File          = {2404.00756v1.pdf}
}
@article{2410.07962v1,
Author        = {Tomas Bueno Momcilovic and Beat Buesser and Giulio Zizzo and Mark Purcell and Dian Balta},
Title         = {Towards Assurance of LLM Adversarial Robustness using Ontology-Driven
  Argumentation},
Eprint        = {2410.07962v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Despite the impressive adaptability of large language models (LLMs),
challenges remain in ensuring their security, transparency, and
interpretability. Given their susceptibility to adversarial attacks, LLMs need
to be defended with an evolving combination of adversarial training and
guardrails. However, managing the implicit and heterogeneous knowledge for
continuously assuring robustness is difficult. We introduce a novel approach
for assurance of the adversarial robustness of LLMs based on formal
argumentation. Using ontologies for formalization, we structure
state-of-the-art attacks and defenses, facilitating the creation of a
human-readable assurance case, and a machine-readable representation. We
demonstrate its application with examples in English language and code
translation tasks, and provide implications for theory and practice, by
targeting engineers, data scientists, users, and auditors.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.07962v1},
File          = {2410.07962v1.pdf}
}
@article{2408.08878v1,
Author        = {Elisavet Koutsiana and Johanna Walker and Michelle Nwachukwu and Albert Meroño-Peñuela and Elena Simperl},
Title         = {Knowledge Prompting: How Knowledge Engineers Use Large Language Models},
Eprint        = {2408.08878v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {Despite many advances in knowledge engineering (KE), challenges remain in
areas such as engineering knowledge graphs (KGs) at scale, keeping up with
evolving domain knowledge, multilingualism, and multimodality. Recently, KE has
used LLMs to support semi-automatic tasks, but the most effective use of LLMs
to support knowledge engineers across the KE activites is still in its infancy.
To explore the vision of LLM copilots for KE and change existing KE practices,
we conducted a multimethod study during a KE hackathon. We investigated
participants' views on the use of LLMs, the challenges they face, the skills
they may need to integrate LLMs into their practices, and how they use LLMs
responsibly. We found participants felt LLMs could contribute to improving
efficiency when engineering KGs, but presented increased challenges around the
already complex issues of evaluating the KE tasks. We discovered prompting to
be a useful but undervalued skill for knowledge engineers working with LLMs,
and note that natural language processing skills may become more relevant
across more roles in KG construction. Integrating LLMs into KE tasks needs to
be mindful of potential risks and harms related to responsible AI. Given the
limited ethical training, most knowledge engineers receive solutions such as
our suggested `KG cards' based on data cards could be a useful guide for KG
construction. Our findings can support designers of KE AI copilots, KE
researchers, and practitioners using advanced AI to develop trustworthy
applications, propose new methodologies for KE and operate new technologies
responsibly.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.08878v1},
File          = {2408.08878v1.pdf}
}
@article{2305.16755v2,
Author        = {Hiba Arnaout and Simon Razniewski},
Title         = {Can large language models generate salient negative statements?},
Eprint        = {2305.16755v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We examine the ability of large language models (LLMs) to generate salient
(interesting) negative statements about real-world entities; an emerging
research topic of the last few years. We probe the LLMs using zero- and k-shot
unconstrained probes, and compare with traditional methods for negation
generation, i.e., pattern-based textual extractions and knowledge-graph-based
inferences, as well as crowdsourced gold statements. We measure the correctness
and salience of the generated lists about subjects from different domains. Our
evaluation shows that guided probes do in fact improve the quality of generated
negatives, compared to the zero-shot variant. Nevertheless, using both prompts,
LLMs still struggle with the notion of factuality of negatives, frequently
generating many ambiguous statements, or statements with negative keywords but
a positive meaning.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.16755v2},
File          = {2305.16755v2.pdf}
}
@article{2402.00414v1,
Author        = {Tolga Çöplü and Arto Bendiken and Andrii Skomorokhov and Eduard Bateiko and Stephen Cobb and Joshua J. Bouw},
Title         = {Prompt-Time Symbolic Knowledge Capture with Large Language Models},
Eprint        = {2402.00414v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Augmenting large language models (LLMs) with user-specific knowledge is
crucial for real-world applications, such as personal AI assistants. However,
LLMs inherently lack mechanisms for prompt-driven knowledge capture. This paper
investigates utilizing the existing LLM capabilities to enable prompt-driven
knowledge capture, with a particular emphasis on knowledge graphs. We address
this challenge by focusing on prompt-to-triple (P2T) generation. We explore
three methods: zero-shot prompting, few-shot prompting, and fine-tuning, and
then assess their performance via a specialized synthetic dataset. Our code and
datasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.00414v1},
File          = {2402.00414v1.pdf}
}
@article{2405.10288v3,
Author        = {Jianhao Chen and Haoyuan Ouyang and Junyang Ren and Wentao Ding and Wei Hu and Yuzhong Qu},
Title         = {Timeline-based Sentence Decomposition with In-Context Learning for
  Temporal Fact Extraction},
Eprint        = {2405.10288v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Facts extraction is pivotal for constructing knowledge graphs. Recently, the
increasing demand for temporal facts in downstream tasks has led to the
emergence of the task of temporal fact extraction. In this paper, we
specifically address the extraction of temporal facts from natural language
text. Previous studies fail to handle the challenge of establishing
time-to-fact correspondences in complex sentences. To overcome this hurdle, we
propose a timeline-based sentence decomposition strategy using large language
models (LLMs) with in-context learning, ensuring a fine-grained understanding
of the timeline associated with various facts. In addition, we evaluate the
performance of LLMs for direct temporal fact extraction and get unsatisfactory
results. To this end, we introduce TSDRE, a method that incorporates the
decomposition capabilities of LLMs into the traditional fine-tuning of smaller
pre-trained language models (PLMs). To support the evaluation, we construct
ComplexTRED, a complex temporal fact extraction dataset. Our experiments show
that TSDRE achieves state-of-the-art results on both HyperRED-Temporal and
ComplexTRED datasets.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.10288v3},
File          = {2405.10288v3.pdf}
}
@article{2407.09506v1,
Author        = {Parag Jain and Mirella Lapata},
Title         = {Integrating Large Language Models with Graph-based Reasoning for
  Conversational Question Answering},
Eprint        = {2407.09506v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We focus on a conversational question answering task which combines the
challenges of understanding questions in context and reasoning over evidence
gathered from heterogeneous sources like text, knowledge graphs, tables, and
infoboxes. Our method utilizes a graph structured representation to aggregate
information about a question and its context (i.e., the conversation so far and
evidence retrieved to find an answer), while also harnessing the reasoning and
text generation capabilities of large language models (LLMs). Graph embeddings
are directly injected into the LLM, bypassing the token embedding layers, and
learned end-to-end by minimizing cross-entropy. Our model maintains a memory
module to track and update past evidence, thus influencing the graph's
structure, as the conversation evolves. Experimental results on the ConvMix
benchmark(Christmann et al., 2022a) show that graph embeddings enhance the
LLM's ability to reason, while the memory module provides robustness against
noise and retrieval errors.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2407.09506v1},
File          = {2407.09506v1.pdf}
}
@article{2411.02523v1,
Author        = {Balu Bhasuran and Qiao Jin and Yuzhang Xie and Carl Yang and Karim Hanna and Jennifer Costa and Cindy Shavor and Zhiyong Lu and Zhe He},
Title         = {Evaluating the Impact of Lab Test Results on Large Language Models
  Generated Differential Diagnoses from Clinical Case Vignettes},
Eprint        = {2411.02523v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Differential diagnosis is crucial for medicine as it helps healthcare
providers systematically distinguish between conditions that share similar
symptoms. This study assesses the impact of lab test results on differential
diagnoses (DDx) made by large language models (LLMs). Clinical vignettes from
50 case reports from PubMed Central were created incorporating patient
demographics, symptoms, and lab results. Five LLMs GPT-4, GPT-3.5, Llama-2-70b,
Claude-2, and Mixtral-8x7B were tested to generate Top 10, Top 5, and Top 1 DDx
with and without lab data. A comprehensive evaluation involving GPT-4, a
knowledge graph, and clinicians was conducted. GPT-4 performed best, achieving
55% accuracy for Top 1 diagnoses and 60% for Top 10 with lab data, with lenient
accuracy up to 80%. Lab results significantly improved accuracy, with GPT-4 and
Mixtral excelling, though exact match rates were low. Lab tests, including
liver function, metabolic/toxicology panels, and serology/immune tests, were
generally interpreted correctly by LLMs for differential diagnosis.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.02523v1},
File          = {2411.02523v1.pdf}
}
@article{2502.06257v1,
Author        = {Lingbing Guo and Yichi Zhang and Zhongpu Bo and Zhuo Chen and Mengshu Sun and Zhiqiang Zhang and Wen Zhang and Huajun Chen},
Title         = {K-ON: Stacking Knowledge On the Head Layer of Large Language Model},
Eprint        = {2502.06257v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent advancements in large language models (LLMs) have significantly
improved various natural language processing (NLP) tasks. Typically, LLMs are
trained to predict the next token, aligning well with many NLP tasks. However,
in knowledge graph (KG) scenarios, entities are the fundamental units and
identifying an entity requires at least several tokens. This leads to a
granularity mismatch between KGs and natural languages. To address this issue,
we propose K-ON, which integrates KG knowledge into the LLM by employing
multiple head layers for next k-step prediction. K-ON can not only generate
entity-level results in one step, but also enables contrastive loss against
entities, which is the most powerful tool in KG representation learning.
Experimental results show that K-ON outperforms state-of-the-art methods that
incorporate text and even the other modalities.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.06257v1},
File          = {2502.06257v1.pdf}
}
@article{2405.11581v2,
Author        = {Panos Fitsilis and Vyron Damasiotis and Vasileios Kyriatzis and Paraskevi Tsoutsa},
Title         = {DOLLmC: DevOps for Large Language model Customization},
Eprint        = {2405.11581v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {The rapid integration of Large Language Models (LLMs) into various industries
presents both revolutionary opportunities and unique challenges. This research
aims to establish a scalable and efficient framework for LLM customization,
exploring how DevOps practices should be adapted to meet the specific demands
of LLM customization. By integrating ontologies, knowledge maps, and prompt
engineering into the DevOps pipeline, we propose a robust framework that
enhances continuous learning, seamless deployment, and rigorous version control
of LLMs. This methodology is demonstrated through the development of a
domain-specific chatbot for the agricultural sector, utilizing heterogeneous
data to deliver actionable insights. The proposed methodology, so called
DOLLmC, not only addresses the immediate challenges of LLM customization but
also promotes scalability and operational efficiency. However, the
methodology's primary limitation lies in the need for extensive testing,
validation, and broader adoption across different domains.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.11581v2},
File          = {2405.11581v2.pdf}
}
@article{2408.15256v3,
Author        = {Yihang Zhao and Bohui Zhang and Xi Hu and Shuyin Ouyang and Jongmo Kim and Nitisha Jain and Jacopo de Berardinis and Albert Meroño-Peñuela and Elena Simperl},
Title         = {Improving Ontology Requirements Engineering with OntoChat and
  Participatory Prompting},
Eprint        = {2408.15256v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {Past ontology requirements engineering (ORE) has primarily relied on manual
methods, such as interviews and collaborative forums, to gather user
requirements from domain experts, especially in large projects. Current
OntoChat offers a framework for ORE that utilises large language models (LLMs)
to streamline the process through four key functions: user story creation,
competency question (CQ) extraction, CQ filtration and analysis, and ontology
testing support. In OntoChat, users are expected to prompt the chatbot to
generate user stories. However, preliminary evaluations revealed that they
struggle to do this effectively. To address this issue, we experimented with a
research method called participatory prompting, which involves
researcher-mediated interactions to help users without deep knowledge of LLMs
use the chatbot more effectively. This participatory prompting user study
produces pre-defined prompt templates based on user queries, focusing on
creating and refining personas, goals, scenarios, sample data, and data
resources for user stories. These refined user stories will subsequently be
converted into CQs.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.15256v3},
File          = {2408.15256v3.pdf}
}
@article{1908.07690v1,
Author        = {Hiroaki Hayashi and Zecong Hu and Chenyan Xiong and Graham Neubig},
Title         = {Latent Relation Language Models},
Eprint        = {1908.07690v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this paper, we propose Latent Relation Language Models (LRLMs), a class of
language models that parameterizes the joint distribution over the words in a
document and the entities that occur therein via knowledge graph relations.
This model has a number of attractive properties: it not only improves language
modeling performance, but is also able to annotate the posterior probability of
entity spans for a given text through relations. Experiments demonstrate
empirical improvements over both a word-based baseline language model and a
previous approach that incorporates knowledge graph information. Qualitative
analysis further demonstrates the proposed model's ability to learn to predict
appropriate relations in context.},
Year          = {2019},
Month         = {Aug},
Url           = {http://arxiv.org/abs/1908.07690v1},
File          = {1908.07690v1.pdf}
}
@article{2312.04818v1,
Author        = {M. Xie and T. Rahat and W. Wang and Y. Tian},
Title         = {Using Program Knowledge Graph to Uncover Software Vulnerabilities},
Eprint        = {2312.04818v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {In an increasingly interconnected and data-driven world, the importance of
robust security measures cannot be overstated. A knowledge graph constructed
with information extracted from the system along with the desired security
behavior can be utilized to identify complex security vulnerabilities hidden
underneath the systems. Unfortunately, existing security knowledge graphs are
constructed from coarse-grained information extracted from publicly available
vulnerability reports, which are not equipped to check actual security
violations in real-world system implementations. In this poster, we present a
novel approach of using Program Knowledge Graph that is embedded with
fine-grained execution information of the systems (e.g., callgraph, data-flow,
etc.) along with information extracted from the public vulnerability and
weakness datasets (e.g., CVE and CWE). We further demonstrate that our custom
security knowledge graph can be checked against the standard queries generated
by LLM, providing a powerful way to identify security vulnerabilities and
weaknesses in critical systems.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.04818v1},
File          = {2312.04818v1.pdf}
}
@article{2410.24021v1,
Author        = {Lucian Li and Eryclis Silva},
Title         = {Detecting text level intellectual influence with knowledge graph
  embeddings},
Eprint        = {2410.24021v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Introduction: Tracing the spread of ideas and the presence of influence is a
question of special importance across a wide range of disciplines, ranging from
intellectual history to cultural analytics, computational social science, and
the science of science.
  Method: We collect a corpus of open source journal articles, generate
Knowledge Graph representations using the Gemini LLM, and attempt to predict
the existence of citations between sampled pairs of articles using previously
published methods and a novel Graph Neural Network based embedding model.
  Results: We demonstrate that our knowledge graph embedding method is superior
at distinguishing pairs of articles with and without citation. Once trained, it
runs efficiently and can be fine-tuned on specific corpora to suit individual
researcher needs.
  Conclusion(s): This experiment demonstrates that the relationships encoded in
a knowledge graph, especially the types of concepts brought together by
specific relations can encode information capable of revealing intellectual
influence. This suggests that further work in analyzing document level
knowledge graphs to understand latent structures could provide valuable
insights.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.24021v1},
File          = {2410.24021v1.pdf}
}
@article{2203.04703v3,
Author        = {Md Rashad Al Hasan Rony and Mirza Mohtashim Alam and Semab Ali and Jens Lehmann and Sahar Vahdati},
Title         = {LEMON: LanguagE ModeL for Negative Sampling of Knowledge Graph
  Embeddings},
Eprint        = {2203.04703v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge Graph Embedding models have become an important area of machine
learning.Those models provide a latent representation of entities and relations
in a knowledge graph which can then be used in downstream machine learning
tasks such as link prediction. The learning process of such models can be
performed by contrasting positive and negative triples. While all triples of a
KG are considered positive, negative triples are usually not readily available.
Therefore, the choice of the sampling method to obtain the negative triples
play a crucial role in the performance and effectiveness of Knowledge Graph
Embedding models. Most of the current methods fetch negative samples from a
random distribution of entities in the underlying Knowledge Graph which also
often includes meaningless triples. Other known methods use adversarial
techniques or generative neural networks which consequently reduce the
efficiency of the process. In this paper, we propose an approach for generating
informative negative samples considering available complementary knowledge
about entities. Particularly, Pre-trained Language Models are used to form
neighborhood clusters by utilizing the distances between entities to obtain
representations of symbolic entities via their textual information. Our
comprehensive evaluations demonstrate the effectiveness of the proposed
approach on benchmark Knowledge Graphs with textual information for the link
prediction task.},
Year          = {2022},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2203.04703v3},
File          = {2203.04703v3.pdf}
}
@article{2309.16357v1,
Author        = {Duygu Sezen Islakoglu and Mel Chekol and Yannis Velegrakis},
Title         = {Leveraging Pre-trained Language Models for Time Interval Prediction in
  Text-Enhanced Temporal Knowledge Graphs},
Eprint        = {2309.16357v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Most knowledge graph completion (KGC) methods learn latent representations of
entities and relations of a given graph by mapping them into a vector space.
Although the majority of these methods focus on static knowledge graphs, a
large number of publicly available KGs contain temporal information stating the
time instant/period over which a certain fact has been true. Such graphs are
often known as temporal knowledge graphs. Furthermore, knowledge graphs may
also contain textual descriptions of entities and relations. Both temporal
information and textual descriptions are not taken into account during
representation learning by static KGC methods, and only structural information
of the graph is leveraged. Recently, some studies have used temporal
information to improve link prediction, yet they do not exploit textual
descriptions and do not support inductive inference (prediction on entities
that have not been seen in training).
  We propose a novel framework called TEMT that exploits the power of
pre-trained language models (PLMs) for text-enhanced temporal knowledge graph
completion. The knowledge stored in the parameters of a PLM allows TEMT to
produce rich semantic representations of facts and to generalize on previously
unseen entities. TEMT leverages textual and temporal information available in a
KG, treats them separately, and fuses them to get plausibility scores of facts.
Unlike previous approaches, TEMT effectively captures dependencies across
different time points and enables predictions on unseen entities. To assess the
performance of TEMT, we carried out several experiments including time interval
prediction, both in transductive and inductive settings, and triple
classification. The experimental results show that TEMT is competitive with the
state-of-the-art.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.16357v1},
File          = {2309.16357v1.pdf}
}
@article{2403.18325v1,
Author        = {Shenghao Yang and Weizhi Ma and Peijie Sun and Min Zhang and Qingyao Ai and Yiqun Liu and Mingchen Cai},
Title         = {Common Sense Enhanced Knowledge-based Recommendation with Large Language
  Model},
Eprint        = {2403.18325v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Knowledge-based recommendation models effectively alleviate the data sparsity
issue leveraging the side information in the knowledge graph, and have achieved
considerable performance. Nevertheless, the knowledge graphs used in previous
work, namely metadata-based knowledge graphs, are usually constructed based on
the attributes of items and co-occurring relations (e.g., also buy), in which
the former provides limited information and the latter relies on sufficient
interaction data and still suffers from cold start issue. Common sense, as a
form of knowledge with generality and universality, can be used as a supplement
to the metadata-based knowledge graph and provides a new perspective for
modeling users' preferences. Recently, benefiting from the emergent world
knowledge of the large language model, efficient acquisition of common sense
has become possible. In this paper, we propose a novel knowledge-based
recommendation framework incorporating common sense, CSRec, which can be
flexibly coupled to existing knowledge-based methods. Considering the challenge
of the knowledge gap between the common sense-based knowledge graph and
metadata-based knowledge graph, we propose a knowledge fusion approach based on
mutual information maximization theory. Experimental results on public datasets
demonstrate that our approach significantly improves the performance of
existing knowledge-based recommendation models.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.18325v1},
File          = {2403.18325v1.pdf}
}
@article{2407.00466v1,
Author        = {Xinna Lin and Siqi Ma and Junjie Shan and Xiaojing Zhang and Shell Xu Hu and Tiannan Guo and Stan Z. Li and Kaicheng Yu},
Title         = {BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for
  Biomedical Science},
Eprint        = {2407.00466v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist,
draws increasing attention, where one common approach is to build a copilot
agent driven by Large Language Models (LLMs). However, to evaluate such
systems, people either rely on direct Question-Answering (QA) to the LLM
itself, or in a biomedical experimental manner. How to precisely benchmark
biomedical agents from an AI Scientist perspective remains largely unexplored.
To this end, we draw inspiration from one most important abilities of
scientists, understanding the literature, and introduce BioKGBench. In contrast
to traditional evaluation benchmark that only focuses on factual QA, where the
LLMs are known to have hallucination issues, we first disentangle
"Understanding Literature" into two atomic abilities, i) "Understanding" the
unstructured text from research papers by performing scientific claim
verification, and ii) Ability to interact with structured Knowledge-Graph
Question-Answering (KGQA) as a form of "Literature" grounding. We then
formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based
Retrieval-Augmented Generation (RAG) to identify the factual errors of existing
large-scale knowledge graph databases. We collect over two thousand data for
two atomic tasks and 225 high-quality annotated data for the agent task.
Surprisingly, we discover that state-of-the-art agents, both daily scenarios
and biomedical ones, have either failed or inferior performance on our
benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent.
On the widely used popular knowledge graph, we discover over 90 factual errors
which provide scenarios for agents to make discoveries and demonstrate the
effectiveness of our approach. The code and data are available at
https://github.com/westlake-autolab/BioKGBench.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2407.00466v1},
File          = {2407.00466v1.pdf}
}
@article{2310.03269v1,
Author        = {Zeyuan Wang and Qiang Zhang and Keyan Ding and Ming Qin and Xiang Zhuang and Xiaotong Li and Huajun Chen},
Title         = {InstructProtein: Aligning Human and Protein Language via Knowledge
  Instruction},
Eprint        = {2310.03269v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.BM},
Abstract      = {Large Language Models (LLMs) have revolutionized the field of natural
language processing, but they fall short in comprehending biological sequences
such as proteins. To address this challenge, we propose InstructProtein, an
innovative LLM that possesses bidirectional generation capabilities in both
human and protein languages: (i) taking a protein sequence as input to predict
its textual function description and (ii) using natural language to prompt
protein sequence generation. To achieve this, we first pre-train an LLM on both
protein and natural language corpora, enabling it to comprehend individual
languages. Then supervised instruction tuning is employed to facilitate the
alignment of these two distinct languages. Herein, we introduce a knowledge
graph-based instruction generation framework to construct a high-quality
instruction dataset, addressing annotation imbalance and instruction deficits
in existing protein-text corpus. In particular, the instructions inherit the
structural relations between proteins and function annotations in knowledge
graphs, which empowers our model to engage in the causal modeling of protein
functions, akin to the chain-of-thought processes in natural languages.
Extensive experiments on bidirectional protein-text generation tasks show that
InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover,
InstructProtein serves as a pioneering step towards text-based protein function
prediction and sequence design, effectively bridging the gap between protein
and human language understanding.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.03269v1},
File          = {2310.03269v1.pdf}
}
@article{2402.11804v3,
Author        = {Kai Wang and Yuwei Xu and Zhiyong Wu and Siqiang Luo},
Title         = {LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge
  Graphs},
Eprint        = {2402.11804v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts
from new KGs that are not seen during training, has been widely adopted in
various applications. One critical challenge of KG inductive reasoning is
handling low-resource scenarios with scarcity in both textual and structural
aspects. In this paper, we attempt to address this challenge with Large
Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to
generate a graph-structural prompt to enhance the pre-trained Graph Neural
Networks (GNNs), which brings us new methodological insights into the KG
inductive reasoning methods, as well as high generalizability in practice. On
the methodological side, we introduce a novel pretraining and prompting
framework ProLINK, designed for low-resource inductive reasoning across
arbitrary KGs without requiring additional training. On the practical side, we
experimentally evaluate our approach on 36 low-resource KG datasets and find
that ProLINK outperforms previous methods in three-shot, one-shot, and
zero-shot reasoning tasks, exhibiting average performance improvements by 20%,
45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong
robustness for various LLM promptings as well as full-shot scenarios.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.11804v3},
File          = {2402.11804v3.pdf}
}
@article{2402.12352v1,
Author        = {Julien Delile and Srayanta Mukherjee and Anton Van Pamel and Leonid Zhukov},
Title         = {Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge},
Eprint        = {2402.12352v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) are transforming the way information is
retrieved with vast amounts of knowledge being summarized and presented via
natural language conversations. Yet, LLMs are prone to highlight the most
frequently seen pieces of information from the training set and to neglect the
rare ones. In the field of biomedical research, latest discoveries are key to
academic and industrial actors and are obscured by the abundance of an
ever-increasing literature corpus (the information overload problem). Surfacing
new associations between biomedical entities, e.g., drugs, genes, diseases,
with LLMs becomes a challenge of capturing the long-tail knowledge of the
biomedical scientific production. To overcome this challenge, Retrieval
Augmented Generation (RAG) has been proposed to alleviate some of the
shortcomings of LLMs by augmenting the prompts with context retrieved from
external datasets. RAG methods typically select the context via maximum
similarity search over text embeddings. In this study, we show that RAG methods
leave out a significant proportion of relevant information due to clusters of
over-represented concepts in the biomedical literature. We introduce a novel
information-retrieval method that leverages a knowledge graph to downsample
these clusters and mitigate the information overload problem. Its retrieval
performance is about twice better than embedding similarity alternatives on
both precision and recall. Finally, we demonstrate that both embedding
similarity and knowledge graph retrieval methods can be advantageously combined
into a hybrid model that outperforms both, enabling potential improvements to
biomedical question-answering models.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.12352v1},
File          = {2402.12352v1.pdf}
}
@article{2411.14479v1,
Author        = {Yuze Liu and Tingjie Liu and Tiehua Zhang and Youhua Xia and Jinze Wang and Zhishu Shen and Jiong Jin and Fei Richard Yu},
Title         = {GRL-Prompt: Towards Knowledge Graph based Prompt Optimization via
  Reinforcement Learning},
Eprint        = {2411.14479v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have demonstrated impressive success in a wide
range of natural language processing (NLP) tasks due to their extensive general
knowledge of the world. Recent works discovered that the performance of LLMs is
heavily dependent on the input prompt. However, prompt engineering is usually
done manually in a trial-and-error fashion, which can be labor-intensive and
challenging in order to find the optimal prompts. To address these problems and
unleash the utmost potential of LLMs, we propose a novel LLMs-agnostic
framework for prompt optimization, namely GRL-Prompt, which aims to
automatically construct optimal prompts via reinforcement learning (RL) in an
end-to-end manner. To provide structured action/state representation for
optimizing prompts, we construct a knowledge graph (KG) that better encodes the
correlation between the user query and candidate in-context examples.
Furthermore, a policy network is formulated to generate the optimal action by
selecting a set of in-context examples in a rewardable order to construct the
prompt. Additionally, the embedding-based reward shaping is utilized to
stabilize the RL training process. The experimental results show that
GRL-Prompt outperforms recent state-of-the-art methods, achieving an average
increase of 0.10 in ROUGE-1, 0.07 in ROUGE-2, 0.07 in ROUGE-L, and 0.05 in
BLEU.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.14479v1},
File          = {2411.14479v1.pdf}
}
@article{2501.12300v1,
Author        = {Hasan Abu-Rasheed and Constance Jumbo and Rashed Al Amin and Christian Weber and Veit Wiese and Roman Obermaisser and Madjid Fathi},
Title         = {LLM-Assisted Knowledge Graph Completion for Curriculum and Domain
  Modelling in Personalized Higher Education Recommendations},
Eprint        = {2501.12300v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {While learning personalization offers great potential for learners, modern
practices in higher education require a deeper consideration of domain models
and learning contexts, to develop effective personalization algorithms. This
paper introduces an innovative approach to higher education curriculum
modelling that utilizes large language models (LLMs) for knowledge graph (KG)
completion, with the goal of creating personalized learning-path
recommendations. Our research focuses on modelling university subjects and
linking their topics to corresponding domain models, enabling the integration
of learning modules from different faculties and institutions in the student's
learning path. Central to our approach is a collaborative process, where LLMs
assist human experts in extracting high-quality, fine-grained topics from
lecture materials. We develop a domain, curriculum, and user models for
university modules and stakeholders. We implement this model to create the KG
from two study modules: Embedded Systems and Development of Embedded Systems
Using FPGA. The resulting KG structures the curriculum and links it to the
domain models. We evaluate our approach through qualitative expert feedback and
quantitative graph quality metrics. Domain experts validated the relevance and
accuracy of the model, while the graph quality metrics measured the structural
properties of our KG. Our results show that the LLM-assisted graph completion
approach enhances the ability to connect related courses across disciplines to
personalize the learning experience. Expert feedback also showed high
acceptance of the proposed collaborative approach for concept extraction and
classification.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.12300v1},
File          = {2501.12300v1.pdf}
}
@article{2305.04989v1,
Author        = {Kaushik Roy and Tarun Garg and Vedant Palit and Yuxin Zi and Vignesh Narayanan and Amit Sheth},
Title         = {Knowledge Graph Guided Semantic Evaluation of Language Models For User
  Trust},
Eprint        = {2305.04989v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {A fundamental question in natural language processing is - what kind of
language structure and semantics is the language model capturing? Graph formats
such as knowledge graphs are easy to evaluate as they explicitly express
language semantics and structure. This study evaluates the semantics encoded in
the self-attention transformers by leveraging explicit knowledge graph
structures. We propose novel metrics to measure the reconstruction error when
providing graph path sequences from a knowledge graph and trying to
reproduce/reconstruct the same from the outputs of the self-attention
transformer models. The opacity of language models has an immense bearing on
societal issues of trust and explainable decision outcomes. Our findings
suggest that language models are models of stochastic control processes for
plausible language pattern generation. However, they do not ascribe object and
concept-level meaning and semantics to the learned stochastic patterns such as
those described in knowledge graphs. Furthermore, to enable robust evaluation
of concept understanding by language models, we construct and make public an
augmented language understanding benchmark built on the General Language
Understanding Evaluation (GLUE) benchmark. This has significant
application-level user trust implications as stochastic patterns without a
strong sense of meaning cannot be trusted in high-stakes applications.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.04989v1},
File          = {2305.04989v1.pdf}
}
@article{2310.01061v2,
Author        = {Linhao Luo and Yuan-Fang Li and Gholamreza Haffari and Shirui Pan},
Title         = {Reasoning on Graphs: Faithful and Interpretable Large Language Model
  Reasoning},
Eprint        = {2310.01061v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have demonstrated impressive reasoning abilities
in complex tasks. However, they lack up-to-date knowledge and experience
hallucinations during reasoning, which can lead to incorrect reasoning
processes and diminish their performance and trustworthiness. Knowledge graphs
(KGs), which capture vast amounts of facts in a structured format, offer a
reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM
reasoning methods only treat KGs as factual knowledge bases and overlook the
importance of their structural information for reasoning. In this paper, we
propose a novel method called reasoning on graphs (RoG) that synergizes LLMs
with KGs to enable faithful and interpretable reasoning. Specifically, we
present a planning-retrieval-reasoning framework, where RoG first generates
relation paths grounded by KGs as faithful plans. These plans are then used to
retrieve valid reasoning paths from the KGs for LLMs to conduct faithful
reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the
reasoning ability of LLMs through training but also allows seamless integration
with any arbitrary LLMs during inference. Extensive experiments on two
benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art
performance on KG reasoning tasks and generates faithful and interpretable
reasoning results.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.01061v2},
File          = {2310.01061v2.pdf}
}
@article{2310.17892v2,
Author        = {Wujun Shao and Pengli Ji and Dongwei Fan and Yaohua Hu and Xiaoran Yan and Chenzhou Cui and Linying Mi and Lang Chen and Rui Zhang},
Title         = {Astronomical Knowledge Entity Extraction in Astrophysics Journal
  Articles via Large Language Models},
Eprint        = {2310.17892v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {astro-ph.IM},
Abstract      = {Astronomical knowledge entities, such as celestial object identifiers, are
crucial for literature retrieval and knowledge graph construction, and other
research and applications in the field of astronomy. Traditional methods of
extracting knowledge entities from texts face challenges like high manual
effort, poor generalization, and costly maintenance. Consequently, there is a
pressing need for improved methods to efficiently extract them. This study
explores the potential of pre-trained Large Language Models (LLMs) to perform
astronomical knowledge entity extraction (KEE) task from astrophysical journal
articles using prompts. We propose a prompting strategy called Prompt-KEE,
which includes five prompt elements, and design eight combination prompts based
on them. Celestial object identifier and telescope name, two most typical
astronomical knowledge entities, are selected to be experimental object. And we
introduce four currently representative LLMs, namely Llama-2-70B, GPT-3.5,
GPT-4, and Claude 2. To accommodate their token limitations, we construct two
datasets: the full texts and paragraph collections of 30 articles. Leveraging
the eight prompts, we test on full texts with GPT-4 and Claude 2, on paragraph
collections with all LLMs. The experimental results demonstrated that
pre-trained LLMs have the significant potential to perform KEE tasks in
astrophysics journal articles, but there are differences in their performance.
Furthermore, we analyze some important factors that influence the performance
of LLMs in entity extraction and provide insights for future KEE tasks in
astrophysical articles using LLMs.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.17892v2},
File          = {2310.17892v2.pdf}
}
@article{2312.02706v2,
Author        = {Huajun Chen},
Title         = {Large Knowledge Model: Perspectives and Challenges},
Eprint        = {2312.02706v2},
DOI           = {10.3724/2096-7004.di.2024.0001},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Humankind's understanding of the world is fundamentally linked to our
perception and cognition, with \emph{human languages} serving as one of the
major carriers of \emph{world knowledge}. In this vein, \emph{Large Language
Models} (LLMs) like ChatGPT epitomize the pre-training of extensive,
sequence-based world knowledge into neural networks, facilitating the
processing and manipulation of this knowledge in a parametric space. This
article explores large models through the lens of "knowledge". We initially
investigate the role of symbolic knowledge such as Knowledge Graphs (KGs) in
enhancing LLMs, covering aspects like knowledge-augmented language model,
structure-inducing pre-training, knowledgeable prompts, structured CoT,
knowledge editing, semantic tools for LLM and knowledgeable AI agents.
Subsequently, we examine how LLMs can boost traditional symbolic knowledge
bases, encompassing aspects like using LLM as KG builder and controller,
structured knowledge pretraining, and LLM-enhanced symbolic reasoning.
Considering the intricate nature of human knowledge, we advocate for the
creation of \emph{Large Knowledge Models} (LKM), specifically engineered to
manage diversified spectrum of knowledge structures. This promising undertaking
would entail several key challenges, such as disentangling knowledge base from
language models, cognitive alignment with human knowledge, integration of
perception and cognition, and building large commonsense models for interacting
with physical world, among others. We finally propose a five-"A" principle to
distinguish the concept of LKM.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.02706v2},
File          = {2312.02706v2.pdf}
}
@article{2405.13401v4,
Author        = {Pengzhou Cheng and Yidong Ding and Tianjie Ju and Zongru Wu and Wei Du and Ping Yi and Zhuosheng Zhang and Gongshen Liu},
Title         = {TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in
  Large Language Models},
Eprint        = {2405.13401v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Large language models (LLMs) have raised concerns about potential security
threats despite performing significantly in Natural Language Processing (NLP).
Backdoor attacks initially verified that LLM is doing substantial harm at all
stages, but the cost and robustness have been criticized. Attacking LLMs is
inherently risky in security review, while prohibitively expensive. Besides,
the continuous iteration of LLMs will degrade the robustness of backdoors. In
this paper, we propose TrojanRAG, which employs a joint backdoor attack in the
Retrieval-Augmented Generation, thereby manipulating LLMs in universal attack
scenarios. Specifically, the adversary constructs elaborate target contexts and
trigger sets. Multiple pairs of backdoor shortcuts are orthogonally optimized
by contrastive learning, thus constraining the triggering conditions to a
parameter subspace to improve the matching. To improve the recall of the RAG
for the target contexts, we introduce a knowledge graph to construct structured
data to achieve hard matching at a fine-grained level. Moreover, we normalize
the backdoor scenarios in LLMs to analyze the real harm caused by backdoors
from both attackers' and users' perspectives and further verify whether the
context is a favorable tool for jailbreaking models. Extensive experimental
results on truthfulness, language understanding, and harmfulness show that
TrojanRAG exhibits versatility threats while maintaining retrieval capabilities
on normal queries.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.13401v4},
File          = {2405.13401v4.pdf}
}
@article{2406.10710v2,
Author        = {Ziije Zhong and Linqing Zhong and Zhaoze Sun and Qingyun Jin and Zengchang Qin and Xiaofan Zhang},
Title         = {SyntheT2C: Generating Synthetic Data for Fine-Tuning Large Language
  Models on the Text2Cypher Task},
Eprint        = {2406.10710v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Integrating Large Language Models (LLMs) with existing Knowledge Graph (KG)
databases presents a promising avenue for enhancing LLMs' efficacy and
mitigating their "hallucinations". Given that most KGs reside in graph
databases accessible solely through specialized query languages (e.g., Cypher),
it is critical to connect LLMs with KG databases by automating the translation
of natural language into Cypher queries (termed as "Text2Cypher" task). Prior
efforts tried to bolster LLMs' proficiency in Cypher generation through
Supervised Fine-Tuning (SFT). However, these explorations are hindered by the
lack of annotated datasets of Query-Cypher pairs, resulting from the
labor-intensive and domain-specific nature of such annotation. In this study,
we propose SyntheT2C, a methodology for constructing a synthetic Query-Cypher
pair dataset, comprising two distinct pipelines: (1) LLM-based prompting and
(2) template-filling. SyntheT2C is applied to two medical KG databases,
culminating in the creation of a synthetic dataset, MedT2C. Comprehensive
experiments demonstrate that the MedT2C dataset effectively enhances the
performance of backbone LLMs on Text2Cypher task via SFT. Both the SyntheT2C
codebase and the MedT2C dataset are released in
https://github.com/ZGChung/SyntheT2C.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.10710v2},
File          = {2406.10710v2.pdf}
}
@article{2407.13989v3,
Author        = {Quan Li and Tianxiang Zhao and Lingwei Chen and Junjie Xu and Suhang Wang},
Title         = {Enhancing Graph Neural Networks with Limited Labeled Data by Actively
  Distilling Knowledge from Large Language Models},
Eprint        = {2407.13989v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Graphs are pervasive in the real-world, such as social network analysis,
bioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great
ability in node classification, a fundamental task on graphs. Unfortunately,
conventional GNNs still face challenges in scenarios with few labeled nodes,
despite the prevalence of few-shot node classification tasks in real-world
applications. To address this challenge, various approaches have been proposed,
including graph meta-learning, transfer learning, and methods based on Large
Language Models (LLMs). However, traditional meta-learning and transfer
learning methods often require prior knowledge from base classes or fail to
exploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based
methods may overlook the zero-shot capabilities of LLMs and rely heavily on the
quality of generated contexts. In this paper, we propose a novel approach that
integrates LLMs and GNNs, leveraging the zero-shot inference and reasoning
capabilities of LLMs and employing a Graph-LLM-based active learning paradigm
to enhance GNNs' performance. Extensive experiments demonstrate the
effectiveness of our model in improving node classification accuracy with
considerably limited labeled data, surpassing state-of-the-art baselines by
significant margins.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.13989v3},
File          = {2407.13989v3.pdf}
}
@article{2410.12130v1,
Author        = {Huiwen Wu and Xiaohan Li and Xiaogang Xu and Jiafei Wu and Deyi Zhang and Zhe Liu},
Title         = {Iter-AHMCL: Alleviate Hallucination for Large Language Model via
  Iterative Model-level Contrastive Learning},
Eprint        = {2410.12130v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The development of Large Language Models (LLMs) has significantly advanced
various AI applications in commercial and scientific research fields, such as
scientific literature summarization, writing assistance, and knowledge graph
construction. However, a significant challenge is the high risk of
hallucination during LLM inference, which can lead to security concerns like
factual inaccuracies, inconsistent information, and fabricated content. To
tackle this issue, it is essential to develop effective methods for reducing
hallucination while maintaining the original capabilities of the LLM. This
paper introduces a novel approach called Iterative Model-level Contrastive
Learning (Iter-AHMCL) to address hallucination. This method modifies the
representation layers of pre-trained LLMs by using contrastive `positive' and
`negative' models, trained on data with and without hallucinations. By
leveraging the differences between these two models, we create a more
straightforward pathway to eliminate hallucinations, and the iterative nature
of contrastive learning further enhances performance. Experimental validation
on four pre-trained foundation LLMs (LLaMA2, Alpaca, LLaMA3, and Qwen)
finetuning with a specially designed dataset shows that our approach achieves
an average improvement of 10.1 points on the TruthfulQA benchmark.
Comprehensive experiments demonstrate the effectiveness of Iter-AHMCL in
reducing hallucination while maintaining the general capabilities of LLMs.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.12130v1},
File          = {2410.12130v1.pdf}
}
@article{2411.02382v1,
Author        = {Guangzhi Xiong and Eric Xie and Amir Hassan Shariatmadari and Sikun Guo and Stefan Bekiranov and Aidong Zhang},
Title         = {Improving Scientific Hypothesis Generation with Knowledge Grounded Large
  Language Models},
Eprint        = {2411.02382v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have demonstrated remarkable capabilities in
various scientific domains, from natural language processing to complex
problem-solving tasks. Their ability to understand and generate human-like text
has opened up new possibilities for advancing scientific research, enabling
tasks such as data analysis, literature review, and even experimental design.
One of the most promising applications of LLMs in this context is hypothesis
generation, where they can identify novel research directions by analyzing
existing knowledge. However, despite their potential, LLMs are prone to
generating ``hallucinations'', outputs that are plausible-sounding but
factually incorrect. Such a problem presents significant challenges in
scientific fields that demand rigorous accuracy and verifiability, potentially
leading to erroneous or misleading conclusions. To overcome these challenges,
we propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that
enhances LLM hypothesis generation by integrating external, structured
knowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured
reasoning process, organizing their output as a chain of ideas (CoI), and
includes a KG-supported module for the detection of hallucinations. With
experiments on our newly constructed hypothesis generation dataset, we
demonstrate that KG-CoI not only improves the accuracy of LLM-generated
hypotheses but also reduces the hallucination in their reasoning chains,
highlighting its effectiveness in advancing real-world scientific research.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.02382v1},
File          = {2411.02382v1.pdf}
}
@article{2404.14928v2,
Author        = {Wenqi Fan and Shijie Wang and Jiani Huang and Zhikai Chen and Yu Song and Wenzhuo Tang and Haitao Mao and Hui Liu and Xiaorui Liu and Dawei Yin and Qing Li},
Title         = {Graph Machine Learning in the Era of Large Language Models (LLMs)},
Eprint        = {2404.14928v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Graphs play an important role in representing complex relationships in
various domains like social networks, knowledge graphs, and molecular
discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have
emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the
representation and processing of graph structures. Recently, LLMs have
demonstrated unprecedented capabilities in language tasks and are widely
adopted in a variety of applications such as computer vision and recommender
systems. This remarkable success has also attracted interest in applying LLMs
to the graph domain. Increasing efforts have been made to explore the potential
of LLMs in advancing Graph ML's generalization, transferability, and few-shot
learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in
reliable factual knowledge, which can be utilized to enhance the reasoning
capabilities of LLMs and potentially alleviate their limitations such as
hallucinations and the lack of explainability. Given the rapid progress of this
research direction, a systematic review summarizing the latest advancements for
Graph ML in the era of LLMs is necessary to provide an in-depth understanding
to researchers and practitioners. Therefore, in this survey, we first review
the recent developments in Graph ML. We then explore how LLMs can be utilized
to enhance the quality of graph features, alleviate the reliance on labeled
data, and address challenges such as graph heterogeneity and
out-of-distribution (OOD) generalization. Afterward, we delve into how graphs
can enhance LLMs, highlighting their abilities to enhance LLM pre-training and
inference. Furthermore, we investigate various applications and discuss the
potential future directions in this promising field.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.14928v2},
File          = {2404.14928v2.pdf}
}
@article{2307.11769v1,
Author        = {Yun Tang and Antonio A. Bruto da Costa and Jason Zhang and Irvine Patrick and Siddartha Khastgir and Paul Jennings},
Title         = {Domain Knowledge Distillation from Large Language Model: An Empirical
  Study in the Autonomous Driving Domain},
Eprint        = {2307.11769v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Engineering knowledge-based (or expert) systems require extensive manual
effort and domain knowledge. As Large Language Models (LLMs) are trained using
an enormous amount of cross-domain knowledge, it becomes possible to automate
such engineering processes. This paper presents an empirical automation and
semi-automation framework for domain knowledge distillation using prompt
engineering and the LLM ChatGPT. We assess the framework empirically in the
autonomous driving domain and present our key observations. In our
implementation, we construct the domain knowledge ontology by "chatting" with
ChatGPT. The key finding is that while fully automated domain ontology
construction is possible, human supervision and early intervention typically
improve efficiency and output quality as they lessen the effects of response
randomness and the butterfly effect. We, therefore, also develop a web-based
distillation assistant enabling supervision and flexible intervention at
runtime. We hope our findings and tools could inspire future research toward
revolutionizing the engineering of knowledge-based systems across application
domains.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.11769v1},
File          = {2307.11769v1.pdf}
}
@article{2307.15833v1,
Author        = {Wei Zhou and Xiangyu Peng and Mark Riedl},
Title         = {Dialogue Shaping: Empowering Agents through NPC Interaction},
Eprint        = {2307.15833v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {One major challenge in reinforcement learning (RL) is the large amount of
steps for the RL agent needs to converge in the training process and learn the
optimal policy, especially in text-based game environments where the action
space is extensive. However, non-player characters (NPCs) sometimes hold some
key information about the game, which can potentially help to train RL agents
faster. Thus, this paper explores how to interact and converse with NPC agents
to get the key information using large language models (LLMs), as well as
incorporate this information to speed up RL agent's training using knowledge
graphs (KGs) and Story Shaping.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.15833v1},
File          = {2307.15833v1.pdf}
}
@article{2311.11334v1,
Author        = {Robert B. Allen},
Title         = {Using Causal Threads to Explain Changes in a Dynamic System},
Eprint        = {2311.11334v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {We explore developing rich semantic models of systems. Specifically, we
consider structured causal explanations about state changes in those systems.
Essentially, we are developing process-based dynamic knowledge graphs. As an
example, we construct a model of the causal threads for geological changes
proposed by the Snowball Earth theory. Further, we describe an early prototype
of a graphical interface to present the explanations. Unlike statistical
approaches to summarization and explanation such as Large Language Models
(LLMs), our approach of direct representation can be inspected and verified
directly.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.11334v1},
File          = {2311.11334v1.pdf}
}
@article{2305.13338v3,
Author        = {Marcin P. Joachimiak and J. Harry Caufield and Nomi L. Harris and Hyeongsik Kim and Christopher J. Mungall},
Title         = {Gene Set Summarization using Large Language Models},
Eprint        = {2305.13338v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.GN},
Abstract      = {Molecular biologists frequently interpret gene lists derived from
high-throughput experiments and computational analysis. This is typically done
as a statistical enrichment analysis that measures the over- or
under-representation of biological function terms associated with genes or
their properties, based on curated assertions from a knowledge base (KB) such
as the Gene Ontology (GO). Interpreting gene lists can also be framed as a
textual summarization task, enabling the use of Large Language Models (LLMs),
potentially utilizing scientific texts directly and avoiding reliance on a KB.
  We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language
Descriptions of Controlled Terms for Ontology Reporting), a method that uses
GPT models to perform gene set function summarization as a complement to
standard enrichment analysis. This method can use different sources of gene
functional information: (1) structured text derived from curated ontological KB
annotations, (2) ontology-free narrative gene summaries, or (3) direct model
retrieval.
  We demonstrate that these methods are able to generate plausible and
biologically valid summary GO term lists for gene sets. However, GPT-based
approaches are unable to deliver reliable scores or p-values and often return
terms that are not statistically significant. Crucially, these methods were
rarely able to recapitulate the most precise and informative term from standard
enrichment, likely due to an inability to generalize and reason using an
ontology. Results are highly nondeterministic, with minor variations in prompt
resulting in radically different term lists. Our results show that at this
point, LLM-based methods are unsuitable as a replacement for standard term
enrichment analysis and that manual curation of ontological assertions remains
necessary.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.13338v3},
File          = {2305.13338v3.pdf}
}
@article{2309.11361v2,
Author        = {Yuan An and Jane Greenberg and Alex Kalinowski and Xintong Zhao and Xiaohua Hu and Fernando J. Uribe-Romo and Kyle Langlois and Jacob Furst and Diego A. Gómez-Gualdrón},
Title         = {Knowledge Graph Question Answering for Materials Science (KGQA4MAT):
  Developing Natural Language Interface for Metal-Organic Frameworks Knowledge
  Graph (MOF-KG) Using LLM},
Eprint        = {2309.11361v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We present a comprehensive benchmark dataset for Knowledge Graph Question
Answering in Materials Science (KGQA4MAT), with a focus on metal-organic
frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) has
been constructed by integrating structured databases and knowledge extracted
from the literature. To enhance MOF-KG accessibility for domain experts, we aim
to develop a natural language interface for querying the knowledge graph. We
have developed a benchmark comprised of 161 complex questions involving
comparison, aggregation, and complicated graph structures. Each question is
rephrased in three additional variations, resulting in 644 questions and 161 KG
queries. To evaluate the benchmark, we have developed a systematic approach for
utilizing the LLM, ChatGPT, to translate natural language questions into formal
KG queries. We also apply the approach to the well-known QALD-9 dataset,
demonstrating ChatGPT's potential in addressing KGQA issues for different
platforms and query languages. The benchmark and the proposed approach aim to
stimulate further research and development of user-friendly and efficient
interfaces for querying domain-specific materials science knowledge graphs,
thereby accelerating the discovery of novel materials.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.11361v2},
File          = {2309.11361v2.pdf}
}
@article{2403.08593v2,
Author        = {Sitao Cheng and Ziyuan Zhuang and Yong Xu and Fangkai Yang and Chaoyun Zhang and Xiaoting Qin and Xiang Huang and Ling Chen and Qingwei Lin and Dongmei Zhang and Saravan Rajmohan and Qi Zhang},
Title         = {Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over
  Structured Environments},
Eprint        = {2403.08593v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have shown potential in reasoning over
structured environments, e.g., knowledge graph and table. Such tasks typically
require multi-hop reasoning, i.e., match natural language utterance with
instances in the environment. Previous methods leverage LLMs to incrementally
build a reasoning path, where the LLMs either invoke tools or pick up schemas
by step-by-step interacting with the environment. We propose
Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently
and faithfully reason over structured environments. In Readi, LLMs initially
generate a reasoning path given a query, and edit the path only when necessary.
We instantiate the path on structured environments and provide feedback to edit
the path if anything goes wrong. Experimental results on three KGQA and two
TableQA datasets show the effectiveness of Readi, significantly surpassing
previous LLM-based methods (by 9.1% Hit@1 on WebQSP, 12.4% on MQA-3H and 9.5%
on WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and
74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ).
Our code will be available on https://aka.ms/readi.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.08593v2},
File          = {2403.08593v2.pdf}
}
@article{2405.04819v4,
Author        = {Dawei Li and Shu Yang and Zhen Tan and Jae Young Baik and Sukwon Yun and Joseph Lee and Aaron Chacko and Bojian Hou and Duy Duong-Tran and Ying Ding and Huan Liu and Li Shen and Tianlong Chen},
Title         = {DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's
  Disease Questions with Scientific Literature},
Eprint        = {2405.04819v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent advancements in large language models (LLMs) have achieved promising
performances across various applications. Nonetheless, the ongoing challenge of
integrating long-tail knowledge continues to impede the seamless adoption of
LLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic
Co-Augmentation of LLMs and KG, to address this limitation and demonstrate its
ability on studying Alzheimer's Disease (AD), a specialized sub-field in
biomedicine and a global health priority. With a synergized framework of LLM
and KG mutually enhancing each other, we first leverage LLM to construct an
evolving AD-specific knowledge graph (KG) sourced from AD-related scientific
literature, and then we utilize a coarse-to-fine sampling method with a novel
self-aware knowledge retrieval approach to select appropriate knowledge from
the KG to augment LLM inference capabilities. The experimental results,
conducted on our constructed AD question answering (ADQA) benchmark, underscore
the efficacy of DALK. Additionally, we perform a series of detailed analyses
that can offer valuable insights and guidelines for the emerging topic of
mutually enhancing KG and LLM. We will release the code and data at
https://github.com/David-Li0406/DALK.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.04819v4},
File          = {2405.04819v4.pdf}
}
@article{2409.19058v1,
Author        = {Haobo Li and Zhaowei Wang and Jiachen Wang and Alexis Kai Hon Lau and Huamin Qu},
Title         = {CLLMate: A Multimodal LLM for Weather and Climate Events Forecasting},
Eprint        = {2409.19058v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Forecasting weather and climate events is crucial for making appropriate
measures to mitigate environmental hazards and minimize associated losses.
Previous research on environmental forecasting focuses on predicting numerical
meteorological variables related to closed-set events rather than forecasting
open-set events directly, which limits the comprehensiveness of event
forecasting. We propose Weather and Climate Event Forecasting (WCEF), a new
task that leverages meteorological raster data and textual event data to
predict potential weather and climate events. However, due to difficulties in
aligning multimodal data and the lack of sufficient supervised datasets, this
task is challenging to accomplish. Therefore, we first propose a framework to
align historical meteorological data with past weather and climate events using
the large language model (LLM). In this framework, we construct a knowledge
graph by using LLM to extract information about weather and climate events from
a corpus of over 41k highly environment-focused news articles. Subsequently, we
mapped these events with meteorological raster data, creating a supervised
dataset, which is the largest and most novel for LLM tuning on the WCEF task.
Finally, we introduced our aligned models, CLLMate (LLM for climate), a
multimodal LLM to forecast weather and climate events using meteorological
raster data. In evaluating CLLMate, we conducted extensive experiments. The
results indicate that CLLMate surpasses both the baselines and other multimodal
LLMs, showcasing the potential of utilizing LLM to align weather and climate
events with meteorological data and highlighting the promising future for
research on the WCEF task.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.19058v1},
File          = {2409.19058v1.pdf}
}
@article{2402.07016v1,
Author        = {Yinghao Zhu and Changyu Ren and Shiyun Xie and Shukai Liu and Hangyuan Ji and Zixiang Wang and Tao Sun and Long He and Zhoujun Li and Xi Zhu and Chengwei Pan},
Title         = {REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records
  Analysis via Large Language Models},
Eprint        = {2402.07016v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The integration of multimodal Electronic Health Records (EHR) data has
significantly improved clinical predictive capabilities. Leveraging clinical
notes and multivariate time-series EHR, existing models often lack the medical
context relevent to clinical tasks, prompting the incorporation of external
knowledge, particularly from the knowledge graph (KG). Previous approaches with
KG knowledge have primarily focused on structured knowledge extraction,
neglecting unstructured data modalities and semantic high dimensional medical
knowledge. In response, we propose REALM, a Retrieval-Augmented Generation
(RAG) driven framework to enhance multimodal EHR representations that address
these limitations. Firstly, we apply Large Language Model (LLM) to encode long
context clinical notes and GRU model to encode time-series EHR data. Secondly,
we prompt LLM to extract task-relevant medical entities and match entities in
professionally labeled external knowledge graph (PrimeKG) with corresponding
medical knowledge. By matching and aligning with clinical standards, our
framework eliminates hallucinations and ensures consistency. Lastly, we propose
an adaptive multimodal fusion network to integrate extracted knowledge with
multimodal EHR data. Our extensive experiments on MIMIC-III mortality and
readmission tasks showcase the superior performance of our REALM framework over
baselines, emphasizing the effectiveness of each module. REALM framework
contributes to refining the use of multimodal EHR data in healthcare and
bridging the gap with nuanced medical context essential for informed clinical
predictions.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.07016v1},
File          = {2402.07016v1.pdf}
}
@article{2406.09923v2,
Author        = {Mingyu Derek Ma and Chenchen Ye and Yu Yan and Xiaoxuan Wang and Peipei Ping and Timothy S Chang and Wei Wang},
Title         = {CliBench: A Multifaceted and Multigranular Evaluation of Large Language
  Models for Clinical Decision Making},
Eprint        = {2406.09923v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The integration of Artificial Intelligence (AI), especially Large Language
Models (LLMs), into the clinical diagnosis process offers significant potential
to improve the efficiency and accessibility of medical care. While LLMs have
shown some promise in the medical domain, their application in clinical
diagnosis remains underexplored, especially in real-world clinical practice,
where highly sophisticated, patient-specific decisions need to be made. Current
evaluations of LLMs in this field are often narrow in scope, focusing on
specific diseases or specialties and employing simplified diagnostic tasks. To
bridge this gap, we introduce CliBench, a novel benchmark developed from the
MIMIC IV dataset, offering a comprehensive and realistic assessment of LLMs'
capabilities in clinical diagnosis. This benchmark not only covers diagnoses
from a diverse range of medical cases across various specialties but also
incorporates tasks of clinical significance: treatment procedure
identification, lab test ordering and medication prescriptions. Supported by
structured output ontologies, CliBench enables a precise and multi-granular
evaluation, offering an in-depth understanding of LLM's capability on diverse
clinical tasks of desired granularity. We conduct a zero-shot evaluation of
leading LLMs to assess their proficiency in clinical decision-making. Our
preliminary results shed light on the potential and limitations of current LLMs
in clinical settings, providing valuable insights for future advancements in
LLM-powered healthcare.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.09923v2},
File          = {2406.09923v2.pdf}
}
@article{2407.10735v2,
Author        = {Xabier E. Barandiaran and Lola S. Almendros},
Title         = {Transforming Agency. On the mode of existence of Large Language Models},
Eprint        = {2407.10735v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper investigates the ontological characterization of Large Language
Models (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we
pay special attention to their status as agents. This requires explaining in
detail the architecture, processing, and training procedures that enable LLMs
to display their capacities, and the extensions used to turn LLMs into
agent-like systems. After a systematic analysis we conclude that a LLM fails to
meet necessary and sufficient conditions for autonomous agency in the light of
embodied theories of mind: the individuality condition (it is not the product
of its own activity, it is not even directly affected by it), the normativity
condition (it does not generate its own norms or goals), and, partially the
interactional asymmetry condition (it is not the origin and sustained source of
its interaction with the environment). If not agents, then ... what are LLMs?
We argue that ChatGPT should be characterized as an interlocutor or linguistic
automaton, a library-that-talks, devoid of (autonomous) agency, but capable to
engage performatively on non-purposeful yet purpose-structured and
purpose-bounded tasks. When interacting with humans, a "ghostly" component of
the human-machine interaction makes it possible to enact genuine conversational
experiences with LLMs. Despite their lack of sensorimotor and biological
embodiment, LLMs textual embodiment (the training corpus) and resource-hungry
computational embodiment, significantly transform existing forms of human
agency. Beyond assisted and extended agency, the LLM-human coupling can produce
midtended forms of agency, closer to the production of intentional agency than
to the extended instrumentality of any previous technologies.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.10735v2},
File          = {2407.10735v2.pdf}
}
@article{2201.09680v1,
Author        = {Qi Liu and Dani Yogatama and Phil Blunsom},
Title         = {Relational Memory Augmented Language Models},
Eprint        = {2201.09680v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present a memory-augmented approach to condition an autoregressive
language model on a knowledge graph. We represent the graph as a collection of
relation triples and retrieve relevant relations for a given context to improve
text generation. Experiments on WikiText-103, WMT19, and enwik8 English
datasets demonstrate that our approach produces a better language model in
terms of perplexity and bits per character. We also show that relational memory
improves coherence, is complementary to token-based memory, and enables causal
interventions. Our model provides a simple yet effective way to combine an
autoregressive language model with a knowledge graph for a more coherent and
logical generation.},
Year          = {2022},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2201.09680v1},
File          = {2201.09680v1.pdf}
}
@article{2412.03815v1,
Author        = {Samuel Abedu and SayedHassan Khatoonabadi and Emad Shihab},
Title         = {Synergizing LLMs and Knowledge Graphs: A Novel Approach to Software
  Repository-Related Question Answering},
Eprint        = {2412.03815v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Software repositories contain valuable information for gaining insights into
their development process. However, extracting insights from these repository
data is time-consuming and requires technical expertise. While software
engineering chatbots have been developed to facilitate natural language
interactions with repositories, they struggle with understanding natural
language and accurately retrieving relevant data. This study aims to improve
the accuracy of LLM-based chatbots in answering repository-related questions by
augmenting them with knowledge graphs. We achieve this in a two-step approach;
(1) constructing a knowledge graph from the repository data and (2) synergizing
the knowledge graph with LLM to allow for the natural language questions and
answers. We curated a set of 20 questions with different complexities and
evaluated our approach on five popular open-source projects. Our approach
achieved an accuracy of 65%. We further investigated the limitations and
identified six key issues, with the majority relating to the reasoning
capability of the LLM. We experimented with a few-shot chain-of-thought
prompting to determine if it could enhance our approach. This technique
improved the overall accuracy to 84%. Our findings demonstrate the synergy
between LLMs and knowledge graphs as a viable solution for making repository
data accessible to both technical and non-technical stakeholders.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.03815v1},
File          = {2412.03815v1.pdf}
}
@article{2410.16196v1,
Author        = {Alex Clay and Ernesto Jiménez-Ruiz},
Title         = {Information for Conversation Generation: Proposals Utilising Knowledge
  Graphs},
Eprint        = {2410.16196v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {LLMs are frequently used tools for conversational generation. Without
additional information LLMs can generate lower quality responses due to lacking
relevant content and hallucinations, as well as the perception of poor
emotional capability, and an inability to maintain a consistent character.
Knowledge graphs are commonly used forms of external knowledge and may provide
solutions to these challenges. This paper introduces three proposals, utilizing
knowledge graphs to enhance LLM generation. Firstly, dynamic knowledge graph
embeddings and recommendation could allow for the integration of new
information and the selection of relevant knowledge for response generation.
Secondly, storing entities with emotional values as additional features may
provide knowledge that is better emotionally aligned with the user input.
Thirdly, integrating character information through narrative bubbles would
maintain character consistency, as well as introducing a structure that would
readily incorporate new information.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.16196v1},
File          = {2410.16196v1.pdf}
}
@article{2308.12028v1,
Author        = {Chen hao and Xie Runfeng and Cui Xiangyang and Yan Zhou and Wang Xin and Xuan Zhanwei and Zhang Kai},
Title         = {LKPNR: LLM and KG for Personalized News Recommendation Framework},
Eprint        = {2308.12028v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Accurately recommending candidate news articles to users is a basic challenge
faced by personalized news recommendation systems. Traditional methods are
usually difficult to grasp the complex semantic information in news texts,
resulting in unsatisfactory recommendation results. Besides, these traditional
methods are more friendly to active users with rich historical behaviors.
However, they can not effectively solve the "long tail problem" of inactive
users. To address these issues, this research presents a novel general
framework that combines Large Language Models (LLM) and Knowledge Graphs (KG)
into semantic representations of traditional methods. In order to improve
semantic understanding in complex news texts, we use LLMs' powerful text
understanding ability to generate news representations containing rich semantic
information. In addition, our method combines the information about news
entities and mines high-order structural information through multiple hops in
KG, thus alleviating the challenge of long tail distribution. Experimental
results demonstrate that compared with various traditional models, the
framework significantly improves the recommendation effect. The successful
integration of LLM and KG in our framework has established a feasible path for
achieving more accurate personalized recommendations in the news field. Our
code is available at https://github.com/Xuan-ZW/LKPNR.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.12028v1},
File          = {2308.12028v1.pdf}
}
@article{2403.01382v1,
Author        = {Rohan Kumar and Youngmin Kim and Sunitha Ravi and Haitian Sun and Christos Faloutsos and Ruslan Salakhutdinov and Minji Yoon},
Title         = {Automatic Question-Answer Generation for Long-Tail Knowledge},
Eprint        = {2403.01382v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pretrained Large Language Models (LLMs) have gained significant attention for
addressing open-domain Question Answering (QA). While they exhibit high
accuracy in answering questions related to common knowledge, LLMs encounter
difficulties in learning about uncommon long-tail knowledge (tail entities).
Since manually constructing QA datasets demands substantial human resources,
the types of existing QA datasets are limited, leaving us with a scarcity of
datasets to study the performance of LLMs on tail entities. In this paper, we
propose an automatic approach to generate specialized QA datasets for tail
entities and present the associated research challenges. We conduct extensive
experiments by employing pretrained LLMs on our newly generated long-tail QA
datasets, comparing their performance with and without external resources
including Wikipedia and Wikidata knowledge graphs.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.01382v1},
File          = {2403.01382v1.pdf}
}
@article{2403.02966v3,
Author        = {Sungho Ko and Hyunjin Cho and Hyungjoo Chae and Jinyoung Yeo and Dongha Lee},
Title         = {Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot
  Question Answering},
Eprint        = {2403.02966v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance
Quesetion Answering (QA) performance of Large Language Models (LLMs), yet
structured KG verbalization remains challengin. Existing methods, such as
triple-form or free-form textual conversion of triple-form facts, encounter
several issues. These include reduced evidence density due to duplicated
entities or relationships, and reduced evidence clarity due to an inability to
emphasize crucial evidence. To address these issues, we propose EFSum, an
Evidence-focused Fact Summarization framework for enhanced QA with
knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer
through distillation and preference alignment. Our extensive experiments show
that EFSum improves LLM's zero-shot QA performance, and it is possible to
ensure both the helpfulness and faithfulness of the summary.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.02966v3},
File          = {2403.02966v3.pdf}
}
@article{2403.06611v1,
Author        = {Jiageng Wu and Xian Wu and Yefeng Zheng and Jie Yang},
Title         = {MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway
  Encoding},
Eprint        = {2403.06611v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {With appropriate data selection and training techniques, Large Language
Models (LLMs) have demonstrated exceptional success in various medical
examinations and multiple-choice questions. However, the application of LLMs in
medical dialogue generation-a task more closely aligned with actual medical
practice-has been less explored. This gap is attributed to the insufficient
medical knowledge of LLMs, which leads to inaccuracies and hallucinated
information in the generated medical responses. In this work, we introduce the
Medical dialogue with Knowledge enhancement and clinical Pathway encoding
(MedKP) framework, which integrates an external knowledge enhancement module
through a medical knowledge graph and an internal clinical pathway encoding via
medical entities and physician actions. Evaluated with comprehensive metrics,
our experiments on two large-scale, real-world online medical consultation
datasets (MedDG and KaMed) demonstrate that MedKP surpasses multiple baselines
and mitigates the incidence of hallucinations, achieving a new
state-of-the-art. Extensive ablation studies further reveal the effectiveness
of each component of MedKP. This enhancement advances the development of
reliable, automated medical consultation responses using LLMs, thereby
broadening the potential accessibility of precise and real-time medical
assistance.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.06611v1},
File          = {2403.06611v1.pdf}
}
@article{2408.15264v1,
Author        = {Georg Fuellen and Anton Kulaga and Sebastian Lobentanzer and Maximilian Unfried and Roberto Avelar and Daniel Palmer and Brian K. Kennedy},
Title         = {Validation Requirements for AI-based Intervention-Evaluation in Aging
  and Longevity Research and Practice},
Eprint        = {2408.15264v1},
DOI           = {10.1016/j.arr.2024.102617},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {The field of aging and longevity research is overwhelmed by vast amounts of
data, calling for the use of Artificial Intelligence (AI), including Large
Language Models (LLMs), for the evaluation of geroprotective interventions.
Such evaluations should be correct, useful, comprehensive, explainable, and
they should consider causality, interdisciplinarity, adherence to standards,
longitudinal data and known aging biology. In particular, comprehensive
analyses should go beyond comparing data based on canonical biomedical
databases, suggesting the use of AI to interpret changes in biomarkers and
outcomes. Our requirements motivate the use of LLMs with Knowledge Graphs and
dedicated workflows employing, e.g., Retrieval-Augmented Generation. While
naive trust in the responses of AI tools can cause harm, adding our
requirements to LLM queries can improve response quality, calling for
benchmarking efforts and justifying the informed use of LLMs for advice on
longevity interventions.},
Year          = {2024},
Month         = {Aug},
Note          = {Ageing Research Reviews (2025), 104, 102617},
Url           = {http://arxiv.org/abs/2408.15264v1},
File          = {2408.15264v1.pdf}
}
@article{2409.12887v2,
Author        = {Peichao Lai and Zhengfeng Zhang and Wentao Zhang and Fangcheng Fu and Bin Cui},
Title         = {Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data
  Augmentation and Gaussian-Decayed Contrastive Learning},
Eprint        = {2409.12887v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recently, using large language models (LLMs) for data augmentation has led to
considerable improvements in unsupervised sentence embedding models. However,
existing methods encounter two primary challenges: limited data diversity and
high data noise. Current approaches often neglect fine-grained knowledge, such
as entities and quantities, leading to insufficient diversity. Additionally,
unsupervised data frequently lacks discriminative information, and the
generated synthetic samples may introduce noise. In this paper, we propose a
pipeline-based data augmentation method via LLMs and introduce the
Gaussian-decayed gradient-assisted Contrastive Sentence Embedding (GCSE) model
to enhance unsupervised sentence embeddings. To tackle the issue of low data
diversity, our pipeline utilizes knowledge graphs (KGs) to extract entities and
quantities, enabling LLMs to generate more diverse, knowledge-enriched samples.
To address high data noise, the GCSE model uses a Gaussian-decayed function to
limit the impact of false hard negative samples, enhancing the model's
discriminative capability. Experimental results show that our approach achieves
state-of-the-art performance in semantic textual similarity (STS) tasks, using
fewer data samples and smaller LLMs, demonstrating its efficiency and
robustness across various models.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.12887v2},
File          = {2409.12887v2.pdf}
}
@article{2412.04342v1,
Author        = {Jiaan Wang and Fandong Meng and Yingxue Zhang and Jie Zhou},
Title         = {Retrieval-Augmented Machine Translation with Unstructured Knowledge},
Eprint        = {2412.04342v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Retrieval-augmented generation (RAG) introduces additional information to
enhance large language models (LLMs). In machine translation (MT), previous
work typically retrieves in-context examples from paired MT corpora, or
domain-specific knowledge from knowledge graphs, to enhance models' MT ability.
However, a large amount of world knowledge is organized in unstructured
documents, and might not be fully paired across different languages. In this
paper, we study retrieval-augmented MT using unstructured documents.
Specifically, we build RAGtrans, the first benchmark to train and evaluate
LLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samples
collected via GPT-4o and human translators. Besides, documents from different
languages are also provided to supply the knowledge to these samples. Based on
RAGtrans, we further propose a multi-task training method to teach LLMs how to
use information from multilingual documents during their translation. The
method uses existing multilingual corpora to create auxiliary training
objectives without additional labeling requirements. Extensive experiments show
that the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.04342v1},
File          = {2412.04342v1.pdf}
}
@article{2501.16842v1,
Author        = {Tiao Tan and Fengxiao Tang and Ming Zhao},
Title         = {Adapting Network Information to Semantics for Generalizable and
  Plug-and-Play Multi-Scenario Network Diagnosis},
Eprint        = {2501.16842v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NI},
Abstract      = {Network fault diagnosis is a core challenge in ensuring the stability and
reliability of modern network operations. Traditional approaches, limited by
their training on specific performance metrics for predefined scenarios,
struggle to generalize across diverse faults and anomalies in varying network
environments. In recent years, large language models (LLMs) have demonstrated
strong generalization capabilities across various domains. Building on this
success, we propose NetSemantic, a plug-and-play intelligent network fault
diagnosis framework based on LLMs. NetSemantic transforms multimodal network
information into unified textual representations, enabling LLMs to perform
reasoning and generate efficient fault resolutions and health assessment
reports. To further enhance the logical reasoning capabilities of LLMs, we
introduce a novel symbolic representation method that transforms logically
strong network information into symbols. Additionally, we propose a
self-adaptive data updating mechanism that dynamically incorporates network
information into a knowledge graph to ensure the validity and timeliness of the
knowledge base. Experimental results demonstrate that NetSemantic excels in
network fault diagnosis across various complex scenarios, significantly
improving diagnostic accuracy and reliability.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.16842v1},
File          = {2501.16842v1.pdf}
}
@article{2502.04644v1,
Author        = {Junde Wu and Jiayuan Zhu and Yuyuan Liu},
Title         = {Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research},
Eprint        = {2502.04644v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We introduce Agentic Reasoning, a framework that enhances large language
model (LLM) reasoning by integrating external tool-using agents. Unlike
conventional LLM-based reasoning approaches, which rely solely on internal
inference, Agentic Reasoning dynamically engages web search, code execution,
and structured reasoning-context memory to solve complex problems requiring
deep research and multi-step logical deduction. Our framework introduces the
Mind Map agent, which constructs a structured knowledge graph to track logical
relationships, improving deductive reasoning. Additionally, the integration of
web-search and coding agents enables real-time retrieval and computational
analysis, enhancing reasoning accuracy and decision-making. Evaluations on
PhD-level scientific reasoning (GPQA) and domain-specific deep research tasks
demonstrate that our approach significantly outperforms existing models,
including leading retrieval-augmented generation (RAG) systems and
closed-source LLMs. Moreover, our results indicate that agentic reasoning
improves expert-level knowledge synthesis, test-time scalability, and
structured problem-solving. The code is at:
https://github.com/theworldofagents/Agentic-Reasoning.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.04644v1},
File          = {2502.04644v1.pdf}
}
@article{2410.23703v1,
Author        = {Junda Wu and Xintong Li and Ruoyu Wang and Yu Xia and Yuxin Xiong and Jianing Wang and Tong Yu and Xiang Chen and Branislav Kveton and Lina Yao and Jingbo Shang and Julian McAuley},
Title         = {OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large
  Language Models},
Eprint        = {2410.23703v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Offline evaluation of LLMs is crucial in understanding their capacities,
though current methods remain underexplored in existing research. In this work,
we focus on the offline evaluation of the chain-of-thought capabilities and
show how to optimize LLMs based on the proposed evaluation method. To enable
offline feedback with rich knowledge and reasoning paths, we use knowledge
graphs (e.g., Wikidata5m) to provide feedback on the generated chain of
thoughts. Due to the heterogeneity between LLM reasoning and KG structures,
direct interaction and feedback from KGs on LLM behavior are challenging, as
they require accurate entity linking and grounding of LLM-generated chains of
thought in the KG. To address the above challenge, we propose an offline
chain-of-thought evaluation framework, OCEAN, which models chain-of-thought
reasoning in LLMs as an MDP and evaluate the policy's alignment with KG
preference modeling. To overcome the reasoning heterogeneity and grounding
problems, we leverage on-policy KG exploration and RL to model a KG policy that
generates token-level likelihood distributions for LLM-generated
chain-of-thought reasoning paths, simulating KG reasoning preference. Then we
incorporate the knowledge-graph feedback on the validity and alignment of the
generated reasoning paths into inverse propensity scores and propose KG-IPS
estimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS
estimator and provide a lower bound on its variance. With the off-policy
evaluated value function, we can directly enable off-policy optimization to
further enhance chain-of-thought alignment. Our empirical study shows that
OCEAN can be efficiently optimized for generating chain-of-thought reasoning
paths with higher estimated values without affecting LLMs' general abilities in
downstream tasks or their internal knowledge.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.23703v1},
File          = {2410.23703v1.pdf}
}
@article{2307.03067v2,
Author        = {Yuan He and Jiaoyan Chen and Hang Dong and Ian Horrocks and Carlo Allocca and Taehun Kim and Brahmananda Sapkota},
Title         = {DeepOnto: A Python Package for Ontology Engineering with Deep Learning},
Eprint        = {2307.03067v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Integrating deep learning techniques, particularly language models (LMs),
with knowledge representation techniques like ontologies has raised widespread
attention, urging the need of a platform that supports both paradigms. Although
packages such as OWL API and Jena offer robust support for basic ontology
processing features, they lack the capability to transform various types of
information within ontologies into formats suitable for downstream deep
learning-based applications. Moreover, widely-used ontology APIs are primarily
Java-based while deep learning frameworks like PyTorch and Tensorflow are
mainly for Python programming. To address the needs, we present DeepOnto, a
Python package designed for ontology engineering with deep learning. The
package encompasses a core ontology processing module founded on the
widely-recognised and reliable OWL API, encapsulating its fundamental features
in a more "Pythonic" manner and extending its capabilities to incorporate other
essential components including reasoning, verbalisation, normalisation,
taxonomy, projection, and more. Building on this module, DeepOnto offers a
suite of tools, resources, and algorithms that support various ontology
engineering tasks, such as ontology alignment and completion, by harnessing
deep learning methods, primarily pre-trained LMs. In this paper, we also
demonstrate the practical utility of DeepOnto through two use-cases: the
Digital Health Coaching in Samsung Research UK and the Bio-ML track of the
Ontology Alignment Evaluation Initiative (OAEI).},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.03067v2},
File          = {2307.03067v2.pdf}
}
@article{2308.00447v1,
Author        = {Eren Unlu},
Title         = {Structural Embeddings of Tools for Large Language Models},
Eprint        = {2308.00447v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {It is evident that the current state of Large Language Models (LLMs)
necessitates the incorporation of external tools. The lack of straightforward
algebraic and logical reasoning is well documented and prompted researchers to
develop frameworks which allow LLMs to operate via external tools. The
ontological nature of tool utilization for a specific task can be well
formulated with a Directed Acyclic Graph (DAG). The central aim of the paper is
to highlight the importance of graph based approaches to LLM-tool interaction
in near future. We propose an exemplary framework to guide the orchestration of
exponentially increasing numbers of external tools with LLMs,where objectives
and functionalities of tools are graph encoded hierarchically. Assuming that
textual segments of a Chain-of-Thought (CoT) can be imagined as a tool as
defined here, the graph based framework can pave new avenues in that particular
direction as well.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.00447v1},
File          = {2308.00447v1.pdf}
}
@article{2410.14399v2,
Author        = {Magdalena Wysocka and Danilo Carvalho and Oskar Wysocki and Marco Valentino and Andre Freitas},
Title         = {SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic
  Reasoning},
Eprint        = {2410.14399v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Syllogistic reasoning is crucial for Natural Language Inference (NLI). This
capability is particularly significant in specialized domains such as
biomedicine, where it can support automatic evidence interpretation and
scientific discovery. This paper presents SylloBio-NLI, a novel framework that
leverages external ontologies to systematically instantiate diverse syllogistic
arguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language
Models (LLMs) on identifying valid conclusions and extracting supporting
evidence across 28 syllogistic schemes instantiated with human genome pathways.
Extensive experiments reveal that biomedical syllogistic reasoning is
particularly challenging for zero-shot LLMs, which achieve an average accuracy
between 70% on generalized modus ponens and 23% on disjunctive syllogism. At
the same time, we found that few-shot prompting can boost the performance of
different LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper
analysis shows that both techniques exhibit high sensitivity to superficial
lexical variations, highlighting a dependency between reliability, models'
architecture, and pre-training regime. Overall, our results indicate that,
while in-context examples have the potential to elicit syllogistic reasoning in
LLMs, existing models are still far from achieving the robustness and
consistency required for safe biomedical NLI applications.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.14399v2},
File          = {2410.14399v2.pdf}
}
@article{2103.15950v1,
Author        = {Kalpa Gunaratna and Yu Wang and Hongxia Jin},
Title         = {Entity Context Graph: Learning Entity Representations
  fromSemi-Structured Textual Sources on the Web},
Eprint        = {2103.15950v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Knowledge is captured in the form of entities and their relationships and
stored in knowledge graphs. Knowledge graphs enhance the capabilities of
applications in many different areas including Web search, recommendation, and
natural language understanding. This is mainly because, entities enable
machines to understand things that go beyond simple tokens. Many modern
algorithms use learned entity embeddings from these structured representations.
However, building a knowledge graph takes time and effort, hence very costly
and nontrivial. On the other hand, many Web sources describe entities in some
structured format and therefore, finding ways to get them into useful entity
knowledge is advantageous. We propose an approach that processes entity centric
textual knowledge sources to learn entity embeddings and in turn avoids the
need for a traditional knowledge graph. We first extract triples into the new
representation format that does not use traditional complex triple extraction
methods defined by pre-determined relationship labels. Then we learn entity
embeddings through this new type of triples. We show that the embeddings
learned from our approach are: (i) high quality and comparable to a known
knowledge graph-based embeddings and can be used to improve them further, (ii)
better than a contextual language model-based entity embeddings, and (iii) easy
to compute and versatile in domain-specific applications where a knowledge
graph is not readily available},
Year          = {2021},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2103.15950v1},
File          = {2103.15950v1.pdf}
}
@article{2402.12728v2,
Author        = {Junnan Dong and Qinggang Zhang and Huachi Zhou and Daochen Zha and Pai Zheng and Xiao Huang},
Title         = {Modality-Aware Integration with Large Language Models for
  Knowledge-based Visual Question Answering},
Eprint        = {2402.12728v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Knowledge-based visual question answering (KVQA) has been extensively studied
to answer visual questions with external knowledge, e.g., knowledge graphs
(KGs). While several attempts have been proposed to leverage large language
models (LLMs) as an implicit knowledge source, it remains challenging since
LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g.,
images, KGs and LLMs, cannot be readily aligned for complex scenarios. To
tackle these, we present a novel modality-aware integration with LLMs for KVQA
(MAIL). It carefully leverages multimodal knowledge for both image
understanding and knowledge reasoning. Specifically, (i) we propose a two-stage
prompting strategy with LLMs to densely embody the image into a scene graph
with detailed visual features; (ii) We construct a coupled concept graph by
linking the mentioned entities with external facts. (iii) A tailored
pseudo-siamese graph medium fusion is designed for sufficient multimodal
fusion. We utilize the shared mentioned entities in two graphs as mediums to
bridge a tight inter-modal exchange, while maximally preserving insightful
intra-modal learning by constraining the fusion within mediums. Extensive
experiments on two benchmark datasets show the superiority of MAIL with 24x
less resources.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.12728v2},
File          = {2402.12728v2.pdf}
}
@article{2405.17337v1,
Author        = {Junnan Dong and Qinggang Zhang and Chuang Zhou and Hao Chen and Daochen Zha and Xiao Huang},
Title         = {Cost-efficient Knowledge-based Question Answering with Large Language
  Models},
Eprint        = {2405.17337v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge-based question answering (KBQA) is widely used in many scenarios
that necessitate domain knowledge. Large language models (LLMs) bring
opportunities to KBQA, while their costs are significantly higher and absence
of domain-specific knowledge during pre-training. We are motivated to combine
LLMs and prior small models on knowledge graphs (KGMs) for both inferential
accuracy and cost saving. However, it remains challenging since accuracy and
cost are not readily combined in the optimization as two distinct metrics. It
is also laborious for model selection since different models excel in diverse
knowledge. To this end, we propose Coke, a novel cost-efficient strategy for
KBQA with LLMs, modeled as a tailored multi-armed bandit problem to minimize
calls to LLMs within limited budgets. We first formulate the accuracy
expectation with a cluster-level Thompson Sampling for either KGMs or LLMs. A
context-aware policy is optimized to further distinguish the expert model
subject to the question semantics. The overall decision is bounded by the cost
regret according to historical expenditure on failures. Extensive experiments
showcase the superior performance of Coke, which moves the Pareto frontier with
up to 20.89% saving of GPT-4 fees while achieving a 2.74% higher accuracy on
the benchmark datasets.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.17337v1},
File          = {2405.17337v1.pdf}
}
@article{2405.20139v1,
Author        = {Costas Mavromatis and George Karypis},
Title         = {GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning},
Eprint        = {2405.20139v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form
of triplets (head, relation, tail), which collectively form a graph. Question
Answering over KGs (KGQA) is the task of answering natural questions grounding
the reasoning to the information provided by the KG. Large Language Models
(LLMs) are the state-of-the-art models for QA tasks due to their remarkable
ability to understand natural language. On the other hand, Graph Neural
Networks (GNNs) have been widely used for KGQA as they can handle the complex
graph information stored in the KG. In this work, we introduce GNN-RAG, a novel
method for combining language understanding abilities of LLMs with the
reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.
First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for
a given question. Second, the shortest paths in the KG that connect question
entities and answer candidates are extracted to represent KG reasoning paths.
The extracted paths are verbalized and given as input for LLM reasoning with
RAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to
extract useful graph information, while the LLM leverages its natural language
processing ability for ultimate KGQA. Furthermore, we develop a retrieval
augmentation (RA) technique to further boost KGQA performance with GNN-RAG.
Experimental results show that GNN-RAG achieves state-of-the-art performance in
two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching
GPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop
and multi-entity questions outperforming competing approaches by 8.9--15.5%
points at answer F1.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.20139v1},
File          = {2405.20139v1.pdf}
}
@article{2305.12788v3,
Author        = {Pengcheng Jiang and Cao Xiao and Adam Cross and Jimeng Sun},
Title         = {GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge
  Graphs},
Eprint        = {2305.12788v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Clinical predictive models often rely on patients' electronic health records
(EHR), but integrating medical knowledge to enhance predictions and
decision-making is challenging. This is because personalized predictions
require personalized knowledge graphs (KGs), which are difficult to generate
from patient EHR data. To address this, we propose \textsc{GraphCare}, an
open-world framework that uses external KGs to improve EHR-based predictions.
Our method extracts knowledge from large language models (LLMs) and external
biomedical KGs to build patient-specific KGs, which are then used to train our
proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare
predictions. On two public datasets, MIMIC-III and MIMIC-IV, \textsc{GraphCare}
surpasses baselines in four vital healthcare prediction tasks: mortality,
readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it
boosts AUROC by 17.6\% and 6.6\% for mortality and readmission, and F1-score by
7.9\% and 10.8\% for LOS and drug recommendation, respectively. Notably,
\textsc{GraphCare} demonstrates a substantial edge in scenarios with limited
data availability. Our findings highlight the potential of using external KGs
in healthcare prediction tasks and demonstrate the promise of
\textsc{GraphCare} in generating personalized KGs for promoting personalized
medicine.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.12788v3},
File          = {2305.12788v3.pdf}
}
@article{2403.05814v1,
Author        = {Yerin Hwang and Yongil Kim and Yunah Jang and Jeesoo Bang and Hyunkyung Bae and Kyomin Jung},
Title         = {MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging
  Knowledge Graphs},
Eprint        = {2403.05814v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Despite advancements in on-topic dialogue systems, effectively managing topic
shifts within dialogues remains a persistent challenge, largely attributed to
the limited availability of training datasets. To address this issue, we
propose Multi-Passage to Dialogue (MP2D), a data generation framework that
automatically creates conversational question-answering datasets with natural
topic transitions. By leveraging the relationships between entities in a
knowledge graph, MP2D maps the flow of topics within a dialogue, effectively
mirroring the dynamics of human conversation. It retrieves relevant passages
corresponding to the topics and transforms them into dialogues through the
passage-to-dialogue method. Through quantitative and qualitative experiments,
we demonstrate MP2D's efficacy in generating dialogue with natural topic
shifts. Furthermore, this study introduces a novel benchmark for topic shift
dialogues, TS-WikiDialog. Utilizing the dataset, we demonstrate that even Large
Language Models (LLMs) struggle to handle topic shifts in dialogue effectively,
and we showcase the performance improvements of models trained on datasets
generated by MP2D across diverse topic shift dialogue tasks.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.05814v1},
File          = {2403.05814v1.pdf}
}
@article{2009.11692v1,
Author        = {Haozhe Ji and Pei Ke and Shaohan Huang and Furu Wei and Xiaoyan Zhu and Minlie Huang},
Title         = {Language Generation with Multi-Hop Reasoning on Commonsense Knowledge
  Graph},
Eprint        = {2009.11692v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Despite the success of generative pre-trained language models on a series of
text generation tasks, they still suffer in cases where reasoning over
underlying commonsense knowledge is required during generation. Existing
approaches that integrate commonsense knowledge into generative pre-trained
language models simply transfer relational knowledge by post-training on
individual knowledge triples while ignoring rich connections within the
knowledge graph. We argue that exploiting both the structural and semantic
information of the knowledge graph facilitates commonsense-aware text
generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow
(GRF) that enables pre-trained models with dynamic multi-hop reasoning on
multi-relational paths extracted from the external commonsense knowledge graph.
We empirically show that our model outperforms existing baselines on three text
generation tasks that require reasoning over commonsense knowledge. We also
demonstrate the effectiveness of the dynamic multi-hop reasoning module with
reasoning paths inferred by the model that provide rationale to the generation.},
Year          = {2020},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2009.11692v1},
File          = {2009.11692v1.pdf}
}
@article{2202.02113v7,
Author        = {Xin Xie and Ningyu Zhang and Zhoubo Li and Shumin Deng and Hui Chen and Feiyu Xiong and Mosha Chen and Huajun Chen},
Title         = {From Discrimination to Generation: Knowledge Graph Completion with
  Generative Transformer},
Eprint        = {2202.02113v7},
DOI           = {10.1145/3487553.3524238},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/GenKGC.},
Year          = {2022},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2202.02113v7},
File          = {2202.02113v7.pdf}
}
@article{2409.15902v1,
Author        = {Maria Lysyuk and Mikhail Salnikov and Pavel Braslavski and Alexander Panchenko},
Title         = {Konstruktor: A Strong Baseline for Simple Knowledge Graph Question
  Answering},
Eprint        = {2409.15902v1},
DOI           = {10.1007/978-3-031-70242-6_11},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {While being one of the most popular question types, simple questions such as
"Who is the author of Cinderella?", are still not completely solved.
Surprisingly, even the most powerful modern Large Language Models are prone to
errors when dealing with such questions, especially when dealing with rare
entities. At the same time, as an answer may be one hop away from the question
entity, one can try to develop a method that uses structured knowledge graphs
(KGs) to answer such questions. In this paper, we introduce Konstruktor - an
efficient and robust approach that breaks down the problem into three steps:
(i) entity extraction and entity linking, (ii) relation prediction, and (iii)
querying the knowledge graph. Our approach integrates language models and
knowledge graphs, exploiting the power of the former and the interpretability
of the latter. We experiment with two named entity recognition and entity
linking methods and several relation detection techniques. We show that for
relation detection, the most challenging step of the workflow, a combination of
relation classification/generation and ranking outperforms other methods. We
report Konstruktor's strong results on four datasets.},
Year          = {2024},
Month         = {Sep},
Note          = {International Conference on Applications of Natural Language to
  Information Systems, pages: 107-118, year: 2024, organization: Springer},
Url           = {http://arxiv.org/abs/2409.15902v1},
File          = {2409.15902v1.pdf}
}
@article{2410.09350v1,
Author        = {Jinyoung Park and Minseok Joo and Joo-Kyung Kim and Hyunwoo J. Kim},
Title         = {Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog
  Generation},
Eprint        = {2410.09350v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graph-grounded dialog generation requires retrieving a
dialog-relevant subgraph from the given knowledge base graph and integrating it
with the dialog history. Previous works typically represent the graph using an
external encoder, such as graph neural networks, and retrieve relevant triplets
based on the similarity between single-vector representations of triplets and
the dialog history. However, these external encoders fail to leverage the rich
knowledge of pretrained language models, and the retrieval process is also
suboptimal due to the information bottleneck caused by the single-vector
abstraction of the dialog history. In this work, we propose Dialog generation
with Generative Subgraph Retrieval (DialogGSR), which retrieves relevant
knowledge subgraphs by directly generating their token sequences on top of
language models. For effective generative subgraph retrieval, we introduce two
key methods: (i) structure-aware knowledge graph linearization with
self-supervised graph-specific tokens and (ii) graph-constrained decoding
utilizing graph structural proximity-based entity informativeness scores for
valid and relevant generative retrieval. DialogGSR achieves state-of-the-art
performance in knowledge graph-grounded dialog generation, as demonstrated on
OpenDialKG and KOMODIS datasets.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.09350v1},
File          = {2410.09350v1.pdf}
}
@article{2409.18753v1,
Author        = {Jihen Amara and Birgitta König-Ries and Sheeba Samuel},
Title         = {Enhancing Explainability in Multimodal Large Language Models Using
  Ontological Context},
Eprint        = {2409.18753v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Recently, there has been a growing interest in Multimodal Large Language
Models (MLLMs) due to their remarkable potential in various tasks integrating
different modalities, such as image and text, as well as applications such as
image captioning and visual question answering. However, such models still face
challenges in accurately captioning and interpreting specific visual concepts
and classes, particularly in domain-specific applications. We argue that
integrating domain knowledge in the form of an ontology can significantly
address these issues. In this work, as a proof of concept, we propose a new
framework that combines ontology with MLLMs to classify images of plant
diseases. Our method uses concepts about plant diseases from an existing
disease ontology to query MLLMs and extract relevant visual concepts from
images. Then, we use the reasoning capabilities of the ontology to classify the
disease according to the identified concepts. Ensuring that the model
accurately uses the concepts describing the disease is crucial in
domain-specific applications. By employing an ontology, we can assist in
verifying this alignment. Additionally, using the ontology's inference
capabilities increases transparency, explainability, and trust in the
decision-making process while serving as a judge by checking if the annotations
of the concepts by MLLMs are aligned with those in the ontology and displaying
the rationales behind their errors. Our framework offers a new direction for
synergizing ontologies and MLLMs, supported by an empirical study using
different well-known MLLMs.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.18753v1},
File          = {2409.18753v1.pdf}
}
@article{2407.10793v1,
Author        = {Hannah Sansford and Nicholas Richardson and Hermina Petric Maretic and Juba Nait Saada},
Title         = {GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation
  Framework},
Eprint        = {2407.10793v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Methods to evaluate Large Language Model (LLM) responses and detect
inconsistencies, also known as hallucinations, with respect to the provided
knowledge, are becoming increasingly important for LLM applications. Current
metrics fall short in their ability to provide explainable decisions,
systematically check all pieces of information in the response, and are often
too computationally expensive to be used in practice. We present GraphEval: a
hallucination evaluation framework based on representing information in
Knowledge Graph (KG) structures. Our method identifies the specific triples in
the KG that are prone to hallucinations and hence provides more insight into
where in the response a hallucination has occurred, if at all, than previous
methods. Furthermore, using our approach in conjunction with state-of-the-art
natural language inference (NLI) models leads to an improvement in balanced
accuracy on various hallucination benchmarks, compared to using the raw NLI
models. Lastly, we explore the use of GraphEval for hallucination correction by
leveraging the structure of the KG, a method we name GraphCorrect, and
demonstrate that the majority of hallucinations can indeed be rectified.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.10793v1},
File          = {2407.10793v1.pdf}
}
@article{2501.09957v2,
Author        = {Zengyi Gao and Yukun Cao and Hairu Wang and Ao Ke and Yuan Feng and Xike Xie and S Kevin Zhou},
Title         = {FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation
  based on Knowledge Graphs},
Eprint        = {2501.09957v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {To mitigate the hallucination and knowledge deficiency in large language
models (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG)
has shown promising potential by utilizing KGs as external resource to enhance
LLMs reasoning. However, existing KG-RAG approaches struggle with a trade-off
between flexibility and retrieval quality. Modular methods prioritize
flexibility by avoiding the use of KG-fine-tuned models during retrieval,
leading to fixed retrieval strategies and suboptimal retrieval quality.
Conversely, coupled methods embed KG information within models to improve
retrieval quality, but at the expense of flexibility. In this paper, we propose
a novel flexible modular KG-RAG framework, termed FRAG, which synergizes the
advantages of both approaches. FRAG estimates the hop range of reasoning paths
based solely on the query and classify it as either simple or complex. To match
the complexity of the query, tailored pipelines are applied to ensure efficient
and accurate reasoning path retrieval, thus fostering the final reasoning
process. By using the query text instead of the KG to infer the structural
information of reasoning paths and employing adaptable retrieval strategies,
FRAG improves retrieval quality while maintaining flexibility. Moreover, FRAG
does not require extra LLMs fine-tuning or calls, significantly boosting
efficiency and conserving resources. Extensive experiments show that FRAG
achieves state-of-the-art performance with high efficiency and low resource
consumption.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.09957v2},
File          = {2501.09957v2.pdf}
}
@article{2306.04802v5,
Author        = {Hejie Cui and Jiaying Lu and Ran Xu and Shiyu Wang and Wenjing Ma and Yue Yu and Shaojun Yu and Xuan Kan and Chen Ling and Liang Zhao and Zhaohui S. Qin and Joyce C. Ho and Tianfan Fu and Jing Ma and Mengdi Huai and Fei Wang and Carl Yang},
Title         = {A Review on Knowledge Graphs for Healthcare: Resources, Applications,
  and Promises},
Eprint        = {2306.04802v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This comprehensive review aims to provide an overview of the current state of
Healthcare Knowledge Graphs (HKGs), including their construction, utilization
models, and applications across various healthcare and biomedical research
domains. We thoroughly analyzed existing literature on HKGs, covering their
construction methodologies, utilization techniques, and applications in basic
science research, pharmaceutical research and development, clinical decision
support, and public health. The review encompasses both model-free and
model-based utilization approaches and the integration of HKGs with large
language models (LLMs). We searched Google Scholar for relevant papers on HKGs
and classified them into the following topics: HKG construction, HKG
utilization, and their downstream applications in various domains. We also
discussed their special challenges and the promise for future work. The review
highlights the potential of HKGs to significantly impact biomedical research
and clinical practice by integrating vast amounts of biomedical knowledge from
multiple domains. The synergy between HKGs and LLMs offers promising
opportunities for constructing more comprehensive knowledge graphs and
improving the accuracy of healthcare applications. HKGs have emerged as a
powerful tool for structuring medical knowledge, with broad applications across
biomedical research, clinical decision-making, and public health. This survey
serves as a roadmap for future research and development in the field of HKGs,
highlighting the potential of combining knowledge graphs with advanced machine
learning models for healthcare transformation.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.04802v5},
File          = {2306.04802v5.pdf}
}
@article{2501.06224v3,
Author        = {Wen-Dong Jiang and Chih-Yung Chang and Diptendu Sinha Roy},
Title         = {Detection, Retrieval, and Explanation Unified: A Violence Detection
  System Based on Knowledge Graphs and GAT},
Eprint        = {2501.06224v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Recently, violence detection systems developed using unified multimodal
models have achieved significant success and attracted widespread attention.
However, most of these systems face two critical challenges: the lack of
interpretability as black-box models and limited functionality, offering only
classification or retrieval capabilities. To address these challenges, this
paper proposes a novel interpretable violence detection system, termed the
Three-in-One (TIO) System. The TIO system integrates knowledge graphs (KG) and
graph attention networks (GAT) to provide three core functionalities:
detection, retrieval, and explanation. Specifically, the system processes each
video frame along with text descriptions generated by a large language model
(LLM) for videos containing potential violent behavior. It employs ImageBind to
generate high-dimensional embeddings for constructing a knowledge graph, uses
GAT for reasoning, and applies lightweight time series modules to extract video
embedding features. The final step connects a classifier and retriever for
multi-functional outputs. The interpretability of KG enables the system to
verify the reasoning process behind each output. Additionally, the paper
introduces several lightweight methods to reduce the resource consumption of
the TIO system and enhance its efficiency. Extensive experiments conducted on
the XD-Violence and UCF-Crime datasets validate the effectiveness of the
proposed system. A case study further reveals an intriguing phenomenon: as the
number of bystanders increases, the occurrence of violent behavior tends to
decrease.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.06224v3},
File          = {2501.06224v3.pdf}
}
@article{2304.11116v3,
Author        = {Jiawei Zhang},
Title         = {Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via
  Prompt Augmented by ChatGPT},
Eprint        = {2304.11116v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In this paper, we aim to develop a large language model (LLM) with the
reasoning ability on complex graph data. Currently, LLMs have achieved very
impressive performance on various natural language learning tasks, extensions
of which have also been applied to study the vision tasks with multi-modal
data. However, when it comes to the graph learning tasks, existing LLMs present
very serious flaws due to their several inherited weaknesses in performing
{multi-step logic reasoning}, {precise mathematical calculation} and
{perception about the spatial and temporal factors}.
  To address such challenges, in this paper, we will investigate the
principles, methodologies and algorithms to empower existing LLMs with graph
reasoning ability, which will have tremendous impacts on the current research
of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer
models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer)
framework to teach LLMs themselves with prompts augmented by ChatGPT to use
external graph reasoning API tools. Specifically, we will investigate to teach
Graph-ToolFormer to handle various graph data reasoning tasks in this paper,
including both (1) very basic graph data loading and graph property reasoning
tasks, ranging from simple graph order and size to the graph diameter and
periphery, and (2) more advanced reasoning tasks on real-world graph data, such
as bibliographic networks, protein molecules, sequential recommender systems,
social networks and knowledge graphs.},
Year          = {2023},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2304.11116v3},
File          = {2304.11116v3.pdf}
}
@article{2306.11025v1,
Author        = {Xinli Yu and Zheng Chen and Yuan Ling and Shujing Dong and Zongyi Liu and Yanbin Lu},
Title         = {Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting},
Eprint        = {2306.11025v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {This paper presents a novel study on harnessing Large Language Models' (LLMs)
outstanding knowledge and reasoning abilities for explainable financial time
series forecasting. The application of machine learning models to financial
time series comes with several challenges, including the difficulty in
cross-sequence reasoning and inference, the hurdle of incorporating multi-modal
signals from historical news, financial knowledge graphs, etc., and the issue
of interpreting and explaining the model results. In this paper, we focus on
NASDAQ-100 stocks, making use of publicly accessible historical stock price
data, company metadata, and historical economic/financial news. We conduct
experiments to illustrate the potential of LLMs in offering a unified solution
to the aforementioned challenges. Our experiments include trying
zero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with
a public LLM model Open LLaMA. We demonstrate our approach outperforms a few
baselines, including the widely applied classic ARMA-GARCH model and a
gradient-boosting tree model. Through the performance comparison results and a
few examples, we find LLMs can make a well-thought decision by reasoning over
information from both textual news and price time series and extracting
insights, leveraging cross-sequence information, and utilizing the inherent
knowledge embedded within the LLM. Additionally, we show that a publicly
available LLM such as Open-LLaMA, after fine-tuning, can comprehend the
instruction to generate explainable forecasts and achieve reasonable
performance, albeit relatively inferior in comparison to GPT-4.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.11025v1},
File          = {2306.11025v1.pdf}
}
@article{2306.13865v1,
Author        = {Yuxin Zi and Kaushik Roy and Vignesh Narayanan and Manas Gaur and Amit Sheth},
Title         = {IERL: Interpretable Ensemble Representation Learning -- Combining
  CrowdSourced Knowledge and Distributed Semantic Representations},
Eprint        = {2306.13865v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) encode meanings of words in the form of
distributed semantics. Distributed semantics capture common statistical
patterns among language tokens (words, phrases, and sentences) from large
amounts of data. LLMs perform exceedingly well across General Language
Understanding Evaluation (GLUE) tasks designed to test a model's understanding
of the meanings of the input tokens. However, recent studies have shown that
LLMs tend to generate unintended, inconsistent, or wrong texts as outputs when
processing inputs that were seen rarely during training, or inputs that are
associated with diverse contexts (e.g., well-known hallucination phenomenon in
language generation tasks). Crowdsourced and expert-curated knowledge graphs
such as ConceptNet are designed to capture the meaning of words from a compact
set of well-defined contexts. Thus LLMs may benefit from leveraging such
knowledge contexts to reduce inconsistencies in outputs. We propose a novel
ensemble learning method, Interpretable Ensemble Representation Learning
(IERL), that systematically combines LLM and crowdsourced knowledge
representations of input tokens. IERL has the distinct advantage of being
interpretable by design (when was the LLM context used vs. when was the
knowledge context used?) over state-of-the-art (SOTA) methods, allowing
scrutiny of the inputs in conjunction with the parameters of the model,
facilitating the analysis of models' inconsistent or irrelevant outputs.
Although IERL is agnostic to the choice of LLM and crowdsourced knowledge, we
demonstrate our approach using BERT and ConceptNet. We report improved or
competitive results with IERL across GLUE tasks over current SOTA methods and
significantly enhanced model interpretability.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.13865v1},
File          = {2306.13865v1.pdf}
}
@article{2402.01730v1,
Author        = {Dimitrios P. Panagoulias and Maria Virvou and George A. Tsihrintzis},
Title         = {Evaluating LLM -- Generated Multimodal Diagnosis from Medical Images and
  Symptom Analysis},
Eprint        = {2402.01730v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) constitute a breakthrough state-of-the-art
Artificial Intelligence technology which is rapidly evolving and promises to
aid in medical diagnosis. However, the correctness and the accuracy of their
returns has not yet been properly evaluated. In this work, we propose an LLM
evaluation paradigm that incorporates two independent steps of a novel
methodology, namely (1) multimodal LLM evaluation via structured interactions
and (2) follow-up, domain-specific analysis based on data extracted via the
previous interactions. Using this paradigm, (1) we evaluate the correctness and
accuracy of LLM-generated medical diagnosis with publicly available multimodal
multiple-choice questions(MCQs) in the domain of Pathology and (2) proceed to a
systemic and comprehensive analysis of extracted results. We used
GPT-4-Vision-Preview as the LLM to respond to complex, medical questions
consisting of both images and text, and we explored a wide range of diseases,
conditions, chemical compounds, and related entity types that are included in
the vast knowledge domain of Pathology. GPT-4-Vision-Preview performed quite
well, scoring approximately 84\% of correct diagnoses. Next, we further
analyzed the findings of our work, following an analytical approach which
included Image Metadata Analysis, Named Entity Recognition and Knowledge
Graphs. Weaknesses of GPT-4-Vision-Preview were revealed on specific knowledge
paths, leading to a further understanding of its shortcomings in specific
areas. Our methodology and findings are not limited to the use of
GPT-4-Vision-Preview, but a similar approach can be followed to evaluate the
usefulness and accuracy of other LLMs and, thus, improve their use with further
optimization.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2402.01730v1},
File          = {2402.01730v1.pdf}
}
@article{2406.14703v2,
Author        = {Seungbeen Lee and Seungwon Lim and Seungju Han and Giyeong Oh and Hyungjoo Chae and Jiwan Chung and Minju Kim and Beong-woo Kwak and Yeonsoo Lee and Dongha Lee and Jinyoung Yeo and Youngjae Yu},
Title         = {Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality
  Testset designed for LLMs with Psychometrics},
Eprint        = {2406.14703v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent advancements in Large Language Models (LLMs) have led to their
adaptation in various domains as conversational agents. We wonder: can
personality tests be applied to these agents to analyze their behavior, similar
to humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice
questions designed to assess the personality of LLMs. TRAIT is built on two
psychometrically validated small human questionnaires, Big Five Inventory (BFI)
and Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a
variety of real-world scenarios. TRAIT also outperforms existing personality
tests for LLMs in terms of reliability and validity, achieving the highest
scores across four key metrics: Content Validity, Internal Validity, Refusal
Rate, and Reliability. Using TRAIT, we reveal two notable insights into
personalities of LLMs: 1) LLMs exhibit distinct and consistent personality,
which is highly influenced by their training data (e.g., data used for
alignment tuning), and 2) current prompting techniques have limited
effectiveness in eliciting certain traits, such as high psychopathy or low
conscientiousness, suggesting the need for further research in this direction.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.14703v2},
File          = {2406.14703v2.pdf}
}
@article{2210.13617v2,
Author        = {Yifan Hou and Wenxiang Jiao and Meizhen Liu and Carl Allen and Zhaopeng Tu and Mrinmaya Sachan},
Title         = {Adapters for Enhanced Modeling of Multilingual Knowledge and Text},
Eprint        = {2210.13617v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models appear to learn facts from the large text corpora they
are trained on. Such facts are encoded implicitly within their many parameters,
making it difficult to verify or manipulate what knowledge has been learned.
Language models have recently been extended to multilingual language models
(MLLMs), enabling knowledge to be learned across hundreds of languages.
Meanwhile, knowledge graphs contain facts in an explicit triple format, which
require careful and costly curation and are only available in a few
high-resource languages, restricting their research and application. To address
these issues, we propose to enhance MLLMs with knowledge from multilingual
knowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks
across many languages, including low-resource ones. Specifically, we introduce
a lightweight adapter set to enhance MLLMs with cross-lingual entity alignment
and facts from MLKGs for many languages. Experiments on common benchmarks show
that such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable
or improved performance for knowledge graph completion and entity alignment
relative to baselines, especially for low-resource languages (for which
knowledge graphs are unavailable); and (2) improved MLLM performance on
language understanding tasks that require multilingual factual knowledge; all
while maintaining performance on other general language tasks.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.13617v2},
File          = {2210.13617v2.pdf}
}
@article{2406.01391v2,
Author        = {Zechang Sun and Yuan-Sen Ting and Yaobo Liang and Nan Duan and Song Huang and Zheng Cai},
Title         = {Knowledge Graph in Astronomical Research with Large Language Models:
  Quantifying Driving Forces in Interdisciplinary Scientific Discovery},
Eprint        = {2406.01391v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {astro-ph.IM},
Abstract      = {Identifying and predicting the factors that contribute to the success of
interdisciplinary research is crucial for advancing scientific discovery.
However, there is a lack of methods to quantify the integration of new ideas
and technological advancements in astronomical research and how these new
technologies drive further scientific breakthroughs. Large language models,
with their ability to extract key concepts from vast literature beyond keyword
searches, provide a new tool to quantify such processes. In this study, we
extracted concepts in astronomical research from 297,807 publications between
1993 and 2024 using large language models, resulting in a set of 24,939
concepts. These concepts were then used to form a knowledge graph, where the
link strength between any two concepts was determined by their relevance
through the citation-reference relationships. By calculating this relevance
across different time periods, we quantified the impact of numerical
simulations and machine learning on astronomical research. The knowledge graph
demonstrates two phases of development: a phase where the technology was
integrated and another where the technology was explored in scientific
discovery. The knowledge graph reveals that despite machine learning has made
much inroad in astronomy, there is currently a lack of new concept development
at the intersection of AI and Astronomy, which may be the current bottleneck
preventing machine learning from further transforming the field of astronomy.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.01391v2},
File          = {2406.01391v2.pdf}
}
@article{2410.10899v2,
Author        = {Rongbin Li and Wenbo Chen and Jinbo Li and Hanwen Xing and Hua Xu and Zhao Li and W. Jim Zheng},
Title         = {GPTON: Generative Pre-trained Transformers enhanced with Ontology
  Narration for accurate annotation of biological data},
Eprint        = {2410.10899v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.QM},
Abstract      = {By leveraging GPT-4 for ontology narration, we developed GPTON to infuse
structured knowledge into LLMs through verbalized ontology terms, achieving
accurate text and ontology annotations for over 68% of gene sets in the top
five predictions. Manual evaluations confirm GPTON's robustness, highlighting
its potential to harness LLMs and structured knowledge to significantly advance
biomedical research beyond gene set annotation.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.10899v2},
File          = {2410.10899v2.pdf}
}
@article{2407.00936v3,
Author        = {Xin Wang and Zirui Chen and Haofen Wang and Leong Hou U and Zhao Li and Wenbin Guo},
Title         = {Large Language Model Enhanced Knowledge Representation Learning: A
  Survey},
Eprint        = {2407.00936v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Representation Learning (KRL) is crucial for enabling applications
of symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by
projecting knowledge facts into vector spaces. Despite their effectiveness in
modeling KG structural information, KRL methods are suffering from the
sparseness of KGs. The rise of Large Language Models (LLMs) built on the
Transformer architecture present promising opportunities for enhancing KRL by
incorporating textual information to address information sparsity in KGs.
LLM-enhanced KRL methods, including three key approaches, encoder-based methods
that leverage detailed contextual information, encoder-decoder-based methods
that utilize a unified seq2seq model for comprehensive encoding and decoding,
and decoder-based methods that utilize extensive knowledge from large corpora,
has significantly advanced the effectiveness and generalization of KRL in
addressing a wide range of downstream tasks. This work provides a broad
overview of downstream tasks while simultaneously identifying emerging research
directions in these evolving domains.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.00936v3},
File          = {2407.00936v3.pdf}
}
@article{2407.20382v1,
Author        = {Navapat Nananukul and Wichayaporn Wongkamjan},
Title         = {What if Red Can Talk? Dynamic Dialogue Generation Using Large Language
  Models},
Eprint        = {2407.20382v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Role-playing games (RPGs) provide players with a rich, interactive world to
explore. Dialogue serves as the primary means of communication between
developers and players, manifesting in various forms such as guides, NPC
interactions, and storytelling. While most games rely on written scripts to
define the main story and character personalities, player immersion can be
significantly enhanced through casual interactions between characters. With the
advent of large language models (LLMs), we introduce a dialogue filler
framework that utilizes LLMs enhanced by knowledge graphs to generate dynamic
and contextually appropriate character interactions. We test this framework
within the environments of Final Fantasy VII Remake and Pokemon, providing
qualitative and quantitative evidence that demonstrates GPT-4's capability to
act with defined personalities and generate dialogue. However, some flaws
remain, such as GPT-4 being overly positive or more subtle personalities, such
as maturity, tend to be of lower quality compared to more overt traits like
timidity. This study aims to assist developers in crafting more nuanced filler
dialogues, thereby enriching player immersion and enhancing the overall RPG
experience.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.20382v1},
File          = {2407.20382v1.pdf}
}
@article{2412.17332v2,
Author        = {Yujie Lin and Jingyao Liu and Yan Gao and Ante Wang and Jinsong Su},
Title         = {A Dual-Perspective Metaphor Detection Framework Using Large Language
  Models},
Eprint        = {2412.17332v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Metaphor detection, a critical task in natural language processing, involves
identifying whether a particular word in a sentence is used metaphorically.
Traditional approaches often rely on supervised learning models that implicitly
encode semantic relationships based on metaphor theories. However, these
methods often suffer from a lack of transparency in their decision-making
processes, which undermines the reliability of their predictions. Recent
research indicates that LLMs (large language models) exhibit significant
potential in metaphor detection. Nevertheless, their reasoning capabilities are
constrained by predefined knowledge graphs. To overcome these limitations, we
propose DMD, a novel dual-perspective framework that harnesses both implicit
and explicit applications of metaphor theories to guide LLMs in metaphor
detection and adopts a self-judgment mechanism to validate the responses from
the aforementioned forms of guidance. In comparison to previous methods, our
framework offers more transparent reasoning processes and delivers more
reliable predictions. Experimental results prove the effectiveness of DMD,
demonstrating state-of-the-art performance across widely-used datasets.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.17332v2},
File          = {2412.17332v2.pdf}
}
@article{2412.18627v1,
Author        = {Xingyu Xiao and Peng Chen and Ben Qi and Hongru Zhao and Jingang Liang and Jiejuan Tong and Haitao Wang},
Title         = {KRAIL: A Knowledge-Driven Framework for Base Human Reliability Analysis
  Integrating IDHEAS and Large Language Models},
Eprint        = {2412.18627v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Human reliability analysis (HRA) is crucial for evaluating and improving the
safety of complex systems. Recent efforts have focused on estimating human
error probability (HEP), but existing methods often rely heavily on expert
knowledge,which can be subjective and time-consuming. Inspired by the success
of large language models (LLMs) in natural language processing, this paper
introduces a novel two-stage framework for knowledge-driven reliability
analysis, integrating IDHEAS and LLMs (KRAIL). This innovative framework
enables the semi-automated computation of base HEP values. Additionally,
knowledge graphs are utilized as a form of retrieval-augmented generation (RAG)
for enhancing the framework' s capability to retrieve and process relevant data
efficiently. Experiments are systematically conducted and evaluated on
authoritative datasets of human reliability. The experimental results of the
proposed methodology demonstrate its superior performance on base HEP
estimation under partial information for reliability assessment.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.18627v1},
File          = {2412.18627v1.pdf}
}
@article{2502.00681v1,
Author        = {Qika Lin and Zhen Peng and Kaize Shi and Kai He and Yiming Xu and Erik Cambria and Mengling Feng},
Title         = {A Survey of Quantized Graph Representation Learning: Connecting Graph
  Structures with Large Language Models},
Eprint        = {2502.00681v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Recent years have witnessed rapid advances in graph representation learning,
with the continuous embedding approach emerging as the dominant paradigm.
However, such methods encounter issues regarding parameter efficiency,
interpretability, and robustness. Thus, Quantized Graph Representation (QGR)
learning has recently gained increasing interest, which represents the graph
structure with discrete codes instead of conventional continuous embeddings.
Given its analogous representation form to natural language, QGR also possesses
the capability to seamlessly integrate graph structures with large language
models (LLMs). As this emerging paradigm is still in its infancy yet holds
significant promise, we undertake this thorough survey to promote its rapid
future prosperity. We first present the background of the general quantization
methods and their merits. Moreover, we provide an in-depth demonstration of
current QGR studies from the perspectives of quantized strategies, training
objectives, distinctive designs, knowledge graph quantization, and
applications. We further explore the strategies for code dependence learning
and integration with LLMs. At last, we give discussions and conclude future
directions, aiming to provide a comprehensive picture of QGR and inspire future
research.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.00681v1},
File          = {2502.00681v1.pdf}
}
@article{2405.15436v1,
Author        = {Candace Edwards},
Title         = {Hybrid Context Retrieval Augmented Generation Pipeline: LLM-Augmented
  Knowledge Graphs and Vector Database for Accreditation Reporting Assistance},
Eprint        = {2405.15436v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {In higher education, accreditation is a quality assurance process, where an
institution demonstrates a commitment to delivering high quality programs and
services to their students. For business schools nationally and internationally
the Association to Advance Collegiate Schools of Business (AACSB) accreditation
is the gold standard. For a business school to receive and subsequently
maintain accreditation, the school must undertake a rigorous, time consuming
reporting and peer review process, to demonstrate alignment with the AACSB
Standards. For this project we create a hybrid context retrieval augmented
generation pipeline that can assist in the documentation alignment and
reporting process necessary for accreditation. We implement both a vector
database and knowledge graph, as knowledge stores containing both institutional
data and AACSB Standard data. The output of the pipeline can be used by
institution stakeholders to build their accreditation report, dually grounded
by the context from the knowledge stores. To develop our knowledge graphs we
utilized both a manual construction process as well as an LLM Augmented
Knowledge Graph approach. We evaluated the pipeline using the RAGAs framework
and observed optimal performance on answer relevancy and answer correctness
metrics.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.15436v1},
File          = {2405.15436v1.pdf}
}
@article{2208.12539v2,
Author        = {Tianyi Li and Wenyu Huang and Nikos Papasarantopoulos and Pavlos Vougiouklis and Jeff Z. Pan},
Title         = {Task-specific Pre-training and Prompt Decomposition for Knowledge Graph
  Population with Language Models},
Eprint        = {2208.12539v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present a system for knowledge graph population with Language Models,
evaluated on the Knowledge Base Construction from Pre-trained Language Models
(LM-KBC) challenge at ISWC 2022. Our system involves task-specific pre-training
to improve LM representation of the masked object tokens, prompt decomposition
for progressive generation of candidate objects, among other methods for
higher-quality retrieval. Our system is the winner of track 1 of the LM-KBC
challenge, based on BERT LM; it achieves 55.0% F-1 score on the hidden test set
of the challenge.},
Year          = {2022},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2208.12539v2},
File          = {2208.12539v2.pdf}
}
@article{2404.02389v1,
Author        = {Yutong Shao and Ndapa Nakashole},
Title         = {On Linearizing Structured Data in Encoder-Decoder Language Models:
  Insights from Text-to-SQL},
Eprint        = {2404.02389v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Structured data, prevalent in tables, databases, and knowledge graphs, poses
a significant challenge in its representation. With the advent of large
language models (LLMs), there has been a shift towards linearization-based
methods, which process structured data as sequential token streams, diverging
from approaches that explicitly model structure, often as a graph. Crucially,
there remains a gap in our understanding of how these linearization-based
methods handle structured data, which is inherently non-linear. This work
investigates the linear handling of structured data in encoder-decoder language
models, specifically T5. Our findings reveal the model's ability to mimic
human-designed processes such as schema linking and syntax prediction,
indicating a deep, meaningful learning of structure beyond simple token
sequencing. We also uncover insights into the model's internal mechanisms,
including the ego-centric nature of structure node encodings and the potential
for model compression due to modality fusion redundancy. Overall, this work
sheds light on the inner workings of linearization-based methods and could
potentially provide guidance for future research.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.02389v1},
File          = {2404.02389v1.pdf}
}
@article{2408.14494v1,
Author        = {Sakhinana Sagar Srinivas and Vijay Sri Vaikunth and Venkataramana Runkana},
Title         = {Knowledge Graph Modeling-Driven Large Language Model Operating System
  (LLM OS) for Task Automation in Process Engineering Problem-Solving},
Eprint        = {2408.14494v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We present the Process Engineering Operations Assistant (PEOA), an AI-driven
framework designed to solve complex problems in the chemical and process
industries. The framework employs a modular architecture orchestrated by a
meta-agent, which serves as the central coordinator, managing an action
generator and instruction-tuned small-scale language models (expert models).
The action generator decomposes complex problems into sub-tasks and identifies
suitable expert models to execute each, delivering precise solutions for
multi-step problem-solving. Key techniques include advanced knowledge modeling
using property graphs for improved information retrieval, facilitating more
accurate and contextually relevant solutions. Additionally, the framework
utilizes a teacher-student transfer-learning approach with GPT-4 (Omni) to
fine-tune the action generator and expert models for domain adaptation,
alongside an iterative problem-solving mechanism with sophisticated error
handling. Custom datasets were developed to evaluate the framework against
leading proprietary language models on various engineering tasks. The results
demonstrate the framework effectiveness in automating calculations,
accelerating prototyping, and providing AI-augmented decision support for
industrial processes, marking a significant advancement in process engineering
capabilities.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.14494v1},
File          = {2408.14494v1.pdf}
}
@article{2402.01677v3,
Author        = {Keyu Wang and Guilin Qi and Jiaoyan Chen and Yi Huang and Tianxing Wu},
Title         = {Embedding Ontologies via Incorporating Extensional and Intensional
  Knowledge},
Eprint        = {2402.01677v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontologies contain rich knowledge within domain, which can be divided into
two categories, namely extensional knowledge and intensional knowledge.
Extensional knowledge provides information about the concrete instances that
belong to specific concepts in the ontology, while intensional knowledge
details inherent properties, characteristics, and semantic associations among
concepts. However, existing ontology embedding approaches fail to take both
extensional knowledge and intensional knowledge into fine consideration
simultaneously. In this paper, we propose a novel ontology embedding approach
named EIKE (Extensional and Intensional Knowledge Embedding) by representing
ontologies in two spaces, called extensional space and intensional space. EIKE
presents a unified framework for embedding instances, concepts and their
relations in an ontology, applying a geometry-based method to model extensional
knowledge and a pretrained language model to model intensional knowledge, which
can capture both structure information and textual information. Experimental
results show that EIKE significantly outperforms state-of-the-art methods in
three datasets for both triple classification and link prediction, indicating
that EIKE provides a more comprehensive and representative perspective of the
domain.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2402.01677v3},
File          = {2402.01677v3.pdf}
}
@article{2406.10492v2,
Author        = {Libo Zhang and Yue Ning},
Title         = {Large Language Models as Interpolated and Extrapolated Event Predictors},
Eprint        = {2406.10492v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Salient facts of sociopolitical events are distilled into quadruples
following a format of subject, relation, object, and timestamp. Machine
learning methods, such as graph neural networks (GNNs) and recurrent neural
networks (RNNs), have been built to make predictions and infer relations on the
quadruple-based knowledge graphs (KGs). In many applications, quadruples are
extended to quintuples with auxiliary attributes such as text summaries that
describe the quadruple events. In this paper, we comprehensively investigate
how large language models (LLMs) streamline the design of event prediction
frameworks using quadruple-based or quintuple-based data while maintaining
competitive accuracy. We propose LEAP, a unified framework that leverages large
language models as event predictors. Specifically, we develop multiple prompt
templates to frame the object prediction (OP) task as a standard
question-answering (QA) task, suitable for instruction fine-tuning with an
encoder-decoder LLM. For multi-event forecasting (MEF) task, we design a simple
yet effective prompt template for each event quintuple. This novel approach
removes the need for GNNs and RNNs, instead utilizing an encoder-only LLM to
generate fixed intermediate embeddings, which are processed by a customized
downstream head with a self-attention mechanism to predict potential relation
occurrences in the future. Extensive experiments on multiple real-world
datasets using various evaluation metrics validate the effectiveness of our
approach.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.10492v2},
File          = {2406.10492v2.pdf}
}
@article{2111.08546v1,
Author        = {Vinitra Swamy and Angelika Romanou and Martin Jaggi},
Title         = {Interpreting Language Models Through Knowledge Graph Extraction},
Eprint        = {2111.08546v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Transformer-based language models trained on large text corpora have enjoyed
immense popularity in the natural language processing community and are
commonly used as a starting point for downstream tasks. While these models are
undeniably useful, it is a challenge to quantify their performance beyond
traditional accuracy metrics. In this paper, we compare BERT-based language
models through snapshots of acquired knowledge at sequential stages of the
training process. Structured relationships from training corpora may be
uncovered through querying a masked language model with probing tasks. We
present a methodology to unveil a knowledge acquisition timeline by generating
knowledge graph extracts from cloze "fill-in-the-blank" statements at various
stages of RoBERTa's early training. We extend this analysis to a comparison of
pretrained variations of BERT models (DistilBERT, BERT-base, RoBERTa). This
work proposes a quantitative framework to compare language models through
knowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech
analysis (POSOR) to identify the linguistic strengths of each model variant.
Using these metrics, machine learning practitioners can compare models,
diagnose their models' behavioral strengths and weaknesses, and identify new
targeted datasets to improve model performance.},
Year          = {2021},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2111.08546v1},
File          = {2111.08546v1.pdf}
}
@article{2305.07912v2,
Author        = {Wenjie Xu and Ben Liu and Miao Peng and Xu Jia and Min Peng},
Title         = {Pre-trained Language Model with Prompts for Temporal Knowledge Graph
  Completion},
Eprint        = {2305.07912v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Temporal Knowledge graph completion (TKGC) is a crucial task that involves
reasoning at known timestamps to complete the missing part of facts and has
attracted more and more attention in recent years. Most existing methods focus
on learning representations based on graph neural networks while inaccurately
extracting information from timestamps and insufficiently utilizing the implied
information in relations. To address these problems, we propose a novel TKGC
model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We
convert a series of sampled quadruples into pre-trained language model inputs
and convert intervals between timestamps into different prompts to make
coherent sentences with implicit semantic information. We train our model with
a masking strategy to convert TKGC task into a masked token prediction task,
which can leverage the semantic information in pre-trained language models.
Experiments on three benchmark datasets and extensive analysis demonstrate that
our model has great competitiveness compared to other models with four metrics.
Our model can effectively incorporate information from temporal knowledge
graphs into the language models.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.07912v2},
File          = {2305.07912v2.pdf}
}
@article{2412.18672v1,
Author        = {Ratnesh Kumar Joshi and Sagnik Sengupta and Asif Ekbal},
Title         = {From Hallucinations to Facts: Enhancing Language Models with Curated
  Knowledge Graphs},
Eprint        = {2412.18672v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Hallucination, a persistent challenge plaguing language models, undermines
their efficacy and trustworthiness in various natural language processing
endeavors by generating responses that deviate from factual accuracy or
coherence. This paper addresses language model hallucination by integrating
curated knowledge graph (KG) triples to anchor responses in empirical data. We
meticulously select and integrate relevant KG triples tailored to specific
contexts, enhancing factual grounding and alignment with input. Our
contribution involves constructing a comprehensive KG repository from Wikipedia
and refining data to spotlight essential information for model training. By
imbuing language models with access to this curated knowledge, we aim to
generate both linguistically fluent responses and deeply rooted in factual
accuracy and context relevance. This integration mitigates hallucinations by
providing a robust foundation of information, enabling models to draw upon a
rich reservoir of factual data during response generation. Experimental
evaluations demonstrate the effectiveness of multiple approaches in reducing
hallucinatory responses, underscoring the role of curated knowledge graphs in
improving the reliability and trustworthiness of language model outputs.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.18672v1},
File          = {2412.18672v1.pdf}
}
@article{2306.02130v2,
Author        = {Jana Straková and Eva Fučíková and Jan Hajič and Zdeňka Urešová},
Title         = {Extending an Event-type Ontology: Adding Verbs and Classes Using
  Fine-tuned LLMs Suggestions},
Eprint        = {2306.02130v2},
DOI           = {10.18653/v1/2023.law-1.9},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this project, we have investigated the use of advanced machine learning
methods, specifically fine-tuned large language models, for pre-annotating data
for a lexical extension task, namely adding descriptive words (verbs) to an
existing (but incomplete, as of yet) ontology of event types. Several research
questions have been focused on, from the investigation of a possible heuristics
to provide at least hints to annotators which verbs to include and which are
outside the current version of the ontology, to the possible use of the
automatic scores to help the annotators to be more efficient in finding a
threshold for identifying verbs that cannot be assigned to any existing class
and therefore they are to be used as seeds for a new class. We have also
carefully examined the correlation of the automatic scores with the human
annotation. While the correlation turned out to be strong, its influence on the
annotation proper is modest due to its near linearity, even though the mere
fact of such pre-annotation leads to relatively short annotation times.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.02130v2},
File          = {2306.02130v2.pdf}
}
@article{2410.11141v1,
Author        = {Shriram M S and Sushmitha S and Gayathri K S and Shahina A},
Title         = {Can Structured Data Reduce Epistemic Uncertainty?},
Eprint        = {2410.11141v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In this work, we present a framework that utilizes ontology alignment to
improve the learning process of deep learning models. With this approach we
show that models fine-tuned using ontologies learn a downstream task at a
higher rate with better performance on a sequential classification task
compared to the native version of the model. Additionally, we extend our work
to showcase how subsumption mappings retrieved during the process of ontology
alignment can help enhance Retrieval-Augmented Generation in Large Language
Models. The results show that the responses obtained by using subsumption
mappings show an increase of 8.97% in contextual similarity and a 1% increase
in factual accuracy. We also use these scores to define our Hallucination Index
and show that this approach reduces hallucination in LLMs by 4.847%.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.11141v1},
File          = {2410.11141v1.pdf}
}
@article{2411.03962v3,
Author        = {Zhangcheng Qiang and Kerry Taylor and Weiqing Wang},
Title         = {How Does A Text Preprocessing Pipeline Affect Ontology Syntactic
  Matching?},
Eprint        = {2411.03962v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The generic text preprocessing pipeline, comprising Tokenisation,
Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been
implemented in many ontology matching (OM) systems. However, the lack of
standardisation in text preprocessing creates diversity in mapping results. In
this paper, we investigate the effect of the text preprocessing pipeline on OM
tasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation
Initiative (OAEI) track repositories with 49 distinct alignments indicate: (1)
Tokenisation and Normalisation are currently more effective than Stop Words
Removal and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and
Stemming is task-specific. We recommend standalone Lemmatisation or Stemming
with post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer
perform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)
Tagging does not help Lemmatisation. To repair less effective Stop Words
Removal and Stemming/Lemmatisation used in OM tasks, we propose a novel
context-based pipeline repair approach that significantly improves matching
correctness and overall matching performance. We also discuss the use of text
preprocessing pipeline in the new era of large language models (LLMs).},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.03962v3},
File          = {2411.03962v3.pdf}
}
@article{2411.15666v1,
Author        = {Gaya Mehenni and Amal Zouaq},
Title         = {Ontology-Constrained Generation of Domain-Specific Clinical Summaries},
Eprint        = {2411.15666v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) offer promising solutions for text
summarization. However, some domains require specific information to be
available in the summaries. Generating these domain-adapted summaries is still
an open challenge. Similarly, hallucinations in generated content is a major
drawback of current approaches, preventing their deployment. This study
proposes a novel approach that leverages ontologies to create domain-adapted
summaries both structured and unstructured. We employ an ontology-guided
constrained decoding process to reduce hallucinations while improving
relevance. When applied to the medical domain, our method shows potential in
summarizing Electronic Health Records (EHRs) across different specialties,
allowing doctors to focus on the most relevant information to their domain.
Evaluation on the MIMIC-III dataset demonstrates improvements in generating
domain-adapted summaries of clinical notes and hallucination reduction.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.15666v1},
File          = {2411.15666v1.pdf}
}
@article{2310.09089v2,
Author        = {Qichen Ye and Junling Liu and Dading Chong and Peilin Zhou and Yining Hua and Fenglin Liu and Meng Cao and Ziming Wang and Xuxin Cheng and Zhu Lei and Zhenhua Guo},
Title         = {Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large
  Language Model},
Eprint        = {2310.09089v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Integrating large language models (LLMs) into healthcare holds great
potential but faces challenges. Pre-training LLMs from scratch for domains like
medicine is resource-heavy and often unfeasible. On the other hand, sole
reliance on Supervised Fine-tuning (SFT) can result in overconfident
predictions and may not tap into domain-specific insights. In response, we
present a multi-stage training method combining Domain-specific Continued
Pre-training (DCPT), SFT, and Direct Preference Optimization (DPO). In
addition, we publish a 3Gb Chinese Medicine (ChiMed) dataset, encompassing
medical question answering, plain texts, knowledge graphs, and dialogues,
segmented into three training stages. The medical LLM trained with our
pipeline, Qilin-Med, shows substantial performance improvement. In the CPT and
SFT phases, Qilin-Med achieved 38.4% and 40.0% accuracy on the CMExam test set,
respectively. It outperformed the basemodel Baichuan-7B (accuracy: 33.5%), by
7.5%. In the DPO phase, it scored 16.66 in BLEU-1 and 27.44 in ROUGE-1 on the
Huatuo-26M test set, bringing further improvement to the SFT phase (12.69 in
BLEU-1 and 24.21 in ROUGE-1). Additionally, we have further enhanced the
model's performance through the Retrieval Augmented Generation (RAG) approach.
Experiments demonstrate that Qilin-Med-RAG achieves an accuracy rate of 42.8%
on CMExam. These results highlight the contribution of our novel training
approach in building LLMs for medical applications.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.09089v2},
File          = {2310.09089v2.pdf}
}
@article{2311.06318v2,
Author        = {Jinheon Baek and Nirupama Chandrasekaran and Silviu Cucerzan and Allen herring and Sujay Kumar Jauhar},
Title         = {Knowledge-Augmented Large Language Models for Personalized Contextual
  Query Suggestion},
Eprint        = {2311.06318v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Large Language Models (LLMs) excel at tackling various natural language
tasks. However, due to the significant costs involved in re-training or
fine-tuning them, they remain largely static and difficult to personalize.
Nevertheless, a variety of applications could benefit from generations that are
tailored to users' preferences, goals, and knowledge. Among them is web search,
where knowing what a user is trying to accomplish, what they care about, and
what they know can lead to improved search experiences. In this work, we
propose a novel and general approach that augments an LLM with relevant context
from users' interaction histories with a search engine in order to personalize
its outputs. Specifically, we construct an entity-centric knowledge store for
each user based on their search and browsing activities on the web, which is
then leveraged to provide contextually relevant LLM prompt augmentations. This
knowledge store is light-weight, since it only produces user-specific aggregate
projections of interests and knowledge onto public knowledge graphs, and
leverages existing search log infrastructure, thereby mitigating the privacy,
compliance, and scalability concerns associated with building deep user
profiles for personalization. We validate our approach on the task of
contextual query suggestion, which requires understanding not only the user's
current search context but also what they historically know and care about.
Through a number of experiments based on human evaluation, we show that our
approach is significantly better than several other LLM-powered baselines,
generating query suggestions that are contextually more relevant, personalized,
and useful.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.06318v2},
File          = {2311.06318v2.pdf}
}
@article{2405.16806v2,
Author        = {Shengyuan Chen and Qinggang Zhang and Junnan Dong and Wen Hua and Qing Li and Xiao Huang},
Title         = {Entity Alignment with Noisy Annotations from Large Language Models},
Eprint        = {2405.16806v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying
equivalent entity pairs. While existing methods heavily rely on human-generated
labels, it is prohibitively expensive to incorporate cross-domain experts for
annotation in real-world scenarios. The advent of Large Language Models (LLMs)
presents new avenues for automating EA with annotations, inspired by their
comprehensive capability to process semantic information. However, it is
nontrivial to directly apply LLMs for EA since the annotation space in
real-world KGs is large. LLMs could also generate noisy labels that may mislead
the alignment. To this end, we propose a unified framework, LLM4EA, to
effectively leverage LLMs for EA. Specifically, we design a novel active
learning policy to significantly reduce the annotation space by prioritizing
the most valuable entities based on the entire inter-KG and intra-KG structure.
Moreover, we introduce an unsupervised label refiner to continuously enhance
label accuracy through in-depth probabilistic reasoning. We iteratively
optimize the policy based on the feedback from a base EA model. Extensive
experiments demonstrate the advantages of LLM4EA on four benchmark datasets in
terms of effectiveness, robustness, and efficiency. Codes are available via
https://github.com/chensyCN/llm4ea_official.},
Year          = {2024},
Month         = {May},
Note          = {NeurIPS 2024},
Url           = {http://arxiv.org/abs/2405.16806v2},
File          = {2405.16806v2.pdf}
}
@article{2409.10077v1,
Author        = {Le Xiao and Yunfei Xu and Jing Zhao},
Title         = {LLM-DER:A Named Entity Recognition Method Based on Large Language Models
  for Chinese Coal Chemical Domain},
Eprint        = {2409.10077v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Domain-specific Named Entity Recognition (NER), whose goal is to recognize
domain-specific entities and their categories, provides an important support
for constructing domain knowledge graphs. Currently, deep learning-based
methods are widely used and effective in NER tasks, but due to the reliance on
large-scale labeled data. As a result, the scarcity of labeled data in a
specific domain will limit its application.Therefore, many researches started
to introduce few-shot methods and achieved some results. However, the entity
structures in specific domains are often complex, and the current few-shot
methods are difficult to adapt to NER tasks with complex features.Taking the
Chinese coal chemical industry domain as an example,there exists a complex
structure of multiple entities sharing a single entity, as well as multiple
relationships for the same pair of entities, which affects the NER task under
the sample less condition.In this paper, we propose a Large Language Models
(LLMs)-based entity recognition framework LLM-DER for the domain-specific
entity recognition problem in Chinese, which enriches the entity information by
generating a list of relationships containing entity types through LLMs, and
designing a plausibility and consistency evaluation method to remove
misrecognized entities, which can effectively solve the complex structural
entity recognition problem in a specific domain.The experimental results of
this paper on the Resume dataset and the self-constructed coal chemical dataset
Coal show that LLM-DER performs outstandingly in domain-specific entity
recognition, not only outperforming the existing GPT-3.5-turbo baseline, but
also exceeding the fully-supervised baseline, verifying its effectiveness in
entity recognition.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.10077v1},
File          = {2409.10077v1.pdf}
}
@article{2210.10709v5,
Author        = {Yunzhi Yao and Shengyu Mao and Ningyu Zhang and Xiang Chen and Shumin Deng and Xi Chen and Huajun Chen},
Title         = {Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph
  Construction},
Eprint        = {2210.10709v5},
DOI           = {10.1145/3539618.3591763},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {With the development of pre-trained language models, many prompt-based
approaches to data-efficient knowledge graph construction have been proposed
and achieved impressive performance. However, existing prompt-based learning
methods for knowledge graph construction are still susceptible to several
potential limitations: (i) semantic gap between natural language and output
structured knowledge with pre-defined schema, which means model cannot fully
exploit semantic knowledge with the constrained templates; (ii) representation
learning with locally individual instances limits the performance given the
insufficient features, which are unable to unleash the potential analogical
capability of pre-trained language models. Motivated by these observations, we
propose a retrieval-augmented approach, which retrieves schema-aware Reference
As Prompt (RAP), for data-efficient knowledge graph construction. It can
dynamically leverage schema and knowledge inherited from human-annotated and
weak-supervised data as a prompt for each sample, which is model-agnostic and
can be plugged into widespread existing approaches. Experimental results
demonstrate that previous methods integrated with RAP can achieve impressive
performance gains in low-resource settings on five datasets of relational
triple extraction and event extraction for knowledge graph construction. Code
is available in https://github.com/zjunlp/RAP.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.10709v5},
File          = {2210.10709v5.pdf}
}
@article{2407.12703v5,
Author        = {Youmin Ko and Hyemin Yang and Taeuk Kim and Hyunjoon Kim},
Title         = {Subgraph-Aware Training of Language Models for Knowledge Graph
  Completion Using Structure-Aware Contrastive Learning},
Eprint        = {2407.12703v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Fine-tuning pre-trained language models (PLMs) has recently shown a potential
to improve knowledge graph completion (KGC). However, most PLM-based methods
focus solely on encoding textual information, neglecting the long-tailed nature
of knowledge graphs and their various topological structures, e.g., subgraphs,
shortest paths, and degrees. We claim that this is a major obstacle to
achieving higher accuracy of PLMs for KGC. To this end, we propose a
Subgraph-Aware Training framework for KGC (SATKGC) with two ideas: (i)
subgraph-aware mini-batching to encourage hard negative sampling and to
mitigate an imbalance in the frequency of entity occurrences during training,
and (ii) new contrastive learning to focus more on harder in-batch negative
triples and harder positive triples in terms of the structural properties of
the knowledge graph. To the best of our knowledge, this is the first study to
comprehensively incorporate the structural inductive bias of the knowledge
graph into fine-tuning PLMs. Extensive experiments on three KGC benchmarks
demonstrate the superiority of SATKGC. Our code is available.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.12703v5},
File          = {2407.12703v5.pdf}
}
@article{2311.00444v1,
Author        = {Peter A. Zachares and Vahan Hovhannisyan and Alan Mosca and Yarin Gal},
Title         = {Form follows Function: Text-to-Text Conditional Graph Generation based
  on Functional Requirements},
Eprint        = {2311.00444v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {This work focuses on the novel problem setting of generating graphs
conditioned on a description of the graph's functional requirements in a
downstream task. We pose the problem as a text-to-text generation problem and
focus on the approach of fine-tuning a pretrained large language model (LLM) to
generate graphs. We propose an inductive bias which incorporates information
about the structure of the graph into the LLM's generation process by
incorporating message passing layers into an LLM's architecture. To evaluate
our proposed method, we design a novel set of experiments using publicly
available and widely studied molecule and knowledge graph data sets. Results
suggest our proposed approach generates graphs which more closely meet the
requested functional requirements, outperforming baselines developed on similar
tasks by a statistically significant margin.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.00444v1},
File          = {2311.00444v1.pdf}
}
@article{2402.05862v1,
Author        = {Bryan Perozzi and Bahare Fatemi and Dustin Zelle and Anton Tsitsulin and Mehran Kazemi and Rami Al-Rfou and Jonathan Halcrow},
Title         = {Let Your Graph Do the Talking: Encoding Structured Data for LLMs},
Eprint        = {2402.05862v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {How can we best encode structured data into sequential form for use in large
language models (LLMs)? In this work, we introduce a parameter-efficient method
to explicitly represent structured data for LLMs. Our method, GraphToken,
learns an encoding function to extend prompts with explicit structured
information. Unlike other work which focuses on limited domains (e.g. knowledge
graph representation), our work is the first effort focused on the general
encoding of structured data to be used for various reasoning tasks. We show
that explicitly representing the graph structure allows significant
improvements to graph reasoning tasks. Specifically, we see across the board
improvements - up to 73% points - on node, edge and, graph-level tasks from the
GraphQA benchmark.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.05862v1},
File          = {2402.05862v1.pdf}
}
@article{2404.07456v1,
Author        = {Xu Huang and Weiwen Liu and Xiaolong Chen and Xingmei Wang and Defu Lian and Yasheng Wang and Ruiming Tang and Enhong Chen},
Title         = {WESE: Weak Exploration to Strong Exploitation for LLM Agents},
Eprint        = {2404.07456v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Recently, large language models (LLMs) have demonstrated remarkable potential
as an intelligent agent. However, existing researches mainly focus on enhancing
the agent's reasoning or decision-making abilities through well-designed prompt
engineering or task-specific fine-tuning, ignoring the procedure of exploration
and exploitation. When addressing complex tasks within open-world interactive
environments, these methods exhibit limitations. Firstly, the lack of global
information of environments leads to greedy decisions, resulting in sub-optimal
solutions. On the other hand, irrelevant information acquired from the
environment not only adversely introduces noise, but also incurs additional
cost. This paper proposes a novel approach, Weak Exploration to Strong
Exploitation (WESE), to enhance LLM agents in solving open-world interactive
tasks. Concretely, WESE involves decoupling the exploration and exploitation
process, employing a cost-effective weak agent to perform exploration tasks for
global knowledge. A knowledge graph-based strategy is then introduced to store
the acquired knowledge and extract task-relevant knowledge, enhancing the
stronger agent in success rate and efficiency for the exploitation task. Our
approach is flexible enough to incorporate diverse tasks, and obtains
significant improvements in both success rates and efficiency across four
interactive benchmarks.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.07456v1},
File          = {2404.07456v1.pdf}
}
@article{2405.16506v2,
Author        = {Yuntong Hu and Zhihan Lei and Zheng Zhang and Bo Pan and Chen Ling and Liang Zhao},
Title         = {GRAG: Graph Retrieval-Augmented Generation},
Eprint        = {2405.16506v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Naive Retrieval-Augmented Generation (RAG) focuses on individual documents
during retrieval and, as a result, falls short in handling networked documents
which are very popular in many applications such as citation graphs, social
media, and knowledge graphs. To overcome this limitation, we introduce Graph
Retrieval-Augmented Generation (GRAG), which tackles the fundamental challenges
in retrieving textual subgraphs and integrating the joint textual and
topological information into Large Language Models (LLMs) to enhance its
generation. To enable efficient textual subgraph retrieval, we propose a novel
divide-and-conquer strategy that retrieves the optimal subgraph structure in
linear time. To achieve graph context-aware generation, incorporate textual
graphs into LLMs through two complementary views-the text view and the graph
view-enabling LLMs to more effectively comprehend and utilize the graph
context. Extensive experiments on graph reasoning benchmarks demonstrate that
in scenarios requiring multi-hop reasoning on textual graphs, our GRAG approach
significantly outperforms current state-of-the-art RAG methods.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.16506v2},
File          = {2405.16506v2.pdf}
}
@article{2412.12513v1,
Author        = {Rabimba Karanjai and Sam Blackshear and Lei Xu and Weidong Shi},
Title         = {Generating Move Smart Contracts based on Concepts},
Eprint        = {2412.12513v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {The growing adoption of formal verification for smart contracts has spurred
the development of new verifiable languages like Move. However, the limited
availability of training data for these languages hinders effective code
generation by large language models (LLMs). This paper presents ConMover, a
novel framework that enhances LLM-based code generation for Move by leveraging
a knowledge graph of Move concepts and a small set of verified code examples.
ConMover integrates concept retrieval, planning, coding, and debugging agents
in an iterative process to refine generated code. Evaluations with various
open-source LLMs demonstrate substantial accuracy improvements over baseline
models. These results underscore ConMover's potential to address low-resource
code generation challenges, bridging the gap between natural language
descriptions and reliable smart contract development.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.12513v1},
File          = {2412.12513v1.pdf}
}
@article{2501.09993v1,
Author        = {Yeonseok Jeong and Minsoo Kim and Seung-won Hwang and Byung-Hak Kim},
Title         = {Agent-as-Judge for Factual Summarization of Long Narratives},
Eprint        = {2501.09993v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have demonstrated near-human performance in
summarization tasks based on traditional metrics such as ROUGE and BERTScore.
However, these metrics do not adequately capture critical aspects of
summarization quality, such as factual accuracy, particularly for long
narratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the
limitations of metrics based on lexical similarity but still exhibit factual
inconsistencies, especially in understanding character relationships and
states. In this work, we introduce NarrativeFactScore, a novel
"Agent-as-a-Judge" framework for evaluating and refining summaries. By
leveraging a Character Knowledge Graph (CKG) extracted from input and generated
summaries, NarrativeFactScore assesses the factual consistency and provides
actionable guidance for refinement, such as identifying missing or erroneous
facts. We demonstrate the effectiveness of NarrativeFactScore through a
detailed workflow illustration and extensive validation on widely adopted
benchmarks, achieving superior performance compared to competitive methods. Our
results highlight the potential of agent-driven evaluation systems to improve
the factual reliability of LLM-generated summaries.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.09993v1},
File          = {2501.09993v1.pdf}
}
@article{2412.20331v1,
Author        = {Moe Kayali and Fabian Wenz and Nesime Tatbul and Çağatay Demiralp},
Title         = {Mind the Data Gap: Bridging LLMs to Enterprise Data Integration},
Eprint        = {2412.20331v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Leading large language models (LLMs) are trained on public data. However,
most of the world's data is dark data that is not publicly accessible, mainly
in the form of private organizational or enterprise data. We show that the
performance of methods based on LLMs seriously degrades when tested on
real-world enterprise datasets. Current benchmarks, based on public data,
overestimate the performance of LLMs. We release a new benchmark dataset, the
GOBY Benchmark, to advance discovery in enterprise data integration. Based on
our experience with this enterprise benchmark, we propose techniques to uplift
the performance of LLMs on enterprise data, including (1) hierarchical
annotation, (2) runtime class-learning, and (3) ontology synthesis. We show
that, once these techniques are deployed, the performance on enterprise data
becomes on par with that of public data. The Goby benchmark can be obtained at
https://goby-benchmark.github.io/.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.20331v1},
File          = {2412.20331v1.pdf}
}
@article{2307.11346v1,
Author        = {Zihan Guan and Zihao Wu and Zhengliang Liu and Dufan Wu and Hui Ren and Quanzheng Li and Xiang Li and Ninghao Liu},
Title         = {CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study},
Eprint        = {2307.11346v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Participant recruitment based on unstructured medical texts such as clinical
notes and radiology reports has been a challenging yet important task for the
cohort establishment in clinical research. Recently, Large Language Models
(LLMs) such as ChatGPT have achieved tremendous success in various downstream
tasks thanks to their promising performance in language understanding,
inference, and generation. It is then natural to test their feasibility in
solving the cohort recruitment task, which involves the classification of a
given paragraph of medical text into disease label(s). However, when applied to
knowledge-intensive problem settings such as medical text classification, where
the LLMs are expected to understand the decision made by human experts and
accurately identify the implied disease labels, the LLMs show a mediocre
performance. A possible explanation is that, by only using the medical text,
the LLMs neglect to use the rich context of additional information that
languages afford. To this end, we propose to use a knowledge graph as auxiliary
information to guide the LLMs in making predictions. Moreover, to further boost
the LLMs adapt to the problem setting, we apply a chain-of-thought (CoT) sample
selection strategy enhanced by reinforcement learning, which selects a set of
CoT samples given each individual medical report. Experimental results and
various ablation studies show that our few-shot learning method achieves
satisfactory performance compared with fine-tuning strategies and gains superb
advantages when the available data is limited. The code and sample dataset of
the proposed CohortGPT model is available at:
https://anonymous.4open.science/r/CohortGPT-4872/},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.11346v1},
File          = {2307.11346v1.pdf}
}
@article{2401.12671v3,
Author        = {Somnath Banerjee and Amruit Sahoo and Sayan Layek and Avik Dutta and Rima Hazra and Animesh Mukherjee},
Title         = {Context Matters: Pushing the Boundaries of Open-Ended Answer Generation
  with Graph-Structured Knowledge Context},
Eprint        = {2401.12671v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In the continuously advancing AI landscape, crafting context-rich and
meaningful responses via Large Language Models (LLMs) is essential. Researchers
are becoming more aware of the challenges that LLMs with fewer parameters
encounter when trying to provide suitable answers to open-ended questions. To
address these hurdles, the integration of cutting-edge strategies, augmentation
of rich external domain knowledge to LLMs, offers significant improvements.
This paper introduces a novel framework that combines graph-driven context
retrieval in conjunction to knowledge graphs based enhancement, honing the
proficiency of LLMs, especially in domain specific community question answering
platforms like AskUbuntu, Unix, and ServerFault. We conduct experiments on
various LLMs with different parameter sizes to evaluate their ability to ground
knowledge and determine factual accuracy in answers to open-ended questions.
Our methodology GraphContextGen consistently outperforms dominant text-based
retrieval systems, demonstrating its robustness and adaptability to a larger
number of use cases. This advancement highlights the importance of pairing
context rich data retrieval with LLMs, offering a renewed approach to knowledge
sourcing and generation in AI systems. We also show that, due to rich
contextual data retrieval, the crucial entities, along with the generated
answer, remain factually coherent with the gold answer.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.12671v3},
File          = {2401.12671v3.pdf}
}
@article{2402.06341v2,
Author        = {Xuanzhong Chen and Xiaohao Mao and Qihan Guo and Lun Wang and Shuyang Zhang and Ting Chen},
Title         = {RareBench: Can LLMs Serve as Rare Diseases Specialists?},
Eprint        = {2402.06341v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Generalist Large Language Models (LLMs), such as GPT-4, have shown
considerable promise in various domains, including medical diagnosis. Rare
diseases, affecting approximately 300 million people worldwide, often have
unsatisfactory clinical diagnosis rates primarily due to a lack of experienced
physicians and the complexity of differentiating among many rare diseases. In
this context, recent news such as "ChatGPT correctly diagnosed a 4-year-old's
rare disease after 17 doctors failed" underscore LLMs' potential, yet
underexplored, role in clinically diagnosing rare diseases. To bridge this
research gap, we introduce RareBench, a pioneering benchmark designed to
systematically evaluate the capabilities of LLMs on 4 critical dimensions
within the realm of rare diseases. Meanwhile, we have compiled the largest
open-source dataset on rare disease patients, establishing a benchmark for
future studies in this domain. To facilitate differential diagnosis of rare
diseases, we develop a dynamic few-shot prompt methodology, leveraging a
comprehensive rare disease knowledge graph synthesized from multiple knowledge
bases, significantly enhancing LLMs' diagnostic performance. Moreover, we
present an exhaustive comparative study of GPT-4's diagnostic capabilities
against those of specialist physicians. Our experimental findings underscore
the promising potential of integrating LLMs into the clinical diagnostic
process for rare diseases. This paves the way for exciting possibilities in
future advancements in this field.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.06341v2},
File          = {2402.06341v2.pdf}
}
@article{2501.16191v1,
Author        = {Antony Bartlett and Cynthia Liem and Annibale Panichella},
Title         = {Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python
  using LLMs},
Eprint        = {2501.16191v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Fixing Python dependency issues is a tedious and error-prone task for
developers, who must manually identify and resolve environment dependencies and
version constraints of third-party modules and Python interpreters. Researchers
have attempted to automate this process by relying on large knowledge graphs
and database lookup tables. However, these traditional approaches face
limitations due to the variety of dependency error types, large sets of
possible module versions, and conflicts among transitive dependencies. This
study explores the potential of using large language models (LLMs) to
automatically fix dependency issues in Python programs. We introduce PLLM
(pronounced "plum"), a novel technique that employs retrieval-augmented
generation (RAG) to help an LLM infer Python versions and required modules for
a given Python file. PLLM builds a testing environment that iteratively (1)
prompts the LLM for module combinations, (2) tests the suggested changes, and
(3) provides feedback (error messages) to the LLM to refine the fix. This
feedback cycle leverages natural language processing (NLP) to intelligently
parse and interpret build error messages. We benchmark PLLM on the Gistable
HG2.9K dataset, a collection of challenging single-file Python gists. We
compare PLLM against two state-of-the-art automatic dependency inference
approaches, namely PyEGo and ReadPyE, w.r.t. the ability to resolve dependency
issues. Our results indicate that PLLM can fix more dependency issues than the
two baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%)
over PyEGo. Our deeper analyses suggest that PLLM is particularly beneficial
for projects with many dependencies and for specific third-party numerical and
machine-learning modules. Our findings demonstrate the potential of LLM-based
approaches to iteratively resolve Python dependency issues.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.16191v1},
File          = {2501.16191v1.pdf}
}
@article{1911.03876v2,
Author        = {Antoine Bosselut and Ronan Le Bras and Yejin Choi},
Title         = {Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot
  Commonsense Question Answering},
Eprint        = {1911.03876v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Understanding narratives requires reasoning about implicit world knowledge
related to the causes, effects, and states of situations described in text. At
the core of this challenge is how to access contextually relevant knowledge on
demand and reason over it.
  In this paper, we present initial studies toward zero-shot commonsense
question answering by formulating the task as inference over dynamically
generated commonsense knowledge graphs. In contrast to previous studies for
knowledge integration that rely on retrieval of existing knowledge from static
knowledge graphs, our study requires commonsense knowledge integration where
contextually relevant knowledge is often not present in existing knowledge
bases. Therefore, we present a novel approach that generates
contextually-relevant symbolic knowledge structures on demand using generative
neural commonsense knowledge models.
  Empirical results on two datasets demonstrate the efficacy of our
neuro-symbolic approach for dynamically constructing knowledge graphs for
reasoning. Our approach achieves significant performance boosts over pretrained
language models and vanilla knowledge models, all while providing interpretable
reasoning paths for its predictions.},
Year          = {2019},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1911.03876v2},
File          = {1911.03876v2.pdf}
}
@article{2111.11845v1,
Author        = {Mohamad Yaser Jaradeh and Kuldeep Singh and Markus Stocker and Sören Auer},
Title         = {Triple Classification for Scholarly Knowledge Graph Completion},
Eprint        = {2111.11845v1},
DOI           = {10.1145/3460210.3493582},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Scholarly Knowledge Graphs (KGs) provide a rich source of structured
information representing knowledge encoded in scientific publications. With the
sheer volume of published scientific literature comprising a plethora of
inhomogeneous entities and relations to describe scientific concepts, these KGs
are inherently incomplete. We present exBERT, a method for leveraging
pre-trained transformer language models to perform scholarly knowledge graph
completion. We model triples of a knowledge graph as text and perform triple
classification (i.e., belongs to KG or not). The evaluation shows that exBERT
outperforms other baselines on three scholarly KG completion datasets in the
tasks of triple classification, link prediction, and relation prediction.
Furthermore, we present two scholarly datasets as resources for the research
community, collected from public KGs and online resources.},
Year          = {2021},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2111.11845v1},
File          = {2111.11845v1.pdf}
}
@article{2407.14146v1,
Author        = {Rui Zhang and Yafen Lu and Pengli Ji and Junxiao Xue and Xiaoran Yan},
Title         = {Fine-grained Knowledge Graph-driven Video-Language Learning for Action
  Recognition},
Eprint        = {2407.14146v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.MM},
Abstract      = {Recent work has explored video action recognition as a video-text matching
problem and several effective methods have been proposed based on large-scale
pre-trained vision-language models. However, these approaches primarily operate
at a coarse-grained level without the detailed and semantic understanding of
action concepts by exploiting fine-grained semantic connections between actions
and body movements. To address this gap, we propose a contrastive
video-language learning framework guided by a knowledge graph, termed KG-CLIP,
which incorporates structured information into the CLIP model in the video
domain. Specifically, we construct a multi-modal knowledge graph composed of
multi-grained concepts by parsing actions based on compositional learning. By
implementing a triplet encoder and deviation compensation to adaptively
optimize the margin in the entity distance function, our model aims to improve
alignment of entities in the knowledge graph to better suit complex
relationship learning. This allows for enhanced video action recognition
capabilities by accommodating nuanced associations between graph components. We
comprehensively evaluate KG-CLIP on Kinetics-TPS, a large-scale action parsing
dataset, demonstrating its effectiveness compared to competitive baselines.
Especially, our method excels at action recognition with few sample frames or
limited training data, which exhibits excellent data utilization and learning
capabilities.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.14146v1},
File          = {2407.14146v1.pdf}
}
@article{2501.01644v1,
Author        = {Tien Dang and Viet Thanh Duy Nguyen and Minh Tuan Le and Truong-Son Hy},
Title         = {Multimodal Contrastive Representation Learning in Augmented Biomedical
  Knowledge Graphs},
Eprint        = {2501.01644v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Biomedical Knowledge Graphs (BKGs) integrate diverse datasets to elucidate
complex relationships within the biomedical field. Effective link prediction on
these graphs can uncover valuable connections, such as potential novel
drug-disease relations. We introduce a novel multimodal approach that unifies
embeddings from specialized Language Models (LMs) with Graph Contrastive
Learning (GCL) to enhance intra-entity relationships while employing a
Knowledge Graph Embedding (KGE) model to capture inter-entity relationships for
effective link prediction. To address limitations in existing BKGs, we present
PrimeKG++, an enriched knowledge graph incorporating multimodal data, including
biological sequences and textual descriptions for each entity type. By
combining semantic and relational information in a unified representation, our
approach demonstrates strong generalizability, enabling accurate link
predictions even for unseen nodes. Experimental results on PrimeKG++ and the
DrugBank drug-target interaction dataset demonstrate the effectiveness and
robustness of our method across diverse biomedical datasets. Our source code,
pre-trained models, and data are publicly available at
https://github.com/HySonLab/BioMedKG},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.01644v1},
File          = {2501.01644v1.pdf}
}
@article{2501.16350v1,
Author        = {Arash Ghafouri and Mahdi Firouzmandi and Hasan Naderi},
Title         = {A Method for Multi-Hop Question Answering on Persian Knowledge Graph},
Eprint        = {2501.16350v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Question answering systems are the latest evolution in information retrieval
technology, designed to accept complex queries in natural language and provide
accurate answers using both unstructured and structured knowledge sources.
Knowledge Graph Question Answering (KGQA) systems fulfill users' information
needs by utilizing structured data, representing a vast number of facts as a
graph. However, despite significant advancements, major challenges persist in
answering multi-hop complex questions, particularly in Persian. One of the main
challenges is the accurate understanding and transformation of these multi-hop
complex questions into semantically equivalent SPARQL queries, which allows for
precise answer retrieval from knowledge graphs. In this study, to address this
issue, a dataset of 5,600 Persian multi-hop complex questions was developed,
along with their decomposed forms based on the semantic representation of the
questions. Following this, Persian language models were trained using this
dataset, and an architecture was proposed for answering complex questions using
a Persian knowledge graph. Finally, the proposed method was evaluated against
similar systems on the PeCoQ dataset. The results demonstrated the superiority
of our approach, with an improvement of 12.57% in F1-score and 12.06% in
accuracy compared to the best comparable method.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.16350v1},
File          = {2501.16350v1.pdf}
}
@article{2409.13744v1,
Author        = {Daniel B. Hier and Thanh Son Do and Tayo Obafemi-Ajayi},
Title         = {A Simplified Retriever to Improve Accuracy of Phenotype Normalizations
  by Large Language Models},
Eprint        = {2409.13744v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have shown improved accuracy in phenotype term
normalization tasks when augmented with retrievers that suggest candidate
normalizations based on term definitions. In this work, we introduce a
simplified retriever that enhances LLM accuracy by searching the Human
Phenotype Ontology (HPO) for candidate matches using contextual word embeddings
from BioBERT without the need for explicit term definitions. Testing this
method on terms derived from the clinical synopses of Online Mendelian
Inheritance in Man (OMIM), we demonstrate that the normalization accuracy of a
state-of-the-art LLM increases from a baseline of 62.3% without augmentation to
90.3% with retriever augmentation. This approach is potentially generalizable
to other biomedical term normalization tasks and offers an efficient
alternative to more complex retrieval methods.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.13744v1},
File          = {2409.13744v1.pdf}
}
@article{2411.08469v2,
Author        = {Fadi Al Machot and Martin Thomas Horsch and Habib Ullah},
Title         = {Building Trustworthy AI: Transparent AI Systems via Large Language
  Models, Ontologies, and Logical Reasoning (TranspNet)},
Eprint        = {2411.08469v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Growing concerns over the lack of transparency in AI, particularly in
high-stakes fields like healthcare and finance, drive the need for explainable
and trustworthy systems. While Large Language Models (LLMs) perform
exceptionally well in generating accurate outputs, their "black box" nature
poses significant challenges to transparency and trust. To address this, the
paper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs.
By leveraging domain expert knowledge, retrieval-augmented generation (RAG),
and formal reasoning frameworks like Answer Set Programming (ASP), TranspNet
enhances LLM outputs with structured reasoning and verification.This approach
strives to help AI systems deliver results that are as accurate, explainable,
and trustworthy as possible, aligning with regulatory expectations for
transparency and accountability. TranspNet provides a solution for developing
AI systems that are reliable and interpretable, making it suitable for
real-world applications where trust is critical.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.08469v2},
File          = {2411.08469v2.pdf}
}
@article{2412.11043v1,
Author        = {Minhao Bai and Jinshuai Yang and Kaiyi Pang and Yongfeng Huang and Yue Gao},
Title         = {Semantic Steganography: A Framework for Robust and High-Capacity
  Information Hiding using Large Language Models},
Eprint        = {2412.11043v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {In the era of Large Language Models (LLMs), generative linguistic
steganography has become a prevalent technique for hiding information within
model-generated texts. However, traditional steganography methods struggle to
effectively align steganographic texts with original model-generated texts due
to the lower entropy of the predicted probability distribution of LLMs. This
results in a decrease in embedding capacity and poses challenges for decoding
stegos in real-world communication channels. To address these challenges, we
propose a semantic steganography framework based on LLMs, which construct a
semantic space and map secret messages onto this space using ontology-entity
trees. This framework offers robustness and reliability for transmission in
complex channels, as well as resistance to text rendering and word blocking.
Additionally, the stegos generated by our framework are indistinguishable from
the covers and achieve a higher embedding capacity compared to state-of-the-art
steganography methods, while producing higher quality stegos.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.11043v1},
File          = {2412.11043v1.pdf}
}
@article{2310.06552v3,
Author        = {Joseph S. Boyle and Antanas Kascenas and Pat Lok and Maria Liakata and Alison Q. O'Neil},
Title         = {Automated clinical coding using off-the-shelf large language models},
Eprint        = {2310.06552v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The task of assigning diagnostic ICD codes to patient hospital admissions is
typically performed by expert human coders. Efforts towards automated ICD
coding are dominated by supervised deep learning models. However, difficulties
in learning to predict the large number of rare codes remain a barrier to
adoption in clinical practice. In this work, we leverage off-the-shelf
pre-trained generative large language models (LLMs) to develop a practical
solution that is suitable for zero-shot and few-shot code assignment, with no
need for further task-specific training. Unsupervised pre-training alone does
not guarantee precise knowledge of the ICD ontology and specialist clinical
coding task, therefore we frame the task as information extraction, providing a
description of each coded concept and asking the model to retrieve related
mentions. For efficiency, rather than iterating over all codes, we leverage the
hierarchical nature of the ICD ontology to sparsely search for relevant codes.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.06552v3},
File          = {2310.06552v3.pdf}
}
@article{2403.02014v1,
Author        = {Daniel Alfasi and Tal Shapira and Anat Bremler Barr},
Title         = {Unveiling Hidden Links Between Unseen Security Entities},
Eprint        = {2403.02014v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {The proliferation of software vulnerabilities poses a significant challenge
for security databases and analysts tasked with their timely identification,
classification, and remediation. With the National Vulnerability Database (NVD)
reporting an ever-increasing number of vulnerabilities, the traditional manual
analysis becomes untenably time-consuming and prone to errors. This paper
introduces VulnScopper, an innovative approach that utilizes multi-modal
representation learning, combining Knowledge Graphs (KG) and Natural Language
Processing (NLP), to automate and enhance the analysis of software
vulnerabilities. Leveraging ULTRA, a knowledge graph foundation model, combined
with a Large Language Model (LLM), VulnScopper effectively handles unseen
entities, overcoming the limitations of previous KG approaches. We evaluate
VulnScopper on two major security datasets, the NVD and the Red Hat CVE
database. Our method significantly improves the link prediction accuracy
between Common Vulnerabilities and Exposures (CVEs), Common Weakness
Enumeration (CWEs), and Common Platform Enumerations (CPEs). Our results show
that VulnScopper outperforms existing methods, achieving up to 78% Hits@10
accuracy in linking CVEs to CPEs and CWEs and presenting an 11.7% improvement
over large language models in predicting CWE labels based on the Red Hat
database. Based on the NVD, only 6.37% of the linked CPEs are being published
during the first 30 days; many of them are related to critical and high-risk
vulnerabilities which, according to multiple compliance frameworks (such as
CISA and PCI), should be remediated within 15-30 days. Our model can uncover
new products linked to vulnerabilities, reducing remediation time and improving
vulnerability management. We analyzed several CVEs from 2023 to showcase this
ability.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.02014v1},
File          = {2403.02014v1.pdf}
}
@article{2404.19744v1,
Author        = {Leon Garza and Lavanya Elluri and Anantaa Kotal and Aritran Piplai and Deepti Gupta and Anupam Joshi},
Title         = {PrivComp-KG : Leveraging Knowledge Graph and Large Language Models for
  Privacy Policy Compliance Verification},
Eprint        = {2404.19744v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Data protection and privacy is becoming increasingly crucial in the digital
era. Numerous companies depend on third-party vendors and service providers to
carry out critical functions within their operations, encompassing tasks such
as data handling and storage. However, this reliance introduces potential
vulnerabilities, as these vendors' security measures and practices may not
always align with the standards expected by regulatory bodies. Businesses are
required, often under the penalty of law, to ensure compliance with the
evolving regulatory rules. Interpreting and implementing these regulations pose
challenges due to their complexity. Regulatory documents are extensive,
demanding significant effort for interpretation, while vendor-drafted privacy
policies often lack the detail required for full legal compliance, leading to
ambiguity. To ensure a concise interpretation of the regulatory requirements
and compliance of organizational privacy policy with said regulations, we
propose a Large Language Model (LLM) and Semantic Web based approach for
privacy compliance. In this paper, we develop the novel Privacy Policy
Compliance Verification Knowledge Graph, PrivComp-KG. It is designed to
efficiently store and retrieve comprehensive information concerning privacy
policies, regulatory frameworks, and domain-specific knowledge pertaining to
the legal landscape of privacy. Using Retrieval Augmented Generation, we
identify the relevant sections in a privacy policy with corresponding
regulatory rules. This information about individual privacy policies is
populated into the PrivComp-KG. Combining this with the domain context and
rules, the PrivComp-KG can be queried to check for compliance with privacy
policies by each vendor against relevant policy regulations. We demonstrate the
relevance of the PrivComp-KG, by verifying compliance of privacy policy
documents for various organizations.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.19744v1},
File          = {2404.19744v1.pdf}
}
@article{2405.13873v2,
Author        = {Yuan Sui and Yufei He and Nian Liu and Xiaoxin He and Kun Wang and Bryan Hooi},
Title         = {FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph
  Question Answering},
Eprint        = {2405.13873v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large language models are often challenged by generating erroneous or
`hallucinated' responses, especially in complex reasoning tasks. To mitigate
this, we propose a retrieval augmented reasoning method, FiDeLiS, which
enhances knowledge graph question answering by anchoring responses to
structured, verifiable reasoning paths. FiDeLiS uses a keyword-enhanced
retrieval mechanism that fetches relevant entities and relations from a
vector-based index of KGs to ensure high-recall retrieval. Once these entities
and relations are retrieved, our method constructs candidate reasoning paths
which are then refined using a stepwise beam search. This ensures that all the
paths we create can be confidently linked back to KGs, ensuring they are
accurate and reliable. A distinctive feature of our approach is its blend of
natural language planning with beam search to optimize the selection of
reasoning paths. Moreover, we redesign the way reasoning paths are scored by
transforming this process into a deductive reasoning task, allowing the LLM to
assess the validity of the paths through deductive reasoning rather than
traditional logit-based scoring. This helps avoid misleading reasoning chains
and reduces unnecessary computational demand. Extensive experiments demonstrate
that our method, even as a training-free method which has lower computational
costs and superior generality, outperforms established strong baselines across
three datasets.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.13873v2},
File          = {2405.13873v2.pdf}
}
@article{2212.05251v2,
Author        = {Ruiqing Ding and Xiao Han and Leye Wang},
Title         = {A Unified Knowledge Graph Augmentation Service for Boosting
  Domain-specific NLP Tasks},
Eprint        = {2212.05251v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {By focusing the pre-training process on domain-specific corpora, some
domain-specific pre-trained language models (PLMs) have achieved
state-of-the-art results. However, it is under-investigated to design a unified
paradigm to inject domain knowledge in the PLM fine-tuning stage. We propose
KnowledgeDA, a unified domain language model development service to enhance the
task-specific training procedure with domain knowledge graphs. Given
domain-specific task texts input, KnowledgeDA can automatically generate a
domain-specific language model following three steps: (i) localize domain
knowledge entities in texts via an embedding-similarity approach; (ii) generate
augmented samples by retrieving replaceable domain entity pairs from two views
of both knowledge graph and training data; (iii) select high-quality augmented
samples for fine-tuning via confidence-based assessment. We implement a
prototype of KnowledgeDA to learn language models for two domains, healthcare
and software development. Experiments on domain-specific text classification
and QA tasks verify the effectiveness and generalizability of KnowledgeDA.},
Year          = {2022},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2212.05251v2},
File          = {2212.05251v2.pdf}
}
@article{2310.02166v1,
Author        = {Mikhail Salnikov and Hai Le and Prateek Rajput and Irina Nikishina and Pavel Braslavski and Valentin Malykh and Alexander Panchenko},
Title         = {Large Language Models Meet Knowledge Graphs to Answer Factoid Questions},
Eprint        = {2310.02166v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recently, it has been shown that the incorporation of structured knowledge
into Large Language Models significantly improves the results for a variety of
NLP tasks. In this paper, we propose a method for exploring pre-trained
Text-to-Text Language Models enriched with additional information from
Knowledge Graphs for answering factoid questions. More specifically, we propose
an algorithm for subgraphs extraction from a Knowledge Graph based on question
entities and answer candidates. Then, we procure easily interpreted information
with Transformer-based models through the linearization of the extracted
subgraphs. Final re-ranking of the answer candidates with the extracted
information boosts Hits@1 scores of the pre-trained text-to-text language
models by 4-6%.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.02166v1},
File          = {2310.02166v1.pdf}
}
@article{2409.15861v2,
Author        = {Abdulfattah Safa and Gözde Gül Şahin},
Title         = {A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding},
Eprint        = {2409.15861v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Dialogue State Tracking (DST) is crucial for understanding user needs and
executing appropriate system actions in task-oriented dialogues. Majority of
existing DST methods are designed to work within predefined ontologies and
assume the availability of gold domain labels, struggling with adapting to new
slots values. While Large Language Models (LLMs)-based systems show promising
zero-shot DST performance, they either require extensive computational
resources or they underperform existing fully-trained systems, limiting their
practicality. To address these limitations, we propose a zero-shot,
open-vocabulary system that integrates domain classification and DST in a
single pipeline. Our approach includes reformulating DST as a
question-answering task for less capable models and employing self-refining
prompts for more adaptable ones. Our system does not rely on fixed slot values
defined in the ontology allowing the system to adapt dynamically. We compare
our approach with existing SOTA, and show that it provides up to 20% better
Joint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1,
with up to 90% fewer requests to the LLM API.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.15861v2},
File          = {2409.15861v2.pdf}
}
@article{2412.00573v2,
Author        = {Théo Fagnoni and Bellinda Mesbah and Mahsun Altin and Phillip Kingston},
Title         = {Opus: A Large Work Model for Complex Workflow Generation},
Eprint        = {2412.00573v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper introduces Opus, a novel framework for generating and optimizing
Workflows tailored to complex Business Process Outsourcing (BPO) use cases,
focusing on cost reduction and quality enhancement while adhering to
established industry processes and operational constraints. Our approach
generates executable Workflows from Intention, defined as the alignment of
Client Input, Client Output, and Process Context. These Workflows are
represented as Directed Acyclic Graphs (DAGs), with nodes as Tasks consisting
of sequences of executable Instructions, including tools and human expert
reviews. We adopt a two-phase methodology: Workflow Generation and Workflow
Optimization. In the Generation phase, Workflows are generated using a Large
Work Model (LWM) informed by a Work Knowledge Graph (WKG) that encodes
domain-specific procedural and operational knowledge. In the Optimization
phase, Workflows are transformed into Workflow Graphs (WFGs), where optimal
Workflows are determined through path optimization. Our experiments demonstrate
that state-of-the-art Large Language Models (LLMs) face challenges in reliably
retrieving detailed process data as well as generating industry-compliant
workflows. The key contributions of this paper include integrating a Work
Knowledge Graph (WKG) into a Large Work Model (LWM) to enable the generation of
context-aware, semantically aligned, structured and auditable Workflows. It
further introduces a two-phase approach that combines Workflow Generation from
Intention with graph-based Workflow Optimization. Finally, we present Opus
Alpha 1 Large and Opus Alpha 1 Small that outperform state-of-the-art LLMs by
38% and 29% respectively in Workflow Generation for a Medical Coding use case.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2412.00573v2},
File          = {2412.00573v2.pdf}
}
@article{2310.07008v1,
Author        = {Mikhail Salnikov and Maria Lysyuk and Pavel Braslavski and Anton Razzhigaev and Valentin Malykh and Alexander Panchenko},
Title         = {Answer Candidate Type Selection: Text-to-Text Language Model for Closed
  Book Question Answering Meets Knowledge Graphs},
Eprint        = {2310.07008v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield
promising results in the Knowledge Graph Question Answering (KGQA) task.
However, the capacity of the models is limited and the quality decreases for
questions with less popular entities. In this paper, we present a novel
approach which works on top of the pre-trained Text-to-Text QA system to
address this issue. Our simple yet effective method performs filtering and
re-ranking of generated candidates based on their types derived from Wikidata
"instance_of" property.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.07008v1},
File          = {2310.07008v1.pdf}
}
@article{2110.07178v2,
Author        = {Peter West and Chandra Bhagavatula and Jack Hessel and Jena D. Hwang and Liwei Jiang and Ronan Le Bras and Ximing Lu and Sean Welleck and Yejin Choi},
Title         = {Symbolic Knowledge Distillation: from General Language Models to
  Commonsense Models},
Eprint        = {2110.07178v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The common practice for training commonsense models has gone
from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in
order to train commonsense models. In this work, we investigate an alternative,
from-machine-to-corpus-to-machine: general language models author these
commonsense knowledge graphs to train commonsense models. Our study leads to a
new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge
Distillation (Hinton et al., 2015), our approach uses larger models to teach
smaller models. A key difference is that we distill knowledge symbolically-as
text-in addition to the neural model. We also distill only one aspect-the
commonsense of a general language model teacher, allowing the student to be a
different type, a commonsense model. Altogether, we show that careful prompt
engineering and a separately trained critic model allow us to selectively
distill high-quality causal commonsense from GPT-3, a general language model.
Empirical results demonstrate that, for the first time, a human-authored
commonsense knowledge graph is surpassed by our automatically distilled variant
in all three criteria: quantity, quality, and diversity. In addition, it
results in a neural commonsense model that surpasses the teacher model's
commonsense capabilities despite its 100x smaller size. We apply this to the
ATOMIC resource, and share our new symbolic knowledge graph and commonsense
models.},
Year          = {2021},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2110.07178v2},
File          = {2110.07178v2.pdf}
}
@article{2312.11713v2,
Author        = {Jared Strader and Nathan Hughes and William Chen and Alberto Speranzon and Luca Carlone},
Title         = {Indoor and Outdoor 3D Scene Graph Generation via Language-Enabled
  Spatial Ontologies},
Eprint        = {2312.11713v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {This paper proposes an approach to build 3D scene graphs in arbitrary indoor
and outdoor environments. Such extension is challenging; the hierarchy of
concepts that describe an outdoor environment is more complex than for indoors,
and manually defining such hierarchy is time-consuming and does not scale.
Furthermore, the lack of training data prevents the straightforward application
of learning-based tools used in indoor settings. To address these challenges,
we propose two novel extensions. First, we develop methods to build a spatial
ontology defining concepts and relations relevant for indoor and outdoor robot
operation. In particular, we use a Large Language Model (LLM) to build such an
ontology, thus largely reducing the amount of manual effort required. Second,
we leverage the spatial ontology for 3D scene graph construction using Logic
Tensor Networks (LTN) to add logical rules, or axioms (e.g., "a beach contains
sand"), which provide additional supervisory signals at training time thus
reducing the need for labelled data, providing better predictions, and even
allowing predicting concepts unseen at training time. We test our approach in a
variety of datasets, including indoor, rural, and coastal environments, and
show that it leads to a significant increase in the quality of the 3D scene
graph generation with sparsely annotated data.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.11713v2},
File          = {2312.11713v2.pdf}
}
@article{2409.14043v1,
Author        = {Pranav Gupta and Raunak Sharma and Rashmi Kumari and Sri Krishna Aditya and Shwetank Choudhary and Sumit Kumar and Kanchana M and Thilagavathy R},
Title         = {ECHO: Environmental Sound Classification with Hierarchical
  Ontology-guided Semi-Supervised Learning},
Eprint        = {2409.14043v1},
DOI           = {10.1109/CONECCT62155.2024.10677303},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SD},
Abstract      = {Environment Sound Classification has been a well-studied research problem in
the field of signal processing and up till now more focus has been laid on
fully supervised approaches. Over the last few years, focus has moved towards
semi-supervised methods which concentrate on the utilization of unlabeled data,
and self-supervised methods which learn the intermediate representation through
pretext task or contrastive learning. However, both approaches require a vast
amount of unlabelled data to improve performance. In this work, we propose a
novel framework called Environmental Sound Classification with Hierarchical
Ontology-guided semi-supervised Learning (ECHO) that utilizes label
ontology-based hierarchy to learn semantic representation by defining a novel
pretext task. In the pretext task, the model tries to predict coarse labels
defined by the Large Language Model (LLM) based on ground truth label ontology.
The trained model is further fine-tuned in a supervised way to predict the
actual task. Our proposed novel semi-supervised framework achieves an accuracy
improvement in the range of 1\% to 8\% over baseline systems across three
datasets namely UrbanSound8K, ESC-10, and ESC-50.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.14043v1},
File          = {2409.14043v1.pdf}
}
@article{2401.07237v3,
Author        = {Somin Wadhwa and Oktie Hassanzadeh and Debarun Bhattacharjya and Ken Barker and Jian Ni},
Title         = {Distilling Event Sequence Knowledge From Large Language Models},
Eprint        = {2401.07237v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Event sequence models have been found to be highly effective in the analysis
and prediction of events. Building such models requires availability of
abundant high-quality event sequence data. In certain applications, however,
clean structured event sequences are not available, and automated sequence
extraction results in data that is too noisy and incomplete. In this work, we
explore the use of Large Language Models (LLMs) to generate event sequences
that can effectively be used for probabilistic event model construction. This
can be viewed as a mechanism of distilling event sequence knowledge from LLMs.
Our approach relies on a Knowledge Graph (KG) of event concepts with partial
causal relations to guide the generative language model for causal event
sequence generation. We show that our approach can generate high-quality event
sequences, filling a knowledge gap in the input KG. Furthermore, we explore how
the generated sequences can be leveraged to discover useful and more complex
structured knowledge from pattern mining and probabilistic event models. We
release our sequence generation code and evaluation framework, as well as
corpus of event sequence data.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.07237v3},
File          = {2401.07237v3.pdf}
}
@article{2410.11235v2,
Author        = {Jiacheng Lin and Kun Qian and Haoyu Han and Nurendra Choudhary and Tianxin Wei and Zhongruo Wang and Sahika Genc and Edward W Huang and Sheng Wang and Karthik Subbian and Danai Koutra and Jimeng Sun},
Title         = {GT2Vec: Large Language Models as Multi-Modal Encoders for Text and
  Graph-Structured Data},
Eprint        = {2410.11235v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Graph-structured information offers rich contextual information that can
enhance language models by providing structured relationships and hierarchies,
leading to more expressive embeddings for various applications such as
retrieval, question answering, and classification. However, existing methods
for integrating graph and text embeddings, often based on Multi-layer
Perceptrons (MLPs) or shallow transformers, are limited in their ability to
fully exploit the heterogeneous nature of these modalities. To overcome this,
we propose GT2Vec, a simple yet effective framework that leverages Large
Language Models (LLMs) to jointly encode text and graph data. Specifically,
GT2Vec employs an MLP adapter to project graph embeddings into the same space
as text embeddings, allowing the LLM to process both modalities jointly. Unlike
prior work, we also introduce contrastive learning to align the graph and text
spaces more effectively, thereby improving the quality of learned joint
embeddings. Empirical results across six datasets spanning three tasks,
knowledge graph-contextualized question answering, graph-text pair
classification, and retrieval, demonstrate that GT2Vec consistently outperforms
existing baselines, achieving significant improvements across multiple
datasets. These results highlight GT2Vec's effectiveness in integrating graph
and text data. Ablation studies further validate the effectiveness of our
method.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.11235v2},
File          = {2410.11235v2.pdf}
}
@article{2310.20170v1,
Author        = {Wenting Zhao and Ye Liu and Tong Niu and Yao Wan and Philip S. Yu and Shafiq Joty and Yingbo Zhou and Semih Yavuz},
Title         = {DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain
  Question Answering over Knowledge Base and Text},
Eprint        = {2310.20170v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have exhibited impressive generation
capabilities, but they suffer from hallucinations when solely relying on their
internal knowledge, especially when answering questions that require less
commonly known information. Retrieval-augmented LLMs have emerged as a
potential solution to ground LLMs in external knowledge. Nonetheless, recent
approaches have primarily emphasized retrieval from unstructured text corpora,
owing to its seamless integration into prompts. When using structured data such
as knowledge graphs, most methods simplify it into natural text, neglecting the
underlying structures. Moreover, a significant gap in the current landscape is
the absence of a realistic benchmark for evaluating the effectiveness of
grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and
text). To fill this gap, we have curated a comprehensive dataset that poses two
unique challenges: (1) Two-hop multi-source questions that require retrieving
information from both open-domain structured and unstructured knowledge
sources; retrieving information from structured knowledge sources is a critical
component in correctly answering the questions. (2) The generation of symbolic
queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another
layer of challenge. Our dataset is created using a combination of automatic
generation through predefined reasoning chains and human annotation. We also
introduce a novel approach that leverages multiple retrieval tools, including
text passage retrieval and symbolic language-assisted retrieval. Our model
outperforms previous approaches by a significant margin, demonstrating its
effectiveness in addressing the above-mentioned reasoning challenges.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.20170v1},
File          = {2310.20170v1.pdf}
}
@article{2409.00861v1,
Author        = {Derian Boer and Fabian Koch and Stefan Kramer},
Title         = {Harnessing the Power of Semi-Structured Knowledge and LLMs with
  Triplet-Based Prefiltering for Question Answering},
Eprint        = {2409.00861v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) frequently lack domain-specific knowledge and
even fine-tuned models tend to hallucinate. Hence, more reliable models that
can include external knowledge are needed. We present a pipeline, 4StepFocus,
and specifically a preprocessing step, that can substantially improve the
answers of LLMs. This is achieved by providing guided access to external
knowledge making use of the model's ability to capture relational context and
conduct rudimentary reasoning by themselves. The method narrows down
potentially correct answers by triplets-based searches in a semi-structured
knowledge base in a direct, traceable fashion, before switching to latent
representations for ranking those candidates based on unstructured data. This
distinguishes it from related methods that are purely based on latent
representations. 4StepFocus consists of the steps: 1) Triplet generation for
extraction of relational data by an LLM, 2) substitution of variables in those
triplets to narrow down answer candidates employing a knowledge graph, 3)
sorting remaining candidates with a vector similarity search involving
associated non-structured data, 4) reranking the best candidates by the LLM
with background data provided. Experiments on a medical, a product
recommendation, and an academic paper search test set demonstrate that this
approach is indeed a powerful augmentation. It not only adds relevant traceable
background information from information retrieval, but also improves
performance considerably in comparison to state-of-the-art methods. This paper
presents a novel, largely unexplored direction and therefore provides a wide
range of future work opportunities. Used source code is available at
https://github.com/kramerlab/4StepFocus.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.00861v1},
File          = {2409.00861v1.pdf}
}
@article{2410.16708v1,
Author        = {Zhichao Yan and Jiapu Wang and Jiaoyan Chen and Xiaoli Li and Ru Li and Jeff Z. Pan},
Title         = {Atomic Fact Decomposition Helps Attributed Question Answering},
Eprint        = {2410.16708v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Attributed Question Answering (AQA) aims to provide both a trustworthy answer
and a reliable attribution report for a given question. Retrieval is a widely
adopted approach, including two general paradigms: Retrieval-Then-Read (RTR)
and post-hoc retrieval. Recently, Large Language Models (LLMs) have shown
remarkable proficiency, prompting growing interest in AQA among researchers.
However, RTR-based AQA often suffers from irrelevant knowledge and rapidly
changing information, even when LLMs are adopted, while post-hoc
retrieval-based AQA struggles with comprehending long-form answers with complex
logic, and precisely identifying the content needing revision and preserving
the original intent. To tackle these problems, this paper proposes an Atomic
fact decomposition-based Retrieval and Editing (ARE) framework, which
decomposes the generated long-form answers into molecular clauses and atomic
facts by the instruction-tuned LLMs. Notably, the instruction-tuned LLMs are
fine-tuned using a well-constructed dataset, generated from large scale
Knowledge Graphs (KGs). This process involves extracting one-hop neighbors from
a given set of entities and transforming the result into coherent long-form
text. Subsequently, ARE leverages a search engine to retrieve evidences related
to atomic facts, inputting these evidences into an LLM-based verifier to
determine whether the facts require expansion for re-retrieval or editing.
Furthermore, the edited facts are backtracked into the original answer, with
evidence aggregated based on the relationship between molecular clauses and
atomic facts. Extensive evaluations demonstrate the superior performance of our
proposed method over the state-of-the-arts on several datasets, with an
additionally proposed new metric $Attr_{p}$ for evaluating the precision of
evidence attribution.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.16708v1},
File          = {2410.16708v1.pdf}
}
@article{2411.12449v2,
Author        = {Sneha Singhania and Silviu Cucerzan and Allen Herring and Sujay Kumar Jauhar},
Title         = {Neon: News Entity-Interaction Extraction for Enhanced Question Answering},
Eprint        = {2411.12449v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Capturing fresh information in near real-time and using it to augment
existing large language models (LLMs) is essential to generate up-to-date,
grounded, and reliable output. This problem becomes particularly challenging
when LLMs are used for informational tasks in rapidly evolving fields, such as
Web search related to recent or unfolding events involving entities, where
generating temporally relevant responses requires access to up-to-the-hour news
sources. However, the information modeled by the parametric memory of LLMs is
often outdated, and Web results from prototypical retrieval systems may fail to
capture the latest relevant information and struggle to handle conflicting
reports in evolving news. To address this challenge, we present the NEON
framework, designed to extract emerging entity interactions -- such as events
or activities -- as described in news articles. NEON constructs an
entity-centric timestamped knowledge graph that captures such interactions,
thereby facilitating enhanced QA capabilities related to news events. Our
framework innovates by integrating open Information Extraction (openIE) style
tuples into LLMs to enable in-context retrieval-augmented generation. This
integration demonstrates substantial improvements in QA performance when
tackling temporal, entity-centric search queries. Through NEON, LLMs can
deliver more accurate, reliable, and up-to-date responses.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.12449v2},
File          = {2411.12449v2.pdf}
}
@article{2009.05812v1,
Author        = {Ashutosh Tiwari and Sandeep Varma},
Title         = {Learning semantic Image attributes using Image recognition and knowledge
  graph embeddings},
Eprint        = {2009.05812v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Extracting structured knowledge from texts has traditionally been used for
knowledge base generation. However, other sources of information, such as
images can be leveraged into this process to build more complete and richer
knowledge bases. Structured semantic representation of the content of an image
and knowledge graph embeddings can provide a unique representation of semantic
relationships between image entities. Linking known entities in knowledge
graphs and learning open-world images using language models has attracted lots
of interest over the years. In this paper, we propose a shared learning
approach to learn semantic attributes of images by combining a knowledge graph
embedding model with the recognized attributes of images. The proposed model
premises to help us understand the semantic relationship between the entities
of an image and implicitly provide a link for the extracted entities through a
knowledge graph embedding model. Under the limitation of using a custom
user-defined knowledge base with limited data, the proposed model presents
significant accuracy and provides a new alternative to the earlier approaches.
The proposed approach is a step towards bridging the gap between frameworks
which learn from large amounts of data and frameworks which use a limited set
of predicates to infer new knowledge.},
Year          = {2020},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2009.05812v1},
File          = {2009.05812v1.pdf}
}
@article{2409.10294v2,
Author        = {Shanshan Wang and Chun Zhang and Ning Zhang},
Title         = {MGSA: Multi-Granularity Graph Structure Attention for Knowledge
  Graph-to-Text Generation},
Eprint        = {2409.10294v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The Knowledge Graph-to-Text Generation task aims to convert structured
knowledge graphs into coherent and human-readable natural language text. Recent
efforts in this field have focused on enhancing pre-trained language models
(PLMs) by incorporating graph structure information to capture the intricate
structure details of knowledge graphs. However, most of these approaches tend
to capture only single-granularity structure information, concentrating either
on the relationships between entities within the original graph or on the
relationships between words within the same entity or across different
entities. This narrow focus results in a significant limitation: models that
concentrate solely on entity-level structure fail to capture the nuanced
semantic relationships between words, while those that focus only on word-level
structure overlook the broader relationships between original entire entities.
To overcome these limitations, this paper introduces the Multi-granularity
Graph Structure Attention (MGSA), which is based on PLMs. The encoder of the
model architecture features an entity-level structure encoding module, a
word-level structure encoding module, and an aggregation module that
synthesizes information from both structure. This multi-granularity structure
encoding approach allows the model to simultaneously capture both entity-level
and word-level structure information, providing a more comprehensive
understanding of the knowledge graph's structure information, thereby
significantly improving the quality of the generated text. We conducted
extensive evaluations of the MGSA model using two widely recognized KG-to-Text
Generation benchmark datasets, WebNLG and EventNarrative, where it consistently
outperformed models that rely solely on single-granularity structure
information, demonstrating the effectiveness of our approach.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.10294v2},
File          = {2409.10294v2.pdf}
}
@article{2501.15688v1,
Author        = {Haodi Ma and Dzmitry Kasinets and Daisy Zhe Wang},
Title         = {Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware
  Contexts},
Eprint        = {2501.15688v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Multimodal knowledge graph completion (MMKGC) aims to predict missing links
in multimodal knowledge graphs (MMKGs) by leveraging information from various
modalities alongside structural data. Existing MMKGC approaches primarily
extend traditional knowledge graph embedding (KGE) models, which often require
creating an embedding for every entity. This results in large model sizes and
inefficiencies in integrating multimodal information, particularly for
real-world graphs. Meanwhile, Transformer-based models have demonstrated
competitive performance in knowledge graph completion (KGC). However, their
focus on single-modal knowledge limits their capacity to utilize cross-modal
information. Recently, Large vision-language models (VLMs) have shown potential
in cross-modal tasks but are constrained by the high cost of training. In this
work, we propose a novel approach that integrates Transformer-based KGE models
with cross-modal context generated by pre-trained VLMs, thereby extending their
applicability to MMKGC. Specifically, we employ a pre-trained VLM to transform
relevant visual information from entities and their neighbors into textual
sequences. We then frame KGC as a sequence-to-sequence task, fine-tuning the
model with the generated cross-modal context. This simple yet effective method
significantly reduces model size compared to traditional KGE approaches while
achieving competitive performance across multiple large-scale datasets with
minimal hyperparameter tuning.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.15688v1},
File          = {2501.15688v1.pdf}
}
@article{2311.07914v2,
Author        = {Garima Agrawal and Tharindu Kumarage and Zeyad Alghamdi and Huan Liu},
Title         = {Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey},
Eprint        = {2311.07914v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The contemporary LLMs are prone to producing hallucinations, stemming mainly
from the knowledge gaps within the models. To address this critical limitation,
researchers employ diverse strategies to augment the LLMs by incorporating
external knowledge, aiming to reduce hallucinations and enhance reasoning
accuracy. Among these strategies, leveraging knowledge graphs as a source of
external information has demonstrated promising results. In this survey, we
comprehensively review these knowledge-graph-based augmentation techniques in
LLMs, focusing on their efficacy in mitigating hallucinations. We
systematically categorize these methods into three overarching groups, offering
methodological comparisons and performance evaluations. Lastly, this survey
explores the current trends and challenges associated with these techniques and
outlines potential avenues for future research in this emerging field.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.07914v2},
File          = {2311.07914v2.pdf}
}
@article{2012.04808v3,
Author        = {Yichong Xu and Chenguang Zhu and Ruochen Xu and Yang Liu and Michael Zeng and Xuedong Huang},
Title         = {Fusing Context Into Knowledge Graph for Commonsense Question Answering},
Eprint        = {2012.04808v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Commonsense question answering (QA) requires a model to grasp commonsense and
factual knowledge to answer questions about world events. Many prior methods
couple language modeling with knowledge graphs (KG). However, although a KG
contains rich structural information, it lacks the context to provide a more
precise understanding of the concepts. This creates a gap when fusing knowledge
graphs into language modeling, especially when there is insufficient labeled
data. Thus, we propose to employ external entity descriptions to provide
contextual information for knowledge understanding. We retrieve descriptions of
related concepts from Wiktionary and feed them as additional input to
pre-trained language models. The resulting model achieves state-of-the-art
result in the CommonsenseQA dataset and the best result among non-generative
models in OpenBookQA.},
Year          = {2020},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2012.04808v3},
File          = {2012.04808v3.pdf}
}
@article{2210.00305v3,
Author        = {Xin Xie and Zhoubo Li and Xiaohan Wang and Zekun Xi and Ningyu Zhang},
Title         = {LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph
  Embeddings},
Eprint        = {2210.00305v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph
structure and text-rich entity/relation information. Text-based KG embeddings
can represent entities by encoding descriptions with pre-trained language
models, but no open-sourced library is specifically designed for KGs with PLMs
at present. In this paper, we present LambdaKG, a library for KGE that equips
with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and
supports various tasks (e.g., knowledge graph completion, question answering,
recommendation, and knowledge probing). LambdaKG is publicly open-sourced at
https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at
http://deepke.zjukg.cn/lambdakg.mp4 and long-term maintenance.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.00305v3},
File          = {2210.00305v3.pdf}
}
@article{2211.07065v1,
Author        = {Byeongmin Choi and YongHyun Lee and Yeunwoong Kyung and Eunchan Kim},
Title         = {ALBERT with Knowledge Graph Encoder Utilizing Semantic Similarity for
  Commonsense Question Answering},
Eprint        = {2211.07065v1},
DOI           = {10.32604/iasc.2023.032783},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recently, pre-trained language representation models such as bidirectional
encoder representations from transformers (BERT) have been performing well in
commonsense question answering (CSQA). However, there is a problem that the
models do not directly use explicit information of knowledge sources existing
outside. To augment this, additional methods such as knowledge-aware graph
network (KagNet) and multi-hop graph relation network (MHGRN) have been
proposed. In this study, we propose to use the latest pre-trained language
model a lite bidirectional encoder representations from transformers (ALBERT)
with knowledge graph information extraction technique. We also propose to
applying the novel method, schema graph expansion to recent language models.
Then, we analyze the effect of applying knowledge graph-based knowledge
extraction techniques to recent pre-trained language models and confirm that
schema graph expansion is effective in some extent. Furthermore, we show that
our proposed model can achieve better performance than existing KagNet and
MHGRN models in CommonsenseQA dataset.},
Year          = {2022},
Month         = {Nov},
Note          = {Intelligent Automation & Soft Computing, vol. 36, no.1, pp. 71-82,
  2023},
Url           = {http://arxiv.org/abs/2211.07065v1},
File          = {2211.07065v1.pdf}
}
@article{2211.08380v1,
Author        = {Ziniu Hu and Yichong Xu and Wenhao Yu and Shuohang Wang and Ziyi Yang and Chenguang Zhu and Kai-Wei Chang and Yizhou Sun},
Title         = {Empowering Language Models with Knowledge Graph Reasoning for Question
  Answering},
Eprint        = {2211.08380v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Answering open-domain questions requires world knowledge about in-context
entities. As pre-trained Language Models (LMs) lack the power to store all
required knowledge, external knowledge sources, such as knowledge graphs, are
often used to augment LMs. In this work, we propose knOwledge REasOning
empowered Language Model (OREO-LM), which consists of a novel Knowledge
Interaction Layer that can be flexibly plugged into existing Transformer-based
LMs to interact with a differentiable Knowledge Graph Reasoning module
collaboratively. In this way, LM guides KG to walk towards the desired answer,
while the retrieved knowledge improves LM. By adopting OREO-LM to RoBERTa and
T5, we show significant performance gain, achieving state-of-art results in the
Closed-Book setting. The performance enhancement is mainly from the KG
reasoning's capacity to infer missing relational facts. In addition, OREO-LM
provides reasoning paths as rationales to interpret the model's decision.},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.08380v1},
File          = {2211.08380v1.pdf}
}
@article{2311.01326v1,
Author        = {Alla Chepurova and Aydar Bulatov and Yuri Kuratov and Mikhail Burtsev},
Title         = {Better Together: Enhancing Generative Knowledge Graph Completion with
  Language Models and Neighborhood Information},
Eprint        = {2311.01326v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Real-world Knowledge Graphs (KGs) often suffer from incompleteness, which
limits their potential performance. Knowledge Graph Completion (KGC) techniques
aim to address this issue. However, traditional KGC methods are computationally
intensive and impractical for large-scale KGs, necessitating the learning of
dense node embeddings and computing pairwise distances. Generative
transformer-based language models (e.g., T5 and recent KGT5) offer a promising
solution as they can predict the tail nodes directly. In this study, we propose
to include node neighborhoods as additional information to improve KGC methods
based on language models. We examine the effects of this imputation and show
that, on both inductive and transductive Wikidata subsets, our method
outperforms KGT5 and conventional KGC approaches. We also provide an extensive
analysis of the impact of neighborhood on model prediction and show its
importance. Furthermore, we point the way to significantly improve KGC through
more effective neighborhood selection.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.01326v1},
File          = {2311.01326v1.pdf}
}
@article{2406.18085v1,
Author        = {Ran Song and Shizhu He and Shengxiang Gao and Li Cai and Kang Liu and Zhengtao Yu and Jun Zhao},
Title         = {Multilingual Knowledge Graph Completion from Pretrained Language Models
  with Knowledge Constraints},
Eprint        = {2406.18085v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Multilingual Knowledge Graph Completion (mKGC) aim at solving queries like
(h, r, ?) in different languages by reasoning a tail entity t thus improving
multilingual knowledge graphs. Previous studies leverage multilingual
pretrained language models (PLMs) and the generative paradigm to achieve mKGC.
Although multilingual pretrained language models contain extensive knowledge of
different languages, its pretraining tasks cannot be directly aligned with the
mKGC tasks. Moreover, the majority of KGs and PLMs currently available exhibit
a pronounced English-centric bias. This makes it difficult for mKGC to achieve
good results, particularly in the context of low-resource languages. To
overcome previous problems, this paper introduces global and local knowledge
constraints for mKGC. The former is used to constrain the reasoning of answer
entities, while the latter is used to enhance the representation of query
contexts. The proposed method makes the pretrained model better adapt to the
mKGC task. Experimental results on public datasets demonstrate that our method
outperforms the previous SOTA on Hits@1 and Hits@10 by an average of 12.32% and
16.03%, which indicates that our proposed method has significant enhancement on
mKGC.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.18085v1},
File          = {2406.18085v1.pdf}
}
@article{2305.15002v2,
Author        = {Asahi Ushio and Jose Camacho Collados and Steven Schockaert},
Title         = {A RelEntLess Benchmark for Modelling Graded Relations between Named
  Entities},
Eprint        = {2305.15002v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Relations such as "is influenced by", "is known for" or "is a competitor of"
are inherently graded: we can rank entity pairs based on how well they satisfy
these relations, but it is hard to draw a line between those pairs that satisfy
them and those that do not. Such graded relations play a central role in many
applications, yet they are typically not covered by existing Knowledge Graphs.
In this paper, we consider the possibility of using Large Language Models
(LLMs) to fill this gap. To this end, we introduce a new benchmark, in which
entity pairs have to be ranked according to how much they satisfy a given
graded relation. The task is formulated as a few-shot ranking problem, where
models only have access to a description of the relation and five prototypical
instances. We use the proposed benchmark to evaluate state-of-the-art relation
embedding strategies as well as several recent LLMs, covering both publicly
available LLMs and closed models such as GPT-4. Overall, we find a strong
correlation between model size and performance, with smaller Language Models
struggling to outperform a naive baseline. The results of the largest Flan-T5
and OPT models are remarkably strong, although a clear gap with human
performance remains.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.15002v2},
File          = {2305.15002v2.pdf}
}
@article{2311.01266v1,
Author        = {Qing Huang and Yanbang Sun and Zhenchang Xing and Yuanlong Cao and Jieshan Chen and Xiwei Xu and Huan Jin and Jiaxing Lu},
Title         = {Let's Discover More API Relations: A Large Language Model-based AI Chain
  for Unsupervised API Relation Inference},
Eprint        = {2311.01266v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {APIs have intricate relations that can be described in text and represented
as knowledge graphs to aid software engineering tasks. Existing relation
extraction methods have limitations, such as limited API text corpus and
affected by the characteristics of the input text.To address these limitations,
we propose utilizing large language models (LLMs) (e.g., GPT-3.5) as a neural
knowledge base for API relation inference. This approach leverages the entire
Web used to pre-train LLMs as a knowledge base and is insensitive to the
context and complexity of input texts. To ensure accurate inference, we design
our analytic flow as an AI Chain with three AI modules: API FQN Parser, API
Knowledge Extractor, and API Relation Decider. The accuracy of the API FQN
parser and API Relation Decider module are 0.81 and 0.83, respectively. Using
the generative capacity of the LLM and our approach's inference capability, we
achieve an average F1 value of 0.76 under the three datasets, significantly
higher than the state-of-the-art method's average F1 value of 0.40. Compared to
CoT-based method, our AI Chain design improves the inference reliability by
67%, and the AI-crowd-intelligence strategy enhances the robustness of our
approach by 26%.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.01266v1},
File          = {2311.01266v1.pdf}
}
@article{2403.18348v1,
Author        = {Shenghao Yang and Weizhi Ma and Peijie Sun and Qingyao Ai and Yiqun Liu and Mingchen Cai and Min Zhang},
Title         = {Sequential Recommendation with Latent Relations based on Large Language
  Model},
Eprint        = {2403.18348v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Sequential recommender systems predict items that may interest users by
modeling their preferences based on historical interactions. Traditional
sequential recommendation methods rely on capturing implicit collaborative
filtering signals among items. Recent relation-aware sequential recommendation
models have achieved promising performance by explicitly incorporating item
relations into the modeling of user historical sequences, where most relations
are extracted from knowledge graphs. However, existing methods rely on manually
predefined relations and suffer the sparsity issue, limiting the generalization
ability in diverse scenarios with varied item relations. In this paper, we
propose a novel relation-aware sequential recommendation framework with Latent
Relation Discovery (LRD). Different from previous relation-aware models that
rely on predefined rules, we propose to leverage the Large Language Model (LLM)
to provide new types of relations and connections between items. The motivation
is that LLM contains abundant world knowledge, which can be adopted to mine
latent relations of items for recommendation. Specifically, inspired by that
humans can describe relations between items using natural language, LRD
harnesses the LLM that has demonstrated human-like knowledge to obtain language
knowledge representations of items. These representations are fed into a latent
relation discovery module based on the discrete state variational autoencoder
(DVAE). Then the self-supervised relation discovery tasks and recommendation
tasks are jointly optimized. Experimental results on multiple public datasets
demonstrate our proposed latent relations discovery method can be incorporated
with existing relation-aware sequential recommendation models and significantly
improve the performance. Further analysis experiments indicate the
effectiveness and reliability of the discovered latent relations.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.18348v1},
File          = {2403.18348v1.pdf}
}
@article{2404.00589v2,
Author        = {Zhenyu Qian and Yiming Qian and Yuting Song and Fei Gao and Hai Jin and Chen Yu and Xia Xie},
Title         = {Harnessing the Power of Large Language Model for Uncertainty Aware Graph
  Processing},
Eprint        = {2404.00589v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Handling graph data is one of the most difficult tasks. Traditional
techniques, such as those based on geometry and matrix factorization, rely on
assumptions about the data relations that become inadequate when handling large
and complex graph data. On the other hand, deep learning approaches demonstrate
promising results in handling large graph data, but they often fall short of
providing interpretable explanations. To equip the graph processing with both
high accuracy and explainability, we introduce a novel approach that harnesses
the power of a large language model (LLM), enhanced by an uncertainty-aware
module to provide a confidence score on the generated answer. We experiment
with our approach on two graph processing tasks: few-shot knowledge graph
completion and graph classification. Our results demonstrate that through
parameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms
by a substantial margin across ten diverse benchmark datasets. Moreover, to
address the challenge of explainability, we propose an uncertainty estimation
based on perturbation, along with a calibration scheme to quantify the
confidence scores of the generated answers. Our confidence measure achieves an
AUC of 0.8 or higher on seven out of the ten datasets in predicting the
correctness of the answer generated by LLM.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2404.00589v2},
File          = {2404.00589v2.pdf}
}
@article{2404.13865v1,
Author        = {Avinash Anand and Kritarth Prasad and Ujjwal Goel and Mohit Gupta and Naman Lal and Astha Verma and Rajiv Ratn Shah},
Title         = {Context-Enhanced Language Models for Generating Multi-Paper Citations},
Eprint        = {2404.13865v1},
DOI           = {10.1007/978-3-031-49601-1_6},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Citation text plays a pivotal role in elucidating the connection between
scientific documents, demanding an in-depth comprehension of the cited paper.
Constructing citations is often time-consuming, requiring researchers to delve
into extensive literature and grapple with articulating relevant content. To
address this challenge, the field of citation text generation (CTG) has
emerged. However, while earlier methods have primarily centered on creating
single-sentence citations, practical scenarios frequently necessitate citing
multiple papers within a single paragraph. To bridge this gap, we propose a
method that leverages Large Language Models (LLMs) to generate multi-citation
sentences. Our approach involves a single source paper and a collection of
target papers, culminating in a coherent paragraph containing multi-sentence
citation text. Furthermore, we introduce a curated dataset named MCG-S2ORC,
composed of English-language academic research papers in Computer Science,
showcasing multiple citation instances. In our experiments, we evaluate three
LLMs LLaMA, Alpaca, and Vicuna to ascertain the most effective model for this
endeavor. Additionally, we exhibit enhanced performance by integrating
knowledge graphs from target papers into the prompts for generating citation
text. This research underscores the potential of harnessing LLMs for citation
generation, opening a compelling avenue for exploring the intricate connections
between scientific documents.},
Year          = {2024},
Month         = {Apr},
Note          = {Big Data and Artificial Intelligence 2023, Delhi, India, December
  7, 80 94},
Url           = {http://arxiv.org/abs/2404.13865v1},
File          = {2404.13865v1.pdf}
}
@article{2410.13765v2,
Author        = {Yu Xia and Junda Wu and Sungchul Kim and Tong Yu and Ryan A. Rossi and Haoliang Wang and Julian McAuley},
Title         = {Knowledge-Aware Query Expansion with Large Language Models for Textual
  and Relational Retrieval},
Eprint        = {2410.13765v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have been used to generate query expansions
augmenting original queries for improving information search. Recent studies
also explore providing LLMs with initial retrieval results to generate query
expansions more grounded to document corpus. However, these methods mostly
focus on enhancing textual similarities between search queries and target
documents, overlooking document relations. For queries like "Find me a highly
rated camera for wildlife photography compatible with my Nikon F-Mount lenses",
existing methods may generate expansions that are semantically similar but
structurally unrelated to user intents. To handle such semi-structured queries
with both textual and relational requirements, in this paper we propose a
knowledge-aware query expansion framework, augmenting LLMs with structured
document relations from knowledge graph (KG). To further address the limitation
of entity-based scoring in existing KG-based methods, we leverage document
texts as rich KG node representations and use document-based relation filtering
for our Knowledge-Aware Retrieval (KAR). Extensive experiments on three
datasets of diverse domains show the advantages of our method compared against
state-of-the-art baselines on textual and relational semi-structured retrieval.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.13765v2},
File          = {2410.13765v2.pdf}
}
@article{2501.00031v1,
Author        = {Karthik S. Vedula and Annika Gupta and Akshay Swaminathan and Ivan Lopez and Suhana Bedi and Nigam H. Shah},
Title         = {Distilling Large Language Models for Efficient Clinical Information
  Extraction},
Eprint        = {2501.00031v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) excel at clinical information extraction but
their computational demands limit practical deployment. Knowledge
distillation--the process of transferring knowledge from larger to smaller
models--offers a potential solution. We evaluate the performance of distilled
BERT models, which are approximately 1,000 times smaller than modern LLMs, for
clinical named entity recognition (NER) tasks. We leveraged state-of-the-art
LLMs (Gemini and OpenAI models) and medical ontologies (RxNorm and SNOMED) as
teacher labelers for medication, disease, and symptom extraction. We applied
our approach to over 3,300 clinical notes spanning five publicly available
datasets, comparing distilled BERT models against both their teacher labelers
and BERT models fine-tuned on human labels. External validation was conducted
using clinical notes from the MedAlign dataset. For disease extraction, F1
scores were 0.82 (teacher model), 0.89 (BioBERT trained on human labels), and
0.84 (BioBERT-distilled). For medication, F1 scores were 0.84 (teacher model),
0.91 (BioBERT-human), and 0.87 (BioBERT-distilled). For symptoms: F1 score of
0.73 (teacher model) and 0.68 (BioBERT-distilled). Distilled BERT models had
faster inference (12x, 4x, 8x faster than GPT-4o, o1-mini, and Gemini Flash
respectively) and lower costs (85x, 101x, 2x cheaper than GPT-4o, o1-mini, and
Gemini Flash respectively). On the external validation dataset, the distilled
BERT model achieved F1 scores of 0.883 (medication), 0.726 (disease), and 0.699
(symptom). Distilled BERT models were up to 101x cheaper and 12x faster than
state-of-the-art LLMs while achieving similar performance on NER tasks.
Distillation offers a computationally efficient and scalable alternative to
large LLMs for clinical information extraction.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2501.00031v1},
File          = {2501.00031v1.pdf}
}
@article{2501.18794v1,
Author        = {Matthew Neeley and Guantong Qi and Guanchu Wang and Ruixiang Tang and Dongxue Mao and Chaozhong Liu and Sasidhar Pasupuleti and Bo Yuan and Fan Xia and Pengfei Liu and Zhandong Liu and Xia Hu},
Title         = {Survey and Improvement Strategies for Gene Prioritization with Large
  Language Models},
Eprint        = {2501.18794v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.GN},
Abstract      = {Rare diseases are challenging to diagnose due to limited patient data and
genetic diversity. Despite advances in variant prioritization, many cases
remain undiagnosed. While large language models (LLMs) have performed well in
medical exams, their effectiveness in diagnosing rare genetic diseases has not
been assessed. To identify causal genes, we benchmarked various LLMs for gene
prioritization. Using multi-agent and Human Phenotype Ontology (HPO)
classification, we categorized patients based on phenotypes and solvability
levels. As gene set size increased, LLM performance deteriorated, so we used a
divide-and-conquer strategy to break the task into smaller subsets. At
baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking
causal genes correctly. The multi-agent and HPO approaches helped distinguish
confidently solved cases from challenging ones, highlighting the importance of
known gene-phenotype associations and phenotype specificity. We found that
cases with specific phenotypes or clear associations were more accurately
solved. However, we observed biases toward well-studied genes and input order
sensitivity, which hindered gene prioritization. Our divide-and-conquer
strategy improved accuracy by overcoming these biases. By utilizing HPO
classification, novel multi-agent techniques, and our LLM strategy, we improved
causal gene identification accuracy compared to our baseline evaluation. This
approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved
cases, and accelerates gene discovery, supporting the development of targeted
diagnostics and therapies.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.18794v1},
File          = {2501.18794v1.pdf}
}
@article{2410.18489v1,
Author        = {Ahmed R. Sadik and Sebastian Brulin and Markus Olhofer and Antonello Ceravola and Frank Joublin},
Title         = {LLM as a code generator in Agile Model Driven Development},
Eprint        = {2410.18489v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Leveraging Large Language Models (LLM) like GPT4 in the auto generation of
code represents a significant advancement, yet it is not without its
challenges. The ambiguity inherent in natural language descriptions of software
poses substantial obstacles to generating deployable, structured artifacts.
This research champions Model Driven Development (MDD) as a viable strategy to
overcome these challenges, proposing an Agile Model Driven Development (AMDD)
approach that employs GPT4 as a code generator. This approach enhances the
flexibility and scalability of the code auto generation process and offers
agility that allows seamless adaptation to changes in models or deployment
environments. We illustrate this by modeling a multi agent Unmanned Vehicle
Fleet (UVF) system using the Unified Modeling Language (UML), significantly
reducing model ambiguity by integrating the Object Constraint Language (OCL)
for code structure meta modeling, and the FIPA ontology language for
communication semantics meta modeling. Applying GPT4 auto generation
capabilities yields Java and Python code that is compatible with the JADE and
PADE frameworks, respectively. Our thorough evaluation of the auto generated
code verifies its alignment with expected behaviors and identifies enhancements
in agent interactions. Structurally, we assessed the complexity of code derived
from a model constrained solely by OCL meta models, against that influenced by
both OCL and FIPA ontology meta models. The results indicate that the ontology
constrained meta model produces inherently more complex code, yet its
cyclomatic complexity remains within manageable levels, suggesting that
additional meta model constraints can be incorporated without exceeding the
high risk threshold for complexity.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.18489v1},
File          = {2410.18489v1.pdf}
}
@article{2303.01580v2,
Author        = {Derek Chen and Celine Lee and Yunan Lu and Domenic Rosati and Zhou Yu},
Title         = {Mixture of Soft Prompts for Controllable Data Generation},
Eprint        = {2303.01580v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) effectively generate fluent text when the target
output follows natural language patterns. However, structured prediction tasks
confine the output format to a limited ontology, causing even very large models
to struggle since they were never trained with such restrictions in mind. The
difficulty of using LLMs for direct prediction is exacerbated in few-shot
learning scenarios, which commonly arise due to domain shift and resource
limitations. We flip the problem on its head by leveraging the LLM as a tool
for data augmentation rather than direct prediction. Our proposed Mixture of
Soft Prompts (MSP) serves as a parameter-efficient procedure for generating
data in a controlled manner. Denoising mechanisms are further applied to
improve the quality of synthesized data. Automatic metrics show our method is
capable of producing diverse and natural text, while preserving label
semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks
when compared against strong baselines. Our method offers an alternate
data-centric approach for applying LLMs to complex prediction tasks.},
Year          = {2023},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2303.01580v2},
File          = {2303.01580v2.pdf}
}
@article{2312.16378v1,
Author        = {Sanjay Oruganti and Sergei Nirenburg and Jesse English and Marjorie McShane},
Title         = {Automating Knowledge Acquisition for Content-Centric Cognitive Agents
  Using LLMs},
Eprint        = {2312.16378v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The paper describes a system that uses large language model (LLM) technology
to support the automatic learning of new entries in an intelligent agent's
semantic lexicon. The process is bootstrapped by an existing non-toy lexicon
and a natural language generator that converts formal, ontologically-grounded
representations of meaning into natural language sentences. The learning method
involves a sequence of LLM requests and includes an automatic quality control
step. To date, this learning method has been applied to learning multiword
expressions whose meanings are equivalent to those of transitive verbs in the
agent's lexicon. The experiment demonstrates the benefits of a hybrid learning
architecture that integrates knowledge-based methods and resources with both
traditional data analytics and LLMs.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.16378v1},
File          = {2312.16378v1.pdf}
}
@article{2410.20695v2,
Author        = {Gal Beeri and Benoit Chamot and Elena Latchem and Shruthi Venkatesh and Sarah Whalan and Van Zyl Kruger and David Martino},
Title         = {Combining Domain-Specific Models and LLMs for Automated Disease
  Phenotyping from Survey Data},
Eprint        = {2410.20695v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This exploratory pilot study investigated the potential of combining a
domain-specific model, BERN2, with large language models (LLMs) to enhance
automated disease phenotyping from research survey data. Motivated by the need
for efficient and accurate methods to harmonize the growing volume of survey
data with standardized disease ontologies, we employed BERN2, a biomedical
named entity recognition and normalization model, to extract disease
information from the ORIGINS birth cohort survey data. After rigorously
evaluating BERN2's performance against a manually curated ground truth dataset,
we integrated various LLMs using prompt engineering, Retrieval-Augmented
Generation (RAG), and Instructional Fine-Tuning (IFT) to refine the model's
outputs. BERN2 demonstrated high performance in extracting and normalizing
disease mentions, and the integration of LLMs, particularly with Few Shot
Inference and RAG orchestration, further improved accuracy. This approach,
especially when incorporating structured examples, logical reasoning prompts,
and detailed context, offers a promising avenue for developing tools to enable
efficient cohort profiling and data harmonization across large, heterogeneous
research datasets.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.20695v2},
File          = {2410.20695v2.pdf}
}
@article{2405.00449v1,
Author        = {Mohamed Manzour Hussien and Angie Nataly Melo and Augusto Luis Ballardini and Carlota Salinas Maldonado and Rubén Izquierdo and Miguel Ángel Sotelo},
Title         = {RAG-based Explainable Prediction of Road Users Behaviors for Automated
  Driving using Knowledge Graphs and Large Language Models},
Eprint        = {2405.00449v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Prediction of road users' behaviors in the context of autonomous driving has
gained considerable attention by the scientific community in the last years.
Most works focus on predicting behaviors based on kinematic information alone,
a simplification of the reality since road users are humans, and as such they
are highly influenced by their surrounding context. In addition, a large
plethora of research works rely on powerful Deep Learning techniques, which
exhibit high performance metrics in prediction tasks but may lack the ability
to fully understand and exploit the contextual semantic information contained
in the road scene, not to mention their inability to provide explainable
predictions that can be understood by humans. In this work, we propose an
explainable road users' behavior prediction system that integrates the
reasoning abilities of Knowledge Graphs (KG) and the expressiveness
capabilities of Large Language Models (LLM) by using Retrieval Augmented
Generation (RAG) techniques. For that purpose, Knowledge Graph Embeddings (KGE)
and Bayesian inference are combined to allow the deployment of a fully
inductive reasoning system that enables the issuing of predictions that rely on
legacy information contained in the graph as well as on current evidence
gathered in real time by onboard sensors. Two use cases have been implemented
following the proposed approach: 1) Prediction of pedestrians' crossing
actions; 2) Prediction of lane change maneuvers. In both cases, the performance
attained surpasses the current state of the art in terms of anticipation and
F1-score, showing a promising avenue for future research in this field.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.00449v1},
File          = {2405.00449v1.pdf}
}
@article{2406.03855v3,
Author        = {Eden Avnat and Michal Levy and Daniel Herstain and Elia Yanko and Daniel Ben Joya and Michal Tzuchman Katz and Dafna Eshel and Sahar Laros and Yael Dagan and Shahar Barami and Joseph Mermelstein and Shahar Ovadia and Noam Shomron and Varda Shalev and Raja-Elie E. Abdulnour},
Title         = {Performance of large language models in numerical vs. semantic medical
  knowledge: Benchmarking on evidence-based Q&As},
Eprint        = {2406.03855v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Clinical problem-solving requires processing of semantic medical knowledge
such as illness scripts and numerical medical knowledge of diagnostic tests for
evidence-based decision-making. As large language models (LLMs) show promising
results in many aspects of language-based clinical practice, their ability to
generate non-language evidence-based answers to clinical questions is
inherently limited by tokenization. Therefore, we evaluated LLMs' performance
on two question types: numeric (correlating findings) and semantic
(differentiating entities) while examining differences within and between LLMs
in medical aspects and comparing their performance to humans. To generate
straightforward multi-choice questions and answers (QAs) based on
evidence-based medicine (EBM), we used a comprehensive medical knowledge graph
(encompassed data from more than 50,00 peer-reviewed articles) and created the
"EBMQA". EBMQA contains 105,000 QAs labeled with medical and non-medical topics
and classified into numerical or semantic questions. We benchmarked this
dataset using more than 24,500 QAs on two state-of-the-art LLMs: Chat-GPT4 and
Claude3-Opus. We evaluated the LLMs accuracy on semantic and numerical question
types and according to sub-labeled topics. For validation, six medical experts
were tested on 100 numerical EBMQA questions. We found that both LLMs excelled
more in semantic than numerical QAs, with Claude3 surpassing GPT4 in numerical
QAs. However, both LLMs showed inter and intra gaps in different medical
aspects and remained inferior to humans. Thus, their medical advice should be
addressed carefully.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.03855v3},
File          = {2406.03855v3.pdf}
}
@article{1403.4887v2,
Author        = {Andrew Warren and Joao Setubal},
Title         = {Using Entropy Estimates for DAG-Based Ontologies},
Eprint        = {1403.4887v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Motivation: Entropy measurements on hierarchical structures have been used in
methods for information retrieval and natural language modeling. Here we
explore its application to semantic similarity. By finding shared ontology
terms, semantic similarity can be established between annotated genes. A common
procedure for establishing semantic similarity is to calculate the
descriptiveness (information content) of ontology terms and use these values to
determine the similarity of annotations. Most often information content is
calculated for an ontology term by analyzing its frequency in an annotation
corpus. The inherent problems in using these values to model functional
similarity motivates our work. Summary: We present a novel calculation for
establishing the entropy of a DAG-based ontology, which can be used in an
alternative method for establishing the information content of its terms. We
also compare our IC metric to two others using semantic and sequence
similarity.},
Year          = {2014},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1403.4887v2},
File          = {1403.4887v2.pdf}
}
@article{1804.04838v1,
Author        = {Duygu Altinok},
Title         = {An Ontology-Based Dialogue Management System for Banking and Finance
  Dialogue Systems},
Eprint        = {1804.04838v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Keeping the dialogue state in dialogue systems is a notoriously difficult
task. We introduce an ontology-based dialogue manage(OntoDM), a dialogue
manager that keeps the state of the conversation, provides a basis for anaphora
resolution and drives the conversation via domain ontologies. The banking and
finance area promises great potential for disambiguating the context via a rich
set of products and specificity of proper nouns, named entities and verbs. We
used ontologies both as a knowledge base and a basis for the dialogue manager;
the knowledge base component and dialogue manager components coalesce in a
sense. Domain knowledge is used to track Entities of Interest, i.e. nodes
(classes) of the ontology which happen to be products and services. In this way
we also introduced conversation memory and attention in a sense. We finely
blended linguistic methods, domain-driven keyword ranking and domain ontologies
to create ways of domain-driven conversation. Proposed framework is used in our
in-house German language banking and finance chatbots. General challenges of
German language processing and finance-banking domain chatbot language models
and lexicons are also introduced. This work is still in progress, hence no
success metrics have been introduced yet.},
Year          = {2018},
Month         = {Apr},
Url           = {http://arxiv.org/abs/1804.04838v1},
File          = {1804.04838v1.pdf}
}
@article{2303.02206v2,
Author        = {Navid Madani and Rohini K. Srihari and Kenneth Joseph},
Title         = {Domain Specific Question Answering Over Knowledge Graphs Using Logical
  Programming and Large Language Models},
Eprint        = {2303.02206v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Answering questions over domain-specific graphs requires a tailored approach
due to the limited number of relations and the specific nature of the domain.
Our approach integrates classic logical programming languages into large
language models (LLMs), enabling the utilization of logical reasoning
capabilities to tackle the KGQA task. By representing the questions as Prolog
queries, which are readable and near close to natural language in
representation, we facilitate the generation of programmatically derived
answers. To validate the effectiveness of our approach, we evaluate it using a
well-known benchmark dataset, MetaQA. Our experimental results demonstrate that
our method achieves accurate identification of correct answer entities for all
test questions, even when trained on a small fraction of annotated data.
Overall, our work presents a promising approach to addressing question
answering over domain-specific graphs, offering an explainable and robust
solution by incorporating logical programming languages.},
Year          = {2023},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2303.02206v2},
File          = {2303.02206v2.pdf}
}
@article{2404.05587v2,
Author        = {Wolfgang Otto and Sharmila Upadhyaya and Stefan Dietze},
Title         = {Enhancing Software-Related Information Extraction via Single-Choice
  Question Answering with Large Language Models},
Eprint        = {2404.05587v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper describes our participation in the Shared Task on Software
Mentions Disambiguation (SOMD), with a focus on improving relation extraction
in scholarly texts through generative Large Language Models (LLMs) using
single-choice question-answering. The methodology prioritises the use of
in-context learning capabilities of GLMs to extract software-related entities
and their descriptive attributes, such as distributive information. Our
approach uses Retrieval-Augmented Generation (RAG) techniques and GLMs for
Named Entity Recognition (NER) and Attributive NER to identify relationships
between extracted software entities, providing a structured solution for
analysing software citations in academic literature. The paper provides a
detailed description of our approach, demonstrating how using GLMs in a
single-choice QA paradigm can greatly enhance IE methodologies. Our
participation in the SOMD shared task highlights the importance of precise
software citation practices and showcases our system's ability to overcome the
challenges of disambiguating and extracting relationships between software
mentions. This sets the groundwork for future research and development in this
field.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.05587v2},
File          = {2404.05587v2.pdf}
}
@article{2405.17044v3,
Author        = {Xuemei Gu and Mario Krenn},
Title         = {Interesting Scientific Idea Generation using Knowledge Graphs and LLMs:
  Evaluations with 100 Research Group Leaders},
Eprint        = {2405.17044v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The rapid growth of scientific literature makes it challenging for
researchers to identify novel and impactful ideas, especially across
disciplines. Modern artificial intelligence (AI) systems offer new approaches,
potentially inspiring ideas not conceived by humans alone. But how compelling
are these AI-generated ideas, and how can we improve their quality? Here, we
introduce SciMuse, which uses 58 million research papers and a large-language
model to generate research ideas. We conduct a large-scale evaluation in which
over 100 research group leaders -- from natural sciences to humanities --
ranked more than 4,400 personalized ideas based on their interest. This data
allows us to predict research interest using (1) supervised neural networks
trained on human evaluations, and (2) unsupervised zero-shot ranking with
large-language models. Our results demonstrate how future systems can help
generating compelling research ideas and foster unforeseen interdisciplinary
collaborations.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.17044v3},
File          = {2405.17044v3.pdf}
}
@article{2408.07840v1,
Author        = {Xuanqing Yu and Wangtao Sun and Jingwei Li and Kang Liu and Chengbao Liu and Jie Tan},
Title         = {ONSEP: A Novel Online Neural-Symbolic Framework for Event Prediction
  Based on Large Language Model},
Eprint        = {2408.07840v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In the realm of event prediction, temporal knowledge graph forecasting (TKGF)
stands as a pivotal technique. Previous approaches face the challenges of not
utilizing experience during testing and relying on a single short-term history,
which limits adaptation to evolving data. In this paper, we introduce the
Online Neural-Symbolic Event Prediction (ONSEP) framework, which innovates by
integrating dynamic causal rule mining (DCRM) and dual history augmented
generation (DHAG). DCRM dynamically constructs causal rules from real-time
data, allowing for swift adaptation to new causal relationships. In parallel,
DHAG merges short-term and long-term historical contexts, leveraging a
bi-branch approach to enrich event prediction. Our framework demonstrates
notable performance enhancements across diverse datasets, with significant
Hit@k (k=1,3,10) improvements, showcasing its ability to augment large language
models (LLMs) for event prediction without necessitating extensive retraining.
The ONSEP framework not only advances the field of TKGF but also underscores
the potential of neural-symbolic approaches in adapting to dynamic data
environments.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.07840v1},
File          = {2408.07840v1.pdf}
}
@article{2409.16176v1,
Author        = {Braden K Webb and Sumit Purohit and Rounak Meyur},
Title         = {Cyber Knowledge Completion Using Large Language Models},
Eprint        = {2409.16176v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {The integration of the Internet of Things (IoT) into Cyber-Physical Systems
(CPSs) has expanded their cyber-attack surface, introducing new and
sophisticated threats with potential to exploit emerging vulnerabilities.
Assessing the risks of CPSs is increasingly difficult due to incomplete and
outdated cybersecurity knowledge. This highlights the urgent need for
better-informed risk assessments and mitigation strategies. While previous
efforts have relied on rule-based natural language processing (NLP) tools to
map vulnerabilities, weaknesses, and attack patterns, recent advancements in
Large Language Models (LLMs) present a unique opportunity to enhance
cyber-attack knowledge completion through improved reasoning, inference, and
summarization capabilities. We apply embedding models to encapsulate
information on attack patterns and adversarial techniques, generating mappings
between them using vector embeddings. Additionally, we propose a
Retrieval-Augmented Generation (RAG)-based approach that leverages pre-trained
models to create structured mappings between different taxonomies of threat
patterns. Further, we use a small hand-labeled dataset to compare the proposed
RAG-based approach to a baseline standard binary classification model. Thus,
the proposed approach provides a comprehensive framework to address the
challenge of cyber-attack knowledge graph completion.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.16176v1},
File          = {2409.16176v1.pdf}
}
@article{2410.21067v1,
Author        = {Meiqi Chen and Fandong Meng and Yingxue Zhang and Yan Zhang and Jie Zhou},
Title         = {CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and
  Retrieval-Augmented Translation with Large Language Models},
Eprint        = {2410.21067v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have shown great promise in machine translation,
but they still struggle with contextually dependent terms, such as new or
domain-specific words. This leads to inconsistencies and errors that are
difficult to address. Existing solutions often depend on manual identification
of such terms, which is impractical given the complexity and evolving nature of
language. While Retrieval-Augmented Generation (RAG) could provide some
assistance, its application to translation is limited by issues such as
hallucinations from information overload. In this paper, we propose CRAT, a
novel multi-agent translation framework that leverages RAG and
causality-enhanced self-reflection to address these challenges. This framework
consists of several specialized agents: the Unknown Terms Identification agent
detects unknown terms within the context, the Knowledge Graph (KG) Constructor
agent extracts relevant internal knowledge about these terms and retrieves
bilingual information from external sources, the Causality-enhanced Judge agent
validates the accuracy of the information, and the Translator agent
incorporates the refined information into the final output. This automated
process allows for more precise and consistent handling of key terms during
translation. Our results show that CRAT significantly improves translation
accuracy, particularly in handling context-sensitive terms and emerging
vocabulary.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.21067v1},
File          = {2410.21067v1.pdf}
}
@article{2501.02157v1,
Author        = {Steven Au and Cameron J. Dimacali and Ojasmitha Pedirappagari and Namyong Park and Franck Dernoncourt and Yu Wang and Nikos Kanakaris and Hanieh Deilamsalehy and Ryan A. Rossi and Nesreen K. Ahmed},
Title         = {Personalized Graph-Based Retrieval for Large Language Models},
Eprint        = {2501.02157v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {As large language models (LLMs) evolve, their ability to deliver personalized
and context-aware responses offers transformative potential for improving user
experiences. Existing personalization approaches, however, often rely solely on
user history to augment the prompt, limiting their effectiveness in generating
tailored outputs, especially in cold-start scenarios with sparse data. To
address these limitations, we propose Personalized Graph-based
Retrieval-Augmented Generation (PGraphRAG), a framework that leverages
user-centric knowledge graphs to enrich personalization. By directly
integrating structured user knowledge into the retrieval process and augmenting
prompts with user-relevant context, PGraphRAG enhances contextual understanding
and output quality. We also introduce the Personalized Graph-based Benchmark
for Text Generation, designed to evaluate personalized text generation tasks in
real-world settings where user history is sparse or unavailable. Experimental
results show that PGraphRAG significantly outperforms state-of-the-art
personalization methods across diverse tasks, demonstrating the unique
advantages of graph-based retrieval for personalization.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.02157v1},
File          = {2501.02157v1.pdf}
}
@article{2309.05936v1,
Author        = {Weiqi Wu and Chengyue Jiang and Yong Jiang and Pengjun Xie and Kewei Tu},
Title         = {Do PLMs Know and Understand Ontological Knowledge?},
Eprint        = {2309.05936v1},
DOI           = {10.18653/v1/2023.acl-long.173},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Ontological knowledge, which comprises classes and properties and their
relationships, is integral to world knowledge. It is significant to explore
whether Pretrained Language Models (PLMs) know and understand such knowledge.
However, existing PLM-probing studies focus mainly on factual knowledge,
lacking a systematic probing of ontological knowledge. In this paper, we focus
on probing whether PLMs store ontological knowledge and have a semantic
understanding of the knowledge rather than rote memorization of the surface
form. To probe whether PLMs know ontological knowledge, we investigate how well
PLMs memorize: (1) types of entities; (2) hierarchical relationships among
classes and properties, e.g., Person is a subclass of Animal and Member of
Sports Team is a subproperty of Member of ; (3) domain and range constraints of
properties, e.g., the subject of Member of Sports Team should be a Person and
the object should be a Sports Team. To further probe whether PLMs truly
understand ontological knowledge beyond memorization, we comprehensively study
whether they can reliably perform logical reasoning with given knowledge
according to ontological entailment rules. Our probing results show that PLMs
can memorize certain ontological knowledge and utilize implicit knowledge in
reasoning. However, both the memorizing and reasoning performances are less
than perfect, indicating incomplete knowledge and understanding.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.05936v1},
File          = {2309.05936v1.pdf}
}
@article{2310.03659v1,
Author        = {Thorsten Händler},
Title         = {Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for
  Autonomous LLM-powered Multi-Agent Architectures},
Eprint        = {2310.03659v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large language models (LLMs) have revolutionized the field of artificial
intelligence, endowing it with sophisticated language understanding and
generation capabilities. However, when faced with more complex and
interconnected tasks that demand a profound and iterative thought process, LLMs
reveal their inherent limitations. Autonomous LLM-powered multi-agent systems
represent a strategic response to these challenges. Such systems strive for
autonomously tackling user-prompted goals by decomposing them into manageable
tasks and orchestrating their execution and result synthesis through a
collective of specialized intelligent agents. Equipped with LLM-powered
reasoning capabilities, these agents harness the cognitive synergy of
collaborating with their peers, enhanced by leveraging contextual resources
such as tools and datasets. While these architectures hold promising potential
in amplifying AI capabilities, striking the right balance between different
levels of autonomy and alignment remains the crucial challenge for their
effective operation. This paper proposes a comprehensive multi-dimensional
taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems
balance the dynamic interplay between autonomy and alignment across various
aspects inherent to architectural viewpoints such as goal-driven task
management, agent composition, multi-agent collaboration, and context
interaction. It also includes a domain-ontology model specifying fundamental
architectural concepts. Our taxonomy aims to empower researchers, engineers,
and AI practitioners to systematically analyze the architectural dynamics and
balancing strategies employed by these increasingly prevalent AI systems. The
exploratory taxonomic classification of selected representative LLM-powered
multi-agent systems illustrates its practical utility and reveals potential for
future research and development.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.03659v1},
File          = {2310.03659v1.pdf}
}
@article{2307.02796v2,
Author        = {Nan Tang and Chenyu Yang and Ju Fan and Lei Cao and Yuyu Luo and Alon Halevy},
Title         = {VerifAI: Verified Generative AI},
Eprint        = {2307.02796v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Generative AI has made significant strides, yet concerns about the accuracy
and reliability of its outputs continue to grow. Such inaccuracies can have
serious consequences such as inaccurate decision-making, the spread of false
information, privacy violations, legal liabilities, and more. Although efforts
to address these risks are underway, including explainable AI and responsible
AI practices such as transparency, privacy protection, bias mitigation, and
social and environmental responsibility, misinformation caused by generative AI
will remain a significant challenge. We propose that verifying the outputs of
generative AI from a data management perspective is an emerging issue for
generative AI. This involves analyzing the underlying data from multi-modal
data lakes, including text files, tables, and knowledge graphs, and assessing
its quality and consistency. By doing so, we can establish a stronger
foundation for evaluating the outputs of generative AI models. Such an approach
can ensure the correctness of generative AI, promote transparency, and enable
decision-making with greater confidence. Our vision is to promote the
development of verifiable generative AI and contribute to a more trustworthy
and responsible use of AI.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.02796v2},
File          = {2307.02796v2.pdf}
}
@article{2211.05351v1,
Author        = {Dattaraj J. Rao and Shraddha S. Mane and Mukta A. Paliwal},
Title         = {Biomedical Multi-hop Question Answering Using Knowledge Graph Embeddings
  and Language Models},
Eprint        = {2211.05351v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Biomedical knowledge graphs (KG) are heterogenous networks consisting of
biological entities as nodes and relations between them as edges. These
entities and relations are extracted from millions of research papers and
unified in a single resource. The goal of biomedical multi-hop
question-answering over knowledge graph (KGQA) is to help biologist and
scientist to get valuable insights by asking questions in natural language.
Relevant answers can be found by first understanding the question and then
querying the KG for right set of nodes and relationships to arrive at an
answer. To model the question, language models such as RoBERTa and BioBERT are
used to understand context from natural language question. One of the
challenges in KGQA is missing links in the KG. Knowledge graph embeddings (KGE)
help to overcome this problem by encoding nodes and edges in a dense and more
efficient way. In this paper, we use a publicly available KG called Hetionet
which is an integrative network of biomedical knowledge assembled from 29
different databases of genes, compounds, diseases, and more. We have enriched
this KG dataset by creating a multi-hop biomedical question-answering dataset
in natural language for testing the biomedical multi-hop question-answering
system and this dataset will be made available to the research community. The
major contribution of this research is an integrated system that combines
language models with KG embeddings to give highly relevant answers to free-form
questions asked by biologists in an intuitive interface. Biomedical multi-hop
question-answering system is tested on this data and results are highly
encouraging.},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.05351v1},
File          = {2211.05351v1.pdf}
}
@article{2303.02411v1,
Author        = {Maria Lymperaiou and Giorgos Stamou},
Title         = {The Contribution of Knowledge in Visiolinguistic Learning: A Survey on
  Tasks and Challenges},
Eprint        = {2303.02411v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent advancements in visiolinguistic (VL) learning have allowed the
development of multiple models and techniques that offer several impressive
implementations, able to currently resolve a variety of tasks that require the
collaboration of vision and language. Current datasets used for VL pre-training
only contain a limited amount of visual and linguistic knowledge, thus
significantly limiting the generalization capabilities of many VL models.
External knowledge sources such as knowledge graphs (KGs) and Large Language
Models (LLMs) are able to cover such generalization gaps by filling in missing
knowledge, resulting in the emergence of hybrid architectures. In the current
survey, we analyze tasks that have benefited from such hybrid approaches.
Moreover, we categorize existing knowledge sources and types, proceeding to
discussion regarding the KG vs LLM dilemma and its potential impact to future
hybrid approaches.},
Year          = {2023},
Month         = {Mar},
Note          = {AAAI MAKE 2023},
Url           = {http://arxiv.org/abs/2303.02411v1},
File          = {2303.02411v1.pdf}
}
@article{2401.12863v1,
Author        = {Debjyoti Mondal and Suraj Modi and Subhadarshi Panda and Rituraj Singh and Godawari Sudhakar Rao},
Title         = {KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning},
Eprint        = {2401.12863v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large Language Models (LLMs) have demonstrated impressive performance in
natural language processing tasks by leveraging chain of thought (CoT) that
enables step-by-step thinking. Extending LLMs with multimodal capabilities is
the recent interest, but incurs computational cost and requires substantial
hardware resources. To address these challenges, we propose KAM-CoT a framework
that integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities
for a comprehensive understanding of multimodal tasks. KAM-CoT adopts a
two-stage training process with KG grounding to generate effective rationales
and answers. By incorporating external knowledge from KGs during reasoning, the
model gains a deeper contextual understanding reducing hallucinations and
enhancing the quality of answers. This knowledge-augmented CoT reasoning
empowers the model to handle questions requiring external context, providing
more informed answers. Experimental findings show KAM-CoT outperforms the
state-of-the-art methods. On the ScienceQA dataset, we achieve an average
accuracy of 93.87%, surpassing GPT-3.5 (75.17%) by 18% and GPT-4 (83.99%) by
10%. Remarkably, KAM-CoT achieves these results with only 280M trainable
parameters at a time, demonstrating its cost-efficiency and effectiveness.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.12863v1},
File          = {2401.12863v1.pdf}
}
@article{2402.00292v1,
Author        = {Jiayi Wu and Zhengyu Wu and Ronghua Li and Hongchao Qin and Guoren Wang},
Title         = {Effective Bug Detection in Graph Database Engines: An LLM-based Approach},
Eprint        = {2402.00292v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Graph database engines play a pivotal role in efficiently storing and
managing graph data across various domains, including bioinformatics, knowledge
graphs, and recommender systems. Ensuring data accuracy within graph database
engines is paramount, as inaccuracies can yield unreliable analytical outcomes.
Current bug-detection approaches are confined to specific graph query
languages, limiting their applicabilities when handling graph database engines
that use various graph query languages across various domains. Moreover, they
require extensive prior knowledge to generate queries for detecting bugs. To
address these challenges, we introduces DGDB, a novel paradigm harnessing large
language models(LLM), such as ChatGPT, for comprehensive bug detection in graph
database engines. DGDB leverages ChatGPT to generate high-quality queries for
different graph query languages. It subsequently employs differential testing
to identify bugs in graph database engines. We applied this paradigm to graph
database engines using the Gremlin query language and those using the Cypher
query language, generating approximately 4,000 queries each. In the latest
versions of Neo4j, Agensgraph, and JanusGraph databases, we detected 2, 5, and
3 wrong-result bugs, respectively.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.00292v1},
File          = {2402.00292v1.pdf}
}
@article{2406.13217v1,
Author        = {Xiaoxi Kang and Lizhen Qu and Lay-Ki Soon and Zhuang Li and Adnan Trakic},
Title         = {Bridging Law and Data: Augmenting Reasoning via a Semi-Structured
  Dataset with IRAC methodology},
Eprint        = {2406.13217v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The effectiveness of Large Language Models (LLMs) in legal reasoning is often
limited due to the unique legal terminologies and the necessity for highly
specialized knowledge. These limitations highlight the need for high-quality
data tailored for complex legal reasoning tasks. This paper introduces
LEGALSEMI, a benchmark specifically curated for legal scenario analysis.
LEGALSEMI comprises 54 legal scenarios, each rigorously annotated by legal
experts, based on the comprehensive IRAC (Issue, Rule, Application, Conclusion)
framework. In addition, LEGALSEMI is accompanied by a structured knowledge
graph (SKG). A series of experiments were conducted to assess the usefulness of
LEGALSEMI for IRAC analysis. The experimental results demonstrate the
effectiveness of incorporating the SKG for issue identification, rule
retrieval, application and conclusion generation using four different LLMs.
LEGALSEMI will be publicly available upon acceptance of this paper.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.13217v1},
File          = {2406.13217v1.pdf}
}
@article{2407.19616v1,
Author        = {Selma Wanna and Ryan Barron and Nick Solovyev and Maksim E. Eren and Manish Bhattarai and Kim Rasmussen and Boian S. Alexandrov},
Title         = {TopicTag: Automatic Annotation of NMF Topic Models Using Chain of
  Thought and Prompt Tuning with LLMs},
Eprint        = {2407.19616v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Topic modeling is a technique for organizing and extracting themes from large
collections of unstructured text. Non-negative matrix factorization (NMF) is a
common unsupervised approach that decomposes a term frequency-inverse document
frequency (TF-IDF) matrix to uncover latent topics and segment the dataset
accordingly. While useful for highlighting patterns and clustering documents,
NMF does not provide explicit topic labels, necessitating subject matter
experts (SMEs) to assign labels manually. We present a methodology for
automating topic labeling in documents clustered via NMF with automatic model
determination (NMFk). By leveraging the output of NMFk and employing prompt
engineering, we utilize large language models (LLMs) to generate accurate topic
labels. Our case study on over 34,000 scientific abstracts on Knowledge Graphs
demonstrates the effectiveness of our method in enhancing knowledge management
and document organization.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.19616v1},
File          = {2407.19616v1.pdf}
}
@article{2409.18678v1,
Author        = {Yung-Yu Shih and Ziwei Xu and Hiroya Takamura and Yun-Nung Chen and Chung-Chi Chen},
Title         = {Rehearsing Answers to Probable Questions with Perspective-Taking},
Eprint        = {2409.18678v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Question answering (QA) has been a long-standing focus in the NLP field,
predominantly addressing reading comprehension and common sense QA. However,
scenarios involving the preparation of answers to probable questions during
professional oral presentations remain underexplored. In this paper, we pioneer
the examination of this crucial yet overlooked topic by utilizing real-world QA
conversation transcripts between company managers and professional analysts. We
explore the proposed task using three causal knowledge graphs (KGs) and three
large language models (LLMs). This work provides foundational insights into the
application of LLMs in professional QA scenarios, highlighting the importance
of causal KGs and perspective-taking in generating effective responses.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.18678v1},
File          = {2409.18678v1.pdf}
}
@article{2411.13773v1,
Author        = {Amar Abane and Anis Bekri and Abdella Battou},
Title         = {FastRAG: Retrieval Augmented Generation for Semi-structured Data},
Eprint        = {2411.13773v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NI},
Abstract      = {Efficiently processing and interpreting network data is critical for the
operation of increasingly complex networks. Recent advances in Large Language
Models (LLM) and Retrieval-Augmented Generation (RAG) techniques have improved
data processing in network management. However, existing RAG methods like
VectorRAG and GraphRAG struggle with the complexity and implicit nature of
semi-structured technical data, leading to inefficiencies in time, cost, and
retrieval. This paper introduces FastRAG, a novel RAG approach designed for
semi-structured data. FastRAG employs schema learning and script learning to
extract and structure data without needing to submit entire data sources to an
LLM. It integrates text search with knowledge graph (KG) querying to improve
accuracy in retrieving context-rich information. Evaluation results demonstrate
that FastRAG provides accurate question answering, while improving up to 90% in
time and 85% in cost compared to GraphRAG.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.13773v1},
File          = {2411.13773v1.pdf}
}
@article{2411.15758v1,
Author        = {Siqi Wang and Chao Liang and Yunfan Gao and Yang Liu and Jing Li and Haofen Wang},
Title         = {Decoding Urban Industrial Complexity: Enhancing Knowledge-Driven
  Insights via IndustryScopeGPT},
Eprint        = {2411.15758v1},
DOI           = {10.1145/3664647.3681705},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Industrial parks are critical to urban economic growth. Yet, their
development often encounters challenges stemming from imbalances between
industrial requirements and urban services, underscoring the need for strategic
planning and operations. This paper introduces IndustryScopeKG, a pioneering
large-scale multi-modal, multi-level industrial park knowledge graph, which
integrates diverse urban data including street views, corporate,
socio-economic, and geospatial information, capturing the complex relationships
and semantics within industrial parks. Alongside this, we present the
IndustryScopeGPT framework, which leverages Large Language Models (LLMs) with
Monte Carlo Tree Search to enhance tool-augmented reasoning and decision-making
in Industrial Park Planning and Operation (IPPO). Our work significantly
improves site recommendation and functional planning, demonstrating the
potential of combining LLMs with structured datasets to advance industrial park
management. This approach sets a new benchmark for intelligent IPPO research
and lays a robust foundation for advancing urban industrial development. The
dataset and related code are available at
https://github.com/Tongji-KGLLM/IndustryScope.},
Year          = {2024},
Month         = {Nov},
Note          = {In Proceedings of the 32nd ACM International Conference on
  Multimedia, pp. 4757-4765 (2024, October)},
Url           = {http://arxiv.org/abs/2411.15758v1},
File          = {2411.15758v1.pdf}
}
@article{2412.05447v1,
Author        = {Savini Kashmira and Jayanaka L. Dantanarayana and Joshua Brodsky and Ashish Mahendra and Yiping Kang and Krisztian Flautner and Lingjia Tang and Jason Mars},
Title         = {A Graph-Based Approach for Conversational AI-Driven Personal Memory
  Capture and Retrieval in a Real-world Application},
Eprint        = {2412.05447v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {TOBU is a novel mobile application that captures and retrieves `personal
memories' (pictures/videos together with stories and context around those
moments) in a user-engaging AI-guided conversational approach. Our initial
prototype showed that existing retrieval techniques such as retrieval-augmented
generation (RAG) systems fall short due to their limitations in understanding
memory relationships, causing low recall, hallucination, and unsatisfactory
user experience. We design TOBUGraph, a novel graph-based retrieval approach.
During capturing, TOBUGraph leverages large language models (LLMs) to
automatically create a dynamic knowledge graph of memories, establishing
context and relationships of those memories. During retrieval, TOBUGraph
combines LLMs with the memory graph to achieve comprehensive recall through
graph traversal. Our evaluation using real user data demonstrates that
TOBUGraph outperforms multiple RAG implementations in both precision and
recall, significantly improving user experience through improved retrieval
accuracy and reduced hallucination.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.05447v1},
File          = {2412.05447v1.pdf}
}
@article{2412.10982v2,
Author        = {Gabriel R. Rosenbaum and Lavender Yao Jiang and Ivaxi Sheth and Jaden Stryker and Anton Alyakin and Daniel Alexander Alber and Nicolas K. Goff and Young Joon Fred Kwon and John Markert and Mustafa Nasir-Moin and Jan Moritz Niehues and Karl L. Sangwon and Eunice Yang and Eric Karl Oermann},
Title         = {MedG-KRP: Medical Graph Knowledge Representation Probing},
Eprint        = {2412.10982v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large language models (LLMs) have recently emerged as powerful tools, finding
many medical applications. LLMs' ability to coalesce vast amounts of
information from many sources to generate a response-a process similar to that
of a human expert-has led many to see potential in deploying LLMs for clinical
use. However, medicine is a setting where accurate reasoning is paramount. Many
researchers are questioning the effectiveness of multiple choice question
answering (MCQA) benchmarks, frequently used to test LLMs. Researchers and
clinicians alike must have complete confidence in LLMs' abilities for them to
be deployed in a medical setting. To address this need for understanding, we
introduce a knowledge graph (KG)-based method to evaluate the biomedical
reasoning abilities of LLMs. Essentially, we map how LLMs link medical concepts
in order to better understand how they reason. We test GPT-4, Llama3-70b, and
PalmyraMed-70b, a specialized medical model. We enlist a panel of medical
students to review a total of 60 LLM-generated graphs and compare these graphs
to BIOS, a large biomedical KG. We observe GPT-4 to perform best in our human
review but worst in our ground truth comparison; vice-versa with PalmyraMed,
the medical model. Our work provides a means of visualizing the medical
reasoning pathways of LLMs so they can be implemented in clinical settings
safely and effectively.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.10982v2},
File          = {2412.10982v2.pdf}
}
@article{2309.04019v2,
Author        = {Mengzhou Hu and Sahar Alkhairy and Ingoo Lee and Rudolf T. Pillich and Dylan Fong and Kevin Smith and Robin Bachelder and Trey Ideker and Dexter Pratt},
Title         = {Evaluation of large language models for discovery of gene set function},
Eprint        = {2309.04019v2},
DOI           = {10.1038/s41592-024-02525-x},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.GN},
Abstract      = {Gene set analysis is a mainstay of functional genomics, but it relies on
curated databases of gene functions that are incomplete. Here we evaluate five
Large Language Models (LLMs) for their ability to discover the common
biological functions represented by a gene set, substantiated by supporting
rationale, citations and a confidence assessment. Benchmarking against
canonical gene sets from the Gene Ontology, GPT-4 confidently recovered the
curated name or a more general concept (73% of cases), while benchmarking
against random gene sets correctly yielded zero confidence. Gemini-Pro and
Mixtral-Instruct showed ability in naming but were falsely confident for random
sets, whereas Llama2-70b had poor performance overall. In gene sets derived
from 'omics data, GPT-4 identified novel functions not reported by classical
functional enrichment (32% of cases), which independent review indicated were
largely verifiable and not hallucinations. The ability to rapidly synthesize
common gene functions positions LLMs as valuable 'omics assistants.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.04019v2},
File          = {2309.04019v2.pdf}
}
@article{2405.10440v3,
Author        = {Jinge Wu and Hang Dong and Zexi Li and Haowei Wang and Runci Li and Arijit Patra and Chengliang Dai and Waqar Ali and Phil Scordis and Honghan Wu},
Title         = {A Hybrid Framework with Large Language Models for Rare Disease
  Phenotyping},
Eprint        = {2405.10440v3},
DOI           = {10.1186/s12911-024-02698-7},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Rare diseases pose significant challenges in diagnosis and treatment due to
their low prevalence and heterogeneous clinical presentations. Unstructured
clinical notes contain valuable information for identifying rare diseases, but
manual curation is time-consuming and prone to subjectivity. This study aims to
develop a hybrid approach combining dictionary-based natural language
processing (NLP) tools with large language models (LLMs) to improve rare
disease identification from unstructured clinical reports. We propose a novel
hybrid framework that integrates the Orphanet Rare Disease Ontology (ORDO) and
the Unified Medical Language System (UMLS) to create a comprehensive rare
disease vocabulary. The proposed hybrid approach demonstrates superior
performance compared to traditional NLP systems and standalone LLMs. Notably,
the approach uncovers a significant number of potential rare disease cases not
documented in structured diagnostic records, highlighting its ability to
identify previously unrecognized patients.},
Year          = {2024},
Month         = {May},
Note          = {BMC Med Inform Decis Mak 24, 289 (2024)},
Url           = {http://arxiv.org/abs/2405.10440v3},
File          = {2405.10440v3.pdf}
}
@article{2412.07743v1,
Author        = {Zijian Chen and John-Michael Gamble and Micaela Jantzi and John P. Hirdes and Jimmy Lin},
Title         = {Zero-Shot ATC Coding with Large Language Models for Clinical Assessments},
Eprint        = {2412.07743v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Manual assignment of Anatomical Therapeutic Chemical (ATC) codes to
prescription records is a significant bottleneck in healthcare research and
operations at Ontario Health and InterRAI Canada, requiring extensive expert
time and effort. To automate this process while maintaining data privacy, we
develop a practical approach using locally deployable large language models
(LLMs). Inspired by recent advances in automatic International Classification
of Diseases (ICD) coding, our method frames ATC coding as a hierarchical
information extraction task, guiding LLMs through the ATC ontology level by
level. We evaluate our approach using GPT-4o as an accuracy ceiling and focus
development on open-source Llama models suitable for privacy-sensitive
deployment. Testing across Health Canada drug product data, the RABBITS
benchmark, and real clinical notes from Ontario Health, our method achieves 78%
exact match accuracy with GPT-4o and 60% with Llama 3.1 70B. We investigate
knowledge grounding through drug definitions, finding modest improvements in
accuracy. Further, we show that fine-tuned Llama 3.1 8B matches zero-shot Llama
3.1 70B accuracy, suggesting that effective ATC coding is feasible with smaller
models. Our results demonstrate the feasibility of automatic ATC coding in
privacy-sensitive healthcare environments, providing a foundation for future
deployments.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.07743v1},
File          = {2412.07743v1.pdf}
}
@article{2403.05921v2,
Author        = {Bohui Zhang and Valentina Anita Carriero and Katrin Schreiberhuber and Stefani Tsaneva and Lucía Sánchez González and Jongmo Kim and Jacopo de Berardinis},
Title         = {OntoChat: a Framework for Conversational Ontology Engineering using
  Language Models},
Eprint        = {2403.05921v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Ontology engineering (OE) in large projects poses a number of challenges
arising from the heterogeneous backgrounds of the various stakeholders, domain
experts, and their complex interactions with ontology designers. This
multi-party interaction often creates systematic ambiguities and biases from
the elicitation of ontology requirements, which directly affect the design,
evaluation and may jeopardise the target reuse. Meanwhile, current OE
methodologies strongly rely on manual activities (e.g., interviews, discussion
pages). After collecting evidence on the most crucial OE activities, we
introduce \textbf{OntoChat}, a framework for conversational ontology
engineering that supports requirement elicitation, analysis, and testing. By
interacting with a conversational agent, users can steer the creation of user
stories and the extraction of competency questions, while receiving
computational support to analyse the overall requirements and test early
versions of the resulting ontologies. We evaluate OntoChat by replicating the
engineering of the Music Meta Ontology, and collecting preliminary metrics on
the effectiveness of each component from users. We release all code at
https://github.com/King-s-Knowledge-Graph-Lab/OntoChat.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.05921v2},
File          = {2403.05921v2.pdf}
}
@article{2408.02361v1,
Author        = {Renato Vukovic and David Arps and Carel van Niekerk and Benjamin Matthias Ruppik and Hsien-Chin Lin and Michael Heck and Milica Gašić},
Title         = {Dialogue Ontology Relation Extraction via Constrained Chain-of-Thought
  Decoding},
Eprint        = {2408.02361v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {State-of-the-art task-oriented dialogue systems typically rely on
task-specific ontologies for fulfilling user queries. The majority of
task-oriented dialogue data, such as customer service recordings, comes without
ontology and annotation. Such ontologies are normally built manually, limiting
the application of specialised systems. Dialogue ontology construction is an
approach for automating that process and typically consists of two steps: term
extraction and relation extraction. In this work, we focus on relation
extraction in a transfer learning set-up. To improve the generalisation, we
propose an extension to the decoding mechanism of large language models. We
adapt Chain-of-Thought (CoT) decoding, recently developed for reasoning
problems, to generative relation extraction. Here, we generate multiple
branches in the decoding space and select the relations based on a confidence
threshold. By constraining the decoding to ontology terms and relations, we aim
to decrease the risk of hallucination. We conduct extensive experimentation on
two widely used datasets and find improvements in performance on target
ontology for source fine-tuned and one-shot prompted large language models.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.02361v1},
File          = {2408.02361v1.pdf}
}
@article{2408.04023v1,
Author        = {Wrick Talukdar and Anjanava Biswas},
Title         = {Improving Large Language Model (LLM) fidelity through context-aware
  grounding: A systematic approach to reliability and veracity},
Eprint        = {2408.04023v1},
DOI           = {10.30574/wjaets.2023.10.2.0317},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {As Large Language Models (LLMs) become increasingly sophisticated and
ubiquitous in natural language processing (NLP) applications, ensuring their
robustness, trustworthiness, and alignment with human values has become a
critical challenge. This paper presents a novel framework for contextual
grounding in textual models, with a particular emphasis on the Context
Representation stage. Our approach aims to enhance the reliability and ethical
alignment of these models through a comprehensive, context-aware methodology.
By explicitly capturing and representing relevant situational, cultural, and
ethical contexts in a machine-readable format, we lay the foundation for
anchoring a model's behavior within these contexts. Our approach leverages
techniques from knowledge representation and reasoning, such as ontologies,
semantic web technologies, and logic-based formalisms. We evaluate our
framework on real-world textual datasets, demonstrating its effectiveness in
improving model performance, fairness, and alignment with human expectations,
while maintaining high accuracy. Furthermore, we discuss the other key
components of the framework, including context-aware encoding, context-aware
learning, interpretability and explainability, and continuous monitoring and
adaptation. This research contributes to the growing body of work on
responsible AI, offering a practical approach to developing more reliable,
trustworthy, and ethically-aligned language models. Our findings have
significant implications for the deployment of LLMs in sensitive domains such
as healthcare, legal systems, and social services, where contextual
understanding is paramount.},
Year          = {2024},
Month         = {Aug},
Note          = {World Journal of Advanced Engineering Technology and Sciences,
  2023, 10(2), 283-296},
Url           = {http://arxiv.org/abs/2408.04023v1},
File          = {2408.04023v1.pdf}
}
@article{2310.01074v2,
Author        = {Chenhan Yuan and Qianqian Xie and Jimin Huang and Sophia Ananiadou},
Title         = {Back to the Future: Towards Explainable Temporal Reasoning with Large
  Language Models},
Eprint        = {2310.01074v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Temporal reasoning is a crucial NLP task, providing a nuanced understanding
of time-sensitive contexts within textual data. Although recent advancements in
LLMs have demonstrated their potential in temporal reasoning, the predominant
focus has been on tasks such as temporal expression and temporal relation
extraction. These tasks are primarily designed for the extraction of direct and
past temporal cues and to engage in simple reasoning processes. A significant
gap remains when considering complex reasoning tasks such as event forecasting,
which requires multi-step temporal reasoning on events and prediction on the
future timestamp. Another notable limitation of existing methods is their
incapability to provide an illustration of their reasoning process, hindering
explainability. In this paper, we introduce the first task of explainable
temporal reasoning, to predict an event's occurrence at a future timestamp
based on context which requires multiple reasoning over multiple events, and
subsequently provide a clear explanation for their prediction. Our task offers
a comprehensive evaluation of both the LLMs' complex temporal reasoning
ability, the future event prediction ability, and explainability-a critical
attribute for AI applications. To support this task, we present the first
multi-source instruction-tuning dataset of explainable temporal reasoning
(ExpTime) with 26k derived from the temporal knowledge graph datasets and their
temporal reasoning paths, using a novel knowledge-graph-instructed-generation
strategy. Based on the dataset, we propose the first open-source LLM series
TimeLlaMA based on the foundation LlaMA2, with the ability of instruction
following for explainable temporal reasoning. We compare the performance of our
method and a variety of LLMs, where our method achieves the state-of-the-art
performance of temporal prediction and explanation.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.01074v2},
File          = {2310.01074v2.pdf}
}
@article{2411.13560v1,
Author        = {Yichen Shi and Zhuofu Tao and Yuhao Gao and Tianjia Zhou and Cheng Chang and Yaxing Wang and Bingyu Chen and Genhao Zhang and Alvin Liu and Zhiping Yu and Ting-Jung Lin and Lei He},
Title         = {AMSnet-KG: A Netlist Dataset for LLM-based AMS Circuit Auto-Design Using
  Knowledge Graph RAG},
Eprint        = {2411.13560v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {High-performance analog and mixed-signal (AMS) circuits are mainly
full-custom designed, which is time-consuming and labor-intensive. A
significant portion of the effort is experience-driven, which makes the
automation of AMS circuit design a formidable challenge. Large language models
(LLMs) have emerged as powerful tools for Electronic Design Automation (EDA)
applications, fostering advancements in the automatic design process for
large-scale AMS circuits. However, the absence of high-quality datasets has led
to issues such as model hallucination, which undermines the robustness of
automatically generated circuit designs. To address this issue, this paper
introduces AMSnet-KG, a dataset encompassing various AMS circuit schematics and
netlists. We construct a knowledge graph with annotations on detailed
functional and performance characteristics. Facilitated by AMSnet-KG, we
propose an automated AMS circuit generation framework that utilizes the
comprehensive knowledge embedded in LLMs. We first formulate a design strategy
(e.g., circuit architecture using a number of circuit components) based on
required specifications. Next, matched circuit components are retrieved and
assembled into a complete topology, and transistor sizing is obtained through
Bayesian optimization. Simulation results of the netlist are fed back to the
LLM for further topology refinement, ensuring the circuit design specifications
are met. We perform case studies of operational amplifier and comparator design
to verify the automatic design flow from specifications to netlists with
minimal human effort. The dataset used in this paper will be open-sourced upon
publishing of this paper.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.13560v1},
File          = {2411.13560v1.pdf}
}
@article{2302.06761v3,
Author        = {Yuan He and Jiaoyan Chen and Ernesto Jiménez-Ruiz and Hang Dong and Ian Horrocks},
Title         = {Language Model Analysis for Ontology Subsumption Inference},
Eprint        = {2302.06761v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Investigating whether pre-trained language models (LMs) can function as
knowledge bases (KBs) has raised wide research interests recently. However,
existing works focus on simple, triple-based, relational KBs, but omit more
sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To
investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of
inference-based probing tasks and datasets from ontology subsumption axioms
involving both atomic and complex concepts. We conduct extensive experiments on
ontologies of different domains and scales, and our results demonstrate that
LMs encode relatively less background knowledge of Subsumption Inference (SI)
than traditional Natural Language Inference (NLI) but can improve on SI
significantly when a small number of samples are given. We will open-source our
code and datasets.},
Year          = {2023},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2302.06761v3},
File          = {2302.06761v3.pdf}
}
@article{2407.08516v5,
Author        = {Haoyi Xiong and Zhiyuan Wang and Xuhong Li and Jiang Bian and Zeke Xie and Shahid Mumtaz and Anwer Al-Dulaimi and Laura E. Barnes},
Title         = {Converging Paradigms: The Synergy of Symbolic and Connectionist AI in
  LLM-Empowered Autonomous Agents},
Eprint        = {2407.08516v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This article explores the convergence of connectionist and symbolic
artificial intelligence (AI), from historical debates to contemporary
advancements. Traditionally considered distinct paradigms, connectionist AI
focuses on neural networks, while symbolic AI emphasizes symbolic
representation and logic. Recent advancements in large language models (LLMs),
exemplified by ChatGPT and GPT-4, highlight the potential of connectionist
architectures in handling human language as a form of symbols. The study argues
that LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence.
By utilizing LLMs for text-based knowledge modeling and representation, LAAs
integrate neuro-symbolic AI principles, showcasing enhanced reasoning and
decision-making capabilities. Comparing LAAs with Knowledge Graphs within the
neuro-symbolic AI theme highlights the unique strengths of LAAs in mimicking
human-like reasoning processes, scaling effectively with large datasets, and
leveraging in-context samples without explicit re-training. The research
underscores promising avenues in neuro-vector-symbolic integration,
instructional encoding, and implicit reasoning, aimed at further enhancing LAA
capabilities. By exploring the progression of neuro-symbolic AI and proposing
future research trajectories, this work advances the understanding and
development of AI technologies.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.08516v5},
File          = {2407.08516v5.pdf}
}
@article{2407.17190v1,
Author        = {Guanyuan Yu and Xv Wang and Qing Li and Yu Zhao},
Title         = {Fusing LLMs and KGs for Formal Causal Reasoning behind Financial Risk
  Contagion},
Eprint        = {2407.17190v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CE},
Abstract      = {Financial risks trend to spread from one entity to another, ultimately
leading to systemic risks. The key to preventing such risks lies in
understanding the causal chains behind risk contagion. Despite this, prevailing
approaches primarily emphasize identifying risks, overlooking the underlying
causal analysis of risk. To address such an issue, we propose a Risk Contagion
Causal Reasoning model called RC2R, which uses the logical reasoning
capabilities of large language models (LLMs) to dissect the causal mechanisms
of risk contagion grounded in the factual and expert knowledge embedded within
financial knowledge graphs (KGs). At the data level, we utilize financial KGs
to construct causal instructions, empowering LLMs to perform formal causal
reasoning on risk propagation and tackle the "causal parrot" problem of LLMs.
In terms of model architecture, we integrate a fusion module that aligns tokens
and nodes across various granularities via multi-scale contrastive learning,
followed by the amalgamation of textual and graph-structured data through soft
prompt with cross multi-head attention mechanisms. To quantify risk contagion,
we introduce a risk pathway inference module for calculating risk scores for
each node in the graph. Finally, we visualize the risk contagion pathways and
their intensities using Sankey diagrams, providing detailed causal
explanations. Comprehensive experiments on financial KGs and supply chain
datasets demonstrate that our model outperforms several state-of-the-art models
in prediction performance and out-of-distribution (OOD) generalization
capabilities. We will make our dataset and code publicly accessible to
encourage further research and development in this field.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.17190v1},
File          = {2407.17190v1.pdf}
}
@article{2409.07497v1,
Author        = {Ningyu Zhang and Zekun Xi and Yujie Luo and Peng Wang and Bozhong Tian and Yunzhi Yao and Jintian Zhang and Shumin Deng and Mengshu Sun and Lei Liang and Zhiqiang Zhang and Xiaowei Zhu and Jun Zhou and Huajun Chen},
Title         = {OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System},
Eprint        = {2409.07497v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge representation has been a central aim of AI since its inception.
Symbolic Knowledge Graphs (KGs) and neural Large Language Models (LLMs) can
both represent knowledge. KGs provide highly accurate and explicit knowledge
representation, but face scalability issue; while LLMs offer expansive coverage
of knowledge, but incur significant training costs and struggle with precise
and reliable knowledge manipulation. To this end, we introduce OneEdit, a
neural-symbolic prototype system for collaborative knowledge editing using
natural language, which facilitates easy-to-use knowledge management with KG
and LLM. OneEdit consists of three modules: 1) The Interpreter serves for user
interaction with natural language; 2) The Controller manages editing requests
from various users, leveraging the KG with rollbacks to handle knowledge
conflicts and prevent toxic knowledge attacks; 3) The Editor utilizes the
knowledge from the Controller to edit KG and LLM. We conduct experiments on two
new datasets with KGs which demonstrate that OneEdit can achieve superior
performance.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.07497v1},
File          = {2409.07497v1.pdf}
}
@article{2411.08041v1,
Author        = {Sumit Purohit and George Chin and Patrick S Mackey and Joseph A Cottam},
Title         = {GraphAide: Advanced Graph-Assisted Query and Reasoning System},
Eprint        = {2411.08041v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Curating knowledge from multiple siloed sources that contain both structured
and unstructured data is a major challenge in many real-world applications.
Pattern matching and querying represent fundamental tasks in modern data
analytics that leverage this curated knowledge. The development of such
applications necessitates overcoming several research challenges, including
data extraction, named entity recognition, data modeling, and designing query
interfaces. Moreover, the explainability of these functionalities is critical
for their broader adoption.
  The emergence of Large Language Models (LLMs) has accelerated the development
lifecycle of new capabilities. Nonetheless, there is an ongoing need for
domain-specific tools tailored to user activities. The creation of digital
assistants has gained considerable traction in recent years, with LLMs offering
a promising avenue to develop such assistants utilizing domain-specific
knowledge and assumptions.
  In this context, we introduce an advanced query and reasoning system,
GraphAide, which constructs a knowledge graph (KG) from diverse sources and
allows to query and reason over the resulting KG. GraphAide harnesses both the
KG and LLMs to rapidly develop domain-specific digital assistants. It
integrates design patterns from retrieval augmented generation (RAG) and the
semantic web to create an agentic LLM application. GraphAide underscores the
potential for streamlined and efficient development of specialized digital
assistants, thereby enhancing their applicability across various domains.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2411.08041v1},
File          = {2411.08041v1.pdf}
}
@article{1912.00147v3,
Author        = {Bin He and Di Zhou and Jinghui Xiao and Xin jiang and Qun Liu and Nicholas Jing Yuan and Tong Xu},
Title         = {Integrating Graph Contextualized Knowledge into Pre-trained Language
  Models},
Eprint        = {1912.00147v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Complex node interactions are common in knowledge graphs, and these
interactions also contain rich knowledge information. However, traditional
methods usually treat a triple as a training unit during the knowledge
representation learning (KRL) procedure, neglecting contextualized information
of the nodes in knowledge graphs (KGs). We generalize the modeling object to a
very general form, which theoretically supports any subgraph extracted from the
knowledge graph, and these subgraphs are fed into a novel transformer-based
model to learn the knowledge embeddings. To broaden usage scenarios of
knowledge, pre-trained language models are utilized to build a model that
incorporates the learned knowledge representations. Experimental results
demonstrate that our model achieves the state-of-the-art performance on several
medical NLP tasks, and improvement above TransE indicates that our KRL method
captures the graph contextualized information effectively.},
Year          = {2019},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1912.00147v3},
File          = {1912.00147v3.pdf}
}
@article{2204.11673v1,
Author        = {Qian Dong and Yiding Liu and Suqi Cheng and Shuaiqiang Wang and Zhicong Cheng and Shuzi Niu and Dawei Yin},
Title         = {Incorporating Explicit Knowledge in Pre-trained Language Models for
  Passage Re-ranking},
Eprint        = {2204.11673v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Passage re-ranking is to obtain a permutation over the candidate passage set
from retrieval stage. Re-rankers have been boomed by Pre-trained Language
Models (PLMs) due to their overwhelming advantages in natural language
understanding. However, existing PLM based re-rankers may easily suffer from
vocabulary mismatch and lack of domain specific knowledge. To alleviate these
problems, explicit knowledge contained in knowledge graph is carefully
introduced in our work. Specifically, we employ the existing knowledge graph
which is incomplete and noisy, and first apply it in passage re-ranking task.
To leverage a reliable knowledge, we propose a novel knowledge graph
distillation method and obtain a knowledge meta graph as the bridge between
query and passage. To align both kinds of embedding in the latent space, we
employ PLM as text encoder and graph neural network over knowledge meta graph
as knowledge encoder. Besides, a novel knowledge injector is designed for the
dynamic interaction between text and knowledge encoder. Experimental results
demonstrate the effectiveness of our method especially in queries requiring
in-depth domain knowledge.},
Year          = {2022},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2204.11673v1},
File          = {2204.11673v1.pdf}
}
@article{2206.09259v1,
Author        = {Tarun Garg and Kaushik Roy and Amit Sheth},
Title         = {Can Language Models Capture Graph Semantics? From Graphs to Language
  Model and Vice-Versa},
Eprint        = {2206.09259v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graphs are a great resource to capture semantic knowledge in terms
of entities and relationships between the entities. However, current deep
learning models takes as input distributed representations or vectors. Thus,
the graph is compressed in a vectorized representation. We conduct a study to
examine if the deep learning model can compress a graph and then output the
same graph with most of the semantics intact. Our experiments show that
Transformer models are not able to express the full semantics of the input
knowledge graph. We find that this is due to the disparity between the
directed, relationship and type based information contained in a Knowledge
Graph and the fully connected token-token undirected graphical interpretation
of the Transformer Attention matrix.},
Year          = {2022},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2206.09259v1},
File          = {2206.09259v1.pdf}
}
@article{2310.03932v1,
Author        = {Chen Jiang and Martin Jagersand},
Title         = {Bridging Low-level Geometry to High-level Concepts in Visual Servoing of
  Robot Manipulation Task Using Event Knowledge Graphs and Vision-Language
  Models},
Eprint        = {2310.03932v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {In this paper, we propose a framework of building knowledgeable robot control
in the scope of smart human-robot interaction, by empowering a basic
uncalibrated visual servoing controller with contextual knowledge through the
joint usage of event knowledge graphs (EKGs) and large-scale pretrained
vision-language models (VLMs). The framework is expanded in twofold: first, we
interpret low-level image geometry as high-level concepts, allowing us to
prompt VLMs and to select geometric features of points and lines for motor
control skills; then, we create an event knowledge graph (EKG) to conceptualize
a robot manipulation task of interest, where the main body of the EKG is
characterized by an executable behavior tree, and the leaves by semantic
concepts relevant to the manipulation context. We demonstrate, in an
uncalibrated environment with real robot trials, that our method lowers the
reliance of human annotation during task interfacing, allows the robot to
perform activities of daily living more easily by treating low-level
geometric-based motor control skills as high-level concepts, and is beneficial
in building cognitive thinking for smart robot applications.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.03932v1},
File          = {2310.03932v1.pdf}
}
@article{2311.07588v1,
Author        = {Ruijie Wang and Zhiruo Zhang and Luca Rossetto and Florian Ruosch and Abraham Bernstein},
Title         = {NLQxform: A Language Model-based Question to SPARQL Transformer},
Eprint        = {2311.07588v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In recent years, scholarly data has grown dramatically in terms of both scale
and complexity. It becomes increasingly challenging to retrieve information
from scholarly knowledge graphs that include large-scale heterogeneous
relationships, such as authorship, affiliation, and citation, between various
types of entities, e.g., scholars, papers, and organizations. As part of the
Scholarly QALD Challenge, this paper presents a question-answering (QA) system
called NLQxform, which provides an easy-to-use natural language interface to
facilitate accessing scholarly knowledge graphs. NLQxform allows users to
express their complex query intentions in natural language questions. A
transformer-based language model, i.e., BART, is employed to translate
questions into standard SPARQL queries, which can be evaluated to retrieve the
required information. According to the public leaderboard of the Scholarly QALD
Challenge at ISWC 2023 (Task 1: DBLP-QUAD - Knowledge Graph Question Answering
over DBLP), NLQxform achieved an F1 score of 0.85 and ranked first on the QA
task, demonstrating the competitiveness of the system.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.07588v1},
File          = {2311.07588v1.pdf}
}
@article{2312.10645v1,
Author        = {Wei Tang and Zhiqian Wu and Yixin Cao and Yong Liao and Pengyuan Zhou},
Title         = {FedMKGC: Privacy-Preserving Federated Multilingual Knowledge Graph
  Completion},
Eprint        = {2312.10645v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graph completion (KGC) aims to predict missing facts in knowledge
graphs (KGs), which is crucial as modern KGs remain largely incomplete. While
training KGC models on multiple aligned KGs can improve performance, previous
methods that rely on transferring raw data among KGs raise privacy concerns. To
address this challenge, we propose a new federated learning framework that
implicitly aggregates knowledge from multiple KGs without demanding raw data
exchange and entity alignment. We treat each KG as a client that trains a local
language model through textbased knowledge representation learning. A central
server then aggregates the model weights from clients. As natural language
provides a universal representation, the same knowledge thus has similar
semantic representations across KGs. As such, the aggregated language model can
leverage complementary knowledge from multilingual KGs without demanding raw
user data sharing. Extensive experiments on a benchmark dataset demonstrate
that our method substantially improves KGC on multilingual KGs, achieving
comparable performance to state-of-the-art alignment-based models without
requiring any labeled alignments or raw user data sharing. Our codes will be
publicly available.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.10645v1},
File          = {2312.10645v1.pdf}
}
@article{2401.00388v1,
Author        = {Shreyas Verma and Manoj Parmar and Palash Choudhary and Sanchita Porwal},
Title         = {FusionMind -- Improving question and answering with external context
  fusion},
Eprint        = {2401.00388v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Answering questions using pre-trained language models (LMs) and knowledge
graphs (KGs) presents challenges in identifying relevant knowledge and
performing joint reasoning.We compared LMs (fine-tuned for the task) with the
previously published QAGNN method for the Question-answering (QA) objective and
further measured the impact of additional factual context on the QAGNN
performance. The QAGNN method employs LMs to encode QA context and estimate KG
node importance, and effectively update the question choice entity
representations using Graph Neural Networks (GNNs). We further experimented
with enhancing the QA context encoding by incorporating relevant knowledge
facts for the question stem. The models are trained on the OpenbookQA dataset,
which contains ~6000 4-way multiple choice questions and is widely used as a
benchmark for QA tasks. Through our experimentation, we found that
incorporating knowledge facts context led to a significant improvement in
performance. In contrast, the addition of knowledge graphs to language models
resulted in only a modest increase. This suggests that the integration of
contextual knowledge facts may be more impactful for enhancing question
answering performance compared to solely adding knowledge graphs.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2401.00388v1},
File          = {2401.00388v1.pdf}
}
@article{2408.01088v2,
Author        = {Phillip Schneider and Nektarios Machner and Kristiina Jokinen and Florian Matthes},
Title         = {Bridging Information Gaps in Dialogues With Grounded Exchanges Using
  Knowledge Graphs},
Eprint        = {2408.01088v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge models are fundamental to dialogue systems for enabling
conversational interactions, which require handling domain-specific knowledge.
Ensuring effective communication in information-providing conversations entails
aligning user understanding with the knowledge available to the system.
However, dialogue systems often face challenges arising from semantic
inconsistencies in how information is expressed in natural language compared to
how it is represented within the system's internal knowledge. To address this
problem, we study the potential of large language models for conversational
grounding, a mechanism to bridge information gaps by establishing shared
knowledge between dialogue participants. Our approach involves annotating human
conversations across five knowledge domains to create a new dialogue corpus
called BridgeKG. Through a series of experiments on this dataset, we
empirically evaluate the capabilities of large language models in classifying
grounding acts and identifying grounded information items within a knowledge
graph structure. Our findings offer insights into how these models use
in-context learning for conversational grounding tasks and common prediction
errors, which we illustrate with examples from challenging dialogues. We
discuss how the models handle knowledge graphs as a semantic layer between
unstructured dialogue utterances and structured information items.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.01088v2},
File          = {2408.01088v2.pdf}
}
@article{2411.06660v1,
Author        = {Qiao Qiao and Yuepei Li and Qing Wang and Kang Zhou and Qi Li},
Title         = {Bridge: A Unified Framework to Knowledge Graph Completion via Language
  Models and Knowledge Representation},
Eprint        = {2411.06660v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graph completion (KGC) is a task of inferring missing triples based
on existing Knowledge Graphs (KGs). Both structural and semantic information
are vital for successful KGC. However, existing methods only use either the
structural knowledge from the KG embeddings or the semantic information from
pre-trained language models (PLMs), leading to suboptimal model performance.
Moreover, since PLMs are not trained on KGs, directly using PLMs to encode
triples may be inappropriate. To overcome these limitations, we propose a novel
framework called Bridge, which jointly encodes structural and semantic
information of KGs. Specifically, we strategically encode entities and
relations separately by PLMs to better utilize the semantic knowledge of PLMs
and enable structured representation learning via a structural learning
principle. Furthermore, to bridge the gap between KGs and PLMs, we employ a
self-supervised representation learning method called BYOL to fine-tune PLMs
with two different views of a triple. Unlike BYOL, which uses augmentation
methods to create two semantically similar views of the same image, potentially
altering the semantic information. We strategically separate the triple into
two parts to create different views, thus avoiding semantic alteration.
Experiments demonstrate that Bridge outperforms the SOTA models on three
benchmark datasets.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.06660v1},
File          = {2411.06660v1.pdf}
}
@article{2501.00136v1,
Author        = {Taniya Das and Louis Mahon and Thomas Lukasiewicz},
Title         = {Detection-Fusion for Knowledge Graph Extraction from Videos},
Eprint        = {2501.00136v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {One of the challenging tasks in the field of video understanding is
extracting semantic content from video inputs. Most existing systems use
language models to describe videos in natural language sentences, but this has
several major shortcomings. Such systems can rely too heavily on the language
model component and base their output on statistical regularities in natural
language text rather than on the visual contents of the video. Additionally,
natural language annotations cannot be readily processed by a computer, are
difficult to evaluate with performance metrics and cannot be easily translated
into a different natural language. In this paper, we propose a method to
annotate videos with knowledge graphs, and so avoid these problems.
Specifically, we propose a deep-learning-based model for this task that first
predicts pairs of individuals and then the relations between them.
Additionally, we propose an extension of our model for the inclusion of
background knowledge in the construction of knowledge graphs.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2501.00136v1},
File          = {2501.00136v1.pdf}
}
@article{2308.14199v1,
Author        = {Walid S. Saba},
Title         = {Symbolic and Language Agnostic Large Language Models},
Eprint        = {2308.14199v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We argue that the relative success of large language models (LLMs) is not a
reflection on the symbolic vs. subsymbolic debate but a reflection on employing
an appropriate strategy of bottom-up reverse engineering of language at scale.
However, due to the subsymbolic nature of these models whatever knowledge these
systems acquire about language will always be buried in millions of
microfeatures (weights) none of which is meaningful on its own. Moreover, and
due to their stochastic nature, these models will often fail in capturing
various inferential aspects that are prevalent in natural language. What we
suggest here is employing the successful bottom-up strategy in a symbolic
setting, producing symbolic, language agnostic and ontologically grounded large
language models.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.14199v1},
File          = {2308.14199v1.pdf}
}
@article{2310.03376v1,
Author        = {Anisa Rula and Jennifer D'Souza},
Title         = {Procedural Text Mining with Large Language Models},
Eprint        = {2310.03376v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent advancements in the field of Natural Language Processing, particularly
the development of large-scale language models that are pretrained on vast
amounts of knowledge, are creating novel opportunities within the realm of
Knowledge Engineering. In this paper, we investigate the usage of large
language models (LLMs) in both zero-shot and in-context learning settings to
tackle the problem of extracting procedures from unstructured PDF text in an
incremental question-answering fashion. In particular, we leverage the current
state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,
accompanied by two variations of in-context learning that involve an ontology
with definitions of procedures and steps and a limited number of samples of
few-shot learning. The findings highlight both the promise of this approach and
the value of the in-context learning customisations. These modifications have
the potential to significantly address the challenge of obtaining sufficient
training data, a hurdle often encountered in deep learning-based Natural
Language Processing techniques for procedure extraction.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.03376v1},
File          = {2310.03376v1.pdf}
}
@article{1904.00313v1,
Author        = {Bruno Godefroy and Christopher Potts},
Title         = {Modeling Drug-Disease Relations with Linguistic and Knowledge Graph
  Constraints},
Eprint        = {1904.00313v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {FDA drug labels are rich sources of information about drugs and drug-disease
relations, but their complexity makes them challenging texts to analyze in
isolation. To overcome this, we situate these labels in two health knowledge
graphs: one built from precise structured information about drugs and diseases,
and another built entirely from a database of clinical narrative texts using
simple heuristic methods. We show that Probabilistic Soft Logic models defined
over these graphs are superior to text-only and relation-only variants, and
that the clinical narratives graph delivers exceptional results with little
manual effort. Finally, we release a new dataset of drug labels with
annotations for five distinct drug-disease relations.},
Year          = {2019},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1904.00313v1},
File          = {1904.00313v1.pdf}
}
@article{1909.08402v1,
Author        = {Malte Ostendorff and Peter Bourgonje and Maria Berger and Julian Moreno-Schneider and Georg Rehm and Bela Gipp},
Title         = {Enriching BERT with Knowledge Graph Embeddings for Document
  Classification},
Eprint        = {1909.08402v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this paper, we focus on the classification of books using short
descriptive texts (cover blurbs) and additional metadata. Building upon BERT, a
deep neural language model, we demonstrate how to combine text representations
with metadata and knowledge graph embeddings, which encode author information.
Compared to the standard BERT approach we achieve considerably better results
for the classification task. For a more coarse-grained classification using
eight labels we achieve an F1- score of 87.20, while a detailed classification
using 343 labels yields an F1-score of 64.70. We make the source code and
trained models of our experiments publicly available},
Year          = {2019},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1909.08402v1},
File          = {1909.08402v1.pdf}
}
@article{2310.04949v1,
Author        = {Yueling Zeng and Li-C. Wang},
Title         = {Domain Knowledge Graph Construction Via A Simple Checker},
Eprint        = {2310.04949v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {With the availability of large language models, there is a growing interest
for semiconductor chip design companies to leverage the technologies. For those
companies, deployment of a new methodology must include two important
considerations: confidentiality and scalability. In this context, this work
tackles the problem of knowledge graph construction from hardware-design domain
texts. We propose an oracle-checker scheme to leverage the power of GPT3.5 and
demonstrate that the essence of the problem is in distillation of domain
expert's background knowledge. Using RISC-V unprivileged ISA specification as
an example, we explain key ideas and discuss practicality of our proposed
oracle-checker approach.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.04949v1},
File          = {2310.04949v1.pdf}
}
@article{2312.11785v1,
Author        = {Zhangdie Yuan and Andreas Vlachos},
Title         = {Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs},
Eprint        = {2312.11785v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Despite progress in automated fact-checking, most systems require a
significant amount of labeled training data, which is expensive. In this paper,
we propose a novel zero-shot method, which instead of operating directly on the
claim and evidence sentences, decomposes them into semantic triples augmented
using external knowledge graphs, and uses large language models trained for
natural language inference. This allows it to generalize to adversarial
datasets and domains that supervised models require specific training data for.
Our empirical results show that our approach outperforms previous zero-shot
approaches on FEVER, FEVER-Symmetric, FEVER 2.0, and Climate-FEVER, while being
comparable or better than supervised models on the adversarial and the
out-of-domain datasets.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.11785v1},
File          = {2312.11785v1.pdf}
}
@article{2403.11786v1,
Author        = {Preetha Datta and Fedor Vitiugin and Anastasiia Chizhikova and Nitin Sawhney},
Title         = {Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained
  Large Language Models},
Eprint        = {2403.11786v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Extracting hyper-relations is crucial for constructing comprehensive
knowledge graphs, but there are limited supervised methods available for this
task. To address this gap, we introduce a zero-shot prompt-based method using
OpenAI's GPT-3.5 model for extracting hyper-relational knowledge from text.
Comparing our model with a baseline, we achieved promising results, with a
recall of 0.77. Although our precision is currently lower, a detailed analysis
of the model outputs has uncovered potential pathways for future research in
this area.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.11786v1},
File          = {2403.11786v1.pdf}
}
@article{2403.14253v2,
Author        = {Kyuhee Kim and Surin Lee and Sangah Lee},
Title         = {K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional
  Expression},
Eprint        = {2403.14253v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In many literary texts, emotions are indirectly conveyed through descriptions
of actions, facial expressions, and appearances, necessitating emotion
inference for narrative understanding. In this paper, we introduce K-Act2Emo, a
Korean commonsense knowledge graph (CSKG) comprising 1,900 indirect emotional
expressions and the emotions inferable from them. We categorize reasoning types
into inferences in positive situations, inferences in negative situations, and
inferences when expressions do not serve as emotional cues. Unlike existing
CSKGs, K-Act2Emo specializes in emotional contexts, and experimental results
validate its effectiveness for training emotion inference models.
Significantly, the BART-based knowledge model fine-tuned with K-Act2Emo
outperforms various existing Korean large language models, achieving
performance levels comparable to GPT-4 Turbo.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.14253v2},
File          = {2403.14253v2.pdf}
}
@article{2404.16206v1,
Author        = {Sakher Khalil Alqaaidi and Krzysztof Kochut},
Title         = {Knowledge Graph Completion using Structural and Textual Embeddings},
Eprint        = {2404.16206v1},
DOI           = {10.1007/978-3-031-63219-8_18},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge Graphs (KGs) are widely employed in artificial intelligence
applications, such as question-answering and recommendation systems. However,
KGs are frequently found to be incomplete. While much of the existing
literature focuses on predicting missing nodes for given incomplete KG triples,
there remains an opportunity to complete KGs by exploring relations between
existing nodes, a task known as relation prediction. In this study, we propose
a relations prediction model that harnesses both textual and structural
information within KGs. Our approach integrates walks-based embeddings with
language model embeddings to effectively represent nodes. We demonstrate that
our model achieves competitive results in the relation prediction task when
evaluated on a widely used dataset.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.16206v1},
File          = {2404.16206v1.pdf}
}
@article{2407.01409v1,
Author        = {Jacopo D'Abramo and Andrea Zugarini and Paolo Torroni},
Title         = {Dynamic Few-Shot Learning for Knowledge Graph Question Answering},
Eprint        = {2407.01409v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models present opportunities for innovative Question Answering
over Knowledge Graphs (KGQA). However, they are not inherently designed for
query generation. To bridge this gap, solutions have been proposed that rely on
fine-tuning or ad-hoc architectures, achieving good results but limited
out-of-domain distribution generalization. In this study, we introduce a novel
approach called Dynamic Few-Shot Learning (DFSL). DFSL integrates the
efficiency of in-context learning and semantic similarity and provides a
generally applicable solution for KGQA with state-of-the-art performance. We
run an extensive evaluation across multiple benchmark datasets and architecture
configurations.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.01409v1},
File          = {2407.01409v1.pdf}
}
@article{2501.03067v1,
Author        = {Daniel Naro and Jaime Delgado and Silvia Llorente and Amanda Palomo},
Title         = {Design and implementation of tools to build an ontology of Security
  Requirements for Internet of Medical Things},
Eprint        = {2501.03067v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {When developing devices, architectures and services for the Internet of
Medical Things (IoMT) world, manufacturers or integrators must be aware of the
security requirements expressed by both laws and specifications. To provide
tools guiding through these requirements and to assure a third party of the
correct compliance, an ontology charting the relevant laws and specifications
(for the European context) is very useful. We here address the development of
this ontology. Due to the very high number and size of the considered
specification documents, we have put in place a methodology and tools to
simplify the transition from natural text to an ontology. The first step is a
manual highlighting of relevant concepts in the corpus, then a manual
translation to XML/XSD is operated. We have developed a tool allowing us to
convert this semi-structured data into an ontology. Because the different
specifications use similar but different wording, our approach favors the
creation of similar instances in the ontology. To improve the ontology
simplification through instance merging, we consider the use of LLMs. The
responses of the LLMs are compared against our manually defined correct
responses. The quality of the responses of the automated system does not prove
to be good enough to be trusted blindly, and should only be used as a starting
point for a manual correction.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.03067v1},
File          = {2501.03067v1.pdf}
}
@article{2403.06586v1,
Author        = {Luca Arrotta and Claudio Bettini and Gabriele Civitarese and Michele Fiori},
Title         = {ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity
  Recognition Models},
Eprint        = {2403.06586v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Context-aware Human Activity Recognition (HAR) is a hot research area in
mobile computing, and the most effective solutions in the literature are based
on supervised deep learning models. However, the actual deployment of these
systems is limited by the scarcity of labeled data that is required for
training. Neuro-Symbolic AI (NeSy) provides an interesting research direction
to mitigate this issue, by infusing common-sense knowledge about human
activities and the contexts in which they can be performed into HAR deep
learning classifiers. Existing NeSy methods for context-aware HAR rely on
knowledge encoded in logic-based models (e.g., ontologies) whose design,
implementation, and maintenance to capture new activities and contexts require
significant human engineering efforts, technical knowledge, and domain
expertise. Recent works show that pre-trained Large Language Models (LLMs)
effectively encode common-sense knowledge about human activities. In this work,
we propose ContextGPT: a novel prompt engineering approach to retrieve from
LLMs common-sense knowledge about the relationship between human activities and
the context in which they are performed. Unlike ontologies, ContextGPT requires
limited human effort and expertise. An extensive evaluation carried out on two
public datasets shows how a NeSy model obtained by infusing common-sense
knowledge from ContextGPT is effective in data scarcity scenarios, leading to
similar (and sometimes better) recognition rates than logic-based approaches
with a fraction of the effort.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.06586v1},
File          = {2403.06586v1.pdf}
}
@article{2412.07493v1,
Author        = {Muhayy Ud Din and Jan Rosell and Waseem Akram and Isiah Zaplana and Maximo A Roa and Lakmal Seneviratne and Irfan Hussain},
Title         = {Ontology-driven Prompt Tuning for LLM-based Task and Motion Planning},
Eprint        = {2412.07493v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Performing complex manipulation tasks in dynamic environments requires
efficient Task and Motion Planning (TAMP) approaches, which combine high-level
symbolic plan with low-level motion planning. Advances in Large Language Models
(LLMs), such as GPT-4, are transforming task planning by offering natural
language as an intuitive and flexible way to describe tasks, generate symbolic
plans, and reason. However, the effectiveness of LLM-based TAMP approaches is
limited due to static and template-based prompting, which struggles in adapting
to dynamic environments and complex task contexts. To address these
limitations, this work proposes a novel ontology-driven prompt-tuning framework
that employs knowledge-based reasoning to refine and expand user prompts with
task contextual reasoning and knowledge-based environment state descriptions.
Integrating domain-specific knowledge into the prompt ensures semantically
accurate and context-aware task plans. The proposed framework demonstrates its
effectiveness by resolving semantic errors in symbolic plan generation, such as
maintaining logical temporal goal ordering in scenarios involving hierarchical
object placement. The proposed framework is validated through both simulation
and real-world scenarios, demonstrating significant improvements over the
baseline approach in terms of adaptability to dynamic environments, and the
generation of semantically correct task plans.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.07493v1},
File          = {2412.07493v1.pdf}
}
@article{2403.14801v2,
Author        = {Junyoung Kim and Jingye Yang and Kai Wang and Chunhua Weng and Cong Liu},
Title         = {Assessing the Utility of Large Language Models for Phenotype-Driven Gene
  Prioritization in Rare Genetic Disorder Diagnosis},
Eprint        = {2403.14801v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.QM},
Abstract      = {Phenotype-driven gene prioritization is a critical process in the diagnosis
of rare genetic disorders for identifying and ranking potential disease-causing
genes based on observed physical traits or phenotypes. While traditional
approaches rely on curated knowledge graphs with phenotype-gene relations,
recent advancements in large language models have opened doors to the potential
of AI predictions through extensive training on diverse corpora and complex
models. This study conducted a comprehensive evaluation of five large language
models, including two Generative Pre-trained Transformers series, and three
Llama2 series, assessing their performance across three key metrics: task
completeness, gene prediction accuracy, and adherence to required output
structures. Various experiments explored combinations of models, prompts, input
types, and task difficulty levels. Our findings reveal that even the
best-performing LLM, GPT-4, achieved an accuracy of 16.0%, which still lags
behind traditional bioinformatics tools. Prediction accuracy increased with the
parameter/model size. A similar increasing trend was observed for the task
completion rate, with complicated prompts more likely to increase task
completeness in models smaller than GPT-4. However, complicated prompts are
more likely to decrease the structure compliance rate, but no prompt effects on
GPT-4. Compared to HPO term-based input, LLM was also able to achieve better
than random prediction accuracy by taking free-text input, but slightly lower
than with the HPO input. Bias analysis showed that certain genes, such as
MECP2, CDKL5, and SCN1A, are more likely to be top-ranked, potentially
explaining the variances observed across different datasets. This study
provides valuable insights into the integration of LLMs within genomic
analysis, contributing to the ongoing discussion on the utilization of advanced
LLMs in clinical workflows.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.14801v2},
File          = {2403.14801v2.pdf}
}
@article{2405.19266v4,
Author        = {Dingkang Yang and Jinjie Wei and Dongling Xiao and Shunli Wang and Tong Wu and Gang Li and Mingcheng Li and Shuaibing Wang and Jiawei Chen and Yue Jiang and Qingyao Xu and Ke Li and Peng Zhai and Lihua Zhang},
Title         = {PediatricsGPT: Large Language Models as Chinese Medical Assistants for
  Pediatric Applications},
Eprint        = {2405.19266v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Developing intelligent pediatric consultation systems offers promising
prospects for improving diagnostic efficiency, especially in China, where
healthcare resources are scarce. Despite recent advances in Large Language
Models (LLMs) for Chinese medicine, their performance is sub-optimal in
pediatric applications due to inadequate instruction data and vulnerable
training procedures. To address the above issues, this paper builds PedCorpus,
a high-quality dataset of over 300,000 multi-task instructions from pediatric
textbooks, guidelines, and knowledge graph resources to fulfil diverse
diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the
first Chinese pediatric LLM assistant built on a systematic and robust training
pipeline. In the continuous pre-training phase, we introduce a hybrid
instruction pre-training mechanism to mitigate the internal-injected knowledge
inconsistency of LLMs for medical domain adaptation. Immediately, the
full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the
general medical knowledge schema into the models. After that, we devise a
direct following preference optimization to enhance the generation of
pediatrician-like humanistic responses. In the parameter-efficient secondary
SFT phase, a mixture of universal-specific experts strategy is presented to
resolve the competency conflict between medical generalist and pediatric
expertise mastery. Extensive results based on the metrics, GPT-4, and doctor
evaluations on distinct doctor downstream tasks show that PediatricsGPT
consistently outperforms previous Chinese medical LLMs. Our model and dataset
will be open-source for community development.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.19266v4},
File          = {2405.19266v4.pdf}
}
@article{2310.19503v2,
Author        = {Luis-Daniel Ibáñez and John Domingue and Sabrina Kirrane and Oshani Seneviratne and Aisling Third and Maria-Esther Vidal},
Title         = {Trust, Accountability, and Autonomy in Knowledge Graph-based AI for
  Self-determination},
Eprint        = {2310.19503v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge Graphs (KGs) have emerged as fundamental platforms for powering
intelligent decision-making and a wide range of Artificial Intelligence (AI)
services across major corporations such as Google, Walmart, and AirBnb. KGs
complement Machine Learning (ML) algorithms by providing data context and
semantics, thereby enabling further inference and question-answering
capabilities. The integration of KGs with neuronal learning (e.g., Large
Language Models (LLMs)) is currently a topic of active research, commonly named
neuro-symbolic AI. Despite the numerous benefits that can be accomplished with
KG-based AI, its growing ubiquity within online services may result in the loss
of self-determination for citizens as a fundamental societal issue. The more we
rely on these technologies, which are often centralised, the less citizens will
be able to determine their own destinies. To counter this threat, AI
regulation, such as the European Union (EU) AI Act, is being proposed in
certain regions. The regulation sets what technologists need to do, leading to
questions concerning: How can the output of AI systems be trusted? What is
needed to ensure that the data fuelling and the inner workings of these
artefacts are transparent? How can AI be made accountable for its
decision-making? This paper conceptualises the foundational topics and research
pillars to support KG-based AI for self-determination. Drawing upon this
conceptual framework, challenges and opportunities for citizen
self-determination are illustrated and analysed in a real-world scenario. As a
result, we propose a research agenda aimed at accomplishing the recommended
objectives.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.19503v2},
File          = {2310.19503v2.pdf}
}
@article{2311.17696v7,
Author        = {Chenxi Dong and Yimin Yuan and Kan Chen and Shupei Cheng and Chujie Wen},
Title         = {How to Build an Adaptive AI Tutor for Any Course Using Knowledge
  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)},
Eprint        = {2311.17696v7},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Integrating Large Language Models (LLMs) in Intelligent Tutoring Systems
(ITS) presents transformative opportunities for personalized education.
However, current implementations face two critical challenges: maintaining
factual accuracy and delivering coherent, context-aware instruction. While
Retrieval-Augmented Generation (RAG) partially addresses these issues, its
reliance on pure semantic similarity limits its effectiveness in educational
contexts where conceptual relationships are crucial. This paper introduces
Knowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG), a novel
framework that integrates structured knowledge representation with
context-aware retrieval to enable more effective AI tutoring. We present three
key contributions: (1) a novel architecture that grounds AI responses in
structured domain knowledge, (2) empirical validation through controlled
experiments (n=76) demonstrating significant learning improvements (35%
increase in assessment scores, p<0.001), and (3) a comprehensive implementation
framework addressing practical deployment considerations. These results
establish KG-RAG as a robust solution for developing adaptable AI tutoring
systems across diverse educational contexts.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.17696v7},
File          = {2311.17696v7.pdf}
}
@article{2404.17723v2,
Author        = {Zhentao Xu and Mark Jerome Cruz and Matthew Guevara and Tie Wang and Manasi Deshpande and Xiaofeng Wang and Zheng Li},
Title         = {Retrieval-Augmented Generation with Knowledge Graphs for Customer
  Service Question Answering},
Eprint        = {2404.17723v2},
DOI           = {10.1145/3626772.3661370},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {In customer service technical support, swiftly and accurately retrieving
relevant past issues is critical for efficiently resolving customer inquiries.
The conventional retrieval methods in retrieval-augmented generation (RAG) for
large language models (LLMs) treat a large corpus of past issue tracking
tickets as plain text, ignoring the crucial intra-issue structure and
inter-issue relations, which limits performance. We introduce a novel customer
service question-answering method that amalgamates RAG with a knowledge graph
(KG). Our method constructs a KG from historical issues for use in retrieval,
retaining the intra-issue structure and inter-issue relations. During the
question-answering phase, our method parses consumer queries and retrieves
related sub-graphs from the KG to generate answers. This integration of a KG
not only improves retrieval accuracy by preserving customer service structure
information but also enhances answering quality by mitigating the effects of
text segmentation. Empirical assessments on our benchmark datasets, utilizing
key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR)
metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by
0.32 in BLEU. Our method has been deployed within LinkedIn's customer service
team for approximately six months and has reduced the median per-issue
resolution time by 28.6%.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.17723v2},
File          = {2404.17723v2.pdf}
}
@article{2408.04948v1,
Author        = {Bhaskarjit Sarmah and Benika Hall and Rohan Rao and Sunil Patel and Stefano Pasquali and Dhagash Mehta},
Title         = {HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented
  Generation for Efficient Information Extraction},
Eprint        = {2408.04948v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Extraction and interpretation of intricate information from unstructured text
data arising in financial applications, such as earnings call transcripts,
present substantial challenges to large language models (LLMs) even using the
current best practices to use Retrieval Augmented Generation (RAG) (referred to
as VectorRAG techniques which utilize vector databases for information
retrieval) due to challenges such as domain specific terminology and complex
formats of the documents. We introduce a novel approach based on a combination,
called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called
GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for
information extraction from financial documents that is shown to be capable of
generating accurate and contextually relevant answers. Using experiments on a
set of financial earning call transcripts documents which come in the form of
Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we
show that HybridRAG which retrieves context from both vector database and KG
outperforms both traditional VectorRAG and GraphRAG individually when evaluated
at both the retrieval and generation stages in terms of retrieval accuracy and
answer generation. The proposed technique has applications beyond the financial
domain},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.04948v1},
File          = {2408.04948v1.pdf}
}
@article{2409.12171v1,
Author        = {William Van Woensel and Oshani Seneviratne},
Title         = {Semantic Interoperability on Blockchain by Generating Smart Contracts
  Based on Knowledge Graphs},
Eprint        = {2409.12171v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Background: Health 3.0 allows decision making to be based on longitudinal
data from multiple institutions, from across the patient's healthcare journey.
In such a distributed setting, blockchain smart contracts can act as neutral
intermediaries to implement trustworthy decision making.
  Objective: In a distributed setting, transmitted data will be structured
using standards (such as HL7 FHIR) for semantic interoperability. In turn, the
smart contract will require interoperability with this standard, implement a
complex communication setup (e.g., using oracles), and be developed using
blockchain languages (e.g., Solidity). We propose the encoding of smart
contract logic using a high-level semantic Knowledge Graph, using concepts from
the domain standard. We then deploy this semantic KG on blockchain.
  Methods: Off-chain, a code generation pipeline compiles the KG into a
concrete smart contract, which is then deployed on-chain. Our pipeline targets
an intermediary bridge representation, which can be transpiled into a specific
blockchain language. Our choice avoids on-chain rule engines, with
unpredictable and likely higher computational cost; it is thus in line with the
economic rules of blockchain.
  Results: We applied our code generation approach to generate smart contracts
for 3 health insurance cases from Medicare. We discuss the suitability of our
approach - the need for a neutral intermediary - for a number of healthcare use
cases. Our evaluation finds that the generated contracts perform well in terms
of correctness and execution cost ("gas") on blockchain.
  Conclusions: We showed that it is feasible to automatically generate smart
contract code based on a semantic KG, in a way that respects the economic rules
of blockchain. Future work includes studying the use of Large Language Models
(LLM) in our approach, and evaluations on other blockchains.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.12171v1},
File          = {2409.12171v1.pdf}
}
@article{2411.08165v1,
Author        = {Muzhi Li and Cehao Yang and Chengjin Xu and Xuhui Jiang and Yiyan Qi and Jian Guo and Ho-fung Leung and Irwin King},
Title         = {Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for
  Knowledge Graph Completion},
Eprint        = {2411.08165v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The Knowledge Graph Completion~(KGC) task aims to infer the missing entity
from an incomplete triple. Existing embedding-based methods rely solely on
triples in the KG, which is vulnerable to specious relation patterns and
long-tail entities. On the other hand, text-based methods struggle with the
semantic gap between KG triples and natural language. Apart from triples,
entity contexts (e.g., labels, descriptions, aliases) also play a significant
role in augmenting KGs. To address these limitations, we propose KGR3, a
context-enriched framework for KGC. KGR3 is composed of three modules. Firstly,
the Retrieval module gathers supporting triples from the KG, collects plausible
candidate answers from a base embedding model, and retrieves context for each
related entity. Then, the Reasoning module employs a large language model to
generate potential answers for each query triple. Finally, the Re-ranking
module combines candidate answers from the two modules mentioned above, and
fine-tunes an LLM to provide the best answer. Extensive experiments on widely
used datasets demonstrate that KGR3 consistently improves various KGC methods.
Specifically, the best variant of KGR3 achieves absolute Hits@1 improvements of
12.3% and 5.6% on the FB15k237 and WN18RR datasets.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.08165v1},
File          = {2411.08165v1.pdf}
}
@article{2411.09072v2,
Author        = {Sanggeon Yun and Ryozo Masukawa and William Youngwoo Chung and Minhyoung Na and Nathaniel Bastian and Mohsen Imani},
Title         = {Continuous GNN-based Anomaly Detection on Edge using Efficient Adaptive
  Knowledge Graph Learning},
Eprint        = {2411.09072v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The increasing demand for robust security solutions across various industries
has made Video Anomaly Detection (VAD) a critical task in applications such as
intelligent surveillance, evidence investigation, and violence detection.
Traditional approaches to VAD often rely on finetuning large pre-trained
models, which can be computationally expensive and impractical for real-time or
resource-constrained environments. To address this, MissionGNN introduced a
more efficient method by training a graph neural network (GNN) using a fixed
knowledge graph (KG) derived from large language models (LLMs) like GPT-4.
While this approach demonstrated significant efficiency in computational power
and memory, it faces limitations in dynamic environments where frequent updates
to the KG are necessary due to evolving behavior trends and shifting data
patterns. These updates typically require cloud-based computation, posing
challenges for edge computing applications. In this paper, we propose a novel
framework that facilitates continuous KG adaptation directly on edge devices,
overcoming the limitations of cloud dependency. Our method dynamically modifies
the KG through a three-phase process: pruning, alternating, and creating nodes,
enabling real-time adaptation to changing data trends. This continuous learning
approach enhances the robustness of anomaly detection models, making them more
suitable for deployment in dynamic and resource-constrained environments.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.09072v2},
File          = {2411.09072v2.pdf}
}
@article{2412.07618v2,
Author        = {Xiaqiang Tang and Jian Li and Nan Du and Sihong Xie},
Title         = {Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced
  Retrieval-Augmented Generation on Knowledge Graphs},
Eprint        = {2412.07618v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Despite the superior performance of Large language models on many NLP tasks,
they still face significant limitations in memorizing extensive world
knowledge. Recent studies have demonstrated that leveraging the
Retrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs
that encapsulate extensive factual data in a structured format, robustly
enhances the reasoning capabilities of LLMs. However, deploying such systems in
real-world scenarios presents challenges: the continuous evolution of
non-stationary environments may lead to performance degradation and user
satisfaction requires a careful balance of performance and responsiveness. To
address these challenges, we introduce a Multi-objective Multi-Armed Bandit
enhanced RAG framework, supported by multiple retrieval methods with diverse
capabilities under rich and evolving retrieval contexts in practice. Within
this framework, each retrieval method is treated as a distinct ``arm''. The
system utilizes real-time user feedback to adapt to dynamic environments, by
selecting the appropriate retrieval method based on input queries and the
historical multi-objective performance of each arm. Extensive experiments
conducted on two benchmark KGQA datasets demonstrate that our method
significantly outperforms baseline methods in non-stationary settings while
achieving state-of-the-art performance in stationary environments. Code and
data are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.07618v2},
File          = {2412.07618v2.pdf}
}
@article{2501.13956v1,
Author        = {Preston Rasmussen and Pavlo Paliychuk and Travis Beauvais and Jack Ryan and Daniel Chalef},
Title         = {Zep: A Temporal Knowledge Graph Architecture for Agent Memory},
Eprint        = {2501.13956v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce Zep, a novel memory layer service for AI agents that outperforms
the current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR)
benchmark. Additionally, Zep excels in more comprehensive and challenging
evaluations than DMR that better reflect real-world enterprise use cases. While
existing retrieval-augmented generation (RAG) frameworks for large language
model (LLM)-based agents are limited to static document retrieval, enterprise
applications demand dynamic knowledge integration from diverse sources
including ongoing conversations and business data. Zep addresses this
fundamental limitation through its core component Graphiti -- a
temporally-aware knowledge graph engine that dynamically synthesizes both
unstructured conversational data and structured business data while maintaining
historical relationships. In the DMR benchmark, which the MemGPT team
established as their primary evaluation metric, Zep demonstrates superior
performance (94.8% vs 93.4%). Beyond DMR, Zep's capabilities are further
validated through the more challenging LongMemEval benchmark, which better
reflects enterprise use cases through complex temporal reasoning tasks. In this
evaluation, Zep achieves substantial results with accuracy improvements of up
to 18.5% while simultaneously reducing response latency by 90% compared to
baseline implementations. These results are particularly pronounced in
enterprise-critical tasks such as cross-session information synthesis and
long-term context maintenance, demonstrating Zep's effectiveness for deployment
in real-world applications.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.13956v1},
File          = {2501.13956v1.pdf}
}
@article{2308.02555v1,
Author        = {Quanxiu Wang and Xinlei Cao and Jianyong Wang and Wei Zhang},
Title         = {Knowledge-aware Collaborative Filtering with Pre-trained Language Model
  for Personalized Review-based Rating Prediction},
Eprint        = {2308.02555v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Personalized review-based rating prediction aims at leveraging existing
reviews to model user interests and item characteristics for rating prediction.
Most of the existing studies mainly encounter two issues. First, the rich
knowledge contained in the fine-grained aspects of each review and the
knowledge graph is rarely considered to complement the pure text for better
modeling user-item interactions. Second, the power of pre-trained language
models is not carefully studied for personalized review-based rating
prediction. To address these issues, we propose an approach named
Knowledge-aware Collaborative Filtering with Pre-trained Language Model
(KCF-PLM). For the first issue, to utilize rich knowledge, KCF-PLM develops a
transformer network to model the interactions of the extracted aspects w.r.t. a
user-item pair. For the second issue, to better represent users and items,
KCF-PLM takes all the historical reviews of a user or an item as input to
pre-trained language models. Moreover, KCF-PLM integrates the transformer
network and the pre-trained language models through representation propagation
on the knowledge graph and user-item guided attention of the aspect
representations. Thus KCF-PLM combines review text, aspect, knowledge graph,
and pre-trained language models together for review-based rating prediction. We
conduct comprehensive experiments on several public datasets, demonstrating the
effectiveness of KCF-PLM.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.02555v1},
File          = {2308.02555v1.pdf}
}
@article{2411.07264v1,
Author        = {Shalin Shah and Srikanth Ryali and Ramasubbu Venkatesh},
Title         = {Multi-Document Financial Question Answering using LLMs},
Eprint        = {2411.07264v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {We propose two new methods for multi-document financial question answering.
First, a method that uses semantic tagging, and then, queries the index to get
the context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that
uses semantic tagging, and, retrieves knowledge graph triples from a graph
database, as context. KG_RAG uses knowledge graphs constructed using a small
model that is fine-tuned using knowledge distillation using a large teacher
model. The data consists of 18 10K reports of Apple, Microsoft, Alphabet,
NVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of
questions in the data consists of 111 complex questions including many esoteric
questions that are difficult to answer and the answers are not completely
obvious. As evaluation metrics, we use overall scores as well as segmented
scores for measurement including the faithfulness, relevance, correctness,
similarity, an LLM based overall score and the rouge scores as well as a
similarity of embeddings. We find that both methods outperform plain RAG
significantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.07264v1},
File          = {2411.07264v1.pdf}
}
@article{2408.00914v1,
Author        = {Steven Fincke and Adrien Bibal and Elizabeth Boschee},
Title         = {Granting GPT-4 License and Opportunity: Enhancing Accuracy and
  Confidence Estimation for Few-Shot Event Detection},
Eprint        = {2408.00914v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models (LLMs) such as GPT-4 have shown enough promise in the
few-shot learning context to suggest use in the generation of "silver" data and
refinement of new ontologies through iterative application and review. Such
workflows become more effective with reliable confidence estimation.
Unfortunately, confidence estimation is a documented weakness of models such as
GPT-4, and established methods to compensate require significant additional
complexity and computation. The present effort explores methods for effective
confidence estimation with GPT-4 with few-shot learning for event detection in
the BETTER ontology as a vehicle. The key innovation is expanding the prompt
and task presented to GPT-4 to provide License to speculate when unsure and
Opportunity to quantify and explain its uncertainty (L&O). This approach
improves accuracy and provides usable confidence measures (0.759 AUC) with no
additional machinery.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.00914v1},
File          = {2408.00914v1.pdf}
}
@article{2402.10779v2,
Author        = {Mingchen Li and Chen Ling and Rui Zhang and Liang Zhao},
Title         = {A Condensed Transition Graph Framework for Zero-shot Link Prediction
  with Large Language Models},
Eprint        = {2402.10779v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Zero-shot link prediction (ZSLP) on knowledge graphs aims at automatically
identifying relations between given entities. Existing methods primarily employ
auxiliary information to predict tail entity given head entity and its
relation, yet face challenges due to the occasional unavailability of such
detailed information and the inherent simplicity of predicting tail entities
based on semantic similarities. Even though Large Language Models (LLMs) offer
a promising solution to predict unobserved relations between the head and tail
entity in a zero-shot manner, their performance is still restricted due to the
inability to leverage all the (exponentially many) paths' information between
two entities, which are critical in collectively indicating their relation
types. To address this, in this work, we introduce a Condensed Transition Graph
Framework for Zero-Shot Link Prediction (CTLP), which encodes all the paths'
information in linear time complexity to predict unseen relations between
entities, attaining both efficiency and information preservation. Specifically,
we design a condensed transition graph encoder with theoretical guarantees on
its coverage, expressiveness, and efficiency. It is learned by a transition
graph contrastive learning strategy. Subsequently, we design a soft instruction
tuning to learn and map the all-path embedding to the input of LLMs.
Experimental results show that our proposed CTLP method achieves
state-of-the-art performance on three standard ZSLP datasets},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.10779v2},
File          = {2402.10779v2.pdf}
}
@article{2403.04964v2,
Author        = {Carlo Lipizzi},
Title         = {Tell me the truth: A system to measure the trustworthiness of Large
  Language Models},
Eprint        = {2403.04964v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large Language Models (LLM) have taken the front seat in most of the news
since November 2022, when ChatGPT was introduced. After more than one year, one
of the major reasons companies are resistant to adopting them is the limited
confidence they have in the trustworthiness of those systems. In a study by
(Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in
identifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics
found that ChatGPT has an accuracy rate of 17% percent when diagnosing
pediatric medical cases (Barile et al., 2024). But then, what is "trust"? Trust
is a relative, subject condition that can change based on culture, domain,
individuals. And then, given a domain, how can the trustworthiness of a system
be measured? In this paper, I present a systematic approach to measure
trustworthiness based on a predefined ground truth, represented as a knowledge
graph of the domain. The approach is a process with humans in the loop to
validate the representation of the domain and to fine-tune the system.
  Measuring the trustworthiness would be essential for all the entities
operating in critical environments, such as healthcare, defense, finance, but
it would be very relevant for all the users of LLMs.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.04964v2},
File          = {2403.04964v2.pdf}
}
@article{2405.14831v3,
Author        = {Bernal Jiménez Gutiérrez and Yiheng Shu and Yu Gu and Michihiro Yasunaga and Yu Su},
Title         = {HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language
  Models},
Eprint        = {2405.14831v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In order to thrive in hostile and ever-changing natural environments,
mammalian brains evolved to store large amounts of knowledge about the world
and continually integrate new information while avoiding catastrophic
forgetting. Despite the impressive accomplishments, large language models
(LLMs), even with retrieval-augmented generation (RAG), still struggle to
efficiently and effectively integrate a large amount of new experiences after
pre-training. In this work, we introduce HippoRAG, a novel retrieval framework
inspired by the hippocampal indexing theory of human long-term memory to enable
deeper and more efficient knowledge integration over new experiences. HippoRAG
synergistically orchestrates LLMs, knowledge graphs, and the Personalized
PageRank algorithm to mimic the different roles of neocortex and hippocampus in
human memory. We compare HippoRAG with existing RAG methods on multi-hop
question answering and show that our method outperforms the state-of-the-art
methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves
comparable or better performance than iterative retrieval like IRCoT while
being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into
IRCoT brings further substantial gains. Finally, we show that our method can
tackle new types of scenarios that are out of reach of existing methods. Code
and data are available at https://github.com/OSU-NLP-Group/HippoRAG.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.14831v3},
File          = {2405.14831v3.pdf}
}
@article{2409.00092v1,
Author        = {Runtao Ren and Jian Ma},
Title         = {PatentGPT: A Large Language Model for Patent Drafting Using
  Knowledge-based Fine-tuning Method},
Eprint        = {2409.00092v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {As humanity stands on the brink of a new era of technological innovation, the
ability to rapidly transform creative ideas into protected intellectual
property (IP) is more crucial than ever. However, the conventional processes
for patent drafting are fraught with challenges, demanding a nuanced
understanding of advanced field knowledge and technical concepts. Existing
large language models (LLMs), while powerful, often fall short in this IP
creation domain due to their lack of specialized knowledge and
context-awareness necessary for generating technically accurate patent
documents. To bridge this critical gap, we propose a groundbreaking framework
for Knowledge Fine-Tuning (KFT) of LLMs, designed to endow AI with the ability
to autonomously mine, understand, and apply domain-specific knowledge. Our
model, PatentGPT leverages a unique combination of knowledge graph-based
pre-training, domain-specific supervised fine-tuning (SFT), and reinforcement
learning from human feedback (RLHF). Through extensive evaluation, PatentGPT
has demonstrated outstanding performance, scoring up to approximately 400%
higher in patent related benchmark tests compared to state-of-the-art models.
By KFT method the model's capability to not only assist but also augment human
creativity and innovation, our approach sets a new standard for AI-driven
intellectual property generation, paving the way for more efficient and
effective invention processes.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2409.00092v1},
File          = {2409.00092v1.pdf}
}
@article{2410.11201v1,
Author        = {Tong Ding and Wanhua Li and Zhongqi Miao and Hanspeter Pfister},
Title         = {Tree of Attributes Prompt Learning for Vision-Language Models},
Eprint        = {2410.11201v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Prompt learning has proven effective in adapting vision language models for
downstream tasks. However, existing methods usually append learnable prompt
tokens solely with the category names to obtain textual features, which fails
to fully leverage the rich context indicated in the category name. To address
this issue, we propose the Tree of Attributes Prompt learning (TAP), which
first instructs LLMs to generate a tree of attributes with a "concept -
attribute - description" structure for each category, and then learn the
hierarchy with vision and text prompt tokens. Unlike existing methods that
merely augment category names with a set of unstructured descriptions, our
approach essentially distills structured knowledge graphs associated with class
names from LLMs. Furthermore, our approach introduces text and vision prompts
designed to explicitly learn the corresponding visual attributes, effectively
serving as domain experts. Additionally, the general and diverse descriptions
generated based on the class names may be wrong or absent in the specific given
images. To address this misalignment, we further introduce a vision-conditional
pooling module to extract instance-specific text features. Extensive
experimental results demonstrate that our approach outperforms state-of-the-art
methods on the zero-shot base-to-novel generalization, cross-dataset transfer,
as well as few-shot classification across 11 diverse datasets.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.11201v1},
File          = {2410.11201v1.pdf}
}
@article{2502.01059v1,
Author        = {Seungri Yoon and Woosang Jeon and Sanghyeok Choi and Taehyeong Kim and Tae In Ahn},
Title         = {Knowledge Synthesis of Photosynthesis Research Using a Large Language
  Model},
Eprint        = {2502.01059v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The development of biological data analysis tools and large language models
(LLMs) has opened up new possibilities for utilizing AI in plant science
research, with the potential to contribute significantly to knowledge
integration and research gap identification. Nonetheless, current LLMs struggle
to handle complex biological data and theoretical models in photosynthesis
research and often fail to provide accurate scientific contexts. Therefore,
this study proposed a photosynthesis research assistant (PRAG) based on
OpenAI's GPT-4o with retrieval-augmented generation (RAG) techniques and prompt
optimization. Vector databases and an automated feedback loop were used in the
prompt optimization process to enhance the accuracy and relevance of the
responses to photosynthesis-related queries. PRAG showed an average improvement
of 8.7% across five metrics related to scientific writing, with a 25.4%
increase in source transparency. Additionally, its scientific depth and domain
coverage were comparable to those of photosynthesis research papers. A
knowledge graph was used to structure PRAG's responses with papers within and
outside the database, which allowed PRAG to match key entities with 63% and
39.5% of the database and test papers, respectively. PRAG can be applied for
photosynthesis research and broader plant science domains, paving the way for
more in-depth data analysis and predictive capabilities.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.01059v1},
File          = {2502.01059v1.pdf}
}
@article{2010.11967v1,
Author        = {Chenguang Wang and Xiao Liu and Dawn Song},
Title         = {Language Models are Open Knowledge Graphs},
Eprint        = {2010.11967v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper shows how to construct knowledge graphs (KGs) from pre-trained
language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs
(e.g, Wikidata, NELL) are built in either a supervised or semi-supervised
manner, requiring humans to create knowledge. Recent deep language models
automatically acquire knowledge from large-scale corpora via pre-training. The
stored knowledge has enabled the language models to improve downstream NLP
tasks, e.g., answering questions, and writing code and articles. In this paper,
we propose an unsupervised method to cast the knowledge contained within
language models into KGs. We show that KGs are constructed with a single
forward pass of the pre-trained language models (without fine-tuning) over the
corpora. We demonstrate the quality of the constructed KGs by comparing to two
KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual
knowledge that is new in the existing KGs. Our code and KGs will be made
publicly available.},
Year          = {2020},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2010.11967v1},
File          = {2010.11967v1.pdf}
}
@article{2403.07350v3,
Author        = {Han Huang and Haitian Zhong and Tao Yu and Qiang Liu and Shu Wu and Liang Wang and Tieniu Tan},
Title         = {VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark},
Eprint        = {2403.07350v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recently, knowledge editing on large language models (LLMs) has received
considerable attention. Compared to this, editing Large Vision-Language Models
(LVLMs) faces extra challenges from diverse data modalities and complicated
model components, and data for LVLMs editing are limited. The existing LVLM
editing benchmark, which comprises three metrics (Reliability, Locality, and
Generality), falls short in the quality of synthesized evaluation images and
cannot assess whether models apply edited knowledge in relevant content.
Therefore, we employ more reliable data collection methods to construct a new
Large $\textbf{V}$ision-$\textbf{L}$anguage Model $\textbf{K}$nowledge
$\textbf{E}$diting $\textbf{B}$enchmark, $\textbf{VLKEB}$, and extend the
Portability metric for more comprehensive evaluation. Leveraging a multi-modal
knowledge graph, our image data are bound with knowledge entities. This can be
further used to extract entity-related knowledge, which constitutes the base of
editing data. We conduct experiments of different editing methods on five
LVLMs, and thoroughly analyze how do they impact the models. The results reveal
strengths and deficiencies of these methods and hopefully provide insights for
future research. The codes and dataset are available at:
https://github.com/VLKEB/VLKEB.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.07350v3},
File          = {2403.07350v3.pdf}
}
@article{2410.12375v1,
Author        = {Markus J. Buehler},
Title         = {PRefLexOR: Preference-based Recursive Language Modeling for Exploratory
  Optimization of Reasoning and Agentic Thinking},
Eprint        = {2410.12375v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {PRefLexOR (Preference-based Recursive Language Modeling for Exploratory
Optimization of Reasoning) combines preference optimization with concepts from
Reinforcement Learning to enable models to self-teach through iterative
reasoning improvements. We propose a recursive learning approach that engages
the model in multi-step reasoning, revisiting, and refining intermediate steps
before producing a final output in training and inference phases. Through
multiple training stages, the model first learns to align its reasoning with
accurate decision paths by optimizing the log odds between preferred and
non-preferred responses. During this process, PRefLexOR builds a dynamic
knowledge graph by generating questions from random text chunks and
retrieval-augmentation to contextualize relevant details from the entire
training corpus. In the second stage, preference optimization enhances model
performance by using rejection sampling to fine-tune reasoning quality by
continually producing in-situ training data while masking the reasoning steps.
Recursive optimization within a thinking token framework introduces iterative
feedback loops, where the model refines reasoning, achieving deeper coherence,
consistency, and adaptability. Implemented in small language models with only 3
billion parameters, we should that even tiny models can iteratively teach
themselves to reason with greater depth and reflectivity. Our implementation is
straightforward and can be incorporated into any existing pretrained LLM. We
focus our examples on applications in biological materials science and
demonstrate the method in a variety of case studies that range from in-domain
to cross-domain applications. Using reasoning strategies that include thinking
and reflection modalities we build a multi-agent recursive self-improving
inference approach to successively improve responses via repeated sampling in
inference time.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.12375v1},
File          = {2410.12375v1.pdf}
}
@article{2409.13731v3,
Author        = {Lei Liang and Mengshu Sun and Zhengke Gui and Zhongshu Zhu and Zhouyu Jiang and Ling Zhong and Yuan Qu and Peilong Zhao and Zhongpu Bo and Jin Yang and Huaidong Xiong and Lin Yuan and Jun Xu and Zaoyang Wang and Zhiqiang Zhang and Wen Zhang and Huajun Chen and Wenguang Chen and Jun Zhou},
Title         = {KAG: Boosting LLMs in Professional Domains via Knowledge Augmented
  Generation},
Eprint        = {2409.13731v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The recently developed retrieval-augmented generation (RAG) technology has
enabled the efficient construction of domain-specific applications. However, it
also has limitations, including the gap between vector similarity and the
relevance of knowledge reasoning, as well as insensitivity to knowledge logic,
such as numerical values, temporal relations, expert rules, and others, which
hinder the effectiveness of professional knowledge services. In this work, we
introduce a professional domain knowledge service framework called Knowledge
Augmented Generation (KAG). KAG is designed to address the aforementioned
challenges with the motivation of making full use of the advantages of
knowledge graph(KG) and vector retrieval, and to improve generation and
reasoning performance by bidirectionally enhancing large language models (LLMs)
and KGs through five key aspects: (1) LLM-friendly knowledge representation,
(2) mutual-indexing between knowledge graphs and original chunks, (3)
logical-form-guided hybrid reasoning engine, (4) knowledge alignment with
semantic reasoning, and (5) model capability enhancement for KAG. We compared
KAG with existing RAG methods in multihop question answering and found that it
significantly outperforms state-of-theart methods, achieving a relative
improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We
have successfully applied KAG to two professional knowledge Q&A tasks of Ant
Group, including E-Government Q&A and E-Health Q&A, achieving significant
improvement in professionalism compared to RAG methods.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.13731v3},
File          = {2409.13731v3.pdf}
}
@article{2301.04013v1,
Author        = {Ankush Agarwal and Sakharam Gawade and Sachin Channabasavarajendra and Pushpak Bhattacharyya},
Title         = {There is No Big Brother or Small Brother: Knowledge Infusion in Language
  Models for Link Prediction and Question Answering},
Eprint        = {2301.04013v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The integration of knowledge graphs with deep learning is thriving in
improving the performance of various natural language processing (NLP) tasks.
In this paper, we focus on knowledge-infused link prediction and question
answering using language models, T5, and BLOOM across three domains: Aviation,
Movie, and Web. In this context, we infuse knowledge in large and small
language models and study their performance, and find the performance to be
similar. For the link prediction task on the Aviation Knowledge Graph, we
obtain a 0.2 hits@1 score using T5-small, T5-base, T5-large, and BLOOM. Using
template-based scripts, we create a set of 1 million synthetic factoid QA pairs
in the aviation domain from National Transportation Safety Board (NTSB)
reports. On our curated QA pairs, the three models of T5 achieve a 0.7 hits@1
score. We validate out findings with the paired student t-test and Cohen's
kappa scores. For link prediction on Aviation Knowledge Graph using T5-small
and T5-large, we obtain a Cohen's kappa score of 0.76, showing substantial
agreement between the models. Thus, we infer that small language models perform
similar to large language models with the infusion of knowledge.},
Year          = {2023},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2301.04013v1},
File          = {2301.04013v1.pdf}
}
@article{2402.14382v2,
Author        = {Yuwei Xia and Ding Wang and Qiang Liu and Liang Wang and Shu Wu and Xiaoyu Zhang},
Title         = {Chain-of-History Reasoning for Temporal Knowledge Graph Forecasting},
Eprint        = {2402.14382v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based
on given histories. Most recent graph-based models excel at capturing
structural information within TKGs but lack semantic comprehension abilities.
Nowadays, with the surge of LLMs, the LLM-based TKG prediction model has
emerged. However, the existing LLM-based model exhibits three shortcomings: (1)
It only focuses on the first-order history for prediction while ignoring
high-order historical information, resulting in the provided information for
LLMs being extremely limited. (2) LLMs struggle with optimal reasoning
performance under heavy historical information loads. (3) For TKG prediction,
the temporal reasoning capability of LLM alone is limited. To address the first
two challenges, we propose Chain-of-History (CoH) reasoning which explores
high-order histories step-by-step, achieving effective utilization of
high-order historical information for LLMs on TKG prediction. To address the
third issue, we design CoH as a plug-and-play module to enhance the performance
of graph-based models for TKG prediction. Extensive experiments on three
datasets and backbones demonstrate the effectiveness of CoH.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.14382v2},
File          = {2402.14382v2.pdf}
}
@article{2406.10393v1,
Author        = {Mohammad Dehghan and Mohammad Ali Alomrani and Sunyam Bagga and David Alfonso-Hermelo and Khalil Bibi and Abbas Ghaddar and Yingxue Zhang and Xiaoguang Li and Jianye Hao and Qun Liu and Jimmy Lin and Boxing Chen and Prasanna Parthasarathi and Mahdi Biparva and Mehdi Rezagholizadeh},
Title         = {EWEK-QA: Enhanced Web and Efficient Knowledge Graph Retrieval for
  Citation-based Question Answering Systems},
Eprint        = {2406.10393v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The emerging citation-based QA systems are gaining more attention especially
in generative AI search applications. The importance of extracted knowledge
provided to these systems is vital from both accuracy (completeness of
information) and efficiency (extracting the information in a timely manner). In
this regard, citation-based QA systems are suffering from two shortcomings.
First, they usually rely only on web as a source of extracted knowledge and
adding other external knowledge sources can hamper the efficiency of the
system. Second, web-retrieved contents are usually obtained by some simple
heuristics such as fixed length or breakpoints which might lead to splitting
information into pieces. To mitigate these issues, we propose our enhanced web
and efficient knowledge graph (KG) retrieval solution (EWEK-QA) to enrich the
content of the extracted knowledge fed to the system. This has been done
through designing an adaptive web retriever and incorporating KGs triples in an
efficient manner. We demonstrate the effectiveness of EWEK-QA over the
open-source state-of-the-art (SoTA) web-based and KG baseline models using a
comprehensive set of quantitative and human evaluation experiments. Our model
is able to: first, improve the web-retriever baseline in terms of extracting
more relevant passages (>20\%), the coverage of answer span (>25\%) and self
containment (>35\%); second, obtain and integrate KG triples into its pipeline
very efficiently (by avoiding any LLM calls) to outperform the web-only and
KG-only SoTA baselines significantly in 7 quantitative QA tasks and our human
evaluation.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.10393v1},
File          = {2406.10393v1.pdf}
}
@article{2309.15074v2,
Author        = {Haoyi Xiong and Jiang Bian and Sijia Yang and Xiaofei Zhang and Linghe Kong and Daqing Zhang},
Title         = {Natural Language based Context Modeling and Reasoning for Ubiquitous
  Computing with Large Language Models: A Tutorial},
Eprint        = {2309.15074v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large language models (LLMs) have become phenomenally surging, since
2018--two decades after introducing context-awareness into computing systems.
Through taking into account the situations of ubiquitous devices, users and the
societies, context-aware computing has enabled a wide spectrum of innovative
applications, such as assisted living, location-based social network services
and so on. To recognize contexts and make decisions for actions accordingly,
various artificial intelligence technologies, such as Ontology and OWL, have
been adopted as representations for context modeling and reasoning. Recently,
with the rise of LLMs and their improved natural language understanding and
reasoning capabilities, it has become feasible to model contexts using natural
language and perform context reasoning by interacting with LLMs such as ChatGPT
and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and
autonomous agents (AutoAgents) that enable LLMs to perform context modeling and
reasoning without requiring fine-tuning of the model. We organize and introduce
works in the related field, and name this computing paradigm as the LLM-driven
Context-aware Computing (LCaC). In the LCaC paradigm, users' requests, sensors
reading data, and the command to actuators are supposed to be represented as
texts. Given the text of users' request and sensor data, the AutoAgent models
the context by prompting and sends to the LLM for context reasoning. LLM
generates a plan of actions and responds to the AutoAgent, which later follows
the action plan to foster context-awareness. To prove the concepts, we use two
showcases--(1) operating a mobile z-arm in an apartment for assisted living,
and (2) planning a trip and scheduling the itinerary in a context-aware and
personalized manner.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.15074v2},
File          = {2309.15074v2.pdf}
}
@article{2308.13565v1,
Author        = {Tong Xie and Yuwei Wan and Wei Huang and Zhenyu Yin and Yixuan Liu and Shaozhou Wang and Qingyuan Linghu and Chunyu Kit and Clara Grazian and Wenjie Zhang and Imran Razzak and Bram Hoex},
Title         = {DARWIN Series: Domain Specific Large Language Models for Natural Science},
Eprint        = {2308.13565v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Emerging tools bring forth fresh approaches to work, and the field of natural
science is no different. In natural science, traditional manual, serial, and
labour-intensive work is being augmented by automated, parallel, and iterative
processes driven by artificial intelligence-based experimental automation and
more. To add new capabilities in natural science, enabling the acceleration and
enrichment of automation of the discovery process, we present DARWIN, a series
of tailored LLMs for natural science, mainly in physics, chemistry, and
material science. This series relies on open-source LLM, incorporating
structured and unstructured scientific knowledge from public datasets and
literature. We fine-tuned the models using over 60,000 instruction data points,
emphasizing factual correctness. During the fine-tuning, we introduce the
Scientific Instruction Generation (SIG) model, automating instruction
generation from scientific texts. This eliminates the need for manual
extraction or domain-specific knowledge graphs and efficiently injects
scientific knowledge into the model. We also explore multi-task training
strategies, revealing interconnections between scientific tasks. DARWIN series
not only achieves state-of-the-art results on various scientific tasks but also
diminishes reliance on closed-source AI models. Our research showcases the
ability of LLM in the scientific domain, with the overarching goal of fostering
prosperity within the broader AI for science community.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.13565v1},
File          = {2308.13565v1.pdf}
}
@article{2405.20455v5,
Author        = {Mohannad Alhanahnah and Yazan Boshmaf},
Title         = {DepsRAG: Towards Agentic Reasoning and Planning for Software Dependency
  Management},
Eprint        = {2405.20455v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {In the era of Large Language Models (LLMs) with their advanced capabilities,
a unique opportunity arises to develop LLM-based digital assistant tools that
can support software developers by facilitating comprehensive reasoning about
software dependencies and open-source libraries before importing them. This
reasoning process is daunting, mandating multiple specialized tools and
dedicated expertise, each focusing on distinct aspects (e.g., security analysis
tools may overlook design flaws such as circular dependencies, which hinder
software maintainability). Creating a significant bottleneck in the software
development lifecycle. In this paper, we introduce DepsRAG, a multi-agent
framework designed to assist developers in reasoning about software
dependencies. DepsRAG first constructs a comprehensive Knowledge Graph (KG)
that includes both direct and transitive dependencies. Developers can interact
with DepsRAG through a conversational interface, posing queries about the
dependencies. DepsRAG employs Retrieval-Augmented Generation (RAG) to enhance
these queries by retrieving relevant information from the KG as well as
external sources, such as the Web and vulnerability databases, thus
demonstrating its adaptability to novel scenarios. DepsRAG incorporates a
Critic-Agent feedback loop to ensure the accuracy and clarity of LLM-generated
responses. We evaluated DepsRAG using GPT-4-Turbo and Llama-3 on three
multi-step reasoning tasks, observing a threefold increase in accuracy with the
integration of the Critic-Agent mechanism. DepsRAG demo and implementation are
available: https://github.com/Mohannadcse/DepsRAG.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.20455v5},
File          = {2405.20455v5.pdf}
}
@article{2406.18916v2,
Author        = {Wen Zhang and Long Jin and Yushan Zhu and Jiaoyan Chen and Zhiwei Huang and Junjie Wang and Yin Hua and Lei Liang and Huajun Chen},
Title         = {TrustUQA: A Trustful Framework for Unified Structured Data Question
  Answering},
Eprint        = {2406.18916v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Natural language question answering (QA) over structured data sources such as
tables and knowledge graphs have been widely investigated, especially with
Large Language Models (LLMs) in recent years. The main solutions include
question to formal query parsing and retrieval-based answer generation.
However, current methods of the former often suffer from weak generalization,
failing to dealing with multi-types of sources, while the later is limited in
trustfulness. In this paper, we propose TrustUQA, a trustful QA framework that
can simultaneously support multiple types of structured data in a unified way.
To this end, it adopts an LLM-friendly and unified knowledge representation
method called Condition Graph(CG), and uses an LLM and demonstration-based
two-level method for CG querying. For enhancement, it is also equipped with
dynamic demonstration retrieval. We have evaluated TrustUQA with 5 benchmarks
covering 3 types of structured data. It outperforms 2 existing unified
structured data QA methods. In comparison with the baselines that are specific
to one data type, it achieves state-of-the-art on 2 of the datasets. Further
more, we have demonstrated the potential of our method for more general QA
tasks, QA over mixed structured data and QA across structured data. The code is
available at https://github.com/zjukg/TrustUQA.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.18916v2},
File          = {2406.18916v2.pdf}
}
@article{2412.16922v1,
Author        = {Bohan Jin and Qianyou Sun and Lihua Chen},
Title         = {Enhancing Supply Chain Transparency in Emerging Economies Using Online
  Contents and LLMs},
Eprint        = {2412.16922v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {In the current global economy, supply chain transparency plays a pivotal role
in ensuring this security by enabling companies to monitor supplier performance
and fostering accountability and responsibility. Despite the advancements in
supply chain relationship datasets like Bloomberg and FactSet, supply chain
transparency remains a significant challenge in emerging economies due to
issues such as information asymmetry and institutional gaps in regulation. This
study proposes a novel approach to enhance supply chain transparency in
emerging economies by leveraging online content and large language models
(LLMs). We develop a Supply Chain Knowledge Graph Mining System that integrates
advanced LLMs with web crawler technology to automatically collect and analyze
supply chain information. The system's effectiveness is validated through a
case study focusing on the semiconductor supply chain, a domain that has
recently gained significant attention due to supply chain risks. Our results
demonstrate that the proposed system provides greater applicability for
emerging economies, such as mainland China, complementing the data gaps in
existing datasets. However, challenges including the accurate estimation of
monetary and material flows, the handling of time series data, synonyms
disambiguation, and mitigating biases from online contents still remains.
Future research should focus on addressing these issues to further enhance the
system's capabilities and broaden its application to other emerging economies
and industries.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.16922v1},
File          = {2412.16922v1.pdf}
}
@article{2501.16382v1,
Author        = {Ziwen Li and Xiang 'Anthony' Chen and Youngseung Jeon},
Title         = {GraPPI: A Retrieve-Divide-Solve GraphRAG Framework for Large-scale
  Protein-protein Interaction Exploration},
Eprint        = {2501.16382v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.QM},
Abstract      = {Drug discovery (DD) has tremendously contributed to maintaining and improving
public health. Hypothesizing that inhibiting protein misfolding can slow
disease progression, researchers focus on target identification (Target ID) to
find protein structures for drug binding. While Large Language Models (LLMs)
and Retrieval-Augmented Generation (RAG) frameworks have accelerated drug
discovery, integrating models into cohesive workflows remains challenging. We
conducted a user study with drug discovery researchers to identify the
applicability of LLMs and RAGs in Target ID. We identified two main findings:
1) an LLM should provide multiple Protein-Protein Interactions (PPIs) based on
an initial protein and protein candidates that have a therapeutic impact; 2)
the model must provide the PPI and relevant explanations for better
understanding. Based on these observations, we identified three limitations in
previous approaches for Target ID: 1) semantic ambiguity, 2) lack of
explainability, and 3) short retrieval units. To address these issues, we
propose GraPPI, a large-scale knowledge graph (KG)-based retrieve-divide-solve
agent pipeline RAG framework to support large-scale PPI signaling pathway
exploration in understanding therapeutic impacts by decomposing the analysis of
entire PPI pathways into sub-tasks focused on the analysis of PPI edges.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.16382v1},
File          = {2501.16382v1.pdf}
}
@article{1909.00160v1,
Author        = {Soumya Sharma and Bishal Santra and Abhik Jana and T. Y. S. S. Santosh and Niloy Ganguly and Pawan Goyal},
Title         = {Incorporating Domain Knowledge into Medical NLI using Knowledge Graphs},
Eprint        = {1909.00160v1},
DOI           = {10.18653/v1/D19-1631},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recently, biomedical version of embeddings obtained from language models such
as BioELMo have shown state-of-the-art results for the textual inference task
in the medical domain. In this paper, we explore how to incorporate structured
domain knowledge, available in the form of a knowledge graph (UMLS), for the
Medical NLI task. Specifically, we experiment with fusing embeddings obtained
from knowledge graph with the state-of-the-art approaches for NLI task (ESIM
model). We also experiment with fusing the domain-specific sentiment
information for the task. Experiments conducted on MedNLI dataset clearly show
that this strategy improves the baseline BioELMo architecture for the Medical
NLI task.},
Year          = {2019},
Month         = {Aug},
Url           = {http://arxiv.org/abs/1909.00160v1},
File          = {1909.00160v1.pdf}
}
@article{1906.05317v2,
Author        = {Antoine Bosselut and Hannah Rashkin and Maarten Sap and Chaitanya Malaviya and Asli Celikyilmaz and Yejin Choi},
Title         = {COMET: Commonsense Transformers for Automatic Knowledge Graph
  Construction},
Eprint        = {1906.05317v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present the first comprehensive study on automatic knowledge base
construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et
al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional
KBs that store knowledge with canonical templates, commonsense KBs only store
loosely structured open-text descriptions of knowledge. We posit that an
important step toward automatic commonsense completion is the development of
generative models of commonsense knowledge, and propose COMmonsEnse
Transformers (COMET) that learn to generate rich and diverse commonsense
descriptions in natural language. Despite the challenges of commonsense
modeling, our investigation reveals promising results when implicit knowledge
from deep pre-trained language models is transferred to generate explicit
knowledge in commonsense knowledge graphs. Empirical results demonstrate that
COMET is able to generate novel knowledge that humans rate as high quality,
with up to 77.5% (ATOMIC) and 91.7% (ConceptNet) precision at top 1, which
approaches human performance for these resources. Our findings suggest that
using generative commonsense models for automatic commonsense KB completion
could soon be a plausible alternative to extractive methods.},
Year          = {2019},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1906.05317v2},
File          = {1906.05317v2.pdf}
}
@article{2002.03140v1,
Author        = {Qiming Bao and Lin Ni and Jiamou Liu},
Title         = {HHH: An Online Medical Chatbot System based on Knowledge Graph and
  Hierarchical Bi-Directional Attention},
Eprint        = {2002.03140v1},
DOI           = {10.1145/3373017.3373049},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper proposes a chatbot framework that adopts a hybrid model which
consists of a knowledge graph and a text similarity model. Based on this
chatbot framework, we build HHH, an online question-and-answer (QA) Healthcare
Helper system for answering complex medical questions. HHH maintains a
knowledge graph constructed from medical data collected from the Internet. HHH
also implements a novel text representation and similarity deep learning model,
Hierarchical BiLSTM Attention Model (HBAM), to find the most similar question
from a large QA dataset. We compare HBAM with other state-of-the-art language
models such as bidirectional encoder representation from transformers (BERT)
and Manhattan LSTM Model (MaLSTM). We train and test the models with a subset
of the Quora duplicate questions dataset in the medical area. The experimental
results show that our model is able to achieve a superior performance than
these existing methods.},
Year          = {2020},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2002.03140v1},
File          = {2002.03140v1.pdf}
}
@article{2109.09380v1,
Author        = {Shahid Latif and Shivam Agarwal and Simon Gottschalk and Carina Chrosch and Felix Feit and Johannes Jahn and Tobias Braun and Yanick Christian Tchenko and Elena Demidova and Fabian Beck},
Title         = {Visually Connecting Historical Figures Through Event Knowledge Graphs},
Eprint        = {2109.09380v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.HC},
Abstract      = {Knowledge graphs store information about historical figures and their
relationships indirectly through shared events. We developed a visualization
system, VisKonnect, for analyzing the intertwined lives of historical figures
based on the events they participated in. A user's query is parsed for
identifying named entities, and related data is retrieved from an event
knowledge graph. While a short textual answer to the query is generated using
the GPT-3 language model, various linked visualizations provide context,
display additional information related to the query, and allow exploration.},
Year          = {2021},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2109.09380v1},
File          = {2109.09380v1.pdf}
}
@article{2201.05575v4,
Author        = {Peng Wang and Xin Xie and Xiaohan Wang and Ningyu Zhang},
Title         = {Reasoning Through Memorization: Nearest Neighbor Knowledge Graph
  Embeddings},
Eprint        = {2201.05575v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Previous knowledge graph embedding approaches usually map entities to
representations and utilize score functions to predict the target entities, yet
they typically struggle to reason rare or emerging unseen entities. In this
paper, we propose kNN-KGE, a new knowledge graph embedding approach with
pre-trained language models, by linearly interpolating its entity distribution
with k-nearest neighbors. We compute the nearest neighbors based on the
distance in the entity embedding space from the knowledge store. Our approach
can allow rare or emerging entities to be memorized explicitly rather than
implicitly in model parameters. Experimental results demonstrate that our
approach can improve inductive and transductive link prediction results and
yield better performance for low-resource settings with only a few triples,
which might be easier to reason via explicit memory. Code is available at
https://github.com/zjunlp/KNN-KG.},
Year          = {2022},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2201.05575v4},
File          = {2201.05575v4.pdf}
}
@article{2012.11490v2,
Author        = {Filip Ilievski and Pedro Szekely and Bin Zhang},
Title         = {CSKG: The CommonSense Knowledge Graph},
Eprint        = {2012.11490v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Sources of commonsense knowledge support applications in natural language
understanding, computer vision, and knowledge graphs. Given their
complementarity, their integration is desired. Yet, their different foci,
modeling approaches, and sparse overlap make integration difficult. In this
paper, we consolidate commonsense knowledge by following five principles, which
we apply to combine seven key sources into a first integrated CommonSense
Knowledge Graph (CSKG). We analyze CSKG and its various text and graph
embeddings, showing that CSKG is well-connected and that its embeddings provide
a useful entry point to the graph. We demonstrate how CSKG can provide evidence
for generalizable downstream reasoning and for pre-training of language models.
CSKG and all its embeddings are made publicly available to support further
research on commonsense knowledge integration and reasoning.},
Year          = {2020},
Month         = {Dec},
Note          = {ESWC 2021 Resource Track},
Url           = {http://arxiv.org/abs/2012.11490v2},
File          = {2012.11490v2.pdf}
}
@article{2104.08145v2,
Author        = {Keyur Faldu and Amit Sheth and Prashant Kikani and Hemang Akbari},
Title         = {KI-BERT: Infusing Knowledge Context for Better Language and Domain
  Understanding},
Eprint        = {2104.08145v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Contextualized entity representations learned by state-of-the-art
transformer-based language models (TLMs) like BERT, GPT, T5, etc., leverage the
attention mechanism to learn the data context from training data corpus.
However, these models do not use the knowledge context. Knowledge context can
be understood as semantics about entities and their relationship with
neighboring entities in knowledge graphs. We propose a novel and effective
technique to infuse knowledge context from multiple knowledge graphs for
conceptual and ambiguous entities into TLMs during fine-tuning. It projects
knowledge graph embeddings in the homogeneous vector-space, introduces new
token-types for entities, aligns entity position ids, and a selective attention
mechanism. We take BERT as a baseline model and implement the
"Knowledge-Infused BERT" by infusing knowledge context from ConceptNet and
WordNet, which significantly outperforms BERT and other recent knowledge-aware
BERT variants like ERNIE, SenseBERT, and BERT_CS over eight different subtasks
of GLUE benchmark. The KI-BERT-base model even significantly outperforms
BERT-large for domain-specific tasks like SciTail and academic subsets of QQP,
QNLI, and MNLI.},
Year          = {2021},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2104.08145v2},
File          = {2104.08145v2.pdf}
}
@article{2112.04087v1,
Author        = {Ganqiang Ye and Wen Zhang and Zhen Bi and Chi Man Wong and Chen Hui and Huajun Chen},
Title         = {Improving Knowledge Graph Representation Learning by Structure
  Contextual Pre-training},
Eprint        = {2112.04087v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Representation learning models for Knowledge Graphs (KG) have proven to be
effective in encoding structural information and performing reasoning over KGs.
In this paper, we propose a novel pre-training-then-fine-tuning framework for
knowledge graph representation learning, in which a KG model is firstly
pre-trained with triple classification task, followed by discriminative
fine-tuning on specific downstream tasks such as entity type prediction and
entity alignment. Drawing on the general ideas of learning deep contextualized
word representations in typical pre-trained language models, we propose SCoP to
learn pre-trained KG representations with structural and contextual triples of
the target triple encoded. Experimental results demonstrate that fine-tuning
SCoP not only outperforms results of baselines on a portfolio of downstream
tasks but also avoids tedious task-specific model design and parameter
training.},
Year          = {2021},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2112.04087v1},
File          = {2112.04087v1.pdf}
}
@article{2204.00391v1,
Author        = {Sihang Zeng and Zheng Yuan and Sheng Yu},
Title         = {Automatic Biomedical Term Clustering by Learning Fine-grained Term
  Representations},
Eprint        = {2204.00391v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Term clustering is important in biomedical knowledge graph construction.
Using similarities between terms embedding is helpful for term clustering.
State-of-the-art term embeddings leverage pretrained language models to encode
terms, and use synonyms and relation knowledge from knowledge graphs to guide
contrastive learning. These embeddings provide close embeddings for terms
belonging to the same concept. However, from our probing experiments, these
embeddings are not sensitive to minor textual differences which leads to
failure for biomedical term clustering. To alleviate this problem, we adjust
the sampling strategy in pretraining term embeddings by providing dynamic hard
positive and negative samples during contrastive learning to learn fine-grained
representations which result in better biomedical term clustering. We name our
proposed method as CODER++, and it has been applied in clustering biomedical
concepts in the newly released Biomedical Knowledge Graph named BIOS.},
Year          = {2022},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2204.00391v1},
File          = {2204.00391v1.pdf}
}
@article{2204.09149v1,
Author        = {Md Rashad Al Hasan Rony and Ricardo Usbeck and Jens Lehmann},
Title         = {DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation},
Eprint        = {2204.09149v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Task-oriented dialogue generation is challenging since the underlying
knowledge is often dynamic and effectively incorporating knowledge into the
learning process is hard. It is particularly challenging to generate both
human-like and informative responses in this setting. Recent research primarily
focused on various knowledge distillation methods where the underlying
relationship between the facts in a knowledge base is not effectively captured.
In this paper, we go one step further and demonstrate how the structural
information of a knowledge graph can improve the system's inference
capabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue
system that effectively incorporates knowledge into a language model. Our
proposed system views relational knowledge as a knowledge graph and introduces
(1) a structure-aware knowledge embedding technique, and (2) a knowledge
graph-weighted attention masking strategy to facilitate the system selecting
relevant information during the dialogue generation. An empirical evaluation
demonstrates the effectiveness of DialoKG over state-of-the-art methods on
several standard benchmark datasets.},
Year          = {2022},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2204.09149v1},
File          = {2204.09149v1.pdf}
}
@article{2305.02364v2,
Author        = {Silin Gao and Beatriz Borges and Soyoung Oh and Deniz Bayazit and Saya Kanno and Hiromi Wakaki and Yuki Mitsufuji and Antoine Bosselut},
Title         = {PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging
  Narratives},
Eprint        = {2305.02364v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Sustaining coherent and engaging narratives requires dialogue or storytelling
agents to understand how the personas of speakers or listeners ground the
narrative. Specifically, these agents must infer personas of their listeners to
produce statements that cater to their interests. They must also learn to
maintain consistent speaker personas for themselves throughout the narrative,
so that their counterparts feel involved in a realistic conversation or story.
  However, personas are diverse and complex: they entail large quantities of
rich interconnected world knowledge that is challenging to robustly represent
in general narrative systems (e.g., a singer is good at singing, and may have
attended conservatoire). In this work, we construct a new large-scale persona
commonsense knowledge graph, PeaCoK, containing ~100K human-validated persona
facts. Our knowledge graph schematizes five dimensions of persona knowledge
identified in previous studies of human interactive behaviours, and distils
facts in this schema from both existing commonsense knowledge graphs and
large-scale pretrained language models. Our analysis indicates that PeaCoK
contains rich and precise world persona inferences that help downstream systems
generate more consistent and engaging narratives.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.02364v2},
File          = {2305.02364v2.pdf}
}
@article{2305.11301v1,
Author        = {Ishaan Singh and Navdeep Kaur and Garima Gaur and  Mausam},
Title         = {NeuSTIP: A Novel Neuro-Symbolic Model for Link and Time Prediction in
  Temporal Knowledge Graphs},
Eprint        = {2305.11301v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {While Knowledge Graph Completion (KGC) on static facts is a matured field,
Temporal Knowledge Graph Completion (TKGC), that incorporates validity time
into static facts is still in its nascent stage. The KGC methods fall into
multiple categories including embedding-based, rule-based, GNN-based,
pretrained Language Model based approaches. However, such dimensions have not
been explored in TKG. To that end, we propose a novel temporal neuro-symbolic
model, NeuSTIP, that performs link prediction and time interval prediction in a
TKG. NeuSTIP learns temporal rules in the presence of the Allen predicates that
ensure the temporal consistency between neighboring predicates in a given rule.
We further design a unique scoring function that evaluates the confidence of
the candidate answers while performing link prediction and time interval
prediction by utilizing the learned rules. Our empirical evaluation on two time
interval based TKGC datasets suggests that our model outperforms
state-of-the-art models for both link prediction and the time interval
prediction task.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.11301v1},
File          = {2305.11301v1.pdf}
}
@article{2306.04203v1,
Author        = {Fréjus A. A. Laleye and Loïc Rakotoson and Sylvain Massip},
Title         = {Leveraging Knowledge Graph Embeddings to Enhance Contextual
  Representations for Relation Extraction},
Eprint        = {2306.04203v1},
DOI           = {10.1007/978-3-031-41501-2_2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Relation extraction task is a crucial and challenging aspect of Natural
Language Processing. Several methods have surfaced as of late, exhibiting
notable performance in addressing the task; however, most of these approaches
rely on vast amounts of data from large-scale knowledge graphs or language
models pretrained on voluminous corpora. In this paper, we hone in on the
effective utilization of solely the knowledge supplied by a corpus to create a
high-performing model. Our objective is to showcase that by leveraging the
hierarchical structure and relational distribution of entities within a corpus
without introducing external knowledge, a relation extraction model can achieve
significantly enhanced performance. We therefore proposed a relation extraction
approach based on the incorporation of pretrained knowledge graph embeddings at
the corpus scale into the sentence-level contextual representation. We
conducted a series of experiments which revealed promising and very interesting
results for our proposed approach.The obtained results demonstrated an
outperformance of our method compared to context-based relation extraction
models.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.04203v1},
File          = {2306.04203v1.pdf}
}
@article{2407.14030v1,
Author        = {Prerana Sanjay Kulkarni and Muskaan Jain and Disha Sheshanarayana and Srinivasan Parthiban},
Title         = {HeCiX: Integrating Knowledge Graphs and Large Language Models for
  Biomedical Research},
Eprint        = {2407.14030v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Despite advancements in drug development strategies, 90% of clinical trials
fail. This suggests overlooked aspects in target validation and drug
optimization. In order to address this, we introduce HeCiX-KG,
Hetionet-Clinicaltrials neXus Knowledge Graph, a novel fusion of data from
ClinicalTrials.gov and Hetionet in a single knowledge graph. HeCiX-KG combines
data on previously conducted clinical trials from ClinicalTrials.gov, and
domain expertise on diseases and genes from Hetionet. This offers a thorough
resource for clinical researchers. Further, we introduce HeCiX, a system that
uses LangChain to integrate HeCiX-KG with GPT-4, and increase its usability.
HeCiX shows high performance during evaluation against a range of clinically
relevant issues, proving this model to be promising for enhancing the
effectiveness of clinical research. Thus, this approach provides a more
holistic view of clinical trials and existing biological data.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.14030v1},
File          = {2407.14030v1.pdf}
}
@article{2408.07453v1,
Author        = {Tobias A. Opsahl},
Title         = {Fact or Fiction? Improving Fact Verification with Knowledge Graphs
  through Simplified Subgraph Retrievals},
Eprint        = {2408.07453v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Despite recent success in natural language processing (NLP), fact
verification still remains a difficult task. Due to misinformation spreading
increasingly fast, attention has been directed towards automatically verifying
the correctness of claims. In the domain of NLP, this is usually done by
training supervised machine learning models to verify claims by utilizing
evidence from trustworthy corpora. We present efficient methods for verifying
claims on a dataset where the evidence is in the form of structured knowledge
graphs. We use the FactKG dataset, which is constructed from the DBpedia
knowledge graph extracted from Wikipedia. By simplifying the evidence retrieval
process, from fine-tuned language models to simple logical retrievals, we are
able to construct models that both require less computational resources and
achieve better test-set accuracy.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.07453v1},
File          = {2408.07453v1.pdf}
}
@article{2412.07430v2,
Author        = {Kinshuk Vasisht and Navreet Kaur and Danish Pruthi},
Title         = {Knowledge Graph Guided Evaluation of Abstention Techniques},
Eprint        = {2412.07430v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {To deploy language models safely, it is crucial that they abstain from
responding to inappropriate requests. Several prior studies test the safety
promises of models based on their effectiveness in blocking malicious requests.
In this work, we focus on evaluating the underlying techniques that cause
models to abstain. We create SELECT, a benchmark derived from a set of benign
concepts (e.g., "rivers") from a knowledge graph. Focusing on benign concepts
isolates the effect of safety training, and grounding these concepts in a
knowledge graph allows us to study the generalization and specificity of
abstention techniques. Using SELECT, we benchmark different abstention
techniques over six open-weight and closed-source models. We find that the
examined techniques indeed cause models to abstain with over $80\%$ abstention
rates. However, these techniques are not as effective for descendants of the
target concepts, where abstention rates drop by $19\%$. We also characterize
the generalization-specificity trade-offs for different techniques. Overall, no
single technique is invariably better than others, and our findings inform
practitioners of the various trade-offs involved.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.07430v2},
File          = {2412.07430v2.pdf}
}
@article{2308.10173v1,
Author        = {Zhixiao Qi and Yijiong Yu and Meiqi Tu and Junyi Tan and Yongfeng Huang},
Title         = {FoodGPT: A Large Language Model in Food Testing Domain with Incremental
  Pre-training and Knowledge Graph Prompt},
Eprint        = {2308.10173v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Currently, the construction of large language models in specific domains is
done by fine-tuning on a base model. Some models also incorporate knowledge
bases without the need for pre-training. This is because the base model already
contains domain-specific knowledge during the pre-training process. We build a
large language model for food testing. Unlike the above approach, a significant
amount of data in this domain exists in Scanning format for domain standard
documents. In addition, there is a large amount of untrained structured
knowledge. Therefore, we introduce an incremental pre-training step to inject
this knowledge into a large language model. In this paper, we propose a method
for handling structured knowledge and scanned documents in incremental
pre-training. To overcome the problem of machine hallucination, we constructe a
knowledge graph to serve as an external knowledge base for supporting retrieval
in the large language model. It is worth mentioning that this paper is a
technical report of our pre-release version, and we will report our specific
experimental data in future versions.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.10173v1},
File          = {2308.10173v1.pdf}
}
@article{2304.02711v2,
Author        = {J. Harry Caufield and Harshad Hegde and Vincent Emonet and Nomi L. Harris and Marcin P. Joachimiak and Nicolas Matentzoglu and HyeongSik Kim and Sierra A. T. Moxon and Justin T. Reese and Melissa A. Haendel and Peter N. Robinson and Christopher J. Mungall},
Title         = {Structured prompt interrogation and recursive extraction of semantics
  (SPIRES): A method for populating knowledge bases using zero-shot learning},
Eprint        = {2304.02711v2},
DOI           = {10.1093/bioinformatics/btae104},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Creating knowledge bases and ontologies is a time consuming task that relies
on a manual curation. AI/NLP approaches can assist expert curators in
populating these knowledge bases, but current approaches rely on extensive
training data, and are not able to populate arbitrary complex nested knowledge
schemas.
  Here we present Structured Prompt Interrogation and Recursive Extraction of
Semantics (SPIRES), a Knowledge Extraction approach that relies on the ability
of Large Language Models (LLMs) to perform zero-shot learning (ZSL) and
general-purpose query answering from flexible prompts and return information
conforming to a specified schema. Given a detailed, user-defined knowledge
schema and an input text, SPIRES recursively performs prompt interrogation
against GPT-3+ to obtain a set of responses matching the provided schema.
SPIRES uses existing ontologies and vocabularies to provide identifiers for all
matched elements.
  We present examples of use of SPIRES in different domains, including
extraction of food recipes, multi-species cellular signaling pathways, disease
treatments, multi-step drug mechanisms, and chemical to disease causation
graphs. Current SPIRES accuracy is comparable to the mid-range of existing
Relation Extraction (RE) methods, but has the advantage of easy customization,
flexibility, and, crucially, the ability to perform new tasks in the absence of
any training data. This method supports a general strategy of leveraging the
language interpreting capabilities of LLMs to assemble knowledge bases,
assisting manual knowledge curation and acquisition while supporting validation
with publicly-available databases and ontologies external to the LLM.
  SPIRES is available as part of the open source OntoGPT package:
https://github.com/ monarch-initiative/ontogpt.},
Year          = {2023},
Month         = {Apr},
Note          = {Bioinformatics, 2024 Mar 4;40(3)},
Url           = {http://arxiv.org/abs/2304.02711v2},
File          = {2304.02711v2.pdf}
}
@article{2501.07238v1,
Author        = {Blake Bullwinkel and Amanda Minnich and Shiven Chawla and Gary Lopez and Martin Pouliot and Whitney Maxwell and Joris de Gruyter and Katherine Pratt and Saphir Qi and Nina Chikanov and Roman Lutz and Raja Sekhar Rao Dheekonda and Bolor-Erdene Jagdagdorj and Eugenia Kim and Justin Song and Keegan Hines and Daniel Jones and Giorgio Severi and Richard Lundeen and Sam Vaughan and Victoria Westerhoff and Pete Bryan and Ram Shankar Siva Kumar and Yonatan Zunger and Chang Kawaguchi and Mark Russinovich},
Title         = {Lessons From Red Teaming 100 Generative AI Products},
Eprint        = {2501.07238v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In recent years, AI red teaming has emerged as a practice for probing the
safety and security of generative AI systems. Due to the nascency of the field,
there are many open questions about how red teaming operations should be
conducted. Based on our experience red teaming over 100 generative AI products
at Microsoft, we present our internal threat model ontology and eight main
lessons we have learned:
  1. Understand what the system can do and where it is applied
  2. You don't have to compute gradients to break an AI system
  3. AI red teaming is not safety benchmarking
  4. Automation can help cover more of the risk landscape
  5. The human element of AI red teaming is crucial
  6. Responsible AI harms are pervasive but difficult to measure
  7. LLMs amplify existing security risks and introduce new ones
  8. The work of securing AI systems will never be complete
  By sharing these insights alongside case studies from our operations, we
offer practical recommendations aimed at aligning red teaming efforts with real
world risks. We also highlight aspects of AI red teaming that we believe are
often misunderstood and discuss open questions for the field to consider.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.07238v1},
File          = {2501.07238v1.pdf}
}
@article{2308.14217v1,
Author        = {Xin Luna Dong},
Title         = {Generations of Knowledge Graphs: The Crazy Ideas and the Business Impact},
Eprint        = {2308.14217v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Knowledge Graphs (KGs) have been used to support a wide range of
applications, from web search to personal assistant. In this paper, we describe
three generations of knowledge graphs: entity-based KGs, which have been
supporting general search and question answering (e.g., at Google and Bing);
text-rich KGs, which have been supporting search and recommendations for
products, bio-informatics, etc. (e.g., at Amazon and Alibaba); and the emerging
integration of KGs and LLMs, which we call dual neural KGs. We describe the
characteristics of each generation of KGs, the crazy ideas behind the scenes in
constructing such KGs, and the techniques developed over time to enable
industry impact. In addition, we use KGs as examples to demonstrate a recipe to
evolve research ideas from innovations to production practice, and then to the
next level of innovations, to advance both science and business.},
Year          = {2023},
Month         = {Aug},
Note          = {PVLDB 2023},
Url           = {http://arxiv.org/abs/2308.14217v1},
File          = {2308.14217v1.pdf}
}
@article{2412.11016v1,
Author        = {Haji Gul and Abdul Ghani Naim and Ajaz A. Bhat},
Title         = {A Contextualized BERT model for Knowledge Graph Completion},
Eprint        = {2412.11016v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs (KGs) are valuable for representing structured,
interconnected information across domains, enabling tasks like semantic search,
recommendation systems and inference. A pertinent challenge with KGs, however,
is that many entities (i.e., heads, tails) or relationships are unknown.
Knowledge Graph Completion (KGC) addresses this by predicting these missing
nodes or links, enhancing the graph's informational depth and utility.
Traditional methods like TransE and ComplEx predict tail entities but struggle
with unseen entities. Textual-based models leverage additional semantics but
come with high computational costs, semantic inconsistencies, and data
imbalance issues. Recent LLM-based models show improvement but overlook
contextual information and rely heavily on entity descriptions. In this study,
we introduce a contextualized BERT model for KGC that overcomes these
limitations by utilizing the contextual information from neighbouring entities
and relationships to predict tail entities. Our model eliminates the need for
entity descriptions and negative triplet sampling, reducing computational
demands while improving performance. Our model outperforms state-of-the-art
methods on standard datasets, improving Hit@1 by 5.3% and 4.88% on FB15k-237
and WN18RR respectively, setting a new benchmark in KGC.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.11016v1},
File          = {2412.11016v1.pdf}
}
@article{2407.10805v7,
Author        = {Shengjie Ma and Chengjin Xu and Xuhui Jiang and Muzhi Li and Huaren Qu and Cehao Yang and Jiaxin Mao and Jian Guo},
Title         = {Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning
  with Knowledge-guided Retrieval Augmented Generation},
Eprint        = {2407.10805v7},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Retrieval-augmented generation (RAG) has improved large language models
(LLMs) by using knowledge retrieval to overcome knowledge deficiencies.
However, current RAG methods often fall short of ensuring the depth and
completeness of retrieved information, which is necessary for complex reasoning
tasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG
framework that iteratively retrieves information from both unstructured and
structured knowledge sources in a tight-coupling manner. Specifically, ToG-2
leverages knowledge graphs (KGs) to link documents via entities, facilitating
deep and knowledge-guided context retrieval. Simultaneously, it utilizes
documents as entity contexts to achieve precise and efficient graph retrieval.
ToG-2 alternates between graph retrieval and context retrieval to search for
in-depth clues relevant to the question, enabling LLMs to generate answers. We
conduct a series of well-designed experiments to highlight the following
advantages of ToG-2: 1) ToG-2 tightly couples the processes of context
retrieval and graph retrieval, deepening context retrieval via the KG while
enabling reliable graph retrieval based on contexts; 2) it achieves deep and
faithful reasoning in LLMs through an iterative knowledge retrieval process of
collaboration between contexts and the KG; and 3) ToG-2 is training-free and
plug-and-play compatible with various LLMs. Extensive experiments demonstrate
that ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7
knowledge-intensive datasets with GPT-3.5, and can elevate the performance of
smaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.
The source code is available on https://github.com/IDEA-FinAI/ToG-2.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.10805v7},
File          = {2407.10805v7.pdf}
}
@article{2410.07951v1,
Author        = {Kuleen Sasse and Shinjitha Vadlakonda and Richard E. Kennedy and John D. Osborne},
Title         = {Disease Entity Recognition and Normalization is Improved with Large
  Language Model Derived Synthetic Normalized Mentions},
Eprint        = {2410.07951v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Background: Machine learning methods for clinical named entity recognition
and entity normalization systems can utilize both labeled corpora and Knowledge
Graphs (KGs) for learning. However, infrequently occurring concepts may have
few mentions in training corpora and lack detailed descriptions or synonyms,
even in large KGs. For Disease Entity Recognition (DER) and Disease Entity
Normalization (DEN), this can result in fewer high quality training examples
relative to the number of known diseases. Large Language Model (LLM) generation
of synthetic training examples could improve performance in these information
extraction tasks.
  Methods: We fine-tuned a LLaMa-2 13B Chat LLM to generate a synthetic corpus
containing normalized mentions of concepts from the Unified Medical Language
System (UMLS) Disease Semantic Group. We measured overall and Out of
Distribution (OOD) performance for DER and DEN, with and without synthetic data
augmentation. We evaluated performance on 3 different disease corpora using 4
different data augmentation strategies, assessed using BioBERT for DER and
SapBERT and KrissBERT for DEN.
  Results: Our synthetic data yielded a substantial improvement for DEN, in all
3 training corpora the top 1 accuracy of both SapBERT and KrissBERT improved by
3-9 points in overall performance and by 20-55 points in OOD data. A small
improvement (1-2 points) was also seen for DER in overall performance, but only
one dataset showed OOD improvement.
  Conclusion: LLM generation of normalized disease mentions can improve DEN
relative to normalization approaches that do not utilize LLMs to augment data
with synthetic mentions. Ablation studies indicate that performance gains for
DEN were only partially attributable to improvements in OOD performance. The
same approach has only a limited ability to improve DER. We make our software
and dataset publicly available.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.07951v1},
File          = {2410.07951v1.pdf}
}
@article{2201.04843v2,
Author        = {Da Li and Sen Yang and Kele Xu and Ming Yi and Yukai He and Huaimin Wang},
Title         = {Multi-task Pre-training Language Model for Semantic Network Completion},
Eprint        = {2201.04843v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Semantic networks, such as the knowledge graph, can represent the knowledge
leveraging the graph structure. Although the knowledge graph shows promising
values in natural language processing, it suffers from incompleteness. This
paper focuses on knowledge graph completion by predicting linkage between
entities, which is a fundamental yet critical task. Semantic matching is a
potential solution as it can deal with unseen entities, which the translational
distance based methods struggle with. However, to achieve competitive
performance as translational distance based methods, semantic matching based
methods require large-scale datasets for the training purpose, which are
typically unavailable in practical settings. Therefore, we employ the language
model and introduce a novel knowledge graph architecture named LP-BERT, which
contains two main stages: multi-task pre-training and knowledge graph
fine-tuning. In the pre-training phase, three tasks are taken to drive the
model to learn the relationship from triples by predicting either entities or
relations. While in the fine-tuning phase, inspired by contrastive learning, we
design a triple-style negative sampling in a batch, which greatly increases the
proportion of negative sampling while keeping the training time almost
unchanged. Furthermore, we propose a new data augmentation method utilizing the
inverse relationship of triples to improve the performance and robustness of
the model. To demonstrate the effectiveness of our method, we conduct extensive
experiments on three widely-used datasets, WN18RR, FB15k-237, and UMLS. The
experimental results demonstrate the superiority of our methods, and our
approach achieves state-of-the-art results on WN18RR and FB15k-237 datasets.
Significantly, Hits@10 indicator is improved by 5% from previous
state-of-the-art result on the WN18RR dataset while reaching 100% on the UMLS
dataset.},
Year          = {2022},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2201.04843v2},
File          = {2201.04843v2.pdf}
}
@article{2305.03513v2,
Author        = {Yucheng Shi and Hehuan Ma and Wenliang Zhong and Qiaoyu Tan and Gengchen Mai and Xiang Li and Tianming Liu and Junzhou Huang},
Title         = {ChatGraph: Interpretable Text Classification by Converting ChatGPT
  Knowledge to Graphs},
Eprint        = {2305.03513v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {ChatGPT, as a recently launched large language model (LLM), has shown
superior performance in various natural language processing (NLP) tasks.
However, two major limitations hinder its potential applications: (1) the
inflexibility of finetuning on downstream tasks and (2) the lack of
interpretability in the decision-making process. To tackle these limitations,
we propose a novel framework that leverages the power of ChatGPT for specific
tasks, such as text classification, while improving its interpretability. The
proposed framework conducts a knowledge graph extraction task to extract
refined and structural knowledge from the raw data using ChatGPT. The rich
knowledge is then converted into a graph, which is further used to train an
interpretable linear classifier to make predictions. To evaluate the
effectiveness of our proposed method, we conduct experiments on four datasets.
The result shows that our method can significantly improve the performance
compared to directly utilizing ChatGPT for text classification tasks. And our
method provides a more transparent decision-making process compared with
previous text classification methods.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.03513v2},
File          = {2305.03513v2.pdf}
}
@article{2306.01540v1,
Author        = {Ayush Agrawal and Raghav Arora and Ahana Datta and Snehasis Banerjee and Brojeshwar Bhowmick and Krishna Murthy Jatavallabhula and Mohan Sridharan and Madhava Krishna},
Title         = {CLIPGraphs: Multimodal Graph Networks to Infer Object-Room Affinities},
Eprint        = {2306.01540v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {This paper introduces a novel method for determining the best room to place
an object in, for embodied scene rearrangement. While state-of-the-art
approaches rely on large language models (LLMs) or reinforcement learned (RL)
policies for this task, our approach, CLIPGraphs, efficiently combines
commonsense domain knowledge, data-driven methods, and recent advances in
multimodal learning. Specifically, it (a)encodes a knowledge graph of prior
human preferences about the room location of different objects in home
environments, (b) incorporates vision-language features to support multimodal
queries based on images or text, and (c) uses a graph network to learn
object-room affinities based on embeddings of the prior knowledge and the
vision-language features. We demonstrate that our approach provides better
estimates of the most appropriate location of objects from a benchmark set of
object categories in comparison with state-of-the-art baselines},
Year          = {2023},
Month         = {Jun},
Note          = {RO-MAN 2023 Conference},
Url           = {http://arxiv.org/abs/2306.01540v1},
File          = {2306.01540v1.pdf}
}
@article{2402.03339v1,
Author        = {Fei Ni and Bingyan Wang and Rongpeng Li and Zhifeng Zhao and Honggang Zhang},
Title         = {Interplay of Semantic Communication and Knowledge Learning},
Eprint        = {2402.03339v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In the swiftly advancing realm of communication technologies, Semantic
Communication (SemCom), which emphasizes knowledge understanding and
processing, has emerged as a hot topic. By integrating artificial intelligence
technologies, SemCom facilitates a profound understanding, analysis and
transmission of communication content. In this chapter, we clarify the means of
knowledge learning in SemCom with a particular focus on the utilization of
Knowledge Graphs (KGs). Specifically, we first review existing efforts that
combine SemCom with knowledge learning. Subsequently, we introduce a
KG-enhanced SemCom system, wherein the receiver is carefully calibrated to
leverage knowledge from its static knowledge base for ameliorating the decoding
performance. Contingent upon this framework, we further explore potential
approaches that can empower the system to operate in evolving knowledge base
more effectively. Furthermore, we investigate the possibility of integration
with Large Language Models (LLMs) for data augmentation, offering additional
perspective into the potential implementation means of SemCom. Extensive
numerical results demonstrate that the proposed framework yields superior
performance on top of the KG-enhanced decoding and manifests its versatility
under different scenarios.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2402.03339v1},
File          = {2402.03339v1.pdf}
}
@article{2403.11073v1,
Author        = {Haoxi Zhang and Xinxu Zhang and Yuanxin Lin and Maiqi Wang and Yi Lai and Yu Wang and Linfeng Yu and Yufeng Xu and Ran Cheng and Edward Szczerbicki},
Title         = {Tokensome: Towards a Genetic Vision-Language GPT for Explainable and
  Cognitive Karyotyping},
Eprint        = {2403.11073v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Automatic karyotype analysis is often defined as a visual perception task
focused solely on chromosomal object-level modeling. This definition has led
most existing methods to overlook componential and holistic information,
significantly constraining model performance. Moreover, the lack of
interpretability in current technologies hinders clinical adoption. In this
paper, we introduce Tokensome, a novel vision-language model based on
chromosome tokenization for explainable and cognitive karyotyping. Tokensome
elevates the method from the conventional visual perception layer to the
cognitive decision-making layer. This elevation enables the integration of
domain knowledge and cognitive reasoning via knowledge graphs and LLMs,
markedly enhancing model's explainability and facilitating abnormality
detection.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.11073v1},
File          = {2403.11073v1.pdf}
}
@article{2404.18428v1,
Author        = {Jiayang Wu and Wensheng Gan and Han-Chieh Chao and Philip S. Yu},
Title         = {Geospatial Big Data: Survey and Challenges},
Eprint        = {2404.18428v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {In recent years, geospatial big data (GBD) has obtained attention across
various disciplines, categorized into big earth observation data and big human
behavior data. Identifying geospatial patterns from GBD has been a vital
research focus in the fields of urban management and environmental
sustainability. This paper reviews the evolution of GBD mining and its
integration with advanced artificial intelligence (AI) techniques. GBD consists
of data generated by satellites, sensors, mobile devices, and geographical
information systems, and we categorize geospatial data based on different
perspectives. We outline the process of GBD mining and demonstrate how it can
be incorporated into a unified framework. Additionally, we explore new
technologies like large language models (LLM), the Metaverse, and knowledge
graphs, and how they could make GBD even more useful. We also share examples of
GBD helping with city management and protecting the environment. Finally, we
discuss the real challenges that come up when working with GBD, such as issues
with data retrieval and security. Our goal is to give readers a clear view of
where GBD mining stands today and where it might go next.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.18428v1},
File          = {2404.18428v1.pdf}
}
@article{2406.11160v3,
Author        = {Chengjin Xu and Muzhi Li and Cehao Yang and Xuhui Jiang and Lumingyuan Tang and Yiyan Qi and Jian Guo},
Title         = {Context Graph},
Eprint        = {2406.11160v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge Graphs (KGs) are foundational structures in many AI applications,
representing entities and their interrelations through triples. However,
triple-based KGs lack the contextual information of relational knowledge, like
temporal dynamics and provenance details, which are crucial for comprehensive
knowledge representation and effective reasoning. Instead, \textbf{Context
Graphs} (CGs) expand upon the conventional structure by incorporating
additional information such as time validity, geographic location, and source
provenance. This integration provides a more nuanced and accurate understanding
of knowledge, enabling KGs to offer richer insights and support more
sophisticated reasoning processes. In this work, we first discuss the inherent
limitations of triple-based KGs and introduce the concept of CGs, highlighting
their advantages in knowledge representation and reasoning. We then present a
context graph reasoning \textbf{CGR$^3$} paradigm that leverages large language
models (LLMs) to retrieve candidate entities and related contexts, rank them
based on the retrieved information, and reason whether sufficient information
has been obtained to answer a query. Our experimental results demonstrate that
CGR$^3$ significantly improves performance on KG completion (KGC) and KG
question answering (KGQA) tasks, validating the effectiveness of incorporating
contextual information on KG representation and reasoning.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.11160v3},
File          = {2406.11160v3.pdf}
}
@article{2408.02056v1,
Author        = {Gleb Kumichev and Pavel Blinov and Yulia Kuzkina and Vasily Goncharov and Galina Zubkova and Nikolai Zenovkin and Aleksei Goncharov and Andrey Savchenko},
Title         = {MedSyn: LLM-based Synthetic Medical Text Generation Framework},
Eprint        = {2408.02056v1},
DOI           = {10.1007/978-3-031-70381-2_14},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Generating synthetic text addresses the challenge of data availability in
privacy-sensitive domains such as healthcare. This study explores the
applicability of synthetic data in real-world medical settings. We introduce
MedSyn, a novel medical text generation framework that integrates large
language models with a Medical Knowledge Graph (MKG). We use MKG to sample
prior medical information for the prompt and generate synthetic clinical notes
with GPT-4 and fine-tuned LLaMA models. We assess the benefit of synthetic data
through application in the ICD code prediction task. Our research indicates
that synthetic data can increase the classification accuracy of vital and
challenging codes by up to 17.8% compared to settings without synthetic data.
Furthermore, to provide new data for further research in the healthcare domain,
we present the largest open-source synthetic dataset of clinical notes for the
Russian language, comprising over 41k samples covering 219 ICD-10 codes.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.02056v1},
File          = {2408.02056v1.pdf}
}
@article{2408.02337v1,
Author        = {Albert Sawczyn and Katsiaryna Viarenich and Konrad Wojtasik and Aleksandra Domogała and Marcin Oleksy and Maciej Piasecki and Tomasz Kajdanowicz},
Title         = {Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR
  Dataset Construction},
Eprint        = {2408.02337v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Advancements in AI and natural language processing have revolutionized
machine-human language interactions, with question answering (QA) systems
playing a pivotal role. The knowledge base question answering (KBQA) task,
utilizing structured knowledge graphs (KG), allows for handling extensive
knowledge-intensive questions. However, a significant gap exists in KBQA
datasets, especially for low-resource languages. Many existing construction
pipelines for these datasets are outdated and inefficient in human labor, and
modern assisting tools like Large Language Models (LLM) are not utilized to
reduce the workload. To address this, we have designed and implemented a
modern, semi-automated approach for creating datasets, encompassing tasks such
as KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR),
tailored explicitly for low-resource environments. We executed this pipeline
and introduced the PUGG dataset, the first Polish KBQA dataset, and novel
datasets for MRC and IR. Additionally, we provide a comprehensive
implementation, insightful findings, detailed statistics, and evaluation of
baseline models.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.02337v1},
File          = {2408.02337v1.pdf}
}
@article{2409.09362v1,
Author        = {Yuanjie Lyu and Tong Xu and Zihan Niu and Bo Peng and Jing Ke and Enhong Chen},
Title         = {Generating Event-oriented Attribution for Movies via Two-Stage
  Prefix-Enhanced Multimodal LLM},
Eprint        = {2409.09362v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The prosperity of social media platforms has raised the urgent demand for
semantic-rich services, e.g., event and storyline attribution. However, most
existing research focuses on clip-level event understanding, primarily through
basic captioning tasks, without analyzing the causes of events across an entire
movie. This is a significant challenge, as even advanced multimodal large
language models (MLLMs) struggle with extensive multimodal information due to
limited context length. To address this issue, we propose a Two-Stage
Prefix-Enhanced MLLM (TSPE) approach for event attribution, i.e., connecting
associated events with their causal semantics, in movie videos. In the local
stage, we introduce an interaction-aware prefix that guides the model to focus
on the relevant multimodal information within a single clip, briefly
summarizing the single event. Correspondingly, in the global stage, we
strengthen the connections between associated events using an inferential
knowledge graph, and design an event-aware prefix that directs the model to
focus on associated events rather than all preceding clips, resulting in
accurate event attribution. Comprehensive evaluations of two real-world
datasets demonstrate that our framework outperforms state-of-the-art methods.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.09362v1},
File          = {2409.09362v1.pdf}
}
@article{2410.03264v1,
Author        = {SeungHeon Doh and Minhee Lee and Dasaem Jeong and Juhan Nam},
Title         = {Enriching Music Descriptions with a Finetuned-LLM and Metadata for
  Text-to-Music Retrieval},
Eprint        = {2410.03264v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SD},
Abstract      = {Text-to-Music Retrieval, finding music based on a given natural language
query, plays a pivotal role in content discovery within extensive music
databases. To address this challenge, prior research has predominantly focused
on a joint embedding of music audio and text, utilizing it to retrieve music
tracks that exactly match descriptive queries related to musical attributes
(i.e. genre, instrument) and contextual elements (i.e. mood, theme). However,
users also articulate a need to explore music that shares similarities with
their favorite tracks or artists, such as \textit{I need a similar track to
Superstition by Stevie Wonder}. To address these concerns, this paper proposes
an improved Text-to-Music Retrieval model, denoted as TTMR++, which utilizes
rich text descriptions generated with a finetuned large language model and
metadata. To accomplish this, we obtained various types of seed text from
several existing music tag and caption datasets and a knowledge graph dataset
of artists and tracks. The experimental results show the effectiveness of
TTMR++ in comparison to state-of-the-art music-text joint embedding models
through a comprehensive evaluation involving various musical text queries.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.03264v1},
File          = {2410.03264v1.pdf}
}
@article{2410.19727v1,
Author        = {Nicole Cho and Nishan Srishankar and Lucas Cecchi and William Watson},
Title         = {FISHNET: Financial Intelligence from Sub-querying, Harmonizing,
  Neural-Conditioning, Expert Swarms, and Task Planning},
Eprint        = {2410.19727v1},
DOI           = {10.1145/3677052.3698597},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Financial intelligence generation from vast data sources has typically relied
on traditional methods of knowledge-graph construction or database engineering.
Recently, fine-tuned financial domain-specific Large Language Models (LLMs),
have emerged. While these advancements are promising, limitations such as high
inference costs, hallucinations, and the complexity of concurrently analyzing
high-dimensional financial data, emerge. This motivates our invention FISHNET
(Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning,
Expert swarming, and Task planning), an agentic architecture that accomplishes
highly complex analytical tasks for more than 98,000 regulatory filings that
vary immensely in terms of semantics, data hierarchy, or format. FISHNET shows
remarkable performance for financial insight generation (61.8% success rate
over 5.0% Routing, 45.6% RAG R-Precision). We conduct rigorous ablations to
empirically prove the success of FISHNET, each agent's importance, and the
optimized performance of assembling all agents. Our modular architecture can be
leveraged for a myriad of use-cases, enabling scalability, flexibility, and
data integrity that are critical for financial tasks.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.19727v1},
File          = {2410.19727v1.pdf}
}
@article{2411.05844v2,
Author        = {Yukun Cao and Zengyi Gao and Zhiyang Li and Xike Xie and Kevin Zhou and Jianliang Xu},
Title         = {LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation
  for Design Space Exploration},
Eprint        = {2411.05844v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {GraphRAG integrates (knowledge) graphs with large language models (LLMs) to
improve reasoning accuracy and contextual relevance. Despite its promising
applications and strong relevance to multiple research communities, such as
databases and natural language processing, GraphRAG currently lacks modular
workflow analysis, systematic solution frameworks, and insightful empirical
studies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework
that enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)
systematic classification of existing techniques and implemented GraphRAG
instances, and 3) creation of new GraphRAG instances. Our framework facilitates
comprehensive empirical studies of GraphRAG on large-scale real-world graphs
and diverse query sets, revealing insights into balancing reasoning quality,
runtime efficiency, and token or GPU cost, that are essential for building
advanced GraphRAG systems.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.05844v2},
File          = {2411.05844v2.pdf}
}
@article{2411.07965v3,
Author        = {Chuyi Kong and Ziyang Luo and Hongzhan Lin and Zhiyuan Fan and Yaxin Fan and Yuxi Sun and Jing Ma},
Title         = {SHARP: Unlocking Interactive Hallucination via Stance Transfer in
  Role-Playing Agents},
Eprint        = {2411.07965v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The advanced role-playing capabilities of Large Language Models (LLMs) have
paved the way for developing Role-Playing Agents (RPAs). However, existing
benchmarks in social interaction such as HPD and SocialBench have not
investigated hallucination and face limitations like poor generalizability and
implicit judgments for character fidelity. To address these issues, we propose
a generalizable, explicit and effective paradigm to unlock the interactive
patterns in diverse worldviews. Specifically, we define the interactive
hallucination based on stance transfer and construct a benchmark, SHARP, by
extracting relations from a general commonsense knowledge graph and leveraging
the inherent hallucination properties of RPAs to simulate interactions across
roles. Extensive experiments validate the effectiveness and stability of our
paradigm. Our findings further explore the factors influencing these metrics
and discuss the trade-off between blind loyalty to roles and adherence to facts
in RPAs.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.07965v3},
File          = {2411.07965v3.pdf}
}
@article{2412.04977v1,
Author        = {Allard Oelen and Mohamad Yaser Jaradeh and Sören Auer},
Title         = {ORKG ASK: a Neuro-symbolic Scholarly Search and Exploration System},
Eprint        = {2412.04977v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Purpose: Finding scholarly articles is a time-consuming and cumbersome
activity, yet crucial for conducting science. Due to the growing number of
scholarly articles, new scholarly search systems are needed to effectively
assist researchers in finding relevant literature.
  Methodology: We take a neuro-symbolic approach to scholarly search and
exploration by leveraging state-of-the-art components, including semantic
search, Large Language Models (LLMs), and Knowledge Graphs (KGs). The semantic
search component composes a set of relevant articles. From this set of
articles, information is extracted and presented to the user.
  Findings: The presented system, called ORKG ASK (Assistant for Scientific
Knowledge), provides a production-ready search and exploration system. Our
preliminary evaluation indicates that our proposed approach is indeed suitable
for the task of scholarly information retrieval.
  Value: With ORKG ASK, we present a next-generation scholarly search and
exploration system and make it available online. Additionally, the system
components are open source with a permissive license.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.04977v1},
File          = {2412.04977v1.pdf}
}
@article{2412.08864v1,
Author        = {Jiankang Wang and Jianjun Xu and Xiaorui Wang and Yuxin Wang and Mengting Xing and Shancheng Fang and Zhineng Chen and Hongtao Xie and Yongdong Zhang},
Title         = {A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning
  Instructions},
Eprint        = {2412.08864v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Synthesizing high-quality reasoning data for continual training has been
proven to be effective in enhancing the performance of Large Language Models
(LLMs). However, previous synthetic approaches struggle to easily scale up data
and incur high costs in the pursuit of high quality. In this paper, we propose
the Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable
framework for high-quality reasoning data synthesis. Inspired by knowledge
graphs, we extracted knowledge points from seed data and constructed a
knowledge point relationships graph to explore their interconnections. By
exploring the implicit relationships among knowledge, our method achieves
$\times$255 data expansion. Furthermore, GSDP led by open-source models,
achieves synthesis quality comparable to GPT-4-0613 while maintaining
$\times$100 lower costs. To tackle the most challenging mathematical reasoning
task, we present the GSDP-MATH dataset comprising over 1.91 million pairs of
math problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on
Mistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating
the effectiveness of our method. The dataset and models trained in this paper
will be available.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.08864v1},
File          = {2412.08864v1.pdf}
}
@article{2412.18644v3,
Author        = {Karishma Thakrar},
Title         = {DynaGRAG | Exploring the Topology of Information for Advancing Language
  Understanding and Generation in Graph Retrieval-Augmented Generation},
Eprint        = {2412.18644v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to
enhance language understanding and generation by leveraging external knowledge.
However, effectively capturing and integrating the rich semantic information
present in textual and structured data remains a challenge. To address this, a
novel GRAG framework, Dynamic Graph Retrieval-Agumented Generation (DynaGRAG),
is proposed to focus on enhancing subgraph representation and diversity within
the knowledge graph. By improving graph density, capturing entity and relation
information more effectively, and dynamically prioritizing relevant and diverse
subgraphs and information within them, the proposed approach enables a more
comprehensive understanding of the underlying semantic structure. This is
achieved through a combination of de-duplication processes, two-step mean
pooling of embeddings, query-aware retrieval considering unique nodes, and a
Dynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph
Convolutional Networks (GCNs) and Large Language Models (LLMs) through hard
prompting further enhances the learning of rich node and edge representations
while preserving the hierarchical subgraph structure. Experimental results
demonstrate the effectiveness of DynaGRAG, showcasing the significance of
enhanced subgraph representation and diversity for improved language
understanding and generation.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.18644v3},
File          = {2412.18644v3.pdf}
}
@article{2502.01549v1,
Author        = {Xubin Ren and Lingrui Xu and Long Xia and Shuaiqiang Wang and Dawei Yin and Chao Huang},
Title         = {VideoRAG: Retrieval-Augmented Generation with Extreme Long-Context
  Videos},
Eprint        = {2502.01549v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Retrieval-Augmented Generation (RAG) has demonstrated remarkable success in
enhancing Large Language Models (LLMs) through external knowledge integration,
yet its application has primarily focused on textual content, leaving the rich
domain of multi-modal video knowledge predominantly unexplored. This paper
introduces VideoRAG, the first retrieval-augmented generation framework
specifically designed for processing and understanding extremely long-context
videos. Our core innovation lies in its dual-channel architecture that
seamlessly integrates (i) graph-based textual knowledge grounding for capturing
cross-video semantic relationships, and (ii) multi-modal context encoding for
efficiently preserving visual features. This novel design empowers VideoRAG to
process unlimited-length videos by constructing precise knowledge graphs that
span multiple videos while maintaining semantic dependencies through
specialized multi-modal retrieval paradigms. Through comprehensive empirical
evaluation on our proposed LongerVideos benchmark-comprising over 160 videos
totaling 134+ hours across lecture, documentary, and entertainment
categories-VideoRAG demonstrates substantial performance compared to existing
RAG alternatives and long video understanding methods. The source code of
VideoRAG implementation and the benchmark dataset are openly available at:
https://github.com/HKUDS/VideoRAG.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.01549v1},
File          = {2502.01549v1.pdf}
}
@article{2307.10778v1,
Author        = {Jens-Joris Decorte and Severine Verlinden and Jeroen Van Hautte and Johannes Deleu and Chris Develder and Thomas Demeester},
Title         = {Extreme Multi-Label Skill Extraction Training using Large Language
  Models},
Eprint        = {2307.10778v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Online job ads serve as a valuable source of information for skill
requirements, playing a crucial role in labor market analysis and e-recruitment
processes. Since such ads are typically formatted in free text, natural
language processing (NLP) technologies are required to automatically process
them. We specifically focus on the task of detecting skills (mentioned
literally, or implicitly described) and linking them to a large skill ontology,
making it a challenging case of extreme multi-label classification (XMLC).
Given that there is no sizable labeled (training) dataset are available for
this specific XMLC task, we propose techniques to leverage general Large
Language Models (LLMs). We describe a cost-effective approach to generate an
accurate, fully synthetic labeled dataset for skill extraction, and present a
contrastive learning strategy that proves effective in the task. Our results
across three skill extraction benchmarks show a consistent increase of between
15 to 25 percentage points in \textit{R-Precision@5} compared to previously
published results that relied solely on distant supervision through literal
matches.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.10778v1},
File          = {2307.10778v1.pdf}
}
@article{2401.10558v1,
Author        = {Rex W. Douglass and Thomas Leo Scherer and J. Andrés Gannon and Erik Gartzke},
Title         = {ICBeLLM: High Quality International Events Data with Open Source Large
  Language Models on Consumer Hardware},
Eprint        = {2401.10558v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.AP},
Abstract      = {The International Crises Behavior Events (ICBe) ontology provides high
coverage over the thoughts, communications, and actions that constitute
international relations. A major disadvantage of that level of detail is that
it requires large human capital costs to apply it manually to new texts.
Whether such an ontolgy is practical for international relations research given
limited human and financial resources is a pressing concern. We introduce a
working proof of concept showing that ICBe codings can be reliably extracted
from new texts using the current generation of open source large language
models (LLM) running on consumer grade computer hardware. Our solution requires
no finetuning and only limited prompt engineering. We detail our solution and
present benchmarks against the original ICBe codings. We conclude by discussing
the implications of very high quality event coding of any text being within
reach of individual researchers with limited resources.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.10558v1},
File          = {2401.10558v1.pdf}
}
@article{2010.12688v2,
Author        = {Oshin Agarwal and Heming Ge and Siamak Shakeri and Rami Al-Rfou},
Title         = {Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced
  Language Model Pre-training},
Eprint        = {2010.12688v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Prior work on Data-To-Text Generation, the task of converting knowledge graph
(KG) triples into natural text, focused on domain-specific benchmark datasets.
In this paper, however, we verbalize the entire English Wikidata KG, and
discuss the unique challenges associated with a broad, open-domain, large-scale
verbalization. We further show that verbalizing a comprehensive, encyclopedic
KG like Wikidata can be used to integrate structured KGs and natural language
corpora. In contrast to the many architectures that have been developed to
integrate these two sources, our approach converts the KG into natural text,
allowing it to be seamlessly integrated into existing language models. It
carries the further advantages of improved factual accuracy and reduced
toxicity in the resulting language model. We evaluate this approach by
augmenting the retrieval corpus in a retrieval language model and showing
significant improvements on the knowledge intensive tasks of open domain QA and
the LAMA knowledge probe.},
Year          = {2020},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2010.12688v2},
File          = {2010.12688v2.pdf}
}
@article{2205.10661v1,
Author        = {Jiarui Zhang and Filip Ilievski and Kaixin Ma and Jonathan Francis and Alessandro Oltramari},
Title         = {An Empirical Investigation of Commonsense Self-Supervision with
  Knowledge Graphs},
Eprint        = {2205.10661v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Self-supervision based on the information extracted from large knowledge
graphs has been shown to improve the generalization of language models, in
zero-shot evaluation on various downstream language reasoning tasks. Since
these improvements are reported in aggregate, however, little is known about
(i) how to select the appropriate knowledge for solid performance across tasks,
(ii) how to combine this knowledge with neural language models, and (iii) how
these pairings affect granular task performance. In this paper, we study the
effect of knowledge sampling strategies and sizes that can be used to generate
synthetic data for adapting language models. We study the effect of different
synthetic datasets on language models with various architectures and sizes. The
resulting models are evaluated against four task properties: domain overlap,
answer similarity, vocabulary overlap, and answer length. Our experiments show
that encoder-decoder models benefit from more data to learn from, whereas
sampling strategies that balance across different aspects yield best
performance. Most of the improvement occurs on questions with short answers and
dissimilar answer candidates, which corresponds to the characteristics of the
data used for pre-training.},
Year          = {2022},
Month         = {May},
Url           = {http://arxiv.org/abs/2205.10661v1},
File          = {2205.10661v1.pdf}
}
@article{2301.12810v1,
Author        = {Roi Cohen and Mor Geva and Jonathan Berant and Amir Globerson},
Title         = {Crawling the Internal Knowledge-Base of Language Models},
Eprint        = {2301.12810v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Language models are trained on large volumes of text, and as a result their
parameters might contain a significant body of factual knowledge. Any
downstream task performed by these models implicitly builds on these facts, and
thus it is highly desirable to have means for representing this body of
knowledge in an interpretable way. However, there is currently no mechanism for
such a representation. Here, we propose to address this goal by extracting a
knowledge-graph of facts from a given language model. We describe a procedure
for ``crawling'' the internal knowledge-base of a language model. Specifically,
given a seed entity, we expand a knowledge-graph around it. The crawling
procedure is decomposed into sub-tasks, realized through specially designed
prompts that control for both precision (i.e., that no wrong facts are
generated) and recall (i.e., the number of facts generated). We evaluate our
approach on graphs crawled starting from dozens of seed entities, and show it
yields high precision graphs (82-92%), while emitting a reasonable number of
facts per entity.},
Year          = {2023},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2301.12810v1},
File          = {2301.12810v1.pdf}
}
@article{2404.12827v2,
Author        = {Anthony Yazdani and Alban Bornet and Philipp Khlebnikov and Boya Zhang and Hossein Rouhizadeh and Poorya Amini and Douglas Teodoro},
Title         = {CT-ADE: An Evaluation Benchmark for Adverse Drug Event Prediction from
  Clinical Trial Results},
Eprint        = {2404.12827v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Adverse drug events (ADEs) significantly impact clinical research, causing
many clinical trial failures. ADE prediction is key for developing safer
medications and enhancing patient outcomes. To support this effort, we
introduce CT-ADE, a dataset for multilabel predictive modeling of ADEs in
monopharmacy treatments. CT-ADE integrates data from 2,497 unique drugs,
encompassing 168,984 drug-ADE pairs extracted from clinical trials, annotated
with patient and contextual information, and comprehensive ADE concepts
standardized across multiple levels of the MedDRA ontology. Preliminary
analyses with large language models (LLMs) achieved F1-scores up to 55.90%.
Models using patient and contextual information showed F1-score improvements of
21%-38% over models using only chemical structure data. Our results highlight
the importance of target population and treatment regimens in the predictive
modeling of ADEs, offering greater performance gains than LLM domain
specialization and scaling. CT-ADE provides an essential tool for researchers
aiming to leverage artificial intelligence and machine learning to enhance
patient safety and minimize the impact of ADEs on pharmaceutical research and
development. The dataset is publicly accessible at
https://github.com/ds4dh/CT-ADE.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.12827v2},
File          = {2404.12827v2.pdf}
}
@article{2410.15238v1,
Author        = {Zachary Sheldon and Peeyush Kumar},
Title         = {Economic Anthropology in the Era of Generative Artificial Intelligence},
Eprint        = {2410.15238v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper explores the intersection of economic anthropology and generative
artificial intelligence (GenAI). It examines how large language models (LLMs)
can simulate human decision-making and the inductive biases present in AI
research. The study introduces two AI models: C.A.L.L.O.N. (Conventionally
Average Late Liberal ONtology) and M.A.U.S.S. (More Accurate Understanding of
Society and its Symbols). The former is trained on standard data, while the
latter is adapted with anthropological knowledge. The research highlights how
anthropological training can enhance LLMs' ability to recognize diverse
economic systems and concepts. The findings suggest that integrating economic
anthropology with AI can provide a more pluralistic understanding of economics
and improve the sustainability of non-market economic systems.},
Year          = {2024},
Month         = {Oct},
Note          = {Annual Joint Meeting of Society of Economic Anthropology and
  Society for the Anthropology of Work 2024},
Url           = {http://arxiv.org/abs/2410.15238v1},
File          = {2410.15238v1.pdf}
}
@article{1910.02915v2,
Author        = {Chaitanya Malaviya and Chandra Bhagavatula and Antoine Bosselut and Yejin Choi},
Title         = {Commonsense Knowledge Base Completion with Structural and Semantic
  Context},
Eprint        = {1910.02915v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Automatic KB completion for commonsense knowledge graphs (e.g., ATOMIC and
ConceptNet) poses unique challenges compared to the much studied conventional
knowledge bases (e.g., Freebase). Commonsense knowledge graphs use free-form
text to represent nodes, resulting in orders of magnitude more nodes compared
to conventional KBs (18x more nodes in ATOMIC compared to Freebase
(FB15K-237)). Importantly, this implies significantly sparser graph structures
- a major challenge for existing KB completion methods that assume densely
connected graphs over a relatively smaller set of nodes. In this paper, we
present novel KB completion models that can address these challenges by
exploiting the structural and semantic context of nodes. Specifically, we
investigate two key ideas: (1) learning from local graph structure, using graph
convolutional networks and automatic graph densification and (2) transfer
learning from pre-trained language models to knowledge graphs for enhanced
contextual representation of knowledge. We describe our method to incorporate
information from both these sources in a joint model and provide the first
empirical results for KB completion on ATOMIC and evaluation with ranking
metrics on ConceptNet. Our results demonstrate the effectiveness of language
model representations in boosting link prediction performance and the
advantages of learning from local graph structure (+1.5 points in MRR for
ConceptNet) when training on subgraphs for computational efficiency. Further
analysis on model predictions shines light on the types of commonsense
knowledge that language models capture well.},
Year          = {2019},
Month         = {Oct},
Url           = {http://arxiv.org/abs/1910.02915v2},
File          = {1910.02915v2.pdf}
}
@article{2308.13676v1,
Author        = {Vishwas Mruthyunjaya and Pouya Pezeshkpour and Estevam Hruschka and Nikita Bhutani},
Title         = {Rethinking Language Models as Symbolic Knowledge Graphs},
Eprint        = {2308.13676v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Symbolic knowledge graphs (KGs) play a pivotal role in knowledge-centric
applications such as search, question answering and recommendation. As
contemporary language models (LMs) trained on extensive textual data have
gained prominence, researchers have extensively explored whether the parametric
knowledge within these models can match up to that present in knowledge graphs.
Various methodologies have indicated that enhancing the size of the model or
the volume of training data enhances its capacity to retrieve symbolic
knowledge, often with minimal or no human supervision. Despite these
advancements, there is a void in comprehensively evaluating whether LMs can
encompass the intricate topological and semantic attributes of KGs, attributes
crucial for reasoning processes. In this work, we provide an exhaustive
evaluation of language models of varying sizes and capabilities. We construct
nine qualitative benchmarks that encompass a spectrum of attributes including
symmetry, asymmetry, hierarchy, bidirectionality, compositionality, paths,
entity-centricity, bias and ambiguity. Additionally, we propose novel
evaluation metrics tailored for each of these attributes. Our extensive
evaluation of various LMs shows that while these models exhibit considerable
potential in recalling factual information, their ability to capture intricate
topological and semantic traits of KGs remains significantly constrained. We
note that our proposed evaluation metrics are more reliable in evaluating these
abilities than the existing metrics. Lastly, some of our benchmarks challenge
the common notion that larger LMs (e.g., GPT-4) universally outshine their
smaller counterparts (e.g., BERT).},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.13676v1},
File          = {2308.13676v1.pdf}
}
@article{2310.16452v3,
Author        = {Giacomo Balloccu and Ludovico Boratto and Christian Cancedda and Gianni Fenu and Mirko Marras},
Title         = {Faithful Path Language Modeling for Explainable Recommendation over
  Knowledge Graph},
Eprint        = {2310.16452v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The integration of path reasoning with language modeling in recommender
systems has shown promise for enhancing explainability but often struggles with
the authenticity of the explanations provided. Traditional models modify their
architecture to produce entities and relations alternately--for example,
employing separate heads for each in the model--which does not ensure the
authenticity of paths reflective of actual Knowledge Graph (KG) connections.
This misalignment can lead to user distrust due to the generation of corrupted
paths. Addressing this, we introduce PEARLM (Path-based Explainable-Accurate
Recommender based on Language Modelling), which innovates with a Knowledge
Graph Constraint Decoding (KGCD) mechanism. This mechanism ensures zero
incidence of corrupted paths by enforcing adherence to valid KG connections at
the decoding level, agnostic of the underlying model architecture. By
integrating direct token embedding learning from KG paths, PEARLM not only
guarantees the generation of plausible and verifiable explanations but also
highly enhances recommendation accuracy. We validate the effectiveness of our
approach through a rigorous empirical assessment, employing a newly proposed
metric that quantifies the integrity of explanation paths. Our results
demonstrate a significant improvement over existing methods, effectively
eliminating the generation of inaccurate paths and advancing the
state-of-the-art in explainable recommender systems.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.16452v3},
File          = {2310.16452v3.pdf}
}
@article{2311.16119v3,
Author        = {Sander Schulhoff and Jeremy Pinto and Anaum Khan and Louis-François Bouchard and Chenglei Si and Svetlina Anati and Valen Tagliabue and Anson Liu Kost and Christopher Carnahan and Jordan Boyd-Graber},
Title         = {Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of
  LLMs through a Global Scale Prompt Hacking Competition},
Eprint        = {2311.16119v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Large Language Models (LLMs) are deployed in interactive contexts with direct
user engagement, such as chatbots and writing assistants. These deployments are
vulnerable to prompt injection and jailbreaking (collectively, prompt hacking),
in which models are manipulated to ignore their original instructions and
follow potentially malicious ones. Although widely acknowledged as a
significant security threat, there is a dearth of large-scale resources and
quantitative studies on prompt hacking. To address this lacuna, we launch a
global prompt hacking competition, which allows for free-form human input
attacks. We elicit 600K+ adversarial prompts against three state-of-the-art
LLMs. We describe the dataset, which empirically verifies that current LLMs can
indeed be manipulated via prompt hacking. We also present a comprehensive
taxonomical ontology of the types of adversarial prompts.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2311.16119v3},
File          = {2311.16119v3.pdf}
}
@article{2405.20526v1,
Author        = {Steven Moore and Robin Schmucker and Tom Mitchell and John Stamper},
Title         = {Automated Generation and Tagging of Knowledge Components from
  Multiple-Choice Questions},
Eprint        = {2405.20526v1},
DOI           = {10.1145/3657604.3662030},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge Components (KCs) linked to assessments enhance the measurement of
student learning, enrich analytics, and facilitate adaptivity. However,
generating and linking KCs to assessment items requires significant effort and
domain-specific knowledge. To streamline this process for higher-education
courses, we employed GPT-4 to generate KCs for multiple-choice questions (MCQs)
in Chemistry and E-Learning. We analyzed discrepancies between the KCs
generated by the Large Language Model (LLM) and those made by humans through
evaluation from three domain experts in each subject area. This evaluation
aimed to determine whether, in instances of non-matching KCs, evaluators showed
a preference for the LLM-generated KCs over their human-created counterparts.
We also developed an ontology induction algorithm to cluster questions that
assess similar KCs based on their content. Our most effective LLM strategy
accurately matched KCs for 56% of Chemistry and 35% of E-Learning MCQs, with
even higher success when considering the top five KC suggestions. Human
evaluators favored LLM-generated KCs, choosing them over human-assigned ones
approximately two-thirds of the time, a preference that was statistically
significant across both domains. Our clustering algorithm successfully grouped
questions by their underlying KCs without needing explicit labels or contextual
information. This research advances the automation of KC generation and
classification for assessment items, alleviating the need for student data or
predefined KC labels.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.20526v1},
File          = {2405.20526v1.pdf}
}
@article{2406.15859v2,
Author        = {Guangsi Shi and Xiaofeng Deng and Linhao Luo and Lijuan Xia and Lei Bao and Bei Ye and Fei Du and Shirui Pan and Yuxiao Li},
Title         = {LLM-Powered Explanations: Unraveling Recommendations Through Subgraph
  Reasoning},
Eprint        = {2406.15859v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Recommender systems are pivotal in enhancing user experiences across various
web applications by analyzing the complicated relationships between users and
items. Knowledge graphs(KGs) have been widely used to enhance the performance
of recommender systems. However, KGs are known to be noisy and incomplete,
which are hard to provide reliable explanations for recommendation results. An
explainable recommender system is crucial for the product development and
subsequent decision-making. To address these challenges, we introduce a novel
recommender that synergies Large Language Models (LLMs) and KGs to enhance the
recommendation and provide interpretable results. Specifically, we first
harness the power of LLMs to augment KG reconstruction. LLMs comprehend and
decompose user reviews into new triples that are added into KG. In this way, we
can enrich KGs with explainable paths that express user preferences. To enhance
the recommendation on augmented KGs, we introduce a novel subgraph reasoning
module that effectively measures the importance of nodes and discovers
reasoning for recommendation. Finally, these reasoning paths are fed into the
LLMs to generate interpretable explanations of the recommendation results. Our
approach significantly enhances both the effectiveness and interpretability of
recommender systems, especially in cross-selling scenarios where traditional
methods falter. The effectiveness of our approach has been rigorously tested on
four open real-world datasets, with our methods demonstrating a superior
performance over contemporary state-of-the-art techniques by an average
improvement of 12%. The application of our model in a multinational engineering
and technology company cross-selling recommendation system further underscores
its practical utility and potential to redefine recommendation practices
through improved accuracy and user trust.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.15859v2},
File          = {2406.15859v2.pdf}
}
@article{2301.05880v3,
Author        = {Hongpeng Lin and Ludan Ruan and Wenke Xia and Peiyu Liu and Jingyuan Wen and Yixin Xu and Di Hu and Ruihua Song and Wayne Xin Zhao and Qin Jin and Zhiwu Lu},
Title         = {TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real
  World},
Eprint        = {2301.05880v3},
DOI           = {10.1145/3581783.3612425},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {To facilitate the research on intelligent and human-like chatbots with
multi-modal context, we introduce a new video-based multi-modal dialogue
dataset, called TikTalk. We collect 38K videos from a popular video-sharing
platform, along with 367K conversations posted by users beneath them. Users
engage in spontaneous conversations based on their multi-modal experiences from
watching videos, which helps recreate real-world chitchat context. Compared to
previous multi-modal dialogue datasets, the richer context types in TikTalk
lead to more diverse conversations, but also increase the difficulty in
capturing human interests from intricate multi-modal information to generate
personalized responses. Moreover, external knowledge is more frequently evoked
in our dataset. These facts reveal new challenges for multi-modal dialogue
models. We quantitatively demonstrate the characteristics of TikTalk, propose a
video-based multi-modal chitchat task, and evaluate several dialogue baselines.
Experimental results indicate that the models incorporating large language
models (LLM) can generate more diverse responses, while the model utilizing
knowledge graphs to introduce external knowledge performs the best overall.
Furthermore, no existing model can solve all the above challenges well. There
is still a large room for future improvements, even for LLM with visual
extensions. Our dataset is available at
\url{https://ruc-aimind.github.io/projects/TikTalk/}.},
Year          = {2023},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2301.05880v3},
File          = {2301.05880v3.pdf}
}
@article{2310.08975v3,
Author        = {Haoran Luo and Haihong E and Zichen Tang and Shiyao Peng and Yikai Guo and Wentai Zhang and Chenghao Ma and Guanting Dong and Meina Song and Wei Lin and Yifan Zhu and Luu Anh Tuan},
Title         = {ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question
  Answering with Fine-tuned Large Language Models},
Eprint        = {2310.08975v3},
DOI           = {10.18653/v1/2024.findings-acl.122},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Base Question Answering (KBQA) aims to answer natural language
questions over large-scale knowledge bases (KBs), which can be summarized into
two crucial steps: knowledge retrieval and semantic parsing. However, three
core challenges remain: inefficient knowledge retrieval, mistakes of retrieval
adversely impacting semantic parsing, and the complexity of previous KBQA
methods. To tackle these challenges, we introduce ChatKBQA, a novel and simple
generate-then-retrieve KBQA framework, which proposes first generating the
logical form with fine-tuned LLMs, then retrieving and replacing entities and
relations with an unsupervised retrieval method, to improve both generation and
retrieval more directly. Experimental results show that ChatKBQA achieves new
state-of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This
work can also be regarded as a new paradigm for combining LLMs with knowledge
graphs (KGs) for interpretable and knowledge-required question answering. Our
code is publicly available.},
Year          = {2023},
Month         = {Oct},
Note          = {ACL 2024},
Url           = {http://arxiv.org/abs/2310.08975v3},
File          = {2310.08975v3.pdf}
}
@article{2312.11813v1,
Author        = {Fengli Xu and Jun Zhang and Chen Gao and Jie Feng and Yong Li},
Title         = {Urban Generative Intelligence (UGI): A Foundational Platform for Agents
  in Embodied City Environment},
Eprint        = {2312.11813v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Urban environments, characterized by their complex, multi-layered networks
encompassing physical, social, economic, and environmental dimensions, face
significant challenges in the face of rapid urbanization. These challenges,
ranging from traffic congestion and pollution to social inequality, call for
advanced technological interventions. Recent developments in big data,
artificial intelligence, urban computing, and digital twins have laid the
groundwork for sophisticated city modeling and simulation. However, a gap
persists between these technological capabilities and their practical
implementation in addressing urban challenges in an systemic-intelligent way.
This paper proposes Urban Generative Intelligence (UGI), a novel foundational
platform integrating Large Language Models (LLMs) into urban systems to foster
a new paradigm of urban intelligence. UGI leverages CityGPT, a foundation model
trained on city-specific multi-source data, to create embodied agents for
various urban tasks. These agents, operating within a textual urban environment
emulated by city simulator and urban knowledge graph, interact through a
natural language interface, offering an open platform for diverse intelligent
and embodied agent development. This platform not only addresses specific urban
issues but also simulates complex urban systems, providing a multidisciplinary
approach to understand and manage urban complexity. This work signifies a
transformative step in city science and urban intelligence, harnessing the
power of LLMs to unravel and address the intricate dynamics of urban systems.
The code repository with demonstrations will soon be released here
https://github.com/tsinghua-fib-lab/UGI.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.11813v1},
File          = {2312.11813v1.pdf}
}
@article{2402.07630v3,
Author        = {Xiaoxin He and Yijun Tian and Yifei Sun and Nitesh V. Chawla and Thomas Laurent and Yann LeCun and Xavier Bresson and Bryan Hooi},
Title         = {G-Retriever: Retrieval-Augmented Generation for Textual Graph
  Understanding and Question Answering},
Eprint        = {2402.07630v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Given a graph with textual attributes, we enable users to `chat with their
graph': that is, to ask questions about the graph using a conversational
interface. In response to a user's questions, our method provides textual
replies and highlights the relevant parts of the graph. While existing works
integrate large language models (LLMs) and graph neural networks (GNNs) in
various ways, they mostly focus on either conventional graph tasks (such as
node, edge, and graph classification), or on answering simple graph queries on
small or synthetic graphs. In contrast, we develop a flexible
question-answering framework targeting real-world textual graphs, applicable to
multiple applications including scene graph understanding, common sense
reasoning, and knowledge graph reasoning. Toward this goal, we first develop a
Graph Question Answering (GraphQA) benchmark with data collected from different
tasks. Then, we propose our G-Retriever method, introducing the first
retrieval-augmented generation (RAG) approach for general textual graphs, which
can be fine-tuned to enhance graph understanding via soft prompting. To resist
hallucination and to allow for textual graphs that greatly exceed the LLM's
context window size, G-Retriever performs RAG over a graph by formulating this
task as a Prize-Collecting Steiner Tree optimization problem. Empirical
evaluations show that our method outperforms baselines on textual graph tasks
from multiple domains, scales well with larger graph sizes, and mitigates
hallucination.~\footnote{Our codes and datasets are available at:
\url{https://github.com/XiaoxinHe/G-Retriever}}},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.07630v3},
File          = {2402.07630v3.pdf}
}
@article{2402.11034v2,
Author        = {Jannat Ara Meem and Muhammad Shihab Rashid and Yue Dong and Vagelis Hristidis},
Title         = {PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal
  Question-Answering},
Eprint        = {2402.11034v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Existing work on Temporal Question Answering (TQA) has predominantly focused
on questions anchored to specific timestamps or events (e.g. "Who was the US
president in 1970?"). Little work has studied questions whose temporal context
is relative to the present time (e.g. "Who was the previous US president?"). We
refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses
unique challenges: (1) large language models (LLMs) may have outdated
knowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are
hard to reason, (3) multi-hop reasoning may be required, and (4) the gold
answers of benchmarks must be continuously updated. To address these
challenges, we introduce the PAT-Questions benchmark, which includes single and
multi-hop temporal questions. The answers in PAT-Questions can be automatically
refreshed by re-running SPARQL queries on a knowledge graph, if available. We
evaluate several state-of-the-art LLMs and a SOTA temporal reasoning model
(TEMPREASON-T5) on PAT-Questions through direct prompting and
retrieval-augmented generation (RAG). The results highlight the limitations of
existing solutions in PATQA and motivate the need for new methods to improve
PATQA reasoning capabilities.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.11034v2},
File          = {2402.11034v2.pdf}
}
@article{2404.16130v1,
Author        = {Darren Edge and Ha Trinh and Newman Cheng and Joshua Bradley and Alex Chao and Apurva Mody and Steven Truitt and Jonathan Larson},
Title         = {From Local to Global: A Graph RAG Approach to Query-Focused
  Summarization},
Eprint        = {2404.16130v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The use of retrieval-augmented generation (RAG) to retrieve relevant
information from an external knowledge source enables large language models
(LLMs) to answer questions over private and/or previously unseen document
collections. However, RAG fails on global questions directed at an entire text
corpus, such as "What are the main themes in the dataset?", since this is
inherently a query-focused summarization (QFS) task, rather than an explicit
retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities
of text indexed by typical RAG systems. To combine the strengths of these
contrasting methods, we propose a Graph RAG approach to question answering over
private text corpora that scales with both the generality of user questions and
the quantity of source text to be indexed. Our approach uses an LLM to build a
graph-based text index in two stages: first to derive an entity knowledge graph
from the source documents, then to pregenerate community summaries for all
groups of closely-related entities. Given a question, each community summary is
used to generate a partial response, before all partial responses are again
summarized in a final response to the user. For a class of global sensemaking
questions over datasets in the 1 million token range, we show that Graph RAG
leads to substantial improvements over a na\"ive RAG baseline for both the
comprehensiveness and diversity of generated answers. An open-source,
Python-based implementation of both global and local Graph RAG approaches is
forthcoming at https://aka.ms/graphrag.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.16130v1},
File          = {2404.16130v1.pdf}
}
@article{2406.19967v1,
Author        = {Tzuf Paz-Argaman and John Palowitch and Sayali Kulkarni and Reut Tsarfaty and Jason Baldridge},
Title         = {Into the Unknown: Generating Geospatial Descriptions for New
  Environments},
Eprint        = {2406.19967v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Similar to vision-and-language navigation (VLN) tasks that focus on bridging
the gap between vision and language for embodied navigation, the new Rendezvous
(RVS) task requires reasoning over allocentric spatial relationships
(independent of the observer's viewpoint) using non-sequential navigation
instructions and maps. However, performance substantially drops in new
environments with no training data. Using opensource descriptions paired with
coordinates (e.g., Wikipedia) provides training data but suffers from limited
spatially-oriented text resulting in low geolocation resolution. We propose a
large-scale augmentation method for generating high-quality synthetic data for
new environments using readily available geospatial data. Our method constructs
a grounded knowledge-graph, capturing entity relationships. Sampled entities
and relations (`shop north of school') generate navigation instructions via (i)
generating numerous templates using context-free grammar (CFG) to embed
specific entities and relations; (ii) feeding the entities and relation into a
large language model (LLM) for instruction generation. A comprehensive
evaluation on RVS, showed that our approach improves the 100-meter accuracy by
45.83% on unseen environments. Furthermore, we demonstrate that models trained
with CFG-based augmentation achieve superior performance compared with those
trained with LLM-based augmentation, both in unseen and seen environments.
These findings suggest that the potential advantages of explicitly structuring
spatial information for text-based geospatial reasoning in previously unknown,
can unlock data-scarce scenarios.},
Year          = {2024},
Month         = {Jun},
Note          = {ACL 2024 Findings},
Url           = {http://arxiv.org/abs/2406.19967v1},
File          = {2406.19967v1.pdf}
}
@article{2407.18607v2,
Author        = {Anthony C. Constantinou and Neville K. Kitson and Alessio Zanga},
Title         = {Using GPT-4 to guide causal machine learning},
Eprint        = {2407.18607v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Since its introduction to the public, ChatGPT has had an unprecedented
impact. While some experts praised AI advancements and highlighted their
potential risks, others have been critical about the accuracy and usefulness of
Large Language Models (LLMs). In this paper, we are interested in the ability
of LLMs to identify causal relationships. We focus on the well-established
GPT-4 (Turbo) and evaluate its performance under the most restrictive
conditions, by isolating its ability to infer causal relationships based solely
on the variable labels without being given any other context by humans,
demonstrating the minimum level of effectiveness one can expect when it is
provided with label-only information. We show that questionnaire participants
judge the GPT-4 graphs as the most accurate in the evaluated categories,
closely followed by knowledge graphs constructed by domain experts, with causal
Machine Learning (ML) far behind. We use these results to highlight the
important limitation of causal ML, which often produces causal graphs that
violate common sense, affecting trust in them. However, we show that pairing
GPT-4 with causal ML overcomes this limitation, resulting in graphical
structures learnt from real data that align more closely with those identified
by domain experts, compared to structures learnt by causal ML alone. Overall,
our findings suggest that despite GPT-4 not being explicitly designed to reason
causally, it can still be a valuable tool for causal representation, as it
improves the causal discovery process of causal ML algorithms that are designed
to do just that.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.18607v2},
File          = {2407.18607v2.pdf}
}
@article{2408.05141v3,
Author        = {Ye Yuan and Chengwu Liu and Jingyang Yuan and Gongbo Sun and Siqi Li and Ming Zhang},
Title         = {A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning},
Eprint        = {2408.05141v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Retrieval-augmented generation (RAG) is a framework enabling large language
models (LLMs) to enhance their accuracy and reduce hallucinations by
integrating external knowledge bases. In this paper, we introduce a hybrid RAG
system enhanced through a comprehensive suite of optimizations that
significantly improve retrieval quality, augment reasoning capabilities, and
refine numerical computation ability. We refined the text chunks and tables in
web pages, added attribute predictors to reduce hallucinations, conducted LLM
Knowledge Extractor and Knowledge Graph Extractor, and finally built a
reasoning strategy with all the references. We evaluated our system on the CRAG
dataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and
online evaluations demonstrate that our system significantly enhances complex
reasoning capabilities. In local evaluations, we have significantly improved
accuracy and reduced error rates compared to the baseline model, achieving a
notable increase in scores. In the meanwhile, we have attained outstanding
results in online assessments, demonstrating the performance and generalization
capabilities of the proposed system. The source code for our system is released
in \url{https://gitlab.aicrowd.com/shizueyy/crag-new}.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.05141v3},
File          = {2408.05141v3.pdf}
}
@article{2408.12236v1,
Author        = {Yanzeng Li and Cheng Zeng and Jinchao Zhang and Jie Zhou and Lei Zou},
Title         = {MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for
  Dynamic Medical Image Generation in Virtual Simulated Patient},
Eprint        = {2408.12236v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Medical education relies heavily on Simulated Patients (SPs) to provide a
safe environment for students to practice clinical skills, including medical
image analysis. However, the high cost of recruiting qualified SPs and the lack
of diverse medical imaging datasets have presented significant challenges. To
address these issues, this paper introduces MedDiT, a novel
knowledge-controlled conversational framework that can dynamically generate
plausible medical images aligned with simulated patient symptoms, enabling
diverse diagnostic skill training. Specifically, MedDiT integrates various
patient Knowledge Graphs (KGs), which describe the attributes and symptoms of
patients, to dynamically prompt Large Language Models' (LLMs) behavior and
control the patient characteristics, mitigating hallucination during medical
conversation. Additionally, a well-tuned Diffusion Transformer (DiT) model is
incorporated to generate medical images according to the specified patient
attributes in the KG. In this paper, we present the capabilities of MedDiT
through a practical demonstration, showcasing its ability to act in diverse
simulated patient cases and generate the corresponding medical images. This can
provide an abundant and interactive learning experience for students, advancing
medical education by offering an immersive simulation platform for future
healthcare professionals. The work sheds light on the feasibility of
incorporating advanced technologies like LLM, KG, and DiT in education
applications, highlighting their potential to address the challenges faced in
simulated patient-based medical education.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.12236v1},
File          = {2408.12236v1.pdf}
}
@article{2410.08255v1,
Author        = {David D. Baek and Yuxiao Li and Max Tegmark},
Title         = {Generalization from Starvation: Hints of Universality in LLM Knowledge
  Graph Learning},
Eprint        = {2410.08255v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Motivated by interpretability and reliability, we investigate how neural
networks represent knowledge during graph learning, We find hints of
universality, where equivalent representations are learned across a range of
model sizes (from $10^2$ to $10^9$ parameters) and contexts (MLP toy models,
LLM in-context learning and LLM training). We show that these attractor
representations optimize generalization to unseen examples by exploiting
properties of knowledge graph relations (e.g. symmetry and meta-transitivity).
We find experimental support for such universality by showing that LLMs and
simpler neural networks can be stitched, i.e., by stitching the first part of
one model to the last part of another, mediated only by an affine or almost
affine transformation. We hypothesize that this dynamic toward simplicity and
generalization is driven by "intelligence from starvation": where overfitting
is minimized by pressure to minimize the use of resources that are either
scarce or competed for against other tasks.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.08255v1},
File          = {2410.08255v1.pdf}
}
@article{2303.14425v1,
Author        = {Zhouhong Gu and Sihang Jiang and Wenhao Huang and Jiaqing Liang and Hongwei Feng and Yanghua Xiao},
Title         = {Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For
  Language Model Synonym-Aware Pretraining},
Eprint        = {2303.14425v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The model's ability to understand synonymous expression is crucial in many
kinds of downstream tasks. It will make the model to better understand the
similarity between context, and more robust to the synonym substitution attack.
However, many Pretrained Language Model (PLM) lack synonym knowledge due to
limitation of small-scale synsets and PLM's pretraining objectives. In this
paper, we propose a framework called Sem4SAP to mine synsets from Open
Knowledge Graph (Open-KG) and using the mined synsets to do synonym-aware
pretraining for language models. We propose to coarsly filter the content in
Open-KG and use the frequency information to better help the clustering process
under low-resource unsupervised conditions. We expand the mined synsets by
migrating core semantics between synonymous expressions.We also propose two
novel and effective synonym-aware pre-training methods for injecting synonym
knowledge into PLMs.Extensive experiments demonstrate that Sem4SAP can
dramatically outperform the original PLMs and other baselines on ten different
tasks.},
Year          = {2023},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2303.14425v1},
File          = {2303.14425v1.pdf}
}
@article{2312.00874v1,
Author        = {Jingcong Liang and Rong Ye and Meng Han and Qi Zhang and Ruofei Lai and Xinyu Zhang and Zhao Cao and Xuanjing Huang and Zhongyu Wei},
Title         = {Hi-ArG: Exploring the Integration of Hierarchical Argumentation Graphs
  in Language Pretraining},
Eprint        = {2312.00874v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The knowledge graph is a structure to store and represent knowledge, and
recent studies have discussed its capability to assist language models for
various applications. Some variations of knowledge graphs aim to record
arguments and their relations for computational argumentation tasks. However,
many must simplify semantic types to fit specific schemas, thus losing
flexibility and expression ability. In this paper, we propose the Hierarchical
Argumentation Graph (Hi-ArG), a new structure to organize arguments. We also
introduce two approaches to exploit Hi-ArG, including a text-graph multi-modal
model GreaseArG and a new pre-training framework augmented with graph
information. Experiments on two argumentation tasks have shown that after
further pre-training and fine-tuning, GreaseArG supersedes same-scale language
models on these tasks, while incorporating graph information during further
pre-training can also improve the performance of vanilla language models. Code
for this paper is available at https://github.com/ljcleo/Hi-ArG .},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.00874v1},
File          = {2312.00874v1.pdf}
}
@article{2401.01711v1,
Author        = {Phillip Schneider and Manuel Klettner and Kristiina Jokinen and Elena Simperl and Florian Matthes},
Title         = {Evaluating Large Language Models in Semantic Parsing for Conversational
  Question Answering over Knowledge Graphs},
Eprint        = {2401.01711v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Conversational question answering systems often rely on semantic parsing to
enable interactive information retrieval, which involves the generation of
structured database queries from a natural language input. For
information-seeking conversations about facts stored within a knowledge graph,
dialogue utterances are transformed into graph queries in a process that is
called knowledge-based conversational question answering. This paper evaluates
the performance of large language models that have not been explicitly
pre-trained on this task. Through a series of experiments on an extensive
benchmark dataset, we compare models of varying sizes with different prompting
techniques and identify common issue types in the generated output. Our results
demonstrate that large language models are capable of generating graph queries
from dialogues, with significant improvements achievable through few-shot
prompting and fine-tuning techniques, especially for smaller models that
exhibit lower zero-shot performance.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.01711v1},
File          = {2401.01711v1.pdf}
}
@article{2401.16293v1,
Author        = {Andrés García-Silva and Cristian Berrío and José Manuel Gómez-Pérez},
Title         = {Textual Entailment for Effective Triple Validation in Object Prediction},
Eprint        = {2401.16293v1},
DOI           = {10.1007/978-3-031-47240-4_5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge base population seeks to expand knowledge graphs with facts that
are typically extracted from a text corpus. Recently, language models
pretrained on large corpora have been shown to contain factual knowledge that
can be retrieved using cloze-style strategies. Such approach enables zero-shot
recall of facts, showing competitive results in object prediction compared to
supervised baselines. However, prompt-based fact retrieval can be brittle and
heavily depend on the prompts and context used, which may produce results that
are unintended or hallucinatory.We propose to use textual entailment to
validate facts extracted from language models through cloze statements. Our
results show that triple validation based on textual entailment improves
language model predictions in different training regimes. Furthermore, we show
that entailment-based triple validation is also effective to validate candidate
facts extracted from other sources including existing knowledge graphs and text
passages where named entities are recognized.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.16293v1},
File          = {2401.16293v1.pdf}
}
@article{2403.07398v2,
Author        = {Tianqing Fang and Zeming Chen and Yangqiu Song and Antoine Bosselut},
Title         = {Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs},
Eprint        = {2403.07398v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Event commonsense reasoning requires the ability to reason about the
relationship between events, as well as infer implicit context underlying that
relationship. However, data scarcity makes it challenging for language models
to learn to generate commonsense inferences for contexts and questions
involving interactions between complex events. To address this demand, we
present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop
logical queries (e.g., the joint effect or cause of both event A and B, or the
effect of the effect of event C) from an existing commonsense knowledge graph
(CSKG), and verbalizing them using handcrafted rules and large language models
into multiple-choice and text generation questions. Our experiments show that
language models trained on COM2 exhibit significant improvements in complex
reasoning ability, resulting in enhanced zero-shot performance in both
in-domain and out-of-domain tasks for question answering and generative
commonsense reasoning, without expensive human annotations. Code and data are
available at https://github.com/tqfang/complex-commonsense-reasoning.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.07398v2},
File          = {2403.07398v2.pdf}
}
@article{2406.09994v1,
Author        = {Manas Jhalani and Annervaz K M and Pushpak Bhattacharyya},
Title         = {Precision Empowers, Excess Distracts: Visual Question Answering With
  Dynamically Infused Knowledge In Language Models},
Eprint        = {2406.09994v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In the realm of multimodal tasks, Visual Question Answering (VQA) plays a
crucial role by addressing natural language questions grounded in visual
content. Knowledge-Based Visual Question Answering (KBVQA) advances this
concept by adding external knowledge along with images to respond to questions.
We introduce an approach for KBVQA, augmenting the existing vision-language
transformer encoder-decoder (OFA) model. Our main contribution involves
enhancing questions by incorporating relevant external knowledge extracted from
knowledge graphs, using a dynamic triple extraction method. We supply a
flexible number of triples from the knowledge graph as context, tailored to
meet the requirements for answering the question. Our model, enriched with
knowledge, demonstrates an average improvement of 4.75\% in Exact Match Score
over the state-of-the-art on three different KBVQA datasets. Through
experiments and analysis, we demonstrate that furnishing variable triples for
each question improves the reasoning capabilities of the language model in
contrast to supplying a fixed number of triples. This is illustrated even for
recent large language models. Additionally, we highlight the model's
generalization capability by showcasing its SOTA-beating performance on a small
dataset, achieved through straightforward fine-tuning.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.09994v1},
File          = {2406.09994v1.pdf}
}
@article{2407.06041v1,
Author        = {Nikit Srivastava and Mengshi Ma and Daniel Vollmers and Hamada Zahera and Diego Moussallem and Axel-Cyrille Ngonga Ngomo},
Title         = {MST5 -- Multilingual Question Answering over Knowledge Graphs},
Eprint        = {2407.06041v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graph Question Answering (KGQA) simplifies querying vast amounts of
knowledge stored in a graph-based model using natural language. However, the
research has largely concentrated on English, putting non-English speakers at a
disadvantage. Meanwhile, existing multilingual KGQA systems face challenges in
achieving performance comparable to English systems, highlighting the
difficulty of generating SPARQL queries from diverse languages. In this
research, we propose a simplified approach to enhance multilingual KGQA systems
by incorporating linguistic context and entity information directly into the
processing pipeline of a language model. Unlike existing methods that rely on
separate encoders for integrating auxiliary information, our strategy leverages
a single, pretrained multilingual transformer-based language model to manage
both the primary input and the auxiliary data. Our methodology significantly
improves the language model's ability to accurately convert a natural language
query into a relevant SPARQL query. It demonstrates promising results on the
most recent QALD datasets, namely QALD-9-Plus and QALD-10. Furthermore, we
introduce and evaluate our approach on Chinese and Japanese, thereby expanding
the language diversity of the existing datasets.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.06041v1},
File          = {2407.06041v1.pdf}
}
@article{2310.03666v1,
Author        = {Nicolas Matentzoglu and J. Harry Caufield and Harshad B. Hegde and Justin T. Reese and Sierra Moxon and Hyeongsik Kim and Nomi L. Harris and Melissa A Haendel and Christopher J. Mungall},
Title         = {MapperGPT: Large Language Models for Linking and Mapping Entities},
Eprint        = {2310.03666v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Aligning terminological resources, including ontologies, controlled
vocabularies, taxonomies, and value sets is a critical part of data integration
in many domains such as healthcare, chemistry, and biomedical research. Entity
mapping is the process of determining correspondences between entities across
these resources, such as gene identifiers, disease concepts, or chemical entity
identifiers. Many tools have been developed to compute such mappings based on
common structural features and lexical information such as labels and synonyms.
Lexical approaches in particular often provide very high recall, but low
precision, due to lexical ambiguity. As a consequence of this, mapping efforts
often resort to a labor intensive manual mapping refinement through a human
curator.
  Large Language Models (LLMs), such as the ones employed by ChatGPT, have
generalizable abilities to perform a wide range of tasks, including
question-answering and information extraction. Here we present MapperGPT, an
approach that uses LLMs to review and refine mapping relationships as a
post-processing step, in concert with existing high-recall methods that are
based on lexical and structural heuristics.
  We evaluated MapperGPT on a series of alignment tasks from different domains,
including anatomy, developmental biology, and renal diseases. We devised a
collection of tasks that are designed to be particularly challenging for
lexical methods. We show that when used in combination with high-recall
methods, MapperGPT can provide a substantial improvement in accuracy, beating
state-of-the-art (SOTA) methods such as LogMap.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.03666v1},
File          = {2310.03666v1.pdf}
}
@article{2403.15587v1,
Author        = {Cristina Zuheros and David Herrera-Poyatos and Rosana Montes and Francisco Herrera},
Title         = {Large language models for crowd decision making based on prompt design
  strategies using ChatGPT: models, analysis and challenges},
Eprint        = {2403.15587v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Social Media and Internet have the potential to be exploited as a source of
opinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a
methodology able to infer opinions and decisions from plain texts, such as
reviews published in social media platforms, by means of Sentiment Analysis.
Currently, the emergence and potential of Large Language Models (LLMs) lead us
to explore new scenarios of automatically understand written texts, also known
as natural language processing. This paper analyzes the use of ChatGPT based on
prompt design strategies to assist in CDM processes to extract opinions and
make decisions. We integrate ChatGPT in CDM processes as a flexible tool that
infer the opinions expressed in texts, providing numerical or linguistic
evaluations where the decision making models are based on the prompt design
strategies. We include a multi-criteria decision making scenario with a
category ontology for criteria. We also consider ChatGPT as an end-to-end CDM
model able to provide a general opinion and score on the alternatives. We
conduct empirical experiments on real data extracted from TripAdvisor, the
TripR-2020Large dataset. The analysis of results show a promising branch for
developing quality decision making models using ChatGPT. Finally, we discuss
the challenges of consistency, sensitivity and explainability associated to the
use of LLMs in CDM processes, raising open questions for future studies.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.15587v1},
File          = {2403.15587v1.pdf}
}
@article{2208.02743v3,
Author        = {Mojtaba Nayyeri and Zihao Wang and Mst. Mahfuja Akter and Mirza Mohtashim Alam and Md Rashad Al Hasan Rony and Jens Lehmann and Steffen Staab},
Title         = {Integrating Knowledge Graph embedding and pretrained Language Models in
  Hypercomplex Spaces},
Eprint        = {2208.02743v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graphs, such as Wikidata, comprise structural and textual knowledge
in order to represent knowledge. For each of the two modalities dedicated
approaches for graph embedding and language models learn patterns that allow
for predicting novel structural knowledge. Few approaches have integrated
learning and inference with both modalities and these existing ones could only
partially exploit the interaction of structural and textual knowledge. In our
approach, we build on existing strong representations of single modalities and
we use hypercomplex algebra to represent both, (i), single-modality embedding
as well as, (ii), the interaction between different modalities and their
complementary means of knowledge representation. More specifically, we suggest
Dihedron and Quaternion representations of 4D hypercomplex numbers to integrate
four modalities namely structural knowledge graph embedding, word-level
representations (e.g.\ Word2vec, Fasttext), sentence-level representations
(Sentence transformer), and document-level representations (sentence
transformer, Doc2vec). Our unified vector representation scores the
plausibility of labelled edges via Hamilton and Dihedron products, thus
modeling pairwise interactions between different modalities. Extensive
experimental evaluation on standard benchmark datasets shows the superiority of
our two new models using abundant textual information besides sparse structural
knowledge to enhance performance in link prediction tasks.},
Year          = {2022},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2208.02743v3},
File          = {2208.02743v3.pdf}
}
@article{2205.08012v2,
Author        = {Tara Safavi and Doug Downey and Tom Hope},
Title         = {CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction},
Eprint        = {2205.08012v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graph (KG) link prediction is a fundamental task in artificial
intelligence, with applications in natural language processing, information
retrieval, and biomedicine. Recently, promising results have been achieved by
leveraging cross-modal information in KGs, using ensembles that combine
knowledge graph embeddings (KGEs) and contextual language models (LMs).
However, existing ensembles are either (1) not consistently effective in terms
of ranking accuracy gains or (2) impractically inefficient on larger datasets
due to the combinatorial explosion problem of pairwise ranking with deep
language models. In this paper, we propose a novel tiered ranking architecture
CascadER to maintain the ranking accuracy of full ensembling while improving
efficiency considerably. CascadER uses LMs to rerank the outputs of more
efficient base KGEs, relying on an adaptive subset selection scheme aimed at
invoking the LMs minimally while maximizing accuracy gain over the KGE.
Extensive experiments demonstrate that CascadER improves MRR by up to 9 points
over KGE baselines, setting new state-of-the-art performance on four benchmarks
while improving efficiency by one or more orders of magnitude over competitive
cross-modal baselines. Our empirical analyses reveal that diversity of models
across modalities and preservation of individual models' confidence signals
help explain the effectiveness of CascadER, and suggest promising directions
for cross-modal cascaded architectures. Code and pretrained models are
available at https://github.com/tsafavi/cascader.},
Year          = {2022},
Month         = {May},
Url           = {http://arxiv.org/abs/2205.08012v2},
File          = {2205.08012v2.pdf}
}
@article{2210.04105v2,
Author        = {Shangbin Feng and Zhaoxuan Tan and Wenqian Zhang and Zhenyu Lei and Yulia Tsvetkov},
Title         = {KALM: Knowledge-Aware Integration of Local, Document, and Global
  Contexts for Long Document Understanding},
Eprint        = {2210.04105v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {With the advent of pretrained language models (LMs), increasing research
efforts have been focusing on infusing commonsense and domain-specific
knowledge to prepare LMs for downstream tasks. These works attempt to leverage
knowledge graphs, the de facto standard of symbolic knowledge representation,
along with pretrained LMs. While existing approaches have leveraged external
knowledge, it remains an open question how to jointly incorporate knowledge
graphs representing varying contexts, from local (e.g., sentence), to
document-level, to global knowledge, to enable knowledge-rich exchange across
these contexts. Such rich contextualization can be especially beneficial for
long document understanding tasks since standard pretrained LMs are typically
bounded by the input sequence length. In light of these challenges, we propose
KALM, a Knowledge-Aware Language Model that jointly leverages knowledge in
local, document-level, and global contexts for long document understanding.
KALM first encodes long documents and knowledge graphs into the three
knowledge-aware context representations. It then processes each context with
context-specific layers, followed by a context fusion layer that facilitates
knowledge exchange to derive an overarching document representation. Extensive
experiments demonstrate that KALM achieves state-of-the-art performance on six
long document understanding tasks and datasets. Further analyses reveal that
the three knowledge-aware contexts are complementary and they all contribute to
model performance, while the importance and information exchange patterns of
different contexts vary with respect to different tasks and datasets.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.04105v2},
File          = {2210.04105v2.pdf}
}
@article{2210.09338v2,
Author        = {Michihiro Yasunaga and Antoine Bosselut and Hongyu Ren and Xikun Zhang and Christopher D Manning and Percy Liang and Jure Leskovec},
Title         = {Deep Bidirectional Language-Knowledge Graph Pretraining},
Eprint        = {2210.09338v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pretraining a language model (LM) on text has been shown to help various
downstream NLP tasks. Recent works show that a knowledge graph (KG) can
complement text data, offering structured background knowledge that provides a
useful scaffold for reasoning. However, these works are not pretrained to learn
a deep fusion of the two modalities at scale, limiting the potential to acquire
fully joint representations of text and KG. Here we propose DRAGON (Deep
Bidirectional Language-Knowledge Graph Pretraining), a self-supervised approach
to pretraining a deeply joint language-knowledge foundation model from text and
KG at scale. Specifically, our model takes pairs of text segments and relevant
KG subgraphs as input and bidirectionally fuses information from both
modalities. We pretrain this model by unifying two self-supervised reasoning
tasks, masked language modeling and KG link prediction. DRAGON outperforms
existing LM and LM+KG models on diverse downstream tasks including question
answering across general and biomedical domains, with +5% absolute gain on
average. In particular, DRAGON achieves notable performance on complex
reasoning about language and knowledge (+10% on questions involving long
contexts or multi-step reasoning) and low-resource QA (+8% on OBQA and
RiddleSense), and new state-of-the-art results on various BioNLP tasks. Our
code and trained models are available at
https://github.com/michiyasunaga/dragon.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.09338v2},
File          = {2210.09338v2.pdf}
}
@article{2309.13625v1,
Author        = {Xin Li and Dongze Lian and Zhihe Lu and Jiawang Bai and Zhibo Chen and Xinchao Wang},
Title         = {GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph},
Eprint        = {2309.13625v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Adapter-style efficient transfer learning (ETL) has shown excellent
performance in the tuning of vision-language models (VLMs) under the low-data
regime, where only a few additional parameters are introduced to excavate the
task-specific knowledge based on the general and powerful representation of
VLMs. However, most adapter-style works face two limitations: (i) modeling
task-specific knowledge with a single modality only; and (ii) overlooking the
exploitation of the inter-class relationships in downstream tasks, thereby
leading to sub-optimal solutions. To mitigate that, we propose an effective
adapter-style tuning strategy, dubbed GraphAdapter, which performs the textual
adapter by explicitly modeling the dual-modality structure knowledge (i.e., the
correlation of different semantics/classes in textual and visual modalities)
with a dual knowledge graph. In particular, the dual knowledge graph is
established with two sub-graphs, i.e., a textual knowledge sub-graph, and a
visual knowledge sub-graph, where the nodes and edges represent the
semantics/classes and their correlations in two modalities, respectively. This
enables the textual feature of each prompt to leverage the task-specific
structure knowledge from both textual and visual modalities, yielding a more
effective classifier for downstream tasks. Extensive experimental results on 11
benchmark datasets reveal that our GraphAdapter significantly outperforms
previous adapter-based methods. The code will be released at
https://github.com/lixinustc/GraphAdapter},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.13625v1},
File          = {2309.13625v1.pdf}
}
@article{2311.09109v2,
Author        = {Yusuke Sakai and Hidetaka Kamigaito and Katsuhiko Hayashi and Taro Watanabe},
Title         = {Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge
  Graph Completion?},
Eprint        = {2311.09109v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs (KGs) consist of links that describe relationships between
entities. Due to the difficulty of manually enumerating all relationships
between entities, automatically completing them is essential for KGs. Knowledge
Graph Completion (KGC) is a task that infers unseen relationships between
entities in a KG. Traditional embedding-based KGC methods, such as RESCAL,
TransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using
only the knowledge from training data. In contrast, the recent Pre-trained
Language Model (PLM)-based KGC utilizes knowledge obtained during pre-training.
Therefore, PLM-based KGC can estimate missing links between entities by reusing
memorized knowledge from pre-training without inference. This approach is
problematic because building KGC models aims to infer unseen links between
entities. However, conventional evaluations in KGC do not consider inference
and memorization abilities separately. Thus, a PLM-based KGC method, which
achieves high performance in current KGC evaluations, may be ineffective in
practical applications. To address this issue, we analyze whether PLM-based KGC
methods make inferences or merely access memorized knowledge. For this purpose,
we propose a method for constructing synthetic datasets specified in this
analysis and conclude that PLMs acquire the inference abilities required for
KGC through pre-training, even though the performance improvements mostly come
from textual information of entities and relations.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.09109v2},
File          = {2311.09109v2.pdf}
}
@article{2401.00158v1,
Author        = {Jinhao Jiang and Kun Zhou and Wayne Xin Zhao and Yaliang Li and Ji-Rong Wen},
Title         = {ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained
  Language Models for Question Answering over Knowledge Graph},
Eprint        = {2401.00158v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Question Answering over Knowledge Graph (KGQA) aims to seek answer entities
for the natural language question from a large-scale Knowledge Graph~(KG). To
better perform reasoning on KG, recent work typically adopts a pre-trained
language model~(PLM) to model the question, and a graph neural network~(GNN)
based module to perform multi-hop reasoning on the KG. Despite the
effectiveness, due to the divergence in model architecture, the PLM and GNN are
not closely integrated, limiting the knowledge sharing and fine-grained feature
interactions. To solve it, we aim to simplify the above two-module approach,
and develop a more capable PLM that can directly support subgraph reasoning for
KGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware
self-attention mechanism to imitate the GNN for performing structured
reasoning, and also adopt an adaptation tuning strategy to adapt the model
parameters with 20,000 subgraphs with synthesized questions. After adaptation,
the PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments
show that ReasoningLM surpasses state-of-the-art models by a large margin, even
with fewer updated parameters and less training data. Our codes and data are
publicly available at~\url{https://github.com/RUCAIBox/ReasoningLM}.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2401.00158v1},
File          = {2401.00158v1.pdf}
}
@article{2401.07977v3,
Author        = {Saptarshi Sengupta and Connor Heaton and Suhan Cui and Soumalya Sarkar and Prasenjit Mitra},
Title         = {Towards Efficient Methods in Medical Question Answering using Knowledge
  Graph Embeddings},
Eprint        = {2401.07977v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In Natural Language Processing (NLP), Machine Reading Comprehension (MRC) is
the task of answering a question based on a given context. To handle questions
in the medical domain, modern language models such as BioBERT, SciBERT and even
ChatGPT are trained on vast amounts of in-domain medical corpora. However,
in-domain pre-training is expensive in terms of time and resources. In this
paper, we propose a resource-efficient approach for injecting domain knowledge
into a model without relying on such domain-specific pre-training.
  Knowledge graphs are powerful resources for accessing medical information.
Building on existing work, we introduce a method using Multi-Layer Perceptrons
(MLPs) for aligning and integrating embeddings extracted from medical knowledge
graphs with the embedding spaces of pre-trained language models (LMs). The
aligned embeddings are fused with open-domain LMs BERT and RoBERTa that are
fine-tuned for two MRC tasks, span detection (COVID-QA) and multiple-choice
questions (PubMedQA). We compare our method to prior techniques that rely on a
vocabulary overlap for embedding alignment and show how our method circumvents
this requirement to deliver better performance. On both datasets, our method
allows BERT/RoBERTa to either perform on par (occasionally exceeding) with
stronger domain-specific models or show improvements in general over prior
techniques. With the proposed approach, we signal an alternative method to
in-domain pre-training to achieve domain proficiency. Our code is available
here.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.07977v3},
File          = {2401.07977v3.pdf}
}
@article{2411.03225v2,
Author        = {Ruwan Wickramarachchi and Cory Henson and Amit Sheth},
Title         = {Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities
  of Neurosymbolic AI},
Eprint        = {2411.03225v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In the era of Generative AI, Neurosymbolic AI is emerging as a powerful
approach for tasks spanning from perception to cognition. The use of
Neurosymbolic AI has been shown to achieve enhanced capabilities, including
improved grounding, alignment, explainability, and reliability. However, due to
its nascent stage, there is a lack of widely available real-world benchmark
datasets tailored to Neurosymbolic AI tasks. To address this gap and support
the evaluation of current and future methods, we introduce DSceneKG -- a suite
of knowledge graphs of driving scenes built from real-world, high-quality
scenes from multiple open autonomous driving datasets. In this article, we
detail the construction process of DSceneKG and highlight its application in
seven different tasks. DSceneKG is publicly accessible at:
https://github.com/ruwantw/DSceneKG},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.03225v2},
File          = {2411.03225v2.pdf}
}
@article{2408.14236v1,
Author        = {Hanna Abi Akl},
Title         = {DSTI at LLMs4OL 2024 Task A: Intrinsic versus extrinsic knowledge for
  type classification},
Eprint        = {2408.14236v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce semantic towers, an extrinsic knowledge representation method,
and compare it to intrinsic knowledge in large language models for ontology
learning. Our experiments show a trade-off between performance and semantic
grounding for extrinsic knowledge compared to a fine-tuned model intrinsic
knowledge. We report our findings on the Large Language Models for Ontology
Learning (LLMs4OL) 2024 challenge.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.14236v1},
File          = {2408.14236v1.pdf}
}
@article{2201.10262v1,
Author        = {Mael Jullien and Marco Valentino and Andre Freitas},
Title         = {Do Transformers Encode a Foundational Ontology? Probing Abstract Classes
  in Natural Language},
Eprint        = {2201.10262v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {With the methodological support of probing (or diagnostic classification),
recent studies have demonstrated that Transformers encode syntactic and
semantic information to some extent. Following this line of research, this
paper aims at taking semantic probing to an abstraction extreme with the goal
of answering the following research question: can contemporary
Transformer-based models reflect an underlying Foundational Ontology? To this
end, we present a systematic Foundational Ontology (FO) probing methodology to
investigate whether Transformers-based models encode abstract semantic
information. Following different pre-training and fine-tuning regimes, we
present an extensive evaluation of a diverse set of large-scale language models
over three distinct and complementary FO tagging experiments. Specifically, we
present and discuss the following conclusions: (1) The probing results indicate
that Transformer-based models incidentally encode information related to
Foundational Ontologies during the pre-training pro-cess; (2) Robust FO taggers
(accuracy of 90 percent)can be efficiently built leveraging on this knowledge.},
Year          = {2022},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2201.10262v1},
File          = {2201.10262v1.pdf}
}
@article{2009.13984v1,
Author        = {Nuobei Shi and Qin Zeng and Raymond Lee},
Title         = {The design and implementation of Language Learning Chatbot with XAI
  using Ontology and Transfer Learning},
Eprint        = {2009.13984v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In this paper, we proposed a transfer learning-based English language
learning chatbot, whose output generated by GPT-2 can be explained by
corresponding ontology graph rooted by fine-tuning dataset. We design three
levels for systematically English learning, including phonetics level for
speech recognition and pronunciation correction, semantic level for specific
domain conversation, and the simulation of free-style conversation in English -
the highest level of language chatbot communication as free-style conversation
agent. For academic contribution, we implement the ontology graph to explain
the performance of free-style conversation, following the concept of XAI
(Explainable Artificial Intelligence) to visualize the connections of neural
network in bionics, and explain the output sentence from language model. From
implementation perspective, our Language Learning agent integrated the
mini-program in WeChat as front-end, and fine-tuned GPT-2 model of transfer
learning as back-end to interpret the responses by ontology graph.},
Year          = {2020},
Month         = {Sep},
Note          = {Dhinaharan Nagamalai et al. (Eds): CSEIT, WiMoNe, NCS, CIoT, CMLA,
  DMSE, NLPD - 2020 pp. 305-323, 2020. CS & IT - CSCP 2020},
Url           = {http://arxiv.org/abs/2009.13984v1},
File          = {2009.13984v1.pdf}
}
@article{2210.00907v1,
Author        = {Sondre Wold},
Title         = {The Effectiveness of Masked Language Modeling and Adapters for Factual
  Knowledge Injection},
Eprint        = {2210.00907v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper studies the problem of injecting factual knowledge into large
pre-trained language models. We train adapter modules on parts of the
ConceptNet knowledge graph using the masked language modeling objective and
evaluate the success of the method by a series of probing experiments on the
LAMA probe. Mean P@K curves for different configurations indicate that the
technique is effective, increasing the performance on subsets of the LAMA probe
for large values of k by adding as little as 2.1% additional parameters to the
original models.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.00907v1},
File          = {2210.00907v1.pdf}
}
@article{2305.16756v2,
Author        = {Nicolò Tamagnone and Selim Fekih and Ximena Contla and Nayid Orozco and Navid Rekabsaz},
Title         = {Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian
  Response Entry Classification},
Eprint        = {2305.16756v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Accurate and rapid situation analysis during humanitarian crises is critical
to delivering humanitarian aid efficiently and is fundamental to humanitarian
imperatives and the Leave No One Behind (LNOB) principle. This data analysis
can highly benefit from language processing systems, e.g., by classifying the
text data according to a humanitarian ontology. However, approaching this by
simply fine-tuning a generic large language model (LLM) involves considerable
practical and ethical issues, particularly the lack of effectiveness on
data-sparse and complex subdomains, and the encoding of societal biases and
unwanted associations. In this work, we aim to provide an effective and
ethically-aware system for humanitarian data analysis. We approach this by (1)
introducing a novel architecture adjusted to the humanitarian analysis
framework, (2) creating and releasing a novel humanitarian-specific LLM called
HumBert, and (3) proposing a systematic way to measure and mitigate biases. Our
experiments' results show the better performance of our approach on zero-shot
and full-training settings in comparison with strong baseline models, while
also revealing the existence of biases in the resulting LLMs. Utilizing a
targeted counterfactual data augmentation approach, we significantly reduce
these biases without compromising performance.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.16756v2},
File          = {2305.16756v2.pdf}
}
@article{2412.12688v1,
Author        = {Yuwei Miao and Yuzhi Guo and Hehuan Ma and Jingquan Yan and Feng Jiang and Weizhi An and Jean Gao and Junzhou Huang},
Title         = {UniEntrezDB: Large-scale Gene Ontology Annotation Dataset and Evaluation
  Benchmarks with Unified Entrez Gene Identifiers},
Eprint        = {2412.12688v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Gene studies are crucial for fields such as protein structure prediction,
drug discovery, and cancer genomics, yet they face challenges in fully
utilizing the vast and diverse information available. Gene studies require
clean, factual datasets to ensure reliable results. Ontology graphs, neatly
organized domain terminology graphs, provide ideal sources for domain facts.
However, available gene ontology annotations are currently distributed across
various databases without unified identifiers for genes and gene products. To
address these challenges, we introduce Unified Entrez Gene Identifier Dataset
and Benchmarks (UniEntrezDB), the first systematic effort to unify large-scale
public Gene Ontology Annotations (GOA) from various databases using unique gene
identifiers. UniEntrezDB includes a pre-training dataset and four downstream
tasks designed to comprehensively evaluate gene embedding performance from
gene, protein, and cell levels, ultimately enhancing the reliability and
applicability of LLMs in gene research and other professional settings.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.12688v1},
File          = {2412.12688v1.pdf}
}
@article{2405.01392v1,
Author        = {David Maranto},
Title         = {LLMSat: A Large Language Model-Based Goal-Oriented Agent for Autonomous
  Space Exploration},
Eprint        = {2405.01392v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {As spacecraft journey further from Earth with more complex missions, systems
of greater autonomy and onboard intelligence are called for. Reducing reliance
on human-based mission control becomes increasingly critical if we are to
increase our rate of solar-system-wide exploration. Recent work has explored
AI-based goal-oriented systems to increase the level of autonomy in mission
execution. These systems make use of symbolic reasoning managers to make
inferences from the state of a spacecraft and a handcrafted knowledge base,
enabling autonomous generation of tasks and re-planning. Such systems have
proven to be successful in controlled cases, but they are difficult to
implement as they require human-crafted ontological models to allow the
spacecraft to understand the world. Reinforcement learning has been applied to
train robotic agents to pursue a goal. A new architecture for autonomy is
called for. This work explores the application of Large Language Models (LLMs)
as the high-level control system of a spacecraft. Using a systems engineering
approach, this work presents the design and development of an agentic
spacecraft controller by leveraging an LLM as a reasoning engine, to evaluate
the utility of such an architecture in achieving higher levels of spacecraft
autonomy. A series of deep space mission scenarios simulated within the popular
game engine Kerbal Space Program (KSP) are used as case studies to evaluate the
implementation against the requirements. It is shown the reasoning and planning
abilities of present-day LLMs do not scale well as the complexity of a mission
increases, but this can be alleviated with adequate prompting frameworks and
strategic selection of the agent's level of authority over the host spacecraft.
This research evaluates the potential of LLMs in augmenting autonomous
decision-making systems for future robotic space applications.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2405.01392v1},
File          = {2405.01392v1.pdf}
}
@article{2302.01842v1,
Author        = {Vladimir Ershov},
Title         = {A Case Study for Compliance as Code with Graphs and Language Models:
  Public release of the Regulatory Knowledge Graph},
Eprint        = {2302.01842v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The paper presents a study on using language models to automate the
construction of executable Knowledge Graph (KG) for compliance. The paper
focuses on Abu Dhabi Global Market regulations and taxonomy, involves manual
tagging a portion of the regulations, training BERT-based models, which are
then applied to the rest of the corpus. Coreference resolution and syntax
analysis were used to parse the relationships between the tagged entities and
to form KG stored in a Neo4j database. The paper states that the use of machine
learning models released by regulators to automate the interpretation of rules
is a vital step towards compliance automation, demonstrates the concept
querying with Cypher, and states that the produced sub-graphs combined with
Graph Neural Networks (GNN) will achieve expandability in judgment automation
systems. The graph is open sourced on GitHub to provide structured data for
future advancements in the field.},
Year          = {2023},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2302.01842v1},
File          = {2302.01842v1.pdf}
}
@article{2012.07589v1,
Author        = {Ananda Das and Partha Pratim Das},
Title         = {Incorporating Domain Knowledge To Improve Topic Segmentation Of Long
  MOOC Lecture Videos},
Eprint        = {2012.07589v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Topical Segmentation poses a great role in reducing search space of the
topics taught in a lecture video specially when the video metadata lacks topic
wise segmentation information. This segmentation information eases user efforts
of searching, locating and browsing a topic inside a lecture video. In this
work we propose an algorithm, that combines state-of-the art language model and
domain knowledge graph for automatically detecting different coherent topics
present inside a long lecture video. We use the language model on
speech-to-text transcription to capture the implicit meaning of the whole video
while the knowledge graph provides us the domain specific dependencies between
different concepts of that subjects. Also leveraging the domain knowledge we
can capture the way instructor binds and connects different concepts while
teaching, which helps us in achieving better segmentation accuracy. We tested
our approach on NPTEL lecture videos and holistic evaluation shows that it out
performs the other methods described in the literature.},
Year          = {2020},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2012.07589v1},
File          = {2012.07589v1.pdf}
}
@article{2104.06378v5,
Author        = {Michihiro Yasunaga and Hongyu Ren and Antoine Bosselut and Percy Liang and Jure Leskovec},
Title         = {QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question
  Answering},
Eprint        = {2104.06378v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The problem of answering questions using knowledge from pre-trained language
models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA
context (question and answer choice), methods need to (i) identify relevant
knowledge from large KGs, and (ii) perform joint reasoning over the QA context
and KG. In this work, we propose a new model, QA-GNN, which addresses the above
challenges through two key innovations: (i) relevance scoring, where we use LMs
to estimate the importance of KG nodes relative to the given QA context, and
(ii) joint reasoning, where we connect the QA context and KG to form a joint
graph, and mutually update their representations through graph neural networks.
We evaluate our model on QA benchmarks in the commonsense (CommonsenseQA,
OpenBookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing
LM and LM+KG models, and exhibits capabilities to perform interpretable and
structured reasoning, e.g., correctly handling negation in questions.},
Year          = {2021},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2104.06378v5},
File          = {2104.06378v5.pdf}
}
@article{2105.05457v1,
Author        = {Ting-Yun Chang and Yang Liu and Karthik Gopalakrishnan and Behnam Hedayatnia and Pei Zhou and Dilek Hakkani-Tur},
Title         = {Incorporating Commonsense Knowledge Graph in Pretrained Models for
  Social Commonsense Tasks},
Eprint        = {2105.05457v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pretrained language models have excelled at many NLP tasks recently; however,
their social intelligence is still unsatisfactory. To enable this, machines
need to have a more general understanding of our complicated world and develop
the ability to perform commonsense reasoning besides fitting the specific
downstream tasks. External commonsense knowledge graphs (KGs), such as
ConceptNet, provide rich information about words and their relationships. Thus,
towards general commonsense learning, we propose two approaches to
\emph{implicitly} and \emph{explicitly} infuse such KGs into pretrained
language models. We demonstrate our proposed methods perform well on SocialIQA,
a social commonsense reasoning task, in both limited and full training data
regimes.},
Year          = {2021},
Month         = {May},
Url           = {http://arxiv.org/abs/2105.05457v1},
File          = {2105.05457v1.pdf}
}
@article{2105.13471v2,
Author        = {Carlos Aspillaga and Marcelo Mendoza and Alvaro Soto},
Title         = {Inspecting the concept knowledge graph encoded by modern language models},
Eprint        = {2105.13471v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The field of natural language understanding has experienced exponential
progress in the last few years, with impressive results in several tasks. This
success has motivated researchers to study the underlying knowledge encoded by
these models. Despite this, attempts to understand their semantic capabilities
have not been successful, often leading to non-conclusive, or contradictory
conclusions among different works. Via a probing classifier, we extract the
underlying knowledge graph of nine of the most influential language models of
the last years, including word embeddings, text generators, and context
encoders. This probe is based on concept relatedness, grounded on WordNet. Our
results reveal that all the models encode this knowledge, but suffer from
several inaccuracies. Furthermore, we show that the different architectures and
training strategies lead to different model biases. We conduct a systematic
evaluation to discover specific factors that explain why some concepts are
challenging. We hope our insights will motivate the development of models that
capture concepts more precisely.},
Year          = {2021},
Month         = {May},
Url           = {http://arxiv.org/abs/2105.13471v2},
File          = {2105.13471v2.pdf}
}
@article{2106.01623v1,
Author        = {Junyi Li and Tianyi Tang and Wayne Xin Zhao and Zhicheng Wei and Nicholas Jing Yuan and Ji-Rong Wen},
Title         = {Few-shot Knowledge Graph-to-Text Generation with Pretrained Language
  Models},
Eprint        = {2106.01623v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper studies how to automatically generate a natural language text that
describes the facts in knowledge graph (KG). Considering the few-shot setting,
we leverage the excellent capacities of pretrained language models (PLMs) in
language understanding and generation. We make three major technical
contributions, namely representation alignment for bridging the semantic gap
between KG encodings and PLMs, relation-biased KG linearization for deriving
better input representations, and multi-task learning for learning the
correspondence between KG and text. Extensive experiments on three benchmark
datasets have demonstrated the effectiveness of our model on KG-to-text
generation task. In particular, our model outperforms all comparison methods on
both fully-supervised and few-shot settings. Our code and datasets are
available at https://github.com/RUCAIBox/Few-Shot-KG2Text.},
Year          = {2021},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2106.01623v1},
File          = {2106.01623v1.pdf}
}
@article{2107.05885v1,
Author        = {Chao Feng and Shi-jie We},
Title         = {Exploiting Network Structures to Improve Semantic Representation for the
  Financial Domain},
Eprint        = {2107.05885v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper presents the participation of the MiniTrue team in the FinSim-3
shared task on learning semantic similarities for the financial domain in
English language. Our approach combines contextual embeddings learned by
transformer-based language models with network structures embeddings extracted
on external knowledge sources, to create more meaningful representations of
financial domain entities and terms. For this, two BERT based language models
and a knowledge graph embedding model are used. Besides, we propose a voting
function to joint three basic models for the final inference. Experimental
results show that the model with the knowledge graph embeddings has achieved a
superior result than these models with only contextual embeddings.
Nevertheless, we also observe that our voting function brings an extra benefit
to the final system.},
Year          = {2021},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2107.05885v1},
File          = {2107.05885v1.pdf}
}
@article{2204.13796v1,
Author        = {Shuyang Li and Mukund Sridhar and Chandana Satya Prakash and Jin Cao and Wael Hamza and Julian McAuley},
Title         = {Instilling Type Knowledge in Language Models via Multi-Task QA},
Eprint        = {2204.13796v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Understanding human language often necessitates understanding entities and
their place in a taxonomy of knowledge -- their types. Previous methods to
learn entity types rely on training classifiers on datasets with coarse, noisy,
and incomplete labels. We introduce a method to instill fine-grained type
knowledge in language models with text-to-text pre-training on type-centric
questions leveraging knowledge base documents and knowledge graphs. We create
the WikiWiki dataset: entities and passages from 10M Wikipedia articles linked
to the Wikidata knowledge graph with 41K types. Models trained on WikiWiki
achieve state-of-the-art performance in zero-shot dialog state tracking
benchmarks, accurately infer entity types in Wikipedia articles, and can
discover new types deemed useful by human judges.},
Year          = {2022},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2204.13796v1},
File          = {2204.13796v1.pdf}
}
@article{2206.14574v1,
Author        = {Nimesh Bhana and Terence L. van Zyl},
Title         = {Knowledge Graph Fusion for Language Model Fine-tuning},
Eprint        = {2206.14574v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Language Models such as BERT have grown in popularity due to their ability to
be pre-trained and perform robustly on a wide range of Natural Language
Processing tasks. Often seen as an evolution over traditional word embedding
techniques, they can produce semantic representations of text, useful for tasks
such as semantic similarity. However, state-of-the-art models often have high
computational requirements and lack global context or domain knowledge which is
required for complete language understanding. To address these limitations, we
investigate the benefits of knowledge incorporation into the fine-tuning stages
of BERT. An existing K-BERT model, which enriches sentences with triplets from
a Knowledge Graph, is adapted for the English language and extended to inject
contextually relevant information into sentences. As a side-effect, changes
made to K-BERT for accommodating the English language also extend to other
word-based languages. Experiments conducted indicate that injected knowledge
introduces noise. We see statistically significant improvements for
knowledge-driven tasks when this noise is minimised. We show evidence that,
given the appropriate task, modest injection with relevant, high-quality
knowledge is most performant.},
Year          = {2022},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2206.14574v1},
File          = {2206.14574v1.pdf}
}
@article{2210.13715v1,
Author        = {Jianhao Shen and Chenguang Wang and Ye Yuan and Jiawei Han and Heng Ji and Koushik Sen and Ming Zhang and Dawn Song},
Title         = {PALT: Parameter-Lite Transfer of Language Models for Knowledge Graph
  Completion},
Eprint        = {2210.13715v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper presents a parameter-lite transfer learning approach of pretrained
language models (LM) for knowledge graph (KG) completion. Instead of
finetuning, which modifies all LM parameters, we only tune a few new parameters
while keeping the original LM parameters fixed. We establish this via
reformulating KG completion as a "fill-in-the-blank" task, and introducing a
parameter-lite encoder on top of the original LMs. We show that, by tuning far
fewer parameters than finetuning, LMs transfer non-trivially to most tasks and
reach competitiveness with prior state-of-the-art approaches. For instance, we
outperform the fully finetuning approaches on a KG completion benchmark by
tuning only 1% of the parameters. The code and datasets are available at
\url{https://github.com/yuanyehome/PALT}.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.13715v1},
File          = {2210.13715v1.pdf}
}
@article{2305.15597v1,
Author        = {Pengcheng Jiang and Shivam Agarwal and Bowen Jin and Xuan Wang and Jimeng Sun and Jiawei Han},
Title         = {Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language
  Models},
Eprint        = {2305.15597v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The mission of open knowledge graph (KG) completion is to draw new findings
from known facts. Existing works that augment KG completion require either (1)
factual triples to enlarge the graph reasoning space or (2) manually designed
prompts to extract knowledge from a pre-trained language model (PLM),
exhibiting limited performance and requiring expensive efforts from experts. To
this end, we propose TAGREAL that automatically generates quality query prompts
and retrieves support information from large text corpora to probe knowledge
from PLM for KG completion. The results show that TAGREAL achieves
state-of-the-art performance on two benchmark datasets. We find that TAGREAL
has superb performance even with limited training data, outperforming existing
embedding-based, graph-based, and PLM-based methods.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.15597v1},
File          = {2305.15597v1.pdf}
}
@article{2305.18846v1,
Author        = {Minki Kang and Jin Myung Kwak and Jinheon Baek and Sung Ju Hwang},
Title         = {Knowledge Graph-Augmented Language Models for Knowledge-Grounded
  Dialogue Generation},
Eprint        = {2305.18846v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Language models have achieved impressive performances on dialogue generation
tasks. However, when generating responses for a conversation that requires
factual knowledge, they are far from perfect, due to an absence of mechanisms
to retrieve, encode, and reflect the knowledge in the generated responses. Some
knowledge-grounded dialogue generation methods tackle this problem by
leveraging facts from Knowledge Graphs (KGs); however, they do not guarantee
that the model utilizes a relevant piece of knowledge from the KG. To overcome
this limitation, we propose SUbgraph Retrieval-augmented GEneration (SURGE), a
framework for generating context-relevant and knowledge-grounded dialogues with
the KG. Specifically, our SURGE framework first retrieves the relevant subgraph
from the KG, and then enforces consistency across facts by perturbing their
word embeddings conditioned by the retrieved subgraph. Then, we utilize
contrastive learning to ensure that the generated texts have high similarity to
the retrieved subgraphs. We validate our SURGE framework on OpendialKG and
KOMODIS datasets, showing that it generates high-quality dialogues that
faithfully reflect the knowledge from KG.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.18846v1},
File          = {2305.18846v1.pdf}
}
@article{2310.07059v2,
Author        = {Xueren Ge and Satpathy Abhishek and Ronald Dean Williams and John A. Stankovic and Homa Alemzadeh},
Title         = {DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis
  Prediction},
Eprint        = {2310.07059v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Multi-label text classification (MLTC) tasks in the medical domain often face
the long-tail label distribution problem. Prior works have explored
hierarchical label structures to find relevant information for few-shot
classes, but mostly neglected to incorporate external knowledge from medical
guidelines. This paper presents DKEC, Domain Knowledge Enhanced Classification
for diagnosis prediction with two innovations: (1) automated construction of
heterogeneous knowledge graphs from external sources to capture semantic
relations among diverse medical entities, (2) incorporating the heterogeneous
knowledge graphs in few-shot classification using a label-wise attention
mechanism. We construct DKEC using three online medical knowledge sources and
evaluate it on a real-world Emergency Medical Services (EMS) dataset and a
public electronic health record (EHR) dataset. Results show that DKEC
outperforms the state-of-the-art label-wise attention networks and transformer
models of different sizes, particularly for the few-shot classes. More
importantly, it helps the smaller language models achieve comparable
performance to large language models.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.07059v2},
File          = {2310.07059v2.pdf}
}
@article{2401.14194v3,
Author        = {Mathieu Ravaut and Hao Zhang and Lu Xu and Aixin Sun and Yong Liu},
Title         = {Parameter-Efficient Conversational Recommender System as a Language
  Processing Task},
Eprint        = {2401.14194v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Conversational recommender systems (CRS) aim to recommend relevant items to
users by eliciting user preference through natural language conversation. Prior
work often utilizes external knowledge graphs for items' semantic information,
a language model for dialogue generation, and a recommendation module for
ranking relevant items. This combination of multiple components suffers from a
cumbersome training process, and leads to semantic misalignment issues between
dialogue generation and item recommendation. In this paper, we represent items
in natural language and formulate CRS as a natural language processing task.
Accordingly, we leverage the power of pre-trained language models to encode
items, understand user intent via conversation, perform item recommendation
through semantic matching, and generate dialogues. As a unified model, our
PECRS (Parameter-Efficient CRS), can be optimized in a single stage, without
relying on non-textual metadata such as a knowledge graph. Experiments on two
benchmark CRS datasets, ReDial and INSPIRED, demonstrate the effectiveness of
PECRS on recommendation and conversation. Our code is available at:
https://github.com/Ravoxsg/efficient_unified_crs.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.14194v3},
File          = {2401.14194v3.pdf}
}
@article{2408.07222v1,
Author        = {Rohan Shawn Sunil and Shan Chun Lim and Manoj Itharajula and Marek Mutwil},
Title         = {The gene function prediction challenge: large language models and
  knowledge graphs to the rescue},
Eprint        = {2408.07222v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.MN},
Abstract      = {Elucidating gene function is one of the ultimate goals of plant science.
Despite this, only ~15% of all genes in the model plant Arabidopsis thaliana
have comprehensively experimentally verified functions. While bioinformatical
gene function prediction approaches can guide biologists in their experimental
efforts, neither the performance of the gene function prediction methods nor
the number of experimental characterisation of genes has increased dramatically
in recent years. In this review, we will discuss the status quo and the
trajectory of gene function elucidation and outline the recent advances in gene
function prediction approaches. We will then discuss how recent artificial
intelligence advances in large language models and knowledge graphs can be
leveraged to accelerate gene function predictions and keep us updated with
scientific literature.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.07222v1},
File          = {2408.07222v1.pdf}
}
@article{2412.17029v1,
Author        = {Yuhao Yang and Jiabin Tang and Lianghao Xia and Xingchen Zou and Yuxuan Liang and Chao Huang},
Title         = {GraphAgent: Agentic Graph Language Assistant},
Eprint        = {2412.17029v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Real-world data is represented in both structured (e.g., graph connections)
and unstructured (e.g., textual, visual information) formats, encompassing
complex relationships that include explicit links (such as social connections
and user behaviors) and implicit interdependencies among semantic entities,
often illustrated through knowledge graphs. In this work, we propose
GraphAgent, an automated agent pipeline that addresses both explicit graph
dependencies and implicit graph-enhanced semantic inter-dependencies, aligning
with practical data scenarios for predictive tasks (e.g., node classification)
and generative tasks (e.g., text generation). GraphAgent comprises three key
components: (i) a Graph Generator Agent that builds knowledge graphs to reflect
complex semantic dependencies; (ii) a Task Planning Agent that interprets
diverse user queries and formulates corresponding tasks through agentic
self-planning; and (iii) a Task Execution Agent that efficiently executes
planned tasks while automating tool matching and invocation in response to user
queries. These agents collaborate seamlessly, integrating language models with
graph language models to uncover intricate relational information and data
semantic dependencies. Through extensive experiments on various graph-related
predictive and text generative tasks on diverse datasets, we demonstrate the
effectiveness of our GraphAgent across various settings. We have made our
proposed GraphAgent open-source at: https://github.com/HKUDS/GraphAgent.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.17029v1},
File          = {2412.17029v1.pdf}
}
@article{1401.6679v1,
Author        = {Robert Jeansoulin and Nic Wilson},
Title         = {Quality of Geographic Information: Ontological approach and Artificial
  Intelligence Tools},
Eprint        = {1401.6679v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The objective is to present one important aspect of the European IST-FET
project "REV!GIS"1: the methodology which has been developed for the
translation (interpretation) of the quality of the data into a "fitness for
use" information, that we can confront to the user needs in its application.
This methodology is based upon the notion of "ontologies" as a conceptual
framework able to capture the explicit and implicit knowledge involved in the
application. We do not address the general problem of formalizing such
ontologies, instead, we rather try to illustrate this with three applications
which are particular cases of the more general "data fusion" problem. In each
application, we show how to deploy our methodology, by comparing several
possible solutions, and we try to enlighten where are the quality issues, and
what kind of solution to privilege, even at the expense of a highly complex
computational approach. The expectation of the REV!GIS project is that
computationally tractable solutions will be available among the next generation
AI tools.},
Year          = {2014},
Month         = {Jan},
Url           = {http://arxiv.org/abs/1401.6679v1},
File          = {1401.6679v1.pdf}
}
@article{2310.07795v1,
Author        = {Siru Ouyang and Jiaxin Huang and Pranav Pillai and Yunyi Zhang and Yu Zhang and Jiawei Han},
Title         = {Ontology Enrichment for Effective Fine-grained Entity Typing},
Eprint        = {2310.07795v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Fine-grained entity typing (FET) is the task of identifying specific entity
types at a fine-grained level for entity mentions based on their contextual
information. Conventional methods for FET require extensive human annotation,
which is time-consuming and costly. Recent studies have been developing weakly
supervised or zero-shot approaches. We study the setting of zero-shot FET where
only an ontology is provided. However, most existing ontology structures lack
rich supporting information and even contain ambiguous relations, making them
ineffective in guiding FET. Recently developed language models, though
promising in various few-shot and zero-shot NLP tasks, may face challenges in
zero-shot FET due to their lack of interaction with task-specific ontology. In
this study, we propose OnEFET, where we (1) enrich each node in the ontology
structure with two types of extra information: instance information for
training sample augmentation and topic information to relate types to contexts,
and (2) develop a coarse-to-fine typing algorithm that exploits the enriched
information by training an entailment model with contrasting topics and
instance-based augmented training samples. Our experiments show that OnEFET
achieves high-quality fine-grained entity typing without human annotation,
outperforming existing zero-shot methods by a large margin and rivaling
supervised methods.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.07795v1},
File          = {2310.07795v1.pdf}
}
@article{2110.05633v1,
Author        = {Mandar Sharma and John S. Brownstein and Naren Ramakrishnan},
Title         = {TCube: Domain-Agnostic Neural Time-series Narration},
Eprint        = {2110.05633v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The task of generating rich and fluent narratives that aptly describe the
characteristics, trends, and anomalies of time-series data is invaluable to the
sciences (geology, meteorology, epidemiology) or finance (trades, stocks, or
sales and inventory). The efforts for time-series narration hitherto are
domain-specific and use predefined templates that offer consistency but lead to
mechanical narratives. We present TCube (Time-series-to-text), a
domain-agnostic neural framework for time-series narration, that couples the
representation of essential time-series elements in the form of a dense
knowledge graph and the translation of said knowledge graph into rich and
fluent narratives through the transfer-learning capabilities of PLMs
(Pre-trained Language Models). TCube's design primarily addresses the challenge
that lies in building a neural framework in the complete paucity of annotated
training data for time-series. The design incorporates knowledge graphs as an
intermediary for the representation of essential time-series elements which can
be linearized for textual translation. To the best of our knowledge, TCube is
the first investigation of the use of neural strategies for time-series
narration. Through extensive evaluations, we show that TCube can improve the
lexical diversity of the generated narratives by up to 65.38% while still
maintaining grammatical integrity. The practicality and deployability of TCube
is further validated through an expert review (n=21) where 76.2% of
participating experts wary of auto-generated narratives favored TCube as a
deployable system for time-series narration due to its richer narratives. Our
code-base, models, and datasets, with detailed instructions for reproducibility
is publicly hosted at https://github.com/Mandar-Sharma/TCube.},
Year          = {2021},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2110.05633v1},
File          = {2110.05633v1.pdf}
}
@article{2211.10738v4,
Author        = {Ke Liang and Yue Liu and Sihang Zhou and Wenxuan Tu and Yi Wen and Xihong Yang and Xiangjun Dong and Xinwang Liu},
Title         = {Knowledge Graph Contrastive Learning Based on Relation-Symmetrical
  Structure},
Eprint        = {2211.10738v4},
DOI           = {10.1109/TKDE.2023.3282989},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge graph embedding (KGE) aims at learning powerful representations to
benefit various artificial intelligence applications. Meanwhile, contrastive
learning has been widely leveraged in graph learning as an effective mechanism
to enhance the discriminative capacity of the learned representations. However,
the complex structures of KG make it hard to construct appropriate contrastive
pairs. Only a few attempts have integrated contrastive learning strategies with
KGE. But, most of them rely on language models ( e.g., Bert) for contrastive
pair construction instead of fully mining information underlying the graph
structure, hindering expressive ability. Surprisingly, we find that the
entities within a relational symmetrical structure are usually similar and
correlated. To this end, we propose a knowledge graph contrastive learning
framework based on relation-symmetrical structure, KGE-SymCL, which mines
symmetrical structure information in KGs to enhance the discriminative ability
of KGE models. Concretely, a plug-and-play approach is proposed by taking
entities in the relation-symmetrical positions as positive pairs. Besides, a
self-supervised alignment loss is designed to pull together positive pairs.
Experimental results on link prediction and entity classification datasets
demonstrate that our KGE-SymCL can be easily adopted to various KGE models for
performance improvements. Moreover, extensive experiments show that our model
could outperform other state-of-the-art baselines.},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.10738v4},
File          = {2211.10738v4.pdf}
}
@article{2211.16504v1,
Author        = {Shuquan Ye and Yujia Xie and Dongdong Chen and Yichong Xu and Lu Yuan and Chenguang Zhu and Jing Liao},
Title         = {Improving Commonsense in Vision-Language Models via Knowledge Graph
  Riddles},
Eprint        = {2211.16504v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {This paper focuses on analyzing and improving the commonsense ability of
recent popular vision-language (VL) models. Despite the great success, we
observe that existing VL-models still lack commonsense knowledge/reasoning
ability (e.g., "Lemons are sour"), which is a vital component towards
artificial general intelligence. Through our analysis, we find one important
reason is that existing large-scale VL datasets do not contain much commonsense
knowledge, which motivates us to improve the commonsense of VL-models from the
data perspective. Rather than collecting a new VL training dataset, we propose
a more scalable strategy, i.e., "Data Augmentation with kNowledge graph
linearization for CommonsensE capability" (DANCE). It can be viewed as one type
of data augmentation technique, which can inject commonsense knowledge into
existing VL datasets on the fly during training. More specifically, we leverage
the commonsense knowledge graph (e.g., ConceptNet) and create variants of text
description in VL datasets via bidirectional sub-graph sequentialization. For
better commonsense evaluation, we further propose the first retrieval-based
commonsense diagnostic benchmark. By conducting extensive experiments on some
representative VL-models, we demonstrate that our DANCE technique is able to
significantly improve the commonsense ability while maintaining the performance
on vanilla retrieval tasks. The code and data are available at
https://github.com/pleaseconnectwifi/DANCE},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.16504v1},
File          = {2211.16504v1.pdf}
}
@article{2212.00959v2,
Author        = {Jinhao Jiang and Kun Zhou and Wayne Xin Zhao and Ji-Rong Wen},
Title         = {UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question
  Answering Over Knowledge Graph},
Eprint        = {2212.00959v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the
answer entities that are multiple hops away from the topic entities mentioned
in a natural language question on a large-scale Knowledge Graph (KG). To cope
with the vast search space, existing work usually adopts a two-stage approach:
it first retrieves a relatively small subgraph related to the question and then
performs the reasoning on the subgraph to find the answer entities accurately.
Although these two stages are highly related, previous work employs very
different technical solutions for developing the retrieval and reasoning
models, neglecting their relatedness in task essence. In this paper, we propose
UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and
reasoning in both model architecture and parameter learning. For model
architecture, UniKGQA consists of a semantic matching module based on a
pre-trained language model~(PLM) for question-relation semantic matching, and a
matching information propagation module to propagate the matching information
along the directed edges on KGs. For parameter learning, we design a shared
pre-training task based on question-relation matching for both retrieval and
reasoning models, and then propose retrieval- and reasoning-oriented
fine-tuning strategies. Compared with previous studies, our approach is more
unified, tightly relating the retrieval and reasoning stages. Extensive
experiments on three benchmark datasets have demonstrated the effectiveness of
our method on the multi-hop KGQA task. Our codes and data are publicly
available at~\url{https://github.com/RUCAIBox/UniKGQA}.},
Year          = {2022},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2212.00959v2},
File          = {2212.00959v2.pdf}
}
@article{2305.12900v2,
Author        = {Jennifer D'Souza and Moussab Hrou and Sören Auer},
Title         = {Evaluating Prompt-based Question Answering for Object Prediction in the
  Open Research Knowledge Graph},
Eprint        = {2305.12900v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {There have been many recent investigations into prompt-based training of
transformer language models for new text genres in low-resource settings. The
prompt-based training approach has been found to be effective in generalizing
pre-trained or fine-tuned models for transfer to resource-scarce settings. This
work, for the first time, reports results on adopting prompt-based training of
transformers for \textit{scholarly knowledge graph object prediction}. The work
is unique in the following two main aspects. 1) It deviates from the other
works proposing entity and relation extraction pipelines for predicting objects
of a scholarly knowledge graph. 2) While other works have tested the method on
text genera relatively close to the general knowledge domain, we test the
method for a significantly different domain, i.e. scholarly knowledge, in turn
testing the linguistic, probabilistic, and factual generalizability of these
large-scale transformer models. We find that (i) per expectations, transformer
models when tested out-of-the-box underperform on a new domain of data, (ii)
prompt-based training of the models achieve performance boosts of up to 40\% in
a relaxed evaluation setting, and (iii) testing the models on a starkly
different domain even with a clever training objective in a low resource
setting makes evident the domain knowledge capture gap offering an
empirically-verified incentive for investing more attention and resources to
the scholarly domain in the context of transformer models.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.12900v2},
File          = {2305.12900v2.pdf}
}
@article{2308.06488v1,
Author        = {Tahsina Hashem and Weiqing Wang and Derry Tanti Wijaya and Mohammed Eunus Ali and Yuan-Fang Li},
Title         = {Generating Faithful Text From a Knowledge Graph with Noisy Reference
  Text},
Eprint        = {2308.06488v1},
DOI           = {10.18653/v1/2023.inlg-main.8},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graph (KG)-to-Text generation aims at generating fluent
natural-language text that accurately represents the information of a given
knowledge graph. While significant progress has been made in this task by
exploiting the power of pre-trained language models (PLMs) with appropriate
graph structure-aware modules, existing models still fall short of generating
faithful text, especially when the ground-truth natural-language text contains
additional information that is not present in the graph. In this paper, we
develop a KG-to-text generation model that can generate faithful
natural-language text from a given graph, in the presence of noisy reference
text. Our framework incorporates two core ideas: Firstly, we utilize
contrastive learning to enhance the model's ability to differentiate between
faithful and hallucinated information in the text, thereby encouraging the
decoder to generate text that aligns with the input graph. Secondly, we empower
the decoder to control the level of hallucination in the generated text by
employing a controllable text generation technique. We evaluate our model's
performance through the standard quantitative metrics as well as a
ChatGPT-based quantitative and qualitative analysis. Our evaluation
demonstrates the superior performance of our model over state-of-the-art
KG-to-text models on faithfulness.},
Year          = {2023},
Month         = {Aug},
Note          = {https://aclanthology.org/2023.inlg-main.8},
Url           = {http://arxiv.org/abs/2308.06488v1},
File          = {2308.06488v1.pdf}
}
@article{2403.06832v4,
Author        = {Zhuo Chen and Yin Fang and Yichi Zhang and Lingbing Guo and Jiaoyan Chen and Jeff Z. Pan and Huajun Chen and Wen Zhang},
Title         = {Noise-powered Multi-modal Knowledge Graph Representation Framework},
Eprint        = {2403.06832v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The rise of Multi-modal Pre-training highlights the necessity for a unified
Multi-Modal Knowledge Graph (MMKG) representation learning framework. Such a
framework is essential for embedding structured knowledge into multi-modal
Large Language Models effectively, alleviating issues like knowledge
misconceptions and multi-modal hallucinations. In this work, we explore the
efficacy of models in accurately embedding entities within MMKGs through two
pivotal tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal
Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG
method that utilizes a Transformer-based architecture equipped with
modality-level noise masking to robustly integrate multi-modal entity features
in KGs. By incorporating specific training objectives for both MKGC and MMEA,
our approach achieves SOTA performance across a total of ten datasets,
demonstrating its versatility. Moreover, SNAG can not only function as a
standalone model but also enhance other existing methods, providing stable
performance improvements. Code and data are available at
https://github.com/zjukg/SNAG.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.06832v4},
File          = {2403.06832v4.pdf}
}
@article{2406.02962v1,
Author        = {Qiang Sun and Yuanyi Luo and Wenxiao Zhang and Sirui Li and Jichunyang Li and Kai Niu and Xiangrui Kong and Wei Liu},
Title         = {Docs2KG: Unified Knowledge Graph Construction from Heterogeneous
  Documents Assisted by Large Language Models},
Eprint        = {2406.02962v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Even for a conservative estimate, 80% of enterprise data reside in
unstructured files, stored in data lakes that accommodate heterogeneous
formats. Classical search engines can no longer meet information seeking needs,
especially when the task is to browse and explore for insight formulation. In
other words, there are no obvious search keywords to use. Knowledge graphs, due
to their natural visual appeals that reduce the human cognitive load, become
the winning candidate for heterogeneous data integration and knowledge
representation.
  In this paper, we introduce Docs2KG, a novel framework designed to extract
multimodal information from diverse and heterogeneous unstructured documents,
including emails, web pages, PDF files, and Excel files. Dynamically generates
a unified knowledge graph that represents the extracted key information,
Docs2KG enables efficient querying and exploration of document data lakes.
Unlike existing approaches that focus on domain-specific data sources or
pre-designed schemas, Docs2KG offers a flexible and extensible solution that
can adapt to various document structures and content types. The proposed
framework unifies data processing supporting a multitude of downstream tasks
with improved domain interpretability. Docs2KG is publicly accessible at
https://docs2kg.ai4wa.com, and a demonstration video is available at
https://docs2kg.ai4wa.com/Video.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.02962v1},
File          = {2406.02962v1.pdf}
}
@article{2406.18815v2,
Author        = {Sanggeon Yun and Ryozo Masukawa and Minhyoung Na and Mohsen Imani},
Title         = {MissionGNN: Hierarchical Multimodal GNN-based Weakly Supervised Video
  Anomaly Recognition with Mission-Specific Knowledge Graph Generation},
Eprint        = {2406.18815v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {In the context of escalating safety concerns across various domains, the
tasks of Video Anomaly Detection (VAD) and Video Anomaly Recognition (VAR) have
emerged as critically important for applications in intelligent surveillance,
evidence investigation, violence alerting, etc. These tasks, aimed at
identifying and classifying deviations from normal behavior in video data, face
significant challenges due to the rarity of anomalies which leads to extremely
imbalanced data and the impracticality of extensive frame-level data annotation
for supervised learning. This paper introduces a novel hierarchical graph
neural network (GNN) based model MissionGNN that addresses these challenges by
leveraging a state-of-the-art large language model and a comprehensive
knowledge graph for efficient weakly supervised learning in VAR. Our approach
circumvents the limitations of previous methods by avoiding heavy gradient
computations on large multimodal models and enabling fully frame-level training
without fixed video segmentation. Utilizing automated, mission-specific
knowledge graph generation, our model provides a practical and efficient
solution for real-time video analysis without the constraints of previous
segmentation-based or multimodal approaches. Experimental validation on
benchmark datasets demonstrates our model's performance in VAD and VAR,
highlighting its potential to redefine the landscape of anomaly detection and
recognition in video surveillance systems.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.18815v2},
File          = {2406.18815v2.pdf}
}
@article{2407.02779v1,
Author        = {Yushan Zhu and Wen Zhang and Zhiqiang Liu and Mingyang Chen and Lei Liang and Huajun Chen},
Title         = {Croppable Knowledge Graph Embedding},
Eprint        = {2407.02779v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge Graph Embedding (KGE) is a common method for Knowledge Graphs (KGs)
to serve various artificial intelligence tasks. The suitable dimensions of the
embeddings depend on the storage and computing conditions of the specific
application scenarios. Once a new dimension is required, a new KGE model needs
to be trained from scratch, which greatly increases the training cost and
limits the efficiency and flexibility of KGE in serving various scenarios. In
this work, we propose a novel KGE training framework MED, through which we
could train once to get a croppable KGE model applicable to multiple scenarios
with different dimensional requirements, sub-models of the required dimensions
can be cropped out of it and used directly without any additional training. In
MED, we propose a mutual learning mechanism to improve the low-dimensional
sub-models performance and make the high-dimensional sub-models retain the
capacity that low-dimensional sub-models have, an evolutionary improvement
mechanism to promote the high-dimensional sub-models to master the knowledge
that the low-dimensional sub-models can not learn, and a dynamic loss weight to
balance the multiple losses adaptively. Experiments on 3 KGE models over 4
standard KG completion datasets, 3 real application scenarios over a real-world
large-scale KG, and the experiments of extending MED to the language model BERT
show the effectiveness, high efficiency, and flexible extensibility of MED.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.02779v1},
File          = {2407.02779v1.pdf}
}
@article{2408.00057v2,
Author        = {Dan Kalifa and Uriel Singer and Kira Radinsky},
Title         = {GOProteinGNN: Leveraging Protein Knowledge Graphs for Protein
  Representation Learning},
Eprint        = {2408.00057v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.BM},
Abstract      = {Proteins play a vital role in biological processes and are indispensable for
living organisms. Accurate representation of proteins is crucial, especially in
drug development. Recently, there has been a notable increase in interest in
utilizing machine learning and deep learning techniques for unsupervised
learning of protein representations. However, these approaches often focus
solely on the amino acid sequence of proteins and lack factual knowledge about
proteins and their interactions, thus limiting their performance. In this
study, we present GOProteinGNN, a novel architecture that enhances protein
language models by integrating protein knowledge graph information during the
creation of amino acid level representations. Our approach allows for the
integration of information at both the individual amino acid level and the
entire protein level, enabling a comprehensive and effective learning process
through graph-based learning. By doing so, we can capture complex relationships
and dependencies between proteins and their functional annotations, resulting
in more robust and contextually enriched protein representations. Unlike
previous methods, GOProteinGNN uniquely learns the entire protein knowledge
graph during training, which allows it to capture broader relational nuances
and dependencies beyond mere triplets as done in previous work. We perform a
comprehensive evaluation on several downstream tasks demonstrating that
GOProteinGNN consistently outperforms previous methods, showcasing its
effectiveness and establishing it as a state-of-the-art solution for protein
representation learning.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2408.00057v2},
File          = {2408.00057v2.pdf}
}
@article{2410.13018v1,
Author        = {Zhaocheng Zhu},
Title         = {Learning Representations for Reasoning: Generalizing Across Diverse
  Structures},
Eprint        = {2410.13018v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Reasoning, the ability to logically draw conclusions from existing knowledge,
is a hallmark of human. Together with perception, they constitute the two major
themes of artificial intelligence. While deep learning has pushed the limit of
perception beyond human-level performance, the progress in reasoning domains is
way behind. One fundamental reason is that reasoning problems usually have
flexible structures for both knowledge and queries, and many existing models
only perform well on structures seen during training. Here we aim to push the
boundary of reasoning models by devising algorithms that generalize across
knowledge and query structures, as well as systems that accelerate development
on structured data. This thesis consists of three parts. In Part I, we study
models that can inductively generalize to unseen knowledge graphs with new
entity and relation vocabularies. For new entities, we propose a framework that
learns neural operators in a dynamic programming algorithm computing path
representations. For relations, we construct a relation graph to capture the
interactions between relations, thereby converting new relations into new
entities. In Part II, we propose two solutions for generalizing across
multi-step queries on knowledge graphs and text respectively. For knowledge
graphs, we show that multi-step queries can be solved by multiple calls of
graph neural networks and fuzzy logic operations. For text, we devise an
algorithm to learn explicit knowledge as textual rules to improve large
language models on multi-step queries. In Part III, we propose two systems to
facilitate machine learning development on structured data. Our library treats
structured data as first-class citizens and removes the barrier for developing
algorithms on structured data. Our node embedding system solves the GPU memory
bottleneck of embedding matrices and scales to graphs with billion nodes.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.13018v1},
File          = {2410.13018v1.pdf}
}
@article{2410.09252v1,
Author        = {Minh Pham Dinh and Munira Syed and Michael G Yankoski and Trenton W. Ford},
Title         = {ReasonPlanner: Enhancing Autonomous Planning in Dynamic Environments
  with Temporal Knowledge Graphs and LLMs},
Eprint        = {2410.09252v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Planning and performing interactive tasks, such as conducting experiments to
determine the melting point of an unknown substance, is straightforward for
humans but poses significant challenges for autonomous agents. We introduce
ReasonPlanner, a novel generalist agent designed for reflective thinking,
planning, and interactive reasoning. This agent leverages LLMs to plan
hypothetical trajectories by building a World Model based on a Temporal
Knowledge Graph. The agent interacts with the environment using a natural
language actor-critic module, where the actor translates the imagined
trajectory into a sequence of actionable steps, and the critic determines if
replanning is necessary. ReasonPlanner significantly outperforms previous
state-of-the-art prompting-based methods on the ScienceWorld benchmark by more
than 1.8 times, while being more sample-efficient and interpretable. It relies
solely on frozen weights thus requiring no gradient updates. ReasonPlanner can
be deployed and utilized without specialized knowledge of Machine Learning,
making it accessible to a wide range of users.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.09252v1},
File          = {2410.09252v1.pdf}
}
@article{2408.13432v1,
Author        = {Yi-Hui Chen and Eric Jui-Lin Lu and Kwan-Ho Cheng},
Title         = {Integrating Multi-Head Convolutional Encoders with Cross-Attention for
  Improved SPARQL Query Translation},
Eprint        = {2408.13432v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The main task of the KGQA system (Knowledge Graph Question Answering) is to
convert user input questions into query syntax (such as SPARQL). With the rise
of modern popular encoders and decoders like Transformer and ConvS2S, many
scholars have shifted the research direction of SPARQL generation to the Neural
Machine Translation (NMT) architecture or the generative AI field of
Text-to-SPARQL. In NMT-based QA systems, the system treats knowledge base query
syntax as a language. It uses NMT-based translation models to translate natural
language questions into query syntax. Scholars use popular architectures
equipped with cross-attention, such as Transformer, ConvS2S, and BiLSTM, to
train translation models for query syntax. To achieve better query results,
this paper improved the ConvS2S encoder and added multi-head attention from the
Transformer, proposing a Multi-Head Conv encoder (MHC encoder) based on the
n-gram language model. The principle is to use convolutional layers to capture
local hidden features in the input sequence with different receptive fields,
using multi-head attention to calculate dependencies between them. Ultimately,
we found that the translation model based on the Multi-Head Conv encoder
achieved better performance than other encoders, obtaining 76.52\% and 83.37\%
BLEU-1 (BiLingual Evaluation Understudy) on the QALD-9 and LC-QuAD-1.0
datasets, respectively. Additionally, in the end-to-end system experiments on
the QALD-9 and LC-QuAD-1.0 datasets, we achieved leading results over other
KGQA systems, with Macro F1-measures reaching 52\% and 66\%, respectively.
Moreover, the experimental results show that with limited computational
resources, if one possesses an excellent encoder-decoder architecture and
cross-attention, experts and scholars can achieve outstanding performance
equivalent to large pre-trained models using only general embeddings.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.13432v1},
File          = {2408.13432v1.pdf}
}
@article{2310.13595v2,
Author        = {Nathan Lambert and Thomas Krendl Gilbert and Tom Zick},
Title         = {The History and Risks of Reinforcement Learning and Human Feedback},
Eprint        = {2310.13595v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {Reinforcement learning from human feedback (RLHF) has emerged as a powerful
technique to make large language models (LLMs) easier to use and more
effective. A core piece of the RLHF process is the training and utilization of
a model of human preferences that acts as a reward function for optimization.
This approach, which operates at the intersection of many stakeholders and
academic disciplines, remains poorly understood. RLHF reward models are often
cited as being central to achieving performance, yet very few descriptors of
capabilities, evaluations, training methods, or open-source models exist. Given
this lack of information, further study and transparency is needed for learned
RLHF reward models. In this paper, we illustrate the complex history of
optimizing preferences, and articulate lines of inquiry to understand the
sociotechnical context of reward models. In particular, we highlight the
ontological differences between costs, rewards, and preferences at stake in
RLHF's foundations, related methodological tensions, and possible research
directions to improve general understanding of how reward models function.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.13595v2},
File          = {2310.13595v2.pdf}
}
@article{2404.18681v1,
Author        = {Fabian Biester and Mohamed Abdelaal and Daniel Del Gaudio},
Title         = {LLMClean: Context-Aware Tabular Data Cleaning via LLM-Generated OFDs},
Eprint        = {2404.18681v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Machine learning's influence is expanding rapidly, now integral to
decision-making processes from corporate strategy to the advancements in
Industry 4.0. The efficacy of Artificial Intelligence broadly hinges on the
caliber of data used during its training phase; optimal performance is tied to
exceptional data quality. Data cleaning tools, particularly those that exploit
functional dependencies within ontological frameworks or context models, are
instrumental in augmenting data quality. Nevertheless, crafting these context
models is a demanding task, both in terms of resources and expertise, often
necessitating specialized knowledge from domain experts.
  In light of these challenges, this paper introduces an innovative approach,
called LLMClean, for the automated generation of context models, utilizing
Large Language Models to analyze and understand various datasets. LLMClean
encompasses a sequence of actions, starting with categorizing the dataset,
extracting or mapping relevant models, and ultimately synthesizing the context
model. To demonstrate its potential, we have developed and tested a prototype
that applies our approach to three distinct datasets from the Internet of
Things, healthcare, and Industry 4.0 sectors. The results of our evaluation
indicate that our automated approach can achieve data cleaning efficacy
comparable with that of context models crafted by human experts.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.18681v1},
File          = {2404.18681v1.pdf}
}
@article{2410.05252v1,
Author        = {Mourad Heddaya and Qingcheng Zeng and Chenhao Tan and Rob Voigt and Alexander Zentefis},
Title         = {Causal Micro-Narratives},
Eprint        = {2410.05252v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present a novel approach to classify causal micro-narratives from text.
These narratives are sentence-level explanations of the cause(s) and/or
effect(s) of a target subject. The approach requires only a subject-specific
ontology of causes and effects, and we demonstrate it with an application to
inflation narratives. Using a human-annotated dataset spanning historical and
contemporary US news articles for training, we evaluate several large language
models (LLMs) on this multi-label classification task. The best-performing
model--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative
detection and 0.71 on narrative classification. Comprehensive error analysis
reveals challenges arising from linguistic ambiguity and highlights how model
errors often mirror human annotator disagreements. This research establishes a
framework for extracting causal micro-narratives from real-world data, with
wide-ranging applications to social science research.},
Year          = {2024},
Month         = {Oct},
Note          = {Proceedings of the The 6th Workshop on Narrative Understanding at
  EMNLP 2024, pages 67-84, Miami, Florida, USA. Association for Computational
  Linguistics},
Url           = {http://arxiv.org/abs/2410.05252v1},
File          = {2410.05252v1.pdf}
}
@article{2310.04304v1,
Author        = {Ahmed R. Sadik and Sebastian Brulin and Markus Olhofer},
Title         = {Coding by Design: GPT-4 empowers Agile Model Driven Development},
Eprint        = {2310.04304v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Generating code from a natural language using Large Language Models (LLMs)
such as ChatGPT, seems groundbreaking. Yet, with more extensive use, it's
evident that this approach has its own limitations. The inherent ambiguity of
natural language presents challenges for complex software designs. Accordingly,
our research offers an Agile Model-Driven Development (MDD) approach that
enhances code auto-generation using OpenAI's GPT-4. Our work emphasizes
"Agility" as a significant contribution to the current MDD method, particularly
when the model undergoes changes or needs deployment in a different programming
language. Thus, we present a case-study showcasing a multi-agent simulation
system of an Unmanned Vehicle Fleet. In the first and second layer of our
approach, we constructed a textual representation of the case-study using
Unified Model Language (UML) diagrams. In the next layer, we introduced two
sets of constraints that minimize model ambiguity. Object Constraints Language
(OCL) is applied to fine-tune the code constructions details, while FIPA
ontology is used to shape communication semantics and protocols. Ultimately,
leveraging GPT-4, our last layer auto-generates code in both Java and Python.
The Java code is deployed within the JADE framework, while the Python code is
deployed in PADE framework. Concluding our research, we engaged in a
comprehensive evaluation of the generated code. From a behavioural standpoint,
the auto-generated code aligned perfectly with the expected UML sequence
diagram. Structurally, we compared the complexity of code derived from UML
diagrams constrained solely by OCL to that influenced by both OCL and
FIPA-ontology. Results indicate that ontology-constrained model produce
inherently more intricate code, but it remains manageable and low-risk for
further testing and maintenance.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.04304v1},
File          = {2310.04304v1.pdf}
}
@article{2402.00048v1,
Author        = {Bruno Sartini},
Title         = {IICONGRAPH: improved Iconographic and Iconological Statements in
  Knowledge Graphs},
Eprint        = {2402.00048v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Iconography and iconology are fundamental domains when it comes to
understanding artifacts of cultural heritage. Iconography deals with the study
and interpretation of visual elements depicted in artifacts and their
symbolism, while iconology delves deeper, exploring the underlying cultural and
historical meanings. Despite the advances in representing cultural heritage
with Linked Open Data (LOD), recent studies show persistent gaps in the
representation of iconographic and iconological statements in current knowledge
graphs (KGs). To address them, this paper presents IICONGRAPH, a KG that was
created by refining and extending the iconographic and iconological statements
of ArCo (the Italian KG of cultural heritage) and Wikidata. The development of
IICONGRAPH was also driven by a series of requirements emerging from research
case studies that were unattainable in the non-reengineered versions of the
KGs. The evaluation results demonstrate that IICONGRAPH not only outperforms
ArCo and Wikidata through domain-specific assessments from the literature but
also serves as a robust platform for addressing the formulated research
questions. IICONGRAPH is released and documented in accordance with the FAIR
principles to guarantee the resource's reusability. The algorithms used to
create it and assess the research questions have also been made available to
ensure transparency and reproducibility. While future work focuses on ingesting
more data into the KG, and on implementing it as a backbone of LLM-based
question answering systems, the current version of IICONGRAPH still emerges as
a valuable asset, contributing to the evolving landscape of cultural heritage
representation within Knowledge Graphs, the Semantic Web, and beyond.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2402.00048v1},
File          = {2402.00048v1.pdf}
}
@article{2409.18924v2,
Author        = {Huizi Yu and Jiayan Zhou and Lingyao Li and Shan Chen and Jack Gallifant and Anye Shi and Xiang Li and Wenyue Hua and Mingyu Jin and Guang Chen and Yang Zhou and Zhao Li and Trisha Gupte and Ming-Li Chen and Zahra Azizi and Yongfeng Zhang and Themistocles L. Assimes and Xin Ma and Danielle S. Bitterman and Lin Lu and Lizhou Fan},
Title         = {AIPatient: Simulating Patients with EHRs and LLM Powered Agentic
  Workflow},
Eprint        = {2409.18924v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Simulated patient systems play a crucial role in modern medical education and
research, providing safe, integrative learning environments and enabling
clinical decision-making simulations. Large Language Models (LLM) could advance
simulated patient systems by replicating medical conditions and patient-doctor
interactions with high fidelity and low cost. However, ensuring the
effectiveness and trustworthiness of these systems remains a challenge, as they
require a large, diverse, and precise patient knowledgebase, along with a
robust and stable knowledge diffusion to users. Here, we developed AIPatient,
an advanced simulated patient system with AIPatient Knowledge Graph (AIPatient
KG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning
RAG) agentic workflow as the generation backbone. AIPatient KG samples data
from Electronic Health Records (EHRs) in the Medical Information Mart for
Intensive Care (MIMIC)-III database, producing a clinically diverse and
relevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).
Reasoning RAG leverages six LLM powered agents spanning tasks including
retrieval, KG query generation, abstraction, checker, rewrite, and
summarization. This agentic framework reaches an overall accuracy of 94.15% in
EHR-based medical Question Answering (QA), outperforming benchmarks that use
either no agent or only partial agent integration. Our system also presents
high readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade
5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value
0.782, p>0.1). The promising performance of the AIPatient system highlights its
potential to support a wide range of applications, including medical education,
model evaluation, and system integration.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.18924v2},
File          = {2409.18924v2.pdf}
}
@article{2308.05567v1,
Author        = {Pan Liang and Danwei Ye and Zihao Zhu and Yunchao Wang and Wang Xia and Ronghua Liang and Guodao Sun},
Title         = {C5: Towards Better Conversation Comprehension and Contextual Continuity
  for ChatGPT},
Eprint        = {2308.05567v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Large language models (LLMs), such as ChatGPT, have demonstrated outstanding
performance in various fields, particularly in natural language understanding
and generation tasks. In complex application scenarios, users tend to engage in
multi-turn conversations with ChatGPT to keep contextual information and obtain
comprehensive responses. However, human forgetting and model contextual
forgetting remain prominent issues in multi-turn conversation scenarios, which
challenge the users' conversation comprehension and contextual continuity for
ChatGPT. To address these challenges, we propose an interactive conversation
visualization system called C5, which includes Global View, Topic View, and
Context-associated Q\&A View. The Global View uses the GitLog diagram metaphor
to represent the conversation structure, presenting the trend of conversation
evolution and supporting the exploration of locally salient features. The Topic
View is designed to display all the question and answer nodes and their
relationships within a topic using the structure of a knowledge graph, thereby
display the relevance and evolution of conversations. The Context-associated
Q\&A View consists of three linked views, which allow users to explore
individual conversations deeply while providing specific contextual information
when posing questions. The usefulness and effectiveness of C5 were evaluated
through a case study and a user study.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.05567v1},
File          = {2308.05567v1.pdf}
}
@article{2310.13848v2,
Author        = {Priyanka Ranade and Anupam Joshi},
Title         = {FABULA: Intelligence Report Generation Using Retrieval-Augmented
  Narrative Construction},
Eprint        = {2310.13848v2},
DOI           = {10.1145/3625007.3627505},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Narrative construction is the process of representing disparate event
information into a logical plot structure that models an end to end story.
Intelligence analysis is an example of a domain that can benefit tremendously
from narrative construction techniques, particularly in aiding analysts during
the largely manual and costly process of synthesizing event information into
comprehensive intelligence reports. Manual intelligence report generation is
often prone to challenges such as integrating dynamic event information,
writing fine-grained queries, and closing information gaps. This motivates the
development of a system that retrieves and represents critical aspects of
events in a form that aids in automatic generation of intelligence reports.
  We introduce a Retrieval Augmented Generation (RAG) approach to augment
prompting of an autoregressive decoder by retrieving structured information
asserted in a knowledge graph to generate targeted information based on a
narrative plot model. We apply our approach to the problem of neural
intelligence report generation and introduce FABULA, framework to augment
intelligence analysis workflows using RAG. An analyst can use FABULA to query
an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be
used to augment prompting of a Large Language Model (LLM) during intelligence
report generation. Our evaluation studies show that the plot points included in
the generated intelligence reports have high semantic relevance, high
coherency, and low data redundancy.},
Year          = {2023},
Month         = {Oct},
Note          = {2023 IEEE/ACM International Conference on Advances in Social
  Networks Analysis and Mining (ASONAM)},
Url           = {http://arxiv.org/abs/2310.13848v2},
File          = {2310.13848v2.pdf}
}
@article{2312.08579v2,
Author        = {Golnaz Shapurian and Michael J Kurtz and Alberto Accomazzi},
Title         = {Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach},
Eprint        = {2312.08579v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The automatic identification of planetary feature names in astronomy
publications presents numerous challenges. These features include craters,
defined as roughly circular depressions resulting from impact or volcanic
activity; dorsas, which are elongate raised structures or wrinkle ridges; and
lacus, small irregular patches of dark, smooth material on the Moon, referred
to as "lake" (Planetary Names Working Group, n.d.). Many feature names overlap
with places or people's names that they are named after, for example, Syria,
Tempe, Einstein, and Sagan, to name a few (U.S. Geological Survey, n.d.). Some
feature names have been used in many contexts, for instance, Apollo, which can
refer to mission, program, sample, astronaut, seismic, seismometers, core, era,
data, collection, instrument, and station, in addition to the crater on the
Moon. Some feature names can appear in the text as adjectives, like the lunar
craters Black, Green, and White. Some feature names in other contexts serve as
directions, like craters West and South on the Moon. Additionally, some
features share identical names across different celestial bodies, requiring
disambiguation, such as the Adams crater, which exists on both the Moon and
Mars. We present a multi-step pipeline combining rule-based filtering,
statistical relevance analysis, part-of-speech (POS) tagging, named entity
recognition (NER) model, hybrid keyword harvesting, knowledge graph (KG)
matching, and inference with a locally installed large language model (LLM) to
reliably identify planetary names despite these challenges. When evaluated on a
dataset of astronomy papers from the Astrophysics Data System (ADS), this
methodology achieves an F1-score over 0.97 in disambiguating planetary feature
names.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.08579v2},
File          = {2312.08579v2.pdf}
}
@article{2401.04361v1,
Author        = {Jiaan Wang and Jianfeng Qu and Kexin Wang and Zhixu Li and Wen Hua and Ximing Li and An Liu},
Title         = {Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive
  Learning},
Eprint        = {2401.04361v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge-grounded dialogue (KGD) learns to generate an informative response
based on a given dialogue context and external knowledge (\emph{e.g.},
knowledge graphs; KGs). Recently, the emergence of large language models (LLMs)
and pre-training techniques has brought great success to knowledge-grounded
dialogue. However, when building KGD systems in real applications, there are
various real-world noises that are inevitable to face. For example, the
dialogue context might involve perturbations such as misspellings and
abbreviations. In addition, KGs typically suffer from incompletion and also
might contain erroneous and outdated facts. Such real-world noises pose a
challenge to the robustness of KGD systems and hinder their applications in the
real world. In this paper, we propose an entity-based contrastive learning
framework for improving the robustness of KGD. Specifically, we make use of the
entity information in a KGD sample to create both its positive and negative
samples which involve semantic-irrelevant and semantic-relevant perturbations,
respectively. The contrastive learning framework ensures the KGD model is aware
of these two types of perturbations, thus generating informative responses with
the potentially noisy inputs in real applications. Experimental results on
three benchmark datasets show that our method achieves new state-of-the-art
performance in terms of automatic evaluation scores, verifying its
effectiveness and potentiality. Furthermore, we show that our method can
generate better responses than comparison models in both the noisy and the
few-shot settings.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.04361v1},
File          = {2401.04361v1.pdf}
}
@article{2403.10171v2,
Author        = {Arkajit Datta and Tushar Verma and Rajat Chawla and Mukunda N. S and Ishaan Bhola},
Title         = {AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI
  Automation},
Eprint        = {2403.10171v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In recent advancements within the domain of Large Language Models (LLMs),
there has been a notable emergence of agents capable of addressing Robotic
Process Automation (RPA) challenges through enhanced cognitive capabilities and
sophisticated reasoning. This development heralds a new era of scalability and
human-like adaptability in goal attainment. In this context, we introduce
AUTONODE (Autonomous User-interface Transformation through Online Neuro-graphic
Operations and Deep Exploration). AUTONODE employs advanced neuro-graphical
techniques to facilitate autonomous navigation and task execution on web
interfaces, thereby obviating the necessity for predefined scripts or manual
intervention. Our engine empowers agents to comprehend and implement complex
workflows, adapting to dynamic web environments with unparalleled efficiency.
Our methodology synergizes cognitive functionalities with robotic automation,
endowing AUTONODE with the ability to learn from experience. We have integrated
an exploratory module, DoRA (Discovery and mapping Operation for graph
Retrieval Agent), which is instrumental in constructing a knowledge graph that
the engine utilizes to optimize its actions and achieve objectives with minimal
supervision. The versatility and efficacy of AUTONODE are demonstrated through
a series of experiments, highlighting its proficiency in managing a diverse
array of web-based tasks, ranging from data extraction to transaction
processing.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.10171v2},
File          = {2403.10171v2.pdf}
}
@article{2405.09713v2,
Author        = {Andong Wang and Bo Wu and Sunli Chen and Zhenfang Chen and Haotian Guan and Wei-Ning Lee and Li Erran Li and Chuang Gan},
Title         = {SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World
  Knowledge},
Eprint        = {2405.09713v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Learning commonsense reasoning from visual contexts and scenes in real-world
is a crucial step toward advanced artificial intelligence. However, existing
video reasoning benchmarks are still inadequate since they were mainly designed
for factual or situated reasoning and rarely involve broader knowledge in the
real world. Our work aims to delve deeper into reasoning evaluations,
specifically within dynamic, open-world, and structured context knowledge. We
propose a new benchmark (SOK-Bench), consisting of 44K questions and 10K
situations with instance-level annotations depicted in the videos. The
reasoning process is required to understand and apply situated knowledge and
general knowledge for problem-solving. To create such a dataset, we propose an
automatic and scalable generation method to generate question-answer pairs,
knowledge graphs, and rationales by instructing the combinations of LLMs and
MLLMs. Concretely, we first extract observable situated entities, relations,
and processes from videos for situated knowledge and then extend to open-world
knowledge beyond the visible content. The task generation is facilitated
through multiple dialogues as iterations and subsequently corrected and refined
by our designed self-promptings and demonstrations. With a corpus of both
explicit situated facts and implicit commonsense, we generate associated
question-answer pairs and reasoning processes, finally followed by manual
reviews for quality assurance. We evaluated recent mainstream large
vision-language models on the benchmark and found several insightful
conclusions. For more information, please refer to our benchmark at
www.bobbywu.com/SOKBench.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.09713v2},
File          = {2405.09713v2.pdf}
}
@article{2406.00036v1,
Author        = {Yinghao Zhu and Changyu Ren and Zixiang Wang and Xiaochen Zheng and Shiyun Xie and Junlan Feng and Xi Zhu and Zhoujun Li and Liantao Ma and Chengwei Pan},
Title         = {EMERGE: Integrating RAG for Improved Multimodal EHR Predictive Modeling},
Eprint        = {2406.00036v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The integration of multimodal Electronic Health Records (EHR) data has
notably advanced clinical predictive capabilities. However, current models that
utilize clinical notes and multivariate time-series EHR data often lack the
necessary medical context for precise clinical tasks. Previous methods using
knowledge graphs (KGs) primarily focus on structured knowledge extraction. To
address this, we propose EMERGE, a Retrieval-Augmented Generation (RAG) driven
framework aimed at enhancing multimodal EHR predictive modeling. Our approach
extracts entities from both time-series data and clinical notes by prompting
Large Language Models (LLMs) and aligns them with professional PrimeKG to
ensure consistency. Beyond triplet relationships, we include entities'
definitions and descriptions to provide richer semantics. The extracted
knowledge is then used to generate task-relevant summaries of patients' health
statuses. These summaries are fused with other modalities utilizing an adaptive
multimodal fusion network with cross-attention. Extensive experiments on the
MIMIC-III and MIMIC-IV datasets for in-hospital mortality and 30-day
readmission tasks demonstrate the superior performance of the EMERGE framework
compared to baseline models. Comprehensive ablation studies and analyses
underscore the efficacy of each designed module and the framework's robustness
to data sparsity. EMERGE significantly enhances the use of multimodal EHR data
in healthcare, bridging the gap with nuanced medical contexts crucial for
informed clinical predictions.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2406.00036v1},
File          = {2406.00036v1.pdf}
}
@article{2406.01422v1,
Author        = {Yingwei Ma and Qingping Yang and Rongyu Cao and Binhua Li and Fei Huang and Yongbin Li},
Title         = {How to Understand Whole Software Repository?},
Eprint        = {2406.01422v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Recently, Large Language Model (LLM) based agents have advanced the
significant development of Automatic Software Engineering (ASE). Although
verified effectiveness, the designs of the existing methods mainly focus on the
local information of codes, e.g., issues, classes, and functions, leading to
limitations in capturing the global context and interdependencies within the
software system. From the practical experiences of the human SE developers, we
argue that an excellent understanding of the whole repository will be the
critical path to ASE. However, understanding the whole repository raises
various challenges, e.g., the extremely long code input, the noisy code
information, the complex dependency relationships, etc. To this end, we develop
a novel ASE method named RepoUnderstander by guiding agents to comprehensively
understand the whole repositories. Specifically, we first condense the critical
information of the whole repository into the repository knowledge graph in a
top-to-down mode to decrease the complexity of repository. Subsequently, we
empower the agents the ability of understanding whole repository by proposing a
Monte Carlo tree search based repository exploration strategy. In addition, to
better utilize the repository-level knowledge, we guide the agents to
summarize, analyze, and plan. Then, they can manipulate the tools to
dynamically acquire information and generate the patches to solve the
real-world GitHub issues. Extensive experiments demonstrate the superiority and
effectiveness of the proposed RepoUnderstander. It achieved 18.5\% relative
improvement on the SWE-bench Lite benchmark compared to SWE-agent.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.01422v1},
File          = {2406.01422v1.pdf}
}
@article{2409.09046v1,
Author        = {Rishi Kalra and Zekun Wu and Ayesha Gulley and Airlie Hilliard and Xin Guan and Adriano Koshiyama and Philip Treleaven},
Title         = {HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation
  System for AI Legal and Policy Applications},
Eprint        = {2409.09046v1},
DOI           = {10.18653/v1/2024.customnlp4u-1.18},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {While Large Language Models (LLMs) excel in text generation and
question-answering, their effectiveness in AI legal and policy is limited by
outdated knowledge, hallucinations, and inadequate reasoning in complex
contexts. Retrieval-Augmented Generation (RAG) systems improve response
accuracy by integrating external knowledge but struggle with retrieval errors,
poor context integration, and high costs, particularly in interpreting
qualitative and quantitative AI legal texts. This paper introduces a Hybrid
Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy,
exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity
classifier for adaptive parameter tuning, a hybrid retrieval strategy combining
dense, sparse, and knowledge graph methods, and an evaluation framework with
specific question types and metrics. By dynamically adjusting parameters,
HyPA-RAG significantly improves retrieval accuracy and response fidelity.
Testing on LL144 shows enhanced correctness, faithfulness, and contextual
precision, addressing the need for adaptable NLP systems in complex,
high-stakes AI legal and policy applications.},
Year          = {2024},
Month         = {Aug},
Note          = {Proceedings of the 1st Workshop on Customizable NLP: Progress and
  Challenges in Customizing NLP for a Domain, Application, Group, or Individual
  (CustomNLP4U), ACL 2024, Miami, Florida, USA},
Url           = {http://arxiv.org/abs/2409.09046v1},
File          = {2409.09046v1.pdf}
}
@article{2411.07163v1,
Author        = {Vedant Khandelwal and Manas Gaur and Ugur Kursuncu and Valerie Shalin and Amit Sheth},
Title         = {A Domain-Agnostic Neurosymbolic Approach for Big Social Data Analysis:
  Evaluating Mental Health Sentiment on Social Media during COVID-19},
Eprint        = {2411.07163v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Monitoring public sentiment via social media is potentially helpful during
health crises such as the COVID-19 pandemic. However, traditional
frequency-based, data-driven neural network-based approaches can miss newly
relevant content due to the evolving nature of language in a dynamically
evolving environment. Human-curated symbolic knowledge sources, such as
lexicons for standard language and slang terms, can potentially elevate social
media signals in evolving language. We introduce a neurosymbolic method that
integrates neural networks with symbolic knowledge sources, enhancing the
detection and interpretation of mental health-related tweets relevant to
COVID-19. Our method was evaluated using a corpus of large datasets
(approximately 12 billion tweets, 2.5 million subreddit data, and 700k news
articles) and multiple knowledge graphs. This method dynamically adapts to
evolving language, outperforming purely data-driven models with an F1 score
exceeding 92\%. This approach also showed faster adaptation to new data and
lower computational demands than fine-tuning pre-trained large language models
(LLMs). This study demonstrates the benefit of neurosymbolic methods in
interpreting text in a dynamic environment for tasks such as health
surveillance.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.07163v1},
File          = {2411.07163v1.pdf}
}
@article{2411.11714v1,
Author        = {Mingchao Qi and Yuanjin Li and Xing Liu and Zhengxiong Liu and Panfeng Huang},
Title         = {Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via
  Skill Library and Tactile Representation},
Eprint        = {2411.11714v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Deploying robots in open-world environments involves complex tasks
characterized by long sequences and rich interactions, necessitating efficient
transfer of robotic skills across diverse and complex scenarios. To address
this challenge, we propose a skill library framework based on knowledge graphs,
which endows robots with high-level skill awareness and spatial semantic
understanding. The framework hierarchically organizes operational knowledge by
constructing a "task graph" and a "scene graph" to represent task and scene
semantic information, respectively. We introduce a "state graph" to facilitate
interaction between high-level task planning and low-level scene information.
Furthermore, we propose a hierarchical transfer framework for operational
skills. At the task level, the framework integrates contextual learning and
chain-of-thought prompting within a four-stage prompt paradigm, leveraging
large language models' (LLMs) reasoning and generalization capabilities to
achieve task-level subtask sequence transfer. At the motion level, an adaptive
trajectory transfer method is developed using the A* algorithm and the skill
library, enabling motion-level adaptive trajectory transfer. At the physical
level, we introduce an adaptive contour extraction and posture perception
method based on tactile perception. This method dynamically obtains
high-precision contour and posture information from visual-tactile texture data
and adjusts transferred skills, such as contact positions and postures, to
ensure effectiveness in new environments. Experimental results validate the
effectiveness of the proposed methods. Project
website:https://github.com/MingchaoQi/skill_transfer},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.11714v1},
File          = {2411.11714v1.pdf}
}
@article{2412.00224v1,
Author        = {Saurabh Mishra and Mahendra Shinde and Aniket Yadav and Bilal Ayyub and Anand Rao},
Title         = {An AI-Driven Data Mesh Architecture Enhancing Decision-Making in
  Infrastructure Construction and Public Procurement},
Eprint        = {2412.00224v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Infrastructure construction, often dubbed an "industry of industries," is
closely linked with government spending and public procurement, offering
significant opportunities for improved efficiency and productivity through
better transparency and information access. By leveraging these opportunities,
we can achieve notable gains in productivity, cost savings, and broader
economic benefits. Our approach introduces an integrated software ecosystem
utilizing Data Mesh and Service Mesh architectures. This system includes the
largest training dataset for infrastructure and procurement, encompassing over
100 billion tokens, scientific publications, activities, and risk data, all
structured by a systematic AI framework. Supported by a Knowledge Graph linked
to domain-specific multi-agent tasks and Q&A capabilities, our platform
standardizes and ingests diverse data sources, transforming them into
structured knowledge. Leveraging large language models (LLMs) and automation,
our system revolutionizes data structuring and knowledge creation, aiding
decision-making in early-stage project planning, detailed research, market
trend analysis, and qualitative assessments. Its web-scalable architecture
delivers domain-curated information, enabling AI agents to facilitate reasoning
and manage uncertainties, while preparing for future expansions with
specialized agents targeting particular challenges. This integration of AI with
domain expertise not only boosts efficiency and decision-making in construction
and infrastructure but also establishes a framework for enhancing government
efficiency and accelerating the transition of traditional industries to digital
workflows. This work is poised to significantly influence AI-driven initiatives
in this sector and guide best practices in AI Operations.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2412.00224v1},
File          = {2412.00224v1.pdf}
}
@article{2502.01113v1,
Author        = {Linhao Luo and Zicheng Zhao and Gholamreza Haffari and Dinh Phung and Chen Gong and Shirui Pan},
Title         = {GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation},
Eprint        = {2502.01113v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Retrieval-augmented generation (RAG) has proven effective in integrating
knowledge into large language models (LLMs). However, conventional RAGs
struggle to capture complex relationships between pieces of knowledge, limiting
their performance in intricate reasoning that requires integrating knowledge
from multiple sources. Recently, graph-enhanced retrieval augmented generation
(GraphRAG) builds graph structure to explicitly model these relationships,
enabling more effective and efficient retrievers. Nevertheless, its performance
is still hindered by the noise and incompleteness within the graph structure.
To address this, we introduce GFM-RAG, a novel graph foundation model (GFM) for
retrieval augmented generation. GFM-RAG is powered by an innovative graph
neural network that reasons over graph structure to capture complex
query-knowledge relationships. The GFM with 8M parameters undergoes a two-stage
training process on large-scale datasets, comprising 60 knowledge graphs with
over 14M triples and 700k documents. This results in impressive performance and
generalizability for GFM-RAG, making it the first graph foundation model
applicable to unseen datasets for retrieval without any fine-tuning required.
Extensive experiments on three multi-hop QA datasets and seven domain-specific
RAG datasets demonstrate that GFM-RAG achieves state-of-the-art performance
while maintaining efficiency and alignment with neural scaling laws,
highlighting its potential for further improvement.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.01113v1},
File          = {2502.01113v1.pdf}
}
@article{2403.17216v1,
Author        = {Na Li and Thomas Bailleux and Zied Bouraoui and Steven Schockaert},
Title         = {Ontology Completion with Natural Language Inference and Concept
  Embeddings: An Analysis},
Eprint        = {2403.17216v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We consider the problem of finding plausible knowledge that is missing from a
given ontology, as a generalisation of the well-studied taxonomy expansion
task. One line of work treats this task as a Natural Language Inference (NLI)
problem, thus relying on the knowledge captured by language models to identify
the missing knowledge. Another line of work uses concept embeddings to identify
what different concepts have in common, taking inspiration from cognitive
models for category based induction. These two approaches are intuitively
complementary, but their effectiveness has not yet been compared. In this
paper, we introduce a benchmark for evaluating ontology completion methods and
thoroughly analyse the strengths and weaknesses of both approaches. We find
that both approaches are indeed complementary, with hybrid strategies achieving
the best overall results. We also find that the task is highly challenging for
Large Language Models, even after fine-tuning.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.17216v1},
File          = {2403.17216v1.pdf}
}
@article{2402.05391v4,
Author        = {Zhuo Chen and Yichi Zhang and Yin Fang and Yuxia Geng and Lingbing Guo and Xiang Chen and Qian Li and Wen Zhang and Jiaoyan Chen and Yushan Zhu and Jiaqi Li and Xiaoze Liu and Jeff Z. Pan and Ningyu Zhang and Huajun Chen},
Title         = {Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey},
Eprint        = {2402.05391v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge Graphs (KGs) play a pivotal role in advancing various AI
applications, with the semantic web community's exploration into multi-modal
dimensions unlocking new avenues for innovation. In this survey, we carefully
review over 300 articles, focusing on KG-aware research in two principal
aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal
tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into
the MMKG realm. We begin by defining KGs and MMKGs, then explore their
construction progress. Our review includes two primary task categories:
KG-aware multi-modal learning tasks, such as Image Classification and Visual
Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph
Completion and Entity Alignment, highlighting specific research trajectories.
For most of these tasks, we provide definitions, evaluation benchmarks, and
additionally outline essential insights for conducting relevant research.
Finally, we discuss current challenges and identify emerging trends, such as
progress in Large Language Modeling and Multi-modal Pre-training strategies.
This survey aims to serve as a comprehensive reference for researchers already
involved in or considering delving into KG and multi-modal learning research,
offering insights into the evolving landscape of MMKG research and supporting
future work.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.05391v4},
File          = {2402.05391v4.pdf}
}
@article{2411.02730v1,
Author        = {Zexu Li and Suraj P. Prabhu and Zachary T. Popp and Shubhi S. Jain and Vijetha Balakundi and Ting Fang Alvin Ang and Rhoda Au and Jinying Chen},
Title         = {A Natural Language Processing Approach to Support Biomedical Data
  Harmonization: Leveraging Large Language Models},
Eprint        = {2411.02730v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Biomedical research requires large, diverse samples to produce unbiased
results. Automated methods for matching variables across datasets can
accelerate this process. Research in this area has been limited, primarily
focusing on lexical matching and ontology based semantic matching. We aimed to
develop new methods, leveraging large language models (LLM) and ensemble
learning, to automate variable matching. Methods: We utilized data from two
GERAS cohort (European and Japan) studies to develop variable matching methods.
We first manually created a dataset by matching 352 EU variables with 1322
candidate JP variables, where matched variable pairs were positive and
unmatched pairs were negative instances. Using this dataset, we developed and
evaluated two types of natural language processing (NLP) methods, which matched
variables based on variable labels and definitions from data dictionaries: (1)
LLM-based and (2) fuzzy matching. We then developed an ensemble-learning
method, using the Random Forest model, to integrate individual NLP methods. RF
was trained and evaluated on 50 trials. Each trial had a random split (4:1) of
training and test sets, with the model's hyperparameters optimized through
cross-validation on the training set. For each EU variable, 1322 candidate JP
variables were ranked based on NLP-derived similarity scores or RF's
probability scores, denoting their likelihood to match the EU variable. Ranking
performance was measured by top-n hit ratio (HRn) and mean reciprocal rank
(MRR). Results:E5 performed best among individual methods, achieving 0.90 HR-30
and 0.70 MRR. RF performed better than E5 on all metrics over 50 trials (P less
than 0.001) and achieved an average HR 30 of 0.98 and MRR of 0.73. LLM-derived
features contributed most to RF's performance. One major cause of errors in
automatic variable matching was ambiguous variable definitions within data
dictionaries.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.02730v1},
File          = {2411.02730v1.pdf}
}
@article{2412.09587v1,
Author        = {Chester Palen-Michel and Maxwell Pickering and Maya Kruse and Jonne Sälevä and Constantine Lignos},
Title         = {OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets
  in 50+ Languages},
Eprint        = {2412.09587v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present OpenNER 1.0, a standardized collection of openly available named
entity recognition (NER) datasets. OpenNER contains 34 datasets spanning 51
languages, annotated in varying named entity ontologies. We correct annotation
format issues, standardize the original datasets into a uniform representation,
map entity type names to be more consistent across corpora, and provide the
collection in a structure that enables research in multilingual and
multi-ontology NER. We provide baseline models using three pretrained
multilingual language models to compare the performance of recent models and
facilitate future research in NER.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.09587v1},
File          = {2412.09587v1.pdf}
}
@article{2301.10405v8,
Author        = {Siyuan Cheng and Ningyu Zhang and Bozhong Tian and Xi Chen and Qingbing Liu and Huajun Chen},
Title         = {Editing Language Model-based Knowledge Graph Embeddings},
Eprint        = {2301.10405v8},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recently decades have witnessed the empirical success of framing Knowledge
Graph (KG) embeddings via language models. However, language model-based KG
embeddings are usually deployed as static artifacts, making them difficult to
modify post-deployment without re-training after deployment. To address this
issue, we propose a new task of editing language model-based KG embeddings in
this paper. This task is designed to facilitate rapid, data-efficient updates
to KG embeddings without compromising the performance of other aspects. We
build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and
evaluate several knowledge editing baselines demonstrating the limited ability
of previous models to handle the proposed challenging task. We further propose
a simple yet strong baseline dubbed KGEditor, which utilizes additional
parametric layers of the hypernetwork to edit/add facts. Our comprehensive
experimental results reveal that KGEditor excels in updating specific facts
without impacting the overall performance, even when faced with limited
training resources. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/deltaKG.},
Year          = {2023},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2301.10405v8},
File          = {2301.10405v8.pdf}
}
@article{2407.18479v1,
Author        = {Yuandong Wang and Xuhui Ren and Tong Chen and Yuxiao Dong and Nguyen Quoc Viet Hung and Jie Tang},
Title         = {Multi-turn Response Selection with Commonsense-enhanced Language Models},
Eprint        = {2407.18479v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {As a branch of advanced artificial intelligence, dialogue systems are
prospering. Multi-turn response selection is a general research problem in
dialogue systems. With the assistance of background information and pre-trained
language models, the performance of state-of-the-art methods on this problem
gains impressive improvement. However, existing studies neglect the importance
of external commonsense knowledge. Hence, we design a Siamese network where a
pre-trained Language model merges with a Graph neural network (SinLG). SinLG
takes advantage of Pre-trained Language Models (PLMs) to catch the word
correlations in the context and response candidates and utilizes a Graph Neural
Network (GNN) to reason helpful common sense from an external knowledge graph.
The GNN aims to assist the PLM in fine-tuning, and arousing its related
memories to attain better performance. Specifically, we first extract related
concepts as nodes from an external knowledge graph to construct a subgraph with
the context response pair as a super node for each sample. Next, we learn two
representations for the context response pair via both the PLM and GNN. A
similarity loss between the two representations is utilized to transfer the
commonsense knowledge from the GNN to the PLM. Then only the PLM is used to
infer online so that efficiency can be guaranteed. Finally, we conduct
extensive experiments on two variants of the PERSONA-CHAT dataset, which proves
that our solution can not only improve the performance of the PLM but also
achieve an efficient inference.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.18479v1},
File          = {2407.18479v1.pdf}
}
@article{2302.05406v1,
Author        = {Pedro Colon-Hernandez and Henry Lieberman and Yida Xin and Claire Yin and Cynthia Breazeal and Peter Chin},
Title         = {Adversarial Transformer Language Models for Contextual Commonsense
  Inference},
Eprint        = {2302.05406v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Contextualized or discourse aware commonsense inference is the task of
generating coherent commonsense assertions (i.e., facts) from a given story,
and a particular sentence from that story. Some problems with the task are:
lack of controllability for topics of the inferred facts; lack of commonsense
knowledge during training; and, possibly, hallucinated or false facts. In this
work, we utilize a transformer model for this task and develop techniques to
address the aforementioned problems in the task. We control the inference by
introducing a new technique we call "hinting". Hinting is a kind of language
model prompting, that utilizes both hard prompts (specific words) and soft
prompts (virtual learnable templates). This serves as a control signal to
advise the language model "what to talk about". Next, we establish a
methodology for performing joint inference with multiple commonsense knowledge
bases. Joint inference of commonsense requires care, because it is imprecise
and the level of generality is more flexible. You want to be sure that the
results "still make sense" for the context. To this end, we align the textual
version of assertions from three knowledge graphs (ConceptNet, ATOMIC2020, and
GLUCOSE) with a story and a target sentence. This combination allows us to
train a single model to perform joint inference with multiple knowledge graphs.
We show experimental results for the three knowledge graphs on joint inference.
Our final contribution is exploring a GAN architecture that generates the
contextualized commonsense assertions and scores them as to their plausibility
through a discriminator. The result is an integrated system for contextual
commonsense inference in stories, that can controllably generate plausible
commonsense assertions, and takes advantage of joint inference between multiple
commonsense knowledge bases.},
Year          = {2023},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2302.05406v1},
File          = {2302.05406v1.pdf}
}
@article{2309.17169v2,
Author        = {Tudor Groza and Harry Caufield and Dylan Gration and Gareth Baynam and Melissa A Haendel and Peter N Robinson and Christopher J Mungall and Justin T Reese},
Title         = {An evaluation of GPT models for phenotype concept recognition},
Eprint        = {2309.17169v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Objective: Clinical deep phenotyping and phenotype annotation play a critical
role in both the diagnosis of patients with rare disorders as well as in
building computationally-tractable knowledge in the rare disorders field. These
processes rely on using ontology concepts, often from the Human Phenotype
Ontology, in conjunction with a phenotype concept recognition task (supported
usually by machine learning methods) to curate patient profiles or existing
scientific literature. With the significant shift in the use of large language
models (LLMs) for most NLP tasks, we examine the performance of the latest
Generative Pre-trained Transformer (GPT) models underpinning ChatGPT as a
foundation for the tasks of clinical phenotyping and phenotype annotation.
Materials and Methods: The experimental setup of the study included seven
prompts of various levels of specificity, two GPT models (gpt-3.5-turbo and
gpt-4.0) and two established gold standard corpora for phenotype recognition,
one consisting of publication abstracts and the other clinical observations.
Results: Our results show that, with an appropriate setup, these models can
achieve state of the art performance. The best run, using few-shot learning,
achieved 0.58 macro F1 score on publication abstracts and 0.75 macro F1 score
on clinical observations, the former being comparable with the state of the
art, while the latter surpassing the current best in class tool. Conclusion:
While the results are promising, the non-deterministic nature of the outcomes,
the high cost and the lack of concordance between different runs using the same
prompt and input make the use of these LLMs challenging for this particular
task.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.17169v2},
File          = {2309.17169v2.pdf}
}
@article{2005.00646v2,
Author        = {Yanlin Feng and Xinyue Chen and Bill Yuchen Lin and Peifeng Wang and Jun Yan and Xiang Ren},
Title         = {Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question
  Answering},
Eprint        = {2005.00646v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Existing work on augmenting question answering (QA) models with external
knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations
efficiently, or lack transparency into the model's prediction rationale. In
this paper, we propose a novel knowledge-aware approach that equips pre-trained
language models (PTLMs) with a multi-hop relational reasoning module, named
multi-hop graph relation network (MHGRN). It performs multi-hop,
multi-relational reasoning over subgraphs extracted from external knowledge
graphs. The proposed reasoning module unifies path-based reasoning methods and
graph neural networks to achieve better interpretability and scalability. We
also empirically show its effectiveness and scalability on CommonsenseQA and
OpenbookQA datasets, and interpret its behaviors with case studies.},
Year          = {2020},
Month         = {May},
Url           = {http://arxiv.org/abs/2005.00646v2},
File          = {2005.00646v2.pdf}
}
@article{2005.01159v1,
Author        = {Luyang Huang and Lingfei Wu and Lu Wang},
Title         = {Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven
  Cloze Reward},
Eprint        = {2005.01159v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Sequence-to-sequence models for abstractive summarization have been studied
extensively, yet the generated summaries commonly suffer from fabricated
content, and are often found to be near-extractive. We argue that, to address
these issues, the summarizer should acquire semantic interpretation over input,
e.g., via structured representation, to allow the generation of more
informative summaries. In this paper, we present ASGARD, a novel framework for
Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD.
We propose the use of dual encoders---a sequential document encoder and a
graph-structured encoder---to maintain the global context and local
characteristics of entities, complementing each other. We further design a
reward based on a multiple choice cloze test to drive the model to better
capture entity interactions. Results show that our models produce significantly
higher ROUGE scores than a variant without knowledge graph as input on both New
York Times and CNN/Daily Mail datasets. We also obtain better or comparable
performance compared to systems that are fine-tuned from large pretrained
language models. Human judges further rate our model outputs as more
informative and containing fewer unfaithful errors.},
Year          = {2020},
Month         = {May},
Url           = {http://arxiv.org/abs/2005.01159v1},
File          = {2005.01159v1.pdf}
}
@article{1602.03551v1,
Author        = {Stephanie L. Hyland and Theofanis Karaletsos and Gunnar Rätsch},
Title         = {Knowledge Transfer with Medical Language Embeddings},
Eprint        = {1602.03551v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Identifying relationships between concepts is a key aspect of scientific
knowledge synthesis. Finding these links often requires a researcher to
laboriously search through scien- tific papers and databases, as the size of
these resources grows ever larger. In this paper we describe how distributional
semantics can be used to unify structured knowledge graphs with unstructured
text to predict new relationships between medical concepts, using a
probabilistic generative model. Our approach is also designed to ameliorate
data sparsity and scarcity issues in the medical domain, which make language
modelling more challenging. Specifically, we integrate the medical relational
database (SemMedDB) with text from electronic health records (EHRs) to perform
knowledge graph completion. We further demonstrate the ability of our model to
predict relationships between tokens not appearing in the relational database.},
Year          = {2016},
Month         = {Feb},
Url           = {http://arxiv.org/abs/1602.03551v1},
File          = {1602.03551v1.pdf}
}
@article{2003.03239v2,
Author        = {Mutian He and Yangqiu Song and Kun Xu and Dong Yu},
Title         = {On the Role of Conceptualization in Commonsense Knowledge Graph
  Construction},
Eprint        = {2003.03239v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Commonsense knowledge graphs (CKGs) like Atomic and ASER are substantially
different from conventional KGs as they consist of much larger number of nodes
formed by loosely-structured text, which, though, enables them to handle highly
diverse queries in natural language related to commonsense, leads to unique
challenges for automatic KG construction methods. Besides identifying relations
absent from the KG between nodes, such methods are also expected to explore
absent nodes represented by text, in which different real-world things, or
entities, may appear. To deal with the innumerable entities involved with
commonsense in the real world, we introduce to CKG construction methods
conceptualization, i.e., to view entities mentioned in text as instances of
specific concepts or vice versa. We build synthetic triples by
conceptualization, and further formulate the task as triple classification,
handled by a discriminatory model with knowledge transferred from pretrained
language models and fine-tuned by negative sampling. Experiments demonstrate
that our methods can effectively identify plausible triples and expand the KG
by triples of both new nodes and edges of high diversity and novelty.},
Year          = {2020},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2003.03239v2},
File          = {2003.03239v2.pdf}
}
@article{2003.11687v1,
Author        = {Jitin Krishnan and Patrick Coronado and Hemant Purohit and Huzefa Rangwala},
Title         = {Common-Knowledge Concept Recognition for SEVA},
Eprint        = {2003.11687v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We build a common-knowledge concept recognition system for a Systems
Engineer's Virtual Assistant (SEVA) which can be used for downstream tasks such
as relation extraction, knowledge graph construction, and question-answering.
The problem is formulated as a token classification task similar to named
entity extraction. With the help of a domain expert and text processing
methods, we construct a dataset annotated at the word-level by carefully
defining a labelling scheme to train a sequence model to recognize systems
engineering concepts. We use a pre-trained language model and fine-tune it with
the labeled dataset of concepts. In addition, we also create some essential
datasets for information such as abbreviations and definitions from the systems
engineering domain. Finally, we construct a simple knowledge graph using these
extracted concepts along with some hyponym relations.},
Year          = {2020},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2003.11687v1},
File          = {2003.11687v1.pdf}
}
@article{1812.10901v1,
Author        = {Yankai Lin and Xu Han and Ruobing Xie and Zhiyuan Liu and Maosong Sun},
Title         = {Knowledge Representation Learning: A Quantitative Review},
Eprint        = {1812.10901v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge representation learning (KRL) aims to represent entities and
relations in knowledge graph in low-dimensional semantic space, which have been
widely used in massive knowledge-driven tasks. In this article, we introduce
the reader to the motivations for KRL, and overview existing approaches for
KRL. Afterwards, we extensively conduct and quantitative comparison and
analysis of several typical KRL methods on three evaluation tasks of knowledge
acquisition including knowledge graph completion, triple classification, and
relation extraction. We also review the real-world applications of KRL, such as
language modeling, question answering, information retrieval, and recommender
systems. Finally, we discuss the remaining challenges and outlook the future
directions for KRL. The codes and datasets used in the experiments can be found
in https://github.com/thunlp/OpenKE.},
Year          = {2018},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1812.10901v1},
File          = {1812.10901v1.pdf}
}
@article{1705.08018v1,
Author        = {Ashwini Jaya Kumar and Camilo Morales and Maria-Esther Vidal and Christoph Schmidt and Sören Auer},
Title         = {Use of Knowledge Graph in Rescoring the N-Best List in Automatic Speech
  Recognition},
Eprint        = {1705.08018v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {With the evolution of neural network based methods, automatic speech
recognition (ASR) field has been advanced to a level where building an
application with speech interface is a reality. In spite of these advances,
building a real-time speech recogniser faces several problems such as low
recognition accuracy, domain constraint, and out-of-vocabulary words. The low
recognition accuracy problem is addressed by improving the acoustic model,
language model, decoder and by rescoring the N-best list at the output of the
decoder. We are considering the N-best list rescoring approach to improve the
recognition accuracy. Most of the methods in the literature use the
grammatical, lexical, syntactic and semantic connection between the words in a
recognised sentence as a feature to rescore. In this paper, we have tried to
see the semantic relatedness between the words in a sentence to rescore the
N-best list. Semantic relatedness is computed using
TransE~\cite{bordes2013translating}, a method for low dimensional embedding of
a triple in a knowledge graph. The novelty of the paper is the application of
semantic web to automatic speech recognition.},
Year          = {2017},
Month         = {May},
Url           = {http://arxiv.org/abs/1705.08018v1},
File          = {1705.08018v1.pdf}
}
@article{2009.09335v3,
Author        = {Kung-Hsiang Huang and Mu Yang and Nanyun Peng},
Title         = {Biomedical Event Extraction with Hierarchical Knowledge Graphs},
Eprint        = {2009.09335v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Biomedical event extraction is critical in understanding biomolecular
interactions described in scientific corpus. One of the main challenges is to
identify nested structured events that are associated with non-indicative
trigger words. We propose to incorporate domain knowledge from Unified Medical
Language System (UMLS) to a pre-trained language model via Graph
Edge-conditioned Attention Networks (GEANet) and hierarchical graph
representation. To better recognize the trigger words, each sentence is first
grounded to a sentence graph based on a jointly modeled hierarchical knowledge
graph from UMLS. The grounded graphs are then propagated by GEANet, a novel
graph neural networks for enhanced capabilities in inferring complex events. On
BioNLP 2011 GENIA Event Extraction task, our approach achieved 1.41% F1 and
3.19% F1 improvements on all events and complex events, respectively. Ablation
studies confirm the importance of GEANet and hierarchical KG.},
Year          = {2020},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2009.09335v3},
File          = {2009.09335v3.pdf}
}
@article{2010.00309v1,
Author        = {Tianxiang Sun and Yunfan Shao and Xipeng Qiu and Qipeng Guo and Yaru Hu and Xuanjing Huang and Zheng Zhang},
Title         = {CoLAKE: Contextualized Language and Knowledge Embedding},
Eprint        = {2010.00309v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {With the emerging branch of incorporating factual knowledge into pre-trained
language models such as BERT, most existing models consider shallow, static,
and separately pre-trained entity embeddings, which limits the performance
gains of these models. Few works explore the potential of deep contextualized
knowledge representation when injecting knowledge. In this paper, we propose
the Contextualized Language and Knowledge Embedding (CoLAKE), which jointly
learns contextualized representation for both language and knowledge with the
extended MLM objective. Instead of injecting only entity embeddings, CoLAKE
extracts the knowledge context of an entity from large-scale knowledge bases.
To handle the heterogeneity of knowledge context and language context, we
integrate them in a unified data structure, word-knowledge graph (WK graph).
CoLAKE is pre-trained on large-scale WK graphs with the modified Transformer
encoder. We conduct experiments on knowledge-driven tasks, knowledge probing
tasks, and language understanding tasks. Experimental results show that CoLAKE
outperforms previous counterparts on most of the tasks. Besides, CoLAKE
achieves surprisingly high performance on our synthetic task called
word-knowledge graph completion, which shows the superiority of simultaneously
contextualizing language and knowledge representation.},
Year          = {2020},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2010.00309v1},
File          = {2010.00309v1.pdf}
}
@article{2012.11142v1,
Author        = {Ishani Mondal},
Title         = {Towards Incorporating Entity-specific Knowledge Graph Information in
  Predicting Drug-Drug Interactions},
Eprint        = {2012.11142v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Off-the-shelf biomedical embeddings obtained from the recently released
various pre-trained language models (such as BERT, XLNET) have demonstrated
state-of-the-art results (in terms of accuracy) for the various natural
language understanding tasks (NLU) in the biomedical domain. Relation
Classification (RC) falls into one of the most critical tasks. In this paper,
we explore how to incorporate domain knowledge of the biomedical entities (such
as drug, disease, genes), obtained from Knowledge Graph (KG) Embeddings, for
predicting Drug-Drug Interaction from textual corpus. We propose a new method,
BERTKG-DDI, to combine drug embeddings obtained from its interaction with other
biomedical entities along with domain-specific BioBERT embedding-based RC
architecture. Experiments conducted on the DDIExtraction 2013 corpus clearly
indicate that this strategy improves other baselines architectures by 4.1%
macro F1-score.},
Year          = {2020},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2012.11142v1},
File          = {2012.11142v1.pdf}
}
@article{2102.04760v2,
Author        = {Sahand Sharifzadeh and Sina Moayed Baharlou and Martin Schmitt and Hinrich Schütze and Volker Tresp},
Title         = {Improving Scene Graph Classification by Exploiting Knowledge from Texts},
Eprint        = {2102.04760v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Training scene graph classification models requires a large amount of
annotated image data. Meanwhile, scene graphs represent relational knowledge
that can be modeled with symbolic data from texts or knowledge graphs. While
image annotation demands extensive labor, collecting textual descriptions of
natural scenes requires less effort. In this work, we investigate whether
textual scene descriptions can substitute for annotated image data. To this
end, we employ a scene graph classification framework that is trained not only
from annotated images but also from symbolic data. In our architecture, the
symbolic entities are first mapped to their correspondent image-grounded
representations and then fed into the relational reasoning pipeline. Even
though a structured form of knowledge, such as the form in knowledge graphs, is
not always available, we can generate it from unstructured texts using a
transformer-based language model. We show that by fine-tuning the
classification pipeline with the extracted knowledge from texts, we can achieve
~8x more accurate results in scene graph classification, ~3x in object
classification, and ~1.5x in predicate classification, compared to the
supervised baselines with only 1% of the annotated images.},
Year          = {2021},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2102.04760v2},
File          = {2102.04760v2.pdf}
}
@article{2108.11899v3,
Author        = {Haoyu Zuo and Yuan Yin and Peter Childs},
Title         = {Patent-KG: Patent Knowledge Graph Use for Engineering Design},
Eprint        = {2108.11899v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {To facilitate knowledge reuse in engineering design, several dataset
approaches have been proposed and applied by designers. This paper builds a
patent-based knowledge graph, patent-KG, to represent the knowledge facts in
patents for engineering design. The arising patent-KG approach proposes a new
unsupervised mechanism to extract knowledge facts in a patent, by searching the
attention graph in language models. This method avoids using expensive labelled
data in supervised learning or listing complex syntactic rules in rule-based
extraction. The extracted entities are compared with other benchmarks in the
criteria of recall rate. The result reaches the highest 0.9 recall rate in the
standard list of mechanical engineering related technical terms, which means
the highest coverage of engineering words. The extracted relationships are also
compared with other benchmarks. The result shows that our method provides more
contextual information in relationships, and extracts more relationship types
including positional and negation relationships.},
Year          = {2021},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2108.11899v3},
File          = {2108.11899v3.pdf}
}
@article{2112.02732v2,
Author        = {Yueqing Sun and Qi Shi and Le Qi and Yu Zhang},
Title         = {JointLK: Joint Reasoning with Language Models and Knowledge Graphs for
  Commonsense Question Answering},
Eprint        = {2112.02732v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Existing KG-augmented models for commonsense question answering primarily
focus on designing elaborate Graph Neural Networks (GNNs) to model knowledge
graphs (KGs). However, they ignore (i) the effectively fusing and reasoning
over question context representations and the KG representations, and (ii)
automatically selecting relevant nodes from the noisy KGs during reasoning. In
this paper, we propose a novel model, JointLK, which solves the above
limitations through the joint reasoning of LM and GNN and the dynamic KGs
pruning mechanism. Specifically, JointLK performs joint reasoning between LM
and GNN through a novel dense bidirectional attention module, in which each
question token attends on KG nodes and each KG node attends on question tokens,
and the two modal representations fuse and update mutually by multi-step
interactions. Then, the dynamic pruning module uses the attention weights
generated by joint reasoning to prune irrelevant KG nodes recursively. We
evaluate JointLK on the CommonsenseQA and OpenBookQA datasets, and demonstrate
its improvements to the existing LM and LM+KG models, as well as its capability
to perform interpretable reasoning.},
Year          = {2021},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2112.02732v2},
File          = {2112.02732v2.pdf}
}
@article{2203.02167v1,
Author        = {Liang Wang and Wei Zhao and Zhuoyu Wei and Jingming Liu},
Title         = {SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained
  Language Models},
Eprint        = {2203.02167v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graph completion (KGC) aims to reason over known facts and infer
the missing links. Text-based methods such as KGBERT (Yao et al., 2019) learn
entity representations from natural language descriptions, and have the
potential for inductive KGC. However, the performance of text-based methods
still largely lag behind graph embedding-based methods like TransE (Bordes et
al., 2013) and RotatE (Sun et al., 2019b). In this paper, we identify that the
key issue is efficient contrastive learning. To improve the learning
efficiency, we introduce three types of negatives: in-batch negatives,
pre-batch negatives, and self-negatives which act as a simple form of hard
negatives. Combined with InfoNCE loss, our proposed model SimKGC can
substantially outperform embedding-based methods on several benchmark datasets.
In terms of mean reciprocal rank (MRR), we advance the state-of-the-art by +19%
on WN18RR, +6.8% on the Wikidata5M transductive setting, and +22% on the
Wikidata5M inductive setting. Thorough analyses are conducted to gain insights
into each component. Our code is available at
https://github.com/intfloat/SimKGC .},
Year          = {2022},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2203.02167v1},
File          = {2203.02167v1.pdf}
}
@article{2205.14036v1,
Author        = {Awantee Deshpande and Dana Ruiter and Marius Mosbach and Dietrich Klakow},
Title         = {StereoKG: Data-Driven Knowledge Graph Construction for Cultural
  Knowledge and Stereotypes},
Eprint        = {2205.14036v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Analyzing ethnic or religious bias is important for improving fairness,
accountability, and transparency of natural language processing models.
However, many techniques rely on human-compiled lists of bias terms, which are
expensive to create and are limited in coverage. In this study, we present a
fully data-driven pipeline for generating a knowledge graph (KG) of cultural
knowledge and stereotypes. Our resulting KG covers 5 religious groups and 5
nationalities and can easily be extended to include more entities. Our human
evaluation shows that the majority (59.2%) of non-singleton entries are
coherent and complete stereotypes. We further show that performing intermediate
masked language model training on the verbalized KG leads to a higher level of
cultural awareness in the model and has the potential to increase
classification performance on knowledge-crucial samples on a related task,
i.e., hate speech detection.},
Year          = {2022},
Month         = {May},
Url           = {http://arxiv.org/abs/2205.14036v1},
File          = {2205.14036v1.pdf}
}
@article{2206.13163v1,
Author        = {Ningyuan Huang and Yash R. Deshpande and Yibo Liu and Houda Alberts and Kyunghyun Cho and Clara Vania and Iacer Calixto},
Title         = {Endowing Language Models with Multimodal Knowledge Graph Representations},
Eprint        = {2206.13163v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We propose a method to make natural language understanding models more
parameter efficient by storing knowledge in an external knowledge graph (KG)
and retrieving from this KG using a dense index. Given (possibly multilingual)
downstream task data, e.g., sentences in German, we retrieve entities from the
KG and use their multimodal representations to improve downstream task
performance. We use the recently released VisualSem KG as our external
knowledge repository, which covers a subset of Wikipedia and WordNet entities,
and compare a mix of tuple-based and graph-based algorithms to learn entity and
relation representations that are grounded on the KG multimodal information. We
demonstrate the usefulness of the learned entity representations on two
downstream tasks, and show improved performance on the multilingual named
entity recognition task by $0.3\%$--$0.7\%$ F1, while we achieve up to $2.5\%$
improvement in accuracy on the visual sense disambiguation task. All our code
and data are available in: \url{https://github.com/iacercalixto/visualsem-kg}.},
Year          = {2022},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2206.13163v1},
File          = {2206.13163v1.pdf}
}
@article{2210.07373v3,
Author        = {Zdeněk Kasner and Ioannis Konstas and Ondřej Dušek},
Title         = {Mind the Labels: Describing Relations in Knowledge Graphs With
  Pretrained Models},
Eprint        = {2210.07373v3},
DOI           = {10.18653/v1/2023.eacl-main.176},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pretrained language models (PLMs) for data-to-text (D2T) generation can use
human-readable data labels such as column headings, keys, or relation names to
generalize to out-of-domain examples. However, the models are well-known in
producing semantically inaccurate outputs if these labels are ambiguous or
incomplete, which is often the case in D2T datasets. In this paper, we expose
this issue on the task of descibing a relation between two entities. For our
experiments, we collect a novel dataset for verbalizing a diverse set of 1,522
unique relations from three large-scale knowledge graphs (Wikidata, DBPedia,
YAGO). We find that although PLMs for D2T generation expectedly fail on unclear
cases, models trained with a large variety of relation labels are surprisingly
robust in verbalizing novel, unseen relations. We argue that using data with a
diverse set of clear and meaningful labels is key to training D2T generation
systems capable of generalizing to novel domains.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.07373v3},
File          = {2210.07373v3.pdf}
}
@article{2210.14463v1,
Author        = {Bohua Peng and Shihao Liang and Mobarakol Islam},
Title         = {Bi-Link: Bridging Inductive Link Predictions from Text via Contrastive
  Learning of Transformers and Prompts},
Eprint        = {2210.14463v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Inductive knowledge graph completion requires models to comprehend the
underlying semantics and logic patterns of relations. With the advance of
pretrained language models, recent research have designed transformers for link
prediction tasks. However, empirical studies show that linearizing triples
affects the learning of relational patterns, such as inversion and symmetry. In
this paper, we propose Bi-Link, a contrastive learning framework with
probabilistic syntax prompts for link predictions. Using grammatical knowledge
of BERT, we efficiently search for relational prompts according to learnt
syntactical patterns that generalize to large knowledge graphs. To better
express symmetric relations, we design a symmetric link prediction model,
establishing bidirectional linking between forward prediction and backward
prediction. This bidirectional linking accommodates flexible self-ensemble
strategies at test time. In our experiments, Bi-Link outperforms recent
baselines on link prediction datasets (WN18RR, FB15K-237, and Wikidata5M).
Furthermore, we construct Zeshel-Ind as an in-domain inductive entity linking
the environment to evaluate Bi-Link. The experimental results demonstrate that
our method yields robust representations which can generalize under domain
shift.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.14463v1},
File          = {2210.14463v1.pdf}
}
@article{2211.10511v1,
Author        = {Igor Melnyk and Pierre Dognin and Payel Das},
Title         = {Knowledge Graph Generation From Text},
Eprint        = {2211.10511v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this work we propose a novel end-to-end multi-stage Knowledge Graph (KG)
generation system from textual inputs, separating the overall process into two
stages. The graph nodes are generated first using pretrained language model,
followed by a simple edge construction head, enabling efficient KG extraction
from the text. For each stage we consider several architectural choices that
can be used depending on the available training resources. We evaluated the
model on a recent WebNLG 2020 Challenge dataset, matching the state-of-the-art
performance on text-to-RDF generation task, as well as on New York Times (NYT)
and a large-scale TekGen datasets, showing strong overall performance,
outperforming the existing baselines. We believe that the proposed system can
serve as a viable KG construction alternative to the existing linearization or
sampling-based graph generation approaches. Our code can be found at
https://github.com/IBM/Grapher},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.10511v1},
File          = {2211.10511v1.pdf}
}
@article{2211.11407v1,
Author        = {Genet Asefa Gesese and Harald Sack and Mehwish Alam},
Title         = {RAILD: Towards Leveraging Relation Features for Inductive Link
  Prediction In Knowledge Graphs},
Eprint        = {2211.11407v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Due to the open world assumption, Knowledge Graphs (KGs) are never complete.
In order to address this issue, various Link Prediction (LP) methods are
proposed so far. Some of these methods are inductive LP models which are
capable of learning representations for entities not seen during training.
However, to the best of our knowledge, none of the existing inductive LP models
focus on learning representations for unseen relations. In this work, a novel
Relation Aware Inductive Link preDiction (RAILD) is proposed for KG completion
which learns representations for both unseen entities and unseen relations. In
addition to leveraging textual literals associated with both entities and
relations by employing language models, RAILD also introduces a novel
graph-based approach to generate features for relations. Experiments are
conducted with different existing and newly created challenging benchmark
datasets and the results indicate that RAILD leads to performance improvement
over the state-of-the-art models. Moreover, since there are no existing
inductive LP models which learn representations for unseen relations, we have
created our own baselines and the results obtained with RAILD also outperform
these baselines.},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.11407v1},
File          = {2211.11407v1.pdf}
}
@article{2303.13284v3,
Author        = {Debayan Banerjee and Pranav Ajit Nair and Ricardo Usbeck and Chris Biemann},
Title         = {GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph
  Question Answering},
Eprint        = {2303.13284v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this work, we present an end-to-end Knowledge Graph Question Answering
(KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text
pre-trained language model. The model takes a question in natural language as
input and produces a simpler form of the intended SPARQL query. In the simpler
form, the model does not directly produce entity and relation IDs. Instead, it
produces corresponding entity and relation labels. The labels are grounded to
KG entity and relation IDs in a subsequent step. To further improve the
results, we instruct the model to produce a truncated version of the KG
embedding for each entity. The truncated KG embedding enables a finer search
for disambiguation purposes. We find that T5 is able to learn the truncated KG
embeddings without any change of loss function, improving KGQA performance. As
a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata
datasets on end-to-end KGQA over Wikidata.},
Year          = {2023},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2303.13284v3},
File          = {2303.13284v3.pdf}
}
@article{2303.15682v1,
Author        = {Sanxing Chen and Hao Cheng and Xiaodong Liu and Jian Jiao and Yangfeng Ji and Jianfeng Gao},
Title         = {Pre-training Transformers for Knowledge Graph Completion},
Eprint        = {2303.15682v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Learning transferable representation of knowledge graphs (KGs) is challenging
due to the heterogeneous, multi-relational nature of graph structures. Inspired
by Transformer-based pretrained language models' success on learning
transferable representation for texts, we introduce a novel inductive KG
representation model (iHT) for KG completion by large-scale pre-training. iHT
consists of a entity encoder (e.g., BERT) and a neighbor-aware relational
scoring function both parameterized by Transformers. We first pre-train iHT on
a large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art
results on matched evaluations, with a relative improvement of more than 25% in
mean reciprocal rank over previous SOTA models. When further fine-tuned on
smaller KGs with either entity and relational shifts, pre-trained iHT
representations are shown to be transferable, significantly improving the
performance on FB15K-237 and WN18RR.},
Year          = {2023},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2303.15682v1},
File          = {2303.15682v1.pdf}
}
@article{2304.00592v1,
Author        = {Cheng Deng and Bo Tong and Luoyi Fu and Jiaxin Ding and Dexing Cao and Xinbing Wang and Chenghu Zhou},
Title         = {PK-Chat: Pointer Network Guided Knowledge Driven Generative Dialogue
  Model},
Eprint        = {2304.00592v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In the research of end-to-end dialogue systems, using real-world knowledge to
generate natural, fluent, and human-like utterances with correct answers is
crucial. However, domain-specific conversational dialogue systems may be
incoherent and introduce erroneous external information to answer questions due
to the out-of-vocabulary issue or the wrong knowledge from the parameters of
the neural network. In this work, we propose PK-Chat, a Pointer network guided
Knowledge-driven generative dialogue model, incorporating a unified pretrained
language model and a pointer network over knowledge graphs. The words generated
by PK-Chat in the dialogue are derived from the prediction of word lists and
the direct prediction of the external knowledge graph knowledge. Moreover,
based on the PK-Chat, a dialogue system is built for academic scenarios in the
case of geosciences. Finally, an academic dialogue benchmark is constructed to
evaluate the quality of dialogue systems in academic scenarios and the source
code is available online.},
Year          = {2023},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2304.00592v1},
File          = {2304.00592v1.pdf}
}
@article{2305.07633v1,
Author        = {Ziwei Fan and Zhiwei Liu and Shelby Heinecke and Jianguo Zhang and Huan Wang and Caiming Xiong and Philip S. Yu},
Title         = {Zero-shot Item-based Recommendation via Multi-task Product Knowledge
  Graph Pre-Training},
Eprint        = {2305.07633v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Existing recommender systems face difficulties with zero-shot items, i.e.
items that have no historical interactions with users during the training
stage. Though recent works extract universal item representation via
pre-trained language models (PLMs), they ignore the crucial item relationships.
This paper presents a novel paradigm for the Zero-Shot Item-based
Recommendation (ZSIR) task, which pre-trains a model on product knowledge graph
(PKG) to refine the item features from PLMs. We identify three challenges for
pre-training PKG, which are multi-type relations in PKG, semantic divergence
between item generic information and relations and domain discrepancy from PKG
to downstream ZSIR task. We address the challenges by proposing four
pre-training tasks and novel task-oriented adaptation (ToA) layers. Moreover,
this paper discusses how to fine-tune the model on new recommendation task such
that the ToA layers are adapted to ZSIR task. Comprehensive experiments on 18
markets dataset are conducted to verify the effectiveness of the proposed model
in both knowledge prediction and ZSIR task.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.07633v1},
File          = {2305.07633v1.pdf}
}
@article{2305.13585v1,
Author        = {Siyuan Wang and Zhongyu Wei and Meng Han and Zhihao Fan and Haijun Shan and Qi Zhang and Xuanjing Huang},
Title         = {Query Structure Modeling for Inductive Logical Reasoning Over Knowledge
  Graphs},
Eprint        = {2305.13585v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Logical reasoning over incomplete knowledge graphs to answer complex logical
queries is a challenging task. With the emergence of new entities and relations
in constantly evolving KGs, inductive logical reasoning over KGs has become a
crucial problem. However, previous PLMs-based methods struggle to model the
logical structures of complex queries, which limits their ability to generalize
within the same structure. In this paper, we propose a structure-modeled
textual encoding framework for inductive logical reasoning over KGs. It encodes
linearized query structures and entities using pre-trained language models to
find answers. For structure modeling of complex queries, we design stepwise
instructions that implicitly prompt PLMs on the execution order of geometric
operations in each query. We further separately model different geometric
operations (i.e., projection, intersection, and union) on the representation
space using a pre-trained encoder with additional attention and maxout layers
to enhance structured modeling. We conduct experiments on two inductive logical
reasoning datasets and three transductive datasets. The results demonstrate the
effectiveness of our method on logical reasoning over KGs in both inductive and
transductive settings.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.13585v1},
File          = {2305.13585v1.pdf}
}
@article{2305.15932v2,
Author        = {Jie He and Simon Chi Lok U and Víctor Gutiérrez-Basulto and Jeff Z. Pan},
Title         = {BUCA: A Binary Classification Approach to Unsupervised Commonsense
  Question Answering},
Eprint        = {2305.15932v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as
the construction of commonsense reasoning datasets is expensive, and they are
inevitably limited in their scope. A popular approach to UCR is to fine-tune
language models with external knowledge (e.g., knowledge graphs), but this
usually requires a large number of training examples. In this paper, we propose
to transform the downstream multiple choice question answering task into a
simpler binary classification task by ranking all candidate answers according
to their reasonableness. To this end, for training the model, we convert the
knowledge graph triples into reasonable and unreasonable texts. Extensive
experimental results show the effectiveness of our approach on various multiple
choice question answering benchmarks. Furthermore, compared with existing UCR
approaches using KGs, ours is less data hungry. Our code is available at
https://github.com/probe2/BUCA.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.15932v2},
File          = {2305.15932v2.pdf}
}
@article{2306.14470v1,
Author        = {Dahyun Jung and Jaehyung Seo and Jaewook Lee and Chanjun Park and Heuiseok Lim},
Title         = {Knowledge Graph-Augmented Korean Generative Commonsense Reasoning},
Eprint        = {2306.14470v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Generative commonsense reasoning refers to the task of generating acceptable
and logical assumptions about everyday situations based on commonsense
understanding. By utilizing an existing dataset such as Korean CommonGen,
language generation models can learn commonsense reasoning specific to the
Korean language. However, language models often fail to consider the
relationships between concepts and the deep knowledge inherent to concepts. To
address these limitations, we propose a method to utilize the Korean knowledge
graph data for text generation. Our experimental result shows that the proposed
method can enhance the efficiency of Korean commonsense inference, thereby
underlining the significance of employing supplementary data.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.14470v1},
File          = {2306.14470v1.pdf}
}
@article{2307.01709v1,
Author        = {Chen Chen and Yufei Wang and Aixin Sun and Bing Li and Kwok-Yan Lam},
Title         = {Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge
  Graph Completion via Conditional Soft Prompting},
Eprint        = {2307.01709v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graph Completion (KGC) often requires both KG structural and
textual information to be effective. Pre-trained Language Models (PLMs) have
been used to learn the textual information, usually under the fine-tune
paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly
focus on the textual information and overlook structural knowledge. To tackle
this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC)
which maintains a balance between structural information and textual knowledge.
CSProm-KG only tunes the parameters of Conditional Soft Prompts that are
generated by the entities and relations representations. We verify the
effectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR,
FB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and
ICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new
state-of-the-art on these benchmarks. We conduct further analysis to show (i)
the effectiveness of our proposed components, (ii) the efficiency of CSProm-KG,
and (iii) the flexibility of CSProm-KG.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.01709v1},
File          = {2307.01709v1.pdf}
}
@article{2308.03671v1,
Author        = {Michael Färber and David Lamprecht and Johan Krause and Linn Aung and Peter Haase},
Title         = {SemOpenAlex: The Scientific Landscape in 26 Billion RDF Triples},
Eprint        = {2308.03671v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {We present SemOpenAlex, an extensive RDF knowledge graph that contains over
26 billion triples about scientific publications and their associated entities,
such as authors, institutions, journals, and concepts. SemOpenAlex is licensed
under CC0, providing free and open access to the data. We offer the data
through multiple channels, including RDF dump files, a SPARQL endpoint, and as
a data source in the Linked Open Data cloud, complete with resolvable URIs and
links to other data sources. Moreover, we provide embeddings for knowledge
graph entities using high-performance computing. SemOpenAlex enables a broad
range of use-case scenarios, such as exploratory semantic search via our
website, large-scale scientific impact quantification, and other forms of
scholarly big data analytics within and across scientific disciplines.
Additionally, it enables academic recommender systems, such as recommending
collaborators, publications, and venues, including explainability capabilities.
Finally, SemOpenAlex can serve for RDF query optimization benchmarks, creating
scholarly knowledge-guided language models, and as a hub for semantic
scientific publishing.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.03671v1},
File          = {2308.03671v1.pdf}
}
@article{2309.07545v2,
Author        = {Debayan Banerjee and  Arefa and Ricardo Usbeck and Chris Biemann},
Title         = {DBLPLink: An Entity Linker for the DBLP Scholarly Knowledge Graph},
Eprint        = {2309.07545v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this work, we present a web application named DBLPLink, which performs
entity linking over the DBLP scholarly knowledge graph. DBLPLink uses
text-to-text pre-trained language models, such as T5, to produce entity label
spans from an input text question. Entity candidates are fetched from a
database based on the labels, and an entity re-ranker sorts them based on
entity embeddings, such as TransE, DistMult and ComplEx. The results are
displayed so that users may compare and contrast the results between T5-small,
T5-base and the different KG embeddings used. The demo can be accessed at
https://ltdemos.informatik.uni-hamburg.de/dblplink/.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.07545v2},
File          = {2309.07545v2.pdf}
}
@article{2309.12501v1,
Author        = {Xiou Ge and Yun-Cheng Wang and Bin Wang and C. -C. Jay Kuo},
Title         = {Knowledge Graph Embedding: An Overview},
Eprint        = {2309.12501v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Many mathematical models have been leveraged to design embeddings for
representing Knowledge Graph (KG) entities and relations for link prediction
and many downstream tasks. These mathematically-inspired models are not only
highly scalable for inference in large KGs, but also have many explainable
advantages in modeling different relation patterns that can be validated
through both formal proofs and empirical results. In this paper, we make a
comprehensive overview of the current state of research in KG completion. In
particular, we focus on two main branches of KG embedding (KGE) design: 1)
distance-based methods and 2) semantic matching-based methods. We discover the
connections between recently proposed models and present an underlying trend
that might help researchers invent novel and more effective models. Next, we
delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D
affine operations, respectively. They encompass a broad spectrum of techniques
including distance-based and semantic-based methods. We will also discuss an
emerging approach for KG completion which leverages pre-trained language models
(PLMs) and textual descriptions of entities and relations and offer insights
into the integration of KGE embedding methods with PLMs for KG completion.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.12501v1},
File          = {2309.12501v1.pdf}
}
@article{2310.04910v5,
Author        = {Weihe Zhai and Arkaitz Zubiaga},
Title         = {Towards Faithful Knowledge Graph Explanation Through Deep Alignment in
  Commonsense Question Answering},
Eprint        = {2310.04910v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The fusion of language models (LMs) and knowledge graphs (KGs) is widely used
in commonsense question answering, but generating faithful explanations remains
challenging. Current methods often overlook path decoding faithfulness, leading
to divergence between graph encoder outputs and model predictions. We identify
confounding effects and LM-KG misalignment as key factors causing spurious
explanations. To address this, we introduce the LM-KG Fidelity metric to assess
KG representation reliability and propose the LM-KG Distribution-aware
Alignment (\textit{LKDA}) algorithm to improve explanation faithfulness.
Without ground truth, we evaluate KG explanations using the proposed
Fidelity-Sparsity Trade-off Curve. Experiments on CommonsenseQA and OpenBookQA
show that LKDA significantly enhances explanation fidelity and model
performance, highlighting the need to address distributional misalignment for
reliable commonsense reasoning.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.04910v5},
File          = {2310.04910v5.pdf}
}
@article{2310.13566v1,
Author        = {Nicholas Thomas Walker and Stefan Ultes and Pierre Lison},
Title         = {Retrieval-Augmented Neural Response Generation Using Logical Reasoning
  and Relevance Scoring},
Eprint        = {2310.13566v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Constructing responses in task-oriented dialogue systems typically relies on
information sources such the current dialogue state or external databases. This
paper presents a novel approach to knowledge-grounded response generation that
combines retrieval-augmented language models with logical reasoning. The
approach revolves around a knowledge graph representing the current dialogue
state and background information, and proceeds in three steps. The knowledge
graph is first enriched with logically derived facts inferred using
probabilistic logical programming. A neural model is then employed at each turn
to score the conversational relevance of each node and edge of this extended
graph. Finally, the elements with highest relevance scores are converted to a
natural language form, and are integrated into the prompt for the neural
conversational model employed to generate the system response.
  We investigate the benefits of the proposed approach on two datasets (KVRET
and GraphWOZ) along with a human evaluation. Experimental results show that the
combination of (probabilistic) logical reasoning with conversational relevance
scoring does increase both the factuality and fluency of the responses.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.13566v1},
File          = {2310.13566v1.pdf}
}
@article{2310.20588v1,
Author        = {Yuqi Wang and Zeqiang Wang and Wei Wang and Qi Chen and Kaizhu Huang and Anh Nguyen and Suparna De},
Title         = {Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding},
Eprint        = {2310.20588v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In the era of the Internet of Things (IoT), the retrieval of relevant medical
information has become essential for efficient clinical decision-making. This
paper introduces MedFusionRank, a novel approach to zero-shot medical
information retrieval (MIR) that combines the strengths of pre-trained language
models and statistical methods while addressing their limitations. The proposed
approach leverages a pre-trained BERT-style model to extract compact yet
informative keywords. These keywords are then enriched with domain knowledge by
linking them to conceptual entities within a medical knowledge graph.
Experimental evaluations on medical datasets demonstrate MedFusion Rank's
superior performance over existing methods, with promising results with a
variety of evaluation metrics. MedFusionRank demonstrates efficacy in
retrieving relevant information, even from short or single-term queries.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.20588v1},
File          = {2310.20588v1.pdf}
}
@article{2311.06414v1,
Author        = {Nedelina Teneva and Estevam Hruschka},
Title         = {Knowledge Graphs are not Created Equal: Exploring the Properties and
  Structure of Real KGs},
Eprint        = {2311.06414v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Despite the recent popularity of knowledge graph (KG) related tasks and
benchmarks such as KG embeddings, link prediction, entity alignment and
evaluation of the reasoning abilities of pretrained language models as KGs, the
structure and properties of real KGs are not well studied. In this paper, we
perform a large scale comparative study of 29 real KG datasets from diverse
domains such as the natural sciences, medicine, and NLP to analyze their
properties and structural patterns. Based on our findings, we make several
recommendations regarding KG-based model development and evaluation. We believe
that the rich structural information contained in KGs can benefit the
development of better KG models across fields and we hope this study will
contribute to breaking the existing data silos between different areas of
research (e.g., ML, NLP, AI for sciences).},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.06414v1},
File          = {2311.06414v1.pdf}
}
@article{2312.06053v1,
Author        = {Ziheng Zeng and Kellen Tan Cheng and Srihari Venkat Nanniyur and Jianing Zhou and Suma Bhat},
Title         = {IEKG: A Commonsense Knowledge Graph for Idiomatic Expressions},
Eprint        = {2312.06053v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Idiomatic expression (IE) processing and comprehension have challenged
pre-trained language models (PTLMs) because their meanings are
non-compositional. Unlike prior works that enable IE comprehension through
fine-tuning PTLMs with sentences containing IEs, in this work, we construct
IEKG, a commonsense knowledge graph for figurative interpretations of IEs. This
extends the established ATOMIC2020 graph, converting PTLMs into knowledge
models (KMs) that encode and infer commonsense knowledge related to IE use.
Experiments show that various PTLMs can be converted into KMs with IEKG. We
verify the quality of IEKG and the ability of the trained KMs with automatic
and human evaluation. Through applications in natural language understanding,
we show that a PTLM injected with knowledge from IEKG exhibits improved IE
comprehension ability and can generalize to IEs unseen during training.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.06053v1},
File          = {2312.06053v1.pdf}
}
@article{2401.09553v1,
Author        = {Shreya Rajpal and Ricardo Usbeck},
Title         = {BERTologyNavigator: Advanced Question Answering with BERT-based
  Semantics},
Eprint        = {2401.09553v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The development and integration of knowledge graphs and language models has
significance in artificial intelligence and natural language processing. In
this study, we introduce the BERTologyNavigator -- a two-phased system that
combines relation extraction techniques and BERT embeddings to navigate the
relationships within the DBLP Knowledge Graph (KG). Our approach focuses on
extracting one-hop relations and labelled candidate pairs in the first phases.
This is followed by employing BERT's CLS embeddings and additional heuristics
for relation selection in the second phase. Our system reaches an F1 score of
0.2175 on the DBLP QuAD Final test dataset for Scholarly QALD and 0.98 F1 score
on the subset of the DBLP QuAD test dataset during the QA phase.},
Year          = {2024},
Month         = {Jan},
Note          = {Joint Proceedings of Scholarly QALD 2023 and SemREC 2023
  co-located with 22nd International Semantic Web Conference ISWC 2023. Athens,
  Greece, November 6-10, 2023},
Url           = {http://arxiv.org/abs/2401.09553v1},
File          = {2401.09553v1.pdf}
}
@article{2406.13578v1,
Author        = {Han-Cheng Yu and Yu-An Shih and Kin-Man Law and Kai-Yu Hsieh and Yu-Chen Cheng and Hsin-Chih Ho and Zih-An Lin and Wen-Chuan Hsu and Yao-Chung Fan},
Title         = {Enhancing Distractor Generation for Multiple-Choice Questions with
  Retrieval Augmented Pretraining and Knowledge Graph Integration},
Eprint        = {2406.13578v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this paper, we tackle the task of distractor generation (DG) for
multiple-choice questions. Our study introduces two key designs. First, we
propose \textit{retrieval augmented pretraining}, which involves refining the
language model pretraining to align it more closely with the downstream task of
DG. Second, we explore the integration of knowledge graphs to enhance the
performance of DG. Through experiments with benchmarking datasets, we show that
our models significantly outperform the state-of-the-art results. Our
best-performing model advances the F1@3 score from 14.80 to 16.47 in MCQ
dataset and from 15.92 to 16.50 in Sciq dataset.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.13578v1},
File          = {2406.13578v1.pdf}
}
@article{2408.01154v1,
Author        = {Zhichun Wang and Xuan Chen},
Title         = {DERA: Dense Entity Retrieval for Entity Alignment in Knowledge Graphs},
Eprint        = {2408.01154v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Entity Alignment (EA) aims to match equivalent entities in different
Knowledge Graphs (KGs), which is essential for knowledge fusion and
integration. Recently, embedding-based EA has attracted significant attention
and many approaches have been proposed. Early approaches primarily focus on
learning entity embeddings from the structural features of KGs, defined by
relation triples. Later methods incorporated entities' names and attributes as
auxiliary information to enhance embeddings for EA. However, these approaches
often used different techniques to encode structural and attribute information,
limiting their interaction and mutual enhancement. In this work, we propose a
dense entity retrieval framework for EA, leveraging language models to
uniformly encode various features of entities and facilitate nearest entity
search across KGs. Alignment candidates are first generated through entity
retrieval, which are subsequently reranked to determine the final alignments.
We conduct comprehensive experiments on both cross-lingual and monolingual EA
datasets, demonstrating that our approach achieves state-of-the-art performance
compared to existing EA methods.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.01154v1},
File          = {2408.01154v1.pdf}
}
@article{2408.14895v2,
Author        = {Shusaku Egami and Takahiro Ugai and Swe Nwe Nwe Htun and Ken Fukuda},
Title         = {VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view
  Videos of Daily Activities},
Eprint        = {2408.14895v2},
DOI           = {10.1145/3627673.3679175},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data
(e.g., images and videos) into symbols, have attracted attention as resources
enabling knowledge processing and machine learning across modalities. However,
the construction of MMKGs for videos consisting of multiple events, such as
daily activities, is still in the early stages. In this paper, we construct an
MMKG based on synchronized multi-view simulated videos of daily activities.
Besides representing the content of daily life videos as event-centric
knowledge, our MMKG also includes frame-by-frame fine-grained changes, such as
bounding boxes within video frames. In addition, we provide support tools for
querying our MMKG. As an application example, we demonstrate that our MMKG
facilitates benchmarking vision-language models by providing the necessary
vision-language datasets for a tailored task.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.14895v2},
File          = {2408.14895v2.pdf}
}
@article{2409.10403v1,
Author        = {Zhang Zheng},
Title         = {A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning
  and BERT Integration},
Eprint        = {2409.10403v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper proposes a knowledge-enhanced disease diagnosis method based on a
prompt learning framework. The method retrieves structured knowledge from
external knowledge graphs related to clinical cases, encodes it, and injects it
into the prompt templates to enhance the language model's understanding and
reasoning capabilities for the task.We conducted experiments on three public
datasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the
proposed method significantly outperforms existing models across multiple
evaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC
dataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.
Additionally,ablation studies confirmed the critical role of the knowledge
injection module,as the removal of this module resulted in a significant drop
in F1 score. The experimental results demonstrate that the proposed method not
only effectively improves the accuracy of disease diagnosis but also enhances
the interpretability of the predictions, providing more reliable support and
evidence for clinical diagnosis.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.10403v1},
File          = {2409.10403v1.pdf}
}
@article{2410.14057v1,
Author        = {Simone Conia and Daniel Lee and Min Li and Umar Farooq Minhas and Saloni Potdar and Yunyao Li},
Title         = {Towards Cross-Cultural Machine Translation with Retrieval-Augmented
  Generation from Multilingual Knowledge Graphs},
Eprint        = {2410.14057v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Translating text that contains entity names is a challenging task, as
cultural-related references can vary significantly across languages. These
variations may also be caused by transcreation, an adaptation process that
entails more than transliteration and word-for-word translation. In this paper,
we address the problem of cross-cultural translation on two fronts: (i) we
introduce XC-Translate, the first large-scale, manually-created benchmark for
machine translation that focuses on text that contains potentially
culturally-nuanced entity names, and (ii) we propose KG-MT, a novel end-to-end
method to integrate information from a multilingual knowledge graph into a
neural machine translation model by leveraging a dense retrieval mechanism. Our
experiments and analyses show that current machine translation systems and
large language models still struggle to translate texts containing entity
names, whereas KG-MT outperforms state-of-the-art approaches by a large margin,
obtaining a 129% and 62% relative improvement compared to NLLB-200 and GPT-4,
respectively.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.14057v1},
File          = {2410.14057v1.pdf}
}
@article{2411.03568v1,
Author        = {Lee Kezar and Nidhi Munikote and Zian Zeng and Zed Sehyr and Naomi Caselli and Jesse Thomason},
Title         = {The American Sign Language Knowledge Graph: Infusing ASL Models with
  Linguistic Knowledge},
Eprint        = {2411.03568v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Language models for American Sign Language (ASL) could make language
technologies substantially more accessible to those who sign. To train models
on tasks such as isolated sign recognition (ISR) and ASL-to-English
translation, datasets provide annotated video examples of ASL signs. To
facilitate the generalizability and explainability of these models, we
introduce the American Sign Language Knowledge Graph (ASLKG), compiled from
twelve sources of expert linguistic knowledge. We use the ASLKG to train
neuro-symbolic models for 3 ASL understanding tasks, achieving accuracies of
91% on ISR, 14% for predicting the semantic features of unseen signs, and 36%
for classifying the topic of Youtube-ASL videos.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.03568v1},
File          = {2411.03568v1.pdf}
}
@article{2411.12174v1,
Author        = {Rahul Garg and Trilok Padhi and Hemang Jain and Ugur Kursuncu and Ponnurangam Kumaraguru},
Title         = {Just KIDDIN: Knowledge Infusion and Distillation for Detection of
  INdecent Memes},
Eprint        = {2411.12174v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Toxicity identification in online multimodal environments remains a
challenging task due to the complexity of contextual connections across
modalities (e.g., textual and visual). In this paper, we propose a novel
framework that integrates Knowledge Distillation (KD) from Large Visual
Language Models (LVLMs) and knowledge infusion to enhance the performance of
toxicity detection in hateful memes. Our approach extracts sub-knowledge graphs
from ConceptNet, a large-scale commonsense Knowledge Graph (KG) to be infused
within a compact VLM framework. The relational context between toxic phrases in
captions and memes, as well as visual concepts in memes enhance the model's
reasoning capabilities. Experimental results from our study on two hate speech
benchmark datasets demonstrate superior performance over the state-of-the-art
baselines across AU-ROC, F1, and Recall with improvements of 1.1%, 7%, and 35%,
respectively. Given the contextual complexity of the toxicity detection task,
our approach showcases the significance of learning from both explicit (i.e.
KG) as well as implicit (i.e. LVLMs) contextual cues incorporated through a
hybrid neurosymbolic approach. This is crucial for real-world applications
where accurate and scalable recognition of toxic content is critical for
creating safer online environments.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.12174v1},
File          = {2411.12174v1.pdf}
}
@article{2501.14050v1,
Author        = {Jiacheng Liang and Yuhui Wang and Changjiang Li and Rongyi Zhu and Tanqiu Jiang and Neil Gong and Ting Wang},
Title         = {GraphRAG under Fire},
Eprint        = {2501.14050v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {GraphRAG advances retrieval-augmented generation (RAG) by structuring
external knowledge as multi-scale knowledge graphs, enabling language models to
integrate both broad context and granular details in their reasoning. While
GraphRAG has demonstrated success across domains, its security implications
remain largely unexplored. To bridge this gap, this work examines GraphRAG's
vulnerability to poisoning attacks, uncovering an intriguing security paradox:
compared to conventional RAG, GraphRAG's graph-based indexing and retrieval
enhance resilience against simple poisoning attacks; meanwhile, the same
features also create new attack surfaces. We present GRAGPoison, a novel attack
that exploits shared relations in the knowledge graph to craft poisoning text
capable of compromising multiple queries simultaneously. GRAGPoison employs
three key strategies: i) relation injection to introduce false knowledge, ii)
relation enhancement to amplify poisoning influence, and iii) narrative
generation to embed malicious content within coherent text. Empirical
evaluation across diverse datasets and models shows that GRAGPoison
substantially outperforms existing attacks in terms of effectiveness (up to 98%
success rate) and scalability (using less than 68% poisoning text). We also
explore potential defensive measures and their limitations, identifying
promising directions for future research.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.14050v1},
File          = {2501.14050v1.pdf}
}
@article{2404.03662v1,
Author        = {Drishti Goel and Fiza Husain and Aditya Singh and Supriyo Ghosh and Anjaly Parayil and Chetan Bansal and Xuchao Zhang and Saravan Rajmohan},
Title         = {X-lifecycle Learning for Cloud Incident Management using LLMs},
Eprint        = {2404.03662v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NI},
Abstract      = {Incident management for large cloud services is a complex and tedious process
and requires significant amount of manual efforts from on-call engineers
(OCEs). OCEs typically leverage data from different stages of the software
development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service
properties, service dependencies, trouble-shooting documents, etc.) to generate
insights for detection, root causing and mitigating of incidents. Recent
advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini)
created opportunities to automatically generate contextual recommendations to
the OCEs assisting them to quickly identify and mitigate critical issues.
However, existing research typically takes a silo-ed view for solving a certain
task in incident management by leveraging data from a single stage of SDLC. In
this paper, we demonstrate that augmenting additional contextual data from
different stages of SDLC improves the performance of two critically important
and practically challenging tasks: (1) automatically generating root cause
recommendations for dependency failure related incidents, and (2) identifying
ontology of service monitors used for automatically detecting incidents. By
leveraging 353 incident and 260 monitor dataset from Microsoft, we demonstrate
that augmenting contextual information from different stages of the SDLC
improves the performance over State-of-The-Art methods.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2404.03662v1},
File          = {2404.03662v1.pdf}
}
@article{2502.06874v1,
Author        = {Yanming Guo and Xiao Qian and Kevin Credit and Jin Ma},
Title         = {Group Reasoning Emission Estimation Networks},
Eprint        = {2502.06874v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Accurate greenhouse gas (GHG) emission reporting is critical for governments,
businesses, and investors. However, adoption remains limited particularly among
small and medium enterprises due to high implementation costs, fragmented
emission factor databases, and a lack of robust sector classification methods.
To address these challenges, we introduce Group Reasoning Emission Estimation
Networks (GREEN), an AI-driven carbon accounting framework that standardizes
enterprise-level emission estimation, constructs a large-scale benchmark
dataset, and leverages a novel reasoning approach with large language models
(LLMs). Specifically, we compile textual descriptions for 20,850 companies with
validated North American Industry Classification System (NAICS) labels and
align these with an economic model of carbon intensity factors. By reframing
sector classification as an information retrieval task, we fine-tune
Sentence-BERT models using a contrastive learning loss. To overcome the
limitations of single-stage models in handling thousands of hierarchical
categories, we propose a Group Reasoning method that ensembles LLM classifiers
based on the natural NAICS ontology, decomposing the task into multiple
sub-classification steps. We theoretically prove that this approach reduces
classification uncertainty and computational complexity. Experiments on 1,114
NAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47%
Top-10 accuracy), and case studies on 20 companies report a mean absolute
percentage error (MAPE) of 45.88%. The project is available at:
https://huggingface.co/datasets/Yvnminc/ExioNAICS.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.06874v1},
File          = {2502.06874v1.pdf}
}
@article{2305.05390v2,
Author        = {Jincenzi Wu and Zhuang Chen and Jiawen Deng and Sahand Sabour and Helen Meng and Minlie Huang},
Title         = {COKE: A Cognitive Knowledge Graph for Machine Theory of Mind},
Eprint        = {2305.05390v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Theory of mind (ToM) refers to humans' ability to understand and infer the
desires, beliefs, and intentions of others. The acquisition of ToM plays a key
role in humans' social cognition and interpersonal relations. Though
indispensable for social intelligence, ToM is still lacking for modern AI and
NLP systems since they cannot access the human mental state and cognitive
process beneath the training corpus. To empower AI systems with the ToM ability
and narrow the gap between them and humans, in this paper, we propose COKE: the
first cognitive knowledge graph for machine theory of mind. Specifically, COKE
formalizes ToM as a collection of 45k+ manually verified cognitive chains that
characterize human mental activities and subsequent behavioral/affective
responses when facing specific social circumstances. In addition, we further
generalize COKE using LLMs and build a powerful generation model COLM tailored
for cognitive reasoning. Experimental results in both automatic and human
evaluation demonstrate the high quality of COKE, the superior ToM ability of
COLM, and its potential to significantly enhance social applications.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.05390v2},
File          = {2305.05390v2.pdf}
}
@article{2307.00769v1,
Author        = {Xiang Wei and Yufeng Chen and Ning Cheng and Xingyu Cui and Jinan Xu and Wenjuan Han},
Title         = {CollabKG: A Learnable Human-Machine-Cooperative Information Extraction
  Toolkit for (Event) Knowledge Graph Construction},
Eprint        = {2307.00769v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In order to construct or extend entity-centric and event-centric knowledge
graphs (KG and EKG), the information extraction (IE) annotation toolkit is
essential. However, existing IE toolkits have several non-trivial problems,
such as not supporting multi-tasks, not supporting automatic updates. In this
work, we present CollabKG, a learnable human-machine-cooperative IE toolkit for
KG and EKG construction. Specifically, for the multi-task issue, CollabKG
unifies different IE subtasks, including named entity recognition (NER),
entity-relation triple extraction (RE), and event extraction (EE), and supports
both KG and EKG. Then, combining advanced prompting-based IE technology, the
human-machine-cooperation mechanism with LLMs as the assistant machine is
presented which can provide a lower cost as well as a higher performance.
Lastly, owing to the two-way interaction between the human and machine,
CollabKG with learning ability allows self-renewal. Besides, CollabKG has
several appealing features (e.g., customization, training-free, propagation,
etc.) that make the system powerful, easy-to-use, and high-productivity. We
holistically compare our toolkit with other existing tools on these features.
Human evaluation quantitatively illustrates that CollabKG significantly
improves annotation quality, efficiency, and stability simultaneously.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.00769v1},
File          = {2307.00769v1.pdf}
}
@article{2410.04660v1,
Author        = {Xiaorui Su and Yibo Wang and Shanghua Gao and Xiaolong Liu and Valentina Giunchiglia and Djork-Arné Clevert and Marinka Zitnik},
Title         = {Knowledge Graph Based Agent for Complex, Knowledge-Intensive QA in
  Medicine},
Eprint        = {2410.04660v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Biomedical knowledge is uniquely complex and structured, requiring distinct
reasoning strategies compared to other scientific disciplines like physics or
chemistry. Biomedical scientists do not rely on a single approach to reasoning;
instead, they use various strategies, including rule-based, prototype-based,
and case-based reasoning. This diversity calls for flexible approaches that
accommodate multiple reasoning strategies while leveraging in-domain knowledge.
We introduce KGARevion, a knowledge graph (KG) based agent designed to address
the complexity of knowledge-intensive medical queries. Upon receiving a query,
KGARevion generates relevant triplets by using the knowledge base of the LLM.
These triplets are then verified against a grounded KG to filter out erroneous
information and ensure that only accurate, relevant data contribute to the
final answer. Unlike RAG-based models, this multi-step process ensures
robustness in reasoning while adapting to different models of medical
reasoning. Evaluations on four gold-standard medical QA datasets show that
KGARevion improves accuracy by over 5.2%, outperforming 15 models in handling
complex medical questions. To test its capabilities, we curated three new
medical QA datasets with varying levels of semantic complexity, where KGARevion
achieved a 10.4% improvement in accuracy.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.04660v1},
File          = {2410.04660v1.pdf}
}
@article{2412.13575v1,
Author        = {Qianyue Wang and Jinwu Hu and Zhengping Li and Yufeng Wang and daiyuan li and Yu Hu and Mingkui Tan},
Title         = {Generating Long-form Story Using Dynamic Hierarchical Outlining with
  Memory-Enhancement},
Eprint        = {2412.13575v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Long-form story generation task aims to produce coherent and sufficiently
lengthy text, essential for applications such as novel writingand interactive
storytelling. However, existing methods, including LLMs, rely on rigid outlines
or lack macro-level planning, making it difficult to achieve both contextual
consistency and coherent plot development in long-form story generation. To
address this issues, we propose Dynamic Hierarchical Outlining with
Memory-Enhancement long-form story generation method, named DOME, to generate
the long-form story with coherent content and plot. Specifically, the Dynamic
Hierarchical Outline(DHO) mechanism incorporates the novel writing theory into
outline planning and fuses the plan and writing stages together, improving the
coherence of the plot by ensuring the plot completeness and adapting to the
uncertainty during story generation. A Memory-Enhancement Module (MEM) based on
temporal knowledge graphs is introduced to store and access the generated
content, reducing contextual conflicts and improving story coherence. Finally,
we propose a Temporal Conflict Analyzer leveraging temporal knowledge graphs to
automatically evaluate the contextual consistency of long-form story.
Experiments demonstrate that DOME significantly improves the fluency,
coherence, and overall quality of generated long stories compared to
state-of-the-art methods.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.13575v1},
File          = {2412.13575v1.pdf}
}
@article{2412.17690v3,
Author        = {Rishiraj Saha Roy and Chris Hinze and Joel Schlotthauer and Farzad Naderi and Viktor Hangya and Andreas Foltyn and Luzian Hahn and Fabian Kuech},
Title         = {RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF
  for Conversational QA over KGs with RAG},
Eprint        = {2412.17690v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Conversational question answering (ConvQA) is a convenient means of searching
over RDF knowledge graphs (KGs), where a prevalent approach is to translate
natural language questions to SPARQL queries. However, SPARQL has certain
shortcomings: (i) it is brittle for complex intents and conversational
questions, and (ii) it is not suitable for more abstract needs. Instead, we
propose a novel two-pronged system where we fuse: (i) SQL-query results over a
database automatically derived from the KG, and (ii) text-search results over
verbalizations of KG facts. Our pipeline supports iterative retrieval: when the
results of any branch are found to be unsatisfactory, the system can
automatically opt for further rounds. We put everything together in a retrieval
augmented generation (RAG) setup, where an LLM generates a coherent response
from accumulated search results. We demonstrate the superiority of our proposed
system over several baselines on a knowledge graph of BMW automobiles.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.17690v3},
File          = {2412.17690v3.pdf}
}
@article{2011.03863v2,
Author        = {Kaixin Ma and Filip Ilievski and Jonathan Francis and Yonatan Bisk and Eric Nyberg and Alessandro Oltramari},
Title         = {Knowledge-driven Data Construction for Zero-shot Evaluation in
  Commonsense Question Answering},
Eprint        = {2011.03863v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent developments in pre-trained neural language modeling have led to leaps
in accuracy on commonsense question-answering benchmarks. However, there is
increasing concern that models overfit to specific tasks, without learning to
utilize external knowledge or perform general semantic reasoning. In contrast,
zero-shot evaluations have shown promise as a more robust measure of a model's
general reasoning abilities. In this paper, we propose a novel neuro-symbolic
framework for zero-shot question answering across commonsense tasks. Guided by
a set of hypotheses, the framework studies how to transform various
pre-existing knowledge resources into a form that is most effective for
pre-training models. We vary the set of language models, training regimes,
knowledge sources, and data generation strategies, and measure their impact
across tasks. Extending on prior work, we devise and compare four constrained
distractor-sampling strategies. We provide empirical results across five
commonsense question-answering tasks with data generated from five external
knowledge resources. We show that, while an individual knowledge graph is
better suited for specific tasks, a global knowledge graph brings consistent
gains across different tasks. In addition, both preserving the structure of the
task as well as generating fair and informative questions help language models
learn more effectively.},
Year          = {2020},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2011.03863v2},
File          = {2011.03863v2.pdf}
}
@article{2112.01047v2,
Author        = {Taolin Zhang and Chengyu Wang and Nan Hu and Minghui Qiu and Chengguang Tang and Xiaofeng He and Jun Huang},
Title         = {DKPLM: Decomposable Knowledge-enhanced Pre-trained Language Model for
  Natural Language Understanding},
Eprint        = {2112.01047v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge-Enhanced Pre-trained Language Models (KEPLMs) are pre-trained
models with relation triples injecting from knowledge graphs to improve
language understanding abilities. To guarantee effective knowledge injection,
previous studies integrate models with knowledge encoders for representing
knowledge retrieved from knowledge graphs. The operations for knowledge
retrieval and encoding bring significant computational burdens, restricting the
usage of such models in real-world applications that require high inference
speed. In this paper, we propose a novel KEPLM named DKPLM that Decomposes
Knowledge injection process of the Pre-trained Language Models in pre-training,
fine-tuning and inference stages, which facilitates the applications of KEPLMs
in real-world scenarios. Specifically, we first detect knowledge-aware
long-tail entities as the target for knowledge injection, enhancing the KEPLMs'
semantic understanding abilities and avoiding injecting redundant information.
The embeddings of long-tail entities are replaced by "pseudo token
representations" formed by relevant knowledge triples. We further design the
relational knowledge decoding task for pre-training to force the models to
truly understand the injected knowledge by relation triple reconstruction.
Experiments show that our model outperforms other KEPLMs significantly over
zero-shot knowledge probing tasks and multiple knowledge-aware language
understanding tasks. We further show that DKPLM has a higher inference speed
than other competing models due to the decomposing mechanism.},
Year          = {2021},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2112.01047v2},
File          = {2112.01047v2.pdf}
}
@article{2408.13273v1,
Author        = {Geethan Sannidhi and Sagar Srinivas Sakhinana and Venkataramana Runkana},
Title         = {Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach
  for Temporal Knowledge Graph Forecasting},
Eprint        = {2408.13273v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pre-trained large language models (PLLMs) like OpenAI ChatGPT and Google
Gemini face challenges such as inaccurate factual recall, hallucinations,
biases, and future data leakage for temporal Knowledge Graph (tKG) forecasting.
To address these issues, we introduce sLA-tKGF (small-scale language assistant
for tKG forecasting), which utilizes Retrieval-Augmented Generation (RAG)
aided, custom-trained small-scale language models through a tabula rasa
approach from scratch for effective tKG forecasting. Our framework constructs
knowledge-infused prompts with relevant historical data from tKGs, web search
results, and PLLMs-generated textual descriptions to understand historical
entity relationships prior to the target time. It leverages these external
knowledge-infused prompts for deeper understanding and reasoning of
context-specific semantic and temporal information to zero-shot prompt
small-scale language models for more accurate predictions of future events
within tKGs. It reduces hallucinations and mitigates distributional shift
challenges through comprehending changing trends over time. As a result, it
enables more accurate and contextually grounded forecasts of future events
while minimizing computational demands. Rigorous empirical studies demonstrate
our framework robustness, scalability, and state-of-the-art (SOTA) performance
on benchmark datasets with interpretable and trustworthy tKG forecasting.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.13273v1},
File          = {2408.13273v1.pdf}
}
@article{2403.02253v2,
Author        = {Yuexin Li and Chengyu Huang and Shumin Deng and Mei Lin Lock and Tri Cao and Nay Oo and Hoon Wei Lim and Bryan Hooi},
Title         = {KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for
  Enhancing Reference-Based Phishing Detection},
Eprint        = {2403.02253v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Phishing attacks have inflicted substantial losses on individuals and
businesses alike, necessitating the development of robust and efficient
automated phishing detection approaches. Reference-based phishing detectors
(RBPDs), which compare the logos on a target webpage to a known set of logos,
have emerged as the state-of-the-art approach. However, a major limitation of
existing RBPDs is that they rely on a manually constructed brand knowledge
base, making it infeasible to scale to a large number of brands, which results
in false negative errors due to the insufficient brand coverage of the
knowledge base. To address this issue, we propose an automated knowledge
collection pipeline, using which we collect a large-scale multimodal brand
knowledge base, KnowPhish, containing 20k brands with rich information about
each brand. KnowPhish can be used to boost the performance of existing RBPDs in
a plug-and-play manner. A second limitation of existing RBPDs is that they
solely rely on the image modality, ignoring useful textual information present
in the webpage HTML. To utilize this textual information, we propose a Large
Language Model (LLM)-based approach to extract brand information of webpages
from text. Our resulting multimodal phishing detection approach, KnowPhish
Detector (KPD), can detect phishing webpages with or without logos. We evaluate
KnowPhish and KPD on a manually validated dataset, and a field study under
Singapore's local context, showing substantial improvements in effectiveness
and efficiency compared to state-of-the-art baselines.},
Year          = {2024},
Month         = {Mar},
Note          = {33rd USENIX Security Symposium (USENIX Security 2024), 793--810},
Url           = {http://arxiv.org/abs/2403.02253v2},
File          = {2403.02253v2.pdf}
}
@article{1805.06503v1,
Author        = {Ameet Deshpande and Vedant Somani},
Title         = {Weight Initialization in Neural Language Models},
Eprint        = {1805.06503v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Semantic Similarity is an important application which finds its use in many
downstream NLP applications. Though the task is mathematically defined,
semantic similarity's essence is to capture the notions of similarity
impregnated in humans. Machines use some heuristics to calculate the similarity
between words, but these are typically corpus dependent or are useful for
specific domains. The difference between Semantic Similarity and Semantic
Relatedness motivates the development of new algorithms. For a human, the word
car and road are probably as related as car and bus. But this may not be the
case for computational methods. Ontological methods are good at encoding
Semantic Similarity and Vector Space models are better at encoding Semantic
Relatedness. There is a dearth of methods which leverage ontologies to create
better vector representations. The aim of this proposal is to explore in the
direction of a hybrid method which combines statistical/vector space methods
like Word2Vec and Ontological methods like WordNet to leverage the advantages
provided by both.},
Year          = {2018},
Month         = {May},
Url           = {http://arxiv.org/abs/1805.06503v1},
File          = {1805.06503v1.pdf}
}
@article{2408.01455v1,
Author        = {Tyler Fischella and Erin van Liemt and  Qiuyi and  Zhang},
Title         = {Ontology of Belief Diversity: A Community-Based Epistemological Approach},
Eprint        = {2408.01455v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {AI applications across classification, fairness, and human interaction often
implicitly require ontologies of social concepts. Constructing these well,
especially when there are many relevant categories, is a controversial task but
is crucial for achieving meaningful inclusivity. Here, we focus on developing a
pragmatic ontology of belief systems, which is a complex and often
controversial space. By iterating on our community-based design until mutual
agreement is reached, we found that epistemological methods were best for
categorizing the fundamental ways beliefs differ, maximally respecting our
principles of inclusivity and brevity. We demonstrate our methodology's utility
and interpretability via user studies in term annotation and sentiment analysis
experiments for belief fairness in language models.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2408.01455v1},
File          = {2408.01455v1.pdf}
}
@article{2410.12631v1,
Author        = {Nicolas Lazzari and Stefano De Giorgis and Aldo Gangemi and Valentina Presutti},
Title         = {Explainable Moral Values: a neuro-symbolic approach to value
  classification},
Eprint        = {2410.12631v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This work explores the integration of ontology-based reasoning and Machine
Learning techniques for explainable value classification. By relying on an
ontological formalization of moral values as in the Moral Foundations Theory,
relying on the DnS Ontology Design Pattern, the \textit{sandra} neuro-symbolic
reasoner is used to infer values (fomalized as descriptions) that are
\emph{satisfied by} a certain sentence. Sentences, alongside their structured
representation, are automatically generated using an open-source Large Language
Model. The inferred descriptions are used to automatically detect the value
associated with a sentence. We show that only relying on the reasoner's
inference results in explainable classification comparable to other more
complex approaches. We show that combining the reasoner's inferences with
distributional semantics methods largely outperforms all the baselines,
including complex models based on neural network architectures. Finally, we
build a visualization tool to explore the potential of theory-based values
classification, which is publicly available at http://xmv.geomeaning.com/.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.12631v1},
File          = {2410.12631v1.pdf}
}
@article{2406.04744v2,
Author        = {Xiao Yang and Kai Sun and Hao Xin and Yushi Sun and Nikita Bhalla and Xiangsen Chen and Sajal Choudhary and Rongze Daniel Gui and Ziran Will Jiang and Ziyu Jiang and Lingkun Kong and Brian Moran and Jiaqi Wang and Yifan Ethan Xu and An Yan and Chenyu Yang and Eting Yuan and Hanwen Zha and Nan Tang and Lei Chen and Nicolas Scheffer and Yue Liu and Nirav Shah and Rakesh Wanga and Anuj Kumar and Wen-tau Yih and Xin Luna Dong},
Title         = {CRAG -- Comprehensive RAG Benchmark},
Eprint        = {2406.04744v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Retrieval-Augmented Generation (RAG) has recently emerged as a promising
solution to alleviate Large Language Model (LLM)'s deficiency in lack of
knowledge. Existing RAG datasets, however, do not adequately represent the
diverse and dynamic nature of real-world Question Answering (QA) tasks. To
bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual
question answering benchmark of 4,409 question-answer pairs and mock APIs to
simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a
diverse array of questions across five domains and eight question categories,
reflecting varied entity popularity from popular to long-tail, and temporal
dynamisms ranging from years to seconds. Our evaluation of this benchmark
highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve
<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the
accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%
of questions without any hallucination. CRAG also reveals much lower accuracy
in answering questions regarding facts with higher dynamism, lower popularity,
or higher complexity, suggesting future research directions. The CRAG benchmark
laid the groundwork for a KDD Cup 2024 challenge and attracted thousands of
participants and submissions. We commit to maintaining CRAG to serve research
communities in advancing RAG solutions and general QA solutions. CRAG is
available at https://github.com/facebookresearch/CRAG/.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.04744v2},
File          = {2406.04744v2.pdf}
}
@article{2401.17664v1,
Author        = {Yuanhuiyi Lyu and Xu Zheng and Lin Wang},
Title         = {Image Anything: Towards Reasoning-coherent and Training-free Multi-modal
  Image Generation},
Eprint        = {2401.17664v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {The multifaceted nature of human perception and comprehension indicates that,
when we think, our body can naturally take any combination of senses, a.k.a.,
modalities and form a beautiful picture in our brain. For example, when we see
a cattery and simultaneously perceive the cat's purring sound, our brain can
construct a picture of a cat in the cattery. Intuitively, generative AI models
should hold the versatility of humans and be capable of generating images from
any combination of modalities efficiently and collaboratively. This paper
presents ImgAny, a novel end-to-end multi-modal generative model that can mimic
human reasoning and generate high-quality images. Our method serves as the
first attempt in its capacity of efficiently and flexibly taking any
combination of seven modalities, ranging from language, audio to vision
modalities, including image, point cloud, thermal, depth, and event data. Our
key idea is inspired by human-level cognitive processes and involves the
integration and harmonization of multiple input modalities at both the entity
and attribute levels without specific tuning across modalities. Accordingly,
our method brings two novel training-free technical branches: 1) Entity Fusion
Branch ensures the coherence between inputs and outputs. It extracts entity
features from the multi-modal representations powered by our specially
constructed entity knowledge graph; 2) Attribute Fusion Branch adeptly
preserves and processes the attributes. It efficiently amalgamates distinct
attributes from diverse input modalities via our proposed attribute knowledge
graph. Lastly, the entity and attribute features are adaptively fused as the
conditional inputs to the pre-trained Stable Diffusion model for image
generation. Extensive experiments under diverse modality combinations
demonstrate its exceptional capability for visual content creation.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.17664v1},
File          = {2401.17664v1.pdf}
}
@article{2410.12860v1,
Author        = {Robert Porter and Adam Diehl and Benjamin Pastel and J. Henry Hinnefeld and Lawson Nerenberg and Pye Maung and Sebastien Kerbrat and Gillian Hanson and Troy Astorino and Stephen J. Tarsa},
Title         = {LLMD: A Large Language Model for Interpreting Longitudinal Medical
  Records},
Eprint        = {2410.12860v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce LLMD, a large language model designed to analyze a patient's
medical history based on their medical records. Along with domain knowledge,
LLMD is trained on a large corpus of records collected over time and across
facilities, as well as tasks and labels that make nuanced connections among
them. This approach is critical to an accurate picture of patient health, and
has distinctive advantages over models trained on knowledge alone, unlabeled
records, structured EHR data, or records from a single health system.
  The recipe for LLMD continues pretraining a foundational model on both domain
knowledge and the contents of millions of records. These span an average of 10
years of care and as many as 140 care sites per patient. LLMD is then
instruction fine-tuned on structuring and abstraction tasks. The former jointly
identify and normalize document metadata, provenance information, clinical
named-entities, and ontology mappings, while the latter roll these into
higher-level representations, such a continuous era of time a patient was on a
medication. LLMD is deployed within a layered validation system that includes
continual random audits and review by experts, e.g. based on uncertainty,
disease-specific rules, or use-case.
  LLMD exhibits large gains over both more-powerful generalized models and
domain-specific models. On medical knowledge benchmarks, LLMD-8B achieves state
of the art accuracy on PubMedQA text responses, besting orders-of-magnitude
larger models. On production tasks, we show that LLMD significantly outperforms
all other models evaluated, and among alternatives, large general purpose LLMs
like GPT-4o are more accurate than models emphasizing medical knowledge. We
find strong evidence that accuracy on today's medical benchmarks is not the
most significant factor when analyzing real-world patient data, an insight with
implications for future medical LLMs.'},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.12860v1},
File          = {2410.12860v1.pdf}
}
@article{2208.11701v1,
Author        = {Jinge Wu and Rowena Smith and Honghan Wu},
Title         = {Ontology-Driven Self-Supervision for Adverse Childhood Experiences
  Identification Using Social Media Datasets},
Eprint        = {2208.11701v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Adverse Childhood Experiences (ACEs) are defined as a collection of highly
stressful, and potentially traumatic, events or circumstances that occur
throughout childhood and/or adolescence. They have been shown to be associated
with increased risks of mental health diseases or other abnormal behaviours in
later lives. However, the identification of ACEs from textual data with Natural
Language Processing (NLP) is challenging because (a) there are no NLP ready ACE
ontologies; (b) there are few resources available for machine learning,
necessitating the data annotation from clinical experts; (c) costly annotations
by domain experts and large number of documents for supporting large machine
learning models. In this paper, we present an ontology-driven self-supervised
approach (derive concept embeddings using an auto-encoder from baseline NLP
results) for producing a publicly available resource that would support
large-scale machine learning (e.g., training transformer based large language
models) on social media corpus. This resource as well as the proposed approach
are aimed to facilitate the community in training transferable NLP models for
effectively surfacing ACEs in low-resource scenarios like NLP on clinical notes
within Electronic Health Records. The resource including a list of ACE ontology
terms, ACE concept embeddings and the NLP annotated corpus is available at
https://github.com/knowlab/ACE-NLP.},
Year          = {2022},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2208.11701v1},
File          = {2208.11701v1.pdf}
}
@article{2501.09928v1,
Author        = {Reham Omar and Omij Mangukiya and Essam Mansour},
Title         = {Dialogue Benchmark Generation from Knowledge Graphs with Cost-Effective
  Retrieval-Augmented LLMs},
Eprint        = {2501.09928v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Dialogue benchmarks are crucial in training and evaluating chatbots engaging
in domain-specific conversations. Knowledge graphs (KGs) represent semantically
rich and well-organized data spanning various domains, such as DBLP, DBpedia,
and YAGO. Traditionally, dialogue benchmarks have been manually created from
documents, neglecting the potential of KGs in automating this process. Some
question-answering benchmarks are automatically generated using extensive
preprocessing from KGs, but they do not support dialogue generation. This paper
introduces Chatty-Gen, a novel multi-stage retrieval-augmented generation
platform for automatically generating high-quality dialogue benchmarks tailored
to a specific domain using a KG. Chatty-Gen decomposes the generation process
into manageable stages and uses assertion rules for automatic validation
between stages. Our approach enables control over intermediate results to
prevent time-consuming restarts due to hallucinations. It also reduces reliance
on costly and more powerful commercial LLMs. Chatty-Gen eliminates upfront
processing of the entire KG using efficient query-based retrieval to find
representative subgraphs based on the dialogue context. Our experiments with
several real and large KGs demonstrate that Chatty-Gen significantly
outperforms state-of-the-art systems and ensures consistent model and system
performance across multiple LLMs of diverse capabilities, such as GPT-4o,
Gemini 1.5, Llama 3, and Mistral.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.09928v1},
File          = {2501.09928v1.pdf}
}
@article{1908.06203v3,
Author        = {Xiao Zhang and Dejing Dou and Ji Wu},
Title         = {Learning Conceptual-Contextual Embeddings for Medical Text},
Eprint        = {1908.06203v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {External knowledge is often useful for natural language understanding tasks.
We introduce a contextual text representation model called
Conceptual-Contextual (CC) embeddings, which incorporates structured knowledge
into text representations. Unlike entity embedding methods, our approach
encodes a knowledge graph into a context model. CC embeddings can be easily
reused for a wide range of tasks just like pre-trained language models. Our
model effectively encodes the huge UMLS database by leveraging semantic
generalizability. Experiments on electronic health records (EHRs) and medical
text processing benchmarks showed our model gives a major boost to the
performance of supervised medical NLP tasks.},
Year          = {2019},
Month         = {Aug},
Url           = {http://arxiv.org/abs/1908.06203v3},
File          = {1908.06203v3.pdf}
}
@article{2201.06230v1,
Author        = {Alessandro Oltramari and Jonathan Francis and Filip Ilievski and Kaixin Ma and Roshanak Mirzaee},
Title         = {Generalizable Neuro-symbolic Systems for Commonsense Question Answering},
Eprint        = {2201.06230v1},
DOI           = {10.3233/FAIA210360},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This chapter illustrates how suitable neuro-symbolic models for language
understanding can enable domain generalizability and robustness in downstream
tasks. Different methods for integrating neural language models and knowledge
graphs are discussed. The situations in which this combination is most
appropriate are characterized, including quantitative evaluation and
qualitative error analysis on a variety of commonsense question answering
benchmark datasets.},
Year          = {2022},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2201.06230v1},
File          = {2201.06230v1.pdf}
}
@article{2103.07102v1,
Author        = {Hanwen Zha and Zhiyu Chen and Xifeng Yan},
Title         = {Inductive Relation Prediction by BERT},
Eprint        = {2103.07102v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Relation prediction in knowledge graphs is dominated by embedding based
methods which mainly focus on the transductive setting. Unfortunately, they are
not able to handle inductive learning where unseen entities and relations are
present and cannot take advantage of prior knowledge. Furthermore, their
inference process is not easily explainable. In this work, we propose an
all-in-one solution, called BERTRL (BERT-based Relational Learning), which
leverages pre-trained language model and fine-tunes it by taking relation
instances and their possible reasoning paths as training samples. BERTRL
outperforms the SOTAs in 15 out of 18 cases in both inductive and transductive
settings. Meanwhile, it demonstrates strong generalization capability in
few-shot learning and is explainable.},
Year          = {2021},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2103.07102v1},
File          = {2103.07102v1.pdf}
}
@article{2304.13149v1,
Author        = {Christophe Van Gysel},
Title         = {Modeling Spoken Information Queries for Virtual Assistants: Open
  Problems, Challenges and Opportunities},
Eprint        = {2304.13149v1},
DOI           = {10.1145/3539618.3591849},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Virtual assistants are becoming increasingly important speech-driven
Information Retrieval platforms that assist users with various tasks.
  We discuss open problems and challenges with respect to modeling spoken
information queries for virtual assistants, and list opportunities where
Information Retrieval methods and research can be applied to improve the
quality of virtual assistant speech recognition.
  We discuss how query domain classification, knowledge graphs and user
interaction data, and query personalization can be helpful to improve the
accurate recognition of spoken information domain queries. Finally, we also
provide a brief overview of current problems and challenges in speech
recognition.},
Year          = {2023},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2304.13149v1},
File          = {2304.13149v1.pdf}
}
@article{2012.15643v2,
Author        = {Changlong Yu and Hongming Zhang and Yangqiu Song and Wilfred Ng},
Title         = {CoCoLM: COmplex COmmonsense Enhanced Language Model with Discourse
  Relations},
Eprint        = {2012.15643v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large-scale pre-trained language models have demonstrated strong knowledge
representation ability. However, recent studies suggest that even though these
giant models contains rich simple commonsense knowledge (e.g., bird can fly and
fish can swim.), they often struggle with the complex commonsense knowledge
that involves multiple eventualities (verb-centric phrases, e.g., identifying
the relationship between ``Jim yells at Bob'' and ``Bob is upset'').To address
this problem, in this paper, we propose to help pre-trained language models
better incorporate complex commonsense knowledge. Different from existing
fine-tuning approaches, we do not focus on a specific task and propose a
general language model named CoCoLM. Through the careful training over a
large-scale eventuality knowledge graphs ASER, we successfully teach
pre-trained language models (i.e., BERT and RoBERTa) rich complex commonsense
knowledge among eventualities. Experiments on multiple downstream commonsense
tasks that requires the correct understanding of eventualities demonstrate the
effectiveness of CoCoLM.},
Year          = {2020},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2012.15643v2},
File          = {2012.15643v2.pdf}
}
@article{2110.15705v1,
Author        = {Asahi Ushio and Jose Camacho-Collados and Steven Schockaert},
Title         = {Distilling Relation Embeddings from Pre-trained Language Models},
Eprint        = {2110.15705v1},
DOI           = {10.18653/v1/2021.emnlp-main.712},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pre-trained language models have been found to capture a surprisingly rich
amount of lexical knowledge, ranging from commonsense properties of everyday
concepts to detailed factual knowledge about named entities. Among others, this
makes it possible to distill high-quality word vectors from pre-trained
language models. However, it is currently unclear to what extent it is possible
to distill relation embeddings, i.e. vectors that characterize the relationship
between two words. Such relation embeddings are appealing because they can, in
principle, encode relational knowledge in a more fine-grained way than is
possible with knowledge graphs. To obtain relation embeddings from a
pre-trained language model, we encode word pairs using a (manually or
automatically generated) prompt, and we fine-tune the language model such that
relationally similar word pairs yield similar output vectors. We find that the
resulting relation embeddings are highly competitive on analogy (unsupervised)
and relation classification (supervised) benchmarks, even without any
task-specific fine-tuning. Source code to reproduce our experimental results
and the model checkpoints are available in the following repository:
https://github.com/asahi417/relbert},
Year          = {2021},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2110.15705v1},
File          = {2110.15705v1.pdf}
}
@article{2112.08593v1,
Author        = {Amal Alabdulkarim and Winston Li and Lara J. Martin and Mark O. Riedl},
Title         = {Goal-Directed Story Generation: Augmenting Generative Language Models
  with Reinforcement Learning},
Eprint        = {2112.08593v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The advent of large pre-trained generative language models has provided a
common framework for AI story generation via sampling the model to create
sequences that continue the story. However, sampling alone is insufficient for
story generation. In particular, it is hard to direct a language model to
create stories to reach a specific goal event. We present two automated
techniques grounded in deep reinforcement learning and reward shaping to
control the plot of computer-generated stories. The first utilizes proximal
policy optimization to fine-tune an existing transformer-based language model
to generate text continuations but also be goal-seeking. The second extracts a
knowledge graph from the unfolding story, which is used by a policy network
with graph attention to select a candidate continuation generated by a language
model. We report on automated metrics pertaining to how often stories achieve a
given goal event as well as human participant rankings of coherence and overall
story quality compared to baselines and ablations.},
Year          = {2021},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2112.08593v1},
File          = {2112.08593v1.pdf}
}
@article{2308.15047v1,
Author        = {Mathias Lykke Gammelgaard and Jonathan Gabel Christiansen and Anders Søgaard},
Title         = {Large language models converge toward human-like concept organization},
Eprint        = {2308.15047v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Large language models show human-like performance in knowledge extraction,
reasoning and dialogue, but it remains controversial whether this performance
is best explained by memorization and pattern matching, or whether it reflects
human-like inferential semantics and world knowledge. Knowledge bases such as
WikiData provide large-scale, high-quality representations of inferential
semantics and world knowledge. We show that large language models learn to
organize concepts in ways that are strikingly similar to how concepts are
organized in such knowledge bases. Knowledge bases model collective,
institutional knowledge, and large language models seem to induce such
knowledge from raw text. We show that bigger and better models exhibit more
human-like concept organization, across four families of language models and
three knowledge graph embeddings.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.15047v1},
File          = {2308.15047v1.pdf}
}
@article{2402.01495v1,
Author        = {Phillip Schneider and Manuel Klettner and Elena Simperl and Florian Matthes},
Title         = {A Comparative Analysis of Conversational Large Language Models in
  Knowledge-Based Text Generation},
Eprint        = {2402.01495v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Generating natural language text from graph-structured data is essential for
conversational information seeking. Semantic triples derived from knowledge
graphs can serve as a valuable source for grounding responses from
conversational agents by providing a factual basis for the information they
communicate. This is especially relevant in the context of large language
models, which offer great potential for conversational interaction but are
prone to hallucinating, omitting, or producing conflicting information. In this
study, we conduct an empirical analysis of conversational large language models
in generating natural language text from semantic triples. We compare four
large language models of varying sizes with different prompting techniques.
Through a series of benchmark experiments on the WebNLG dataset, we analyze the
models' performance and identify the most common issues in the generated
predictions. Our findings show that the capabilities of large language models
in triple verbalization can be significantly improved through few-shot
prompting, post-processing, and efficient fine-tuning techniques, particularly
for smaller models that exhibit lower zero-shot performance.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.01495v1},
File          = {2402.01495v1.pdf}
}
@article{2209.04595v1,
Author        = {Zhi Chen and Yuncong Liu and Lu Chen and Su Zhu and Mengyue Wu and Kai Yu},
Title         = {OPAL: Ontology-Aware Pretrained Language Model for End-to-End
  Task-Oriented Dialogue},
Eprint        = {2209.04595v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper presents an ontology-aware pretrained language model (OPAL) for
end-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models,
task-oriented dialogue models fulfill at least two task-specific modules:
dialogue state tracker (DST) and response generator (RG). The dialogue state
consists of the domain-slot-value triples, which are regarded as the user's
constraints to search the domain-related databases. The large-scale
task-oriented dialogue data with the annotated structured dialogue state
usually are inaccessible. It prevents the development of the pretrained
language model for the task-oriented dialogue. We propose a simple yet
effective pretraining method to alleviate this problem, which consists of two
pretraining phases. The first phase is to pretrain on large-scale contextual
text data, where the structured information of the text is extracted by the
information extracting tool. To bridge the gap between the pretraining method
and downstream tasks, we design two pretraining tasks: ontology-like triple
recovery and next-text generation, which simulates the DST and RG,
respectively. The second phase is to fine-tune the pretrained model on the TOD
data. The experimental results show that our proposed method achieves an
exciting boost and get competitive performance even without any TOD data on
CamRest676 and MultiWOZ benchmarks.},
Year          = {2022},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2209.04595v1},
File          = {2209.04595v1.pdf}
}
@article{2208.09828v3,
Author        = {Yang Liu and Zequn Sun and Guangyao Li and Wei Hu},
Title         = {I Know What You Do Not Know: Knowledge Graph Embedding via
  Co-distillation Learning},
Eprint        = {2208.09828v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graph (KG) embedding seeks to learn vector representations for
entities and relations. Conventional models reason over graph structures, but
they suffer from the issues of graph incompleteness and long-tail entities.
Recent studies have used pre-trained language models to learn embeddings based
on the textual information of entities and relations, but they cannot take
advantage of graph structures. In the paper, we show empirically that these two
kinds of features are complementary for KG embedding. To this end, we propose
CoLE, a Co-distillation Learning method for KG Embedding that exploits the
complementarity of graph structures and text information. Its graph embedding
model employs Transformer to reconstruct the representation of an entity from
its neighborhood subgraph. Its text embedding model uses a pre-trained language
model to generate entity representations from the soft prompts of their names,
descriptions, and relational neighbors. To let the two model promote each
other, we propose co-distillation learning that allows them to distill
selective knowledge from each other's prediction logits. In our co-distillation
learning, each model serves as both a teacher and a student. Experiments on
benchmark datasets demonstrate that the two models outperform their related
baselines, and the ensemble method CoLE with co-distillation learning advances
the state-of-the-art of KG embedding.},
Year          = {2022},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2208.09828v3},
File          = {2208.09828v3.pdf}
}
@article{2205.00820v1,
Author        = {Emma J. Gerritse and Faegheh Hasibi and Arjen P. de Vries},
Title         = {Entity-aware Transformers for Entity Search},
Eprint        = {2205.00820v1},
DOI           = {10.1145/3477495.3531971},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Pre-trained language models such as BERT have been a key ingredient to
achieve state-of-the-art results on a variety of tasks in natural language
processing and, more recently, also in information retrieval.Recent research
even claims that BERT is able to capture factual knowledge about entity
relations and properties, the information that is commonly obtained from
knowledge graphs. This paper investigates the following question: Do BERT-based
entity retrieval models benefit from additional entity information stored in
knowledge graphs? To address this research question, we map entity embeddings
into the same input space as a pre-trained BERT model and inject these entity
embeddings into the BERT model. This entity-enriched language model is then
employed on the entity retrieval task. We show that the entity-enriched BERT
model improves effectiveness on entity-oriented queries over a regular BERT
model, establishing a new state-of-the-art result for the entity retrieval
task, with substantial improvements for complex natural language queries and
queries requesting a list of entities with a certain property. Additionally, we
show that the entity information provided by our entity-enriched model
particularly helps queries related to less popular entities. Last, we observe
empirically that the entity-enriched BERT models enable fine-tuning on limited
training data, which otherwise would not be feasible due to the known
instabilities of BERT in few-sample fine-tuning, thereby contributing to
data-efficient training of BERT for entity search.},
Year          = {2022},
Month         = {May},
Url           = {http://arxiv.org/abs/2205.00820v1},
File          = {2205.00820v1.pdf}
}
@article{2206.14268v3,
Author        = {Shibo Hao and Bowen Tan and Kaiwen Tang and Bin Ni and Xiyan Shao and Hengzhe Zhang and Eric P. Xing and Zhiting Hu},
Title         = {BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from
  Pretrained Language Models},
Eprint        = {2206.14268v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {It is crucial to automatically construct knowledge graphs (KGs) of diverse
new relations to support knowledge discovery and broad applications. Previous
KG construction methods, based on either crowdsourcing or text mining, are
often limited to a small predefined set of relations due to manual cost or
restrictions in text corpus. Recent research proposed to use pretrained
language models (LMs) as implicit knowledge bases that accept knowledge queries
with prompts. Yet, the implicit knowledge lacks many desirable properties of a
full-scale symbolic KG, such as easy access, navigation, editing, and quality
assurance. In this paper, we propose a new approach of harvesting massive KGs
of arbitrary relations from pretrained LMs. With minimal input of a relation
definition (a prompt and a few shot of example entity pairs), the approach
efficiently searches in the vast entity pair space to extract diverse accurate
knowledge of the desired relation. We develop an effective search-and-rescore
mechanism for improved efficiency and accuracy. We deploy the approach to
harvest KGs of over 400 new relations from different LMs. Extensive human and
automatic evaluations show our approach manages to extract diverse accurate
knowledge, including tuples of complex relations (e.g., "A is capable of but
not good at B"). The resulting KGs as a symbolic interpretation of the source
LMs also reveal new insights into the LMs' knowledge capacities.},
Year          = {2022},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2206.14268v3},
File          = {2206.14268v3.pdf}
}
@article{2212.07798v1,
Author        = {Jiarui Zhang and Filip Ilievski and Aravinda Kollaa and Jonathan Francis and Kaixin Ma and Alessandro Oltramari},
Title         = {Utilizing Background Knowledge for Robust Reasoning over Traffic
  Situations},
Eprint        = {2212.07798v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Understanding novel situations in the traffic domain requires an intricate
combination of domain-specific and causal commonsense knowledge. Prior work has
provided sufficient perception-based modalities for traffic monitoring, in this
paper, we focus on a complementary research aspect of Intelligent
Transportation: traffic understanding. We scope our study to text-based methods
and datasets given the abundant commonsense knowledge that can be extracted
using language models from large corpus and knowledge graphs. We adopt three
knowledge-driven approaches for zero-shot QA over traffic situations, based on
prior natural language inference methods, commonsense models with knowledge
graph self-supervision, and dense retriever-based models. We constructed two
text-based multiple-choice question answering sets: BDD-QA for evaluating
causal reasoning in the traffic domain and HDT-QA for measuring the possession
of domain knowledge akin to human driving license tests. Among the methods,
Unified-QA reaches the best performance on the BDD-QA dataset with the
adaptation of multiple formats of question answers. Language models trained
with inference information and commonsense knowledge are also good at
predicting the cause and effect in the traffic domain but perform badly at
answering human-driving QA sets. For such sets, DPR+Unified-QA performs the
best due to its efficient knowledge extraction.},
Year          = {2022},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2212.07798v1},
File          = {2212.07798v1.pdf}
}
@article{2301.11293v1,
Author        = {Mehran Kazemi and Sid Mittal and Deepak Ramachandran},
Title         = {Understanding Finetuning for Factual Knowledge Extraction from Language
  Models},
Eprint        = {2301.11293v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Language models (LMs) pretrained on large corpora of text from the web have
been observed to contain large amounts of various types of knowledge about the
world. This observation has led to a new and exciting paradigm in knowledge
graph construction where, instead of manual curation or text mining, one
extracts knowledge from the parameters of an LM. Recently, it has been shown
that finetuning LMs on a set of factual knowledge makes them produce better
answers to queries from a different set, thus making finetuned LMs a good
candidate for knowledge extraction and, consequently, knowledge graph
construction. In this paper, we analyze finetuned LMs for factual knowledge
extraction. We show that along with its previously known positive effects,
finetuning also leads to a (potentially harmful) phenomenon which we call
Frequency Shock, where at the test time the model over-predicts rare entities
that appear in the training set and under-predicts common entities that do not
appear in the training set enough times. We show that Frequency Shock leads to
a degradation in the predictions of the model and beyond a point, the harm from
Frequency Shock can even outweigh the positive effects of finetuning, making
finetuning harmful overall. We then consider two solutions to remedy the
identified negative effect: 1- model mixing and 2- mixture finetuning with the
LM's pre-training task. The two solutions combined lead to significant
improvements compared to vanilla finetuning.},
Year          = {2023},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2301.11293v1},
File          = {2301.11293v1.pdf}
}
@article{2303.10368v1,
Author        = {Nan Hu and Yike Wu and Guilin Qi and Dehai Min and Jiaoyan Chen and Jeff Z. Pan and Zafar Ali},
Title         = {An Empirical Study of Pre-trained Language Models in Simple Knowledge
  Graph Question Answering},
Eprint        = {2303.10368v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Large-scale pre-trained language models (PLMs) such as BERT have recently
achieved great success and become a milestone in natural language processing
(NLP). It is now the consensus of the NLP community to adopt PLMs as the
backbone for downstream tasks. In recent works on knowledge graph question
answering (KGQA), BERT or its variants have become necessary in their KGQA
models. However, there is still a lack of comprehensive research and comparison
of the performance of different PLMs in KGQA. To this end, we summarize two
basic KGQA frameworks based on PLMs without additional neural network modules
to compare the performance of nine PLMs in terms of accuracy and efficiency. In
addition, we present three benchmarks for larger-scale KGs based on the popular
SimpleQuestions benchmark to investigate the scalability of PLMs. We carefully
analyze the results of all PLMs-based KGQA basic frameworks on these benchmarks
and two other popular datasets, WebQuestionSP and FreebaseQA, and find that
knowledge distillation techniques and knowledge enhancement methods in PLMs are
promising for KGQA. Furthermore, we test ChatGPT, which has drawn a great deal
of attention in the NLP community, demonstrating its impressive capabilities
and limitations in zero-shot KGQA. We have released the code and benchmarks to
promote the use of PLMs on KGQA.},
Year          = {2023},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2303.10368v1},
File          = {2303.10368v1.pdf}
}
@article{2306.16195v1,
Author        = {Chen Tang and Hongbo Zhang and Tyler Loakman and Chenghua Lin and Frank Guerin},
Title         = {Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation},
Eprint        = {2306.16195v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Incorporating external graph knowledge into neural chatbot models has been
proven effective for enhancing dialogue generation. However, in conventional
graph neural networks (GNNs), message passing on a graph is independent from
text, resulting in the graph representation hidden space differing from that of
the text. This training regime of existing models therefore leads to a semantic
gap between graph knowledge and text. In this study, we propose a novel
framework for knowledge graph enhanced dialogue generation. We dynamically
construct a multi-hop knowledge graph with pseudo nodes to involve the language
model in feature aggregation within the graph at all steps. To avoid the
semantic biases caused by learning on vanilla subgraphs, the proposed framework
applies hierarchical graph attention to aggregate graph features on pseudo
nodes and then attains a global feature. Therefore, the framework can better
utilise the heterogeneous features from both the post and external graph
knowledge. Extensive experiments demonstrate that our framework outperforms
state-of-the-art (SOTA) baselines on dialogue generation. Further analysis also
shows that our representation learning framework can fill the semantic gap by
coagulating representations of both text and graph knowledge. Moreover, the
language model also learns how to better select knowledge triples for a more
informative response via exploiting subgraph patterns within our feature
aggregation process. Our code and resources are available at
https://github.com/tangg555/SaBART.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.16195v1},
File          = {2306.16195v1.pdf}
}
@article{2307.11772v3,
Author        = {Rui Zhang and Yixin Su and Bayu Distiawan Trisedya and Xiaoyan Zhao and Min Yang and Hong Cheng and Jianzhong Qi},
Title         = {AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment
  enabled by Large Language Models},
Eprint        = {2307.11772v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The task of entity alignment between knowledge graphs (KGs) aims to identify
every pair of entities from two different KGs that represent the same entity.
Many machine learning-based methods have been proposed for this task. However,
to our best knowledge, existing methods all require manually crafted seed
alignments, which are expensive to obtain. In this paper, we propose the first
fully automatic alignment method named AutoAlign, which does not require any
manually crafted seed alignments. Specifically, for predicate embeddings,
AutoAlign constructs a predicate-proximity-graph with the help of large
language models to automatically capture the similarity between predicates
across two KGs. For entity embeddings, AutoAlign first computes the entity
embeddings of each KG independently using TransE, and then shifts the two KGs'
entity embeddings into the same vector space by computing the similarity
between entities based on their attributes. Thus, both predicate alignment and
entity alignment can be done without manually crafted seed alignments.
AutoAlign is not only fully automatic, but also highly effective. Experiments
using real-world KGs show that AutoAlign improves the performance of entity
alignment significantly compared to state-of-the-art methods.},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.11772v3},
File          = {2307.11772v3.pdf}
}
@article{2312.01837v2,
Author        = {Yuxia Geng and Jiaoyan Chen and Yuhang Zeng and Zhuo Chen and Wen Zhang and Jeff Z. Pan and Yuxiang Wang and Xiaoliang Xu},
Title         = {Prompting Disentangled Embeddings for Knowledge Graph Completion with
  Pre-trained Language Model},
Eprint        = {2312.01837v2},
DOI           = {10.1016/j.eswa.2024.126175},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Both graph structures and textual information play a critical role in
Knowledge Graph Completion (KGC). With the success of Pre-trained Language
Models (PLMs) such as BERT, they have been applied for text encoding for KGC.
However, the current methods mostly prefer to fine-tune PLMs, leading to huge
training costs and limited scalability to larger PLMs. In contrast, we propose
to utilize prompts and perform KGC on a frozen PLM with only the prompts
trained. Accordingly, we propose a new KGC method named PDKGC with two prompts
-- a hard task prompt which is to adapt the KGC task to the PLM pre-training
task of token prediction, and a disentangled structure prompt which learns
disentangled graph representation so as to enable the PLM to combine more
relevant structure knowledge with the text information. With the two prompts,
PDKGC builds a textual predictor and a structural predictor, respectively, and
their combination leads to more comprehensive entity prediction. Solid
evaluation on three widely used KGC datasets has shown that PDKGC often
outperforms the baselines including the state-of-the-art, and its components
are all effective. Our codes and data are available at
https://github.com/genggengcss/PDKGC.},
Year          = {2023},
Month         = {Dec},
Note          = {Expert Systems With Applications 268 (2025) 126175},
Url           = {http://arxiv.org/abs/2312.01837v2},
File          = {2312.01837v2.pdf}
}
@article{2406.00318v1,
Author        = {Yubo Wang and Hao Xin and Lei Chen},
Title         = {KGLink: A column type annotation method that combines knowledge graph
  and pre-trained language model},
Eprint        = {2406.00318v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The semantic annotation of tabular data plays a crucial role in various
downstream tasks. Previous research has proposed knowledge graph (KG)-based and
deep learning-based methods, each with its inherent limitations. KG-based
methods encounter difficulties annotating columns when there is no match for
column cells in the KG. Moreover, KG-based methods can provide multiple
predictions for one column, making it challenging to determine the semantic
type with the most suitable granularity for the dataset. This type granularity
issue limits their scalability.
  On the other hand, deep learning-based methods face challenges related to the
valuable context missing issue. This occurs when the information within the
table is insufficient for determining the correct column type.
  This paper presents KGLink, a method that combines WikiData KG information
with a pre-trained deep learning language model for table column annotation,
effectively addressing both type granularity and valuable context missing
issues. Through comprehensive experiments on widely used tabular datasets
encompassing numeric and string columns with varying type granularity, we
showcase the effectiveness and efficiency of KGLink. By leveraging the
strengths of KGLink, we successfully surmount challenges related to type
granularity and valuable context issues, establishing it as a robust solution
for the semantic annotation of tabular data.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.00318v1},
File          = {2406.00318v1.pdf}
}
@article{2409.03284v1,
Author        = {Yassir Lairgi and Ludovic Moncla and Rémy Cazabet and Khalid Benabdeslem and Pierre Cléau},
Title         = {iText2KG: Incremental Knowledge Graphs Construction Using Large Language
  Models},
Eprint        = {2409.03284v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Most available data is unstructured, making it challenging to access valuable
information. Automatically building Knowledge Graphs (KGs) is crucial for
structuring data and making it accessible, allowing users to search for
information effectively. KGs also facilitate insights, inference, and
reasoning. Traditional NLP methods, such as named entity recognition and
relation extraction, are key in information retrieval but face limitations,
including the use of predefined entity types and the need for supervised
learning. Current research leverages large language models' capabilities, such
as zero- or few-shot learning. However, unresolved and semantically duplicated
entities and relations still pose challenges, leading to inconsistent graphs
and requiring extensive post-processing. Additionally, most approaches are
topic-dependent. In this paper, we propose iText2KG, a method for incremental,
topic-independent KG construction without post-processing. This plug-and-play,
zero-shot method is applicable across a wide range of KG construction scenarios
and comprises four modules: Document Distiller, Incremental Entity Extractor,
Incremental Relation Extractor, and Graph Integrator and Visualization. Our
method demonstrates superior performance compared to baseline methods across
three scenarios: converting scientific papers to graphs, websites to graphs,
and CVs to graphs.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.03284v1},
File          = {2409.03284v1.pdf}
}
@article{2311.08147v1,
Author        = {Yi Liu and Lianzhe Huang and Shicheng Li and Sishuo Chen and Hao Zhou and Fandong Meng and Jie Zhou and Xu Sun},
Title         = {RECALL: A Benchmark for LLMs Robustness against External Counterfactual
  Knowledge},
Eprint        = {2311.08147v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {LLMs and AI chatbots have improved people's efficiency in various fields.
However, the necessary knowledge for answering the question may be beyond the
models' knowledge boundaries. To mitigate this issue, many researchers try to
introduce external knowledge, such as knowledge graphs and Internet contents,
into LLMs for up-to-date information. However, the external information from
the Internet may include counterfactual information that will confuse the model
and lead to an incorrect response. Thus there is a pressing need for LLMs to
possess the ability to distinguish reliable information from external
knowledge. Therefore, to evaluate the ability of LLMs to discern the
reliability of external knowledge, we create a benchmark from existing
knowledge bases. Our benchmark consists of two tasks, Question Answering and
Text Generation, and for each task, we provide models with a context containing
counterfactual information. Evaluation results show that existing LLMs are
susceptible to interference from unreliable external knowledge with
counterfactual information, and simple intervention methods make limited
contributions to the alleviation of this issue.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.08147v1},
File          = {2311.08147v1.pdf}
}
@article{2404.08700v3,
Author        = {Seyed Mahed Mousavi and Simone Alghisi and Giuseppe Riccardi},
Title         = {DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs},
Eprint        = {2404.08700v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {LLMs acquire knowledge from massive data snapshots collected at different
timestamps. Their knowledge is then commonly evaluated using static benchmarks.
However, factual knowledge is generally subject to time-sensitive changes, and
static benchmarks cannot address those cases. We present an approach to
dynamically evaluate the knowledge in LLMs and their time-sensitiveness against
Wikidata, a publicly available up-to-date knowledge graph. We evaluate the
time-sensitive knowledge in twenty-four private and open-source LLMs, as well
as the effectiveness of four editing methods in updating the outdated facts.
Our results show that 1) outdatedness is a critical problem across
state-of-the-art LLMs; 2) LLMs output inconsistent answers when prompted with
slight variations of the question prompt; and 3) the performance of the
state-of-the-art knowledge editing algorithms is very limited, as they can not
reduce the cases of outdatedness and output inconsistency.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.08700v3},
File          = {2404.08700v3.pdf}
}
@article{2112.12672v1,
Author        = {Francesco Moramarco and Damir Juric and Aleksandar Savkov and Jack Flann and Maria Lehl and Kristian Boda and Tessa Grafen and Vitalii Zhelezniak and Sunir Gohil and Alex Papadopoulos Korfiatis and Nils Hammerla},
Title         = {Towards more patient friendly clinical notes through language models and
  ontologies},
Eprint        = {2112.12672v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Clinical notes are an efficient way to record patient information but are
notoriously hard to decipher for non-experts. Automatically simplifying medical
text can empower patients with valuable information about their health, while
saving clinicians time. We present a novel approach to automated simplification
of medical text based on word frequencies and language modelling, grounded on
medical ontologies enriched with layman terms. We release a new dataset of
pairs of publicly available medical sentences and a version of them simplified
by clinicians. Also, we define a novel text simplification metric and
evaluation framework, which we use to conduct a large-scale human evaluation of
our method against the state of the art. Our method based on a language model
trained on medical forum data generates simpler sentences while preserving both
grammar and the original meaning, surpassing the current state of the art.},
Year          = {2021},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2112.12672v1},
File          = {2112.12672v1.pdf}
}
@article{2312.13530v1,
Author        = {Yu-Zheng Lin and Muntasir Mamun and Muhtasim Alam Chowdhury and Shuyu Cai and Mingyu Zhu and Banafsheh Saber Latibari and Kevin Immanuel Gubbi and Najmeh Nazari Bavarsad and Arjun Caputo and Avesta Sasan and Houman Homayoun and Setareh Rafatirad and Pratik Satam and Soheil Salehi},
Title         = {HW-V2W-Map: Hardware Vulnerability to Weakness Mapping Framework for
  Root Cause Analysis with GPT-assisted Mitigation Suggestion},
Eprint        = {2312.13530v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {The escalating complexity of modern computing frameworks has resulted in a
surge in the cybersecurity vulnerabilities reported to the National
Vulnerability Database (NVD) by practitioners. Despite the fact that the
stature of NVD is one of the most significant databases for the latest insights
into vulnerabilities, extracting meaningful trends from such a large amount of
unstructured data is still challenging without the application of suitable
technological methodologies. Previous efforts have mostly concentrated on
software vulnerabilities; however, a holistic strategy incorporates approaches
for mitigating vulnerabilities, score prediction, and a knowledge-generating
system that may extract relevant insights from the Common Weakness Enumeration
(CWE) and Common Vulnerability Exchange (CVE) databases is notably absent. As
the number of hardware attacks on Internet of Things (IoT) devices continues to
rapidly increase, we present the Hardware Vulnerability to Weakness Mapping
(HW-V2W-Map) Framework, which is a Machine Learning (ML) framework focusing on
hardware vulnerabilities and IoT security. The architecture that we have
proposed incorporates an Ontology-driven Storytelling framework, which
automates the process of updating the ontology in order to recognize patterns
and evolution of vulnerabilities over time and provides approaches for
mitigating the vulnerabilities. The repercussions of vulnerabilities can be
mitigated as a result of this, and conversely, future exposures can be
predicted and prevented. Furthermore, our proposed framework utilized
Generative Pre-trained Transformer (GPT) Large Language Models (LLMs) to
provide mitigation suggestions.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.13530v1},
File          = {2312.13530v1.pdf}
}
@article{2305.12363v3,
Author        = {Laksh Nanwani and Anmol Agarwal and Kanishk Jain and Raghav Prabhakar and Aaron Monis and Aditya Mathur and Krishna Murthy and Abdul Hafez and Vineet Gandhi and K. Madhava Krishna},
Title         = {Instance-Level Semantic Maps for Vision Language Navigation},
Eprint        = {2305.12363v3},
DOI           = {10.1109/RO-MAN57019.2023.10309534},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Humans have a natural ability to perform semantic associations with the
surrounding objects in the environment. This allows them to create a mental map
of the environment, allowing them to navigate on-demand when given linguistic
instructions. A natural goal in Vision Language Navigation (VLN) research is to
impart autonomous agents with similar capabilities. Recent works take a step
towards this goal by creating a semantic spatial map representation of the
environment without any labeled data. However, their representations are
limited for practical applicability as they do not distinguish between
different instances of the same object. In this work, we address this
limitation by integrating instance-level information into spatial map
representation using a community detection algorithm and utilizing word
ontology learned by large language models (LLMs) to perform open-set semantic
associations in the mapping representation. The resulting map representation
improves the navigation performance by two-fold (233%) on realistic language
commands with instance-specific descriptions compared to the baseline. We
validate the practicality and effectiveness of our approach through extensive
qualitative and quantitative experiments.},
Year          = {2023},
Month         = {May},
Note          = {IEEE RO-MAN 2023},
Url           = {http://arxiv.org/abs/2305.12363v3},
File          = {2305.12363v3.pdf}
}
@article{2306.08937v3,
Author        = {Lijun Yu and Jin Miao and Xiaoyu Sun and Jiayi Chen and Alexander G. Hauptmann and Hanjun Dai and Wei Wei},
Title         = {DocumentNet: Bridging the Data Gap in Document Pre-Training},
Eprint        = {2306.08937v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Document understanding tasks, in particular, Visually-rich Document Entity
Retrieval (VDER), have gained significant attention in recent years thanks to
their broad applications in enterprise AI. However, publicly available data
have been scarce for these tasks due to strict privacy constraints and high
annotation costs. To make things worse, the non-overlapping entity spaces from
different datasets hinder the knowledge transfer between document types. In
this paper, we propose a method to collect massive-scale and weakly labeled
data from the web to benefit the training of VDER models. The collected
dataset, named DocumentNet, does not depend on specific document types or
entity sets, making it universally applicable to all VDER tasks. The current
DocumentNet consists of 30M documents spanning nearly 400 document types
organized in a four-level ontology. Experiments on a set of broadly adopted
VDER tasks show significant improvements when DocumentNet is incorporated into
the pre-training for both classic and few-shot learning settings. With the
recent emergence of large language models (LLMs), DocumentNet provides a large
data source to extend their multi-modal capabilities for VDER.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.08937v3},
File          = {2306.08937v3.pdf}
}
@article{1608.00318v2,
Author        = {Sungjin Ahn and Heeyoul Choi and Tanel Pärnamaa and Yoshua Bengio},
Title         = {A Neural Knowledge Language Model},
Eprint        = {1608.00318v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Current language models have a significant limitation in the ability to
encode and decode factual knowledge. This is mainly because they acquire such
knowledge from statistical co-occurrences although most of the knowledge words
are rarely observed. In this paper, we propose a Neural Knowledge Language
Model (NKLM) which combines symbolic knowledge provided by the knowledge graph
with the RNN language model. By predicting whether the word to generate has an
underlying fact or not, the model can generate such knowledge-related words by
copying from the description of the predicted fact. In experiments, we show
that the NKLM significantly improves the performance while generating a much
smaller number of unknown words.},
Year          = {2016},
Month         = {Aug},
Url           = {http://arxiv.org/abs/1608.00318v2},
File          = {1608.00318v2.pdf}
}
@article{2112.08615v3,
Author        = {Pedram Hosseini and David A. Broniatowski and Mona Diab},
Title         = {Knowledge-Augmented Language Models for Cause-Effect Relation
  Classification},
Eprint        = {2112.08615v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Previous studies have shown the efficacy of knowledge augmentation methods in
pretrained language models. However, these methods behave differently across
domains and downstream tasks. In this work, we investigate the augmentation of
pretrained language models with commonsense knowledge in the cause-effect
relation classification and commonsense causal reasoning tasks. After
automatically verbalizing ATOMIC2020, a wide coverage commonsense reasoning
knowledge graph, and GLUCOSE, a dataset of implicit commonsense causal
knowledge, we continually pretrain BERT and RoBERTa with the verbalized data.
Then we evaluate the resulting models on cause-effect pair classification and
answering commonsense causal reasoning questions. Our results show that
continually pretrained language models augmented with commonsense knowledge
outperform our baselines on two commonsense causal reasoning benchmarks, COPA
and BCOPA-CE, and the Temporal and Causal Reasoning (TCR) dataset, without
additional improvement in model architecture or using quality-enhanced data for
fine-tuning.},
Year          = {2021},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2112.08615v3},
File          = {2112.08615v3.pdf}
}
@article{2501.17549v1,
Author        = {Wooyoung Kim and Byungyoon Park and Wooju Kim},
Title         = {Query-Aware Learnable Graph Pooling Tokens as Prompt for Large Language
  Models},
Eprint        = {2501.17549v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Graph-structured data plays a vital role in numerous domains, such as social
networks, citation networks, commonsense reasoning graphs and knowledge graphs.
While graph neural networks have been employed for graph processing, recent
advancements have explored integrating large language models for graph-based
tasks. In this paper, we propose a novel approach named Learnable Graph Pooling
Token (LGPT), which addresses the limitations of the scalability issues in
node-level projection and information loss in graph-level projection. LGPT
enables flexible and efficient graph representation by introducing learnable
parameters that act as tokens in large language models, balancing fine-grained
and global graph information. Additionally, we investigate an Early Query
Fusion technique, which fuses query context before constructing the graph
representation, leading to more effective graph embeddings. Our method achieves
a 4.13\% performance improvement on the GraphQA benchmark without training the
large language model, demonstrating significant gains in handling complex
textual-attributed graph data.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.17549v1},
File          = {2501.17549v1.pdf}
}
@article{2409.07507v1,
Author        = {Daniel Adam and Tomáš Kliegr},
Title         = {Traceable LLM-based validation of statements in knowledge graphs},
Eprint        = {2409.07507v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This article presents a method for verifying RDF triples using LLMs, with an
emphasis on providing traceable arguments. Because the LLMs cannot currently
reliably identify the origin of the information used to construct the response
to the user query, our approach is to avoid using internal LLM factual
knowledge altogether. Instead, verified RDF statements are compared to chunks
of external documents retrieved through a web search or Wikipedia. To assess
the possible application of this workflow on biosciences content, we evaluated
1,719 positive statements from the BioRED dataset and the same number of newly
generated negative statements. The resulting precision is 88%, and recall is
44%. This indicates that the method requires human oversight. We demonstrate
the method on Wikidata, where a SPARQL query is used to automatically retrieve
statements needing verification. Overall, the results suggest that LLMs could
be used for large-scale verification of statements in KGs, a task previously
unfeasible due to human annotation costs.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.07507v1},
File          = {2409.07507v1.pdf}
}
@article{2310.08291v1,
Author        = {Dong Yang and Xu Wang and Remzi Celebi},
Title         = {Expanding the Vocabulary of BERT for Knowledge Base Construction},
Eprint        = {2310.08291v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge base construction entails acquiring structured information to
create a knowledge base of factual and relational data, facilitating question
answering, information retrieval, and semantic understanding. The challenge
called "Knowledge Base Construction from Pretrained Language Models" at
International Semantic Web Conference 2023 defines tasks focused on
constructing knowledge base using language model. Our focus was on Track 1 of
the challenge, where the parameters are constrained to a maximum of 1 billion,
and the inclusion of entity descriptions within the prompt is prohibited.
  Although the masked language model offers sufficient flexibility to extend
its vocabulary, it is not inherently designed for multi-token prediction. To
address this, we present Vocabulary Expandable BERT for knowledge base
construction, which expand the language model's vocabulary while preserving
semantic embeddings for newly added words. We adopt task-specific
re-pre-training on masked language model to further enhance the language model.
  Through experimentation, the results show the effectiveness of our
approaches. Our framework achieves F1 score of 0.323 on the hidden test set and
0.362 on the validation set, both data set is provided by the challenge.
Notably, our framework adopts a lightweight language model (BERT-base, 0.13
billion parameters) and surpasses the model using prompts directly on large
language model (Chatgpt-3, 175 billion parameters). Besides, Token-Recode
achieves comparable performances as Re-pretrain. This research advances
language understanding models by enabling the direct embedding of multi-token
entities, signifying a substantial step forward in link prediction task in
knowledge graph and metadata completion in data management.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.08291v1},
File          = {2310.08291v1.pdf}
}
@article{2009.09263v2,
Author        = {Bin Wang and Guangtao Wang and Jing Huang and Jiaxuan You and Jure Leskovec and C. -C. Jay Kuo},
Title         = {Inductive Learning on Commonsense Knowledge Graph Completion},
Eprint        = {2009.09263v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Commonsense knowledge graph (CKG) is a special type of knowledge graph (KG),
where entities are composed of free-form text. However, most existing CKG
completion methods focus on the setting where all the entities are presented at
training time. Although this setting is standard for conventional KG
completion, it has limitations for CKG completion. At test time, entities in
CKGs can be unseen because they may have unseen text/names and entities may be
disconnected from the training graph, since CKGs are generally very sparse.
Here, we propose to study the inductive learning setting for CKG completion
where unseen entities may present at test time. We develop a novel learning
framework named InductivE. Different from previous approaches, InductiveE
ensures the inductive learning capability by directly computing entity
embeddings from raw entity attributes/text. InductiveE consists of a free-text
encoder, a graph encoder, and a KG completion decoder. Specifically, the
free-text encoder first extracts the textual representation of each entity
based on the pre-trained language model and word embedding. The graph encoder
is a gated relational graph convolutional neural network that learns from a
densified graph for more informative entity representation learning. We develop
a method that densifies CKGs by adding edges among semantic-related entities
and provide more supportive information for unseen entities, leading to better
generalization ability of entity embedding for unseen entities. Finally,
inductiveE employs Conv-TransE as the CKG completion decoder. Experimental
results show that InductiveE significantly outperforms state-of-the-art
baselines in both standard and inductive settings on ATOMIC and ConceptNet
benchmarks. InductivE performs especially well on inductive scenarios where it
achieves above 48% improvement over present methods.},
Year          = {2020},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2009.09263v2},
File          = {2009.09263v2.pdf}
}
@article{2401.07812v1,
Author        = {Kunpeng Guo and Dennis Diefenbach and Antoine Gourru and Christophe Gravier},
Title         = {Wikidata as a seed for Web Extraction},
Eprint        = {2401.07812v1},
DOI           = {10.1145/3543507},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Wikidata has grown to a knowledge graph with an impressive size. To date, it
contains more than 17 billion triples collecting information about people,
places, films, stars, publications, proteins, and many more. On the other side,
most of the information on the Web is not published in highly structured data
repositories like Wikidata, but rather as unstructured and semi-structured
content, more concretely in HTML pages containing text and tables. Finding,
monitoring, and organizing this data in a knowledge graph is requiring
considerable work from human editors. The volume and complexity of the data
make this task difficult and time-consuming. In this work, we present a
framework that is able to identify and extract new facts that are published
under multiple Web domains so that they can be proposed for validation by
Wikidata editors. The framework is relying on question-answering technologies.
We take inspiration from ideas that are used to extract facts from textual
collections and adapt them to extract facts from Web pages. For achieving this,
we demonstrate that language models can be adapted to extract facts not only
from textual collections but also from Web pages. By exploiting the information
already contained in Wikidata the proposed framework can be trained without the
need for any additional learning signals and can extract new facts for a wide
range of properties and domains. Following this path, Wikidata can be used as a
seed to extract facts on the Web. Our experiments show that we can achieve a
mean performance of 84.07 at F1-score. Moreover, our estimations show that we
can potentially extract millions of facts that can be proposed for human
validation. The goal is to help editors in their daily tasks and contribute to
the completion of the Wikidata knowledge graph.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.07812v1},
File          = {2401.07812v1.pdf}
}
@article{2308.03929v4,
Author        = {Ahmed Abdeen Hamed and Byung Suk Lee and Alessandro Crimi and Magdalena M. Misiak},
Title         = {Fact-Checking Generative AI: Ontology-Driven Biological Graphs for
  Disease-Gene Link Verification},
Eprint        = {2308.03929v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Since the launch of various generative AI tools, scientists have been
striving to evaluate their capabilities and contents, in the hope of
establishing trust in their generative abilities. Regulations and guidelines
are emerging to verify generated contents and identify novel uses. we aspire to
demonstrate how ChatGPT claims are checked computationally using the rigor of
network models. We aim to achieve fact-checking of the knowledge embedded in
biological graphs that were contrived from ChatGPT contents at the aggregate
level. We adopted a biological networks approach that enables the systematic
interrogation of ChatGPT's linked entities. We designed an ontology-driven
fact-checking algorithm that compares biological graphs constructed from
approximately 200,000 PubMed abstracts with counterparts constructed from a
dataset generated using the ChatGPT-3.5 Turbo model. In 10-samples of 250
randomly selected records a ChatGPT dataset of 1000 "simulated" articles , the
fact-checking link accuracy ranged from 70% to 86%. This study demonstrated
high accuracy of aggregate disease-gene links relationships found in
ChatGPT-generated texts.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.03929v4},
File          = {2308.03929v4.pdf}
}
@article{2401.01065v2,
Author        = {Tao Tang and Dafeng Wei and Zhengyu Jia and Tian Gao and Changwei Cai and Chengkai Hou and Peng Jia and Kun Zhan and Haiyang Sun and Jingchen Fan and Yixing Zhao and Fu Liu and Xiaodan Liang and Xianpeng Lang and Yang Wang},
Title         = {BEV-TSR: Text-Scene Retrieval in BEV Space for Autonomous Driving},
Eprint        = {2401.01065v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {The rapid development of the autonomous driving industry has led to a
significant accumulation of autonomous driving data. Consequently, there comes
a growing demand for retrieving data to provide specialized optimization.
However, directly applying previous image retrieval methods faces several
challenges, such as the lack of global feature representation and inadequate
text retrieval ability for complex driving scenes. To address these issues,
firstly, we propose the BEV-TSR framework which leverages descriptive text as
an input to retrieve corresponding scenes in the Bird's Eye View (BEV) space.
Then to facilitate complex scene retrieval with extensive text descriptions, we
employ a large language model (LLM) to extract the semantic features of the
text inputs and incorporate knowledge graph embeddings to enhance the semantic
richness of the language embedding. To achieve feature alignment between the
BEV feature and language embedding, we propose Shared Cross-modal Embedding
with a set of shared learnable embeddings to bridge the gap between these two
modalities, and employ a caption generation task to further enhance the
alignment. Furthermore, there lack of well-formed retrieval datasets for
effective evaluation. To this end, we establish a multi-level retrieval
dataset, nuScenes-Retrieval, based on the widely adopted nuScenes dataset.
Experimental results on the multi-level nuScenes-Retrieval show that BEV-TSR
achieves state-of-the-art performance, e.g., 85.78% and 87.66% top-1 accuracy
on scene-to-text and text-to-scene retrieval respectively. Codes and datasets
will be available.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.01065v2},
File          = {2401.01065v2.pdf}
}
@article{2302.12529v2,
Author        = {Yonghao Liu and Di Liang and Fang Fang and Sirui Wang and Wei Wu and Rui Jiang},
Title         = {Time-aware Multiway Adaptive Fusion Network for Temporal Knowledge Graph
  Question Answering},
Eprint        = {2302.12529v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs (KGs) have received increasing attention due to its wide
applications on natural language processing. However, its use case on temporal
question answering (QA) has not been well-explored. Most of existing methods
are developed based on pre-trained language models, which might not be capable
to learn \emph{temporal-specific} presentations of entities in terms of
temporal KGQA task. To alleviate this problem, we propose a novel
\textbf{T}ime-aware \textbf{M}ultiway \textbf{A}daptive (\textbf{TMA}) fusion
network. Inspired by the step-by-step reasoning behavior of humans. For each
given question, TMA first extracts the relevant concepts from the KG, and then
feeds them into a multiway adaptive module to produce a
\emph{temporal-specific} representation of the question. This representation
can be incorporated with the pre-trained KG embedding to generate the final
prediction. Empirical results verify that the proposed model achieves better
performance than the state-of-the-art models in the benchmark dataset. Notably,
the Hits@1 and Hits@10 results of TMA on the CronQuestions dataset's complex
questions are absolutely improved by 24\% and 10\% compared to the
best-performing baseline. Furthermore, we also show that TMA employing an
adaptive fusion mechanism can provide interpretability by analyzing the
proportion of information in question representations.},
Year          = {2023},
Month         = {Feb},
Note          = {ICASSP 2023},
Url           = {http://arxiv.org/abs/2302.12529v2},
File          = {2302.12529v2.pdf}
}
@article{2004.06153v2,
Author        = {Ming Jiang and Jennifer D'Souza and Sören Auer and J. Stephen Downie},
Title         = {Improving Scholarly Knowledge Representation: Evaluating BERT-based
  Models for Scientific Relation Classification},
Eprint        = {2004.06153v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {With the rapid growth of research publications, there is a vast amount of
scholarly knowledge that needs to be organized in digital libraries. To deal
with this challenge, techniques relying on knowledge-graph structures are being
advocated. Within such graph-based pipelines, inferring relation types between
related scientific concepts is a crucial step. Recently, advanced techniques
relying on language models pre-trained on the large corpus have been popularly
explored for automatic relation classification. Despite remarkable
contributions that have been made, many of these methods were evaluated under
different scenarios, which limits their comparability. To this end, we present
a thorough empirical evaluation on eight Bert-based classification models by
focusing on two key factors: 1) Bert model variants, and 2) classification
strategies. Experiments on three corpora show that domain-specific pre-training
corpus benefits the Bert-based classification model to identify the type of
scientific relations. Although the strategy of predicting a single relation
each time achieves a higher classification accuracy than the strategy of
identifying multiple relation types simultaneously in general, the latter
strategy demonstrates a more consistent performance in the corpus with either a
large or small size of annotations. Our study aims to offer recommendations to
the stakeholders of digital libraries for selecting the appropriate technique
to build knowledge-graph-based systems for enhanced scholarly information
organization.},
Year          = {2020},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2004.06153v2},
File          = {2004.06153v2.pdf}
}
@article{2008.09150v2,
Author        = {Houda Alberts and Teresa Huang and Yash Deshpande and Yibo Liu and Kyunghyun Cho and Clara Vania and Iacer Calixto},
Title         = {VisualSem: A High-quality Knowledge Graph for Vision and Language},
Eprint        = {2008.09150v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {An exciting frontier in natural language understanding (NLU) and generation
(NLG) calls for (vision-and-) language models that can efficiently access
external structured knowledge repositories. However, many existing knowledge
bases only cover limited domains, or suffer from noisy data, and most of all
are typically hard to integrate into neural language pipelines. To fill this
gap, we release VisualSem: a high-quality knowledge graph (KG) which includes
nodes with multilingual glosses, multiple illustrative images, and visually
relevant relations. We also release a neural multi-modal retrieval model that
can use images or sentences as inputs and retrieves entities in the KG. This
multi-modal retrieval model can be integrated into any (neural network) model
pipeline. We encourage the research community to use VisualSem for data
augmentation and/or as a source of grounding, among other possible uses.
VisualSem as well as the multi-modal retrieval models are publicly available
and can be downloaded in this URL: https://github.com/iacercalixto/visualsem},
Year          = {2020},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2008.09150v2},
File          = {2008.09150v2.pdf}
}
@article{2010.05953v2,
Author        = {Jena D. Hwang and Chandra Bhagavatula and Ronan Le Bras and Jeff Da and Keisuke Sakaguchi and Antoine Bosselut and Yejin Choi},
Title         = {COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs},
Eprint        = {2010.05953v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent years have brought about a renewed interest in commonsense
representation and reasoning in the field of natural language understanding.
The development of new commonsense knowledge graphs (CSKG) has been central to
these advances as their diverse facts can be used and referenced by machine
learning models for tackling new and challenging tasks. At the same time, there
remain questions about the quality and coverage of these resources due to the
massive scale required to comprehensively encompass general commonsense
knowledge.
  In this work, we posit that manually constructed CSKGs will never achieve the
coverage necessary to be applicable in all situations encountered by NLP
agents. Therefore, we propose a new evaluation framework for testing the
utility of KGs based on how effectively implicit knowledge representations can
be learned from them.
  With this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose
commonsense knowledge containing knowledge that is not readily available in
pretrained language models. We evaluate its properties in comparison with other
leading CSKGs, performing the first large-scale pairwise study of commonsense
knowledge resources. Next, we show that ATOMIC 2020 is better suited for
training knowledge models that can generate accurate, representative knowledge
for new, unseen entities and events. Finally, through human evaluation, we show
that the few-shot performance of GPT-3 (175B parameters), while impressive,
remains ~12 absolute points lower than a BART-based knowledge model trained on
ATOMIC 2020 despite using over 430x fewer parameters.},
Year          = {2020},
Month         = {Oct},
Note          = {Proceedings of the AAAI Conference on Artificial Intelligence
  (2021), 35(7), 6384-6392},
Url           = {http://arxiv.org/abs/2010.05953v2},
File          = {2010.05953v2.pdf}
}
@article{2102.04351v3,
Author        = {Priyanka Ranade and Aritran Piplai and Sudip Mittal and Anupam Joshi and Tim Finin},
Title         = {Generating Fake Cyber Threat Intelligence Using Transformer-Based Models},
Eprint        = {2102.04351v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CR},
Abstract      = {Cyber-defense systems are being developed to automatically ingest Cyber
Threat Intelligence (CTI) that contains semi-structured data and/or text to
populate knowledge graphs. A potential risk is that fake CTI can be generated
and spread through Open-Source Intelligence (OSINT) communities or on the Web
to effect a data poisoning attack on these systems. Adversaries can use fake
CTI examples as training input to subvert cyber defense systems, forcing the
model to learn incorrect inputs to serve their malicious needs.
  In this paper, we automatically generate fake CTI text descriptions using
transformers. We show that given an initial prompt sentence, a public language
model like GPT-2 with fine-tuning, can generate plausible CTI text with the
ability of corrupting cyber-defense systems. We utilize the generated fake CTI
text to perform a data poisoning attack on a Cybersecurity Knowledge Graph
(CKG) and a cybersecurity corpus. The poisoning attack introduced adverse
impacts such as returning incorrect reasoning outputs, representation
poisoning, and corruption of other dependent AI-based cyber defense systems. We
evaluate with traditional approaches and conduct a human evaluation study with
cybersecurity professionals and threat hunters. Based on the study,
professional threat hunters were equally likely to consider our fake generated
CTI as true.},
Year          = {2021},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2102.04351v3},
File          = {2102.04351v3.pdf}
}
@article{2104.01619v1,
Author        = {Shashank Shailabh and Sajal Chaurasia and Ashutosh Modi},
Title         = {KnowGraph@IITK at SemEval-2021 Task 11: Building KnowledgeGraph for NLP
  Research},
Eprint        = {2104.01619v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Research in Natural Language Processing is making rapid advances, resulting
in the publication of a large number of research papers. Finding relevant
research papers and their contribution to the domain is a challenging problem.
In this paper, we address this challenge via the SemEval 2021 Task 11:
NLPContributionGraph, by developing a system for a research paper
contributions-focused knowledge graph over Natural Language Processing
literature. The task is divided into three sub-tasks: extracting contribution
sentences that show important contributions in the research article, extracting
phrases from the contribution sentences, and predicting the information units
in the research article together with triplet formation from the phrases. The
proposed system is agnostic to the subject domain and can be applied for
building a knowledge graph for any area. We found that transformer-based
language models can significantly improve existing techniques and utilized the
SciBERT-based model. Our first sub-task uses Bidirectional LSTM (BiLSTM)
stacked on top of SciBERT model layers, while the second sub-task uses
Conditional Random Field (CRF) on top of SciBERT with BiLSTM. The third
sub-task uses a combined SciBERT based neural approach with heuristics for
information unit prediction and triplet formation from the phrases. Our system
achieved F1 score of 0.38, 0.63 and 0.76 in end-to-end pipeline testing, phrase
extraction testing and triplet extraction testing respectively.},
Year          = {2021},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2104.01619v1},
File          = {2104.01619v1.pdf}
}
@article{2104.08793v5,
Author        = {Aaron Chan and Jiashu Xu and Boyuan Long and Soumya Sanyal and Tanishq Gupta and Xiang Ren},
Title         = {SalKG: Learning From Knowledge Graph Explanations for Commonsense
  Reasoning},
Eprint        = {2104.08793v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Augmenting pre-trained language models with knowledge graphs (KGs) has
achieved success on various commonsense reasoning tasks. However, for a given
task instance, the KG, or certain parts of the KG, may not be useful. Although
KG-augmented models often use attention to focus on specific KG components, the
KG is still always used, and the attention mechanism is never explicitly taught
which KG components should be used. Meanwhile, saliency methods can measure how
much a KG feature (e.g., graph, node, path) influences the model to make the
correct prediction, thus explaining which KG features are useful. This paper
explores how saliency explanations can be used to improve KG-augmented models'
performance. First, we propose to create coarse (Is the KG useful?) and fine
(Which nodes/paths in the KG are useful?) saliency explanations. Second, to
motivate saliency-based supervision, we analyze oracle KG-augmented models
which directly use saliency explanations as extra inputs for guiding their
attention. Third, we propose SalKG, a framework for KG-augmented models to
learn from coarse and/or fine saliency explanations. Given saliency
explanations created from a task's training set, SalKG jointly trains the model
to predict the explanations, then solve the task by attending to KG features
highlighted by the predicted explanations. On three commonsense QA benchmarks
(CSQA, OBQA, CODAH) and a range of KG-augmented models, we show that SalKG can
yield considerable performance gains -- up to 2.76% absolute improvement on
CSQA.},
Year          = {2021},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2104.08793v5},
File          = {2104.08793v5.pdf}
}
@article{2206.09591v1,
Author        = {Tian Li and Xiang Chen and Zhen Dong and Weijiang Yu and Yijun Yan and Kurt Keutzer and Shanghang Zhang},
Title         = {Domain-Adaptive Text Classification with Structured Knowledge from
  Unlabeled Data},
Eprint        = {2206.09591v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Domain adaptive text classification is a challenging problem for the
large-scale pretrained language models because they often require expensive
additional labeled data to adapt to new domains. Existing works usually fails
to leverage the implicit relationships among words across domains. In this
paper, we propose a novel method, called Domain Adaptation with Structured
Knowledge (DASK), to enhance domain adaptation by exploiting word-level
semantic relationships. DASK first builds a knowledge graph to capture the
relationship between pivot terms (domain-independent words) and non-pivot terms
in the target domain. Then during training, DASK injects pivot-related
knowledge graph information into source domain texts. For the downstream task,
these knowledge-injected texts are fed into a BERT variant capable of
processing knowledge-injected textual data. Thanks to the knowledge injection,
our model learns domain-invariant features for non-pivots according to their
relationships with pivots. DASK ensures the pivots to have domain-invariant
behaviors by dynamically inferring via the polarity scores of candidate pivots
during training with pseudo-labels. We validate DASK on a wide range of
cross-domain sentiment classification tasks and observe up to 2.9% absolute
performance improvement over baselines for 20 different domain pairs. Code will
be made available at https://github.com/hikaru-nara/DASK.},
Year          = {2022},
Month         = {Jun},
Note          = {IJCAI-ECAI 2022},
Url           = {http://arxiv.org/abs/2206.09591v1},
File          = {2206.09591v1.pdf}
}
@article{2304.04717v1,
Author        = {Zhongwu Chen and Chengjin Xu and Fenglong Su and Zhen Huang and Yong Dou},
Title         = {Incorporating Structured Sentences with Time-enhanced BERT for
  Fully-inductive Temporal Relation Prediction},
Eprint        = {2304.04717v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Temporal relation prediction in incomplete temporal knowledge graphs (TKGs)
is a popular temporal knowledge graph completion (TKGC) problem in both
transductive and inductive settings. Traditional embedding-based TKGC models
(TKGE) rely on structured connections and can only handle a fixed set of
entities, i.e., the transductive setting. In the inductive setting where test
TKGs contain emerging entities, the latest methods are based on symbolic rules
or pre-trained language models (PLMs). However, they suffer from being
inflexible and not time-specific, respectively. In this work, we extend the
fully-inductive setting, where entities in the training and test sets are
totally disjoint, into TKGs and take a further step towards a more flexible and
time-sensitive temporal relation prediction approach SST-BERT, incorporating
Structured Sentences with Time-enhanced BERT. Our model can obtain the entity
history and implicitly learn rules in the semantic space by encoding structured
sentences, solving the problem of inflexibility. We propose to use a time
masking MLM task to pre-train BERT in a corpus rich in temporal tokens
specially generated for TKGs, enhancing the time sensitivity of SST-BERT. To
compute the probability of occurrence of a target quadruple, we aggregate all
its structured sentences from both temporal and semantic perspectives into a
score. Experiments on the transductive datasets and newly generated
fully-inductive benchmarks show that SST-BERT successfully improves over
state-of-the-art baselines.},
Year          = {2023},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2304.04717v1},
File          = {2304.04717v1.pdf}
}
@article{2305.12416v1,
Author        = {Jinheon Baek and Alham Fikri Aji and Jens Lehmann and Sung Ju Hwang},
Title         = {Direct Fact Retrieval from Knowledge Graphs without Entity Linking},
Eprint        = {2305.12416v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {There has been a surge of interest in utilizing Knowledge Graphs (KGs) for
various natural language processing/understanding tasks. The conventional
mechanism to retrieve facts in KGs usually involves three steps: entity span
detection, entity disambiguation, and relation classification. However, this
approach requires additional labels for training each of the three
subcomponents in addition to pairs of input texts and facts, and also may
accumulate errors propagated from failures in previous steps. To tackle these
limitations, we propose a simple knowledge retrieval framework, which directly
retrieves facts from the KGs given the input text based on their
representational similarities, which we refer to as Direct Fact Retrieval
(DiFaR). Specifically, we first embed all facts in KGs onto a dense embedding
space by using a language model trained by only pairs of input texts and facts,
and then provide the nearest facts in response to the input text. Since the
fact, consisting of only two entities and one relation, has little context to
encode, we propose to further refine ranks of top-k retrieved facts with a
reranker that contextualizes the input text and the fact jointly. We validate
our DiFaR framework on multiple fact retrieval tasks, showing that it
significantly outperforms relevant baselines that use the three-step approach.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.12416v1},
File          = {2305.12416v1.pdf}
}
@article{2306.02520v2,
Author        = {Jiarui Zhang and Filip Ilievski and Kaixin Ma and Aravinda Kollaa and Jonathan Francis and Alessandro Oltramari},
Title         = {A Study of Situational Reasoning for Traffic Understanding},
Eprint        = {2306.02520v2},
DOI           = {10.1145/3580305.3599246},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Intelligent Traffic Monitoring (ITMo) technologies hold the potential for
improving road safety/security and for enabling smart city infrastructure.
Understanding traffic situations requires a complex fusion of perceptual
information with domain-specific and causal commonsense knowledge. Whereas
prior work has provided benchmarks and methods for traffic monitoring, it
remains unclear whether models can effectively align these information sources
and reason in novel scenarios. To address this assessment gap, we devise three
novel text-based tasks for situational reasoning in the traffic domain: i)
BDD-QA, which evaluates the ability of Language Models (LMs) to perform
situational decision-making, ii) TV-QA, which assesses LMs' abilities to reason
about complex event causality, and iii) HDT-QA, which evaluates the ability of
models to solve human driving exams. We adopt four knowledge-enhanced methods
that have shown generalization capability across language reasoning tasks in
prior work, based on natural language inference, commonsense knowledge-graph
self-supervision, multi-QA joint training, and dense retrieval of domain
information. We associate each method with a relevant knowledge source,
including knowledge graphs, relevant benchmarks, and driving manuals. In
extensive experiments, we benchmark various knowledge-aware methods against the
three datasets, under zero-shot evaluation; we provide in-depth analyses of
model performance on data partitions and examine model predictions
categorically, to yield useful insights on traffic understanding, given
different background knowledge and reasoning strategies.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.02520v2},
File          = {2306.02520v2.pdf}
}
@article{2309.13054v1,
Author        = {Ramanathan V. Guha and Prashanth Radhakrishnan and Bo Xu and Wei Sun and Carolyn Au and Ajai Tirumali and Muhammad J. Amjad and Samantha Piekos and Natalie Diaz and Jennifer Chen and Julia Wu and Prem Ramaswami and James Manyika},
Title         = {Data Commons},
Eprint        = {2309.13054v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {Publicly available data from open sources (e.g., United States Census Bureau
(Census), World Health Organization (WHO), Intergovernmental Panel on Climate
Change (IPCC)) are vital resources for policy makers, students and researchers
across different disciplines. Combining data from different sources requires
the user to reconcile the differences in schemas, formats, assumptions, and
more. This data wrangling is time consuming, tedious and needs to be repeated
by every user of the data. Our goal with Data Commons (DC) is to help make
public data accessible and useful to those who want to understand this data and
use it to solve societal challenges and opportunities. We do the data
processing and make the processed data widely available via standard schemas
and Cloud APIs. Data Commons is a distributed network of sites that publish
data in a common schema and interoperate using the Data Commons APIs. Data from
different Data Commons can be joined easily. The aggregate of these Data
Commons can be viewed as a single Knowledge Graph. This Knowledge Graph can
then be searched over using Natural Language questions utilizing advances in
Large Language Models. This paper describes the architecture of Data Commons,
some of the major deployments and highlights directions for future work.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.13054v1},
File          = {2309.13054v1.pdf}
}
@article{2310.11088v1,
Author        = {Xin Su and Yao Zhou and Zifei Shan and Qian Chen},
Title         = {MeKB-Rec: Personal Knowledge Graph Learning for Cross-Domain
  Recommendation},
Eprint        = {2310.11088v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {It is a long-standing challenge in modern recommender systems to effectively
make recommendations for new users, namely the cold-start problem. Cross-Domain
Recommendation (CDR) has been proposed to address this challenge, but current
ways to represent users' interests across systems are still severely limited.
We introduce Personal Knowledge Graph (PKG) as a domain-invariant interest
representation, and propose a novel CDR paradigm named MeKB-Rec. We first link
users and entities in a knowledge base to construct a PKG of users' interests,
named MeKB. Then we learn a semantic representation of MeKB for the
cross-domain recommendation. To efficiently utilize limited training data in
CDR, MeKB-Rec employs Pretrained Language Models to inject world knowledge into
understanding users' interests. Beyond most existing systems, our approach
builds a semantic mapping across domains which breaks the requirement for
in-domain user behaviors, enabling zero-shot recommendations for new users in a
low-resource domain. We experiment MeKB-Rec on well-established public CDR
datasets, and demonstrate that the new formulation % is more powerful than
previous approaches, achieves a new state-of-the-art that significantly
improves HR@10 and NDCG@10 metrics over best previous approaches by 24\%--91\%,
with a 105\% improvement for HR@10 of zero-shot users with no behavior in the
target domain. We deploy MeKB-Rec in WeiXin recommendation scenarios and
achieve significant gains in core online metrics. MeKB-Rec is now serving
hundreds of millions of users in real-world products.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.11088v1},
File          = {2310.11088v1.pdf}
}
@article{2311.03783v2,
Author        = {Song Yaoxian and Sun Penglei and Liu Haoyu and Li Zhixu and Song Wei and Xiao Yanghua and Zhou Xiaofang},
Title         = {Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI},
Eprint        = {2311.03783v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Embodied AI is one of the most popular studies in artificial intelligence and
robotics, which can effectively improve the intelligence of real-world agents
(i.e. robots) serving human beings. Scene knowledge is important for an agent
to understand the surroundings and make correct decisions in the varied open
world. Currently, knowledge base for embodied tasks is missing and most
existing work use general knowledge base or pre-trained models to enhance the
intelligence of an agent. For conventional knowledge base, it is sparse,
insufficient in capacity and cost in data collection. For pre-trained models,
they face the uncertainty of knowledge and hard maintenance. To overcome the
challenges of scene knowledge, we propose a scene-driven multimodal knowledge
graph (Scene-MMKG) construction method combining conventional knowledge
engineering and large language models. A unified scene knowledge injection
framework is introduced for knowledge representation. To evaluate the
advantages of our proposed method, we instantiate Scene-MMKG considering
typical indoor robotic functionalities (Manipulation and Mobility), named
ManipMob-MMKG. Comparisons in characteristics indicate our instantiated
ManipMob-MMKG has broad superiority in data-collection efficiency and knowledge
quality. Experimental results on typical embodied tasks show that
knowledge-enhanced methods using our instantiated ManipMob-MMKG can improve the
performance obviously without re-designing model structures complexly. Our
project can be found at https://sites.google.com/view/manipmob-mmkg},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.03783v2},
File          = {2311.03783v2.pdf}
}
@article{2311.04250v1,
Author        = {Sang-Hyun Je and Wontae Choi and Kwangjin Oh},
Title         = {Unifying Structure and Language Semantic for Efficient Contrastive
  Knowledge Graph Completion with Structured Entity Anchors},
Eprint        = {2311.04250v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The goal of knowledge graph completion (KGC) is to predict missing links in a
KG using trained facts that are already known. In recent, pre-trained language
model (PLM) based methods that utilize both textual and structural information
are emerging, but their performances lag behind state-of-the-art (SOTA)
structure-based methods or some methods lose their inductive inference
capabilities in the process of fusing structure embedding to text encoder. In
this paper, we propose a novel method to effectively unify structure
information and language semantics without losing the power of inductive
reasoning. We adopt entity anchors and these anchors and textual description of
KG elements are fed together into the PLM-based encoder to learn unified
representations. In addition, the proposed method utilizes additional random
negative samples which can be reused in the each mini-batch during contrastive
learning to learn a generalized entity representations. We verify the
effectiveness of the our proposed method through various experiments and
analysis. The experimental results on standard benchmark widely used in link
prediction task show that the proposed model outperforms existing the SOTA KGC
models. Especially, our method show the largest performance improvement on
FB15K-237, which is competitive to the SOTA of structure-based KGC methods.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.04250v1},
File          = {2311.04250v1.pdf}
}
@article{2401.12997v2,
Author        = {Cunhang Fan and Yujie Chen and Jun Xue and Yonghui Kong and Jianhua Tao and Zhao Lv},
Title         = {Progressive Distillation Based on Masked Generation Feature Method for
  Knowledge Graph Completion},
Eprint        = {2401.12997v2},
DOI           = {10.1609/aaai.v38i8.28680},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In recent years, knowledge graph completion (KGC) models based on pre-trained
language model (PLM) have shown promising results. However, the large number of
parameters and high computational cost of PLM models pose challenges for their
application in downstream tasks. This paper proposes a progressive distillation
method based on masked generation features for KGC task, aiming to
significantly reduce the complexity of pre-trained models. Specifically, we
perform pre-distillation on PLM to obtain high-quality teacher models, and
compress the PLM network to obtain multi-grade student models. However,
traditional feature distillation suffers from the limitation of having a single
representation of information in teacher models. To solve this problem, we
propose masked generation of teacher-student features, which contain richer
representation information. Furthermore, there is a significant gap in
representation ability between teacher and student. Therefore, we design a
progressive distillation method to distill student models at each grade level,
enabling efficient knowledge transfer from teachers to students. The
experimental results demonstrate that the model in the pre-distillation stage
surpasses the existing state-of-the-art methods. Furthermore, in the
progressive distillation stage, the model significantly reduces the model
parameters while maintaining a certain level of performance. Specifically, the
model parameters of the lower-grade student model are reduced by 56.7\%
compared to the baseline.},
Year          = {2024},
Month         = {Jan},
Note          = {(2024) Vol. 38 No. 8: AAAI-24 Technical Tracks 8 Vol. 38 No. 8:
  AAAI-24 Technical Tracks 8 Vol. 38 No. 8: AAAI-24 Technical Tracks 8
  Proceedings of the AAAI Conference on Artificial Intelligence, 38(8),
  8380-8388},
Url           = {http://arxiv.org/abs/2401.12997v2},
File          = {2401.12997v2.pdf}
}
@article{2404.00051v1,
Author        = {Miao Peng and Ben Liu and Wenjie Xu and Zihao Jiang and Jiahui Zhu and Min Peng},
Title         = {Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal
  Knowledge Graph Reasoning},
Eprint        = {2404.00051v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing
facts for incomplete TKGs in complex scenarios (e.g., transductive and
inductive settings), which has been gaining increasing attention. Recently, to
mitigate dependence on structured connections in TKGs, text-based methods have
been developed to utilize rich linguistic information from entity descriptions.
However, suffering from the enormous parameters and inflexibility of
pre-trained language models, existing text-based methods struggle to balance
the textual knowledge and temporal information with computationally expensive
purpose-built training strategies. To tap the potential of text-based models
for TKGR in various complex scenarios, we propose ChapTER, a Contrastive
historical modeling framework with prefix-tuning for TEmporal Reasoning.
ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to
strike a textual-temporal balance via contrastive estimation between queries
and candidates. By introducing virtual time prefix tokens, it applies a
prefix-based tuning method to facilitate the frozen PLM capable for TKGR tasks
under different settings. We evaluate ChapTER on four transductive and three
few-shot inductive TKGR benchmarks, and experimental results demonstrate that
ChapTER achieves superior performance compared to competitive baselines with
only 0.17% tuned parameters. We conduct thorough analysis to verify the
effectiveness, flexibility and efficiency of ChapTER.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2404.00051v1},
File          = {2404.00051v1.pdf}
}
@article{2405.04136v1,
Author        = {Benjamin Wolff and Eva Seidlmayer and Konrad U. Förstner},
Title         = {Enriched BERT Embeddings for Scholarly Publication Classification},
Eprint        = {2405.04136v1},
DOI           = {10.1007/978-3-031-65794-8_16},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {With the rapid expansion of academic literature and the proliferation of
preprints, researchers face growing challenges in manually organizing and
labeling large volumes of articles. The NSLP 2024 FoRC Shared Task I addresses
this challenge organized as a competition. The goal is to develop a classifier
capable of predicting one of 123 predefined classes from the Open Research
Knowledge Graph (ORKG) taxonomy of research fields for a given article.This
paper presents our results. Initially, we enrich the dataset (containing
English scholarly articles sourced from ORKG and arXiv), then leverage
different pre-trained language Models (PLMs), specifically BERT, and explore
their efficacy in transfer learning for this downstream task. Our experiments
encompass feature-based and fine-tuned transfer learning approaches using
diverse PLMs, optimized for scientific tasks, including SciBERT, SciNCL, and
SPECTER2. We conduct hyperparameter tuning and investigate the impact of data
augmentation from bibliographic databases such as OpenAlex, Semantic Scholar,
and Crossref. Our results demonstrate that fine-tuning pre-trained models
substantially enhances classification performance, with SPECTER2 emerging as
the most accurate model. Moreover, enriching the dataset with additional
metadata improves classification outcomes significantly, especially when
integrating information from S2AG, OpenAlex and Crossref. Our best-performing
approach achieves a weighted F1-score of 0.7415. Overall, our study contributes
to the advancement of reliable automated systems for scholarly publication
categorization, offering a potential solution to the laborious manual curation
process, thereby facilitating researchers in efficiently locating relevant
resources.},
Year          = {2024},
Month         = {May},
Note          = {Natural Scientific Language Processing and Research Knowledge
  Graphs (2024), LNAI 14770, 234-243},
Url           = {http://arxiv.org/abs/2405.04136v1},
File          = {2405.04136v1.pdf}
}
@article{2408.02535v1,
Author        = {Zhao Kaichen and Song Yaoxian and Zhao Haiquan and Liu Haoyu and Li Tiefeng and Li Zhixu},
Title         = {Towards Coarse-grained Visual Language Navigation Task Planning Enhanced
  by Event Knowledge Graph},
Eprint        = {2408.02535v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Visual language navigation (VLN) is one of the important research in embodied
AI. It aims to enable an agent to understand the surrounding environment and
complete navigation tasks. VLN instructions could be categorized into
coarse-grained and fine-grained commands. Fine-grained command describes a
whole task with subtasks step-by-step. In contrast, coarse-grained command
gives an abstract task description, which more suites human habits. Most
existing work focuses on the former kind of instruction in VLN tasks, ignoring
the latter abstract instructions belonging to daily life scenarios. To overcome
the above challenge in abstract instruction, we attempt to consider
coarse-grained instruction in VLN by event knowledge enhancement. Specifically,
we first propose a prompt-based framework to extract an event knowledge graph
(named VLN-EventKG) for VLN integrally over multiple mainstream benchmark
datasets. Through small and large language model collaboration, we realize
knowledge-enhanced navigation planning (named EventNav) for VLN tasks with
coarse-grained instruction input. Additionally, we design a novel dynamic
history backtracking module to correct potential error action planning in real
time. Experimental results in various public benchmarks show our
knowledge-enhanced method has superiority in coarse-grained-instruction VLN
using our proposed VLN-EventKG with over $5\%$ improvement in success rate. Our
project is available at https://sites.google.com/view/vln-eventkg},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.02535v1},
File          = {2408.02535v1.pdf}
}
@article{2410.16803v3,
Author        = {Muzhi Li and Cehao Yang and Chengjin Xu and Zixing Song and Xuhui Jiang and Jian Guo and Ho-fung Leung and Irwin King},
Title         = {Context-aware Inductive Knowledge Graph Completion with Latent Type
  Constraints and Subgraph Reasoning},
Eprint        = {2410.16803v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Inductive knowledge graph completion (KGC) aims to predict missing triples
with unseen entities. Recent works focus on modeling reasoning paths between
the head and tail entity as direct supporting evidence. However, these methods
depend heavily on the existence and quality of reasoning paths, which limits
their general applicability in different scenarios. In addition, we observe
that latent type constraints and neighboring facts inherent in KGs are also
vital in inferring missing triples. To effectively utilize all useful
information in KGs, we introduce CATS, a novel context-aware inductive KGC
solution. With sufficient guidance from proper prompts and supervised
fine-tuning, CATS activates the strong semantic understanding and reasoning
capabilities of large language models to assess the existence of query triples,
which consist of two modules. First, the type-aware reasoning module evaluates
whether the candidate entity matches the latent entity type as required by the
query relation. Then, the subgraph reasoning module selects relevant reasoning
paths and neighboring facts, and evaluates their correlation to the query
triple. Experiment results on three widely used datasets demonstrate that CATS
significantly outperforms state-of-the-art methods in 16 out of 18
transductive, inductive, and few-shot settings with an average absolute MRR
improvement of 7.2%.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.16803v3},
File          = {2410.16803v3.pdf}
}
@article{2410.20321v1,
Author        = {Xingrui Zhuo and Jiapu Wang and Gongqing Wu and Shirui Pan and Xindong Wu},
Title         = {Effective Instruction Parsing Plugin for Complex Logical Query Answering
  on Knowledge Graphs},
Eprint        = {2410.20321v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Knowledge Graph Query Embedding (KGQE) aims to embed First-Order Logic (FOL)
queries in a low-dimensional KG space for complex reasoning over incomplete
KGs. To enhance the generalization of KGQE models, recent studies integrate
various external information (such as entity types and relation context) to
better capture the logical semantics of FOL queries. The whole process is
commonly referred to as Query Pattern Learning (QPL). However, current QPL
methods typically suffer from the pattern-entity alignment bias problem,
leading to the learned defective query patterns limiting KGQE models'
performance. To address this problem, we propose an effective Query Instruction
Parsing Plugin (QIPP) that leverages the context awareness of Pre-trained
Language Models (PLMs) to capture latent query patterns from code-like query
instructions. Unlike the external information introduced by previous QPL
methods, we first propose code-like instructions to express FOL queries in an
alternative format. This format utilizes textual variables and nested tuples to
convey the logical semantics within FOL queries, serving as raw materials for a
PLM-based instruction encoder to obtain complete query patterns. Building on
this, we design a query-guided instruction decoder to adapt query patterns to
KGQE models. To further enhance QIPP's effectiveness across various KGQE
models, we propose a query pattern injection mechanism based on compressed
optimization boundaries and an adaptive normalization component, allowing KGQE
models to utilize query patterns more efficiently. Extensive experiments
demonstrate that our plug-and-play method improves the performance of eight
basic KGQE models and outperforms two state-of-the-art QPL methods.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.20321v1},
File          = {2410.20321v1.pdf}
}
@article{2501.06873v1,
Author        = {Prashant Garg and Thiemo Fetzer},
Title         = {Causal Claims in Economics},
Eprint        = {2501.06873v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {econ.GN},
Abstract      = {We analyze over 44,000 NBER and CEPR working papers from 1980 to 2023 using a
custom language model to construct knowledge graphs that map economic concepts
and their relationships. We distinguish between general claims and those
documented via causal inference methods (e.g., DiD, IV, RDD, RCTs). We document
a substantial rise in the share of causal claims-from roughly 4% in 1990 to
nearly 28% in 2020-reflecting the growing influence of the "credibility
revolution." We find that causal narrative complexity (e.g., the depth of
causal chains) strongly predicts both publication in top-5 journals and higher
citation counts, whereas non-causal complexity tends to be uncorrelated or
negatively associated with these outcomes. Novelty is also pivotal for top-5
publication, but only when grounded in credible causal methods: introducing
genuinely new causal edges or paths markedly increases both the likelihood of
acceptance at leading outlets and long-run citations, while non-causal novelty
exhibits weak or even negative effects. Papers engaging with central, widely
recognized concepts tend to attract more citations, highlighting a divergence
between factors driving publication success and long-term academic impact.
Finally, bridging underexplored concept pairs is rewarded primarily when
grounded in causal methods, yet such gap filling exhibits no consistent link
with future citations. Overall, our findings suggest that methodological rigor
and causal innovation are key drivers of academic recognition, but sustained
impact may require balancing novel contributions with conceptual integration
into established economic discourse.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.06873v1},
File          = {2501.06873v1.pdf}
}
@article{2502.04413v1,
Author        = {Xuejiao Zhao and Siyan Liu and Su-Yin Yang and Chunyan Miao},
Title         = {MedRAG: Enhancing Retrieval-augmented Generation with Knowledge
  Graph-Elicited Reasoning for Healthcare Copilot},
Eprint        = {2502.04413v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Retrieval-augmented generation (RAG) is a well-suited technique for
retrieving privacy-sensitive Electronic Health Records (EHR). It can serve as a
key module of the healthcare copilot, helping reduce misdiagnosis for
healthcare practitioners and patients. However, the diagnostic accuracy and
specificity of existing heuristic-based RAG models used in the medical domain
are inadequate, particularly for diseases with similar manifestations. This
paper proposes MedRAG, a RAG model enhanced by knowledge graph (KG)-elicited
reasoning for the medical domain that retrieves diagnosis and treatment
recommendations based on manifestations. MedRAG systematically constructs a
comprehensive four-tier hierarchical diagnostic KG encompassing critical
diagnostic differences of various diseases. These differences are dynamically
integrated with similar EHRs retrieved from an EHR database, and reasoned
within a large language model. This process enables more accurate and specific
decision support, while also proactively providing follow-up questions to
enhance personalized medical decision-making. MedRAG is evaluated on both a
public dataset DDXPlus and a private chronic pain diagnostic dataset (CPDD)
collected from Tan Tock Seng Hospital, and its performance is compared against
various existing RAG methods. Experimental results show that, leveraging the
information integration and relational abilities of the KG, our MedRAG provides
more specific diagnostic insights and outperforms state-of-the-art models in
reducing misdiagnosis rates. Our code will be available at
https://github.com/SNOWTEAM2023/MedRAG},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.04413v1},
File          = {2502.04413v1.pdf}
}
@article{2210.07370v1,
Author        = {Machel Reid and Victor Zhong and Suchin Gururangan and Luke Zettlemoyer},
Title         = {M2D2: A Massively Multi-domain Language Modeling Dataset},
Eprint        = {2210.07370v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present M2D2, a fine-grained, massively multi-domain corpus for studying
domain adaptation in language models (LMs). M2D2 consists of 8.5B tokens and
spans 145 domains extracted from Wikipedia and Semantic Scholar. Using
ontologies derived from Wikipedia and ArXiv categories, we organize the domains
in each data source into 22 groups. This two-level hierarchy enables the study
of relationships between domains and their effects on in- and out-of-domain
performance after adaptation. We also present a number of insights into the
nature of effective domain adaptation in LMs, as examples of the new types of
studies M2D2 enables. To improve in-domain performance, we show the benefits of
adapting the LM along a domain hierarchy; adapting to smaller amounts of
fine-grained domain-specific data can lead to larger in-domain performance
gains than larger amounts of weakly relevant data. We further demonstrate a
trade-off between in-domain specialization and out-of-domain generalization
within and across ontologies, as well as a strong correlation between
out-of-domain performance and lexical overlap between domains.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.07370v1},
File          = {2210.07370v1.pdf}
}
@article{2305.06801v1,
Author        = {François Remy and Alfiya Khabibullina and Thomas Demeester},
Title         = {Detecting Idiomatic Multiword Expressions in Clinical Terminology using
  Definition-Based Representation Learning},
Eprint        = {2305.06801v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper shines a light on the potential of definition-based semantic
models for detecting idiomatic and semi-idiomatic multiword expressions (MWEs)
in clinical terminology. Our study focuses on biomedical entities defined in
the UMLS ontology and aims to help prioritize the translation efforts of these
entities. In particular, we develop an effective tool for scoring the
idiomaticity of biomedical MWEs based on the degree of similarity between the
semantic representations of those MWEs and a weighted average of the
representation of their constituents. We achieve this using a biomedical
language model trained to produce similar representations for entity names and
their definitions, called BioLORD. The importance of this definition-based
approach is highlighted by comparing the BioLORD model to two other
state-of-the-art biomedical language models based on Transformer: SapBERT and
CODER. Our results show that the BioLORD model has a strong ability to identify
idiomatic MWEs, not replicated in other models. Our corpus-free idiomaticity
estimation helps ontology translators to focus on more challenging MWEs.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.06801v1},
File          = {2305.06801v1.pdf}
}
@article{2412.03176v1,
Author        = {Leon-Paul Schaub Torre and Pelayo Quiros and Helena Garcia Mieres},
Title         = {Automatic detection of diseases in Spanish clinical notes combining
  medical language models and ontologies},
Eprint        = {2412.03176v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this paper we present a hybrid method for the automatic detection of
dermatological pathologies in medical reports. We use a large language model
combined with medical ontologies to predict, given a first appointment or
follow-up medical report, the pathology a person may suffer from. The results
show that teaching the model to learn the type, severity and location on the
body of a dermatological pathology, as well as in which order it has to learn
these three features, significantly increases its accuracy. The article
presents the demonstration of state-of-the-art results for classification of
medical texts with a precision of 0.84, micro and macro F1-score of 0.82 and
0.75, and makes both the method and the data set used available to the
community.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.03176v1},
File          = {2412.03176v1.pdf}
}
@article{2101.12294v2,
Author        = {Pedro Colon-Hernandez and Catherine Havasi and Jason Alonso and Matthew Huggins and Cynthia Breazeal},
Title         = {Combining pre-trained language models and structured knowledge},
Eprint        = {2101.12294v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In recent years, transformer-based language models have achieved state of the
art performance in various NLP benchmarks. These models are able to extract
mostly distributional information with some semantics from unstructured text,
however it has proven challenging to integrate structured information, such as
knowledge graphs into these models. We examine a variety of approaches to
integrate structured knowledge into current language models and determine
challenges, and possible opportunities to leverage both structured and
unstructured information sources. From our survey, we find that there are still
opportunities at exploiting adapter-based injections and that it may be
possible to further combine various of the explored approaches into one system.},
Year          = {2021},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2101.12294v2},
File          = {2101.12294v2.pdf}
}
@article{2111.10962v4,
Author        = {Linlin Liu and Xin Li and Ruidan He and Lidong Bing and Shafiq Joty and Luo Si},
Title         = {Enhancing Multilingual Language Model with Massive Multilingual
  Knowledge Triples},
Eprint        = {2111.10962v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge-enhanced language representation learning has shown promising
results across various knowledge-intensive NLP tasks. However, prior methods
are limited in efficient utilization of multilingual knowledge graph (KG) data
for language model (LM) pretraining. They often train LMs with KGs in indirect
ways, relying on extra entity/relation embeddings to facilitate knowledge
injection. In this work, we explore methods to make better use of the
multilingual annotation and language agnostic property of KG triples, and
present novel knowledge based multilingual language models (KMLMs) trained
directly on the knowledge triples. We first generate a large amount of
multilingual synthetic sentences using the Wikidata KG triples. Then based on
the intra- and inter-sentence structures of the generated data, we design
pretraining tasks to enable the LMs to not only memorize the factual knowledge
but also learn useful logical patterns. Our pretrained KMLMs demonstrate
significant performance improvements on a wide range of knowledge-intensive
cross-lingual tasks, including named entity recognition (NER), factual
knowledge retrieval, relation classification, and a newly designed logical
reasoning task.},
Year          = {2021},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2111.10962v4},
File          = {2111.10962v4.pdf}
}
@article{2206.12617v3,
Author        = {Xintao Wang and Qianyu He and Jiaqing Liang and Yanghua Xiao},
Title         = {Language Models as Knowledge Embeddings},
Eprint        = {2206.12617v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding
entities and relations into continuous vector spaces. Existing methods are
mainly structure-based or description-based. Structure-based methods learn
representations that preserve the inherent structure of KGs. They cannot well
represent abundant long-tail entities in real-world KGs with limited structural
information. Description-based methods leverage textual information and
language models. Prior approaches in this direction barely outperform
structure-based ones, and suffer from problems like expensive negative sampling
and restrictive description demand. In this paper, we propose LMKE, which
adopts Language Models to derive Knowledge Embeddings, aiming at both enriching
representations of long-tail entities and solving problems of prior
description-based methods. We formulate description-based KE learning with a
contrastive learning framework to improve efficiency in training and
evaluation. Experimental results show that LMKE achieves state-of-the-art
performance on KE benchmarks of link prediction and triple classification,
especially for long-tail entities.},
Year          = {2022},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2206.12617v3},
File          = {2206.12617v3.pdf}
}
@article{2212.07617v1,
Author        = {Mingyu Lee and Jun-Hyung Park and Junho Kim and Kang-Min Kim and SangKeun Lee},
Title         = {Efficient Pre-training of Masked Language Model via Concept-based
  Curriculum Masking},
Eprint        = {2212.07617v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Masked language modeling (MLM) has been widely used for pre-training
effective bidirectional representations, but incurs substantial training costs.
In this paper, we propose a novel concept-based curriculum masking (CCM) method
to efficiently pre-train a language model. CCM has two key differences from
existing curriculum learning approaches to effectively reflect the nature of
MLM. First, we introduce a carefully-designed linguistic difficulty criterion
that evaluates the MLM difficulty of each token. Second, we construct a
curriculum that gradually masks words related to the previously masked words by
retrieving a knowledge graph. Experimental results show that CCM significantly
improves pre-training efficiency. Specifically, the model trained with CCM
shows comparative performance with the original BERT on the General Language
Understanding Evaluation benchmark at half of the training cost.},
Year          = {2022},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2212.07617v1},
File          = {2212.07617v1.pdf}
}
@article{2311.08505v2,
Author        = {Xin Su and Tiep Le and Steven Bethard and Phillip Howard},
Title         = {Semi-Structured Chain-of-Thought: Integrating Multiple Sources of
  Knowledge for Improved Language Model Reasoning},
Eprint        = {2311.08505v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {An important open question in the use of large language models for
knowledge-intensive tasks is how to effectively integrate knowledge from three
sources: the model's parametric memory, external structured knowledge, and
external unstructured knowledge. Most existing prompting methods either rely on
one or two of these sources, or require repeatedly invoking large language
models to generate similar or identical content. In this work, we overcome
these limitations by introducing a novel semi-structured prompting approach
that seamlessly integrates the model's parametric memory with unstructured
knowledge from text documents and structured knowledge from knowledge graphs.
Experimental results on open-domain multi-hop question answering datasets
demonstrate that our prompting method significantly surpasses existing
techniques, even exceeding those that require fine-tuning.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.08505v2},
File          = {2311.08505v2.pdf}
}
@article{2401.07105v3,
Author        = {Moritz Plenz and Anette Frank},
Title         = {Graph Language Models},
Eprint        = {2401.07105v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {While Language Models (LMs) are the workhorses of NLP, their interplay with
structured knowledge graphs (KGs) is still actively researched. Current methods
for encoding such graphs typically either (i) linearize them for embedding with
LMs -- which underutilize structural information, or (ii) use Graph Neural
Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent
text features as well as pretrained LMs. In our work we introduce a novel LM
type, the Graph Language Model (GLM), that integrates the strengths of both
approaches and mitigates their weaknesses. The GLM parameters are initialized
from a pretrained LM to enhance understanding of individual graph concepts and
triplets. Simultaneously, we design the GLM's architecture to incorporate graph
biases, thereby promoting effective knowledge distribution within the graph.
This enables GLMs to process graphs, texts, and interleaved inputs of both.
Empirical evaluations on relation classification tasks show that GLM embeddings
surpass both LM- and GNN-based baselines in supervised and zero-shot setting,
demonstrating their versatility.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.07105v3},
File          = {2401.07105v3.pdf}
}
@article{2406.10902v1,
Author        = {Yikai Zhang and Qianyu He and Xintao Wang and Siyu Yuan and Jiaqing Liang and Yanghua Xiao},
Title         = {Light Up the Shadows: Enhance Long-Tailed Entity Grounding with
  Concept-Guided Vision-Language Models},
Eprint        = {2406.10902v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Multi-Modal Knowledge Graphs (MMKGs) have proven valuable for various
downstream tasks. However, scaling them up is challenging because building
large-scale MMKGs often introduces mismatched images (i.e., noise). Most
entities in KGs belong to the long tail, meaning there are few images of them
available online. This scarcity makes it difficult to determine whether a found
image matches the entity. To address this, we draw on the Triangle of Reference
Theory and suggest enhancing vision-language models with concept guidance.
Specifically, we introduce COG, a two-stage framework with COncept-Guided
vision-language models. The framework comprises a Concept Integration module,
which effectively identifies image-text pairs of long-tailed entities, and an
Evidence Fusion module, which offers explainability and enables human
verification. To demonstrate the effectiveness of COG, we create a dataset of
25k image-text pairs of long-tailed entities. Our comprehensive experiments
show that COG not only improves the accuracy of recognizing long-tailed
image-text pairs compared to baselines but also offers flexibility and
explainability.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.10902v1},
File          = {2406.10902v1.pdf}
}
@article{2411.16403v1,
Author        = {Alexander Fichtl and Juraj Vladika and Georg Groh},
Title         = {Adapter-based Approaches to Knowledge-enhanced Language Models -- A
  Survey},
Eprint        = {2411.16403v1},
DOI           = {10.5220/0013058500003838},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge-enhanced language models (KELMs) have emerged as promising tools to
bridge the gap between large-scale language models and domain-specific
knowledge. KELMs can achieve higher factual accuracy and mitigate
hallucinations by leveraging knowledge graphs (KGs). They are frequently
combined with adapter modules to reduce the computational load and risk of
catastrophic forgetting. In this paper, we conduct a systematic literature
review (SLR) on adapter-based approaches to KELMs. We provide a structured
overview of existing methodologies in the field through quantitative and
qualitative analysis and explore the strengths and potential shortcomings of
individual approaches. We show that general knowledge and domain-specific
approaches have been frequently explored along with various adapter
architectures and downstream tasks. We particularly focused on the popular
biomedical domain, where we provided an insightful performance comparison of
existing KELMs. We outline the main trends and propose promising future
directions.},
Year          = {2024},
Month         = {Nov},
Note          = {In Proceedings of the 16th International Joint Conference on
  Knowledge Discovery, Knowledge Engineering and Knowledge Management - KEOD
  2024; ISBN 978-989-758-716-0; ISSN 2184-3228, SciTePress, pages 95-107},
Url           = {http://arxiv.org/abs/2411.16403v1},
File          = {2411.16403v1.pdf}
}
@article{2404.03528v3,
Author        = {Azmine Toushik Wasi and Taki Hasan Rafi and Raima Islam and Dong-Kyu Chae},
Title         = {BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with
  Semantic Neural Graph Filtering},
Eprint        = {2404.03528v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graphs (KGs) have proven essential in information processing and
reasoning applications because they link related entities and give context-rich
information, supporting efficient information retrieval and knowledge
discovery; presenting information flow in a very effective manner. Despite
being widely used globally, Bangla is relatively underrepresented in KGs due to
a lack of comprehensive datasets, encoders, NER (named entity recognition)
models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient
information processing and reasoning applications in the language. Addressing
the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework
that is able to automatically construct Bengali KGs from any Bangla text. We
utilize multilingual LLMs to understand various languages and correlate
entities and relations universally. By employing a translation dictionary to
identify English equivalents and extracting word features from pre-trained BERT
models, we construct the foundational KG. To reduce noise and align word
embeddings with our goal, we employ graph-based polynomial filters. Lastly, we
implement a GNN-based semantic filter, which elevates contextual understanding
and trims unnecessary edges, culminating in the formation of the definitive KG.
Empirical findings and case studies demonstrate the universal effectiveness of
our model, capable of autonomously constructing semantically enriched KGs from
any text.},
Year          = {2024},
Month         = {Apr},
Note          = {The 2024 Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
Url           = {http://arxiv.org/abs/2404.03528v3},
File          = {2404.03528v3.pdf}
}
@article{2406.16333v1,
Author        = {Yichen Sun and Zhixuan Chu and Zhan Qin and Kui Ren},
Title         = {Prompt-Consistency Image Generation (PCIG): A Unified Framework
  Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models},
Eprint        = {2406.16333v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {The rapid advancement of Text-to-Image(T2I) generative models has enabled the
synthesis of high-quality images guided by textual descriptions. Despite this
significant progress, these models are often susceptible in generating contents
that contradict the input text, which poses a challenge to their reliability
and practical deployment. To address this problem, we introduce a novel
diffusion-based framework to significantly enhance the alignment of generated
images with their corresponding descriptions, addressing the inconsistency
between visual output and textual input. Our framework is built upon a
comprehensive analysis of inconsistency phenomena, categorizing them based on
their manifestation in the image. Leveraging a state-of-the-art large language
module, we first extract objects and construct a knowledge graph to predict the
locations of these objects in potentially generated images. We then integrate a
state-of-the-art controllable image generation model with a visual text
generation module to generate an image that is consistent with the original
prompt, guided by the predicted object locations. Through extensive experiments
on an advanced multimodal hallucination benchmark, we demonstrate the efficacy
of our approach in accurately generating the images without the inconsistency
with the original prompt. The code can be accessed via
https://github.com/TruthAI-Lab/PCIG.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.16333v1},
File          = {2406.16333v1.pdf}
}
@article{2412.09644v1,
Author        = {Marcos Da Silveira and Louis Deladiennee and Kheira Acem and Oona Freudenthal},
Title         = {Combining knowledge graphs and LLMs for hazardous chemical information
  management and reuse},
Eprint        = {2412.09644v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Human health is increasingly threatened by exposure to hazardous substances,
particularly persistent and toxic chemicals. The link between these substances,
often encountered in complex mixtures, and various diseases are demonstrated in
scientific studies. However, this information is scattered across several
sources and hardly accessible by humans and machines. This paper evaluates
current practices for publishing/accessing information on hazardous chemicals
and proposes a novel platform designed to facilitate retrieval of critical
chemical data in urgent situations. The platform aggregates information from
multiple sources and organizes it into a structured knowledge graph. Users can
access this information through a visual interface such as Neo4J Bloom and
dashboards, or via natural language queries using a Chatbot. Our findings
demonstrate a significant reduction in the time and effort required to access
vital chemical information when datasets follow FAIR principles. Furthermore,
we discuss the lessons learned from the development and implementation of this
platform and provide recommendations for data owners and publishers to enhance
data reuse and interoperability. This work aims to improve the accessibility
and usability of chemical information by healthcare professionals, thereby
supporting better health outcomes and informed decision-making in the face of
patients exposed to chemical intoxication risks.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.09644v1},
File          = {2412.09644v1.pdf}
}
@article{2303.13466v2,
Author        = {Sonish Sivarajkumar and Fengyi Gao and Parker E. Denny and Bayan M. Aldhahwani and Shyam Visweswaran and Allyn Bove and Yanshan Wang},
Title         = {Mining Clinical Notes for Physical Rehabilitation Exercise Information:
  Natural Language Processing Algorithm Development and Validation Study},
Eprint        = {2303.13466v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Post-stroke patient rehabilitation requires precise, personalized treatment
plans. Natural Language Processing (NLP) offers potential to extract valuable
exercise information from clinical notes, aiding in the development of more
effective rehabilitation strategies. Objective: This study aims to develop and
evaluate a variety of NLP algorithms to extract and categorize physical
rehabilitation exercise information from the clinical notes of post-stroke
patients treated at the University of Pittsburgh Medical Center. A cohort of
13,605 patients diagnosed with stroke was identified, and their clinical notes
containing rehabilitation therapy notes were retrieved. A comprehensive
clinical ontology was created to represent various aspects of physical
rehabilitation exercises. State-of-the-art NLP algorithms were then developed
and compared, including rule-based, machine learning-based algorithms, and
large language model (LLM)-based algorithms (ChatGPT). Analysis was conducted
on a dataset comprising 23,724 notes with detailed demographic and clinical
characteristics. The rule-based NLP algorithm demonstrated superior performance
in most areas, particularly in detecting the 'Right Side' location with an F1
score of 0.975, outperforming Gradient Boosting by 0.063. Gradient Boosting
excelled in 'Lower Extremity' location detection (F1 score: 0.978), surpassing
rule-based NLP by 0.023. It also showed notable performance in 'Passive Range
of Motion' with an F1 score of 0.970, a 0.032 improvement over rule-based NLP.
The rule-based algorithm efficiently handled 'Duration', 'Sets', and 'Reps'
with F1 scores up to 0.65. LLM-based NLP, particularly ChatGPT with few-shot
prompts, achieved high recall but generally lower precision and F1 scores.
However, it notably excelled in 'Backward Plane' motion detection, achieving an
F1 score of 0.846, surpassing the rule-based algorithm's 0.720.},
Year          = {2023},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2303.13466v2},
File          = {2303.13466v2.pdf}
}
@article{2004.13952v2,
Author        = {Baolin Peng and Chenguang Zhu and Michael Zeng and Jianfeng Gao},
Title         = {Data Augmentation for Spoken Language Understanding via Pretrained
  Language Models},
Eprint        = {2004.13952v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The training of spoken language understanding (SLU) models often faces the
problem of data scarcity. In this paper, we put forward a data augmentation
method using pretrained language models to boost the variability and accuracy
of generated utterances. Furthermore, we investigate and propose solutions to
two previously overlooked semi-supervised learning scenarios of data scarcity
in SLU: i) Rich-in-Ontology: ontology information with numerous valid dialogue
acts is given; ii) Rich-in-Utterance: a large number of unlabelled utterances
are available. Empirical results show that our method can produce synthetic
training data that boosts the performance of language understanding models in
various scenarios.},
Year          = {2020},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2004.13952v2},
File          = {2004.13952v2.pdf}
}
@article{2403.05920v1,
Author        = {Syed I. Munzir and Daniel B. Hier and Michael D. Carrithers},
Title         = {High Throughput Phenotyping of Physician Notes with Large Language and
  Hybrid NLP Models},
Eprint        = {2403.05920v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Deep phenotyping is the detailed description of patient signs and symptoms
using concepts from an ontology. The deep phenotyping of the numerous physician
notes in electronic health records requires high throughput methods. Over the
past thirty years, progress toward making high throughput phenotyping feasible.
In this study, we demonstrate that a large language model and a hybrid NLP
model (combining word vectors with a machine learning classifier) can perform
high throughput phenotyping on physician notes with high accuracy. Large
language models will likely emerge as the preferred method for high throughput
deep phenotyping of physician notes.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.05920v1},
File          = {2403.05920v1.pdf}
}
@article{2211.10678v1,
Author        = {Ting Su and Craig Macdonald and Iadh Ounis},
Title         = {Entity-Assisted Language Models for Identifying Check-worthy Sentences},
Eprint        = {2211.10678v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We propose a new uniform framework for text classification and ranking that
can automate the process of identifying check-worthy sentences in political
debates and speech transcripts. Our framework combines the semantic analysis of
the sentences, with additional entity embeddings obtained through the
identified entities within the sentences. In particular, we analyse the
semantic meaning of each sentence using state-of-the-art neural language models
such as BERT, ALBERT, and RoBERTa, while embeddings for entities are obtained
from knowledge graph (KG) embedding models. Specifically, we instantiate our
framework using five different language models, entity embeddings obtained from
six different KG embedding models, as well as two combination methods leading
to several Entity-Assisted neural language models. We extensively evaluate the
effectiveness of our framework using two publicly available datasets from the
CLEF' 2019 & 2020 CheckThat! Labs. Our results show that the neural language
models significantly outperform traditional TF.IDF and LSTM methods. In
addition, we show that the ALBERT model is consistently the most effective
model among all the tested neural language models. Our entity embeddings
significantly outperform other existing approaches from the literature that are
based on similarity and relatedness scores between the entities in a sentence,
when used alongside a KG embedding.},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.10678v1},
File          = {2211.10678v1.pdf}
}
@article{2007.08426v3,
Author        = {Leonardo F. R. Ribeiro and Martin Schmitt and Hinrich Schütze and Iryna Gurevych},
Title         = {Investigating Pretrained Language Models for Graph-to-Text Generation},
Eprint        = {2007.08426v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Graph-to-text generation aims to generate fluent texts from graph-based data.
In this paper, we investigate two recently proposed pretrained language models
(PLMs) and analyze the impact of different task-adaptive pretraining strategies
for PLMs in graph-to-text generation. We present a study across three graph
domains: meaning representations, Wikipedia knowledge graphs (KGs) and
scientific KGs. We show that the PLMs BART and T5 achieve new state-of-the-art
results and that task-adaptive pretraining strategies improve their performance
even further. In particular, we report new state-of-the-art BLEU scores of
49.72 on LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative
improvement of 31.8%, 4.5%, and 42.4%, respectively. In an extensive analysis,
we identify possible reasons for the PLMs' success on graph-to-text tasks. We
find evidence that their knowledge about true facts helps them perform well
even when the input graph representation is reduced to a simple bag of node and
edge labels.},
Year          = {2020},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2007.08426v3},
File          = {2007.08426v3.pdf}
}
@article{2011.05257v1,
Author        = {Shweta Yadav and Vishal Pallagani and Amit Sheth},
Title         = {Medical Knowledge-enriched Textual Entailment Framework},
Eprint        = {2011.05257v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {One of the cardinal tasks in achieving robust medical question answering
systems is textual entailment. The existing approaches make use of an ensemble
of pre-trained language models or data augmentation, often to clock higher
numbers on the validation metrics. However, two major shortcomings impede
higher success in identifying entailment: (1) understanding the focus/intent of
the question and (2) ability to utilize the real-world background knowledge to
capture the context beyond the sentence. In this paper, we present a novel
Medical Knowledge-Enriched Textual Entailment framework that allows the model
to acquire a semantic and global representation of the input medical text with
the help of a relevant domain-specific knowledge graph. We evaluate our
framework on the benchmark MEDIQA-RQE dataset and manifest that the use of
knowledge enriched dual-encoding mechanism help in achieving an absolute
improvement of 8.27% over SOTA language models. We have made the source code
available here.},
Year          = {2020},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2011.05257v1},
File          = {2011.05257v1.pdf}
}
@article{2101.00297v3,
Author        = {Jeff Da and Ronan Le Bras and Ximing Lu and Yejin Choi and Antoine Bosselut},
Title         = {Analyzing Commonsense Emergence in Few-shot Knowledge Models},
Eprint        = {2101.00297v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recently, commonsense knowledge models - pretrained language models (LM)
fine-tuned on knowledge graph (KG) tuples - showed that considerable amounts of
commonsense knowledge can be encoded in the parameters of large language
models. However, as parallel studies show that LMs are poor hypothesizers of
declarative commonsense relationships on their own, it remains unclear whether
this knowledge is learned during pretraining or from fine-tuning on KG
examples. To investigate this question, we train commonsense knowledge models
in few-shot settings to study the emergence of their commonsense representation
abilities. Our results show that commonsense knowledge models can rapidly adapt
from limited examples, indicating that KG fine-tuning serves to learn an
interface to encoded knowledge learned during pretraining. Importantly, our
analysis of absolute, angular, and distributional parameter changes during
few-shot fine-tuning provides novel insights into how this interface is
learned.},
Year          = {2021},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2101.00297v3},
File          = {2101.00297v3.pdf}
}
@article{2104.10649v2,
Author        = {Ruiqing Yan and Lanchang Sun and Fang Wang and Xiaoming Zhang},
Title         = {K-XLNet: A General Method for Combining Explicit Knowledge with Language
  Model Pretraining},
Eprint        = {2104.10649v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Though pre-trained language models such as Bert and XLNet, have rapidly
advanced the state-of-the-art on many NLP tasks, they implicit semantics only
relying on surface information between words in corpus. Intuitively, background
knowledge influences the efficacy of understanding. Inspired by this common
sense, we focus on improving model pretraining by leveraging explicit
knowledge. Different from recent research that optimize pretraining model by
knowledge masking strategies, we propose a simple but general method to combine
explicit knowledge with pretraining. To be specific, we first match knowledge
facts from knowledge graph (KG) and then add a knowledge injunction layer to
transformer directly without changing its architecture. The present study seeks
to find the direct impact of explicit knowledge on transformer per-training. We
conduct experiments on various datasets for different downstream tasks. The
experimental results show that solely by adding external knowledge to
transformer can improve the learning performance on many NLP tasks.},
Year          = {2021},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2104.10649v2},
File          = {2104.10649v2.pdf}
}
@article{2105.08021v2,
Author        = {Qingyun Wang and Semih Yavuz and Victoria Lin and Heng Ji and Nazneen Rajani},
Title         = {Stage-wise Fine-tuning for Graph-to-Text Generation},
Eprint        = {2105.08021v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Graph-to-text generation has benefited from pre-trained language models
(PLMs) in achieving better performance than structured graph encoders. However,
they fail to fully utilize the structure information of the input graph. In
this paper, we aim to further improve the performance of the pre-trained
language model by proposing a structured graph-to-text model with a two-step
fine-tuning mechanism which first fine-tunes the model on Wikipedia before
adapting to the graph-to-text generation. In addition to using the traditional
token and position embeddings to encode the knowledge graph (KG), we propose a
novel tree-level embedding method to capture the inter-dependency structures of
the input graph. This new approach has significantly improved the performance
of all text generation metrics for the English WebNLG 2017 dataset.},
Year          = {2021},
Month         = {May},
Url           = {http://arxiv.org/abs/2105.08021v2},
File          = {2105.08021v2.pdf}
}
@article{2106.09700v2,
Author        = {Rahul Nadkarni and David Wadden and Iz Beltagy and Noah A. Smith and Hannaneh Hajishirzi and Tom Hope},
Title         = {Scientific Language Models for Biomedical Knowledge Base Completion: An
  Empirical Study},
Eprint        = {2106.09700v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Biomedical knowledge graphs (KGs) hold rich information on entities such as
diseases, drugs, and genes. Predicting missing links in these graphs can boost
many important applications, such as drug design and repurposing. Recent work
has shown that general-domain language models (LMs) can serve as "soft" KGs,
and that they can be fine-tuned for the task of KG completion. In this work, we
study scientific LMs for KG completion, exploring whether we can tap into their
latent knowledge to enhance biomedical link prediction. We evaluate several
domain-specific LMs, fine-tuning them on datasets centered on drugs and
diseases that we represent as KGs and enrich with textual entity descriptions.
We integrate the LM-based models with KG embedding models, using a router
method that learns to assign each input example to either type of model and
provides a substantial boost in performance. Finally, we demonstrate the
advantage of LM models in the inductive setting with novel scientific entities.
Our datasets and code are made publicly available.},
Year          = {2021},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2106.09700v2},
File          = {2106.09700v2.pdf}
}
@article{2106.09997v2,
Author        = {Hieu Tran and Long Phan and James Anibal and Binh T. Nguyen and Truong-Son Nguyen},
Title         = {SPBERT: An Efficient Pre-training BERT on SPARQL Queries for Question
  Answering over Knowledge Graphs},
Eprint        = {2106.09997v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this paper, we propose SPBERT, a transformer-based language model
pre-trained on massive SPARQL query logs. By incorporating masked language
modeling objectives and the word structural objective, SPBERT can learn
general-purpose representations in both natural language and SPARQL query
language. We investigate how SPBERT and encoder-decoder architecture can be
adapted for Knowledge-based QA corpora. We conduct exhaustive experiments on
two additional tasks, including SPARQL Query Construction and Answer
Verbalization Generation. The experimental results show that SPBERT can obtain
promising results, achieving state-of-the-art BLEU scores on several of these
tasks.},
Year          = {2021},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2106.09997v2},
File          = {2106.09997v2.pdf}
}
@article{2106.11292v1,
Author        = {Mandana Saebi and Ernest Pusateri and Aaksha Meghawat and Christophe Van Gysel},
Title         = {A Discriminative Entity-Aware Language Model for Virtual Assistants},
Eprint        = {2106.11292v1},
DOI           = {10.21437/Interspeech.2021-1767},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {High-quality automatic speech recognition (ASR) is essential for virtual
assistants (VAs) to work well. However, ASR often performs poorly on VA
requests containing named entities. In this work, we start from the observation
that many ASR errors on named entities are inconsistent with real-world
knowledge. We extend previous discriminative n-gram language modeling
approaches to incorporate real-world knowledge from a Knowledge Graph (KG),
using features that capture entity type-entity and entity-entity relationships.
We apply our model through an efficient lattice rescoring process, achieving
relative sentence error rate reductions of more than 25% on some synthesized
test sets covering less popular entities, with minimal degradation on a
uniformly sampled VA test set.},
Year          = {2021},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2106.11292v1},
File          = {2106.11292v1.pdf}
}
@article{2106.11533v1,
Author        = {Peifeng Wang and Filip Ilievski and Muhao Chen and Xiang Ren},
Title         = {Do Language Models Perform Generalizable Commonsense Inference?},
Eprint        = {2106.11533v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Inspired by evidence that pretrained language models (LMs) encode commonsense
knowledge, recent work has applied LMs to automatically populate commonsense
knowledge graphs (CKGs). However, there is a lack of understanding on their
generalization to multiple CKGs, unseen relations, and novel entities. This
paper analyzes the ability of LMs to perform generalizable commonsense
inference, in terms of knowledge capacity, transferability, and induction. Our
experiments with these three aspects show that: (1) LMs can adapt to different
schemas defined by multiple CKGs but fail to reuse the knowledge to generalize
to new relations. (2) Adapted LMs generalize well to unseen subjects, but less
so on novel objects. Future work should investigate how to improve the
transferability and induction of commonsense mining from LMs.},
Year          = {2021},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2106.11533v1},
File          = {2106.11533v1.pdf}
}
@article{2108.08983v1,
Author        = {Taolin Zhang and Zerui Cai and Chengyu Wang and Minghui Qiu and Bite Yang and Xiaofeng He},
Title         = {SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with
  Structured Semantics for Medical Text Mining},
Eprint        = {2108.08983v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recently, the performance of Pre-trained Language Models (PLMs) has been
significantly improved by injecting knowledge facts to enhance their abilities
of language understanding. For medical domains, the background knowledge
sources are especially useful, due to the massive medical terms and their
complicated relations are difficult to understand in text. In this work, we
introduce SMedBERT, a medical PLM trained on large-scale medical corpora,
incorporating deep structured semantic knowledge from neighbors of
linked-entity.In SMedBERT, the mention-neighbor hybrid attention is proposed to
learn heterogeneous-entity information, which infuses the semantic
representations of entity types into the homogeneous neighboring entity
structure. Apart from knowledge integration as external features, we propose to
employ the neighbors of linked-entities in the knowledge graph as additional
global contexts of text mentions, allowing them to communicate via shared
neighbors, thus enrich their semantic representations. Experiments demonstrate
that SMedBERT significantly outperforms strong baselines in various
knowledge-intensive Chinese medical tasks. It also improves the performance of
other tasks such as question answering, question matching and natural language
inference.},
Year          = {2021},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2108.08983v1},
File          = {2108.08983v1.pdf}
}
@article{2112.08140v1,
Author        = {Bowen Yang and Cong Han and Yu Li and Lei Zuo and Zhou Yu},
Title         = {Improving Conversational Recommendation Systems' Quality with
  Context-Aware Item Meta Information},
Eprint        = {2112.08140v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Conversational recommendation systems (CRS) engage with users by inferring
user preferences from dialog history, providing accurate recommendations, and
generating appropriate responses. Previous CRSs use knowledge graph (KG) based
recommendation modules and integrate KG with language models for response
generation. Although KG-based approaches prove effective, two issues remain to
be solved. First, KG-based approaches ignore the information in the
conversational context but only rely on entity relations and bag of words to
recommend items. Second, it requires substantial engineering efforts to
maintain KGs that model domain-specific relations, thus leading to less
flexibility. In this paper, we propose a simple yet effective architecture
comprising a pre-trained language model (PLM) and an item metadata encoder. The
encoder learns to map item metadata to embeddings that can reflect the semantic
information in the dialog context. The PLM then consumes the semantic-aligned
item embeddings together with dialog context to generate high-quality
recommendations and responses. Instead of modeling entity relations with KGs,
our model reduces engineering complexity by directly converting each item to an
embedding. Experimental results on the benchmark dataset ReDial show that our
model obtains state-of-the-art results on both recommendation and response
generation tasks.},
Year          = {2021},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2112.08140v1},
File          = {2112.08140v1.pdf}
}
@article{2204.06674v4,
Author        = {Anthony Colas and Mehrdad Alvandipour and Daisy Zhe Wang},
Title         = {GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text
  Generation},
Eprint        = {2204.06674v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent improvements in KG-to-text generation are due to additional auxiliary
pre-training tasks designed to give the fine-tune task a boost in performance.
These tasks require extensive computational resources while only suggesting
marginal improvements. Here, we demonstrate that by fusing graph-aware elements
into existing pre-trained language models, we are able to outperform
state-of-the-art models and close the gap imposed by additional pre-training
tasks. We do so by proposing a mask structure to capture neighborhood
information and a novel type encoder that adds a bias to the graph-attention
weights depending on the connection type. Experiments on two KG-to-text
benchmark datasets show our models are competitive while involving fewer
parameters and no additional pre-training tasks. By formulating the problem as
a framework, we can interchange the various proposed components and begin
interpreting KG-to-text generative models based on the topological and type
information found in a graph.},
Year          = {2022},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2204.06674v4},
File          = {2204.06674v4.pdf}
}
@article{2210.05287v2,
Author        = {Taolin Zhang and Junwei Dong and Jianing Wang and Chengyu Wang and Ang Wang and Yinghui Liu and Jun Huang and Yong Li and Xiaofeng He},
Title         = {Revisiting and Advancing Chinese Natural Language Understanding with
  Accelerated Heterogeneous Knowledge Pre-training},
Eprint        = {2210.05287v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recently, knowledge-enhanced pre-trained language models (KEPLMs) improve
context-aware representations via learning from structured relations in
knowledge graphs, and/or linguistic knowledge from syntactic or dependency
analysis. Unlike English, there is a lack of high-performing open-source
Chinese KEPLMs in the natural language processing (NLP) community to support
various language understanding applications. In this paper, we revisit and
advance the development of Chinese natural language understanding with a series
of novel Chinese KEPLMs released in various parameter sizes, namely CKBERT
(Chinese knowledge-enhanced BERT).Specifically, both relational and linguistic
knowledge is effectively injected into CKBERT based on two novel pre-training
tasks, i.e., linguistic-aware masked language modeling and contrastive
multi-hop relation modeling. Based on the above two pre-training paradigms and
our in-house implemented TorchAccelerator, we have pre-trained base (110M),
large (345M) and huge (1.3B) versions of CKBERT efficiently on GPU clusters.
Experiments demonstrate that CKBERT outperforms strong baselines for Chinese
over various benchmark NLP tasks and in terms of different model sizes.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.05287v2},
File          = {2210.05287v2.pdf}
}
@article{2305.18742v1,
Author        = {Shiyang Li and Yifan Gao and Haoming Jiang and Qingyu Yin and Zheng Li and Xifeng Yan and Chao Zhang and Bing Yin},
Title         = {Graph Reasoning for Question Answering with Triplet Retrieval},
Eprint        = {2305.18742v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Answering complex questions often requires reasoning over knowledge graphs
(KGs). State-of-the-art methods often utilize entities in questions to retrieve
local subgraphs, which are then fed into KG encoder, e.g. graph neural networks
(GNNs), to model their local structures and integrated into language models for
question answering. However, this paradigm constrains retrieved knowledge in
local subgraphs and discards more diverse triplets buried in KGs that are
disconnected but useful for question answering. In this paper, we propose a
simple yet effective method to first retrieve the most relevant triplets from
KGs and then rerank them, which are then concatenated with questions to be fed
into language models. Extensive results on both CommonsenseQA and OpenbookQA
datasets show that our method can outperform state-of-the-art up to 4.6%
absolute accuracy.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.18742v1},
File          = {2305.18742v1.pdf}
}
@article{2306.04009v1,
Author        = {Kanishka Misra and Cicero Nogueira dos Santos and Siamak Shakeri},
Title         = {Triggering Multi-Hop Reasoning for Question Answering in Language Models
  using Soft Prompts and Random Walks},
Eprint        = {2306.04009v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Despite readily memorizing world knowledge about entities, pre-trained
language models (LMs) struggle to compose together two or more facts to perform
multi-hop reasoning in question-answering tasks. In this work, we propose
techniques that improve upon this limitation by relying on random walks over
structured knowledge graphs. Specifically, we use soft prompts to guide LMs to
chain together their encoded knowledge by learning to map multi-hop questions
to random walk paths that lead to the answer. Applying our methods on two T5
LMs shows substantial improvements over standard tuning approaches in answering
questions that require 2-hop reasoning.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.04009v1},
File          = {2306.04009v1.pdf}
}
@article{2308.13467v1,
Author        = {Nancy Tyagi and Surjodeep Sarkar and Manas Gaur},
Title         = {Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability
  of Language Models},
Eprint        = {2308.13467v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The Natural Language Processing(NLP) community has been using crowd sourcing
techniques to create benchmark datasets such as General Language Understanding
and Evaluation(GLUE) for training modern Language Models such as BERT. GLUE
tasks measure the reliability scores using inter annotator metrics i.e. Cohens
Kappa. However, the reliability aspect of LMs has often been overlooked. To
counter this problem, we explore a knowledge-guided LM ensembling approach that
leverages reinforcement learning to integrate knowledge from ConceptNet and
Wikipedia as knowledge graph embeddings. This approach mimics human annotators
resorting to external knowledge to compensate for information deficits in the
datasets. Across nine GLUE datasets, our research shows that ensembling
strengthens reliability and accuracy scores, outperforming state of the art.},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.13467v1},
File          = {2308.13467v1.pdf}
}
@article{2312.10638v2,
Author        = {Tarek Saier and Mayumi Ohta and Takuto Asakura and Michael Färber},
Title         = {HyperPIE: Hyperparameter Information Extraction from Scientific
  Publications},
Eprint        = {2312.10638v2},
DOI           = {10.1007/978-3-031-56060-6_17},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Automatic extraction of information from publications is key to making
scientific knowledge machine readable at a large scale. The extracted
information can, for example, facilitate academic search, decision making, and
knowledge graph construction. An important type of information not covered by
existing approaches is hyperparameters. In this paper, we formalize and tackle
hyperparameter information extraction (HyperPIE) as an entity recognition and
relation extraction task. We create a labeled data set covering publications
from a variety of computer science disciplines. Using this data set, we train
and evaluate BERT-based fine-tuned models as well as five large language
models: GPT-3.5, GALACTICA, Falcon, Vicuna, and WizardLM. For fine-tuned
models, we develop a relation extraction approach that achieves an improvement
of 29% F1 over a state-of-the-art baseline. For large language models, we
develop an approach leveraging YAML output for structured data extraction,
which achieves an average improvement of 5.5% F1 in entity recognition over
using JSON. With our best performing model we extract hyperparameter
information from a large number of unannotated papers, and analyze patterns
across disciplines. All our data and source code is publicly available at
https://github.com/IllDepence/hyperpie},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.10638v2},
File          = {2312.10638v2.pdf}
}
@article{2402.02289v1,
Author        = {Costas Mavromatis and Petros Karypis and George Karypis},
Title         = {SemPool: Simple, robust, and interpretable KG pooling for enhancing
  language models},
Eprint        = {2402.02289v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graph (KG) powered question answering (QA) performs complex
reasoning over language semantics as well as knowledge facts. Graph Neural
Networks (GNNs) learn to aggregate information from the underlying KG, which is
combined with Language Models (LMs) for effective reasoning with the given
question. However, GNN-based methods for QA rely on the graph information of
the candidate answer nodes, which limits their effectiveness in more
challenging settings where critical answer information is not included in the
KG. We propose a simple graph pooling approach that learns useful semantics of
the KG that can aid the LM's reasoning and that its effectiveness is robust
under graph perturbations. Our method, termed SemPool, represents KG facts with
pre-trained LMs, learns to aggregate their semantic information, and fuses it
at different layers of the LM. Our experimental results show that SemPool
outperforms state-of-the-art GNN-based methods by 2.27% accuracy points on
average when answer information is missing from the KG. In addition, SemPool
offers interpretability on what type of graph information is fused at different
LM layers.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.02289v1},
File          = {2402.02289v1.pdf}
}
@article{2402.03268v3,
Author        = {Xinyi Wang and Alfonso Amayuelas and Kexun Zhang and Liangming Pan and Wenhu Chen and William Yang Wang},
Title         = {Understanding Reasoning Ability of Language Models From the Perspective
  of Reasoning Paths Aggregation},
Eprint        = {2402.03268v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Pre-trained language models (LMs) are able to perform complex reasoning
without explicit fine-tuning. To understand how pre-training with a next-token
prediction objective contributes to the emergence of such reasoning capability,
we propose that we can view an LM as deriving new conclusions by aggregating
indirect reasoning paths seen at pre-training time. We found this perspective
effective in two important cases of reasoning: logic reasoning with knowledge
graphs (KGs) and chain-of-thought (CoT) reasoning. More specifically, we
formalize the reasoning paths as random walk paths on the knowledge/reasoning
graphs. Analyses of learned LM distributions suggest that a weighted sum of
relevant random walk path probabilities is a reasonable way to explain how LMs
reason. Experiments and analysis on multiple KG and CoT datasets reveal the
effect of training on random walk paths and suggest that augmenting unlabeled
random walk reasoning paths can improve real-world multi-step reasoning
performance. code: https://github.com/WANGXinyiLinda/LM_random_walk},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.03268v3},
File          = {2402.03268v3.pdf}
}
@article{2402.17786v1,
Author        = {Zilong Zhao and Yao Rong and Dongyang Guo and Emek Gözlüklü and Emir Gülboy and Enkelejda Kasneci},
Title         = {Stepwise Self-Consistent Mathematical Reasoning with Large Language
  Models},
Eprint        = {2402.17786v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Using Large Language Models for complex mathematical reasoning is difficult,
primarily due to the complexity of multi-step reasoning. The main challenges of
this process include (1) selecting critical intermediate results to advance the
procedure, and (2) limited exploration of potential solutions. To address these
issues, we introduce a novel algorithm, namely Stepwise Self-Consistent
Chain-of-Thought (SSC-CoT). SSC-CoT employs a strategy of selecting
intermediate steps based on the intersection of various reasoning chains.
Additionally, SSC-CoT enables the model to discover critical intermediate steps
by querying a knowledge graph comprising relevant domain knowledge. To validate
SSC-CoT, we present a new dataset, TriMaster100, tailored for complex
trigonometry problems. This dataset contains 100 questions, with each solution
broken down into scored intermediate steps, facilitating a comprehensive
evaluation of the mathematical reasoning process. On TriMaster100, SSC-CoT
triples the effectiveness of the state-of-the-art methods. Furthermore, we
benchmark SSC-CoT on the widely recognized complex mathematical question
dataset, MATH level 5, and it surpasses the second-best method by 7.2% in
accuracy. Code and the TriMaster100 dataset can be found at:
https://github.com/zhao-zilong/ssc-cot.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.17786v1},
File          = {2402.17786v1.pdf}
}
@article{2406.06032v1,
Author        = {Ryosuke Takahashi and Go Kamoda and Benjamin Heinzerling and Keisuke Sakaguchi and Kentaro Inui},
Title         = {The Curse of Popularity: Popular Entities have Catastrophic Side Effects
  when Deleting Knowledge from Language Models},
Eprint        = {2406.06032v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Language models (LMs) encode world knowledge in their internal parameters
through training. However, LMs may learn personal and confidential information
from the training data, leading to privacy concerns such as data leakage.
Therefore, research on knowledge deletion from LMs is essential. This study
focuses on the knowledge stored in LMs and analyzes the relationship between
the side effects of knowledge deletion and the entities related to the
knowledge. Our findings reveal that deleting knowledge related to popular
entities can have catastrophic side effects. Furthermore, this research is the
first to analyze knowledge deletion in models trained on synthetic knowledge
graphs, indicating a new direction for controlled experiments.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.06032v1},
File          = {2406.06032v1.pdf}
}
@article{2406.16374v1,
Author        = {Dongyang Li and Taolin Zhang and Longtao Huang and Chengyu Wang and Xiaofeng He and Hui Xue},
Title         = {KEHRL: Learning Knowledge-Enhanced Language Representations with
  Hierarchical Reinforcement Learning},
Eprint        = {2406.16374v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge-enhanced pre-trained language models (KEPLMs) leverage relation
triples from knowledge graphs (KGs) and integrate these external data sources
into language models via self-supervised learning. Previous works treat
knowledge enhancement as two independent operations, i.e., knowledge injection
and knowledge integration. In this paper, we propose to learn
Knowledge-Enhanced language representations with Hierarchical Reinforcement
Learning (KEHRL), which jointly addresses the problems of detecting positions
for knowledge injection and integrating external knowledge into the model in
order to avoid injecting inaccurate or irrelevant knowledge. Specifically, a
high-level reinforcement learning (RL) agent utilizes both internal and prior
knowledge to iteratively detect essential positions in texts for knowledge
injection, which filters out less meaningful entities to avoid diverting the
knowledge learning direction. Once the entity positions are selected, a
relevant triple filtration module is triggered to perform low-level RL to
dynamically refine the triples associated with polysemic entities through
binary-valued actions. Experiments validate KEHRL's effectiveness in probing
factual knowledge and enhancing the model's performance on various natural
language understanding tasks.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.16374v1},
File          = {2406.16374v1.pdf}
}
@article{2408.02377v1,
Author        = {Vanni Zavarella and Juan Carlos Gamero-Salinas and Sergio Consoli},
Title         = {A Few-Shot Approach for Relation Extraction Domain Adaptation using
  Large Language Models},
Eprint        = {2408.02377v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge graphs (KGs) have been successfully applied to the analysis of
complex scientific and technological domains, with automatic KG generation
methods typically building upon relation extraction models capturing
fine-grained relations between domain entities in text. While these relations
are fully applicable across scientific areas, existing models are trained on
few domain-specific datasets such as SciERC and do not perform well on new
target domains. In this paper, we experiment with leveraging in-context
learning capabilities of Large Language Models to perform schema-constrained
data annotation, collecting in-domain training instances for a
Transformer-based relation extraction model deployed on titles and abstracts of
research papers in the Architecture, Construction, Engineering and Operations
(AECO) domain. By assessing the performance gain with respect to a baseline
Deep Learning architecture trained on off-domain data, we show that by using a
few-shot learning strategy with structured prompts and only minimal expert
annotation the presented approach can potentially support domain adaptation of
a science KG generation model.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.02377v1},
File          = {2408.02377v1.pdf}
}
@article{2409.10921v1,
Author        = {Yanbei Jiang and Krista A. Ehinger and Jey Han Lau},
Title         = {KALE: An Artwork Image Captioning System Augmented with Heterogeneous
  Graph},
Eprint        = {2409.10921v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Exploring the narratives conveyed by fine-art paintings is a challenge in
image captioning, where the goal is to generate descriptions that not only
precisely represent the visual content but also offer a in-depth interpretation
of the artwork's meaning. The task is particularly complex for artwork images
due to their diverse interpretations and varied aesthetic principles across
different artistic schools and styles. In response to this, we present KALE
Knowledge-Augmented vision-Language model for artwork Elaborations), a novel
approach that enhances existing vision-language models by integrating artwork
metadata as additional knowledge. KALE incorporates the metadata in two ways:
firstly as direct textual input, and secondly through a multimodal
heterogeneous knowledge graph. To optimize the learning of graph
representations, we introduce a new cross-modal alignment loss that maximizes
the similarity between the image and its corresponding metadata. Experimental
results demonstrate that KALE achieves strong performance (when evaluated with
CIDEr, in particular) over existing state-of-the-art work across several
artwork datasets. Source code of the project is available at
https://github.com/Yanbei-Jiang/Artwork-Interpretation.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.10921v1},
File          = {2409.10921v1.pdf}
}
@article{2409.17580v1,
Author        = {Zahra Sepasdar and Sushant Gautam and Cise Midoglu and Michael A. Riegler and Pål Halvorsen},
Title         = {Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case
  Study},
Eprint        = {2409.17580v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Extracting meaningful insights from large and complex datasets poses
significant challenges, particularly in ensuring the accuracy and relevance of
retrieved information. Traditional data retrieval methods such as sequential
search and index-based retrieval often fail when handling intricate and
interconnected data structures, resulting in incomplete or misleading outputs.
To overcome these limitations, we introduce Structured-GraphRAG, a versatile
framework designed to enhance information retrieval across structured datasets
in natural language queries. Structured-GraphRAG utilizes multiple knowledge
graphs, which represent data in a structured format and capture complex
relationships between entities, enabling a more nuanced and comprehensive
retrieval of information. This graph-based approach reduces the risk of errors
in language model outputs by grounding responses in a structured format,
thereby enhancing the reliability of results. We demonstrate the effectiveness
of Structured-GraphRAG by comparing its performance with that of a recently
published method using traditional retrieval-augmented generation. Our findings
show that Structured-GraphRAG significantly improves query processing
efficiency and reduces response times. While our case study focuses on soccer
data, the framework's design is broadly applicable, offering a powerful tool
for data analysis and enhancing language model applications across various
structured domains.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.17580v1},
File          = {2409.17580v1.pdf}
}
@article{2410.05731v1,
Author        = {Chang Su and Jiexing Qi and He Yan and Kai Zou and Zhouhan Lin},
Title         = {Enhancing SPARQL Generation by Triplet-order-sensitive Pre-training},
Eprint        = {2410.05731v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Semantic parsing that translates natural language queries to SPARQL is of
great importance for Knowledge Graph Question Answering (KGQA) systems.
Although pre-trained language models like T5 have achieved significant success
in the Text-to-SPARQL task, their generated outputs still exhibit notable
errors specific to the SPARQL language, such as triplet flips. To address this
challenge and further improve the performance, we propose an additional
pre-training stage with a new objective, Triplet Order Correction (TOC), along
with the commonly used Masked Language Modeling (MLM), to collectively enhance
the model's sensitivity to triplet order and SPARQL syntax. Our method achieves
state-of-the-art performances on three widely-used benchmarks.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.05731v1},
File          = {2410.05731v1.pdf}
}
@article{2410.15116v1,
Author        = {Qitan Lv and Jie Wang and Hanzhu Chen and Bin Li and Yongdong Zhang and Feng Wu},
Title         = {Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large
  Language Models},
Eprint        = {2410.15116v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Generation of plausible but incorrect factual information, often termed
hallucination, has attracted significant research interest. Retrieval-augmented
language model (RALM) -- which enhances models with up-to-date knowledge --
emerges as a promising method to reduce hallucination. However, existing RALMs
may instead exacerbate hallucination when retrieving lengthy contexts. To
address this challenge, we propose COFT, a novel
\textbf{CO}arse-to-\textbf{F}ine highligh\textbf{T}ing method to focus on
different granularity-level key texts, thereby avoiding getting lost in lengthy
contexts. Specifically, COFT consists of three components: \textit{recaller},
\textit{scorer}, and \textit{selector}. First, \textit{recaller} applies a
knowledge graph to extract potential key entities in a given context. Second,
\textit{scorer} measures the importance of each entity by calculating its
contextual weight. Finally, \textit{selector} selects high contextual weight
entities with a dynamic threshold algorithm and highlights the corresponding
paragraphs, sentences, or words in a coarse-to-fine manner. Extensive
experiments on the knowledge hallucination benchmark demonstrate the
effectiveness of COFT, leading to a superior performance over $30\%$ in the F1
score metric. Moreover, COFT also exhibits remarkable versatility across
various long-form tasks, such as reading comprehension and question answering.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.15116v1},
File          = {2410.15116v1.pdf}
}
@article{2411.18147v1,
Author        = {Felix Igelbrink and Marian Renz and Martin Günther and Piper Powell and Lennart Niecksch and Oscar Lima and Martin Atzmueller and Joachim Hertzberg},
Title         = {Online Knowledge Integration for 3D Semantic Mapping: A Survey},
Eprint        = {2411.18147v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Semantic mapping is a key component of robots operating in and interacting
with objects in structured environments. Traditionally, geometric and knowledge
representations within a semantic map have only been loosely integrated.
However, recent advances in deep learning now allow full integration of prior
knowledge, represented as knowledge graphs or language concepts, into sensor
data processing and semantic mapping pipelines. Semantic scene graphs and
language models enable modern semantic mapping approaches to incorporate
graph-based prior knowledge or to leverage the rich information in human
language both during and after the mapping process. This has sparked
substantial advances in semantic mapping, leading to previously impossible
novel applications. This survey reviews these recent developments
comprehensively, with a focus on online integration of knowledge into semantic
mapping. We specifically focus on methods using semantic scene graphs for
integrating symbolic prior knowledge and language models for respective capture
of implicit common-sense knowledge and natural language concepts},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.18147v1},
File          = {2411.18147v1.pdf}
}
@article{2412.10079v1,
Author        = {George Arthur Baker and Ankush Raut and Sagi Shaier and Lawrence E Hunter and Katharina von der Wense},
Title         = {Lost in the Middle, and In-Between: Enhancing Language Models' Ability
  to Reason Over Long Contexts in Multi-Hop QA},
Eprint        = {2412.10079v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Previous work finds that recent long-context language models fail to make
equal use of information in the middle of their inputs, preferring pieces of
information located at the tail ends which creates an undue bias in situations
where we would like models to be equally capable of using different parts of
the input. Thus far, the problem has mainly only been considered in settings
with single pieces of critical information, leading us to question what happens
when multiple necessary pieces of information are spread out over the inputs.
Here, we demonstrate the effects of the "lost in the middle" problem in the
multi-hop question answering setting -- in which multiple reasoning "hops" over
disconnected documents are required -- and show that performance degrades not
only with respect to the distance of information from the edges of the context,
but also between pieces of information. Additionally, we experiment with means
of alleviating the problem by reducing superfluous document contents through
knowledge graph triple extraction and summarization, and prompting models to
reason more thoroughly using chain-of-thought prompting.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.10079v1},
File          = {2412.10079v1.pdf}
}
@article{2412.15557v1,
Author        = {Guoxiang Guo and Aldeida Aleti and Neelofar Neelofar and Chakkrit Tantithamthavorn},
Title         = {MORTAR: Metamorphic Multi-turn Testing for LLM-based Dialogue Systems},
Eprint        = {2412.15557v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {With the widespread application of LLM-based dialogue systems in daily life,
quality assurance has become more important than ever. Recent research has
successfully introduced methods to identify unexpected behaviour in single-turn
scenarios. However, multi-turn dialogue testing remains underexplored, with the
Oracle problem in multi-turn testing posing a persistent challenge for dialogue
system developers and researchers. In this paper, we propose MORTAR, a
MetamORphic multi-TuRn diAlogue testing appRoach, which mitigates the test
oracle problem in the assessment of LLM-based dialogue systems. MORTAR
automates the generation of follow-up question-answer (QA) dialogue test cases
with multiple dialogue-level perturbations and metamorphic relations. MORTAR
employs a novel knowledge graph-based dialogue information model which
effectively generates perturbed dialogue test datasets and detects bugs of
multi-turn dialogue systems in a low-cost manner. The proposed approach does
not require an LLM as a judge, eliminating potential of any biases in the
evaluation step. According to the experiment results on multiple LLM-based
dialogue systems and comparisons with single-turn metamorphic testing
approaches, MORTAR explores more unique bugs in LLM-based dialogue systems,
especially for severe bugs that MORTAR detects up to four times more unique
bugs than the most effective existing metamorphic testing approach.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.15557v1},
File          = {2412.15557v1.pdf}
}
@article{2004.11083v1,
Author        = {Bhawani Selvaretnam and Mohammed Belkhatir},
Title         = {Coupled intrinsic and extrinsic human language resource-based query
  expansion},
Eprint        = {2004.11083v1},
DOI           = {10.1007/s10115-018-1267-x},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Poor information retrieval performance has often been attributed to the
query-document vocabulary mismatch problem which is defined as the difficulty
for human users to formulate precise natural language queries that are in line
with the vocabulary of the documents deemed relevant to a specific search goal.
To alleviate this problem, query expansion processes are applied in order to
spawn and integrate additional terms to an initial query. This requires
accurate identification of main query concepts to ensure the intended search
goal is duly emphasized and relevant expansion concepts are extracted and
included in the enriched query. Natural language queries have intrinsic
linguistic properties such as parts-of-speech labels and grammatical relations
which can be utilized in determining the intended search goal. Additionally,
extrinsic language-based resources such as ontologies are needed to suggest
expansion concepts semantically coherent with the query content. We present
here a query expansion framework which capitalizes on both linguistic
characteristics of user queries and ontology resources for query constituent
encoding, expansion concept extraction and concept weighting. A thorough
empirical evaluation on real-world datasets validates our approach against
unigram language model, relevance model and a sequential dependence based
technique.},
Year          = {2020},
Month         = {Apr},
Note          = {Knowledge & Information Systems 2018},
Url           = {http://arxiv.org/abs/2004.11083v1},
File          = {2004.11083v1.pdf}
}
@article{2101.08333v1,
Author        = {Shuyang Li and Jin Cao and Mukund Sridhar and Henghui Zhu and Shang-Wen Li and Wael Hamza and Julian McAuley},
Title         = {Zero-shot Generalization in Dialog State Tracking through Generative
  Question Answering},
Eprint        = {2101.08333v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Dialog State Tracking (DST), an integral part of modern dialog systems, aims
to track user preferences and constraints (slots) in task-oriented dialogs. In
real-world settings with constantly changing services, DST systems must
generalize to new domains and unseen slot types. Existing methods for DST do
not generalize well to new slot names and many require known ontologies of slot
types and values for inference. We introduce a novel ontology-free framework
that supports natural language queries for unseen constraints and slots in
multi-domain task-oriented dialogs. Our approach is based on generative
question-answering using a conditional language model pre-trained on
substantive English sentences. Our model improves joint goal accuracy in
zero-shot domain adaptation settings by up to 9% (absolute) over the previous
state-of-the-art on the MultiWOZ 2.1 dataset.},
Year          = {2021},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2101.08333v1},
File          = {2101.08333v1.pdf}
}
@article{2211.16716v1,
Author        = {Ziyan Zhao and Li Zhang and Xiaoyun Gao and Xiaoli Lian and Heyang Lv and Lin Shi},
Title         = {Automated Generating Natural Language Requirements based on Domain
  Ontology},
Eprint        = {2211.16716v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Software requirements specification is undoubtedly critical for the whole
software life-cycle. Nowadays, writing software requirements specifications
primarily depends on human work. Although massive studies have been proposed to
fasten the process via proposing advanced elicitation and analysis techniques,
it is still a time-consuming and error-prone task that needs to take domain
knowledge and business information into consideration. In this paper, we
propose an approach, named ReqGen, which can provide recommendations by
automatically generating natural language requirements specifications based on
certain given keywords. Specifically, ReqGen consists of three critical steps.
First, keywords-oriented knowledge is selected from domain ontology and is
injected to the basic Unified pre-trained Language Model (UniLM) for domain
fine-tuning. Second, a copy mechanism is integrated to ensure the occurrence of
keywords in the generated statements. Finally, a requirement syntax constrained
decoding is designed to close the semantic and syntax distance between the
candidate and reference specifications. Experiments on two public datasets from
different groups and domains show that ReqGen outperforms six popular natural
language generation approaches with respect to the hard constraint of
keywords(phrases) inclusion, BLEU, ROUGE and syntax compliance. We believe that
ReqGen can promote the efficiency and intelligence of specifying software
requirements.},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.16716v1},
File          = {2211.16716v1.pdf}
}
@article{2306.14704v3,
Author        = {Hang Dong and Jiaoyan Chen and Yuan He and Ian Horrocks},
Title         = {Ontology Enrichment from Texts: A Biomedical Dataset for Concept
  Discovery and Placement},
Eprint        = {2306.14704v3},
DOI           = {10.1145/3583780.3615126},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Mentions of new concepts appear regularly in texts and require automated
approaches to harvest and place them into Knowledge Bases (KB), e.g.,
ontologies and taxonomies. Existing datasets suffer from three issues, (i)
mostly assuming that a new concept is pre-discovered and cannot support
out-of-KB mention discovery; (ii) only using the concept label as the input
along with the KB and thus lacking the contexts of a concept label; and (iii)
mostly focusing on concept placement w.r.t a taxonomy of atomic concepts,
instead of complex concepts, i.e., with logical operators. To address these
issues, we propose a new benchmark, adapting MedMentions dataset (PubMed
abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases
sub-category and the broader categories of Clinical finding, Procedure, and
Pharmaceutical / biologic product. We provide usage on the evaluation with the
dataset for out-of-KB mention discovery and concept placement, adapting recent
Large Language Model based methods.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.14704v3},
File          = {2306.14704v3.pdf}
}
@article{2202.08960v1,
Author        = {Amine Barrak and Bram Adams and Amal Zouaq},
Title         = {Toward a traceable, explainable, and fairJD/Resume recommendation system},
Eprint        = {2202.08960v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {In the last few decades, companies are interested to adopt an online
automated recruitment process in an international recruitment environment. The
problem is that the recruitment of employees through the manual procedure is a
time and money consuming process. As a result, processing a significant number
of applications through conventional methods can lead to the recruitment of
clumsy individuals. Different JD/Resume matching model architectures have been
proposed and reveal a high accuracy level in selecting relevant candidatesfor
the required job positions. However, the development of an automatic
recruitment system is still one of the main challenges. The reason is that the
development of a fully automated recruitment system is a difficult task and
poses different challenges. For example, providing a detailed matching
explanation for the targeted stakeholders is needed to ensure a transparent
recommendation. There are several knowledge bases that represent skills and
competencies (e.g, ESCO, O*NET) that are used to identify the candidate and the
required job skills for a matching purpose. Besides, modernpre-trained language
models are fine-tuned for this context such as identifying lines where a
specific feature was introduced. Typically, pre-trained language models use
transfer-based machine learning models to be fine-tuned for a specific field.
In this proposal, our aim is to explore how modern language models (based on
transformers) can be combined with knowledge bases and ontologies to enhance
the JD/Resume matching process. Our system aims at using knowledge bases and
features to support the explainability of the JD/Resume matching. Finally,
given that multiple software components, datasets, ontology, andmachine
learning models will be explored, we aim at proposing a fair, ex-plainable, and
traceable architecture for a Resume/JD matching purpose.},
Year          = {2022},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2202.08960v1},
File          = {2202.08960v1.pdf}
}
@article{2408.07852v1,
Author        = {Jiri Hron and Laura Culp and Gamaleldin Elsayed and Rosanne Liu and Ben Adlam and Maxwell Bileschi and Bernd Bohnet and JD Co-Reyes and Noah Fiedel and C. Daniel Freeman and Izzeddin Gur and Kathleen Kenealy and Jaehoon Lee and Peter J. Liu and Gaurav Mishra and Igor Mordatch and Azade Nova and Roman Novak and Aaron Parisi and Jeffrey Pennington and Alex Rizkowsky and Isabelle Simpson and Hanie Sedghi and Jascha Sohl-dickstein and Kevin Swersky and Sharad Vikram and Tris Warkentin and Lechao Xiao and Kelvin Xu and Jasper Snoek and Simon Kornblith},
Title         = {Training Language Models on the Knowledge Graph: Insights on
  Hallucinations and Their Detectability},
Eprint        = {2408.07852v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {While many capabilities of language models (LMs) improve with increased
training budget, the influence of scale on hallucinations is not yet fully
understood. Hallucinations come in many forms, and there is no universally
accepted definition. We thus focus on studying only those hallucinations where
a correct answer appears verbatim in the training set. To fully control the
training data content, we construct a knowledge graph (KG)-based dataset, and
use it to train a set of increasingly large LMs. We find that for a fixed
dataset, larger and longer-trained LMs hallucinate less. However, hallucinating
on $\leq5$% of the training data requires an order of magnitude larger model,
and thus an order of magnitude more compute, than Hoffmann et al. (2022)
reported was optimal. Given this costliness, we study how hallucination
detectors depend on scale. While we see detector size improves performance on
fixed LM's outputs, we find an inverse relationship between the scale of the LM
and the detectability of its hallucinations.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.07852v1},
File          = {2408.07852v1.pdf}
}
@article{2404.08579v1,
Author        = {William Gantt and Aaron Steven White},
Title         = {Small Models Are (Still) Effective Cross-Domain Argument Extractors},
Eprint        = {2404.08579v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Effective ontology transfer has been a major goal of recent work on event
argument extraction (EAE). Two methods in particular -- question answering (QA)
and template infilling (TI) -- have emerged as promising approaches to this
problem. However, detailed explorations of these techniques' ability to
actually enable this transfer are lacking. In this work, we provide such a
study, exploring zero-shot transfer using both techniques on six major EAE
datasets at both the sentence and document levels. Further, we challenge the
growing reliance on LLMs for zero-shot extraction, showing that vastly smaller
models trained on an appropriate source ontology can yield zero-shot
performance superior to that of GPT-3.5 or GPT-4.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.08579v1},
File          = {2404.08579v1.pdf}
}
@article{2312.09126v2,
Author        = {Daniel Maninger and Krishna Narasimhan and Mira Mezini},
Title         = {Towards Trustworthy AI Software Development Assistance},
Eprint        = {2312.09126v2},
DOI           = {10.1145/3639476.3639770},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {It is expected that in the near future, AI software development assistants
will play an important role in the software industry. However, current software
development assistants tend to be unreliable, often producing incorrect,
unsafe, or low-quality code. We seek to resolve these issues by introducing a
holistic architecture for constructing, training, and using trustworthy AI
software development assistants. In the center of the architecture, there is a
foundational LLM trained on datasets representative of real-world coding
scenarios and complex software architectures, and fine-tuned on code quality
criteria beyond correctness. The LLM will make use of graph-based code
representations for advanced semantic comprehension. We envision a knowledge
graph integrated into the system to provide up-to-date background knowledge and
to enable the assistant to provide appropriate explanations. Finally, a modular
framework for constrained decoding will ensure that certain guarantees (e.g.,
for correctness and security) hold for the generated code.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.09126v2},
File          = {2312.09126v2.pdf}
}
@article{2409.15113v2,
Author        = {Lior Forer and Tom Hope},
Title         = {Inferring Scientific Cross-Document Coreference and Hierarchy with
  Definition-Augmented Relational Reasoning},
Eprint        = {2409.15113v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We address the fundamental task of inferring cross-document coreference and
hierarchy in scientific texts, which has important applications in knowledge
graph construction, search, recommendation and discovery. LLMs can struggle
when faced with many long-tail technical concepts with nuanced variations. We
present a novel method which generates context-dependent definitions of concept
mentions by retrieving full-text literature, and uses the definitions to
enhance detection of cross-document relations. We further generate relational
definitions, which describe how two concept mentions are related or different,
and design an efficient re-ranking approach to address the combinatorial
explosion involved in inferring links across papers. In both fine-tuning and
in-context learning settings we achieve large gains in performance. We provide
analysis of generated definitions, shedding light on the relational reasoning
ability of LLMs over fine-grained scientific concepts.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.15113v2},
File          = {2409.15113v2.pdf}
}
@article{2407.20284v1,
Author        = {Shyam Dongre and Ritesh Chandra and Sonali Agarwal},
Title         = {MLtoGAI: Semantic Web based with Machine Learning for Enhanced Disease
  Prediction and Personalized Recommendations using Generative AI},
Eprint        = {2407.20284v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In modern healthcare, addressing the complexities of accurate disease
prediction and personalized recommendations is both crucial and challenging.
This research introduces MLtoGAI, which integrates Semantic Web technology with
Machine Learning (ML) to enhance disease prediction and offer user-friendly
explanations through ChatGPT. The system comprises three key components: a
reusable disease ontology that incorporates detailed knowledge about various
diseases, a diagnostic classification model that uses patient symptoms to
detect specific diseases accurately, and the integration of Semantic Web Rule
Language (SWRL) with ontology and ChatGPT to generate clear, personalized
health advice. This approach significantly improves prediction accuracy and
ensures results that are easy to understand, addressing the complexity of
diseases and diverse symptoms. The MLtoGAI system demonstrates substantial
advancements in accuracy and user satisfaction, contributing to developing more
intelligent and accessible healthcare solutions. This innovative approach
combines the strengths of ML algorithms with the ability to provide
transparent, human-understandable explanations through ChatGPT, achieving
significant improvements in prediction accuracy and user comprehension. By
leveraging semantic technology and explainable AI, the system enhances the
accuracy of disease prediction and ensures that the recommendations are
relevant and easily understood by individual patients. Our research highlights
the potential of integrating advanced technologies to overcome existing
challenges in medical diagnostics, paving the way for future developments in
intelligent healthcare systems. Additionally, the system is validated using 200
synthetic patient data records, ensuring robust performance and reliability.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.20284v1},
File          = {2407.20284v1.pdf}
}
@article{2406.16810v1,
Author        = {Xinchi Qiu and William F. Shen and Yihong Chen and Nicola Cancedda and Pontus Stenetorp and Nicholas D. Lane},
Title         = {PISTOL: Dataset Compilation Pipeline for Structural Unlearning of LLMs},
Eprint        = {2406.16810v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Recently, machine unlearning, which seeks to erase specific data stored in
the pre-trained or fine-tuned models, has emerged as a crucial protective
measure for LLMs. However, unlearning approaches for LLMs that have been
considered thus far have focused on the removal of independent data points and
have not taken into account that the stored facts are logically connected to
one another and form an implicit knowledge graph. To facilitate the development
of structural unlearning methods, which are essential for the practical
application of unlearning, we propose PISTOL, a pipeline for compiling
multi-scenario datasets for benchmarking structural LLM unlearning.
Additionally, leveraging sample datasets synthesized using PISTOL, we conducted
benchmarks with four distinct unlearning methods on both Llama2-7B and
Mistral-7B models. This analysis helps to illustrate the prevailing challenges
in effectively and robustly removing highly inter-connected data, batched data,
or data skewed towards a specific domain. It also highlights the choice of
pre-trained model can impact unlearning performance. This work not only
advances our understandings on the limitation of current LLMs unlearning
methods and proposes future research directions, but also provides a replicable
framework for ongoing exploration and validation in the field.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.16810v1},
File          = {2406.16810v1.pdf}
}
@article{2408.10053v1,
Author        = {Haoran Li and Wei Fan and Yulin Chen and Jiayang Cheng and Tianshu Chu and Xuebing Zhou and Peizhao Hu and Yangqiu Song},
Title         = {Privacy Checklist: Privacy Violation Detection Grounding on Contextual
  Integrity Theory},
Eprint        = {2408.10053v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Privacy research has attracted wide attention as individuals worry that their
private data can be easily leaked during interactions with smart devices,
social platforms, and AI applications. Computer science researchers, on the
other hand, commonly study privacy issues through privacy attacks and defenses
on segmented fields. Privacy research is conducted on various sub-fields,
including Computer Vision (CV), Natural Language Processing (NLP), and Computer
Networks. Within each field, privacy has its own formulation. Though pioneering
works on attacks and defenses reveal sensitive privacy issues, they are
narrowly trapped and cannot fully cover people's actual privacy concerns.
Consequently, the research on general and human-centric privacy research
remains rather unexplored. In this paper, we formulate the privacy issue as a
reasoning problem rather than simple pattern matching. We ground on the
Contextual Integrity (CI) theory which posits that people's perceptions of
privacy are highly correlated with the corresponding social context. Based on
such an assumption, we develop the first comprehensive checklist that covers
social identities, private attributes, and existing privacy regulations. Unlike
prior works on CI that either cover limited expert annotated norms or model
incomplete social context, our proposed privacy checklist uses the whole Health
Insurance Portability and Accountability Act of 1996 (HIPAA) as an example, to
show that we can resort to large language models (LLMs) to completely cover the
HIPAA's regulations. Additionally, our checklist also gathers expert
annotations across multiple ontologies to determine private information
including but not limited to personally identifiable information (PII). We use
our preliminary results on the HIPAA to shed light on future context-centric
privacy research to cover more privacy regulations, social norms and standards.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.10053v1},
File          = {2408.10053v1.pdf}
}
@article{1410.0718v2,
Author        = {Felix Hill and KyungHyun Cho and Sebastien Jean and Coline Devin and Yoshua Bengio},
Title         = {Not All Neural Embeddings are Born Equal},
Eprint        = {1410.0718v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Neural language models learn word representations that capture rich
linguistic and conceptual information. Here we investigate the embeddings
learned by neural machine translation models. We show that translation-based
embeddings outperform those learned by cutting-edge monolingual models at
single-language tasks requiring knowledge of conceptual similarity and/or
syntactic role. The findings suggest that, while monolingual models learn
information about how concepts are related, neural-translation models better
capture their true ontological status.},
Year          = {2014},
Month         = {Oct},
Url           = {http://arxiv.org/abs/1410.0718v2},
File          = {1410.0718v2.pdf}
}
@article{2112.02810v1,
Author        = {Kyudam Choi and Yurim Lee and Cheongwon Kim and Minsung Yoon},
Title         = {An Effective GCN-based Hierarchical Multi-label classification for
  Protein Function Prediction},
Eprint        = {2112.02810v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {We propose an effective method to improve Protein Function Prediction (PFP)
utilizing hierarchical features of Gene Ontology (GO) terms. Our method
consists of a language model for encoding the protein sequence and a Graph
Convolutional Network (GCN) for representing GO terms. To reflect the
hierarchical structure of GO to GCN, we employ node(GO term)-wise
representations containing the whole hierarchical information. Our algorithm
shows effectiveness in a large-scale graph by expanding the GO graph compared
to previous models. Experimental results show that our method outperformed
state-of-the-art PFP approaches.},
Year          = {2021},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2112.02810v1},
File          = {2112.02810v1.pdf}
}
@article{2202.08063v6,
Author        = {Shumin Deng and Yubo Ma and Ningyu Zhang and Yixin Cao and Bryan Hooi},
Title         = {Information Extraction in Low-Resource Scenarios: Survey and Perspective},
Eprint        = {2202.08063v6},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Information Extraction (IE) seeks to derive structured information from
unstructured texts, often facing challenges in low-resource scenarios due to
data scarcity and unseen classes. This paper presents a review of neural
approaches to low-resource IE from \emph{traditional} and \emph{LLM-based}
perspectives, systematically categorizing them into a fine-grained taxonomy.
Then we conduct empirical study on LLM-based methods compared with previous
state-of-the-art models, and discover that (1) well-tuned LMs are still
predominant; (2) tuning open-resource LLMs and ICL with GPT family is promising
in general; (3) the optimal LLM-based technical solution for low-resource IE
can be task-dependent. In addition, we discuss low-resource IE with LLMs,
highlight promising applications, and outline potential research directions.
This survey aims to foster understanding of this field, inspire new ideas, and
encourage widespread applications in both academia and industry.},
Year          = {2022},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2202.08063v6},
File          = {2202.08063v6.pdf}
}
@article{2109.04223v2,
Author        = {Yinquan Lu and Haonan Lu and Guirong Fu and Qun Liu},
Title         = {KELM: Knowledge Enhanced Pre-Trained Language Representations with
  Message Passing on Hierarchical Relational Graphs},
Eprint        = {2109.04223v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Incorporating factual knowledge into pre-trained language models (PLM) such
as BERT is an emerging trend in recent NLP studies. However, most of the
existing methods combine the external knowledge integration module with a
modified pre-training loss and re-implement the pre-training process on the
large-scale corpus. Re-pretraining these models is usually resource-consuming,
and difficult to adapt to another domain with a different knowledge graph (KG).
Besides, those works either cannot embed knowledge context dynamically
according to textual context or struggle with the knowledge ambiguity issue. In
this paper, we propose a novel knowledge-aware language model framework based
on fine-tuning process, which equips PLM with a unified knowledge-enhanced text
graph that contains both text and multi-relational sub-graphs extracted from
KG. We design a hierarchical relational-graph-based message passing mechanism,
which can allow the representations of injected KG and text to mutually update
each other and can dynamically select ambiguous mentioned entities that share
the same text. Our empirical results show that our model can efficiently
incorporate world knowledge from KGs into existing language models such as
BERT, and achieve significant improvement on the machine reading comprehension
(MRC) task compared with other knowledge-enhanced models.},
Year          = {2021},
Month         = {Sep},
Note          = {ICLR 2022 Workshop on Deep Learning on Graphs for Natural Language
  Processing,2022},
Url           = {http://arxiv.org/abs/2109.04223v2},
File          = {2109.04223v2.pdf}
}
@article{2211.05994v4,
Author        = {Linmei Hu and Zeyi Liu and Ziwang Zhao and Lei Hou and Liqiang Nie and Juanzi Li},
Title         = {A Survey of Knowledge Enhanced Pre-trained Language Models},
Eprint        = {2211.05994v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pre-trained Language Models (PLMs) which are trained on large text corpus via
self-supervised learning method, have yielded promising performance on various
tasks in Natural Language Processing (NLP). However, though PLMs with huge
parameters can effectively possess rich knowledge learned from massive training
text and benefit downstream tasks at the fine-tuning stage, they still have
some limitations such as poor reasoning ability due to the lack of external
knowledge. Research has been dedicated to incorporating knowledge into PLMs to
tackle these issues. In this paper, we present a comprehensive review of
Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear
insight into this thriving field. We introduce appropriate taxonomies
respectively for Natural Language Understanding (NLU) and Natural Language
Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide
the types of knowledge into four categories: linguistic knowledge, text
knowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are
categorized into KG-based and retrieval-based methods. Finally, we point out
some promising future directions of KE-PLMs.},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.05994v4},
File          = {2211.05994v4.pdf}
}
@article{2403.11203v1,
Author        = {Junbing Yan and Chengyu Wang and Taolin Zhang and Xiaofeng He and Jun Huang and Longtao Huang and Hui Xue and Wei Zhang},
Title         = {TRELM: Towards Robust and Efficient Pre-training for Knowledge-Enhanced
  Language Models},
Eprint        = {2403.11203v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {KEPLMs are pre-trained models that utilize external knowledge to enhance
language understanding. Previous language models facilitated knowledge
acquisition by incorporating knowledge-related pre-training tasks learned from
relation triples in knowledge graphs. However, these models do not prioritize
learning embeddings for entity-related tokens. Moreover, updating the entire
set of parameters in KEPLMs is computationally demanding. This paper introduces
TRELM, a Robust and Efficient Pre-training framework for Knowledge-Enhanced
Language Models. We observe that entities in text corpora usually follow the
long-tail distribution, where the representations of some entities are
suboptimally optimized and hinder the pre-training process for KEPLMs. To
tackle this, we employ a robust approach to inject knowledge triples and employ
a knowledge-augmented memory bank to capture valuable information. Furthermore,
updating a small subset of neurons in the feed-forward networks (FFNs) that
store factual knowledge is both sufficient and efficient. Specifically, we
utilize dynamic knowledge routing to identify knowledge paths in FFNs and
selectively update parameters during pre-training. Experimental results show
that TRELM reduces pre-training time by at least 50% and outperforms other
KEPLMs in knowledge probing tasks and multiple knowledge-aware language
understanding tasks.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.11203v1},
File          = {2403.11203v1.pdf}
}
@article{2409.04286v2,
Author        = {Desiree Heim and Christian Jilek and Adrian Ulges and Andreas Dengel},
Title         = {Using Large Language Models to Generate Authentic Multi-agent Knowledge
  Work Datasets},
Eprint        = {2409.04286v2},
DOI           = {10.18420/inf2024_118},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Current publicly available knowledge work data collections lack diversity,
extensive annotations, and contextual information about the users and their
documents. These issues hinder objective and comparable data-driven evaluations
and optimizations of knowledge work assistance systems. Due to the considerable
resources needed to collect such data in real-life settings and the necessity
of data censorship, collecting such a dataset appears nearly impossible. For
this reason, we propose a configurable, multi-agent knowledge work dataset
generator. This system simulates collaborative knowledge work among agents
producing Large Language Model-generated documents and accompanying data
traces. Additionally, the generator captures all background information, given
in its configuration or created during the simulation process, in a knowledge
graph. Finally, the resulting dataset can be utilized and shared without
privacy or confidentiality concerns.
  This paper introduces our approach's design and vision and focuses on
generating authentic knowledge work documents using Large Language Models. Our
study involving human raters who assessed 53% of the generated and 74% of the
real documents as realistic demonstrates the potential of our approach.
Furthermore, we analyze the authenticity criteria mentioned in the
participants' comments and elaborate on potential improvements for identified
common issues.},
Year          = {2024},
Month         = {Sep},
Note          = {INFORMATIK 2024},
Url           = {http://arxiv.org/abs/2409.04286v2},
File          = {2409.04286v2.pdf}
}
@article{2005.00691v2,
Author        = {Peifeng Wang and Nanyun Peng and Filip Ilievski and Pedro Szekely and Xiang Ren},
Title         = {Connecting the Dots: A Knowledgeable Path Generator for Commonsense
  Question Answering},
Eprint        = {2005.00691v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Commonsense question answering (QA) requires background knowledge which is
not explicitly stated in a given context. Prior works use commonsense knowledge
graphs (KGs) to obtain this knowledge for reasoning. However, relying entirely
on these KGs may not suffice, considering their limited coverage and the
contextual dependence of their knowledge. In this paper, we augment a general
commonsense QA framework with a knowledgeable path generator. By extrapolating
over existing paths in a KG with a state-of-the-art language model, our
generator learns to connect a pair of entities in text with a dynamic, and
potentially novel, multi-hop relational path. Such paths can provide structured
evidence for solving commonsense questions without fine-tuning the path
generator. Experiments on two datasets show the superiority of our method over
previous works which fully rely on knowledge from KGs (with up to 6%
improvement in accuracy), across various amounts of training data. Further
evaluation suggests that the generated paths are typically interpretable,
novel, and relevant to the task.},
Year          = {2020},
Month         = {May},
Url           = {http://arxiv.org/abs/2005.00691v2},
File          = {2005.00691v2.pdf}
}
@article{2005.05240v1,
Author        = {Ye Liu and Tao Yang and Zeyu You and Wei Fan and Philip S. Yu},
Title         = {Commonsense Evidence Generation and Injection in Reading Comprehension},
Eprint        = {2005.05240v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Human tackle reading comprehension not only based on the given context itself
but often rely on the commonsense beyond. To empower the machine with
commonsense reasoning, in this paper, we propose a Commonsense Evidence
Generation and Injection framework in reading comprehension, named CEGI. The
framework injects two kinds of auxiliary commonsense evidence into
comprehensive reading to equip the machine with the ability of rational
thinking. Specifically, we build two evidence generators: the first generator
aims to generate textual evidence via a language model; the other generator
aims to extract factual evidence (automatically aligned text-triples) from a
commonsense knowledge graph after graph completion. Those evidences incorporate
contextual commonsense and serve as the additional inputs to the model.
Thereafter, we propose a deep contextual encoder to extract semantic
relationships among the paragraph, question, option, and evidence. Finally, we
employ a capsule network to extract different linguistic units (word and
phrase) from the relations, and dynamically predict the optimal option based on
the extracted units. Experiments on the CosmosQA dataset demonstrate that the
proposed CEGI model outperforms the current state-of-the-art approaches and
achieves the accuracy (83.6%) on the leaderboard.},
Year          = {2020},
Month         = {May},
Url           = {http://arxiv.org/abs/2005.05240v1},
File          = {2005.05240v1.pdf}
}
@article{1903.10122v1,
Author        = {Christy Y. Li and Xiaodan Liang and Zhiting Hu and Eric P. Xing},
Title         = {Knowledge-driven Encode, Retrieve, Paraphrase for Medical Image Report
  Generation},
Eprint        = {1903.10122v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Generating long and semantic-coherent reports to describe medical images
poses great challenges towards bridging visual and linguistic modalities,
incorporating medical domain knowledge, and generating realistic and accurate
descriptions. We propose a novel Knowledge-driven Encode, Retrieve, Paraphrase
(KERP) approach which reconciles traditional knowledge- and retrieval-based
methods with modern learning-based methods for accurate and robust medical
report generation. Specifically, KERP decomposes medical report generation into
explicit medical abnormality graph learning and subsequent natural language
modeling. KERP first employs an Encode module that transforms visual features
into a structured abnormality graph by incorporating prior medical knowledge;
then a Retrieve module that retrieves text templates based on the detected
abnormalities; and lastly, a Paraphrase module that rewrites the templates
according to specific cases. The core of KERP is a proposed generic
implementation unit---Graph Transformer (GTR) that dynamically transforms
high-level semantics between graph-structured data of multiple domains such as
knowledge graphs, images and sequences. Experiments show that the proposed
approach generates structured and robust reports supported with accurate
abnormality description and explainable attentive regions, achieving the
state-of-the-art results on two medical report benchmarks, with the best
medical abnormality and disease classification accuracy and improved human
evaluation performance.},
Year          = {2019},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1903.10122v1},
File          = {1903.10122v1.pdf}
}
@article{2001.11003v2,
Author        = {Leonardo F. R. Ribeiro and Yue Zhang and Claire Gardent and Iryna Gurevych},
Title         = {Modeling Global and Local Node Contexts for Text Generation from
  Knowledge Graphs},
Eprint        = {2001.11003v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent graph-to-text models generate text from graph-based data using either
global or local aggregation to learn node representations. Global node encoding
allows explicit communication between two distant nodes, thereby neglecting
graph topology as all nodes are directly connected. In contrast, local node
encoding considers the relations between neighbor nodes capturing the graph
structure, but it can fail to capture long-range relations. In this work, we
gather both encoding strategies, proposing novel neural models which encode an
input graph combining both global and local node contexts, in order to learn
better contextualized node embeddings. In our experiments, we demonstrate that
our approaches lead to significant improvements on two graph-to-text datasets
achieving BLEU scores of 18.01 on AGENDA dataset, and 63.69 on the WebNLG
dataset for seen categories, outperforming state-of-the-art models by 3.7 and
3.1 points, respectively.},
Year          = {2020},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2001.11003v2},
File          = {2001.11003v2.pdf}
}
@article{2109.05327v5,
Author        = {Francesco Sovrano and Fabio Vitali},
Title         = {An Objective Metric for Explainable AI: How and Why to Estimate the
  Degree of Explainability},
Eprint        = {2109.05327v5},
DOI           = {10.1016/j.knosys.2023.110866},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Explainable AI was born as a pathway to allow humans to explore and
understand the inner working of complex systems. However, establishing what is
an explanation and objectively evaluating explainability are not trivial tasks.
This paper presents a new model-agnostic metric to measure the Degree of
Explainability of information in an objective way. We exploit a specific
theoretical model from Ordinary Language Philosophy called the Achinstein's
Theory of Explanations, implemented with an algorithm relying on deep language
models for knowledge graph extraction and information retrieval. To understand
whether this metric can measure explainability, we devised a few experiments
and user studies involving more than 190 participants, evaluating two realistic
systems for healthcare and finance using famous AI technology, including
Artificial Neural Networks and TreeSHAP. The results we obtained are
statistically significant (with P values lower than .01), suggesting that our
proposed metric for measuring the Degree of Explainability is robust in several
scenarios, and it aligns with concrete expectations.},
Year          = {2021},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2109.05327v5},
File          = {2109.05327v5.pdf}
}
@article{2302.04790v1,
Author        = {Bhavyajeet Singh and Pavan Kandru and Anubhav Sharma and Vasudeva Varma},
Title         = {Massively Multilingual Language Models for Cross Lingual Fact Extraction
  from Low Resource Indian Languages},
Eprint        = {2302.04790v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Massive knowledge graphs like Wikidata attempt to capture world knowledge
about multiple entities. Recent approaches concentrate on automatically
enriching these KGs from text. However a lot of information present in the form
of natural text in low resource languages is often missed out. Cross Lingual
Information Extraction aims at extracting factual information in the form of
English triples from low resource Indian Language text. Despite its massive
potential, progress made on this task is lagging when compared to Monolingual
Information Extraction. In this paper, we propose the task of Cross Lingual
Fact Extraction(CLFE) from text and devise an end-to-end generative approach
for the same which achieves an overall F1 score of 77.46.},
Year          = {2023},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2302.04790v1},
File          = {2302.04790v1.pdf}
}
@article{2302.05608v1,
Author        = {Zhu Wang and Sourav Medya and Sathya N. Ravi},
Title         = {Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis},
Eprint        = {2302.05608v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Often, deep network models are purely inductive during training and while
performing inference on unseen data. Thus, when such models are used for
predictions, it is well known that they often fail to capture the semantic
information and implicit dependencies that exist among objects (or concepts) on
a population level. Moreover, it is still unclear how domain or prior modal
knowledge can be specified in a backpropagation friendly manner, especially in
large-scale and noisy settings. In this work, we propose an end-to-end vision
and language model incorporating explicit knowledge graphs. We also introduce
an interactive out-of-distribution (OOD) layer using implicit network operator.
The layer is used to filter noise that is brought by external knowledge base.
In practice, we apply our model on several vision and language downstream tasks
including visual question answering, visual reasoning, and image-text retrieval
on different datasets. Our experiments show that it is possible to design
models that perform similarly to state-of-art results but with significantly
fewer samples and training time.},
Year          = {2023},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2302.05608v1},
File          = {2302.05608v1.pdf}
}
@article{2004.13988v2,
Author        = {Junlong Li and Zhuosheng Zhang and Hai Zhao},
Title         = {Knowledgeable Dialogue Reading Comprehension on Key Turns},
Eprint        = {2004.13988v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Multi-choice machine reading comprehension (MRC) requires models to choose
the correct answer from candidate options given a passage and a question. Our
research focuses dialogue-based MRC, where the passages are multi-turn
dialogues. It suffers from two challenges, the answer selection decision is
made without support of latently helpful commonsense, and the multi-turn
context may hide considerable irrelevant information. This work thus makes the
first attempt to tackle those two challenges by extracting substantially
important turns and utilizing external knowledge to enhance the representation
of context. In this paper, the relevance of each turn to the question are
calculated to choose key turns. Besides, terms related to the context and the
question in a knowledge graph are extracted as external knowledge. The original
context, question and external knowledge are encoded with the pre-trained
language model, then the language representation and key turns are combined
together with a will-designed mechanism to predict the answer. Experimental
results on a DREAM dataset show that our proposed model achieves great
improvements on baselines.},
Year          = {2020},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2004.13988v2},
File          = {2004.13988v2.pdf}
}
@article{2006.09242v3,
Author        = {Martin Schmitt and Leonardo F. R. Ribeiro and Philipp Dufter and Iryna Gurevych and Hinrich Schütze},
Title         = {Modeling Graph Structure via Relative Position for Text Generation from
  Knowledge Graphs},
Eprint        = {2006.09242v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present Graformer, a novel Transformer-based encoder-decoder architecture
for graph-to-text generation. With our novel graph self-attention, the encoding
of a node relies on all nodes in the input graph - not only direct neighbors -
facilitating the detection of global patterns. We represent the relation
between two nodes as the length of the shortest path between them. Graformer
learns to weight these node-node relations differently for different attention
heads, thus virtually learning differently connected views of the input graph.
We evaluate Graformer on two popular graph-to-text generation benchmarks,
AGENDA and WebNLG, where it achieves strong performance while using many fewer
parameters than other approaches.},
Year          = {2020},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2006.09242v3},
File          = {2006.09242v3.pdf}
}
@article{2006.15435v1,
Author        = {Beliz Gunel and Chenguang Zhu and Michael Zeng and Xuedong Huang},
Title         = {Mind The Facts: Knowledge-Boosted Coherent Abstractive Text
  Summarization},
Eprint        = {2006.15435v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Neural models have become successful at producing abstractive summaries that
are human-readable and fluent. However, these models have two critical
shortcomings: they often don't respect the facts that are either included in
the source article or are known to humans as commonsense knowledge, and they
don't produce coherent summaries when the source article is long. In this work,
we propose a novel architecture that extends Transformer encoder-decoder
architecture in order to improve on these shortcomings. First, we incorporate
entity-level knowledge from the Wikidata knowledge graph into the
encoder-decoder architecture. Injecting structural world knowledge from
Wikidata helps our abstractive summarization model to be more fact-aware.
Second, we utilize the ideas used in Transformer-XL language model in our
proposed encoder-decoder architecture. This helps our model with producing
coherent summaries even when the source article is long. We test our model on
CNN/Daily Mail summarization dataset and show improvements on ROUGE scores over
the baseline Transformer model. We also include model predictions for which our
model accurately conveys the facts, while the baseline Transformer model
doesn't.},
Year          = {2020},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2006.15435v1},
File          = {2006.15435v1.pdf}
}
@article{1905.07129v3,
Author        = {Zhengyan Zhang and Xu Han and Zhiyuan Liu and Xin Jiang and Maosong Sun and Qun Liu},
Title         = {ERNIE: Enhanced Language Representation with Informative Entities},
Eprint        = {1905.07129v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Neural language representation models such as BERT pre-trained on large-scale
corpora can well capture rich semantic patterns from plain text, and be
fine-tuned to consistently improve the performance of various NLP tasks.
However, the existing pre-trained language models rarely consider incorporating
knowledge graphs (KGs), which can provide rich structured knowledge facts for
better language understanding. We argue that informative entities in KGs can
enhance language representation with external knowledge. In this paper, we
utilize both large-scale textual corpora and KGs to train an enhanced language
representation model (ERNIE), which can take full advantage of lexical,
syntactic, and knowledge information simultaneously. The experimental results
have demonstrated that ERNIE achieves significant improvements on various
knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art
model BERT on other common NLP tasks. The source code of this paper can be
obtained from https://github.com/thunlp/ERNIE.},
Year          = {2019},
Month         = {May},
Url           = {http://arxiv.org/abs/1905.07129v3},
File          = {1905.07129v3.pdf}
}
@article{2009.04404v1,
Author        = {Gilles Vandewiele and Bram Steenwinckel and Pieter Bonte and Michael Weyns and Heiko Paulheim and Petar Ristoski and Filip De Turck and Femke Ongenae},
Title         = {Walk Extraction Strategies for Node Embeddings with RDF2Vec in Knowledge
  Graphs},
Eprint        = {2009.04404v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {As KGs are symbolic constructs, specialized techniques have to be applied in
order to make them compatible with data mining techniques. RDF2Vec is an
unsupervised technique that can create task-agnostic numerical representations
of the nodes in a KG by extending successful language modelling techniques. The
original work proposed the Weisfeiler-Lehman (WL) kernel to improve the quality
of the representations. However, in this work, we show both formally and
empirically that the WL kernel does little to improve walk embeddings in the
context of a single KG. As an alternative to the WL kernel, we propose five
different strategies to extract information complementary to basic random
walks. We compare these walks on several benchmark datasets to show that the
\emph{n-gram} strategy performs best on average on node classification tasks
and that tuning the walk strategy can result in improved predictive
performances.},
Year          = {2020},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2009.04404v1},
File          = {2009.04404v1.pdf}
}
@article{2012.07379v2,
Author        = {Tianyang Cao and Shuang Zeng and Songge Zhao and Mairgup Mansur and Baobao Chang},
Title         = {Generating Math Word Problems from Equations with Topic Controlling and
  Commonsense Enforcement},
Eprint        = {2012.07379v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent years have seen significant advancement in text generation tasks with
the help of neural language models. However, there exists a challenging task:
generating math problem text based on mathematical equations, which has made
little progress so far. In this paper, we present a novel equation-to-problem
text generation model. In our model, 1) we propose a flexible scheme to
effectively encode math equations, we then enhance the equation encoder by a
Varitional Autoen-coder (VAE) 2) given a math equation, we perform topic
selection, followed by which a dynamic topic memory mechanism is introduced to
restrict the topic distribution of the generator 3) to avoid commonsense
violation in traditional generation model, we pretrain word embedding with
background knowledge graph (KG), and we link decoded words to related words in
KG, targeted at injecting background knowledge into our model. We evaluate our
model through both automatic metrices and human evaluation, experiments
demonstrate our model outperforms baseline and previous models in both accuracy
and richness of generated problem text.},
Year          = {2020},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2012.07379v2},
File          = {2012.07379v2.pdf}
}
@article{2101.00419v2,
Author        = {Yiran Xing and Zai Shi and Zhao Meng and Gerhard Lakemeyer and Yunpu Ma and Roger Wattenhofer},
Title         = {KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense
  Generation},
Eprint        = {2101.00419v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present Knowledge Enhanced Multimodal BART (KM-BART), which is a
Transformer-based sequence-to-sequence model capable of reasoning about
commonsense knowledge from multimodal inputs of images and texts. We adapt the
generative BART architecture to a multimodal model with visual and textual
inputs. We further develop novel pretraining tasks to improve the model
performance on the Visual Commonsense Generation (VCG) task. In particular, our
pretraining task of Knowledge-based Commonsense Generation (KCG) boosts model
performance on the VCG task by leveraging commonsense knowledge from a large
language model pretrained on external commonsense knowledge graphs. To the best
of our knowledge, we are the first to propose a dedicated task for improving
model performance on the VCG task. Experimental results show that our model
reaches state-of-the-art performance on the VCG task by applying these novel
pretraining tasks.},
Year          = {2021},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2101.00419v2},
File          = {2101.00419v2.pdf}
}
@article{2104.08455v2,
Author        = {Nouha Dziri and Andrea Madotto and Osmar Zaiane and Avishek Joey Bose},
Title         = {Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path
  Grounding},
Eprint        = {2104.08455v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Dialogue systems powered by large pre-trained language models (LM) exhibit an
innate ability to deliver fluent and natural-looking responses. Despite their
impressive generation performance, these models can often generate factually
incorrect statements impeding their widespread adoption. In this paper, we
focus on the task of improving the faithfulness -- and thus reduce
hallucination -- of Neural Dialogue Systems to known facts supplied by a
Knowledge Graph (KG). We propose Neural Path Hunter which follows a
generate-then-refine strategy whereby a generated response is amended using the
k-hop subgraph of a KG. Neural Path Hunter leverages a separate token-level
fact critic to identify plausible sources of hallucination followed by a
refinement stage consisting of a chain of two neural LM's that retrieves
correct entities by crafting a query signal that is propagated over the k-hop
subgraph. Our proposed model can easily be applied to any dialogue generated
responses without retraining the model. We empirically validate our proposed
approach on the OpenDialKG dataset against a suite of metrics and report a
relative improvement of faithfulness over dialogue responses by 20.35% based on
FeQA (Durmus et al., 2020).},
Year          = {2021},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2104.08455v2},
File          = {2104.08455v2.pdf}
}
@article{2107.13435v2,
Author        = {Zhenwen Liang and Jipeng Zhang and Lei Wang and Wei Qin and Yunshi Lan and Jie Shao and Xiangliang Zhang},
Title         = {MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving},
Eprint        = {2107.13435v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Math word problem (MWP) solving faces a dilemma in number representation
learning. In order to avoid the number representation issue and reduce the
search space of feasible solutions, existing works striving for MWP solving
usually replace real numbers with symbolic placeholders to focus on logic
reasoning. However, different from common symbolic reasoning tasks like program
synthesis and knowledge graph reasoning, MWP solving has extra requirements in
numerical reasoning. In other words, instead of the number value itself, it is
the reusable numerical property that matters more in numerical reasoning.
Therefore, we argue that injecting numerical properties into symbolic
placeholders with contextualized representation learning schema can provide a
way out of the dilemma in the number representation issue here. In this work,
we introduce this idea to the popular pre-training language model (PLM)
techniques and build MWP-BERT, an effective contextual number representation
PLM. We demonstrate the effectiveness of our MWP-BERT on MWP solving and
several MWP-specific understanding tasks on both English and Chinese
benchmarks.},
Year          = {2021},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2107.13435v2},
File          = {2107.13435v2.pdf}
}
@article{2108.09241v2,
Author        = {Jie Huang and Kevin Chen-Chuan Chang and Jinjun Xiong and Wen-mei Hwu},
Title         = {Open Relation Modeling: Learning to Define Relations between Entities},
Eprint        = {2108.09241v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Relations between entities can be represented by different instances, e.g., a
sentence containing both entities or a fact in a Knowledge Graph (KG). However,
these instances may not well capture the general relations between entities,
may be difficult to understand by humans, even may not be found due to the
incompleteness of the knowledge source. In this paper, we introduce the Open
Relation Modeling problem - given two entities, generate a coherent sentence
describing the relation between them. To solve this problem, we propose to
teach machines to generate definition-like relation descriptions by letting
them learn from defining entities. Specifically, we fine-tune Pre-trained
Language Models (PLMs) to produce definitions conditioned on extracted entity
pairs. To help PLMs reason between entities and provide additional relational
knowledge to PLMs for open relation modeling, we incorporate reasoning paths in
KGs and include a reasoning path selection mechanism. Experimental results show
that our model can generate concise but informative relation descriptions that
capture the representative characteristics of entities.},
Year          = {2021},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2108.09241v2},
File          = {2108.09241v2.pdf}
}
@article{2112.08596v2,
Author        = {Xiangyu Peng and Kaige Xie and Amal Alabdulkarim and Harshith Kayam and Samihan Dani and Mark O. Riedl},
Title         = {Guiding Neural Story Generation with Reader Models},
Eprint        = {2112.08596v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Automated storytelling has long captured the attention of researchers for the
ubiquity of narratives in everyday life. However, it is challenging to maintain
coherence and stay on-topic toward a specific ending when generating narratives
with neural language models. In this paper, we introduce Story generation with
Reader Models (StoRM), a framework in which a reader model is used to reason
about the story should progress. A reader model infers what a human reader
believes about the concepts, entities, and relations about the fictional story
world. We show how an explicit reader model represented as a knowledge graph
affords story coherence and provides controllability in the form of achieving a
given story world state goal. Experiments show that our model produces
significantly more coherent and on-topic stories, outperforming baselines in
dimensions including plot plausibility and staying on topic.},
Year          = {2021},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2112.08596v2},
File          = {2112.08596v2.pdf}
}
@article{2203.08517v1,
Author        = {Chao-Hong Tan and Jia-Chen Gu and Chongyang Tao and Zhen-Hua Ling and Can Xu and Huang Hu and Xiubo Geng and Daxin Jiang},
Title         = {TegTok: Augmenting Text Generation via Task-specific and Open-world
  Knowledge},
Eprint        = {2203.08517v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Generating natural and informative texts has been a long-standing problem in
NLP. Much effort has been dedicated into incorporating pre-trained language
models (PLMs) with various open-world knowledge, such as knowledge graphs or
wiki pages. However, their ability to access and manipulate the task-specific
knowledge is still limited on downstream tasks, as this type of knowledge is
usually not well covered in PLMs and is hard to acquire. To address the
problem, we propose augmenting TExt Generation via Task-specific and Open-world
Knowledge (TegTok) in a unified framework. Our model selects knowledge entries
from two types of knowledge sources through dense retrieval and then injects
them into the input encoding and output decoding stages respectively on the
basis of PLMs. With the help of these two types of knowledge, our model can
learn what and how to generate. Experiments on two text generation tasks of
dialogue generation and question generation, and on two datasets show that our
method achieves better performance than various baseline models.},
Year          = {2022},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2203.08517v1},
File          = {2203.08517v1.pdf}
}
@article{2204.04437v1,
Author        = {Xinnian Liang and Shuangzhi Wu and Mu Li and Zhoujun Li},
Title         = {Modeling Multi-Granularity Hierarchical Features for Relation Extraction},
Eprint        = {2204.04437v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Relation extraction is a key task in Natural Language Processing (NLP), which
aims to extract relations between entity pairs from given texts. Recently,
relation extraction (RE) has achieved remarkable progress with the development
of deep neural networks. Most existing research focuses on constructing
explicit structured features using external knowledge such as knowledge graph
and dependency tree. In this paper, we propose a novel method to extract
multi-granularity features based solely on the original input sentences. We
show that effective structured features can be attained even without external
knowledge. Three kinds of features based on the input sentences are fully
exploited, which are in entity mention level, segment level, and sentence
level. All the three are jointly and hierarchically modeled. We evaluate our
method on three public benchmarks: SemEval 2010 Task 8, Tacred, and Tacred
Revisited. To verify the effectiveness, we apply our method to different
encoders such as LSTM and BERT. Experimental results show that our method
significantly outperforms existing state-of-the-art models that even use
external knowledge. Extensive analyses demonstrate that the performance of our
model is contributed by the capture of multi-granularity features and the model
of their hierarchical structure. Code and data are available at
\url{https://github.com/xnliang98/sms}.},
Year          = {2022},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2204.04437v1},
File          = {2204.04437v1.pdf}
}
@article{2204.04779v2,
Author        = {Saadullah Amin and Pasquale Minervini and David Chang and Pontus Stenetorp and Günter Neumann},
Title         = {MedDistant19: Towards an Accurate Benchmark for Broad-Coverage
  Biomedical Relation Extraction},
Eprint        = {2204.04779v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Relation extraction in the biomedical domain is challenging due to the lack
of labeled data and high annotation costs, needing domain experts. Distant
supervision is commonly used to tackle the scarcity of annotated data by
automatically pairing knowledge graph relationships with raw texts. Such a
pipeline is prone to noise and has added challenges to scale for covering a
large number of biomedical concepts. We investigated existing broad-coverage
distantly supervised biomedical relation extraction benchmarks and found a
significant overlap between training and test relationships ranging from 26% to
86%. Furthermore, we noticed several inconsistencies in the data construction
process of these benchmarks, and where there is no train-test leakage, the
focus is on interactions between narrower entity types. This work presents a
more accurate benchmark MedDistant19 for broad-coverage distantly supervised
biomedical relation extraction that addresses these shortcomings and is
obtained by aligning the MEDLINE abstracts with the widely used SNOMED Clinical
Terms knowledge base. Lacking thorough evaluation with domain-specific language
models, we also conduct experiments validating general domain relation
extraction findings to biomedical relation extraction.},
Year          = {2022},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2204.04779v2},
File          = {2204.04779v2.pdf}
}
@article{2204.12793v3,
Author        = {Debayan Banerjee and Pranav Ajit Nair and Jivat Neet Kaur and Ricardo Usbeck and Chris Biemann},
Title         = {Modern Baselines for SPARQL Semantic Parsing},
Eprint        = {2204.12793v3},
DOI           = {10.1145/3477495.3531841},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {In this work, we focus on the task of generating SPARQL queries from natural
language questions, which can then be executed on Knowledge Graphs (KGs). We
assume that gold entity and relations have been provided, and the remaining
task is to arrange them in the right order along with SPARQL vocabulary, and
input tokens to produce the correct SPARQL query. Pre-trained Language Models
(PLMs) have not been explored in depth on this task so far, so we experiment
with BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings,
looking for new baselines in the PLM era for this task, on DBpedia and Wikidata
KGs. We show that T5 requires special input tokenisation, but produces state of
the art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms
task-specific models from previous works. Moreover, the methods enable semantic
parsing for questions where a part of the input needs to be copied to the
output query, thus enabling a new paradigm in KG semantic parsing.},
Year          = {2022},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2204.12793v3},
File          = {2204.12793v3.pdf}
}
@article{2205.01841v1,
Author        = {Jinhao Jiang and Kun Zhou and Wayne Xin Zhao and Ji-Rong Wen},
Title         = {Great Truths are Always Simple: A Rather Simple Knowledge Encoder for
  Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models},
Eprint        = {2205.01841v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Commonsense reasoning in natural language is a desired ability of artificial
intelligent systems. For solving complex commonsense reasoning tasks, a typical
solution is to enhance pre-trained language models~(PTMs) with a
knowledge-aware graph neural network~(GNN) encoder that models a commonsense
knowledge graph~(CSKG). Despite the effectiveness, these approaches are built
on heavy architectures, and can't clearly explain how external knowledge
resources improve the reasoning capacity of PTMs. Considering this issue, we
conduct a deep empirical analysis, and find that it is indeed relation features
from CSKGs (but not node features) that mainly contribute to the performance
improvement of PTMs. Based on this finding, we design a simple MLP-based
knowledge encoder that utilizes statistical relation paths as features.
Extensive experiments conducted on five benchmarks demonstrate the
effectiveness of our approach, which also largely reduces the parameters for
encoding CSKGs. Our codes and data are publicly available at
https://github.com/RUCAIBox/SAFE.},
Year          = {2022},
Month         = {May},
Url           = {http://arxiv.org/abs/2205.01841v1},
File          = {2205.01841v1.pdf}
}
@article{2206.05123v3,
Author        = {Sheng Zhang and Patrick Ng and Zhiguo Wang and Bing Xiang},
Title         = {REKnow: Enhanced Knowledge for Joint Entity and Relation Extraction},
Eprint        = {2206.05123v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Relation extraction is an important but challenging task that aims to extract
all hidden relational facts from the text. With the development of deep
language models, relation extraction methods have achieved good performance on
various benchmarks. However, we observe two shortcomings of previous methods:
first, there is no unified framework that works well under various relation
extraction settings; second, effectively utilizing external knowledge as
background information is absent. In this work, we propose a knowledge-enhanced
generative model to mitigate these two issues. Our generative model is a
unified framework to sequentially generate relational triplets under various
relation extraction settings and explicitly utilizes relevant knowledge from
Knowledge Graph (KG) to resolve ambiguities. Our model achieves superior
performance on multiple benchmarks and settings, including WebNLG, NYT10, and
TACRED.},
Year          = {2022},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2206.05123v3},
File          = {2206.05123v3.pdf}
}
@article{2207.08286v1,
Author        = {William Hogan},
Title         = {An Overview of Distant Supervision for Relation Extraction with a Focus
  on Denoising and Pre-training Methods},
Eprint        = {2207.08286v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Relation Extraction (RE) is a foundational task of natural language
processing. RE seeks to transform raw, unstructured text into structured
knowledge by identifying relational information between entity pairs found in
text. RE has numerous uses, such as knowledge graph completion, text
summarization, question-answering, and search querying. The history of RE
methods can be roughly organized into four phases: pattern-based RE,
statistical-based RE, neural-based RE, and large language model-based RE. This
survey begins with an overview of a few exemplary works in the earlier phases
of RE, highlighting limitations and shortcomings to contextualize progress.
Next, we review popular benchmarks and critically examine metrics used to
assess RE performance. We then discuss distant supervision, a paradigm that has
shaped the development of modern RE methods. Lastly, we review recent RE works
focusing on denoising and pre-training methods.},
Year          = {2022},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2207.08286v1},
File          = {2207.08286v1.pdf}
}
@article{2207.12888v2,
Author        = {Zhuo Chen and Yufeng Huang and Jiaoyan Chen and Yuxia Geng and Yin Fang and Jeff Pan and Ningyu Zhang and Wen Zhang},
Title         = {LaKo: Knowledge-driven Visual Question Answering via Late
  Knowledge-to-Text Injection},
Eprint        = {2207.12888v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Visual question answering (VQA) often requires an understanding of visual
concepts and language semantics, which relies on external knowledge. Most
existing methods exploit pre-trained language models or/and unstructured text,
but the knowledge in these resources are often incomplete and noisy. Some other
methods prefer to use knowledge graphs (KGs) which often have intensive
structured knowledge, but the research is still quite preliminary. In this
paper, we propose LaKo, a knowledge-driven VQA method via Late
Knowledge-to-text Injection. To effectively incorporate an external KG, we
transfer triples into textual format and propose a late injection mechanism for
knowledge fusion. Finally we address VQA as a text generation task with an
effective encoder-decoder paradigm, which achieves state-of-the-art results on
OKVQA dataset.},
Year          = {2022},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2207.12888v2},
File          = {2207.12888v2.pdf}
}
@article{2207.14094v2,
Author        = {Russa Biswas and Jan Portisch and Heiko Paulheim and Harald Sack and Mehwish Alam},
Title         = {Entity Type Prediction Leveraging Graph Walks and Entity Descriptions},
Eprint        = {2207.14094v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The entity type information in Knowledge Graphs (KGs) such as DBpedia,
Freebase, etc. is often incomplete due to automated generation or human
curation. Entity typing is the task of assigning or inferring the semantic type
of an entity in a KG. This paper presents \textit{GRAND}, a novel approach for
entity typing leveraging different graph walk strategies in RDF2vec together
with textual entity descriptions. RDF2vec first generates graph walks and then
uses a language model to obtain embeddings for each node in the graph. This
study shows that the walk generation strategy and the embedding model have a
significant effect on the performance of the entity typing task. The proposed
approach outperforms the baseline approaches on the benchmark datasets DBpedia
and FIGER for entity typing in KGs for both fine-grained and coarse-grained
classes. The results show that the combination of order-aware RDF2vec variants
together with the contextual embeddings of the textual entity descriptions
achieve the best results.},
Year          = {2022},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2207.14094v2},
File          = {2207.14094v2.pdf}
}
@article{2209.08284v3,
Author        = {Yujie Lu and Siqi Ouyang and Kairui Zhou},
Title         = {Structured Knowledge Grounding for Question Answering},
Eprint        = {2209.08284v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Can language models (LM) ground question-answering (QA) tasks in the
knowledge base via inherent relational reasoning ability? While previous models
that use only LMs have seen some success on many QA tasks, more recent methods
include knowledge graphs (KG) to complement LMs with their more logic-driven
implicit knowledge. However, effectively extracting information from structured
data, like KGs, empowers LMs to remain an open question, and current models
rely on graph techniques to extract knowledge. In this paper, we propose to
solely leverage the LMs to combine the language and knowledge for knowledge
based question-answering with flexibility, breadth of coverage and structured
reasoning. Specifically, we devise a knowledge construction method that
retrieves the relevant context with a dynamic hop, which expresses more
comprehensivenes than traditional GNN-based techniques. And we devise a deep
fusion mechanism to further bridge the information exchanging bottleneck
between the language and the knowledge. Extensive experiments show that our
model consistently demonstrates its state-of-the-art performance over
CommensenseQA benchmark, showcasing the possibility to leverage LMs solely to
robustly ground QA into the knowledge base.},
Year          = {2022},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2209.08284v3},
File          = {2209.08284v3.pdf}
}
@article{2209.14812v1,
Author        = {Aneta Koleva and Martin Ringsquandl and Mark Buckley and Rakebul Hasan and Volker Tresp},
Title         = {Named Entity Recognition in Industrial Tables using Tabular Language
  Models},
Eprint        = {2209.14812v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Specialized transformer-based models for encoding tabular data have gained
interest in academia. Although tabular data is omnipresent in industry,
applications of table transformers are still missing. In this paper, we study
how these models can be applied to an industrial Named Entity Recognition (NER)
problem where the entities are mentioned in tabular-structured spreadsheets.
The highly technical nature of spreadsheets as well as the lack of labeled data
present major challenges for fine-tuning transformer-based models. Therefore,
we develop a dedicated table data augmentation strategy based on available
domain-specific knowledge graphs. We show that this boosts performance in our
low-resource scenario considerably. Further, we investigate the benefits of
tabular structure as inductive bias compared to tables as linearized sequences.
Our experiments confirm that a table transformer outperforms other baselines
and that its tabular inductive bias is vital for convergence of
transformer-based models.},
Year          = {2022},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2209.14812v1},
File          = {2209.14812v1.pdf}
}
@article{2210.13952v5,
Author        = {Gaetano Rossiello and Md Faisal Mahbub Chowdhury and Nandana Mihindukulasooriya and Owen Cornec and Alfio Massimiliano Gliozzo},
Title         = {KnowGL: Knowledge Generation and Linking from Text},
Eprint        = {2210.13952v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We propose KnowGL, a tool that allows converting text into structured
relational data represented as a set of ABox assertions compliant with the TBox
of a given Knowledge Graph (KG), such as Wikidata. We address this problem as a
sequence generation task by leveraging pre-trained sequence-to-sequence
language models, e.g. BART. Given a sentence, we fine-tune such models to
detect pairs of entity mentions and jointly generate a set of facts consisting
of the full set of semantic annotations for a KG, such as entity labels, entity
types, and their relationships. To showcase the capabilities of our tool, we
build a web application consisting of a set of UI widgets that help users to
navigate through the semantic data extracted from a given input text. We make
the KnowGL model available at https://huggingface.co/ibm/knowgl-large.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.13952v5},
File          = {2210.13952v5.pdf}
}
@article{2212.01588v2,
Author        = {Ziwei Ji and Zihan Liu and Nayeon Lee and Tiezheng Yu and Bryan Wilie and Min Zeng and Pascale Fung},
Title         = {RHO ($ρ$): Reducing Hallucination in Open-domain Dialogues with
  Knowledge Grounding},
Eprint        = {2212.01588v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Dialogue systems can leverage large pre-trained language models and knowledge
to generate fluent and informative responses. However, these models are still
prone to produce hallucinated responses not supported by the input source,
which greatly hinders their application. The heterogeneity between external
knowledge and dialogue context challenges representation learning and source
integration, and further contributes to unfaithfulness. To handle this
challenge and generate more faithful responses, this paper presents RHO
($\rho$) utilizing the representations of linked entities and relation
predicates from a knowledge graph (KG). We propose (1) local knowledge
grounding to combine textual embeddings with the corresponding KG embeddings;
and (2) global knowledge grounding to equip RHO with multi-hop reasoning
abilities via the attention mechanism. In addition, we devise a response
re-ranking technique based on walks over KG sub-graphs for better
conversational reasoning. Experimental results on OpenDialKG show that our
approach significantly outperforms state-of-the-art methods on both automatic
and human evaluation by a large margin, especially in hallucination reduction
(17.54% in FeQA).},
Year          = {2022},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2212.01588v2},
File          = {2212.01588v2.pdf}
}
@article{2212.02271v1,
Author        = {Yu Zhang and Yunyi Zhang and Yucheng Jiang and Martin Michalski and Yu Deng and Lucian Popa and ChengXiang Zhai and Jiawei Han},
Title         = {Entity Set Co-Expansion in StackOverflow},
Eprint        = {2212.02271v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Given a few seed entities of a certain type (e.g., Software or Programming
Language), entity set expansion aims to discover an extensive set of entities
that share the same type as the seeds. Entity set expansion in software-related
domains such as StackOverflow can benefit many downstream tasks (e.g., software
knowledge graph construction) and facilitate better IT operations and service
management. Meanwhile, existing approaches are less concerned with two
problems: (1) How to deal with multiple types of seed entities simultaneously?
(2) How to leverage the power of pre-trained language models (PLMs)? Being
aware of these two problems, in this paper, we study the entity set
co-expansion task in StackOverflow, which extracts Library, OS, Application,
and Language entities from StackOverflow question-answer threads. During the
co-expansion process, we use PLMs to derive embeddings of candidate entities
for calculating similarities between entities. Experimental results show that
our proposed SECoExpan framework outperforms previous approaches significantly.},
Year          = {2022},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2212.02271v1},
File          = {2212.02271v1.pdf}
}
@article{2301.01664v2,
Author        = {Zhixiang Su and Di Wang and Chunyan Miao and Lizhen Cui},
Title         = {Multi-Aspect Explainable Inductive Relation Prediction by Sentence
  Transformer},
Eprint        = {2301.01664v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent studies on knowledge graphs (KGs) show that path-based methods
empowered by pre-trained language models perform well in the provision of
inductive and explainable relation predictions. In this paper, we introduce the
concepts of relation path coverage and relation path confidence to filter out
unreliable paths prior to model training to elevate the model performance.
Moreover, we propose Knowledge Reasoning Sentence Transformer (KRST) to predict
inductive relations in KGs. KRST is designed to encode the extracted reliable
paths in KGs, allowing us to properly cluster paths and provide multi-aspect
explanations. We conduct extensive experiments on three real-world datasets.
The experimental results show that compared to SOTA models, KRST achieves the
best performance in most transductive and inductive test cases (4 of 6), and in
11 of 12 few-shot test cases.},
Year          = {2023},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2301.01664v2},
File          = {2301.01664v2.pdf}
}
@article{2301.13154v2,
Author        = {Hong-Yu Zhou and Yunxiang Fu and Zhicheng Zhang and Cheng Bian and Yizhou Yu},
Title         = {Protein Representation Learning via Knowledge Enhanced Primary Structure
  Modeling},
Eprint        = {2301.13154v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Protein representation learning has primarily benefited from the remarkable
development of language models (LMs). Accordingly, pre-trained protein models
also suffer from a problem in LMs: a lack of factual knowledge. The recent
solution models the relationships between protein and associated knowledge
terms as the knowledge encoding objective. However, it fails to explore the
relationships at a more granular level, i.e., the token level. To mitigate
this, we propose Knowledge-exploited Auto-encoder for Protein (KeAP), which
performs token-level knowledge graph exploration for protein representation
learning. In practice, non-masked amino acids iteratively query the associated
knowledge tokens to extract and integrate helpful information for restoring
masked amino acids via attention. We show that KeAP can consistently outperform
the previous counterpart on 9 representative downstream applications, sometimes
surpassing it by large margins. These results suggest that KeAP provides an
alternative yet effective way to perform knowledge enhanced protein
representation learning.},
Year          = {2023},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2301.13154v2},
File          = {2301.13154v2.pdf}
}
@article{2303.12320v2,
Author        = {Dhaval Taunk and Lakshya Khanna and Pavan Kandru and Vasudeva Varma and Charu Sharma and Makarand Tapaswi},
Title         = {GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering},
Eprint        = {2303.12320v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Commonsense question-answering (QA) methods combine the power of pre-trained
Language Models (LM) with the reasoning provided by Knowledge Graphs (KG). A
typical approach collects nodes relevant to the QA pair from a KG to form a
Working Graph (WG) followed by reasoning using Graph Neural Networks(GNNs).
This faces two major challenges: (i) it is difficult to capture all the
information from the QA in the WG, and (ii) the WG contains some irrelevant
nodes from the KG. To address these, we propose GrapeQA with two simple
improvements on the WG: (i) Prominent Entities for Graph Augmentation
identifies relevant text chunks from the QA pair and augments the WG with
corresponding latent representations from the LM, and (ii) Context-Aware Node
Pruning removes nodes that are less relevant to the QA pair. We evaluate our
results on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows
consistent improvements over its LM + KG predecessor (QA-GNN in particular) and
large improvements on OpenBookQA.},
Year          = {2023},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2303.12320v2},
File          = {2303.12320v2.pdf}
}
@article{2304.05973v1,
Author        = {Jiaying Lu and Jiaming Shen and Bo Xiong and Wenjing Ma and Steffen Staab and Carl Yang},
Title         = {HiPrompt: Few-Shot Biomedical Knowledge Fusion via Hierarchy-Oriented
  Prompting},
Eprint        = {2304.05973v1},
DOI           = {10.1145/3539618.3591997},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Medical decision-making processes can be enhanced by comprehensive biomedical
knowledge bases, which require fusing knowledge graphs constructed from
different sources via a uniform index system. The index system often organizes
biomedical terms in a hierarchy to provide the aligned entities with
fine-grained granularity. To address the challenge of scarce supervision in the
biomedical knowledge fusion (BKF) task, researchers have proposed various
unsupervised methods. However, these methods heavily rely on ad-hoc lexical and
structural matching algorithms, which fail to capture the rich semantics
conveyed by biomedical entities and terms. Recently, neural embedding models
have proved effective in semantic-rich tasks, but they rely on sufficient
labeled data to be adequately trained. To bridge the gap between the
scarce-labeled BKF and neural embedding models, we propose HiPrompt, a
supervision-efficient knowledge fusion framework that elicits the few-shot
reasoning ability of large language models through hierarchy-oriented prompts.
Empirical results on the collected KG-Hi-BKF benchmark datasets demonstrate the
effectiveness of HiPrompt.},
Year          = {2023},
Month         = {Apr},
Note          = {In Proceedings of the 46th International ACM SIGIR Conference on
  Research and Development in Information Retrieval (Short-Paper Track), 2023},
Url           = {http://arxiv.org/abs/2304.05973v1},
File          = {2304.05973v1.pdf}
}
@article{2305.04082v2,
Author        = {Dongwon Kelvin Ryu and Meng Fang and Shirui Pan and Gholamreza Haffari and Ehsan Shareghi},
Title         = {A Minimal Approach for Natural Language Action Space in Text-based Games},
Eprint        = {2305.04082v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Text-based games (TGs) are language-based interactive environments for
reinforcement learning. While language models (LMs) and knowledge graphs (KGs)
are commonly used for handling large action space in TGs, it is unclear whether
these techniques are necessary or overused. In this paper, we revisit the
challenge of exploring the action space in TGs and propose $
\epsilon$-admissible exploration, a minimal approach of utilizing admissible
actions, for training phase. Additionally, we present a text-based actor-critic
(TAC) agent that produces textual commands for game, solely from game
observations, without requiring any KG or LM. Our method, on average across 10
games from Jericho, outperforms strong baselines and state-of-the-art agents
that use LM and KG. Our approach highlights that a much lighter model design,
with a fresh perspective on utilizing the information within the environments,
suffices for an effective exploration of exponentially large action spaces.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.04082v2},
File          = {2305.04082v2.pdf}
}
@article{2305.06294v4,
Author        = {Hongbo Zhang and Chen Tang and Tyler Loakman and Bohao Yang and Stefan Goetze and Chenghua Lin},
Title         = {CADGE: Context-Aware Dialogue Generation Enhanced with Graph-Structured
  Knowledge Aggregation},
Eprint        = {2305.06294v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Commonsense knowledge is crucial to many natural language processing tasks.
Existing works usually incorporate graph knowledge with conventional graph
neural networks (GNNs), resulting in a sequential pipeline that
compartmentalizes the encoding processes for textual and graph-based knowledge.
This compartmentalization does, however, not fully exploit the contextual
interplay between these two types of input knowledge. In this paper, a novel
context-aware graph-attention model (Context-aware GAT) is proposed, designed
to effectively assimilate global features from relevant knowledge graphs
through a context-enhanced knowledge aggregation mechanism. Specifically, the
proposed framework employs an innovative approach to representation learning
that harmonizes heterogeneous features by amalgamating flattened graph
knowledge with text data. The hierarchical application of graph knowledge
aggregation within connected subgraphs, complemented by contextual information,
to bolster the generation of commonsense-driven dialogues is analyzed.
Empirical results demonstrate that our framework outperforms conventional
GNN-based language models in terms of performance. Both, automated and human
evaluations affirm the significant performance enhancements achieved by our
proposed model over the concept flow baseline.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.06294v4},
File          = {2305.06294v4.pdf}
}
@article{2305.15108v1,
Author        = {Debayan Banerjee and Pranav Ajit Nair and Ricardo Usbeck and Chris Biemann},
Title         = {The Role of Output Vocabulary in T2T LMs for SPARQL Semantic Parsing},
Eprint        = {2305.15108v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this work, we analyse the role of output vocabulary for text-to-text (T2T)
models on the task of SPARQL semantic parsing. We perform experiments within
the the context of knowledge graph question answering (KGQA), where the task is
to convert questions in natural language to the SPARQL query language. We
observe that the query vocabulary is distinct from human vocabulary. Language
Models (LMs) are pre-dominantly trained for human language tasks, and hence, if
the query vocabulary is replaced with a vocabulary more attuned to the LM
tokenizer, the performance of models may improve. We carry out carefully
selected vocabulary substitutions on the queries and find absolute gains in the
range of 17% on the GrailQA dataset.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.15108v1},
File          = {2305.15108v1.pdf}
}
@article{2306.02051v3,
Author        = {Xiaoyan Zhao and Yang Deng and Min Yang and Lingzhi Wang and Rui Zhang and Hong Cheng and Wai Lam and Ying Shen and Ruifeng Xu},
Title         = {A Comprehensive Survey on Relation Extraction: Recent Advances and New
  Frontiers},
Eprint        = {2306.02051v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Relation extraction (RE) involves identifying the relations between entities
from underlying content. RE serves as the foundation for many natural language
processing (NLP) and information retrieval applications, such as knowledge
graph completion and question answering. In recent years, deep neural networks
have dominated the field of RE and made noticeable progress. Subsequently, the
large pre-trained language models have taken the state-of-the-art RE to a new
level. This survey provides a comprehensive review of existing deep learning
techniques for RE. First, we introduce RE resources, including datasets and
evaluation metrics. Second, we propose a new taxonomy to categorize existing
works from three perspectives, i.e., text representation, context encoding, and
triplet prediction. Third, we discuss several important challenges faced by RE
and summarize potential techniques to tackle these challenges. Finally, we
outline some promising future directions and prospects in this field. This
survey is expected to facilitate researchers' collaborative efforts to address
the challenges of real-world RE systems.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.02051v3},
File          = {2306.02051v3.pdf}
}
@article{2306.02871v1,
Author        = {Sondre Wold and Lilja Øvrelid and Erik Velldal},
Title         = {Text-To-KG Alignment: Comparing Current Methods on Classification Tasks},
Eprint        = {2306.02871v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In contrast to large text corpora, knowledge graphs (KG) provide dense and
structured representations of factual information. This makes them attractive
for systems that supplement or ground the knowledge found in pre-trained
language models with an external knowledge source. This has especially been the
case for classification tasks, where recent work has focused on creating
pipeline models that retrieve information from KGs like ConceptNet as
additional context. Many of these models consist of multiple components, and
although they differ in the number and nature of these parts, they all have in
common that for some given text query, they attempt to identify and retrieve a
relevant subgraph from the KG. Due to the noise and idiosyncrasies often found
in KGs, it is not known how current methods compare to a scenario where the
aligned subgraph is completely relevant to the query. In this work, we try to
bridge this knowledge gap by reviewing current approaches to text-to-KG
alignment and evaluating them on two datasets where manually created graphs are
available, providing insights into the effectiveness of current methods.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.02871v1},
File          = {2306.02871v1.pdf}
}
@article{2309.07974v1,
Author        = {Jack Lanchantin and Sainbayar Sukhbaatar and Gabriel Synnaeve and Yuxuan Sun and Kavya Srinet and Arthur Szlam},
Title         = {A Data Source for Reasoning Embodied Agents},
Eprint        = {2309.07974v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Recent progress in using machine learning models for reasoning tasks has been
driven by novel model architectures, large-scale pre-training protocols, and
dedicated reasoning datasets for fine-tuning. In this work, to further pursue
these advances, we introduce a new data generator for machine reasoning that
integrates with an embodied agent. The generated data consists of templated
text queries and answers, matched with world-states encoded into a database.
The world-states are a result of both world dynamics and the actions of the
agent. We show the results of several baseline models on instantiations of
train sets. These include pre-trained language models fine-tuned on a
text-formatted representation of the database, and graph-structured
Transformers operating on a knowledge-graph representation of the database. We
find that these models can answer some questions about the world-state, but
struggle with others. These results hint at new research directions in
designing neural reasoning models and database representations. Code to
generate the data will be released at github.com/facebookresearch/neuralmemory},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.07974v1},
File          = {2309.07974v1.pdf}
}
@article{2309.10015v1,
Author        = {Christopher Richardson and Anirudh Sundar and Larry Heck},
Title         = {SYNDICOM: Improving Conversational Commonsense with Error-Injection and
  Natural Language Feedback},
Eprint        = {2309.10015v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Commonsense reasoning is a critical aspect of human communication. Despite
recent advances in conversational AI driven by large language models,
commonsense reasoning remains a challenging task. In this work, we introduce
SYNDICOM - a method for improving commonsense in dialogue response generation.
SYNDICOM consists of two components. The first component is a dataset composed
of commonsense dialogues created from a knowledge graph and synthesized into
natural language. This dataset includes both valid and invalid responses to
dialogue contexts, along with natural language feedback (NLF) for the invalid
responses. The second contribution is a two-step procedure: training a model to
predict natural language feedback (NLF) for invalid responses, and then
training a response generation model conditioned on the predicted NLF, the
invalid response, and the dialogue. SYNDICOM is scalable and does not require
reinforcement learning. Empirical results on three tasks are evaluated using a
broad range of metrics. SYNDICOM achieves a relative improvement of 53% over
ChatGPT on ROUGE1, and human evaluators prefer SYNDICOM over ChatGPT 57% of the
time. We will publicly release the code and the full dataset.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.10015v1},
File          = {2309.10015v1.pdf}
}
@article{2310.12379v1,
Author        = {Nitesh Kumar and Steven Schockaert},
Title         = {Solving Hard Analogy Questions with Relation Embedding Chains},
Eprint        = {2310.12379v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Modelling how concepts are related is a central topic in Lexical Semantics. A
common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to
model the relation between two concepts as a set of paths. However, KGs are
limited to a fixed set of relation types, and they are incomplete and often
noisy. Another strategy is to distill relation embeddings from a fine-tuned
language model. However, this is less suitable for words that are only
indirectly related and it does not readily allow us to incorporate structured
domain knowledge. In this paper, we aim to combine the best of both worlds. We
model relations as paths but associate their edges with relation embeddings.
The paths are obtained by first identifying suitable intermediate words and
then selecting those words for which informative relation embeddings can be
obtained. We empirically show that our proposed representations are useful for
solving hard analogy questions.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.12379v1},
File          = {2310.12379v1.pdf}
}
@article{2312.02334v1,
Author        = {Steve Fonin Mbouadeu and Martin Lorenzo and Ken Barker and Oktie Hassanzadeh},
Title         = {An Evaluation Framework for Mapping News Headlines to Event Classes in a
  Knowledge Graph},
Eprint        = {2312.02334v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Mapping ongoing news headlines to event-related classes in a rich knowledge
base can be an important component in a knowledge-based event analysis and
forecasting solution. In this paper, we present a methodology for creating a
benchmark dataset of news headlines mapped to event classes in Wikidata, and
resources for the evaluation of methods that perform the mapping. We use the
dataset to study two classes of unsupervised methods for this task: 1)
adaptations of classic entity linking methods, and 2) methods that treat the
problem as a zero-shot text classification problem. For the first approach, we
evaluate off-the-shelf entity linking systems. For the second approach, we
explore a) pre-trained natural language inference (NLI) models, and b)
pre-trained large generative language models. We present the results of our
evaluation, lessons learned, and directions for future work. The dataset and
scripts for evaluation are made publicly available.},
Year          = {2023},
Month         = {Dec},
Note          = {Proceedings of the 6th Workshop on Challenges and Applications of
  Automated Extraction of Socio-political Events from Text (CASE 2023)},
Url           = {http://arxiv.org/abs/2312.02334v1},
File          = {2312.02334v1.pdf}
}
@article{2402.05135v1,
Author        = {Zijie Zhong and Yunhui Zhang and Ziyi Chang and Zengchang Qin},
Title         = {CADReN: Contextual Anchor-Driven Relational Network for Controllable
  Cross-Graphs Node Importance Estimation},
Eprint        = {2402.05135v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Node Importance Estimation (NIE) is crucial for integrating external
information into Large Language Models through Retriever-Augmented Generation.
Traditional methods, focusing on static, single-graph characteristics, lack
adaptability to new graphs and user-specific requirements. CADReN, our proposed
method, addresses these limitations by introducing a Contextual Anchor (CA)
mechanism. This approach enables the network to assess node importance relative
to the CA, considering both structural and semantic features within Knowledge
Graphs (KGs). Extensive experiments show that CADReN achieves better
performance in cross-graph NIE task, with zero-shot prediction ability. CADReN
is also proven to match the performance of previous models on single-graph NIE
task. Additionally, we introduce and opensource two new datasets, RIC200 and
WK1K, specifically designed for cross-graph NIE research, providing a valuable
resource for future developments in this domain.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.05135v1},
File          = {2402.05135v1.pdf}
}
@article{2403.02176v1,
Author        = {Zhanghao Hu and Yijun Yang and Junjie Xu and Yifu Qiu and Pinzhen Chen},
Title         = {EEE-QA: Exploring Effective and Efficient Question-Answer
  Representations},
Eprint        = {2403.02176v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Current approaches to question answering rely on pre-trained language models
(PLMs) like RoBERTa. This work challenges the existing question-answer encoding
convention and explores finer representations. We begin with testing various
pooling methods compared to using the begin-of-sentence token as a question
representation for better quality. Next, we explore opportunities to
simultaneously embed all answer candidates with the question. This enables
cross-reference between answer choices and improves inference throughput via
reduced memory usage. Despite their simplicity and effectiveness, these methods
have yet to be widely studied in current frameworks. We experiment with
different PLMs, and with and without the integration of knowledge graphs.
Results prove that the memory efficacy of the proposed techniques with little
sacrifice in performance. Practically, our work enhances 38-100% throughput
with 26-65% speedups on consumer-grade GPUs by allowing for considerably larger
batch sizes. Our work sends a message to the community with promising
directions in both representation quality and efficiency for the
question-answering task in natural language processing.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.02176v1},
File          = {2403.02176v1.pdf}
}
@article{2403.16129v1,
Author        = {Miuru Abeysiriwardana and Deshan Sumanathilaka},
Title         = {A Survey on Lexical Ambiguity Detection and Word Sense Disambiguation},
Eprint        = {2403.16129v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper explores techniques that focus on understanding and resolving
ambiguity in language within the field of natural language processing (NLP),
highlighting the complexity of linguistic phenomena such as polysemy and
homonymy and their implications for computational models. Focusing extensively
on Word Sense Disambiguation (WSD), it outlines diverse approaches ranging from
deep learning techniques to leveraging lexical resources and knowledge graphs
like WordNet. The paper introduces cutting-edge methodologies like word sense
extension (WSE) and neuromyotonic approaches, enhancing disambiguation accuracy
by predicting new word senses. It examines specific applications in biomedical
disambiguation and language specific optimisation and discusses the
significance of cognitive metaphors in discourse analysis. The research
identifies persistent challenges in the field, such as the scarcity of sense
annotated corpora and the complexity of informal clinical texts. It concludes
by suggesting future directions, including using large language models, visual
WSD, and multilingual WSD systems, emphasising the ongoing evolution in
addressing lexical complexities in NLP. This thinking perspective highlights
the advancement in this field to enable computers to understand language more
accurately.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.16129v1},
File          = {2403.16129v1.pdf}
}
@article{2405.03373v2,
Author        = {Li Mi and Xianjie Dai and Javiera Castillo-Navarro and Devis Tuia},
Title         = {Knowledge-aware Text-Image Retrieval for Remote Sensing Images},
Eprint        = {2405.03373v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Image-based retrieval in large Earth observation archives is challenging
because one needs to navigate across thousands of candidate matches only with
the query image as a guide. By using text as information supporting the visual
query, the retrieval system gains in usability, but at the same time faces
difficulties due to the diversity of visual signals that cannot be summarized
by a short caption only. For this reason, as a matching-based task, cross-modal
text-image retrieval often suffers from information asymmetry between texts and
images. To address this challenge, we propose a Knowledge-aware Text-Image
Retrieval (KTIR) method for remote sensing images. By mining relevant
information from an external knowledge graph, KTIR enriches the text scope
available in the search query and alleviates the information gaps between texts
and images for better matching. Moreover, by integrating domain-specific
knowledge, KTIR also enhances the adaptation of pre-trained vision-language
models to remote sensing applications. Experimental results on three commonly
used remote sensing text-image retrieval benchmarks show that the proposed
knowledge-aware method leads to varied and consistent retrievals, outperforming
state-of-the-art retrieval methods.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.03373v2},
File          = {2405.03373v2.pdf}
}
@article{2405.08514v1,
Author        = {AmeerAli Khan and Qusai Ramadan and Cong Yang and Zeyd Boukhers},
Title         = {Falcon 7b for Software Mention Detection in Scholarly Documents},
Eprint        = {2405.08514v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {This paper aims to tackle the challenge posed by the increasing integration
of software tools in research across various disciplines by investigating the
application of Falcon-7b for the detection and classification of software
mentions within scholarly texts. Specifically, the study focuses on solving
Subtask I of the Software Mention Detection in Scholarly Publications (SOMD),
which entails identifying and categorizing software mentions from academic
literature. Through comprehensive experimentation, the paper explores different
training strategies, including a dual-classifier approach, adaptive sampling,
and weighted loss scaling, to enhance detection accuracy while overcoming the
complexities of class imbalance and the nuanced syntax of scholarly writing.
The findings highlight the benefits of selective labelling and adaptive
sampling in improving the model's performance. However, they also indicate that
integrating multiple strategies does not necessarily result in cumulative
improvements. This research offers insights into the effective application of
large language models for specific tasks such as SOMD, underlining the
importance of tailored approaches to address the unique challenges presented by
academic text analysis.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.08514v1},
File          = {2405.08514v1.pdf}
}
@article{2406.03953v1,
Author        = {Neemesh Yadav and Sarah Masud and Vikram Goyal and Vikram Goyal and Md Shad Akhtar and Tanmoy Chakraborty},
Title         = {Tox-BART: Leveraging Toxicity Attributes for Explanation Generation of
  Implicit Hate Speech},
Eprint        = {2406.03953v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Employing language models to generate explanations for an incoming implicit
hate post is an active area of research. The explanation is intended to make
explicit the underlying stereotype and aid content moderators. The training
often combines top-k relevant knowledge graph (KG) tuples to provide world
knowledge and improve performance on standard metrics. Interestingly, our study
presents conflicting evidence for the role of the quality of KG tuples in
generating implicit explanations. Consequently, simpler models incorporating
external toxicity signals outperform KG-infused models. Compared to the
KG-based setup, we observe a comparable performance for SBIC (LatentHatred)
datasets with a performance variation of +0.44 (+0.49), +1.83 (-1.56), and
-4.59 (+0.77) in BLEU, ROUGE-L, and BERTScore. Further human evaluation and
error analysis reveal that our proposed setup produces more precise
explanations than zero-shot GPT-3.5, highlighting the intricate nature of the
task.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.03953v1},
File          = {2406.03953v1.pdf}
}
@article{2407.17722v2,
Author        = {Aobo Xu and Bingyu Chang and Qingpeng Liu and Ling Jian},
Title         = {Text-Driven Neural Collaborative Filtering Model for Paper Source
  Tracing},
Eprint        = {2407.17722v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Identifying significant references within the complex interrelations of a
citation knowledge graph is challenging, which encompasses connections through
citations, authorship, keywords, and other relational attributes. The Paper
Source Tracing (PST) task seeks to automate the identification of pivotal
references for given scholarly articles utilizing advanced data mining
techniques. In the KDD CUP OAG-Challenge PST track, we design a
recommendation-based framework tailored for the PST task. This framework
employs the Neural Collaborative Filtering (NCF) model to generate final
predictions. To process the textual attributes of the papers and extract input
features for the model, we utilize SciBERT, a pre-trained language model.
According to the experimental results, our method achieved a score of 0.37814
on the Mean Average Precision (MAP) metric, outperforming baseline models and
ranking 11th among all participating teams. The source code is publicly
available at https://github.com/MyLove-XAB/KDDCupFinal.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.17722v2},
File          = {2407.17722v2.pdf}
}
@article{2407.20595v1,
Author        = {Francis Kulumba and Wissam Antoun and Guillaume Vimont and Laurent Romary},
Title         = {Harvesting Textual and Structured Data from the HAL Publication
  Repository},
Eprint        = {2407.20595v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {HAL (Hyper Articles en Ligne) is the French national publication repository,
used by most higher education and research organizations for their open science
policy. As a digital library, it is a rich repository of scholarly documents,
but its potential for advanced research has been underutilized. We present
HALvest, a unique dataset that bridges the gap between citation networks and
the full text of papers submitted on HAL. We craft our dataset by filtering HAL
for scholarly publications, resulting in approximately 700,000 documents,
spanning 34 languages across 13 identified domains, suitable for language model
training, and yielding approximately 16.5 billion tokens (with 8 billion in
French and 7 billion in English, the most represented languages). We transform
the metadata of each paper into a citation network, producing a directed
heterogeneous graph. This graph includes uniquely identified authors on HAL, as
well as all open submitted papers, and their citations. We provide a baseline
for authorship attribution using the dataset, implement a range of
state-of-the-art models in graph representation learning for link prediction,
and discuss the usefulness of our generated knowledge graph structure.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.20595v1},
File          = {2407.20595v1.pdf}
}
@article{2411.12752v1,
Author        = {Hermann Kroll and Pascal Sackhoff and Bill Matthias Thang and Maha Ksouri and Wolf-Tilo Balke},
Title         = {A Library Perspective on Supervised Text Processing in Digital
  Libraries: An Investigation in the Biomedical Domain},
Eprint        = {2411.12752v1},
DOI           = {10.1145/3677389.3702557},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {Digital libraries that maintain extensive textual collections may want to
further enrich their content for certain downstream applications, e.g.,
building knowledge graphs, semantic enrichment of documents, or implementing
novel access paths. All of these applications require some text processing,
either to identify relevant entities, extract semantic relationships between
them, or to classify documents into some categories. However, implementing
reliable, supervised workflows can become quite challenging for a digital
library because suitable training data must be crafted, and reliable models
must be trained. While many works focus on achieving the highest accuracy on
some benchmarks, we tackle the problem from a digital library practitioner. In
other words, we also consider trade-offs between accuracy and application
costs, dive into training data generation through distant supervision and large
language models such as ChatGPT, LLama, and Olmo, and discuss how to design
final pipelines. Therefore, we focus on relation extraction and text
classification, using the showcase of eight biomedical benchmarks.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.12752v1},
File          = {2411.12752v1.pdf}
}
@article{2412.02788v2,
Author        = {Tilahun Abedissa Taffa and Debayan Banerjee and Yaregal Assabie and Ricardo Usbeck},
Title         = {Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset},
Eprint        = {2412.02788v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Existing Scholarly Question Answering (QA) methods typically target
homogeneous data sources, relying solely on either text or Knowledge Graphs
(KGs). However, scholarly information often spans heterogeneous sources,
necessitating the development of QA systems that integrate information from
multiple heterogeneous data sources. To address this challenge, we introduce
Hybrid-SQuAD (Hybrid Scholarly Question Answering Dataset), a novel large-scale
QA dataset designed to facilitate answering questions incorporating both text
and KG facts. The dataset consists of 10.5K question-answer pairs generated by
a large language model, leveraging the KGs DBLP and SemOpenAlex alongside
corresponding text from Wikipedia. In addition, we propose a RAG-based baseline
hybrid QA model, achieving an exact match score of 69.65 on the Hybrid-SQuAD
test set.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.02788v2},
File          = {2412.02788v2.pdf}
}
@article{2412.07420v1,
Author        = {Philipp Christmann and Gerhard Weikum},
Title         = {RAG-based Question Answering over Heterogeneous Data and Text},
Eprint        = {2412.07420v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This article presents the QUASAR system for question answering over
unstructured text, structured tables, and knowledge graphs, with unified
treatment of all sources. The system adopts a RAG-based architecture, with a
pipeline of evidence retrieval followed by answer generation, with the latter
powered by a moderate-sized language model. Additionally and uniquely, QUASAR
has components for question understanding, to derive crisper input for evidence
retrieval, and for re-ranking and filtering the retrieved evidence before
feeding the most informative pieces into the answer generation. Experiments
with three different benchmarks demonstrate the high answering quality of our
approach, being on par with or better than large GPT models, while keeping the
computational cost and energy consumption orders of magnitude lower.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.07420v1},
File          = {2412.07420v1.pdf}
}
@article{2502.01772v1,
Author        = {Prashant Garg},
Title         = {On Bob Dylan: A Computational Perspective},
Eprint        = {2502.01772v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Cass Sunstein's essay 'On Bob Dylan' describes Dylan's 'dishabituating' style
-- a constant refusal to conform to expectation and a penchant for reinventing
his musical and lyrical identity. In this paper, I extend Sunstein's
observations through a large-scale computational analysis of Dylan's lyrics
from 1962 to 2012. Using o3-mini-high (a large language model), I extract
concept-to-concept relationships from the lyrics and construct directed
knowledge graphs that capture Dylan's thematic structure. I then quantify
shifts in sentiment, metaphorical expression, thematic diversity, and network
complexity over time. The results indicate that Dylan's lyrics increasingly
rely on metaphor, display an evolving sentiment profile, and exhibit heightened
dishabituation -- measured here as a growing variance in the network centrality
of key concepts. I also find that references to movement, protest, and mythic
imagery fluctuate in ways that align with well-known phases of Dylan's career,
reflecting the dynamic and unpredictable quality of his art. These findings not
only deepen our empirical understanding of Sunstein's thesis but also introduce
a novel computational method for analyzing an artist's evolution-offering
broader applicability to the study of cultural and creative change.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.01772v1},
File          = {2502.01772v1.pdf}
}
@article{2010.03496v3,
Author        = {Daniel Daza and Michael Cochez and Paul Groth},
Title         = {Inductive Entity Representations from Text via Link Prediction},
Eprint        = {2010.03496v3},
DOI           = {10.1145/3442381.3450141},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Graphs (KG) are of vital importance for multiple applications on
the web, including information retrieval, recommender systems, and metadata
annotation. Regardless of whether they are built manually by domain experts or
with automatic pipelines, KGs are often incomplete. Recent work has begun to
explore the use of textual descriptions available in knowledge graphs to learn
vector representations of entities in order to preform link prediction.
However, the extent to which these representations learned for link prediction
generalize to other tasks is unclear. This is important given the cost of
learning such representations. Ideally, we would prefer representations that do
not need to be trained again when transferring to a different task, while
retaining reasonable performance.
  In this work, we propose a holistic evaluation protocol for entity
representations learned via a link prediction objective. We consider the
inductive link prediction and entity classification tasks, which involve
entities not seen during training. We also consider an information retrieval
task for entity-oriented search. We evaluate an architecture based on a
pretrained language model, that exhibits strong generalization to entities not
observed during training, and outperforms related state-of-the-art methods (22%
MRR improvement in link prediction on average). We further provide evidence
that the learned representations transfer well to other tasks without
fine-tuning. In the entity classification task we obtain an average improvement
of 16% in accuracy compared with baselines that also employ pre-trained models.
In the information retrieval task, we obtain significant improvements of up to
8.8% in NDCG@10 for natural language queries. We thus show that the learned
representations are not limited KG-specific tasks, and have greater
generalization properties than evaluated in previous work.},
Year          = {2020},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2010.03496v3},
File          = {2010.03496v3.pdf}
}
@article{2311.16137v1,
Author        = {Nicholas Thomas Walker and Stefan Ultes and Pierre Lison},
Title         = {A Graph-to-Text Approach to Knowledge-Grounded Response Generation in
  Human-Robot Interaction},
Eprint        = {2311.16137v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.RO},
Abstract      = {Knowledge graphs are often used to represent structured information in a
flexible and efficient manner, but their use in situated dialogue remains
under-explored. This paper presents a novel conversational model for
human--robot interaction that rests upon a graph-based representation of the
dialogue state. The knowledge graph representing the dialogue state is
continuously updated with new observations from the robot sensors, including
linguistic, situated and multimodal inputs, and is further enriched by other
modules, in particular for spatial understanding. The neural conversational
model employed to respond to user utterances relies on a simple but effective
graph-to-text mechanism that traverses the dialogue state graph and converts
the traversals into a natural language form. This conversion of the state graph
into text is performed using a set of parameterized functions, and the values
for those parameters are optimized based on a small set of Wizard-of-Oz
interactions. After this conversion, the text representation of the dialogue
state graph is included as part of the prompt of a large language model used to
decode the agent response. The proposed approach is empirically evaluated
through a user study with a humanoid robot that acts as conversation partner to
evaluate the impact of the graph-to-text mechanism on the response generation.
After moving a robot along a tour of an indoor environment, participants
interacted with the robot using spoken dialogue and evaluated how well the
robot was able to answer questions about what the robot observed during the
tour. User scores show a statistically significant improvement in the perceived
factuality of the robot responses when the graph-to-text approach is employed,
compared to a baseline using inputs structured as semantic triples.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.16137v1},
File          = {2311.16137v1.pdf}
}
@article{2312.14781v1,
Author        = {Shuo Wang and Xinjun Mao and Shuo Yang and Menghan Wu and Zhang Zhang},
Title         = {ROS package search for robot software development: a knowledge
  graph-based approach},
Eprint        = {2312.14781v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {ROS (Robot Operating System) packages have become increasingly popular as a
type of software artifact that can be effectively reused in robotic software
development. Indeed, finding suitable ROS packages that closely match the
software's functional requirements from the vast number of available packages
is a nontrivial task using current search methods. The traditional search
methods for ROS packages often involve inputting keywords related to robotic
tasks into general-purpose search engines or code hosting platforms to obtain
approximate results of all potentially suitable ROS packages. However, the
accuracy of these search methods remains relatively low because the
task-related keywords may not precisely match the functionalities offered by
the ROS packages. To improve the search accuracy of ROS packages, this paper
presents a novel semantic-based search approach that relies on the
semantic-level ROS Package Knowledge Graph (RPKG) to automatically retrieve the
most suitable ROS packages. Firstly, to construct the RPKG, we employ
multi-dimensional feature extraction techniques to extract semantic concepts
from the dataset of ROS package text descriptions. The semantic features
extracted from this process result in a substantial number of entities and
relationships. Subsequently, we create a robot domain-specific small corpus and
further fine-tune a pre-trained language model, BERT-ROS, to generate
embeddings that effectively represent the semantics of the extracted features.
These embeddings play a crucial role in facilitating semantic-level
understanding and comparisons during the ROS package search process within the
RPKG. Secondly, we introduce a novel semantic matching-based search algorithm
that incorporates the weighted similarities of multiple features from user
search queries, which searches out more accurate ROS packages than the
traditional keyword search method.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.14781v1},
File          = {2312.14781v1.pdf}
}
@article{2410.07567v2,
Author        = {Enrique Noriega-Atala and Robert Vacareanu and Salena Torres Ashton and Adarsh Pyarelal and Clayton T. Morrison and Mihai Surdeanu},
Title         = {When and Where Did it Happen? An Encoder-Decoder Model to Identify
  Scenario Context},
Eprint        = {2410.07567v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce a neural architecture finetuned for the task of scenario context
generation: The relevant location and time of an event or entity mentioned in
text. Contextualizing information extraction helps to scope the validity of
automated finings when aggregating them as knowledge graphs. Our approach uses
a high-quality curated dataset of time and location annotations in a corpus of
epidemiology papers to train an encoder-decoder architecture. We also explored
the use of data augmentation techniques during training. Our findings suggest
that a relatively small fine-tuned encoder-decoder model performs better than
out-of-the-box LLMs and semantic role labeling parsers to accurate predict the
relevant scenario information of a particular entity or event.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.07567v2},
File          = {2410.07567v2.pdf}
}
@article{2501.14954v1,
Author        = {Subhasis Dasgupta and Hans Taparia and Laura Schmidt and Amarnath Gupta},
Title         = {MISCON: A Mission-Driven Conversational Consultant for Pre-Venture
  Entrepreneurs in Food Deserts},
Eprint        = {2501.14954v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This work-in-progress report describes MISCON, a conversational consultant
being developed for a public mission project called NOURISH. With MISCON,
aspiring small business owners in a food-insecure region and their advisors in
Community-based organizations would be able to get information, recommendation
and analysis regarding setting up food businesses. MISCON conversations are
modeled as state machine that uses a heterogeneous knowledge graph as well as
several analytical tools and services including a variety of LLMs. In this
short report, we present the functional architecture and some design
considerations behind MISCON.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.14954v1},
File          = {2501.14954v1.pdf}
}
@article{2412.05248v2,
Author        = {Saransh Kumar Gupta and Lipika Dey and Partha Pratim Das and Geeta Trilok-Kumar and Ramesh Jain},
Title         = {Enhancing FKG.in: automating Indian food composition analysis},
Eprint        = {2412.05248v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {This paper presents a novel approach to compute food composition data for
Indian recipes using a knowledge graph for Indian food (FKG.in) and LLMs. The
primary focus is to provide a broad overview of an automated food composition
analysis workflow and describe its core functionalities: nutrition data
aggregation, food composition analysis, and LLM-augmented information
resolution. This workflow aims to complement FKG.in and iteratively supplement
food composition data from verified knowledge bases. Additionally, this paper
highlights the challenges of representing Indian food and accessing food
composition data digitally. It also reviews three key sources of food
composition data: the Indian Food Composition Tables, the Indian Nutrient
Databank, and the Nutritionix API. Furthermore, it briefly outlines how users
can interact with the workflow to obtain diet-based health recommendations and
detailed food composition information for numerous recipes. We then explore the
complex challenges of analyzing Indian recipe information across dimensions
such as structure, multilingualism, and uncertainty as well as present our
ongoing work on LLM-based solutions to address these issues. The methods
proposed in this workshop paper for AI-driven knowledge curation and
information resolution are application-agnostic, generalizable, and replicable
for any domain.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.05248v2},
File          = {2412.05248v2.pdf}
}
@article{2305.09785v1,
Author        = {Na Li and Hanane Kteich and Zied Bouraoui and Steven Schockaert},
Title         = {Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned
  Language Models},
Eprint        = {2305.09785v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Learning vectors that capture the meaning of concepts remains a fundamental
challenge. Somewhat surprisingly, perhaps, pre-trained language models have
thus far only enabled modest improvements to the quality of such concept
embeddings. Current strategies for using language models typically represent a
concept by averaging the contextualised representations of its mentions in some
corpus. This is potentially sub-optimal for at least two reasons. First,
contextualised word vectors have an unusual geometry, which hampers downstream
tasks. Second, concept embeddings should capture the semantic properties of
concepts, whereas contextualised word vectors are also affected by other
factors. To address these issues, we propose two contrastive learning
strategies, based on the view that whenever two sentences reveal similar
properties, the corresponding contextualised vectors should also be similar.
One strategy is fully unsupervised, estimating the properties which are
expressed in a sentence from the neighbourhood structure of the contextualised
word embeddings. The second strategy instead relies on a distant supervision
signal from ConceptNet. Our experimental results show that the resulting
vectors substantially outperform existing concept embeddings in predicting the
semantic properties of concepts, with the ConceptNet-based strategy achieving
the best results. These findings are furthermore confirmed in a clustering task
and in the downstream task of ontology completion.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.09785v1},
File          = {2305.09785v1.pdf}
}
@article{2406.03062v1,
Author        = {Jinge Wu and Abul Hasan and Honghan Wu},
Title         = {RadBARTsum: Domain Specific Adaption of Denoising Sequence-to-Sequence
  Models for Abstractive Radiology Report Summarization},
Eprint        = {2406.03062v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Radiology report summarization is a crucial task that can help doctors
quickly identify clinically significant findings without the need to review
detailed sections of reports. This study proposes RadBARTsum, a domain-specific
and ontology facilitated adaptation of the BART model for abstractive radiology
report summarization. The approach involves two main steps: 1) re-training the
BART model on a large corpus of radiology reports using a novel entity masking
strategy to improving biomedical domain knowledge learning, and 2) fine-tuning
the model for the summarization task using the Findings and Background sections
to predict the Impression section. Experiments are conducted using different
masking strategies. Results show that the re-training process with domain
knowledge facilitated masking improves performances consistently across various
settings. This work contributes a domain-specific generative language model for
radiology report summarization and a method for utilising medical knowledge to
realise entity masking language model. The proposed approach demonstrates a
promising direction of enhancing the efficiency of language models by deepening
its understanding of clinical knowledge in radiology reports.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.03062v1},
File          = {2406.03062v1.pdf}
}
@article{2408.01214v1,
Author        = {Daniel B. Hier and S. Ilyas Munzir and Anne Stahlfeld and Tayo Obafemi-Ajayi and Michael D. Carrithers},
Title         = {High-Throughput Phenotyping of Clinical Text Using Large Language Models},
Eprint        = {2408.01214v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {High-throughput phenotyping automates the mapping of patient signs to
standardized ontology concepts and is essential for precision medicine. This
study evaluates the automation of phenotyping of clinical summaries from the
Online Mendelian Inheritance in Man (OMIM) database using large language
models. Due to their rich phenotype data, these summaries can be surrogates for
physician notes. We conduct a performance comparison of GPT-4 and
GPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in
identifying, categorizing, and normalizing signs, achieving concordance with
manual annotators comparable to inter-rater agreement. Despite some limitations
in sign normalization, the extensive pre-training of GPT-4 results in high
performance and generalizability across several phenotyping tasks while
obviating the need for manually annotated training data. Large language models
are expected to be the dominant method for automating high-throughput
phenotyping of clinical text.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.01214v1},
File          = {2408.01214v1.pdf}
}
@article{2109.01479v1,
Author        = {Maximilian Stubbemann and Gerd Stumme},
Title         = {LG4AV: Combining Language Models and Graph Neural Networks for Author
  Verification},
Eprint        = {2109.01479v1},
DOI           = {10.1007/978-3-031-01333-1_25},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The automatic verification of document authorships is important in various
settings. Researchers are for example judged and compared by the amount and
impact of their publications and public figures are confronted by their posts
on social media platforms. Therefore, it is important that authorship
information in frequently used web services and platforms is correct. The
question whether a given document is written by a given author is commonly
referred to as authorship verification (AV). While AV is a widely investigated
problem in general, only few works consider settings where the documents are
short and written in a rather uniform style. This makes most approaches
unpractical for online databases and knowledge graphs in the scholarly domain.
Here, authorships of scientific publications have to be verified, often with
just abstracts and titles available. To this point, we present our novel
approach LG4AV which combines language models and graph neural networks for
authorship verification. By directly feeding the available texts in a
pre-trained transformer architecture, our model does not need any hand-crafted
stylometric features that are not meaningful in scenarios where the writing
style is, at least to some extent, standardized. By the incorporation of a
graph neural network structure, our model can benefit from relations between
authors that are meaningful with respect to the verification process. For
example, scientific authors are more likely to write about topics that are
addressed by their co-authors and twitter users tend to post about the same
subjects as people they follow. We experimentally evaluate our model and study
to which extent the inclusion of co-authorships enhances verification decisions
in bibliometric environments.},
Year          = {2021},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2109.01479v1},
File          = {2109.01479v1.pdf}
}
@article{2201.08860v1,
Author        = {Xikun Zhang and Antoine Bosselut and Michihiro Yasunaga and Hongyu Ren and Percy Liang and Christopher D. Manning and Jure Leskovec},
Title         = {GreaseLM: Graph REASoning Enhanced Language Models for Question
  Answering},
Eprint        = {2201.08860v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Answering complex questions about textual narratives requires reasoning over
both stated context and the world knowledge that underlies it. However,
pretrained language models (LM), the foundation of most modern QA systems, do
not robustly represent latent relationships between concepts, which is
necessary for reasoning. While knowledge graphs (KG) are often used to augment
LMs with structured representations of world knowledge, it remains an open
question how to effectively fuse and reason over the KG representations and the
language context, which provides situational constraints and nuances. In this
work, we propose GreaseLM, a new model that fuses encoded representations from
pretrained LMs and graph neural networks over multiple layers of modality
interaction operations. Information from both modalities propagates to the
other, allowing language context representations to be grounded by structured
world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in
the context to inform the graph representations of knowledge. Our results on
three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA)
and medical question answering (i.e., MedQA-USMLE) domains demonstrate that
GreaseLM can more reliably answer questions that require reasoning over both
situational constraints and structured knowledge, even outperforming models 8x
larger.},
Year          = {2022},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2201.08860v1},
File          = {2201.08860v1.pdf}
}
@article{2302.03512v3,
Author        = {Xiaoye Qu and Yingjie Gu and Qingrong Xia and Zechang Li and Zhefeng Wang and Baoxing Huai},
Title         = {A Survey on Arabic Named Entity Recognition: Past, Recent Advances, and
  Future Trends},
Eprint        = {2302.03512v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {As more and more Arabic texts emerged on the Internet, extracting important
information from these Arabic texts is especially useful. As a fundamental
technology, Named entity recognition (NER) serves as the core component in
information extraction technology, while also playing a critical role in many
other Natural Language Processing (NLP) systems, such as question answering and
knowledge graph building. In this paper, we provide a comprehensive review of
the development of Arabic NER, especially the recent advances in deep learning
and pre-trained language model. Specifically, we first introduce the background
of Arabic NER, including the characteristics of Arabic and existing resources
for Arabic NER. Then, we systematically review the development of Arabic NER
methods. Traditional Arabic NER systems focus on feature engineering and
designing domain-specific rules. In recent years, deep learning methods achieve
significant progress by representing texts via continuous vector
representations. With the growth of pre-trained language model, Arabic NER
yields better performance. Finally, we conclude the method gap between Arabic
NER and NER methods from other languages, which helps outline future directions
for Arabic NER.},
Year          = {2023},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2302.03512v3},
File          = {2302.03512v3.pdf}
}
@article{2302.11799v2,
Author        = {Qichen Ye and Bowen Cao and Nuo Chen and Weiyuan Xu and Yuexian Zou},
Title         = {FiTs: Fine-grained Two-stage Training for Knowledge-aware Question
  Answering},
Eprint        = {2302.11799v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge-aware question answering (KAQA) requires the model to answer
questions over a knowledge base, which is essential for both open-domain QA and
domain-specific QA, especially when language models alone cannot provide all
the knowledge needed. Despite the promising result of recent KAQA systems which
tend to integrate linguistic knowledge from pre-trained language models (PLM)
and factual knowledge from knowledge graphs (KG) to answer complex questions, a
bottleneck exists in effectively fusing the representations from PLMs and KGs
because of (i) the semantic and distributional gaps between them, and (ii) the
difficulties in joint reasoning over the provided knowledge from both
modalities. To address the above two problems, we propose a Fine-grained
Two-stage training framework (FiTs) to boost the KAQA system performance: The
first stage aims at aligning representations from the PLM and the KG, thus
bridging the modality gaps between them, named knowledge adaptive
post-training. The second stage, called knowledge-aware fine-tuning, aims to
improve the model's joint reasoning ability based on the aligned
representations. In detail, we fine-tune the post-trained model via two
auxiliary self-supervised tasks in addition to the QA supervision. Extensive
experiments demonstrate that our approach achieves state-of-the-art performance
on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA,
OpenbookQA) and medical question answering (i.e., MedQA-USMILE) domains.},
Year          = {2023},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2302.11799v2},
File          = {2302.11799v2.pdf}
}
@article{2208.00635v1,
Author        = {Qianglong Chen and Feng-Lin Li and Guohai Xu and Ming Yan and Ji Zhang and Yin Zhang},
Title         = {DictBERT: Dictionary Description Knowledge Enhanced Language Model
  Pre-training via Contrastive Learning},
Eprint        = {2208.00635v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Although pre-trained language models (PLMs) have achieved state-of-the-art
performance on various natural language processing (NLP) tasks, they are shown
to be lacking in knowledge when dealing with knowledge driven tasks. Despite
the many efforts made for injecting knowledge into PLMs, this problem remains
open. To address the challenge, we propose \textbf{DictBERT}, a novel approach
that enhances PLMs with dictionary knowledge which is easier to acquire than
knowledge graph (KG). During pre-training, we present two novel pre-training
tasks to inject dictionary knowledge into PLMs via contrastive learning:
\textit{dictionary entry prediction} and \textit{entry description
discrimination}. In fine-tuning, we use the pre-trained DictBERT as a plugin
knowledge base (KB) to retrieve implicit knowledge for identified entries in an
input sequence, and infuse the retrieved knowledge into the input to enhance
its representation via a novel extra-hop attention mechanism. We evaluate our
approach on a variety of knowledge driven and language understanding tasks,
including NER, relation extraction, CommonsenseQA, OpenBookQA and GLUE.
Experimental results demonstrate that our model can significantly improve
typical PLMs: it gains a substantial improvement of 0.5\%, 2.9\%, 9.0\%, 7.1\%
and 3.3\% on BERT-large respectively, and is also effective on RoBERTa-large.},
Year          = {2022},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2208.00635v1},
File          = {2208.00635v1.pdf}
}
@article{2208.09625v2,
Author        = {Jiacheng Li and Yannis Katsis and Tyler Baldwin and Ho-Cheol Kim and Andrew Bartko and Julian McAuley and Chun-Nan Hsu},
Title         = {SPOT: Knowledge-Enhanced Language Representations for Information
  Extraction},
Eprint        = {2208.09625v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge-enhanced pre-trained models for language representation have been
shown to be more effective in knowledge base construction tasks (i.e.,~relation
extraction) than language models such as BERT. These knowledge-enhanced
language models incorporate knowledge into pre-training to generate
representations of entities or relationships. However, existing methods
typically represent each entity with a separate embedding. As a result, these
methods struggle to represent out-of-vocabulary entities and a large amount of
parameters, on top of their underlying token models (i.e.,~the transformer),
must be used and the number of entities that can be handled is limited in
practice due to memory constraints. Moreover, existing models still struggle to
represent entities and relationships simultaneously. To address these problems,
we propose a new pre-trained model that learns representations of both entities
and relationships from token spans and span pairs in the text respectively. By
encoding spans efficiently with span modules, our model can represent both
entities and their relationships but requires fewer parameters than existing
models. We pre-trained our model with the knowledge graph extracted from
Wikipedia and test it on a broad range of supervised and unsupervised
information extraction tasks. Results show that our model learns better
representations for both entities and relationships than baselines, while in
supervised settings, fine-tuning our model outperforms RoBERTa consistently and
achieves competitive results on information extraction tasks.},
Year          = {2022},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2208.09625v2},
File          = {2208.09625v2.pdf}
}
@article{2009.03506v1,
Author        = {Yucong Lin and Keming Lu and Yulin Chen and Chuan Hong and Sheng Yu},
Title         = {High-throughput relation extraction algorithm development associating
  knowledge articles and electronic health records},
Eprint        = {2009.03506v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Objective: Medical relations are the core components of medical knowledge
graphs that are needed for healthcare artificial intelligence. However, the
requirement of expert annotation by conventional algorithm development
processes creates a major bottleneck for mining new relations. In this paper,
we present Hi-RES, a framework for high-throughput relation extraction
algorithm development. We also show that combining knowledge articles with
electronic health records (EHRs) significantly increases the classification
accuracy. Methods: We use relation triplets obtained from structured databases
and semistructured webpages to label sentences from target corpora as positive
training samples. Two methods are also provided for creating improved negative
samples by combining positive samples with na\"ive negative samples. We propose
a common model that summarizes sentence information using large-scale
pretrained language models and multi-instance attention, which then joins with
the concept embeddings trained from the EHRs for relation prediction. Results:
We apply the Hi-RES framework to develop classification algorithms for
disorder-disorder relations and disorder-location relations. Millions of
sentences are created as training data. Using pretrained language models and
EHR-based embeddings individually provides considerable accuracy increases over
those of previous models. Joining them together further tremendously increases
the accuracy to 0.947 and 0.998 for the two sets of relations, respectively,
which are 10-17 percentage points higher than those of previous models.
Conclusion: Hi-RES is an efficient framework for achieving high-throughput and
accurate relation extraction algorithm development.},
Year          = {2020},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2009.03506v1},
File          = {2009.03506v1.pdf}
}
@article{2009.13964v5,
Author        = {Yusheng Su and Xu Han and Zhengyan Zhang and Peng Li and Zhiyuan Liu and Yankai Lin and Jie Zhou and Maosong Sun},
Title         = {CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced
  Pre-Trained Language Models},
Eprint        = {2009.13964v5},
DOI           = {10.1016/j.aiopen.2021.06.004},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Several recent efforts have been devoted to enhancing pre-trained language
models (PLMs) by utilizing extra heterogeneous knowledge in knowledge graphs
(KGs) and achieved consistent improvements on various knowledge-driven NLP
tasks. However, most of these knowledge-enhanced PLMs embed static sub-graphs
of KGs ("knowledge context"), regardless of that the knowledge required by PLMs
may change dynamically according to specific text ("textual context"). In this
paper, we propose a novel framework named Coke to dynamically select contextual
knowledge and embed knowledge context according to textual context for PLMs,
which can avoid the effect of redundant and ambiguous knowledge in KGs that
cannot match the input text. Our experimental results show that Coke
outperforms various baselines on typical knowledge-driven NLP tasks, indicating
the effectiveness of utilizing dynamic knowledge context for language
understanding. Besides the performance improvements, the dynamically selected
knowledge in Coke can describe the semantics of text-related knowledge in a
more interpretable form than the conventional PLMs. Our source code and
datasets will be available to provide more details for Coke.},
Year          = {2020},
Month         = {Sep},
Note          = {AI Open 2021},
Url           = {http://arxiv.org/abs/2009.13964v5},
File          = {2009.13964v5.pdf}
}
@article{2011.07956v2,
Author        = {Wangchunshu Zhou and Dong-Ho Lee and Ravi Kiran Selvam and Seyeon Lee and Bill Yuchen Lin and Xiang Ren},
Title         = {Pre-training Text-to-Text Transformers for Concept-centric Common Sense},
Eprint        = {2011.07956v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pre-trained language models (PTLM) have achieved impressive results in a
range of natural language understanding (NLU) and generation (NLG) tasks.
However, current pre-training objectives such as masked token prediction (for
BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not
explicitly model the relational commonsense knowledge about everyday concepts,
which is crucial to many downstream tasks that need common sense to understand
or generate. To augment PTLMs with concept-centric commonsense knowledge, in
this paper, we propose both generative and contrastive objectives for learning
common sense from the text, and use them as intermediate self-supervised
learning tasks for incrementally pre-training PTLMs (before task-specific
fine-tuning on downstream datasets). Furthermore, we develop a joint
pre-training framework to unify generative and contrastive objectives so that
they can mutually reinforce each other. Extensive experimental results show
that our method, concept-aware language model (CALM), can pack more commonsense
knowledge into the parameters of a pre-trained text-to-text transformer without
relying on external knowledge graphs, yielding better performance on both NLU
and NLG tasks. We show that while only incrementally pre-trained on a
relatively small corpus for a few steps, CALM outperforms baseline methods by a
consistent margin and even comparable with some larger PTLMs, which suggests
that CALM can serve as a general, plug-and-play method for improving the
commonsense reasoning ability of a PTLM.},
Year          = {2020},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2011.07956v2},
File          = {2011.07956v2.pdf}
}
@article{2012.10813v1,
Author        = {Yikang Li and Pulkit Goel and Varsha Kuppur Rajendra and Har Simrat Singh and Jonathan Francis and Kaixin Ma and Eric Nyberg and Alessandro Oltramari},
Title         = {Lexically-constrained Text Generation through Commonsense Knowledge
  Extraction and Injection},
Eprint        = {2012.10813v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Conditional text generation has been a challenging task that is yet to see
human-level performance from state-of-the-art models. In this work, we
specifically focus on the Commongen benchmark, wherein the aim is to generate a
plausible sentence for a given set of input concepts. Despite advances in other
tasks, large pre-trained language models that are fine-tuned on this dataset
often produce sentences that are syntactically correct but qualitatively
deviate from a human understanding of common sense. Furthermore, generated
sequences are unable to fulfill such lexical requirements as matching
part-of-speech and full concept coverage. In this paper, we explore how
commonsense knowledge graphs can enhance model performance, with respect to
commonsense reasoning and lexically-constrained decoding. We propose strategies
for enhancing the semantic correctness of the generated text, which we
accomplish through: extracting commonsense relations from Conceptnet, injecting
these relations into the Unified Language Model (UniLM) through attention
mechanisms, and enforcing the aforementioned lexical requirements through
output constraints. By performing several ablations, we find that commonsense
injection enables the generation of sentences that are more aligned with human
understanding, while remaining compliant with lexical requirements.},
Year          = {2020},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2012.10813v1},
File          = {2012.10813v1.pdf}
}
@article{2104.08610v1,
Author        = {Michael Glass and Gaetano Rossiello and Alfio Gliozzo},
Title         = {Zero-shot Slot Filling with DPR and RAG},
Eprint        = {2104.08610v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The ability to automatically extract Knowledge Graphs (KG) from a given
collection of documents is a long-standing problem in Artificial Intelligence.
One way to assess this capability is through the task of slot filling. Given an
entity query in form of [Entity, Slot, ?], a system is asked to `fill' the slot
by generating or extracting the missing value from a relevant passage or
passages. This capability is crucial to create systems for automatic knowledge
base population, which is becoming in ever-increasing demand, especially in
enterprise applications. Recently, there has been a promising direction in
evaluating language models in the same way we would evaluate knowledge bases,
and the task of slot filling is the most suitable to this intent. The recent
advancements in the field try to solve this task in an end-to-end fashion using
retrieval-based language models. Models like Retrieval Augmented Generation
(RAG) show surprisingly good performance without involving complex information
extraction pipelines. However, the results achieved by these models on the two
slot filling tasks in the KILT benchmark are still not at the level required by
real-world information extraction systems. In this paper, we describe several
strategies we adopted to improve the retriever and the generator of RAG in
order to make it a better slot filler. Our KGI0 system (available at
https://github.com/IBM/retrieve-write-slot-filling) reached the top-1 position
on the KILT leaderboard on both T-REx and zsRE dataset with a large margin.},
Year          = {2021},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2104.08610v1},
File          = {2104.08610v1.pdf}
}
@article{2110.07477v2,
Author        = {Lingzhi Wang and Huang Hu and Lei Sha and Can Xu and Kam-Fai Wong and Daxin Jiang},
Title         = {RecInDial: A Unified Framework for Conversational Recommendation with
  Pretrained Language Models},
Eprint        = {2110.07477v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Conversational Recommender System (CRS), which aims to recommend high-quality
items to users through interactive conversations, has gained great research
interest recently. A CRS is usually composed of a recommendation module and a
generation module. In the previous work, these two modules are loosely
connected in the model training and are shallowly integrated during inference,
where a simple switching or copy mechanism is adopted to incorporate
recommended items into generated responses. Moreover, the current end-to-end
neural models trained on small crowd-sourcing datasets (e.g., 10K dialogs in
the ReDial dataset) tend to overfit and have poor chit-chat ability. In this
work, we propose a novel unified framework that integrates recommendation into
the dialog (RecInDial) generation by introducing a vocabulary pointer. To
tackle the low-resource issue in CRS, we finetune the large-scale pretrained
language models to generate fluent and diverse responses, and introduce a
knowledge-aware bias learned from an entity-oriented knowledge graph to enhance
the recommendation performance. Furthermore, we propose to evaluate the CRS
models in an end-to-end manner, which can reflect the overall performance of
the entire system rather than the performance of individual modules, compared
to the separate evaluations of the two modules used in previous work.
Experiments on the benchmark dataset ReDial show our RecInDial model
significantly surpasses the state-of-the-art methods. More extensive analyses
show the effectiveness of our model.},
Year          = {2021},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2110.07477v2},
File          = {2110.07477v2.pdf}
}
@article{2210.14353v2,
Author        = {Victor Zhong and Weijia Shi and Wen-tau Yih and Luke Zettlemoyer},
Title         = {RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question
  Answering},
Eprint        = {2210.14353v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We introduce RoMQA, the first benchmark for robust, multi-evidence,
multi-answer question answering (QA). RoMQA contains clusters of questions that
are derived from related constraints mined from the Wikidata knowledge graph.
RoMQA evaluates robustness of QA models to varying constraints by measuring
worst-case performance within each question cluster. Compared to prior QA
datasets, RoMQA has more human-written questions that require reasoning over
more evidence text and have, on average, many more correct answers. In
addition, human annotators rate RoMQA questions as more natural or likely to be
asked by people. We evaluate state-of-the-art large language models in
zero-shot, few-shot, and fine-tuning settings, and find that RoMQA is
challenging: zero-shot and few-shot models perform similarly to naive
baselines, while supervised retrieval methods perform well below gold evidence
upper bounds. Moreover, existing models are not robust to variations in
question constraints, but can be made more robust by tuning on clusters of
related questions. Our results show that RoMQA is a challenging benchmark for
large language models, and provides a quantifiable test to build more robust QA
methods.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.14353v2},
File          = {2210.14353v2.pdf}
}
@article{2303.16537v3,
Author        = {Zichen Chen and Jianda Chen and Yuanyuan Chen and Han Yu and Ambuj K Singh and Misha Sra},
Title         = {LMExplainer: Grounding Knowledge and Explaining Language Models},
Eprint        = {2303.16537v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Language models (LMs) like GPT-4 are important in AI applications, but their
opaque decision-making process reduces user trust, especially in
safety-critical areas. We introduce LMExplainer, a novel knowledge-grounded
explainer that clarifies the reasoning process of LMs through intuitive,
human-understandable explanations. By leveraging a graph attention network
(GAT) with a large-scale knowledge graph (KG), LMExplainer not only precisely
narrows the reasoning space to focus on the most relevant knowledge but also
grounds its reasoning in structured, verifiable knowledge to reduce
hallucinations and enhance interpretability. LMExplainer effectively generates
human-understandable explanations to enhance transparency and streamline the
decision-making process. Additionally, by incorporating debugging into the
explanation, it offers expertise suggestions that improve LMs from a
developmental perspective. Thus, LMExplainer stands as an enhancement in making
LMs more accessible and understandable to users. We evaluate LMExplainer on
benchmark datasets such as CommonsenseQA and OpenBookQA, demonstrating that it
outperforms most existing methods. By comparing the explanations generated by
LMExplainer with those of other models, we show that our approach offers more
comprehensive and clearer explanations of the reasoning process. LMExplainer
provides a deeper understanding of the inner workings of LMs, advancing towards
more reliable, transparent, and equitable AI.},
Year          = {2023},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2303.16537v3},
File          = {2303.16537v3.pdf}
}
@article{2306.13501v1,
Author        = {Kaushik Roy and Yuxin Zi and Vignesh Narayanan and Manas Gaur and Amit Sheth},
Title         = {Knowledge-Infused Self Attention Transformers},
Eprint        = {2306.13501v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Transformer-based language models have achieved impressive success in various
natural language processing tasks due to their ability to capture complex
dependencies and contextual information using self-attention mechanisms.
However, they are not without limitations. These limitations include
hallucinations, where they produce incorrect outputs with high confidence, and
alignment issues, where they generate unhelpful and unsafe outputs for human
users. These limitations stem from the absence of implicit and missing context
in the data alone. To address this, researchers have explored augmenting these
models with external knowledge from knowledge graphs to provide the necessary
additional context. However, the ad-hoc nature of existing methods makes it
difficult to properly analyze the effects of knowledge infusion on the many
moving parts or components of a transformer. This paper introduces a systematic
method for infusing knowledge into different components of a transformer-based
model. A modular framework is proposed to identify specific components within
the transformer architecture, such as the self-attention mechanism, encoder
layers, or the input embedding layer, where knowledge infusion can be applied.
Additionally, extensive experiments are conducted on the General Language
Understanding Evaluation (GLUE) benchmark tasks, and the findings are reported.
This systematic approach aims to facilitate more principled approaches to
incorporating knowledge into language model architectures.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.13501v1},
File          = {2306.13501v1.pdf}
}
@article{2307.00266v1,
Author        = {Bryan Cai and Sihang Zeng and Yucong Lin and Zheng Yuan and Doudou Zhou and Lu Tian},
Title         = {Hierarchical Pretraining for Biomedical Term Embeddings},
Eprint        = {2307.00266v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Electronic health records (EHR) contain narrative notes that provide
extensive details on the medical condition and management of patients. Natural
language processing (NLP) of clinical notes can use observed frequencies of
clinical terms as predictive features for downstream applications such as
clinical decision making and patient trajectory prediction. However, due to the
vast number of highly similar and related clinical concepts, a more effective
modeling strategy is to represent clinical terms as semantic embeddings via
representation learning and use the low dimensional embeddings as feature
vectors for predictive modeling. To achieve efficient representation,
fine-tuning pretrained language models with biomedical knowledge graphs may
generate better embeddings for biomedical terms than those from standard
language models alone. These embeddings can effectively discriminate synonymous
pairs of from those that are unrelated. However, they often fail to capture
different degrees of similarity or relatedness for concepts that are
hierarchical in nature. To overcome this limitation, we propose HiPrBERT, a
novel biomedical term representation model trained on additionally complied
data that contains hierarchical structures for various biomedical terms. We
modify an existing contrastive loss function to extract information from these
hierarchies. Our numerical experiments demonstrate that HiPrBERT effectively
learns the pair-wise distance from hierarchical information, resulting in a
substantially more informative embeddings for further biomedical applications},
Year          = {2023},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2307.00266v1},
File          = {2307.00266v1.pdf}
}
@article{2312.03749v2,
Author        = {Bradley P. Allen},
Title         = {Conceptual Engineering Using Large Language Models},
Eprint        = {2312.03749v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We describe a method, based on Jennifer Nado's proposal for classification
procedures as targets of conceptual engineering, that implements such
procedures by prompting a large language model. We apply this method, using
data from the Wikidata knowledge graph, to evaluate stipulative definitions
related to two paradigmatic conceptual engineering projects: the International
Astronomical Union's redefinition of PLANET and Haslanger's ameliorative
analysis of WOMAN. Our results show that classification procedures built using
our approach can exhibit good classification performance and, through the
generation of rationales for their classifications, can contribute to the
identification of issues in either the definitions or the data against which
they are being evaluated. We consider objections to this method, and discuss
implications of this work for three aspects of theory and practice of
conceptual engineering: the definition of its targets, empirical methods for
their investigation, and their practical roles. The data and code used for our
experiments, together with the experimental results, are available in a Github
repository.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.03749v2},
File          = {2312.03749v2.pdf}
}
@article{2403.04261v2,
Author        = {Hui Zong and Rongrong Wu and Jiaxue Cha and Weizhe Feng and Erman Wu and Jiakun Li and Aibin Shao and Liang Tao and Zuofeng Li and Buzhou Tang and Bairong Shen},
Title         = {Advancing Chinese biomedical text mining with community challenges},
Eprint        = {2403.04261v2},
DOI           = {10.1016/j.jbi.2024.104716},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Objective: This study aims to review the recent advances in community
challenges for biomedical text mining in China. Methods: We collected
information of evaluation tasks released in community challenges of biomedical
text mining, including task description, dataset description, data source, task
type and related links. A systematic summary and comparative analysis were
conducted on various biomedical natural language processing tasks, such as
named entity recognition, entity normalization, attribute extraction, relation
extraction, event extraction, text classification, text similarity, knowledge
graph construction, question answering, text generation, and large language
model evaluation. Results: We identified 39 evaluation tasks from 6 community
challenges that spanned from 2017 to 2023. Our analysis revealed the diverse
range of evaluation task types and data sources in biomedical text mining. We
explored the potential clinical applications of these community challenge tasks
from a translational biomedical informatics perspective. We compared with their
English counterparts, and discussed the contributions, limitations, lessons and
guidelines of these community challenges, while highlighting future directions
in the era of large language models. Conclusion: Community challenge evaluation
competitions have played a crucial role in promoting technology innovation and
fostering interdisciplinary collaboration in the field of biomedical text
mining. These challenges provide valuable platforms for researchers to develop
state-of-the-art solutions.},
Year          = {2024},
Month         = {Mar},
Note          = {Journal of Biomedical Informatics. 2024;157:104716.},
Url           = {http://arxiv.org/abs/2403.04261v2},
File          = {2403.04261v2.pdf}
}
@article{2409.07869v1,
Author        = {Zihang Peng and Daria Stepanova and Vinh Thinh Ho and Heike Adel and Alessandra Russo and Simon Ott},
Title         = {Learning Rules from KGs Guided by Language Models},
Eprint        = {2409.07869v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Advances in information extraction have enabled the automatic construction of
large knowledge graphs (e.g., Yago, Wikidata or Google KG), which are widely
used in many applications like semantic search or data analytics. However, due
to their semi-automatic construction, KGs are often incomplete. Rule learning
methods, concerned with the extraction of frequent patterns from KGs and
casting them into rules, can be applied to predict potentially missing facts. A
crucial step in this process is rule ranking. Ranking of rules is especially
challenging over highly incomplete or biased KGs (e.g., KGs predominantly
storing facts about famous people), as in this case biased rules might fit the
data best and be ranked at the top based on standard statistical metrics like
rule confidence. To address this issue, prior works proposed to rank rules not
only relying on the original KG but also facts predicted by a KG embedding
model. At the same time, with the recent rise of Language Models (LMs), several
works have claimed that LMs can be used as alternative means for KG completion.
In this work, our goal is to verify to which extent the exploitation of LMs is
helpful for improving the quality of rule learning systems.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.07869v1},
File          = {2409.07869v1.pdf}
}
@article{2410.10144v1,
Author        = {Hongyi Yuan and Suqi Liu and Kelly Cho and Katherine Liao and Alexandre Pereira and Tianxi Cai},
Title         = {Unified Representation of Genomic and Biomedical Concepts through
  Multi-Task, Multi-Source Contrastive Learning},
Eprint        = {2410.10144v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We introduce GENomic Encoding REpresentation with Language Model (GENEREL), a
framework designed to bridge genetic and biomedical knowledge bases. What sets
GENEREL apart is its ability to fine-tune language models to infuse biological
knowledge behind clinical concepts such as diseases and medications. This
fine-tuning enables the model to capture complex biomedical relationships more
effectively, enriching the understanding of how genomic data connects to
clinical outcomes. By constructing a unified embedding space for biomedical
concepts and a wide range of common SNPs from sources such as patient-level
data, biomedical knowledge graphs, and GWAS summaries, GENEREL aligns the
embeddings of SNPs and clinical concepts through multi-task contrastive
learning. This allows the model to adapt to diverse natural language
representations of biomedical concepts while bypassing the limitations of
traditional code mapping systems across different data sources. Our experiments
demonstrate GENEREL's ability to effectively capture the nuanced relationships
between SNPs and clinical concepts. GENEREL also emerges to discern the degree
of relatedness, potentially allowing for a more refined identification of
concepts. This pioneering approach in constructing a unified embedding system
for both SNPs and biomedical concepts enhances the potential for data
integration and discovery in biomedical research.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.10144v1},
File          = {2410.10144v1.pdf}
}
@article{2501.14530v1,
Author        = {Zhenguang Zhong and Jia Tang},
Title         = {Design and Implementation of a Psychiatry Resident Training System Based
  on Large Language Models},
Eprint        = {2501.14530v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {Mental disorders have become a significant global public health issue, while
the shortage of psychiatrists and inefficient training systems severely hinder
the accessibility of mental health services. This paper designs and implements
an artificial intelligence-based training system for psychiatrists. By
integrating technologies such as large language models, knowledge graphs, and
expert systems, the system constructs an intelligent and standardized training
platform. It includes six functional modules: case generation, consultation
dialogue, examination prescription, diagnostic decision-making, integrated
traditional Chinese and Western medicine prescription, and expert evaluation,
providing comprehensive support from clinical skill training to professional
level assessment.The system adopts a B/S architecture, developed using the
Vue.js and Node.js technology stack, and innovatively applies deep learning
algorithms for case generation and doctor-patient dialogue. In a clinical trial
involving 60 psychiatrists at different levels, the system demonstrated
excellent performance and training outcomes: system stability reached 99.95%,
AI dialogue accuracy achieved 96.5%, diagnostic accuracy reached 92.5%, and
user satisfaction scored 92.3%. Experimental data showed that doctors using the
system improved their knowledge mastery, clinical thinking, and diagnostic
skills by 35.6%, 28.4%, and 23.7%, respectively.The research results provide an
innovative solution for improving the efficiency of psychiatrist training and
hold significant importance for promoting the standardization and scalability
of mental health professional development.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.14530v1},
File          = {2501.14530v1.pdf}
}
@article{2108.05454v1,
Author        = {Sharad Dixit and Varish Mulwad and Abhinav Saxena},
Title         = {Extracting Semantics from Maintenance Records},
Eprint        = {2108.05454v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Rapid progress in natural language processing has led to its utilization in a
variety of industrial and enterprise settings, including in its use for
information extraction, specifically named entity recognition and relation
extraction, from documents such as engineering manuals and field maintenance
reports. While named entity recognition is a well-studied problem, existing
state-of-the-art approaches require large labelled datasets which are hard to
acquire for sensitive data such as maintenance records. Further, industrial
domain experts tend to distrust results from black box machine learning models,
especially when the extracted information is used in downstream predictive
maintenance analytics. We overcome these challenges by developing three
approaches built on the foundation of domain expert knowledge captured in
dictionaries and ontologies. We develop a syntactic and semantic rules-based
approach and an approach leveraging a pre-trained language model, fine-tuned
for a question-answering task on top of our base dictionary lookup to extract
entities of interest from maintenance records. We also develop a preliminary
ontology to represent and capture the semantics of maintenance records. Our
evaluations on a real-world aviation maintenance records dataset show promising
results and help identify challenges specific to named entity recognition in
the context of noisy industrial data.},
Year          = {2021},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2108.05454v1},
File          = {2108.05454v1.pdf}
}
@article{2405.15134v2,
Author        = {Akshit Achara and Sanand Sasidharan and Gagan N},
Title         = {Efficient Biomedical Entity Linking: Clinical Text Standardization with
  Low-Resource Techniques},
Eprint        = {2405.15134v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Clinical text is rich in information, with mentions of treatment, medication
and anatomy among many other clinical terms. Multiple terms can refer to the
same core concepts which can be referred as a clinical entity. Ontologies like
the Unified Medical Language System (UMLS) are developed and maintained to
store millions of clinical entities including the definitions, relations and
other corresponding information. These ontologies are used for standardization
of clinical text by normalizing varying surface forms of a clinical term
through Biomedical entity linking. With the introduction of transformer-based
language models, there has been significant progress in Biomedical entity
linking. In this work, we focus on learning through synonym pairs associated
with the entities. As compared to the existing approaches, our approach
significantly reduces the training data and resource consumption. Moreover, we
propose a suite of context-based and context-less reranking techniques for
performing the entity disambiguation. Overall, we achieve similar performance
to the state-of-the-art zero-shot and distant supervised entity linking
techniques on the Medmentions dataset, the largest annotated dataset on UMLS,
without any domain-based training. Finally, we show that retrieval performance
alone might not be sufficient as an evaluation metric and introduce an article
level quantitative and qualitative analysis to reveal further insights on the
performance of entity linking methods.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.15134v2},
File          = {2405.15134v2.pdf}
}
@article{2410.17504v1,
Author        = {Shruthi Chari},
Title         = {An Ontology-Enabled Approach For User-Centered and Knowledge-Enabled
  Explanations of AI Systems},
Eprint        = {2410.17504v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Explainable Artificial Intelligence (AI) focuses on helping humans understand
the working of AI systems or their decisions and has been a cornerstone of AI
for decades. Recent research in explainability has focused on explaining the
workings of AI models or model explainability. There have also been several
position statements and review papers detailing the needs of end-users for
user-centered explainability but fewer implementations. Hence, this thesis
seeks to bridge some gaps between model and user-centered explainability. We
create an explanation ontology (EO) to represent literature-derived explanation
types via their supporting components. We implement a knowledge-augmented
question-answering (QA) pipeline to support contextual explanations in a
clinical setting. Finally, we are implementing a system to combine explanations
from different AI methods and data modalities. Within the EO, we can represent
fifteen different explanation types, and we have tested these representations
in six exemplar use cases. We find that knowledge augmentations improve the
performance of base large language models in the contextualized QA, and the
performance is variable across disease groups. In the same setting, clinicians
also indicated that they prefer to see actionability as one of the main foci in
explanations. In our explanations combination method, we plan to use similarity
metrics to determine the similarity of explanations in a chronic disease
detection setting. Overall, through this thesis, we design methods that can
support knowledge-enabled explanations across different use cases, accounting
for the methods in today's AI era that can generate the supporting components
of these explanations and domain knowledge sources that can enhance them.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.17504v1},
File          = {2410.17504v1.pdf}
}
@article{2412.01331v1,
Author        = {Elizabeth Remfry and Rafael Henkin and Michael R Barnes and Aakanksha Naik},
Title         = {Exploring Long-Term Prediction of Type 2 Diabetes Microvascular
  Complications},
Eprint        = {2412.01331v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Electronic healthcare records (EHR) contain a huge wealth of data that can
support the prediction of clinical outcomes. EHR data is often stored and
analysed using clinical codes (ICD10, SNOMED), however these can differ across
registries and healthcare providers. Integrating data across systems involves
mapping between different clinical ontologies requiring domain expertise, and
at times resulting in data loss. To overcome this, code-agnostic models have
been proposed. We assess the effectiveness of a code-agnostic representation
approach on the task of long-term microvascular complication prediction for
individuals living with Type 2 Diabetes. Our method encodes individual EHRs as
text using fine-tuned, pretrained clinical language models. Leveraging
large-scale EHR data from the UK, we employ a multi-label approach to
simultaneously predict the risk of microvascular complications across 1-, 5-,
and 10-year windows. We demonstrate that a code-agnostic approach outperforms a
code-based model and illustrate that performance is better with longer
prediction windows but is biased to the first occurring complication. Overall,
we highlight that context length is vitally important for model performance.
This study highlights the possibility of including data from across different
clinical ontologies and is a starting point for generalisable clinical models.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.01331v1},
File          = {2412.01331v1.pdf}
}
@article{2311.07850v2,
Author        = {Dhruv Agarwal and Rajarshi Das and Sopan Khosla and Rashmi Gangadharaiah},
Title         = {Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA},
Eprint        = {2311.07850v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present BYOKG, a universal question-answering (QA) system that can operate
on any knowledge graph (KG), requires no human-annotated training data, and can
be ready to use within a day -- attributes that are out-of-scope for current
KGQA systems. BYOKG draws inspiration from the remarkable ability of humans to
comprehend information present in an unseen KG through exploration -- starting
at random nodes, inspecting the labels of adjacent nodes and edges, and
combining them with their prior world knowledge. In BYOKG, exploration
leverages an LLM-backed symbolic agent that generates a diverse set of
query-program exemplars, which are then used to ground a retrieval-augmented
reasoning procedure to predict programs for arbitrary questions. BYOKG is
effective over both small- and large-scale graphs, showing dramatic gains in QA
accuracy over a zero-shot baseline of 27.89 and 58.02 F1 on GrailQA and MetaQA,
respectively. On GrailQA, we further show that our unsupervised BYOKG
outperforms a supervised in-context learning method, demonstrating the
effectiveness of exploration. Lastly, we find that performance of BYOKG
reliably improves with continued exploration as well as improvements in the
base LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1
on a sub-sampled zero-shot split of GrailQA.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.07850v2},
File          = {2311.07850v2.pdf}
}
@article{2409.19006v2,
Author        = {Sakhinana Sagar Srinivas and Vijay Sri Vaikunth and Venkataramana Runkana},
Title         = {Towards Automated Patent Workflows: AI-Orchestrated Multi-Agent
  Framework for Intellectual Property Management and Analysis},
Eprint        = {2409.19006v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Patents are the currency of innovation, and like any currency, they need to
be managed and protected (Gavin Potenza). Patents, as legal documents that
secure intellectual property rights, play a critical role in technological
innovation. The growing complexity of patent documents and the surge in patent
applications have created a need for automated solutions in patent analysis. In
this work, we present PatExpert, an autonomous multi-agent conversational
framework designed to streamline and optimize patent-related tasks. The
framework consists of a metaagent that coordinates task-specific expert agents
for various patent-related tasks and a critique agent for error handling and
feedback provision. The meta-agent orchestrates specialized expert agents, each
fine-tuned for specific tasks such as patent classification, acceptance, claim
generation, abstractive summarization, multi-patent analysis, and scientific
hypothesis generation. For multi-patent analysis, the framework incorporates
advanced methods like Graph Retrieval-Augmented Generation (GRAG) to enhance
response accuracy and relevance by combining semantic similarity with knowledge
graphs. Error handling is managed by critique agents (Gold-LLM-as-a-Judge and
Reward-LLM-as-a-Judge), which evaluate output responses for accuracy and
provide iterative feedback. The framework also prioritizes explainability,
ensuring transparent justifications for decisions made during patent analysis.
Its comprehensive capabilities make it a valuable tool for automating complex
patent workflows, enhancing efficiency, accuracy, and compliance in
patent-related tasks. Empirical evidence demonstrates significant improvements
in patent processing tasks, concluding that the framework offers a robust
solution for automating and optimizing patent analysis.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.19006v2},
File          = {2409.19006v2.pdf}
}
@article{2412.08581v1,
Author        = {Yanqi Su and Zhenchang Xing and Chong Wang and Chunyang Chen and Xiwei Xu and Qinghua Lu and Liming Zhu},
Title         = {Automated Soap Opera Testing Directed by LLMs and Scenario Knowledge:
  Feasibility, Challenges, and Road Ahead},
Eprint        = {2412.08581v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Exploratory testing (ET) harnesses tester's knowledge, creativity, and
experience to create varying tests that uncover unexpected bugs from the
end-user's perspective. Although ET has proven effective in system-level
testing of interactive systems, the need for manual execution has hindered
large-scale adoption. In this work, we explore the feasibility, challenges and
road ahead of automated scenario-based ET (a.k.a soap opera testing). We
conduct a formative study, identifying key insights for effective manual soap
opera testing and challenges in automating the process. We then develop a
multi-agent system leveraging LLMs and a Scenario Knowledge Graph (SKG) to
automate soap opera testing. The system consists of three multi-modal agents,
Planner, Player, and Detector that collaborate to execute tests and identify
potential bugs. Experimental results demonstrate the potential of automated
soap opera testing, but there remains a significant gap compared to manual
execution, especially under-explored scenario boundaries and incorrectly
identified bugs. Based on the observation, we envision road ahead for the
future of automated soap opera testing, focusing on three key aspects: the
synergy of neural and symbolic approaches, human-AI co-learning, and the
integration of soap opera testing with broader software engineering practices.
These insights aim to guide and inspire the future research.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.08581v1},
File          = {2412.08581v1.pdf}
}
@article{2404.16198v1,
Author        = {Mojdeh Rahmanian and Seyed Mostafa Fakhrahmad and Seyedeh Zahra Mousavi},
Title         = {Towards Efficient Patient Recruitment for Clinical Trials: Application
  of a Prompt-Based Learning Model},
Eprint        = {2404.16198v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Objective: Clinical trials are essential for advancing pharmaceutical
interventions, but they face a bottleneck in selecting eligible participants.
Although leveraging electronic health records (EHR) for recruitment has gained
popularity, the complex nature of unstructured medical texts presents
challenges in efficiently identifying participants. Natural Language Processing
(NLP) techniques have emerged as a solution with a recent focus on transformer
models. In this study, we aimed to evaluate the performance of a prompt-based
large language model for the cohort selection task from unstructured medical
notes collected in the EHR. Methods: To process the medical records, we
selected the most related sentences of the records to the eligibility criteria
needed for the trial. The SNOMED CT concepts related to each eligibility
criterion were collected. Medical records were also annotated with MedCAT based
on the SNOMED CT ontology. Annotated sentences including concepts matched with
the criteria-relevant terms were extracted. A prompt-based large language model
(Generative Pre-trained Transformer (GPT) in this study) was then used with the
extracted sentences as the training set. To assess its effectiveness, we
evaluated the model's performance using the dataset from the 2018 n2c2
challenge, which aimed to classify medical records of 311 patients based on 13
eligibility criteria through NLP techniques. Results: Our proposed model showed
the overall micro and macro F measures of 0.9061 and 0.8060 which were among
the highest scores achieved by the experiments performed with this dataset.
Conclusion: The application of a prompt-based large language model in this
study to classify patients based on eligibility criteria received promising
scores. Besides, we proposed a method of extractive summarization with the aid
of SNOMED CT ontology that can be also applied to other medical texts.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.16198v1},
File          = {2404.16198v1.pdf}
}
@article{1505.01606v1,
Author        = {Harsh Thakkar and Ganesh Iyer and Prasenjit Majumder},
Title         = {A comparative study of approaches in user-centered health information
  retrieval},
Eprint        = {1505.01606v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {In this paper, we survey various user-centered or context-based biomedical
health information retrieval systems. We present and discuss the performance of
systems submitted in CLEF eHealth 2014 Task 3 for this purpose. We classify and
focus on comparing the two most prevalent retrieval models in biomedical
information retrieval namely: Language Model (LM) and Vector Space Model (VSM).
We also report on the effectiveness of using external medical resources and
ontologies like MeSH, Metamap, UMLS, etc. We observed that the L.M. based
retrieval systems outperform VSM based systems on various fronts. From the
results we conclude that the state-of-art system scores for MAP was 0.4146,
P@10 was 0.7560 and NDCG@10 was 0.7445, respectively. All of these score were
reported by systems built on language modelling approaches.},
Year          = {2015},
Month         = {May},
Url           = {http://arxiv.org/abs/1505.01606v1},
File          = {1505.01606v1.pdf}
}
@article{2210.06376v1,
Author        = {Daniel Loureiro and Alípio Mário Jorge},
Title         = {Probing Commonsense Knowledge in Pre-trained Language Models with
  Sense-level Precision and Expanded Vocabulary},
Eprint        = {2210.06376v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Progress on commonsense reasoning is usually measured from performance
improvements on Question Answering tasks designed to require commonsense
knowledge. However, fine-tuning large Language Models (LMs) on these specific
tasks does not directly evaluate commonsense learned during pre-training. The
most direct assessments of commonsense knowledge in pre-trained LMs are
arguably cloze-style tasks targeting commonsense assertions (e.g., A pen is
used for [MASK].). However, this approach is restricted by the LM's vocabulary
available for masked predictions, and its precision is subject to the context
provided by the assertion. In this work, we present a method for enriching LMs
with a grounded sense inventory (i.e., WordNet) available at the vocabulary
level, without further training. This modification augments the prediction
space of cloze-style prompts to the size of a large ontology while enabling
finer-grained (sense-level) queries and predictions. In order to evaluate LMs
with higher precision, we propose SenseLAMA, a cloze-style task featuring
verbalized relations from disambiguated triples sourced from WordNet, WikiData,
and ConceptNet. Applying our method to BERT, producing a WordNet-enriched
version named SynBERT, we find that LMs can learn non-trivial commonsense
knowledge from self-supervision, covering numerous relations, and more
effectively than comparable similarity-based approaches.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.06376v1},
File          = {2210.06376v1.pdf}
}
@article{2310.05381v1,
Author        = {Yang Liu and Melissa Xiaohui Qin and Long Wang and Chao Huang},
Title         = {CCAE: A Corpus of Chinese-based Asian Englishes},
Eprint        = {2310.05381v1},
DOI           = {10.1007/978-3-031-44696-2_48},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Language models have been foundations in various scenarios of NLP
applications, but it has not been well applied in language variety studies,
even for the most popular language like English. This paper represents one of
the few initial efforts to utilize the NLP technology in the paradigm of World
Englishes, specifically in creating a multi-variety corpus for studying Asian
Englishes. We present an overview of the CCAE -- Corpus of Chinese-based Asian
English, a suite of corpora comprising six Chinese-based Asian English
varieties. It is based on 340 million tokens in 448 thousand web documents from
six regions. The ontology of data would make the corpus a helpful resource with
enormous research potential for Asian Englishes (especially for Chinese
Englishes for which there has not been a publicly accessible corpus yet so far)
and an ideal source for variety-specific language modeling and downstream
tasks, thus setting the stage for NLP-based World Englishes studies. And
preliminary experiments on this corpus reveal the practical value of CCAE.
Finally, we make CCAE available at
\href{https://huggingface.co/datasets/CCAE/CCAE-Corpus}{this https URL}.},
Year          = {2023},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2310.05381v1},
File          = {2310.05381v1.pdf}
}
@article{2412.04529v1,
Author        = {Alexander Chervov and Anton Vakhrushev and Sergei Fironov and Loredana Martignetti},
Title         = {ProtBoost: protein function prediction with Py-Boost and Graph Neural
  Networks -- CAFA5 top2 solution},
Eprint        = {2412.04529v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {q-bio.QM},
Abstract      = {Predicting protein properties, functions and localizations are important
tasks in bioinformatics. Recent progress in machine learning offers an
opportunities for improving existing methods. We developed a new approach
called ProtBoost, which relies on the strength of pretrained protein language
models, the new Py-Boost gradient boosting method and Graph Neural Networks
(GCN). The ProtBoost method was ranked second best model in the recent Critical
Assessment of Functional Annotation (CAFA5) international challenge with more
than 1600 participants. Py-Boost is the first gradient boosting method capable
of predicting thousands of targets simultaneously, making it an ideal fit for
tasks like the CAFA challange. Our GCN-based approach performs stacking of many
individual models and boosts the performance significantly. Notably, it can be
applied to any task where targets are arranged in a hierarchical structure,
such as Gene Ontology. Additionally, we introduced new methods for leveraging
the graph structure of targets and present an analysis of protein language
models for protein function prediction task. ProtBoost is publicly available
at: https://github.com/btbpanda/CAFA5-protein-function-prediction-2nd-place.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.04529v1},
File          = {2412.04529v1.pdf}
}
@article{2501.00644v1,
Author        = {Daniel B. Hier and Michael D. Carrithers and Thanh Son Do and Tayo Obafemi-Ajayi},
Title         = {Efficient Standardization of Clinical Notes using Large Language Models},
Eprint        = {2501.00644v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Clinician notes are a rich source of patient information but often contain
inconsistencies due to varied writing styles, colloquialisms, abbreviations,
medical jargon, grammatical errors, and non-standard formatting. These
inconsistencies hinder the extraction of meaningful data from electronic health
records (EHRs), posing challenges for quality improvement, population health,
precision medicine, decision support, and research.
  We present a large language model approach to standardizing a corpus of 1,618
clinical notes. Standardization corrected an average of $4.9 +/- 1.8$
grammatical errors, $3.3 +/- 5.2$ spelling errors, converted $3.1 +/- 3.0$
non-standard terms to standard terminology, and expanded $15.8 +/- 9.1$
abbreviations and acronyms per note. Additionally, notes were re-organized into
canonical sections with standardized headings. This process prepared notes for
key concept extraction, mapping to medical ontologies, and conversion to
interoperable data formats such as FHIR.
  Expert review of randomly sampled notes found no significant data loss after
standardization. This proof-of-concept study demonstrates that standardization
of clinical notes can improve their readability, consistency, and usability,
while also facilitating their conversion into interoperable data formats.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2501.00644v1},
File          = {2501.00644v1.pdf}
}
@article{2312.07250v2,
Author        = {Lifeng Han and Serge Gladkoff and Gleb Erofeev and Irina Sorokina and Betty Galiano and Goran Nenadic},
Title         = {Neural Machine Translation of Clinical Text: An Empirical Investigation
  into Multilingual Pre-Trained Language Models and Transfer-Learning},
Eprint        = {2312.07250v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We conduct investigations on clinical text machine translation by examining
multilingual neural network models using deep learning such as Transformer
based structures. Furthermore, to address the language resource imbalance
issue, we also carry out experiments using a transfer learning methodology
based on massive multilingual pre-trained language models (MMPLMs). The
experimental results on three subtasks including 1) clinical case (CC), 2)
clinical terminology (CT), and 3) ontological concept (OC) show that our models
achieved top-level performances in the ClinSpEn-2022 shared task on
English-Spanish clinical domain data. Furthermore, our expert-based human
evaluations demonstrate that the small-sized pre-trained language model (PLM)
won over the other two extra-large language models by a large margin, in the
clinical domain fine-tuning, which finding was never reported in the field.
Finally, the transfer learning method works well in our experimental setting
using the WMT21fb model to accommodate a new language space Spanish that was
not seen at the pre-training stage within WMT21fb itself, which deserves more
exploitation for clinical knowledge transformation, e.g. to investigate into
more languages. These research findings can shed some light on domain-specific
machine translation development, especially in clinical and healthcare fields.
Further research projects can be carried out based on our work to improve
healthcare text analytics and knowledge transformation. Our data will be openly
available for research purposes at https://github.com/HECTA-UoM/ClinicalNMT},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.07250v2},
File          = {2312.07250v2.pdf}
}
@article{2406.14312v1,
Author        = {Abul Hasan and Jinge Wu and Quang Ngoc Nguyen and Salomé Andres and Imane Guellil and Huayu Zhang and Arlene Casey and Beatrice Alex and Bruce Guthrie and Honghan Wu},
Title         = {Infusing clinical knowledge into tokenisers for language models},
Eprint        = {2406.14312v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This study introduces a novel knowledge enhanced tokenisation mechanism,
K-Tokeniser, for clinical text processing. Technically, at initialisation
stage, K-Tokeniser populates global representations of tokens based on semantic
types of domain concepts (such as drugs or diseases) from either a domain
ontology like Unified Medical Language System or the training data of the task
related corpus. At training or inference stage, sentence level localised
context will be utilised for choosing the optimal global token representation
to realise the semantic-based tokenisation. To avoid pretraining using the new
tokeniser, an embedding initialisation approach is proposed to generate
representations for new tokens. Using three transformer-based language models,
a comprehensive set of experiments are conducted on four real-world datasets
for evaluating K-Tokeniser in a wide range of clinical text analytics tasks
including clinical concept and relation extraction, automated clinical coding,
clinical phenotype identification, and clinical research article
classification. Overall, our models demonstrate consistent improvements over
their counterparts in all tasks. In particular, substantial improvements are
observed in the automated clinical coding task with 13\% increase on Micro
$F_1$ score. Furthermore, K-Tokeniser also shows significant capacities in
facilitating quicker converge of language models. Specifically, using
K-Tokeniser, the language models would only require 50\% of the training data
to achieve the best performance of the baseline tokeniser using all training
data in the concept extraction task and less than 20\% of the data for the
automated coding task. It is worth mentioning that all these improvements
require no pre-training process, making the approach generalisable.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.14312v1},
File          = {2406.14312v1.pdf}
}
@article{1805.03947v2,
Author        = {Paolo Cifariello and Paolo Ferragina and Marco Ponza},
Title         = {WISER: A Semantic Approach for Expert Finding in Academia based on
  Entity Linking},
Eprint        = {1805.03947v2},
DOI           = {10.1016/j.is.2018.12.003},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {We present WISER, a new semantic search engine for expert finding in
academia. Our system is unsupervised and it jointly combines classical language
modeling techniques, based on text evidences, with the Wikipedia Knowledge
Graph, via entity linking.
  WISER indexes each academic author through a novel profiling technique which
models her expertise with a small, labeled and weighted graph drawn from
Wikipedia. Nodes in this graph are the Wikipedia entities mentioned in the
author's publications, whereas the weighted edges express the semantic
relatedness among these entities computed via textual and graph-based
relatedness functions. Every node is also labeled with a relevance score which
models the pertinence of the corresponding entity to author's expertise, and is
computed by means of a proper random-walk calculation over that graph; and with
a latent vector representation which is learned via entity and other kinds of
structural embeddings derived from Wikipedia.
  At query time, experts are retrieved by combining classic document-centric
approaches, which exploit the occurrences of query terms in the author's
documents, with a novel set of profile-centric scoring strategies, which
compute the semantic relatedness between the author's expertise and the query
topic via the above graph-based profiles.
  The effectiveness of our system is established over a large-scale
experimental test on a standard dataset for this task. We show that WISER
achieves better performance than all the other competitors, thus proving the
effectiveness of modelling author's profile via our "semantic" graph of
entities. Finally, we comment on the use of WISER for indexing and profiling
the whole research community within the University of Pisa, and its application
to technology transfer in our University.},
Year          = {2018},
Month         = {May},
Note          = {Information Systems, Elsevier (2019)},
Url           = {http://arxiv.org/abs/1805.03947v2},
File          = {1805.03947v2.pdf}
}
@article{1911.06136v3,
Author        = {Xiaozhi Wang and Tianyu Gao and Zhaocheng Zhu and Zhengyan Zhang and Zhiyuan Liu and Juanzi Li and Jian Tang},
Title         = {KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language
  Representation},
Eprint        = {1911.06136v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pre-trained language representation models (PLMs) cannot well capture factual
knowledge from text. In contrast, knowledge embedding (KE) methods can
effectively represent the relational facts in knowledge graphs (KGs) with
informative entity embeddings, but conventional KE models cannot take full
advantage of the abundant textual information. In this paper, we propose a
unified model for Knowledge Embedding and Pre-trained LanguagE Representation
(KEPLER), which can not only better integrate factual knowledge into PLMs but
also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we
encode textual entity descriptions with a PLM as their embeddings, and then
jointly optimize the KE and language modeling objectives. Experimental results
show that KEPLER achieves state-of-the-art performances on various NLP tasks,
and also works remarkably well as an inductive KE model on KG link prediction.
Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M, a
large-scale KG dataset with aligned entity descriptions, and benchmark
state-of-the-art KE methods on it. It shall serve as a new KE benchmark and
facilitate the research on large KG, inductive KE, and KG with text. The source
code can be obtained from https://github.com/THU-KEG/KEPLER.},
Year          = {2019},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1911.06136v3},
File          = {1911.06136v3.pdf}
}
@article{2302.03950v1,
Author        = {Yun Luo and Zihan Liu and Stan Z. Li and Yue Zhang},
Title         = {Improving (Dis)agreement Detection with Inductive Social Relation
  Information From Comment-Reply Interactions},
Eprint        = {2302.03950v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {(Dis)agreement detection aims to identify the authors' attitudes or positions
(\textit{{agree, disagree, neutral}}) towards a specific text. It is limited
for existing methods merely using textual information for identifying
(dis)agreements, especially for cross-domain settings. Social relation
information can play an assistant role in the (dis)agreement task besides
textual information. We propose a novel method to extract such relation
information from (dis)agreement data into an inductive social relation graph,
merely using the comment-reply pairs without any additional platform-specific
information. The inductive social relation globally considers the historical
discussion and the relation between authors. Textual information based on a
pre-trained language model and social relation information encoded by
pre-trained RGCN are jointly considered for (dis)agreement detection.
Experimental results show that our model achieves state-of-the-art performance
for both the in-domain and cross-domain tasks on the benchmark -- DEBAGREEMENT.
We find social relations can boost the performance of the (dis)agreement
detection model, especially for the long-token comment-reply pairs,
demonstrating the effectiveness of the social relation graph. We also explore
the effect of the knowledge graph embedding methods, the information fusing
method, and the time interval in constructing the social relation graph, which
shows the effectiveness of our model.},
Year          = {2023},
Month         = {Feb},
Note          = {WWW 2023},
Url           = {http://arxiv.org/abs/2302.03950v1},
File          = {2302.03950v1.pdf}
}
@article{1912.11270v7,
Author        = {Ahmad Sakor and Kuldeep Singh and Anery Patel and Maria-Esther Vidal},
Title         = {Falcon 2.0: An Entity and Relation Linking Tool over Wikidata},
Eprint        = {1912.11270v7},
DOI           = {10.1145/3340531.3412777},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The Natural Language Processing (NLP) community has significantly contributed
to the solutions for entity and relation recognition from the text, and
possibly linking them to proper matches in Knowledge Graphs (KGs). Considering
Wikidata as the background KG, still, there are limited tools to link knowledge
within the text to Wikidata. In this paper, we present Falcon 2.0, first joint
entity, and relation linking tool over Wikidata. It receives a short natural
language text in the English language and outputs a ranked list of entities and
relations annotated with the proper candidates in Wikidata. The candidates are
represented by their Internationalized Resource Identifier (IRI) in Wikidata.
Falcon 2.0 resorts to the English language model for the recognition task
(e.g., N-Gram tiling and N-Gram splitting), and then an optimization approach
for linking task. We have empirically studied the performance of Falcon 2.0 on
Wikidata and concluded that it outperforms all the existing baselines. Falcon
2.0 is public and can be reused by the community; all the required instructions
of Falcon 2.0 are well-documented at our GitHub repository. We also demonstrate
an online API, which can be run without any technical expertise. Falcon 2.0 and
its background knowledge bases are available as resources at
https://labs.tib.eu/falcon/falcon2/.},
Year          = {2019},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1912.11270v7},
File          = {1912.11270v7.pdf}
}
@article{2103.13009v1,
Author        = {Nicholas Lourie and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
Title         = {UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New
  Multitask Benchmark},
Eprint        = {2103.13009v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Commonsense AI has long been seen as a near impossible goal -- until
recently. Now, research interest has sharply increased with an influx of new
benchmarks and models.
  We propose two new ways to evaluate commonsense models, emphasizing their
generality on new tasks and building on diverse, recently introduced
benchmarks. First, we propose a new multitask benchmark, RAINBOW, to promote
research on commonsense models that generalize well over multiple tasks and
datasets. Second, we propose a novel evaluation, the cost equivalent curve,
that sheds new insight on how the choice of source datasets, pretrained
language models, and transfer learning methods impacts performance and data
efficiency.
  We perform extensive experiments -- over 200 experiments encompassing 4800
models -- and report multiple valuable and sometimes surprising findings, e.g.,
that transfer almost always leads to better or equivalent performance if
following a particular recipe, that QA-based commonsense datasets transfer well
with each other, while commonsense knowledge graphs do not, and that perhaps
counter-intuitively, larger models benefit more from transfer than smaller
ones.
  Last but not least, we introduce a new universal commonsense reasoning model,
UNICORN, that establishes new state-of-the-art performance across 8 popular
commonsense benchmarks, aNLI (87.3%), CosmosQA (91.8%), HellaSWAG (93.9%), PIQA
(90.1%), SocialIQa (83.2%), WinoGrande (86.6%), CycIC (94.0%) and CommonsenseQA
(79.3%).},
Year          = {2021},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2103.13009v1},
File          = {2103.13009v1.pdf}
}
@article{2208.12461v1,
Author        = {Guanming Xiong and Junwei Bao and Wen Zhao and Youzheng Wu and Xiaodong He},
Title         = {AutoQGS: Auto-Prompt for Low-Resource Knowledge-based Question
  Generation from SPARQL},
Eprint        = {2208.12461v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This study investigates the task of knowledge-based question generation
(KBQG). Conventional KBQG works generated questions from fact triples in the
knowledge graph, which could not express complex operations like aggregation
and comparison in SPARQL. Moreover, due to the costly annotation of large-scale
SPARQL-question pairs, KBQG from SPARQL under low-resource scenarios urgently
needs to be explored. Recently, since the generative pre-trained language
models (PLMs) typically trained in natural language (NL)-to-NL paradigm have
been proven effective for low-resource generation, e.g., T5 and BART, how to
effectively utilize them to generate NL-question from non-NL SPARQL is
challenging. To address these challenges, AutoQGS, an auto-prompt approach for
low-resource KBQG from SPARQL, is proposed. Firstly, we put forward to generate
questions directly from SPARQL for the KBQG task to handle complex operations.
Secondly, we propose an auto-prompter trained on large-scale unsupervised data
to rephrase SPARQL into NL description, smoothing the low-resource
transformation from non-NL SPARQL to NL question with PLMs. Experimental
results on the WebQuestionsSP, ComlexWebQuestions 1.1, and PathQuestions show
that our model achieves state-of-the-art performance, especially in
low-resource settings. Furthermore, a corpus of 330k factoid complex
question-SPARQL pairs is generated for further KBQG research.},
Year          = {2022},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2208.12461v1},
File          = {2208.12461v1.pdf}
}
@article{2102.00827v1,
Author        = {Albert Weichselbraun and Jakob Steixner and Adrian M. P. Braşoveanu and Arno Scharl and Max Göbel and Lyndon J. B. Nixon},
Title         = {Automatic Expansion of Domain-Specific Affective Models for Web
  Intelligence Applications},
Eprint        = {2102.00827v1},
DOI           = {10.1007/s12559-021-09839-4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Sentic computing relies on well-defined affective models of different
complexity - polarity to distinguish positive and negative sentiment, for
example, or more nuanced models to capture expressions of human emotions. When
used to measure communication success, even the most granular affective model
combined with sophisticated machine learning approaches may not fully capture
an organisation's strategic positioning goals. Such goals often deviate from
the assumptions of standardised affective models. While certain emotions such
as Joy and Trust typically represent desirable brand associations, specific
communication goals formulated by marketing professionals often go beyond such
standard dimensions. For instance, the brand manager of a television show may
consider fear or sadness to be desired emotions for its audience. This article
introduces expansion techniques for affective models, combining common and
commonsense knowledge available in knowledge graphs with language models and
affective reasoning, improving coverage and consistency as well as supporting
domain-specific interpretations of emotions. An extensive evaluation compares
the performance of different expansion techniques: (i) a quantitative
evaluation based on the revisited Hourglass of Emotions model to assess
performance on complex models that cover multiple affective categories, using
manually compiled gold standard data, and (ii) a qualitative evaluation of a
domain-specific affective model for television programme brands. The results of
these evaluations demonstrate that the introduced techniques support a variety
of embeddings and pre-trained models. The paper concludes with a discussion on
applying this approach to other scenarios where affective model resources are
scarce.},
Year          = {2021},
Month         = {Feb},
Note          = {Cognitive Computation, (2021), 1-18},
Url           = {http://arxiv.org/abs/2102.00827v1},
File          = {2102.00827v1.pdf}
}
@article{2102.05474v1,
Author        = {Zhuosheng Zhang and Junlong Li and Hai Zhao},
Title         = {Multi-turn Dialogue Reading Comprehension with Pivot Turns and Knowledge},
Eprint        = {2102.05474v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Multi-turn dialogue reading comprehension aims to teach machines to read
dialogue contexts and solve tasks such as response selection and answering
questions. The major challenges involve noisy history contexts and especial
prerequisites of commonsense knowledge that is unseen in the given material.
Existing works mainly focus on context and response matching approaches. This
work thus makes the first attempt to tackle the above two challenges by
extracting substantially important turns as pivot utterances and utilizing
external knowledge to enhance the representation of context. We propose a
pivot-oriented deep selection model (PoDS) on top of the Transformer-based
language models for dialogue comprehension. In detail, our model first picks
out the pivot utterances from the conversation history according to the
semantic matching with the candidate response or question, if any. Besides,
knowledge items related to the dialogue context are extracted from a knowledge
graph as external knowledge. Then, the pivot utterances and the external
knowledge are combined with a well-designed mechanism for refining predictions.
Experimental results on four dialogue comprehension benchmark tasks show that
our proposed model achieves great improvements on baselines. A series of
empirical comparisons are conducted to show how our selection strategies and
the extra knowledge injection influence the results.},
Year          = {2021},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2102.05474v1},
File          = {2102.05474v1.pdf}
}
@article{2102.09460v1,
Author        = {Daheng Wang and Prashant Shiralkar and Colin Lockard and Binxuan Huang and Xin Luna Dong and Meng Jiang},
Title         = {TCN: Table Convolutional Network for Web Table Interpretation},
Eprint        = {2102.09460v1},
DOI           = {10.1145/3442381.3450090},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Information extraction from semi-structured webpages provides valuable
long-tailed facts for augmenting knowledge graph. Relational Web tables are a
critical component containing additional entities and attributes of rich and
diverse knowledge. However, extracting knowledge from relational tables is
challenging because of sparse contextual information. Existing work linearize
table cells and heavily rely on modifying deep language models such as BERT
which only captures related cells information in the same table. In this work,
we propose a novel relational table representation learning approach
considering both the intra- and inter-table contextual information. On one
hand, the proposed Table Convolutional Network model employs the attention
mechanism to adaptively focus on the most informative intra-table cells of the
same row or column; and, on the other hand, it aggregates inter-table
contextual information from various types of implicit connections between cells
across different tables. Specifically, we propose three novel aggregation
modules for (i) cells of the same value, (ii) cells of the same schema
position, and (iii) cells linked to the same page topic. We further devise a
supervised multi-task training objective for jointly predicting column type and
pairwise column relation, as well as a table cell recovery objective for
pre-training. Experiments on real Web table datasets demonstrate our method can
outperform competitive baselines by +4.8% of F1 for column type prediction and
by +4.1% of F1 for pairwise column relation prediction.},
Year          = {2021},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2102.09460v1},
File          = {2102.09460v1.pdf}
}
@article{2108.11063v1,
Author        = {Hyundong Cho and Basel Shbita and Kartik Shenoy and Shuai Liu and Nikhil Patel and Hitesh Pindikanti and Jennifer Lee and Jonathan May},
Title         = {Viola: A Topic Agnostic Generate-and-Rank Dialogue System},
Eprint        = {2108.11063v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We present Viola, an open-domain dialogue system for spoken conversation that
uses a topic-agnostic dialogue manager based on a simple generate-and-rank
approach. Leveraging recent advances of generative dialogue systems powered by
large language models, Viola fetches a batch of response candidates from
various neural dialogue models trained with different datasets and
knowledge-grounding inputs. Additional responses originating from
template-based generators are also considered, depending on the user's input
and detected entities. The hand-crafted generators build on a dynamic knowledge
graph injected with rich content that is crawled from the web and automatically
processed on a daily basis. Viola's response ranker is a fine-tuned polyencoder
that chooses the best response given the dialogue history. While dedicated
annotations for the polyencoder alone can indirectly steer it away from
choosing problematic responses, we add rule-based safety nets to detect neural
degeneration and a dedicated classifier to filter out offensive content. We
analyze conversations that Viola took part in for the Alexa Prize Socialbot
Grand Challenge 4 and discuss the strengths and weaknesses of our approach.
Lastly, we suggest future work with a focus on curating conversation data
specifcially for socialbots that will contribute towards a more robust
data-driven socialbot.},
Year          = {2021},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2108.11063v1},
File          = {2108.11063v1.pdf}
}
@article{2108.13934v2,
Author        = {Michael Glass and Gaetano Rossiello and Md Faisal Mahbub Chowdhury and Alfio Gliozzo},
Title         = {Robust Retrieval Augmented Generation for Zero-shot Slot Filling},
Eprint        = {2108.13934v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Automatically inducing high quality knowledge graphs from a given collection
of documents still remains a challenging problem in AI. One way to make headway
for this problem is through advancements in a related task known as slot
filling. In this task, given an entity query in form of [Entity, Slot, ?], a
system is asked to fill the slot by generating or extracting the missing value
exploiting evidence extracted from relevant passage(s) in the given document
collection. The recent works in the field try to solve this task in an
end-to-end fashion using retrieval-based language models. In this paper, we
present a novel approach to zero-shot slot filling that extends dense passage
retrieval with hard negatives and robust training procedures for retrieval
augmented generation models. Our model reports large improvements on both T-REx
and zsRE slot filling datasets, improving both passage retrieval and slot value
generation, and ranking at the top-1 position in the KILT leaderboard.
Moreover, we demonstrate the robustness of our system showing its domain
adaptation capability on a new variant of the TACRED dataset for slot filling,
through a combination of zero/few-shot learning. We release the source code and
pre-trained models.},
Year          = {2021},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2108.13934v2},
File          = {2108.13934v2.pdf}
}
@article{2110.03192v1,
Author        = {Kuan Wang and Yuyu Zhang and Diyi Yang and Le Song and Tao Qin},
Title         = {GNN is a Counter? Revisiting GNN for Question Answering},
Eprint        = {2110.03192v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Question Answering (QA) has been a long-standing research topic in AI and NLP
fields, and a wealth of studies have been conducted to attempt to equip QA
systems with human-level reasoning capability. To approximate the complicated
human reasoning process, state-of-the-art QA systems commonly use pre-trained
language models (LMs) to access knowledge encoded in LMs together with
elaborately designed modules based on Graph Neural Networks (GNNs) to perform
reasoning over knowledge graphs (KGs). However, many problems remain open
regarding the reasoning functionality of these GNN-based modules. Can these
GNN-based modules really perform a complex reasoning process? Are they under-
or over-complicated for QA? To open the black box of GNN and investigate these
problems, we dissect state-of-the-art GNN modules for QA and analyze their
reasoning capability. We discover that even a very simple graph neural counter
can outperform all the existing GNN modules on CommonsenseQA and OpenBookQA,
two popular QA benchmark datasets which heavily rely on knowledge-aware
reasoning. Our work reveals that existing knowledge-aware GNN modules may only
carry out some simple reasoning such as counting. It remains a challenging open
problem to build comprehensive reasoning modules for knowledge-powered QA.},
Year          = {2021},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2110.03192v1},
File          = {2110.03192v1.pdf}
}
@article{2111.07658v4,
Author        = {Hanyu Zhao and Sha Yuan and Jiahong Leng and Xiang Pan and Guoqiang Wang and Ledell Wu and Jie Tang},
Title         = {Calculating Question Similarity is Enough: A New Method for KBQA Tasks},
Eprint        = {2111.07658v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Base Question Answering (KBQA) aims to answer natural language
questions with the help of an external knowledge base. The core idea is to find
the link between the internal knowledge behind questions and known triples of
the knowledge base. Traditional KBQA task pipelines contain several steps,
including entity recognition, entity linking, answering selection, etc. In this
kind of pipeline methods, errors in any procedure will inevitably propagate to
the final prediction. To address this challenge, this paper proposes a Corpus
Generation - Retrieve Method (CGRM) with Pre-training Language Model (PLM) for
the KBQA task. The major novelty lies in the design of the new method, wherein
our approach, the knowledge enhanced T5 (kT5) model aims to generate natural
language QA pairs based on Knowledge Graph triples and directly solve the QA by
retrieving the synthetic dataset. The new method can extract more information
about the entities from PLM to improve accuracy and simplify the processes. We
test our method on NLPCC-ICCPOL 2016 KBQA dataset, and the results show that
our method improves the performance of KBQA and the out straight-forward method
is competitive with the state-of-the-art.},
Year          = {2021},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2111.07658v4},
File          = {2111.07658v4.pdf}
}
@article{2112.06318v3,
Author        = {PeiFeng Wang and Jonathan Zamora and Junfeng Liu and Filip Ilievski and Muhao Chen and Xiang Ren},
Title         = {Contextualized Scene Imagination for Generative Commonsense Reasoning},
Eprint        = {2112.06318v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Humans use natural language to compose common concepts from their environment
into plausible, day-to-day scene descriptions. However, such generative
commonsense reasoning (GCSR) skills are lacking in state-of-the-art text
generation methods. Descriptive sentences about arbitrary concepts generated by
neural text generation models (e.g., pre-trained text-to-text Transformers) are
often grammatically fluent but may not correspond to human common sense,
largely due to their lack of mechanisms to capture concept relations, to
identify implicit concepts, and to perform generalizable reasoning about unseen
concept compositions. In this paper, we propose an Imagine-and-Verbalize (I&V)
method, which learns to imagine a relational scene knowledge graph (SKG) with
relations between the input concepts, and leverage the SKG as a constraint when
generating a plausible scene description. We collect and harmonize a set of
knowledge resources from different domains and modalities, providing a rich
auxiliary supervision signal for I&V. The experiments demonstrate the
effectiveness of I&V in improving language models on both concept-to-sentence
and concept-to-story generation tasks, while enabling the model to learn well
from fewer task examples and generate SKGs that make common sense to human
annotators.},
Year          = {2021},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2112.06318v3},
File          = {2112.06318v3.pdf}
}
@article{2112.06888v1,
Author        = {Diego Garcia-Olano and Yasumasa Onoe and Joydeep Ghosh},
Title         = {Improving and Diagnosing Knowledge-Based Visual Question Answering via
  Entity Enhanced Knowledge Injection},
Eprint        = {2112.06888v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge-Based Visual Question Answering (KBVQA) is a bi-modal task
requiring external world knowledge in order to correctly answer a text question
and associated image. Recent single modality text work has shown knowledge
injection into pre-trained language models, specifically entity enhanced
knowledge graph embeddings, can improve performance on downstream
entity-centric tasks. In this work, we empirically study how and whether such
methods, applied in a bi-modal setting, can improve an existing VQA system's
performance on the KBVQA task. We experiment with two large publicly available
VQA datasets, (1) KVQA which contains mostly rare Wikipedia entities and (2)
OKVQA which is less entity-centric and more aligned with common sense
reasoning. Both lack explicit entity spans and we study the effect of different
weakly supervised and manual methods for obtaining them. Additionally we
analyze how recently proposed bi-modal and single modal attention explanations
are affected by the incorporation of such entity enhanced representations. Our
results show substantial improved performance on the KBVQA task without the
need for additional costly pre-training and we provide insights for when entity
knowledge injection helps improve a model's understanding. We provide code and
enhanced datasets for reproducibility.},
Year          = {2021},
Month         = {Dec},
Note          = {Proceedings of the 1st International Workshop on Multimodal
  Understanding for the Web and Social Media, co-located with the Web
  Conference 2022 (WWW '22 Companion), April 25--29, 2022, Virtual Event, Lyon,
  France},
Url           = {http://arxiv.org/abs/2112.06888v1},
File          = {2112.06888v1.pdf}
}
@article{2202.11345v2,
Author        = {Yi Zhu and Xinke Zhou and Jipeng Qiang and Yun Li and Yunhao Yuan and Xindong Wu},
Title         = {Prompt-Learning for Short Text Classification},
Eprint        = {2202.11345v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In the short text, the extremely short length, feature sparsity, and high
ambiguity pose huge challenges to classification tasks. Recently, as an
effective method for tuning Pre-trained Language Models for specific downstream
tasks, prompt-learning has attracted a vast amount of attention and research.
The main intuition behind the prompt-learning is to insert the template into
the input and convert the text classification tasks into equivalent cloze-style
tasks. However, most prompt-learning methods expand label words manually or
only consider the class name for knowledge incorporating in cloze-style
prediction, which will inevitably incur omissions and bias in short text
classification tasks. In this paper, we propose a simple short text
classification approach that makes use of prompt-learning based on
knowledgeable expansion. Taking the special characteristics of short text into
consideration, the method can consider both the short text itself and class
name during expanding label words space. Specifically, the top $N$ concepts
related to the entity in the short text are retrieved from the open Knowledge
Graph like Probase, and we further refine the expanded label words by the
distance calculation between selected concepts and class labels. Experimental
results show that our approach obtains obvious improvement compared with other
fine-tuning, prompt-learning, and knowledgeable prompt-tuning methods,
outperforming the state-of-the-art by up to 6 Accuracy points on three
well-known datasets.},
Year          = {2022},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2202.11345v2},
File          = {2202.11345v2.pdf}
}
@article{2206.01532v2,
Author        = {Mutian He and Tianqing Fang and Weiqi Wang and Yangqiu Song},
Title         = {Acquiring and Modelling Abstract Commonsense Knowledge via
  Conceptualization},
Eprint        = {2206.01532v2},
DOI           = {10.1016/j.artint.2024.104149},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Conceptualization, or viewing entities and situations as instances of
abstract concepts in mind and making inferences based on that, is a vital
component in human intelligence for commonsense reasoning. Despite recent
progress in artificial intelligence to acquire and model commonsense attributed
to neural language models and commonsense knowledge graphs (CKGs),
conceptualization is yet to be introduced thoroughly, making current approaches
ineffective to cover knowledge about countless diverse entities and situations
in the real world.
  To address the problem, we thoroughly study the role of conceptualization in
commonsense reasoning, and formulate a framework to replicate human conceptual
induction by acquiring abstract knowledge about events regarding abstract
concepts, as well as higher-level triples or inferences upon them. We then
apply the framework to ATOMIC, a large-scale human-annotated CKG, aided by the
taxonomy Probase. We annotate a dataset on the validity of contextualized
conceptualizations from ATOMIC on both event and triple levels, develop a
series of heuristic rules based on linguistic features, and train a set of
neural models to generate and verify abstract knowledge. Based on these
components, a pipeline to acquire abstract knowledge is built. A large abstract
CKG upon ATOMIC is then induced, ready to be instantiated to infer about unseen
entities or situations. Finally, we empirically show the benefits of augmenting
CKGs with abstract knowledge in downstream tasks like commonsense inference and
zero-shot commonsense QA.},
Year          = {2022},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2206.01532v2},
File          = {2206.01532v2.pdf}
}
@article{2206.05706v2,
Author        = {Rachit Bansal and Milan Aggarwal and Sumit Bhatia and Jivat Neet Kaur and Balaji Krishnamurthy},
Title         = {CoSe-Co: Text Conditioned Generative CommonSense Contextualizer},
Eprint        = {2206.05706v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pre-trained Language Models (PTLMs) have been shown to perform well on
natural language tasks. Many prior works have leveraged structured commonsense
present in the form of entities linked through labeled relations in Knowledge
Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static
module which limits coverage since KGs contain finite knowledge. Generative
methods train PTLMs on KG triples to improve the scale at which knowledge can
be obtained. However, training on symbolic KG entities limits their
applicability in tasks involving natural language text where they ignore
overall context. To mitigate this, we propose a CommonSense Contextualizer
(CoSe-Co) conditioned on sentences as input to make it generically usable in
tasks for generating knowledge relevant to the overall context of input text.
To train CoSe-Co, we propose a novel dataset comprising of sentence and
commonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and
contain novel entities not present in the underlying KG. We augment generated
knowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading
to improvements over current best methods on CSQA, ARC, QASC and OBQA datasets.
We also demonstrate its applicability in improving performance of a baseline
model for paraphrase generation task.},
Year          = {2022},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2206.05706v2},
File          = {2206.05706v2.pdf}
}
@article{2207.01328v4,
Author        = {Zhuo Chen and Yufeng Huang and Jiaoyan Chen and Yuxia Geng and Wen Zhang and Yin Fang and Jeff Z. Pan and Huajun Chen},
Title         = {DUET: Cross-modal Semantic Grounding for Contrastive Zero-shot Learning},
Eprint        = {2207.01328v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Zero-shot learning (ZSL) aims to predict unseen classes whose samples have
never appeared during training. One of the most effective and widely used
semantic information for zero-shot image classification are attributes which
are annotations for class-level visual characteristics. However, the current
methods often fail to discriminate those subtle visual distinctions between
images due to not only the shortage of fine-grained annotations, but also the
attribute imbalance and co-occurrence. In this paper, we present a
transformer-based end-to-end ZSL method named DUET, which integrates latent
semantic knowledge from the pre-trained language models (PLMs) via a
self-supervised multi-modal learning paradigm. Specifically, we (1) developed a
cross-modal semantic grounding network to investigate the model's capability of
disentangling semantic attributes from the images; (2) applied an
attribute-level contrastive learning strategy to further enhance the model's
discrimination on fine-grained visual characteristics against the attribute
co-occurrence and imbalance; (3) proposed a multi-task learning policy for
considering multi-model objectives. We find that our DUET can achieve
state-of-the-art performance on three standard ZSL benchmarks and a knowledge
graph equipped ZSL benchmark. Its components are effective and its predictions
are interpretable.},
Year          = {2022},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2207.01328v4},
File          = {2207.01328v4.pdf}
}
@article{2210.11298v2,
Author        = {Zhuo Chen and Wen Zhang and Yufeng Huang and Mingyang Chen and Yuxia Geng and Hongtao Yu and Zhen Bi and Yichi Zhang and Zhen Yao and Wenting Song and Xinliang Wu and Yi Yang and Mingyi Chen and Zhaoyang Lian and Yingying Li and Lei Cheng and Huajun Chen},
Title         = {Tele-Knowledge Pre-training for Fault Analysis},
Eprint        = {2210.11298v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {In this work, we share our experience on tele-knowledge pre-training for
fault analysis, a crucial task in telecommunication applications that requires
a wide range of knowledge normally found in both machine log data and product
documents. To organize this knowledge from experts uniformly, we propose to
create a Tele-KG (tele-knowledge graph). Using this valuable data, we further
propose a tele-domain language pre-training model TeleBERT and its
knowledge-enhanced version, a tele-knowledge re-training model KTeleBERT. which
includes effective prompt hints, adaptive numerical data encoding, and two
knowledge injection paradigms. Concretely, our proposal includes two stages:
first, pre-training TeleBERT on 20 million tele-related corpora, and then
re-training it on 1 million causal and machine-related corpora to obtain
KTeleBERT. Our evaluation on multiple tasks related to fault analysis in
tele-applications, including root-cause analysis, event association prediction,
and fault chain tracing, shows that pre-training a language model with
tele-domain data is beneficial for downstream tasks. Moreover, the KTeleBERT
re-training further improves the performance of task models, highlighting the
effectiveness of incorporating diverse tele-knowledge into the model.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.11298v2},
File          = {2210.11298v2.pdf}
}
@article{2212.00975v2,
Author        = {Jinyoung Park and Hyeong Kyu Choi and Juyeon Ko and Hyeonjin Park and Ji-Hoon Kim and Jisu Jeong and Kyungmin Kim and Hyunwoo J. Kim},
Title         = {Relation-Aware Language-Graph Transformer for Question Answering},
Eprint        = {2212.00975v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Question Answering (QA) is a task that entails reasoning over natural
language contexts, and many relevant works augment language models (LMs) with
graph neural networks (GNNs) to encode the Knowledge Graph (KG) information.
However, most existing GNN-based modules for QA do not take advantage of rich
relational information of KGs and depend on limited information interaction
between the LM and the KG. To address these issues, we propose Question
Answering Transformer (QAT), which is designed to jointly reason over language
and graphs with respect to entity relations in a unified manner. Specifically,
QAT constructs Meta-Path tokens, which learn relation-centric embeddings based
on diverse structural and semantic relations. Then, our Relation-Aware
Self-Attention module comprehensively integrates different modalities via the
Cross-Modal Relative Position Bias, which guides information exchange between
relevant entites of different modalities. We validate the effectiveness of QAT
on commonsense question answering datasets like CommonsenseQA and OpenBookQA,
and on a medical question answering dataset, MedQA-USMLE. On all the datasets,
our method achieves state-of-the-art performance. Our code is available at
http://github.com/mlvlab/QAT.},
Year          = {2022},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2212.00975v2},
File          = {2212.00975v2.pdf}
}
@article{2212.01739v1,
Author        = {Qi Zhu and Fei Mi and Zheng Zhang and Yasheng Wang and Yitong Li and Xin Jiang and Qun Liu and Xiaoyan Zhu and Minlie Huang},
Title         = {KPT: Keyword-guided Pre-training for Grounded Dialog Generation},
Eprint        = {2212.01739v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Incorporating external knowledge into the response generation process is
essential to building more helpful and reliable dialog agents. However,
collecting knowledge-grounded conversations is often costly, calling for a
better pre-trained model for grounded dialog generation that generalizes well
w.r.t. different types of knowledge. In this work, we propose KPT
(Keyword-guided Pre-Training), a novel self-supervised pre-training method for
grounded dialog generation without relying on extra knowledge annotation.
Specifically, we use a pre-trained language model to extract the most uncertain
tokens in the dialog as keywords. With these keywords, we construct two kinds
of knowledge and pre-train a knowledge-grounded response generation model,
aiming at handling two different scenarios: (1) the knowledge should be
faithfully grounded; (2) it can be selectively used. For the former, the
grounding knowledge consists of keywords extracted from the response. For the
latter, the grounding knowledge is additionally augmented with keywords
extracted from other utterances in the same dialog. Since the knowledge is
extracted from the dialog itself, KPT can be easily performed on a large volume
and variety of dialogue data. We considered three data sources (open-domain,
task-oriented, conversational QA) with a total of 2.5M dialogues. We conduct
extensive experiments on various few-shot knowledge-grounded generation tasks,
including grounding on dialog acts, knowledge graphs, persona descriptions, and
Wikipedia passages. Our comprehensive experiments and analyses demonstrate that
KPT consistently outperforms state-of-the-art methods on these tasks with
diverse grounding knowledge.},
Year          = {2022},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2212.01739v1},
File          = {2212.01739v1.pdf}
}
@article{2212.05221v2,
Author        = {Ziniu Hu and Ahmet Iscen and Chen Sun and Zirui Wang and Kai-Wei Chang and Yizhou Sun and Cordelia Schmid and David A. Ross and Alireza Fathi},
Title         = {REVEAL: Retrieval-Augmented Visual-Language Pre-Training with
  Multi-Source Multimodal Knowledge Memory},
Eprint        = {2212.05221v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {In this paper, we propose an end-to-end Retrieval-Augmented Visual Language
Model (REVEAL) that learns to encode world knowledge into a large-scale memory,
and to retrieve from it to answer knowledge-intensive queries. REVEAL consists
of four key components: the memory, the encoder, the retriever and the
generator. The large-scale memory encodes various sources of multimodal world
knowledge (e.g. image-text pairs, question answering pairs, knowledge graph
triplets, etc) via a unified encoder. The retriever finds the most relevant
knowledge entries in the memory, and the generator fuses the retrieved
knowledge with the input query to produce the output. A key novelty in our
approach is that the memory, encoder, retriever and generator are all
pre-trained end-to-end on a massive amount of data. Furthermore, our approach
can use a diverse set of multimodal knowledge sources, which is shown to result
in significant gains. We show that REVEAL achieves state-of-the-art results on
visual question answering and image captioning.},
Year          = {2022},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2212.05221v2},
File          = {2212.05221v2.pdf}
}
@article{2212.10465v3,
Author        = {Hyunwoo Kim and Jack Hessel and Liwei Jiang and Peter West and Ximing Lu and Youngjae Yu and Pei Zhou and Ronan Le Bras and Malihe Alikhani and Gunhee Kim and Maarten Sap and Yejin Choi},
Title         = {SODA: Million-scale Dialogue Distillation with Social Commonsense
  Contextualization},
Eprint        = {2212.10465v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Data scarcity has been a long standing issue in the field of open-domain
social dialogue. To quench this thirst, we present SODA: the first publicly
available, million-scale high-quality social dialogue dataset. By
contextualizing social commonsense knowledge from a knowledge graph, we are
able to distill an exceptionally broad spectrum of social interactions from a
large language model. Human evaluation shows that conversations in SODA are
more consistent, specific, and (surprisingly) natural than those in prior
human-authored datasets.
  Using SODA, we train COSMO: a generalizable conversation model that is
significantly more natural and consistent on unseen datasets than
best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna).
Experiments reveal COSMO is sometimes even preferred to the original
human-written gold responses. Additionally, our results shed light on the
distinction between knowledge-enriched conversations and natural social
chitchats. We plan to make our data, model, and code public.},
Year          = {2022},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2212.10465v3},
File          = {2212.10465v3.pdf}
}
@article{2303.05342v1,
Author        = {Tianyu Yu and Yangning Li and Jiaoyan Chen and Yinghui Li and Hai-Tao Zheng and Xi Chen and Qingbin Liu and Wenqiang Liu and Dongxiao Huang and Bei Wu and Yexin Wang},
Title         = {Knowledge-augmented Few-shot Visual Relation Detection},
Eprint        = {2303.05342v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Visual Relation Detection (VRD) aims to detect relationships between objects
for image understanding. Most existing VRD methods rely on thousands of
training samples of each relationship to achieve satisfactory performance. Some
recent papers tackle this problem by few-shot learning with elaborately
designed pipelines and pre-trained word vectors. However, the performance of
existing few-shot VRD models is severely hampered by the poor generalization
capability, as they struggle to handle the vast semantic diversity of visual
relationships. Nonetheless, humans have the ability to learn new relationships
with just few examples based on their knowledge. Inspired by this, we devise a
knowledge-augmented, few-shot VRD framework leveraging both textual knowledge
and visual relation knowledge to improve the generalization ability of few-shot
VRD. The textual knowledge and visual relation knowledge are acquired from a
pre-trained language model and an automatically constructed visual relation
knowledge graph, respectively. We extensively validate the effectiveness of our
framework. Experiments conducted on three benchmarks from the commonly used
Visual Genome dataset show that our performance surpasses existing
state-of-the-art models with a large improvement.},
Year          = {2023},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2303.05342v1},
File          = {2303.05342v1.pdf}
}
@article{2304.09823v4,
Author        = {Lan Chen and Xi Chen and Shiyu Wu and Yaqi Yang and Meng Chang and Hengshu Zhu},
Title         = {The Future of ChatGPT-enabled Labor Market: A Preliminary Study in China},
Eprint        = {2304.09823v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CY},
Abstract      = {As a phenomenal large language model, ChatGPT has achieved unparalleled
success in various real-world tasks and increasingly plays an important role in
our daily lives and work. However, extensive concerns are also raised about the
potential ethical issues, especially about whether ChatGPT-like artificial
general intelligence (AGI) will replace human jobs. To this end, in this paper,
we introduce a preliminary data-driven study on the future of ChatGPT-enabled
labor market from the view of Human-AI Symbiosis instead of Human-AI
Confrontation. To be specific, we first conduct an in-depth analysis of
large-scale job posting data in BOSS Zhipin, the largest online recruitment
platform in China. The results indicate that about 28% of occupations in the
current labor market require ChatGPT-related skills. Furthermore, based on a
large-scale occupation-centered knowledge graph, we develop a semantic
information enhanced collaborative filtering algorithm to predict the future
occupation-skill relations in the labor market. As a result, we find that
additional 45% occupations in the future will require ChatGPT-related skills.
In particular, industries related to technology, products, and operations are
expected to have higher proficiency requirements for ChatGPT-related skills,
while the manufacturing, services, education, and health science related
industries will have lower requirements for ChatGPT-related skills.},
Year          = {2023},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2304.09823v4},
File          = {2304.09823v4.pdf}
}
@article{2304.12083v1,
Author        = {Xuhui Ren and Wei Yuan and Tong Chen and Chaoqun Yang and Quoc Viet Hung Nguyen and Hongzhi Yin},
Title         = {Joint Semantic and Structural Representation Learning for Enhancing User
  Preference Modelling},
Eprint        = {2304.12083v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Knowledge graphs (KGs) have become important auxiliary information for
helping recommender systems obtain a good understanding of user preferences.
Despite recent advances in KG-based recommender systems, existing methods are
prone to suboptimal performance due to the following two drawbacks: 1) current
KG-based methods over-emphasize the heterogeneous structural information within
a KG and overlook the underlying semantics of its connections, hindering the
recommender from distilling the explicit user preferences; and 2) the inherent
incompleteness of a KG (i.e., missing facts, relations and entities) will
deteriorate the information extracted from KG and weaken the representation
learning of recommender systems.
  To tackle the aforementioned problems, we investigate the potential of
jointly incorporating the structural and semantic information within a KG to
model user preferences in finer granularity. A new framework for KG-based
recommender systems, namely \textit{K}nowledge \textit{I}nfomax
\textit{R}ecommender \textit{S}ystem with \textit{C}ontrastive
\textit{L}earning (KIRS-CL) is proposed in this paper. Distinct from previous
KG-based approaches, KIRS-CL utilizes structural and connectivity information
with high-quality item embeddings learned by encoding KG triples with a
pre-trained language model. These well-trained entity representations enable
KIRS-CL to find the item to recommend via the preference connection between the
user and the item. Additionally, to improve the generalizability of our
framework, we introduce a contrastive warm-up learning strategy, making it
capable of dealing with both warm- and cold-start recommendation scenarios.
Extensive experiments on two real-world datasets demonstrate remarkable
improvements over state-of-the-art baselines.},
Year          = {2023},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2304.12083v1},
File          = {2304.12083v1.pdf}
}
@article{2305.01876v5,
Author        = {Siyu Yuan and Deqing Yang and Jinxi Liu and Shuyu Tian and Jiaqing Liang and Yanghua Xiao and Rui Xie},
Title         = {Causality-aware Concept Extraction based on Knowledge-guided Prompting},
Eprint        = {2305.01876v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Concepts benefit natural language understanding but are far from complete in
existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs)
have been widely used in text-based concept extraction (CE). However, PLMs tend
to mine the co-occurrence associations from massive corpus as pre-trained
knowledge rather than the real causal effect between tokens. As a result, the
pre-trained knowledge confounds PLMs to extract biased concepts based on
spurious co-occurrence correlations, inevitably resulting in low precision. In
this paper, through the lens of a Structural Causal Model (SCM), we propose
equipping the PLM-based extractor with a knowledge-guided prompt as an
intervention to alleviate concept bias. The prompt adopts the topic of the
given entity from the existing knowledge in KGs to mitigate the spurious
co-occurrence correlations between entities and biased concepts. Our extensive
experiments on representative multilingual KG datasets justify that our
proposed prompt can effectively alleviate concept bias and improve the
performance of PLM-based CE models.The code has been released on
https://github.com/siyuyuan/KPCE.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.01876v5},
File          = {2305.01876v5.pdf}
}
@article{2305.05091v2,
Author        = {Prateek Chhikara and Jiarui Zhang and Filip Ilievski and Jonathan Francis and Kaixin Ma},
Title         = {Knowledge-enhanced Agents for Interactive Text Games},
Eprint        = {2305.05091v2},
DOI           = {10.1145/3587259.3627561},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Communication via natural language is a key aspect of machine intelligence,
and it requires computational models to learn and reason about world concepts,
with varying levels of supervision. Significant progress has been made on
fully-supervised non-interactive tasks, such as question-answering and
procedural text understanding. Yet, various sequential interactive tasks, as in
text-based games, have revealed limitations of existing approaches in terms of
coherence, contextual awareness, and their ability to learn effectively from
the environment. In this paper, we propose a knowledge-injection framework for
improved functional grounding of agents in text-based games. Specifically, we
consider two forms of domain knowledge that we inject into learning-based
agents: memory of previous correct actions and affordances of relevant objects
in the environment. Our framework supports two representative model classes:
reinforcement learning agents and language model agents. Furthermore, we devise
multiple injection strategies for the above domain knowledge types and agent
architectures, including injection via knowledge graphs and augmentation of the
existing input encoding strategies. We experiment with four models on the 10
tasks in the ScienceWorld text-based game environment, to illustrate the impact
of knowledge injection on various model configurations and challenging task
settings. Our findings provide crucial insights into the interplay between task
properties, model architectures, and domain knowledge for interactive contexts.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.05091v2},
File          = {2305.05091v2.pdf}
}
@article{2305.11501v1,
Author        = {Yu Zhao and Yike Wu and Xiangrui Cai and Ying Zhang and Haiwei Zhang and Xiaojie Yuan},
Title         = {From Alignment to Entailment: A Unified Textual Entailment Framework for
  Entity Alignment},
Eprint        = {2305.11501v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Entity Alignment (EA) aims to find the equivalent entities between two
Knowledge Graphs (KGs). Existing methods usually encode the triples of entities
as embeddings and learn to align the embeddings, which prevents the direct
interaction between the original information of the cross-KG entities.
Moreover, they encode the relational triples and attribute triples of an entity
in heterogeneous embedding spaces, which prevents them from helping each other.
In this paper, we transform both triples into unified textual sequences, and
model the EA task as a bi-directional textual entailment task between the
sequences of cross-KG entities. Specifically, we feed the sequences of two
entities simultaneously into a pre-trained language model (PLM) and propose two
kinds of PLM-based entity aligners that model the entailment probability
between sequences as the similarity between entities. Our approach captures the
unified correlation pattern of two kinds of information between entities, and
explicitly models the fine-grained interaction between original entity
information. The experiments on five cross-lingual EA datasets show that our
approach outperforms the state-of-the-art EA methods and enables the mutual
enhancement of the heterogeneous information. Codes are available at
https://github.com/OreOZhao/TEA.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.11501v1},
File          = {2305.11501v1.pdf}
}
@article{2305.12600v1,
Author        = {Qian Huang and Hongyu Ren and Peng Chen and Gregor Kržmanc and Daniel Zeng and Percy Liang and Jure Leskovec},
Title         = {PRODIGY: Enabling In-context Learning Over Graphs},
Eprint        = {2305.12600v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {In-context learning is the ability of a pretrained model to adapt to novel
and diverse downstream tasks by conditioning on prompt examples, without
optimizing any parameters. While large language models have demonstrated this
ability, how in-context learning could be performed over graphs is unexplored.
In this paper, we develop \textbf{Pr}etraining \textbf{O}ver \textbf{D}iverse
\textbf{I}n-Context \textbf{G}raph S\textbf{y}stems (PRODIGY), the first
pretraining framework that enables in-context learning over graphs. The key
idea of our framework is to formulate in-context learning over graphs with a
novel \emph{prompt graph} representation, which connects prompt examples and
queries. We then propose a graph neural network architecture over the prompt
graph and a corresponding family of in-context pretraining objectives. With
PRODIGY, the pretrained model can directly perform novel downstream
classification tasks on unseen graphs via in-context learning. We provide
empirical evidence of the effectiveness of our framework by showcasing its
strong in-context learning performance on tasks involving citation networks and
knowledge graphs. Our approach outperforms the in-context learning accuracy of
contrastive pretraining baselines with hard-coded adaptation by 18\% on average
across all setups. Moreover, it also outperforms standard finetuning with
limited data by 33\% on average with in-context learning.},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.12600v1},
File          = {2305.12600v1.pdf}
}
@article{2306.00745v2,
Author        = {Keti Korini and Christian Bizer},
Title         = {Column Type Annotation using ChatGPT},
Eprint        = {2306.00745v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Column type annotation is the task of annotating the columns of a relational
table with the semantic type of the values contained in each column. Column
type annotation is an important pre-processing step for data search and data
integration in the context of data lakes. State-of-the-art column type
annotation methods either rely on matching table columns to properties of a
knowledge graph or fine-tune pre-trained language models such as BERT for
column type annotation. In this work, we take a different approach and explore
using ChatGPT for column type annotation. We evaluate different prompt designs
in zero- and few-shot settings and experiment with providing task definitions
and detailed instructions to the model. We further implement a two-step table
annotation pipeline which first determines the class of the entities described
in the table and depending on this class asks ChatGPT to annotate columns using
only the relevant subset of the overall vocabulary. Using instructions as well
as the two-step pipeline, ChatGPT reaches F1 scores of over 85% in zero- and
one-shot setups. To reach a similar F1 score a RoBERTa model needs to be
fine-tuned with 356 examples. This comparison shows that ChatGPT is able
deliver competitive results for the column type annotation task given no or
only a minimal amount of task-specific demonstrations.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.00745v2},
File          = {2306.00745v2.pdf}
}
@article{2308.08459v1,
Author        = {Jianyang Zhai and Xiawu Zheng and Chang-Dong Wang and Hui Li and Yonghong Tian},
Title         = {Knowledge Prompt-tuning for Sequential Recommendation},
Eprint        = {2308.08459v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Pre-trained language models (PLMs) have demonstrated strong performance in
sequential recommendation (SR), which are utilized to extract general
knowledge. However, existing methods still lack domain knowledge and struggle
to capture users' fine-grained preferences. Meanwhile, many traditional SR
methods improve this issue by integrating side information while suffering from
information loss. To summarize, we believe that a good recommendation system
should utilize both general and domain knowledge simultaneously. Therefore, we
introduce an external knowledge base and propose Knowledge Prompt-tuning for
Sequential Recommendation (\textbf{KP4SR}). Specifically, we construct a set of
relationship templates and transform a structured knowledge graph (KG) into
knowledge prompts to solve the problem of the semantic gap. However, knowledge
prompts disrupt the original data structure and introduce a significant amount
of noise. We further construct a knowledge tree and propose a knowledge tree
mask, which restores the data structure in a mask matrix form, thus mitigating
the noise problem. We evaluate KP4SR on three real-world datasets, and
experimental results show that our approach outperforms state-of-the-art
methods on multiple evaluation metrics. Specifically, compared with PLM-based
methods, our method improves NDCG@5 and HR@5 by \textcolor{red}{40.65\%} and
\textcolor{red}{36.42\%} on the books dataset, \textcolor{red}{11.17\%} and
\textcolor{red}{11.47\%} on the music dataset, and \textcolor{red}{22.17\%} and
\textcolor{red}{19.14\%} on the movies dataset, respectively. Our code is
publicly available at the link:
\href{https://github.com/zhaijianyang/KP4SR}{\textcolor{blue}{https://github.com/zhaijianyang/KP4SR}.}},
Year          = {2023},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2308.08459v1},
File          = {2308.08459v1.pdf}
}
@article{2309.00917v2,
Author        = {Tom van Sonsbeek and Xiantong Zhen and Marcel Worring},
Title         = {Knowledge Graph Embeddings for Multi-Lingual Structured Representations
  of Radiology Reports},
Eprint        = {2309.00917v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The way we analyse clinical texts has undergone major changes over the last
years. The introduction of language models such as BERT led to adaptations for
the (bio)medical domain like PubMedBERT and ClinicalBERT. These models rely on
large databases of archived medical documents. While performing well in terms
of accuracy, both the lack of interpretability and limitations to transfer
across languages limit their use in clinical setting. We introduce a novel
light-weight graph-based embedding method specifically catering radiology
reports. It takes into account the structure and composition of the report,
while also connecting medical terms in the report through the multi-lingual
SNOMED Clinical Terms knowledge base. The resulting graph embedding uncovers
the underlying relationships among clinical terms, achieving a representation
that is better understandable for clinicians and clinically more accurate,
without reliance on large pre-training datasets. We show the use of this
embedding on two tasks namely disease classification of X-ray reports and image
classification. For disease classification our model is competitive with its
BERT-based counterparts, while being magnitudes smaller in size and training
data requirements. For image classification, we show the effectiveness of the
graph embedding leveraging cross-modal knowledge transfer and show how this
method is usable across different languages.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.00917v2},
File          = {2309.00917v2.pdf}
}
@article{2311.06761v1,
Author        = {Ruyao Xu and Taolin Zhang and Chengyu Wang and Zhongjie Duan and Cen Chen and Minghui Qiu and Dawei Cheng and Xiaofeng He and Weining Qian},
Title         = {Learning Knowledge-Enhanced Contextual Language Representations for
  Domain Natural Language Understanding},
Eprint        = {2311.06761v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge-Enhanced Pre-trained Language Models (KEPLMs) improve the
performance of various downstream NLP tasks by injecting knowledge facts from
large-scale Knowledge Graphs (KGs). However, existing methods for pre-training
KEPLMs with relational triples are difficult to be adapted to close domains due
to the lack of sufficient domain graph semantics. In this paper, we propose a
Knowledge-enhanced lANGuAge Representation learning framework for various
clOsed dOmains (KANGAROO) via capturing the implicit graph structure among the
entities. Specifically, since the entity coverage rates of closed-domain KGs
can be relatively low and may exhibit the global sparsity phenomenon for
knowledge injection, we consider not only the shallow relational
representations of triples but also the hyperbolic embeddings of deep
hierarchical entity-class structures for effective knowledge fusion.Moreover,
as two closed-domain entities under the same entity-class often have locally
dense neighbor subgraphs counted by max point biconnected component, we further
propose a data augmentation strategy based on contrastive learning over
subgraphs to construct hard negative samples of higher quality. It makes the
underlying KELPMs better distinguish the semantics of these neighboring
entities to further complement the global semantic sparsity. In the
experiments, we evaluate KANGAROO over various knowledge-aware and general NLP
tasks in both full and few-shot learning settings, outperforming various KEPLM
training paradigms performance in closed-domains significantly.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.06761v1},
File          = {2311.06761v1.pdf}
}
@article{2312.10073v1,
Author        = {Zefeng Chen and Wensheng Gan and Jiayang Wu and Kaixia Hu and Hong Lin},
Title         = {Data Scarcity in Recommendation Systems: A Survey},
Eprint        = {2312.10073v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {The prevalence of online content has led to the widespread adoption of
recommendation systems (RSs), which serve diverse purposes such as news,
advertisements, and e-commerce recommendations. Despite their significance,
data scarcity issues have significantly impaired the effectiveness of existing
RS models and hindered their progress. To address this challenge, the concept
of knowledge transfer, particularly from external sources like pre-trained
language models, emerges as a potential solution to alleviate data scarcity and
enhance RS development. However, the practice of knowledge transfer in RSs is
intricate. Transferring knowledge between domains introduces data disparities,
and the application of knowledge transfer in complex RS scenarios can yield
negative consequences if not carefully designed. Therefore, this article
contributes to this discourse by addressing the implications of data scarcity
on RSs and introducing various strategies, such as data augmentation,
self-supervised learning, transfer learning, broad learning, and knowledge
graph utilization, to mitigate this challenge. Furthermore, it delves into the
challenges and future direction within the RS domain, offering insights that
are poised to facilitate the development and implementation of robust RSs,
particularly when confronted with data scarcity. We aim to provide valuable
guidance and inspiration for researchers and practitioners, ultimately driving
advancements in the field of RS.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.10073v1},
File          = {2312.10073v1.pdf}
}
@article{2312.10417v3,
Author        = {Zhiwei Zha and Jiaan Wang and Zhixu Li and Xiangru Zhu and Wei Song and Yanghua Xiao},
Title         = {M^2ConceptBase: A Fine-Grained Aligned Concept-Centric Multimodal
  Knowledge Base},
Eprint        = {2312.10417v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Multimodal knowledge bases (MMKBs) provide cross-modal aligned knowledge
crucial for multimodal tasks. However, the images in existing MMKBs are
generally collected for entities in encyclopedia knowledge graphs. Therefore,
detailed groundings of visual semantics with linguistic concepts are lacking,
which are essential for the visual concept cognition ability of multimodal
models. Addressing this gap, we introduce M^2ConceptBase, the first
concept-centric MMKB. M^2ConceptBase models concepts as nodes with associated
images and detailed textual descriptions. We propose a context-aware multimodal
symbol grounding approach to align concept-image and concept-description pairs
using context information from image-text datasets. Comprising 951K images and
152K concepts, M^2ConceptBase links each concept to an average of 6.27 images
and a single description, ensuring comprehensive visual and textual semantics.
Human studies confirm more than 95% alignment accuracy, underscoring its
quality. Additionally, our experiments demonstrate that M^2ConceptBase
significantly enhances VQA model performance on the OK-VQA task. M^2ConceptBase
also substantially improves the fine-grained concept understanding capabilities
of multimodal large language models through retrieval augmentation in two
concept-related tasks, highlighting its value.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.10417v3},
File          = {2312.10417v3.pdf}
}
@article{2312.15851v1,
Author        = {Zi-Feng Mai and Chang-Dong Wang and Zhongjie Zeng and Ya Li and Jiaquan Chen and Philip S. Yu},
Title         = {Hypergraph Enhanced Knowledge Tree Prompt Learning for Next-Basket
  Recommendation},
Eprint        = {2312.15851v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Next-basket recommendation (NBR) aims to infer the items in the next basket
given the corresponding basket sequence. Existing NBR methods are mainly based
on either message passing in a plain graph or transition modelling in a basket
sequence. However, these methods only consider point-to-point binary item
relations while item dependencies in real world scenarios are often in higher
order. Additionally, the importance of the same item to different users varies
due to variation of user preferences, and the relations between items usually
involve various aspects. As pretrained language models (PLMs) excel in multiple
tasks in natural language processing (NLP) and computer vision (CV), many
researchers have made great efforts in utilizing PLMs to boost recommendation.
However, existing PLM-based recommendation methods degrade when encountering
Out-Of-Vocabulary (OOV) items. OOV items are those whose IDs are out of PLM's
vocabulary and thus unintelligible to PLM. To settle the above challenges, we
propose a novel method HEKP4NBR, which transforms the knowledge graph (KG) into
prompts, namely Knowledge Tree Prompt (KTP), to help PLM encode the OOV item
IDs in the user's basket sequence. A hypergraph convolutional module is
designed to build a hypergraph based on item similarities measured by an MoE
model from multiple aspects and then employ convolution on the hypergraph to
model correlations among multiple items. Extensive experiments are conducted on
HEKP4NBR on two datasets based on real company data and validate its
effectiveness against multiple state-of-the-art methods.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.15851v1},
File          = {2312.15851v1.pdf}
}
@article{2401.03181v2,
Author        = {Prakash Chandra Sukhwal and Vaibhav Rajan and Atreyi Kankanhalli},
Title         = {A Joint-Reasoning based Disease Q&A System},
Eprint        = {2401.03181v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Medical question answer (QA) assistants respond to lay users' health-related
queries by synthesizing information from multiple sources using natural
language processing and related techniques. They can serve as vital tools to
alleviate issues of misinformation, information overload, and complexity of
medical language, thus addressing lay users' information needs while reducing
the burden on healthcare professionals. QA systems, the engines of such
assistants, have typically used either language models (LMs) or knowledge
graphs (KG), though the approaches could be complementary. LM-based QA systems
excel at understanding complex questions and providing well-formed answers, but
are prone to factual mistakes. KG-based QA systems, which represent facts well,
are mostly limited to answering short-answer questions with pre-created
templates. While a few studies have jointly used LM and KG approaches for
text-based QA, this was done to answer multiple-choice questions. Extant QA
systems also have limitations in terms of automation and performance. We
address these challenges by designing a novel, automated disease QA system
which effectively utilizes both LM and KG techniques through a joint-reasoning
approach to answer disease-related questions appropriate for lay users. Our
evaluation of the system using a range of quality metrics demonstrates its
efficacy over benchmark systems, including the popular ChatGPT.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.03181v2},
File          = {2401.03181v2.pdf}
}
@article{2401.13912v1,
Author        = {John A. Miller and Mohammed Aldosari and Farah Saeed and Nasid Habib Barna and Subas Rana and I. Budak Arpinar and Ninghao Liu},
Title         = {A Survey of Deep Learning and Foundation Models for Time Series
  Forecasting},
Eprint        = {2401.13912v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Deep Learning has been successfully applied to many application domains, yet
its advantages have been slow to emerge for time series forecasting. For
example, in the well-known Makridakis (M) Competitions, hybrids of traditional
statistical or machine learning techniques have only recently become the top
performers. With the recent architectural advances in deep learning being
applied to time series forecasting (e.g., encoder-decoders with attention,
transformers, and graph neural networks), deep learning has begun to show
significant advantages. Still, in the area of pandemic prediction, there remain
challenges for deep learning models: the time series is not long enough for
effective training, unawareness of accumulated scientific knowledge, and
interpretability of the model. To this end, the development of foundation
models (large deep learning models with extensive pre-training) allows models
to understand patterns and acquire knowledge that can be applied to new related
problems before extensive training data becomes available. Furthermore, there
is a vast amount of knowledge available that deep learning models can tap into,
including Knowledge Graphs and Large Language Models fine-tuned with scientific
domain knowledge. There is ongoing research examining how to utilize or inject
such knowledge into deep learning models. In this survey, several
state-of-the-art modeling techniques are reviewed, and suggestions for further
work are provided.},
Year          = {2024},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2401.13912v1},
File          = {2401.13912v1.pdf}
}
@article{2402.03607v2,
Author        = {Trilok Padhi and Ugur Kursuncu and Yaman Kumar and Valerie L. Shalin and Lane Peterson Fronczek},
Title         = {Enhancing Cross-Modal Contextual Congruence for Crowdfunding Success
  using Knowledge-infused Learning},
Eprint        = {2402.03607v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The digital landscape continually evolves with multimodality, enriching the
online experience for users. Creators and marketers aim to weave subtle
contextual cues from various modalities into congruent content to engage users
with a harmonious message. This interplay of multimodal cues is often a crucial
factor in attracting users' attention. However, this richness of multimodality
presents a challenge to computational modeling, as the semantic contextual cues
spanning across modalities need to be unified to capture the true holistic
meaning of the multimodal content. This contextual meaning is critical in
attracting user engagement as it conveys the intended message of the brand or
the organization. In this work, we incorporate external commonsense knowledge
from knowledge graphs to enhance the representation of multimodal data using
compact Visual Language Models (VLMs) and predict the success of multi-modal
crowdfunding campaigns. Our results show that external knowledge commonsense
bridges the semantic gap between text and image modalities, and the enhanced
knowledge-infused representations improve the predictive performance of models
for campaign success upon the baselines without knowledge. Our findings
highlight the significance of contextual congruence in online multimodal
content for engaging and successful crowdfunding campaigns.},
Year          = {2024},
Month         = {Feb},
Note          = {IEEE International Conference on Big Data 2024 (IEEE BigData 2024)},
Url           = {http://arxiv.org/abs/2402.03607v2},
File          = {2402.03607v2.pdf}
}
@article{2402.13188v1,
Author        = {Chao Xue and Di Liang and Pengfei Wang and Jing Zhang},
Title         = {Question Calibration and Multi-Hop Modeling for Temporal Question
  Answering},
Eprint        = {2402.13188v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Many models that leverage knowledge graphs (KGs) have recently demonstrated
remarkable success in question answering (QA) tasks. In the real world, many
facts contained in KGs are time-constrained thus temporal KGQA has received
increasing attention. Despite the fruitful efforts of previous models in
temporal KGQA, they still have several limitations. (I) They adopt pre-trained
language models (PLMs) to obtain question representations, while PLMs tend to
focus on entity information and ignore entity transfer caused by temporal
constraints, and finally fail to learn specific temporal representations of
entities. (II) They neither emphasize the graph structure between entities nor
explicitly model the multi-hop relationship in the graph, which will make it
difficult to solve complex multi-hop question answering. To alleviate this
problem, we propose a novel Question Calibration and Multi-Hop Modeling
(QC-MHM) approach. Specifically, We first calibrate the question representation
by fusing the question and the time-constrained concepts in KG. Then, we
construct the GNN layer to complete multi-hop message passing. Finally, the
question representation is combined with the embedding output by the GNN to
generate the final prediction. Empirical results verify that the proposed model
achieves better performance than the state-of-the-art models in the benchmark
dataset. Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestions
dataset's complex questions are absolutely improved by 5.1% and 1.2% compared
to the best-performing baseline. Moreover, QC-MHM can generate interpretable
and trustworthy predictions.},
Year          = {2024},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2402.13188v1},
File          = {2402.13188v1.pdf}
}
@article{2405.04180v1,
Author        = {Zhixuan Chu and Lei Zhang and Yichen Sun and Siqiao Xue and Zhibo Wang and Zhan Qin and Kui Ren},
Title         = {Sora Detector: A Unified Hallucination Detection for Large Text-to-Video
  Models},
Eprint        = {2405.04180v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {The rapid advancement in text-to-video (T2V) generative models has enabled
the synthesis of high-fidelity video content guided by textual descriptions.
Despite this significant progress, these models are often susceptible to
hallucination, generating contents that contradict the input text, which poses
a challenge to their reliability and practical deployment. To address this
critical issue, we introduce the SoraDetector, a novel unified framework
designed to detect hallucinations across diverse large T2V models, including
the cutting-edge Sora model. Our framework is built upon a comprehensive
analysis of hallucination phenomena, categorizing them based on their
manifestation in the video content. Leveraging the state-of-the-art keyframe
extraction techniques and multimodal large language models, SoraDetector first
evaluates the consistency between extracted video content summary and textual
prompts, then constructs static and dynamic knowledge graphs (KGs) from frames
to detect hallucination both in single frames and across frames. Sora Detector
provides a robust and quantifiable measure of consistency, static and dynamic
hallucination. In addition, we have developed the Sora Detector Agent to
automate the hallucination detection process and generate a complete video
quality report for each input video. Lastly, we present a novel meta-evaluation
benchmark, T2VHaluBench, meticulously crafted to facilitate the evaluation of
advancements in T2V hallucination detection. Through extensive experiments on
videos generated by Sora and other large T2V models, we demonstrate the
efficacy of our approach in accurately detecting hallucinations. The code and
dataset can be accessed via GitHub.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.04180v1},
File          = {2405.04180v1.pdf}
}
@article{2405.05616v1,
Author        = {Ruiting Dai and Yuqiao Tan and Lisi Mo and Shuang Liang and Guohao Huo and Jiayi Luo and Yao Cheng},
Title         = {G-SAP: Graph-based Structure-Aware Prompt Learning over Heterogeneous
  Knowledge for Commonsense Reasoning},
Eprint        = {2405.05616v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Commonsense question answering has demonstrated considerable potential across
various applications like assistants and social robots. Although fully
fine-tuned pre-trained Language Models(LM) have achieved remarkable performance
in commonsense reasoning, their tendency to excessively prioritize textual
information hampers the precise transfer of structural knowledge and undermines
interpretability. Some studies have explored combining LMs with Knowledge
Graphs(KGs) by coarsely fusing the two modalities to perform Graph Neural
Network(GNN)-based reasoning that lacks a profound interaction between
heterogeneous modalities. In this paper, we propose a novel Graph-based
Structure-Aware Prompt Learning Model for commonsense reasoning, named G-SAP,
aiming to maintain a balance between heterogeneous knowledge and enhance the
cross-modal interaction within the LM+GNNs model. In particular, an evidence
graph is constructed by integrating multiple knowledge sources, i.e.
ConceptNet, Wikipedia, and Cambridge Dictionary to boost the performance.
Afterward, a structure-aware frozen PLM is employed to fully incorporate the
structured and textual information from the evidence graph, where the
generation of prompts is driven by graph entities and relations. Finally, a
heterogeneous message-passing reasoning module is used to facilitate deep
interaction of knowledge between the LM and graph-based networks. Empirical
validation, conducted through extensive experiments on three benchmark
datasets, demonstrates the notable performance of the proposed model. The
results reveal a significant advancement over the existing models, especially,
with 6.12% improvement over the SoTA LM+GNNs model on the OpenbookQA dataset.},
Year          = {2024},
Month         = {May},
Url           = {http://arxiv.org/abs/2405.05616v1},
File          = {2405.05616v1.pdf}
}
@article{2408.03079v1,
Author        = {Jinglong Gao and Chen Lu and Xiao Ding and Zhongyang Li and Ting Liu and Bing Qin},
Title         = {Enhancing Complex Causality Extraction via Improved Subtask Interaction
  and Knowledge Fusion},
Eprint        = {2408.03079v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Event Causality Extraction (ECE) aims at extracting causal event pairs from
texts. Despite ChatGPT's recent success, fine-tuning small models remains the
best approach for the ECE task. However, existing fine-tuning based ECE methods
cannot address all three key challenges in ECE simultaneously: 1) Complex
Causality Extraction, where multiple causal-effect pairs occur within a single
sentence; 2) Subtask~ Interaction, which involves modeling the mutual
dependence between the two subtasks of ECE, i.e., extracting events and
identifying the causal relationship between extracted events; and 3) Knowledge
Fusion, which requires effectively fusing the knowledge in two modalities,
i.e., the expressive pretrained language models and the structured knowledge
graphs. In this paper, we propose a unified ECE framework (UniCE to address all
three issues in ECE simultaneously. Specifically, we design a subtask
interaction mechanism to enable mutual interaction between the two ECE
subtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in
the two modalities. Furthermore, we employ separate decoders for each subtask
to facilitate complex causality extraction. Experiments on three benchmark
datasets demonstrate that our method achieves state-of-the-art performance and
outperforms ChatGPT with a margin of at least 30% F1-score. More importantly,
our model can also be used to effectively improve the ECE performance of
ChatGPT via in-context learning.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.03079v1},
File          = {2408.03079v1.pdf}
}
@article{2408.03615v2,
Author        = {Zaijing Li and Yuquan Xie and Rui Shao and Gongwei Chen and Dongmei Jiang and Liqiang Nie},
Title         = {Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in
  Long-Horizon Tasks},
Eprint        = {2408.03615v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Building a general-purpose agent is a long-standing vision in the field of
artificial intelligence. Existing agents have made remarkable progress in many
domains, yet they still struggle to complete long-horizon tasks in an open
world. We attribute this to the lack of necessary world knowledge and
multimodal experience that can guide agents through a variety of long-horizon
tasks. In this paper, we propose a Hybrid Multimodal Memory module to address
the above challenges. It 1) transforms knowledge into Hierarchical Directed
Knowledge Graph that allows agents to explicitly represent and learn world
knowledge, and 2) summarises historical information into Abstracted Multimodal
Experience Pool that provide agents with rich references for in-context
learning. On top of the Hybrid Multimodal Memory module, a multimodal agent,
Optimus-1, is constructed with dedicated Knowledge-guided Planner and
Experience-Driven Reflector, contributing to a better planning and reflection
in the face of long-horizon tasks in Minecraft. Extensive experimental results
show that Optimus-1 significantly outperforms all existing agents on
challenging long-horizon task benchmarks, and exhibits near human-level
performance on many tasks. In addition, we introduce various Multimodal Large
Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show
that Optimus-1 exhibits strong generalization with the help of the Hybrid
Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.03615v2},
File          = {2408.03615v2.pdf}
}
@article{2408.13378v3,
Author        = {Yoshitaka Inoue and Tianci Song and Tianfan Fu},
Title         = {DrugAgent: Explainable Drug Repurposing Agent with Large Language
  Model-based Reasoning},
Eprint        = {2408.13378v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Drug repurposing offers a promising avenue for accelerating drug development
by identifying new therapeutic potentials of existing drugs. In this paper, we
propose a multi-agent framework to enhance the drug repurposing process using
state-of-the-art machine learning techniques and knowledge integration. Our
framework comprises several specialized agents: an AI Agent trains robust
drug-target interaction (DTI) models; a Knowledge Graph Agent utilizes the
drug-gene interaction database (DGIdb), DrugBank, Comparative Toxicogenomics
Database (CTD), and Search Tool for Interactions of Chemicals (STITCH) to
systematically extract DTIs; and a Search Agent interacts with biomedical
literature to annotate and verify computational predictions. By integrating
outputs from these agents, our system effectively harnesses diverse data
sources, including external databases, to propose viable repurposing
candidates. Preliminary results demonstrate the potential of our approach in
not only predicting drug-disease interactions but also in reducing the time and
cost associated with traditional drug discovery methods. This paper highlights
the scalability of multi-agent systems in biomedical research and their role in
driving innovation in drug repurposing. Our approach not only outperforms
existing methods in predicting drug repurposing potential but also provides
interpretable results, paving the way for more efficient and cost-effective
drug discovery processes.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.13378v3},
File          = {2408.13378v3.pdf}
}
@article{2410.22353v2,
Author        = {Zhongwu Chen and Chengjin Xu and Dingmin Wang and Zhen Huang and Yong Dou and Jian Guo},
Title         = {RuleRAG: Rule-Guided Retrieval-Augmented Generation with Language Models
  for Question Answering},
Eprint        = {2410.22353v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Retrieval-augmented generation (RAG) has shown promising potential in
knowledge intensive question answering (QA). However, existing approaches only
consider the query itself, neither specifying the retrieval preferences for the
retrievers nor informing the generators of how to refer to the retrieved
documents for the answers, which poses a significant challenge to the QA
performance. To address these issues, we propose Rule-guided
Retrieval-Augmented Generation with LMs, which explicitly introduces rules for
in-context learning (RuleRAG-ICL) to guide retrievers to recall related
documents in the directions of rules and uniformly guide generators to reason
attributed by the same rules. Moreover, most existing RAG datasets were
constructed without considering rules and Knowledge Graphs (KGs) are recognized
as providing high-quality rules. Therefore, we construct five rule-aware RAG
benchmarks for QA, RuleQA, based on KGs to stress the significance of retrieval
and reasoning with rules. Experiments on RuleQA demonstrate RuleRAG-ICL
improves the retrieval quality of +89.2% in Recall@10 and answer accuracy of
+103.1% in Exact Match, and RuleRAG-FT yields more enhancement. In addition,
experiments on four existing RAG datasets show RuleRAG is also effective by
offering rules in RuleQA to them, further proving the generalization of rule
guidance in RuleRAG.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.22353v2},
File          = {2410.22353v2.pdf}
}
@article{2412.04119v2,
Author        = {Cristian-George Crăciun and Răzvan-Alexandru Smădu and Dumitru-Clementin Cercel and Mihaela-Claudia Cercel},
Title         = {GRAF: Graph Retrieval Augmented by Facts for Romanian Legal Multi-Choice
  Question Answering},
Eprint        = {2412.04119v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pre-trained Language Models (PLMs) have shown remarkable performances in
recent years, setting a new paradigm for NLP research and industry. The legal
domain has received some attention from the NLP community partly due to its
textual nature. Some tasks from this domain are represented by
question-answering (QA) tasks. This work explores the legal domain
Multiple-Choice QA (MCQA) for a low-resource language. The contribution of this
work is multi-fold. We first introduce JuRO, the first openly available
Romanian legal MCQA dataset, comprising three different examinations and a
number of 10,836 total questions. Along with this dataset, we introduce CROL,
an organized corpus of laws that has a total of 93 distinct documents with
their modifications from 763 time spans, that we leveraged in this work for
Information Retrieval (IR) techniques. Moreover, we are the first to propose
Law-RoG, a Knowledge Graph (KG) for the Romanian language, and this KG is
derived from the aforementioned corpus. Lastly, we propose a novel approach for
MCQA, Graph Retrieval Augmented by Facts (GRAF), which achieves competitive
results with generally accepted SOTA methods and even exceeds them in most
settings.},
Year          = {2024},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2412.04119v2},
File          = {2412.04119v2.pdf}
}
@article{2501.08120v1,
Author        = {Markus J. Buehler},
Title         = {In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR},
Eprint        = {2501.08120v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The pursuit of automated scientific discovery has fueled progress from
symbolic logic to modern AI, forging new frontiers in reasoning and pattern
recognition. Transformers function as potential systems, where every possible
relationship remains latent potentiality until tasks impose constraints, akin
to measurement. Yet, refining their sampling requires more than probabilistic
selection: solutions must conform to specific structures or rules, ensuring
consistency and the invocation of general principles. We present
Graph-PReFLexOR (Graph-based Preference-based Recursive Language Modeling for
Exploratory Optimization of Reasoning), a framework that combines graph
reasoning with symbolic abstraction to dynamically expand domain knowledge.
Inspired by reinforcement learning, Graph-PReFLexOR defines reasoning as a
structured mapping, where tasks yield knowledge graphs, abstract patterns, and
ultimately, final answers. Inspired by category theory, it encodes concepts as
nodes and their relationships as edges, supporting hierarchical inference and
adaptive learning through isomorphic representations. Demonstrations include
hypothesis generation, materials design, and creative reasoning, such as
discovering relationships between mythological concepts like 'thin places' with
materials science. We propose a 'knowledge garden growth' strategy that
integrates insights across domains, promoting interdisciplinary connections.
Results with a 3-billion-parameter Graph-PReFLexOR model show superior
reasoning depth and adaptability, underscoring the potential for transparent,
multidisciplinary AI-driven discovery. It lays the groundwork for general
autonomous reasoning solutions.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.08120v1},
File          = {2501.08120v1.pdf}
}
@article{2406.19228v1,
Author        = {Jimin Sun and So Yeon Min and Yingshan Chang and Yonatan Bisk},
Title         = {Tools Fail: Detecting Silent Errors in Faulty Tools},
Eprint        = {2406.19228v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not
in their weights, to perform tasks on the web, and even to control robots.
However, most ontologies and surveys of tool-use have assumed the core
challenge for LLMs is choosing the tool. Instead, we introduce a framework for
tools more broadly which guides us to explore a model's ability to detect
"silent" tool errors, and reflect on how to plan. This more directly aligns
with the increasingly popular use of models as tools. We provide an initial
approach to failure recovery with promising results both on a controlled
calculator setting and embodied agent planning.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.19228v1},
File          = {2406.19228v1.pdf}
}
@article{2406.06566v4,
Author        = {Carolina Fortuna and Vid Hanžel and Blaž Bertalanič},
Title         = {Natural Language Interaction with a Household Electricity
  Knowledge-based Digital Twin},
Eprint        = {2406.06566v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Domain specific digital twins, representing a digital replica of various
segments of the smart grid, are foreseen as able to model, simulate, and
control the respective segments. At the same time, knowledge-based digital
twins, coupled with AI, may also empower humans to understand aspects of the
system through natural language interaction in view of planning and policy
making. This paper is the first to assess and report on the potential of
Retrieval Augmented Generation (RAG) question answers related to household
electrical energy measurement aspects leveraging a knowledge-based energy
digital twin. Relying on the recently published electricity consumption
knowledge graph that actually represents a knowledge-based digital twin, we
study the capabilities of ChatGPT, Gemini and Llama in answering electricity
related questions. Furthermore, we compare the answers with the ones generated
through a RAG techniques that leverages an existing electricity knowledge-based
digital twin. Our findings illustrate that the RAG approach not only reduces
the incidence of incorrect information typically generated by LLMs but also
significantly improves the quality of the output by grounding responses in
verifiable data. This paper details our methodology, presents a comparative
analysis of responses with and without RAG, and discusses the implications of
our findings for future applications of AI in specialized sectors like energy
data analysis.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.06566v4},
File          = {2406.06566v4.pdf}
}
@article{2502.07223v1,
Author        = {Elias Lumer and Pradeep Honaganahalli Basavaraju and Myles Mason and James A. Burke and Vamse Kumar Subbiah},
Title         = {Graph RAG-Tool Fusion},
Eprint        = {2502.07223v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Recent developments in retrieval-augmented generation (RAG) for selecting
relevant tools from a tool knowledge base enable LLM agents to scale their
complex tool calling capabilities to hundreds or thousands of external tools,
APIs, or agents-as-tools. However, traditional RAG-based tool retrieval fails
to capture structured dependencies between tools, limiting the retrieval
accuracy of a retrieved tool's dependencies. For example, among a vector
database of tools, a "get stock price" API requires a "stock ticker" parameter
from a "get stock ticker" API, and both depend on OS-level internet
connectivity tools. In this paper, we address this limitation by introducing
Graph RAG-Tool Fusion, a novel plug-and-play approach that combines the
strengths of vector-based retrieval with efficient graph traversal to capture
all relevant tools (nodes) along with any nested dependencies (edges) within
the predefined tool knowledge graph. We also present ToolLinkOS, a new tool
selection benchmark of 573 fictional tools, spanning over 15 industries, each
with an average of 6.3 tool dependencies. We demonstrate that Graph RAG-Tool
Fusion achieves absolute improvements of 71.7% and 22.1% over na\"ive RAG on
ToolLinkOS and ToolSandbox benchmarks, respectively (mAP@10). ToolLinkOS
dataset is available at
https://github.com/EliasLumer/Graph-RAG-Tool-Fusion-ToolLinkOS},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.07223v1},
File          = {2502.07223v1.pdf}
}
@article{2411.04794v1,
Author        = {Yuxin Zuo and Wenxuan Jiang and Wenxuan Liu and Zixuan Li and Long Bai and Hanbin Wang and Yutao Zeng and Xiaolong Jin and Jiafeng Guo and Xueqi Cheng},
Title         = {AlignXIE: Improving Multilingual Information Extraction by Cross-Lingual
  Alignment},
Eprint        = {2411.04794v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Empirical evidence suggests that LLMs exhibit spontaneous cross-lingual
alignment. Our findings suggest that although LLMs also demonstrate promising
cross-lingual alignment in Information Extraction, there remains significant
imbalance across languages, revealing an underlying deficiency in the IE
alignment. To address this issue, we propose AlignXIE, a powerful code-based
LLM that significantly enhances cross-lingual IE alignment through two
strategies. Firstly, AlignXIE formulates IE across different languages,
especially non-English ones, as code generation tasks, standardizing the
representation of various schemas using Python classes to ensure consistency of
the same ontology in different languages and align the schema. Secondly, it
incorporates an IE cross-lingual alignment phase through a translated instance
prediction task proposed in this paper to align the extraction process,
utilizing ParallelNER, an IE bilingual parallel dataset with 257,190 samples,
generated by our proposed LLM-based automatic pipeline for IE parallel data
construction, with manual annotation to ensure quality. Ultimately, we obtain
AlignXIE through multilingual IE instruction tuning. Although without training
in 9 unseen languages, AlignXIE surpasses ChatGPT by $30.17\%$ and SoTA by
$20.03\%$, thereby demonstrating superior cross-lingual IE capabilities.
Comprehensive evaluations on 63 IE benchmarks in Chinese and English under
various settings, demonstrate that AlignXIE significantly enhances
cross-lingual and multilingual IE through boosting the IE alignment.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.04794v1},
File          = {2411.04794v1.pdf}
}
@article{2010.08883v1,
Author        = {Sai Sharath Japa and Rekabdar Banafsheh},
Title         = {Question Answering over Knowledge Base using Language Model Embeddings},
Eprint        = {2010.08883v1},
DOI           = {10.1109/IJCNN48605.2020.9206698},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge Base, represents facts about the world, often in some form of
subsumption ontology, rather than implicitly, embedded in procedural code, the
way a conventional computer program does. While there is a rapid growth in
knowledge bases, it poses a challenge of retrieving information from them.
Knowledge Base Question Answering is one of the promising approaches for
extracting substantial knowledge from Knowledge Bases. Unlike web search,
Question Answering over a knowledge base gives accurate and concise results,
provided that natural language questions can be understood and mapped precisely
to an answer in the knowledge base. However, some of the existing
embedding-based methods for knowledge base question answering systems ignore
the subtle correlation between the question and the Knowledge Base (e.g.,
entity types, relation paths, and context) and suffer from the Out Of
Vocabulary problem. In this paper, we focused on using a pre-trained language
model for the Knowledge Base Question Answering task. Firstly, we used Bert
base uncased for the initial experiments. We further fine-tuned these
embeddings with a two-way attention mechanism from the knowledge base to the
asked question and from the asked question to the knowledge base answer
aspects. Our method is based on a simple Convolutional Neural Network
architecture with a Multi-Head Attention mechanism to represent the asked
question dynamically in multiple aspects. Our experimental results show the
effectiveness and the superiority of the Bert pre-trained language model
embeddings for question answering systems on knowledge bases over other
well-known embedding methods.},
Year          = {2020},
Month         = {Oct},
Note          = {2020 International Joint Conference on Neural Networks (IJCNN)},
Url           = {http://arxiv.org/abs/2010.08883v1},
File          = {2010.08883v1.pdf}
}
@article{2306.11892v1,
Author        = {Saed Rezayi and Zhengliang Liu and Zihao Wu and Chandra Dhakal and Bao Ge and Haixing Dai and Gengchen Mai and Ninghao Liu and Chen Zhen and Tianming Liu and Sheng Li},
Title         = {Exploring New Frontiers in Agricultural NLP: Investigating the Potential
  of Large Language Models for Food Applications},
Eprint        = {2306.11892v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {This paper explores new frontiers in agricultural natural language processing
by investigating the effectiveness of using food-related text corpora for
pretraining transformer-based language models. In particular, we focus on the
task of semantic matching, which involves establishing mappings between food
descriptions and nutrition data. To accomplish this, we fine-tune a pre-trained
transformer-based language model, AgriBERT, on this task, utilizing an external
source of knowledge, such as the FoodOn ontology. To advance the field of
agricultural NLP, we propose two new avenues of exploration: (1) utilizing
GPT-based models as a baseline and (2) leveraging ChatGPT as an external source
of knowledge. ChatGPT has shown to be a strong baseline in many NLP tasks, and
we believe it has the potential to improve our model in the task of semantic
matching and enhance our model's understanding of food-related concepts and
relationships. Additionally, we experiment with other applications, such as
cuisine prediction based on food ingredients, and expand the scope of our
research to include other NLP tasks beyond semantic matching. Overall, this
paper provides promising avenues for future research in this field, with
potential implications for improving the performance of agricultural NLP
applications.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.11892v1},
File          = {2306.11892v1.pdf}
}
@article{2407.09103v1,
Author        = {Thomas Constum and Pierrick Tranouez and Thierry Paquet},
Title         = {DANIEL: A fast Document Attention Network for Information Extraction and
  Labelling of handwritten documents},
Eprint        = {2407.09103v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Information extraction from handwritten documents involves traditionally
three distinct steps: Document Layout Analysis, Handwritten Text Recognition,
and Named Entity Recognition. Recent approaches have attempted to integrate
these steps into a single process using fully end-to-end architectures. Despite
this, these integrated approaches have not yet matched the performance of
language models, when applied to information extraction in plain text. In this
paper, we introduce DANIEL (Document Attention Network for Information
Extraction and Labelling), a fully end-to-end architecture integrating a
language model and designed for comprehensive handwritten document
understanding. DANIEL performs layout recognition, handwriting recognition, and
named entity recognition on full-page documents. Moreover, it can
simultaneously learn across multiple languages, layouts, and tasks. For named
entity recognition, the ontology to be applied can be specified via the input
prompt. The architecture employs a convolutional encoder capable of processing
images of any size without resizing, paired with an autoregressive decoder
based on a transformer-based language model. DANIEL achieves competitive
results on four datasets, including a new state-of-the-art performance on RIMES
2009 and M-POPP for Handwriting Text Recognition, and IAM NER for Named Entity
Recognition. Furthermore, DANIEL is much faster than existing approaches.
  We provide the source code and the weights of the trained models at
\url{https://github.com/Shulk97/daniel}.},
Year          = {2024},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2407.09103v1},
File          = {2407.09103v1.pdf}
}
@article{2309.10547v1,
Author        = {Zhilun Zhou and Jingtao Ding and Yu Liu and Depeng Jin and Yong Li},
Title         = {Towards Generative Modeling of Urban Flow through Knowledge-enhanced
  Denoising Diffusion},
Eprint        = {2309.10547v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Although generative AI has been successful in many areas, its ability to
model geospatial data is still underexplored. Urban flow, a typical kind of
geospatial data, is critical for a wide range of urban applications. Existing
studies mostly focus on predictive modeling of urban flow that predicts the
future flow based on historical flow data, which may be unavailable in
data-sparse areas or newly planned regions. Some other studies aim to predict
OD flow among regions but they fail to model dynamic changes of urban flow over
time. In this work, we study a new problem of urban flow generation that
generates dynamic urban flow for regions without historical flow data. To
capture the effect of multiple factors on urban flow, such as region features
and urban environment, we employ diffusion model to generate urban flow for
regions under different conditions. We first construct an urban knowledge graph
(UKG) to model the urban environment and relationships between regions, based
on which we design a knowledge-enhanced spatio-temporal diffusion model
(KSTDiff) to generate urban flow for each region. Specifically, to accurately
generate urban flow for regions with different flow volumes, we design a novel
diffusion process guided by a volume estimator, which is learnable and
customized for each region. Moreover, we propose a knowledge-enhanced denoising
network to capture the spatio-temporal dependencies of urban flow as well as
the impact of urban environment in the denoising process. Extensive experiments
on four real-world datasets validate the superiority of our model over
state-of-the-art baselines in urban flow generation. Further in-depth studies
demonstrate the utility of generated urban flow data and the ability of our
model for long-term flow generation and urban flow prediction. Our code is
released at: https://github.com/tsinghua-fib-lab/KSTDiff-Urban-flow-generation.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2309.10547v1},
File          = {2309.10547v1.pdf}
}
@article{1402.0574v1,
Author        = {Kira Radinsky and Sagie Davidovich and Shaul Markovitch},
Title         = {Learning to Predict from Textual Data},
Eprint        = {1402.0574v1},
DOI           = {10.1613/jair.3865},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Given a current news event, we tackle the problem of generating plausible
predictions of future events it might cause. We present a new methodology for
modeling and predicting such future news events using machine learning and data
mining techniques. Our Pundit algorithm generalizes examples of causality pairs
to infer a causality predictor. To obtain precisely labeled causality examples,
we mine 150 years of news articles and apply semantic natural language modeling
techniques to headlines containing certain predefined causality patterns. For
generalization, the model uses a vast number of world knowledge ontologies.
Empirical evaluation on real news articles shows that our Pundit algorithm
performs as well as non-expert humans.},
Year          = {2014},
Month         = {Feb},
Note          = {Journal Of Artificial Intelligence Research, Volume 45, pages
  641-684, 2012},
Url           = {http://arxiv.org/abs/1402.0574v1},
File          = {1402.0574v1.pdf}
}
@article{1906.05468v1,
Author        = {Aditya Joshi and Sarvnaz Karimi and Ross Sparks and Cecile Paris and C Raina MacIntyre},
Title         = {A Comparison of Word-based and Context-based Representations for
  Classification Problems in Health Informatics},
Eprint        = {1906.05468v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Distributed representations of text can be used as features when training a
statistical classifier. These representations may be created as a composition
of word vectors or as context-based sentence vectors. We compare the two kinds
of representations (word versus context) for three classification problems:
influenza infection classification, drug usage classification and personal
health mention classification. For statistical classifiers trained for each of
these problems, context-based representations based on ELMo, Universal Sentence
Encoder, Neural-Net Language Model and FLAIR are better than Word2Vec, GloVe
and the two adapted using the MESH ontology. There is an improvement of 2-4% in
the accuracy when these context-based representations are used instead of
word-based representations.},
Year          = {2019},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1906.05468v1},
File          = {1906.05468v1.pdf}
}
@article{2001.07526v1,
Author        = {Vevake Balaraman and Bernardo Magnini},
Title         = {Domain-Aware Dialogue State Tracker for Multi-Domain Dialogue Systems},
Eprint        = {2001.07526v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In task-oriented dialogue systems the dialogue state tracker (DST) component
is responsible for predicting the state of the dialogue based on the dialogue
history. Current DST approaches rely on a predefined domain ontology, a fact
that limits their effective usage for large scale conversational agents, where
the DST constantly needs to be interfaced with ever-increasing services and
APIs. Focused towards overcoming this drawback, we propose a domain-aware
dialogue state tracker, that is completely data-driven and it is modeled to
predict for dynamic service schemas. The proposed model utilizes domain and
slot information to extract both domain and slot specific representations for a
given dialogue, and then uses such representations to predict the values of the
corresponding slot. Integrating this mechanism with a pretrained language model
(i.e. BERT), our approach can effectively learn semantic relations.},
Year          = {2020},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2001.07526v1},
File          = {2001.07526v1.pdf}
}
@article{1510.00259v2,
Author        = {Stephanie L. Hyland and Theofanis Karaletsos and Gunnar Rätsch},
Title         = {A Generative Model of Words and Relationships from Multiple Sources},
Eprint        = {1510.00259v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Neural language models are a powerful tool to embed words into semantic
vector spaces. However, learning such models generally relies on the
availability of abundant and diverse training examples. In highly specialised
domains this requirement may not be met due to difficulties in obtaining a
large corpus, or the limited range of expression in average use. Such domains
may encode prior knowledge about entities in a knowledge base or ontology. We
propose a generative model which integrates evidence from diverse data sources,
enabling the sharing of semantic information. We achieve this by generalising
the concept of co-occurrence from distributional semantics to include other
relationships between entities or words, which we model as affine
transformations on the embedding space. We demonstrate the effectiveness of
this approach by outperforming recent models on a link prediction task and
demonstrating its ability to profit from partially or fully unobserved data
training labels. We further demonstrate the usefulness of learning from
different data sources with overlapping vocabularies.},
Year          = {2015},
Month         = {Oct},
Url           = {http://arxiv.org/abs/1510.00259v2},
File          = {1510.00259v2.pdf}
}
@article{2010.11784v2,
Author        = {Fangyu Liu and Ehsan Shareghi and Zaiqiao Meng and Marco Basaldella and Nigel Collier},
Title         = {Self-Alignment Pretraining for Biomedical Entity Representations},
Eprint        = {2010.11784v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Despite the widespread success of self-supervised learning via masked
language models (MLM), accurately capturing fine-grained semantic relationships
in the biomedical domain remains a challenge. This is of paramount importance
for entity-level tasks such as entity linking where the ability to model entity
relations (especially synonymy) is pivotal. To address this challenge, we
propose SapBERT, a pretraining scheme that self-aligns the representation space
of biomedical entities. We design a scalable metric learning framework that can
leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts.
In contrast with previous pipeline-based hybrid systems, SapBERT offers an
elegant one-model-for-all solution to the problem of medical entity linking
(MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking
datasets. In the scientific domain, we achieve SOTA even without task-specific
supervision. With substantial improvement over various domain-specific
pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining
scheme proves to be both effective and robust.},
Year          = {2020},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2010.11784v2},
File          = {2010.11784v2.pdf}
}
@article{2210.06254v2,
Author        = {Rotem Dror and Haoyu Wang and Dan Roth},
Title         = {Zero-Shot On-the-Fly Event Schema Induction},
Eprint        = {2210.06254v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {What are the events involved in a pandemic outbreak? What steps should be
taken when planning a wedding? The answers to these questions can be found by
collecting many documents on the complex event of interest, extracting relevant
information, and analyzing it. We present a new approach in which large
language models are utilized to generate source documents that allow
predicting, given a high-level event definition, the specific events,
arguments, and relations between them to construct a schema that describes the
complex event in its entirety. Using our model, complete schemas on any topic
can be generated on-the-fly without any manual data collection, i.e., in a
zero-shot manner. Moreover, we develop efficient methods to extract pertinent
information from texts and demonstrate in a series of experiments that these
schemas are considered to be more complete than human-curated ones in the
majority of examined scenarios. Finally, we show that this framework is
comparable in performance with previous supervised schema induction methods
that rely on collecting real texts while being more general and flexible
without the need for a predefined ontology.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.06254v2},
File          = {2210.06254v2.pdf}
}
@article{2211.06642v1,
Author        = {Firoj Alam and Fahim Dalvi and Nadir Durrani and Hassan Sajjad and Abdul Rafae Khan and Jia Xu},
Title         = {ConceptX: A Framework for Latent Concept Analysis},
Eprint        = {2211.06642v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The opacity of deep neural networks remains a challenge in deploying
solutions where explanation is as important as precision. We present ConceptX,
a human-in-the-loop framework for interpreting and annotating latent
representational space in pre-trained Language Models (pLMs). We use an
unsupervised method to discover concepts learned in these models and enable a
graphical interface for humans to generate explanations for the concepts. To
facilitate the process, we provide auto-annotations of the concepts (based on
traditional linguistic ontologies). Such annotations enable development of a
linguistic resource that directly represents latent concepts learned within
deep NLP models. These include not just traditional linguistic concepts, but
also task-specific or sensitive concepts (words grouped based on gender or
religious connotation) that helps the annotators to mark bias in the model. The
framework consists of two parts (i) concept discovery and (ii) annotation
platform.},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.06642v1},
File          = {2211.06642v1.pdf}
}
@article{2212.09939v2,
Author        = {Jeffrey Zhao and Yuan Cao and Raghav Gupta and Harrison Lee and Abhinav Rastogi and Mingqiu Wang and Hagen Soltau and Izhak Shafran and Yonghui Wu},
Title         = {AnyTOD: A Programmable Task-Oriented Dialog System},
Eprint        = {2212.09939v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {We propose AnyTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system
capable of handling unseen tasks without task-specific training. We view TOD as
a program executed by a language model (LM), where program logic and ontology
is provided by a designer as a schema. To enable generalization to unseen
schemas and programs without prior training, AnyTOD adopts a neuro-symbolic
approach. A neural LM keeps track of events occurring during a conversation and
a symbolic program implementing the dialog policy is executed to recommend next
actions AnyTOD should take. This approach drastically reduces data annotation
and model training requirements, addressing the enduring challenge of rapidly
adapting a TOD system to unseen tasks and domains. We demonstrate
state-of-the-art results on STAR, ABCD and SGD benchmarks. We also demonstrate
strong zero-shot transfer ability in low-resource settings, such as zero-shot
on MultiWOZ. In addition, we release STARv2, an updated version of the STAR
dataset with richer annotations, for benchmarking zero-shot end-to-end TOD
models.},
Year          = {2022},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2212.09939v2},
File          = {2212.09939v2.pdf}
}
@article{2303.09905v1,
Author        = {A. Coca and B. H. Tseng and W. Lin and B. Byrne},
Title         = {More Robust Schema-Guided Dialogue State Tracking via Tree-Based
  Paraphrase Ranking},
Eprint        = {2303.09905v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The schema-guided paradigm overcomes scalability issues inherent in building
task-oriented dialogue (TOD) agents with static ontologies. Instead of
operating on dialogue context alone, agents have access to hierarchical schemas
containing task-relevant natural language descriptions. Fine-tuned language
models excel at schema-guided dialogue state tracking (DST) but are sensitive
to the writing style of the schemas. We explore methods for improving the
robustness of DST models. We propose a framework for generating synthetic
schemas which uses tree-based ranking to jointly optimise lexical diversity and
semantic faithfulness. The generalisation of strong baselines is improved when
augmenting their training data with prompts generated by our framework, as
demonstrated by marked improvements in average joint goal accuracy (JGA) and
schema sensitivity (SS) on the SGD-X benchmark.},
Year          = {2023},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2303.09905v1},
File          = {2303.09905v1.pdf}
}
@article{2304.13130v1,
Author        = {Giacomo Nebbia and Adriana Kovashka},
Title         = {Hypernymization of named entity-rich captions for grounding-based
  multi-modal pretraining},
Eprint        = {2304.13130v1},
DOI           = {10.1145/3591106.3592223},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Named entities are ubiquitous in text that naturally accompanies images,
especially in domains such as news or Wikipedia articles. In previous work,
named entities have been identified as a likely reason for low performance of
image-text retrieval models pretrained on Wikipedia and evaluated on named
entities-free benchmark datasets. Because they are rarely mentioned, named
entities could be challenging to model. They also represent missed learning
opportunities for self-supervised models: the link between named entity and
object in the image may be missed by the model, but it would not be if the
object were mentioned using a more common term. In this work, we investigate
hypernymization as a way to deal with named entities for pretraining
grounding-based multi-modal models and for fine-tuning on open-vocabulary
detection. We propose two ways to perform hypernymization: (1) a ``manual''
pipeline relying on a comprehensive ontology of concepts, and (2) a ``learned''
approach where we train a language model to learn to perform hypernymization.
We run experiments on data from Wikipedia and from The New York Times. We
report improved pretraining performance on objects of interest following
hypernymization, and we show the promise of hypernymization on open-vocabulary
detection, specifically on classes not seen during training.},
Year          = {2023},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2304.13130v1},
File          = {2304.13130v1.pdf}
}
@article{2403.12666v1,
Author        = {Dojun Park and Sebastian Padó},
Title         = {Multi-Dimensional Machine Translation Evaluation: Model Evaluation and
  Resource for Korean},
Eprint        = {2403.12666v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Almost all frameworks for the manual or automatic evaluation of machine
translation characterize the quality of an MT output with a single number. An
exception is the Multidimensional Quality Metrics (MQM) framework which offers
a fine-grained ontology of quality dimensions for scoring (such as style,
fluency, accuracy, and terminology). Previous studies have demonstrated the
feasibility of MQM annotation but there are, to our knowledge, no computational
models that predict MQM scores for novel texts, due to a lack of resources. In
this paper, we address these shortcomings by (a) providing a 1200-sentence MQM
evaluation benchmark for the language pair English-Korean and (b) reframing MT
evaluation as the multi-task problem of simultaneously predicting several MQM
scores using SOTA language models, both in a reference-based MT evaluation
setup and a reference-free quality estimation (QE) setup. We find that
reference-free setup outperforms its counterpart in the style dimension while
reference-based models retain an edge regarding accuracy. Overall, RemBERT
emerges as the most promising model. Through our evaluation, we offer an
insight into the translation quality in a more fine-grained, interpretable
manner.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.12666v1},
File          = {2403.12666v1.pdf}
}
@article{2403.16984v2,
Author        = {Hanane Kteich and Na Li and Usashi Chatterjee and Zied Bouraoui and Steven Schockaert},
Title         = {Modelling Commonsense Commonalities with Multi-Facet Concept Embeddings},
Eprint        = {2403.16984v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {Concept embeddings offer a practical and efficient mechanism for injecting
commonsense knowledge into downstream tasks. Their core purpose is often not to
predict the commonsense properties of concepts themselves, but rather to
identify commonalities, i.e.\ sets of concepts which share some property of
interest. Such commonalities are the basis for inductive generalisation, hence
high-quality concept embeddings can make learning easier and more robust.
Unfortunately, standard embeddings primarily reflect basic taxonomic
categories, making them unsuitable for finding commonalities that refer to more
specific aspects (e.g.\ the colour of objects or the materials they are made
of). In this paper, we address this limitation by explicitly modelling the
different facets of interest when learning concept embeddings. We show that
this leads to embeddings which capture a more diverse range of commonsense
properties, and consistently improves results in downstream tasks such as
ultra-fine entity typing and ontology completion.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.16984v2},
File          = {2403.16984v2.pdf}
}
@article{2404.12447v3,
Author        = {Yoonsang Lee and Xi Ye and Eunsol Choi},
Title         = {AmbigDocs: Reasoning across Documents on Different Entities under the
  Same Name},
Eprint        = {2404.12447v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Different entities with the same name can be difficult to distinguish.
Handling confusing entity mentions is a crucial skill for language models
(LMs). For example, given the question "Where was Michael Jordan educated?" and
a set of documents discussing different people named Michael Jordan, can LMs
distinguish entity mentions to generate a cohesive answer to the question? To
test this ability, we introduce a new benchmark, AmbigDocs. By leveraging
Wikipedia's disambiguation pages, we identify a set of documents, belonging to
different entities who share an ambiguous name. From these documents, we
generate questions containing an ambiguous name and their corresponding sets of
answers. Our analysis reveals that current state-of-the-art models often yield
ambiguous answers or incorrectly merge information belonging to different
entities. We establish an ontology categorizing four types of incomplete
answers and automatic evaluation metrics to identify such categories. We lay
the foundation for future work on reasoning across multiple documents with
ambiguous entities.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.12447v3},
File          = {2404.12447v3.pdf}
}
@article{2408.04673v1,
Author        = {Tingyan Ma and Wei Liu and Bin Lu and Xiaoying Gan and Yunqiang Zhu and Luoyi Fu and Chenghu Zhou},
Title         = {AutoFAIR : Automatic Data FAIRification via Machine Reading},
Eprint        = {2408.04673v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The explosive growth of data fuels data-driven research, facilitating
progress across diverse domains. The FAIR principles emerge as a guiding
standard, aiming to enhance the findability, accessibility, interoperability,
and reusability of data. However, current efforts primarily focus on manual
data FAIRification, which can only handle targeted data and lack efficiency. To
address this issue, we propose AutoFAIR, an architecture designed to enhance
data FAIRness automately. Firstly, We align each data and metadata operation
with specific FAIR indicators to guide machine-executable actions. Then, We
utilize Web Reader to automatically extract metadata based on language models,
even in the absence of structured data webpage schemas. Subsequently, FAIR
Alignment is employed to make metadata comply with FAIR principles by ontology
guidance and semantic matching. Finally, by applying AutoFAIR to various data,
especially in the field of mountain hazards, we observe significant
improvements in findability, accessibility, interoperability, and reusability
of data. The FAIRness scores before and after applying AutoFAIR indicate
enhanced data value.},
Year          = {2024},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2408.04673v1},
File          = {2408.04673v1.pdf}
}
@article{2410.23303v1,
Author        = {Philipp Dechent and Elias Barbers and Simon Clark and Susanne Lehner and Brady Planden and Masaki Adachi and David A. Howey and Sabine Paarmann},
Title         = {Demonstrating Linked Battery Data To Accelerate Knowledge Flow in
  Battery Science},
Eprint        = {2410.23303v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Batteries are pivotal for transitioning to a climate-friendly future, leading
to a surge in battery research. Scopus (Elsevier) lists 14,388 papers that
mention "lithium-ion battery" in 2023 alone, making it infeasible for
individuals to keep up. This paper discusses strategies based on structured,
semantic, and linked data to manage this information overload. Structured data
follows a predefined, machine-readable format; semantic data includes metadata
for context; linked data references other semantic data, forming a web of
interconnected information. We use a battery-related ontology, BattINFO to
standardise terms and enable automated data extraction and analysis. Our
methodology integrates full-text search and machine-readable data, enhancing
data retrieval and battery testing. We aim to unify commercial cell information
and develop tools for the battery community such as manufacturer-independent
cycling procedure descriptions and external memory for Large Language Models.
Although only a first step, this approach significantly accelerates battery
research and digitalizes battery testing, inviting community participation for
continuous improvement. We provide the structured data and the tools to access
them as open source.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.23303v1},
File          = {2410.23303v1.pdf}
}
@article{2411.01205v1,
Author        = {Jianyu Liu and Sheng Bi and Guilin Qi},
Title         = {PRIMO: Progressive Induction for Multi-hop Open Rule Generation},
Eprint        = {2411.01205v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Open rule refer to the implication from premise atoms to hypothesis atoms,
which captures various relations between instances in the real world. Injecting
open rule knowledge into the machine helps to improve the performance of
downstream tasks such as dialogue and relation extraction. Existing approaches
focus on single-hop open rule generation, ignoring multi-hop scenarios, leading
to logical inconsistencies between premise and hypothesis atoms, as well as
semantic duplication of generated rule atoms. To address these issues, we
propose a progressive multi-stage open rule generation method called PRIMO. We
introduce ontology information during the rule generation stage to reduce
ambiguity and improve rule accuracy. PRIMO constructs a multi-stage structure
consisting of generation, extraction, and ranking modules to fully leverage the
latent knowledge within the language model across multiple dimensions.
Furthermore, we employ reinforcement learning from human feedback to further
optimize model, enhancing the model's understanding of commonsense knowledge.
Experiments show that compared to baseline models, PRIMO significantly improves
rule quality and diversity while reducing the repetition rate of rule atoms.},
Year          = {2024},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2411.01205v1},
File          = {2411.01205v1.pdf}
}
@article{2310.00149v3,
Author        = {Hao Liu and Jiarui Feng and Lecheng Kong and Ningyue Liang and Dacheng Tao and Yixin Chen and Muhan Zhang},
Title         = {One for All: Towards Training One Graph Model for All Classification
  Tasks},
Eprint        = {2310.00149v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Designing a single model to address multiple tasks has been a long-standing
objective in artificial intelligence. Recently, large language models have
demonstrated exceptional capability in solving different tasks within the
language domain. However, a unified model for various graph tasks remains
underexplored, primarily due to the challenges unique to the graph learning
domain. First, graph data from different areas carry distinct attributes and
follow different distributions. Such discrepancy makes it hard to represent
graphs in a single representation space. Second, tasks on graphs diversify into
node, link, and graph tasks, requiring distinct embedding strategies. Finally,
an appropriate graph prompting paradigm for in-context learning is unclear. We
propose \textbf{One for All (OFA)}, the first general framework that can use a
single graph model to address the above challenges. Specifically, OFA proposes
text-attributed graphs to unify different graph data by describing nodes and
edges with natural language and uses language models to encode the diverse and
possibly cross-domain text attributes to feature vectors in the same embedding
space. Furthermore, OFA introduces the concept of nodes-of-interest to
standardize different tasks with a single task representation. For in-context
learning on graphs, OFA introduces a novel graph prompting paradigm that
appends prompting substructures to the input graph, which enables it to address
varied tasks without fine-tuning. We train the OFA model using graph data from
multiple domains (including citation networks, molecular graphs, knowledge
graphs, etc.) simultaneously and evaluate its ability in supervised, few-shot,
and zero-shot learning scenarios. OFA performs well across different tasks,
making it the first general-purpose across-domains classification model on
graphs.},
Year          = {2023},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2310.00149v3},
File          = {2310.00149v3.pdf}
}
@article{2312.12989v1,
Author        = {Emily Groves and Minhong Wang and Yusuf Abdulle and Holger Kunz and Jason Hoelscher-Obermaier and Ronin Wu and Honghan Wu},
Title         = {Benchmarking and Analyzing In-context Learning, Fine-tuning and
  Supervised Learning for Biomedical Knowledge Curation: a focused study on
  chemical entities of biological interest},
Eprint        = {2312.12989v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Automated knowledge curation for biomedical ontologies is key to ensure that
they remain comprehensive, high-quality and up-to-date. In the era of
foundational language models, this study compares and analyzes three NLP
paradigms for curation tasks: in-context learning (ICL), fine-tuning (FT), and
supervised learning (ML). Using the Chemical Entities of Biological Interest
(ChEBI) database as a model ontology, three curation tasks were devised. For
ICL, three prompting strategies were employed with GPT-4, GPT-3.5, BioGPT.
PubmedBERT was chosen for the FT paradigm. For ML, six embedding models were
utilized for training Random Forest and Long-Short Term Memory models. Five
setups were designed to assess ML and FT model performance across different
data availability scenarios.Datasets for curation tasks included: task 1
(620,386), task 2 (611,430), and task 3 (617,381), maintaining a 50:50 positive
versus negative ratio. For ICL models, GPT-4 achieved best accuracy scores of
0.916, 0.766 and 0.874 for tasks 1-3 respectively. In a direct comparison, ML
(trained on ~260,000 triples) outperformed ICL in accuracy across all tasks.
(accuracy differences: +.11, +.22 and +.17). Fine-tuned PubmedBERT performed
similarly to leading ML models in tasks 1 & 2 (F1 differences: -.014 and
+.002), but worse in task 3 (-.048). Simulations revealed performance declines
in both ML and FT models with smaller and higher imbalanced training data.
where ICL (particularly GPT-4) excelled in tasks 1 & 3. GPT-4 excelled in tasks
1 and 3 with less than 6,000 triples, surpassing ML/FT. ICL underperformed
ML/FT in task 2.ICL-augmented foundation models can be good assistants for
knowledge curation with correct prompting, however, not making ML and FT
paradigms obsolete. The latter two require task-specific data to beat ICL. In
such cases, ML relies on small pretrained embeddings, minimizing computational
demands.},
Year          = {2023},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2312.12989v1},
File          = {2312.12989v1.pdf}
}
@article{2410.00616v1,
Author        = {Léon-Paul Schaub Torre and Pelayo Quirós and Helena García Mieres},
Title         = {Detección Automática de Patologías en Notas Clínicas en
  Español Combinando Modelos de Lenguaje y Ontologías Médicos},
Eprint        = {2410.00616v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {In this paper we present a hybrid method for the automatic detection of
dermatological pathologies in medical reports. We use a large language model
combined with medical ontologies to predict, given a first appointment or
follow-up medical report, the pathology a person may suffer from. The results
show that teaching the model to learn the type, severity and location on the
body of a dermatological pathology as well as in which order it has to learn
these three features significantly increases its accuracy. The article presents
the demonstration of state-of-the-art results for classification of medical
texts with a precision of 0.84, micro and macro F1-score of 0.82 and 0.75, and
makes both the method and the dataset used available to the community.
  --
  En este art\'iculo presentamos un m\'etodo h\'ibrido para la detecci\'on
autom\'atica de patolog\'ias dermatol\'ogicas en informes m\'edicos. Usamos un
modelo de lenguaje amplio en espa\~nol combinado con ontolog\'ias m\'edicas
para predecir, dado un informe m\'edico de primera cita o de seguimiento, la
patolog\'ia del paciente. Los resultados muestran que el tipo, la gravedad y el
sitio en el cuerpo de una patolog\'ia dermatol\'ogica, as\'i como en qu\'e
orden tiene un modelo que aprender esas tres caracter\'isticas, aumentan su
precisi\'on. El art\'iculo presenta la demostraci\'on de resultados comparables
al estado del arte de clasificaci\'on de textos m\'edicos con una precisi\'on
de 0.84, micro y macro F1-score de 0.82 y 0.75, y deja a disposici\'on de la
comunidad tanto el m\'etodo como el conjunto de datos utilizado.},
Year          = {2024},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2410.00616v1},
File          = {2410.00616v1.pdf}
}
@article{2211.07499v2,
Author        = {Aman Priyanshu and Supriti Vijay},
Title         = {AdaptKeyBERT: An Attention-Based approach towards Few-Shot & Zero-Shot
  Domain Adaptation of KeyBERT},
Eprint        = {2211.07499v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Keyword extraction has been an important topic for modern natural language
processing. With its applications ranging from ontology generation, fact
verification in summarized text, and recommendation systems. While it has had
significant data-intensive applications, it is often hampered when the data set
is small. Downstream training for keyword extractors is a lengthy process and
requires a significant amount of data. Recently, Few-shot Learning (FSL) and
Zero-Shot Learning (ZSL) have been proposed to tackle this problem. Therefore,
we propose AdaptKeyBERT, a pipeline for training keyword extractors with LLM
bases by incorporating the concept of regularized attention into a pre-training
phase for downstream domain adaptation. As we believe our work has implications
to be utilized in the pipeline of FSL/ZSL and keyword extraction, we
open-source our code as well as provide the fine-tuning library of the same
name AdaptKeyBERT at https://github.com/AmanPriyanshu/AdaptKeyBERT.},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.07499v2},
File          = {2211.07499v2.pdf}
}
@article{2501.08540v1,
Author        = {Ning Pei Ding and Jingge Du and Zaiwen Feng},
Title         = {Knowledge prompt chaining for semantic modeling},
Eprint        = {2501.08540v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The task of building semantics for structured data such as CSV, JSON, and XML
files is highly relevant in the knowledge representation field. Even though we
have a vast of structured data on the internet, mapping them to domain
ontologies to build semantics for them is still very challenging as it requires
the construction model to understand and learn graph-structured knowledge.
Otherwise, the task will require human beings' effort and cost. In this paper,
we proposed a novel automatic semantic modeling framework: Knowledge Prompt
Chaining. It can serialize the graph-structured knowledge and inject it into
the LLMs properly in a Prompt Chaining architecture. Through this knowledge
injection and prompting chaining, the model in our framework can learn the
structure information and latent space of the graph and generate the semantic
labels and semantic graphs following the chains' insturction naturally. Based
on experimental results, our method achieves better performance than existing
leading techniques, despite using reduced structured input data.},
Year          = {2025},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2501.08540v1},
File          = {2501.08540v1.pdf}
}
@article{2312.03742v1,
Author        = {Angeela Acharya and Sulabh Shrestha and Anyi Chen and Joseph Conte and Sanja Avramovic and Siddhartha Sikdar and Antonios Anastasopoulos and Sanmay Das},
Title         = {Clinical Risk Prediction Using Language Models: Benefits And
  Considerations},
Eprint        = {2312.03742v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {The utilization of Electronic Health Records (EHRs) for clinical risk
prediction is on the rise. However, strict privacy regulations limit access to
comprehensive health records, making it challenging to apply standard machine
learning algorithms in practical real-world scenarios. Previous research has
addressed this data limitation by incorporating medical ontologies and
employing transfer learning methods. In this study, we investigate the
potential of leveraging language models (LMs) as a means to incorporate
supplementary domain knowledge for improving the performance of various
EHR-based risk prediction tasks. Unlike applying LMs to unstructured EHR data
such as clinical notes, this study focuses on using textual descriptions within
structured EHR to make predictions exclusively based on that information. We
extensively compare against previous approaches across various data types and
sizes. We find that employing LMs to represent structured EHRs, such as
diagnostic histories, leads to improved or at least comparable performance in
diverse risk prediction tasks. Furthermore, LM-based approaches offer numerous
advantages, including few-shot learning, the capability to handle previously
unseen medical concepts, and adaptability to various medical vocabularies.
Nevertheless, we underscore, through various experiments, the importance of
being cautious when employing such models, as concerns regarding the
reliability of LMs persist.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2312.03742v1},
File          = {2312.03742v1.pdf}
}
@article{2409.15881v1,
Author        = {Cezar Sas and Andrea Capiluppi},
Title         = {Automatic Bottom-Up Taxonomy Construction: A Software Application Domain
  Study},
Eprint        = {2409.15881v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {Previous research in software application domain classification has faced
challenges due to the lack of a proper taxonomy that explicitly models
relations between classes. As a result, current solutions are less effective
for real-world usage. This study aims to develop a comprehensive software
application domain taxonomy by integrating multiple datasources and leveraging
ensemble methods. The goal is to overcome the limitations of individual sources
and configurations by creating a more robust, accurate, and reproducible
taxonomy. This study employs a quantitative research design involving three
different datasources: an existing Computer Science Ontology (CSO), Wikidata,
and LLMs. The study utilises a combination of automated and human evaluations
to assess the quality of a taxonomy. The outcome measures include the number of
unlinked terms, self-loops, and overall connectivity of the taxonomy. The
results indicate that individual datasources have advantages and drawbacks: the
CSO datasource showed minimal variance across different configurations, but a
notable issue of missing technical terms and a high number of self-loops. The
Wikipedia datasource required significant filtering during construction to
improve metric performance. LLM-generated taxonomies demonstrated better
performance when using context-rich prompts. An ensemble approach showed the
most promise, successfully reducing the number of unlinked terms and
self-loops, thus creating a more connected and comprehensive taxonomy. The
study addresses the construction of a software application domain taxonomy
relying on pre-existing resources. Our results indicate that an ensemble
approach to taxonomy construction can effectively address the limitations of
individual datasources. Future work should focus on refining the ensemble
techniques and exploring additional datasources to enhance the taxonomy's
accuracy and completeness.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.15881v1},
File          = {2409.15881v1.pdf}
}
@article{2011.07959v1,
Author        = {Rahul Yedida and Saad Mohammad Abrar and Cleber Melo-Filho and Eugene Muratov and Rada Chirkova and Alexander Tropsha},
Title         = {Text Mining to Identify and Extract Novel Disease Treatments From
  Unstructured Datasets},
Eprint        = {2011.07959v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Objective: We aim to learn potential novel cures for diseases from
unstructured text sources. More specifically, we seek to extract drug-disease
pairs of potential cures to diseases by a simple reasoning over the structure
of spoken text.
  Materials and Methods: We use Google Cloud to transcribe podcast episodes of
an NPR radio show. We then build a pipeline for systematically pre-processing
the text to ensure quality input to the core classification model, which feeds
to a series of post-processing steps for obtaining filtered results. Our
classification model itself uses a language model pre-trained on PubMed text.
The modular nature of our pipeline allows for ease of future developments in
this area by substituting higher quality components at each stage of the
pipeline. As a validation measure, we use ROBOKOP, an engine over a medical
knowledge graph with only validated pathways, as a ground truth source for
checking the existence of the proposed pairs. For the proposed pairs not found
in ROBOKOP, we provide further verification using Chemotext.
  Results: We found 30.4% of our proposed pairs in the ROBOKOP database. For
example, our model successfully identified that Omeprazole can help treat
heartburn.We discuss the significance of this result, showing some examples of
the proposed pairs.
  Discussion and Conclusion: The agreement of our results with the existing
knowledge source indicates a step in the right direction. Given the
plug-and-play nature of our framework, it is easy to add, remove, or modify
parts to improve the model as necessary. We discuss the results showing some
examples, and note that this is a potentially new line of research that has
further scope to be explored. Although our approach was originally oriented on
radio podcast transcripts, it is input-agnostic and could be applied to any
source of textual data and to any problem of interest.},
Year          = {2020},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2011.07959v1},
File          = {2011.07959v1.pdf}
}
@article{2107.02137v1,
Author        = {Yu Sun and Shuohuan Wang and Shikun Feng and Siyu Ding and Chao Pang and Junyuan Shang and Jiaxiang Liu and Xuyi Chen and Yanbin Zhao and Yuxiang Lu and Weixin Liu and Zhihua Wu and Weibao Gong and Jianzhong Liang and Zhizhou Shang and Peng Sun and Wei Liu and Xuan Ouyang and Dianhai Yu and Hao Tian and Hua Wu and Haifeng Wang},
Title         = {ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language
  Understanding and Generation},
Eprint        = {2107.02137v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pre-trained models have achieved state-of-the-art results in various Natural
Language Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown
that scaling up pre-trained language models can improve their generalization
abilities. Particularly, the GPT-3 model with 175 billion parameters shows its
strong task-agnostic zero-shot/few-shot learning capabilities. Despite their
success, these large-scale models are trained on plain texts without
introducing knowledge such as linguistic knowledge and world knowledge. In
addition, most large-scale models are trained in an auto-regressive way. As a
result, this kind of traditional fine-tuning approach demonstrates relatively
weak performance when solving downstream language understanding tasks. In order
to solve the above problems, we propose a unified framework named ERNIE 3.0 for
pre-training large-scale knowledge enhanced models. It fuses auto-regressive
network and auto-encoding network, so that the trained model can be easily
tailored for both natural language understanding and generation tasks with
zero-shot learning, few-shot learning or fine-tuning. We trained the model with
10 billion parameters on a 4TB corpus consisting of plain texts and a
large-scale knowledge graph. Empirical results show that the model outperforms
the state-of-the-art models on 54 Chinese NLP tasks, and its English version
achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing
the human performance by +0.8% (90.6% vs. 89.8%).},
Year          = {2021},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2107.02137v1},
File          = {2107.02137v1.pdf}
}
@article{2403.02576v2,
Author        = {Xinbing Wang and Luoyi Fu and Xiaoying Gan and Ying Wen and Guanjie Zheng and Jiaxin Ding and Liyao Xiang and Nanyang Ye and Meng Jin and Shiyu Liang and Bin Lu and Haiwen Wang and Yi Xu and Cheng Deng and Shao Zhang and Huquan Kang and Xingli Wang and Qi Li and Zhixin Guo and Jiexing Qi and Pan Liu and Yuyang Ren and Lyuwen Wu and Jungang Yang and Jianping Zhou and Chenghu Zhou},
Title         = {AceMap: Knowledge Discovery through Academic Graph},
Eprint        = {2403.02576v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DL},
Abstract      = {The exponential growth of scientific literature requires effective management
and extraction of valuable insights. While existing scientific search engines
excel at delivering search results based on relational databases, they often
neglect the analysis of collaborations between scientific entities and the
evolution of ideas, as well as the in-depth analysis of content within
scientific publications. The representation of heterogeneous graphs and the
effective measurement, analysis, and mining of such graphs pose significant
challenges. To address these challenges, we present AceMap, an academic system
designed for knowledge discovery through academic graph. We present advanced
database construction techniques to build the comprehensive AceMap database
with large-scale academic entities that contain rich visual, textual, and
numerical information. AceMap also employs innovative visualization,
quantification, and analysis methods to explore associations and logical
relationships among academic entities. AceMap introduces large-scale academic
network visualization techniques centered on nebular graphs, providing a
comprehensive view of academic networks from multiple perspectives. In
addition, AceMap proposes a unified metric based on structural entropy to
quantitatively measure the knowledge content of different academic entities.
Moreover, AceMap provides advanced analysis capabilities, including tracing the
evolution of academic ideas through citation relationships and concept
co-occurrence, and generating concise summaries informed by this evolutionary
process. In addition, AceMap uses machine reading methods to generate potential
new ideas at the intersection of different fields. Exploring the integration of
large language models and knowledge graphs is a promising direction for future
research in idea evolution. Please visit \url{https://www.acemap.info} for
further exploration.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.02576v2},
File          = {2403.02576v2.pdf}
}
@article{2403.09712v1,
Author        = {Xin Lin and Tianhuang Su and Zhenya Huang and Shangzi Xue and Haifeng Liu and Enhong Chen},
Title         = {A Knowledge-Injected Curriculum Pretraining Framework for Question
  Answering},
Eprint        = {2403.09712v1},
DOI           = {10.1145/3589334.3645406},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Knowledge-based question answering (KBQA) is a key task in NLP research, and
also an approach to access the web data and knowledge, which requires
exploiting knowledge graphs (KGs) for reasoning. In the literature, one
promising solution for KBQA is to incorporate the pretrained language model
(LM) with KGs by generating KG-centered pretraining corpus, which has shown its
superiority. However, these methods often depend on specific techniques and
resources to work, which may not always be available and restrict its
application. Moreover, existing methods focus more on improving language
understanding with KGs, while neglect the more important human-like complex
reasoning. To this end, in this paper, we propose a general Knowledge-Injected
Curriculum Pretraining framework (KICP) to achieve comprehensive KG learning
and exploitation for KBQA tasks, which is composed of knowledge injection (KI),
knowledge adaptation (KA) and curriculum reasoning (CR). Specifically, the KI
module first injects knowledge into the LM by generating KG-centered
pretraining corpus, and generalizes the process into three key steps that could
work with different implementations for flexible application. Next, the KA
module learns knowledge from the generated corpus with LM equipped with an
adapter as well as keeps its original natural language understanding ability to
reduce the negative impacts of the difference between the generated and natural
corpus. Last, to enable the LM with complex reasoning, the CR module follows
human reasoning patterns to construct three corpora with increasing
difficulties of reasoning, and further trains the LM from easy to hard in a
curriculum manner. We provide an implementation of the general framework, and
evaluate the proposed KICP on four real-word datasets. The results demonstrate
that our framework can achieve higher performances.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.09712v1},
File          = {2403.09712v1.pdf}
}
@article{2502.08547v1,
Author        = {Doudou Zhou and Han Tong and Linshanshan Wang and Suqi Liu and Xin Xiong and Ziming Gan and Romain Griffier and Boris Hejblum and Yun-Chung Liu and Chuan Hong and Clara-Lea Bonzel and Tianrun Cai and Kevin Pan and Yuk-Lam Ho and Lauren Costa and Vidul A. Panickan and J. Michael Gaziano and Kenneth Mandl and Vianney Jouhet and Rodolphe Thiebaut and Zongqi Xia and Kelly Cho and Katherine Liao and Tianxi Cai},
Title         = {Representation Learning to Advance Multi-institutional Studies with
  Electronic Health Record Data},
Eprint        = {2502.08547v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The adoption of EHRs has expanded opportunities to leverage data-driven
algorithms in clinical care and research. A major bottleneck in effectively
conducting multi-institutional EHR studies is the data heterogeneity across
systems with numerous codes that either do not exist or represent different
clinical concepts across institutions. The need for data privacy further limits
the feasibility of including multi-institutional patient-level data required to
study similarities and differences across patient subgroups. To address these
challenges, we developed the GAME algorithm. Tested and validated across 7
institutions and 2 languages, GAME integrates data in several levels: (1) at
the institutional level with knowledge graphs to establish relationships
between codes and existing knowledge sources, providing the medical context for
standard codes and their relationship to each other; (2) between institutions,
leveraging language models to determine the relationships between
institution-specific codes with established standard codes; and (3) quantifying
the strength of the relationships between codes using a graph attention
network. Jointly trained embeddings are created using transfer and federated
learning to preserve data privacy. In this study, we demonstrate the
applicability of GAME in selecting relevant features as inputs for AI-driven
algorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis.
We then highlight the application of GAME harmonized multi-institutional EHR
data in a study of Alzheimer's disease outcomes and suicide risk among patients
with mental health disorders, without sharing patient-level data outside
individual institutions.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.08547v1},
File          = {2502.08547v1.pdf}
}
@article{2001.06674v1,
Author        = {Sean MacAvaney and Arman Cohan and Nazli Goharian and Ross Filice},
Title         = {Ranking Significant Discrepancies in Clinical Reports},
Eprint        = {2001.06674v1},
DOI           = {10.1007/978-3-030-45442-5_30},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.IR},
Abstract      = {Medical errors are a major public health concern and a leading cause of death
worldwide. Many healthcare centers and hospitals use reporting systems where
medical practitioners write a preliminary medical report and the report is
later reviewed, revised, and finalized by a more experienced physician. The
revisions range from stylistic to corrections of critical errors or
misinterpretations of the case. Due to the large quantity of reports written
daily, it is often difficult to manually and thoroughly review all the
finalized reports to find such errors and learn from them. To address this
challenge, we propose a novel ranking approach, consisting of textual and
ontological overlaps between the preliminary and final versions of reports. The
approach learns to rank the reports based on the degree of discrepancy between
the versions. This allows medical practitioners to easily identify and learn
from the reports in which their interpretation most substantially differed from
that of the attending physician (who finalized the report). This is a crucial
step towards uncovering potential errors and helping medical practitioners to
learn from such errors, thus improving patient-care in the long run. We
evaluate our model on a dataset of radiology reports and show that our approach
outperforms both previously-proposed approaches and more recent language models
by 4.5% to 15.4%.},
Year          = {2020},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2001.06674v1},
File          = {2001.06674v1.pdf}
}
@article{1912.06492v1,
Author        = {Sabah Al-Fedaghi},
Title         = {Thinging as a Way of Modeling in Poiesis: Applications in Software
  Engineering},
Eprint        = {1912.06492v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.SE},
Abstract      = {From a software design perspective, a clear definition of design can enhance
project success and development productivity. Even though the focus is on
software engineering, in this paper, we view the notion of design from the
wider point of view of poiesis, the field of the study of the phenomena of
creation and production of the artifacts. In poiesis, design operates through
the medium of modeling. According to several sources, there is as yet no
systematic consolidated body of knowledge that a practitioner can refer to when
designing a computer-based modeling language. Modeling languages such as UML
are practice-based and seldom underpinned with a solid theory-be it
mathematical, ontological or concomitant with language use. In this paper, we
propose adopting a recent addition to the diagrammatic languages, the thinging
machine (abbreviated TM), as a design language in the general area of Poiesis
and we exemplify TM by applying it to software engineering design. We show
intermediate steps of design that led to producing a TM model for a case study.
The case study is taken from a source where a full UML-based design was given.
Contrasting the models produced by the two methodologies points to the
viability of TM as an integrating and unifying modeling language in the design
field.},
Year          = {2019},
Month         = {Dec},
Note          = {International Journal of Computer Science and Information
  Security, Vol. 17, No. 11, November 2019},
Url           = {http://arxiv.org/abs/1912.06492v1},
File          = {1912.06492v1.pdf}
}
@article{2208.10448v1,
Author        = {Renato Vukovic and Michael Heck and Benjamin Matthias Ruppik and Carel van Niekerk and Marcus Zibrowius and Milica Gašić},
Title         = {Dialogue Term Extraction using Transfer Learning and Topological Data
  Analysis},
Eprint        = {2208.10448v1},
DOI           = {10.5281/zenodo.6858565},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Goal oriented dialogue systems were originally designed as a natural language
interface to a fixed data-set of entities that users might inquire about,
further described by domain, slots, and values. As we move towards adaptable
dialogue systems where knowledge about domains, slots, and values may change,
there is an increasing need to automatically extract these terms from raw
dialogues or related non-dialogue data on a large scale. In this paper, we take
an important step in this direction by exploring different features that can
enable systems to discover realizations of domains, slots, and values in
dialogues in a purely data-driven fashion. The features that we examine stem
from word embeddings, language modelling features, as well as topological
features of the word embedding space. To examine the utility of each feature
set, we train a seed model based on the widely used MultiWOZ data-set. Then, we
apply this model to a different corpus, the Schema-Guided Dialogue data-set.
Our method outperforms the previously proposed approach that relies solely on
word embeddings. We also demonstrate that each of the features is responsible
for discovering different kinds of content. We believe our results warrant
further research towards ontology induction, and continued harnessing of
topological data analysis for dialogue and natural language processing
research.},
Year          = {2022},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2208.10448v1},
File          = {2208.10448v1.pdf}
}
@article{2208.10817v1,
Author        = {Hsien-Chin Lin and Christian Geishauser and Shutong Feng and Nurul Lubis and Carel van Niekerk and Michael Heck and Milica Gašić},
Title         = {GenTUS: Simulating User Behaviour and Language in Task-oriented
  Dialogues with Generative Transformers},
Eprint        = {2208.10817v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {User simulators (USs) are commonly used to train task-oriented dialogue
systems (DSs) via reinforcement learning. The interactions often take place on
semantic level for efficiency, but there is still a gap from semantic actions
to natural language, which causes a mismatch between training and deployment
environment. Incorporating a natural language generation (NLG) module with USs
during training can partly deal with this problem. However, since the policy
and NLG of USs are optimised separately, these simulated user utterances may
not be natural enough in a given context. In this work, we propose a generative
transformer-based user simulator (GenTUS). GenTUS consists of an
encoder-decoder structure, which means it can optimise both the user policy and
natural language generation jointly. GenTUS generates both semantic actions and
natural language utterances, preserving interpretability and enhancing language
variation. In addition, by representing the inputs and outputs as word
sequences and by using a large pre-trained language model we can achieve
generalisability in feature representation. We evaluate GenTUS with automatic
metrics and human evaluation. Our results show that GenTUS generates more
natural language and is able to transfer to an unseen ontology in a zero-shot
fashion. In addition, its behaviour can be further shaped with reinforcement
learning opening the door to training specialised user simulators.},
Year          = {2022},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2208.10817v1},
File          = {2208.10817v1.pdf}
}
@article{2012.15243v2,
Author        = {Hongming Zhang and Haoyu Wang and Dan Roth},
Title         = {Unsupervised Label-aware Event Trigger and Argument Classification},
Eprint        = {2012.15243v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Identifying events and mapping them to pre-defined event types has long been
an important natural language processing problem. Most previous work has been
heavily relying on labor-intensive and domain-specific annotations while
ignoring the semantic meaning contained in the labels of the event types. As a
result, the learned models cannot effectively generalize to new domains, where
new event types could be introduced. In this paper, we propose an unsupervised
event extraction pipeline, which first identifies events with available tools
(e.g., SRL) and then automatically maps them to pre-defined event types with
our proposed unsupervised classification model. Rather than relying on
annotated data, our model matches the semantics of identified events with those
of event type labels. Specifically, we leverage pre-trained language models to
contextually represent pre-defined types for both event triggers and arguments.
After we map identified events to the target types via representation
similarity, we use the event ontology (e.g., argument type "Victim" can only
appear as the argument of event type "Attack") as global constraints to
regularize the prediction. The proposed approach is shown to be very effective
when tested on the ACE-2005 dataset, which has 33 trigger and 22 argument
types. Without using any annotation, we successfully map 83% of the triggers
and 54% of the arguments to the correct types, almost doubling the performance
of previous zero-shot approaches.},
Year          = {2020},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2012.15243v2},
File          = {2012.15243v2.pdf}
}
@article{2106.09502v1,
Author        = {Diego Garcia-Olano and Yasumasa Onoe and Ioana Baldini and Joydeep Ghosh and Byron C. Wallace and Kush R. Varshney},
Title         = {Biomedical Interpretable Entity Representations},
Eprint        = {2106.09502v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pre-trained language models induce dense entity representations that offer
strong performance on entity-centric NLP tasks, but such representations are
not immediately interpretable. This can be a barrier to model uptake in
important domains such as biomedicine. There has been recent work on general
interpretable representation learning (Onoe and Durrett, 2020), but these
domain-agnostic representations do not readily transfer to the important domain
of biomedicine. In this paper, we create a new entity type system and training
set from a large corpus of biomedical texts by mapping entities to concepts in
a medical ontology, and from these to Wikipedia pages whose categories are our
types. From this mapping we derive Biomedical Interpretable Entity
Representations(BIERs), in which dimensions correspond to fine-grained entity
types, and values are predicted probabilities that a given entity is of the
corresponding type. We propose a novel method that exploits BIER's final sparse
and intermediate dense representations to facilitate model and entity type
debugging. We show that BIERs achieve strong performance in biomedical tasks
including named entity disambiguation and entity label classification, and we
provide error analysis to highlight the utility of their interpretability,
particularly in low-supervision settings. Finally, we provide our induced 68K
biomedical type system, the corresponding 37 million triples of derived data
used to train BIER models and our best performing model.},
Year          = {2021},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2106.09502v1},
File          = {2106.09502v1.pdf}
}
@article{2107.01294v3,
Author        = {Yao Dou and Maxwell Forbes and Rik Koncel-Kedziorski and Noah A. Smith and Yejin Choi},
Title         = {Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework
  for Scrutinizing Machine Text},
Eprint        = {2107.01294v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Modern neural language models can produce remarkably fluent and grammatical
text. So much, in fact, that recent work by Clark et al. (2021) has reported
that conventional crowdsourcing can no longer reliably distinguish between
machine-authored (GPT-3) and human-authored writing. As errors in machine
generations become ever subtler and harder to spot, it poses a new challenge to
the research community for robust machine text evaluation. We propose a new
framework called Scarecrow for scrutinizing machine text via crowd annotation.
To support the broad range of real machine errors that can be identified by
laypeople, the ten error categories of Scarecrow -- such as redundancy,
commonsense errors, and incoherence -- are identified through several rounds of
crowd annotation experiments without a predefined ontology. We then use
Scarecrow to collect over 41k error spans in human-written and
machine-generated paragraphs of English language news text. We isolate factors
for detailed analysis, including parameter count, training data, and various
decoding-time configurations. Our approach successfully quantifies measurable
gaps between human authored text and generations from models of several sizes,
including fourteen configurations of GPT-3. In addition, our analysis unveils
new insights, with detailed rationales provided by laypeople, e.g., that the
commonsense capabilities have been improving with larger models while math
capabilities have not, and that the choices of simple decoding hyperparameters
can make remarkable differences on the perceived quality of machine text. We
release our training material, annotation toolkit and dataset at
https://yao-dou.github.io/scarecrow/.},
Year          = {2021},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2107.01294v3},
File          = {2107.01294v3.pdf}
}
@article{2205.09058v2,
Author        = {Guangzhi Sun and Chao Zhang and Philip C Woodland},
Title         = {Minimising Biasing Word Errors for Contextual ASR with the
  Tree-Constrained Pointer Generator},
Eprint        = {2205.09058v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Contextual knowledge is essential for reducing speech recognition errors on
high-valued long-tail words. This paper proposes a novel tree-constrained
pointer generator (TCPGen) component that enables end-to-end ASR models to bias
towards a list of long-tail words obtained using external contextual
information. With only a small overhead in memory use and computation cost,
TCPGen can structure thousands of biasing words efficiently into a symbolic
prefix-tree and creates a neural shortcut between the tree and the final ASR
output to facilitate the recognition of the biasing words. To enhance TCPGen,
we further propose a novel minimum biasing word error (MBWE) loss that directly
optimises biasing word errors during training, along with a biasing-word-driven
language model discounting (BLMD) method during the test. All contextual ASR
systems were evaluated on the public Librispeech audiobook corpus and the data
from the dialogue state tracking challenges (DSTC) with the biasing lists
extracted from the dialogue-system ontology. Consistent word error rate (WER)
reductions were achieved with TCPGen, which were particularly significant on
the biasing words with around 40\% relative reductions in the recognition error
rates. MBWE and BLMD further improved the effectiveness of TCPGen and achieved
more significant WER reductions on the biasing words. TCPGen also achieved
zero-shot learning of words not in the audio training set with large WER
reductions on the out-of-vocabulary words in the biasing list.},
Year          = {2022},
Month         = {May},
Url           = {http://arxiv.org/abs/2205.09058v2},
File          = {2205.09058v2.pdf}
}
@article{2210.05257v2,
Author        = {Clément Lefebvre and Niklas Stoehr},
Title         = {Rethinking the Event Coding Pipeline with Prompt Entailment},
Eprint        = {2210.05257v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {For monitoring crises, political events are extracted from the news. The
large amount of unstructured full-text event descriptions makes a case-by-case
analysis unmanageable, particularly for low-resource humanitarian aid
organizations. This creates a demand to classify events into event types, a
task referred to as event coding. Typically, domain experts craft an event type
ontology, annotators label a large dataset and technical experts develop a
supervised coding system. In this work, we propose PR-ENT, a new event coding
approach that is more flexible and resource-efficient, while maintaining
competitive accuracy: first, we extend an event description such as "Military
injured two civilians'' by a template, e.g. "People were [Z]" and prompt a
pre-trained (cloze) language model to fill the slot Z. Second, we select answer
candidates Z* = {"injured'', "hurt"...} by treating the event description as
premise and the filled templates as hypothesis in a textual entailment task.
This allows domain experts to draft the codebook directly as labeled prompts
and interpretable answer candidates. This human-in-the-loop process is guided
by our interactive codebook design tool. We evaluate PR-ENT in several
robustness checks: perturbing the event description and prompt template,
restricting the vocabulary and removing contextual information.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.05257v2},
File          = {2210.05257v2.pdf}
}
@article{2210.06068v2,
Author        = {Lifeng Han and Gleb Erofeev and Irina Sorokina and Serge Gladkoff and Goran Nenadic},
Title         = {Investigating Massive Multilingual Pre-Trained Machine Translation
  Models for Clinical Domain via Transfer Learning},
Eprint        = {2210.06068v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Massively multilingual pre-trained language models (MMPLMs) are developed in
recent years demonstrating superpowers and the pre-knowledge they acquire for
downstream tasks. This work investigates whether MMPLMs can be applied to
clinical domain machine translation (MT) towards entirely unseen languages via
transfer learning. We carry out an experimental investigation using Meta-AI's
MMPLMs ``wmt21-dense-24-wide-en-X and X-en (WMT21fb)'' which were pre-trained
on 7 language pairs and 14 translation directions including English to Czech,
German, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite
direction. We fine-tune these MMPLMs towards English-\textit{Spanish} language
pair which \textit{did not exist at all} in their original pre-trained corpora
both implicitly and explicitly. We prepare carefully aligned \textit{clinical}
domain data for this fine-tuning, which is different from their original mixed
domain knowledge. Our experimental result shows that the fine-tuning is very
successful using just 250k well-aligned in-domain EN-ES segments for three
sub-task translation testings: clinical cases, clinical terms, and ontology
concepts. It achieves very close evaluation scores to another MMPLM NLLB from
Meta-AI, which included Spanish as a high-resource setting in the pre-training.
To the best of our knowledge, this is the first work on using MMPLMs towards
\textit{clinical domain transfer-learning NMT} successfully for totally unseen
languages during pre-training.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.06068v2},
File          = {2210.06068v2.pdf}
}
@article{2403.02586v1,
Author        = {Zefan Cai and Po-Nien Kung and Ashima Suvarna and Mingyu Derek Ma and Hritik Bansal and Baobao Chang and P. Jeffrey Brantingham and Wei Wang and Nanyun Peng},
Title         = {Improving Event Definition Following For Zero-Shot Event Detection},
Eprint        = {2403.02586v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Existing approaches on zero-shot event detection usually train models on
datasets annotated with known event types, and prompt them with unseen event
definitions. These approaches yield sporadic successes, yet generally fall
short of expectations. In this work, we aim to improve zero-shot event
detection by training models to better follow event definitions. We hypothesize
that a diverse set of event types and definitions are the key for models to
learn to follow event definitions while existing event extraction datasets
focus on annotating many high-quality examples for a few event types. To verify
our hypothesis, we construct an automatically generated Diverse Event
Definition (DivED) dataset and conduct comparative studies. Our experiments
reveal that a large number of event types (200) and diverse event definitions
can significantly boost event extraction performance; on the other hand, the
performance does not scale with over ten examples per event type. Beyond
scaling, we incorporate event ontology information and hard-negative samples
during training, further boosting the performance. Based on these findings, we
fine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that
surpasses SOTA large language models like GPT-3.5 across three open benchmarks
on zero-shot event detection.},
Year          = {2024},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2403.02586v1},
File          = {2403.02586v1.pdf}
}
@article{2409.04936v1,
Author        = {Amro M. Farid},
Title         = {A Hetero-functional Graph Resilience Analysis for Convergent
  Systems-of-Systems},
Eprint        = {2409.04936v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.SY},
Abstract      = {Our modern life has grown to depend on many and nearly ubiquitous large
complex engineering systems. Many disciplines now seemingly ask the same
question: ``In the face of assumed disruption, to what degree will these
systems continue to perform and when will they be able to bounce back to normal
operation"? Furthermore, there is a growing recognition that the greatest
societal challenges of the Anthropocene era are intertwined, necessitating a
convergent systems-of-systems modeling and analysis framework based upon
reconciled ontologies, data, and theoretical methods. Consequently, this paper
develops a methodology for hetero-functional graph resilience analysis and
demonstrates it on a convergent system-of-systems. It uses the Systems Modeling
Language, model-based systems engineering and Hetero-Functional Graph Theory
(HFGT) to overcome the convergence research challenges when constructing models
and measures from multiple disciplines for systems resilience. The paper
includes both the ``survival" as well as ``recovery" components of resilience.
It also strikes a middle ground between two disparate approaches to resilience
measurement: structural measurement of formal graphs and detailed behavioral
simulation. This paper also generalizes a previous resilience measure based on
HFGT and benefits from recent theoretical and computational developments in
HFGT. To demonstrate the methodological developments, the resilience analysis
is conducted on a hypothetical energy-water nexus system of moderate size as a
type of system-of-systems.},
Year          = {2024},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2409.04936v1},
File          = {2409.04936v1.pdf}
}
@article{2502.04397v1,
Author        = {Xiaorui Su and Shvat Messica and Yepeng Huang and Ruth Johnson and Lukas Fesser and Shanghua Gao and Faryad Sahneh and Marinka Zitnik},
Title         = {Multimodal Medical Code Tokenizer},
Eprint        = {2502.04397v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Foundation models trained on patient electronic health records (EHRs) require
tokenizing medical data into sequences of discrete vocabulary items. Existing
tokenizers treat medical codes from EHRs as isolated textual tokens. However,
each medical code is defined by its textual description, its position in
ontological hierarchies, and its relationships to other codes, such as disease
co-occurrences and drug-treatment associations. Medical vocabularies contain
more than 600,000 codes with critical information for clinical reasoning. We
introduce MedTok, a multimodal medical code tokenizer that uses the text
descriptions and relational context of codes. MedTok processes text using a
language model encoder and encodes the relational structure with a graph
encoder. It then quantizes both modalities into a unified token space,
preserving modality-specific and cross-modality information. We integrate
MedTok into five EHR models and evaluate it on operational and clinical tasks
across in-patient and out-patient datasets, including outcome prediction,
diagnosis classification, drug recommendation, and risk stratification.
Swapping standard EHR tokenizers with MedTok improves AUPRC across all EHR
models, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.30% on EHRShot, with
the largest gains in drug recommendation. Beyond EHR modeling, we demonstrate
using MedTok tokenizer with medical QA systems. Our results demonstrate the
potential of MedTok as a unified tokenizer for medical codes, improving
tokenization for medical foundation models.},
Year          = {2025},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2502.04397v1},
File          = {2502.04397v1.pdf}
}
@article{2209.07417v5,
Author        = {Lifeng Han and Gleb Erofeev and Irina Sorokina and Serge Gladkoff and Goran Nenadic},
Title         = {Examining Large Pre-Trained Language Models for Machine Translation:
  What You Don't Know About It},
Eprint        = {2209.07417v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Pre-trained language models (PLMs) often take advantage of the monolingual
and multilingual dataset that is freely available online to acquire general or
mixed domain knowledge before deployment into specific tasks. Extra-large PLMs
(xLPLMs) are proposed very recently to claim supreme performances over
smaller-sized PLMs such as in machine translation (MT) tasks. These xLPLMs
include Meta-AI's wmt21-dense-24-wide-en-X (2021) and NLLB (2022). In this
work, we examine if xLPLMs are absolutely superior to smaller-sized PLMs in
fine-tuning toward domain-specific MTs. We use two different in-domain data of
different sizes: commercial automotive in-house data and clinical shared task
data from the ClinSpEn2022 challenge at WMT2022. We choose popular Marian
Helsinki as smaller sized PLM and two massive-sized Mega-Transformers from
Meta-AI as xLPLMs.
  Our experimental investigation shows that 1) on smaller-sized in-domain
commercial automotive data, xLPLM wmt21-dense-24-wide-en-X indeed shows much
better evaluation scores using SacreBLEU and hLEPOR metrics than smaller-sized
Marian, even though its score increase rate is lower than Marian after
fine-tuning; 2) on relatively larger-size well prepared clinical data
fine-tuning, the xLPLM NLLB tends to lose its advantage over smaller-sized
Marian on two sub-tasks (clinical terms and ontology concepts) using ClinSpEn
offered metrics METEOR, COMET, and ROUGE-L, and totally lost to Marian on
Task-1 (clinical cases) on all official metrics including SacreBLEU and BLEU;
3) metrics do not always agree with each other on the same tasks using the same
model outputs; 4) clinic-Marian ranked No.2 on Task- 1 (via SACREBLEU/BLEU) and
Task-3 (via METEOR and ROUGE) among all submissions.},
Year          = {2022},
Month         = {Sep},
Url           = {http://arxiv.org/abs/2209.07417v5},
File          = {2209.07417v5.pdf}
}
@article{2404.14209v1,
Author        = {Po-Ting Lai and Elisabeth Coudert and Lucila Aimo and Kristian Axelsen and Lionel Breuza and Edouard de Castro and Marc Feuermann and Anne Morgat and Lucille Pourcel and Ivo Pedruzzi and Sylvain Poux and Nicole Redaschi and Catherine Rivoire and Anastasia Sveshnikova and Chih-Hsuan Wei and Robert Leaman and Ling Luo and Zhiyong Lu and Alan Bridge},
Title         = {EnzChemRED, a rich enzyme chemistry relation extraction dataset},
Eprint        = {2404.14209v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Expert curation is essential to capture knowledge of enzyme functions from
the scientific literature in FAIR open knowledgebases but cannot keep pace with
the rate of new discoveries and new publications. In this work we present
EnzChemRED, for Enzyme Chemistry Relation Extraction Dataset, a new training
and benchmarking dataset to support the development of Natural Language
Processing (NLP) methods such as (large) language models that can assist enzyme
curation. EnzChemRED consists of 1,210 expert curated PubMed abstracts in which
enzymes and the chemical reactions they catalyze are annotated using
identifiers from the UniProt Knowledgebase (UniProtKB) and the ontology of
Chemical Entities of Biological Interest (ChEBI). We show that fine-tuning
pre-trained language models with EnzChemRED can significantly boost their
ability to identify mentions of proteins and chemicals in text (Named Entity
Recognition, or NER) and to extract the chemical conversions in which they
participate (Relation Extraction, or RE), with average F1 score of 86.30% for
NER, 86.66% for RE for chemical conversion pairs, and 83.79% for RE for
chemical conversion pairs and linked enzymes. We combine the best performing
methods after fine-tuning using EnzChemRED to create an end-to-end pipeline for
knowledge extraction from text and apply this to abstracts at PubMed scale to
create a draft map of enzyme functions in literature to guide curation efforts
in UniProtKB and the reaction knowledgebase Rhea. The EnzChemRED corpus is
freely available at https://ftp.expasy.org/databases/rhea/nlp/.},
Year          = {2024},
Month         = {Apr},
Url           = {http://arxiv.org/abs/2404.14209v1},
File          = {2404.14209v1.pdf}
}
@article{2406.06608v5,
Author        = {Sander Schulhoff and Michael Ilie and Nishant Balepur and Konstantine Kahadze and Amanda Liu and Chenglei Si and Yinheng Li and Aayush Gupta and HyoJung Han and Sevien Schulhoff and Pranav Sandeep Dulepet and Saurav Vidyadhara and Dayeon Ki and Sweta Agrawal and Chau Pham and Gerson Kroiz and Feileen Li and Hudson Tao and Ashay Srivastava and Hevander Da Costa and Saloni Gupta and Megan L. Rogers and Inna Goncearenco and Giuseppe Sarli and Igor Galynker and Denis Peskoff and Marine Carpuat and Jules White and Shyamal Anadkat and Alexander Hoyle and Philip Resnik},
Title         = {The Prompt Report: A Systematic Survey of Prompting Techniques},
Eprint        = {2406.06608v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CL},
Abstract      = {Generative Artificial Intelligence (GenAI) systems are increasingly being
deployed across diverse industries and research domains. Developers and
end-users interact with these systems through the use of prompting and prompt
engineering. Although prompt engineering is a widely adopted and extensively
researched area, it suffers from conflicting terminology and a fragmented
ontological understanding of what constitutes an effective prompt due to its
relatively recent emergence. We establish a structured understanding of prompt
engineering by assembling a taxonomy of prompting techniques and analyzing
their applications. We present a detailed vocabulary of 33 vocabulary terms, a
taxonomy of 58 LLM prompting techniques, and 40 techniques for other
modalities. Additionally, we provide best practices and guidelines for prompt
engineering, including advice for prompting state-of-the-art (SOTA) LLMs such
as ChatGPT. We further present a meta-analysis of the entire literature on
natural language prefix-prompting. As a culmination of these efforts, this
paper presents the most comprehensive survey on prompt engineering to date.},
Year          = {2024},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2406.06608v5},
File          = {2406.06608v5.pdf}
}
@article{2212.13074v1,
Author        = {Abdul Quamar and Vasilis Efthymiou and Chuan Lei and Fatma Özcan},
Title         = {Natural Language Interfaces to Data},
Eprint        = {2212.13074v1},
DOI           = {10.1561/1900000078},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.DB},
Abstract      = {Recent advances in NLU and NLP have resulted in renewed interest in natural
language interfaces to data, which provide an easy mechanism for non-technical
users to access and query the data. While early systems evolved from keyword
search and focused on simple factual queries, the complexity of both the input
sentences as well as the generated SQL queries has evolved over time. More
recently, there has also been a lot of focus on using conversational interfaces
for data analytics, empowering a line of non-technical users with quick
insights into the data. There are three main challenges in natural language
querying (NLQ): (1) identifying the entities involved in the user utterance,
(2) connecting the different entities in a meaningful way over the underlying
data source to interpret user intents, and (3) generating a structured query in
the form of SQL or SPARQL.
  There are two main approaches for interpreting a user's NLQ. Rule-based
systems make use of semantic indices, ontologies, and KGs to identify the
entities in the query, understand the intended relationships between those
entities, and utilize grammars to generate the target queries. With the
advances in deep learning (DL)-based language models, there have been many
text-to-SQL approaches that try to interpret the query holistically using DL
models. Hybrid approaches that utilize both rule-based techniques as well as DL
models are also emerging by combining the strengths of both approaches.
Conversational interfaces are the next natural step to one-shot NLQ by
exploiting query context between multiple turns of conversation for
disambiguation. In this article, we review the background technologies that are
used in natural language interfaces, and survey the different approaches to
NLQ. We also describe conversational interfaces for data analytics and discuss
several benchmarks used for NLQ research and evaluation.},
Year          = {2022},
Month         = {Dec},
Note          = {Foundations and Trends in Databases 2022 Vol. 11: No. 4, pp
  319-414},
Url           = {http://arxiv.org/abs/2212.13074v1},
File          = {2212.13074v1.pdf}
}
@article{2311.13852v4,
Author        = {Sumit Dalal and Deepa Tilwani and Kaushik Roy and Manas Gaur and Sarika Jain and Valerie Shalin and Amit Sheth},
Title         = {A Cross Attention Approach to Diagnostic Explainability using Clinical
  Practice Guidelines for Depression},
Eprint        = {2311.13852v4},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.AI},
Abstract      = {The lack of explainability using relevant clinical knowledge hinders the
adoption of Artificial Intelligence-powered analysis of unstructured clinical
dialogue. A wealth of relevant, untapped Mental Health (MH) data is available
in online communities, providing the opportunity to address the explainability
problem with substantial potential impact as a screening tool for both online
and offline applications. We develop a method to enhance attention in popular
transformer models and generate clinician-understandable explanations for
classification by incorporating external clinical knowledge. Inspired by how
clinicians rely on their expertise when interacting with patients, we leverage
relevant clinical knowledge to model patient inputs, providing meaningful
explanations for classification. This will save manual review time and engender
trust. We develop such a system in the context of MH using clinical practice
guidelines (CPG) for diagnosing depression, a mental health disorder of global
concern. We propose an application-specific language model called ProcesS
knowledge-infused cross ATtention (PSAT), which incorporates CPGs when
computing attention. Through rigorous evaluation on three expert-curated
datasets related to depression, we demonstrate application-relevant
explainability of PSAT. PSAT also surpasses the performance of nine baseline
models and can provide explanations where other baselines fall short. We
transform a CPG resource focused on depression, such as the Patient Health
Questionnaire (e.g. PHQ-9) and related questions, into a machine-readable
ontology using SNOMED-CT. With this resource, PSAT enhances the ability of
models like GPT-3.5 to generate application-relevant explanations.},
Year          = {2023},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2311.13852v4},
File          = {2311.13852v4.pdf}
}

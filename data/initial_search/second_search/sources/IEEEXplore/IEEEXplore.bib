@INPROCEEDINGS{10386611,
  author={Liu, Jiehui and Zhan, Jieyu},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={Constructing Knowledge Graph from Cyber Threat Intelligence Using Large Language Model}, 
  year={2023},
  volume={},
  number={},
  pages={516-521},
  abstract={Cyber Threat Intelligence (CTI) reports are valuable resources in various applications but manually extracting information from them is time-consuming. Existing approaches for automating extraction require specialized models trained on a substantial corpus. In this paper, we present an efficient methodology for constructing knowledge graphs from CTI by leveraging the Large Language Model (LLM), using ChatGPT for instance. Our approach automatically extracts attack-related entities and their relationships, organizing them within a CTI knowledge graph. We evaluate our approach on 13 CTIs, demonstrating better performance compared to AttacKG and REBEL while requiring less manual intervention and computational resources. This proves the feasibility and suitability of our method in low-resource scenarios, specifically within the domain of cyber threat intelligence.},
  keywords={Computational modeling;Knowledge graphs;Manuals;Ontologies;Information retrieval;Data models;Cognition;knowledge graph;threat intelligence;large language model;ChatGPT},
  doi={10.1109/BigData59044.2023.10386611},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10475639,
  author={Vizcarra, Julio and Haruta, Shuichiro and Kurokawa, Mori},
  booktitle={2024 IEEE 18th International Conference on Semantic Computing (ICSC)}, 
  title={Representing the Interaction between Users and Products via LLM-assisted Knowledge Graph Construction}, 
  year={2024},
  volume={},
  number={},
  pages={231-232},
  abstract={To understand user behavior, representing the semantic knowledge of user-product interaction is essential. In this paper, we represent the interaction between user and product via large language model (LLM)-assisted knowledge graph construction. We capture users’ behavioral actions and static properties of the products from raw text data of “user review” and “product catalog”. Moreover, the information needed for updating the knowledge graph is captured by raw texts of “news related to the products”. The proposed methodology integrates them as a single knowledge graph to provide causal reasoning on user-product interaction. To alleviate the situation where a small quantity of annotated text exists in these data, we use LLM as a data annotator and augmentor.},
  keywords={Text mining;Reviews;Annotations;Semantics;Knowledge graphs;Data augmentation;Cognition;Knowledge graph;text mining;ontology;causality;LLM;user-product interaction},
  doi={10.1109/ICSC59802.2024.00043},
  ISSN={2472-9671},
  month={Feb},}@INPROCEEDINGS{10588309,
  author={Peng, Tao and Rao, Taiwen and Xu, Yansong and Yang, Chao and Xie, Xiaotian and Yang, Chunhua},
  booktitle={2024 36th Chinese Control and Decision Conference (CCDC)}, 
  title={Construction of Rail Transit Network Fault Knowledge Graph Based on Pseudo-Dynamic Relationship Ontology Architecture}, 
  year={2024},
  volume={},
  number={},
  pages={4582-4587},
  abstract={In this paper, an approach to construction of rail transit network fault knowledge graph based on pseudo-dynamic relationship ontology architecture is proposed. Firstly, an ontology architecture that includes pseudo-dynamic relationships is employed to completely separate fault patterns from fault entities without losing semantics. Secondly, a named entity recognition method based on the BERT-BiGRU-CRF model is adopted, which performs well in short entity extraction tasks. Thirdly, a pseudo-dynamic relationship extraction method based on the BERT-MEA(multi entities attention) model is adopted to extract static simple relationships, followed by treating triples as head and tail entities to determine pseudo-dynamic relationships. The implementation of rail transit network fault knowledge graph demonstrates the effectiveness of the proposed approach.},
  keywords={Rails;Databases;Semantics;Knowledge graphs;Tail;Named entity recognition;Ontologies;Ontology architecture;Pseudo-dynamic relationship;Short entity;Fault knowledge graph},
  doi={10.1109/CCDC62350.2024.10588309},
  ISSN={1948-9447},
  month={May},}@INPROCEEDINGS{10710789,
  author={Schoch, Nicolai and Hoernicke, Mario and Strem, Nika and Stark, Katharina},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Engineering Data Funnel (WIP) – An Ontology-Enhanced LLM-Based Agent and MoE System for Engineering Data Processing}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Automation Engineering of a process automation system is still a very manual effort due to limited support for the interpretation and processing of process design specification documents. Even though standards for digital data exchange between process and automation engineering do exist, those formats are rarely used and consequently the immense automation potential in automation engineering cannot be lifted. This contribution presents an AI -based approach and prototype - using an ontology-enhanced LLM -based agent and a mixture-of-experts system - to structure and formalize multimodal unstructured process design information as in PDF, Excel, and Word formats and make it available for state-of-the-art engineering tools for the long-known “Automation of Automation”.},
  keywords={Process design;Automation;Prototypes;Manuals;Portable document format;Data processing;Artificial intelligence;Standards;Manufacturing automation;engineering design specification;engineering data processing;LLM-based agent;mixture of experts;ontology-driven information processing;automation of automation},
  doi={10.1109/ETFA61755.2024.10710789},
  ISSN={1946-0759},
  month={Sep.},}@ARTICLE{10669737,
  author={Li, Yihao and Zhang, Ru and Liu, Jianyi and Lei, Qi},
  journal={IEEE Signal Processing Letters}, 
  title={A Semantic Controllable Long Text Steganography Framework Based on LLM Prompt Engineering and Knowledge Graph}, 
  year={2024},
  volume={31},
  number={},
  pages={2610-2614},
  abstract={With ongoing advancements in natural language technology, text steganography has achieved notable progress. However, existing methods primarily concentrate on the probability distribution between words, often overlooking comprehensive control over text semantics. Particularly in the case of longer texts, these methods struggle to preserve coherence and contextual consistency, thereby increasing the risk of detection in practical applications. To effectively improve steganography security, we propose a semantic controllable long-text steganography framework based on prompt engineering and knowledge graph (KG) integration, obviating supplementary training. This framework leverages triplets from the KG and task descriptions to construct prompts, directing the large language model (LLM) to generate text that aligns with the triplet content. Subsequently, the model effectively embeds secret information by encoding the candidate pools established around the sampled target words. The experimental results demonstrate that our framework ensures the concealment of steganographic text while maintaining the relevance and consistency of the content as expected. Moreover, it can be flexibly adapted to various application scenarios, showcasing its potential and advantages in practical implementations.},
  keywords={Steganography;Encoding;Semantics;Mathematical models;Training;Prompt engineering;Probability distribution;Text steganography;semantic controllable;LLM prompt engineering;knowledge graph},
  doi={10.1109/LSP.2024.3456636},
  ISSN={1558-2361},
  month={},}@INPROCEEDINGS{10544941,
  author={Dequan, Gao and Pengyu, Zhu and Sheng, Wang and Ziyan, Zhao},
  booktitle={2024 6th Asia Energy and Electrical Engineering Symposium (AEEES)}, 
  title={Deep Learning-Based Fault Knowledge Graph Construction for Power Communication Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1088-1093},
  abstract={Power communication network is a crucial infrastructure in the model power system, and its maintenance capability are crucial to ensuring the stable operation of power grid business. As an organized semantic knowledge base, the knowledge graph effectively organizes power communication network fault documentation and expert experience to enhance intelligent maintenance. This paper outlines a top-down approach to systematically construct a fault knowledge graph in the domain of power communication networks. The approach utilizes a seven-step method to establish a domain ontology model and integrates deep learning algorithms, including pre-trained language models, bidirectional long short time memory networks, convolutional neural networks and attention mechanisms. These algorithms process unstructured text to extract key entities and relationships. The effectiveness of the approach is verified through experiments using a product device document as a test case. Extracted knowledge is then visualized and stored using Neo4j database. Finally, this paper proposes a knowledge service model centered on fault knowledge graph and explores its application in fault diagnosis.},
  keywords={Deep learning;Patents;Semantics;Knowledge graphs;Ontologies;Real-time systems;Power grids;power communication networks;fault knowledge graph;deep learning;fault diagnosis},
  doi={10.1109/AEEES61147.2024.10544941},
  ISSN={},
  month={March},}@ARTICLE{9540703,
  author={Choi, Bonggeun and Jang, Daesik and Ko, Youngjoong},
  journal={IEEE Access}, 
  title={MEM-KGC: Masked Entity Model for Knowledge Graph Completion With Pre-Trained Language Model}, 
  year={2021},
  volume={9},
  number={},
  pages={132025-132032},
  abstract={The knowledge graph completion (KGC) task aims to predict missing links in knowledge graphs. Recently, several KGC models based on translational distance or semantic matching methods have been proposed and have achieved meaningful results. However, existing models have a significant shortcoming–they cannot train entity embedding when an entity does not appear in the training phase. As a result, such models use randomly initialized embeddings for entities that are unseen in the training phase and cause a critical decrease in performance during the test phase. To solve this problem, we propose a new approach that performs KGC task by utilizing the masked language model (MLM) that is used for a pre-trained language model. Given a triple (head entity, relation, tail entity), we mask the tail entity and consider the head entity and the relation as a context for the tail entity. The model then predicts the masked entity from among all entities. Then, the task is conducted by the same process as an MLM, which predicts a masked token with a given context of tokens. Our experimental results show that the proposed model achieves significantly improved performances when unseen entities appear during the test phase and achieves state-of-the-art performance on the WN18RR dataset.},
  keywords={Task analysis;Predictive models;Training;Bit error rate;Semantics;Micromechanical devices;Knowledge graph completion;link prediction;masked language model;pre-trained language model},
  doi={10.1109/ACCESS.2021.3113329},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10412761,
  author={Vijayakumar, Senthilkumar and Louis, Filious},
  booktitle={2023 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={Revolutionizing Staffing and Recruiting with Contextual Knowledge Graphs and QNLP: An End-to-End Quantum Training Paradigm}, 
  year={2023},
  volume={},
  number={},
  pages={45-51},
  abstract={The staffing and recruiting industry is continuously evolving, and recent advancements in Knowledge Graphs (KG) and Quantum Natural Language Processing (QNLP) has garnered considerable attention. The integration of these state-of-the-art technologies is fueled by the necessity to improve language models' capacity to comprehend context and make precise decisions. This research paper presents a novel approach to revolutionize the staffing and recruiting industry by integrating Knowledge Graph (KG) and Quantum Natural Language Processing (QNLP) to formulate an end-to-end QNLP training pipeline. The proposed solution consists of three interdependent subsystems that work in unison to construct contextual KG and train language models. The Information Extraction subsystem extracts semantic relationships and connections between entities from large and complex recruitment data to construct domain specific contextual KG. The QNLP model training pipeline subsystem, which is fed with domain-rich KG data, runs on Quantum Circuits, accelerates the training process by effectively incorporating high-dimensional features to the deep layers of language models. Finally, the Information Retrieval subsystem is based on semantic data taxonomy, retrieving contextual data from the KG for the trained language models to be implemented on various distinctive use cases in the staffing and recruiting industry. This solution provides a faster and more contextual approach to analyze recruitment data, empowering recruiters to concentrate on strategic tasks such as candidate engagement and client relationship building, ultimately leading to better business decision-making capabilities.},
  keywords={Training;Industries;Knowledge graphs;Natural language processing;Data models;Integrated circuit modeling;Context modeling;Artificial Intelligence (AI);Knowledge Graph (KG);Quantum Natural Language Processing (QNLP);Large Language Models (LLM);Contextual Information Extraction & Retrieval Systems},
  doi={10.1109/ICKG59574.2023.00011},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10695412,
  author={Luo, Junjun and Zhu, Zhongyan and Zhu, Haijiang and Dong, Xiaohui},
  booktitle={2024 7th International Conference on Computer Information Science and Application Technology (CISAT)}, 
  title={Research on knowledge graph construction method for mine hoist fault field}, 
  year={2024},
  volume={},
  number={},
  pages={342-346},
  abstract={Mine hoists are integral to mining hoisting systems, with their safe and reliable operation being critical for ensuring the safety of mining operations. The consequences of hoist failure are severe, particularly when the root cause of the malfunction is not promptly identified and addressed, potentially compromising the overall safety of mining activities. The complexity of mine hoist systems stems from the interdependent and restrictive relationships among their components, each of which generates unique operational state information. This information, when aggregated and processed, can be distilled into various fault characteristic parameters. This paper introduces a novel approach to fault diagnosis within mine hoist systems by constructing a fault knowledge graph based on ontological principles. The proposed method harnesses the power of knowledge graphs to systematically represent and analyze the complex interplay of components within the hoist system. By doing so, it enhances the diagnostic capabilities and the preemptive identification of potential faults. The research focuses on the mine hoist as the subject of study and proposes the development of an ontologically-based fault knowledge graph. This approach is not only of significant importance to the coal mining industry but also offers innovative insights for knowledge graph construction across various domains. The implications of this study extend beyond the mining sector, providing a foundation for more robust and intelligent fault diagnosis systems in complex mechanical systems.},
  keywords={Fault diagnosis;Information science;Instruments;Knowledge graphs;Named entity recognition;Information retrieval;Safety;Complexity theory;Mechanical systems;Lifting equipment;artificial intelligence;entity recognition;fault knowledge graph;mine hoist;relation extraction},
  doi={10.1109/CISAT62382.2024.10695412},
  ISSN={},
  month={July},}@INPROCEEDINGS{10611970,
  author={Lee, Chang-Shing and Wang, Mei-Hui and Chiang, Jun-Kui and Kubota, Naoyuki and Sato-Shimokawara, Eri and Nojima, Yusuke and Acampora, Giovanni and Wu, Pei-Yu and Chiu, Szu-Chi and Yang, Sheng-Chi and Siow, Chyan-Zheng},
  booktitle={2024 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
  title={Quantum Computational Intelligence with Generative AI Image for Human-Machine Interaction}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper introduces a Quantum Computational Intelligence (QCI) agent equipped with a content attention ontology model, specifically designed to enhance human-machine interaction based on a Generative Artificial Intelligence (GAI) image generation agent for Taiwanese/English learning and experience. Its diverse primary applications include social media analysis on Facebook groups and YouTube learning videos related to the 2023 IEEE CIS Education Portal (EP) Subcommittee, as well as in the areas of Taiwanese/English language learning and dialogue experience with GAI image generation. To establish the knowledge and inference models for the QCI agent, we initially developed a Taiwanese/English learning and experience ontology, including a content attention ontology, and an image attention ontology. The QCI agent utilizes metrics such as the number of views, posts, and comments to predict the fuzzy number of reactions. In addition, the GAI image agent generates Taiwanese speech-based/English text-based images and evaluates the fuzzy similarity score between Taiwanese/English and the attention ontology together with the Sentence BERT (SBERT) agent. This Taiwanese/English fuzzy similarity score is further validated through human assessments, with these evaluations subsequently serving as an additional metric for comparative analysis of Human-Machine Interaction (HMI). Furthermore, the GAI image agent is designed to create images and Chinese/English texts from text/speech translated by the Meta AI Universal Speech Translator (UST) Taiwanese/English agent. A Particle Swarm Optimization (PSO)-based machine learning mechanism is employed to train the QCI model for assessing learners' performance and predicting the performance of others. The National University of Tainan (NUTN) Taiwan-Large Language Model (NUTN.TW-LLM) agent has been further enhanced to support interactive learning experiences for HMI. An SBERT-based assessment agent is used to calculate fuzzy similarities between questions and answers in Taiwanese/English experiences and dialogues. Experimental results demonstrate the feasibility and efficacy of the proposed QCI model, equipped with QCI&AI-FML (Artificial Intelligence-Fuzzy Markup Language) and machine learning capabilities, for social media and language learning applications on HMI. In the future, we will extend the QCI model to various HMI applications for student learning around the world.},
  keywords={Human computer interaction;Measurement;Quantum computing;Social networking (online);Image synthesis;Generative AI;Computational modeling;Quantum CI Agent;Content Attention Ontology;ChatGPT;Generative AI Image Agent;IEEE CIS Education Portal;Fuzzy Markup Language;Sentence BERT;NUTN.TW-LLM},
  doi={10.1109/FUZZ-IEEE60900.2024.10611970},
  ISSN={1558-4739},
  month={June},}@INPROCEEDINGS{10863933,
  author={Meng, Qi and Wu, Zhenglong and Zhao, Zhongshi and Lian, Xi'nan},
  booktitle={2024 5th International Conference on Artificial Intelligence and Computer Engineering (ICAICE)}, 
  title={Research of Knowledge-Enhanced Large Language Model Based on Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={693-697},
  abstract={The rapid development and wide application of Knowledge Graph(KG) and Large Language Model(LLM) have demonstrated strong basic capabilities of artificial intelligence. In the last two years, related research has explored the combination of KG and LLM to improve the model's ability in text understanding and inference. In view of this kind of research, the basic knowledge of KG enhancing LLM is systematically introduced, and how KG enhances LLM from the perspective of functional realization is analyzed, and the existing research results of KG enhancing LLM is also summarized, which provides reference for further research on LLM enhanced by KG.},
  keywords={Training;Inference mechanisms;Large language models;Computational modeling;Semantics;Knowledge graphs;Natural language processing;Inference algorithms;Graph neural networks;Hardware acceleration;knowledge graph;large language model;knowledge enhancement;prospect of research},
  doi={10.1109/ICAICE63571.2024.10863933},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10822222,
  author={Yang, Hang and Xiao, Liang and Zhu, Rujun and Liu, Ziji and Chen, Jianxia},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={An LLM supported approach to ontology and knowledge graph construction}, 
  year={2024},
  volume={},
  number={},
  pages={5240-5246},
  abstract={The continuous development in the medical field faces multiple challenges in managing a large amount of literature and research results using traditional ontology and knowledge graph construction methods. These challenges include high labor costs, limited coverage, and poor dynamism of traditional ontology and knowledge graph construction methods. Large language models (LLMs) can solve various natural language processing tasks and can understand and generate human-like natural language, which makes automated construction of ontology expansion and knowledge graphs (KGs) possible. This paper proposes an ontology expansion method based on LLMs, using LLMs to formulate competency questions (CQs) to extend the initial ontology, and then constructing the knowledge graph based on the extended ontology. We demonstrated the feasibility of the method by creating a knowledge graph for breast cancer treatment. The combination of LLMs-based medical ontology and knowledge graph can achieve more efficient medical knowledge management and application, promoting the informatization and intelligent development of the medical field.},
  keywords={Semantic Web;Large language models;Refining;Knowledge graphs;Medical services;Ontologies;Natural language processing;Iterative methods;Reliability;Usability;Ontology;Knowledge graph;LLM;Breast cancer treatment},
  doi={10.1109/BIBM62325.2024.10822222},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{9995517,
  author={Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Adaptive Multi-view Graph Convolutional Network for Gene Ontology Annotations of Proteins}, 
  year={2022},
  volume={},
  number={},
  pages={90-93},
  abstract={Gene Ontology (GO) containing a set of standard concepts (or terms) is launched to unify the functional descriptions of proteins. Developing computational models based on GO to automatically annotate protein functions has been a longstanding active research area. In this paper, we propose a novel method to adaptively fuse functional and topological information between GO Terms. Our method is composed of a pre-trained language model for encoding protein sequences and an adaptive multi-view graph convolutional network (Multi-view GCN) for representing GO terms. Particularly, the Multi-view GCN considers multiple views from functional information, topological structures, and their combinations, and extracts multiple corresponding representations of GO terms. Then, an attention mechanism is applied to adaptively learn the importance weights of these representations. Finally, the predicted scores are calculated by using a dot product between protein sequence features and GO term representations. Experimental results on the datasets of two species (i.e., Human and Yeast) show that our method outperforms other state-of-the-art methods. The code of our proposed method is available at: https://github.com/Candyperfect/Master.},
  keywords={Convolutional codes;Adaptation models;Adaptive systems;Computational modeling;RNA;Ontologies;Feature extraction;gene ontology terms;protein function prediction;deep learning;adaptive multi-view graph convolutional network},
  doi={10.1109/BIBM55620.2022.9995517},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10827945,
  author={Chao, Gao and Xiaoyuan, Liu and Dongfang, Zhang},
  booktitle={2024 IEEE 6th International Conference on Civil Aviation Safety and Information Technology (ICCASIT)}, 
  title={The integration of large language model and knowledge graph and its application in flight safety}, 
  year={2024},
  volume={},
  number={},
  pages={491-494},
  abstract={This article introduces the advantages and disadvantages of large language model and knowledge graph. Based on the concept of mutual integration, it summarizes the research on the application of large language model in knowledge graph construction and knowledge graph supporting the application of large model. Finally, it puts forward the application idea of the integration of large language model and knowledge graph in flight safety.},
  keywords={Deep learning;Data analysis;Large language models;Safety management;Knowledge graphs;Safety;Information technology;large language model;knowledge graph;integration;flight safety},
  doi={10.1109/ICCASIT62299.2024.10827945},
  ISSN={},
  month={Oct},}@ARTICLE{10129977,
  author={Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Yang, Yumeng and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
  journal={IEEE Transactions on NanoBioscience}, 
  title={Protein Function Prediction With Functional and Topological Knowledge of Gene Ontology}, 
  year={2023},
  volume={22},
  number={4},
  pages={755-762},
  abstract={Gene Ontology (GO) is a widely used bioinformatics resource for describing biological processes, molecular functions, and cellular components of proteins. It covers more than 5000 terms hierarchically organized into a directed acyclic graph and known functional annotations. Automatically annotating protein functions by using GO-based computational models has been an area of active research for a long time. However, due to the limited functional annotation information and complex topological structures of GO, existing models cannot effectively capture the knowledge representation of GO. To solve this issue, we present a method that fuses the functional and topological knowledge of GO to guide protein function prediction. This method employs a multi-view GCN model to extract a variety of GO representations from functional information, topological structure, and their combinations. To dynamically learn the significance weights of these representations, it adopts an attention mechanism to learn the final knowledge representation of GO. Furthermore, it uses a pre-trained language model (i.e., ESM-1b) to efficiently learn biological features for each protein sequence. Finally, it obtains all predicted scores by calculating the dot product of sequence features and GO representation. Our method outperforms other state-of-the-art methods, as demonstrated by the experimental results on datasets from three different species, namely Yeast, Human and Arabidopsis. Our proposed method’s code can be accessed at: https://github.com/Candyperfect/Master.},
  keywords={Proteins;Feature extraction;Amino acids;Annotations;Predictive models;Biological system modeling;Protein sequence;Convolutional neural networks;Graph neural networks;Protein function prediction;gene ontology;multi-view GCN;pre-trained language model},
  doi={10.1109/TNB.2023.3278033},
  ISSN={1558-2639},
  month={Oct},}@INPROCEEDINGS{10726129,
  author={Vidhate, Hutesh and Khobragade, Anish},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={GnBERT: Graph Neural Networks over BERT for Knowledge Graph Representation Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Many disciplines depend on knowledge graphs to represent and arrange organized data. Nevertheless, understanding knowledge graphs is still quite difficult because of their intricate relationships and great importance. The work develops a new framework named GnBERT by fusing Bidirectional Encoder Representations from Transformers (BERT) with Graph Neural Networks (GNNs). Knowledge graph representations are able to be obtained via this approach. Graph Neural Networks (GNNs) provide relational semantics; GnBERT improves the representation of entities and relationships in knowledge graphs by combining the hierarchical structure of BERT’s pre-trained language model with it. We have shown, using benchmark datasets, that GnBERT achieves the best degree of performance in knowledge graph completion, entity linking, and connection extraction over earlier techniques.},
  keywords={Representation learning;Semantics;Knowledge graphs;Bidirectional control;Benchmark testing;Transformers;Graph neural networks;Encoding;GnBERT;Graph Neural Network;BERT;Knowledge Graph;Representation Learning},
  doi={10.1109/ICCCNT61001.2024.10726129},
  ISSN={2473-7674},
  month={June},}@ARTICLE{10475356,
  author={Zhu, Huashi and Xu, Dexuan and Huang, Yu and Jin, Zhi and Ding, Weiping and Tong, Jiahui and Chong, Guoshuang},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Graph Structure Enhanced Pre-Training Language Model for Knowledge Graph Completion}, 
  year={2024},
  volume={8},
  number={4},
  pages={2697-2708},
  abstract={A vast amount of textual and structural information is required for knowledge graph construction and its downstream tasks. However, most of the current knowledge graphs are incomplete due to the difficulty of knowledge acquisition and integration. Knowledge Graph Completion (KGC) is used to predict missing connections. In previous studies, textual information and graph structural information are utilized independently, without an effective method for fusing these two types of information. In this paper, we propose a graph structure enhanced pre-training language model for knowledge graph completion. Firstly, we design a graph sampling algorithm and a Graph2Seq module for constructing sub-graphs and their corresponding contexts to support large-scale knowledge graph learning and parallel training. It is also the basis for fusing textual data and graph structure. Next, two pre-training tasks based on masked modeling are designed for capturing accurate entity-level and relation-level information. Furthermore, this paper proposes a novel asymmetric Encoder-Decoder architecture to restore masked components, where the encoder is a Pre-trained Language Model (PLM) and the decoder is a multi-relational Graph Neural Network (GNN). The purpose of the architecture is to integrate textual information effectively with graph structural information. Finally, the model is fine-tuned for KGC tasks on two widely used public datasets. The experiments show that the model achieves excellent performance and outperforms baselines in most metrics, which demonstrate the effectiveness of our approach by fusing the structure and semantic information to knowledge graph.},
  keywords={Knowledge graphs;Semantics;Predictive models;Micromechanical devices;Natural language processing;Graph neural networks;Knowledge graph completion;masked modeling;pre-trained language models;graph neural network},
  doi={10.1109/TETCI.2024.3372442},
  ISSN={2471-285X},
  month={Aug},}@INPROCEEDINGS{10512525,
  author={Zhao, Honda and Jiang, Wei and Deng, Jiewen and Ren, Qinghua and Zhang, Li},
  booktitle={2023 IEEE 7th Conference on Energy Internet and Energy System Integration (EI2)}, 
  title={Constructing Knowledge Graph for Electricity Keywords Based on Large Language Model}, 
  year={2023},
  volume={},
  number={},
  pages={4844-4849},
  abstract={In the information age, the electric power industry, as a crucial pillar of modern society, has accumulated a wealth of valuable research literature. Knowledge graph technology offers the potential to tap into this knowledge repository, providing a better understanding of research outcomes in the electric power domain. However, due to the diversity and complexity of knowledge in the power industry, it is difficult to build a comprehensive and complete knowledge graph of power keywords. In recent years, large language models (LLMs) have made significant advancements. This paper harnesses LLM technology along with text similarity analysis and co-occurrence frequency analysis to establish a comprehensive framework for processing keyword knowledge in the field of electric power. Within this framework, various forms of information found in electric power research can be processed. This includes creating a thesaurus of electric power domain keywords; obtaining the individual attributes implied by the information keywords in this thesaurus and their interconnections; and generating a knowledge graph of electric power domain keywords. This knowledge graph includes attributes, interpretations, relationships, and associated literature for keywords. It serves as a valuable reference for the effective utilization of research outcomes in the electric power domain.},
  keywords={Knowledge engineering;Analytical models;Semantics;Knowledge based systems;Knowledge graphs;System integration;Power industry;LLM;knowledge graph;ChatGLM;similarity analysis;co-occurrence},
  doi={10.1109/EI259745.2023.10512525},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9863051,
  author={Ataei, Sima and Butler, Gregory},
  booktitle={2022 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)}, 
  title={Predicting the specific substrate for transmembrane transport proteins using BERT language model}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Transmembrane transport proteins play a vital role in cells' metabolism by the selective passage of substrates through the cell membrane. Metabolic network reconstruction requires transport reactions that describe the specific substrate transported as well as the metabolic reactions of enzyme catalysis. In this paper, we apply BERT (Bidirectional Encoder Representations from Transformers) language model for protein sequences to predict one of 12 specific substrates. Our UniProt-ICAT-100 dataset is automatically constructed from UniProt using the ChEBI and GO ontologies to identify 4,112 proteins transporting 12 inorganic anion or cation substrates. We classified this dataset using three different models including Logistic Regression with an MCC of 0.81 and accuracy of 97.5%; Feed-forward Neural Networks classifier with an MCC of 0.88 and accuracy of 98.5%. Our third model utilizes a Fine-tuned BERT language model to predict the specific substrate with an MCC of 0.95 and accuracy of 99.3% on an independent test set.},
  keywords={Proteins;Biological system modeling;Computational modeling;Bit error rate;Neural networks;Cells (biology);Predictive models;Classification;BERT model;Transport protein;Specific substrate Prediction;ChEBI ontology;Gene Ontology},
  doi={10.1109/CIBCB55180.2022.9863051},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10522716,
  author={Mateiu, Patricia and Groza, Adrian},
  booktitle={2023 25th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)}, 
  title={Ontology engineering with Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={226-229},
  abstract={We tackle the task of enriching ontologies by automatically translating natural language (NL) into Description Logic (DL). Since Large Language Models (LLMs) are the best tools for translations, we fine-tuned a GPT-3 model to convert NL into OWL Functional Syntax. For fine-tuning, we designed pairs of sentences in NL and the corresponding translations. This training pairs cover various aspects from ontology engineering: instances, class subsumption, domain and range of relations, object properties relationships, disjoint classes, complements, or cardinality restrictions. The resulted axioms are used to enrich an ontology, in a human supervised manner. The developed tool is publicly provided as a Protégé plugin.},
  keywords={Training;Scientific computing;Description logic;OWL;Natural languages;Syntactics;Task analysis;ontology engineering;large language models;Protege plugin;fine-tuning},
  doi={10.1109/SYNASC61333.2023.00038},
  ISSN={2470-881X},
  month={Sep.},}@INPROCEEDINGS{10446860,
  author={Luo, Zhizhao and Wang, Youchen and Ke, Wenjun and Qi, Rui and Guo, Yikai and Wang, Peng},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Boosting LLMS with Ontology-Aware Prompt for Ner Data Augmentation}, 
  year={2024},
  volume={},
  number={},
  pages={12361-12365},
  abstract={Named Entity Recognition (NER) data augmentation (DA) aims to improve the performance and generalization capabilities of NER models by generating scalable training data. The key challenge lies in ensuring the generated samples maintain contextual diversity while preserving label consistency. However, existing dominant methods fail to simultaneously satisfy both criteria. Inspired by the extensive generative capabilities of large language models (LLMs), we propose ANGEL, a frAmework integrating the oNtoloGy structure and instructivE prompting within LLMs. Specifically, the hierarchical ontology structure guides prompt ranking, while instructive prompting enhances LLMs’ mastery of domain knowledge, empowering synthetic sample generation and annotation. Experiments show ANGEL surpasses state-of-the-art (SOTA) baselines, conferring absolute F1 increases of 2.86% and 0.93% on two benchmark datasets, respectively.},
  keywords={Training data;Speech recognition;Ontologies;Syntactics;Signal processing;Data augmentation;Boosting;Named Entity Recognition;Data Augmentation;Large language Model;Knowledge Graph},
  doi={10.1109/ICASSP48485.2024.10446860},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{9231227,
  author={Kumar, Abhijeet and Pandey, Abhishek and Gadia, Rohit and Mishra, Mridul},
  booktitle={2020 IEEE International Conference on Computing, Power and Communication Technologies (GUCON)}, 
  title={Building Knowledge Graph using Pre-trained Language Model for Learning Entity-aware Relationships}, 
  year={2020},
  volume={},
  number={},
  pages={310-315},
  abstract={Relations exhibited among entities from textual content can be a potential source of information for any business domain. This paper encompasses a wholesome approach to mine entity-relation and building knowledge graph from textual documents. The paper concentrates on two approaches to classify directional entity relations. We build on extending pretrained language model i.e. BERT for text classification along-side providing entity and directionality information as input making it entity-aware BERT classifier. We also did ablation studies of presented model in terms of various ways of providing entity information on the learning capabilities of model. We demonstrate the end to end pipeline for building an entity-relation extraction system in a business application. The techniques proposed in the paper are also evaluated against SemEval-2010 Task 8, a popular relation classification dataset. The experimental results demonstrate that learning entity-aware relations through language models outperforms almost all the previous state-of-the-art (SOTA) models.},
  keywords={Databases;Computational modeling;Conferences;Bit error rate;Text categorization;Pipelines;Buildings;knowledge graph;entity relations;relationship extraction;deep learning;BERT;language models;graph database},
  doi={10.1109/GUCON48875.2020.9231227},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10291525,
  author={Zhang, Junfeng and Zhang, Yang and Chu, Minnan and Yang, Shun and Zu, Taolei},
  booktitle={2023 IEEE 7th Information Technology and Mechatronics Engineering Conference (ITOEC)}, 
  title={A LLM-Based Simulation Scenario Aided Generation Method}, 
  year={2023},
  volume={7},
  number={},
  pages={1350-1354},
  abstract={In the simulation training system, the generation of simulation scenarios is a basic problem that needs to be studied. Firstly, expounds on the technical characteristics of LLM and knowledge graph; then structurally describe the simulation scenario related content, and build scenario knowledge graph; according to the characteristics of scenario aided generation, a simulation scenario generation method based on LLM is proposed, which uses prompt to fuse knowledge graph and LLM, next, the implementation steps of this method were elaborated; finally, the specific application proves that the method proposed in this paper is a good reference for the generation of simulation scenarios.},
  keywords={Training;Knowledge engineering;Bridges;Mechatronics;Fuses;Knowledge graphs;Information technology;knowledge graph;scenario;LLM;prompt},
  doi={10.1109/ITOEC57671.2023.10291525},
  ISSN={2693-289X},
  month={Sep.},}@INPROCEEDINGS{10490589,
  author={Rao, Qiang and Wang, Tiejun},
  booktitle={2023 7th International Conference on Electrical, Mechanical and Computer Engineering (ICEMCE)}, 
  title={Semantic Enhancement Based Knowledge Graph Completion for Graph Convolutional Neural Networks}, 
  year={2023},
  volume={},
  number={},
  pages={923-927},
  abstract={Knowledge Graph Completion (KGC) is a task that aims to predict missing links in a knowledge graph based on known triples. Recent studies have demonstrated outstanding performance in KGC employing models grounded on Graph Convolutional Networks (GCN). Nevertheless, prevailing GCN-based models solely utilize neighborhood information of entities to reason, disregarding the textual semantic information of entities and relationships in the knowledge graph. Existing GCN models suffer from poor prediction performance when dealing with tail entities due to limitations. Additionally, these models still have shortcomings in the semantic feature interaction between entities and relations. This paper proposes a Semantic-Enhanced Graph Convolutional Network (SEGCN) for knowledge graph completion. The SEGCN leverages textual descriptions of entities and relations to obtain better entity and relation embeddings using a language model. Additionally, a new Attention-Convolutions Network (ACN) has been developed to enhance the semantic interaction among entities and relations. Based on experimental findings, SEGCN outperforms the state-of-the-art GCN-based model, CompGCN, by showing 0.4%, 0.6%, 0.1 %, and 0.2% improvements in MRR, Hits@l, Hits@3, and Hits@10 on the FB15k-237 dataset, and 1.9%, 1.0%, 2.5%, and 2.9% improvements on the WN18RR dataset, respectively. These findings demonstrate that SEGCN displays improved generalization and accuracy.},
  keywords={Filtering;Computational modeling;Semantics;Noise;Neural networks;Knowledge graphs;Tail;component;knowledge graph completion;semantic information;neighborhood information;pre-trained language models;graph convolutional neural networks;attention mechanisms},
  doi={10.1109/ICEMCE60359.2023.10490589},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10193435,
  author={Belani, Hrvoje and Šolić, Petar and Perković, Toni and Pleština, Vladimir},
  booktitle={2023 8th International Conference on Smart and Sustainable Technologies (SpliTech)}, 
  title={IoT Ontology Development Process for Well-Being, Aging and Health: Challenges and Opportunities}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Ontology development processes are not trivial, given the inherently complex nature of knowledge capturing and management, as well as the need to provide structural and methodical approach on the process methodology itself in order for it to be adopted and usable. If aiming to develop an ontology for multidimensional concepts, such as well-being, aging and health, it is certain that knowledge from multiple domains have to be included, which only extends the time needed for ontology engineering. If such environments aim to be supported by the Internet of Things, than challenges rise even more. This paper provides a scoping analysis of existing well-known ontology development methodologies, with a note on the extent of their adoption and readiness to be used in a multi-domain circumstances. The approach to IoT ontology development process tailoring has been presented and elaborated, as well as the challenges specific to IoT ontology development for well-being, aging and health. Finally, research opportunities have been presented and future directions given on providing more comprehensive, more tailored and more usable ontology development methodologies.},
  keywords={Knowledge engineering;Semantics;Ontologies;Aging;Reliability;Internet of Things;Internet of Things;ontology;well-being;e-health;development process},
  doi={10.23919/SpliTech58164.2023.10193435},
  ISSN={},
  month={June},}@ARTICLE{9831788,
  author={Alam, Mirza Mohtashim and Rony, Md Rashad Al Hasan and Nayyeri, Mojtaba and Mohiuddin, Karishma and Akter, M. S. T. Mahfuja and Vahdati, Sahar and Lehmann, Jens},
  journal={IEEE Access}, 
  title={Language Model Guided Knowledge Graph Embeddings}, 
  year={2022},
  volume={10},
  number={},
  pages={76008-76020},
  abstract={Knowledge graph embedding models have become a popular approach for knowledge graph completion through predicting the plausibility of (potential) triples. This is performed by transforming the entities and relations of the knowledge graph into an embedding space. However, knowledge graphs often include further textual information stored in literal, which is ignored by such embedding models. As a consequence, the learning process stays limited to the structure and the connections between the entities, which has the potential to negatively influence the performance. We bridge this gap by leveraging the capabilities of pre-trained language models to include textual knowledge in the learning process of embedding models. This is achieved by introducing a new loss function that guides embedding models in measuring the likelihood of triples by taking such complementary knowledge into consideration. The proposed solution is a model-independent loss function that can be plugged into any knowledge graph embedding model. In this paper, Sentence-BERT and fastText are used as pre-trained language models from which the embeddings of the textual knowledge are obtained and injected into the loss function. The loss function contains a trainable slack variable that determines the degree to which the language models influence the plausibility of triples. Our experimental evaluation on six benchmarks, namely Nations, UMLS, WordNet, and three versions of CodEx confirms the advantage of using pre-trained language models for boosting the accuracy of knowledge graph embedding models. We showcase this by performing evaluations on top of the five well-known knowledge graph embedding models such as TransE, RotatE, ComplEx, DistMult, and QuatE. The results show an improvement in accuracy up to 9% on UMLS dataset for the Distmult model and 4.2% on the Nations dataset for the ComplEx model when they are guided by pre-trained language models. We additionally studied the effect of multiple factors such as the structure of the knowledge graphs and training steps and presented them as ablation studies.},
  keywords={Predictive models;Computational modeling;Task analysis;Adaptation models;Unified modeling language;Knowledge engineering;Knowledge graph;knowledge graph embeddings;language models;link prediction},
  doi={10.1109/ACCESS.2022.3191666},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9852902,
  author={Zhao, Qian and Wang, Rui and Xu, Peng and Yang, Wei},
  booktitle={2022 13th International Conference on Mechanical and Aerospace Engineering (ICMAE)}, 
  title={Construction and Application of NLP-based Knowledge Graph in CNC Equipment Fault Field}, 
  year={2022},
  volume={},
  number={},
  pages={514-519},
  abstract={Computer Numerical Control (CNC) equipment is a technology-intensive mechatronics complex system cov-ering multidisciplinary knowledge. How to effectively convert historical fault data into useful fault knowledge base is an urgent problem to be solved. A construction technology of CNC equipment fault knowledge graph based on Natural Language Processing (NLP) is proposed. BERT deep learning classification technology is used to build the sample classification model, the named entity recognition model is trained based on the Bi-LSTM technology, as well as the CNC equipment fault corpus sample data is trained, recognized, and modeled. Finally, Neo4j is used to form a knowledge graph model. It has been verified that the recognition rate of the model basically meets the requirements, and on this basis, a recommended solution for repairing equipment faults is proposed, which realizes the effective use of fault knowledge.},
  keywords={Training;Knowledge engineering;Deep learning;Mechatronics;Knowledge based systems;Bit error rate;Neural networks;knowledge graph;CNC equipment failure;NLP;BERT;Bi-LSTM},
  doi={10.1109/ICMAE56000.2022.9852902},
  ISSN={},
  month={July},}@INPROCEEDINGS{10343171,
  author={Reynolds, Sarah and Pate, William C. and Ochoa, Omar},
  booktitle={2023 IEEE Frontiers in Education Conference (FIE)}, 
  title={An Ontology and Management System for Learning Outcomes and Student Mastery}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Universities, faculty, and students use Learning Outcomes (LO) to create a shared understanding of the content provided in an individual course, known as Outcome-Based Education (OBE). One area of interest in OBE is evaluating whether the instructor and individual student performance have met the LO, which is integral to ensuring all invested parties are on the same page about class content and student performance. This work proposes a system for the management and evaluation of LO. Primarily, this work defines an ontology to support the management and evaluation of LO via Knowledge Graphs (KG). The KG links individual LO with individual assessment items. Two state-of-the-art Natural Language Processing models, BERT and ChatGPT, are evaluated in respect to their effectiveness in automating this linking. This data allows the educational professional to reflect on how well their assessments match the course's LO. The second part of this system harnesses student data to measure performance in relation to LO. In this Work-in-Progress paper, the system is prototyped and tested on the midterm results of a course in the Software Engineering curriculum. Student performance is documented in relation to each assessment question on the exams to measure student mastery of course material. Through this approach, courses can be evaluated and improved to deliver better quality education to all students. This includes improvements at the course level and possibilities for early intervention to ensure student success. This paper details the development of this system and through its implementation shows how it benefits engineering educators and their students.},
  keywords={Knowledge engineering;Taxonomy;Knowledge graphs;Ontologies;Market research;Chatbots;Software measurement;Learning outcomes;BERT;ontology;knowledge graph;assessment;Bloom's taxonomy},
  doi={10.1109/FIE58773.2023.10343171},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{10679814,
  author={Sun, Jianing and Zhang, Zhichao and He, Xueli},
  booktitle={2024 International Conference on Networking and Network Applications (NaNA)}, 
  title={LLM4EduKG: LLM for Automatic Construction of Educational Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={269-275},
  abstract={The field of education is undergoing a significant transformation towards digital and intelligent education, driven by advancements in artificial intelligence. Knowledge graphs (KGs), as a structured representation of knowledge and information, offering a powerful way to integrate diverse and multi-sourced heterogeneous data from across the Internet. The current methodologies for constructing educational knowledge graphs, however, are confronted with challenges including labor-intensive, time-consuming, and the necessity for substantial computational resources, which severely limit their practical application, especially in resource-constrained environments. In this paper, we proposed an LLM-based automatic construction method to alleviate the labor and time consumption in existing methods, and further explored LLM’s capabilities in Chinese-speaking context. Specifically, we designed a structured prompt framework to automatically extract and evaluate educational triples generated from original text. The prompt encompasses both task and model dimensions, allowing for flexible adjustments to different tasks and models, thus significantly improved the transferability of our method. Comparative experimental results from two real-world Chinese-datasets, across four advanced LLMs, demonstrate the effectiveness of the proposed method. We believe that our work represents a significant attempt by the LLM in the field of education.},
  keywords={Accuracy;Large language models;Education;Knowledge based systems;Knowledge graphs;Internet;Large Language Model;Knowledge graph;Prompt tuning;Intelligent Education},
  doi={10.1109/NaNA63151.2024.00051},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10386121,
  author={Lee, Hyun Jung and Sohn, Mye},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={Context-based Fact-checking Using Knowledge Graph}, 
  year={2023},
  volume={},
  number={},
  pages={6051-6056},
  abstract={There are many attentions for the fact-checking to prevent the hallucination, malfunction of circulating content on the web such as deception, counterfeit, fake news regardless of whether they are intended or not. For fact-checking, ConFcheKG (Context-based Fact-checking using Knowledge Graph) is proposed based on the Knowledge Graph (KG). In the content including multiple entities, the KG is adopted to check coherence between the entities to determine whether there are semantic conflicts among them. The coherence is based on the temporal, spatial, and logical arrangement of the contents based on the associated relationships among the entities using KG. To do this, it checks the existence of intersected KGs and conflicted KGs through mutual comparison of KGs step by step. According to the verifying process, if is the constructed KT with coherent entities of the content, then the reliability of the content’s coherence is high because it has a high probability of not being false, hallucination, fake-news, and so on. Otherwise, the probability of being false increases. To check the fact, ConFcheKG can be applied based on semantic coherence of the content, to reduce the hallucination, to determine the fake news, to detect deceptions, to prevent counterfeits, to generate the well-composed prompt for the generative AI, and so on.},
  keywords={Generative AI;Semantics;Coherence;Knowledge graphs;Big Data;Reliability;Fake news;fact-checking;knowledge graph;coherence;reliability;context},
  doi={10.1109/BigData59044.2023.10386121},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8241212,
  author={Abrahão, Elcio and Hirakawa, André Riyuiti},
  booktitle={2017 Second International Conference on Information Systems Engineering (ICISE)}, 
  title={Task Ontology Modeling for Technical Knowledge Representation in Agriculture Field Operations Domain}, 
  year={2017},
  volume={},
  number={},
  pages={12-16},
  abstract={Nowadays, ontologies have been used to describe knowledge in a wide quantity of domains. Knowledge discovery, reuse and share benefits are a common sense among information systems developers and there are many works related to ontology engineering, conceptual modeling, metadata and semantic representation in the computational systems area. Although there are many ontologies to describe the vocabulary related to generic domains such as medicine, law, engineering and biology, there are no specific ontologies to describe generic task vocabularies like agriculture field operations. This paper describes a model to represent the technical knowledge for the agriculture field operations as a task ontology that can be used with a domain ontology to build an application ontology for agriculture domain purposes. The field operations were analyzed to determine who are the task agents, agent roles, the input resources, task and sub-task decomposition, control-flow, task concepts, attributes and relations. As a result, a formal representation of the agriculture field operations is presented and a conceptual model was built using UML class and activity diagrams. The model consistency was verified in a case study of the sugar cane harvest operation. The model presented describes the domain operations knowledge and it could be the base for the development of computational application systems as field operations control, decision support and precision agriculture.},
  keywords={Unified modeling language;Ontologies;Agriculture;Computational modeling;Sugar industry;Mathematical model;task ontology;ontology engineering;agriculture;UML;modeling},
  doi={10.1109/ICISE.2017.18},
  ISSN={2160-1291},
  month={April},}@INPROCEEDINGS{9061292,
  author={Yang, Hao and Qin, Ying and Deng, Yao and Wang, Minghan},
  booktitle={2020 22nd International Conference on Advanced Communication Technology (ICACT)}, 
  title={NMT Enhancement based on Knowledge Graph Mining with Pre-trained Language Model}, 
  year={2020},
  volume={},
  number={},
  pages={185-189},
  abstract={Pre-trained language models like Bert, RoBERTa, GPT, etc. have achieved SOTA effects on multiple NLP tasks (e.g. sentiment classification, information extraction, event extraction, etc.). We propose a simple method based on knowledge graph to improve the quality of machine translation. First, we propose a multi-task learning model that learns subjects, objects, and predicates at the same time. Second, we treat different predicates as different fields, and improve the recognition ability of NMT models in different fields through classification labels. Finally, beam search combined with L2R, R2L rearranges results through entities. Based on the CWMT2018 experimental data, using the predicate's domain classification identifier, the BLUE score increased from 33.58% to 37.63%, and through L2R, R2L rearrangement, the BLEU score increased to 39.25%, overall improvement is more than 5 percentage.},
  keywords={Training;Optimization;Biological system modeling;Task analysis;Information retrieval;Mathematical model;Data models;NMT;Pre-trained Language Model;Knowledge Graph},
  doi={10.23919/ICACT48636.2020.9061292},
  ISSN={1738-9445},
  month={Feb},}@ARTICLE{10013735,
  author={Benarab, Achref and Sun, Jianguo and Rafique, Fahad and Refoufi, Allaoua},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Global Ontology Entities Embeddings}, 
  year={2023},
  volume={35},
  number={11},
  pages={11449-11460},
  abstract={Ontologies are among the most widely used types of knowledge representation formalisms. The application of deep learning techniques in the field of ontology engineering has reinforced the need to learn and generate representations of ontological data. This allows ontologies to be exploited by such models, and thus automate various ontology engineering tasks, where most of the existing tools and machine learning approaches require a numerical feature vectors associated with each concept. This paper outlines a novel approach for learning global ontology entities embeddings by exploiting the structure and the various taxonomic and semantic relationships present in ontologies, taking into account all the information present in the ontological graph and carried by the OWL/RDF triples. Thus, producing global ontology entities embeddings capturing the global ontological graph semantics and similarities enclosed in the source ontology. Three different neural network models have been proposed based on two architectures: multi-input and multi-output, trained using the contrastive estimation technique. The evaluation on OWL/RDF ontologies and word semantic similarity tasks using various graph and WordNet based similarity measures, show that our approach yields competitive results outperforming the state-of-the-art ontology and word embedding models.},
  keywords={Ontologies;Semantics;Task analysis;Adaptation models;Predictive models;Neural networks;Deep learning;Concept embeddings;feature representation;neural networks;ontology embeddings;ontology entities vector representations},
  doi={10.1109/TKDE.2023.3235779},
  ISSN={1558-2191},
  month={Nov},}@INPROCEEDINGS{9574558,
  author={Zhao, Tong and Wu, Ke-He},
  booktitle={2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI)}, 
  title={Construction of power marketing user knowledge graph based on $\text{BERT}+\text{BILSTM}+\text{CRF}$ model}, 
  year={2021},
  volume={},
  number={},
  pages={396-399},
  abstract={In order to solve the fierce competition in the power market caused by the continuous improvement of the power consumption level of users, to reasonably deal with the contradiction between power supply and demand and to provide effective marketing strategies, the author proposes to construct the user knowledge graph based on the power marketing data. After preliminary data preprocessing, BERT (Pre-training of Deep Bidirectional Transformers for Language Understanding) model is selected for entity recognition. Bi-lstm (Bidirectional Long short-term Memory) model and CRF (Conditional Random Field) model are combined to extract the relationship, build triples and establish knowledge graph. The graph can clearly show the relationship between users and power load, address, price package, etc., which is conducive to relevant enterprises to put forward corresponding marketing or emergency measures, and provide theoretical support for optimization schemes such as “power demand side management” and “peak load shifting and valley filling” under the new situation.},
  keywords={Analytical models;Power demand;Power measurement;Bit error rate;Tools;Transformers;Data models;Knowledge graph;Electric power marketing;Electrical behavior;BERT;Bi - LSTM;CRF},
  doi={10.1109/CEI52496.2021.9574558},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10585481,
  author={Gao, Yue and Luo, Xin and Tao, Ran and Feng, Xiangyang},
  booktitle={2024 4th International Conference on Computer, Control and Robotics (ICCCR)}, 
  title={Knowledge Graph Completion Based on Neighborhood-Aware Double-Layer Transformer}, 
  year={2024},
  volume={},
  number={},
  pages={390-394},
  abstract={In knowledge graph completion, most models embed entities and relations into low-dimensional vectors and use them as inputs to learn their latent interaction features. However, these models primarily focus on static embeddings for individual triplets, neglecting the rich contextual features related to entities. This paper introduces a knowledge graph embedding model based on Neighborhood-Aware Double-Layer Transformer (NADTKE). The model consists of two layers: the bottom layer is used to learn the interaction features between the source entity and its neighborhood with respect to relations, while the top layer is responsible for aggregating contextual information from the outputs of the bottom layer. This dual-layer design effectively balances feature information from both the source entity and its neighboring entities. Experimental evaluations on the FB15k-237 and WN18RR datasets demonstrate that the proposed model achieves an MRR of 0.37 and a Hit@1 score of 0.281 on the FB15k-237 dataset, providing evidence of its effectiveness.},
  keywords={Digital control;Computational modeling;Knowledge graphs;Transformers;Vectors;Robots;Context modeling;knowledge graph completion;contextual features;Neighborhood aware;Double Layer Transformer},
  doi={10.1109/ICCCR61138.2024.10585481},
  ISSN={},
  month={April},}@INPROCEEDINGS{10624590,
  author={Li, Haiping and Duan, Wenjing},
  booktitle={2024 International Conference on Informatics Education and Computer Technology Applications (IECA)}, 
  title={Construction and Application Research of Intelligent Education Knowledge Graph Based on Multi-modal Learning}, 
  year={2024},
  volume={},
  number={},
  pages={117-121},
  abstract={This study explores the methods and applications of constructing an intelligent education knowledge graph based on multi-modal learning materials (text, images, videos) Through deep learning algorithms for feature extraction and relationship modeling, a knowledge graph containing 120 entities and 100 relationships was built with an accuracy of $87 \%$. The graph demonstrates excellent performance in personalized learning path recommendations, capable of recommending based on students’ prior knowledge and learning styles, leading to a $27 \%$ improvement in learning efficiency. The study also considers the “uncanny valley” phenomenon, avoiding discomfort caused by AI mimicking human behavior excessively, ensuring a positive user experience.},
  keywords={Deep learning;Education;Knowledge graphs;Feature extraction;User experience;Artificial intelligence;Informatics;Knowledge graph;Image analysis;Intelligent education;Uncanny valley},
  doi={10.1109/IECA62822.2024.00029},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{9732087,
  author={Wang, Yan and Allouache, Yacine and Joubert, Christian},
  booktitle={2021 Eighth International Conference on Social Network Analysis, Management and Security (SNAMS)}, 
  title={A Staffing Recommender System based on Domain-Specific Knowledge Graph}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={In the economics environment, Job Matching is always a challenge involving the evolution of knowledge and skills. A good matching of skills and jobs can stimulate the growth of economics. Recommender System (RecSys), as one kind of Job Matching, can help the candidates predict the future job relevant to their preferences. However, RecSys still has the problem of cold start and data sparsity. The content-based filtering in RecSys needs the adaptive data for the specific staffing tasks of Bidirectional Encoder Representations from Transformers (BERT). In this paper, we propose a job RecSys based on skills and locations using a domain-specific Knowledge Graph (KG). This system has three parts: a pipeline of Named Entity Recognition (NER) and Relation Extraction (RE) using BERT; a standardization system for pre-processing, semantic enrichment and semantic similarity measurement; a domain-specific Knowledge Graph (KG). Two different relations in the KG are computed by cosine similarity and Term Frequency-Inverse Document Frequency (TF-IDF) respectively. The raw data used in the staffing RecSys include 3000 descriptions of job offers from Indeed, 126 Curriculum Vitae (CV) in English from Kaggle and 106 CV in French from Linx of Capgemini Engineering. The staffing RecSys is integrated under an architecture of Microservices. The autonomy and effectiveness of the staffing RecSys are verified through the experiment using Discounted Cumulative Gain (DCG). Finally, we propose several potential research directions for this research.},
  keywords={Economics;Social networking (online);Semantics;Bit error rate;Microservice architectures;Computer architecture;Standardization;Job Matching;Recommender System;Knowledge Graph;TF-IDF;BERT;NER;Cosine Similarity;K-hop;Discounted Cumulative Gain;Microservices},
  doi={10.1109/SNAMS53716.2021.9732087},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10800696,
  author={Sun, Jiawei and Zhang, Quan and Zhi, Yongqi and Ling, Weiqing},
  booktitle={2024 4th International Conference on Electronic Information Engineering and Computer Science (EIECS)}, 
  title={Knowledge Graph Indexing Enhanced Q&A System Based on Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={548-553},
  abstract={With the advent of the digital age, the explosive growth of data has brought unprecedented opportunities and challenges to societal development. The rapid evolution of data science is driving innovation and progress across various industries. Both the business and academic communities recognize the immense potential hidden within vast amounts of data. Extracting truly valuable information from this data has become a critical issue that needs to be addressed. As technology advances swiftly and data accumulates rapidly, traditional processing and analysis methods can no longer keep up with the current volume and velocity of data. This has prompted data professionals to seek innovative solutions to handle increasingly complex and massive data relationships and quantities. Against this backdrop, two key technologies have emerged due to their exceptional performance-knowledge graphs and large language models},
  keywords={Knowledge engineering;Training;Technological innovation;Large language models;Atmospheric modeling;Computational modeling;Knowledge graphs;Aerospace electronics;Information age;Question answering (information retrieval);Knowledge graph;large language model;knowledge question answering;lapping process knowledge},
  doi={10.1109/EIECS63941.2024.10800696},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9534355,
  author={Wang, Bin and Wang, Guangtao and Huang, Jing and You, Jiaxuan and Leskovec, Jure and Kuo, C.-C. Jay},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Inductive Learning on Commonsense Knowledge Graph Completion}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Commonsense knowledge graph (CKG) is a special type of knowledge graph (KG), where entities are composed of free-form text. Existing CKG completion methods focus on transductive learning setting, where all the entities are present during training. Here, we propose the first inductive learning setting for CKG completion, where unseen entities may appear at test time. We emphasize that the inductive learning setting is crucial for CKGs, because unseen entities are frequently introduced due to the fact that CKGs are dynamic and highly sparse. We propose InductivE as the first framework targeted at the inductive CKG completion task. InductivE first ensures the inductive learning capability by directly computing entity embeddings from raw entity attributes. Second, a graph neural network with novel densification process is proposed to further enhance unseen entity representation with neighboring structural information. Experimental results show that InductivE performs especially well on inductive scenarios where it achieves above 48% improvement over previous methods while also outperforms state-of-the-art baselines in transductive settings.},
  keywords={Training;Transfer learning;Benchmark testing;Graph neural networks;Task analysis;Research and development;Commonsense Knowledge Graph;Inductive Learning;Graph Learning;Knowledge Graph Completion},
  doi={10.1109/IJCNN52387.2021.9534355},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{10743881,
  author={Zhang, Kaiwen and Su, Feiyu and Huang, Yixiang and Li, Yanming and Wu, Fengqi and Mao, Yuhan},
  booktitle={2024 9th International Conference on Intelligent Computing and Signal Processing (ICSP)}, 
  title={The Application of Fine-Tuning on Pretrained Language Model in Information Extraction for Fault Knowledge Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={469-473},
  abstract={Constructing fault knowledge graphs holds significant importance for achieving intelligent maintenance and diagnosis in high-end equipment manufacturing. Effective information extraction and knowledge graph construction have proven challenging due to the lack of standardized representation of semantically complex unstructured text in the industrial domain. Therefore, in this study, we performed fine-tuning on the pre-trained language model (ChatGLM2-6B) with specific prompts to achieve information extraction from fault-related texts, ultimately leading to the construction of a fault knowledge graph. Experimental results demonstrate that the proposed method not only supports fine-tuning with limited data but also exhibits enhanced capability in understanding complex semantics related to fault symptoms and causes.},
  keywords={Computational modeling;Semantics;Knowledge graphs;Signal processing;Ontologies;Information retrieval;Stability analysis;Data models;Manufacturing;Maintenance;Pretrained language model;Parameter-efficient fine-tuning;Information extraction;Fault knowledge graph},
  doi={10.1109/ICSP62122.2024.10743881},
  ISSN={},
  month={April},}@INPROCEEDINGS{10674566,
  author={Li, Chen and Zheng, Haotian and Sun, Yiping and Wang, Cangqing and Yu, Liqiang and Chang, Che and Tian, Xinyu and Liu, Bo},
  booktitle={2024 4th International Conference on Machine Learning and Intelligent Systems Engineering (MLISE)}, 
  title={Enhancing Multi-Hop Knowledge Graph Reasoning through Reward Shaping Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={In the realm of computational knowledge representation, Knowledge Graph Reasoning (KG-R) stands at the fore-front of facilitating sophisticated inferential capabilities across multifarious domains. The quintessence of this research elucidates the employment of reinforcement learning (RL) strategies, notably the REINFORCE algorithm, to navigate the intricacies inherent in multi-hop KG-R. This investigation critically addresses the prevalent challenges introduced by the inherent incompleteness of Knowledge Graphs (KGs), which frequently results in erroneous inferential outcomes, manifesting as both false negatives and misleading positives. By partitioning the Unified Medical Language System (UMLS) benchmark dataset into rich and sparse subsets, we investigate the efficacy of pretrained BERT embeddings and Prompt Learning methodologies to refine the reward shaping process. This approach not only enhances the precision of multi-hop KG-R but also sets a new precedent for future research in the field, aiming to improve the robustness and accuracy of knowledge inference within complex KG frameworks. Our work contributes a novel perspective to the discourse on KG reasoning, offering a methodological advancement that aligns with the academic rigor and scholarly aspirations of the Natural journal, promising to invigorate further advancements in the realm of computational knowledge representation.},
  keywords={Training;Navigation;Unified modeling language;Transfer learning;Knowledge graphs;Reinforcement learning;Cognition;Knowledge Graph Reasoning;Reinforcement Learning;Reward Shaping;Transfer Learning},
  doi={10.1109/MLISE62164.2024.10674566},
  ISSN={},
  month={June},}@INPROCEEDINGS{10719142,
  author={Yang, Zeyu and Duan, Yucong and Xue, Jun and Qi, Qi},
  booktitle={2024 IEEE 9th International Conference on Computational Intelligence and Applications (ICCIA)}, 
  title={An Intra-Network Multi-Teacher Distillation Method Towards Lightweight Knowledge Graph Completion}, 
  year={2024},
  volume={},
  number={},
  pages={109-114},
  abstract={Recently, Knowledge Graph Completion (KGC) based on Pre-trained Language Models (PLM) has made significant advancements. However, PLM typically have a large number of parameters, which makes lightweight research for low-resource challenging. For KGC, knowledge distillation can be an portable method. But traditional knowledge distillation is difficult to achieve efficient knowledge transfer. To solve this issue, this paper proposes an intra-network multi-teacher knowledge distillation, which can effectively reduce knowledge leakage through multi-level information transmission. Specifically, we divide the teacher model into multiple sub-teachers based on network depth, the sub-teachers deliver different knowledge representations. In addition, we use the loss variation of each sub-teacher as a confidence level, which can dynamically regulate the intensity of multi-teacher distillation and enable the student model to perceive distilled knowledge at a finer granularity. A series of experimental results show that our proposed method achieves state-of-the-art performance with the low number of parameters.},
  keywords={Computational modeling;Knowledge based systems;Knowledge graphs;Information processing;Knowledge transfer;Computational intelligence;multi-teacher;knowledge graph completion;knowledge distillation},
  doi={10.1109/ICCIA62557.2024.10719142},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10385745,
  author={Fu, Chengcheng and Yao, Yanan and Wu, Jieyu and Zhao, Weizhong and He, Tingting and Jiang, Xingpeng},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Multimodal reasoning for nutrition and human health via knowledge graph embedding}, 
  year={2023},
  volume={},
  number={},
  pages={1901-1904},
  abstract={The established links between nutrition and human health are widely acknowledged. Dietary nutrients play a crucial role in regulating gut microbial communities, influencing various human diseases. With a growing number of related studies, there’s a need to systematically organize these associations for coherent knowledge reasoning. However, due to the diverse and extensive nature of the knowledge landscape, significant challenges persist. To address this, we propose an approach using multimodal data and knowledge embeddings for effective knowledge reasoning in nutrition and human health. We create a comprehensive knowledge graph, KG4NH, covering dietary nutrition, gut microbiota, and human diseases. To ensure efficient knowledge representation, we employ knowledge embedding techniques to develop modality-specific encoders for structure, category, and description. Additionally, we introduce a mul-timodal fusion method to capture shared information across modalities. Our experimental results demonstrate the superiority of our approach over other state-of-the-art methods.},
  keywords={Knowledge graphs;Ontologies;Feature extraction;Cognition;Bioinformatics;Diseases;Knowledge graph;Multimodal embedding;Knowledge reasoning;Nutrition;Human health},
  doi={10.1109/BIBM58861.2023.10385745},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{9709043,
  author={Wang, Fang and Xie, Yongqiang and Zhang, Kai and Xia, Rui and Zhang, Yunchao},
  booktitle={2021 2nd International Conference on Big Data Economy and Information Management (BDEIM)}, 
  title={Bert-based Knowledge Graph Completion Algorithm for Few-Shot}, 
  year={2021},
  volume={},
  number={},
  pages={217-224},
  abstract={Knowledge graphs have shown increasing value in semantic search, intelligent Q&A, data analysis, natural language processing, visual understanding, IoT devices, etc. It is undeniable that knowledge graphs have become the mandatory path for the artificial intelligence fields development. However, the information contained in existing knowledge graphs is incomplete, which attracts a large number of researchers to enhance the completeness of knowledge graphs utilizing knowledge graph completion methods. Most traditional embedding-based knowledge graph completion models use structural information within data-rich triples from the knowledge graph is limited by the long-tail distribution of the relations in the triples. To address this problem, we propose a Bert-based knowledge graph completion algorithm for few-shot knowledge graphs, with the main goal of implementing the knowledge graph completion task with only a few sample triples of training instances. We improve the baseline model GMatching for handling few-shot knowledge graphs by introducing the Bert pre-trained linguistic representation model to enhance the semantic representation of entities and relations in the triples. Through experiments, we demonstrate that our improved model B-GMatching achieves good results.},
  keywords={Training;Visualization;Semantic search;Biological system modeling;Knowledge based systems;Training data;Linguistics;Knowledge Graph;Knowledge Graph Completion;Bert;few-shot},
  doi={10.1109/BDEIM55082.2021.00051},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10635576,
  author={Wei, Xiaoyang and Vagena, Zografoula and Kurtz, Camille and Cloppet, Florence},
  booktitle={2024 IEEE International Symposium on Biomedical Imaging (ISBI)}, 
  title={Integrating Expert Knowledge with Vision-Language Model for Medical Image Retrieval}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Content-Based Image Retrieval (CBIR) is an image search technique that can offer diagnostic guidance when facing difficult cases in radiology. State-of-the-art approaches propose to extract image features using vision-language models which learn image representations from supervision of text in medical literature. However, existing methods seldom take expert knowledge in medical domain into account. In this article, we propose a knowledge-and-language-guided contrastive visual representation learning framework for image retrieval. Our method consists of two steps: (1) modeling relationships between medical concepts and medical images using a knowledge graph, and translating each node in the graph into a knowledge embedding; (2) injecting knowledge embeddings into a vision-language model by aligning image representations using both encoded textual input and knowledge embeddings. Our experiments show that the proposed framework achieves comparable results to state-of-the-art methods on CBIR tasks using much less training data. Our code is publicly available at https://github.com/Wxy-24/KL-CVR.},
  keywords={Representation learning;Visualization;Biological system modeling;Image retrieval;Training data;Image representation;Radiology;Image retrieval;Representation learning;Vision-language model;Knowledge graph},
  doi={10.1109/ISBI56570.2024.10635576},
  ISSN={1945-8452},
  month={May},}@INPROCEEDINGS{10711054,
  author={Knollmeyer, Simon and Akmal, Muhammad Uzair and Koval, Leonid and Asif, Saara and Mathias, Selvine G. and Groβmann, Daniel},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Document Knowledge Graph to Enhance Question Answering with Retrieval Augmented Generation}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Reusing and managing existing knowledge from available documents is crucial for success in the factory planning domain. By leveraging Artificial Intelligence (AI) and Question Answering (QA) systems, users can query a document corpus through a chat-based application and receive precise answers. The recent advancements in Large Language Models (LLMs) and their linguistic capabilities present new opportunities for such applications. Utilizing the methodology of Retrieval Augmented Generation (RAG), document sections are provided to the LLM based on user queries. However, existing RAG implementations that use vector databases as document repositories face limitations when answering questions that extend beyond the text content of the documents. To address this issue, this paper proposes a concept to enhance RAG systems by integrating a Knowledge Graph (KG) constructed from the document structures.},
  keywords={Databases;Large language models;Knowledge graphs;Linguistics;Question answering (information retrieval);Vectors;Production facilities;Planning;Manufacturing automation;Faces;Information management;Retrieval Augmented Generation;Knowledge Graph;Large Language Models},
  doi={10.1109/ETFA61755.2024.10711054},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10298399,
  author={Huang, Qing and Wan, Zhenyu and Xing, Zhenchang and Wang, Changjing and Chen, Jieshan and Xu, Xiwei and Lu, Qinghua},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Let's Chat to Find the APIs: Connecting Human, LLM and Knowledge Graph through AI Chain}, 
  year={2023},
  volume={},
  number={},
  pages={471-483},
  abstract={API recommendation methods have evolved from literal and semantic keyword matching to query expansion and query clarification. The latest query clarification method is knowledge graph (KG)-based, but limitations include out-of-vocabulary (OOV) failures and rigid question templates. To address these limitations, we propose a novel knowledge-guided query clarification approach for API recommendation that leverages a large language model (LLM) guided by KG. We utilize the LLM as a neural knowledge base to overcome OOV failures, generating fluent and appropriate clarification questions and options. We also leverage the structured API knowledge and entity relationships stored in the KG to filter out noise, and transfer the optimal clarification path from KG to the LLM, increasing the efficiency of the clarification process. Our approach is designed as an AI chain that consists of five steps, each handled by a separate LLM call, to improve accuracy, efficiency, and fluency for query clarification in API recommendation. We verify the usefulness of each unit in our AI chain, which all received high scores close to a perfect 5. When compared to the baselines, our approach shows a significant improvement in MRR, with a maximum increase of 63.9% higher when the query statement is covered in KG and 37.2% when it is not. Ablation experiments reveal that the guidance of knowledge in the KG and the knowledge-guided pathfinding strategy are crucial for our approach's performance, resulting in a 19.0% and 22.2% increase in MAP, respectively. Our approach demonstrates a way to bridge the gap between KG and LLM, effectively compensating for the strengths and weaknesses of both.},
  keywords={Semantics;Knowledge based systems;Knowledge graphs;Oral communication;Data models;Software engineering;API recommendation;query clarification;knowledge graph;large language model;out-of-vocabulary},
  doi={10.1109/ASE56229.2023.00075},
  ISSN={2643-1572},
  month={Sep.},}@INPROCEEDINGS{10721152,
  author={Tian, Xiaoyun and Liu, Bin and Yang, Jianzhong and Ai, Bo and Zhao, Huailiang and Zhang, Shaoyang},
  booktitle={2024 4th Power System and Green Energy Conference (PSGEC)}, 
  title={Knowledge Graph Enhanced Interactive Fault Diagnosis O&M Approach Based on Large Models of Electrical Power}, 
  year={2024},
  volume={},
  number={},
  pages={137-142},
  abstract={Power equipment operation and maintenance has accumulated a large amount of knowledge recorded in the form of text, which provides the basis for power equipment diagnosis and operation and maintenance. However, the huge amount of redundant knowledge makes it difficult for the relevant personnel to quickly access the information, and it is also impossible to integrate it into the existing diagnostic algorithms. For this reason, this paper proposes an interactive equipment diagnosis and operation and maintenance method based on knowledge graph and big model. First, the unstructured knowledge is transformed and constitutes the O&M (Operation and Maintenance) knowledge graph using entity extraction method; second, the conditional probability of the equipment feature quantity to the failure mode is fitted using graph convolution to realize the equipment failure diagnosis; subsequently, an intention recognition algorithm based on self-attention network is proposed and retrieval is realized; finally, the results are generated into natural language using the big language model. It is verified that the proposed diagnosis and intent recognition F1 values are improved by 9.77% and 17.48%, respectively, confirming the effectiveness of the system.},
  keywords={Fault diagnosis;Power measurement;Intent recognition;Natural languages;Knowledge graphs;Feature extraction;Maintenance;Power systems;Power transformers;Personnel;knowledge graph;interactive system;large language model;fault diagnosis},
  doi={10.1109/PSGEC62376.2024.10721152},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10403258,
  author={Wu, Ling-I and Li, Guoqiang},
  booktitle={2023 IEEE International Conference on Medical Artificial Intelligence (MedAI)}, 
  title={Zero-Shot Construction of Chinese Medical Knowledge Graph with ChatGPT}, 
  year={2023},
  volume={},
  number={},
  pages={278-283},
  abstract={Knowledge graphs have revolutionized the organization and retrieval of real-world knowledge, prompting inter-est in automatic NLP-based approaches for extracting medical knowledge from texts. However, the availability of high-quality Chinese medical knowledge remains limited, posing challenges for constructing Chinese medical knowledge graphs. As LLMs like ChatGPT show promise in zero-shot learning for many NLP downstream tasks, their potential on constructing Chinese medical knowledge graphs is still uncertain. In this study, we create a Chinese medical knowledge graph by manually annotating textual data and using ChatGPT to automatically generate the graph. We refine the results using filtering and mapping rules to align with our schema. The manually generated graph serves as the ground truth for evaluation, and we explore different methods to enhance its accuracy through knowledge graph completion techniques. As a result, we emphasize the potential of employing ChatGPT for automated knowledge graph construction within the Chinese medical domain. While ChatGPT successfully identifies a larger number of entities, further en-hancements are required to improve its performance in extracting more qualified relations.},
  keywords={Zero-shot learning;Filtering;Knowledge graphs;Organizations;Chatbots;Task analysis;Artificial intelligence;knowledge graph;ChatGPT;nature language processing;named entity recognition;relation extraction},
  doi={10.1109/MedAI59581.2023.00043},
  ISSN={},
  month={Nov},}@ARTICLE{10463190,
  author={LIU, Peifeng and Qian, Lu and Zhao, Xingwei and Tao, Bo},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Joint Knowledge Graph and Large Language Model for Fault Diagnosis and Its Application in Aviation Assembly}, 
  year={2024},
  volume={20},
  number={6},
  pages={8160-8169},
  abstract={In complex assembly industry settings, fault localization involves rapidly and accurately identifying the source of a fault and obtaining a troubleshooting solution based on fault symptoms. This study proposes a knowledge-enhanced joint model that incorporates aviation assembly knowledge graph (KG) embedding into large language models (LLMs). This model utilizes graph-structured Big Data within KGs to conduct prefix-tuning of the LLMs. The KGs for prefix-tuning enable an online reconfiguration of the LLMs, which avoids a massive computational load. Through the subgraph embedding learning process, the specialized knowledge of the joint model within the aviation assembly domain, especially in fault localization, is strengthened. In the context of aviation assembly functional testing, the joint model can generate knowledge subgraphs, fuse knowledge through retrieval augmentation, and ultimately provide knowledge-based reasoning responses. In practical industrial scenario experiments, the joint enhancement model demonstrates an accuracy of 98.5% for fault diagnosis and troubleshooting schemes.},
  keywords={Task analysis;Fault diagnosis;Training;Hidden Markov models;Cognition;Knowledge engineering;Knowledge graphs;Data-driven;fault localization;intelligent fault diagnosis;knowledge graph (KG);large language model (LLM)},
  doi={10.1109/TII.2024.3366977},
  ISSN={1941-0050},
  month={June},}@INPROCEEDINGS{9845845,
  author={Yiming, Liu and Li, Duan},
  booktitle={2022 7th International Conference on Computer and Communication Systems (ICCCS)}, 
  title={Research on the Construction of Maritime Legal Knowledge Graph}, 
  year={2022},
  volume={},
  number={},
  pages={903-908},
  abstract={As the marine industry booms, the maritime legal documents are of great importance to the maneuver on the sea. However, the traditional way of consulting the text can not meet the demand of maritime operation nowadays. This paper aims to explore a way to extract and strengthen data from maritime legal texts to better support legal question answering. To mine knowledge from unstructured maritime laws and regulations, this paper proposes a method to build the maritime legal knowledge graph. To extract information from unstructured texts, BERT+BiLSTM+CRF is used for named entity recognition. DeepKE toolkit is used for relation extraction. And to strengthen the logics between entities, heterogeneous nodes are introduced to enhance the semantic associations in the maritime legal knowledge graph. The document-enhanced knowledge graph expanded in scale, so it can better support subsequent intelligent applications.},
  keywords={Law;Text recognition;Semantics;Pipelines;Ontologies;Information retrieval;Regulation;knowledge graph;maritime law;named entity recognition;heterogeneous entities},
  doi={10.1109/ICCCS55155.2022.9845845},
  ISSN={},
  month={April},}@ARTICLE{10489926,
  author={Liu, Xingyu and Wang, Zhenxing and Sun, Yue and Han, Junmei and Xiao, Gang and Jiang, Jianchun},
  journal={IEEE Access}, 
  title={ISA-KGC: Integrated Semantics-Structure Analysis in Knowledge Graph Completion}, 
  year={2024},
  volume={12},
  number={},
  pages={57250-57260},
  abstract={This paper presents Integrated Semantics-Structure Analysis in Knowledge Graph Completion (ISA-KGC), a new framework for Knowledge Graph Completion (KGC) aimed at addressing the incompleteness of knowledge graphs (KGs). ISA-KGC integrates Graph Neural Networks (GNN) with Transformer-based models, effectively blending structural and semantic information within Knowledge Graphs. This fusion enhances comprehension of KGs beyond what traditional methods offer. The framework utilizes Knowledge Graph Embedding (KGE) models, with GNN employed to augment these models, thus enhancing the overall analysis and interpretation of Knowledge Graphs. The effectiveness of ISA-KGC is validated through benchmark datasets FB15K-237 and WN18RR, showing notable improvements in performance metrics like hit@10 compared to existing methods.},
  keywords={Knowledge graphs;Semantics;Graph neural networks;Vectors;Adaptation models;Task analysis;Metalearning;Graph neural network;knowledge graph completion;knowledge graph embedding;pre-trained language model},
  doi={10.1109/ACCESS.2024.3384533},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10858688,
  author={Si, Jiaqi and Ouyang, Xiaoye and Zhu, Xiaoling and Zhang, Yi},
  booktitle={2024 IEEE 9th International Conference on Data Science in Cyberspace (DSC)}, 
  title={A Relation Semantic Enhancement Method for Large Language Model Based Knowledge Graph Completion}, 
  year={2024},
  volume={},
  number={},
  pages={209-215},
  abstract={The task of Knowledge Graph Completion (KGC) is vital for leveraging the full potential of Knowledge Graphs (KGs), which encode rich semantic information about entities and their relationships. Recently, LLM-based KGC methods have achieved significant advancements, which usually leverage pre-trained Large Language Models (LLMs) to encode entity’s (or relation’s) descriptions. While promising, current LLM-based KGC methods face limitations due to the quality of available text data and the incomplete nature of KGs, especially the commonly missing relation’s descriptions. To address this kind of challenges, this paper proposes a general framework, which harnesses the power of LLMs, as well as high-quality pre-defined concept semantics from extra Knowledge Bases (KB), to enhance the input data for current LLM-based KGC models. Especially, our framework consists of two main components for better understanding relation’s semantics, from diverse levels (i.e., KG-level and triple-level), offering a more nuanced perspective on relation understanding. Actually, the proposed work could be used as an easy-to-accomplish plug-in unit, for improving many kinds of current native LLM-based KGC models. By leveraging LLMs and KBs from various levels, our framework could improve the understanding of relation meanings. These enhancements collectively contribute to the improved performance of LLM-based KGC models, particularly in scenarios where the KG is sparse or incomplete. We conduct extensive experiments on several LLM-based KGC models and real-world datasets to validate the efficacy of our approach. The empirical results indicate that our framework markedly enhances the efficacy of LLM-based KGC models across both entity prediction and triple classification tasks.},
  keywords={Large language models;Semantics;Knowledge based systems;Cyberspace;Knowledge graphs;Predictive models;Data science;Data models;Faces;Knowledge Graph;Knowledge Graph Completion;Representation Learning;Large Language Model},
  doi={10.1109/DSC63484.2024.00035},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10698632,
  author={Yadav, Divyanshi and Para, Hitesh and Sandhu, Komal and Selvakumar, Prakash},
  booktitle={2024 International Conference on Electrical, Computer and Energy Technologies (ICECET}, 
  title={Enhancing Response Generation Systems: Knowledge Graph & Generative AI Synergy for Business Communication and Strategic Decision Making}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In the contemporary business environment, it is crucial to have an efficient and precise response generation system to build client trust, optimize operations, and provide customized solutions. Current response management systems often face challenges such as insufficient depth, scalability issues, and inconsistencies. There is an urgent requirement for a comprehensive system that can retrieve information for generating high-quality insights and convert it to a reply. To address these needs, an approach has been explored that integrates a domain-oriented Knowledge Graph (KG), vector embeddings, Large Language Model (LLM) and utilize the pathway parsing technique that allows for in-depth multi-hop analysis within the KG, resulting in detailed and contextually rich data retrieval. This combination enhances the performance and precision of handling inquiries, streamlines entity extraction and step identification, leverages KG for Standard Operating Procedure (SOP) guidance, and offers superior recommendations or strategies for informed decision making. The concept will be illustrated through a business study focusing on the collections department use case, which involves customer correspondence. This approach ensures a more efficient, and accurate responses, leading to reduced human intervention and latency, along with that customer satisfaction is improved and business processes are streamlined. By adopting this method, businesses can enhance their communication, make data-driven decisions, and ultimately achieve better results in the competitive market. The efficacy is evident in the practical instances, owing to its profound grasp of context.},
  keywords={Generative AI;Scalability;Large language models;Decision making;Focusing;Knowledge graphs;Vectors;Standards;Faces;Business;Information Retrieval;Email Handling;Response Generation;Decision Making;Pathways parsing;Retrieval Augmented Generation;Knowledge Graphs;Word Embeddings;Natural Language Processing;Large Language Model;Natural Language Understanding},
  doi={10.1109/ICECET61485.2024.10698632},
  ISSN={},
  month={July},}@INPROCEEDINGS{10603818,
  author={Lan, Richeng and Fan, Guangwei and Tian, Maochun and Yang, Yue and Wang, Gaodan and Wang, Qingzheng and Wang, Jingteng},
  booktitle={2024 5th International Conference on Computer Engineering and Application (ICCEA)}, 
  title={Research on the framework of low-cost wide-domain Question-Answering system based on knowledge graph}, 
  year={2024},
  volume={},
  number={},
  pages={419-424},
  abstract={Knowledge graph-based question-answering (KBQA) systems suffer from problems such as low-quality datasets and limited categories of candidate entities, which lead to difficulties in system construction and limited applications. To address this problem, a low-cost wide-domain KBQA system construction framework called LCWD-QA is proposed by combining deep learning, knowledge graphs, and large language models. First, an entity span prediction model was designed to recognize potential entity mentions in sentences. Then, the concept of entity popularity is introduced, and an entity-linking algorithm is designed to link entity mentions to specific entities in the knowledge graph. Finally, the Bert style of text classification models was used for intent recognition, and then different methods were used to generate the answer to that question. In addition, large language models were used to enhance the experimental dataset and generate supplementary answers to the knowledge graph to improve the generalization of the system. The experimental results show that the LCWD-QA system presented in this paper exhibits good performance on both entity span prediction and intent recognition subtasks, with an accuracy of 99.5%, which is better than that of the prevalent benchmark models. The system avoids the high dependence of traditional named entity recognition schemes on manually labeled datasets and has high accuracy and interpretability, which has high application and reference value.},
  keywords={Deep learning;Accuracy;Intent recognition;Large language models;Computational modeling;Text categorization;Knowledge graphs;KBQA;Knowledge Graph;Entity Span Prediction;Large Language Model;Entity linking},
  doi={10.1109/ICCEA62105.2024.10603818},
  ISSN={2159-1288},
  month={April},}@INPROCEEDINGS{10313152,
  author={Ermakov, Ivan and Lanin, Viacheslav and Lyadova, Lyudmila and Proskuryakov, Kirill},
  booktitle={2023 IEEE 17th International Conference on Application of Information and Communication Technologies (AICT)}, 
  title={Approach to the Development of Ontology-Driven Language Toolkits Based on Metamodeling}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The information systems are to be conforming to the requirements defined by domain experts. These requirements are formalized as models created with modeling tools. Applying these tools is complicated for domain experts. Domain specific modeling (DSM) with domain specific languages (DSL) reduces the semantic gap. However, the system development complication shifts to the creation of languages and tools for transforming models and code generation. An approach to automating DSL creation and facilitating code generation based on using multifaceted ontology is proposed. The generalized description of the multifaceted ontology is given. Tools of automating generation of new DSL metamodels based on mapping the corresponding domain ontology onto the metamodels of the selected base languages are described. Metamodels of the visual languages, grammars of the target text languages and transformation rules are also included into the ontology. The proposed approach is implemented as a research prototype of the language toolkits. Examples of metamodels and rules described in the ontology, as well as the results of their application are shown. The results of experiments confirmed practical significance of the approach to the ontology-driven language toolkits development.},
  keywords={Visualization;Codes;Prototypes;Metamodeling;Ontologies;Grammar;DSL;domain-specific modeling;domain-specific languages;metamodeling;multifaceted ontology;metamodel generation;model transformation rules},
  doi={10.1109/AICT59525.2023.10313152},
  ISSN={2472-8586},
  month={Oct},}@INPROCEEDINGS{9994917,
  author={Choi, Kyudam and Lee, Yurim and Kim, Cheongwon},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={GCL-GO: A novel sequence-based hierarchy-aware method for protein function prediction}, 
  year={2022},
  volume={},
  number={},
  pages={51-56},
  abstract={Experimental protein functional annotation does not cover rapidly-expanding protein sequences. Sequence-based methods, one of the computational methods, have been developed for extending functional annotations to fast-growing sequence databases. We propose a novel sequence-based hierarchy-aware method, namely GCL-GO. GCL-GO applies a protein language model to represent sequences, applies graph contrastive learning to represent GO terms, and then predicts protein functions by combining these two features. By contrasting the GO graph and semantic features of GO terms, GCL-GO has generalizability and scalability by accurately embedding the features of GO terms while relying less on training data. We also suggest GCL-GO+, which combines a sequence similarity-based method with GCLGO, to improve performance. GCL-GO+ outperforms sequence-based competing methods on both the CAFA3 and the TALE datasets. Furthermore, GCL-GO and GCL-GO+ demonstrate functional generalization and scalability potential by having the best performance on new GO terms or on GO terms annotated infrequently in the training dataset. Our code is available in https://github.com/kch38896/GCL-GO},
  keywords={Proteins;Training;Protein engineering;Annotations;Databases;Scalability;Semantics;protein function prediction;gene ontology;graph constructive learning;protein language model},
  doi={10.1109/BIBM55620.2022.9994917},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10858988,
  author={Fu, Yibin and Ding, Zhaoyun and Xu, Xiaojie},
  booktitle={2024 IEEE 9th International Conference on Data Science in Cyberspace (DSC)}, 
  title={LLM & Bagging for 1-shot Joint IE}, 
  year={2024},
  volume={},
  number={},
  pages={204-208},
  abstract={Domain-specific few-shot information extraction (IE) has always been the difficulty of domain knowledge graph construction, and there is a new solution in this direction after the emergence of large language models (LLM). In this paper, based on previous research, we propose LLM-based 1-shot relation-entity joint IE scheme, and the bagging enhance LLM IE method is proposed to take advantage of the randomness of the LLM output. Against the background of the concept of Internet of Things (IoT) which has received wide attention globally, we selects the IoT interconnective communication as a domain-specific example, crawls the text of the device pages of L3Harris, RockwellCollins as our corpus, selects large language models that differ in the number of parameters and invocation methods to test the proposed joint IE method in relations given by the IoT interconnective communication ontology. The bagging method is tested based on the IE results of GPT-4 Turbo, and there is an improvement of 1-3%, which shows the effectiveness of traditional machine learning methods in LLM. Finally, the results and shortcomings of this study are analyzed.},
  keywords={Training;Large language models;Knowledge graphs;Ontologies;Data science;Information retrieval;Internet of Things;Bagging;Standards;Random forests;Large Language Model (LLM);Information Extraction (IE);bagging;1-shot;Communication},
  doi={10.1109/DSC63484.2024.00034},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10263217,
  author={Wang, Xiao and Liu, Kai and Wang, Chunlei},
  booktitle={2023 IEEE 9th International Conference on Cloud Computing and Intelligent Systems (CCIS)}, 
  title={Knowledge-enhanced Pre-training large language model for depression diagnosis and treatment}, 
  year={2023},
  volume={},
  number={},
  pages={532-536},
  abstract={Depression, a pervasive psychiatric disorder characterized by concealment, dependence on expert judgment, and a notable rate of misdiagnosis, poses a substantial burden on society. To enhance the diagnosis and treatment of depression, this study puts forth a proposition of employing knowledge-enhanced pre-training technology leveraging large language models. By integrating domain knowledge and depression knowledge graph directives, the pre-trained model undergoes optimization. Expert involvement in depression diagnosis and treatment fosters a guided learning process facilitated by expert feedback. Through the application of dialogue therapy, the efficacy of treatment is augmented. This technical approach aims to ameliorate the societal burden by improving the diagnosis and treatment of depressed individuals.},
  keywords={Cloud computing;Computational modeling;Mental disorders;Medical treatment;Knowledge graphs;Depression;Intelligent systems;Depression;Large language model;Knowledge enhancement;Knowledge graph},
  doi={10.1109/CCIS59572.2023.10263217},
  ISSN={2376-595X},
  month={Aug},}@INPROCEEDINGS{10662704,
  author={Sun, Qi and Li, Yahui and Zhou, Chunjie and Tian, Yu-Chu},
  booktitle={2024 43rd Chinese Control Conference (CCC)}, 
  title={Root Cause Analysis for Industrial Process Anomalies through the Integration of Knowledge Graph and Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={6855-6860},
  abstract={Root cause analysis for industrial process anomalies is critical for manufacturing activities. Industrial process alarms can provide crucial information to enable root cause analysis. However, the complex system structure causes a large number of alarms to emerge at the same time. To address this issue, we proposed an approach that utilizes knowledge graphs and large language models to provide comprehensible root cause analysis. Firstly, we extract knowledge such as historical anomalies from catalytic cracking operation manuals to construct an industrial process safety knowledge graph. Then, named entities in each alarm are extracted as keywords to retrieve factual knowledge from the knowledge graph. Finally, factual knowledge will be provided to the large language model as prior knowledge to infer the root cause of anomalies. Experimental results show that the proposed approach can accurately identify the root cause, thereby ensuring the safety of industrial processes.},
  keywords={Root cause analysis;Large language models;Prevention and mitigation;Oils;Time series analysis;Process control;Knowledge graphs;Root cause analysis;Knowledge graph;Large language model;Named entity recognition},
  doi={10.23919/CCC63176.2024.10662704},
  ISSN={1934-1768},
  month={July},}@INPROCEEDINGS{9621874,
  author={Aigo, Kosuke and Tsunakawa, Takashi and Nishida, Masafumi and Nishimura, Masafumi},
  booktitle={2021 IEEE 10th Global Conference on Consumer Electronics (GCCE)}, 
  title={Question Generation using Knowledge Graphs with the T5 Language Model and Masked Self-Attention}, 
  year={2021},
  volume={},
  number={},
  pages={85-87},
  abstract={Question generation is helpful for understanding reading comprehension, spontaneous questioning in chatting systems, and expanding datasets for answering questions. In previous studies, many models have been used to generate questions from contexts, but none was suitable in large-length contexts. To overcome this challenge, we generated questions from an intermediate representation of a context, such as knowledge graphs. In this study, we focused on developing questions using knowledge graphs with the T5 language model. We used the language model to create questions using the knowledge graph and mask the self-attention of the encoder to train the model by explicitly preserving the graph’s structure. As a result of the automatic evaluation, the T5 language model with and without mask was comparable with the bidirectional Graph2Seq model (G2S), known as the QG model, using knowledge graphs. More-over, the masked language model was slightly better than the non-masked model in t5-small on four benchmarks. The code and data are publicly available at https://github.com/Macho000/T5-for-KGQG.},
  keywords={Codes;Conferences;Benchmark testing;Task analysis;Consumer electronics;Context modeling;KG2QG;Question generation;T5 language model},
  doi={10.1109/GCCE53005.2021.9621874},
  ISSN={2378-8143},
  month={Oct},}@INPROCEEDINGS{10590653,
  author={Zhang, Jialei and Cai, Shubin and Jiang, Zhiwei and Xiao, Jian and Ming, Zhong},
  booktitle={2024 10th IEEE International Conference on Intelligent Data and Security (IDS)}, 
  title={FireRobBrain: Planning for a Firefighting Robot using Knowledge Graph and Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={37-41},
  abstract={Firefighting robots play a crucial role in improving fire safety. However, these robots have limitations in understanding their surroundings and adapting to changing situations. In this paper, we propose the “FireRobBrain”, a combination of a knowledge graph and a large language model, to act as the “brain” of firefighting robots. This approach aims to tackle challenges related to planning robots in dynamic environments. The FireRobBrain consists of a KG with a dynamic information base and a relatively static knowledge base. It also includes a prompting module that helps the language model generate suggestions for the robot's reactions. We evaluated the framework using a dataset of 864 samples and discovered that the combination of LLM and KG, facilitated by a well-designed prompt module, significantly improves the quality of answers, particularly for tasks involving specific contexts and structured information. Furthermore, we noted that providing a task scope in the input prefix contributes to a better understanding of the robot's task, resulting in enhanced performance.},
  keywords={Large language models;Prevention and mitigation;Knowledge based systems;Knowledge graphs;Planning;Security;Reliability;Knowledge Graph;Large Language Model;Robot;Planning},
  doi={10.1109/IDS62739.2024.00014},
  ISSN={},
  month={May},}@INPROCEEDINGS{10013619,
  author={Kulagin, Grigory and Ermakov, Ivan and Lyadova, Lyudmila},
  booktitle={2022 IEEE 16th International Conference on Application of Information and Communication Technologies (AICT)}, 
  title={Ontology-Based Development of Domain-Specific Languages via Customizing Base Language}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={The quality of the systems depends on compliance to the domain requirements. High quality is achieved only with involving experts in the relevant fields to the system design as experts. Modern design methods are based on using professional tools and modeling languages. Using these tools are difficult for domain experts. Domain-Specific Languages (DSLs) can be considered as "user interfaces" for experts because they bridge the gap between the domain experts and the software development tools via customizing modeling languages. Usability of DSLs by domain experts is a key factor for their successful adoption. But DSL creation is challenging task. An approach to DSL customization based on using multifaceted ontology is proposed. General scheme of DSL metamodel generation based on multifaceted ontology is described. Examples of created DSLs and models illustrating the applicability of the proposed method are shown. The DSL metamodels were developed and tested in several domains. The results of experiments confirmed practical significance of the ontology-based approach to DSL creation.},
  keywords={Visualization;Prototypes;Ontologies;User interfaces;Software;DSL;Task analysis;domain-specific modeling;DSM;domain-specific language;DSL;metamodel generation;multifaceted ontology;language customization;GalileoSky;algorithm description language},
  doi={10.1109/AICT55583.2022.10013619},
  ISSN={2472-8586},
  month={Oct},}@INPROCEEDINGS{10286646,
  author={Sadirmekova, Zhanna and Sambetbayeva, Madina and Daiyrbayeva, Elmira and Yerimbetova, Aigerim and Altynbekova, Zhanar and Murzakhmetov, Aslanbek},
  booktitle={2023 8th International Conference on Computer Science and Engineering (UBMK)}, 
  title={Constructing the Terminological Core of NLP Ontology}, 
  year={2023},
  volume={},
  number={},
  pages={81-85},
  abstract={The basis of any intellectual resource is a knowledge base, which, based on the basic terms of the field under consideration, builds relationships between them. Therefore, the first task to building multilingual information system using Natural language processing (NLP)in scientific and educational activities will be the development of a multilingual dictionary on modern NLP methods, including terms in Kazakh, English and Russian. For its construction, linguistic models for semantic dictionaries and thesauruses will be used, as well as methods of automatic extraction of terms from the corpus of texts of a given subject area. In this paper, a system of concepts of the NLP domain will be formalized, which will form the terminological core of the NLP ontology. To systematize information and provide support for multilingualism and accessibility, we plan to apply ontological engineering methods to systematize information and build the upper levels of the NLP ontology (its terminological core) using the dictionary of terms obtained at the previous stage. The ontology developed by us can later become the conceptual basis for a multilingual information system used in scientific and educational activities using NLP. This system will provide systematization of all information, convenient navigation on it, integration into a single information space, as well as access to it.},
  keywords={Dictionaries;Navigation;Semantics;Knowledge based systems;Ontologies;Linguistics;Natural language processing;Natural language processing;ontology;conceptual model;scientific and educational information system;ontological design patterns},
  doi={10.1109/UBMK59864.2023.10286646},
  ISSN={2521-1641},
  month={Sep.},}@INPROCEEDINGS{10020417,
  author={Mijalcheva, Viktorija and Davcheva, Ana and Gramatikov, Sasho and Jovanovik, Milos and Trajanov, Dimitar and Stojanov, Riste},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Learning Robust Food Ontology Alignment}, 
  year={2022},
  volume={},
  number={},
  pages={4097-4104},
  abstract={In today’s knowledge society, large number of information systems use many different individual schemes to represent data. Ontologies are a promising approach for formal knowledge representation and their number is growing rapidly. The semantic linking of these ontologies is a necessary prerequisite for establishing interoperability between the large number of services that structure the data with these ontologies. Consequently, the alignment of ontologies becomes a central issue when building a worldwide Semantic Web. There is a need to develop automatic or at least semi-automatic techniques to reduce the burden of manually creating and maintaining alignments. Ontologies are seen as a solution to data heterogeneity on the Web. However, the available ontologies are themselves a source of heterogeneity. On the Web, there are multiple ontologies that refer to the same domain, and with that comes the challenge of a given graph-based system using multiple ontologies whose taxonomy is different, but the semantics are the same. This can be overcome by aligning the ontologies or by finding the correspondence between their components.In this paper, we propose a method for indexing ontologies as a support to a solution for ontology alignment based on a neural network. In this process, for each semantic resource we combine the graph based representations from the RDF2vec model, together with the text representation from the BERT model in order to capture the semantic and structural features. This methodology is evaluated using the FoodOn and OntoFood ontologies, based on the Food Onto Map alignment dataset, which contains 155 unique and validly aligned resources. Using these limited resources, we managed to obtain accuracy of 74% and F1 score of 75% on the test set, which is a promising result that can be further improved in future. Furthermore, the methodology presented in this paper is both robust and ontology-agnostic. It can be applied to any ontology, regardless of the domain.},
  keywords={Training;Semantic Web;Semantics;Neural networks;Taxonomy;Ontologies;Big Data;Ontology Alignment;Natural language processing;Text representation;Embeddings;Data normalization;Data linking},
  doi={10.1109/BigData55660.2022.10020417},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10809842,
  author={Yang, Weijie and Zhang, Chunyu and Zhang, Min and Jiang, Xunjie and Fan, Yanlin and Bi, Zhongbo and Xiong, Jiansheng and Wang, Danshi},
  booktitle={2024 Asia Communications and Photonics Conference (ACP) and International Conference on Information Photonics and Optical Communications (IPOC)}, 
  title={Multi-Modal Knowledge Graph with Large Language Model for Intelligent Fault Management in Optical Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={We propose a multi-modal knowledge graph with large language model for intelligent fault management in optical networks. Demonstrations of two real-world cases validate its capability to utilize multi-modal data and facilitate intelligent reasoning.},
  keywords={Knowledge engineering;Analytical models;Large language models;Knowledge based systems;Asia;Knowledge graphs;Optical fiber networks;Cognition;Photonics;Multi-modal knowledge graph;Large language models;Intelligent fault management;Optical networks},
  doi={10.1109/ACP/IPOC63121.2024.10809842},
  ISSN={2162-1098},
  month={Nov},}@INPROCEEDINGS{10411632,
  author={Huang, Fei and Deng, Yi and Zhang, Chen and Guo, Menghao and Zhan, Kai and Sun, Shanxin and Jiang, Jinling and Sun, Zeyi and Wu, Xindong},
  booktitle={2023 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
  title={KOSA: KO Enhanced Salary Analytics based on Knowledge Graph and LLM Capabilities}, 
  year={2023},
  volume={},
  number={},
  pages={499-505},
  abstract={Knowledge base question answering (KBQA) is designed to respond to natural language inquiries by utilizing factual information, such as entities, relationships, and attributes, derived from a knowledge base (KB). The advent of large language models (LLMs) has significantly boosted the performance of KBQA, owing to their exceptional capabilities in content comprehension and generation. In this paper, we present a Knowledge Ocean enhanced Salary Analytics (KOSA) system based on knowledge graphs and LLMs tailored to employee salary data from a public university. This system encompasses an interactive conversational interface, visualization of knowledge graphs, and advanced data analysis. By employing the framework of knowledge engineering, we enable knowledge graph modeling, Cypher (the query engine of Neo4j) reasoning, and question answering functionalities. Furthermore, machine learning algorithms are integrated to facilitate advanced features, such as salary prediction and allocation.},
  keywords={Machine learning algorithms;Oceans;Knowledge based systems;Knowledge graphs;Question answering (information retrieval);Remuneration;Resource management;KBQA;LLM;Salary distribution;Knowledge Engineering},
  doi={10.1109/ICDMW60847.2023.00071},
  ISSN={2375-9259},
  month={Dec},}@INPROCEEDINGS{9620493,
  author={Lyadova, Lyudmila N. and Sukhov, Alexander O. and Nureev, Marsel R.},
  booktitle={2021 IEEE 15th International Conference on Application of Information and Communication Technologies (AICT)}, 
  title={An Ontology-Based Approach to the Domain Specific Languages Design}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Developing software systems for various domains is a complex task. The quality of the system, corresponding to the domain requirements, can only be achieved via involving the model development of experts in the relevant fields. Traditional design methods based on the using professional tools and modeling languages are difficult for subject matter experts. Using Domain Specific Languages (DSL) have been increasingly gaining attention of developers because DSLs are created to cope with specific domain particularities. However, DSL development consists of several steps to be performed can be hard. Identifying the correct set of elements and constructions of DSL, defining their constraints can be very error-prone. Automation of the new DSLs development is relevant task. The designing of new DSLs should be based on the knowledge of experts, which can be represented using an ontology. An approach to DSM platform development based on using multifaceted ontology to DSL design is proposed. Examples of DSLs and models illustrating the applicability of the proposed methodology are described.},
  keywords={Natural languages;Prototypes;Metamodeling;Ontologies;Tools;Software systems;DSL;domain specific modeling;DSM;domain specific language;DSL;visual language;metamodeling;DSM platform;language toolkits;metamodel generation;multifaceted ontology},
  doi={10.1109/AICT52784.2021.9620493},
  ISSN={2472-8586},
  month={Oct},}@INPROCEEDINGS{10145722,
  author={Bandara, H. M. R. L. and Ranathunga, L.},
  booktitle={2023 3rd International Conference on Advanced Research in Computing (ICARC)}, 
  title={Ontology Based Restaurant Recommendation Approach}, 
  year={2023},
  volume={},
  number={},
  pages={78-83},
  abstract={As the world moves forward, the restaurant industry is rapidly expanding. Customers may never physically evaluate a restaurant based on its services until that customer has practical experience with it. A better recommendation mechanism can always direct the customer to the correct location, resulting in a positive outcome. The paper discusses an approach of a context rich chatbot that can identify the customer’s mode of thinking using a restaurant ontology that suggests relevant restaurants and foods. Most importantly the defined methodology will use a hybrid version of knowledge bases along with a two-way bind to the primary knowledge base. The chatbot will proceed to find relationships in the ontology by tracing concept definitions and properties while feeding information from the database. The main components related to the proposed system are Natural Language Understanding (NLU) pipeline, dialog management module, action server, knowledge query module, and data repository (MongoDB). This mechanism was evaluated through information retrieval measures.},
  keywords={Industries;Databases;Knowledge based systems;Pipelines;Ontologies;Chatbots;Information retrieval;Rasa Framework;Ontology;Database;Knowledge Query Module;Dialog Management Module;Action Server},
  doi={10.1109/ICARC57651.2023.10145722},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10674576,
  author={Kuai, Ssu-Chi and Liu, Yao-Yu and Lin, Ching-Tzu and Liao, Wen-Hwa},
  booktitle={2024 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)}, 
  title={Research of Image Generation Based on Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={47-48},
  abstract={In recent years, text-to-image generation technology has achieved significant breakthroughs, especially with the advancement of Diffusion models. Sometimes the model's lack of knowledge or insufficient understanding of the text prompts to generate some unreasonable images. This paper aims to explore how combining knowledge graphs with text prompts can enhance the image generation process by providing additional relevant knowledge about the text prompts, thereby improving the capability of image generation without retraining the diffusion model.},
  keywords={Deep learning;Accuracy;Image synthesis;Text to image;Knowledge graphs;Diffusion models;Vectors;Knowledge Graph;Diffusion Model;Generative AI;Multimodal Model},
  doi={10.1109/ICCE-Taiwan62264.2024.10674576},
  ISSN={2575-8284},
  month={July},}@INPROCEEDINGS{10825619,
  author={Zhang, Hongzhi and Shafiq, M. Omair},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={From Graph Paths to Natural Language: Enhancing LLM Reasoning for Multi-choice Question-Answering Tasks}, 
  year={2024},
  volume={},
  number={},
  pages={8882-8884},
  abstract={Language models have achieved good results in many tasks. However, there are still some challenges with reasoning due to insufficient knowledge, which leads to poor performance of the models on specific tasks. We proposed a framework based on the existing solution (i.e., GreaseLM), using a large language model to replace some complex modules. Our approach converts the knowledge graph paths into natural language sentences, providing contextual support to enhance the reasoning capability of the large language models. Using the framework based on a large language model is simpler and easier to manage, and it also achieves better accuracy on multi-choice question-answering tasks.},
  keywords={Accuracy;Large language models;Natural languages;Knowledge graphs;Big Data;Cognition;Data models;Language Model;Large Language Model;Knowledge Graph;Question-Answering Task},
  doi={10.1109/BigData62323.2024.10825619},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10466828,
  author={Ding, Ningpei and Mayer, Wolfgang and Geng, Yilin and Duan, Yucong and Feng, Zaiwen},
  booktitle={2023 IEEE International Conference on High Performance Computing & Communications, Data Science & Systems, Smart City & Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)}, 
  title={Generative Semantic Modeling for Structured Data Source with Large Language Model}, 
  year={2023},
  volume={},
  number={},
  pages={1148-1152},
  abstract={The paper introduces a generative semantic model for representing human knowledge in a way that enables computer understanding and reasoning. The current approach to semantic modeling involves mapping between the space of plausible semantic models and the provided data source. However, this approach has limitations, as the score functions used to search for the best candidate semantic model are either trained on a specific integration knowledge graph or rely on manually designed features. To address these limitations, the authors propose a new approach that combines an encoder made with a pre-trained large language model (LLM) with a graph decoder customized to generate semantics. The encoder-decoder system is designed to be trained on knowledge graphs, and the authors introduce an algorithm to generate training samples from the big knowledge graph by decomposing training samples into construction actions using a method similar to the transition system of the Syntax Parser. The proposed method is novel, as it is the first time a generative method has been applied to the semantic modeling task, empowered with an LLM, and trained on knowledge graphs to achieve better performance on standard benchmarks than in past work. In conclusion, the proposed generative semantic model offers a promising new approach to representing and organizing human knowledge in a more generalizable way, using a combination of a pre-trained LLM and a customized graph decoder trained on knowledge graphs. The approach has shown improved performance on standard benchmarks and has the potential to advance the field of semantic modeling.},
  keywords={Training;Computational modeling;Soft sensors;Semantics;Knowledge graphs;Benchmark testing;Data models;Knowledge graph;Large Language Model;Graph Neural Network},
  doi={10.1109/HPCC-DSS-SmartCity-DependSys60770.2023.00164},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10824535,
  author={Yu, Tong and Fu, Wenhui and Dai, Doneming and Zhang, Kunli and Song, Yu},
  booktitle={2024 3rd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE)}, 
  title={Research on Intelligent Diagnosis Integrating Disease Subgraph and Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={557-562},
  abstract={AI-driven diagnostic tools proved to be an effective aid for healthcare professionals in predicting diseases, thereby streamlining healthcare services. In order to improve the accuracy and efficiency in the field of intelligent diagnosis, we used Chinese electronic medical records as the data base, incorporated knowledge graphs as the external knowledge source, and introduced a large language model to reason about electronic medical records. To solve the problems of difficulty in knowledge graph construction and insufficient semantic relationship extraction, we proposed an intelligent diagnosis method that combined disease subgraphs with a large language model (DeLL). The DeLL model constructed prompt templates using the text of the electronic medical record, guided the large language model to generate new descriptions, and combined these with the multi-granularity information in the EMR to obtain enhanced textual representations. Meanwhile, the DeLL model constructed disease subgraphs by utilizing the target diseases and the knowledge graph, and encoded them using graph convolutional neural networks to fuse the disease knowledge representations with the LLM-enhanced text representations for disease prediction. The model was experimentally evaluated on a dataset consisting of multi-label and single-label electronic medical records and achieved F1 micro scores of 85.01% and 93.19%, respectively, representing improvements of 3.20% and 1.80% over the optimal baseline results.},
  keywords={Large language models;Computational modeling;Semantics;Knowledge graphs;Medical services;Data models;Numerical models;Electronic medical records;Diseases;Software engineering;Intelligent Diagnosis;Knowledge Enhancement;Knowledge Graph;Large Language Model},
  doi={10.1109/CBASE64041.2024.10824535},
  ISSN={},
  month={Oct},}@ARTICLE{10313282,
  author={Razouk, Houssam and Liu, Xing Lan and Kern, Roman},
  journal={IEEE Access}, 
  title={Improving FMEA Comprehensibility via Common-Sense Knowledge Graph Completion Techniques}, 
  year={2023},
  volume={11},
  number={},
  pages={127974-127986},
  abstract={The Failure Mode Effect Analysis process (FMEA) is widely used in industry for risk assessment, as it effectively captures and documents domain-specific knowledge. This process is mainly concerned with causal domain knowledge. In practical applications, FMEAs encounter challenges in terms of comprehensibility, particularly related to inadequate coverage of listed failure modes and their corresponding effects and causes. This can be attributed to the limitations of traditional brainstorming approaches typically employed in the FMEA process. Depending on the size and diversity in terms of disciplines of the team conducting the analysis, these approaches may not adequately capture a comprehensive range of failure modes, leading to gaps in coverage. To this end, methods for improving FMEA knowledge comprehensibility are highly needed. A potential approach to address this gap is rooted in recent advances in common-sense knowledge graph completion, which have demonstrated the effectiveness of text-aware graph embedding techniques. However, the applicability of such methods in an industrial setting is limited. This paper addresses this issue on FMEA documents in an industrial environment. Here, the application of common-sense knowledge graph completion methods on FMEA documents from semiconductor manufacturing is studied. These methods achieve over 20% MRR on the test set and 70% of the top 10 predictions were manually assessed to be plausible by domain experts. Based on the evaluation, this paper confirms that text-aware knowledge graph embedding for common-sense knowledge graph completion are more effective than structure-only knowledge graph embedding for improving FMEA knowledge comprehensibility. Additionally we found that language model in domain fine-tuning is beneficial for extracting more meaningful embedding, thus improving the overall model performance.},
  keywords={Commonsense reasoning;Knowledge graphs;Knowledge based systems;Particle swarm optimization;Heuristic algorithms;Risk management;Semiconductor device manufacture;Natural language processing;Natural language processing;common-sense knowledge;failure mode effect analysis;FMEA;semiconductor manufacturing;knowledge graph completion},
  doi={10.1109/ACCESS.2023.3331585},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9070680,
  author={Han, Seung-Ho and Choi, Ho-Jin},
  booktitle={2020 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Domain-Specific Image Caption Generator with Semantic Ontology}, 
  year={2020},
  volume={},
  number={},
  pages={526-530},
  abstract={Image captioning is the task of generating textual descriptions of a given image, requiring techniques of computer vision and natural language processing. Recent models have utilized deep learning techniques for this task to gain performance improvement. However, these models can neither fully use information included in a given image such as object and attribute, nor generate a domain-specific caption because existing methods use open dataset such as MSCOCO which include general images. To overcome these limitations, this paper proposes a domain-specific image caption generator, which generates a caption based on attention mechanism with object and attribute information, and reconstruct a generate caption using a semantic ontology to provide natural language description for given specific-domain. To show the effectiveness of the proposed model, we evaluate the image caption generator with a dataset, MSCOCO, quantitatively and qualitatively.},
  keywords={Semantics;Visualization;Generators;Ontologies;Feature extraction;Image reconstruction;Machine learning;image captioning;attention model;attribute predictionm;domain-specific ontology},
  doi={10.1109/BigComp48618.2020.00-12},
  ISSN={2375-9356},
  month={Feb},}@INPROCEEDINGS{9031044,
  author={Vijayalakshmi, H C and Dixit, Bhavana S},
  booktitle={2019 4th International Conference on Computational Systems and Information Technology for Sustainable Solution (CSITSS)}, 
  title={Information Retrieval in Kannada using Ontology}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  abstract={As Internet technology has become a part of the lifestyle of the common man, research efforts are extensively made in the fields of Natural Language Processing (NLP) and Information Retrieval. Studying regional languages for developing the system to store, retrieve, extract the information from the database has gained lots of prominence nowadays. Case studies show that Ontological Information Retrieval has many advantages over keyword-based approach. In this paper we have focused on the general architecture of ontology-based Information Retrieval used for Kannada.},
  keywords={Ontologies;Information retrieval;Databases;Semantics;Structured Query Language;Natural language processing;Ontology;Information Retrieval;Kannada;NLP},
  doi={10.1109/CSITSS47250.2019.9031044},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10849391,
  author={Saini, Anmol and Ethier, Jeffrey G. and Shimizu, Cogan},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={An Ontology for Conversations with Virtual Research Assistants}, 
  year={2024},
  volume={},
  number={},
  pages={181-186},
  abstract={Conversational artificial intelligence has expanded rapidly in recent years, especially with the growth of large language models (LLMs). Its incorporation in scientific research in the form of research assistants has also become more common-place but remains limited in some capacities, such as in the realm of polymer science. The limitations of LLMs, especially in terms of domain knowledge, warrant the need for other tools, such as knowledge graphs (KGs), to better guide conversations. While such conversational models have been developed in the past, they are generally restricted to particular domains and lack the ability to integrate semantics from various kinds of conversations. Thus, we make progress toward the construction of a universal conversational model that has a focus on the materials domain by combining aspects of existing models. We aim to implement it in such a way that renders it amenable to modifications and usable in a variety of situations. We posit that this model will be adopted and extended by others seeking to accomplish a similar goal in the future.},
  keywords={Adaptation models;Limiting;Conversational artificial intelligence;Large language models;Semantics;Natural languages;Oral communication;Knowledge graphs;Ontologies;Polymers;artificial intelligence;conversational model;knowledge graph;large language model;ontology;ontology design pattern;polymer science},
  doi={10.1109/ICTAI62512.2024.00034},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{8257822,
  author={Jayawardana, Vindula and Lakmal, Dimuthu and de Silva, Nisansa and Perera, Amal Shehan and Sugathadasa, Keet and Ayesha, Buddhi and Perera, Madhavi},
  booktitle={2017 Seventeenth International Conference on Advances in ICT for Emerging Regions (ICTer)}, 
  title={Semi-supervised instance population of an ontology using word vector embedding}, 
  year={2017},
  volume={},
  number={},
  pages={1-7},
  abstract={In many modern-day systems such as information extraction and knowledge management agents, ontologies play a vital role in maintaining the concept hierarchies of the selected domain. However, ontology population has become a problematic process due to its nature of heavy coupling with manual human intervention. With the use of word embeddings in the field of natural language processing, it became a popular topic due to its ability to cope up with semantic sensitivity. Hence, in this study we propose a novel way of semi-supervised ontology population through word embeddings as the basis. We built several models including traditional benchmark models and new types of models which are based on word embeddings. Finally, we ensemble them together to come up with a synergistic model with better accuracy. We demonstrate that our ensemble model can outperform the individual models.},
  keywords={Ontologies;Sociology;Statistics;Law;Natural language processing;Semantics;Ontology;Ontology Population;Word Embeddings;word2vec},
  doi={10.1109/ICTER.2017.8257822},
  ISSN={2472-7598},
  month={Sep.},}@INPROCEEDINGS{10135342,
  author={Chen, Jiabao and Fan, Yongquan},
  booktitle={2023 4th International Conference on Computer Engineering and Application (ICCEA)}, 
  title={Improving temporal question answering using temporal knowledge graph embedding}, 
  year={2023},
  volume={},
  number={},
  pages={570-575},
  abstract={Knowledge graph question answering is an important research direction for question answering tasks. In recent years, there has been an increasing amount of research on knowledge graph question answering, but temporal knowledge graph question answering is still a relatively unexplored area. In question answering tasks, many natural language questions have explicit or implicit temporal constraints, and most existing research methods lack the temporal awareness to deal with complex temporal questions. To address these challenges, this paper proposes a question answering model based on temporal knowledge graph embedding (TKGETQA). The model uses TKG embeddings to root a question in the entities, relations and time horizons it references, and perceives temporal information from the question to improve the accuracy of answer prediction. Experiments on the dataset CronQuestions in this paper show that the TKGETQA model exhibits better results compared to existing temporal knowledge graph question answering approaches.},
  keywords={Knowledge engineering;Natural languages;Knowledge graphs;Predictive models;Question answering (information retrieval);Data mining;Task analysis;temporal knowledge graph;temporal knowledge graph embedding;question answering},
  doi={10.1109/ICCEA58433.2023.10135342},
  ISSN={2159-1288},
  month={April},}@INPROCEEDINGS{10568296,
  author={Tona, Claudia and Juárez-Ramírez, Reyes and Jiménez, Samantha and Murillo-Muñoz, Fernanda},
  booktitle={2023 11th International Conference in Software Engineering Research and Innovation (CONISOFT)}, 
  title={Q-Story: An Ontology-Based on Quality of User Stories in Scrum. A Quantitative Assessment}, 
  year={2023},
  volume={},
  number={},
  pages={55-64},
  abstract={Q-Story ontology was created utilizing the Methontology approach and further represented through the Meta Object Facility (MOF) and Unified Modeling Language (UML). Because aspects such as their structure, level of granularity, and comprehensibility hold considerable signifi-cance in ensuring a favorable project execution. That is, the quality of user stories significantly impacts the outcome of a software project, influencing its success or failure. Therefore, we performed a quantitative evaluation using the OntoQA method, resulting in a relationship richness value of 0.95, an attribute richness of 4.00, and an inheritance richness of 1.26. The outcome of this work will contribute to developing an ontology that can effectively create user stories with quality. Furthermore, it will serve as a valuable guide for development teams, aiding them in the creation, analysis, and development processes of user stories.},
  keywords={Technological innovation;Unified modeling language;Ontologies;Software;Software engineering;Ontology;Quantitative Assessment;Quality Met-rics;Software Engineering;User Story;Ontology Quality Evaluation},
  doi={10.1109/CONISOFT58849.2023.00017},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10677802,
  author={Gouidis, Filippos and Papantoniou, Katerina and Papoutsakis, Konstantinos and Patkos, Theodore and Argyros, Antonis and Plexousakis, Dimitris},
  booktitle={2024 14th International Conference on Pattern Recognition Systems (ICPRS)}, 
  title={LLM-aided Knowledge Graph construction for Zero-Shot Visual Object State Classification}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={The problem of classifying the states of objects using visual information holds great importance in both applied and theoretical contexts. This work focuses on the special case of Zero-shot Object-Agnostic State Classification (ZS-OaSC). To tackle this problem, we introduce an innovative strategy that capitalizes on the capabilities of Graph Neural Networks to learn to project semantic embeddings into visual space and on the potential of Large Language Models (LLMs) to provide rich content for constructing Knowledge Graphs (KGs). Through a comprehensive ablation study, we explore the synergies between LLMs and KGs, uncovering critical insights about their integration in the context of the ZS-OSC problem. Our proposed methodology is rigorously evaluated against current state-of-the-art (SoA) methods, demonstrating superior performance in various image datasets.},
  keywords={Visualization;Large language models;Semantics;Knowledge graphs;Graph neural networks;Pattern recognition},
  doi={10.1109/ICPRS62101.2024.10677802},
  ISSN={},
  month={July},}@INPROCEEDINGS{9924844,
  author={Ramli, Inigo and Krisnadhi, Adila Alfa and Prasojo, Radityo Eko},
  booktitle={2022 7th International Workshop on Big Data and Information Security (IWBIS)}, 
  title={IndoKEPLER, IndoWiki, and IndoLAMA: A Knowledge-enhanced Language Model, Dataset, and Benchmark for the Indonesian Language}, 
  year={2022},
  volume={},
  number={},
  pages={19-26},
  abstract={Pretrained language models posses an ability to learn the structural representation of a natural language by processing unstructured textual data. However, the current language model design lacks the ability to learn factual knowledge from knowledge graphs. Several attempts have been made to address this issue, such as the development of KEPLER. KEPLER combines the BERT language model and TransE knowledge embedding method to achieve a language model that can incorporate knowledge graphs as training data. Unfortunately, such knowledge enhanced language model is not yet available for the Indonesian language. In this experiment, we propose IndoKEPLER: a language model trained usingWikipedia Bahasa Indonesia andWikidata. We also create a new knowledge probing benchmark named IndoLAMA to test the ability of a language model to recall factual knowledge. The benchmark is based on LAMA, which is designed to test the suitability of our language model to be used as a knowledge base. IndoLAMA tests a language model by giving cloze style question and compare the prediction of the model to the factually correct answer. This experiment shows that IndoKEPLER increases the ability of a normal DistilBERT model to recall factual knowledge by 0.8%. Moreover, the most significant increase happens when dealing with many-to-one relationships, where IndoKEPLER outperforms it’s original text encoder model by 3%.},
  keywords={Conferences;Knowledge based systems;Bit error rate;Training data;Information security;Benchmark testing;Predictive models;Language model;knowledge embedding;knowledge graph;natural language processing;Indonesian language},
  doi={10.1109/IWBIS56557.2022.9924844},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10391548,
  author={Taye, Mohammad Mustafa and Abulail, Rawan and Al-Oudat, Mohammad},
  booktitle={2023 7th International Symposium on Innovative Approaches in Smart Technologies (ISAS)}, 
  title={An Ontology Learning Framework for unstructured Arabic Text}, 
  year={2023},
  volume={},
  number={},
  pages={1-12},
  abstract={Ontologies are widely regarded as valuable sources of semantics and interoperability in all artificially intelligent systems. Due to the rapid growth of unstructured data on the web, studying how to automatically get ontology from unstructured text is important. Therefore, ontology learning (OL) is an important process in the business world. It involves finding and extracting concepts from the text so that these concepts can be used for things such as information retrieval. Unfortunately, learning ontology is not easy for some reasons, and there has not been much research on how to automatically learn a domain-specific ontology from data.Ontology Studying Arabic text is not as developed as learning Latin text. There is almost no automated support for using Arabic literary knowledge in semantically enabled systems. Machine learning (ML) has proven beneficial in numerous fields, including text mining. By employing neural language models such as AraBERT, it is possible to obtain word embeddings as distributed word representations from textual input using machine learning. However, the application of machine learning to aid the development of Arabic ontology is largely unexplored. This research examines the performance of AraBERT for ontology learning tasks in Arabic. Early performance results as an application of Arabic ontology learning are promising. In this research, we provide a method for populating an existing ontology with instance information extracted from the input natural language text. This prototype has achieved an information extraction accuracy of 91%.},
  keywords={Text mining;Semantics;Natural languages;Prototypes;Machine learning;Ontologies;Information retrieval;Arabic Ontology;Natural language Processing (NLP);Ontology;Ontology Learning (OL);Semantic Web;semantic representation},
  doi={10.1109/ISAS60782.2023.10391548},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10386124,
  author={Lee, Jeongbin and Kim, Kunyoung and Sohn, Mye and Kim, Jongmo},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={GCN-based Explainable Recommendation using a Knowledge Graph and a Language Model}, 
  year={2023},
  volume={},
  number={},
  pages={2930-2936},
  abstract={In this paper, we propose a novel graph convolutional network (GCN)-based recommendation method using both knowledge graph (KG) and review texts to solve the cold start problem and provide explainability. In our model, GCN-based collaborative filtering (CF) is parallelly performed on the user-item interaction graph and the KG to utilize the items’ additional information in the recommendation process. Also, we use a pretrained language model to generate embeddings from the user reviews and utilize them in the GCN embedding propagation process to reflect the users’ subjective sentiment and opinion. After the recommendation is performed, we generate the paths from the target user to recommended items by using the KG and embeddings from review texts. To prove the superiority of the proposed method, we conduct the experiment by comparing recommendation performance with baseline models. In the experiment, the proposed method outperformed the other models.},
  keywords={Collaborative filtering;Knowledge graphs;Big Data;Data models;Convolutional neural networks;recommender system;collaborative filtering;graph convolutional network;knowledge graph;language model},
  doi={10.1109/BigData59044.2023.10386124},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10580566,
  author={Li, Wenyue and Phyu, Sapae and Liu, Qin and Du, Bowen and Zhang, Junyan and Zhu, Hongming},
  booktitle={2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
  title={RSTIE-KGC: A Relation Sensitive Textual Information Enhanced Knowledge Graph Completion Model}, 
  year={2024},
  volume={},
  number={},
  pages={2991-2998},
  abstract={Nowadays, many knowledge graph completion models are proposed to assist the automatic construction of large knowledge graphs. While knowledge graph embedding models and textual information enhanced models are two tendencies in this area, they both suffer from some shortcomings, for example, relying too much on one single modal information may cause the limitation of performance. However, only a few works attempt to integrate multiple modalities. Besides, we found that the semantic similarity between head and tail entities correlates to the enhancing effect of textual information, and the semantic similarity is also related to relation types. We call this phenomenon relational sensitivity. To address these issues, we propose a relation sensitive textual information enhanced knowledge graph completion model (RSTIE-KGC). In our work, by integrating pre-trained language models (PLM) with knowledge graph embedding models, we fuse structural information and textual information, taking advantage of both topological and semantic features. Instead of finetuning the whole PLMs as existing models do, we choose a more efficient way, using frozen PLMs followed by an adapter to fit the text embeddings to our tasks, so that we can reduce training costs and enlarge the scale of negative samples, which maintains accuracy and effectiveness of our model. Based on the relational sensitivity, we propose our RelRank block, which selectively enhances textual information by calculating and filtering mean rank proportion (MRP) score for each relation type, thereby making better use of beneficial semantic information and reducing the noise and redundancy caused by textual information. The prediction process makes more refined use of textual information, as a result improving the accuracy of the model in the link prediction task. We conducted link prediction experiments on two real-world datasets. On FB15k-237, our model outperformed the current state-of-art textual information enhanced models in both MRR and Hit@k metrics. On WN18RR, our model also showed a stable prediction performance, and the results of both MRR and Hits@1 metrics were better than the most textual information enhanced models.},
  keywords={Training;Adaptation models;Sensitivity;Accuracy;Computational modeling;Semantics;Noise;Knowledge graph;Link prediction;pre-trained language model;Multi modality},
  doi={10.1109/CSCWD61410.2024.10580566},
  ISSN={2768-1904},
  month={May},}@INPROCEEDINGS{10415793,
  author={Lu, Yi-Hong and Wang, Chang-Dong and Lai, Pei-Yuan and Lai, Jian-Huang},
  booktitle={2023 IEEE International Conference on Data Mining (ICDM)}, 
  title={PKAT: Pre-training in Collaborative Knowledge Graph Attention Network for Recommendation}, 
  year={2023},
  volume={},
  number={},
  pages={448-457},
  abstract={With the rapid growth of online platforms and the abundance of available information, personalized recommender systems have become essential for assisting users in discovering relevant and interesting content. Among the various methods, knowledge-aware recommendation model has achieved notable success by leveraging the rich semantic information encoded in knowledge graphs. However, it overlooks the fact that users’ historical click sequences can better reflect their preferences within a period of time, thus imposing certain limitations on the recommendation performance. On the other hand, the application of pre-trained language models in recommender systems has demonstrated increasingly significant potential, as they can capture sequential patterns and dependencies within users’ historical click sequences and effectively capture contextual information in user-item interactions. To this end, we propose a hybrid recommendation model that leverages Pre-training in the collaborative Knowledge graph Attention neTwork (PKAT), to extract both the high-order connectivity information in collaborative knowledge graphs and the contextual information in users’ historical click sequences captured by Bidirectional Encoder Representations from Transformers (BERT). The collaborative knowledge graph attention network enables the model to effectively capture the intricate relationships between users, items, and knowledge entities, thus enhancing the representation learning process. Furthermore, what sets PKAT apart from other state-of-the-art knowledge-aware recommendation methods is the incorporation of the BERT language model. This integration allows PKAT to capture the contextual sequence information of user behavior, enabling it to generate more accurate and personalized recommendations. Extensive experiments are conducted on multiple benchmark datasets. And the results demonstrate that our PKAT model outperforms several state-of-the-art baselines.},
  keywords={Knowledge engineering;Collaboration;Knowledge graphs;Encoding;Data mining;Recommender systems;Context modeling;Recommendation;Collaborative knowledge graph;Attention mechanism;Pre-trained language model},
  doi={10.1109/ICDM58522.2023.00054},
  ISSN={2374-8486},
  month={Dec},}@INPROCEEDINGS{10859004,
  author={Lai, Jiang and Conghui, Zheng and Xiaohan, Zhang and Fuhui, Sun and Xiaoyan, Wang and Li, Pan},
  booktitle={2024 IEEE 9th International Conference on Data Science in Cyberspace (DSC)}, 
  title={Leveraging LLM based Retrieval-Augmented Generation for Legal Knowledge Graph Completion}, 
  year={2024},
  volume={},
  number={},
  pages={196-203},
  abstract={Aiming at filling missing triple structures, Knowledge Graph Completion (KGC) is a crucial task in knowledge graph reasoning. Especially in the judicial domain, it plays a significant role not only in enhancing the completeness of legal knowledge graphs but also in improving the accuracy of legal supervisory inference. Due to extensive legal expertise, complex entity relationships, and strict requirements for interpretable reasoning, traditional methods, such as embedding-based methods and those that utilize pre-trained language models (e.g., BERT), often fail to delve into deep semantic information or overlook the interconnections among triples, which leads to significant shortcomings in generalizability and capabilities in practical applications. Motivated by the great generation power of Large Language Models (LLMs), we propose a legal knowledge graph completion model based on Retrieval Augmented Generation (RAG), which we have named RA-KG-LLM. This model effectively combines the generative capabilities of LLMs with retrieval-augmented technology to enhance the semantic information and interrelations mining within knowledge graphs. The retrieval framework within our model utilizes text embeddings and vector similarity matching to provide the LLM with relevant triples, while fine-tuning techniques are employed to infuse the semantic information from the knowledge graph into the LLM. Extensive experiments on five real datasets demonstrate the effectiveness and potentiality in the judicial field of the proposed model. Specifically, on the legal domain dataset Cail2022, it achieves better Hits@1 score in the relation prediction task than the state-of-the-art related works.},
  keywords={Law;Large language models;Retrieval augmented generation;Semantics;Knowledge graphs;Predictive models;Data models;Cognition;Vectors;Filling;Large Language Models;Judicial Knowledge Graph;Knowledge Graph Completion;Retrieval-Augmented Generation},
  doi={10.1109/DSC63484.2024.00033},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10826002,
  author={Bharti, Suman and Lo, Dan Chia-Tien and Shi, Yong},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Enhancing Contextual Understanding in Knowledge Graphs: Integration of Quantum Natural Language Processing with Neo4j LLM Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={8628-8630},
  abstract={Traditional Knowledge Graphs (KGs), such as Neo4j, face challenges in managing high-dimensional relationships and capturing semantic nuances due to their deterministic nature. Quantum Natural Language Processing (QNLP) introduces probabilistic reasoning into the KG context. This integration leverages quantum principles, such as superposition, which allows relationships to exist in multiple states simultaneously, and entanglement, where the state of one entity dynamically influences the state of another. This quantum-based probabilistic reasoning provides a richer, more flexible representation of connections, moving beyond binary relationships to model the nuances and variability of real-world interactions. Our research demonstrates that QNLP enhances Neo4j’s ability to analyze context-rich data, improving tasks like entity extraction and knowledge inference. By modeling relationship states probabilistically, QNLP addresses limitations in traditional methods, providing nuanced insights and enabling more advanced, context-aware NLP applications.},
  keywords={Quantum computing;Quantum entanglement;Semantics;Knowledge graphs;Predictive models;Probabilistic logic;Natural language processing;Cognition;Planning;Context modeling;Neo4j;QNLP;LLM;KGs;NLP},
  doi={10.1109/BigData62323.2024.10826002},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10295846,
  author={Wang, Huan and Guo, Huifeng and Mao, Zehui and Li, Haibin and Qu, Qiang and Yang, Yulei},
  booktitle={2023 CAA Symposium on Fault Detection, Supervision and Safety for Technical Processes (SAFEPROCESS)}, 
  title={Bidirectional Mapping RTE for Fault Knowledge Graph Construction}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In this paper, Bidirectional Mapping Relation Triple Extraction (BMRTE) is proposed to address the challenges of numerous overlapping triples and centralized knowledge in fault maintenance texts, enabling the construction of a fault knowledge graph. The pre-trained language model BERT is employed in BMRTE for the generation of an initial vector representation for each token in the texts. A binary tagging framework is used to identify base entities that may be related in the text on the basis of encoding tokens. To extract entity pairs from the text effectively, the bidirectional mapping framework is designed to map the base entities to their associated entities. Finally, multiple relation classification matrices are used to identify entity pairs and determine the relation triple. Our proposed model outperforms the compared baselines, as evidenced by the experimental results obtained from both a widely used public dataset and our labeled fault maintenance dataset.},
  keywords={Fault diagnosis;Fault detection;Knowledge graphs;Maintenance engineering;Tagging;Encoding;Safety;knowledge graph construction;relation triple extraction;overlapping relations;intelligent fault diagnosis},
  doi={10.1109/SAFEPROCESS58597.2023.10295846},
  ISSN={},
  month={Sep.},}@ARTICLE{10064113,
  author={Cui, Zhaojian and Yuan, Zhenming and Wu, Yingfei and Sun, Xiaoyan and Yu, Kai},
  journal={IEEE Access}, 
  title={Intelligent Recommendation for Departments Based on Medical Knowledge Graph}, 
  year={2023},
  volume={11},
  number={},
  pages={25372-25385},
  abstract={The clinical sub-speciality departments are increasing. It is usually difficult for patients without medical education to choose a suitable department when they need to make an online registration with doctor. A novel approach is presented for departments recommendation in this paper. The ICD codes and symptoms of medical health data are used to describe the patients’ characteristics, diseases, and departments in the quantitative relation. The knowledge graph is built with the relations to automatically recommend departments. The MacBERT-BiLSTM-CRF medical entity recognition model (MacNER) is proposed to identify the patient’s symptoms, parts, treatment methods and drug entities to structure a knowledge graph. The knowledge graph is used to provide knowledge for the intelligent department recommendation. The identified entities are mapped to nodes in the graph with the entity mapping method. Finally, an algorithm named Feature Rate Multiply Converse Disease Rate (FRMCDR) is proposed. The most appropriate department can be recommended by fusing the patient’s chief complaint and past medical history. The experiment shows that our method obtained an 88.77% precision rate in the recommendation.},
  keywords={Knowledge graphs;Hidden Markov models;Diseases;Task analysis;Hospitals;Medical diagnostic imaging;Drugs;Intelligent recommendation;departments recommendation;knowledge graph;medical entity recognition},
  doi={10.1109/ACCESS.2023.3254303},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10711065,
  author={Reif, Jonathan and Jeleniewski, Tom and Gill, Milapji Singh and Gehlhoff, Felix and Fay, Alexander},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Chatbot-Based Ontology Interaction Using Large Language Models and Domain-Specific Standards}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={The following contribution introduces a concept that employs Large Language Models (LLMs) and a chatbot interface to enhance SPARQL query generation for ontologies, thereby facilitating intuitive access to formalized knowledge. Utilizing natural language inputs, the system converts user inquiries into accurate SPARQL queries that strictly query the factual content of the ontology, effectively preventing misinformation or fabrication by the LLM. To enhance the quality and precision of outcomes, additional textual information from established domain-specific standards is integrated into the ontology for precise descriptions of its concepts and relationships. An experimental study assesses the accuracy of generated SPARQL queries, revealing significant benefits of using LLMs for querying ontologies and highlighting areas for future research.},
  keywords={Fabrication;Accuracy;Large language models;Semantics;Natural languages;Ontologies;Chatbots;Fake news;Standards;Manufacturing automation;Semantic Web;Ontologies;Large Language Models;Cyber-Physical Systems;Industry 4.0},
  doi={10.1109/ETFA61755.2024.10711065},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10705235,
  author={Baddour, Moussa and Paquelet, Stéphane and Rollier, Paul and De Tayrac, Marie and Dameron, Olivier and Labbe, Thomas},
  booktitle={2024 IEEE 12th International Conference on Intelligent Systems (IS)}, 
  title={Phenotypes Extraction from Text: Analysis and Perspective in the LLM Era}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Collecting the relevant list of patient phenotypes, known as deep phenotyping, can significantly improve the final diagnosis. As textual clinical reports are the richest source of phenotypes information, their automatic extraction is a critical task. The main challenges of this Information Extraction (IE) task are to identify precisely the text spans related to a phenotype and to link them unequivocally to referenced entities from a source such as the Human Phenotype Ontology (HPO). Recently, Language Models (LMs) have been the most suc-cessful approach for extracting phenotypes from clinical reports. Solutions such as PhenoBERT, relying on BERT or GPT, have shown promising results when applied to datasets built on the hypothesis that most phenotypes are explicitly mentioned in the text. However, this assumption is not always true in medical genetics. Hence, although the LMs carry powerful semantic abilities, their contributions are not clear compared to syntactic string-matching steps that are used within the current pipelines. The goal of this study is to improve phenotype extraction from clinical notes related to genetic diseases. Our contributions are threefold: First, we provide a clear definition of the phenotype extraction task from free text, along with a high-level overview of the involved functions. Second, we conduct an in-depth analysis of PhenoBERT, one of the best existing solutions, to evaluate the proportion of phenotypes predicted with simple string-matching. Third, we demonstrate how utilizing and incorporating large language models (LLMs) for span detection step can improve performance especially with implicit phenotypes. In addition, this experiment revealed that the annotations of existing dataset are not exhaustive, and that LLM can identify relevant spans missed by human labelers.},
  keywords={Phenotypes;Annotations;Large language models;Semantics;Detectors;Syntactics;Ontologies;Data mining;Intelligent systems;Medical diagnostic imaging;phenotype;genetic;entity linking;phenoBERT;LLM;embed dings},
  doi={10.1109/IS61756.2024.10705235},
  ISSN={2767-9802},
  month={Aug},}@INPROCEEDINGS{10825937,
  author={Russo, Diego and Orlando, Gian Marco and Romano, Antonio and Riccio, Giuseppe and Gatta, Valerio La and Postiglione, Marco and Moscato, Vincenzo},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Scaling LLM-Based Knowledge Graph Generation: A Case Study of Italian Geopolitical News}, 
  year={2024},
  volume={},
  number={},
  pages={3494-3497},
  abstract={Geopolitical news provides vast amounts of information essential for understanding international relations and political events. However, organizing this information into a coherent, structured format poses challenges due to the complexity and dynamic nature of the domain. This paper introduces a scalable system leveraging Large Language Models to build continuously updated Knowledge Graphs from Italian geopolitical news. The system features a modular architecture, including a Collector Node for scalable article extraction, a Redis-based reliable queue to manage large-scale data ingestion, and a Named Entity Recognition/Relation Extraction Engine to standardize entity-relation triples. The framework addresses key challenges, such as continuous updating and hallucination mitigation, ensuring the reliability of the graph. Our evaluations demonstrate significant improvements in scalability, uniformity of extracted triples, and graph accuracy, making this architecture particularly suitable for real-time geopolitical analysis.},
  keywords={Scalability;Prevention and mitigation;Large language models;Knowledge graphs;International relations;Feature extraction;Real-time systems;Data mining;Reliability;Engines;Knowledge Graph (KG);Retrieval-Augmented Generation (RAG);Named Entity Recognition (NER);Relation Extraction (RE);Large Language Models (LLMs)},
  doi={10.1109/BigData62323.2024.10825937},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10068451,
  author={Bhana, Nimesh and van Zyl, Terence L.},
  booktitle={2022 9th International Conference on Soft Computing & Machine Intelligence (ISCMI)}, 
  title={Knowledge Graph Fusion for Language Model Fine-Tuning}, 
  year={2022},
  volume={},
  number={},
  pages={167-172},
  abstract={Language Models such as BERT (Bidirectional Encoder Representations from Transformers) have grown in popularity due to their ability to be pre-trained and perform robustly on a wide range of Natural Language Processing tasks. Often seen as an evolution over traditional word embedding techniques, they can produce semantic representations of text, useful for tasks such as semantic similarity. However, state-of-the-art models often have high computational requirements and lack global context or domain knowledge which is required for complete language understanding. To address these limitations, we investigate the benefits of knowledge incorporation into the fine-tuning stages of BERT. An existing K-BERT model, which enriches sentences with triplets from a Knowledge Graph, is adapted for the English language and extended to inject contextually relevant information into sentences. As a side-effect, changes made to K-BERT for accommodating the English language also extend to other word-based languages. Experiments conducted indicate that injected knowledge introduces noise. We see statistically significant improvements for knowledge-driven tasks when this noise is minimised. We show evidence that, given the appropriate task, modest injection with relevant, high-quality knowledge is most performant.},
  keywords={Adaptation models;Computational modeling;Semantics;Bit error rate;Knowledge graphs;Transformers;Natural language processing;Language Model;BERT;Knowledge Graph},
  doi={10.1109/ISCMI56532.2022.10068451},
  ISSN={2640-0146},
  month={Nov},}@INPROCEEDINGS{9727616,
  author={Cheng, Zheng and Wu, Jiaju and Ji, Bin and Liu, Huijun},
  booktitle={2021 China Automation Congress (CAC)}, 
  title={Pre-trained Language Model based Medical Named Entity Recognition}, 
  year={2021},
  volume={},
  number={},
  pages={4337-4341},
  abstract={Medical named entity recognition is an important part of structuring Chinese electronic medical records and construction of medical knowledge graph. The CCKS2019 conference organized a medical named entity recognition evaluation task to extract six types of medical entities from unstructured Chinese electronic medical records. Based on the data set of this evaluation task, pre-trained language model based entity recognition approaches are studied. First, select the BiLSTM-CRF model based on random initialized word embedding as the baseline system; secondly, apply word2vec to the baseline system; thirdly, apply ELMo to the baseline system. Experimental results show that the pre-trained language model is comparable to the best approach of this evaluation task, and the context-related pre-trained language model performs better.},
  keywords={Automation;Data models;Task analysis;Electronic medical records;Context modeling;BiLSTM-CRF;ELMo;pre-trained language model;Chinese electronic medical record;named entity recognition},
  doi={10.1109/CAC53003.2021.9727616},
  ISSN={2688-0938},
  month={Oct},}@INPROCEEDINGS{10404356,
  author={Li, Fan and Tian, Yingjie and Su, Yun and Guo, Naiwang and Gao, Jun and Pan, Haiyan},
  booktitle={2023 3rd International Conference on Intelligent Power and Systems (ICIPS)}, 
  title={Construction and Application of Power Grid Fault Handling Knowledge Graph}, 
  year={2023},
  volume={},
  number={},
  pages={431-436},
  abstract={The fault handling emergency plan of power grid has great instructive significance for the quick and proper emergency disposal when the failures or accidents occur. To solve the problem of poor application effectiveness of the current fault handling emergency plan, a new method to construct the knowledge graph for the fault handling based on the BERT-BiLSTM-CRF model is proposed. In response to the problem of insufficient data available for deep learning model training and high data labeling costs in the power grid, the knowledge extraction module utilizes Bert pre-training method to construct the entity recognition model to improve the performance. The experimental results show that F1-score reaches 86.75% and the accuracy rate reaches 95.81%. Finally, the Neo4j graph database is adopted for highly visual management of the knowledge graph. When power grid faults occur, the fault handling knowledge graph can assist dispatchers to improve the capabilities of power grid's emergency disposal and the level of dispatch intelligence.},
  keywords={Training;Visualization;Costs;Knowledge graphs;Data models;Power grids;Labeling;deep learning;fault handling;knowledge graph;entity recognition;BERT-BiLSTM-CRF},
  doi={10.1109/ICIPS59254.2023.10404356},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10291739,
  author={Tian, Erlin and Liang, Weide and Li, Pu},
  booktitle={2023 IEEE 7th Information Technology and Mechatronics Engineering Conference (ITOEC)}, 
  title={Building a knowledge graph for dietary services targeting specific groups of people}, 
  year={2023},
  volume={7},
  number={},
  pages={1633-1637},
  abstract={Currently, most research focuses on users' behavioral preferences while ignoring the impact of food properties on human health. This article extracts a large amount of knowledge about the relationship between food properties and human health from multiple heterogeneous data sources. Based on this, a knowledge graph in the field of food is constructed for special populations to help them plan their diet more reasonably and reduce the risk of common diseases. Using background data sources such as Baidu Baike and Wikipedia, the BERT-BiLSTM-MHA-CRF method is proposed to extract food-related attributes from more than 14,731 descriptions of food properties. Combined with the differentiated features of special populations, a knowledge graph in the field of food is constructed. The knowledge graph mainly includes six entity types: food nutrition, efficacy and function, food name, population, dish name, and seasoning, with a total of 11,218 entities and 96,186 relationships. The experiment shows that compared with traditional static word vector models, BERT can generate dynamic word vectors based on context in large-scale corpus, making semantic encoding more accurate. The multi-head self-attention mechanism weights various entities in the food domain to reduce the interference of invalid information, making the model more accurate in capturing entity features. The BERT-BiLSTM-MHA-CRF method proposed in this article achieves P, R, and F1 greater than 90%.},
  keywords={Knowledge engineering;Soft sensors;Sociology;Web and internet services;Semantics;Knowledge graphs;Encyclopedias;food nutrition;special population;knowledge graph;knowledge extraction},
  doi={10.1109/ITOEC57671.2023.10291739},
  ISSN={2693-289X},
  month={Sep.},}@INPROCEEDINGS{10471711,
  author={Qin, Xiaodong and He, Yuxuan and Ma, Jie and Peng, Weiyuan and Zio, Enrico and Su, Huai},
  booktitle={2023 International Conference on Computer Science and Automation Technology (CSAT)}, 
  title={An Effective Knowledge Mining Method for Compressor Fault Text Data Based on Large Language Model}, 
  year={2023},
  volume={},
  number={},
  pages={44-48},
  abstract={The fault diagnosis method of compressors determines the reliability of the gas transmission pipeline station. Existing compressor fault diagnosis methods mostly relies on data-driven, which leads to a high application threshold from the mechanism. To address this issue, this paper introduces the knowledge graph into the compressor fault diagnosis for the first time and proposes a compressor fault text data knowledge mining method based on large language model. Firstly, the characteristics and principles of compressor faults are analyzed. Then, a text data knowledge mining model called CFRTE for compressors is constructed. Experimental results show that the Fl score of the CFRTE model can reach 0.98, meeting the requirements of compressor fault knowledge mining. Finally, combined with the results of knowledge mining and the graph database, a new system for the storage and indexing of the compressor fault knowledge graph is proposed. To further verify the role of the large language model in compressor fault knowledge mining, this paper conducts a comparative experiment of CFRTE models based on RNN encoder and BERT encoder. Experimental results show that compared with GRU, BiGRU, LSTM, and BiLSTM as the encoder layer, the Fl score of the CFRTE model with BERT as the encoder layer has increased by 26.78%, 6.18%, 21.89%, and 5.49% respectively. This work provides a systematic feasible scheme for introducing knowledge graphs into compressor fault diagnosis, which can be used for reference in the fault diagnosis of related equipment.},
  keywords={Fault diagnosis;Systematics;Computational modeling;Pipelines;Knowledge graphs;Ontologies;Compressors;compressor station;compressor;large language model;knowledge mining;knowledge graph},
  doi={10.1109/CSAT61646.2023.00024},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9194539,
  author={Lin, Zhimin and Lei, Dajiang and Han, Yuting and Wang, Guoyin and Deng, Wei and Huang, Yuan},
  booktitle={2020 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={Siamese BERT Model with Adversarial Training for Relation Classification}, 
  year={2020},
  volume={},
  number={},
  pages={291-296},
  abstract={Relation classification is a very important Natural Language Processing (NLP) task to classify the relations from the plain text. It is one of the basic tasks of constructing a knowledge graph. Most existing state-of-the-art methods are primarily based on Convolutional Neural Networks(CNN) or Long Short-Term Memory Networks(LSTM). Recently, many pre-trained Bidirectional Encoder Representation from Transformers (BERT) models have been successfully used in the sequence labeling and many NLP classification tasks. Relation classification is different in that it needs to pay attention to not only the sentence information but also the entity pairs. In this paper, a Siamese BERT model with Adversarial Training (SBERT-AT) is proposed for relation classification. Firstly, the features of the entities and the sentence can be extracted separately to improve the performance of relation classification. Secondly, the adversarial training is applied to the SBERT architecture to improve the robustness. Lastly, the experimental results demonstrate that we achieve significant improvement compared with the other methods on real-world datasets.},
  keywords={Bit error rate;Task analysis;Training;Robustness;Labeling;Predictive models;Telecommunications;NLP;Relation Classification;Siamese BERT;Adversarial Training;Knowledge Graph},
  doi={10.1109/ICBK50248.2020.00049},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10650003,
  author={Zhou, Xin and Shan, Yongxue and Dong, Zixuan and Liu, Haijiao and Wang, Xiaodong},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Temporal Closing Path for PLM-based Temporal Knowledge Graph Completion}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Temporal Knowledge Graph Completion (TKGC) aims to predict missing parts of quadruples, which is crucial for real-life knowledge graphs. Compared with methods that only use graph neural networks, the emergence of pre-trained model has introduced a trend of simultaneously leveraging text and graph structure information. However, most current methods based on pre-trained models struggle to effectively utilize both text and multi-hop graph structure information concurrently, resulting in insufficient association mining of relations. To address the challenge, we propose a novel model: Temporal Closing Path for Pre-trained Language Model-based TKGC (TCP-PLM). We obtain the temporal closing relation path of the target relation through sampling, and use the relation path as a bridge to simultaneously utilize text and multi-hop graph structure information. Moreover, the relation path serves as a tool for mining associations between relations. At the same time, due to the design of entity-independent relation paths, our model can also handle the inductive setting. Our experiments on three benchmarks, along with extensive analysis, demonstrate that our model not only achieves substantial performance enhancements across four metrics compared to other models but also adeptly handles inductive settings.},
  keywords={Training;Measurement;Knowledge engineering;Bridges;Analytical models;Knowledge graphs;Transforms;Benchmark testing;Market research;Graph neural networks;Temporal Knowledge Graph;Knowledge Graph Completion;Pre-train Language Model},
  doi={10.1109/IJCNN60899.2024.10650003},
  ISSN={2161-4407},
  month={June},}
@INPROCEEDINGS{8355023,
  author={Kurniawan, Andri and Afriyanti, Iis and Azurat, Ade},
  booktitle={2017 International Conference on Advanced Computer Science and Information Systems (ICACSIS)}, 
  title={ABS microservices and ontology-zotonic integration for SPL implementation in information system}, 
  year={2017},
  volume={},
  number={},
  pages={131-136},
  abstract={Software Product Line (SPL) promises to accelerate the development process with higher quality of product and low budget of production. The essential key of SPL is feature diagram which describes the relation between features for the domain and captures the commonalities and variabilities. The approach gains numerous attention in research and industry area. However, the implementation of SPL approach for information system development is still inadequate. Some works explain the inclusion of ontology for SPL such as having feature diagram in ontology language. The inclusion benefits to produce an information system automatically. On the other side, the executable modelling language such as Abstract Behavioural Specification (ABS) can be utilized to implement the feature diagram for distributed object-oriented systems. The trend of ABS expands to accommodate microservices-based software variabilities, that is ABS Microservices Framework. The Framework allows to build different services for different devices who consume the data from ABS. However, these two area of research have not been integrated. We propose an adaptor to integrate the ABS microservices and ontology-based information system to produce automated business logics into the system. We show that by using the ontology as its basis, the system is semantically structured and the business logics required by the system is updated automatically.},
  keywords={Web services;Adaptation models;Business;Information systems;Ontologies;Object oriented modeling;OWL},
  doi={10.1109/ICACSIS.2017.8355023},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10446380,
  author={Doh, SeungHeon and Lee, Minhee and Jeong, Dasaem and Nam, Juhan},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Enriching Music Descriptions with A Finetuned-LLM and Metadata for Text-to-Music Retrieval}, 
  year={2024},
  volume={},
  number={},
  pages={826-830},
  abstract={Text-to-Music Retrieval, finding music based on a given natural language query, plays a pivotal role in content discovery within extensive music databases. To address this challenge, prior research has predominantly focused on a joint embedding of music audio and text, utilizing it to retrieve music tracks that exactly match descriptive queries related to musical attributes (i.e. genre, instrument) and contextual elements (i.e. mood, theme). However, users also articulate a need to explore music that shares similarities with their favorite tracks or artists, such as I need a similar track to Superstition by Stevie Wonder. To address these concerns, this paper proposes an improved Text-to-Music Retrieval model, denoted as TTMR++, which utilizes rich text descriptions generated with a finetuned large language model and metadata. To accomplish this, we obtained various types of seed text from several existing music tag and caption datasets and a knowledge graph dataset of artists and tracks. The experimental results show the effectiveness of TTMR++ in comparison to state-of-the-art music-text joint embedding models through a comprehensive evaluation involving various musical text queries.1},
  keywords={Mood;Databases;Soft sensors;Instruments;Knowledge graphs;Signal processing;Probabilistic logic;Music Informational Retrieval;Text-to-Music Retrieval;Large Language Model},
  doi={10.1109/ICASSP48485.2024.10446380},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{9537870,
  author={Wen, Song and Zeng, Bi and Liao, Wenxiong},
  booktitle={2021 3rd International Conference on Natural Language Processing (ICNLP)}, 
  title={Named Entity Recognition for Instructions of Chinese Medicine Based on Pre-trained Language Model}, 
  year={2021},
  volume={},
  number={},
  pages={139-144},
  abstract={Named Entity Recognition (NER) of Chinese medicine text is a basic task of constructing medical and health knowledge graph. Many scholars have researched the NER task of electronic medical records and drug names, while many factors restrict the research of NER tasks for the instructions of Chinese medicine. For example, there is no obvious boundary between words in Chinese, and it is impossible to capture the interactive information between sentences and the global information at the same time. Considering that this type of data is highly professional and there is no publicly available data set. This paper collected 1,000 pieces of instructions of Chinese medicine, then explored the effectiveness of pre-trained models in NER task in this field. The experimental results showed that compared with the experimental results of the single or joint model on the same data set, the F1 value of pre-trained model was increased by 9.65% and 8.71% respectively.},
  keywords={Drugs;Text recognition;Data models;Natural language processing;Task analysis;Electronic medical records;NER;instruction of Chinese medicine;pre-trained language model;CRF},
  doi={10.1109/ICNLP52887.2021.00029},
  ISSN={},
  month={March},}@INPROCEEDINGS{9206698,
  author={Sai Sharath, Japa and Banafsheh, Rekabdar},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Question Answering over Knowledge Base using Language Model Embeddings}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={Knowledge Base, represents facts about the world, often in some form of subsumption ontology, rather than implicitly, embedded in procedural code, the way a conventional computer program does. While there is a rapid growth in knowledge bases, it poses a challenge of retrieving information from them. Knowledge Base Question Answering is one of the promising approaches for extracting substantial knowledge from Knowledge Bases. Unlike web search, Question Answering over a knowledge base gives accurate and concise results, provided that natural language questions can be understood and mapped precisely to an answer in the knowledge base. However, some of the existing embedding-based methods for knowledge base question answering systems ignore the subtle correlation between the question and the Knowledge Base (e.g., entity types, relation paths, and context) and suffer from the Out Of Vocabulary problem. In this paper, we focused on using a pre-trained language model for the Knowledge Base Question Answering task. Firstly, we used Bert base uncased for the initial experiments. We further fine-tuned these embeddings with a two way attention mechanism from the knowledge base to the asked question and from the asked question to the knowledge base answer aspects. Our method is based on a simple Convolutional Neural Network architecture with a Multi-Head Attention mechanism to represent the asked question dynamically in multiple aspects. Our experimental results show the effectiveness and the superiority of the Bert pre-trained language model embeddings for question answering systems on knowledge bases over other well-known embedding methods.},
  keywords={Knowledge based systems;Task analysis;Bit error rate;Knowledge discovery;Semantics;Natural languages;Context modeling;knowledge base question answering;BERT;Language Model;KBQA;Multi-Head Attention},
  doi={10.1109/IJCNN48605.2020.9206698},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{10429117,
  author={Guo, Zhenyu and Ma, Huan and Liu, Wentao},
  booktitle={2023 10th International Forum on Electrical Engineering and Automation (IFEEA)}, 
  title={Cause Analysis of Substation Faults Based on Multimodal Fusion Detection and Knowledge Graph}, 
  year={2023},
  volume={},
  number={},
  pages={1252-1256},
  abstract={The expansion of the grid scale puts forward higher requirements for the stability of substation operation, so it is necessary to detect faults in time and analyze possible fault causes through intelligent inspection. However, commonly used fault detection methods based on image processing or single-mode data such as voltage and temperature are likely to cause misjudgment of faults and cannot realize the function of fault cause analysis. Based on this, this paper proposes a substation fault cause analysis model based on multi-modal data fusion and knowledge graph. The cause of the fault is inferred based on the substation fault cause knowledge graph. For the first time, this solution comprehensively uses target detection technology, audio recognition neural network and knowledge map, and achieves a fault cause analysis accuracy rate higher than 90% on the substation fault data set we constructed.},
  keywords={Temperature distribution;Substations;Data integration;Knowledge graphs;Object detection;Voltage;Thermal stability;Substation Faults;Multimodal Fusion Detection;Knowledge Graph},
  doi={10.1109/IFEEA60725.2023.10429117},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{7934810,
  author={Shani, Uri and Franke, Marco and Hribernik, Karl A. and Thoben, Klaus-Dieter},
  booktitle={2017 Annual IEEE International Systems Conference (SysCon)}, 
  title={Ontology mediation to rule them all: Managing the plurality in product service systems}, 
  year={2017},
  volume={},
  number={},
  pages={1-7},
  abstract={The lifecycle of a product is managed not only through the Product Lifecycle Management (PLM), but needs to integrate with product services into a Product Service System (PSS). The related activities are performed throughout the entire lifecycle and require sharing information among tools of the different product lifecycle phases. When extending collaboration of PLM with services as integrated within a PSS, the physical product is linked with a vastly extended universe of information during the PSS lifecycle. To achieve robust and maintainable PSS the interoperability must be fulfilled between the physical products related data sources and the relevant services. To meet that end, we use ontologies to define a formal semantic for information sources and targets. Each tool or data source can use its own ontology independently of the other tools and sources, creating the potential of an unmanageable universe of data. Yet, the benefit is that components of the PSS have weak dependencies among them which leads to an open and flexible system that can easily evolve and adapt. This paper focuses on the provision of ontology driven services including the transformation of product related data into different ontologies and the aggregation of different data source specific ontologies to a holistic PSS universe with no specific ontology in its core. We present two approaches that implement ontology mediation (also termed "semantic mediation") as a variant of ontology matching since the level of matching can be rather complex. The application of this technology is also demonstrated in related domains, showing its potential when applied in PSS that is presently an ongoing research within the PSYMBIOSYS EU project. In consequence, the applicable data integration and ontology matching approaches are the hand tools to instantiate sustainable PSS into the market.},
  keywords={Ontologies;Tools;Mediation;Interoperability;Semantics;OWL;Resource description framework;Semantic Mediation;Ontology Mediation;Heterogeneous data management;Ontologies in Product Service Systems;large-scale systems Integration},
  doi={10.1109/SYSCON.2017.7934810},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{10035090,
  author={Shang, Chunnan and Qin, Bo and Peng, Peng and Wang, Hongwei},
  booktitle={2022 IEEE International Conference on e-Business Engineering (ICEBE)}, 
  title={Simultaneous Extraction of Entities and Relations Based on Pre-trained Language Model and Pre-defined Language Templates}, 
  year={2022},
  volume={},
  number={},
  pages={222-227},
  abstract={With the advent of the era of big data, the need to obtain core information from text data is getting stronger and stronger. And the knowledge graph can visually represent the core information of the text. Entity relation extraction is a challenge and key part of knowledge graph construction. In this paper, we propose a model that can effectively perform entity relation extraction. The model adopts a joint training approach based on parameter sharing to solve the error propagation problem in pipelined extraction. At the same time, the model uses a pre-trained language model as the basis to solve the problem of lack of semantic knowledge in traditional models. Further, it uses a linguistic template-based approach to bridge the discrepancies in training and fine-tuning of pre-trained language models. To validate the proposed approach, we conduct comparative experiments on the SemEval2010 dataset and conll04 dataset. The validation results demonstrate that our model can improve the accuracy, recall, and F1 score of joint entity relationship extraction compared to the baseline models.},
  keywords={Training;Bridges;Measurement;Knowledge engineering;Semantics;Linguistics;Big Data;knowledge graph construction;joint extraction of entities and relations;pre-trained language model;language templates},
  doi={10.1109/ICEBE55470.2022.00046},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10148356,
  author={Na, Qionglan and Su, Dan and Zhang, Jiaojiao and Li, Xin and Xiao, Na},
  booktitle={2022 4th International Conference on Intelligent Information Processing (IIP)}, 
  title={Construction of Power Knowledge Graph based on Entity Relation Extraction}, 
  year={2022},
  volume={},
  number={},
  pages={77-80},
  abstract={In order to integrate the fragmented text data in the power domain and solve the problems of disordered and weak correlation of transmission protocols, an improved BERT model was proposed by combining deep learning and knowledge graph for entity relationship extraction in the power domain. This method uses the BERT model based on a full word mask to generate sentence vectors, word vectors with contextual semantics, and then takes the average value of word vectors to get entity vectors. The sentence vectors and entity vectors are combined by the attention machine. Finally, the combined new vectors are put into a fully layer for sequential labeling and finding the optimal tag to implement the entity extracted object. The experimental results show that the precision, recall value, and F1 score of this method are 90.12%, 85.25%, and 87.56 % respectively when entity extraction is performed on the corpus data set of transmission procedures.},
  keywords={Technological innovation;Protocols;Bit error rate;Semantics;Knowledge graphs;Feature extraction;Regulation;Power;BERT;Information Extraction;Deep-learning},
  doi={10.1109/IIP57348.2022.00022},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10475660,
  author={Ogawa, Tomohiro and Yoshioka, Kango and Fukuda, Ken and Morita, Takeshi},
  booktitle={2024 IEEE 18th International Conference on Semantic Computing (ICSC)}, 
  title={Prediction of actions and places by the time series recognition from images with Multimodal LLM}, 
  year={2024},
  volume={},
  number={},
  pages={294-300},
  abstract={In recent years, the risk of accidents in the homes of older adults in an aging society has increased, and there is a need to address this problem. We took up the challenge of utilising explainable AI techniques to identify accident risks at home and suggest safer alternatives. This study combined knowledge graphs and large-scale language models to solve real-world problems. Specifically, we addressed answering questions using a multimodal dataset of videos recording daily activities and a knowledge graph. The dataset represents the living activities in the virtual space and provides environmental information. The task is divided into two main tasks. Task 1 utilises knowledge graph to answer direct questions and processes the data using SPARQL queries. Task 2 addresses more complex questions that cannot be answered by search alone. Consequently, in Task 1, the system could answer all questions using information from the SPARQL knowledge graph. In Task 2, a certain degree of success was achieved for complex questions by reasoning with images created by concatenating multimodal LLMs and time-series images. The source code used in the experiment is available at https://github.com/tomo1115tomo/kg_reasoning_challenge.},
  keywords={Training;Source coding;Time series analysis;Training data;Knowledge graphs;Data models;Cognition;Knowledge Graph Reasoning Challenge},
  doi={10.1109/ICSC59802.2024.00053},
  ISSN={2472-9671},
  month={Feb},}@INPROCEEDINGS{10389253,
  author={Yadav, Divyanshi and Para, Hitesh and Selvakumar, Prakash},
  booktitle={2023 International Conference on Electrical, Computer and Energy Technologies (ICECET)}, 
  title={Unleashing the Power of Large Language Model, Textual Embeddings, and Knowledge Graphs for Advanced Information Retrieval}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Acquiring knowledge beyond the usual expertise is a critical challenge when implementing semantic information solutions for querying a knowledge base. To address this difficulty, one proposed solution was to use knowledge graphs in conjunction with traditional Question & Answering (Q&A) systems. However, this approach struggles with limited facts, difficulty in obtaining further insights into the context, and limited ability to handle complex questions, leading to inaccurate or irrelevant answers. To overcome these limitations, we present an approach for answering inference-based questions that integrates knowledge graphs, a large language model, and relevant embeddings from a vector database. Combining knowledge graphs and word embeddings significantly enhances the strength of both techniques, leading to improved performance of Question and Answering systems. We begin with generating representations of the relevant nodes in the knowledge graph and retrieve the most appropriate information from a collection of stored textual data using word embeddings. This approach tackles the shortcomings of conventional approaches that rely solely on knowledge graphs and are too rigid to handle the nuances of the context. This method provides a sophisticated understanding of language and context, enabling it to handle complex questions that may involve multiple entities and relationships with a better understanding of the facts and context in which the question is being asked. The system's ability to handle complex queries is evidenced through a combination of theoretical analysis and empirical data. Our approach has demonstrated exceptional efficiency on a benchmark dataset, as evidenced by evaluating the F1 score.},
  keywords={Databases;Computational modeling;Semantics;Knowledge based systems;Knowledge graphs;Benchmark testing;Information retrieval;Information Retrieval;Question & Answering Systems;Knowledge Graphs;Word Embeddings;Natural Language Processing;Large Language Model;Natural Language Understanding;Inference-Based Questions},
  doi={10.1109/ICECET58911.2023.10389253},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9843341,
  author={Mordecai, Yaniv and Markina–Khusid, Aleksandra and Quinn, Greg and Crawley, Edward F.},
  booktitle={2022 IEEE Aerospace Conference (AERO)}, 
  title={Applying Model-Based Ontology Coverage Analysis to Mission Architectures}, 
  year={2022},
  volume={},
  number={},
  pages={01-18},
  abstract={This paper introduces a method for Model-based Ontology Coverage Analysis (MOCA) and applies it to SysML models of mission architectures. An ontology is a set of concepts that constitute a common language, standard terminology, and consistent pattern reference across multiple models within an organization, industry, or domain. The purpose of MOCA is to assess the overlap between a system architecture model and a given ontology, and thereby the architecture model's compliance with the ontology and the ontology's utilization by the architecture. We demonstrate MOCA on a SysML model of a humanitarian airlift mission, using a conceptual mission architecting SysML profile model that serves as the ontology. MOCA automates and simplifies reasoning over models, and creates digital model-based artifacts that support stakeholders in concept validation, decision making, and system/mission design. Thus, MOCA enhances digital systems engineering.},
  keywords={Analytical models;Vocabulary;Visualization;Digital systems;Atmospheric modeling;Unified modeling language;Semantics;Digital Engineering;Model-Based Systems Engineering;MBSE;Mission Architecture;Mission Engineering;Ontology;Ontological Analysis},
  doi={10.1109/AERO53065.2022.9843341},
  ISSN={1095-323X},
  month={March},}@INPROCEEDINGS{10350785,
  author={Dhaouadi, Mouna and Oakes, Bentley James and Famelis, Michalis},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Towards Understanding and Analyzing Rationale in Commit Messages Using a Knowledge Graph Approach}, 
  year={2023},
  volume={},
  number={},
  pages={622-630},
  abstract={Extracting rationale information from commit messages allows developers to better understand a system and its past development. Here we present our ongoing work on the Kantara end-to-end rationale reconstruction pipeline to a) structure rationale information in an ontologically-based knowledge graph, b) extract and classify this information from commits, and c) produce analysis reports and visualizations for developers. We also present our work on creating a labelled dataset for our running example of the Out-of-Memory component of the Linux kernel. This dataset is used as ground truth for our evaluation of NLP classification techniques which show promising results, especially the multi-classification technique XGBoost.},
  keywords={Visualization;Analytical models;Linux;Pipelines;Knowledge graphs;Model driven engineering;Data mining;rationale structuring;rationale extraction;Natural Language Processing;Linux;ontology;dataset;openCAESAR},
  doi={10.1109/MODELS-C59198.2023.00101},
  ISSN={},
  month={Oct},}@ARTICLE{10433728,
  author={Pan, Yudai and Liu, Jun and Zhao, Tianzhe and Zhang, Lingling and Wang, Qianying},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Context-Aware Commonsense Knowledge Graph Reasoning With Path-Guided Explanations}, 
  year={2024},
  volume={36},
  number={8},
  pages={3725-3738},
  abstract={Commonsense knowledge graphs (CKGs) store massive commonsense knowledge as triples whose nodes consist of free-form texts. CKG reasoning aims to predict missing nodes in incomplete commonsense triples, which is challenging as it requires more accurate embeddings for reasoning. Compared to conventional knowledge graphs (KGs), CKGs have deficient structural information due to their sparsity and contain nodes indistinguishable due to the conceptual diversity. These issues limit the performance of previous reasoning methods, because they face difficulties obtaining precise CKG representations. To address these issues, we propose a context-aware CKG reasoning framework with path-guided explanations, named CoRPe. First, CoRPe constructs context sentences based on the target commonsense triple using designed templates. The context captures reasoning paths instantiated from the first-order logic. Second, to improve CKG representations, CoRPe injects context semantics and employs a context-augmented tuning strategy on a pre-trained language model (PLM) via a synergistic optimization. Finally, CoRPe embeds structural information using a graph convolutional network (GCN) and associates the textual semantics for joint scoring. Extensive experiments on two CKGs show that CoRPe outperforms state-of-the-art KG and CKG reasoning baselines in terms of embedding and reasoning performance. Furthermore, the interpretability of CoRPe is reflected in the implicit logic during reasoning.},
  keywords={Semantics;Commonsense reasoning;Task analysis;Knowledge graphs;Tuning;Electronic mail;Education;Commonsense knowledge graph reasoning;representation learning;context-augmented tuning;first-order logic},
  doi={10.1109/TKDE.2024.3365103},
  ISSN={1558-2191},
  month={Aug},}@INPROCEEDINGS{10066745,
  author={Kaneda, Ryoya and Okada, Makoto and Mori, Naoki},
  booktitle={2023 IEEE 17th International Conference on Semantic Computing (ICSC)}, 
  title={A Method to Constract a Masked Knowlege Graph Model using Transformer for Knowledge Graph Reasoning}, 
  year={2023},
  volume={},
  number={},
  pages={298-299},
  abstract={Most of the previous methods using machine learning for this challenge generate a new knowledge graph from the original one, and some information is lost in the process of creating a new knowledge graph. Therefore, we proposed a new model to estimate the criminal without changing the original knowledge graph. The proposed model uses a Transformer and allows the estimation of unknown criminals in nonexistent scenes by learning similar to Masked Language Modeling in BERT. This model, which uses the original knowledge graph, is expected to infer information about the crime scene at the same time as predicting the criminal. We confirmed by experiments that the model had gained the ability to estimate the hidden story parts by considering the surrounding stories.},
  keywords={Computational modeling;Semantics;Bit error rate;Estimation;Knowledge graphs;Machine learning;Predictive models;Knowledge Graph;Transformer;Masked learning},
  doi={10.1109/ICSC56153.2023.00061},
  ISSN={2325-6516},
  month={Feb},}@INPROCEEDINGS{10195092,
  author={Roy, Kaushik and Garg, Tarun and Palit, Vedant},
  booktitle={2023 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust}, 
  year={2023},
  volume={},
  number={},
  pages={234-236},
  abstract={A fundamental question in natural language processing is - what kind of language structure and semantics is the language model capturing? Graph formats such as knowledge graphs are easy to evaluate as they explicitly express language semantics and structure. This study evaluates the semantics encoded in the self-attention transformers by leveraging explicit knowledge graph structures. We propose novel metrics to measure the reconstruction error when providing graph path sequences from a knowledge graph and trying to reproduce/reconstruct the same from the outputs of the self-attention transformer models. The opacity of language models has an immense bearing on societal issues of trust and explainable decision outcomes. Our findings suggest that language models are models of stochastic control processes for plausible language pattern generation. However, they do not ascribe object and concept-level meaning and semantics to the learned stochastic patterns such as those described in knowledge graphs. This has significant application-level user trust implications as stochastic patterns without a strong sense of meaning cannot be trusted in high-stakes applications.},
  keywords={Semantics;Measurement uncertainty;Stochastic processes;Process control;Knowledge graphs;Medical services;Predictive models;Knowledge Graph;Graph Neural Networks;Transformers},
  doi={10.1109/CAI54212.2023.00108},
  ISSN={},
  month={June},}@INPROCEEDINGS{10020568,
  author={An, Yuan and Greenberg, Jane and Hu, Xiaohua and Kalinowski, Alex and Fang, Xiao and Zhao, Xintong and McCLellan, Scott and Uribe-Romo, Fernando J. and Langlois, Kyle and Furst, Jacob and Gómez-Gualdrón, Diego A. and Fajardo-Rojas, Fernando and Ardila, Katherine and Saikin, Semion K. and Harper, Corey A. and Daniel, Ron},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Exploring Pre-Trained Language Models to Build Knowledge Graph for Metal-Organic Frameworks (MOFs)}, 
  year={2022},
  volume={},
  number={},
  pages={3651-3658},
  abstract={Building a knowledge graph is a time-consuming and costly process which often applies complex natural language processing (NLP) methods for extracting knowledge graph triples from text corpora. Pre-trained large Language Models (PLM) have emerged as a crucial type of approach that provides readily available knowledge for a range of AI applications. However, it is unclear whether it is feasible to construct domain-specific knowledge graphs from PLMs. Motivated by the capacity of knowledge graphs to accelerate data-driven materials discovery, we explored a set of state-of-the-art pre-trained general-purpose and domain-specific language models to extract knowledge triples for metal-organic frameworks (MOFs). We created a knowledge graph benchmark with 7 relations for 1248 published MOF synonyms. Our experimental results showed that domain-specific PLMs consistently outperformed the general-purpose PLMs for predicting MOF related triples. The overall benchmarking results, however, show that using the present PLMs to create domain-specific knowledge graphs is still far from being practical, motivating the need to develop more capable and knowledgeable pre-trained language models for particular applications in materials science.},
  keywords={Materials science and technology;Big Data;Benchmark testing;Natural language processing;Data models;Artificial intelligence;Domain specific languages;Knowledge Graph;Pre-trained Language Model;Prompt Probing;Materials Science;Metal-Organic Frameworks},
  doi={10.1109/BigData55660.2022.10020568},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8945003,
  author={Nardi, Julio Cesar and Almeida, João Paulo A. and da Silva, Paulo Henrique A. and Guizzardi, Giancarlo},
  booktitle={2019 IEEE 23rd International Enterprise Distributed Object Computing Conference (EDOC)}, 
  title={An Ontology-Based Diagnosis of Mainstream Service Modeling Languages}, 
  year={2019},
  volume={},
  number={},
  pages={112-121},
  abstract={This paper presents a diagnosis of mainstream service modeling languages (SoaML, USDL, and ArchiMate) in light of UFO-S, a reference ontology for services. UFO-S is intended as a broad ontology for service phenomena, harmonizing different perspectives on services (e.g., "service as commitment", and "service as capability"), and addressing several phases of the service lifecycle (service offering, service agreement, and service delivery). As result, UFO-S is used as an "analysis theory" to identify choices in these languages concerning their focus and coverage of service phenomena. We identify a number of possible improvements concerning the representation of service participant (roles), the description of service offerings, service agreements and service delivery.},
  keywords={Computational modeling;Conferences;Ontologies;service modeling languages;service ontology;SoaML;USDL;ArchiMate},
  doi={10.1109/EDOC.2019.00023},
  ISSN={2325-6362},
  month={Oct},}@INPROCEEDINGS{10112187,
  author={Saraswat, Deepak},
  booktitle={2023 6th International Conference on Information Systems and Computer Networks (ISCON)}, 
  title={Ontology Based Agriculture Data Mining using IWO and RNN}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={An ontology is a machine-interpretable formal description of domain knowledge. In current years, ontologies have risen to prominence as a key tool for demonstrating domain knowledge and a key element of several knowledge management systems, decision-support systems (DSS) and other intelligent systems including in agriculture. However, a study of the current literature on agricultural ontologies suggests that the majority of research that suggest agricultural ontologies lack a clear assessment mechanism. This is unwanted because this is impossible to assess the value of ontologies in research and practise without well-structured assessment mechanisms. Furthermore, relying on such ontologies and sharing them on the Semantic Web or amongst semantic-aware apps is problematic. This paper presents a framework for selecting appropriate assessment techniques for Ontology Based Agriculture Data Mining utilizing Invasive Weed Optimization (IWO) and Re-current Neural Network (RNN) that appears to be absent from most recent agricultural ontology research. The framework facilitates the selection of relevant evaluation techniques for a particular ontology based on its intended user.},
  keywords={Decision support systems;Semantic Web;Knowledge engineering;Recurrent neural networks;Prototypes;Ontologies;Agriculture;Data Mining;Ontology;IWO;RNN;Agriculture 4.0;Agriculture 5.0},
  doi={10.1109/ISCON57294.2023.10112187},
  ISSN={2832-143X},
  month={March},}@INPROCEEDINGS{10605389,
  author={Schoch, Nicolai and Hoernicke, Mario},
  booktitle={2024 IEEE Conference on Artificial Intelligence (CAI)}, 
  title={NL2IBE – Ontology-controlled Transformation of Natural Language into Formalized Engineering Artefacts}, 
  year={2024},
  volume={},
  number={},
  pages={997-1004},
  abstract={Looking at Process and Automation Engineering (P&AE) today, for the technically adept engineer, there are many different tools available to support the engineering work from translation of engineering intentions into module and plant descriptions, to definition and parametrization of entire process plant setups, for export to a control system. However, still today, in the very early engineering phases, engineering intentions either need to be entered already in a structured and controlled expert language or require a human expert’s manual efforts for translation from unstructured language into formalized representations, in order for thereon-based consistent further processing in the existing tools. This process is time-consuming, fuzzy, and error-prone due to potential misconceptions and ambiguities, even for domain experts. In this work, we therefore present our NL2IBE Tool, which makes use of modern Natural Language Processing in combination with Ontology Mining, and which, based on and controlled by an underlying ontology, allows for the deterministic transformation of natural language intentions into structured and consistent engineering artefacts. We describe the overall tool architecture as well as crucial functionalities and implementation features, followed by an evaluation by the example of a hydrogen generation and CCSU use case. We conclude with a discussion of the proposed tool and give an outlook on future research. (Abstract)},
  keywords={Automation;Hydrogen;Process control;Manuals;Ontologies;Control systems;Natural language processing;process &Amp; automation engineering;intend-based engineering;natural language processing;NLP;generative AI;ontological domain representation},
  doi={10.1109/CAI59869.2024.00182},
  ISSN={},
  month={June},}@INPROCEEDINGS{10353415,
  author={Bhuvanesh Shathyan, R and Begam, M. Farida and Jashwanth, K and Jayaprakash, Anirudh},
  booktitle={2023 4th IEEE Global Conference for Advancement in Technology (GCAT)}, 
  title={Knowledge Graph Based Medical Chatbot building}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={To have a good and unproblematic life without any health risks, it is very important to get medical advice on any health-related problems. However, getting medical advice incurs costs. Chatbots are AI/ML based software which may be trained with a lot of inquiries and responses and match users’ inquiries against a large repository of evidence-based medical data to provide simple answers. To reduce the healthcare costs and improve accessibility of medical knowledge a medical chatbot can be built using ML and NLP techniques. In the existing system of such chatbots several databases are connected together using join statements making it more complicated to access the data. The medical knowledge is vast and varied hence giving many disadvantages to use fixed schema. In this paper we have proposed a knowledge graph based method of chatbot creation for the healthcare field. When the patient enters the symptoms, they are suffering from, then chatbot evaluates and based on the evaluation recognizes the disease. The basic idea of our work is to build a chatbot which can evaluate the symptoms and using this evaluation, rank the possible disease which the patient could be suffering from. The chatbot gets its knowledge from a knowledge graph built on an extensive TigerGraph database. The data from the TigerGraph database can be accessed using different analysis queries. The chatbot will be considered profitable only when it can diagnose all kinds of disease and provides the necessary advice measures to be taken. The knowledge graph used here can be improvised by including results from lab tests to arrive at a better diagnosis and increase the precautions by including the medicines to be taken. The chatbot can only be as smart as the knowledge graph database and hence the evaluations made must be checked with the medical professional. The chatbot has good accuracy in predicting the disease the user is suffering from.},
  keywords={Costs;Databases;Computational modeling;Web pages;Knowledge graphs;Medical services;Predictive models;Knowledge Graph;Rasa;TigerGraph;Random Forest Classifier},
  doi={10.1109/GCAT59970.2023.10353415},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10743169,
  author={Ye, Weiqi and Zhang, Qiang and Zhou, Xian and Hu, Wenpeng and Tian, Changhai and Cheng, Jiajun},
  booktitle={2024 International Conference on Computational Linguistics and Natural Language Processing (CLNLP)}, 
  title={Correcting Factual Errors in LLMs via Inference Paths Based on Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={12-16},
  abstract={Large language models (LLMs) have been observed to occasionally exhibit hallucination, a phenomenon where they generate statements unsupported by factual evidence, thereby compromising the trustworthiness of their output. Current approaches to mitigating this problem largely rely on extracting a single triplet from a knowledge graph, which fails to adequately capture the complex and interlinked nature of factual reasoning. In an effort to address this critical challenge, this paper delves into the utilization of inference paths based on knowledge graph for factual error correction of LLMs. At the heart of our approach lies the deployment of deep reinforcement learning algorithms, which traverse the knowledge graph to retrieve inference paths. These paths, replete with contextual depth and logical coherence, thereby amending the content and diminishing the incidence of factual discrepancies in the reasoning process of LLMs. Experimental results demonstrate that our approach markedly enhances the factual QA performance of LLMs. Furthermore, it shows great potential in improving the reliability of LLMs in complex reasoning scenarios, highlighting the effectiveness of inference path derived from knowledge graph.},
  keywords={Heart;Large language models;Knowledge graphs;Deep reinforcement learning;Cognition;Natural language processing;Inference algorithms;Error correction;Computational linguistics;Reliability;factual error correction;inference path;knowledge graph;reinforcement learning},
  doi={10.1109/CLNLP64123.2024.00011},
  ISSN={},
  month={July},}@INPROCEEDINGS{10475642,
  author={Fukuda, Ken and Ugai, Takanori and Egami, Shusaku and Matsushita, Kyoumoto},
  booktitle={2024 IEEE 18th International Conference on Semantic Computing (ICSC)}, 
  title={Zero-Shot Query Experiments in Knowledge Graph Reasoning Challenge for Older Adults Safety}, 
  year={2024},
  volume={},
  number={},
  pages={301-305},
  abstract={The 2nd International Knowledge Graph Reasoning Challenge involves social issues focusing on the safety of older adults in their homes. The challenge aims to extract statistical information related to actions and objects that pose risks to daily life. To answer each question in a video, we used Video-LLaVa, a large-scale visual language model (LVLM), using two approaches. The first approach involves inputting question text and video into Video-LLaVa. In this paper, we describe the results of zero-shot queries. The second approach is to obtain a detailed description of the video output using Video-LLaVa and then answer questions based on it. We have yet to achieve good results with these approaches, but we have identified some issues that we will discuss along with the results.},
  keywords={Visualization;Semantics;Focusing;Knowledge graphs;Cognition;Safety;Data mining;Knowledge Graph Reasoning Challenge},
  doi={10.1109/ICSC59802.2024.00054},
  ISSN={2472-9671},
  month={Feb},}@INPROCEEDINGS{10386048,
  author={Guo, Kuo and Li, Yifan and Chen, Hao and Shen, Hong-Bin and Yang, Yang},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Isoform Function Prediction Based on Heterogeneous Graph Attention Networks}, 
  year={2023},
  volume={},
  number={},
  pages={522-527},
  abstract={Isoforms refer to different mRNA molecules transcribed from the same gene, which can be translated into proteins with varying structures and functions. Predicting the functions of isoforms is an essential topic in bioinformatics as it can provide valuable insights into the intricate mechanisms of gene regulation and biological processes. Conventionally, gene function labels are standardized in Gene Ontology (GO) terms. However, traditional methods for predicting isoform function are largely limited by the absence of isoform-specific labels, sparse annotations, and the vast number of GO terms. To address these issues, we propose HANIso, a deep learning-based method for isoform function prediction. HANIso leverages a pretrained protein language model to extract features from protein sequences. It also integrates heterogeneous information, such as isoform sequence features, GO annotations, and isoform interaction data, using a Heterogeneous Graph Attention Network (HAN). This allows the model to learn the importance of different sources of information and their semantic relationships through the attention mechanism. Our method can predict function labels at both the gene level and isoform level. We conduct experiments on two species datasets, and the results demonstrate that our method outperforms existing methods on both AUROC and AUPRC. HANIso has the potential to overcome the limitations of traditional methods and provide a more accurate and comprehensive understanding of isoform function.},
  keywords={Proteins;Annotations;Biological system modeling;Semantics;Predictive models;Ontologies;Feature extraction;alternative splicing;isoform function prediction;protein language model;gene ontology;heterogeneous graph attention network},
  doi={10.1109/BIBM58861.2023.10386048},
  ISSN={2156-1133},
  month={Dec},}@ARTICLE{9534721,
  author={Lu, Jinzhi and Ma, Junda and Zheng, Xiaochen and Wang, Guoxin and Li, Han and Kiritsis, Dimitris},
  journal={IEEE Systems Journal}, 
  title={Design Ontology Supporting Model-Based Systems Engineering Formalisms}, 
  year={2022},
  volume={16},
  number={4},
  pages={5465-5476},
  abstract={Model-based systems engineering (MBSE) provides an important capability for managing the complexities of system development. MBSE empowers the formalism of system architectures for supporting model-based requirement elicitation, specification, design, development, testing, fielding, etc. However, the modeling languages and techniques are heterogeneous, even within the same enterprise system, which leads to difficulties for data interoperability. The discrepancies among data structures and language syntaxes make information exchange among MBSE models more difficult, resulting in considerable information deviations when connecting data flows across the enterprise. Therefore, this article presents an ontology based upon graphs, objects, points, properties, roles, and relationships with extensions (GOPPRRE), providing metamodels that support the various MBSE formalisms across lifecycle stages. In particular, knowledge graph models are developed to support unified model representations to further implement ontological data integration based on GOPPRRE throughout the entire lifecycle. The applicability of the MBSE formalism is verified using quantitative and qualitative approaches. Moreover, the GOPPRRE ontologies are used to create the MBSE formalisms in a domain-specific modeling tool, MetaGraph, for evaluating its availability. The results demonstrate that the proposed ontology supports the formal structures and descriptive logic of the systems engineering lifecycle.},
  keywords={Modeling;Ontologies;Unified modeling language;Tools;Systems engineering and theory;Semantics;Data models;Formalism;interoperability;knowledge graph;model-based systems engineering;ontology},
  doi={10.1109/JSYST.2021.3106195},
  ISSN={1937-9234},
  month={Dec},}@INPROCEEDINGS{9194505,
  author={Jiang, Xuhui and Shen, Yinghan and Wang, Yuanzhuo and Jin, Xiaolong and Cheng, Xueqi},
  booktitle={2020 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={BaKGraSTeC: A Background Knowledge Graph Based Method for Short Text Classification}, 
  year={2020},
  volume={},
  number={},
  pages={360-366},
  abstract={Short text classification is an important task in the area of natural language processing. Recent studies attempt to employ external knowledge to improve classification performance, but they ignore the correlation between external knowledge and have poor interpretability. This paper proposes a novel Background Knowledge Graph based method for Short Text Classification called BaKGraSTeC for short, which can not only employ external knowledge from a knowledge graph to enrich text information, but also utilize its structural information through a graph neural network to promote the understanding of texts. Specifically, we construct a background knowledge graph based on training data, then we propose a novel architecture that integrates background knowledge graph into a graph neural network to model and capture implicit interactions between its concepts and classes. Besides, we propose an attention mechanism considering both similarity and co-occurrence between concepts and classes to identify the informative concepts in texts. Our experimental results demonstrate the effectiveness with good interpretability of BaKGraSTeC through using external knowledge and their structural information for short text classification.},
  keywords={Knowledge engineering;Neural networks;Task analysis;Semantics;Natural language processing;Syntactics;Machine learning;knowledge graph;short text;attention mechanism;graph neural network.},
  doi={10.1109/ICBK50248.2020.00058},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10581693,
  author={Li, Zhidong and Wang, Licai and Luo, Qibin and Qiao, Silong},
  booktitle={2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)}, 
  title={Large Language Model Based on Full-Text Retrieval for Temporal Knowledge Q&A Approach}, 
  year={2024},
  volume={},
  number={},
  pages={441-446},
  abstract={Knowledge Q&A is one of the hot research topics in the field of natural language processing, and temporal knowledge Q&A is a difficult area of Q&A reasoning because it also needs to consider the temporal relationship of knowledge. Today's research usually focuses on the word vector similarity between knowledge and questions as an important basis for answering, while ignoring the sentence granularity semantic information embedded in the knowledge. In this paper, we propose a method of temporal knowledge Q&A for large language models based on full-text retrieval, firstly, the sentence granularity knowledge recall is performed by Elasticsearch so that large language models can learn the knowledge that is highly relevant to the problem, and then verify the temporal knowledge Q&A ability of large language models under Zero-shot, One-shot and Few-shot. The experiments were conducted on the ICEWS05-15 dataset, and the accuracy of answers was significantly improved, demonstrating the effectiveness of the temporal knowledge Q&A method for large language models based on Elasticsearch.},
  keywords={Seminars;Knowledge engineering;Accuracy;Large language models;Semantics;Knowledge graphs;Vectors;temporal knowledge graph question-answering;large language models;prompt learning;natural language processing},
  doi={10.1109/AINIT61980.2024.10581693},
  ISSN={},
  month={March},}@INPROCEEDINGS{10803797,
  author={Kireev, Vasily and Vasiliev, Feodosiy},
  booktitle={2024 6th International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA)}, 
  title={Development of a Tool for Constructing a Knowledge Graph Using Gan}, 
  year={2024},
  volume={},
  number={},
  pages={720-723},
  abstract={The main objective of this research was to develop a tool capable of automatically extracting and structuring information from textual data using generative networks to construct knowledge graphs. The developed tool demonstrated high efficiency in automating the extraction and transformation of text into RDF triplets, ensuring accurate and complete knowledge graph construction. Testing on various text datasets confirmed its high performance and precision. The created model enables automation of information processing and can be applied across different domains to solve knowledge graph construction tasks.},
  keywords={Automation;Accuracy;Scalability;Process control;Knowledge graphs;Resource description framework;Mathematical models;Data models;Data mining;Testing;Knowledge graph;LLM;NLP;automation},
  doi={10.1109/SUMMA64428.2024.10803797},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10632664,
  author={Huang, Weichun and Xiao, Gang and Yang, Jian and Hu, Xinyu},
  booktitle={2023 11th International Conference on Information Technology: IoT and Smart City (ITIoTSC)}, 
  title={Domain Terminology Knowledge Graph Completion Method Based on Bert}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Domain terminology knowledge graphs play a critical role in many applications such as text retrieval and information extraction. However, most domain knowledge graphs are constructed manually and often be incomplete on the relation. To this end, we propose a bert-based relational prediction model, which extract relations between entities automatically. Concretely, we utilize the definition of domain terminologies as context of BERT and add special characters to the input of the model to capture the semantic features of domain terms. Through this method, the model can effectively obtain the semantic information of terminology definition and predict the relationship between them. The experimental results demonstrate that our model achieves significant improvement over the baselines on the domain-specific dataset.},
  keywords={Terminology;Smart cities;Semantics;Knowledge graphs;Tail;Predictive models;Feature extraction;Domain Terminology;Bert;Relation Prediction},
  doi={10.1109/ITIoTSC60379.2023.00007},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9549978,
  author={Hao, Nan and Xie, Pengwei and Chen, Bo},
  booktitle={2021 40th Chinese Control Conference (CCC)}, 
  title={Research on Intent-Slot Recognition Algorithm Based on Knowledge Graph and User Topic}, 
  year={2021},
  volume={},
  number={},
  pages={7276-7281},
  abstract={In this article, we propose a joint recognition method of intention and slot. The model can directly obtain the user's intention and slot information through the user's historical interaction information, knowledge graph and current input. In this method, we first use a knowledge reasoning module based on user topic and knowledge graph. The topic model is used to filter the user's historical information, remove meaningless chat information, retain the most important topic information, and further obtain the external information vector from the knowledge graph which is helpful for current intention recognition and slot filling. In addition, we use attention mechanism modules to fuse the historical information, external knowledge vector and sentence features, and suppress the invalid part of external information. Experiments on the data sets show that our method can effectively improve the accuracy of intention recognition and F value of slot filling.},
  keywords={Fuses;Transforms;Filtering algorithms;Information filters;Filling;Cognition;Intent recognition;Knowledge graph;User topic;Slot filling},
  doi={10.23919/CCC52363.2021.9549978},
  ISSN={1934-1768},
  month={July},}@INPROCEEDINGS{10020882,
  author={Hsiao, Yi-Hao and Chuang, Chia-Yi and Huang, Megn-Chi and Yang, Chia-Lee and Wu, Jyh-Horng},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Using Contextual Text Mining and Ontology Methods to Establish a Novel Technology Trend and Associative Analysis Framework for Sustainable Energy Development in Taiwan}, 
  year={2022},
  volume={},
  number={},
  pages={4491-4494},
  abstract={In 2015, the United Nations proposed 17 Sustainable Development Goals, SDGs, as the guidelines for all countries in the world to promote sustainable development before 2030. Government Research Bulletin (GRB), the research projects and technical reports sponsored by government, which has long-term, numerous, complete research method, technology development and policy analysis information in Taiwan. Therefore, it is an important and effective way to explore SDG-related information from a large amount of GRB text. In this paper, a novel technologies trend and associative analysis framework which uses contextual text mining and ontology methods is proposed and applied to SDG 7, which "Affordable and Clean Energy". First, we integrate dictionary-based method and semantic textual similarity analysis algorithm to obtain a SDG 7 classifier which can exactly and quickly classify a large amount number of GRB text to SDG 7. Then, two major SDG 7 analysis procedures based on the classification results are implemented. One is using contextual text mining algorithm to obtain energy technologies trend information. The other is adopting ontology method to establish energy technologies associative analysis concept map. According to the analysis results mentioned above, we are able to efficiently incorporate the energy technology with long-term trend, energy technology associative information, and the most influential authors on the specify energy technology in order to generate a global strategy for continuous improvement in Taiwan.},
  keywords={Text mining;Government;Semantics;Ontologies;Big Data;Writing;Market research;Contextual text mining;Ontology;SBERTs;Sustainable Development Goals (SDGs)},
  doi={10.1109/BigData55660.2022.10020882},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8901302,
  author={Rui, Jiang},
  booktitle={2019 International Conference on Smart Grid and Electrical Automation (ICSGEA)}, 
  title={Research on Semantic Service Technology in Mobile Geographic Information System}, 
  year={2019},
  volume={},
  number={},
  pages={304-307},
  abstract={This paper combines the conceptual construction and classification methods of geo-ontology, explores the method of formal description language to express geo-domain ontology concepts, and defines the concept of geo-ontology in the process of geo-spatial information semantic expression. Controposing to the problem of inadequate description of spatial information services by traditional description methods, Construction of spatial information application ontology with Protege tool and description service information with OWL-S description language are adopted, to realize the semantic integration between geo-information system. It constructs spatial information application ontology and describes service information with OWL-S description language. The elements of service quality are expanded to improve the comprehensiveness of service description. To verify the feasibility of the combination of theory and practical application of geo-ontology, a mobile GIS is developed from the bottom under the support of current mobile development technology, and an example of spatial semantic retrieval is tested. The superiority of geo-ontology in semantic retrieval is verified, which has important theoretical and practical significance.},
  keywords={Semantics;Information services;Ontologies;Tools;Topology;Smart grids;Geographic information systems;semantic service;GIS;OWL;ontology;Protege},
  doi={10.1109/ICSGEA.2019.00076},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{7930204,
  author={Hoppe, Tobias and Eisenmann, Harald and Viehl, Alexander and Bringmann, Oliver},
  booktitle={2017 IEEE International Conference on Software Architecture (ICSA)}, 
  title={Digital Space Systems Engineering through Semantic Data Models}, 
  year={2017},
  volume={},
  number={},
  pages={93-96},
  abstract={Model-based Systems Engineering requires an intuitive semantically strong data model to enable precise data specification and provide the foundation for fruitful data analyses during data evolution. This paper presents an approach to use the Web Ontology Language (OWL) for specifying a Conceptual Data Model (CDM) being transformed into a format understandable by the Eclipse Modeling Framework (EMF) to profit from powerful data handling and knowledge management functions during runtime. Coalescing OWL with EMF brings up the strength of both approaches leading to considerably better data models with less failure potential and reveal notably more analysis potential by using a common data model specification. This approach also enables the direct application of reasoning functionality for automatic inference of several pieces of knowledge and automatic checks as illustrated by examples from aerospace industry.},
  keywords={Data models;Ontologies;Unified modeling language;OWL;Semantics;Model-based Systems Engineering;Conceptual Data Model Implementation;Eclipse Modeling Framework;Ontology},
  doi={10.1109/ICSA.2017.35},
  ISSN={},
  month={April},}@INPROCEEDINGS{10415662,
  author={Liu, Kangzheng and Zhao, Feng and Xu, Guandong and Wu, Shiqing},
  booktitle={2023 IEEE International Conference on Data Mining (ICDM)}, 
  title={IE-Evo: Internal and External Evolution-Enhanced Temporal Knowledge Graph Forecasting}, 
  year={2023},
  volume={},
  number={},
  pages={408-417},
  abstract={Temporal knowledge graph (TKG) forecasting is widely used in various fields due to its ability to infer future events based on historical information. Modeling the internal structures and chronological dependencies of historical subgraph sequences has been proven effective. Nevertheless, on the one hand, the TKG forecasting process generally suffers from a lack of sufficient sample data due to historical resource limitations; thus, most works focus on continuously mining the patterns of historical sequences while ignoring the semantically-rich background information provided by external knowledge, especially when historical query-related information is scarce. On the other hand, when merely serializing the given subgraph sequence to mimic its temporal evolution process, only the chronological dependencies between the subgraphs can be considered, thus ignoring the evolution of time information. Hence, a method that integrates internal and external knowledge to enhance the representations of entities is urgently needed. To this end, we propose a novel TKG forecasting method, namely, the internal and external evolution-enhanced framework (IE-Evo). For the former issue, we design an external evolution encoder and use a pre-trained language model (PLM) to provide powerful external knowledge semantics for TKG forecasting. To address the latter concern, we propose an internal evolution encoder that explicitly embeds the time information while modeling the aggregation and evolution processes of the observed sequential structural information. IE-Evo has been evaluated on four public benchmark datasets, showcasing its significant improvements across multiple evaluation metrics.},
  keywords={Measurement;Semantics;Knowledge graphs;Predictive models;Data models;Data mining;Forecasting;Temporal knowledge graph extrapolation;External knowledge;Time information evolution},
  doi={10.1109/ICDM58522.2023.00050},
  ISSN={2374-8486},
  month={Dec},}@INPROCEEDINGS{9776230,
  author={Shang, Jialin and Huang, Jingyuan and Zeng, Shihua and Zhang, Jian and Wang, Hongwei},
  booktitle={2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
  title={Representation and Extraction of Physics Knowledge Based on Knowledge Graph and Embedding-Combined Text Classification for Cooperative Learning}, 
  year={2022},
  volume={},
  number={},
  pages={1053-1058},
  abstract={Physical knowledge is the foundation of most engineering fields in particular such as product design, analysis, and operation and maintenance. However, due to the complexity of physical concepts, laws, and calculations, students can be easily overwhelmed by the conceptual ideas in the process of learning physics. This paper proposes a new way for helping students grasp the logical relation between the physics knowledge points based on neural networks and knowledge graph technology. Specifically, we use Python scripts to collect the articles about physics knowledge on the Internet as the raw data. After removing the special characters and other irrelevant text, the rest of the data is passed to several neural networks based on BERT and ERNIE for their effective and efficient classification into seven kinds of physics knowledge. The experimental results show that using ERNIE-BERT for embedding and using RCNN for the downstream model achieve the best performance. Knowledge graph is used to build a tree structure of physics knowledge, holding the physics knowledge picked out by the neural networks under corresponding nodes.},
  keywords={Knowledge engineering;Training;Neural networks;Text categorization;Knowledge based systems;Maintenance engineering;Product design;computer assisted learning;knowledge graph;em-bedding;text classification},
  doi={10.1109/CSCWD54268.2022.9776230},
  ISSN={},
  month={May},}@INPROCEEDINGS{9832371,
  author={Shi, Xiaowen and Yang, Jing and He, Liang},
  booktitle={2022 IEEE 2nd International Conference on Electronic Technology, Communication and Information (ICETCI)}, 
  title={Commonsense Generative Model for Chinese Automatic Knowledge Graph Construction}, 
  year={2022},
  volume={},
  number={},
  pages={20-24},
  abstract={Commonsense knowledge graph support applications in commonsense reasoning, question answering, and so on. However, automatic knowledge graph construction is still a continuing goal for AI researchers due to the difficulty of obtaining tractable and objective commonsense information. Besides, the relative researches have so far been mainly limited to English, making it slow to develop the research of commonsense knowledge in other languages. Previous studies constructed the knowledge bases as the relational schemas which use the expert knowledge, semi-structured text extraction and unstructured text extraction. However, with the way of extraction, these methods can only capture the explicit knowledge mentioned in the text, while the commonsense knowledge in the text is usually implicit. In this paper, we propose a commonsense generative model with a novel attention mechanism and discuss whether pre-trained language models can effectively learn and generate novel knowledge. The empirical results show that our model could generate correct commonsense knowledge with high scores which up to 50.10% precision on ATOMIC dataset humans given.},
  keywords={Training;Adaptation models;Conferences;Knowledge based systems;Question answering (information retrieval);Commonsense reasoning;Commonsense knowledge graph construction;Generative model;Attention mechanism},
  doi={10.1109/ICETCI55101.2022.9832371},
  ISSN={},
  month={May},}@INPROCEEDINGS{10849426,
  author={Yang, Yan and Ye, Feng and Xu, Dong and Zhang, Xuejie and Xu, Jin},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={LLM-Based Digital Twin Water Conservancy Knowledge Graph Construction}, 
  year={2024},
  volume={},
  number={},
  pages={656-662},
  abstract={In the study, a novel method for extracting domain-specific knowledge in digital twin water conservancy construction is proposed, addressing the challenges of interdisciplinary complexity and the limitations in existing knowledge extraction models' understanding capabilities of domain-specific knowledge. Utilizing a large language model, the novel method incorporates the domain knowledge of digital twin water conservancy into a local model framework. It employs prompt-based fine-tuning and leverages the semantic understanding and generative capabilities of the model to enhance knowledge extraction precision. To optimize entity extraction, innovative heterogeneous entity alignment strategies are introduced. The efficacy of the proposed method is validated through comparative and ablation studies conducted on a water domain corpus. Results indicate a significant improvement over conventional models, with F1 scores for entity and relationship extraction at 88.63% and 84.46%, respectively, and an entity extraction precision rate of 90.11%. Ablation experiments further demonstrate that our method outperforms the baseline large language model, enhancing the F1 scores for entity and relation extraction by 5.5 and 3.2 percentage points, respectively. By adjusting the amount of domain knowledge injected, the influence of the amount of domain text on the knowledge extraction effect is explored.},
  keywords={Large language models;Semantics;Knowledge graphs;Water conservation;Digital twins;Complexity theory;Personnel;large language model;prompt learning;knowledge graph;knowledge extraction;digital twin water conservancy construction},
  doi={10.1109/ICTAI62512.2024.00098},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{10152692,
  author={Ma, Zuyang and Yan, Kaihong and Wang, Hongwei},
  booktitle={2023 26th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
  title={BERT-based Question Answering using Knowledge Graph Embeddings in Nuclear Power Domain}, 
  year={2023},
  volume={},
  number={},
  pages={267-272},
  abstract={In order to improve the resource utilization rate of existing nuclear power data and promote workers to efficiently obtain the operation information of nuclear power units and assist them in fault diagnosis and maintenance decision-making, this paper constructs a knowledge graph question answering (KGQA) dataset in the field of nuclear power. The BEm-KGQA model based on the pre-trained language model and knowledge graph embedding method was proposed. Our model learns the embedded representation of the knowledge graph through BERT and fine-tunes the BERT model. In the question embedding stage, it learns the embedded representation of the question based on the fine-tuned BERT model. Through experiments, we demonstrate the effectiveness of the method over other models. In addition, this paper implements a nuclear power question answering system. Based on the question answering system, employees can learn about unit information and efficiently obtain information on unusual operating events of nuclear power.},
  keywords={Industries;Fault diagnosis;Federated learning;Bit error rate;Decision making;Knowledge graphs;Maintenance engineering;Knowledge Graph Question Answering;KBQA;Knowledge Graph Embedding;BERT;Nuclear Power},
  doi={10.1109/CSCWD57460.2023.10152692},
  ISSN={2768-1904},
  month={May},}@INPROCEEDINGS{10090855,
  author={Tang, Jin and Xu, Chengxian and Zhang, Wanda},
  booktitle={2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)}, 
  title={Construction and Accurate Retrieval Method of Knowledge Graph of Automobile Engine Fault}, 
  year={2023},
  volume={},
  number={},
  pages={336-345},
  abstract={In order to improve the efficiency and accuracy of automobile engine fault maintenance, an accurate retrieval method of automobile engine fault driven by knowledge graph was proposed. Firstly, the definition and framework of knowledge graph are discussed. The entity extraction of engine fault features was carried out by multi-source neural network, and the disambiguation of fault entities was carried out by integrating entity link technologies; Secondly, fault knowledge reasoning is carried out to eliminate the wrong knowledge in the knowledge base and infer new knowledge to form a complete knowledge graph.. On this basis, the retrieval subgraph of engine fault semantics is designed. Combined with the influence of physical distance and proximity, the retrieval result evaluation model is established, and the subgraph matching was carried out based on the similarity calculation of graph structure and semantic information. Finally, four knowledge graphs including entity equipment graph, ontology graph, maintenance rule graph and history graph were constructed by selecting some automobile engine fault cases from 2017 to 2020. Finally, the process architecture of engine fault search and analysis is constructed and the effectiveness of the proposed method was verified by precision rate and recall rata, which provides a new idea for accurate and efficient engine maintenance.},
  keywords={Knowledge engineering;Visualization;Semantic search;Decision making;Knowledge graphs;Maintenance engineering;Feature extraction;engine fault;knowledge graph;entity link;Subgraph matching;semantic search},
  doi={10.1109/EEBDA56825.2023.10090855},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10722780,
  author={Cheng, Xiyao and Edara, Lakshmi Srinivas and Zhang, Yuanxun and Kejriwal, Mayank and Calyam, Prasad},
  booktitle={2024 IEEE 11th International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={Influence Role Recognition and LLM-Based Scholar Recommendation in Academic Social Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1-11},
  abstract={Identifying scholars and their relevant publications in interdisciplinary collaborations within an academic social network (ASN) can help drive new scientific knowledge discovery. This involves a challenging and time-consuming process, which requires scholar's influence role recognition in a scholar team for a given research task. In this paper, we propose a novel “ScholarInfluencer” recommendation system that: (a) uses a classification model combined with network analysis on a heterogeneous knowledge graph to recognize the scholar influencers within interdisciplinary teams of collaborators, and (b) features a large language model (LLM) to use influence role recognition results to support user queries to produce pertinent scholar and their publication recommendations. Our novel approach involves building a heterogeneous knowledge graph using diverse ASN datasets involving entities such as scholars, publications, research grants, and the relationship among these entities. We perform an evaluation of ScholarInfluencer using four widely-used ASN datasets (i.e., NSF, DBLP, Cora and CA-HepTh). Our experiment results show that our influence role recognition model outperforms the state-of-the-art models across the different datasets; especially in the case of the NSF dataset, our model outperforms by up to 13.6%. Further, we show how our recommendation model with role recognition outperforms the model without role recognition across the different datasets; especially in the case of the NSF dataset, our model outperforms by 7%.},
  keywords={Analytical models;Social networking (online);Large language models;Knowledge graphs;Network analyzers;Medical services;Data models;System implementation;Recommender systems;Portals;heterogeneous knowledge graph;recommender system;large language model;academic social network},
  doi={10.1109/DSAA61799.2024.10722780},
  ISSN={2766-4112},
  month={Oct},}@INPROCEEDINGS{10825744,
  author={Zeng, Zefan and Cheng, Qing and Hu, Xingchen and Liu, Zhong and Shen, Jingke and Zhang, Yahao},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Aligning the Representation of Knowledge Graph and Large Language Model for Causal Question Answering}, 
  year={2024},
  volume={},
  number={},
  pages={1177-1186},
  abstract={Causal Question Answering (CQA) is essential for knowledge discovery, focusing on the intricate dynamics between events and entities without predefined contexts. Despite advancements of CQA models through Knowledge Graphs (KGs) and Pre-Trained Language Models (PLMs), existing approaches are hindered by knowledge conflict, insufficient capacity, and limitations in information fusion. Large Language Models (LLMs) have significantly improved natural language understanding and reasoning but often suffer from causal hallucinations. To address these challenges, we introduce KLop, a framework that aligns representations of Causal Knowledge Graph (CKG) and Large Language Models for CQA. KLop pre-trains a graph embedding model for entity embedding and uses a frozen LLM for text embedding. The main components of KLop are the descriptor module and the aligner module. The descriptor leverages descriptive texts generated by LLMs to create training data for knowledge alignment, while the aligner utilizes self-attention to train query tokens for modality alignment. Experiments on public CQA datasets validate that KLop outperforms various advanced baselines in reasoning accuracy, as well as achieving causal knowledge integration and joint reasoning.},
  keywords={Accuracy;Large language models;Knowledge based systems;Training data;Focusing;Knowledge graphs;Knowledge discovery;Cognition;Question answering (information retrieval);Optimization;causal question answering;knowledge graph;large language model;reasoning;representation},
  doi={10.1109/BigData62323.2024.10825744},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10076731,
  author={Taru, Uma and Patil, Archana},
  booktitle={2022 International Conference on Machine Learning, Computer Systems and Security (MLCSS)}, 
  title={Building Ontology for Toxic words}, 
  year={2022},
  volume={},
  number={},
  pages={241-246},
  abstract={Many online social media platforms have particular community guidelines for comment sections. The platforms that maintain commentary sections in various posts, videos, and blogs need to adhere to these guidelines. These comment sections may have specific comments that fail to satisfy the rules and regulations to maintain societal norms of communication. These comments are classified as toxic comments. Google's Perspective API defines toxic comments as comments that are rude, offensive, and likely to make someone leave the conversation. In this paper, we have built a toxic words ontology, which is as per our knowledge, first Ontology built on toxic words. This Ontology consists of toxic words and their antonyms and synonyms in increasing order of their toxicity levels. Traversing this ontology, we can find the best-suited word with less toxicity and similar meaning. This is a dynamic ontology and new words can be added easily. Thus letting us convey messages in a civil manner. We propose to reduce toxicity in the most straightforward way. After studying several papers, we found out that the toxicity mainly occurs because of use of toxic words. We also observed that use of less toxic synonyms or no toxic synonyms has huge effects on toxicity score given by the Perspective API, and results section proves that.},
  keywords={Toxicology;Social networking (online);Oral communication;Machine learning;Ontologies;Regulation;Internet;toxicity;ontology;similarity;antonyms;synonyms},
  doi={10.1109/MLCSS57186.2022.00052},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10597678,
  author={Wang, Yubo and Xin, Hao and Chen, Lei},
  booktitle={2024 IEEE 40th International Conference on Data Engineering (ICDE)}, 
  title={KGLink: A Column Type Annotation Method that Combines Knowledge Graph and Pre-Trained Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={1023-1035},
  abstract={The semantic annotation of tabular data plays a crucial role in various downstream tasks. Previous research has proposed knowledge graph (KG)-based and deep learning-based methods, each with its inherent limitations. KG-based methods encounter difficulties annotating columns when there is no match for column cells in the KG. Moreover, KG-based methods can provide multiple predictions for one column, making it challenging to determine the semantic type with the most suitable granularity for the dataset. This type granularity issue limits their scalability. On the other hand, deep learning-based methods face challenges related to the valuable context missing issue. This occurs when the information within the table is insufficient for determining the correct column type. This paper presents KGLink, a method that combines Wiki-Data KG information with a pre-trained deep learning language model for table column annotation, effectively addressing both type granularity and valuable context missing issues. Through comprehensive experiments on widely used tabular datasets encompassing numeric and string columns with varying type granularity, we showcase the effectiveness and efficiency of KGLink. By leveraging the strengths of KGLink, we successfully surmount challenges related to type granularity and valuable context issues, establishing it as a robust solution for the semantic annotation of tabular data.},
  keywords={Learning systems;Deep learning;Knowledge engineering;Annotations;Semantics;Training data;Knowledge graphs;knowledge graph;column type annotation;data mining},
  doi={10.1109/ICDE60146.2024.00083},
  ISSN={2375-026X},
  month={May},}@INPROCEEDINGS{9362652,
  author={Yu, Jianyou and Sun, Jian and Dong, Yunchang and Zhao, Dezhi and Chen, Xiaoyu and Chen, Xianghong},
  booktitle={2021 IEEE International Conference on Power Electronics, Computer Applications (ICPECA)}, 
  title={Entity recognition model of power safety regulations knowledge graph based on BERT-BiLSTM-CRF}, 
  year={2021},
  volume={},
  number={},
  pages={49-53},
  abstract={In the process of constructing the knowledge graph of power safety regulations, the traditional named entity recognition method is difficult to effectively identify the key information of the entity because the boundary of the power safety entity is fuzzy and difficult to define. Therefore, this paper proposes a power safety named entity recognition model based on BERT-BiLSTM-CRF. First, the word vector expression layer based on Transformer's bidirectional encoder (BERT) obtains word-level features; then the bidirectional long-short-term memory neural network (BiLSTM) layer is used to extract contextual features to form a feature matrix, thereby improving the accuracy of text feature extraction; The optimal tag sequence is generated by the conditional random field layer (CRF), and the output result is corrected. Through the analysis of experimental examples, the validity and superiority of the proposed model are verified.},
  keywords={Analytical models;Computational modeling;Neural networks;Feature extraction;Regulation;Power electronics;Safety;knowledge of power safety regulations;entity recognition;knowledge graph;machine learning},
  doi={10.1109/ICPECA51329.2021.9362652},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{9393163,
  author={Jia, Huiran and Li, Yuan and Song, Dandan and Wang, Qinglin},
  booktitle={2021 International Bhurban Conference on Applied Sciences and Technologies (IBCAST)}, 
  title={Entity Classification for Military Knowledge Graph based on Baidu Encyclopedia Distance Learning}, 
  year={2021},
  volume={},
  number={},
  pages={366-371},
  abstract={Entity types are a critical enabler for many NLP tasks that use KGs as a reference source. However, Classifying terminological entities without context remains an important outstanding obstacle in the field of KG completion. In this paper, we put forward a method combining distance learning and deep learning to address the classification of entity with no context. We compare the performance of our method with several text classification methods and shows our approach is empirically effective. Furthermore, the experiment result shows our approach can reduce the labeling work cost and expand the entities for further knowledge graph construction.},
  keywords={Deep learning;Industries;Computer aided instruction;Web and internet services;Text categorization;Encyclopedias;Labeling;Entity classification;Distance learning;Web crawler;Military industry knowledge graph},
  doi={10.1109/IBCAST51254.2021.9393163},
  ISSN={2151-1411},
  month={Jan},}@INPROCEEDINGS{9669508,
  author={Han, Xiaosong and Li, Xiaoran and Liang, Yanchun and Wang, Xinghao and Xu, Dong and Guan, Renchu},
  booktitle={2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Acupuncture and Tuina Knowledge Graph for Ancient Literature of Traditional Chinese Medicine}, 
  year={2021},
  volume={},
  number={},
  pages={674-677},
  abstract={The Traditional Chinese Medicine’s ancient literature recorded the massive medical theories and abundant medical experiences. To better understand and utilize, the knowledge from the literature, the Acupuncture and Tuina Knowledge Graph is proposed in this paper. Meanwhile, a deep learning network is established for acupuncture and tuina-related entity recognition and entity-relationship extraction. Finally, the trained network is able to reach an 82%+ F1-score for NER and 70%+ F1-score for relationship extraction.},
  keywords={Deep learning;Conferences;Labeling;Bioinformatics;Traditional Chinese medicine literature;acupuncture;tuina;named entity recognition;entity relationship extraction;knowledge graph},
  doi={10.1109/BIBM52615.2021.9669508},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10672720,
  author={Liu, Xuan and Liang, Zhirong},
  booktitle={2024 9th International Symposium on Computer and Information Processing Technology (ISCIPT)}, 
  title={Knowledge Distillation Based on Knowledge Graph Data Augmentation}, 
  year={2024},
  volume={},
  number={},
  pages={331-335},
  abstract={Two-stage distillation, as the name suggests, first general distillation using a task-independent general corpus, and then task-specific distillation using an augmented taskspecific corpus, is popular because of its practicality and wide applicability. Currently available data enhancement methods include substituting similar words, adding similar sentences, etc., which we believe do not do a good job of mining the relationships between corpora. There are some obvious relationships between the data, and simply replacing similar words or calculating the similarity between whole sentences does not reveal the relationship between them. We propose a new way of data enhancement, i.e. using knowledge graphs to enrich the corpus. In our experiments, we find that by combining the data augmentation approach with knowledge graphs, we can significantly improve the accuracy of the model, and our approach can improve the model performance faster in the same training time. In addition, we propose a method to vectorise computational similarities in the knowledge graph, which improves computational efficiency. These results suggest that data augmentation using knowledge graphs is a more efficient way to better explore the structure of the data and thus improve model performance.},
  keywords={Training;Accuracy;Computational modeling;Knowledge graphs;Data augmentation;Data models;Computational efficiency;knowledge distillation;knowledge graph;data augmentation;natural language processing},
  doi={10.1109/ISCIPT61983.2024.10672720},
  ISSN={},
  month={May},}@INPROCEEDINGS{10164900,
  author={Qian, Peng and Maalla, Allam and WenHai, He and ShaoQiang, Li},
  booktitle={2023 IEEE 3rd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)}, 
  title={Agricultural Planting Big Data Q & A System Technology Research Based on Knowledge Graph}, 
  year={2023},
  volume={3},
  number={},
  pages={955-959},
  abstract={In the process of agricultural production, planting has the characteristics of decentralization, complexity, seasonal differences, and regional differences, which affects agricultural growers to easily obtain effective planting information to help their own agricultural planting. The purpose of this study is to apply knowledge graph technology and natural language processing technology to the process of agricultural planting, and use the intelligent Q & A system to provide agricultural growers with regional natural conditions and suggestions and decisions of agricultural plant problems. Provide agricultural growers with a new solution for question and answer to high - efficiency and professional crop planting decisions. Finally, the intelligent decision -making and knowledge generation of farming planting, production, and operational process is achieved, and it is agricultural growers and improves farming income and farmland production efficiency. Change the agriculture from “watching the sky” to the intelligent “knowing the sky”.},
  keywords={Training;Computational modeling;Production;Knowledge graphs;Big Data;Data models;Cognition;Agricultural planting;knowledge graph;Natural language processing;Q & A system},
  doi={10.1109/ICIBA56860.2023.10164900},
  ISSN={},
  month={May},}@INPROCEEDINGS{10858986,
  author={Weng, Xinxu and Wang, Yuqing and Weng, Shijing and Xiong, Juan and Weng, Lei},
  booktitle={2024 IEEE 9th International Conference on Data Science in Cyberspace (DSC)}, 
  title={Bibliometric Analysis of Large Language Model Artificial Intelligence Based on Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={428-434},
  abstract={With the advancement of artificial intelligence technology, large-scale pre-trained language models (LLMs) have emerged as a core component of the field. This paper employs knowledge graph and bibliometric analysis tools to systematically examine and quantify research literature on large language model artificial intelligence. We delve into the evolution of research hotspots, collaboration networks, and keywords, spanning from the inception stage to the rapid development phase. Additionally, through the collaboration networks of countries/regions, research institutions, and researchers, the paper showcases the landscape of collaboration within the field and the evolving frontiers of research.},
  keywords={Deep learning;Large language models;Bibliometrics;Collaboration;Knowledge graphs;Speech recognition;Data science;Natural language processing;Artificial intelligence;Speech processing;Large Language Model;Artificial Intelligence;Knowledge Graph;Bibliometrics;Collaboration Network},
  doi={10.1109/DSC63484.2024.00064},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9362571,
  author={Gao, Shuo and Xu, Hao and Chen, Zhongmin and Wang, Yongsheng},
  booktitle={2021 IEEE International Conference on Power Electronics, Computer Applications (ICPECA)}, 
  title={Detection Mechanism of News-text matching based on Knowledge Graph}, 
  year={2021},
  volume={},
  number={},
  pages={264-267},
  abstract={Online news is an important means to spread information. However, in order to attract readers, some online news writers often add pictures that have nothing to do with the news content, which seriously affects the credibility of the media. To solve these problems, we propose a news detection mechanism based on the knowledge graph, which can enrich the news image description and use the news image description to calculate the sentence similarity with the news text, so as to identify whether a news belongs to the news image mismatch. The realization of the image-text matching detection mechanism of news enhance the credibility of social media and purify the network environment.},
  keywords={Social networking (online);Semantics;Tools;Syntactics;Media;Power electronics;Security;Knowledge graph;Image captioning;Network ecosystem},
  doi={10.1109/ICPECA51329.2021.9362571},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{9458782,
  author={Zhang, Wen and Wong, Chi-Man and Ye, Ganqiang and Wen, Bo and Zhang, Wei and Chen, Huajun},
  booktitle={2021 IEEE 37th International Conference on Data Engineering (ICDE)}, 
  title={Billion-scale Pre-trained E-commerce Product Knowledge Graph Model}, 
  year={2021},
  volume={},
  number={},
  pages={2476-2487},
  abstract={In recent years, knowledge graphs have been widely applied to organize data in a uniform way and enhance many tasks that require knowledge, for example, online shopping which has greatly facilitated people’s life. As a backbone for online shopping platforms, we built a billion-scale e-commerce product knowledge graph for various item knowledge services such as item recommendation. However, such knowledge services usually include tedious data selection and model design for knowledge infusion, which might bring inappropriate results. Thus, to avoid this problem, we propose a Pre-trained Knowledge Graph Model (PKGM) for our billion-scale e-commerce product knowledge graph, providing item knowledge services in a uniform way for embedding-based models without accessing triple data in the knowledge graph. Notably, PKGM could also complete knowledge graphs during servicing, thereby overcoming the common incompleteness issue in knowledge graphs. We test PKGM in three knowledge-related tasks including item classification, same item identification, and recommendation. Experimental results show PKGM successfully improves the performance of each task.},
  keywords={Knowledge engineering;Conferences;Data engineering;Data models;Electronic commerce;Task analysis;knowledge graph;pre-training;e-commerce},
  doi={10.1109/ICDE51399.2021.00280},
  ISSN={2375-026X},
  month={April},}@INPROCEEDINGS{10780864,
  author={Dominic, Nicholas and Pardamean, Bens},
  booktitle={2024 International Conference on Information Management and Technology (ICIMTech)}, 
  title={Knowledge Graph-Enhanced Semantic Cache for Low-Latency and Cost-Effective Inference in Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={340-344},
  abstract={In organizational knowledge management, Large Language Model (LLM) caches act as a semantic repository gathered from previous LLM responses. Due to intensive calls from multiple users, LLM may suffer from high inference latency. While there are many prior available approaches to solve this problem, most of them are inherently complex. This paper introduced a Knowledge Graph-enhanced Semantic Cache mechanism as an alternative, lightweight technique to boost retrieval for similar prompts. The latest state-of-the-art open-source LLM, named Google's Gemma-2B-it, was used to generate sample prompts and responses as a draft, while a knowledge graph (KG) was built from Wikipedia sentences. To create embeddings of prompts and KG, all-MiniLM-L6-v2 from SentenceTransformer was used. This new cache system resulted in up to 28% improvement over a standard model. In particular, reinforcement with KG cache embeddings yielded more than 85% semantic cache accuracy. To map the next trajectory of this pilot study, an overview of the extended framework for LLM knowledge management was also presented in this paper. The framework includes the new KG- enhanced cache system equipped with scalable security and fallback mechanisms that can promote green technology through substantial improvements in latency, throughput, and overall LLM costs.},
  keywords={Large language models;Semantics;Throughput;Knowledge management;Internet;Trajectory;Security;Online services;Low latency communication;Standards;semantic cache;knowledge graph;symmetric similarity search;large language models;green technology},
  doi={10.1109/ICIMTech63123.2024.10780864},
  ISSN={2837-2778},
  month={Aug},}@ARTICLE{8015195,
  author={Schneider, Georg Ferdinand and Pauwels, Pieter and Steiger, Simone},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Ontology-Based Modeling of Control Logic in Building Automation Systems}, 
  year={2017},
  volume={13},
  number={6},
  pages={3350-3360},
  abstract={The control logic implemented in building automation systems (BAS) has a significant impact on the overall energy demand of the building. However, information on the control logic, if documented, is often concealed from further data integration and reuse in heterogeneous information silos using disparate data formats. In particular, existing data formats and information models offer limited support to describe control logic explicitly. Ontology-based modeling of the control logic of BAS can potentially result in a versatile source of information for information-driven processes to further increase the performance of technical equipment in a building. Therefore, we present a novel information model, CTRLont, which allows to formally specify the domain of control logic in BAS. We demonstrate the usefulness of the novel information model by using it as a knowledge base for automating rule-based verification of designed control logic in BAS. We successfully apply the methodology to a simple control of an air handling unit and indicate a number of future steps.},
  keywords={Automation;Ontologies;Unified modeling language;OWL;Home appliances;Informatics;Process control;Building automation system (BAS);control;ontology;schedule;state graph;state machine},
  doi={10.1109/TII.2017.2743221},
  ISSN={1941-0050},
  month={Dec},}@ARTICLE{10313062,
  author={Zhang, Liyuan and Jiang, Yongquan and Yang, Yan},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={GNNGO3D: Protein Function Prediction Based on 3D Structure and Functional Hierarchy Learning}, 
  year={2024},
  volume={36},
  number={8},
  pages={3867-3878},
  abstract={Protein sequences accumulate in large quantities, and the traditional method of annotating protein function by experiment has been unable to bridge the gap between annotated proteins and unannotated proteins. Machine learning-based protein function prediction is an effective approach to solve this problem. Most of the existing methods only use the protein sequence but ignore the three-dimensional structure which is closely related to the protein function. And the hierarchy of protein functions is not adequately considered. To solve this problem, we propose a graph neural network (GNNGO3D) that combines the three-dimensional structure and functional hierarchy learning. GNNGO3D simultaneously uses three kinds of information: protein sequence, tertiary structure, and hierarchical relationship of protein function to predict protein function. The novelty of GNNGO3D lies in that it integrates the learning of functional level information into the method of predicting protein function by using tertiary structure information, fully learning the relationship between protein functions, and helping to better predict protein function. Experimental results show that our method is superior to existing methods for predicting protein function based on sequence and structure.},
  keywords={Proteins;Feature extraction;Protein sequence;Three-dimensional displays;Task analysis;Ontologies;Convolutional neural networks;Graph neural networks;gene ontology;language model;machine learning;protein function prediction},
  doi={10.1109/TKDE.2023.3331005},
  ISSN={1558-2191},
  month={Aug},}@INPROCEEDINGS{8710519,
  author={Landolfi, Giuseppe and Bami, Andrea and Izzo, Gabriele and Montini, Elias and Bettoni, Andrea and Vujasinovic, Marko and Gugliotta, Alessio and Soares, Antόnio Lucas and Diogo Silva, Henrique},
  booktitle={2018 International Conference on Intelligent Systems (IS)}, 
  title={An Ontology Based Semantic Data Model Supporting A Maas Digital Platform}, 
  year={2018},
  volume={},
  number={},
  pages={896-904},
  abstract={The integration of IoT infrastructures across production systems, together with the extensive digitalisation of industrial processes, are drastically impacting manufacturing value chains and the business models built on the top of them. By exploiting these capabilities companies are evolving the nature of their businesses shifting value proposition towards models relying on product servitization and share, instead of ownership. In this paper, we describe the semantic data-model developed to support a digital platform fostering the reintroduction in the loop and optimization of unused industrial capacity. Such data-model aims to establish the main propositions of the semantic representation that constitutes the essential nature of the ecosystem to depict their interactions, the flow of resources and exchange of production services. The inference reasoning on the semantic representation of the ecosystem allows to make emerge nontrivial and previously unknown opportunities. This will apply not only to the matching of demand and supply of manufacturing services, but to possible and unpredictable relations. For instance, a particular kind of waste being produced at an ecosystem node can be linked to the requirements for an input material needed in a new product being developed on the platform, or new technologies can be suggested to enhance processes under improvement. The overall architecture and individual ontologies are presented and their usefulness is motivated via the application to use cases.},
  keywords={US Department of Transportation;Technological innovation;Production;Reliability;Manganese;Ions;Stakeholders;servitization;manufacturing ontologies;semantic data-model;knowledge discovery},
  doi={10.1109/IS.2018.8710519},
  ISSN={1541-1672},
  month={Sep.},}@INPROCEEDINGS{10578654,
  author={Abu-Rasheed, Hasan and Weber, Christian and Fathi, Madjid},
  booktitle={2024 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={In the era of personalized education, the provision of comprehensible explanations for learning recommendations is of great value to enhance the learner's understanding and engagement with the recommended learning content. Large language models (LLMs) and generative AI have recently opened new doors for generating human-like explanations, for and along learning recommendations. However, their precision is still far away from acceptable in a sensitive field like education. To harness the abilities of LLMs, while still ensuring a high level of precision towards the intent of the learners, this paper proposes an approach to utilize knowledge graphs (KG) as a source of factual context for LLM prompts, reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context. We utilize the semantic relations in the knowledge graph to offer curated knowledge about learning recommendations. With domain-experts in the loop, we design the explanation as a textual template, which is filled and completed by the LLM. Domain experts were integrated in the prompt engineering phase as part of a study, to ensure that explanations include information that is relevant to the learner. We evaluate our approach quantitatively using Rouge-N and Rouge-L measures, as well as qualitatively with experts and learners. Our results show an enhanced recall and precision of the generated explanations compared to those generated solely by the GPT model, with a greatly reduced risk of generating imprecise information in the final learning explanation.},
  keywords={Shape;Large language models;Semantics;Knowledge graphs;Prompt engineering;Data mining;Engineering education;Large language models (LLMs);Knowledge graphs;ChatGPT;Generative AI (GenAI) Learning recommendations;explainable AI (XAI)},
  doi={10.1109/EDUCON60312.2024.10578654},
  ISSN={2165-9567},
  month={May},}@INPROCEEDINGS{9724493,
  author={Liu, Yuefeng and Guo, Wei and Zhang, Hanyu and Bian, Haodong and He, Yingjie and Zhang, Xiaoyan and Gong, Yanzhang and Dong, Jianmin and Liu, Zhen},
  booktitle={2021 IEEE 20th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)}, 
  title={Construction of Knowledge Graph Based on Discipline Inspection and Supervision}, 
  year={2021},
  volume={},
  number={},
  pages={1467-1472},
  abstract={To solve the problems of large number of notifications, low relevance and no relevant knowledge base in the field of discipline inspection, a method of constructing a knowledge map of discipline inspection and supervision based on the BERT-BiLSTM-CRF model is proposed. Firstly, the unstructured data is collected from the content of the disciplinary inspection and supervision report. Through the bottom-up method the notification concept layer is constructed. By using deep learning models to extract entities. Then the entities and semantic relations are stored in the graph database Neo4j and displayed in the form of a knowledge graph. This method realizes the whole process from unstructured data to knowledge graph, and provides technical reference for the construction of domain-based knowledge graph. Simultaneously, the knowledge map of discipline inspection field established through the example can find the hidden association between those who break the law and discipline, prevent criminal facts in advance, and provide support and help for discipline inspection personnel to implement the spirit of the eight-point regulation of the Central Committee and continue to fight against the “four winds” and other actions.},
  keywords={Deep learning;Databases;Soft sensors;Knowledge based systems;Semantics;Inspection;Transformers;discipline inspection and supervision;knowledge graph;NER(natural language processing);BERT(Bidirectional Encoder Representations from Transformers);graph database},
  doi={10.1109/TrustCom53373.2021.00209},
  ISSN={2324-9013},
  month={Oct},}@INPROCEEDINGS{9095669,
  author={Chondamrongkul, Nacha and Sun, Jing and Warren, Ian},
  booktitle={2020 IEEE International Conference on Software Architecture Companion (ICSA-C)}, 
  title={Automated Security Analysis for Microservice Architecture}, 
  year={2020},
  volume={},
  number={},
  pages={79-82},
  abstract={Designing a software system that applied the microservice architecture style is a challenging task, as its characteristics are vulnerable to various security attacks. Software architect, therefore, needs to pinpoint the security flaws in the design before the implementation can proceed. This task is error-prone as it requires manual analysis on the design model, to identify security threats and trace possible attack scenarios. This paper presents an automated security analysis approach for microservice architecture. Our approach can automatically identify security threats according to a collection of formally defined security characteristics and provide an insightful result that demonstrates how the attack scenarios may happen. A collection of formally defined security characteristics can be extended to support other security characteristics not addressed in this paper.},
  keywords={Security;Computer architecture;Connectors;Ontologies;Containers;Tools;Analytical models;Microservice Architecture;Security Analysis;Ontology Web Language;Model Checking},
  doi={10.1109/ICSA-C50368.2020.00024},
  ISSN={},
  month={March},}@INPROCEEDINGS{9603816,
  author={Lu, Yonghe and Zhao, Ruijie and Huang, Shan and Liu, Runjia},
  booktitle={2021 7th Annual International Conference on Network and Information Systems for Computers (ICNISC)}, 
  title={Construction of Diabetes Knowledge Graph Based on Deep Learning}, 
  year={2021},
  volume={},
  number={},
  pages={966-970},
  abstract={To integrate medical data which is scattered over the internet, natural language processing (NLP) is widely used in medical text mining. BERT (Bidirectional Encoder Representations from Transformers) is outstanding among many other representation models and vector representation based on Bert pre-training language model can help the target task learn more semantic information. The knowledge graph intuitively reveals the relationship between entities and helps explore deeper semantic connections between entities. There are three important parts in the construction of a knowledge graph, including entity extraction, relation extraction, and graph generation. Based on these methods this paper proposes a Bert-based named entities identification model Bert-BiLSTM-CRF and it is outperforming the established methods. In the relation extraction part, use the BERT-Softmax to improve the semantic expression and its F1-value increased by 12 percent compared with the traditional entity relation extraction model. Based on the above redefined the entities of diabetes and their relationships to enrich the semantics of the knowledge graph. Finally, the Neo4j graph database was used to realize the visualization of the diabetes knowledge map.},
  keywords={Knowledge engineering;Text mining;Semantics;Search engines;Transformers;Natural language processing;Diabetes;named entity recognition;relation extraction;knowledge extraction},
  doi={10.1109/ICNISC54316.2021.00181},
  ISSN={},
  month={July},}@INPROCEEDINGS{10710806,
  author={Meyer, Frederic and Freitag, Lennart and Hinrichsen, Sven and Niggemann, Oliver},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Potentials of Large Language Models for Generating Assembly Instructions}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={With the increasing complexity in manual assembly and a demographic decline in skilled workforce, the importance of well-documented processes through assembly instructions has grown. Creating these instructions is a time-consuming and knowledge-intensive task that typically relies on experienced employees. Although various automation solutions have been proposed to assist in generating assembly instructions, they often fall short in providing detailed textual guidance. With the rise of generative artificial intelligence (AI), new potentials arise in this domain. Therefore, this paper explores these potentials by employing various large language models (LLMs), prompting techniques and input data in an experimental setup for generating detailed assembly instructions, including the planning of assembly sequences as well as textual guidance on tools, assembly activities, and quality assurance measures. The findings reveal promising opportunities in leveraging LLMs but also substantial challenges, particularly in assembly sequence planning. To improve the reliability of generating assembly instructions, we propose a multi-agent concept that decomposes the complex task into simpler subtasks, each managed by specialized agents.},
  keywords={Quality assurance;Generative AI;Large language models;Decision making;Manuals;Planning;Complexity theory;Reliability;Assembly;Manufacturing automation;agent;assembly instruction;experiment;GPT;large language model;LLM;prompt},
  doi={10.1109/ETFA61755.2024.10710806},
  ISSN={1946-0759},
  month={Sep.},}@ARTICLE{10141863,
  author={Xie, Zhiwen and Zhu, Runjie and Liu, Jin and Zhou, Guangyou and Huang, Jimmy Xiangji},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={TARGAT: A Time-Aware Relational Graph Attention Model for Temporal Knowledge Graph Embedding}, 
  year={2023},
  volume={31},
  number={},
  pages={2246-2258},
  abstract={Temporal knowledge graph embedding (TKGE) aims to learn the embedding of entities and relations in a temporal knowledge graph (TKG). Although the previous graph neural networks (GNN) based models have achieved promising results, they cannot directly capture the interactions of multi-facts at different timestamps. To address the above limitation, we propose a time-aware relational graph attention model (TARGAT), which takes the multi-facts at different timestamps as a unified graph. First, we develop a relational generator to dynamically generate a series of time-aware relational message transformation matrices, which jointly models the relations and the timestamp information into a unified way. Then, we apply the generated message transformation matrices to project the neighborhood features into different time-aware spaces and aggregate these neighborhood features to explicitly capture the interactions of multi-facts. Finally, a temporal transformer classifier is applied to learn the representation of the query quadruples and predict the missing entities. The experimental results show that our TARGAT model beats the GNN-based models by a large margin and achieves new state-of-the-art results on four popular benchmark datasets.},
  keywords={Graph neural networks;Task analysis;Knowledge graphs;Convolution;Aggregates;Speech processing;Interpolation;Graph embedding;link prediction;knowledge graph;representation learning},
  doi={10.1109/TASLP.2023.3282101},
  ISSN={2329-9304},
  month={},}@INPROCEEDINGS{9533753,
  author={Yang, Xia and Chiang, Meng-Fen and Lee, Wang-Chien and Chang, Yi},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Cost-Effective Knowledge Graph Reasoning for Complex Factoid Questions}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={The task of reasoning over knowledge graph for factoid questions has received significant interest from the research community of natural language processing. Performing this task inevitably faces the issues of question complexity and reasoning efficiency. In this paper, we investigate modern reasoning approaches over knowledge graph to tackle complex factoid questions of diverse reasoning schemas with attractive speedup in computational efficiency. To this end, we propose two evidence retrieval strategies to generate concise and informative evidence graph of high semantic-relevance and factual coverage to the question. Then, we adopt DELFT, a graph neural networks based framework that takes the linguistic structure representation of a question and the evidence graph as input, to predict the answer by reasoning over the evidence graph. We evaluate the performance across several baselines in terms of effectiveness and efficiency on two real-world datasets, MOOCQA and MetaQA. The results show the superiority of message passing paradigm in delivering a robust reasoner with better answer quality and significantly improved computational efficiency.},
  keywords={Message passing;Linguistics;Cognition;Natural language processing;Graph neural networks;Computational efficiency;Complexity theory;Factoid Question Answering;Knowledge Graph;Reasoning},
  doi={10.1109/IJCNN52387.2021.9533753},
  ISSN={2161-4407},
  month={July},}@ARTICLE{10185050,
  author={Razzaq, Muhammad Saad and Maqbool, Fahad and Ilyas, Muhammad and Jabeen, Hajira},
  journal={IEEE Access}, 
  title={EvoRecipes: A Generative Approach for Evolving Context-Aware Recipes}, 
  year={2023},
  volume={11},
  number={},
  pages={74148-74164},
  abstract={Generative AI e.g. Large Language Models (LLMs) can be used to generate new recipes. However, LLMs struggle with more complex aspects like recipe semantics and process comprehension. Furthermore, LLMs have limited ability to account for user preferences since they are based on statistical patterns. As a result, these recipes may be invalid. Evolutionary algorithms inspired by the process of natural selection are optimization algorithms that use stochastic operators to generate new solutions. These algorithms can generate large number of solutions from the set of possible solution space. Moreover, these algorithms have the capability to incorporate user preferences in fitness function to generate novel recipes that are more aligned with the fitness objective. In this paper, we propose the  $EvoRecipes$  framework to generate novel recipes. The  $EvoRecipes$  framework utilizes both Genetic Algorithm and generative AI in addition to  $RecipeOn$  ontology, and  $RecipeKG$  knowledge graph. Genetic Algorithm explore the large solution space of encoded recipe solutions and are capable of incorporating user preferences, while LLMs are used to generate recipe text from encoded recipe solutions.  $EvoRecipes$  uses a population of context-aware recipe solutions from the  $RecipeKG$  knowledge graph.  $RecipeKG$  encodes recipes in RDF format using classes and properties as defined in the  $RecipeOn$  ontology. Moreover, to evaluate the alignment of  $EvoRecipe$  generated recipes with multiple intended objectives, we propose a fitness function that incorporates novelty, simplicity, visual appeal, and feasibility. Additionally, to evaluate the quality of the  $EvoRecipe$  generated recipes while considering the subjective nature of recipes, we conducted a survey using multi-dimensional metrics (i.e. contextual, procedural, and novelty). Results show that  $EvoRecipes$  generated recipes are novel, valid and incorporate user preferences.},
  keywords={Ontologies;Creativity;Resource description framework;Knowledge graphs;Genetic algorithms;Sociology;Semantics;Food products;Knowledge graph;ontology;computational creativity;recipe evolution;recipe;food},
  doi={10.1109/ACCESS.2023.3296144},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{7950320,
  author={Janulevičius, Justinas and Marozas, Leonardas and Čenys, Antanas and Goranin, Nikolaj and Ramanauskaitė, Simona},
  booktitle={2017 Open Conference of Electrical, Electronic and Information Sciences (eStream)}, 
  title={Enterprise architecture modeling based on cloud computing security ontology as a reference model}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  abstract={The recent growth of popularity of cloud computing services delivers various benefits in multiple fields of activity, including reduced resource costs due to optimized hardware setup, as well as improving mobility. However, cloud computing has some issues that need to be clarified, one of which is the security of cloud computing. The dispersion of cloud service vendors means that most security issues are still addressed often as unique to the particular vendor or cloud computing system, lacking standardization of the procedures and protocols that users and vendors follow when implementing a cloud computing system and running it. This paper deals with cloud computing security management. It offers an ontology, designed by the authors of the paper, specifically built to deal with cloud security controls covering the most important documents of the domain of cloud security management and the implementation of the proposed ontology to an enterprise architecture modeling language.},
  keywords={Cloud computing;Security;Ontologies;Computer architecture;Computational modeling;Analytical models;Documentation;ontology;cloud computing;information security;enterprise architecture},
  doi={10.1109/eStream.2017.7950320},
  ISSN={},
  month={April},}@INPROCEEDINGS{10803829,
  author={Suleykin, Alexander and Panfilov, Peter},
  booktitle={2024 6th International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA)}, 
  title={Smart Technical Support System Development Using Knowledge Map-Aided Approach}, 
  year={2024},
  volume={},
  number={},
  pages={455-460},
  abstract={Strong technical support is crucial to success of any brand. It provides answers to customer's questions and solutions to different situations and is an important factor in keeping customers loyal. A new level of technical support can be achieved through combination of traditional service automation offers with the current advancement of Generative Artificial Intelligence (GenAI) embedded into technical support. GenAI in general and the large language models (LLM s) in particular create new opportunities for natural conversational interfaces and the development of ‘smart’ technical support systems. Based on experiences and observations from other intelligent assistance projects, this paper presents new methodological perspectives from academia and best practice from industry on architecting intelligent technical support systems. It discusses the impact of GenAI and LLMs through real cases supporting an ongoing validation. The maj or focus of this paper is on architectural models for intelligent technical support systems, showing the fundamental Knowledge Map mechanism of AIbased customer service, which allows for not only controlling the information used to generate responses and minimizing errors but also automatically processes requests in accounting systems. An example of successful implementation of Knowledge Mapbased approach is given, where the intelligent assistant chat-bot demonstrates high accuracy of answers and in case of uncertainty redirects requests to human operators. The paper presents the key architectural model perspectives for the development of intelligent technical support systems.},
  keywords={Industries;Automation;Uncertainty;Generative AI;Large language models;Customer services;Process control;Control systems;Mathematical models;Energy efficiency;technical support;smart assistant;chat-bot;large language model;knowledge map;knowledge graph},
  doi={10.1109/SUMMA64428.2024.10803829},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9084840,
  author={Zhang, Xiao and Li, Chuanzhen and Du, Huaichang},
  booktitle={2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)}, 
  title={Named Entity Recognition for Terahertz Domain Knowledge Graph based on Albert-BiLSTM-CRF}, 
  year={2020},
  volume={1},
  number={},
  pages={2602-2606},
  abstract={Named entity recognition is a vital part of the question answering system. The current methods for identifying named entities are mostly for short entities. In our terahertz domain question answering system, there are both long and short entities in questions. In this paper, an entity recognition method based on Albert-BiLSTM-CRF is applied to recognize long entity completely. Specifically, a pre-trained model of deep bidirectional representation is applied to fully understand the semantics of the entire sentence. The experimental results on the terahertz domain dataset show that the proposed method improves the accuracy of long domain entity recognition and improves the performance of the terahertz domain question answering system.},
  keywords={Training;Automation;Conferences;Semantics;Knowledge discovery;Information technology;Context modeling;albert;BiLSTM;CRF;named entity recognition;terahertz domain;knowledge graph},
  doi={10.1109/ITNEC48623.2020.9084840},
  ISSN={},
  month={June},}@INPROCEEDINGS{10422308,
  author={Tang, Yun and Da Costa, Antonio A. Bruto and Zhang, Xizhe and Patrick, Irvine and Khastgir, Siddartha and Jennings, Paul},
  booktitle={2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)}, 
  title={Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain}, 
  year={2023},
  volume={},
  number={},
  pages={3893-3900},
  abstract={Engineering knowledge-based (or expert) systems require extensive manual effort and domain knowledge. As Large Language Models (LLMs) are trained using an enormous amount of cross-domain knowledge, it becomes possible to automate such engineering processes. This paper presents an empirical automation and semi-automation framework for domain knowledge distillation using prompt engineering and the LLM ChatGPT. We assess the framework empirically in the autonomous driving domain and present our key observations. In our implementation, we construct the domain knowledge ontology by “chatting” with ChatGPT. The key finding is that while fully automated domain ontology construction is possible, human supervision and early intervention typically improve efficiency and output quality as they lessen the effects of response randomness and the butterfly effect. We, therefore, also develop a web-based distillation assistant enabling supervision and flexible intervention at runtime. We hope our findings and tools could inspire future research toward revolutionizing the engineering of knowledge-based systems across application domains.},
  keywords={Knowledge engineering;Runtime;Manuals;Ontologies;Chatbots;Autonomous vehicles;Intelligent transportation systems;large language model;domain ontology distillation;autonomous driving},
  doi={10.1109/ITSC57777.2023.10422308},
  ISSN={2153-0017},
  month={Sep.},}@INPROCEEDINGS{10730393,
  author={Bhat, Vani and Cheerla, Sree Divya and Mathew, Jinu Rose and Pathak, Nupur and Liu, Guannan and Gao, Jerry},
  booktitle={2024 IEEE 10th International Conference on Big Data Computing Service and Machine Learning Applications (BigDataService)}, 
  title={Retrieval Augmented Generation (RAG) Based Restaurant Chatbot with AI Testability}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={Post-COVID the restaurant industry is experiencing a surge in demand, presenting a unique challenge of efficiently managing increased customer flow while ensuring seamless interactions. Chatbots have emerged as an innovative solution to meet the demand increase. The paper addresses the enhancement of AI chatbots through the integration of Retrieval-Augmented Generation (RAG) with the Large Language Model (LLM). This paper focuses on the development of a restaurant chatbot that not only engages in natural-language conversations but also addresses context optimization and LLM optimization for restaurant context learning. The approach uses a Neo4j Knowledge graph built using the restaurant data as an external source of knowledge. The graph is traversed to match the user question with appropriate answer tokens using Term Frequency - Inverse Document Frequency (TF-IDF) embeddings. The relevant tokens along with user questions are used to provide additional context to the T5 language model to provide nuanced responses to the users. This improvement is quantitatively evidenced by a Bilingual Evaluation Understudy (BLEU) score of 0.60, indicating a high level of precision in language understanding and generation. An extensive evaluation of the chatbot includes assessing AI testability on the level of words, sentences, and information. These evaluations include simulated dialogue assessments and performance analyses, with a focus on the chatbot's ability to retrieve and integrate information. Based on the AI testability evaluation, the models consistently produce more knowledgeable, diverse, and relevant answers as compared with state-of-the-art models with an average information score in the range of 0.6-0.8.},
  keywords={Industries;Large language models;Knowledge graphs;Oral communication;Chatbots;Performance analysis;Artificial intelligence;Surges;Standards;Optimization;Natural Language Processing (NLP);Retrieval Augmented Generation (RAG);Large Language Model (LLM);Information Retrieval (IR);Knowledge Graph;AI Testability},
  doi={10.1109/BigDataService62917.2024.00008},
  ISSN={2690-828X},
  month={July},}@INPROCEEDINGS{10386454,
  author={Chen, Bohan and Bertozzi, Andrea L.},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={AutoKG: Efficient Automated Knowledge Graph Generation for Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={3117-3126},
  abstract={Traditional methods of linking large language models (LLMs) to knowledge bases via the semantic similarity search often fall short of capturing complex relational dynamics. To address these limitations, we introduce AutoKG, a lightweight and efficient approach for automated knowledge graph (KG) construction. For a given knowledge base consisting of text blocks, AutoKG first extracts keywords using a LLM and then evaluates the relationship weight between each pair of keywords using graph Laplace learning. We employ a hybrid search scheme combining vector similarity and graph-based associations to enrich LLM responses. Preliminary experiments demonstrate that AutoKG offers a more comprehensive and interconnected knowledge retrieval mechanism compared to the semantic similarity search, thereby enhancing the capabilities of LLMs in generating more insightful and relevant outputs.},
  keywords={Training;Semantics;Knowledge based systems;Data visualization;Knowledge graphs;Computer architecture;Big Data;Language model;Knowledge Graph;Graph Learning;Retrieval-augmented Generation},
  doi={10.1109/BigData59044.2023.10386454},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10407599,
  author={Tu, Ming-Yu and Ehm, Hans and Ismail, Abdelgafar and Ulrich, Philipp},
  booktitle={2023 Winter Simulation Conference (WSC)}, 
  title={Reusable Ontology Generation and Matching from Simulation Models}, 
  year={2023},
  volume={},
  number={},
  pages={2298-2309},
  abstract={As simulating semiconductor manufacturing grows complex, model reuse becomes appealing since it can reduce the time incurred in developing future models. Also, considering a large network of the semiconductor supply chain, knowledge sharing can enable the efficient development of simulation models in a collaborative organization. Such necessity of reusability and interoperability of simulation models motivates this paper. We will address these challenges through ontological modeling and linking of the simulation components. The first application is generating reusable ontologies from simulation models. Another discussed application is ontology matching for knowledge sharing between simulation components and a meta-model of the semiconductor supply chain. The proposed approach succeeds in automatically transforming simulation into reusable knowledge and identifying interconnection in a semiconductor manufacturing system.},
  keywords={Semiconductor device modeling;Knowledge engineering;Supply chains;Organizations;Ontologies;Semiconductor device manufacture;Interoperability},
  doi={10.1109/WSC60868.2023.10407599},
  ISSN={1558-4305},
  month={Dec},}@INPROCEEDINGS{8756968,
  author={Bučko, B. and Zábovská, K. and Zábovský, M.},
  booktitle={2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)}, 
  title={Ontology as a Modeling Tool within Model Driven Architecture Abstraction}, 
  year={2019},
  volume={},
  number={},
  pages={1525-1530},
  abstract={This paper is focused on automatic transformation process of top levels of Model Driven Architecture (MDA) within the information system development phase. System architects are always trying to find easier, complex and more united way of information system development. Although the Model Driven Architecture (MDA) provides a set of guidelines for the structuring of specifications it also comes with challenging tasks of transformations between the various levels of abstraction. The primary objective of this work is to design a universal automated approach within the Computer Independent Model (CIM) and Platform Independent Model (PIM) manual transformation. The manual process of the transformations within MDA could be automated using ontology model with the combination of mapping rules and Extensible Markup Process Definition Language (XPDL) and Extensible Markup Language Metadata interchange (XMI) conversion.},
  keywords={Unified modeling language;Ontologies;Business;Computational modeling;Data models;Computer architecture;Information systems;Model driven architecture;Computer independent model;Platform independent model;Ontology;Information system development},
  doi={10.23919/MIPRO.2019.8756968},
  ISSN={2623-8764},
  month={May},}@INPROCEEDINGS{8051363,
  author={Zhou, Jiale and Hänninen, Kaj and Lundqvist, Kristina},
  booktitle={2017 43rd Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, 
  title={A Hazard Modeling Language for Safety-Critical Systems Based on the Hazard Ontology}, 
  year={2017},
  volume={},
  number={},
  pages={301-304},
  abstract={Preliminary hazard analysis (PHA) is a key safetyconcerned activity to identify potential hazards. However, since various stakeholders will be involved in the identification process, a common understanding of the nature of hazards among stakeholders, such as what a hazard consists of and how to describe it without ambiguities, is of crucial importance to achieve the goal of PHA. In this work, we propose a hazard modeling language (HML) based on a domain ontology to facilitate the specification of identified hazards. In addition, we present an approach to guide the transformation from natural language hazard descriptions into the HML specification. Finally, an industrial PHA example is used to illustrate the usefulness of our work.},
  keywords={Hazards;Ontologies;Unified modeling language;Stakeholders;Analytical models;Natural languages;Injuries;preliminary hazard analysis;hazard ontology;hazard modeling language},
  doi={10.1109/SEAA.2017.48},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10740163,
  author={Wang, Li-C.},
  booktitle={2024 ACM/IEEE 6th Symposium on Machine Learning for CAD (MLCAD)}, 
  title={LLM-Assisted Analytics in Semiconductor Test (Invited)}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={The emergence of Large Language Models (LLMs) has impacted our perspective on applying Machine Learning (ML) in semiconductor test. This paper shares our experience in leveraging the power of LLMs to build an AI agent for test data analytics. We advocate for an end-to-end approach where the Knowledge Graph (KG) plays a central role. Using wafermap analytics as an example, we highlight the key ideas behind developing the LLM-assisted AI agent named IEA-Plot, and discuss its practical applications.CCS Concepts• Hardware → Hardware test; • Computing methodologies → Artificial intelligence.},
  keywords={Solid modeling;Analytical models;Data analysis;Grounding;Large language models;Machine learning;Knowledge graphs;Hardware;Data models;Optimization;Test Data Analytics;Large Language Model;Knowledge Graph;Machine Learning},
  doi={10.1109/MLCAD62225.2024.10740163},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10030025,
  author={Kabir, Anowarul and Shehu, Amarda},
  booktitle={2022 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={Sequence-Structure Embeddings via Protein Language Models Improve on Prediction Tasks}, 
  year={2022},
  volume={},
  number={},
  pages={105-112},
  abstract={Building on the transformer architecture and its revolutionizing of language models for natural language processing, protein language models (PLMs) are now emerging as a powerful tool for learning over large numbers of sequences in protein sequence databases and linking protein sequence to function. PLMs are shown to learn useful, task-agnostic sequence representations that allow predicting protein secondary structure, protein subcellular localization, and evolutionary relationships within protein families. However, existing models are strictly trained over protein sequences and miss an opportunity to leverage and integrate the information present in heterogeneous data sources. In this paper, inspired by the intrinsic role of three-dimensional/tertiary protein structure in determining a broad range of protein properties, we propose a PLM that integrates and attends to both protein sequence and tertiary structure. In particular, this paper posits that learning joint sequence-structure representations yields better representations for function-related prediction tasks. A detailed experimental evaluation shows that such joint sequence-structure representations are more powerful than sequence-based representations, yield better performance on superfamily membership across various metrics, and capture interesting relationships in the PLM-learned embedding space.},
  keywords={Location awareness;Soft sensors;Semantics;Training data;Predictive models;Transformers;Protein sequence;Protein language model;Transformer;Sequence structure transformer;Protein function;superfamily},
  doi={10.1109/ICKG55886.2022.00021},
  ISSN={},
  month={Nov},}@ARTICLE{10417790,
  author={Yang, Linyao and Chen, Hongyang and Li, Zhao and Ding, Xiao and Wu, Xindong},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling}, 
  year={2024},
  volume={36},
  number={7},
  pages={3091-3110},
  abstract={Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention. Due to their powerful emergent abilities, recent LLMs are considered as a possible alternative to structured knowledge bases like knowledge graphs (KGs). However, while LLMs are proficient at learning probabilistic language patterns and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance in generating texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-trained language models (KGPLMs) as well as their applications. Inspired by existing studies on KGPLM, this paper proposes enhancing LLMs with KGs by developing knowledge graph-enhanced large language models (KGLLMs). KGLLM provides a solution to enhance LLMs’ factual reasoning ability, opening up new avenues for LLM research.},
  keywords={Task analysis;Training;Long short term memory;Knowledge graphs;Transformers;Knowledge based systems;Chatbots;Large language model;knowledge graph;ChatGPT;knowledge reasoning;knowledge management},
  doi={10.1109/TKDE.2024.3360454},
  ISSN={1558-2191},
  month={July},}@INPROCEEDINGS{9669736,
  author={Huang, Lan and Guan, Hongrui and Liang, Yanchun and Guan, Renchu and Feng, Xiaoyue},
  booktitle={2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={COVID-19 Knowledge Graph for Drug and Vaccine Development}, 
  year={2021},
  volume={},
  number={},
  pages={328-333},
  abstract={The worldwide spread of COVID-19 has made a severe impact on human health and life. It has shown rapid propagation, long in vitro survival, and a long incubation period. More seriously, COVID-19 is more susceptible to variation, as it is an RNA virus. Mutations of COVID-19 have been reported in multiple countries worldwide, which makes drug and vaccine development a significant challenge. To search for potential drugs and vaccines and reveal the atlas of COVID-19 evolution, we extract information from massive unstructured data and construct a COVID-19 knowledge graph using the COVID-19 data. Based on machine learning approaches, we infer and predict novel coronavirus pneumonia-related diseases, drug action targets, etc. to speculate on new and more effective treatment methods. In addition, to study transcriptome of SARS-CoV-2, new ideas can be provided to biomedical experts with flexible responses to viral variation. An in-depth analysis of the COVID-19 pathomechanism at the pharmaceutical, genetic, and protein levels provides effective means and tools for novel coronavirus pneumonia vaccines, drug development, and therapeutic program design.},
  keywords={COVID-19;Drugs;Proteins;Protein engineering;Pulmonary diseases;RNA;Machine learning;COVID-19;knowledge graph;vaccine;mutation},
  doi={10.1109/BIBM52615.2021.9669736},
  ISSN={},
  month={Dec},}@ARTICLE{10531671,
  author={Song, Yaoxian and Sun, Penglei and Liu, Haoyu and Li, Zhixu and Song, Wei and Xiao, Yanghua and Zhou, Xiaofang},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI}, 
  year={2024},
  volume={36},
  number={11},
  pages={6962-6976},
  abstract={Embodied AI is one of the most popular studies in artificial intelligence and robotics, which can effectively improve the intelligence of real-world agents (i.e. robots) serving human beings. Scene knowledge is important for an agent to understand the surroundings and make correct decisions in the varied open world. Currently, knowledge base for embodied tasks is missing and most existing work use general knowledge base or pre-trained models to enhance the intelligence of an agent. For conventional knowledge base, it is sparse, insufficient in capacity and cost in data collection. For pre-trained models, they face the uncertainty of knowledge and hard maintenance. To overcome the challenges of scene knowledge, we propose a scene-driven multimodal knowledge graph (Scene-MMKG) construction method combining conventional knowledge engineering and large language models. A unified scene knowledge injection framework is introduced for knowledge representation. To evaluate the advantages of our proposed method, we instantiate Scene-MMKG considering typical indoor robotic functionalities (Manipulation and Mobility), named ManipMob-MMKG. Comparisons in characteristics indicate our instantiated ManipMob-MMKG has broad superiority on data-collection efficiency and knowledge quality. Experimental results on typical embodied tasks show that knowledge-enhanced methods using our instantiated ManipMob-MMKG can improve the performance obviously without re-designing model structures complexly.},
  keywords={Task analysis;Knowledge graphs;Artificial intelligence;Knowledge based systems;Robots;Knowledge engineering;Visualization;Multimodal knowledge graph;scene driven;embodied AI;robotic intelligence},
  doi={10.1109/TKDE.2024.3399746},
  ISSN={1558-2191},
  month={Nov},}@INPROCEEDINGS{9667755,
  author={Lu, Yuxun and Ichise, Ryutaro},
  booktitle={2021 IEEE International Conference on Big Knowledge (ICBK)}, 
  title={Unsupervised Type Constraint Inference in Bilinear Knowledge Graph Completion Models}, 
  year={2021},
  volume={},
  number={},
  pages={15-22},
  abstract={Knowledge graph completion (KGC) models aim to provide a feasible way of manipulating facts in knowledge graphs. Most KGC models do not consider type constraint in relations due to the scarcity of type information in training data. We proposed an unsupervised method for inferring type constraint based on existing bilinear KGC models. Our method induces two type indicators into every relation and adjusts the location of entity embeddings in feature space to match the type indicators. Our approach eliminates the external feature space for entity types and type constraints in relations and has a consistent feature space; therefore, it has fewer parameters than other methods. Experiments show that our methods can improve the performance of the base models and outperform other methods on datasets about general knowledge.},
  keywords={Conferences;Refining;Training data;Benchmark testing;Data models;Type Constraint Inference;Knowledge Graph Completion Model;Unsupervised Learning},
  doi={10.1109/ICKG52313.2021.00012},
  ISSN={},
  month={Dec},}@ARTICLE{10219140,
  author={Ding, Yi and Li, He and Zhu, Feng and Wang, Zhe and Peng, Weiwen and Xie, Min},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={A Semi-Supervised Failure Knowledge Graph Construction Method for Decision Support in Operations and Maintenance}, 
  year={2024},
  volume={20},
  number={3},
  pages={3104-3114},
  abstract={Maintenance logs of industrial equipment record descriptive and unstructured operation and maintenance (O&M) information, which is the basis of reliability, availability, and maintainability investigations. However, the construction of failure knowledge graphs as a basis for understanding the failure and maintenance properties of systems is challenging due to the requirement of annotation efforts and domain knowledge. This article proposes a novel semi-supervised method for failure knowledge graph construction. Initially, a semantic module is proposed to extract hidden contextual information from maintenance records and identify corresponding failure modes. The semantic module is trained by unlabeled maintenance records with the assistance of the hard pseudo-label acquisition and the proposed self-training algorithm. Subsequently, a taxonomy induction module is presented to extract failure items and their relationships to construct failure knowledge graphs that provide decision support. The feasibility and superiority of the proposed method are validated by maintenance logs from real wind farms. Overall, the proposed method provides an effective tool for semantic information digitalization of well-cumulated industrial O&M data.},
  keywords={Maintenance engineering;Knowledge graphs;Semantics;Feature extraction;Data mining;Bit error rate;Taxonomy;Knowledge graph;operation and maintenance;unstructured maintenance logs},
  doi={10.1109/TII.2023.3299078},
  ISSN={1941-0050},
  month={March},}@ARTICLE{10052691,
  author={Jha, Kanchan and Saha, Sriparna and Karmakar, Sourav},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
  title={Prediction of Protein-Protein Interactions Using Vision Transformer and Language Model}, 
  year={2023},
  volume={20},
  number={5},
  pages={3215-3225},
  abstract={The knowledge of protein-protein interaction (PPI) helps us to understand proteins’ functions, the causes and growth of several diseases, and can aid in designing new drugs. The majority of existing PPI research has relied mainly on sequence-based approaches. With the availability of multi-omics datasets (sequence, 3D structure) and advancements in deep learning techniques, it is feasible to develop a deep multi-modal framework that fuses the features learned from different sources of information to predict PPI. In this work, we propose a multi-modal approach utilizing protein sequence and 3D structure. To extract features from the 3D structure of proteins, we use a pre-trained vision transformer model that has been fine-tuned on the structural representation of proteins. The protein sequence is encoded into a feature vector using a pre-trained language model. The feature vectors extracted from the two modalities are fused and then fed to the neural network classifier to predict the protein interactions. To showcase the effectiveness of the proposed methodology, we conduct experiments on two popular PPI datasets, namely, the human dataset and the S. cerevisiae dataset. Our approach outperforms the existing methodologies to predict PPI, including multi-modal approaches. We also evaluate the contributions of each modality by designing uni-modal baselines. We perform experiments with three modalities as well, having gene ontology as the third modality.},
  keywords={Proteins;Feature extraction;Three-dimensional displays;Protein sequence;Amino acids;Deep learning;Transformers;Language model;protein-protein interaction;vision transformer},
  doi={10.1109/TCBB.2023.3248797},
  ISSN={1557-9964},
  month={Sep.},}@INPROCEEDINGS{10821904,
  author={Lijuan, Ke and Lianting, Lai and Xiaoyi, Xu and Qianyun, Yang and Guoshan, Zhang and Lei, Lei},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Construction of a knowledge graph of acupuncture famous doctors' medical cases based on information extraction from large language model}, 
  year={2024},
  volume={},
  number={},
  pages={4854-4860},
  abstract={Objective: To construct a knowledge graph of acupuncture case studies from renowned acupuncturists, analyze acupuncture medical case knowledge, mine implicit knowledge, and perform visual representation, providing methodological references for the research of acupuncture medical cases. Methods: The types of knowledge entities and relationships between entities involved in acupuncture case studies from renowned acupuncturists were sorted out. The TCM Miner platform was used for text annotation, and a large language model was employed to identify and label entities. The relationships between entities were determined based on Chinese medicine-related standards, literature reviews, and established rules. After knowledge fusion, the data was imported into the Neo4j graph database using the Cypher language for storage and visual representation, constructing a knowledge graph of acupuncture medical cases. Results: The acupuncture medical case knowledge graph contained 577 nodes and 3,905 relationships, with a schema comprising 5 entity classes and 5 relationship types. Through Cypher language queries, knowledge can be visually presented in four aspects: commonly used treatment methods of practitioners, research on the relationship between acupuncture techniques and diseases, exploration of disease-acupoint patterns, and research on other therapies and diseases. Discussion: The knowledge graph constructed in this study can visually display the knowledge and implicit relationships recorded in acupuncture medical cases, making it suitable for knowledge mining and visual representation of acupuncture medical cases.},
  keywords={Visualization;Ciphers;Large language models;Knowledge graphs;Information retrieval;Acupuncture;Visual databases;Standards;Diseases;Systematic literature review;acupuncture famous doctors' medical cases;knowledge graph;large language models;visualization},
  doi={10.1109/BIBM62325.2024.10821904},
  ISSN={2156-1133},
  month={Dec},}@ARTICLE{10748382,
  author={Hu, Binhao and Zhang, Jianpeng and Chen, Hongchang},
  journal={Chinese Journal of Electronics}, 
  title={Knowledge Graph Completion Method of Combining Structural Information with Semantic Information}, 
  year={2024},
  volume={33},
  number={6},
  pages={1412-1420},
  abstract={With the development of knowledge graphs, a series of applications based on knowledge graphs have emerged. The incompleteness of knowledge graphs makes the effect of the downstream applications affected by the quality of the knowledge graphs. To improve the quality of knowledge graphs, translation-based graph embeddings such as TransE, learn structural information by representing triples as low-dimensional dense vectors. However, it is difficult to generalize to the unseen entities that are not observed during training but appear during testing. Other methods use the powerful representational ability of pre-trained language models to learn entity descriptions and contextual representation of triples. Although they are robust to incompleteness, they need to calculate the score of all candidate entities for each triple during inference. We consider combining two models to enhance the robustness of unseen entities by semantic information, and prevent combined explosion by reducing inference overhead through structured information. We use a pre-training language model to code triples and learn the semantic information within them, and use a hyperbolic space-based distance model to learn structural information, then integrate the two types of information together. We evaluate our model by performing link prediction experiments on standard datasets. The experimental results show that our model achieves better performances than state-of-the-art methods on two standard datasets.},
  keywords={Training;Semantics;Knowledge graphs;Predictive models;Vectors;Robustness;Explosions;Standards;Testing;Context modeling;Knowledge graph;Knowledge graph completion;Representation learning},
  doi={10.23919/cje.2022.00.299},
  ISSN={2075-5597},
  month={November},}@INPROCEEDINGS{10451936,
  author={Zhan, Ding and Wu, Xingyu and Chang, Yubiao and Tian, Chao and Wang, Tian and Chen, Cui and Xie, Liping and Huang, Wang and Tong, Endong and Niu, Wenjia},
  booktitle={2023 China Automation Congress (CAC)}, 
  title={Improving Knowledge Graph-based BERT Pre-Training Through Entity Hot Partition and Adapter Fusion}, 
  year={2023},
  volume={},
  number={},
  pages={7669-7675},
  abstract={Enhancing pre-training models based on knowledge graphs is an emerging technique used to handle knowledge-intensive tasks. In the latest research presented at EMNLP 2021, an approach to multi-subgraph BERT embedding using knowledge graph partitioning is developed, demonstrating the effectiveness of parallel multiple subgraphs-based BERT embeddings. However, this approach is task-agnostic and only consider topological features during the knowledge graph partitioning process, while in practical applications such as question answering, the graph partitioning can dynamically change as users' interaction, that may neglect entity popularity features and potentially affect the performance of pre-training models. In this paper, we propose an entity hot partition(EHP) approach to multi-subgraph BERT embedding. We improve the performance of the BERT model through integrating entity hot partitioning and adapter fusion. By calculating entity popularity based on knowledge graph triplets, we construct an entity popularity weight vector and introduce it into the classic METIS method to achieve popularity-sensitive graph partitioning. Finally, the partitioned graph results are fed into adapters to facilitate fine-tuning-based BERT pretraining. We conducted extensive experiments on three biomedical BERT models, namely SciBERT, BioBERT, and PubMedBERT, across six downstream tasks. The results demonstrate the effectiveness of our approach in achieving the performance improvements under simulated popularity scenarios.},
  keywords={Adaptation models;Automation;Computational modeling;Biological system modeling;Knowledge graphs;Vectors;Question answering (information retrieval);Knowledge Graph;Hot Partition;BERT;Embedding},
  doi={10.1109/CAC59555.2023.10451936},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{9534398,
  author={Ahmad, Zishan and Ekbal, Asif and Sengupta, Shubhashis and Maitra, Anutosh and Ramnani, Roshni and Bhattacharyya, Pushpak},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Unsupervised Approach for Knowledge-Graph Creation from Conversation: The Use of Intent Supervision for Slot Filling}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={In this paper, we propose an unsupervised approach for knowledge graph (KG) creation from conversational data. We make use of intent classification and slot-filling, the two important components of any dialogue agent, exploit their interconnectedness, and finally construct a KG. We build a supervised intent classifier to extract the intent classes, and then on top of this we run our occlusion based slot-information extraction algorithm. Our algorithm is able to make use of supervised training of intent classifiers for extracting the relevant slot-information in an unsupervised way. To test the effectiveness of our system, we perform both automatic and manual evaluation of our intent-classifier and slot-filling system on three dialog datasets. Finally, we construct a knowledge graph from the dialogue conversation using an algorithm that makes use of our occlusion based slot-information extraction module. Empirical evaluation shows that our occlusion based method is able to successfully extract slot information from conversations, resulting in a high-quality KG.},
  keywords={Training;Codes;Neural networks;Bit error rate;Manuals;Information retrieval;Filling;Deep-learning;Unsupervised;Style-transfer;Text generation},
  doi={10.1109/IJCNN52387.2021.9534398},
  ISSN={2161-4407},
  month={July},}@ARTICLE{10198434,
  author={Jaradeh, Amer and Kurdy, Mohamad-Bassam},
  journal={IEEE Access}, 
  title={ArEmotive Bridging the Gap: Automatic Ontology Augmentation Using Zero-Shot Classification for Fine-Grained Sentiment Analysis of Arabic Text}, 
  year={2023},
  volume={11},
  number={},
  pages={81318-81330},
  abstract={Human-computer interaction remains one of the final frontiers to conquer while held in perspective with the rapid developments and technology growth over recent years. It is an arduous task to convey the true human intent to the machine in order to generate a computerized relevant decision in a certain field. In recent years, focus has shifted to cover fields of study that relate to Sentiment Analysis (SA) to improve and ease the tasks of our daily lives. We Propose ArEmotive (Arabic Emotive), a fine-grained sentiment analysis system that is human-independent which can automatically grow its source of information allowing for more precision and a greater dataset each time it is used through ontology augmentation and classification. Our proposed architecture relies on multiple data sources running through certain pipelines to generate a central online repository utilized by any mobile system to access this info-base. This system is important because many researchers in the field of automated ontology alignment and ontology mapping achieved a semi-automated approach to map new ontologies out of old ones or to extend already existing ontologies with data from new ones. ArEmotive identifies fine-grained emotions in text based on a dynamic ontology enriched through ontology alignment, mapping and machine learning assisted classification, resulting in a structure that contributes in: a centralized dataset ever growing to fit the need of the users, a sustainable structure able to allocate new data sources without the need to modify the system, ability to generate appropriate information even with the absence of “parent” sources.},
  keywords={Ontologies;Task analysis;Social networking (online);Sentiment analysis;Bridges;Blogs;Soft sensors;Emotion recognition;Arabic NLP;fine-grained emotions;ontology augmentation},
  doi={10.1109/ACCESS.2023.3300737},
  ISSN={2169-3536},
  month={},}@ARTICLE{10505027,
  author={Gao, Xingyu and Wang, Xi and Chen, Zhenyu and Zhou, Wei and Hoi, Steven C. H.},
  journal={IEEE Transactions on Multimedia}, 
  title={Knowledge Enhanced Vision and Language Model for Multi-Modal Fake News Detection}, 
  year={2024},
  volume={26},
  number={},
  pages={8312-8322},
  abstract={The rapid dissemination of fake news and rumors through the Internet and social media platforms poses significant challenges and raises concerns in the public sphere. Automatic detection of fake news plays a crucial role in mitigating the spread of misinformation. While recent approaches have focused on leveraging neural networks to improve textual and visual representations in multi-modal fake news analysis, they often overlook the potential of incorporating knowledge information to verify facts within news articles. In this paper, we present a vision and language model that incorporates knowledge to enhance multi-modal fake news detection. Our proposed model integrates information from large scale open knowledge graphs to augment its ability to discern the veracity of news content. Unlike previous methods that utilize separate models to extract textual and visual features, we synthesize a unified model capable of extracting both types of features simultaneously. To represent news articles, we introduce a graph structure where nodes encompass entities, relationships extracted from the textual content, and objects depicted in associated images. By utilizing the knowledge graph, we establish meaningful relationships between nodes within the news articles. Experimental evaluations on a real-world multi-modal dataset from Twitter demonstrate significant performance improvement by incorporating knowledge information.},
  keywords={Fake news;Feature extraction;Visualization;Knowledge graphs;Social networking (online);Knowledge engineering;Task analysis;Deep neural network;fake news detection;knowledge graph;vision and language model},
  doi={10.1109/TMM.2023.3330296},
  ISSN={1941-0077},
  month={},}@INPROCEEDINGS{10659709,
  author={Tobola, Kirill V. and Dorodnykh, Nikita O.},
  booktitle={2024 Ivannikov Memorial Workshop (IVMEM)}, 
  title={Semantic Annotation of Russian-Language Tables Based on a Pre-Trained Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={62-68},
  abstract={Tables are widely used in organizing, presenting, and storing data. However, as a rule, they have different arbitrary layouts and are not accompanied by explicit semantics necessary for machine interpretation of their content. Recent research has shown great progress in solving various problems in the field of automatic table understanding. Despite this, there is still a need to develop more effective solutions that are applicable in practice and take into account data with different linguistic backgrounds. In this paper, we propose a new RuTaBERT framework based on a pre-trained language model and provide semantic column annotation of Russian-language tables. We used a preprocessed large-scale corpus of Russian-language web tables extracted from Wikipedia pages as training data to fine-tune the language model. The experimental results obtained show the prospect of using the proposed framework, including when solving applied problems.},
  keywords={Annotations;Semantics;Layout;Training data;Knowledge graphs;Encyclopedias;Linguistics;tabular data;Russian-language tables;pre-trained language model;semantic table interpretation;column annotation;knowledge graph},
  doi={10.1109/IVMEM63006.2024.10659709},
  ISSN={2831-5847},
  month={May},}@ARTICLE{10752733,
  author={Xie, Bingbing and Ma, Xiaoxiao and Shan, Xue and Beheshti, Amin and Yang, Jian and Fan, Hao and Wu, Jia},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Multiknowledge and LLM-Inspired Heterogeneous Graph Neural Network for Fake News Detection}, 
  year={2024},
  volume={},
  number={},
  pages={1-13},
  abstract={The widespread diffusion of fake news has become a critical problem on dynamic social media worldwide, which requires effective strategies for fake news detection to alleviate its hazardous consequences for society. However, most recent efforts only focus on the features of news content and social context without realizing the benefits of large language models (LLMs) and multiple knowledge graphs (KGs), thus failing to improve detection capabilities further. To tackle this issue, we present a multiknowledge and LLM-inspired heterogeneous graph neural network for fake news detection (MiLk-FD), by combining KGs, LLMs, and graph neural networks (GNNs). Specifically, we first model news content as a heterogeneous graph (HG) containing news, entity, and topic nodes and then fuse the knowledge from three KGs to augment the factual basis of news articles. Meanwhile, we leverage TransE to initialize the knowledge features and employ LLaMa2-7B to obtain the initial feature vectors of news articles. After that, we utilize the devised HG transformer to learn news embeddings with specific feature distribution in high-dimensional spaces by aggregating neighborhood information according to metapaths. Finally, a classifier based on multilayer perceptron (MLP) is trained to predict each news article as fake or true. Through experiments, we demonstrate that our proposed framework surpasses ten baselines according to accuracy, precision, F1-score, recall, and ROC in four public real-world benchmarks (i.e., COVID-19, FakeNewsNet, PAN2020, Liar).},
  keywords={Deep learning;fake news detection;graph anomaly detection;knowledge graph (KG);large language model (LLM)},
  doi={10.1109/TCSS.2024.3488191},
  ISSN={2329-924X},
  month={},}@ARTICLE{10807340,
  author={Guo, Wenying and Du, Shengdong and Hu, Jie and Teng, Fei and Yang, Yan and Li, Tianrui},
  journal={Big Data Mining and Analytics}, 
  title={RP-KGC: A Knowledge Graph Completion Model Integrating Rule-Based Knowledge for Pretraining and Inference}, 
  year={2025},
  volume={8},
  number={1},
  pages={18-30},
  abstract={The objective of knowledge graph completion is to comprehend the structure and inherent relationships of domain knowledge, thereby providing a valuable foundation for knowledge reasoning and analysis. However, existing methods for knowledge graph completion face challenges. For instance, rule-based completion methods exhibit high accuracy and interpretability, but encounter difficulties when handling large knowledge graphs. In contrast, embedding-based completion methods demonstrate strong scalability and efficiency, but also have limited utilisation of domain knowledge. In response to the aforementioned issues, we propose a method of pre-training and inference for knowledge graph completion based on integrated rules. The approach combines rule mining and reasoning to generate precise candidate facts. Subsequently, a pre-trained language model is fine-tuned and probabilistic structural loss is incorporated to embed the knowledge graph. This enables the language model to capture more deep semantic information while the loss function reconstructs the structure of the knowledge graph. This enables the language model to capture more deep semantic information while the loss function reconstructs the structure of the knowledge graph. Extensive tests using various publicly accessible datasets have indicated that the suggested model performs better than current techniques in tackling knowledge graph completion problems.},
  keywords={Scalability;Semantics;Knowledge graphs;Transforms;Probabilistic logic;Cognition;Encoding;Data models;Data mining;Faces;Knowledge Graph Completion (KGC);Bidirectional Encoder Representation from Transforms (BERT) fine-tuning;knowledge graph embedding},
  doi={10.26599/BDMA.2024.9020063},
  ISSN={2097-406X},
  month={February},}@INPROCEEDINGS{9686335,
  author={Palchunov, Dmitry and Tregubov, A.S.},
  booktitle={2021 International Symposium on Knowledge, Ontology, and Theory (KNOTH)}, 
  title={Semantic methods of intelligent assistant developing}, 
  year={2021},
  volume={},
  number={},
  pages={30-35},
  abstract={Human-computer interaction with people whose visual perception is limited is possible only with tactile and voice interfaces, the latter are being used more and more recently. The aim of the work is to create an intelligent assistant for an indoor navigation system designed for blind and visually impaired people. The development of an intelligent assistant is based on a semantic user model and a four-level ontological model of the subject domain. To build a dialogue between an intelligent assistant and a user, we use the theory of speech acts, argumentation theory and case-based reasoning. The developed software system is aimed at identifying the desires and user needs and proposing possible user actions aimed at achieving them. The system allows for the decomposition of user tasks and the formation of a sequence of their execution based on semantic models of the user and the subject domain.},
  keywords={Knowledge engineering;Uncertainty;Web services;Semantics;Speech recognition;Ontologies;Software systems;intelligent assistant;ontology;machine learning;natural language processing;intent recognition;argumentation theory;case-based reasoning},
  doi={10.1109/KNOTH54462.2021.9686335},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10074709,
  author={Chen, Luming and Qi, Yifan and Wu, Aiping and Deng, Lizong and Jiang, Taijiao},
  booktitle={2022 IEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)}, 
  title={Enhancing Cross-lingual Medical Concept Alignment by Leveraging Synonyms and Translations of the Unified Medical Language System}, 
  year={2022},
  volume={},
  number={},
  pages={2078-2083},
  abstract={Well-developed medical terminology systems like the Unified Medical Language System (UMLS) improve the ability of language models to handle medical entity linking tasks. However, such magnificent terminology systems are only available for few languages, such as English. For Chinese, both simplified and traditional, the lack of well-developed terminology systems remains a big challenge to unify Chinese medical terminologies by linking medical entities as concepts. In this study, we purpose a translation enhanced contrastive learning scheme which leverages translations and synonyms of UMLS to infuse knowledge into the language model, and present a cross-lingual pre-trained language model called TeaBERT that aligns cross-lingual Chinese and English medical synonyms well at semantic level. Comparing with former cross-lingual language models, TeaBERT significantly outperforms on evaluation datasets, with 93.21%, 89.89% and 76.45% of Top 5 accuracy on ICDI0-CN, CHPO and RealWorld dataset respectively, and achieves new state-of-theart performance without task specific fine-tuning. Our contrastive learning scheme can not only be used for enhancing Chinese-English medical concepts alignment, but also be applied to other languages facing the same challenges.},
  keywords={Terminology;Biological system modeling;Unified modeling language;Semantics;Knowledge representation;Task analysis;NLP;pre-trained language model;cross-lingual medical entity linking;UMLS},
  doi={10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00309},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9340905,
  author={Jiang, Chen and Dehghan, Masood and Jagersand, Martin},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Understanding Contexts Inside Robot and Human Manipulation Tasks through Vision-Language Model and Ontology System in Video Streams}, 
  year={2020},
  volume={},
  number={},
  pages={8366-8372},
  abstract={Manipulation tasks in daily life, such as pouring water, unfold through human intentions. Being able to process contextual knowledge from these Activities of Daily Living (ADLs) over time can help us understand manipulation intentions, which are essential for an intelligent robot to transition smoothly between various manipulation actions. In this paper, to model the intended concepts of manipulation, we present a vision dataset under a strictly constrained knowledge domain for both robot and human manipulations, where manipulation concepts and relations are stored by an ontology system in a taxonomic manner. Furthermore, we propose a scheme to generate a combination of visual attentions and an evolving knowledge graph filled with commonsense knowledge. Our scheme works with real-world camera streams and fuses an attention-based Vision-Language model with the ontology system. The experimental results demonstrate that the proposed scheme can successfully represent the evolution of an intended object manipulation procedure for both robots and humans. The proposed scheme allows the robot to mimic human-like intentional behaviors by watching real-time videos. We aim to develop this scheme further for real-world robot intelligence in Human-Robot Interaction.},
  keywords={Visualization;Fuses;Ontologies;Real-time systems;Task analysis;Intelligent robots;Videos},
  doi={10.1109/IROS45743.2020.9340905},
  ISSN={2153-0866},
  month={Oct},}@ARTICLE{10539104,
  author={Li Chong, You and Poo Lee, Chin and Zen Muhd-Yassin, Shahrin and Ming Lim, Kian and Kamsani Samingan, Ahmad},
  journal={IEEE Access}, 
  title={TransKGQA: Enhanced Knowledge Graph Question Answering With Sentence Transformers}, 
  year={2024},
  volume={12},
  number={},
  pages={74872-74887},
  abstract={Knowledge Graph Question Answering (KGQA) plays a crucial role in extracting valuable insights from interconnected information. Existing methods, while commendable, face challenges such as contextual ambiguity and limited adaptability to diverse knowledge domains. This paper introduces TransKGQA, a novel approach addressing these challenges. Leveraging Sentence Transformers, TransKGQA enhances contextual understanding, making it adaptable to various knowledge domains. The model employs question-answer pair augmentation for robustness and introduces a threshold mechanism for reliable answer retrieval. TransKGQA overcomes limitations in existing works by offering a versatile solution for diverse question types. Experimental results, notably with the sentence-transformers/all-MiniLM-L12-v2 model, showcase remarkable performance with an F1 score of 78%. This work advances KGQA systems, contributing to knowledge graph construction, enhanced question answering, and automated Cypher query execution.},
  keywords={Knowledge graphs;Transformers;Question answering (information retrieval);Natural languages;Task analysis;Noise;Encoding;Question answering;knowledge graph;machine learning;natural language processing;Neo4j;sentence transformer},
  doi={10.1109/ACCESS.2024.3405583},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10393780,
  author={Yang, Yahu and Zhang, Hu and Xu, Jiashu and Song, Shenmin},
  booktitle={2023 IEEE 6th International Conference on Information Systems and Computer Aided Education (ICISCAE)}, 
  title={MATEKG: A Large-scale Multi-class Equipment Knowledge Graph for Military Auxiliary Tasks}, 
  year={2023},
  volume={},
  number={},
  pages={133-138},
  abstract={In the era of rapid advances in artificial intelligence, the digitization of armed forces around the world has seen unprecedented growth. A wealth of military information and knowledge is available on the Internet. The task of harnessing and analyzing this vast amount of data, and structuring the existing information to facilitate the development of information-centric weapons, has emerged as a prominent area of research. Knowledge graphs, which encapsulate and represent human knowledge in a format that machines can understand, are essential for performing tasks such as information retrieval, data analysis and inference. The creation of a knowledge graph specific to military equipment is a cornerstone in the process of military digitization. In this paper, a military equipment dataset is first constructed through structured data collection and processing based on the globally popular open data websites on equipment. Then, based on this military equipment dataset, a large-scale multi-class equipment knowledge graph for military auxiliary tasks is constructed by the proposed entity recognition method and relational triple extraction method. The experimental results show that the proposed BERT-enhanced BiLSTM and SPN achieve 90.43% and 93% F1 scores, respectively, proving the effectiveness of the proposed methods on the military equipment data.},
  keywords={Military computing;Weapons;Knowledge based systems;Knowledge graphs;Data collection;Information retrieval;Internet;knowledge graph;named entity recognition;relationship extraction;military equipment},
  doi={10.1109/ICISCAE59047.2023.10393780},
  ISSN={2770-663X},
  month={Sep.},}@INPROCEEDINGS{10679399,
  author={Karalka, Christina and Meditskos, Georgios and Papoutsoglou, Maria and Bassiliades, Nick},
  booktitle={2024 IEEE International Conference on Cyber Security and Resilience (CSR)}, 
  title={Towards a Generic Knowledge Graph Construction Framework for Privacy Awareness}, 
  year={2024},
  volume={},
  number={},
  pages={700-705},
  abstract={Knowledge graphs (KGs) organize data from multi-ple sources, capturing information about entities of interest in a given domain or task, such as people, places, or events, and forge connections between them. In this paper, we introduce a generic framework for building knowledge graphs designed to enhance data privacy through semantic interpretation. We demonstrate the effectiveness of our framework by applying it to the healthcare sector, where it helps organize and analyze com-plex information, support data analysis, improve decision-making processes, and uncover hidden relationships between entities. Our approach leverages domain-specific ontologies like SNOMED CT and integrates vector databases to assess and mitigate privacy risks. By using semantic techniques, we enhance the robustness of data against reidentification attacks and suggest appropriate de-identification methods. The integration of SNOMED with vector databases allows for efficient storage, retrieval, and analysis of high-dimensional healthcare data, facilitating advanced data an-alytics and knowledge discovery while maintaining data privacy. Through this framework, we aim to provide sufficient insights for identifying privacy vulnerabilities and ensuring the security and usability of sensitive health information.},
  keywords={Data privacy;Privacy;Databases;Semantics;Knowledge graphs;Medical services;Vectors},
  doi={10.1109/CSR61664.2024.10679399},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9447453,
  author={Xu, Wenwen and Fang, Mingzhe and Yang, Li and Jiang, Huaxi and Liang, Geng and Zuo, Chun},
  booktitle={2021 International Conference on Computer Communication and Artificial Intelligence (CCAI)}, 
  title={Enabling Language Representation with Knowledge Graph and Structured Semantic Information}, 
  year={2021},
  volume={},
  number={},
  pages={91-96},
  abstract={Pre-trained language models have been widely recognized and applied. While common pre-training language representation models(PLMs) usually focus on grasping the co-occurrence of words or sentences in simple tasks, more and more researchers realize that external information, i.e., knowledge graph (KG) and clear structured semantics, can be vital in natural language understanding tasks. Therefore, using external information to enhance PLMs (such as BERT) has gradually become a popular direction. However, the existing improvement methods often only use a certain type of external information, and it is difficult to solve the problems of common PLMs that lack common sense and semantic incompleteness in one fell swoop. Suppose the model wants to integrate multiple external information. In that case, it not only requires the model to deal with the noise problem that external information may bring but also requires the model to ensure that different information can work together effectively. In this paper, we propose Sem-K-BERT, which integrates the information of KG and semantic role labeling(SRL) before and after the BERT encoding layer, and introduces a context-aware knowledge screening mechanism based on semantic correlation calculation and a text-semantic alignment mechanism to effectively integrate the two external information and reduce the impact of noise. Experiments and analysis on 8 different Chinese natural language processing tasks show that Sem-K-BERT has better performance than BERT and the BERT model that only incorporates KG. This indicates that the simultaneous use of knowledge graph and SRL information offers a promising solution to improve the performance of PLMs.},
  keywords={Analytical models;Correlation;Computational modeling;Semantics;Bit error rate;Grasping;Natural language processing;language model;knowledge graph;semantic information},
  doi={10.1109/CCAI50917.2021.9447453},
  ISSN={},
  month={May},}@INPROCEEDINGS{10754778,
  author={Govindharajan, Hariharan and Vijayakumar, Senthilkumar},
  booktitle={2024 IEEE 15th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)}, 
  title={A Framework for automated selective Fine-Tuning of Domain-Specific Large Language Models Using Graph-Based Retrieval Augmented Generation}, 
  year={2024},
  volume={},
  number={},
  pages={431-439},
  abstract={Graph based retrieval augmented generation technique in Large Language Model (LLM) brings in major advantages by providing deep context to LLMs through relational knowledge graph for text generation, classification, question and answering use cases. However, maintaining vast data volume of domain specific data in a knowledge graph with complex relationships and querying from it every time a prompt is being posted to LLM, is a time consuming and expensive process. This paper presents a novel framework for selectively fine-tuning domain-specific large language models (LLMs) using a multi-stage Knowledge Graph (KG) based Retrieval Augmented Generation (RAG) pipeline and an Automated Incremental Fine-tuning System (AIFS). The proposed system aims to enhance the accuracy and relevance of LLM responses for text generation and Question Answering use cases by finetuning the LLM incrementally based on highly sought and highly relevant information in knowledge graph identified by leveraging page rank algorithm in KG. The framework comprises three major subsystems: Knowledge Graph Generation, Automated Incremental fine-tuning system (AIFS), and Domain Based Information Retrieval (DBIR). The effectiveness of the system is demonstrated through its ability to incrementally fine-tune LLMs based on selected highly relevant nodes within the KG, thereby improving the model’s domain-specific knowledge, response accuracy by 90% and reduce cost by 71.8%.},
  keywords={Accuracy;Costs;Large language models;Pipelines;Knowledge graphs;Information retrieval;Mobile communication;Question answering (information retrieval);Artificial Intelligence(AI);Knowledge Graph(KG);Large Language Models(LLM);LLM Fine-tuning;Contextual Information Extraction &Retrieval Systems},
  doi={10.1109/UEMCON62879.2024.10754778},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10808357,
  author={Yongfei, Miao and Yihang, Zhang and Li, Wang and Xiaoxue, Song and Yuze, Song and Zekun, Tang},
  booktitle={2024 10th International Conference on Big Data and Information Analytics (BigDIA)}, 
  title={Millitary Knowledge Graph Construction Based on Universal Information Extraction Models}, 
  year={2024},
  volume={},
  number={},
  pages={877-881},
  abstract={Data in military domain is characterized by diverse types, scattered storage and high confidentiality, which greatly limits the promotion and application of knowledge graph in military domain. The advantage of big language model in understanding complex text and semantic relations provides a possibility for the automated construction of knowledge graph in military domain. This paper proposed a knowledge graph construction method for military domain based on Universal information extraction(UIE) models, which improved the automation level of knowledge graph construction in military domain through the steps of generating high-quality corpus by domain expert annotation, unified modeling information extraction task and model fine-tuning, and provided a reference for the wide application of knowledge graph in military domain.},
  keywords={Accuracy;Information resources;Decision making;Semantics;Knowledge graphs;Information retrieval;Data models;Data mining;Security;Military equipment;Large Language Models;Knowledge Graph;Information Extraction;Military Domain},
  doi={10.1109/BigDIA63733.2024.10808357},
  ISSN={2771-6902},
  month={Oct},}@INPROCEEDINGS{7934726,
  author={Shani, Uri},
  booktitle={2017 Annual IEEE International Systems Conference (SysCon)}, 
  title={Can ontologies prevent MBSE models from becoming obsolete?}, 
  year={2017},
  volume={},
  number={},
  pages={1-8},
  abstract={Model-based systems engineering provides a soft, manageable, and query-able representation of product design and lifecycle. However, at the same time, it creates a discontinuous space of data that does not integrate, constantly becomes outdated, and has to be upgraded as tools evolve. Models that are not maintained become legacy, and then become obsolete and useless. We look at how ontologies that follow the semantic web technology for data representation can create interoperability among the modeling tools, support model reuse, and fight the aging and obsolescence of models.},
  keywords={Tools;Ontologies;Resource description framework;OWL;Semantics;Data models;Ontology;MBSE;modeling;obsolescence;reusability;interoperability},
  doi={10.1109/SYSCON.2017.7934726},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{10578858,
  author={Lehmann, Alexander and Landes, Dieter},
  booktitle={2024 IEEE Global Engineering Education Conference (EDUCON)}, 
  title={Extracting Metadata from Learning Videos for Ontology-Based Recommender Systems Using Whisper & GPT}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={In modern education, individualized learning environments play a vital role by allowing learners to tailor their learning paths based on personal needs, interests, and abilities. Achieving effective individualization relies on dynamic adaptation of the learning path, typically facilitated by recommender systems. These systems offer personalized suggestions, commonly employing content-based or collaborative filtering approaches. However, traditional recommender systems often lack consideration of the semantics of learning elements. To address this limitation, ontology-based recommender systems integrate semantic modeling, establishing additional connections within a domain to enhance precision and context in recommendations. Notably, these systems mitigate the cold start problem and are particularly advantageous in learning environments with limited data. While videos are prevalent in learning platforms, their unstructured nature poses challenges for processing. This paper introduces an innovative approach, leveraging Large Language Models, specifically GPT, to extract metadata from learning videos. The proposed method intelligently augments videos and links them to a domain ontology, enabling the integration of videos into ontology-based recommender systems. The application of this approach is demonstrated through a case study in software engineering education, showcasing its potential to enhance individualized learning experiences in specific domains. The presented method offers an automated alternative to manual video processing, aligning with the evolving landscape of education technology.},
  keywords={Large language models;Semantics;Manuals;Metadata;Ontologies;Engineering education;Recommender systems;learning analytics;adaptive learning environments;generative AI;large language models;ontology-based recommender systems;learning videos},
  doi={10.1109/EDUCON60312.2024.10578858},
  ISSN={2165-9567},
  month={May},}@INPROCEEDINGS{10096669,
  author={Su, Ruolin and Yang, Jingfeng and Wu, Ting-Wei and Juang, Biing-Hwang},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Choice Fusion As Knowledge For Zero-Shot Dialogue State Tracking}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={With the demanding need for deploying dialogue systems in new domains with less cost, zero-shot dialogue state tracking (DST), which tracks user’s requirements in task-oriented dialogues without training on desired domains, draws attention increasingly. Although prior works have leveraged question-answering (QA) data to reduce the need for in-domain training in DST, they fail to explicitly model knowledge transfer and fusion for tracking dialogue states. To address this issue, we propose CoFunDST, which is trained on domain-agnostic QA datasets and directly uses candidate choices of slot-values as knowledge for zero-shot dialogue-state generation, based on a T5 pre-trained language model. Specifically, CoFunDST selects highly-relevant choices to the reference context and fuses them to initialize the decoder to constrain the model outputs. Our experimental results show that our proposed model achieves outperformed joint goal accuracy compared to existing zero-shot DST approaches in most domains on the MultiWOZ 2.1. Extensive analyses demonstrate the effectiveness of our proposed approach for improving zero-shot DST learning from QA.},
  keywords={Training;Costs;Fuses;Signal processing;Data models;Decoding;Task analysis;Dialogue state tracking;zero-shot;question answering;pre-trained language model;knowledge fusion},
  doi={10.1109/ICASSP49357.2023.10096669},
  ISSN={2379-190X},
  month={June},}
@ARTICLE{10367969,
  author={Fatemi, Bahareh and Rabbi, Fazle and Opdahl, Andreas L.},
  journal={IEEE Access}, 
  title={Evaluating the Effectiveness of GPT Large Language Model for News Classification in the IPTC News Ontology}, 
  year={2023},
  volume={11},
  number={},
  pages={145386-145394},
  abstract={News classification plays a vital role in newsrooms, as it involves the time-consuming task of categorizing news articles and requires domain knowledge. Effective news classification is essential for categorizing and organizing a constant flow of information, serving as the foundation for subsequent tasks, such as news aggregation, monitoring, filtering, and organization. The automation of this process can significantly benefit newsrooms by saving time and resources. In this study, we explore the potential of the GPT large language model in a zero-shot setting for multi-class classification of news articles within the widely accepted International Press Telecommunications Council (IPTC) news ontology. The IPTC news ontology provides a structured framework for categorizing news, facilitating the efficient organization and retrieval of news content. By investigating the effectiveness of the GPT language model in this classification task, we aimed to understand its capabilities and potential applications in the news domain. This study was conducted as part of our ongoing research in the field of automated journalism.},
  keywords={Task analysis;Annotations;Ontologies;Adaptation models;Tag clouds;Support vector machines;Sports;IPTC media topics;journalism;large language models;news classification},
  doi={10.1109/ACCESS.2023.3345414},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9980157,
  author={Vasantharajan, Charangan and Tun, Kyaw Zin and Thi-Nga, Ho and Jain, Sparsh and Rong, Tong and Siong, Chng Eng},
  booktitle={2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)}, 
  title={MedBERT: A Pre-trained Language Model for Biomedical Named Entity Recognition}, 
  year={2022},
  volume={},
  number={},
  pages={1482-1488},
  abstract={This paper introduces MedBERT, a new pre-trained transformer-based model for biomedical named entity recognition. MedBERT is trained with 57.46M tokens collected from biomedical-related data sources, i.e. datasets acquired from N2C2, BioNLP, CRAFT challenges, and biomedical-related articles crawled from Wikipedia. We validate the effectiveness of MedBERT by comparing it with four publicly available pre-trained models on ten biomedical datasets from BioNLP and CRAFT shared tasks. Our experimental results show that models fine-tuned on MedBERT achieve state-of-the-art performance in nine datasets that predict Protein, Gene, Chemical, Cellular/Component, Gene Ontology, and Taxonomy entities. Specifically, the model achieved an average of 84.04% F1-micro score on ten test sets from BioNLP and CRAFT challenges with an improvement of 3.7% and 7.83% as compared to models that were fine-tuned on BioBERT and Bio_ClinicalBERT, respectively.},
  keywords={Proteins;Protein engineering;Biological system modeling;Taxonomy;Predictive models;Ontologies;Transformers},
  doi={10.23919/APSIPAASC55919.2022.9980157},
  ISSN={2640-0103},
  month={Nov},}@ARTICLE{10623627,
  author={Bahaj, Adil and Ghogho, Mounir},
  journal={IEEE Access}, 
  title={Uncertain Knowledge Graph Embedding Using Auxiliary Information}, 
  year={2024},
  volume={12},
  number={},
  pages={138351-138361},
  abstract={Uncertain knowledge graphs (UKGs) offer a more realistic representation of knowledge by capturing the uncertainty associated with facts. However, existing UKG embedding methods primarily rely on structural information for confidence score prediction, neglecting other sources of uncertainty. This paper investigates the effectiveness of incorporating auxiliary information into UKG embeddings. We propose two approaches: Text-BEUrRE, which leverages textual information, and UCompGCN, which utilizes topological information. Our extensive experiments demonstrate that both methods successfully integrate these auxiliary data sources. Notably, Text-BEUrRE and UCompGCN outperform state-of-the-art baselines on most metrics in the confidence prediction task. On the CN15K dataset, Text-BEUrRE achieves a 7.39% improvement in Mean Squared Error (MSE) compared to the best existing model, while UCompGCN achieves an 8.27% improvement in Mean Absolute Error (MAE).},
  keywords={Semantics;Knowledge graphs;Task analysis;Uncertainty;Training;Computational modeling;Adaptation models;Predictive models;Uncertain knowledge graphs;knowledge graph embedding;box embedding;confidence prediction},
  doi={10.1109/ACCESS.2024.3439610},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10513309,
  author={Zhang, Chenchen and Jian, Sun and Chao, Luo and Fan, Chen and Bo, Li and Chen, Luo},
  booktitle={2023 IEEE 7th Conference on Energy Internet and Energy System Integration (EI2)}, 
  title={A Semantic Understanding Method for Patent Text Based on Large Language Model}, 
  year={2023},
  volume={},
  number={},
  pages={2179-2182},
  abstract={The large language model has powerful natural language understanding and generation capabilities, which can automatically extract keywords and key information from literature, thus achieving fast and efficient patent retrieval. At the same time, the large model can also be customized according to user needs, improving retrieval accuracy and efficiency. A semantic understanding method for patent text based on large language model is proposed in this paper. This method first requires preprocessing of the dataset, inputting the problem into ChatGPT according to the specified instruction format, and then classifying the output content for analysis based on different classifications. Then, the entity recognition results are input into the patent system, and the knowledge graph in the patent system is used for inference analysis to obtain the corresponding answer to the problem. The research results show that the method based on the large language model has high accuracy and generalization ability, and can achieve semantic understanding and analysis of patent texts.},
  keywords={Patents;Technological innovation;Semantics;Data preprocessing;Energy Internet;System integration;Knowledge graphs;large language models;patent text;semantic understanding;ChatGPT},
  doi={10.1109/EI259745.2023.10513309},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10605700,
  author={Safronov, Artyom A.},
  booktitle={2024 4th International Conference on Technology Enhanced Learning in Higher Education (TELE)}, 
  title={Using Neural Networks in Building an Ontology of Educational Subjects for Solving Educational Tasks}, 
  year={2024},
  volume={},
  number={},
  pages={189-191},
  abstract={In the modern world, information technologies have become an integral part of a human life, including the professional level. The rapid development of technologies based on artificial intelligence over the past few years has opened up new opportunities for their application in solving various educational tasks. One of the relevant topics for study is the investigation of ontological constructs in texts, identifying the terminology of concepts and determining the relationships between them. This article is dedicated to studying artificial intelligence systems as a tool for solving educational process tasks: it proposes the use of chatbots based on AI systems in conjunction with various digital tools in researching ontological constructs in educational texts. As part of the research, an example is provided with the processing an educational text on mathematics. Methodological characteristics are considered and a model for researching educational text using the ChatGPT is briefly described. Conclusions are drawn about the existing possibilities and difficulties in implementing the aforementioned model.},
  keywords={Systematics;Terminology;Neural networks;Chatbots;Educational courses;Mathematical models;Thesauri;artificial intelligence systems;ontology;thesaurus;chatbot;educational texts;digital tools},
  doi={10.1109/TELE62556.2024.10605700},
  ISSN={},
  month={June},}@INPROCEEDINGS{10466986,
  author={Du, Zhihong and Xu, Duo and Huang, Danruo and Hu, Yuren and He, Keqing and Wang, Chong and Wang, Jian and Zhang, Hong-Yu and Mayer, Wolfgang and Duan, Yucong and Wang, Ying and Feng, Zaiwen},
  booktitle={2023 IEEE International Conference on High Performance Computing & Communications, Data Science & Systems, Smart City & Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)}, 
  title={An Ontology-based Method for Heterogeneous Data Governance with MFI and MDR}, 
  year={2023},
  volume={},
  number={},
  pages={1106-1113},
  abstract={Currently, data governance and integration in the fields are hindered by semantic ambiguity and syntactic inconsistencies among different sub domain data and models, making it difficult to use these valuable data for further application and research jointly. Therefore, it is necessary to unify and integrate existing metadata and meta-models more effectively for information resource sharing and interoperability in the field. This paper proposes an ontology-based approach with a hybrid ISO/IEC 11179 (MDR) and ISO/IEC 19763 (MFI) framework for data governance and integration. This framework takes a Global Ontology Model (GOM) constructed for the global domain as a bridge and basis for integrating and aligning heterogeneous sub domains. It extends MDR by adding ontology registration items to implement the mapping between the GOM and multiple subdomains, thereby promoting the semantic sharing of metadata between subdomains. In addition, the MFI-12 and MFI-10 are used to solve the model interoperability between different subdomains. A detailed case study is provided to illustrate the concrete registration process and demonstrate the validity of our method.},
  keywords={Information resources;ISO Standards;Semantics;Standardization;Metadata;Ontologies;Syntactics;Ontology;MDR;MFI;Data Governance;Data Standardization},
  doi={10.1109/HPCC-DSS-SmartCity-DependSys60770.2023.00158},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8934256,
  author={Bikmullina, I. and Barkov, I.},
  booktitle={2019 International Multi-Conference on Industrial Engineering and Modern Technologies (FarEastCon)}, 
  title={Instrumentation and Control System for Test Bench}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  abstract={The main problem of the article is the lack of automated structural synthesis of information systems based on the semantic relations of subject area. As presented in the article, in technology creation software system expert developing the domain ontology with help user-friendly the program interfaces. This program is a mechanism for automatically formalizing domain description and automatically synthesizing the class diagram in the Unified Modeling Language. In the article discusses the method for automatic design based on automated synthesis of Unified Modeling Language models of the application program to control the test stand. The method of its use in solving the problems of designing an instrumentation and control system for test bench of an unmanned aerial vehicle is described. Expert inputs a domain ontology, then system automatically formalizes domain description and synthesizes the class diagram in the Unified Modeling Language.},
  keywords={Unified modeling language;Software systems;Semantics;Ontologies;Information systems;Complexity theory;Industrial engineering;UML;synthesis of class diagrams;ontology;domain;an unmanned aerial vehicle},
  doi={10.1109/FarEastCon.2019.8934256},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8549977,
  author={Albab, Muhammad Ulil and Arman, Arry Akhmad},
  booktitle={2018 International Conference on ICT for Smart Society (ICISS)}, 
  title={Resource-Based and Value-Based Extension for Archimate}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={The concept of resource and value in the archimate that is too general allows for modeling and communication problems. Therefore, an ontological analysis is needed to evaluate and redesign the concept of resource and value on the archimate. In this paper, Unified foundational ontology (UFO) is used in conducting an ontological analysis on the concept of resource and value. An ontological analysis is done by considering the mapping between the concept on Archimate and ontology concept on UFO. Semantic problems in resource and value concepts are found when the ontological analysis is performed. The result of this study is Archimate extension with the improvement of resource and value concept. The resulting extension was successfully applied in case studies and was able to handle semantic problems that were identified.},
  keywords={Ontologies;Semantics;Business;Analytical models;Electrical engineering;Informatics;Information technology;resource;value;enterprise architecture modeling;ontology-based semantics;Archimate},
  doi={10.1109/ICTSS.2018.8549977},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10772514,
  author={Li, Yi and Tian, Liwei and Yi, Chengyi and Li, Jingjing and Qin, Xiaodong and He, Yuxuan and Su, Huai},
  booktitle={2024 6th International Conference on System Reliability and Safety Engineering (SRSE)}, 
  title={A Large Language Model Based Knowledge Mining Method for Improving the Reliability of Fire Water Systems}, 
  year={2024},
  volume={},
  number={},
  pages={410-413},
  abstract={The fire water system plays a critical role in protecting both infrastructure and human lives. An essential aspect of enhancing the reliability of this system is fault diagnosis. However, the current fault diagnosis methods primarily rely on data-driven approaches, which often result in a high threshold for application due to their lack of interpretability. To tackle this challenge, this paper introduces a novel approach based on large language models for knowledge mining from textual data to extract fault information related to the fire water system, thereby enhancing the interpretability of data-driven fault diagnosis methods. The methodology followed in this paper consists of two main steps: firstly, analyzing the characteristics and principles of fire water system faults to develop a fault ontology, and secondly, creating a knowledge mining model using a large language model guided by the established fault ontology. Experimental findings indicate that the proposed model achieves an F1 score of 0.944, meeting the necessary criteria for effective knowledge mining in fire water system fault analysis. Furthermore, a comparative experiment was conducted to evaluate the performance of various encoder models, including GRU, BiGRU, LSTM, BiLSTM, and pre-trained large language model BERT. The results revealed a significant improvement in performance with the BERT encoder, showing increases in F1 scores of $22.12 \%$, $2.27 \%, 17.41 \%$, and $3.16 \%$ compared to the other models, respectively. This study provides valuable interpretative insights that can enhance the engineering applicability and reliability of data-driven fault diagnosis methods in fire water system.},
  keywords={Fault diagnosis;Water;Analytical models;Accuracy;Large language models;Bidirectional control;Ontologies;Reliability engineering;Encoding;Data mining;large language model;system reliability;fire water system;safety engineering;knowledge mining},
  doi={10.1109/SRSE63568.2024.10772514},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10818303,
  author={Anaguchi, Fumikatsu and Chakraborty, Sudesna and Morita, Takeshi and Egami, Shusaku and Ugai, Takanori and Fukuda, Ken},
  booktitle={2024 Twelfth International Symposium on Computing and Networking (CANDAR)}, 
  title={Reasoning and Justification System for Domestic Hazardous Behaviors Based on Knowledge Graph of Daily Activities and Retrieval-Augmented Generation}, 
  year={2024},
  volume={},
  number={},
  pages={11-20},
  abstract={Accidents among people over 65 years of age predominantly occur within residential settings, making the maintenance of a safe home environment a crucial social issue. To address this issue, previous research has developed systems that construct Knowledge Graphs (KG) based on simulations of daily household activities, and studies have been conducted on detecting hazardous behaviors using such KG analysis. In this current study, we propose a system capable of presenting the reason and justification for the detected domestic hazardous behaviors. Our system will first generates the reason for the detected behavior using a Large Language Model (LLM). To ensure the accuracy, reliability and reproducibility of the LLM output, the system will provides reliable sources to support the output. We employed Retrieval-Augmented Generation (RAG) to search for sentences similar to the reason generated by the LLM within reliable, authoritative documents describing domestic accident cases and their causes and these will be presented as the evidence alongside the search engine results to the users. Consequently, a knowledge graph (KG) of domestic hazardous behavior is developed based on evidence ontology. Finally, to evaluate the ability of our proposed system in appropriately generating reasons for domestic hazardous behaviors and the adequacy of the justifications provided, the output was rated using LLMs and human volunteers. The rating results showed a significant correlation between LLMs and human evaluation, indicating that the proposed system can provide sufficient reasons and justifications for domestic hazardous behaviors at residential setting.},
  keywords={Correlation;Retrieval augmented generation;Knowledge graphs;Search engines;Ontologies;Reproducibility of results;Behavioral sciences;Sensors;Optimization;Accidents;Retrieval-Augmented Generation;Large Language Model;Explainable AI;Knowledge Graph},
  doi={10.1109/CANDAR64496.2024.00010},
  ISSN={2379-1896},
  month={Nov},}@INPROCEEDINGS{10773450,
  author={Kömeçoğlu, Başak Buluz and Yılmaz, Burcu},
  booktitle={2024 9th International Conference on Computer Science and Engineering (UBMK)}, 
  title={Knowledge-Augmented Large Language Model Prompting for Cypher Query Construction}, 
  year={2024},
  volume={},
  number={},
  pages={187-192},
  abstract={While existing large language models demonstrate remarkable proficiency inn a turall a nguage processing tasks, their capacity for knowledge graph structuring remains a potential avenue for enhancement, particularly in the context of knowledge graph question answering. This paper presents a novel method for knowledge-enriched LLM routing for the construction of Cypher queries utilising knowledge graphs. It is proposed that the prompts of LLMs be enriched with knowledge by the addition of relevant triples taken from the knowledge graph. The proposed method facilitates the generation of more precise and pertinent Cypher queries by LLMs, thereby enhancing their question-answering capabilities. The method is evaluated using a story-based knowledge graph question answering dataset, which is introduced in this paper as a new contribution to the literature. The results demonstrate that the incorporation of knowledge enhances the performance of knowledge graph question answering (KGQA), particularly in the context of complex and temporal inquiries. Furthermore, the utilisation of a story graph structure for the modelling of events in news texts facilitates the effective resolution of complex questions by following the paths on the graph. Finally, it is demonstrated that the extraction of temporal labels and their incorporation into the knowledge graph is of paramount importance in answering temporal questions.},
  keywords={Knowledge engineering;Computer science;Ciphers;Large language models;Computational modeling;Knowledge graphs;Routing;Question answering (information retrieval);knowledge graph;prompt engineering;language model;question answering;natural language processing},
  doi={10.1109/UBMK63289.2024.10773450},
  ISSN={2521-1641},
  month={Oct},}@INPROCEEDINGS{9204062,
  author={Zhu, Mingda and Xue, Jiqing and Zhou, Gaoyuan},
  booktitle={2020 12th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)}, 
  title={Joint Extraction of Entity and Relation Based on Pre-trained Language Model}, 
  year={2020},
  volume={2},
  number={},
  pages={179-183},
  abstract={Extracting entities and relations from unstructured text is the key to building a large-scale knowledge graph. In recent years, the relation extraction approaches based on the neural network has achieved good results. However, the existing methods cannot accurately extract overlapping entities (i.e., one entity is shared by multiple relations). In this paper, we propose a simple Electra-based joint model for relation extraction. We use the subject extractor to identify the subject first, and then use the predicate-object extractor to predict its corresponding predicates and objects based on the encoded representation vector of the detected subject. Experiments on DuIE2.0 show good performance on the extraction of Chinese dataset with overlapping triples.},
  keywords={Conferences;Man-machine systems;Cybernetics;Erbium;component;joint extraction of entities and relations;overlapping triples;ELECTRA;tagging scheme},
  doi={10.1109/IHMSC49165.2020.10119},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10835639,
  author={Garza, Leon and Elluri, Lavanya and Piplai, Aritran and Kotal, Anantaa and Gupta, Deepti and Joshi, Anupam},
  booktitle={2024 IEEE 6th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)}, 
  title={PrivComp-KG: Leveraging KG and LLM for Compliance Verification}, 
  year={2024},
  volume={},
  number={},
  pages={97-106},
  abstract={Regulatory documents are complex and lengthy, making full compliance a challenging task for businesses. Similarly, privacy policies provided by vendors frequently fall short of the necessary legal standards due to insufficient detail. To address these issues, we propose a solution that leverages a Large Language Model (LLM) in combination with Semantic Web technology. This approach aims to clarify regulatory requirements and ensure that organizations’ privacy policies align with the relevant legal frameworks, ultimately simplifying the compliance process, reducing privacy risks, and improving efficiency. In this paper, we introduce a novel tool, the Privacy Policy Compliance Verification Knowledge Graph, referred to as PrivComp-KG. PrivComp-KG is designed to efficiently store and retrieve comprehensive information related to privacy policies, regulatory frameworks, and domain-specific legal knowledge. By utilizing LLM and Retrieval Augmented Generation (RAG), we can accurately identify relevant sections in privacy policies and map them to the corresponding regulatory rules. Our LLM-based retrieval system has demonstrated a high level of accuracy, achieving a correctness score of 0.9, outperforming other models in privacy policy analysis. The extracted information from individual privacy policies is then integrated into the PrivComp-KG. By combining this data with contextual domain knowledge and regulatory rules, PrivComp-KG can be queried to assess each vendor’s compliance with applicable regulations. We demonstrate the practical utility of PrivComp-KG by verifying the compliance of privacy policies across various organizations. This approach not only helps policy writers better understand legal requirements but also enables them to identify gaps in existing policies and update them in response to evolving regulations.},
  keywords={Semantic Web;Privacy;Law;Large language models;Retrieval augmented generation;Standards organizations;Organizations;Knowledge graphs;Regulation;Security;Privacy Policy;Policy Compliance;Large Language Model;Knowledge Graph},
  doi={10.1109/TPS-ISA62245.2024.00021},
  ISSN={},
  month={Oct},}@ARTICLE{10836698,
  author={Phan, Truong H. V. and do, Phuc},
  journal={IEEE Access}, 
  title={QUERY2BERT: Combining Knowledge Graph and Language Model for Reasoning on Logical Queries}, 
  year={2025},
  volume={13},
  number={},
  pages={16103-16119},
  abstract={Answering logical questions with a knowledge graph has been a critical research focus because this needs to reason and synthesize information. Previous studies have mainly dealt with logical operations using graph embedding techniques, such as conjunctions, disjunctions, and negation. However, these studies have neither effectively organized the data to retrieve multi-hop reasoning quickly nor combined text description to enhance logical operations’ semantics. Our study introduces a model called QUERY2BERT, which solves two of the above limitations. Specifically, QUERY2BERT first combined the node2vec and the BERT models to embed a knowledge graph with description information of every entity. Then, embedded nodes were indexed with a K-D tree structure. Finally, we used nearest neighbor search on K-D tree to retrieve neighbor-embedded nodes and implemented logical operations like projection, intersection, union, and negation to find answers to complex questions. We tested our model on three benchmark knowledge graph datasets and showed that QUERY2BERT significantly improved accuracy and speed compared to other state-of-the-art models.},
  keywords={Knowledge graphs;Cognition;Semantics;Vectors;Computational modeling;Logic;Encoding;Transforms;Question answering (information retrieval);Knowledge based systems;BERT;K-D tree;k-NN;knowledge graph embedding;multi-hop reasoning;logical query},
  doi={10.1109/ACCESS.2025.3528097},
  ISSN={2169-3536},
  month={},}@ARTICLE{10552698,
  author={Larhrib, Mohamed and Escribano, Miguel and Cerrada, Carlos and Escribano, Juan Jose},
  journal={IEEE Access}, 
  title={An Ontological Behavioral Modeling Approach With SHACL, SPARQL, and RDF Applied to Smart Grids}, 
  year={2024},
  volume={12},
  number={},
  pages={82041-82056},
  abstract={Every engineering process, especially software, involves two complementary aspects: structural and behavioral. Behavior is, in essence, transforming the structure associated with the system. As a language for the object-oriented paradigm, Unified Modeling Language (UML) offers constructs for both aspects, for example, class diagrams for the structural aspect and activity diagrams for the behavioral aspect. However, without obtaining directly executable models, in glass-box terms, or reasoning support, on the other hand, when software engineering is approached with ontologies, only constructs for structural aspects are provided to develop a directly executable model, thanks to their reasoning capability. However, there are no constructs or approaches for this paradigm’s specification or definition of behavior. This lack appears mainly in the early stages of the software engineering process, where there are no constructs similar to, e.g., the activity diagram in the object-oriented domain. Object Management Group (OMG) already addressed the transformation between the two paradigms in structural terms throughout Ontology Definition Metamodel (ODM) from UML to Resource Description Framework (RDF) and Web Ontology Language (OWL). However, there is no transformation of the object-oriented behavioral constructs into ontologies because they are not defined in the ontological paradigm. This paper addresses the definition of behavior in the ontology paradigm and the transformation of behavioral constructs between the two paradigms. The foundation of behavior specification is the flow concept, and the basis of this is the transformation of the structural model in an evaluative sense. Therefore, once the behavior has been defined in the ontology domain, the artifacts obtained throughout the life cycle are directly executable, and their validation and testing are automatic. With this approach, the life cycle is reduced to a modeling process. Thus, the resulting software engineering process improves features such as agility, simplicity, productivity, and formalism. The target audience for this work is the software engineering community, especially in the Model-Driven Engineering (MDE) paradigm approached from object-oriented and ontology perspectives. The evaluation of the proposed approach has been performed in the electric utilities, solving the problem of the validation flow for the interoperability process specified by the Common Grid Model Exchange Standard (CGMES) standard.},
  keywords={Unified modeling language;Ontologies;Object oriented modeling;Resource description framework;Software engineering;OWL;Cognition;Behavioral sciences;Behavioral modeling;CIM for ENTSO-E (CGMES);directly executable;ontology RDF/RDFS/OWL/SHACL},
  doi={10.1109/ACCESS.2024.3412656},
  ISSN={2169-3536},
  month={},}@ARTICLE{8502923,
  author={Halawi, Bahia and Mourad, Azzam and Otrok, Hadi and Damiani, Ernesto},
  journal={IEEE Access}, 
  title={Few are as Good as Many: An Ontology-Based Tweet Spam Detection Approach}, 
  year={2018},
  volume={6},
  number={},
  pages={63890-63904},
  abstract={Due to the high popularity of Twitter, spammers tend to favor its use in spreading their commercial messages. In the context of detecting twitter spams, different statistical and behavioral analysis approaches were proposed. However, these techniques suffer from many limitations due to: 1) ongoing changes to Twitter's streaming API which constrains access to a user's list of followers/followees; 2) spammer's creativity in building diverse messages; 3) use of embedded links and new accounts; and 4) need for analyzing different characteristics about users without their consent. To address the aforementioned challenges, we propose a novel ontology-based approach for spam detection over Twitter during events by analyzing the relationship between ham user tweets versus spams. Our approach relies solely on public tweet messages while performing the analysis and classification tasks. In this context, ontologies are derived and used to generate a dictionary that validates real tweet messages from random topics. Similarity ratio among the dictionary and tweets is used to reflect the legitimacy of the messages. Experiments conducted on real tweet data illustrate that message-to-message techniques achieved a low detection rate compared with our ontology-based approach which outperforms them by approximately 200%, in addition to promising scalability for large data analysis.},
  keywords={Twitter;Ontologies;Electronic mail;Feature extraction;Uniform resource locators;Tagging;Analytical models;Twitter;meta-data;spam detection;text based analysis;event spammers;ontology},
  doi={10.1109/ACCESS.2018.2877685},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10296153,
  author={Elkodssi, Iman and Sbai, Hanae},
  booktitle={2023 International Conference on Digital Age & Technological Advances for Sustainable Development (ICDATA)}, 
  title={Toward Semantic Framework for Internet of Things-Aware Business Process Discovery}, 
  year={2023},
  volume={},
  number={},
  pages={12-16},
  abstract={The Internet of Things (IoT) is often considered a disruptive technology [1]. By using smart devices, it has the potential to change everyone's daily life. With large sets of advanced sensors and actuators, it can create opportunities for commercial organizations to establish new business models. A fundamental barrier to automatic business process sensing is the lack of modeling concepts that explicitly express Internet elements as components of a business process model. Thus, there is a clear need to model these processes associated with IoT elements in a formal and unambiguous manner. However, in the context of business processes, there is a lack of formalized and explicit descriptions of IoT elements, which hinders their effective modeling and management. This article proposes a semantic formalization of the business process management perspective in an IoT environment by proposing Extended BPMNO for IoT and Domain Ontology. It uses standard semantic technologies to give a semantic representation that allows us to describe concepts relating to the IoT and the elements of an executable business process described in BPMN.},
  keywords={Annotations;Semantics;Ontologies;Data models;Business process management;Internet of Things;Sustainable development;Business Process Management Notation (BPMN);IoT element;The IoT-aware BP;Ontology},
  doi={10.1109/ICDATA58816.2023.00012},
  ISSN={},
  month={May},}@INPROCEEDINGS{10779814,
  author={Zhao, Xinke and Yilahun, Hankiz and Hamdulla, Askar},
  booktitle={2024 IEEE 5th International Conference on Pattern Recognition and Machine Learning (PRML)}, 
  title={Text-to-CQL Based on Large Language Model and Graph Pattern Enhancement}, 
  year={2024},
  volume={},
  number={},
  pages={114-119},
  abstract={With the wide application of knowledge graphs in various scenarios and Neo4j graph databases becoming excellent knowledge graph carriers, Cypher (CQL for short) has become the most popular graph database query language. However, when performing graph database retrieval, the complex pattern and syntax make constructing CQL statements a complicated and time-consuming task. Therefore, similar to Text-to-SQL, it is necessary and urgent to study an effective method for end-to-end transformation of natural languages into CQL. There have been many advances in Text-to-SQL for traditional relational databases, but these methods cannot be well applied to Text-to-CQL, so there is still a lack of effective work on Text-to-CQL for graph databases. In recent years, as large language models have been widely applied to different tasks with good results, and considering that the fundamental difference between CQL and SQL is the representation of graph patterns. We propose in this paper PA-LLM, a Text-to-CQL method that utilizes large language models combined with graph patterns enhancement, which combines large language models and subdivides the graph patterns into three categories according to their respective characteristics. They are simple query patterns, multi-hop query patterns, and function query patterns. For different graph patterns, the method optimizes the process of generating CQL for the model, which can be subdivided into four sub-methods, simple pattern enhancement method, multi-hop pattern enhancement method, function pattern enhancement method, and entity-relationship enhancement method. The experimental results show that the method improves the quality of the CQL statements generated by the model, including the logical accuracy ACCLX and the execution result accuracy ACCEX, and achieves better results on the SpCQL dataset proposed by the National Defense University of Science and Technology (NDUST). It provides an effective solution for the task of converting CQL to natural language end-to-end.},
  keywords={Structured Query Language;Accuracy;Quantization (signal);Large language models;Natural languages;Knowledge graphs;Relational databases;Syntactics;Real-time systems;Pattern recognition;Knowledge graph;Neo4j;Text-to-CQL;ChatGLM;Graph patterns enhancement},
  doi={10.1109/PRML62565.2024.10779814},
  ISSN={},
  month={July},}@INPROCEEDINGS{10677303,
  author={Gupta, Pranav and Sharma, Raunak and Kumari, Rashmi and Aditya, Sri Krishna and Choudhary, Shwetank and Kumar, Sumit and M, Kanchana and R, Thilagavathy},
  booktitle={2024 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)}, 
  title={ECHO: Environmental Sound Classification with Hierarchical Ontology-guided Semi-Supervised Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Environment Sound Classification has been a well-studied research problem in the field of signal processing and till now more focus has been laid on fully supervised approaches. Recently, the focus has moved towards semi-supervised methods which concentrate on utilizing unlabeled data, and self-supervised methods which learn the intermediate representation through pretext tasks or contrastive learning. However, both approaches require a vast amount of unlabelled data to improve performance. In this work, we propose a novel framework called Environmental Sound Classification with Hierarchical Ontology-guided semi-supervised Learning (ECHO) that utilizes label ontology-based hierarchy to learn semantic representation by defining a novel pretext task. The model tries to predict coarse labels represented by the Large Language Model (LLM) based on ground truth label ontology, then further fine-tuned in a supervised way to predict the actual task. ECHO achieves a 1% to 8% accuracy improvement over baseline systems across UrbanSound8K, ESC-10, and ESC-50 datasets.},
  keywords={Accuracy;Large language models;Semantics;Contrastive learning;Ontologies;Semisupervised learning;Signal processing;semi-supervised learning;Environment Sound Classification;Label ontology},
  doi={10.1109/CONECCT62155.2024.10677303},
  ISSN={2766-2101},
  month={July},}@INPROCEEDINGS{10065877,
  author={Hu, Yaxuan and Xu, Weizhi and Liu, Qiang and Wang, Liping and Wu, Shu},
  booktitle={2022 IEEE 8th International Conference on Computer and Communications (ICCC)}, 
  title={Can Pretrained Language Models Reason on Sparse Commonsense Knowledge Graph?}, 
  year={2022},
  volume={},
  number={},
  pages={2016-2022},
  abstract={Commonsense knowledge is the knowledge shared by most humans, which is always stored in commonsense knowledge graph (CKG) as triplets. In this paper, we focus on the task of CKG competition, whose target is to predict the tail (head) target given the head (tail) entity and the relation. Most existing works employ the graph-based models, which aggregate information from neighboring entities on CKG. Despite their effectiveness, they still suffer from two main weaknesses. Firstly, the semantic relations between head and tail entities are neglected. Secondly, due to the sparsity of CKG, they rely on the graph densification that it will bring unexpected noises. To solve these problems, we propose a unified framework for COmmonSense knowledge graph completion based on BERT, namely COS-BERT. Firstly, we transfer each triplet into a natural sentence. Then, we fine-tune the pretrained language model using the transformed sentences. Finally, we rank the candidates based on the output representation of sentences. Furthermore, we add a pre-filter to obtain a subset of candidates on the inference stage to save unnecessary computation costs. Comprehensive experiments have demonstrated the superiority of COS-BERT over the state-of-the-arts.},
  keywords={Costs;Computational modeling;Aggregates;Semantics;Bit error rate;Tail;Task analysis;commonsense reasoning;pretrained language model},
  doi={10.1109/ICCC56324.2022.10065877},
  ISSN={},
  month={Dec},}@ARTICLE{10766600,
  author={Zhang, Ying and Shen, Yangpeng and Xiao, Gang and Peng, Jinghui},
  journal={IEEE Access}, 
  title={Leveraging Non-Parametric Reasoning With Large Language Models for Enhanced Knowledge Graph Completion}, 
  year={2024},
  volume={12},
  number={},
  pages={177012-177027},
  abstract={The completeness of knowledge graphs is critical to their effectiveness across various applications. However, existing knowledge graph completion methods face challenges such as difficulty in adapting to new entity information, parameter explosion, and limited generalization capability. To address these issues, this paper proposes a knowledge graph completion framework that integrates large language models with case-based reasoning (CBR-LLM). By combining non-parametric reasoning with the semantic understanding capabilities of large language models, the framework not only improves completion accuracy but also significantly enhances generalization under various data-missing scenarios. Experimental results demonstrate that CBR-LLM excels in handling complex reasoning tasks and large-scale data-missing scenarios, providing an efficient and scalable solution for knowledge graph completion.},
  keywords={Knowledge graphs;Cognition;Accuracy;Computational modeling;Data models;Semantics;Large language models;Training;User experience;Tail;Case-based reasoning;large language model;information entropy;knowledge graph completion},
  doi={10.1109/ACCESS.2024.3505433},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9689347,
  author={Tseng, Wan-Ting and Wu, Chin-Ying and Hsu, Yung-Chang and Chen, Berlin},
  booktitle={2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)}, 
  title={FAQ Retrieval using Question-Aware Graph Convolutional Network and Contextualized Language Model}, 
  year={2021},
  volume={},
  number={},
  pages={2006-2012},
  abstract={Frequently asked question (FAQ) retrieval, which seeks to provide the most relevant question, or question-answer (QA) pair, in response to a user's query, has found its applications in widespread use cases. More recently, methods based on bidirectional encoder representations from Transformers (BERT) and its variants, which typically take the word embeddings of a question in training time (or query in test time) as the input to predict relevant answers, have shown good promise for FAQ retrieval. However, these BERT-based methods do not pay enough attention to the global information specifically about an FAQ task. To cater for this, we in this paper put forward a question-aware graph convolutional network (QGCN) to induce vector embeddings of vocabulary words, thereby encapsulating the global question-question, question-word and word-word relations which can be used to augment the embeddings derived from BERT for better F AQ retrieval. Meanwhile, we also investigate leverage domain-specific knowledge graphs to enrich the question and query embeddings (denoted by K-BERT). Finally, we conduct extensive experiments to evaluate the utility of the proposed approaches on two publicly-available FAQ datasets (viz. TaipeiQA and StackF AQ), where the associated results confirm the promising efficacy of the proposed approach in comparison to some top-of-the-line methods.},
  keywords={Training;Vocabulary;Convolution;Bit error rate;Information processing;Transformers;Task analysis;Frequently Asked Question;Graph Convolutional Networks;knowledge graph;language model},
  doi={},
  ISSN={2640-0103},
  month={Dec},}@INPROCEEDINGS{10650745,
  author={Jian, Zhaorui and Liu, Shengquan and Gao, Wei and Cheng, Jianming},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Distantly Supervised Relation Extraction based on Non-taxonomic Relation and Self-Optimization}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={Distantly supervised relation extraction (DS-RE) leverages existing knowledge bases to generate annotated data for relation extraction (RE), addressing the issue of scarce labeled data. However, distant supervision (DS) is often limited by coarse annotations and insufficient contextual awareness, leading to relational ambiguity and introducing noise in the labeled results. Moreover, although one can optimize the classifiers in DS-RE models through weight updates, the static nature of the guiding rules for such adjustments often falls short when addressing the challenges posed by diverse non-taxonomic relations and complex noise patterns in datasets. In this paper, we propose a DS-RE framework that capitalizes on non-taxonomic relations and a self-optimizing mechanism. We define a set of consistent DS relation candidates and combine DS with a LLM to enhance the perception of entities’ contextual states during the DS process. Then, we design a Self-Optimizing Ontology-Enhanced Non-taxonomic Relation Extraction Model (SO-NRE). The model incorporates additional entity-relation knowledge to enhance the semantic depth of Non-taxonomic relation ontologies and uses an adaptive dynamic scheduling mechanism to refine the classification strategy through iterations informed by self-perception outcomes. The experimental results show that the improved DS annotation workflow has enhanced accuracy, and SO-NRE outperforms mainstream baselines in RE performance.},
  keywords={Adaptation models;Accuracy;Annotations;Large language models;Noise;Semantics;Neural networks;Distantly Supervised Relation Extraction;Non-taxonomic Relation;LLM;Self-Optimization},
  doi={10.1109/IJCNN60899.2024.10650745},
  ISSN={2161-4407},
  month={June},}@ARTICLE{10517649,
  author={Chen, Xingyu and Liu, Jiaxu and Liu, Zeyang and Wan, Lipeng and Lan, Xuguang and Zheng, Nanning},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Knowledge Graph Enhancement for Fine-Grained Zero-Shot Learning on ImageNet21K}, 
  year={2024},
  volume={34},
  number={10},
  pages={9090-9101},
  abstract={Fine-grained Zero-shot Learning on the large-scale dataset ImageNet21K is an important task that has promising perspectives in many real-world scenarios. One typical solution is to explicitly model the knowledge passing using a Knowledge Graph (KG) to transfer knowledge from seen to unseen instances. By analyzing the hierarchical structure and the word descriptions on ImageNet21K, we find that the noisy semantic information, the sparseness of seen classes, and the lack of supervision of unseen classes make the knowledge passing insufficient, which limits the KG-based fine-grained ZSL. To resolve this problem, in this paper, we enhance the knowledge passing from three aspects. First, we use more powerful models such as the Large Language Model and Vision-Language Model to get more reliable semantic embeddings. Then we propose a strategy that globally enhances the knowledge graph based on the convex combination relationship of the semantic embeddings. It effectively connects the edges between the non-kinship seen and unseen classes that have strong correlations while assigning an importance score to each edge. Based on the enhanced knowledge graph, we further present a novel regularizer that locally enhances the knowledge passing during training. We extensively conducted comparative evaluations to demonstrate the advantages of our method over state-of-the-art approaches.},
  keywords={Semantics;Knowledge graphs;Zero-shot learning;Circuits and systems;Visualization;Training;Task analysis;Fine-grained zero-shot learning;knowledge graph;graph convolutional neural network},
  doi={10.1109/TCSVT.2024.3396215},
  ISSN={1558-2205},
  month={Oct},}@INPROCEEDINGS{10781547,
  author={Abbasian, Mahyar and Yang, Zhongqi and Khatibi, Elahe and Zhang, Pengfei and Nagesh, Nitish and Azimi, Iman and Jain, Ramesh and Rahmani, Amir M.},
  booktitle={2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
  title={Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Effective diabetes management is crucial for maintaining health in diabetic patients. Large Language Models (LLMs) have opened new avenues for diabetes management, facilitating their efficacy. However, current LLM-based approaches are limited by their dependence on general sources and lack of integration with domain-specific knowledge, leading to inaccurate responses. In this paper, we propose a knowledge-infused LLM-powered conversational health agent (CHA) for diabetic patients. We customize and leverage the open-source openCHA framework, enhancing our CHA with external knowledge and analytical capabilities. This integration involves two key components: 1) incorporating the American Diabetes Association dietary guidelines and the Nutritionix information and 2) deploying analytical tools that enable nutritional intake calculation and comparison with the guidelines. We compare the proposed CHA with GPT4. Our evaluation includes 100 diabetes-related questions on daily meal choices and assessing the potential risks associated with the suggested diet. Our findings show that the proposed agent demonstrates superior performance in generating responses to manage essential nutrients.},
  keywords={Large language models;Knowledge based systems;Medical services;Nutrients;Information retrieval;Diabetes;Risk management;Guidelines;LLMs;Knowledge Graph;Diabetes;Nutrition Therapy;Health Agents},
  doi={10.1109/EMBC53108.2024.10781547},
  ISSN={2694-0604},
  month={July},}@INPROCEEDINGS{10479303,
  author={Ferchichi, Olfa and Beltaifa, Raoudha and Labed Jilani, Lamia},
  booktitle={2023 20th ACS/IEEE International Conference on Computer Systems and Applications (AICCSA)}, 
  title={Artificial Intelligence Based SysML Block Diagram Extension and Evolution for Product Lines}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={SysML is a standard language that permits to model systems of any type such as plane, ships and software intensive systems. Software Product Line large scale reuse approach has demonstrated its success. The industry provides benefits in term of cost savings and acceleration of time to maket. The available literature indicates that there have been efforts to enhance the capability of SysML in handling product families. However, these attempts are not yet fully systematic, and there remains a significant amount of work to be undertaken in this area. In this present paper, we deal with the SysML Block Diagram in order to investigate to what extent it permits variability representation and how it can evolve during the system evolution or when agility is needed. We want to capitalize on the knowledge necessary for block diagram extention and evolution and take advantage of knowledge from Product Line domain engineering and application engineering. So, we decide to use an ontology which is an articifial intelligence artifact. An ontology is a powerful mean to represent knowledge and reason about it. Here, we use the ontology to help decision making for Block diagram evolution as well.},
  keywords={Industries;Systematics;Description logic;Systems architecture;Ontologies;Software;Mobile handsets;SysML block diagram;Product Line Engineering;Variability;Evolution;Artificial Intelligence Artifact;Ontology},
  doi={10.1109/AICCSA59173.2023.10479303},
  ISSN={2161-5330},
  month={Dec},}@INPROCEEDINGS{10348639,
  author={Palagin, Oleksandr and Kaverinsky, Vladislav and Petrenko, Mykola and Malakhov, Kyrylo},
  booktitle={2023 IEEE 12th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)}, 
  title={Digital Health Systems: Ontology-Based Universal Dialog Service for Hybrid E-Rehabilitation Activities Support}, 
  year={2023},
  volume={1},
  number={},
  pages={84-89},
  abstract={The medical rehabilitation system in Ukraine encountered a set of crucial challenges that demanded immediate attention and action. The primary objective revolves around rehabilitating patients with Combat stress reaction. Ukraine possesses a network of medical and preventive institutions that cater to the psycho-physiological rehabilitation needs of military personnel. These institutions employ contemporary rehabilitation technologies. Nonetheless, not all individuals have access to long-term rehabilitation within these centers. Hence, the integration of telerehabilitation technology becomes crucial for patients dealing with post-traumatic stress disorder and related conditions. This integration, combined with objective monitoring of the functional state, holds significant importance. Remote patient-centered rehabilitation emerges as one of the most effective approaches within the realm of medical rehabilitation assistance. Moreover, there is a need for efficient methods that support the “Physical therapist - Patient - Multidisciplinary team” system in the field of rehabilitation. Hence, in this paper, we not only explore conventional rehabilitation techniques but also present and elucidate the following advancements: a revised and comprehensive understanding of the hybrid e-rehabilitation concept and its underlying principles, an enhanced formalization notion of the Smart-system for remote support in hybrid e-rehabilitation services and activities, and the conceptual framework and software implementation of the ontology-based universal dialog service within the Smart-system.},
  keywords={Data acquisition;Ontologies;Software;Electronic healthcare;Personnel;IEEE activities;Stress;Ontology engineering;hybrid e-rehabilitation;Telerehabilitation;Transdisciplinary research;Computational linguistics;Universal dialog service},
  doi={10.1109/IDAACS58523.2023.10348639},
  ISSN={2770-4254},
  month={Sep.},}@INPROCEEDINGS{8089860,
  author={Naranjo, David and Sánchez, Mario and Villalobos, Jorge},
  booktitle={2017 IEEE 21st International Enterprise Distributed Object Computing Conference (EDOC)}, 
  title={Visualizing the Bias of Enterprise Metamodels towards Nuanced Concepts}, 
  year={2017},
  volume={},
  number={},
  pages={30-39},
  abstract={In Enterprise Modeling, we use several languages for designing, analyzing, and communicating the different domains of an enterprise. Two important criteria for choosing a domainspecific language are its appropriateness to the requirements of the enterprise, as well as the accuracy of the language in describing the domain at hand. However, in some business domains, core concepts --such as Capability, Service, and Value-- represent constructs that have different (and often conflicting) definitions and interpretations among the literature. In this context, the bias of a language is the preference for a theory, i.e. a particular interpretation of a concept. Currently, there is no explicit way of assessing and communicating this bias, and thus it remains difficult to assess the appropriateness and accuracy of a language for a particular purpose. In this paper, we provide a method for visual comparison of this bias with regard to the Capability concept, comparing three theories and three modeling languages where this concept is pivotal.},
  keywords={Semantics;Unified modeling language;Ontologies;Pragmatics;Syntactics;Analytical models;Visualization;nuance;enterprise modeling;model theory;semantics;ontology},
  doi={10.1109/EDOC.2017.14},
  ISSN={2325-6362},
  month={Oct},}@INPROCEEDINGS{10131050,
  author={Dunbar, Daniel and Vierlboeck, Maximilian and Blackburn, Mark},
  booktitle={2023 IEEE International Systems Conference (SysCon)}, 
  title={Use of Natural Language Processing in Digital Engineering Context to Aid Tagging of Model}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper uses Natural Language Processing to provide augmented intelligence assistance to the resource intensive task of aligning systems engineering artifacts, namely text requirements and system models, with ontologies. Ontologies are a key enabling technology for digital, multidisciplinary interoperability. The approach presented in this paper combines the efficiency of statistical based natural language processing to process large sets of data with expert verification of output to enable accurate alignment to ontologies in a time efficient manner. It applies this approach to an example from the telecommunications domain to demonstrate the workflows and highlight key points in the process. Enabling easier, faster alignment of systems engineering artifacts with ontologies allows for a holistic view of a system under design and enables interoperability between tools and domains.},
  keywords={Measurement;Ontologies;Tagging;Natural language processing;Telecommunications;Requirements engineering;Task analysis;ontology;natural language processing;semantic web;digital engineering;authoritative source of truth;augmented intelligence},
  doi={10.1109/SysCon53073.2023.10131050},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{10186650,
  author={Chondamrongkul, Nacha and Sun, Jing and Warren, Ian and Lee, Scott Uk-Jin},
  booktitle={2020 IEEE/ACM 8th International Conference on Formal Methods in Software Engineering (FormaliSE)}, 
  title={Semantic-based Architecture Smell Analysis}, 
  year={2020},
  volume={},
  number={},
  pages={109-118},
  abstract={Software smells have negative impacts on the reliability and modifiability of software systems. The smells in architecture design can be cascaded down to the implementation level and cause issues that require much effort to fix. Therefore, early detection of the architecture smells can benefit the overall quality of the software system. This paper presents an integration of methods that formally define the software architecture design towards architecture smell detection. Our approach serves as a framework that allows the architectural structures and behaviours to be formally analysed based on a coherent technique. We evaluated the accuracy and performance of our approach with the models generated from open source projects. The results show that our approach is effective and functions well.},
  keywords={Software architecture;OWL;Computer architecture;Ontologies;Model checking;Software systems;Cognition;Architecture Smells;Software Architecture;Ontology Web Language;Model Checking;Smell Detection},
  doi={},
  ISSN={2575-5099},
  month={May},}@INPROCEEDINGS{10387634,
  author={Procko, Tyler Thomas and Ochoa, Omar and Elvira, Timothy},
  booktitle={2023 Fifth International Conference on Transdisciplinary AI (TransAI)}, 
  title={Automatic Generation of BFO-Compliant Aristotelian Definitions in OWL Ontologies with GPT}, 
  year={2023},
  volume={},
  number={},
  pages={141-146},
  abstract={Ontologies are representational artifacts that purport to accurately describe some aspect of reality, including the entities and the relations that hold between them. In computer science, ontologies are software artifacts containing the schematic structure for machine-readable knowledge, typically formed as a graph of subject-predicate-object triples, constrained through Description Logics. These resources and their relations are self-defining, i.e., some resource may be defined by considering all its stated relations. Resources are often attended with natural language annotations, that humans may read and interpret, such as labels and definitions. Many long-standing ontologies have useless lexical definitions that define resources cyclically, e.g., a FOAF: Person is simply defined as “A person”. In Aristotelian terms, the definition of a thing should be reducible, by using terms simpler than itself, such that every definition can be unpacked up to the most general thing, which can only be defined by stating examples and use cases. This paper presents an innovative technique that leverages the Generative Pre-trained Transformer (GPT) large language model, GPT -4, for automatically generating Aristotelian definition annotations for OWL classes that engenders compliance with the Basic Formal Ontology standard.},
  keywords={Annotations;Description logic;OWL;Natural languages;Maintenance engineering;Transformers;Software;ontology;epistemology;Linked Data;BFO;GPT},
  doi={10.1109/TransAI60598.2023.00042},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10807443,
  author={Li, Jinghong and Siritanawan, Prarinya and Gu, Wen and Hasegawa, Shinobu},
  booktitle={2024 IEEE International Conference on Agents (ICA)}, 
  title={Multi-Agent Approach for Dynamic Research Insight Path Generation}, 
  year={2024},
  volume={},
  number={},
  pages={128-129},
  abstract={In recent years, researchers have witnessed rapid advancements in conducting paper surveys using generative AI, enhancing survey efficiency to some extent. However, today’s generative AI lacks deep research training to analyze logical threads woven across multiple papers. A concise visualization method is also expected to present logical connections among various papers. These logical threads are often implicit in the issue ontology authors commonly employ when writing papers. Building on this issue ontology, our method utilizes Dynamic Programming with multiple agents to generate an insight path. The key feature of this approach is the collaboration of multiple agents to adapt to a complex environment and make optimized decisions on issue ontology selection. This path aims to succinctly express longitudinal logical connections among multiple papers, including commonality, difference, and inheritance.},
  keywords={Surveys;Training;Visualization;Generative AI;Collaboration;Ontologies;Writing;Programming;Weaving;Dynamic programming;Issue ontology;Insight path;Longitudinal;Agent;Dynamic programming},
  doi={10.1109/ICA63002.2024.00035},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9728603,
  author={Wang, Kaiqiang and Zhang, Yu and Jiang, Chaoyuan and Luo, Junren and Yang, Xueke and Chen, Shikai},
  booktitle={2021 China Automation Congress (CAC)}, 
  title={Visual Semantic Planning for Service Robot via Natural Language Instructions}, 
  year={2021},
  volume={},
  number={},
  pages={793-798},
  abstract={The interactive instruction following task requires intelligent robots to complete natural language instructions by interacting with the surrounding environment through visual perception and mechanical manipulation. The most representative benchmark is the ALFRED challenge task, which aims to perform restricted sequential actions to accomplish daily housework in a virtual environment. However, recently proposed multi-modal task planning approaches that combine vision and language do not perform well on ALFRED task. In this work, we utilize natural language instructions to build a single-mode model, and translate the task into a sequential decision problem, which focuses on generating continuous high-level action sequences directly from the instructions. We demonstrate that our K-PLM (knowledge enabled pre-trained language model) may successfully generate concrete visual semantic plans in 31.4% tasks on unseen scenarios without visual cues, where 62.2% can be generated if the model merges some visual cues, i.e. the location of the first object in the scene. The results show that our model provides outstanding visual semantic plans for the embodied agent to perform tasks and outperforms prior works.},
  keywords={Visualization;Service robots;Semantics;Natural languages;Virtual environments;Planning;Indoor environment;natural language instructions;knowledge graph;pre-trained model;visual semantic planning;interactive instruction following task},
  doi={10.1109/CAC53003.2021.9728603},
  ISSN={2688-0938},
  month={Oct},}@INPROCEEDINGS{10771088,
  author={Li, Harry and Appleby, Gabriel and Suh, Ashley},
  booktitle={2024 IEEE Visualization and Visual Analytics (VIS)}, 
  title={LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph Question-Answering}, 
  year={2024},
  volume={},
  number={},
  pages={116-120},
  abstract={We present LinkQ, a system that leverages a large language model (LLM) to facilitate knowledge graph (KG) query construction through natural language question-answering. Traditional approaches often require detailed knowledge of a graph querying language, limiting the ability for users – even experts – to acquire valuable insights from KGs. LinkQ simplifies this process by implementing a multistep protocol in which the LLM interprets a user’s question, then systematically converts it into a well-formed query. LinkQ helps users iteratively refine any open-ended questions into precise ones, supporting both targeted and exploratory analysis. Further, LinkQ guards against the LLM hallucinating outputs by ensuring users’ questions are only ever answered from ground truth KG data. We demonstrate the efficacy of LinkQ through a qualitative study with five KG practitioners. Our results indicate that practitioners find LinkQ effective for KG question-answering, and desire future LLM-assisted exploratory data analysis systems.},
  keywords={Analytical models;Protocols;Limiting;Data analysis;Visual analytics;Large language models;Data visualization;Knowledge graphs;Writing;Data models;Knowledge graphs;large language models;query construction;question-answering;natural language interfaces},
  doi={10.1109/VIS55277.2024.00031},
  ISSN={2771-9553},
  month={Oct},}@INPROCEEDINGS{8432294,
  author={Machado Lunardi, Gabriel and Medeiros Machado, Guilherme and Al Machot, Fadi and Maran, Vinícius and Machado, Alencar and C. Mayr, Heinrich and A. Shekhovtsov, Vladimir and Palazzo M. de Oliveira, José},
  booktitle={2018 IEEE 32nd International Conference on Advanced Information Networking and Applications (AINA)}, 
  title={Probabilistic Ontology Reasoning in Ambient Assistance: Predicting Human Actions}, 
  year={2018},
  volume={},
  number={},
  pages={593-600},
  abstract={Providing reminders to elderly people in their home environment, while they perform their daily activities, is considered as a user support activity, and thus a relevant topic in Active and Assisted Living (AAL) research and development. Determining such reminders implies decision-making, since the actions' flow (behavior) usually involves probabilistic branches. An automated system needs to decide which of the next actions is the best one for the user in a given situation. Problems of this nature involve uncertainty levels that have to be dealt with. Many approaches to this problem exploit statistical data only, thus ignoring important semantic data as, for instance, are provided by Ontologies. However, ontologies do not support reasoning over uncertainty natively. In this paper, we present a probabilistic semantic model that enables reasoning over uncertainty without losing semantic information. This model will be exemplified by an extension of the Human Behavior Monitoring and Support [HBMS] approach that provides a conceptual model for representing the user's behavior and its context in her/his living environment. The performance of this approach was evaluated using real data collected from a smart home prototype equipped with sensors. The experiments provided promising results which we will discuss regarding limits and challenges to overcome.},
  keywords={Ontologies;Uncertainty;Cognition;Probabilistic logic;Semantics;Hidden Markov models;Context modeling;Probabilistic ontologies;Uncertainty reasoning;Ambient assistance;Smart home;Context awareness},
  doi={10.1109/AINA.2018.00092},
  ISSN={2332-5658},
  month={May},}@ARTICLE{10288249,
  author={Zhang, Rui and Su, Yixin and Trisedya, Bayu Distiawan and Zhao, Xiaoyan and Yang, Min and Cheng, Hong and Qi, Jianzhong},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment Enabled by Large Language Models}, 
  year={2024},
  volume={36},
  number={6},
  pages={2357-2371},
  abstract={The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs’ entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity alignment can be done without manually crafted seed alignments. AutoAlign is not only fully automatic, but also highly effective. Experiments using real-world KGs show that AutoAlign improves the performance of entity alignment significantly compared to state-of-the-art methods.},
  keywords={Learning systems;Knowledge graphs;Vectors;Data models;Task analysis;Attribute embeddings;deep learning;entity alignment;knowledge base;knowledge graph;knowledge graph alignment;large language model;predicate proximity graph;representation learning},
  doi={10.1109/TKDE.2023.3325484},
  ISSN={1558-2191},
  month={June},}@INPROCEEDINGS{10132129,
  author={Zhang, Pengyu and Gao, Shijie and Shen, Yi and Yang, Shiyu and Sun, Lizhuang},
  booktitle={2023 IEEE 9th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)}, 
  title={SG-RC: SG-CIM Grid Knowledge Graph Relationship Complementation Model Based on Entropy Uncertainty and Semantic Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={181-186},
  abstract={SG-CIM grid knowledge graph is a domain knowledge graph based on the actual business knowledge of State Grid Corporation of China. SG-CIM grid knowledge graph consists of the triad of "entity-relationship-entity " of the grid. The traditional construction of SG-CIM grid knowledge graph needs experts familiar with relevant fields to build it manually, which is a necessary work in the early stage of data accumulation. This work consumes a lot of labor and cost, and some relations are missing due to manual operation. However, completing the relationship on the incomplete SG-CIM grid knowledge graph will lead to the problem that some difficult samples cannot be mined. To solve this problem, this paper proposes an SG-CIM grid knowledge graph relationship complementation method SG-RC based on entropy uncertainty and semantic recognition.SG-RC consists of an active learning module based on entropy uncertainty calculation and a relationship prediction module based on semantic recognition. The SG-RC model first mines all possible entity pairs to calculate the entropy uncertainty, and those with higher entropy uncertainty The higher entropy uncertainty represents the poor relationship prediction effect of the model, and those entity pairs with higher entropy uncertainty are recommended to experts for relationship labeling, and multiple iterations improve the quality of prediction results of the semantic recognition relationship prediction module. Compared with randomly selected entity pairs, this method can improve the completion probability of relation completion.},
  keywords={Uncertainty;Costs;Annotations;Semantics;Knowledge graphs;Predictive models;Entropy;SG-CIM;knowledge graph;semantic recognition;active learning;relationship complementation},
  doi={10.1109/BigDataSecurity-HPSC-IDS58521.2023.00040},
  ISSN={},
  month={May},}@INPROCEEDINGS{10677287,
  author={Saini, Shashwat and Vrindavanam, Jayavrinda and Mondal, Subhash},
  booktitle={2024 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)}, 
  title={Methodological Insights Into Protein Clustering Using BERT & RoBERTa}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Proteins are present in all living organisms, and understanding their processes is vital. Protein databases such as SWISS-PROT include curated information on only 570,000 protein sequences, representing a fraction of the 250 million known evidential and predicted sequences; it becomes crucial to cluster proteins into similar groups. This research explores the application of two transformer architectures, BERT and RoBERTa in clustering proteins in the supervised prediction of Gene Ontology (GO) annotations. The detailed methodology for both the pre-training and fine-tuning processes, as well as results that showcase RoBERTa outperforming BERT in the context of protein clustering, on performance metrics of accuracy and loss. Operating under constrained computational resources, the deployed model exhibits strong performance and highlight the robustness of methodology in protein clustering within resource constraints. This study not only contributes to the understanding of protein clustering but also signifies the potential of transformer models to handle biological data.},
  keywords={Proteins;Training;Analytical models;Annotations;Biological system modeling;Computational modeling;Ontologies;Protein Clustering;BERT;RoBERTa;Natural Language Processing;Transformers;Masked Language Modelling},
  doi={10.1109/CONECCT62155.2024.10677287},
  ISSN={2766-2101},
  month={July},}@INPROCEEDINGS{10350744,
  author={Guizzardi, Giancarlo},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={An Ontological View on Types}, 
  year={2023},
  volume={},
  number={},
  pages={634-634},
  abstract={Types are fundamental for modeling, being an essential construct in all major modeling languages. These include traditional conceptual modeling languages - such as Entity-Relationship models, UML class diagrams, or Object-Role-Modeling (ORM) specifications, and knowledge representation languages alike (e.g., the Web Ontology Language - OWL).},
  keywords={Unified modeling language;OWL;Knowledge representation;Model driven engineering;Ontological Foundations for Modeling;Types and Taxonomic Structures;Multi-Level Modeling},
  doi={10.1109/MODELS-C59198.2023.00103},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9776298,
  author={Qi, Peng and Huang, Zhen and Sun, Yan and Luo, Hong},
  booktitle={2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
  title={A Knowledge Graph-Based Abstractive Model Integrating Semantic and Structural Information for Summarizing Chinese Meetings}, 
  year={2022},
  volume={},
  number={},
  pages={746-751},
  abstract={With the rapid increase of users, online meeting platforms have accumulated massive meeting transcripts. However, it is still a challenge for users to quickly master the chief information and manage the meetings, despite there are already some useful text summarization models. In this paper, a Knowledge Graph-based Meeting Summarization Framework is proposed to tackle this challenge. First, a two-layers meeting domain Knowledge Graph is developed to integrate more information of meetings. Based on which, an encoder-decoder architecture is utilized to summarize meetings. For encoding meetings, a structural-level and semantic-level embedding strategy is considered, concretely, the Knowledge Graph is embedded to obtain the structural information, an interaction intention recognition model and a two-level transformer mechanism are devised to get the semantic information. Finally, the structural information and semantic information are combined and fed into the decoding network to generate meeting summaries. Extensive experiments on the Chinese meeting dataset show that our summarization framework outperforms other state-of-the-art models.},
  keywords={Conferences;Computational modeling;Semantics;Transformers;Collaborative work;Encoding;Decoding;meeting domain Knowledge Graph;an interaction intention recognition model;a two-level transformer mechanism;meeting summarization},
  doi={10.1109/CSCWD54268.2022.9776298},
  ISSN={},
  month={May},}@INPROCEEDINGS{8089878,
  author={Sales, Tiago Prince and Guarino, Nicola and Guizzardi, Giancarlo and Mylopoulos, John},
  booktitle={2017 IEEE 21st International Enterprise Distributed Object Computing Conference (EDOC)}, 
  title={An Ontological Analysis of Value Propositions}, 
  year={2017},
  volume={},
  number={},
  pages={184-193},
  abstract={In competitive markets, companies need well-designed business strategies if they seek to grow and obtain sustainable competitive advantage. At the core of a successful business strategy there is a carefully crafted value proposition, which ultimately defines what a company delivers to its customers. Despite their widely recognized importance, there is however little agreement on what exactly value propositions are. This lack of conceptual clarity harms the communication among stakeholders and the harmonization of current business strategy theories and strategy support frameworks. Furthermore, it hinders the development of systematic methodologies for crafting value propositions, as well as adequate support for representing and analyzing them. In this paper, we present an ontological analysis of value propositions based on a review of most relevant business and marketing theories and on previous work on value ascription, grounded in the Unified Foundational Ontology (UFO). Our investigation clarifies how value propositions are different from value presentations, and shows the difference between value propositions at the business level from those related to specific offerings.},
  keywords={Companies;Ontologies;Unified modeling language;Semantics;Analytical models;Proposals;ontological analysis;formal ontology;value proposition},
  doi={10.1109/EDOC.2017.32},
  ISSN={2325-6362},
  month={Oct},}@INPROCEEDINGS{10250672,
  author={Mahajan, Arpana Dipak and Mahale, Akshay and Deshmukh, Amol S and Vidyadharan, Arun and Hegde, Vijeth S and Vijayaraghavan, Koushik},
  booktitle={2023 Second International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)}, 
  title={Knowledge Graph-based Recommendation Engine: The Review}, 
  year={2023},
  volume={},
  number={},
  pages={1088-1095},
  abstract={With the rapid growth of digital information and the increasing complexity of user preferences, recommender systems have become essential in various application domains. Knowledge graph-based recommendation methods have emerged as a promising approach to enhance the accuracy and interpretability of recommendations. This research study explores the landscape of knowledge graph-based recommendation methods and examine benchmark datasets from the perspectives of different application scenarios. Here, the application scenarios are categorized into e-commerce, social media, content-based, and cross-domain recommendations. This study has analyzed the challenges and opportunities that arise in each scenario and discuss the corresponding knowledge graph-based recommendation techniques. Furthermore, this study investigates the existing benchmark datasets specifically designed for knowledge graph-based recommendation, highlighting their characteristics, strengths, and limitations. By providing an in-depth analysis of knowledge graph-based recommendation methods and benchmark datasets, this review paper serves as a valuable resource for researchers and practitioners working in the field, aiding in the development and evaluation of effective recommendation systems tailored to specific application scenarios.},
  keywords={Technological innovation;Social networking (online);Filtering;Semantics;Knowledge graphs;Benchmark testing;Complexity theory;Knowledge Graph;Recommendation Engine;Generative Ai;Graph Algorithm;Collaborative Filtering},
  doi={10.1109/ICAISS58487.2023.10250672},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{8876971,
  author={Guizzardi, Giancarlo and Figueiredo, Guylerme and Hedblom, Maria M. and Poels, Geert},
  booktitle={2019 13th International Conference on Research Challenges in Information Science (RCIS)}, 
  title={Ontology-Based Model Abstraction}, 
  year={2019},
  volume={},
  number={},
  pages={1-13},
  abstract={In recent years, there has been a growth in the use of reference conceptual models to capture information about complex and critical domains. However, as the complexity of domain increases, so does the size and complexity of the models that represent them. Over the years, different techniques for complexity management in large conceptual models have been developed. In particular, several authors have proposed different techniques for model abstraction. In this paper, we leverage on the ontologically well-founded semantics of the modeling language OntoUML to propose a novel approach for model abstraction in conceptual models. We provide a precise definition for a set of Graph-Rewriting rules that can automatically produce much-reduced versions of OntoUML models that concentrate the models' information content around the ontologically essential types in that domain, i.e., the so-called Kinds. The approach has been implemented using a model-based editor and tested over a repository of OntoUML models.},
  keywords={Unified modeling language;Object oriented modeling;Complexity theory;Ontologies;Semantics;Context modeling;Clustering methods;Model Abstraction;Complexity Management in Conceptual Modeling;Ontology-Based Conceptual Modeling},
  doi={10.1109/RCIS.2019.8876971},
  ISSN={2151-1357},
  month={May},}@INPROCEEDINGS{9892075,
  author={Lin, RuiMing and Cheng, LiangLun and Wang, Tao and Deng, Jianfeng},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Trans-SBLGCN: A Transfer Learning Model for Event Logic Knowledge Graph Construction of Fault Diagnosis}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Taking fault diagnosis corpus as the research object, an event logic knowledge graph construction method is proposed in this paper. Firstly, we propose a data labeling strategy based on a constructed event logic ontology model, then collect large-scale robot transmission system fault diagnosis corpus, and label part of the data according to the strategy. Secondly, we propose a transfer learning model called Trans-SBLGCN for event argument entity and event argument relation joint extraction. A language model is trained based on large-scale unlabeled fault diagnosis corpus and transferred to a model based on stacked bidirectional long short term memory (BiLSTM) and bidirectional graph convolutional network (BiGCN). Experimental results show that the method is superior to other methods. Finally, an event logic knowledge graph of robot transmission system fault diagnosis is constructed to provide decision support for autonomous robot transmission system fault diagnosis.},
  keywords={Fault diagnosis;Knowledge engineering;Transfer learning;Neural networks;Ontologies;Data models;Labeling;Event Logic Knowledge Graph;Fault Diagnosis;Knowledge Joint Extraction;BiGCN},
  doi={10.1109/IJCNN55064.2022.9892075},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{10710783,
  author={Vieira da Silva, Luis Miguel and Kocher, Aljosha and Gehlhoff, Felix and Fay, Alexander},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Toward a Method to Generate Capability Ontologies from Natural Language Descriptions}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={To achieve a flexible and adaptable system, capabil-ity ontologies are increasingly leveraged to describe functions in a machine-interpretable way. However, modeling such complex ontological descriptions is still a manual and error-prone task that requires a significant amount of effort and ontology expertise. This contribution presents an innovative method to automate capability ontology modeling using Large Language Models (LLMs), which have proven to be well suited for such tasks. Our approach requires only a natural language description of a capability, which is then automatically inserted into a predefined prompt using a few-shot prompting technique. After prompting an LLM, the resulting capability ontology is automatically verified through various steps in a loop with the LLM to check the overall correctness of the capability ontology. First, a syntax check is performed, then a check for contradictions, and finally a check for hallucinations and missing ontology elements. Our method greatly reduces manual effort, as only the initial natural language description and a final human review and possible correction are necessary, thereby streamlining the capability ontology generation process.},
  keywords={Adaptation models;Costs;Reviews;Large language models;Natural languages;Manuals;Ontologies;Syntactics;Manufacturing automation;Testing;Large Language Models;LLMs;Capabilities;Skills;Ontologies;Semantic Web;Model-Generation},
  doi={10.1109/ETFA61755.2024.10710783},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10453813,
  author={Albokae, Nazeer and AlKhtib, Bassel and Omar, Khaled},
  booktitle={2023 24th International Arab Conference on Information Technology (ACIT)}, 
  title={Hybrid Method for ICD Prediction Using Word Embedding and Natural Language Processing}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={The international classification of diseases is a standard in medical coding, and it is contains all information and description of diseases in heroical structure, and finding the International Classification of Diseases (ICD) code for diseases is important and essential thing in medical sector, the coding process takes a lot of time and money to find the correct and the exact code of the patient disease, researchers in artificial intelligence and in natural language processing and in machine learning make a huge efforts to build and develop automatic systems and algorithms for automatic ICD encoding, in this paper we propose a hybrid method for automatic ICD encoding from patient claims, the proposed method contains two main parts first for ICD chapter, ICD group classification, and the second one for find the most relevant ICD code based on patient claim diagnosis description, the first step was implemented by using natural language processing techniques, that it include stemming (Porter Stemmer was used for stemming), stop word removing, and the implementation of the second step was done by using PubMed BERT model for embedding for the ICD codes the embedding done based on the descriptions, and also the embedding done for the patient claim diagnosis description, we have tested the developed algorithm on medical dataset The results of our tests indicate that the proposed method is highly efficient, with a precision rate of 87%.},
  keywords={Codes;Prediction algorithms;Natural language processing;Encoding;Classification algorithms;Medical diagnostic imaging;Diseases;automatic ICD coding;PubMed BERT;ICD ontology},
  doi={10.1109/ACIT58888.2023.10453813},
  ISSN={2831-4948},
  month={Dec},}@INPROCEEDINGS{10603963,
  author={Lyu, Yang},
  booktitle={2024 5th International Conference on Computer Engineering and Application (ICCEA)}, 
  title={Research on the Construction of Knowledge Platform for Pumped Storage Power Station}, 
  year={2024},
  volume={},
  number={},
  pages={1714-1718},
  abstract={The purpose of introducing the knowledge platform for pumped storage power station is to make full use of the experiential knowledge accumulated over a long period of time to provide a more effective organization, management and decision support ability. Firstly, this paper elaborates the definition, development, and architecture of the knowledge platform, then the key technologies and overall workflow of the knowledge platform are described in detail. Finally, some practical application scenarios are introduced and analyzed which has a certain reference value for the construction and application of knowledge platform for pumped storage power station.},
  keywords={Knowledge engineering;Industries;Computational modeling;Decision making;Organizations;Knowledge graphs;Computer architecture;knowledge platform;pumped storage power station;knowledge graph;large language model},
  doi={10.1109/ICCEA62105.2024.10603963},
  ISSN={2159-1288},
  month={April},}@INPROCEEDINGS{10387602,
  author={Leventi-Peetz, Anastasia-Maria and Raber, Frederic and Rüll, Annika and Weber, Kai},
  booktitle={2023 Fifth International Conference on Transdisciplinary AI (TransAI)}, 
  title={Biotechnology Machine Learning Techniques for Natural Language Processing}, 
  year={2023},
  volume={},
  number={},
  pages={118-119},
  abstract={The possibility to transfer machine learning techniques from biotechnology to natural language processing models to increase training efficiency will be generally discussed. The motivation and reasoning behind the idea will be briefly outlined.},
  keywords={Training;Biotechnology;Biological system modeling;Machine learning;Natural language processing;Cognition;Sustainable development;Machine learning;natural language processing;gene ontology;protein function;model sustainability},
  doi={10.1109/TransAI60598.2023.00029},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10442824,
  author={Wang, Xiaoyi and Liu, Jie and Zhou, Jianshe and Wang, Jincheng},
  booktitle={2023 IEEE International Conference on Electrical, Automation and Computer Engineering (ICEACE)}, 
  title={A Survey of Pre-Trained Language Models IncorporatingKnowledge Graphs}, 
  year={2023},
  volume={},
  number={},
  pages={1706-1710},
  abstract={Pre-trained models acquire knowledge from vast amounts of unannotated and unstructured data through self-supervised learning. However, they suffer from limitations such as inadequate performance and limited knowledge reasoning capabilities due to the lack of external knowledge guidance. To address these limitations, integrating structured knowledge from knowledge graphs into pretrained models enables them to acquire both general semantic knowledge from free text and real-world knowledge behind the text, thereby effectively addressing downstream knowledge-driven tasks. This paper introduces the concepts of pretrained models and knowledge graphs, discusses research advancements, provides an overview of methods for integrating knowledge into pretrained models, and proposes three classification approaches based on fusion methods. It also outlines the application domains where these approaches can be applied. Finally, the paper summarizes and discusses future research directions for pretrained models Integrated with knowledge.},
  keywords={Knowledge engineering;Surveys;Analytical models;Computational modeling;Semantics;Knowledge graphs;Task analysis;Pre-trained language model;Knowledge graph;Natural language processing},
  doi={10.1109/ICEACE60673.2023.10442824},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8247474,
  author={Fiallos, Angel},
  booktitle={2017 IEEE Second Ecuador Technical Chapters Meeting (ETCM)}, 
  title={Assisted curricula design based on generation of domain ontologies and the use of NLP techniques}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  abstract={The following work proposes an approach that allows educators to manage curricula of different academic disciplines, with the support of recommendations and suggestions of contents and educational materials. The recommendations will be process through by domain ontologies, constructed from digital texts and the use of natural language process techniques. These recommendations will also support students during the learning process on specific academic areas.},
  keywords={Ontologies;Training;Natural language processing;Semantics;Computational modeling;Context modeling;OWL;NLP;curriculum;ontology;topic;modeling;domain},
  doi={10.1109/ETCM.2017.8247474},
  ISSN={},
  month={Oct},}@ARTICLE{10816465,
  author={Xia, Liqiao and Fan, Junming and Parlikad, Ajith and Huang, Xiao and Zheng, Pai},
  journal={IEEE Transactions on Big Data}, 
  title={Unlocking Large Language Model Power in Industry: Privacy-Preserving Collaborative Creation of Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={1-15},
  abstract={Semantic expertise remains a reliable foundation for industrial decision-making, while Large Language Models (LLMs) can augment the often limited empirical knowledge by generating domain-specific insights, though the quality of this generative knowledge is uncertain. Integrating LLMs with the collective wisdom of multiple stakeholders could enhance the quality and scale of knowledge, yet this integration might inadvertently raise privacy concerns for stakeholders. In response to this challenge, Federated Learning (FL) is harnessed to improve the knowledge base quality by cryptically leveraging other stakeholders' knowledge, where knowledge base is represented in Knowledge Graph (KG) form. Initially, a multi-field hyperbolic (MFH) graph embedding method vectorizes entities, furnishing mathematical representations in lieu of solely semantic meanings. The FL framework subsequently encrypted identifies and fuses common entities, whereby the updated entities' embedding can refine other private entities' embedding locally, thus enhancing the overall KG quality. Finally, the KG complement method refines and clarifies triplets to improve the overall quality of the KG. An experiment assesses the proposed approach across different industrial KGs, confirming its effectiveness as a viable solution for collaborative KG creation, all while maintaining data security.},
  keywords={Collaboration;Stakeholders;Data models;Knowledge engineering;Knowledge graphs;Federated learning;Big Data;Uncertainty;Tail;Mathematical models;Large Language Models;Knowledge Graph;Graph Embedding;Federated Learning;Industrial 4.0},
  doi={10.1109/TBDATA.2024.3522814},
  ISSN={2332-7790},
  month={},}@INPROCEEDINGS{10548271,
  author={Su, Yanqi and Liao, Dianshu and Xing, Zhenchang and Huang, Qing and Xie, Mulong and Lu, Qinghua and Xu, Xiwei},
  booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)}, 
  title={Enhancing Exploratory Testing by Large Language Model and Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={1197-1208},
  abstract={Exploratory testing leverages the tester's knowledge and creativity to design test cases for effectively uncovering system-level bugs from the end user's perspective. Researchers have worked on test scenario generation to support exploratory testing based on a system knowledge graph, enriched with scenario and oracle knowledge from bug reports. Nevertheless, the adoption of this approach is hindered by difficulties in handling bug reports of inconsistent quality and varied expression styles, along with the infeasibility of the generated test scenarios. To overcome these limitations, we utilize the superior natural language understanding (NLU) capabilities of Large Language Models (LLMs) to construct a System KG of User Tasks and Failures (SysKG-UTF). Leveraging the system and bug knowledge from the KG, along with the logical reasoning capabilities of LLMs, we generate test scenarios with high feasibility and coherence. Particularly, we design chain-of-thought (CoT) reasoning to extract human-like knowledge and logical reasoning from LLMs, simulating a developer's process of validating test scenario feasibility. Our evaluation shows that our approach significantly enhances the KG construction, particularly for bug reports with low quality. Furthermore, our approach generates test scenarios with high feasibility and coherence. The user study further proves the effectiveness of our generated test scenarios in supporting exploratory testing. Specifically, 8 participants find 36 bugs from 8 seed bugs in two hours using our test scenarios, a significant improvement over the 21 bugs found by the state-of-the-art baseline.},
  keywords={Computer bugs;Knowledge graphs;Coherence;Cognition;Natural language processing;Scenario generation;Task analysis;Exploratory testing;Knowledge graph;AI chain;Prompt engineering},
  doi={10.1145/3597503.3639157},
  ISSN={1558-1225},
  month={April},}@INPROCEEDINGS{10385754,
  author={Wang, Xun and Qu, Peng and Meng, Xiangyu and Yang, Qing and Qiao, Lian and Zhang, Chaogang and Xie, Xianjin},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={MulAxialGO: Multi-Modal Feature-Enhanced Deep Learning Model for Protein Function Prediction}, 
  year={2023},
  volume={},
  number={},
  pages={132-137},
  abstract={Predicting protein function from sequences through machine learning can improve the understanding of novel proteins and biological mechanisms. Existing methods mainly rely on one-dimensional convolution or natural language processing (NLP) techniques to extract features from sequences, but they suffer from limited predictive performance. To address this challenge, we propose MulAxialGO, a new method that leverages multi-modal feature fusion to improve prediction accuracy. MulAxialGO integrates the prior features of a large-scale pre-trained protein language model and the posterior features of dynamic embedding coding and sequence homology. In addition, MulAxialGO employs a comprehensive image feature encoder to extract features from sequences, providing a novel perspective for protein function prediction. MulAxialGO is tested on two benchmark datasets and achieves state-of-the-art results. On the 2016 dataset, MulAxialGO significantly outperforms DeepGOPlus, improving molecular function by 4.5 points, biological process by 2.4 points and cellular component by 1.6 points for the AUPR metric. Similarly, on the NetGO dataset, MulAxialGO outperforms the state-of-the-art NetGO2.0, improving Fmax by 1.1 points for biological process and 2.3 points for cellular component.},
  keywords={Proteins;Measurement;Deep learning;Protein engineering;Convolution;Biological processes;Predictive models;Protein function prediction;Sequence analysis;Bioinformatics;Attention mechanism;Deep learning},
  doi={10.1109/BIBM58861.2023.10385754},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10871140,
  author={Rani, Maneeha and Mishra, Bhupesh Kumar and Thakker, Dhavalkumar and Khan, Mohammad Nouman},
  booktitle={2024 18th International Conference on Open Source Systems and Technologies (ICOSST)}, 
  title={To Enhance Graph-Based Retrieval-Augmented Generation (RAG) with Robust Retrieval Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Large language models have demonstrated exceptional performance in multiple domains. However, practical deployment in the healthcare sector has distinctive challenges. These challenges include hallucination, inconsistency, explainability, reasoning, authenticity, and validity of information sources. Hallucinations in LLM often emerge due to unstructured and obsolete training data and the incompetence to upgrade the model data post-training. Retrieval-augmented generation (RAG) integration with LLM decision-making helps access real-time information from external resources. However, further improvements are needed to improve accurate response generation. A knowledge Graph is a structured data comprising nodes as entities and edges as relationships. When integrated with RAG, Knowledge Graph-based retrieval offers better contextu-ally relevant responses, traceability, and explainability of generated responses than RAG alone. This study proposes a novel knowledge graph-based RAG framework with a refined retrieval pipeline, robust chunking mechanism, and source traceability for enhanced diabetes-focused LLM. The retrieval pipeline integrates three robust retrieval strategies: keyword, graph, and vector. To ensure the authenticity of responses, a knowledge base focusing on diabetes is designed from validated sources. This verified knowledge base is preprocessed and converted to a knowledge graph to design A graph-based RAG pipeline. The empirical results demonstrate effective performance in diabetes-focused LLM, achieving a Rouge 1 score of 82.19%.},
  keywords={Accuracy;Retrieval augmented generation;Pipelines;Knowledge based systems;Training data;Knowledge graphs;Cognition;Vectors;Real-time systems;Diabetes;Retrieval-augmented generation;Large language model;Knowledge graph;Diabetes;Healthcare;Graph-based Retrieval-augmented generation},
  doi={10.1109/ICOSST64562.2024.10871140},
  ISSN={2770-8225},
  month={Dec},}@INPROCEEDINGS{9336541,
  author={Al-Obeidat, Feras and Adedugbe, Oluwasegun and Hani, Anoud Bani and Benkhelifa, Elhadj and Majdalawieh, Munir},
  booktitle={2020 Seventh International Conference on Social Networks Analysis, Management and Security (SNAMS)}, 
  title={Cone-KG: A Semantic Knowledge Graph with News Content and Social Context for Studying Covid-19 News Articles on Social Media}, 
  year={2020},
  volume={},
  number={},
  pages={1-7},
  abstract={Semantic knowledge graphs provide very significant benefits for structuring and analysing huge amounts of aggregated data across diverse heterogeneous sources. Beyond quick and efficient data query and analysis, they facilitate inference from data and generation of insights for several purposes. With the multi-faceted global challenges posed by the COVID-19 pandemic, this research focused on the use of a semantic knowledge graph to model, structure and store COVID-related news articles centrally and semantically towards knowledge discovery, knowledge acquisition and advanced data analytics for understanding varying metrics relating to the virus towards curbing its spread. The semantic knowledge graph provides a platform for researchers, data analysts and data scientists across societal sectors to investigate and recommend strategies towards addressing the challenges it poses to the global society.},
  keywords={COVID-19;Data analysis;Social networking (online);Pandemics;Semantics;Security;Viruses (medical);Knowledge Graphs;Semantic Graphs;Semantic Web;COVID-19 News;Social Media;Social Data Analysis},
  doi={10.1109/SNAMS52053.2020.9336541},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10687798,
  author={He, Jiabang and Liu, Jia and Wang, Lei and Li, Xiyao and Xu, Xing},
  booktitle={2024 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={MoCoSA: Momentum Contrast for Knowledge Graph Completion with Structure-Augmented Pre-trained Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Knowledge Graph Completion (KGC) aims to conduct reasoning on the facts within knowledge graphs and automatically infer missing links. Existing methods can mainly be categorized into structure-based or description-based. Structure-based methods effectively represent relational facts in knowledge graphs using entity embeddings and description-based methods leverage pre-trained language models (PLMs) to understand textual information. In this paper, we propose Momentum Contrast for knowledge graph completion with Structure-Augmented pre-trained language models (MoCoSA), which allows the PLM to perceive the structural information by the adaptable structure encoder. We proposed momentum hard negative and intra-relation negative sampling to improve learning efficiency. Experimental results demonstrate that our approach achieves state-of-the-art performance in terms of mean reciprocal rank (MRR), with improvements of 2.5% on WN18RR and 21% on OpenBG500.},
  keywords={Training;Adaptation models;Knowledge graphs;Contrastive learning;Predictive models;Cognition},
  doi={10.1109/ICME57554.2024.10687798},
  ISSN={1945-788X},
  month={July},}@ARTICLE{10771697,
  author={Gao, Yifu and Qiao, Linbo and Huang, Zhen and Kan, Zhigang and He, Yongquan and Li, Dongsheng},
  journal={IEEE Transactions on Audio, Speech and Language Processing}, 
  title={Unified Contextualized Knowledge Embedding Method for Static and Temporal Knowledge Graph}, 
  year={2025},
  volume={33},
  number={},
  pages={82-95},
  abstract={Recent years, there is a growing interest in knowledge graph embedding (KGE), which maps symbolic entities and relations into low-dimensional vector space to effectively represent structured data from the knowledge graph. In addition, the concept of temporal knowledge graph is proposed to document dynamically changing facts in the real world. Existing works attempt to incorporate temporal information into static KGE methods to accomplish temporal knowledge representations. However, existing static or temporal KGE approaches focus on the single query fact and ignore the query-relevant contextual information in the graph structure. This paper moves beyond the traditional way of scoring facts in distinct vector space and proposes a unified framework with pre-trained language models (PLM) to learn dynamic contextualized static/temporal knowledge graph embeddings, called CoS/TKGE. Given the query-specific subgraph, our model transforms it into an input sequence and uses the PLM to obtain the contextualized knowledge representations, which is flexible adaptive to the input graph contexts. We reformulate the link prediction task as a mask prediction problem to fine-tune the pre-trained language model. And the contrastive learning technique is employed to align dynamic contextual embeddings with static global embeddings. Experimental results on three widely used static and temporal KG datasets show the superiority of our model.},
  keywords={Knowledge graphs;Biological system modeling;Vectors;Contrastive learning;Semantics;Context modeling;Speech processing;Vegetation;Transforms;Tensors;Contrastive learning;knowledge graph embedding;pre-trained language model;representation learning},
  doi={10.1109/TASLP.2024.3507557},
  ISSN={2998-4173},
  month={},}@ARTICLE{10452779,
  author={Hu, Fan and Zhang, Weihong and Huang, Huazhen and Li, Wang and Li, Yang and Yin, Peng},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={A Transferability-Based Method for Evaluating the Protein Representation Learning}, 
  year={2024},
  volume={28},
  number={5},
  pages={3158-3166},
  abstract={Self-supervised pre-trained language models have recently risen as a powerful approach in learning protein representations, showing exceptional effectiveness in various biological tasks, such as drug discovery. Amidst the evolving trend in protein language model development, there is an observable shift towards employing large-scale multimodal and multitask models. However, the predominant reliance on empirical assessments using specific benchmark datasets for evaluating these models raises concerns about the comprehensiveness and efficiency of current evaluation methods. Addressing this gap, our study introduces a novel quantitative approach for estimating the performance of transferring multi-task pre-trained protein representations to downstream tasks. This transferability-based method is designed to quantify the similarities in latent space distributions between pre-trained features and those fine-tuned for downstream tasks. It encompasses a broad spectrum, covering multiple domains and a variety of heterogeneous tasks. To validate this method, we constructed a diverse set of protein-specific pre-training tasks. The resulting protein representations were then evaluated across several downstream biological tasks. Our experimental results demonstrate a robust correlation between the transferability scores obtained using our method and the actual transfer performance observed. This significant correlation highlights the potential of our method as a more comprehensive and efficient tool for evaluating protein representation learning.},
  keywords={Task analysis;Proteins;Protein engineering;Biological system modeling;Computational modeling;Predictive models;Biological information theory;Transferability;protein representation learning;optimal transport},
  doi={10.1109/JBHI.2024.3370680},
  ISSN={2168-2208},
  month={May},}@ARTICLE{10787401,
  author={Sukhwal, Prakash C. and Rajan, Vaibhav and Kankanhalli, Atreyi},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={A Joint LLM-KG System for Disease Q&A}, 
  year={2024},
  volume={},
  number={},
  pages={1-14},
  abstract={Medical question answer (QA) assistants respond to lay users' health-related queries by synthesizing information from multiple sources using natural language processing and related techniques. They can serve as vital tools to alleviate issues of misinformation, information overload, and complexity of medical language, thus addressing lay users' information needs while reducing the burden on healthcare professionals. QA systems, the engines of such assistants, have often used large language models (LLMs) or knowledge graphs (KG), though the approaches could be complementary. LLM-based QA systems excel at understanding complex questions and providing well-formed answers but are prone to factual mistakes. KG-based QA systems, which represent facts well, are mostly limited to answering short-answer questions with pre-created templates. While a few studies have used both LLM and KG for text-based QA, the approaches are still prone to incomplete or inaccurate answers. Extant QA systems also have limitations in terms of automation and performance. We address these challenges by designing a novel, automated disease QA system named Disease Guru - Long-Form Question Answer (DG-LFQA), which effectively utilizes both LLM and KG techniques through a joint reasoning approach to answer disease-related questions appropriate for lay users. Our evaluation of the system using a range of quality metrics demonstrates its efficacy over related baseline systems.},
  keywords={Diseases;Medical services;Cognition;Accuracy;Question answering (information retrieval);Measurement;Knowledge graphs;Fake news;Chatbots;Bioinformatics;Question answering;Healthcare;Large language model;Knowledge graph;Information extraction},
  doi={10.1109/JBHI.2024.3514659},
  ISSN={2168-2208},
  month={},}@INPROCEEDINGS{10822706,
  author={Zou, Haochen and Wang, Yongli},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={A Novel Knowledge Enhanced Large Language Model Augmented Framework for Medical Question Answering}, 
  year={2024},
  volume={},
  number={},
  pages={3034-3041},
  abstract={Leveraging domain-specific knowledge from pre-trained large language models and knowledge graphs for reasoning in the medical question answering task has emerged as a prominent research field. However, the accuracy of the inference results is restricted by multiple factors, including the quality of analyzed topic entities, the selected inference path in the knowledge graph, and the absence of mutual updating for embedding representations from large language models and knowledge graphs. In this paper, we propose a novel medical question answering framework based on the domain-specific large language model, aiming to enhance the quality of topic entities by implementing the retrieval augmentation technique. Inspired by the concept of chain-of-thought reasoning, we introduce a joint reasoning approach based on analyzed topic entities to facilitate the generation of accurate inference paths. Additionally, we design a unified embedding mechanism that combines representations from both the large language model and graph neural networks, incorporating a pooling operation for predicting answers to input questions. To the best of our knowledge, this work signifies the pioneering efforts in implementing the retrieval augmentation technique and the joint reasoning approach within the context of the medical question answering task. Experimental results on public benchmark datasets demonstrate that the introduced method outperforms state-of-the-art baseline approaches.},
  keywords={Accuracy;Large language models;Biological system modeling;Knowledge graphs;Computer architecture;Benchmark testing;Question answering (information retrieval);Cognition;Graph neural networks;Bioinformatics;Medical question answering;knowledge graph;large language model;graph neural networks},
  doi={10.1109/BIBM62325.2024.10822706},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10800230,
  author={Wang, Zihan and Huo, Hong and Xu, Renxin and Yang, Shijie and Fang, Tao},
  booktitle={2024 7th International Conference on Machine Learning and Natural Language Processing (MLNLP)}, 
  title={Ophthalmic Disease Zero-Shot Question Answering Through Knowledge Triple Augmented Language Model Prompting}, 
  year={2024},
  volume={},
  number={},
  pages={1-11},
  abstract={Ophthalmic disease zero-shot question answering aims to answer questions about ophthalmic disease in natural language without any model training. Although the pretrained large language models (LLMs) have stored quantities of general knowledge of the real-world and made zero-shot question answering come true, they ususally have insufficient ophthalmic knowledge, and fine-tuning LLMs to update their internal ophthalmic knowledge is very compute-intensive and expensive. By utilizing the specialized knowledge in the self-built ophthalmic disease knowledge graph (ODKG) to augment LLMs, the proposed Knowledge Triple Augmented language model Prompting (KTAP) has well implemented ophthalmic disease zero-shot question answering. Our KTAP is primarily divided into three modules: entity linking, knowledge retriever and knowledge injection. The entity linking module extracts ophthalmic disease entities from the question by a prompt-based zero-shot entity linking method, and a subgraph related to these ophthalmic disease entities is obtained from ODKG. The knowledge retriever module consists of two submodules, the text embedding and the similarity matching. The former is responsible for calculating embeddings of both each triple in the subgraph and entities in the question, while the latter calculates the semantic similarity between each triple embedding and the question embedding to obtain the top K triples most relevant to the question. In the knowledge injection module, a prompt template based on the top K triples and the question is designed to guide LLMs to perform ophthalmic disease zero-shot question answering. The experiments have shown that in the zero-shot question answering task of ophthalmic disease, the proposed KTAP has outperformed other language model prompting baselines by an average of 16% in the precision of generated answers.},
  keywords={Training;Computational modeling;Large language models;Semantics;Knowledge graphs;Medical services;Machine learning;Chatbots;Question answering (information retrieval);Diseases;Zero-shot Question Answering;Knowledge Graph;Language Model Prompting;Ophthalmic Disease},
  doi={10.1109/MLNLP63328.2024.10800230},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10845527,
  author={Long, Xinyao and Qiu, Houjie and Liu, Qiang and Su, Xin and Li, Chengqing},
  booktitle={2024 12th International Conference on Information Systems and Computing Technology (ISCTech)}, 
  title={Few-Shot Event Extraction with a Dialogue-based Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Event extraction is critical in various fields, including knowledge graph construction, public opinion monitoring, and situational awareness. Existing methods rely on supervised learning, which depends heavily on the scale and quality of the dataset, while the model’s generalization ability remains weak. As a result, their application in real-world scenarios is significantly constrained. To address this issue, we propose a few-shot event extraction method that harnesses large language models’ reading comprehension and event extraction capabilities. Our approach decomposes the task into multiple stages within a dialog-based framework. Experimental results show that our method outperforms most few-shot techniques, achieving a 4.1% performance improvement over ChatIE.},
  keywords={Computers;Computational modeling;Large language models;Supervised learning;Knowledge graphs;Data mining;Monitoring;Information systems;Few-Shot Event Extraction;Large Language Model},
  doi={10.1109/ISCTech63666.2024.10845527},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10447637,
  author={Santiesteban, Sergio Sánchez and Atito, Sara and Awais, Muhammad and Song, Yi-Zhe and Kittler, Josef},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Improved Image Captioning Via Knowledge Graph-Augmented Models}, 
  year={2024},
  volume={},
  number={},
  pages={4290-4294},
  abstract={Multimodal foundation models, pre-trained on large-scale data, effectively capture vast amounts of factual and commonsense knowledge. However, these models store all their knowledge within their parameters, requiring increasingly larger models and training data to capture more knowledge. To address this limitation and achieve a more scalable and modular integration of knowledge, we propose a novel knowledge graph-augmented multimodal model. This approach enables a base multimodal model to access pertinent information from an external knowledge graph. Our methodology leverages existing general domain knowledge to facilitate vision-language pre-training using paired images and text descriptions. We conduct comprehensive evaluations demonstrating that our model outperforms state-of-the-art models and yields comparable results to much larger models trained on more extensive datasets. Notably, our model reached a 145 Cider score on MS COCO Captions using only 2.9 million samples, outperforming a 1.4B parameter model by 1.7% despite having 11 times fewer parameters.},
  keywords={Training;Semantics;Training data;Knowledge graphs;Signal processing;Image representation;Transformers;image captioning;external knowledge;knowledge graphs},
  doi={10.1109/ICASSP48485.2024.10447637},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{9047450,
  author={Wang, Hongzhong and Guo, Kun and Liu, Zhanghui},
  booktitle={2019 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)}, 
  title={Mixed Word Embedding Method Based on Knowledge Graph Augment for Text Classification}, 
  year={2019},
  volume={},
  number={},
  pages={1618-1623},
  abstract={This paper presents a self-training word embedding text classification model based on knowledge graph expansion for text classification. Current mixed word embedding methods are overly dependent on the Fasttext pre-training model and here is still a problem of missing words with rich semantic information are not mapped. First, we propose a method for extracting missing nouns based on shape near word filtering. Second, we design a self-training word embedding method based on knowledge graph that mixes with pre-training word embedding to obtain a high-quality mixed word vector with rich semantics and rich semantics. Third, we designed a GRU model based on improved mixed word embedding to improve the quality of text classification. Experiments conducted on multiple text classification datasets demonstrate that our methods can effectively improve the text classification accuracy.},
  keywords={Text categorization;Context modeling;Feature extraction;Training;Task analysis;Predictive models;Text Classification;Deep Learning;Mixed Word embedding;Self-trained Word embedding},
  doi={10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00238},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9207515,
  author={Pradhan, Abhishek and Todi, Ketan Kumar and Selvarasu, Anbarasan and Sanyal, Atish},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Knowledge Graph Generation with Deep Active Learning}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={Creating a knowledge graph automatically from raw unstructured text has always been a job of domain expert which takes months to curate and refine. In this paper, we propose a domain-independent semi-automatic knowledge graph learning system that can be trained with less amount of data, to identify entities and relations from a large text corpus. The system performs the following tasks to extract knowledge graph from the text: (i) Named Entity Recognition (NER), and (ii) Relation Identification (Open Relation Extraction (OpenRE) and Classification). The system uses deep active learning to calculate confidence scores using maximum normalized log-probability on each prediction for both NER, and relation identification. We experimented with both LSTM and transformer based models for NER and relation identification tasks. We achieved around 88% F1 score for the NER task on OntoNotes-5.0 English data set with 40% training data set and above 83% F1 score for relation identification on TACRED dataset. The OpenRE and relation classification systems were trained on domain-specific datasets. To the best of our knowledge, we are the first to introduce a knowledge graph generation learning system with deep active learning.},
  keywords={Task analysis;Training;Bit error rate;Feature extraction;Information retrieval;Computer architecture;Learning systems;Active Learning;Named Entity Recognition;Open Relation Extraction;Knowledge Graph;Open Information Extraction},
  doi={10.1109/IJCNN48605.2020.9207515},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{10606805,
  author={Chen, Weilin and Qu, Qiang and Guo, Huifeng and Li, Haibin and Mao, Zehui},
  booktitle={2024 IEEE 13th Data Driven Control and Learning Systems Conference (DDCLS)}, 
  title={A Fault Diagnosis Method for Key Equipment of Bearer Network Transmission Based on Knowledge Graph and Multi-modal Neural Network}, 
  year={2024},
  volume={},
  number={},
  pages={2277-2282},
  abstract={Aiming to address the challenge of incorporating multiple fault phenomena and utilizing accompanying information such as negation and time adverbials in the process of diagnosing faults in key equipment for bearer network transmission, this study proposes a fault diagnosis method based on knowledge graph and multi-modal neural network. By constructing a knowledge graph from a fault diagnosis knowledge base, this method retrieves candidate fault root causes based on the fault phenomena and extracts subgraphs along the reasoning paths from the fault phenomena to the candidate fault root causes in the knowledge graph. Subsequently, leveraging both the reasoning paths within the subgraphs and the fault maintenance texts, the multi-modal neural network calculates the confidence levels of the candidate fault root causes, selecting the fault root cause with the highest confidence as the reasoning conclusion. Finally, the proposed method constructs a knowledge graph based on the fault diagnosis knowledge base of key equipment for bearer network transmission provided by a domestic enterprise, and validates the efficacy of the approach using fault data employed to train the multi-modal neural network. Results demonstrate that this method effectively leverages multiple fault phenomena and their accompanying information in fault maintenance texts for accurate fault diagnosis, yielding favorable performance.},
  keywords={Fault diagnosis;Learning systems;Neural networks;Knowledge based systems;Knowledge graphs;Predictive models;Control systems;Fault Diagnosis;Knowledge Graph;Key Equipment of Bearer Network Transmission;Multi-modal Neural Network},
  doi={10.1109/DDCLS61622.2024.10606805},
  ISSN={2767-9861},
  month={May},}@INPROCEEDINGS{10309699,
  author={Lee, Chang-Shing and Wang, Mei-Hui and Chen, Chih-Yu and Reformat, Marek and Nojima, Yusuke and Kubota, Naoyuki},
  booktitle={2023 IEEE International Conference on Fuzzy Systems (FUZZ)}, 
  title={Knowledge Graph-Based Genetic Fuzzy Agent for Human Intelligence and Machine Co-Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper proposes a novel approach for evaluating the co-learning performance of human intelligence (HI) and machine intelligence (MI) using a Knowledge Graph-based genetic fuzzy agent. The agent utilizes a Knowledge Graph structure to represent a specific knowledge domain related to human learning and employs a genetic fuzzy learning mechanism to construct a personalized learning model. Human learners can engage in co-learning with machines using state-of-the-art AI tools such as the Meta AI S2ST Taiwanese-English language model and the OpenAI ChatGPT text model. The proposed approach was evaluated using human learning data from an undergraduate computer science course and a series of Taiwanese and English language translation experience activities. The experimental results indicate that the proposed approach can effectively enhance the co-learning process for both human and machine learners.},
  keywords={Computer science;Learning systems;Human intelligence;Computational modeling;Speech recognition;Genetics;Chatbots;Knowledge Graph;Genetic Algorithm;Fuzzy Agent;MetaAI S2ST;Human Intelligence;OpenAI ChatGPT},
  doi={10.1109/FUZZ52849.2023.10309699},
  ISSN={1558-4739},
  month={Aug},}@INPROCEEDINGS{8010709,
  author={Silva, Nuno and Mira da Silva, Miguel and Sousa, Pedro Manuel Moreira Vaz Antunes de},
  booktitle={2017 IEEE 19th Conference on Business Informatics (CBI)}, 
  title={Modelling the Evolution of Enterprise Architectures Using Ontologies}, 
  year={2017},
  volume={01},
  number={},
  pages={79-88},
  abstract={Enterprise architecture supports enterprise transformation through model-based holistic descriptions that capture and share the knowledge, concepts, and relationships representing the enterprise. Although enterprise architecture models portray the gap between the as-is and to-be enterprise, they disregard the underlying aspects that together enact enterprise transformation. This paper proposes an ontology-based approach for the specification of the concepts and relationships used to express and analyze the evolution of enterprise architecture models.},
  keywords={Ontologies;Unified modeling language;Computational modeling;Computer architecture;Analytical models;OWL;Business;enterprise architecture;enterprise transformation;model evolution;ontology},
  doi={10.1109/CBI.2017.17},
  ISSN={2378-1971},
  month={July},}@INPROCEEDINGS{10043275,
  author={Mendil, Ismail and Rivière, Peter and Ait-Ameur, Yamine and Singh, Neeraj Kumar and Méry, Dominique and Palanque, Philippe},
  booktitle={2022 29th Asia-Pacific Software Engineering Conference (APSEC)}, 
  title={Non-Intrusive Annotation-Based Domain-Specific Analysis to Certify Event-B Models Behaviours}, 
  year={2022},
  volume={},
  number={},
  pages={129-138},
  abstract={System engineering advocates a thorough under-standing of the engineering domain or certification standards (aeronautics, railway, medical, etc.) associated to the system under design. In this context, engineering domain knowledge plays a predominant role in system design and/or certification. Furthermore, it is a prerequisite to achieve the effectiveness and performance of the designed system. This article proposes a formal method for describing and setting up domain-specific behavioural analyses. It defines a formal verification technique for dynamic properties entailed by engineering domain knowledge where Event-B formal models are annotated and analysed in a non-intrusive way, i.e. without destructive alteration. This method is based on the formalisation of behavioural properties analyses relying on domain knowledge as an ontology on the one hand and a meta-theory for Event-B on the other hand. The proposed method is illustrated using a critical interactive system.},
  keywords={Knowledge engineering;Analytical models;Interactive systems;Ontologies;Rail transportation;Proposals;Certification;Domain knowledge;formal methods;Event-B;refinement;proof;ontology;behavioural analyses},
  doi={10.1109/APSEC57359.2022.00025},
  ISSN={2640-0715},
  month={Dec},}@INPROCEEDINGS{10800180,
  author={Wang, Changlong and Sang, Xiujuan and Wang, Xijie and Gao, Yuan and Liu, Yi},
  booktitle={2024 7th International Conference on Machine Learning and Natural Language Processing (MLNLP)}, 
  title={Research on Knowledge Graph Extraction Methods for Chinese STEM Curriculum}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={STEM education, as an innovative teaching model, has gained widespread attention in recent years. However, the lack of relevant textbooks and learning resources has made its implementation challenging. Developing interdisciplinary knowledge graphs tailored for STEM education has become an urgent issue. To address this, a knowledge extraction framework named Llms4edu is proposed, which utilizes a series of effective prompts to guide large language models in knowledge extraction. Specifically, the knowledge extraction task is transformed into multiple rounds of question-and-answer interactions with the LLM, gradually identifying entity-relation triplets from subject data. Through experiments, an F1-score of 89.4% was achieved on the named entity recognition task in the chemistry subject, and an F1-score of 66.7% on the relation extraction task. Finally, a subject ontology model was built for subject text, and a subject data set was constructed using Llms4edu, which includes three subjects of junior high school mathematics, physics, and chemistry, a total of 2,511 entities, 2,010 relationship triples, and cross-disciplinary knowledge is linked to construct a cross-disciplinary knowledge graph.},
  keywords={Knowledge engineering;Training;Chemistry;Annotations;Large language models;Knowledge graphs;Named entity recognition;Ontologies;Data mining;Physics;interdisciplinary knowledge graph;large language model;prompt engineering},
  doi={10.1109/MLNLP63328.2024.10800180},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9742010,
  author={Yang, Wansheng and Deng, Fei and Ma, Siyou and Wu, Linbo and Sun, Zhe and Hu, Chi},
  booktitle={2021 IEEE 21st International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
  title={Test Case Reuse Based on Software Testing Knowledge Graph and Collaborative Filtering Recommendation Algorithm}, 
  year={2021},
  volume={},
  number={},
  pages={67-76},
  abstract={As an important role of software test, the reuse of test cases is essential in terms of finding software defects and locating the causes of them. However, the existing related approaches are insufficient to establish an internal relationship between test cases and defects and their abilities to find or diagnose errors are limited. In this paper, an ontology model based on the software testing process is applied to establish a software testing knowledge graph, which serves as the foundation to build an recommendation system. Specifically, the recommendation system takes the functions of software under test as the “user”, and the defect-occurrence-chain which establishes the correlation between test cases and defects in the knowledge graph as the “item”. Both of them provide the evidence to build collaborative filtering recommendation algorithm based on the user-item scoring matrix. It aims to assist testers in recommending reusable test cases to identify software errors effectively. Against this background, the BERT+Bi-LSTM-CRF model is selected to extract the latent test requirements of the software under test, and an overt variable factorization model is built so as to iteratively optimize the user-item scoring matrix. Further, an empirical study has been conducted, and it is found that the recommended test cases can significantly help testers find software defects faster in a more efficient way, and locate defects more accurately.},
  keywords={Software testing;Collaborative filtering;Software algorithms;Semantics;Software quality;Ontologies;Software;software testing knowledge graph;collaborative filtering;BERT+Bi-LSTM-CRF;defect-occurrence-chain;overt variable factorization model},
  doi={10.1109/QRS-C55045.2021.00020},
  ISSN={2693-9371},
  month={Dec},}@ARTICLE{10734210,
  author={Xu, Jun and Zhang, Hao and Zhang, Haijing and Lu, Jiawei and Xiao, Gang},
  journal={IEEE Access}, 
  title={ChatTf: A Knowledge Graph-Enhanced Intelligent Q&A System for Mitigating Factuality Hallucinations in Traditional Folklore}, 
  year={2024},
  volume={12},
  number={},
  pages={162638-162650},
  abstract={Large language models are rapidly advancing the field of artificial intelligence, with current research focusing primarily on traditional natural language understanding tasks, such as question answering and information extraction. However, in knowledge-intensive domains, such as intangible cultural heritage, hallucination problems due to insufficient domain knowledge persist. To address this, we present ChatTf, a knowledge graph-enhanced intelligent Q&A system, exemplified by Chinese traditional folklore, aimed at reducing factuality hallucinations in this domain. Specifically, we constructed the Traditional Folklore Ontology (TFOnto) and proposed the Zero-shot Traditional Folklore Triplet Extraction (ZFTE) framework. Driven by TFOnto, ZFTE builds a Traditional Folklore Knowledge Graph (TFKG). We then proposed a dual-stage Retrieval-Augmented Generation framework (TFKG-RAG) based on TFKG to provide traditional folklore knowledge to large language models, mitigating factuality hallucinations in folklore Q&A tasks. In the experimental phase, ChatTf achieved an accuracy of 96.7% on a self-built TFCQD test set, outperforming several state-of-the-art baseline methods. This demonstrates the accuracy and reliability of folklore domain question answering.},
  keywords={Ontologies;Knowledge graphs;Cultural differences;Large language models;Knowledge engineering;Training;Cognition;Accuracy;Semantics;Reliability;Knowledge graph;large language model;question answering;retrieval-augmented generation;traditional folklore},
  doi={10.1109/ACCESS.2024.3485877},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10825785,
  author={Mansur, Elijah and Chen, Johnson and Raza, Muhammad Anas and Wardat, Mohammad},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={RAGFix: Enhancing LLM Code Repair Using RAG and Stack Overflow Posts}, 
  year={2024},
  volume={},
  number={},
  pages={7491-7496},
  abstract={Identifying, localizing, and resolving bugs in software engineering is challenging and costly. Approaches to resolve software bugs range from Large Language Model (LLM) code analysis and repair, and automated code repair technology that aims to alleviate the technical burden of difficult to solve bugs. We propose RAGFix, which enhances LLM’s capabilities for bug localization and code repair using Retrieval Augmented Generation (RAG) based on dynamically collected Stack Overflow posts. These posts are searchable via a Question and Answer Knowledge Graph (KGQA). We evaluate our method on the HumanEvalFix benchmark for Python using relevant closed and open-source models. Our approach facilitates error resolution in Python coding problems by creating a searchable, embedded knowledge graph representation of bug and solution information from Stack Overflow, interlinking bugs, and solutions through semi-supervised graph construction methods. We use cosine similarity on embeddings based on LLM-synthesized summaries and algorithmic features describing the coding problem and potential solution to find relevant results that improve LLM in-context performance. Our results indicate that our system enhances small open-source models’ ability to effectively repair code, particularly where these models have less parametric knowledge about relevant coding problems and can leverage nonparametric knowledge to provide accurate, actionable fixes.},
  keywords={Codes;Computer bugs;Retrieval augmented generation;Knowledge graphs;Maintenance engineering;Search problems;Encoding;Software;Python;Software engineering;Retrieval-augmented generation;Large Language Models;knowledge graph;Bug detection;Code Repair},
  doi={10.1109/BigData62323.2024.10825785},
  ISSN={2573-2978},
  month={Dec},}@ARTICLE{9950327,
  author={Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Improving Protein Function Prediction by Adaptively Fusing Information From Protein Sequences and Biomedical Literature}, 
  year={2023},
  volume={27},
  number={2},
  pages={1140-1148},
  abstract={Proteins are the main undertakers of life activities, and accurately predicting their biological functions can help human better understand life mechanism and promote the development of themselves. With the rapid development of high-throughput technologies, an abundance of proteins are discovered. However, the gap between proteins and function annotations is still huge. To accelerate the process of protein function prediction, some computational methods taking advantage of multiple data have been proposed. Among these methods, the deep-learning-based methods are currently the most popular for their capability of learning information automatically from raw data. However, due to the diversity and scale difference between data, it is challenging for existing deep learning methods to capture related information from different data effectively. In this paper, we introduce a deep learning method that can adaptively learn information from protein sequences and biomedical literature, namely DeepAF. DeepAF first extracts the two kinds of information by using different extractors, which are built based on pre-trained language models and can capture rudimentary biological knowledge. Then, to integrate those information, it performs an adaptive fusion layer based on a Cross-attention mechanism that considers the knowledge of mutual interactions between two information. Finally, based on the mixed information, DeepAF utilizes logistic regression to obtain prediction scores. The experimental results on the datasets of two species (i.e., Human and Yeast) show that DeepAF outperforms other state-of-the-art approaches.},
  keywords={Proteins;Protein engineering;Data mining;Biological system modeling;Amino acids;Semantics;Predictive models;Protein function prediction;deep learning;multiple data;pre-trained language models;cross-attention mechanism},
  doi={10.1109/JBHI.2022.3221988},
  ISSN={2168-2208},
  month={Feb},}@INPROCEEDINGS{9574313,
  author={Wang, Xiaochen and Jia, Qinlin and Du, Hui},
  booktitle={2021 IEEE International Conference on Advances in Electrical Engineering and Computer Applications (AEECA)}, 
  title={An Intelligent Question Answering Method Combining Knowledge Graph and Corpus}, 
  year={2021},
  volume={},
  number={},
  pages={1050-1054},
  abstract={Traditional question answering methods are basically divided into the retrieval of KG (Knowledge Graph) and the QA (Question Answering) corpus. The purpose of this paper is to combine both types of methods to devise a quick and comprehensive QA method. In terms of question identifying, a method of question classification label and answer template are defined. The data used to fabricate the KG comes from the books. During processing the text data, Bi-LSTM-CRF combined with the BIO text labeling method is used to carry out named entity recognition. The relationships and attributes of entities are identified using parameter sharing through the previously defined tags and dictionaries. The KG will be stored and retrieved in the graph database. Moreover, the QA corpus comes from three sources: discussion issues, tutor-student interaction, and crowdsourcing platform. It focuses on solving multi-attribute multi-relationship problems that are not included in the KG. QA corpus will output the highest-ranking answer by comparing the semantic similarity of the questions. The final answer output is given both from KG and QA corpus.},
  keywords={Measurement;Electrical engineering;Crowdsourcing;Dictionaries;Text recognition;Databases;Conferences;KG;QA Corpus;Bi-LSTM-CRF;BIO;Text-CNN;Semantic Similarity},
  doi={10.1109/AEECA52519.2021.9574313},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9194542,
  author={Lv, Jianghai and Du, Junping and Zhou, Nan and Xue, Zhe},
  booktitle={2020 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={BERT-BIGRU-CRF: A Novel Entity Relationship Extraction Model}, 
  year={2020},
  volume={},
  number={},
  pages={157-164},
  abstract={Entity name recognition and entity relationship extraction are the most critical foundation for building knowledge graph, and it is also the basic task of NPL. The main purpose of entity relationship extraction is to extract the semantic relationship between the pairs of marked entities in the sentence, that is, to determine the relationship categories between entity pairs in unstructured text based on entity identification, and to form structured data for storage and retrieval. This paper proposes a BERT-BIGRU-CRF entity relationship extraction method, which effectively changes the relationship between the pre-training generated word vector and the downstream specific NLP task, and gradually moves the downstream specific NLP task to the pre-training generated word vector. Our method achieves better performance of relationship extraction and entity name recognition, which helps to construct the knowledge graph more accurately.},
  keywords={Bit error rate;Feature extraction;Task analysis;Predictive models;Semantics;Communications technology;Training;Pre-training deep learning;Transformer encoder;Sentence-level representation;Masked LM},
  doi={10.1109/ICBK50248.2020.00032},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10747006,
  author={Almiani, Muder and Abu-Salih, Bilal and Alotaibi, Salihah and Aljaafari, Mohammed and Azar, Dima and Alrawashdeh, Tawfiq and Tahat, Yasean},
  booktitle={2024 Fifth International Conference on Intelligent Data Science Technologies and Applications (IDSTA)}, 
  title={Knowledge Graph Embedding for Detecting Brand Advocates in Online Social Networks}, 
  year={2024},
  volume={},
  number={},
  pages={137-144},
  abstract={In the dynamic and ever-changing environment of social media, customer advocacy forms part of the key success factors that need to be monitored by brand teams. Focusing on the social media engagement aspect, this paper outlines a conceptual model that guides the evaluation of customer advocacy.The approach involves developing a Knowledge Graph (KG) that captures the complex connections among customers, brands, and even products. The KG is constructed based on state-of-the-art methods of entity and relationship Mining from social media text, backed by a highly layered structure with XLNet, BiLSTM, and CRF ensembles. The performance of the KG embedding models namely TransE, DistMult, ComplEx, HolE, and RotatE models are assessed with parameters such as Hits@k, Median Rank (MR), Mean Reciprocal Rank (MRR), and Mean Rank. The performance of RotatE framework to predict relationships within the KG and its accuracy have been shown by the experimental results. In this respect, the results reveal the importance of KG-based perspectives for investigating and resolving issues related to customer advocacy, as well as for developing relevant marketing strategies in contemporary environment..},
  keywords={Sentiment analysis;Systematics;Social networking (online);Scalability;Soft sensors;Knowledge graphs;Predictive models;Web sites;Multimedia communication;Monitoring;Social Media Marketing;Online Brand Advocates Detection;Knowledge Graphs;KG Embedding;Social Media Analysis},
  doi={10.1109/IDSTA62194.2024.10747006},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{8536159,
  author={Prince Sales, Tiago and Almeida, João Paulo A. and Santini, Sebastiano and Baião, Fernanda and Guizzardi, Giancarlo},
  booktitle={2018 IEEE 22nd International Enterprise Distributed Object Computing Conference (EDOC)}, 
  title={Ontological Analysis and Redesign of Risk Modeling in ArchiMate}, 
  year={2018},
  volume={},
  number={},
  pages={154-163},
  abstract={Risk analysis is a complex and critical activity in various contexts, ranging from strategic planning to IT systems operation. Given its complexity, several Enterprise Architecture (EA) frameworks and modeling languages have been developed to help analysts in representing and analyzing risks. Yet, the notion of risk remains overloaded and conceptually unclear in most of them. In this paper, we investigate the real-world semantics underlying risk-related constructs in one of such approaches, namely ArchiMate's Risk and Security Overlay (RSO). We perform this investigation by means of ontological analysis to reveal semantic limitations in the overlay, such as ambiguity and missing constructs. Building on the results of this analysis, we propose a well-founded redesign of the risk modeling aspects of the RSO.},
  keywords={Security;Organizations;Ontologies;Risk management;Analytical models;Semantics;Risk Modeling;Enterprise Architecture;ArchiMate;Ontological Analysis;Unified Foundational Ontology},
  doi={10.1109/EDOC.2018.00028},
  ISSN={2325-6362},
  month={Oct},}@INPROCEEDINGS{10450693,
  author={Li, Dongze and Qu, Hanbing and Wang, Jiaqiang},
  booktitle={2023 China Automation Congress (CAC)}, 
  title={A Survey on Knowledge Graph-Based Recommender Systems}, 
  year={2023},
  volume={},
  number={},
  pages={2925-2930},
  abstract={Recommender systems have emerged as indispensable tools for information filtering, and the integration of knowledge graphs for auxiliary information is becoming an increasingly popular research topic. This paper reviews recent studies, discussing the current state and practical applications of knowledge graph-based recommender systems. We summarize the strengths and weaknesses of various knowledge graph-based recommendation methods, noting that these systems significantly enhance performance in areas like accuracy, diversity, interpretability, and novelty. Furthermore, the trend of combining different knowledge graph-based methods underscores the mainstream evolution of recommender systems, warranting future exploration. We finish with an analysis of current challenges and a forward-looking perspective on future advances. This review aims to assist the reader in understanding and navigating this research field.},
  keywords={Surveys;Automation;Reviews;Navigation;Diversity reception;Knowledge graphs;Market research;Recommender System;Knowledge Graph;Contrastive Learning;Reinforcement Learning},
  doi={10.1109/CAC59555.2023.10450693},
  ISSN={2688-0938},
  month={Nov},}@INPROCEEDINGS{9581237,
  author={Liu, Xinlong and Xu, Li-Qun},
  booktitle={2021 IEEE International Conference on Digital Health (ICDH)}, 
  title={Knowledge Graph Building from Real-world Multisource “Dirty” Clinical Electronic Medical Records for Intelligent Consultation Applications}, 
  year={2021},
  volume={},
  number={},
  pages={260-265},
  abstract={Intelligent clinical consultation is a diagnostic support system that inferred the likely diseases from the patient's chief complaints as per the established relationship between symptoms and diseases. The key here is to learn and build automatically the general “symptom-disease” medical knowledge graph (MKG) from real-world clinical data. So, the quality of clinical data (chiefly electronic medical records - EMRs) directly affects the quality of the MKG, which in turn determines the quality of the consultation results. The regional public health information platform gathered a large number of front-pages of EMRs' from hospitals of all tiers across the region. The fact that the health IT systems used by hospitals are often sourced from different vendors, and each may have its own data standards and data quality control criteria, would invariably lead to apparent difference in the quality of EMRs collected. This is even so, considering the gaps in knowledge and skills between clinicians at different qualification levels. By detailed analysis of one such collection we found that the two most prominent problems are the inconsistency in diagnosis results and the mismatch between the diagnosis results and the chief complaints and the current illness history. In order to ensure the quality and effectiveness in building a knowledge graph from these real-world data, this paper proposed a “dirty” data cleaning framework including diagnostic results normalization and semantic similarity matching. The symptom-disease knowledge graph constructed from the cleaned data has been applied and verified in the intelligent consultation system.},
  keywords={Hospitals;Data integrity;Semantics;Electronic medical records;Task analysis;Public healthcare;Medical diagnostic imaging;Knowledge graph;data cleaning;diagnostic results normalization;semantic similarity},
  doi={10.1109/ICDH52753.2021.00049},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9313350,
  author={Sastre, Javier and Zaman, Faisal and Duggan, Noirin and McDonagh, Caitlin and Walsh, Paul},
  booktitle={2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={A Deep Learning Knowledge Graph Approach to Drug Labelling}, 
  year={2020},
  volume={},
  number={},
  pages={2513-2521},
  abstract={Ensuring the accuracy and completeness of drug labels is a labour-intensive and potentially error prone process, as labels contain unstructured text that is not suitable for automated processing. To address this, we have developed a novel deep learning system that uses a bidirectional LSTM model to extract and structure drug information in a knowledge graph-based embedding space. This allows us to evaluate drug label consistency with ground truth knowledge, along with the ability to predict additional drug interactions. Annotated sentences from 7,117 drug labels sentences were used to train the LSTM model and 1,779 were used to test it. The drug entity extraction system was able to correctly detect relevant entities and relations with a F1 score of 91% and 81% respectively. The knowledge graph embedding model was able to identify inconsistent facts with ground truth data in 76% of the cases tested. This demonstrates that there is potential in building a natural language processing system that automatically extracts drug interaction information from drug labels and embeds this structured data into a knowledge graph embedding space to help evaluate drug label accuracy. We note that the accuracy of the system needs to be improved significantly before it can fully automate drug labeling related tasks. Rather such a system could provide best utility within a human-in-the-loop approach, where operators augment model training and evaluation.},
  keywords={Drugs;Data mining;Deep learning;Information retrieval;Feature extraction;Task analysis;Labeling;drug labels;deep learning;knowledge graph embeddings;LSTM},
  doi={10.1109/BIBM49941.2020.9313350},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10796042,
  author={Ji, Lintian and Du, Shuxin and Qiu, Yi and Xu, Hongyi and Guo, Xiaorui},
  booktitle={2024 IEEE 6th International Conference on Power, Intelligent Computing and Systems (ICPICS)}, 
  title={Constructing a Medical Domain Functional Knowledge Graph with Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={491-496},
  abstract={The integration of medicine and artificial intelligence is creating novel opportunities to gather, analyze, and generate groundbreaking medical insights from the vast expanse of medical literature. Although these advancements hold great promise, difficulties including manual annotation and precise data retrieval, and transparency persist. Advanced natural language processing (NLP) techniques and large language models (LLMs) have shown potential in addressing these issues. This paper introduces the Medical Knowledge Graph (MedKG), a comprehensive and multidisciplinary medical knowledge graph. By utilizing advanced NLP algorithms, MedKG extracts millions of entities and converts them into triples from a corpus of superior medical research papers published over the last decade. It categorizes unstructured medical information into nine distinct tags, including disease names, treatments, diagnoses, symptoms, genetics, cellular structures of diseases, patient data, medical procedures, and treatment applications, while seamlessly integrating the digital object identifier (DOI) of each paper. As the latest structured database of medical knowledge, MedKG serves as a powerful tool for accelerating medical research and lays the groundwork for constructing a more extensive medical knowledge graph using full-text medical articles. Moreover, our research establishes a foundation for practical knowledge management systems based on text mining, applicable not only to complex medical systems but also to other professional domains.},
  keywords={Text mining;Annotations;Large language models;Pipelines;Knowledge graphs;Prediction algorithms;Natural language processing;Object recognition;Medical diagnostic imaging;Diseases;Medical engineering;NLP;Large language model;Knowledge graph;Database},
  doi={10.1109/ICPICS62053.2024.10796042},
  ISSN={2834-8567},
  month={July},}@INPROCEEDINGS{9945152,
  author={Zhu, Yangfu and Guan, Zhanming and Wei, Siqi and Wu, Bin},
  booktitle={2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={PerKG: A Personality Knowledge Graph for Personality Analysis}, 
  year={2022},
  volume={},
  number={},
  pages={580-585},
  abstract={With the blossoming of online social networks (OSN), personality analysis based on OSN texts has gained much research attention in recent years. The previous methods mainly focus on human-designed features extracted through psychological dictionaries or semantic features extracted through language models. However, the shallow statistics features can not fully convey the personality information and the language models can not capture enough psychological background knowledge. Besides, the lack of large labeled datasets has been a serious obstacle impending further research. To tackle these problems, we propose a personality analysis model, namely PerKG, which combines personality knowledge graph and heterogeneous graph representation learning to exploit external knowledge from psycholinguistics and learn the group-level information to predict users’ personalities accurately. Specifically, we construct a personality knowledge graph based on existing psycholinguistics knowledge. And then, for each user, we align the user information with the knowledge graph to obtain the personality heterogeneous graph. Finally, the personality vector of each entity node is learned for prediction by designing a walk strategy on the personality heterogeneous graph. Detailed experimentation shows that our proposed PerKG architecture can effectively improve the performance and alleviate the label sparsity problem of personality analysis.},
  keywords={Knowledge engineering;Representation learning;Dictionaries;Social networking (online);Semantics;Psychology;Predictive models;knowledge graph;network representation learning;personality analysis;online social network},
  doi={10.1109/SMC53654.2022.9945152},
  ISSN={2577-1655},
  month={Oct},}@INPROCEEDINGS{10340611,
  author={Labbé, Thomas and Castel, Pierre and Sanner, Jean-Michel and Saleh, Majd},
  booktitle={2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={ChatGPT for phenotypes extraction: one model to rule them all?}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Information Extraction (IE) is a core task in Natural Language Processing (NLP) where the objective is to identify factual knowledge in textual documents (often unstructured), and feed downstream use cases with the resulting output. In genomic medicine for instance, being able to extract the most precise list of phenotypes associated to a patient allows to improve genetic disease diagnostic, which represents a vital step in the modern deep phenotyping approach. As most of the phenotypic information lies in clinical reports, the challenge is to build an IE pipeline to automatically recognize phenotype concepts from free-text notes. A new machine learning paradigm around large language models (LLM) has given rise of an increasing number of academic works on this topic lately, where sophisticated combinations of different technics have been employed to improve the phenotypes extraction accuracy. Even more recently released, the ChatGPT1 application nevertheless raises the question of the relevance of these approches compared to this new generic one based on an instruction-oriented LLM. In this paper, we propose a rigorous evaluation of ChatGPT and the current state-of-the-art solutions on this specific task, and discuss the possible impacts and the technical evolutions to consider in the medical domain.Clinical relevance— Deep phenotyping on electronic health records has proven its ability to improve genetic diagnosis by clinical exomes [10]. Thus, comparing state-of-the-art solutions in order to derive insights and improving research paths is essential.},
  keywords={Temperature distribution;Pipelines;Statistical distributions;Machine learning;Ontologies;Chatbots;Information retrieval},
  doi={10.1109/EMBC40787.2023.10340611},
  ISSN={2694-0604},
  month={July},}@ARTICLE{9091850,
  author={Ji, Zizheng and Dai, Lin and Pang, Jin and Shen, Tingting},
  journal={IEEE Access}, 
  title={Leveraging Concept-Enhanced Pre-Training Model and Masked-Entity Language Model for Named Entity Disambiguation}, 
  year={2020},
  volume={8},
  number={},
  pages={100469-100484},
  abstract={Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in an input-text sequence to their correct references in a knowledge graph. We tackle NED problem by leveraging two novel objectives for pre-training framework, and propose a novel pre-training NED model. Especially, the proposed pre-training NED model consists of: (i) concept-enhanced pre-training, aiming at identifying valid lexical semantic relations with the concept semantic constraints derived from external resource Probase; and (ii) masked entity language model, aiming to train the contextualized embedding by predicting randomly masked entities based on words and non-masked entities in the given input-text. Therefore, the proposed pre-training NED model could merge the advantage of pre-training mechanism for generating contextualized embedding with the superiority of the lexical knowledge (e.g., concept knowledge emphasized here) for understanding language semantic. We conduct experiments on the CoNLL dataset and TAC dataset, and various datasets provided by GERBIL platform. The experimental results demonstrate that the proposed model achieves significantly higher performance than previous models.},
  keywords={Context modeling;Task analysis;Semantics;Predictive models;Adaptation models;Natural language processing;Neural networks;Named entity disambiguation;pre-training;lexical knowledge},
  doi={10.1109/ACCESS.2020.2994247},
  ISSN={2169-3536},
  month={},}@ARTICLE{9292918,
  author={Gong, Peizhu and Liu, Jin and Yang, Yihe and He, Huihua},
  journal={IEEE Access}, 
  title={Towards Knowledge Enhanced Language Model for Machine Reading Comprehension}, 
  year={2020},
  volume={8},
  number={},
  pages={224837-224851},
  abstract={Machine reading comprehension is a crucial and challenging task in natural language processing (NLP). Recently, knowledge graph (KG) embedding has gained massive attention as it can effectively provide side information for downstream tasks. However, most previous knowledge-based models do not take into account the structural characteristics of the triples in KGs, and only convert them into vector representations for direct accumulation, leading to deficiencies in knowledge extraction and knowledge fusion. In order to alleviate this problem, we propose a novel deep model KCF-NET, which incorporates knowledge graph representations with context as the basis for predicting answers by leveraging capsule network to encode the intrinsic spatial relationship in triples of KG. In KCF-NET, we fine-tune BERT, a highly performance contextual language representation model, to capture complex linguistic phenomena. Besides, a novel fusion structure based on multi-head attention mechanism is designed to balance the weight of knowledge and context. To evaluate the knowledge expression and reading comprehension ability of our model, we conducted extensive experiments on multiple public datasets such as WN11, FB13, SemEval-2010 Task 8 and SQuAD. Experimental results show that KCF-NET achieves state-of-the-art results in both link prediction and MRC tasks with negligible parameter increase compared to BERT-Base, and gets competitive results in triple classification task with significantly reduced model size.},
  keywords={Task analysis;Semantics;Knowledge engineering;Knowledge based systems;Bit error rate;Encoding;Syntactics;Machine reading comprehension;knowledge graph embedding;BERT;capsule network},
  doi={10.1109/ACCESS.2020.3044308},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10638283,
  author={Hendawi, Rasha and Alian, Shadi and Li, Juan},
  booktitle={2024 15th International Conference on Information and Communication Systems (ICICS)}, 
  title={Breaking Down Barriers: Empowering Diabetes Patients with User-Friendly Medical Explanations}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Effective management of diabetes is contingent upon patients' understanding of their medical conditions and treatments. However, medical documents often contain complex jargon and technical details that can be challenging for patients, especially those with limited health literacy. This paper presents DiaKnow, an innovative tool that simplifies medical documents and customizes explanations to suit individual health literacy levels. Employing a robust self-attention transformer model and a comprehensive diabetes-focused knowledge graph, DiaKnow enhances patient comprehension by providing contextually relevant, simplified medical information. This study assesses DiaKnow’s efficacy in real-world clinical settings through a structured use case evaluation method. We tested the tool’s ability to accurately identify, link, and simplify crucial medical terms using a diverse set of medical documents. Our findings confirm that DiaKnow not only improves the readability of medical documents but also ensures that explanations are medically accurate, clear, and comprehensive.},
  keywords={Accuracy;Communication systems;Transforms;Knowledge graphs;Transformers;Diabetes;Context modeling;health literacy;knowledge graph;ontology;self-attention transformers;medical entity recognition;entity linking},
  doi={10.1109/ICICS63486.2024.10638283},
  ISSN={2573-3346},
  month={Aug},}@INPROCEEDINGS{10456842,
  author={Ollier, Guillaume and Adedjouma, Morayo and Gerasimou, Simos and Mraidha, Chokri},
  booktitle={2023 26th Euromicro Conference on Digital System Design (DSD)}, 
  title={An Ontological Approach for the Dependability Analysis of Automated Systems}, 
  year={2023},
  volume={},
  number={},
  pages={593-601},
  abstract={This paper presents the Ontology Language for the Dependability of Automated Systems (OLDAS), a modeling language based on Unified Modeling Language (UML) that aims to support dependability assessment for Automated Systems (ASs), i.e., systems intended to perform a function with minimal or no human intervention. OLDAS extends the Unified Foundational Ontology (UFO) and embeds validation rules to prevent constraint violations in ASs analysis. Specifically, the paper presents how OLDAS can support different activities during the design of ASs, from the definition of the Operational Design Domain to scenario-based analysis. OLDAS is available as a plugin of the open-source Papyrus for Robotics framework.},
  keywords={Analytical models;Runtime;Unified modeling language;Redundancy;Ontologies;Probabilistic logic;Hazards;Autonomous Systems;Automated Driving Systems;Artificial Intelligence;Safety Engineering;ODD;ML-based Systems},
  doi={10.1109/DSD60849.2023.00087},
  ISSN={2771-2508},
  month={Sep.},}@INPROCEEDINGS{8258079,
  author={Kim, Youngho and Zerfos, Petros and Sheinin, Vadim and Greco, Nancy},
  booktitle={2017 IEEE International Conference on Big Data (Big Data)}, 
  title={Ranking the importance of ontology concepts using document summarization techniques}, 
  year={2017},
  volume={},
  number={},
  pages={1457-1466},
  abstract={Automated Ontology Learning systems are nowadays practical and used in a variety of domains. By using these systems, subject matter experts (SMEs) and ontology designers can readily construct very large ontologies consisting of tens of thousands of concepts and their relations based on a corpus. However, ontologies of this size make it extremely challenging for such SMEs to understand and further tune these ontologies. Prior studies have proposed techniques for concept ranking based solely on the analysis of the structure of the ontology graphs. In this paper, we propose a novel approach, which further exploits a word-level summarization technique applied to the source documents used to generate the ontology. Using the document summarization technique, we devise features that measure concept importance based on source documents where concepts are extracted. We demonstrate the effectiveness of our approach by comparing with existing ranking methods and by devising a scalable evaluation process inspired from the document retrieval domain.},
  keywords={Ontologies;Dogs;Learning systems;Feature extraction;Visualization;Measurement},
  doi={10.1109/BigData.2017.8258079},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8869059,
  author={Patzer, Florian and Volz, Friedrich and Usländer, Thomas and Blöcher, Immanuel and Beyerer, Jürgen},
  booktitle={2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={The Industrie 4.0 Asset Administration Shell as Information Source for Security Analysis}, 
  year={2019},
  volume={},
  number={},
  pages={420-427},
  abstract={One of the essential concepts of the Reference Architecture Model Industrie 4.0 (RAMI4.0) is the uniform modelling of assets by means of a common meta-data model called the Asset Administration Shell (AAS). However, important practical experience with this concept is still missing, as not many use cases for the AAS have yet been implemented. Thus, practical issues within the AAS concept and respective solutions are hard to identify. In this paper, presents our experience with the implementation of an AAS use case. The AAS is used as information source to create an ontology, which is then used for security analysis. The paper discusses the use-case-specific modelling language selection and provides a practical examination of several of our implementations that use OWL and OPC UA together. Furthermore, it provides recommendations for the implementation of Asset Administration Shells for this and similar use cases.},
  keywords={Security;Protocols;Data models;Analytical models;Tools;XML;Cognition;Asset Administration Shell;Security Ontology;Ontology;OPC UA;Semantic Web;Industrial Systems},
  doi={10.1109/ETFA.2019.8869059},
  ISSN={1946-0759},
  month={Sep.},}@ARTICLE{10633699,
  author={Woods, Caitlin and Hodkiewicz, Melinda and French, Tim},
  journal={IEEE Access}, 
  title={Semantic Quality Assurance of Industrial Maintenance Procedures}, 
  year={2024},
  volume={12},
  number={},
  pages={122029-122046},
  abstract={Maintenance technicians in industry follow procedures that guide them through inspection, repair, and service tasks. Organisations seek to convert procedure documentation to machine-readable formats as their digital capabilities improve and regulatory requirements tighten. In this paper, we consider the opportunity for semantic quality assurance of digital procedures. We demonstrate a configurable and repeatable workflow containing three modules. The completeness module makes implicit information in procedures explicit using OpenAI’s Generative Pre-trained Transformer (GPT) model. The consistency module creates Resource Description Framework (RDF) triples that are aligned with, and checked against, the axioms of the open-source Ontology for Maintenance Procedure Documentation (OMPD). Finally, the correctness module performs closed-world checks on the RDF triples using the Shapes Constraints Language (SHACL). Each module can be used in isolation, or together, to realise an end-to-end semi-automated quality assurance workflow. Pre-processing of the raw maintenance procedure documents to extract entities (tools, materials and activities) and relations is achieved in a novel manner using prompt engineering with OpenAI’s GPT-3.5 Turbo model and few-shot learning. This end-to-end workflow enables organisations to perform quality assurance such as assessing the correct order for task sequences, and checking that all maintenance procedures have at least one maintenance task. We demonstrate this workflow on six procedures from the iFixit repository. The outputs of this workflow support maintenance technicians, planners and engineers by realising high-quality procedure documentation and automated procedure management update processes. The code and data used in this work is publicly available at https://github.com/equonto/quokka/.},
  keywords={Maintenance;Ontologies;OWL;Resource description framework;Data models;Documentation;Task analysis;Industrial ontology;ontology templates;OpenAI GPT;OTTR;SHACL;technical language processing},
  doi={10.1109/ACCESS.2024.3441757},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10412837,
  author={Guo, Menghao and Wu, Fan and Jiang, Jinling and Yan, Xiaoran and Chen, Guangyong and Li, Wenhui and Zhao, Yunhong and Sun, Zeyi},
  booktitle={2023 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={Investigations on Scientific Literature Meta Information Extraction Using Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={249-254},
  abstract={The meta information in scientific literature including article title, author, institutions, year, journal, etc., plays a critical role in providing useful information to research peers. Traditional meta information extraction methods usually rely on rules and templates. Recently, due to the booming of Large Language Models (LLMs), its application in scientific literature meta-information extraction has drawn more and more attention. This paper aims to explore and evaluate the effects of meta information extraction for scientific literature using large language models. First, datasets consisting of the publications in given academic areas are built for the experiments. Then, the task definition and evaluation metric (i.e., accuracy rate) are described. Various large language models as well as the traditional methodology are used in experiments to execute the task of meta information extraction. The results are analyzed and compared among the use of various LLMs.},
  keywords={Measurement;Knowledge graphs;Information retrieval;Data mining;Task analysis;information extraction;large language model;scientific literature},
  doi={10.1109/ICKG59574.2023.00036},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10822152,
  author={Wang, Zhirui and Jiang, Xinlong and Gao, Chenlong and Dong, Fan and Dai, Weiwei and Wang, Bingyu and Yan, Bingjie and Chen, Qian and Huang, Wuliang and Zhang, Teng and Chen, Yiqiang},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={EyeGraphGPT: Knowledge Graph Enhanced Multimodal Large Language Model for Ophthalmic Report Generation}, 
  year={2024},
  volume={},
  number={},
  pages={3784-3789},
  abstract={Automatic generation of ophthalmic reports holds significant potential to lessen clinicians’ workload, enhance work efficiency, and alleviate the imbalance between clinicians and patients. Recent advancements in multimodal large language models, represented by GPT-4, have demonstrated remarkable performance in the general domain. However, training such models necessitates a substantial amount of paired image-text data, yet paired ophthalmic data is limited, and ophthalmic reports are laden with specialized terminologies, making it challenging to transfer the training paradigm to the ophthalmic domain. In this paper, we propose EyeGraphGPT, a knowledge graph enhanced multimodal large language model for ophthalmic report generation. Specifically, we construct a knowledge graph by leveraging the knowledge from a medical database and expertise from ophthalmic experts to model relationships among ophthalmic diseases, enhancing the model’s focus on key disease information. We then perform relation-aware modal alignment to incorporate knowledge graph features into visual features, and further enhance modality collaboration through visual instruction fine-tuning to adapt the model to the ophthalmic domain. Our experiments on a real-world dataset demonstrates that EyeGraphGPT outperforms previous state-of-the-art models, highlighting its superiority in scenarios with limited medical data and extensive specialized terminologies.},
  keywords={Training;Adaptation models;Visualization;Terminology;Large language models;Unified modeling language;Collaboration;Knowledge graphs;Data models;Diseases;Ophthalmic Report Generation;Knowledge-enhanced Multimodal Models;Multimodal Large Language Models},
  doi={10.1109/BIBM62325.2024.10822152},
  ISSN={2156-1133},
  month={Dec},}@ARTICLE{10487851,
  author={Strader, Jared and Hughes, Nathan and Chen, William and Speranzon, Alberto and Carlone, Luca},
  journal={IEEE Robotics and Automation Letters}, 
  title={Indoor and Outdoor 3D Scene Graph Generation Via Language-Enabled Spatial Ontologies}, 
  year={2024},
  volume={9},
  number={6},
  pages={4886-4893},
  abstract={This letter proposes an approach to build 3D scene graphs in arbitrary indoor and outdoor environments. Such extension is challenging; the hierarchy of concepts that describe an outdoor environment is more complex than for indoors, and manually defining such hierarchy is time-consuming and does not scale. Furthermore, the lack of training data prevents the straightforward application of learning-based tools used in indoor settings. To address these challenges, we propose two novel extensions. First, we develop methods to build a spatial ontology defining concepts and relations relevant for indoor and outdoor robot operation. In particular, we use a Large Language Model (LLM) to build such an ontology, thus largely reducing the amount of manual effort required. Second, we leverage the spatial ontology for 3D scene graph construction using Logic Tensor Networks (LTN) to add logical rules, or axioms (e.g., “a beach contains sand”), which provide additional supervisory signals at training time thus reducing the need for labelled data, providing better predictions, and even allowing predicting concepts unseen at training time. We test our approach in a variety of datasets, including indoor, rural, and coastal environments, and show that it leads to a significant increase in the quality of the 3D scene graph generation with sparsely annotated data.},
  keywords={Three-dimensional displays;Ontologies;Semantics;Training data;Solid modeling;Artificial intelligence;Semantics;Image analysis;Spatial resolution;Indoor environment;AI-based methods;3D scene graphs;semantic scene understanding;spatial ontologies},
  doi={10.1109/LRA.2024.3384084},
  ISSN={2377-3766},
  month={June},}@ARTICLE{9623547,
  author={Loubach, Denis S. and Bonna, Ricardo and Ungureanu, George and Sander, Ingo and Söderquist, Ingemar},
  journal={IEEE Access}, 
  title={Classification and Mapping of Model Elements for Designing Runtime Reconfigurable Systems}, 
  year={2021},
  volume={9},
  number={},
  pages={156337-156360},
  abstract={Embedded systems are ubiquitous and control many critical functions in society. A fairly new type of embedded system has emerged with the advent of partial reconfiguration, i.e. runtime reconfigurable systems. They are attracting interest in many different applications. Such a system is capable of reconfiguring itself at the hardware level and without the need to halt the application’s execution. While modeling and implementing these systems is far from a trivial task, there is currently a lack of systematic approaches to tackle this issue. In other words, there is no unanimously agreed upon modeling paradigm that can capture adaptive behaviors at the highest level of abstraction, especially when regarding the design entry, namely, the initial high-level application and platform models. Given this, our paper proposes two domain ontologies for application and virtual platform models used to derive a classification system and to provide a set of rules on how the different model elements are allowed to be composed together. The application behavior is captured through a formal model of computation which dictates the semantics of execution, concurrency, and synchronization. The main contribution of this paper is to combine suitable formal models of computation, a functional modeling language, and two domain ontologies to create a systematic design flow from an abstract executable application model into a virtual implementation model based on a runtime reconfigurable architecture (virtual platform model) using well-defined mapping rules. We demonstrate the applicability, generality, and potential of the proposed model element classification system and mapping rules by applying them to representative and complete examples: an encoder/decoder system and an avionics attitude estimation system. Both cases yield a virtual implementation model from an abstract application model.},
  keywords={Computational modeling;Runtime;Unified modeling language;Ontologies;Embedded systems;Adaptation models;Hardware;Embedded systems;runtime reconfiguration;models of computation (MoC);domain ontology;mapping rules},
  doi={10.1109/ACCESS.2021.3129899},
  ISSN={2169-3536},
  month={},}@ARTICLE{10149041,
  author={Liang, Ke and Liu, Yue and Zhou, Sihang and Tu, Wenxuan and Wen, Yi and Yang, Xihong and Dong, Xiangjun and Liu, Xinwang},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Knowledge Graph Contrastive Learning Based on Relation-Symmetrical Structure}, 
  year={2024},
  volume={36},
  number={1},
  pages={226-238},
  abstract={Knowledge graph embedding (KGE) aims at learning powerful representations to benefit various artificial intelligence applications. Meanwhile, contrastive learning has been widely leveraged in graph learning as an effective mechanism to enhance the discriminative capacity of the learned representations. However, the complex structures of KG make it hard to construct appropriate contrastive pairs. Only a few attempts have integrated contrastive learning strategies with KGE. But, most of them rely on language models (e.g., Bert) for contrastive pair construction instead of fully mining information underlying the graph structure, hindering expressive ability. Surprisingly, we find that the entities within a relational symmetrical structure are usually similar and correlated. To this end, we propose a knowledge graph contrastive learning framework based on relation-symmetrical structure, KGE-SymCL, which mines symmetrical structure information in KGs to enhance the discriminative ability of KGE models. Concretely, a plug-and-play approach is proposed by taking entities in the relation-symmetrical positions as positive pairs. Besides, a self-supervised alignment loss is designed to pull together positive pairs. Experimental results on link prediction and entity classification datasets demonstrate that our KGE-SymCL can be easily adopted to various KGE models for performance improvements. Moreover, extensive experiments show that our model could outperform other state-of-the-art baselines.},
  keywords={Semantics;Knowledge graphs;Data models;Computational modeling;Learning systems;Contrastive learning;Graph learning;knowledge graph embedding;self-supervised contrastive learning;symmetrical property},
  doi={10.1109/TKDE.2023.3282989},
  ISSN={1558-2191},
  month={Jan},}@INPROCEEDINGS{9442198,
  author={Zhou, Bin and Bao, Jinsong and Liu, Yahui and Song, Dengqiang},
  booktitle={2020 IEEE 18th International Conference on Industrial Informatics (INDIN)}, 
  title={BA-IKG: BiLSTM Embedded ALBERT for Industrial Knowledge Graph Generation and Reuse}, 
  year={2020},
  volume={1},
  number={},
  pages={63-69},
  abstract={As the industrial production mode is shifting towards digitalization and intelligence in the new era. Enterprises put forward higher requirements for efficient processing and utilization of accumulated unstructured data. At present, the knowledge and data contained in a large number of unstructured documents are scattered. The types of entities and relationships are diverse. And the constraints of production rules are complicated, which increases the difficulty of knowledge management and utilization. Therefore, this paper studies the semantic knowledge graph generation and reuse method for industrial documents, which can form standardized production resources, the knowledge related to the industry, and question and answer strategies for industrial processing. The challenge of the research is to explore a feasible process knowledge model and efficient industrial information extraction method to effectively provide structured knowledge of process documents. We build process knowledge representation models and information extraction models and algorithms based on process knowledge representation model and natural language processing. The entities and relations of the main production factors are extracted. The knowledge representation model associates the extracted entities and relations to form an industrial knowledge graph, which provides information support for processing knowledge retrieval and question answering methods. Finally, the approach is evaluated by employing the aerospace machining documents. And the proposed method can obtain valuable information in the document and improve utilization of industrial unstructured data.},
  keywords={Industries;Semantics;Production;Knowledge representation;Machining;Information retrieval;Knowledge discovery;knowledge modeling;entity relation extraction;industrial knowledge graph;knowledge question and answer},
  doi={10.1109/INDIN45582.2020.9442198},
  ISSN={2378-363X},
  month={July},}@ARTICLE{10472080,
  author={Zhang, Geng and Liu, Jin and Zhou, Guangyou and Zhao, Kunsong and Xie, Zhiwen and Huang, Bo},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Question-Directed Reasoning With Relation-Aware Graph Attention Network for Complex Question Answering Over Knowledge Graph}, 
  year={2024},
  volume={32},
  number={},
  pages={1915-1927},
  abstract={Complex knowledge graph question answering (KGQA) aims at answering natural language questions by entities retrieving from a knowledge graph (KG). Recently, the relation path-based models have shown the unique advantage for complex KGQA. However, these existing models ignore the dependency between different relation paths, which leads to aimless reasoning over the KG. To resolve this issue, we propose the question-directed reasoning with relation-aware graph attention network (QRGAT) that encodes the reasoning process as a reasoning graph. The relation-aware GAT can recognize neighbor entities along with the corresponding relations for each entity. With the relation-aware GAT stacked in multiple layers, it can collaboratively capture the dependency of different relation paths for each entity. The question-directed reasoning utilizes the information learned by the relation-aware GAT to solve the aimless reasoning on the KG by constructing a reasoning graph. Extensive experiments demonstrate that our QRGAT outperforms the baseline models on both popular datasets WebQuestionsSP and ComplexWebQuestions. Compared with the strong GNN-based baseline NSM$_{+h}$, our QRGAT achieves the performance improvements of 2.3% on WebQuestionsSP and 3.6% on ComplexWebQuestions by the metric Hits@1.},
  keywords={Cognition;Semantics;Knowledge graphs;Task analysis;Training;Question answering (information retrieval);Speech processing;Graph attention network;information retrieval;knowledge graph;question answering},
  doi={10.1109/TASLP.2024.3375631},
  ISSN={2329-9304},
  month={},}@INPROCEEDINGS{9904215,
  author={Shi, Rui and Wang, Zhenchuan and Liu, Yang and Lan, Yunliang and Zhao, Wei and Liu, Yitang},
  booktitle={2022 5th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={Improve on Entity Recognition Method Based on BiLSTM-CRF Model for the Nuclear Technology Knowledge Graph}, 
  year={2022},
  volume={},
  number={},
  pages={241-246},
  abstract={The accuracy of entity recognition is particularly important for knowledge graph construction. The traditional Named Entity recognition (NER) model mainly includes HMM, CRF, BiLSTM, BiLSTM-CRF, etc. It is difficult to solve the problem of word meaning confusion resulting from the wrong separation at the end of the entity when using the four models for the labeled nuclear technology knowledge data sets. In order to improve the entity recognition effect and address the problem of polysemy in nuclear technology knowledge data set, an improved nuclear technology entity recognition method based on the BERT-BiLSTM-CRF combination model was proposed by comparative experiments. According to the results, it can conclude that the application of the BERT model instead of the Word2Vec algorithm for word vector training is helpful to the model recognition, and the exclusive dictionary word segmentation and part of speech classification of nuclear technology texts contribute to improving the quality of labeled data. The experiment verified that the usage of BERT pre-training model can solve the problem of polysemy in NER to some extent. Meanwhile, it validated the precision in nuclear technology knowledge entity recognition for collected data, which could be used in pre-procedure in nuclear technology knowledge graph construction.},
  keywords={Training;Dictionaries;Text recognition;Bit error rate;Hidden Markov models;Speech recognition;Data models;nuclear technology knowledge graph;named entity recognition;BERT-BiLSTM-CRF;Vector},
  doi={10.1109/PRAI55851.2022.9904215},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9742130,
  author={Tang, Jing and Ning, Xinran and Wang, Kunfeng and Tan, Ying and Chen, Jianying},
  booktitle={2021 2nd International Conference on Electronics, Communications and Information Technology (CECIT)}, 
  title={A Hybrid Relation Extraction Model for Knowledge Graph of Heroic Epic “Gesar”}, 
  year={2021},
  volume={},
  number={},
  pages={65-70},
  abstract={Extracting relations from unstructured text is the primary step of knowledge graph construction. This step is harder in the Tibetan heroic epic “Epic Appreciation King Gesar” than in other Chinese stories because of the difficulties in the identification of the named entities and the complicated character relationships. Given this, based on the named entity corpus constructed with help of domain experts earlier, a hybrid relation extraction model combined with entity features and syntactic-semantic features, EBERT-BiLSTM, is proposed in this paper. The data set, corpus, and model construction principle and process are described in detail. Experiments show that EBERT-BiLSTM has better effectiveness and performance than just BiLSTM-Attention and BERT. Finally, EBERT-BiLSTM is used to extract 320-character relation triples and then construct the heroic epic “Gesar” knowledge graph.},
  keywords={Bit error rate;Feature extraction;Data models;Information and communication technology;Data mining;Artificial intelligence;Text processing;knowledge graph;relation extraction;Gesar},
  doi={10.1109/CECIT53797.2021.00019},
  ISSN={},
  month={Dec},}
@INPROCEEDINGS{9678627,
  author={Xue, Zengcan and Liu, Hai and Zhang, Zhaoli and Yang, Shuoqiu},
  booktitle={2021 IEEE International Conference on Engineering, Technology & Education (TALE)}, 
  title={Dynamic Educational Knowledge Graph Model via Information Entropy for Knowledge Building}, 
  year={2021},
  volume={},
  number={},
  pages={01-07},
  abstract={Knowledge building is the production and continual improvement of ideas of value to a community, which attaches importance to conceptual engagement and contribution. However, knowledge building community will accumulate large and complex semi-structured educational data over time. It is not conducive to the continuation of in-depth knowledge building activities. To overcome these issues, we propose a dynamic educational knowledge graph with information entropy (IE-DEKG) model for knowledge building community. The model can construct dynamic knowledge graphs that contain instructional concepts and educational relations for learners. Specifically, it adopts the mutual information and adjacent information entropy to detect new terminologies on pedagogical data, and then the topic modeling algorithm is utilized to extract instructional concepts. Moreover, the model employs association rule mining to identify the prerequisite relations and uses pattern matching to obtain the inclusion relations. For the sake of satisfying the needs of educational applications and services, we design and implement the dynamic educational knowledge graph system. Experimental results demonstrate that the proposed IE-DEKG method outperforms the state-of-the-art methods.},
  keywords={Matched filters;Terminology;Education;Production;Information filters;Data models;Data mining;knowledge building;educational knowledge graph;information entropy;concept extraction;educational relation;system design},
  doi={10.1109/TALE52509.2021.9678627},
  ISSN={2470-6698},
  month={Dec},}@INPROCEEDINGS{10538922,
  author={Chen, Chen and Xia, Chunhe and Wang, Tianbo and Lin, Wanshuang and Zhao, Yuan and Li, Yang},
  booktitle={2023 IEEE 22nd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)}, 
  title={EFwork: An Efficient Framework for Constructing a Malware Knowledge Graph}, 
  year={2023},
  volume={},
  number={},
  pages={1258-1265},
  abstract={Malware Knowledge Graph (MKG) serves as an essential auxiliary tool for malware detection and analysis. However, the construction of MKG faces several challenges, such as inadequate dataset quality, incomplete entity feature extraction, and the limitations imposed by deep learning techniques. To address these issues, we present an Efficient Framework for constructing a malware knowledge graph (EFwork). Firstly, we build a High-Quality Dataset (HQDataset) and introduce a metric for data quality assessment based on knowledge coverage, timeliness, and density. Subsequently, we develop a Named Entity Recognition (NER) model that extracts character features, part-of-speech features, and word features from the data, leveraging deep learning models to identify malware-related entities. Finally, we implement a rule-based filtering mechanism, utilizing a comprehensive Rule Database to eliminate entities that do not conform to predefined rules. Experimental result shows that our HQDataset demonstrates superior data quality when compared to other open-source datasets. Furthermore, our NER model combined with our Rule Database outperforms existing models, achieving improvements of 0.67%, 0.74%, and 0.69% in Precision, Recall, and F1-Score, respectively.},
  keywords={Measurement;Deep learning;Databases;Filtering;Data integrity;Knowledge graphs;Feature extraction;Malware;Knowledge Graph;Dataset Quality;Named Entity Recognition},
  doi={10.1109/TrustCom60117.2023.00171},
  ISSN={2324-9013},
  month={Nov},}@INPROCEEDINGS{10365964,
  author={Liu, Zhipeng and Weng, Heng and Yan, Jun and Liu, Hai and Hao, Tianyong},
  booktitle={2023 IEEE International Symposium on Product Compliance Engineering - Asia (ISPCE-ASIA)}, 
  title={Query Reformulator And Contrastive Answer Ranker For Conversational Question Answering Over Knowledge Graph}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Conversational KGQA (Knowledge Graph Question Answering) is a sequential process of question-answering over a knowledge graph (KG). Conversational KGQA involves follow-up questions that are presented in anaphoric or abbreviated form, known as ellipsis. Another challenge is the difficulty in selecting corrects answer for follow-up questions from a large number of candidate answers from KGs. This paper aims to tackle the challenge of alleviating the ellipsis phenomena and selecting correct answers from copious candidates. We present a module called CSR, which effectively merges information-retrieval-based method with pretrained language models. This module aims to identify the most relevant entity for elliptical questions based on instructional templates and subsequently rephrase the questions. Additionally, we propose a contrastive answer ranker to address the challenge of selecting the correct answer from copious candidates by leveraging the relationship between the questions and answers. Leveraging fewer trainable parameters, our method outperforms baselines on ConvQuestions and surpass the F1-Score metric of state-of-the-art on ConvRef dataset.},
  keywords={Measurement;Asia;Knowledge graphs;Oral communication;Question answering (information retrieval);Standards;Question Answering;Conversation;Knowledge Graph},
  doi={10.1109/ISPCE-ASIA60405.2023.10365964},
  ISSN={2831-3410},
  month={Nov},}@INPROCEEDINGS{10386891,
  author={Jiang, Wenjuan and Guo, Yi and Fu, Jiaojiao},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={Temporal Knowledge Graph Question Answering Models Enhanced with GAT}, 
  year={2023},
  volume={},
  number={},
  pages={1162-1167},
  abstract={Temporal Knowledge Graph Question Answering (TKGQA) task aims to find an entity or timestamp from a temporal knowledge graph to answer temporal reasoning questions. However, most existing models fail to capture the implicit temporal information in the questions, resulting in weak performance when handling complex temporal reasoning tasks. To address this issue, this paper proposes a novel TKGQA model called GATQR, which integrates graph attention mechanism. The model utilizes a pre-trained temporal knowledge base in the form of quadruples and introduces Graph Attention Network (GAT) to effectively capture the implicit temporal information in the questions. By integrating with relation representations trained by the RoBERTa, it further enhances the temporal relationship representation in the queries. Finally, this representation is combined with the pre-trained TKG embeddings to predict the entity or timestamp with the highest score as the answer. Experimental results on the largest benchmark dataset CronQuestion demonstrate that compared to baseline models such as CronKGQA, EntityQR, and TempoQR-Soft, the GATQR achieves significant improvements in Hits@l results for handling complex and temporal question types, with increases of 35% and 13%, 18% and 9%, and 9% and 3%, respectively. These results validate the effectiveness and superiority of the GATQR model in capturing implicit temporal information and enhancing complex reasoning capabilities.},
  keywords={Knowledge engineering;Navigation;Merging;Knowledge based systems;Knowledge graphs;Cognition;Question answering (information retrieval);Temporal Knowledge Graph;Complex Questions Answering;Graph Attention Network;Temporal Reasoning;temporal relationship representation},
  doi={10.1109/BigData59044.2023.10386891},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8622503,
  author={Kharlamov, Evgeny and Martin-Recuerda, Francisco and Perry, Brandon and Cameron, David and Fjellheim, Roar and Waaler, Arild},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)}, 
  title={Towards Semantically Enhanced Digital Twins}, 
  year={2018},
  volume={},
  number={},
  pages={4189-4193},
  abstract={Digital twins (DTs) are a powerful mechanism for representing complex industrial assets such as oil platforms as digital models. These models can facilitate temporal analyses and computer simulations of assets. In order to enable this, DTs should be able to capture characteristics of an asset as specified by the manufacturer, its state during the run time, as well as how the asset interacts with other assets in a complex system. We argue that semantic technologies and in particular semantic models or ontologies is promising modelling paradigm for DTs. Semantic models allow to capture complex systems in an intuitive fashion, can be written in standardised ontology languages, and come with a wide range of off-the-shelf systems to design, maintain, query, and navigate semantic models. In this work we report our preliminary results on developing a system that would support semantic-based DTs. In particular, we plan to augment the PI System developed by OSIsoft with ontologies and show how the resulting solution can help in simplifying analytical and machine learning routines for DTs.},
  keywords={Data models;Semantics;Analytical models;Computational modeling;Context modeling;Ontologies;Turbines},
  doi={10.1109/BigData.2018.8622503},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9276945,
  author={Trivedi, Ishdutt and Majhi, Sudhan},
  booktitle={2020 5th International Conference on Computing, Communication and Security (ICCCS)}, 
  title={Span Level Model for the Construction of Scientific Knowledge Graph}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={In the recent years, knowledge graphs (KGs) are getting lots of attention due to their wide applications. Constructing KGs involves two major steps, named entity recognition (NER) and relation extraction (RE). Current approaches for extracting entities and relation use (beginning, outside, inside) BIO/(beginning, inside, last, outside, unit) BILOU based models which faces issues of cascading errors. In this paper, we introduce a novel neural architecture for the construction of scientific KG employing span based methods for extraction of entities, unlike previous models which use BIO/BILOU based approach. We use bidirectional encoder representations from transformers (BERT) word embeddings for this task and neural network classifier for the detection of entities in the span and convolutional neural network (CNN) for extracting relation between the entities. Evaluation results yield the recall, precision and f-measure values of 70.61%, 71.16% and 70.88% respectively for NER, and 54.89%, 50.72% and 52.72% respectively for RE task. Results show that our model outperforms the previous models for the scientific dataset, SciERC, by a margin of 3% in entity extraction task and 7% for the relation extraction task. We achieved these results without raising the overall complexity of the model.},
  keywords={Task analysis;Data mining;Bit error rate;Neural networks;Feature extraction;Training;Faces},
  doi={10.1109/ICCCS49678.2020.9276945},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8104505,
  author={Zambon, Eduardo and Guizzardi, Giancarlo},
  booktitle={2017 Federated Conference on Computer Science and Information Systems (FedCSIS)}, 
  title={Formal definition of a general ontology pattern language using a graph grammar}, 
  year={2017},
  volume={},
  number={},
  pages={1-10},
  abstract={In recent years, there has been a growing interest in the use of ontological theories in the philosophical sense (Foundational Ontologies) to analyze and (re)design conceptual modeling languages. This paper is about an ontologically well-founded conceptual modeling language in this tradition, termed OntoUML. This language embeds a number of ontological patterns that reflect the micro-theories comprising a particular foundational ontology named UFO. We here (re)define OntoUML as a formal graph grammar and demonstrate how the models of this language can be constructed by the combined application of ontological patterns following a number of graph transformation rules. As a result, we obtain a version of this language fully defined as a formal Ontology Pattern Grammar. In other words, this paper presents a formal definition of OntoUML that is both explicit in terms of the ontological patterns that it incorporates and is completely independent of the UML meta-model.},
  keywords={Unified modeling language;Ontologies;Grammar;Computational modeling;Aggregates;Tools},
  doi={10.15439/2017F001},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10650208,
  author={Kang, Jiaju and Pan, Weichao and Zhang, Tian and Wang, Ziming and Yang, Shuqin and Wang, Zhiqin and Wang, Jian and Niu, Xiaofei},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Correcting Factuality Hallucination in Complaint Large Language Model via Entity-Augmented}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Complaint Large Language Model (Complaint-LLM) is designed as a "customer service" tool to address the scenario of handling a massive volume of public complaints, effectively leveraging the "common sense" possessed by Large Language Models (LLMs) to solve issues. Unfortunately, pre-trained LLMs often exhibit significant Factual Hallucination and Causal Errors in knowledge domains with sparse experience distribution, greatly affecting the accuracy of user interactions with LLMs. We propose an architecture that utilizes external data to support pre-trained models, aiming to avoid the expensive cost of retraining LLMs. The core concept involves leveraging prompts to inject strongly correlated additional information into LLMs and adjusting the initialized alternative outputs along the inference pathway of the LLM. To achieve this, we construct a rich knowledge graph as a knowledge base for algorithm retrieval and learning. Each input text is decomposed into subgraphs corresponding to nodes on the knowledge graph, and a graph neural network classifier is trained to obtain classification results and additional knowledge. Numerous experiments demonstrate that the Complaint-LLMs shows a significant improvement in the question-answering evaluation of various subclass scenarios in the complaint domain. Moreover, the graph neural network trained with complaint text data exhibits good transferability in classification tests for open scenarios.},
  keywords={Knowledge engineering;Measurement;Accuracy;Large language models;Semantics;Knowledge graphs;Transforms},
  doi={10.1109/IJCNN60899.2024.10650208},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10750098,
  author={Buchmüller, Raphael and Fürst, Daniel and Frings, Alexander and Schlegel, Udo and Keim, Daniel},
  booktitle={2024 IEEE Visual Analytics Science and Technology VAST Challenge}, 
  title={Visual Bias Detection for Addressing Illegal Fishing Activities}, 
  year={2024},
  volume={},
  number={},
  pages={9-10},
  abstract={In this work, we present a visual analytics approach designed to address the 2024 VAST Challenge Mini-Challenge 1, which focuses on detecting bias in a knowledge graph. Our solution utilizes pixel-based visualizations to explore patterns within the knowledge graph, CatchNet, which is employed to identify potential illegal fishing activities. CatchNet is constructed by FishEye analysts who aggregate open-source data, including news articles and public reports. They have recently begun incorporating knowledge extracted from these sources using advanced language models. Our method combines pixel-based visualizations with ordering techniques and sentiment analysis to uncover hidden patterns in both the news articles and the knowledge graph. Notably, our analysis reveals that news articles covering critiques and convictions of companies are subject to elevated levels of bias.},
  keywords={Sentiment analysis;Visual analytics;Large language models;Pipelines;Data visualization;Knowledge graphs;Companies;Data aggregation;VAST Challenge;Mini-Challenge 1;Visual Analytics;LLM;Pixel Visualization},
  doi={10.1109/VASTChallenge64683.2024.00009},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10446325,
  author={Xu, Hongshen and Cao, Ruisheng and Zhu, Su and Jiang, Sheng and Zhang, Hanchong and Chen, Lu and Yu, Kai},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A Birgat Model for Multi-Intent Spoken Language Understanding with Hierarchical Semantic Frames}, 
  year={2024},
  volume={},
  number={},
  pages={12251-12255},
  abstract={Previous work on spoken language understanding (SLU) mainly focuses on single-intent settings, where each input utterance merely contains one user intent. This configuration significantly limits the surface form of user utterances and the capacity of output semantics. In this work, we firstly propose a Multi-Intent dataset which is collected from a realistic in-Vehicle dialogue System, called MIVS. The target semantic frame is organized in a 3-layer hierarchical structure to tackle the alignment and assignment problems in multi-intent cases. Accordingly, we devise a BiRGAT model to encode the hierarchy of ontology items, the backbone of which is a dual relational graph attention network. Coupled with the 3-way pointer-generator decoder, our method outperforms traditional sequence labeling and classification-based schemes by a large margin. Ablation study in transfer learning settings further uncovers the poor generalizability of current models in multi-intent cases.},
  keywords={Semantics;Transfer learning;Ontologies;Signal processing;Acoustics;Decoding;Labeling;Spoken Language Understanding;relational graph attention network;hierarchical semantic frame},
  doi={10.1109/ICASSP48485.2024.10446325},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10731381,
  author={Grassi, Lucrezia and Recchiuto, Carmine Tommaso and Sgorbissa, Antonio},
  booktitle={2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)}, 
  title={Enhancing LLM-Based Human-Robot Interaction with Nuances for Diversity Awareness}, 
  year={2024},
  volume={},
  number={},
  pages={2287-2294},
  abstract={This paper presents a system for diversity-aware autonomous conversation leveraging the capabilities of large language models (LLMs). The system adapts to diverse populations and individuals, considering factors like background, personality, age, gender, and culture. The conversation flow is guided by the structure of the system’s pre-established knowledge base, while LLMs are tasked with various functions, including generating diversity-aware sentences. Achieving diversity-awareness involves providing carefully crafted prompts to the models, incorporating comprehensive information about users, conversation history, contextual details, and specific guidelines. To assess the system’s performance, we conducted both controlled and real-world experiments, measuring a wide range of performance indicators.},
  keywords={Large language models;Knowledge based systems;Human-robot interaction;Oral communication;Ontologies;Hybrid power systems;Time factors;Noise measurement;History;Robots},
  doi={10.1109/RO-MAN60168.2024.10731381},
  ISSN={1944-9437},
  month={Aug},}@INPROCEEDINGS{10385489,
  author={Qi, JuanZhi and Wang, XinYu and Yang, Tao},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Traditional Chinese Medicine Prescription Recommendation Model Based on Large Language Models and Graph Neural Networks}, 
  year={2023},
  volume={},
  number={},
  pages={4623-4627},
  abstract={Background: Traditional Chinese medicine (TCM) has a millennia-long history, offering unique treatments and insights into global health. Given the intricate symptoms and shifting syndrome patterns, prescribing can be tough for young doctors. TCM prescription recommendations can help these doctors address their experience gap. In recent years, with advancements in technologies such as artificial intelligence and big data, intelligent recommendations for TCM prescriptions have become feasible, holding significant implications for enhancing treatment efficacy and optimizing patient experience. Objective: This study aims to establish a novel TCM prescription recommendation model by integrating large language models with Graph Neural Network (GNN) to enhance the accuracy of prescription suggestions. Method: Based on the co-occurrence of symptoms and herbal medicines, we constructed symptom graphs, symptom-herb graphs, and herb-herb graphs. Using Graph Convolutional Network (GCN), we acquired embeddings for both symptoms and herbs. The symptom embeddings are then integrated with insights from large language model embeddings, while auxiliary information from an external knowledge graph is incorporated into the herb embeddings. A final list of herb recommendations was generated by interacting with the embeddings of symptoms and herbs. Results: The proposed algorithm achieved 22.1%, 17.2%, and 13% on the evaluation metrics P@5, P@10, and P@20, respectively. Concurrently, scores for R@5, R@10, and R@20 were 14%, 24%, and 32.5%, respectively. The P@5 metric surpassed the KDHR by 4.7%, and the R@20 metric exceeded the KDHR by 6%. Overall, the performance of our model outperformed other baseline models across various evaluation criteria. Conclusion: The TCM prescription recommendation model, infused with information from a large language model, can effectively enhance the outcomes of TCM prescription recommendations. The study may offer valuable insights for auxiliary clinical research and treatment in TCM.},
  keywords={Measurement;Knowledge engineering;Biological system modeling;Knowledge graphs;Self-supervised learning;Medical services;Graph neural networks;Prescription recommendation;Large language model;Traditional Chinese Medicine;Graph neural network},
  doi={10.1109/BIBM58861.2023.10385489},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{9202197,
  author={Liang, Xuchao and Cao, Han and Zhang, Weizhen},
  booktitle={2020 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS)}, 
  title={Knowledge Extraction Experiment Based on Tourism Knowledge Graph Q & A Data Set}, 
  year={2020},
  volume={},
  number={},
  pages={828-832},
  abstract={At present, the most natural language processing tasks use common data sets for experiments. However, as the concept of domain knowledge graphs is proposed, domain-based data sets have gradually become a demand. In this article, we collect data from various travel websites and official websites of tourist attractions, and use this to build a question and answer data set. At the same time, we also introduce the current Bert model with outstanding effect in the nlp field, and use this model to conduct experiments in the travelling question and answer data set. The experimental results not only show the feasibility of the constructed tourism data set, but also lay a foundation for the subsequent construction of a knowledge question answering system for tourism knowledge graph.},
  keywords={Natural language processing;Task analysis;Knowledge discovery;Semantics;Computational modeling;Data models;Context modeling;Knowledge map;tourism data;named entity recognition;relationship recognition},
  doi={10.1109/ICPICS50287.2020.9202197},
  ISSN={},
  month={July},}@INPROCEEDINGS{10020689,
  author={Pugazhenthi, Thamizhiniyan and Liang, Huizhi},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Improving Conversational Recommender Systems via Knowledge Graph-based Semantic Fusion with Historical Interaction Data}, 
  year={2022},
  volume={},
  number={},
  pages={4303-4312},
  abstract={Conversational recommender systems (CRS) use interactive discussions to recommend high-quality items to users. Two essential components in a good CRS are the recommendation module that makes pertinent product recommendations to consumers and a conversation component that creates text-based sentences with product recommendations. The most commonly used dataset to train CRS models is ReDial. In this paper, we found that using the INSPIRED dataset in place of the ReDial dataset significantly improves model performance in terms of effectiveness. Along with the INSPIRED dataset, the inclusion of historical data in the input improves efficiency. The accuracy and efficiency of the model increase when we include the historical data into the system in the form of DialoGPT corpus and Gutenberg books. The paper further extends to compare three versions of state-of-the-art knowledge graph based conversational recommender systems called KGSF â one with the INSPIRED dataset with history, one with the INSPIRED dataset without historical data and the last with the ReDial dataset without historical data, which is the original version of the KGSF model. The comparison between three versions of the KGSF model shows that the change of the dataset and the inclusion of historical data can promote the performance of this conversational recommendation system.},
  keywords={Vocabulary;Semantics;Natural languages;Oral communication;Machine learning;Big Data;Data models;natural language processing;conversational recommender systems},
  doi={10.1109/BigData55660.2022.10020689},
  ISSN={},
  month={Dec},}@ARTICLE{10670469,
  author={Yan, Youfu and Hou, Yu and Xiao, Yongkang and Zhang, Rui and Wang, Qianwen},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={KNowNEt:Guided Health Information Seeking from LLMs via Knowledge Graph Integration}, 
  year={2025},
  volume={31},
  number={1},
  pages={547-557},
  abstract={The increasing reliance on Large Language Models (LLMs) for health information seeking can pose severe risks due to the potential for misinformation and the complexity of these topics. This paper introduces KnowNet a visualization system that integrates LLMs with Knowledge Graphs (KG) to provide enhanced accuracy and structured exploration. Specifically, for enhanced accuracy, KnowNet extracts triples (e.g., entities and their relations) from LLM outputs and maps them into the validated information and supported evidence in external KGs. For structured exploration, KnowNet provides next-step recommendations based on the neighborhood of the currently explored entities in KGs, aiming to guide a comprehensive understanding without overlooking critical aspects. To enable reasoning with both the structured data in KGs and the unstructured outputs from LLMs, KnowNet conceptualizes the understanding of a subject as the gradual construction of graph visualization. A progressive graph visualization is introduced to monitor past inquiries, and bridge the current query with the exploration history and next-step recommendations. We demonstrate the effectiveness of our system via use cases and expert interviews.},
  keywords={Knowledge graphs;Visualization;Alzheimer's disease;Accuracy;Data visualization;Large language models;Electronic mail;Human-AI interactions;knowledge graph;conversational agent;large language model;progressive visualization},
  doi={10.1109/TVCG.2024.3456364},
  ISSN={1941-0506},
  month={Jan},}@ARTICLE{9207885,
  author={Mendsaikhan, Otgonpurev and Hasegawa, Hirokazu and Yamaguchi, Yukiko and Shimada, Hajime},
  journal={IEEE Access}, 
  title={Quantifying the Significance and Relevance of Cyber-Security Text Through Textual Similarity and Cyber-Security Knowledge Graph}, 
  year={2020},
  volume={8},
  number={},
  pages={177041-177052},
  abstract={In order to proactively mitigate cyber-security risks, security analysts have to continuously monitor sources of threat information. However, the sheer amount of textual information that needs to be processed is overwhelming, and it requires a great deal of mundane labor to separate the threats from the noise. We propose a novel approach to represent the relevance and significance of the cyber-security text in quantitative numbers. We trained custom Named Entity Recognition (NER) model and constructed a Cyber-security Knowledge Graph (CKG) to infer the subjective relevance of the cyber-security text to the user and to generate correlation features. In addition, the significance of the given text was analyzed in terms of its textual similarity with different repositories of pre-defined “significant” text and the maximum similarities were computed. These analysis results then act as features of the classifier to generate the significance score. The experimental result showed that the overall system could determine the significance and relevance of the text within a controlled environment with 88% accuracy.},
  keywords={Computer security;Data mining;Task analysis;Autonomous systems;Tagging;Computational modeling;Cyber-security knowledge graph;cyber threat;text analysis;textual similarity},
  doi={10.1109/ACCESS.2020.3027321},
  ISSN={2169-3536},
  month={},}@ARTICLE{10058174,
  author={Xu, Liang and Chen, Tao and Hou, Zhaoxiang and Zhang, Weishan and Hon, Chitin and Wang, Xiao and Wang, Di and Chen, Long and Zhu, Wenyin and Tian, Yunlong and Ning, Huansheng and Wang, Fei-Yue},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Knowledge Graph-Based Reinforcement Federated Learning for Chinese Question and Answering}, 
  year={2024},
  volume={11},
  number={1},
  pages={1035-1045},
  abstract={Knowledge question and answering (Q&A) is widely used. However, most existing semantic parsing methods in Q&A usually use cascading, which can incur error accumulation. In addition, using only one institution’s Q&A data definitely will limit the Q&A performance, while data privacy prevents sharing between institutions. This article proposes a knowledge graph-based reinforcement federated learning (KGRFL)-based Q&A approach to address these challenges. We design an end-to-end multitask semantic parsing model [MSP-bidirectional and auto-regressive transformers (BART)] that identifies question categories while converting questions into SPARQL statements to improve semantic parsing. Meanwhile, a reinforcement learning (RL)-based model fusion strategy is proposed to improve the effectiveness of federated learning, which enables multi-institution joint modeling and data privacy protection using cross-domain knowledge. In particular, it also reduces the negative impact of low-quality clients on the global model. Furthermore, a prompt learning-based entity disambiguation method is proposed to address the semantic ambiguity problem because of joint modeling. The experiments show that the proposed method performs well on different datasets. The Q&A results of the proposed approach outperform the approach of using only a single institution. Experiments also demonstrate that the proposed approach is resilient to security attacks, which is required for real applications.},
  keywords={Data models;Semantics;Federated learning;Knowledge graphs;Data privacy;Transformers;Multitasking;Question answering (information retrieval);Reinforcement learning;Knowledge graph;multitask semantic parsing [MSP-bidirectional and auto-regressive transformers (BART)];prompt learning;question and answering (Q&A);reinforcement federated learning (RFL)},
  doi={10.1109/TCSS.2023.3246795},
  ISSN={2329-924X},
  month={Feb},}@INPROCEEDINGS{10299526,
  author={Vuong, Thi-Hai-Yen and Hoang, Minh-Quan and Nguyen, Tan-Minh and Nguyen, Hoang-Trung and Nguyen, Ha-Thanh},
  booktitle={2023 15th International Conference on Knowledge and Systems Engineering (KSE)}, 
  title={Constructing a Knowledge Graph for Vietnamese Legal Cases with Heterogeneous Graphs}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={We develop a novel approach to construct a knowledge graph encompassing legal case documents and relevant legislation to improve legal information organization and retrieval. Our method involves data collection, entity extraction, and graph construction using natural language processing techniques. The constructed heterogeneous graph connects courts, cases, domains, and laws, significantly enriching information provided by retrieval systems. Our approach demonstrates potential in case analysis, legal recommendations, and decision support, providing valuable insights and resources for the legal domain.},
  keywords={Knowledge engineering;Law;Refining;Decision making;Legislation;Knowledge graphs;Organizations;knowledge graph;legal case documents;relevant law identification;heterogeneous graph;unsupervised learning},
  doi={10.1109/KSE59128.2023.10299526},
  ISSN={2694-4804},
  month={Oct},}@INPROCEEDINGS{9345000,
  author={Zhang, Jie and Pei, Zhongmin and Xiong, Wei and Luo, Zhangkai},
  booktitle={2020 IEEE 6th International Conference on Computer and Communications (ICCC)}, 
  title={Answer Extraction with Graph Attention Network for Knowledge Graph Question Answering}, 
  year={2020},
  volume={},
  number={},
  pages={1645-1650},
  abstract={In the knowledge graph question answering, the graph neural network can be used to encode the subgraph nodes related to the question entity to select the correct answer node. However, existing researches mainly focus on the modalities for the node encoding with graph neural network, ignoring that different types of subgraphs have different requirements for encoding information. To overcome the problem, this paper divides the subgraph into two types: the searching graph and the extending graph. Then we propose an answer extraction method with graph attention network for the searching graph, which can weight the information of neighbor nodes with different attention instead of the average. The hierarchical attention is also introduced to integrate question information into the subgraph node embedding to obtain the node presentation with question dependency. The accuracy of 48.2% is achieved on the CommonsenseQA dataset, which is much higher than the random guess (20%). In addition, the accuracy of the simplified model with no hierarchical attention decreases by 3.5%, which indicates the hierarchical attention mechanism can improve the predictive performance of the proposed model.},
  keywords={Computational modeling;Predictive models;Knowledge discovery;Search problems;Graph neural networks;Encoding;Task analysis;knowledge graph question answering;answer extraction;the searching graph;graph attention network;hierarchical attention},
  doi={10.1109/ICCC51575.2020.9345000},
  ISSN={},
  month={Dec},}@ARTICLE{10210018,
  author={Wang, Lihu and Liu, Xuemei and Liu, Yang and Li, Hairui and Liu, Jiaqi and Yang, Libo},
  journal={IEEE Access}, 
  title={Knowledge Graph-Based Method for Intelligent Generation of Emergency Plans for Water Conservancy Projects}, 
  year={2023},
  volume={11},
  number={},
  pages={84414-84429},
  abstract={In response to the issues of poor content correlation and insufficient intelligent decision support in emergency plans for water conservancy projects, a method for intelligent generation of emergency plans based on knowledge graphs is proposed. Utilizing pre-trained language models (PTM) based on entity masking, the accuracy of entity recognition tasks is enhanced by uncovering contextual features surrounding the masked entities. By employing translations, rotations, and superpositions within the vector space, a multiview convolutional neural network (MCNN) is constructed to enhance the accuracy of relation extraction through complementary and integrated feature representation. Integrating PTM with MCNN enables the construction of an emergency entity relationship extraction method based on PTM-MCNN. Neo4j is utilized for storing entity relationship triplets to construct an emergency knowledge graph. Through the utilization of the mutual information criterion, knowledge retrieval and matching are performed to accomplish the intelligent generation of emergency plans. The results indicate that PTM-MCNN achieves high recognition accuracy (F1 score of 92.2%), ensuring the reliability of the generated emergency plans. Related studies can effectively improve the intelligence of emergency management of water conservancy projects.},
  keywords={Transformers;Feature extraction;Knowledge graphs;Computational modeling;Water conservation;Task analysis;Bit error rate;Emergency services;Water conservation;Convolutional neural networks;Large-scale systems;Emergency plan;knowledge graph;water conservancy projects;convolutional neural network;large-scale pre-trained model},
  doi={10.1109/ACCESS.2023.3302399},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9667720,
  author={Li, Weichen and Abels, Patrick and Ahmadi, Zahra and Burkhardt, Sophie and Schiller, Benjamin and Gurevych, Iryna and Kramer, Stefan},
  booktitle={2021 IEEE International Conference on Big Knowledge (ICBK)}, 
  title={Topic-Guided Knowledge Graph Construction for Argument Mining}, 
  year={2021},
  volume={},
  number={},
  pages={315-322},
  abstract={Decision-making tasks usually follow five steps: identifying the problem, collecting data, extracting evidence, iden-tifying arguments, and making the decision. This paper focuses on two steps of decision-making: extracting evidence by building knowledge graphs (KGs) of specialized topics and identifying sentences' arguments through sentence-level argument mining. We present a hybrid model that combines topic modeling using latent Dirichlet allocation (LDA) and word embeddings to obtain external knowledge from structured and unstructured data. We use a topic model to extract topic- and sentence-specific evidence from the structured knowledge base Wikidata. A knowledge graph is constructed based on the cosine similarity between the entity word vectors of Wikidata and the vector of the given sentence. A second graph based on topic-specific articles found via Google supplements the general incompleteness of the structured knowledge base. Combining these graphs, we obtain a graph-based model that, as our evaluation shows, successfully capitalizes on both structured and unstructured data.},
  keywords={Conferences;Knowledge based systems;Decision making;Data models;Internet;Data mining;Resource management;Topic model;knowledge graph;argument mining},
  doi={10.1109/ICKG52313.2021.00049},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10350103,
  author={Parniani, Mohammad Sahand and Reformat, Marek Z.},
  booktitle={2023 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)}, 
  title={Triple Extraction with Generative Technique for Constructing Weighted Knowledge Graph}, 
  year={2023},
  volume={},
  number={},
  pages={127-134},
  abstract={Extracting relational facts from unstructured text is crucial in natural language processing used in many applications, particularly in constructing knowledge graphs. Relational facts are represented as triples in which two entities are connected through a relation. This work introduces a new and effective end-to-end method to generate triples from the input text. In the proposed method, we develop an encoder-decoder-based transformer model and warm-start both the encoder and decoder with pretrained checkpoints that are publicly accessible. These checkpoints can be taken from models such as BERT, GPT-2, and RoBERTa. Experimental results show that our method achieves better results for triple extraction on publicly available datasets (NYT and WebNLG) than the other state-of-the-art techniques. Further, the extracted triples are processed and used to build a knowledge graph. Complete control of this process allows for determining the weights of the relations (triples). The weights reflect the frequency of occurrences of facts represented by the relations and provide the degree of confidence in the facts.},
  keywords={Process control;Knowledge graphs;Transformers;Natural language processing;Decoding;Data mining;Intelligent agents;triple extraction;knowledge graph;encoder-decoder;transformer},
  doi={10.1109/WI-IAT59888.2023.00023},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10628284,
  author={Vergara-Vargas, Jeisson and Sadou, Salah and Tibermacine, Chouki and Restrepo-Calle, Felipe},
  booktitle={2024 IEEE 21st International Conference on Software Architecture Companion (ICSA-C)}, 
  title={Sarch-Checks: A Method for Checking Software Architecture Security Properties Using a Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={135-142},
  abstract={Checking the security properties of a software system during design is essential to enable the construction of a foundationally secure system. However, combining design tasks with security checks leads to a difficult and error-prone activity. This paper presents a checking method for security properties, called Sarch-Checks. This method allows analyzing the context of architectural elements in terms of an expected security property and identifying the presence of countermeasures and vulnerabilities. It uses an architectural description of the system to be analyzed, through the use of a modeling language. It also uses a knowledge graph, modeled and built from the elements of the software architecture, and cybersecurity elements taken from official information sources such as NIST and MITRE. This solution is an aide to the architect to design more secure architectures. Additionally, a validation process of the proposed method is presented through a case study based on a real report of a vulnerability in an open-source software system.},
  keywords={Analytical models;Software architecture;Knowledge graphs;Computer architecture;NIST;Software systems;Security;Security property;software architecture description;checking method;knowledge graph},
  doi={10.1109/ICSA-C63560.2024.00030},
  ISSN={2768-4288},
  month={June},}@INPROCEEDINGS{9667690,
  author={Wang, Yin and Xia, Nan and Luo, Xiangfeng and Li, Jinhui},
  booktitle={2021 IEEE International Conference on Big Knowledge (ICBK)}, 
  title={Global Semantics with Boundary Constraint Knowledge Graph for Chinese Financial Event Detection}, 
  year={2021},
  volume={},
  number={},
  pages={281-289},
  abstract={Chinese financial event detection has a great significance in the application of financial risk analysis, en-terprise management and decision-making. The existing tasks of Chinese event detection are mainly regarded as character-based or word-based classification, which suffers from the ambiguity of trigger words. These tasks only concentrate on local information (e.g character and word), which loses sight of global information like sentence semantics. Furthermore, in the finance field, there exists the problem of fuzzy boundary between different event types. In this paper, we propose a global semantics with boundary constraint knowledge graph (BCKG) for Chinese financial event detection, which considers both sentence semantics and boundary knowledge. At first, Chinese financial dataset (CFD) is constructed by considering the complexity in financial area. And then, the sentence seman-tics embedding is obtained by pre-training BERT fine-tuning mechanism to address the problem of ambiguity of trigger words, which considers both syntactic information and context sentence semantics comprehensively. Finally, we construct the BCKG for financial event, which can add additional prior knowledge to solve fuzzy boundary problem. The proposed method for event detection achieves outstanding performance on standard ACE 2005 Chinese dataset and constructed CFD. The experimental results demonstrate the effectiveness of the proposed method.},
  keywords={Event detection;Conferences;Semantics;Bit error rate;Decision making;Finance;Syntactics;event detection;global semantics;boundary con-straint knowledge graph;Chinese financial dataset},
  doi={10.1109/ICKG52313.2021.00045},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10723870,
  author={Parmar, Darshna and Nagrani, Tanish and Babre, Rushabh and Mazumdar, Pramit},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={From Conversations to Knowledge: Enriching Movie Datasets with Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Knowledge graphs are invaluable assets for facilitating the dissemination of structured and semantically enriched data to both humans and machines while ensuring its accuracy and reliability. However, it has been observed that a direct approach of generating knowledge graph from existing benchmark data sources such as ReDial, DBPedia, etc. fails to highlight many crucial relationships between entities. In this work we proposed a strategy to enrich a knowledge graph by resolving multiple entity relationships generated from the ReDial movie dataset. First the missing entities and their corresponding relationships were identified. Subsequently, the publicly available large TMDb dataset is leveraged to align additional information such as genre, director, actor, writer, and year as nodes. Bidirectional links are then introduced to semantically enrich the knowledge graph. Validating a knowledge graph is considered as another important step. In this direction, we extract triplets in (movie, relationship, genre) format from the knowledge graph. The triplets are validated using IMDb which is a vast repository of movie details, and the Gemini Generative AI model. We compute accuracy of the extracted triplets and the results indicate improvement in the knowledge graph generation strategy and the IMDb as an efficient validator for our approach.},
  keywords={Accuracy;Soft sensors;Semantics;Knowledge graphs;Oral communication;Named entity recognition;Motion pictures;Resource description framework;Reliability;Recommender systems;Knowledge Graph;TMDb;ReDial;Named Entity Recognition;IMDb;Gemini;Fact Validation},
  doi={10.1109/ICCCNT61001.2024.10723870},
  ISSN={2473-7674},
  month={June},}@ARTICLE{10817607,
  author={Kim, Wooyoung and Jung, Haemin and Kim, Wooju},
  journal={IEEE Access}, 
  title={Knowledge Graph as Pre-Training Corpus for Structural Reasoning via Multi-Hop Linearization}, 
  year={2025},
  volume={13},
  number={},
  pages={7273-7283},
  abstract={Large language models have demonstrated exceptional performance across various natural language processing tasks. However, their reliance on unstructured text corpora for pre-training limits their effectiveness in tasks requiring structured reasoning such as multi-hop question-answering. Knowledge Graphs provide a rich, structured source of relational data, offering an opportunity to enhance the reasoning capabilities of Large language models. In this paper, we propose a novel framework, Knowledge Graph as Pre-training Corpus (KGPC), which transforms knowledge graphs into text using a multi-hop linearization process. Unlike existing approaches that linearize singular triples, our method captures the interconnected nature of knowledge graphs by linking multiple triples across multiple hops, preserving their relational structure during the pre-training phase. This structured knowledge injection improves language models to perform complex reasoning tasks. We evaluate our approach on multi-hop reasoning benchmarks, demonstrating significant performance gains over existing models, particularly in question-answering tasks. Our results highlight the potential of multi-hop linearization in enhancing the structural reasoning capacity of language models, reducing error propagation, and improving the integration of structured knowledge into language models.},
  keywords={Cognition;Knowledge graphs;Training;Data models;Periodic structures;Large language models;Accuracy;Natural language processing;Knowledge based systems;Electronic mail;Large language model;knowledge graph;multi-hop reasoning;question-answering},
  doi={10.1109/ACCESS.2024.3523579},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10800729,
  author={Lee, Chang-Shing and Wang, Mei-Hui and Tseng, Guan-Ying and Yue, Chao-Cyuan and Hsieh, Hao-Chun and Reformat, Marek},
  booktitle={2024 27th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)}, 
  title={Cao Robot for Taiwanese/English Knowledge Graph Application}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper proposes a Content Attention Ontology (CAO) robot for constructing Taiwanese/English Knowledge Graphs (KGs) by prompting audio or texts to Large Language Models (LLMs), including TAIDE, Zephyr, and Llama 3.1. The collected data includes lecture videos from the IEEE WCCI 2024 in Japan and the 2024 National Language Development Forum in Taiwan, along with students' learning data from the 2024 Summer School on Taiwanese/English Human and Robot Co-Learning at Rende Elementary School (RDES). In addition, the fundamental concepts of Computational Intelligence (CI) and Quantum CI (QCI) learning were incorporated into the study. The generative KGs highlight important concepts, relations, and communities within the collected teaching and learning data. Additionally, we utilized data from subjects wearing braincomputer interface (BCI) devices while speaking Taiwanese/English to generate KGs. We also compared the differences in these KGs and analyzed the similarities between the transcribed texts of lectures and learners. In the future, we plan to expand the CAO robot to more validation fields across Taiwan, aiming to engage young students in speaking Taiwanese while concurrently enhancing their English language skills through interaction with the robot.},
  keywords={Measurement;Quantum computing;Statistical analysis;Large language models;Knowledge graphs;Speech enhancement;Ontologies;Physiology;Robots;Videos;CAO Robot;Knowledge Graph;Taiwanese/English Language Co-learning;Large Language Model;Llama 3.1;TAIDE},
  doi={10.1109/O-COCOSDA64382.2024.10800729},
  ISSN={2472-7695},
  month={Oct},}@INPROCEEDINGS{9166221,
  author={Alamsyah, Andry and Bastikarana, Rafa Syafiq and Ramadhanti, Alya Rysda and Widiyanesti, Sri},
  booktitle={2020 8th International Conference on Information and Communication Technology (ICoICT)}, 
  title={Recognizing Personality from Social Media Linguistic Cues: A Case Study of Brand Ambassador Personality}, 
  year={2020},
  volume={},
  number={},
  pages={1-5},
  abstract={The burgeoning need of a brand ambassador (BA) as a company representative begin to rise in recent year. The phenomena followed by the increase of method to select the most suitable BA. The universal way of selecting one appropriate ambassador is by understanding their personality, therefore, measurement of a BA personality considered as one way to characterize a company credibility. This research proposes to design a method of measuring the BA personality from their social media data in Bahasa Indonesia. We enrich the methodology to measure human personality using the ontology modeling approach. The ontology model constructed under the ngram language model which provides a rapid and effective way of measuring a BA personality. The results of a BA personality measurement allow the utilization to portray of how an ambassador represent their brand and interact with their customer.},
  keywords={Brand Ambassador;Personality Measurement;Ontology},
  doi={10.1109/ICoICT49345.2020.9166221},
  ISSN={},
  month={June},}@INPROCEEDINGS{8816987,
  author={Oba, Atsushi and Paik, Incheon},
  booktitle={2019 IEEE International Conference on Cognitive Computing (ICCC)}, 
  title={Extraction of Taxonomic Relation of Complex Terms by Recurrent Neural Network}, 
  year={2019},
  volume={},
  number={},
  pages={70-72},
  abstract={In recent years, while the Internet has brought various technological evolutions, a lot of ontology is required to organize and systemize knowledge, and its generation is necessary. Especially, classification of hypernym-hyponym relation which describes taxonomy of ontology has received a lot of attention. As a method to automate the generation, word embedding based method was proposed recently. Although the method enabled high accuracy classification by using semantics, it does not correspond to complex term consisting of multiple words. Based on this background, in this paper, we proposed a new model combined word embedding and Recurrent Neural Network(RNN), evaluated the classification performance with data extracted from WordNet. For the result, it is indicated that the RNN approach is more effective and general for ontology generation.},
  keywords={Ontologies;Data models;Recurrent neural networks;Semantics;Training;Taxonomy;Support vector machines;Ontological Classification;Word Embedding;Word2Vector;Recurrent Neural Network;Natural Language Processing;Recurrent Neural Network Language Model},
  doi={10.1109/ICCC.2019.00024},
  ISSN={},
  month={July},}@INPROCEEDINGS{8569238,
  author={Helmke, Hartmut and Slotty, Michael and Poiger, Michael and Herrer, Damián Ferrer and Ohneiser, Oliver and Vink, Nathan and Cerna, Aneta and Hartikainen, Petri and Josefsson, Billy and Langr, David and Lasheras, Raquel García and Marin, Gabriela and Mevatne, Odd Georg and Moos, Sylvain and Nilsson, Mats N. and Pérez, Mario Boyero},
  booktitle={2018 IEEE/AIAA 37th Digital Avionics Systems Conference (DASC)}, 
  title={Ontology for Transcription of ATC Speech Commands of SESAR 2020 Solution PJ.16-04}, 
  year={2018},
  volume={},
  number={},
  pages={1-10},
  abstract={Nowadays Automatic Speech Recognition (ASR) applications are increasingly successful in the air traffic (ATC) domain. Paramount to achieving this is collecting enough data for speech recognition model training. Thousands of hours of ATC communication are recorded every day. However, the transcription of these data sets is resource intense, i.e. writing down the sequence of spoken words, and more importantly, interpreting the relevant semantics. Many different approaches including CPDLC (Controller Pilot Data Link Communications) currently exist in the ATC community for command transcription, a fact that e.g. complicates exchange of transcriptions. The partners of the SESAR funded solution PJ.16-04 are currently developing on a common ontology for transcription of controller-pilot communications, which will harmonize integration of ASR into controller working positions. The resulting ontology is presented in this paper.},
  keywords={Speech recognition;Training;Ontologies;Standards;Radar;Aircraft;Strips;Automatic Speech Recognition (ASR);CWP HMI;Transcription;Controller Command;Ontology;SESAR;PJ.16-04},
  doi={10.1109/DASC.2018.8569238},
  ISSN={2155-7209},
  month={Sep.},}@INPROCEEDINGS{9194550,
  author={Yu, Houjin and Mao, Xian-Ling and Chi, Zewen and Wei, Wei and Huang, Heyan},
  booktitle={2020 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={A Robust and Domain-Adaptive Approach for Low-Resource Named Entity Recognition}, 
  year={2020},
  volume={},
  number={},
  pages={297-304},
  abstract={Recently, it has attracted much attention to build reliable named entity recognition (NER) systems using limited annotated data. Nearly all existing works heavily rely on domain-specific resources, such as external lexicons and knowledge bases. However, such domain-specific resources are often not available, meanwhile it’s difficult and expensive to construct the resources, which has become a key obstacle to wider adoption. To tackle the problem, in this work, we propose a novel robust and domain-adaptive approach RDANER for low-resource NER, which only uses cheap and easily obtainable resources. Extensive experiments on three benchmark datasets demonstrate that our approach achieves the best performance when only using cheap and easily obtainable resources, and delivers competitive results against state-of-the-art methods which use difficultly obtainable domainspecific resources. All our code and corpora can be found on https://github.com/houking-can/RDANER.},
  keywords={Task analysis;Knowledge based systems;Data models;Bit error rate;Learning systems;Training;Biological system modeling;named entity recognition;low resource;domain adaptive},
  doi={10.1109/ICBK50248.2020.00050},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10412813,
  author={Yang, Ze and Sun, Yimeng and Nakaguchi, Takao and Imai, Masaharu},
  booktitle={2023 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={EMoDi: Entity-Enhanced Momentum-Difference Contrastive Learning for Semantic-Aware Verification of Scientific Information}, 
  year={2023},
  volume={},
  number={},
  pages={142-151},
  abstract={This paper proposes the EMoDi system to improve the performance of the entire scientific information verification pipeline. First, the Momentum-Difference contrastive learning framework is introduced to capture more semantics information. In abstract retrieval, entity-enhancement and noise-ignoration are introduced to improve the ability to retrieve relevant abstracts more accurately. In addition, a two-step verification method is used in label prediction to improve the label prediction ability and reduce the false positive rate of the “NOT ENOUGH INFO” label. The proposed pipeline outperforms the baseline VERISCI and QMUL-SDS. The code of this system is available on GitHub.},
  keywords={Codes;Pipelines;Semantics;Self-supervised learning;Knowledge graphs;Software development management;scientific information verification;entity-enhancement;noise-ignoration;two-step prediction;contrastive learning},
  doi={10.1109/ICKG59574.2023.00023},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10485759,
  author={Zhu, Ruiliang and Song, Xiangshuai and Zhang, Hao and Cai, Xuli},
  booktitle={2024 IEEE 3rd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)}, 
  title={Joint Extraction of Entity Relationships in Walnut Disease and Pest Based on Chinese NLP Models}, 
  year={2024},
  volume={},
  number={},
  pages={1027-1035},
  abstract={This study addresses the limited application of deep learning techniques in the field of walnut disease and pest and the challenges posed by complex relationships and diverse entity types in this domain. We propose a deep learning-based method for constructing a knowledge graph in the walnut disease and pest domain, incorporating ontologies to establish a conceptual model for the disease and pest knowledge graph. To overcome issues such as relationship overlap (e.g., one-to-many, many-to-many) and loss of relationship chains, we introduce a novel labeling scheme called “based on ontology binding BIESO (Begin-Inside-End-Single-Other)” that directly models triplets. By employing a label matching algorithm, we obtain triplet data. We train and predict on the dataset using an end-to-end model consisting of Bidirectional Encoder Representations from Transformers (BERT), Bi-directional Gate Recurrent Unit (BiGRU), and Conditional Random Field (CRF). Experimental results show an F1 score of 75.79%, outperforming models such as BERT-BiLSTM-CRF and word2vec-BiGRU-CRF. We semiautomatically extract unstructured knowledge and store the extracted triplets in a Neo4j graph database, enabling visualization of the knowledge. The research methodology of this knowledge graph can serve as a reference for constructing knowledge graphs in walnut agriculture and developing intelligent question-answering systems for walnut disease and pest.},
  keywords={Knowledge graphs;Bidirectional control;Ontologies;Predictive models;Logic gates;Transformers;Prediction algorithms;Walnut disease and pest;ontology;BERTBiGRU-CRF;knowledge graph;deep learning},
  doi={10.1109/EEBDA60612.2024.10485759},
  ISSN={},
  month={Feb},}@ARTICLE{10210700,
  author={Qu, Xiaoye and Gu, Yingjie and Xia, Qingrong and Li, Zechang and Wang, Zhefeng and Huai, Baoxing},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Arabic Named Entity Recognition: Past, Recent Advances, and Future Trends}, 
  year={2024},
  volume={36},
  number={3},
  pages={943-959},
  abstract={As more and more Arabic texts emerged on the Internet, extracting important information from these Arabic texts is especially useful. As a fundamental technology, Named entity recognition (NER) serves as the core component in information extraction technology, while also playing a critical role in many other Natural Language Processing (NLP) systems, such as question answering and knowledge graph building. In this paper, we provide a comprehensive review of the development of Arabic NER, especially the recent advances in deep learning and pre-trained language model. Specifically, we first introduce the background of Arabic NER, including the characteristics of Arabic and existing resources for Arabic NER. Then, we systematically review the development of Arabic NER methods. Traditional Arabic NER systems focus on feature engineering and designing domain-specific rules. In recent years, deep learning methods achieve significant progress by representing texts via continuous vector representations. With the growth of pre-trained language model, Arabic NER yields better performance. Finally, we conclude the method gap between Arabic NER and NER methods from other languages, which helps outline future directions for Arabic NER.},
  keywords={Surveys;Text recognition;Task analysis;Deep learning;Social networking (online);Focusing;Writing;Named entity recognition;arabic texts;deep learning;pretrained language model},
  doi={10.1109/TKDE.2023.3303136},
  ISSN={1558-2191},
  month={March},}@INPROCEEDINGS{10385760,
  author={Shuai, Yunyan and Wang, Wenkang and Li, Yiming and Zeng, Min and Li, Min},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Protein function prediction using graph neural network with multi-type biological knowledge}, 
  year={2023},
  volume={},
  number={},
  pages={30-35},
  abstract={Proteins play crucial roles in diverse biological functions, and accurately annotating their functions is essential for understanding cellular mechanisms and developing therapies for complex diseases. Computational methods have been proposed as alternatives to laborious experimental approaches. However, existing network-based methods focus on the protein-protein interaction (PPI) networks, while the proteins without interactions are ignored. To address this limitation, we propose a novel deep learning framework for protein function prediction, named PFP-GMB, which incorporates multi-type biological knowledge to consider the proteins not present in the PPI networks. PFP-GMB leverages a pre-trained protein language model to extract sequence representations. Moreover, PPIs and orthology relationships are used to generate functional related features via graph neural networks and attention mechanisms. Finally, these multi-type features are fused for protein function prediction. Compared to eight state-of-the-art methods, PFP-GMB outperforms all of them in terms of F-max and AUPR. The ablation studies further confirm the relevance and significance of the multi-type biological knowledge incorporated into PFP-GMB for protein function prediction.},
  keywords={Proteins;Knowledge engineering;Protein engineering;Medical treatment;Feature extraction;Graph neural networks;Diseases;protein function;orthology network;PPI network;protein sequence;graph neural network},
  doi={10.1109/BIBM58861.2023.10385760},
  ISSN={2156-1133},
  month={Dec},}@ARTICLE{10474025,
  author={Farghaly, Maha and Mounir, Mahmoud and Aref, Mostafa and Moussa, Sherin M.},
  journal={IEEE Access}, 
  title={Investigating the Challenges and Prospects of Construction Models for Dynamic Knowledge Graphs}, 
  year={2024},
  volume={12},
  number={},
  pages={40973-40988},
  abstract={Recently, Dynamic knowledge graphs (DKGs) have been considered the foundation stone for several powerful knowledge-aware applications. DKG has a great advancement over static knowledge graph with the ability to capture the dynamicity of knowledge. The correctness and completeness of DKGs strongly affect the accuracy of the dependent application, in which many factors may have an impact, including data sources, graph construction model, and evaluation methods. Despite the increasing attention to DKGs, the literature of DKG construction is not comprehensively investigated, and the limitations are not fully revealed. In this paper, a comparative study is conducted for the emerging construction models of DKG. An extensive analysis is provided for each of the three main phases of DKG construction: entity extraction, relationship extraction and graph completion. For the different phases, we investigated the employed techniques, the used data sources, as well as the associated challenges, limitations, and evaluation metrics of each model. The learning approach is introduced as a novel categorization perspective for the employed techniques in the DKG construction. Finally, the encountered challenges and limitations are inspected to deduce the possible future directions that can be adopted for effective and advanced DKGs construction. It was found that 100% of the investigated models lack the key aspects of dynamicity in DKGs, 75% suffer from insufficient training features, 58% have a clear exposure to bias, 33% are vulnerable to changes, 25% have a performance and efficiency concerns, while lack of evaluation and comparison represented 25% and 17% of the models respectively.},
  keywords={Data mining;Feature extraction;Knowledge graphs;Soft sensors;Data models;Internet;Task analysis;Dynamic knowledge graph;learning approaches;knowledge graph completion;knowledge graph construction},
  doi={10.1109/ACCESS.2024.3378514},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10373245,
  author={Sheng, Jinghua},
  booktitle={2023 16th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, 
  title={An Augmentable Domain-specific Models for Financial Analysis}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Large-scale language models such as GPT-4 have revolutionized data analysis and interpretation by generating human-like text, automating insights, and detecting data errors. Large-scale language models have been applied in various fields and played an important role in many aspects. Large language models can also perform financial and technical analysis by cleaning data, generating synthetic data, handling bias, and supporting natural language queries. This paper proposes a language model that integrates multimodal data with external knowledge bases and domain-specific data, enhancing its reasoning ability by extending domain-specific data. Reduce hallucinations and fine-tune domain-specific data by incorporating external knowledge bases to deepen model understanding of industry-specific language, concepts, and context. And technologies such as knowledge graph, attention mechanism, cross-modal embedding and federated collaborative training are used to deal with the challenges of different structures and semantics of multi-modal data. The model also employs a feedback loop mechanism to allow the model to adapt to changing conditions, such as changing languages or new domain information. Experimental results show that the proposed model has a preliminary domain-specific ability to analyze and predict multimodal financial and technical data.},
  keywords={Training;Analytical models;Adaptation models;Feedback loop;Biological system modeling;Knowledge based systems;Natural languages;Large-scale language models;financial and technical analysis;external knowledge;domain-specific data;knowledge graph},
  doi={10.1109/CISP-BMEI60920.2023.10373245},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10218646,
  author={Chen, Binghong and Chen, Jenhui},
  booktitle={2023 IEEE 5th Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS)}, 
  title={Inclusion-Exclusion Knowledge Filtering Approach for Conversation-Based Preliminary Diagnosis}, 
  year={2023},
  volume={},
  number={},
  pages={170-174},
  abstract={Using natural language processing (NLP) techniques, we conducted a preliminary diagnosis of the disease from the patient syndrome description. Because patients are not medical professionals, they cannot accurately describe all symptoms. To solve this issue, we build a medical knowledge graph (KG) by constructing symptom-disease relation triples for pre-processing the patient syndrome description. According to the medical KG, the descriptions were reconstructed into KG embedding representation. To avoid the knowledge noise issue, we investigate an inclusion-exclusion knowledge filtering approach (IKFA) for symptom-to-disease triples to load them to a pretrained language model (PLM), i.e., bidirectional encoder representations from Transformers (BERT). To train the IKFA, we built a medical diagnosis question-answer dataset (MDQA dataset), which contains large-scale and high-quality questions (patient symptom description) and answers (diagnosis) (Q&A) corpus with 1.63 million entries in the size of 213 MB. The KG was built based on 8,731 diseases with detailed syndrome descriptions in the size of 1.98 MB. The experimental results showed that the IKFA preliminarily diagnosed 8,731 different diseases based on the patient's initial symptom description with an accuracy of 0.9894.},
  keywords={Training;Filtering;Bidirectional control;Transformers;Natural language processing;Encoding;Medical diagnosis;diagnosis;disease;knowledge graph;NLP;PLM;symptom;syndrome},
  doi={10.1109/ECBIOS57802.2023.10218646},
  ISSN={},
  month={June},}@INPROCEEDINGS{10628465,
  author={Lian, Xiaoli and Ma, Jieping and Lv, Heyang and Zhang, Li},
  booktitle={2024 IEEE 32nd International Requirements Engineering Conference (RE)}, 
  title={ReqCompletion: Domain-Enhanced Automatic Completion for Software Requirements}, 
  year={2024},
  volume={},
  number={},
  pages={142-154},
  abstract={Software requirements are the driving force behind software development. As the cornerstone of the entire software lifecycle, the efficiency of crafting requirement specifications and the quality of these requirements significantly influence the duration of software development. Despite massive research on requirements elicitation, the reality is that requirements are often painstakingly crafted manually, word by word. This manual process is not only time-consuming but also prone to issues such as the misuse of terminology. To address these challenges, we introduce ReqCompletion, an approach designed to recommend the next token in real-time for given prefix of requirements description. ReqCompletion comprises two primary components. First, we have devised and integrated a knowledge-injection module into GPT-2—which stands as the largest available GPT model that allows for fine-tuning on specialized downstream tasks. This injection imbues GPT-2 with richer domain-specific knowledge, thus improving the relevance of the suggested tokens. Additionally, we employ a pointer network to optimize the recommendation quality by utilizing completed requirements as contextual support. Empirical evaluations using two public datasets demonstrate that ReqCompletion surpasses all baselines in performance (Recall@7 gains up to 65.87% than the second-best model). Furthermore, the effectiveness of its two pivotal design elements has been substantiated through rigorous ablation studies. The utility of our work has been evaluated preliminarily through a small user study.},
  keywords={Terminology;Force;Manuals;Computer architecture;Benchmark testing;Software;Real-time systems;Software Requirements;Automatic Text Completion;Knowledge Injection},
  doi={10.1109/RE59067.2024.00023},
  ISSN={2332-6441},
  month={June},}@INPROCEEDINGS{8595067,
  author={Singh, Neeraj Kumar and Ait-Ameur, Yamine and Mery, Dominique},
  booktitle={2018 23rd International Conference on Engineering of Complex Computer Systems (ICECCS)}, 
  title={Formal Ontology Driven Model Refactoring}, 
  year={2018},
  volume={},
  number={},
  pages={136-145},
  abstract={Refactoring, successfully used in the field of programming, can be used in maintenance and restructuring of the large and complex models. In this paper, we present a novel approach for model refactoring and a set of modelling patterns that are applicable for refinement-based formal development. In order to carry out this study, we investigate the previously developed large and complex model and required ontology to develop a domain model and a refactored system model. Further, we use the Rodin tools to check the internal consistency with respect to the desired functional behaviour and the required safety properties. Our main contributions are: to develop a refactoring technique related to the correct by construction approach; to use the domain specific knowledge in a system model explicitly; to define a set of modelling patterns; and to define a restructuring mechanism in the formal development. Finally, this proposed approach is evaluated through a complex medical case study: ECG clinical assessment protocol.},
  keywords={Ontologies;Computational modeling;Safety;Semantics;Tools;Electrocardiography;Analytical models;Refactoring, refinement and proofs, ontologies, do main theories, Event-B},
  doi={10.1109/ICECCS2018.2018.00022},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9874511,
  author={Faramarzi, Noushin Salek and Dara, Akanksha and Banerjee, Ritwik},
  booktitle={2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)}, 
  title={Combining Attention-based Models with the MeSH Ontology for Semantic Textual Similarity in Clinical Notes}, 
  year={2022},
  volume={},
  number={},
  pages={74-83},
  abstract={In this study, we present several transformer-based models as well as traditional machine learning methods to detect semantic textual similarity (STS) in clinical notes. We investigate transformer models pretrained on general English as well as clinical notes, and use generic English STS datasets as a supplemental corpus to clinical notes data. Our work is based on the 2019 National NLP Clinical Challenge (n2c2). We identify and annotate six types of sentences in the clinical notes corpus, and report an ensemble method that combines attention-based contextualized embeddings with a similarity score based on the MeSH ontology obtained by computing least common ancestors of clinical terms. Our approach does not need additional clinical data for model training, while still achieving comparable Pearson's correlation coefficient of 0.901.},
  keywords={Training;Drugs;Vocabulary;Computational modeling;Semantics;Machine learning;Ontologies;Electronic Health Records;Natural Language Processing;Clinical Semantic Textual Similarity;Transformers;MeSH},
  doi={10.1109/ICHI54592.2022.00023},
  ISSN={2575-2634},
  month={June},}@INPROCEEDINGS{9926710,
  author={Ji, Fan and Ocker, Felix and Zou, Minjie and Vogel-Heuser, Birgit and Oligschläger, Marius},
  booktitle={2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)}, 
  title={Identifying Inconsistencies in the Design of Large-scale Casting Systems – An Ontology-based Approach}, 
  year={2022},
  volume={},
  number={},
  pages={319-325},
  abstract={The development of modern automated production systems requires the close cooperation of engineers from different domains. Due to the large amount of domain-specific documents and heterogeneous data they create during the multidisciplinary engineering activities, ensuring the consistency of information is always challenging. Since most of these documents are texted-based and lack a standardized structure, extracting required information from these files is oftentimes problematic. This issue is particularly critical in the development of large-scale production plants due to the high complexity of the systems and the diversity of disciplines involved. To help engineers efficiently utilize unstructured data sources as well as identify potential information contradictions, we propose an ontology-based inconsistency management approach for large-scale production systems that generates the knowledge base from unstructured engineering data and (semi-) automatically detects multiple types of inconsistencies. In addition, the presented framework also supports the tracking of information changes during the system design process.},
  keywords={Production systems;Casting;Computer aided software engineering;Automation;Soft sensors;Knowledge based systems;Complexity theory},
  doi={10.1109/CASE49997.2022.9926710},
  ISSN={2161-8089},
  month={Aug},}@ARTICLE{9940925,
  author={Borrego, Agustín and Dessì, Danilo and Hernández, Inma and Osborne, Francesco and Reforgiato Recupero, Diego and Ruiz, David and Buscaldi, Davide and Motta, Enrico},
  journal={IEEE Access}, 
  title={Completing Scientific Facts in Knowledge Graphs of Research Concepts}, 
  year={2022},
  volume={10},
  number={},
  pages={125867-125880},
  abstract={In the last few years, we have witnessed the emergence of several knowledge graphs that explicitly describe research knowledge with the aim of enabling intelligent systems for supporting and accelerating the scientific process. These resources typically characterize a set of entities in this space (e.g., tasks, methods, evaluation techniques, proteins, chemicals), their relations, and the relevant actors (e.g., researchers, organizations) and documents (e.g., articles, books). However, they are usually very partial representations of the actual research knowledge and may miss several relevant facts. In this paper, we introduce SciCheck, a new triple classification approach for completing scientific statements in knowledge graphs. SciCheck was evaluated against other state-of-the-art approaches on seven benchmarks, yielding excellent results. Finally, we provide a real-world use case and applied SciCheck to the Artificial Intelligence Knowledge Graph (AI-KG), a large-scale automatically-generated open knowledge graph including 1.2M statements extracted from the 333K most cited articles in the field of Artificial Intelligence, and generated a new version of this knowledge graph with 300K additional triples.},
  keywords={Machine learning;Feature extraction;Semantic Web;Task analysis;Context modeling;Computational modeling;Benchmark testing;Knowledge based systems;Knowledge graphs;science of science;knowledge graph completion;triple classification;machine learning;semantic web},
  doi={10.1109/ACCESS.2022.3220241},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9018722,
  author={Qie, Yongiun and Zhu, Weijie and Liu, Aishan and Zhang, Yuchen and Wang, Jun and Li, Teng and Li, Yaqing and Ge, Yufei and Wang, Yufeng},
  booktitle={2018 IEEE CSAA Guidance, Navigation and Control Conference (CGNCC)}, 
  title={A Deep Learning Based Framework for Textual Requirement Analysis and Model Generation}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={Requirement analysis is a key part of systems engineering process. Analyzing requirements correctly and creating design model sequentially could be critical to the whole process of a product development. Nevertheless, requirement text handling and model transferring could be really time-consuming and error-prone. Thus, we proposed an artificial intelligence based framework to deal with textual requirement handling and model creation. With deep learning and natural language process skills, our approach could be able to analyze textual requirements automatically, and then create the related models. This would indeed alleviate the work of engineers and promote the efficiency and quality of product development process. With our limited knowledge, our paper is the first one to propose the deep learning and NLP based framework to automatically create requirement models.},
  keywords={Semantics;Computational modeling;Machine learning;Natural language processing;Air traffic control;Analytical models;requirement modeling;deep learning;NLP},
  doi={10.1109/GNCC42960.2018.9018722},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10825984,
  author={Linxen, Andrea and Schmidt, Vera-Maria and Klinke, Harald and Beecks, Christian},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Ontology-driven knowledge base for digital humanities: Restructuring knowledge organization at the library of the Folkwang University of the Arts}, 
  year={2024},
  volume={},
  number={},
  pages={2449-2455},
  abstract={Academic libraries are increasingly challenged by the need to efficiently manage and analyse vast collections of data and knowledge. The divers formats and organisation methods of these collections, ranging from traditional print media to digital archives and multimedia assets, can hinder researchers’ ability to easily access and retrieve relevant information. This paper introduces an ontology-driven knowledge base to address this issue by enabling the efficient access to knowledge in the application domain and enhancing the semantic search capabilities in the field of Digital Humanities. Our approach focuses on the development of an ontology-drive knowledge base for semantic search in academic libraries by the example of the library of the Folkwang University of Arts that captures the knowledge concepts present in the library’s archival collections. The resulting ontology framework provides a structured representation of domain knowledge, facilitating the integration of diverse data sources, including structured, semi-structured, and unstructured data from the application domain into a triple store knowledge base. By leveraging SPARQL queries generated from Large Language Model (LLM) prompts, we aim to facilitate more intuitive and effective knowledge retrieval. This approach allows users to express their information needs in a more natural and flexible way, leading to more accurate and relevant search results. We evaluate the proposed ontology-driven knowledge base in terms of its integrity, consistency, flexibility, relevance, and scalability. Our evaluation methodology includes a combination of verification and validation techniques, including automated reasoners and query results based on competence questions. Our findings demonstrate the potential of ontology engineering to enhance complex information retrieval in academic libraries. However, we also identify limitations related to processing speed for complex queries and the quality of search results. This research contributes to the field of computational archival science by providing a novel approach to semantic search in academic libraries. By enabling more precise and efficient access to knowledge, our ontology-driven knowledge base has the potential to enrich the academic and Digital Humanities landscape, empowering researchers to delve deeper into the vast resources available within these institutions.},
  keywords={Knowledge engineering;Art;Semantic search;Soft sensors;Scalability;Knowledge based systems;Organizations;Ontologies;Media;Libraries;knowledge engineering;ontology framework;digital humanities;knowledge base;semantic search},
  doi={10.1109/BigData62323.2024.10825984},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{9651799,
  author={Kroll, Hermann and Pirklbauer, Jan and Balke, Wolf-Tilo},
  booktitle={2021 ACM/IEEE Joint Conference on Digital Libraries (JCDL)}, 
  title={A Toolbox for the Nearly-Unsupervised Construction of Digital Library Knowledge Graphs}, 
  year={2021},
  volume={},
  number={},
  pages={21-30},
  abstract={Knowledge graphs are essential for digital libraries to store entity-centric knowledge. The applications of knowledge graphs range from summarizing entity information over answering complex queries to inferring new knowledge. Yet, building knowledge graphs means either relying on manual curation or designing supervised extraction processes to harvest knowledge from unstructured text. Obviously, both approaches are cost-intensive. Yet, the question is whether we can minimize the efforts to build a knowledge graph. And indeed, we propose a toolbox that provides methods to extract knowledge from arbitrary text. Our toolkit bypasses the need for supervision nearly completely and includes a novel algorithm to close the missing gaps. As a practical demonstration, we analyze our toolbox on established biomedical benchmarks. As far as we know, we are the first who propose, analyze and share a nearly unsupervised and complete toolbox for building knowledge graphs from text.},
  keywords={Training;Vocabulary;Biological system modeling;Training data;Benchmark testing;Libraries;Cleaning;Knowledge Graph;Information Extraction;Digital Library},
  doi={10.1109/JCDL52503.2021.00014},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{8926665,
  author={Jue, Wang and Song, Yineng and Wu, Xian and Dai, Wenbin},
  booktitle={IECON 2019 - 45th Annual Conference of the IEEE Industrial Electronics Society}, 
  title={A Semi-Formal Requirement Modeling Pattern for Designing Industrial Cyber-Physical Systems}, 
  year={2019},
  volume={1},
  number={},
  pages={2883-2888},
  abstract={Requirement engineering is a crucial part of the engineering process. The traditional methods of requirement engineering are time-consuming and human-centered. A well-established software requirement description model needs to ensure the accuracy and integrity of the transformation and is also hoped to be scalable, versatile, and efficient in transformation and transmission. This paper presents a method of requirement engineering, including constricted nature language requirement input pattern, and the formalized requirement description JSON model. This method provides convenience for requirement modification and validation that can satisfy the real-time constraints of industrial cyber-physical systems.},
  keywords={Unified modeling language;Software;Solid modeling;Requirements engineering;Analytical models;Real-time systems;requirement engineering;requirement component;formal models;system behavior decomposition},
  doi={10.1109/IECON.2019.8926665},
  ISSN={2577-1647},
  month={Oct},}@INPROCEEDINGS{10633526,
  author={Yang, Hang and Xiao, Liang and Zhu, Rujun and Liu, Ziji and Chen, Jianxia},
  booktitle={2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Interlinking Clinical Guidelines via Mining Medical Literature Knowledge for Multi-Morbidity Decision-Making}, 
  year={2024},
  volume={},
  number={},
  pages={1250-1255},
  abstract={Independently developed clinical guidelines present a systematic challenge in managing patients with multi-morbidity in a consistent and integrated manner. Existing approaches mainly focus on combining multiple guidelines and lack approaches that combine with additional medical resources. The correlations and conflicts between treatment plans in the management of multi-morbidity are well-documented in medical literature but are less explored in the Clinical Decision Support line of research. In this paper, we propose a literature-based guideline interlinking method to address these challenges through the integration of clinical guidelines and the harmonization of conflicting recommendations, thereby providing a more holistic and efficient way to manage patients with multi-morbidity conditions. This method employs an ontology model and knowledge graph technology to represent and analyze the complexity and interrelations of diseases, with the aim of transcending the limitations of traditional single disease guidelines and providing a holistic and integrated framework for multi-morbidity management. The objective is to construct a multi-morbidity knowledge graph by correlating medical literature with clinical guidelines and to provide optimal decision support for patients with multi-morbidity complications in a clinical decision support system (CDSS).},
  keywords={Accuracy;Systematics;Databases;Computational modeling;Biological system modeling;Knowledge graphs;Ontologies;multi-morbidity management;clinical guidelines;ontology model;knowledge graph},
  doi={10.1109/COMPSAC61105.2024.00165},
  ISSN={2836-3795},
  month={July},}@INPROCEEDINGS{10743159,
  author={Hong, Bin and Wu, Jinze and Liu, Jiayu and Ding, Liang and Sha, Jing and Zhang, Kai and Wang, Shijin and Huang, Zhenya},
  booktitle={2024 International Conference on Computational Linguistics and Natural Language Processing (CLNLP)}, 
  title={End-to-End Graph Flattening Method for Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={41-45},
  abstract={In recent years, the breakthrough of Large Language Models (LLMs) offers new ideas for achieving universal methods on graph data. The common practice of converting graphs into natural language for LLMs, which refers to graph flattening, exhibits good generalizability and interpretability. However, the poor organization of the textual format results in poor performance in long-distance scenario understanding. Inspired by human cognitive reasoning habits, we propose a novel method for graph flattening to fit LLMs, termed as End-to-End DAG-Path prompting (EEDP). Experiments on real-world datasets show that EEDP enhances the reasoning performance of LLMs in long-distance scenarios while maintaining excellent performance in short-distance scenarios, demonstrating good robustness in the face of distance variations.},
  keywords={Large language models;Organizations;Cognition;Robustness;Natural language processing;Computational linguistics;Faces;graph flattening;Large Language Model;graph representation},
  doi={10.1109/CLNLP64123.2024.00016},
  ISSN={},
  month={July},}@INPROCEEDINGS{10651190,
  author={Li, Jinlin and Wang, Zikang and Li, Linjing and Zeng, Daniel Dajun},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={BERT-FKGC: Text-Enhanced Few-Shot Representation Learning for Knowledge Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={In recent years, few-shot knowledge graph completion (FKGC) emerged as a prominent research problem, focused on utilizing a limited number of reference entity pairs to complete triples with unseen relations. Recent studies have attempted addressing this problem by modeling interactions between head and tail entities. However, existing FKGC methods represent semantics predominantly based on the neighborhood information of entities in the knowledge graph, thus can only infer the hidden and unobserved relations within the knowledge graph, limiting their reasoning capabilities. To overcome these limitations, we introduce text descriptions to FKGC and propose BERT-FKGC, a model capable of learning the integrated distribution of both the entity text descriptions and neighborhood information. By using a gating network that allows the model to dynamically select weights, our method can flexibly combine neighborhood information and textual descriptions. Besides addressing the prediction of unseen relations, our method is also capable of representing unseen entities. To validate the effectiveness of our model, we introduce a new dataset, FB15K-237-One, which includes textual descriptions for entities. We conduct extensive experiments on the FB15K-237-One dataset to validate the superiority of BERTFKGC.},
  keywords={Representation learning;Adaptation models;Limiting;Semantics;Neural networks;Knowledge graphs;Tail;few-shot learning;knowledge graph completion;pre-trained language model},
  doi={10.1109/IJCNN60899.2024.10651190},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10408849,
  author={Xin, You and Chen, Liu and Yang, Yang},
  booktitle={2023 IEEE 11th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={Online Knowledge Fusion Method for Fault Diagnosis of Power Plant Equipment}, 
  year={2023},
  volume={11},
  number={},
  pages={1236-1240},
  abstract={There are many types of documents in fossil-fuel power station to describe equipment failures, including maintenance records, treatment diagnosis suggestions, historical cases, and equipment knowledge. The knowledge of equipment anomaly diagnosis and handling is scattered in different documents. Extract and fuse scattered knowledge from these scattered documents to generate a knowledge graph for equipment fault di-agnosis, providing necessary decision support for maintenance personnel to discover and handle equipment faults. This article proposes an implementation method for extracting and integrating equipment fault knowledge from diverse and multi type text records to form a knowledge graph. A thermal power plant equipment fault Q&A system based on the fusion of open source large language models and knowledge graphs has been developed. Main contributions: (1) A knowledge extraction algorithm integrating BERT-WWM model and pointer annotation method is proposed to extract entity relations of fault text jointly. Experiments show that the method performs well in extracting overlapped triples, and F1 is improved by 8.51 % compared with existing algorithms; (2) A knowledge fusion model based on RoBERTa-BiLSTM is proposed, which fully utilizes the feature information of the entity text to be disambiguated and the entity mention text, and cap-tures the interdependent features within the sentence through attention mechanism. The experiment shows that this method improves F1 by 9.56% compared to existing fusion algorithms. (3) Based on the open-source large model ChatGLM, a fusion method of knowledge graph and large model ChatGLM was explored, and a device fault question answering system for thermal power plants was implemented, achieving high accuracy in practical applications.},
  keywords={Knowledge graphs;Maintenance engineering;Feature extraction;Question answering (information retrieval);Data mining;Personnel;Power generation;knowledge graph;knowledge extraction;knowledge fusion},
  doi={10.1109/ITAIC58329.2023.10408849},
  ISSN={2693-2865},
  month={Dec},}@ARTICLE{10423114,
  author={Zhao, Yingwen and Yang, Zhihao and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Predicting Protein Functions Based on Heterogeneous Graph Attention Technique}, 
  year={2024},
  volume={28},
  number={4},
  pages={2408-2415},
  abstract={In bioinformatics, protein function prediction stands as a fundamental area of research and plays a crucial role in addressing various biological challenges, such as the identification of potential targets for drug discovery and the elucidation of disease mechanisms. However, known functional annotation databases usually provide positive experimental annotations that proteins carry out a given function, and rarely record negative experimental annotations that proteins do not carry out a given function. Therefore, existing computational methods based on deep learning models focus on these positive annotations for prediction and ignore these scarce but informative negative annotations, leading to an underestimation of precision. To address this issue, we introduce a deep learning method that utilizes a heterogeneous graph attention technique. The method first constructs a heterogeneous graph that covers the protein-protein interaction network, ontology structure, and positive and negative annotation information. Then, it learns embedding representations of proteins and ontology terms by using the heterogeneous graph attention technique. Finally, it leverages these learned representations to reconstruct the positive protein-term associations and score unobserved functional annotations. It can enhance the predictive performance by incorporating these known limited negative annotations into the constructed heterogeneous graph. Experimental results on three species (i.e., Human, Mouse, and Arabidopsis) demonstrate that our method can achieve better performance in predicting new protein annotations than state-of-the-art methods.},
  keywords={Proteins;Protein engineering;Annotations;Feature extraction;Predictive models;Deep learning;Amino acids;Protein function prediction;positive and negative annotations;constructed heterogeneous graph;heterogeneous graph attention},
  doi={10.1109/JBHI.2024.3357834},
  ISSN={2168-2208},
  month={April},}@INPROCEEDINGS{9994899,
  author={Huang, Zhijian and Zheng, Rongtao and Deng, Lei},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={DeepFusionGO: Protein function prediction by fusing heterogeneous features through deep learning}, 
  year={2022},
  volume={},
  number={},
  pages={12-17},
  abstract={Exploring the functions of proteins is crucial for explaining cellular mechanisms, treating diseases, and developing new drugs. Due to experimental limitations, large-scale identification of protein function remains a challenging task in cell biology. Here we propose DeepFusionGo, a novel protein function prediction method that adopts a graph representation learning approach (GraphSAGE) to extract features from heterogeneous data sources. First, we generate embeddings from protein sequences using the pre-trained protein language model and InterPro domains with scaling gradient. Then we integrate these two embeddings with adaptive feature weights to the PPI graph and use GraphSAGE to generate the representation vector. Finally, we build the classification model to predict protein function based on the concatenated feature vector. The experimental results show that DeepFusionGO outperforms existing state-of-the-art methods, including sequence-based DeepGOPLUS, and PPI-based DeepGraphGO. DeepFusionGO also performs well in difficult protein function prediction. We demonstrate that selecting an appropriate protein features fusion method can improve the prediction performance, and using the PPI network and the protein representation vector obtained from the protein language model through the GraphSAGE algorithm is an effective way to mine potential functional clues. The source code and data sets are available at: https://github.com/Hhhzj-7/DeepFusionGO.},
  keywords={Proteins;Representation learning;Adaptation models;Biological system modeling;Source coding;Soft sensors;Predictive models;Protein function prediction;graph representation learning;GraphSAGE;feature fusion},
  doi={10.1109/BIBM55620.2022.9994899},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10795847,
  author={Dong, Haomin and Wang, Wenbin and Sun, Zhenjiang and Kang, Ziyi and Ge, Xiaojun},
  booktitle={2024 4th International Conference on Computer Systems (ICCS)}, 
  title={Iterative LLM Prompting for Intelligent Cockpit Knowledge Graph Construction}, 
  year={2024},
  volume={},
  number={},
  pages={136-145},
  abstract={With the development of digital and intelligent cockpits, the data within these environments exhibit complexity and diversity, leading to issues such as inefficient data processing and difficulty in information extraction. Consequently, effectively capturing and representing the hidden associative knowledge in cockpits is crucial. Against this backdrop, knowledge graphs serve as an effective tool capable of retrieving and organizing a vast amount of information within a connected and interpretable structure. By systematically representing complex data relationships in the cockpit, they help enhance the prediction of precise interaction intentions and provide richer and more relevant knowledge support for personalized recommendations. However, rapidly and flexibly generating domain-specific knowledge graphs still poses certain challenges. This paper introduces an innovative method of knowledge graph construction using generative large language models, implementing a novel iterative zero-shot and domain-agnostic strategy. We propose an innovative strategy of iteratively prompting large language models to extract relevant triples for constructing knowledge graphs, effectively addressing key challenges such as entity recognition ambiguity and relationship extraction complexity in cockpit data. Experiments conducted in a domain-specific dataset demonstrate the feasibility of this method.},
  keywords={Knowledge engineering;Large language models;Pipelines;Knowledge graphs;Information retrieval;Data processing;Complexity theory;Data mining;Iterative methods;Intelligent systems;knowledge graphs;triples;prompting engineering;LLMs;intelligent cockpit},
  doi={10.1109/ICCS62594.2024.10795847},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10650221,
  author={Wang, ZhenYu and Wu, YiFei},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Enhance Large Language Models for Multilingual Sentence Embedding with Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Sentence representation is a major challenge in natural language processing, especially in multilingual environments. Current approaches to sentence representation using Large Language Models (LLMs) often require large amounts of data for fine-tuning, and research has focused on English content. In addition, comparative datasets translated directly from English can contain many semantic and syntactic errors. To address these issues, we propose a new approach to enhance multilingual sentence embeddings using LLMs and knowledge graphs. We first present a dedicated designed prompt that exploits in-context learning of LLMs for sentence embedding without fine-tuning. We further introduce an innovative method that utilizes knowledge graphs, such as Wikidata, for generating diverse multilingual training data for contrastive finetuning. This approach significantly reduces the reliance on translated sentences and mitigates issues related to translation accuracy. Furthermore, we develop a unique multilingual contrastive learning loss function, which, when combined with QLora’s efficient fine-tuning technique, enables LLMs to achieve state-of-the-art performance in Sentence Text Similarity (STS) tasks, even with limited computational resources.},
  keywords={Large language models;Computational modeling;Semantics;Neural networks;Training data;Knowledge graphs;Contrastive learning;sentence embedding;contrastive learning;large language model;data argumentation},
  doi={10.1109/IJCNN60899.2024.10650221},
  ISSN={2161-4407},
  month={June},}@ARTICLE{10604878,
  author={Guo, Yike},
  journal={IEEE Access}, 
  title={Design of Improved Artificial Intelligence Generative Dialogue Algorithm and Dialogue System Model Based on Knowledge Graph}, 
  year={2024},
  volume={12},
  number={},
  pages={102637-102648},
  abstract={Dialogue systems are an important research direction in artificial intelligence, with broad application prospects and market value. In order to improve system efficiency and user satisfaction, an open domain generative dialogue system integrating knowledge graphs has been developed, which facilitates the utilization of rich background knowledge during dialogue generation, thereby generating more coherent and meaningful dialogue content. At the same time, based on the sequence to sequence model, a bidirectional gated loop unit is introduced to better capture contextual information and improve the model’s understanding and generation ability. These results confirmed that the average values of the improved model in the training and validation sets were 98.66% and 87.34%, respectively, with loss values of 0.01 and 0.10. Compared to the baseline model, this improved model improved Hits@1 and Hits@3 by 0.09% and 0.25%, respectively. This improved model had the minimum perplexity of 17.62. The security and diversity of this improved system were 0.80 and 0.82, respectively, taking into account the balance of these two types of performance. Its correlation and fluency were 1.44 and 1.56, respectively. This indicates that this improved model is beneficial for improving the efficiency of generating dialogue and has certain effectiveness, better meeting users’ needs and improve user satisfaction. This system can provide users with a better conversation experience and provide technological and innovative features for artificial intelligence dialogue assistants.},
  keywords={Data mining;Accuracy;Oral communication;Knowledge based systems;Training;Task analysis;Data models;Knowledge graphs;Bidirectional control;Dialogue system;generative;knowledge graph;Seq2Seq model;bidirectional GRU},
  doi={10.1109/ACCESS.2024.3430902},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10538914,
  author={Jinhua, Du and Hao, Yin},
  booktitle={2023 IEEE 22nd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)}, 
  title={KLDP:A Data Profiling Technique Based on Knowledge Graph and Large Language Modeling}, 
  year={2023},
  volume={},
  number={},
  pages={2333-2340},
  abstract={The explosive growth of medical data has perfected the establishment of patients' personal health records and provided favorable conditions for smart healthcare, but its fragmentation also brings challenges to patient management. Mainstream research focuses on utilizing medical data to construct disease knowledge graphs to assist patient management, but does not effectively manage massive patient data. In order to make full use of patient data and facilitate the circulation of patient data elements, we propose a new patient sketching technique, KLDP. it constructs knowledge graphs through pre-training techniques, effectively manages patient data based on patients' personal health records and medical history information throughout the treatment cycle, and elementalizes patient data, providing new ideas and implementation solutions for patient management.},
  keywords={Data privacy;Computational modeling;Knowledge graphs;Medical services;Data models;Explosives;Security;China electronic medical record;knowledge graph;data elements;patient sketch;deep learning;pre-trained model;natural language processing;healthcare informatization},
  doi={10.1109/TrustCom60117.2023.00329},
  ISSN={2324-9013},
  month={Nov},}@INPROCEEDINGS{10858965,
  author={Ou, Lu and Ni, Xiaoya and Wu, Wei and Tian, Zhihong},
  booktitle={2024 IEEE 9th International Conference on Data Science in Cyberspace (DSC)}, 
  title={CyGPT: Knowledge Graph-Based Enhancement Techniques for Large Language Models in Cybersecurity}, 
  year={2024},
  volume={},
  number={},
  pages={216-223},
  abstract={Large Language Models (LLMs) excel in numerous Natural Language Processing (NLP) tasks but encounter significant challenges in practical applications, including hallucinations, outdated information, and a lack of domain-specific external knowledge. This study proposes a collaborative, training-free reasoning approach, leveraging close cooperation between Knowledge Graphs (KG) and LLMs for cybersecurity applications. Our approach employs the ‘Joint Reasoning Chain,’ which dynamically integrates information from network security-specific knowledge graphs, serving as an external knowledge base to enhance the domain-specific external knowledge of LLMs. This cooperative method not only improves reliable knowledge-based reasoning but also enhances the traceability of decision-making processes. Furthermore, we introduce a novel GPT-based technique to evaluate answer quality and have performed systematic experiments on a purpose-built test set. The results confirm that our method significantly boosts GPT’s performance in network security knowledge, demonstrating the potential of knowledge graphs to augment LLMs’ reasoning abilities and their applicability in specialized fields.},
  keywords={Knowledge engineering;Systematics;Large language models;Knowledge based systems;Knowledge graphs;Network security;Cognition;Natural language processing;Data models;Computer security;Large Language Model;Knowledge Graph;Retrieval-Augmented Generation;Cybersecurity},
  doi={10.1109/DSC63484.2024.00036},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9053975,
  author={Lai, Tuan Manh and Hung Tran, Quan and Bui, Trung and Kihara, Daisuke},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A Simple But Effective Bert Model for Dialog State Tracking on Resource-Limited Systems}, 
  year={2020},
  volume={},
  number={},
  pages={8034-8038},
  abstract={In a task-oriented dialog system, the goal of dialog state tracking (DST) is to monitor the state of the conversation from the dialog history. Recently, many deep learning based methods have been proposed for the task. Despite their impressive performance, current neural architectures for DST are typically heavily-engineered and conceptually complex, making it difficult to implement, debug, and maintain them in a production setting. In this work, we propose a simple but effective DST model based on BERT. In addition to its simplicity, our approach also has a number of other advantages: (a) the number of parameters does not grow with the ontology size (b) the model can operate in situations where the domain ontology may change dynamically. Experimental results demonstrate that our BERT-based model outperforms previous methods by a large margin, achieving new state-of-the-art results on the standard WoZ 2.0 dataset 1. Finally, to make the model small and fast enough for resource-restricted systems, we apply the knowledge distillation method to compress our model. The final compressed model achieves comparable results with the original model while being 8x smaller and 7x faster.},
  keywords={Bit error rate;Production;Ontologies;Signal processing;Task analysis;Speech processing;Standards;Task-Oriented Dialog Systems;Dialog State Tracking;BERT;Knowledge Distillation},
  doi={10.1109/ICASSP40776.2020.9053975},
  ISSN={2379-190X},
  month={May},}@INPROCEEDINGS{10387600,
  author={Procko, Tyler Thomas and Elvira, Timothy and Ochoa, Omar},
  booktitle={2023 Fifth International Conference on Transdisciplinary AI (TransAI)}, 
  title={GPT-4: A Stochastic Parrot or Ontological Craftsman? Discovering Implicit Knowledge Structures in Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={147-154},
  abstract={Ontologies are representational artifacts that purport to accurately portray the aspect of reality under the purview of the ontologists laboring upon them. Ontologies exist in a spectrum of formality, from lexical thesauri to knowledge graphs, to collections of statements of first-order logic. The recent proliferation of Large Language Models (LLMs) has brought to bear interactive “knowledge bases” with general awareness of most things. As ontologists create ontologies from their understanding of reality; and as LLMs, presumably, possess some “understanding” of reality, embedded in their vector matrices corresponding to lexical terms from massive quantities of learned texts, a question is posed: what form of ontology can an LLM create when prompted about some novel facet of reality, without explicitly asking it for an ontology? I.e., will an LLM categorize things into bins, or a subsumption hierarchy, or perhaps something else? LLMs, as they are understood, respond when prompted with the most likely response, because they are predictors of next tokens, i.e., they are stochastic parrots. In any case, it is posited that, if prompted without any explicit request for an ontology, an LLM can produce an ontology of novel form, effectively granting insight into the “understanding” an LLM has of the world, as all humans possess an understanding of the world that ontologies are based upon. This paper explores the use of the flagship LLM, GPT-4, in forming an ontology of a novel domain.},
  keywords={Visualization;Taxonomy;Natural languages;Supervised learning;OWL;Stochastic processes;Organizations;ontology;taxonomy;large language models;GPT},
  doi={10.1109/TransAI60598.2023.00043},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9736309,
  author={Burgdorf, Andreas and Paulus, Alexander and Pomp, André and Meisen, Tobias},
  booktitle={2022 IEEE 16th International Conference on Semantic Computing (ICSC)}, 
  title={DocSemMap: Leveraging Textual Data Documentations for Mapping Structured Data Sets into Knowledge Graphs}, 
  year={2022},
  volume={},
  number={},
  pages={209-216},
  abstract={Today, knowledge graphs have been proven to enable the efficient integration of heterogeneous data sets. An important step in creating such knowledge graphs is the mapping of the attributes of a data set to the knowledge graph's ontology. So far, numerous methods have been developed to support this mapping process by using both the schema information as well as the actual data values from a data set in conjunction with external knowledge bases or machine learning approaches. A third source of information, namely textual data documentations, has not yet been considered. In this paper, we present DocSemMap, a novel approach that utilizes textual data documentations of data sets as an additional source for the creation of semantic mappings. We train custom embeddings on the textual data documentations. Further, we utilize pre-trained embeddings that allow us to identify similarities between excerpts of the textual data documentations and descriptions of ontological concepts. Based on this, we build candidate sets of the best suitable concepts for mapping and finally use weighted similarity scores to identify the best fitting concept for each attribute of a data set. The evaluation of our approach outperforms existing approaches for semantic mapping but still has potential for improvement.},
  keywords={Semantics;Natural languages;Knowledge based systems;Fitting;Documentation;Machine learning;Syntactics;semantic mapping;knowledge graph construction;natural language processing;textual data documentation},
  doi={10.1109/ICSC52841.2022.00042},
  ISSN={2325-6516},
  month={Jan},}@INPROCEEDINGS{10553629,
  author={Melzer, Sylvia and Weilkiens, Tim and Muggeo, Christian and Berres, Axel},
  booktitle={2024 IEEE International Systems Conference (SysCon)}, 
  title={Sustainable Development of Information Systems Using SysML, FAS and DOL}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={The use of product families can improve the efficiency of product development as opposed to develop a single-product by reusing existing artefacts and optimizing variability, which leads to a saving of resources and is therefore a sustainable approach. To extend new variants of an already modelled product via the approach of a product family the challenge is to merge the product models, so that the same and varying parts in the model are semantically correct identified and mapped to each other. This paper proposes a comprehensive approach for sustainable development of product families using the Distributed Ontology Language (DOL), Functional Architectures for Systems (FAS), and Systems Modeling Language (SysML). The proposed approach integrates the idea of DOL to represent an ontology of sustainability criteria and to map them to the functional architecture of a product family. FAS is used to model the functional architecture of the product family, while the FAS ontology is used to formalize the functional architecture and provide a standardized vocabulary and set of rules for modelling the functional aspects of the product family. SysML is used to model the product family. The proposed method is demonstrated through a case study of developing a sustainable product family of vacuum cleaner robots. The results show that the proposed method can help to identify opportunities for reducing environ-mental impact and improving social responsibility in the product family design while ensuring that functional requirements and design constraints are met semantically correct.},
  keywords={Vocabulary;Biological system modeling;Ontologies;Systems Modeling Language;Product development;Product design;Sustainable development;sustainability;functional architectures for systems;Distributed Ontology Language;product family;SysML},
  doi={10.1109/SysCon61195.2024.10553629},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{10628803,
  author={Liu, Hao and Zhou, Shuxin and Chen, Zhehuan and Perl, Yehoshua and Wang, Jiayin},
  booktitle={2024 IEEE 12th International Conference on Healthcare Informatics (ICHI)}, 
  title={Using Generative Large Language Models for Hierarchical Relationship Prediction in Medical Ontologies}, 
  year={2024},
  volume={},
  number={},
  pages={248-256},
  abstract={This study extends the exploration of ontology enrichment by evaluating the performance of various open-sourced Large Language Models (LLMs) on the task of predicting hierarchical relationships (IS-A) in medical ontologies including SNOMED CT Clinical Finding and Procedure hierarchies and the human Disease Ontology. With the previous finetuned BERT models for hierarchical relationship prediction as the baseline, we assessed eight open-source generative LLMs for the same task. We observed only three models, without finetuning, demonstrated comparable or superior performance compared to the baseline BERT -based models. The best performance model OpenChat achieved a macro average F1 score of 0.96 (0.95) on SNOMED CT Clinical Finding (Procedure) hierarchy, an increase over 7% from the baseline 0.89 (0.85). On human Disease Ontology, OpenChat excels with an F1 score of 0.91, outperforming the second-best performance model Vicuna (0.84). Notably, some LLMs prove unsuitable for hierarchical relationship prediction tasks or appliable for concept placement of medical ontologies. We also explored various prompt templates and ensemble techniques to uncover potential confounding factors in applying LLMs for IS-A relation predictions for medical ontologies.},
  keywords={Accuracy;Large language models;Medical services;Ontologies;Predictive models;Task analysis;Informatics;Hieratical Relation Prediction;Large Language Models;Medical Ontology;Prompts Design;SNOMED CT},
  doi={10.1109/ICHI61247.2024.00040},
  ISSN={2575-2634},
  month={June},}@ARTICLE{8246712,
  author={Hafeez Khan, Abdul and Hyder Abbas Musavi, Sayed and Rehman, Aqeel-Ur and Shaikh, Asadullah},
  journal={IEEE Access}, 
  title={Ontology-Based Finite Satisfiability of UML Class Model}, 
  year={2018},
  volume={6},
  number={},
  pages={3040-3050},
  abstract={Software models are core artifacts in model driven engineering (MDE) and processable by computer. They are automatically transformed into other models and in MDE, programming code is also produced by the models. The automatic transformation provides a systematic reuse of existing artifacts. However, sometimes models are developed with defects and the defects can implicitly shift into the code, which may be difficult to discover and repair. A promising solution to this problem is model verification. UML class model is a key ingredient of MDE. However, UML only offers graphical components without the support of reasoning, due to lack of the formal foundation. Therefore, the verification of formal properties, such as consistency and finite satisfiability is not possible in UML. This paper proposes an ontology-based optimized verification method for important correctness property “finite satisfiability”of UML class model.},
  keywords={Unified modeling language;Ontologies;Computational modeling;Object oriented modeling;Software;Business;Semantics;Finite satisfiability;model satisfiability;ontology-based satisfiability;model checking;model verification},
  doi={10.1109/ACCESS.2017.2786781},
  ISSN={2169-3536},
  month={},}@ARTICLE{10214033,
  author={Wang, Quanxiu and Cao, Xinlei and Wang, Jianyong and Zhang, Wei},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Knowledge-Aware Collaborative Filtering With Pre-Trained Language Model for Personalized Review-Based Rating Prediction}, 
  year={2024},
  volume={36},
  number={3},
  pages={1170-1182},
  abstract={Personalized review-based rating prediction aims at leveraging existing reviews to model user interests and item characteristics for rating prediction. Most of the existing studies mainly encounter two issues. First, the rich knowledge contained in the fine-grained aspects of each review and the knowledge graph is rarely considered to complement the pure text for better modeling user-item interactions. Second, the power of pre-trained language models is not carefully studied for personalized review-based rating prediction. To address these issues, we propose an approach named Knowledge-aware Collaborative Filtering with Pre-trained Language Model (KCF-PLM). For the first issue, to utilize rich knowledge, KCF-PLM develops a transformer network to model the interactions of the extracted aspects w.r.t. a user-item pair. For the second issue, to better represent users and items, KCF-PLM takes all the historical reviews of a user or an item as input to pre-trained language models. Moreover, KCF-PLM integrates the transformer network and the pre-trained language models through representation propagation on the knowledge graph and user-item guided attention of the aspect representations. Thus KCF-PLM combines review text, aspect, knowledge graph, and pre-trained language models together for review-based rating prediction. We conduct comprehensive experiments on several public datasets, demonstrating the effectiveness of KCF-PLM.},
  keywords={Predictive models;Task analysis;Transformers;Knowledge graphs;Microstrip;Fabrics;Image color analysis;Review-based rating prediction;pre-trained language model;collaborative filtering},
  doi={10.1109/TKDE.2023.3301884},
  ISSN={1558-2191},
  month={March},}@INPROCEEDINGS{10750089,
  author={Diaz, Dany and Moreno-Vera, Felipe and Heredia, Juanpablo and Venturim, Fabrício and Poco, Jorge},
  booktitle={2024 IEEE Visual Analytics Science and Technology VAST Challenge}, 
  title={FishBiasLens: Integrating Large Language Models and Visual Analytics for Bias Detection}, 
  year={2024},
  volume={},
  number={},
  pages={17-18},
  abstract={Identifying unreliable sources is crucial for preventing misinformation and making informed decisions. CatchNet, the Oceanus Knowledge Graph, contains biased perspectives that threaten its credibility. We use Large Language Models (LLMs) and interactive visualization systems to identify these biases. By analyzing police reports and using GPT-3.5 to extract information from articles, we establish the ground truth for our analysis. Our visual analytics system detects anomalies, revealing unreliable news sources such as The News Buoy and biased analysts such as Harvey Janus and Junior Shurdlu.},
  keywords={Law enforcement;Visual analytics;Large language models;Soft sensors;Knowledge graphs;Real-time systems;Data mining;Fake news;GPT;Visualization;Knowledge Graph;Bias detection;LLM},
  doi={10.1109/VASTChallenge64683.2024.00013},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9861086,
  author={Morine, Melissa J. and Priami, Corrado and Coronado, Edith and Haber, Juliana and Kaput, Jim},
  booktitle={2022 IEEE International Conference on Digital Health (ICDH)}, 
  title={A Comprehensive and Holistic Health Database}, 
  year={2022},
  volume={},
  number={},
  pages={202-207},
  abstract={Health and the initiation, progression, and outcome of disease are the result of multiple environmental factors interacting with individual genetic makeups. Collectively, results from primary clinical research on health and disease represent the most compendious and reliable source of actionable knowledge on strategies to optimize health. However, the dispersal of this information as unstructured data, distributed across millions of documents, is a substantial challenge in bridging the gap between primary research and concrete recommendations for improving health. Described here is the development and implementation of a machine reading pipeline that builds a knowledge graph of causal relationships between a broad range of predictive/modifiable diet and lifestyle factors and health outcomes, extracted from the vast biomedical corpus in the National Library of Medicine.},
  keywords={Text mining;Systematics;Pipelines;Semantics;Genetics;Libraries;Environmental factors;Healthware;knowledge graphs;natural language processing},
  doi={10.1109/ICDH55609.2022.00039},
  ISSN={},
  month={July},}@INPROCEEDINGS{10169047,
  author={Xu, Shiyu and Song, Hui and Wu, Renchang and Shi, Junwei},
  booktitle={2023 2nd Asia Conference on Electrical, Power and Computer Engineering (EPCE)}, 
  title={A Natural Language Understanding Model Based on Encoding Fusion For Power Marketing Indicator Answering}, 
  year={2023},
  volume={},
  number={},
  pages={13-17},
  abstract={Accurate understanding of user questions is the core of a domain oriented task oriented dialogue system. To apply the Natural Language Understanding Model (NLU) to power marketing indicator Q&A, the first is to define the NLU task schema based on domain background knowledge, and manually annotate a training dataset for model training. Due to the lack of historical conversation data, manually organizing problem and annotating is labor-intensive. Insufficient sample size affects the performance of the model. We further propose an approach to improve the end-to-end NLU model with marketing domain triple knowledge, which provide rich contextual information for the slot representation. During the NLU model coding stage, the representation of entity relationships is incorporated into the token coding, enhancing the model's understanding of domain terms that do not appear in the samples. Practice has shown that introducing domain knowledge do make up for the lack of training samples and significantly improve the accuracy of slot value recognition.},
  keywords={Training;Databases;Semantics;Training data;Oral communication;Encoding;Natural language processing;Power Marketing;Natural Language Understanding;Encoding Fusion;Knowledge Graph},
  doi={10.1109/EPCE58798.2023.00011},
  ISSN={},
  month={April},}@INPROCEEDINGS{10207605,
  author={Li, Jingyi and Chen, Qi and Wang, Wei and Wu, Fangyu},
  booktitle={2023 IEEE International Conference on Smart Computing (SMARTCOMP)}, 
  title={Knowledge-embedded Prompt Learning for Zero-shot Social Media Text Classification}, 
  year={2023},
  volume={},
  number={},
  pages={222-224},
  abstract={Social media plays an irreplaceable role in shaping the way information is created shared and consumed. While it provides access to a vast amount of data, extracting and analyzing useful insights from complex and dynamic social media data can be challenging. Deep learning models have shown promise in social media analysis tasks, but such models require a massive amount of labelled data which is usually unavailable in real-world settings. Additionally, these models lack common-sense knowledge which can limit their ability to generate comprehensive results. To address these challenges, we propose a knowledge-embedded prompt learning model for zero-shot social media text classification. Our experimental results on four social media datasets demonstrate that our proposed approach outperforms other well-known baselines.},
  keywords={Deep learning;Analytical models;Data analysis;Social networking (online);Computational modeling;Text categorization;Media;Zero-shot text classification;prompt learning;knowledge graph embedding;social media data analysis},
  doi={10.1109/SMARTCOMP58114.2023.00054},
  ISSN={2693-8340},
  month={June},}@INPROCEEDINGS{9378066,
  author={Stojanov, Riste and Kocev, Ilija and Gramatikov, Sasho and Popovski, Gorjan and Koroušić Seljak, Barbara and Eftimov, Tome},
  booktitle={2020 IEEE International Conference on Big Data (Big Data)}, 
  title={Toward Robust Food Ontology Mapping}, 
  year={2020},
  volume={},
  number={},
  pages={3596-3601},
  abstract={Data normalization methodologies are extremely welcome to link extracted information from textual data to different semantic resources. These methodologies have been previously well researched especially in the biomedical domain, where health concepts were normalized and described using semantic tags. Recently, a methodology for normalizing food concepts has been proposed, based on Named-Entity Recognition methods resulting in the FoodOntoMap semantic resource. In this paper, we propose and evaluate a new architecture for linking phrases (i.e. textual name for foods) to concepts from semantic resources in the Food and Nutrition domain. We represent the food phrases (i.e. their textual name) in continuous vector space using state-of-the-art Natural Language Processing (NLP) embedding algorithms, and evaluate their proximity with respect to the annotated semantic food concepts. Additionally, indexing was incorporated to improve efficiency.The GloVe embedding with mean pooling provided best evaluation results, with maximum recall of 74% for the Snomed CT semantic dataset, which is promising result, but also opens a space for future improvement of the phrase representations, and their incorporation in this system.},
  keywords={Semantics;Taxonomy;Ontologies;Big Data;Syntactics;Natural language processing;Indexing;Natural Language Processing;Text representation;Embeddings;Data normalization and linking},
  doi={10.1109/BigData50022.2020.9378066},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9750154,
  author={Zhang, Jian and Qin, Bo and Zhang, Yufei and Zhou, Junhua and Wang, Hongwei},
  booktitle={2021 IEEE International Conference on e-Business Engineering (ICEBE)}, 
  title={A Framework for Effective Knowledge Extraction from A Data Space Formed by Unstructured Technical Reports using Pre-trained Models}, 
  year={2021},
  volume={},
  number={},
  pages={120-125},
  abstract={The transformation of unstructured data into triples is a key task in knowledge graph construction. It remains a great challenge to complete this task on technical reports. In this work, we propose a framework for effectively structuring data structuring in knowledge graph construction from a data space formed by technical reports. This framework specifically consist of two pre-trained language models to provide the embed dings and a sequence labeling model to tag the entity labels. The pre-trained models, i.e. the Flair embedding and the BERT model, are employed to combine the output vectors to downstream tasks. To evaluate the proposed method, we conduct named entity recognition experiments using the status reports of complex equipment in nuclear power plants. The evaluation shows the framework achieves remarkable improvement on F1 score. This paper details the framework, the experiments, and the evaluation of the proposed method.},
  keywords={Knowledge engineering;Conferences;Bit error rate;Writing;Data models;Labeling;Data mining;knowledge graph construction;data space;pre-trained language model;named entity recognition},
  doi={10.1109/ICEBE52470.2021.00028},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10339738,
  author={Stoyanov, Stanimir and Kumurdjieva, Milena and Tabakova-Komsalova, Veneta and Doukovska, Lyubka},
  booktitle={2023 International Conference on Big Data, Knowledge and Control Systems Engineering (BdKCSE)}, 
  title={Using LLMs in Cyber-Physical Systems for Agriculture - ZEMELA}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper presents the idea of developing an advisory service using the capabilities of generative artificial intelligence and in particular of Large Language Model. The service will assess the risks for farmers when preparing projects under different programs, taking into account the Bulgarian legislation related to agriculture, as well as the requirements of the relevant program. The results of a feasibility analysis are summarized in the article. Furthermore, two architectural approaches are discussed. The service will be integrated in the platform for smart agriculture named ZEMELA. A brief overview of this platform is also given in the article.},
  keywords={Smart agriculture;Knowledge engineering;Prototypes;Legislation;Cyber-physical systems;Big Data;Control systems;generative artificial intelligence;large language model;advisory service;smart agriculture},
  doi={10.1109/BdKCSE59280.2023.10339738},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10724185,
  author={Aryan and Thingbaijam, Lenin and Palle, Kowstubha and Prasad, P. Venkata and Mallala, Balasubbareddy and Patil, Shrishailappa},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={Incorporating Knowledge Graphs in Semantic Search}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Information Graphs are a powerful mechanism for incorporating dependent facts into semantic search. They are a form of graph database that represents standards and their relationships, as opposed to just information and their attributes. This additional layer of abstraction enables greater and a sophisticated inferencing and querying, making them perfect to be used in semantic search. Incorporating knowledge Graphs into semantic search entails numerous technical steps: 1. Information Extraction: The first step is to pick out and extract applicable facts from various resources, such as databases, text documents, and APIs. This fact can encompass entities, relationships, and their attributes. 2. Information Alignment: Once the records have been extracted, it needs to be aligned with the ideas in the knowledge graph. This includes mapping the extracted records to the suitable entities and their relationships in the graph. 3. Information Enrichment: To improve the first-class and completeness of the knowledge graph, the extracted statistics may additionally need to be enriched with additional statistics. This will involve incorporating information from external sources or leveraging device mastering techniques to deduce lacking facts. 4. Entity Disambiguation: Considering that entities in the understanding graph may additionally have comparable or ambiguous names, it is very much essential to disambiguate them to ensure correct search outcomes. This can be done through strategies inclusive of named entity recognition.},
  keywords={Knowledge engineering;Accuracy;Semantic search;Databases;Knowledge graphs;Named entity recognition;Information retrieval;Data mining;Standards;Knowledge graph;semantic search;multi-modal knowledge graph},
  doi={10.1109/ICCCNT61001.2024.10724185},
  ISSN={2473-7674},
  month={June},}@ARTICLE{10613766,
  author={Liu, Shuang and Ao, Zhizhuo and Chen, Peng and Kolmanič, Simon},
  journal={IEEE Access}, 
  title={CollRec: Pre-Trained Language Models and Knowledge Graphs Collaborate to Enhance Conversational Recommendation System}, 
  year={2024},
  volume={12},
  number={},
  pages={104663-104675},
  abstract={Existing conversational recommender systems (CRS) use insufficient generality in incorporating external information using knowledge graphs. The recommendation module and generation module are loosely connected during model training and shallowly integrated during inference. A simple switching or copying mechanism is used to merge recommended items into generated responses. These problems significantly degrade the recommendation performance. To alleviate this problem, we propose a novel unified framework for collaboratively enhancing conversational recommendations using pre-trained language models and knowledge graphs (CollRec). We use a fine-tuned pre-trained language model to efficiently extract knowledge graphs from conversational text descriptions, perform entity-based recommendations based on the generated graph nodes and edges, and fine-tune a large-scale pre-trained language model to generate fluent and diverse responses. Experimental results on the WebNLG 2020 Challenge dataset, ReDial dataset, and Reddit-Movie dataset show that our CollRec model significantly outperforms the state-of-the-art methods.},
  keywords={Knowledge graphs;Oral communication;Task analysis;Recommender systems;Motion pictures;Costs;Accuracy;Large language models;Conversational recommendation system;knowledge graph;large language model;end-to-end generation;fine-tuning;ReDial;WebNLG 2020 challenge},
  doi={10.1109/ACCESS.2024.3434720},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10208670,
  author={Srinivasan, Tejas and Ren, Xiang and Thomason, Jesse},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={Curriculum Learning for Data-Efficient Vision-Language Alignment}, 
  year={2023},
  volume={},
  number={},
  pages={5619-5624},
  abstract={Aligning image and text encoders from scratch using contrastive learning requires large amounts of paired image-text data. We alleviate this need by aligning individually pre-trained language and vision representation models using a much smaller amount of paired data with a curriculum learning algorithm to learn fine-grained vision-language alignments. TOnICS (Training with Ontology-Informed Contrastive Sampling) initially samples minibatches whose image-text pairs contain a wide variety of objects to learn object-level vision-language alignment, and progressively samples minibatches where all image-text pairs contain the same object to learn finer-grained contextual alignment. Aligning pre-trained BERT and VinVL-OD models to each other using TOnICS outperforms CLIP on downstream zero-shot image retrieval using < 1% as much training data.},
  keywords={Training;Computer vision;Conferences;Computational modeling;Image retrieval;Bit error rate;Training data},
  doi={10.1109/CVPRW59228.2023.00595},
  ISSN={2160-7516},
  month={June},}@INPROCEEDINGS{10298429,
  author={Phokela, Kanchanjot Kaur and Sikand, Samarth and Singi, Kapil and Dey, Kuntal and Sharma, Vibhu Saujanya and Kaulgud, Vikrant},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Smart Prompt Advisor: Multi-Objective Prompt Framework for Consistency and Best Practices}, 
  year={2023},
  volume={},
  number={},
  pages={1846-1848},
  abstract={Recent breakthroughs in Large Language Models (LLM), comprised of billions of parameters, have achieved the ability to unveil exceptional insight into a wide range of Natural Language Processing (NLP) tasks. The onus of the performance of these models lies in the sophistication and completeness of the input prompt. Minimizing the enhancement cycles of prompt with improvised keywords becomes critically important as it directly affects the time to market and cost of the developing solution. However, this process inevitably has a trade-off between the learning curve/proficiency of the user and completeness of the prompt, as generating such a solutions is an incremental process. In this paper, we have designed a novel solution and implemented it in the form of a plugin for Visual Studio Code IDE, which can optimize this trade-off, by learning the underlying prompt intent to enhance with keywords. This will tend to align with developers' collection of semantics while developing a secure code, ensuring parameter and local variable names, return expressions, simple pre and post-conditions. and basic control and data flow are met.},
  keywords={Visualization;Codes;Costs;Semantics;Time to market;Natural language processing;Task analysis;Prompt Engineering;Artificial Intelligence;Deep Learning;LLM;Ontology},
  doi={10.1109/ASE56229.2023.00019},
  ISSN={2643-1572},
  month={Sep.},}@ARTICLE{9034075,
  author={Jiang, Tianwen and Zeng, Qingkai and Zhao, Tong and Qin, Bing and Liu, Ting and Chawla, Nitesh V. and Jiang, Meng},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
  title={Biomedical Knowledge Graphs Construction From Conditional Statements}, 
  year={2021},
  volume={18},
  number={3},
  pages={823-835},
  abstract={Conditions play an essential role in biomedical statements. However, existing biomedical knowledge graphs (BioKGs) only focus on factual knowledge, organized as a flat relational network of biomedical concepts. These BioKGs ignore the conditions of the facts being valid, which loses essential contexts for knowledge exploration and inference. We consider both facts and their conditions in biomedical statements and proposed a three-layered information-lossless representation of BioKG. The first layer has biomedical concept nodes, attribute nodes. The second layer represents both biomedical fact and condition tuples by nodes of the relation phrases, connecting to the subject and object in the first layer. The third layer has nodes of statements connecting to a set of fact tuples and/or condition tuples in the second layer. We transform the BioKG construction problem into a sequence labeling problem based on a novel designed tag schema. We design a Multi-Input Multi-Output sequence labeling model (MIMO) that learns from multiple input signals and generates proper number of multiple output sequences for tuple extraction. Experiments on a newly constructed dataset show that MIMO outperforms the existing methods. Further case study demonstrates that the BioKGs constructed provide a good understanding of the biomedical statements.},
  keywords={Biological system modeling;Labeling;Tagging;Feeds;MIMO communication;Task analysis;Biomedical knowledge graph;information extraction;conditional statement;sequence labeling},
  doi={10.1109/TCBB.2020.2979959},
  ISSN={1557-9964},
  month={May},}@ARTICLE{10737425,
  author={Wei, Yuting and Hu, Linmei and Zhu, Yangfu and Zhao, Jiaqi and Wu, Bin},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Knowledge-Guided Transformer for Joint Theme and Emotion Classification of Chinese Classical Poetry}, 
  year={2024},
  volume={32},
  number={},
  pages={4783-4794},
  abstract={The classifications of the theme and emotion are essential for understanding and organizing Chinese classical poetry. Existing works often overlook the rich semantic knowledge derived from poem annotations, which contain crucial insights into themes and emotions and are instrumental in semantic understanding. Additionally, the complex interdependence and diversity of themes and emotions within poems are frequently disregarded. Hence, this paper introduces a Poetry Knowledge-augmented Joint Model (Poka) specifically designed for the multi-label classification of themes and emotions in Chinese classical poetry. Specifically, we first employ an automated approach to construct two semantic knowledge graphs for theme and emotion. These graphs facilitate a deeper understanding of the poems by bridging the semantic gap between the obscure ancient words and their modern Chinese counterparts. Representations related to themes and emotions are then acquired through a knowledge-guided mask-transformer. Moreover, Poka leverages the inherent correlations between themes and emotions by adopting a joint classification strategy with shared training parameters. Extensive experiments demonstrate that our model achieves state-of-the-art performance on both theme and emotion classifications, especially on tail labels.},
  keywords={Semantics;Knowledge graphs;Annotations;Correlation;Symbols;Support vector machines;Speech processing;Bayes methods;Transformers;Training;Chinese classical poetry;data mining;joint learning;knowledge graph;multi-label classification},
  doi={10.1109/TASLP.2024.3487409},
  ISSN={2329-9304},
  month={},}@INPROCEEDINGS{9836511,
  author={Liu, Di and Zhang, Yungui and Li, Zhuoqing},
  booktitle={2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={A Survey of Graph Neural Network Methods for Relation Extraction}, 
  year={2022},
  volume={10},
  number={},
  pages={2209-2223},
  abstract={Relation extraction, as an important part of knowl-edge graph and natural language processing, aims to extract semantic relations between entities by understanding text, which has attracted great interest of researchers. Recently, with the rise of the advanced technology of graph neural networks, numer-ous methods have emerged that employ graph neural network techniques to address relation extraction tasks. However, to the best of our knowledge, few studies provide a complete picture of how and to what extent graph neural networks has been applied to this problem. In this paper, we provide a thorough review of graph neural network-based relation extraction methods. We first present a brief introduction of graph neural networks. Besides, we describe the relation extraction task, comprehensively review the corresponding datasets, and discuss the reason why graph neural networks are adopted for relation extraction tasks. After that, we focus on relation extraction existing approaches built upon graph neural networks, including specialized architectures designed for the basic task as well as novel graph neural network models designed to tackle the challenging problems. We also discuss relation extraction methods based on graph neural networks in combination with pre-trained language models. Finally, we give out our conclusion and provide several further research directions for this research topic.},
  keywords={Knowledge engineering;Conferences;Semantics;Learning (artificial intelligence);Graph neural networks;Natural language processing;Data mining;Relation extraction;Knowledge graph;Graph neural network;Natural language processing;Deep learning},
  doi={10.1109/ITAIC54216.2022.9836511},
  ISSN={2693-2865},
  month={June},}@INPROCEEDINGS{8843314,
  author={Paczona, Martin and Mayr, Heinrich C.},
  booktitle={2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)}, 
  title={Model-Driven Mechatronic System Development}, 
  year={2019},
  volume={},
  number={},
  pages={1730-1736},
  abstract={This paper presents an approach for model-driven mechatronic system development. The approach starts with the definition of a suitable domain-specific modeling language and its semantic foundation in a domain ontology. Models created in this language are used to generate application-specific artefacts. We illustrate our approach with the example of the development of Electric Vehicle Testbeds (EVTs), i.e. systems for testing high-voltage electric vehicle components. Companies in the electric vehicle industry (automobile, aircraft and rail vehicle manufacturers) mainly use such systems. Like many other mechatronic systems, EVTs are typically tailor-made solutions. Our approach automates manual development steps and can thus contribute to quality improvement, development time reduction and finally cost reduction.},
  keywords={Ontologies;Integrated circuit modeling;Unified modeling language;Software;Tools;Electric vehicles},
  doi={10.1109/COASE.2019.8843314},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{10540801,
  author={Libro, Mario and Gaiardelli, Sebastiano and Lora, Michele and Fummi, Franco},
  booktitle={2024 IEEE International Conference on Industrial Technology (ICIT)}, 
  title={Integrating Modeling Languages with Ontologies in the Context of Industry 4.0}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={The evolving landscape of manufacturing systems and the increasing complexity of production lines necessitate innovative approaches for efficient information management and process modeling. The System Modeling Language (SysML) provides a powerful language to express such information. However, the expressiveness comes at a cost: on the one hand, the modeling phase requires a deep understanding of the domain; on the other, SysML lacks rigorous semantics. This work introduces a novel methodology that enriches the SysML with ontology reasoning in the context of manufacturing systems. The approach uses ontologies as a comprehensive knowledge base that encapsulates essential details about the machinery, their provided functions, and the associated constraints. The approach offers a reliable and efficient way to verify the consistency and correctness of production recipes: it ensures recipes' practical applicability in the manufacturing process while reducing errors that can occur in the modeling phase. The proposed methodology has been validated through its application to a fully-fledged manufacturing line, showing its applicability in real-world scenarios.},
  keywords={Manufacturing processes;Process modeling;Knowledge based systems;Ontologies;Cognition;Systems Modeling Language;Reliability;Computer-aided manufacturing;process modeling;knowledge representation},
  doi={10.1109/ICIT58233.2024.10540801},
  ISSN={2643-2978},
  month={March},}@INPROCEEDINGS{10645449,
  author={Xie, Qinhua and Liu, Weicong and Yuan, Fan and Shi, Jifan and Liu, Ziyu and Zhang, Yanbing},
  booktitle={2024 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)}, 
  title={VidBot: Intelligent Video Learning Tool for Content Mining and Playback Traffic Statistics}, 
  year={2024},
  volume={},
  number={},
  pages={1-3},
  abstract={With the rapid development of online education, video-based learning has become a significant mode of learning. However, existing video learning tools often fail to fully meet learners' needs for in-depth learning and interactive communication, thereby hindering learning outcomes. To address this challenge, this study proposes and implements an innovative intel-ligent video learning tool named VidBot, which is based on large language models for deep content mining and playback traffic analysis of videos. VidBot integrates Whisper and ChatGLM3-6B language models as well as the Pyltp library to build seven major functional modules, including Video Playback Visualization, Mind Mapping, Knowledge Graph, Cross-referencing of Knowledge Points, Automatic Video Segmentation, Intelligent Tutor, and Shared Notes. These modules aim to provide learners with deeper and more interactive learning experiences. The client of VidBot is released in the form of a plugin, and the server is integrated into the online education platform developed independently by our university.},
  keywords={Industries;Visualization;Analytical models;Large language models;Conferences;Education;Knowledge graphs;Online education;video analysis;interactive learning;text summarization;large language model},
  doi={10.1109/ICMEW63481.2024.10645449},
  ISSN={2995-1429},
  month={July},}@INPROCEEDINGS{10242690,
  author={Luo, Ziqian},
  booktitle={2023 International Conference on Communications, Computing and Artificial Intelligence (CCCAI)}, 
  title={Knowledge-guided Aspect-based Summarization}, 
  year={2023},
  volume={},
  number={},
  pages={17-22},
  abstract={Contextualized pre-trained models, such as BERT [1] and BART [2], have shown great potential in various NLP tasks, pushing the state-of-the-art results to a new level. Although studies have shown that those pre-trained models have captured different kinds of knowledge due to the massive corpus they have been trained on [3], injecting task-specific external knowledge often shows further improvement [4]. Here we choose aspect-based abstractive summarization as a case study and explore two different ways to inject external knowledge into BART. One is through a knowledge graph, the other is through human-defined sequence-level scores. Experiment results show that both methods can get an improvement over vanilla BART.},
  keywords={Computational modeling;Knowledge graphs;Telecommunication computing;Task analysis;Artificial intelligence;Context modeling;pre-trained models;knowledge graph},
  doi={10.1109/CCCAI59026.2023.00012},
  ISSN={},
  month={June},}@INPROCEEDINGS{9643650,
  author={Smajevic, Muhamed and Bork, Dominik},
  booktitle={2021 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={From Conceptual Models to Knowledge Graphs: A Generic Model Transformation Platform}, 
  year={2021},
  volume={},
  number={},
  pages={610-614},
  abstract={Semantic processing of conceptual models is a focus of research since several years, bridging the disciplines of knowledge-based systems, conceptual modeling, and model-driven software engineering. With the uptake of Knowledge Graphs, the research in this area gained further momentum. In this paper, we introduce a generic and extensible platform that enables the automated transformation of conceptual models into Knowledge Graphs. The platform can transform any model created by a state-of-the-art metamodeling platform (EMF and ADOxx) into standardized Knowledge Graph representations like GraphML, RDF, and OWL. In the paper at hand, we introduce our platform and evaluate it with a corpus of 5.000 UML models that we transform into Knowledge Graphs and subsequently exemplify the rich functionalities enabled by the graph structure by an automated detection of UML model smells.},
  keywords={Knowledge engineering;Analytical models;Codes;Scalability;Unified modeling language;Software algorithms;Semantics;Knowledge Graph;Modeling tool;Code smells;Model refactoring;Model transformation},
  doi={10.1109/MODELS-C53483.2021.00093},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9362691,
  author={Chen, Zhongmin and Xu, Hao and Gao, Shuo and Wang, Yongsheng},
  booktitle={2021 IEEE International Conference on Power Electronics, Computer Applications (ICPECA)}, 
  title={Semantic generation mechanism of news images based on Concept-Net}, 
  year={2021},
  volume={},
  number={},
  pages={245-248},
  abstract={Online news is an important means of disseminating information, but some online news texts do not match the content described in the original news pictures, and even cause ambiguity among readers. This phenomenon has seriously affected the authority of news and the credibility of news media. In response to this problem, this paper proposes a rich news image description mechanism based on the Concept-Net knowledge graph. The model we provide consists of two parts, namely, extracting the image content and rendering with natural language to generate an accurate description of the news image.},
  keywords={Conferences;Semantics;Natural languages;Computer applications;Media;Rendering (computer graphics);Power electronics;Social media;picture description;entity association;knowledge graph},
  doi={10.1109/ICPECA51329.2021.9362691},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10574957,
  author={Abane, Amar and Battou, Abdella and Merzouki, Mheni},
  booktitle={NOMS 2024-2024 IEEE Network Operations and Management Symposium}, 
  title={An Adaptable AI Assistant for Network Management}, 
  year={2024},
  volume={},
  number={},
  pages={1-3},
  abstract={This paper presents a network management AI assistant built with Large Language Models. It adapts at runtime to the network state and specific platform, leveraging techniques like prompt engineering, document retrieval, and Knowledge Graph integration. The AI assistant aims to simplify management tasks and is easily reproducible with available source code.},
  keywords={Runtime;Databases;Source coding;Knowledge graphs;Task analysis;LLMs;text embeddings;RAG;network management;knowledge graph;Neo4j;graph database},
  doi={10.1109/NOMS59830.2024.10574957},
  ISSN={2374-9709},
  month={May},}@INPROCEEDINGS{10459794,
  author={Zhang, Hongzhi and Shafiq, M. Omair},
  booktitle={2023 International Conference on Machine Learning and Applications (ICMLA)}, 
  title={ConKGP: A Contrastive Learning Framework on Knowledge Graphs for Commonsense Reasoning with Perturbation}, 
  year={2023},
  volume={},
  number={},
  pages={1120-1125},
  abstract={Question-answering systems are widely used in var-ious productions, and the use of knowledge graphs has been proven to improve their performance. However, current question-answering systems utilizing knowledge graphs suffer from the long tail effect, where high-degree nodes in the knowledge graph are favored by the model. This can lead to a bias in the model towards rare or complex answers, negatively impacting the performance of the question-answering system on questions with uncommon answers. In this paper, we propose a method, ConKGP, to improve the performance of the model QA-GNN. QA-GNN is a state-of-the-art question-answering method that combines language models and knowledge graphs. For the original model structure, we combine contrastive learning to obtain better representation and evenly distribute representation in the data space. Furthermore, we add perturbations to the input of the decoder to enhance the robustness and generalization ability of the model. Our proposed method achieves better results on CommonsenseQA and MedQA-USMLE datasets. The average improvements over QA-GNN are 4.3% and 3.6% respectively. Our experiments show that ConKGP can effectively solve the long tail effect and improve the performance of QA-GNN in processing uncommon answers.},
  keywords={Visualization;Perturbation methods;Self-supervised learning;Knowledge graphs;Tail;Production;Machine learning;Question Answering;Commonsense Reasoning;Knowledge Graph;Contrastive Learning;Perturbations},
  doi={10.1109/ICMLA58977.2023.00167},
  ISSN={1946-0759},
  month={Dec},}@INPROCEEDINGS{9509672,
  author={Meng, Fanqi and Zheng, Yujie and Bao, Songbin and Wang, Jingdong},
  booktitle={2021 10th IEEE International Conference on Communication Systems and Network Technologies (CSNT)}, 
  title={The Research Status of Formulaic Language Identification}, 
  year={2021},
  volume={},
  number={},
  pages={886-891},
  abstract={Formulaic language, also known as formulaic sequences, is essentially a special kind of multiword strings. The words in the string usually have a fixed or semi-fixed combination form that falls in between phrase and sentence in the language. Formulaic language identification is to identifying schematic string patterns in corpora. According to statistics from different researchers, formulaic language is very common in both spoken and written language. Therefore, its research is helpful to the development of machine translation as well as second language acquisition. In order to better sort out the research hotspots of formula identification, this article took 143 papers included in the core database of web of science and 139 papers in CNKI in the past ten years as the research objects, adopted the method of bibliometric to conduct the following research. By using bibliometric online analysis platform, knowledge graph is drawn from the distribution of research forces of countries, institutions, authors and journals, and keyword co-occurrence, so as to deeply analyze the technical topics of formulaic language identification. The research found that the research strength of formula identification is mainly concentrated in universities at home and abroad. The identification methods have changed from initial manual labeling to simple frequency-based identification. At present, the application of computer technology identification has become a new research hotspot.},
  keywords={Knowledge engineering;Databases;Communication systems;Conferences;Bibliometrics;Manuals;Object recognition;bibliometric;knowledge graph;formulaic language identification},
  doi={10.1109/CSNT51715.2021.9509672},
  ISSN={2329-7182},
  month={June},}@INPROCEEDINGS{10405568,
  author={Corrado, Mario and Giliberti, Vincenzo and Gozzi, Manuel and Lanzolla, Vincenzo and Vetere, Guido and Zurlo, Domenico},
  booktitle={2023 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)}, 
  title={VO.I.C.E. FIRST: Supporting Human Assistants with Real-Time Voice Understanding}, 
  year={2023},
  volume={},
  number={},
  pages={1104-1109},
  abstract={While AI and automation have made significant strides in customer support, there are still situations where human intervention via voice channels is necessary to provide the best possible customer experience. In fact, although AI and chatbots have become increasingly sophisticated, they may not always be able to handle complex or nuanced customer issues. Human agents can better understand and respond to these situations, providing tailored solutions. At the same time, solving non-trivial customer problems often requires access to knowledge bases and contextual customer information, for which AI is particularly well suited. Hence the idea of integrating human and artificial intelligence in a hybrid solution. We developed an AI system to help human assistants in the process of handling conversations. This system can be viewed as a collaborative bot (cobot). The cobot captures the audio stream of the conversation, converts it to text and analyzes it in real time. The extracted tokens are classified and sent to a reasoning system based on a knowledge graph, that provides information and action suggestions to the human assistant. Assistants are also capable of providing information to the reasoning system, utilizing their human understanding of the client's circumstances as they unfold. While designing a prototypical solution for utility services, we have faced the problem of real-time use of computationally complex procedures, including spontaneous speech understanding and knowledge-based heuristic rules. Moreover, we adopted a standards-based approach and experimented with open source reasoners and publicly available language models. The paper outlines the system architecture and design, and discusses the results of the first experiments.},
  keywords={Computational modeling;Knowledge based systems;Oral communication;Chatbots;Real-time systems;Cognition;Business;virtual assistants;natural language understanding;knowledge graph;real time reasoning},
  doi={10.1109/MetroXRAINE58569.2023.10405568},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10475614,
  author={Avila, Caio Viktor S. and Vidal, Vânia M.P. and Franco, Wellington and Casanova, Marco A.},
  booktitle={2024 IEEE 18th International Conference on Semantic Computing (ICSC)}, 
  title={Experiments with text-to-SPARQL based on ChatGPT}, 
  year={2024},
  volume={},
  number={},
  pages={277-284},
  abstract={Currently, large language models (LLMs) are the state of the art for pre-trained language models. LLMs have been applied to many tasks, including question and answering over Knowledge Graphs (KGs) and text-to-SPARQL, that is, the translation of Natural Language questions to SPARQL queries. With such motivation, this paper first describes preliminary experiments to evaluate the ability of ChatGPT to answer NL questions over KGs. Based on these experiments, the paper introduces Auto-KGQAGPT, an autonomous domain-independent framework based on LLMs for text-to-SPARQL. The framework selects fragments of the KG, which the LLM uses to translate the user’s NL question to a SPARQL query on the KG. Finally, the paper describes preliminary experiments with Auto-KGQAGPT with ChatGPT that indicate that the framework substantially reduced the number of tokens passed to ChatGPT without sacrificing performance.},
  keywords={Training;Semantics;Natural languages;Knowledge graphs;Benchmark testing;Chatbots;Iterative methods;text-to-SPARQL;ChatGPT;LLM;Knowledge Graph},
  doi={10.1109/ICSC59802.2024.00050},
  ISSN={2472-9671},
  month={Feb},}@INPROCEEDINGS{10817806,
  author={Mitsuji, Fumiya and Chakraborty, Sudesna and Morita, Takeshi and Egami, Shusaku and Ugai, Takanori and Fukuda, Ken},
  booktitle={2024 Twelfth International Symposium on Computing and Networking Workshops (CANDARW)}, 
  title={Entity Linking for Wikidata using Large Language Models and Wikipedia Links}, 
  year={2024},
  volume={},
  number={},
  pages={144-149},
  abstract={Entity Linking (EL), a task that maps named entities in text to corresponding entities in a knowledge base, has gained attention as a fundamental technology in knowledge processing and natural language processing. Conventional EL methods typically tokenize input text and utilize multiple features such as word embeddings and knowledge graph embeddings. Adapting these conventional EL methods to specific languages requires modifying language-dependent modules like tokenizers and word embedding models for the target language. In this study, we propose an EL method targeting Wikidata, based on Large Language Models (LLMs) and links from Wikidata to Wikipedia. Our method prompts LLMs to extract entity names from the input text and generate the corresponding Wikipedia URLs. Furthermore, it queries the Wikidata SPARQL endpoint to obtain Wikidata IDs from the Wikipedia URLs, outputting the entity names and their Wikidata IDs. This method can be applied to various languages by modifying the prompts. To evaluate, we compared the proposed method with conventional EL methods (PNEL and Japanese PNEL) on Japanese and English datasets from LC-QuAD2.0, SimpleQuestions, and WebQSP; using GPT-3.5, GPT-4, and Llama 2 as LLMs. The results showed that our method using GPT-4 outperformed conventional EL methods in recall and F-scores on datasets except for Japanese SimpleQuestions.},
  keywords={Translation;Large language models;Conferences;Computational modeling;Knowledge based systems;Encyclopedias;Knowledge graphs;Natural language processing;Internet;Online services;Entity Linking;Large Language Model;Wiki-data;Wikipedia;Knowledge Graph},
  doi={10.1109/CANDARW64572.2024.00030},
  ISSN={2832-1324},
  month={Nov},}@INPROCEEDINGS{10628949,
  author={Hadar, Ethan and Heursch, Sven T.},
  booktitle={2024 IEEE 32nd International Requirements Engineering Conference Workshops (REW)}, 
  title={Scattered Requirements Dust in the Era of Web3 – a Vision Keynote}, 
  year={2024},
  volume={},
  number={},
  pages={328-329},
  abstract={This extended abstract paper presents a vision for data and AI requirements governance in distributed System-of-Systems (SoS) that involve multiple parties and dynamic alliances, such as in the defense domain (e.g. different weapon systems on the battlefield). The paper proposes to use web3 technologies, such as distributed ledger and secure multi-party computation, to enable collaborative analytics and data interoperability among the SoS members, while preserving their sovereignty and privacy. The paper also suggests a model-driven approach that can generate executable code and workflows from a common ontology and a semantic mapping tool, which can facilitate the alignment and verification of the data and AI requirements among the coalition members. The paper demonstrates the feasibility and effectiveness of the approach through a prototype implementation in the defense domain and a case study involving a multi-party coalition of the willing to detect a SA-8 anti-aircraft battery that in essence is a sensor-Data Fusion with different sensors on different weapon systems.},
  keywords={Weapons;Semantics;Distributed databases;Prototypes;Sensor fusion;Ontologies;Sensor systems;Generative AI;Distributed Requirements Integrity;DataGPT;Ontology driven integration},
  doi={10.1109/REW61692.2024.00050},
  ISSN={2770-6834},
  month={June},}@INPROCEEDINGS{9874473,
  author={Miller, Kurt},
  booktitle={2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)}, 
  title={Comprehension of Contextual Semantics Across Clinical Healthcare Domains}, 
  year={2022},
  volume={},
  number={},
  pages={479-480},
  abstract={The widespread lack of adoption of clinical notetaking standards has rendered information retrieval from Electronic Health Records (EHRs) especially challenging using traditional Natural Language Processing (NLP) techniques. Clinical note authors too commonly adopt their own note-taking structures and styles, limiting the applicability of rule-based and statistical models. While the context of any given sentence within a note carries important implied information, context is notoriously difficult for a language model to infer. However, recent advances in deep learning NLP methods such as pre-training on domain-specific corpora, novel embedding structures, and transformer architectures have enabled an awareness of context not previously attainable. In this work, I study the application of these evidenced NLP approaches to a gold standard annotated corpus of primary care notes of multiple Mayo Clinic EHR systems. The strongly labelled data will be supplemented with large volumes of weakly labelled data curated using distant supervision. The combined dataset will be used to train and evaluate context classification and section boundary detection models that classify the current context of a sentence given adjacent text segments. Once validated against primary care corpora, transfer learning methods will enable access to shared knowledge across more specific clinical domains, enabling generalizability across clinical domains and a degree of transparency into the shared aspects of the integrated model.},
  keywords={Limiting;Transfer learning;Semantics;Medical services;Transformers;Information retrieval;Natural language processing;natural language processing;electronic health records;knowledge graph;transformer architectures},
  doi={10.1109/ICHI54592.2022.00077},
  ISSN={2575-2634},
  month={June},}@INPROCEEDINGS{10684661,
  author={Bo, Lili and He, Yuting and Sun, Xiaobing and Ji, Wangjie and Wu, Xiaohan},
  booktitle={2024 IEEE 24th International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={A Software Bug Fixing Approach Based on Knowledge-Enhanced Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={169-179},
  abstract={Software Bug Fixing is a time-consuming task in software development and maintenance. Despite the success of Large Language Models (LLMs) using in Automatic Program Repair (APR), they still have the limitations of generating patches with low accuracy and explainability. In this paper, we propose a software bug-fixing approach based on knowledge-enhanced large language models. First, we collect bugs as well as their fix information from bug tracking systems, such as Github and Stack Overflow. Then, we extract bug entities and inter-entity relationships using Named Entity Recognition (NER) to construct a Bug Knowledge Graph (BKG). Finally, we utilize LLMs (e.g., GPT-4) which is enhanced by the knowledge of the similar historical bugs as well as fix information from BKG to generate patches for new bugs. The experimental results show that the our approach can fix 28.52% (85\298) bugs correctly, which is significantly better than the state-of-the-art approaches. Furthermore, the generated patches are explainable and more credible.},
  keywords={Codes;Accuracy;Large language models;Computer bugs;Knowledge graphs;Maintenance engineering;Software;Bug fixing;Knowledge Graph;Generative AI;Explainable},
  doi={10.1109/QRS62785.2024.00026},
  ISSN={2693-9177},
  month={July},}@INPROCEEDINGS{10350471,
  author={Chen, Boqi and Yi, Fandi and Varró, Dániel},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction}, 
  year={2023},
  volume={},
  number={},
  pages={588-596},
  abstract={Taxonomies represent hierarchical relations between entities, frequently applied in various software modeling and natural language processing (NLP) activities. They are typically subject to a set of structural constraints restricting their content. However, manual taxonomy construction can be time-consuming, incomplete, and costly to maintain. Recent studies of large language models (LLMs) have demonstrated that appropriate user inputs (called prompting) can effectively guide LLMs, such as GPT-3, in diverse NLP tasks without explicit (re-)training. However, existing approaches for automated taxonomy construction typically involve fine-tuning a language model by adjusting model parameters. In this paper, we present a general framework for taxonomy construction that takes into account structural constraints. We subsequently conduct a systematic comparison between the prompting and fine-tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset. Our result reveals the following: (1) Even without explicit training on the dataset, the prompting approach outperforms fine-tuning-based approaches. Moreover, the performance gap between prompting and fine-tuning widens when the training dataset is small. However, (2) taxonomies generated by the fine-tuning approach can be easily post-processed to satisfy all the constraints, whereas handling violations of the taxonomies produced by the prompting approach can be challenging. These evaluation findings provide guidance on selecting the appropriate method for taxonomy construction and highlight potential enhancements for both approaches.},
  keywords={Training;Computer science;Systematics;Computational modeling;Taxonomy;Ontologies;Software;taxonomy construction;domain-specific constraints;large language models;few-shot learning;fine-tuning},
  doi={10.1109/MODELS-C59198.2023.00097},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8653655,
  author={Rajasekar, M. and Udhayakumar, A.},
  booktitle={2018 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 2018 2nd International Conference on}, 
  title={"E MARUTHUVACHI" – INFORMATION EXTRACTION FRAMEWORK FOR DATA ABOUT OBSTETRICS AND GYNECOLOGY IN TAMIL}, 
  year={2018},
  volume={},
  number={},
  pages={399-407},
  abstract={Technology is transforming the world from traditional into Artificial Intelligence. Human beings are adopting themselves into the change using Technology. India is famous for the name of unique traditional culture. The traditional culture protected people to do useful things. Especially for women, they were protected by the traditional culture to gain knowledge about maternity and gynecology. The target framework to extract the useful information from the raw documents. The extraction process extracts the NLP elements from the raw documents. The framework is developed using modified model of neural network language model (NNLM). The proposed model is evaluated with F-Test. The evaluation produces the good result for accuracy.},
  keywords={Computational modeling;Neural networks;Task analysis;Data mining;Taxonomy;Gynecology;Artificial Intelligence;Information Extraction;Obstetrics and Gynecology;Neural Networks;Neural Network Language Model},
  doi={10.1109/I-SMAC.2018.8653655},
  ISSN={},
  month={Aug},}@ARTICLE{9335254,
  author={Balaraman, Vevake and Magnini, Bernardo},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Domain-Aware Dialogue State Tracker for Multi-Domain Dialogue Systems}, 
  year={2021},
  volume={29},
  number={},
  pages={866-873},
  abstract={In task-oriented dialogue systems the dialogue state tracker component (DST) is responsible for predicting the current state of the dialogue based on the dialogue history and the user utterance. Current DST approaches rely on a predefined domain ontology, a fact that limits their effective usage for large scale conversational agents, where the DST constantly needs to be interfaced with ever-increasing services and APIs. Focused towards overcoming this drawback, we propose a domain-aware dialogue state tracker, that is completely data-driven and it is modeled to predict for dynamic service schemas, including zero-shot domains. Unlike approaches that propose separate models for prediction of intents, requested slots, slot status, categorical slots and non-categorical slots, we propose a single model in an end-to-end architecture. The proposed model utilizes domain and slot information to extract both domain and slot specific representations from a given dialogue, and then uses such representations to predict the values of the corresponding slot in a given domain. Integrating this mechanism with pretrained language models, our approach can effectively learn semantic relations and effectively perform transfer learning between domains or zero-shot tracking for domains not present in training.},
  keywords={Predictive models;Bit error rate;Task analysis;Ontologies;Virtual assistants;Semantics;Training data;Dialogue state tracking;multi-domain dialogue systems;zero-shot tracking;end-to-end},
  doi={10.1109/TASLP.2021.3054309},
  ISSN={2329-9304},
  month={},}@INPROCEEDINGS{10538949,
  author={Kong, Luyue and Zhang, Shu and Li, Jinbao and Liu, Song},
  booktitle={2023 IEEE 22nd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)}, 
  title={An Instruction Inference Graph Optimal Transport Network Model For Biomedical Commonsense Question Answering}, 
  year={2023},
  volume={},
  number={},
  pages={2749-2754},
  abstract={Biomedical commonsense question answering is a challenging learning task that aims to give correct answers to biomedical commonsense questions. Many works have used rule-based or deep learning approachs to accomplish this task. Recently, an extensive research path is that pre-trained language models combined with graph neural networks (GNNs) to improve the accuracy of biomedical commonsense question answering. However, GNN is prone to the over-smoothing problem, causing the models to lose the ability to reason. In order to alleviate the over-smoothing problem and improve the inference ability for biomedical commonsense question answering, we propose a new end-to-end model named BiomGIN. In BiomGIN, we introduce the Graph Optimal Transport Networks (GOTNet) to use node-centroid attention to capture non-local messages in the knowledge graph, which alleviates the model over-smoothing problem. In addition, we design a question parsing module based on Transformer to generate linguistic instructions, which enhances the inference capability of the GNN. Finally, we evaluated our model on MedQA-USMLE dataset to compare with other baseline models. The experimental results demonstrate the method proposed in this paper achieves state-of-the-art results.},
  keywords={Privacy;Biological system modeling;Computational modeling;Message passing;Linguistics;Transformers;Question answering (information retrieval);Biomedical Commonsense Question Answering;Large Pre-trained Language Model;Graph Neural Network;Knowledge Graph;Graph Optimal Transport Network;Question Parsing},
  doi={10.1109/TrustCom60117.2023.00383},
  ISSN={2324-9013},
  month={Nov},}@INPROCEEDINGS{10674574,
  author={Su, Yi-Jen and Wu, Cheng-Wei and Chen, Yi-Ju},
  booktitle={2024 6th International Conference on Computer Communication and the Internet (ICCCI)}, 
  title={Enhancing KBQA Performance in Large-Scale Chinese Knowledge Graphs Using Apache Spark}, 
  year={2024},
  volume={},
  number={},
  pages={181-186},
  abstract={Knowledge graph (KG) is a kind of fundamental technologies in the development of knowledge base question answering (KBQA) systems, primarily designed to assist in rapidly retrieving accurate information from massive knowledge repositories. Due to the content of knowledge bases continues to expand, the search performance of KBQA systems has become a crucial research topic. This study focuses on enhancing the query efficiency of KBQA systems by increasing the throughput of system queries through a distributed computing framework based on Apache Spark. The main research achievements of this study include optimizing the performance of Chinese KBQA systems using Apache Spark's distributed computing architecture, improving the efficiency of question-answering services, enhancing the accuracy of semantic similarity models to gain a deeper understanding of natural language semantics, and increasing the accuracy and reliability of similarity matching. Under the evaluation method of exact matching, the accuracy of knowledge base queries can reach 90.5%.},
  keywords={Accuracy;Systematics;Computational modeling;Knowledge based systems;Semantics;Cluster computing;Knowledge graphs;knowledge base question answering;knowledge graph;BERT},
  doi={10.1109/ICCCI62159.2024.10674574},
  ISSN={2833-2350},
  month={June},}@INPROCEEDINGS{10544639,
  author={S, Stewart Kirubakaran and G, Jasper Wilsie Kathrine and E, Grace Mary Kanaga and J, Mahimai Raja and Singh A, Ruban Gino and E, Yuvaraajan},
  booktitle={2024 International Conference on Inventive Computation Technologies (ICICT)}, 
  title={A RAG-based Medical Assistant Especially for Infectious Diseases}, 
  year={2024},
  volume={},
  number={},
  pages={1128-1133},
  abstract={Infectious diseases like COVID-19 have gained international attention recently. Furthermore, there are significantly fewer doctors per capita in densely populated nations like India, which hurts those in need. Under such circumstances, natural language processing techniques might make it feasible to create an intelligent and engaging chatbot system. The primary objective of the effort is to develop an interactive solution that is entirely open source and can be easily installed on a local computer using the most recent data. Even though there are numerous chatbots on the market, proposed solutions highlight the need to provide individualized and sympathetic responses. Getting Back While the data is stored in the graph database as nodes and relationships, and the knowledge graph is constructed on top of it, augmented generation is utilized to extract the pertinent content from the data. To improve the generator’s context, pertinent sections are collected during the question-answering process. This reduces hallucinations and increases the correctness of abstractions by providing external knowledge streams. Furthermore, the research study employs a text-to-speech model that was replicated from a physician’s voice recording to narrate the produced responses, thereby augmenting user confidence and interaction. Academic institutions and healthcare organizations can benefit from this work by better understanding the value and effectiveness of applying NLP techniques to infectious disease research.},
  keywords={COVID-19;Infectious diseases;Databases;Chatbots;Recording;Reliability;Artificial intelligence;natural language processing;chatbot;COVID-19;large language model;retrieval augmented generation;knowledge graph},
  doi={10.1109/ICICT60155.2024.10544639},
  ISSN={2767-7788},
  month={April},}
@INPROCEEDINGS{10191712,
  author={Yuan, Xiaosong and Chen, Ke and Zuo, Wanli and Zhang, Yijia},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={TC-GAT: Graph Attention Network for Temporal Causality Discovery}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={The present study explores the intricacies of causal relationship extraction, a vital component in the pursuit of causality knowledge. Causality is frequently intertwined with temporal elements, as the progression from cause to effect is not instantaneous but rather ensconced in a temporal dimension. Thus, the extraction of temporal causality holds paramount significance in the field. In light of this, we propose a method for extracting causality from the text that integrates both temporal and causal relations, with a particular focus on the time aspect. To this end, we first compile a dataset that encompasses temporal relationships. Subsequently, we present a novel model, TC-GAT, which employs a graph attention mechanism to assign weights to the temporal relationships and leverages a causal knowledge graph to determine the adjacency matrix. Additionally, we implement an equilibrium mechanism to regulate the interplay between temporal and causal relations. Our experiments demonstrate that our proposed method significantly surpasses baseline models in the task of causality extraction.},
  keywords={Knowledge engineering;Neural networks;Knowledge graphs;Task analysis;causal knowledge graph;graph neural network;temporal causality},
  doi={10.1109/IJCNN54540.2023.10191712},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10485037,
  author={Lotfy, Abdelrahman and Saleh, Kirollos and Mohamed, Saher and Lorance, John and Yehia, Ehab and Mohammed, Khaled and AbdAlbaky, Ibrahim and Fathy, Mostafa and Yasser, Tawfik},
  booktitle={2024 6th International Conference on Computing and Informatics (ICCI)}, 
  title={Sentiment Analysis for Arabic Product Reviews Using LLMs and Knowledge Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={411-417},
  abstract={The exploration of sentiment analysis in multilingual contexts, particularly through the integration of deep learning techniques and knowledge graphs, represents a significant advance in language processing research. This study specifically concentrates on the Arabic language, addressing the challenges presented by its morphological complexity. While the primary focus is Arabic, the research also includes a comprehensive review of related work in other languages such as Bangla and Chinese. This contextualizes the challenges and solutions found in Arabic sentiment analysis within a broader multilingual landscape. Utilizing pre-trained language models like BERT, the research has achieved noteworthy improvements in sentiment analysis accuracy and efficiency, particularly for the Arabic language. The integration of knowledge graphs stands out as a crucial innovation, offering essential contextual insights and mitigating the limitations posed by sparse labeled datasets in Arabic, a language less resourced compared to English. The findings of this study highlight the effectiveness of tailored BERT models for Arabic sentiment analysis, revealing the vast potential and inherent challenges of employing knowledge graphs and large language models for a deeper, more nuanced understanding. The future direction of this research includes enhancing these methods with cutting-edge machine learning techniques, aiming to further refine sentiment analysis processes and knowledge graph construction with a focus on Arabic within a multilingual framework.},
  keywords={Sentiment analysis;Analytical models;Technological innovation;Reviews;Knowledge graphs;Performance gain;Cognition;Sentiment Analysis;Big Data;LLMs;Knowledge Graph;Arabic},
  doi={10.1109/ICCI61671.2024.10485037},
  ISSN={},
  month={March},}@INPROCEEDINGS{9865330,
  author={Dordiuk, Vladislav and Demicheva, Ekaterina and Espino, Fernando Polanco and Ushenin, Konstantin},
  booktitle={2022 Ural-Siberian Conference on Computational Technologies in Cognitive Science, Genomics and Biomedicine (CSGB)}, 
  title={Natural language processing for clusterization of genes according to their functions}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={There are hundreds of methods for analysis of data obtained in mRNA-sequencing. The most of them are focused on small number of genes. In this study, we propose an approach that reduces the analysis of several thousand genes to analysis of several clusters. The list of genes is enriched with information from open databases. Then, the descriptions are encoded as vectors using the pretrained language model (BERT) and some text processing approaches. The encoded gene function pass through the dimensionality reduction and clusterization. Aiming to find the most efficient pipeline, 180 cases of pipeline with different methods in the major pipeline steps were analyzed. The performance was evaluated with clusterization indexes and expert review of the results.},
  keywords={Dimensionality reduction;Databases;Pipelines;Bit error rate;Genomics;Natural language processing;Cognitive science;natural language processing;BERT;semantic analysis;differential gene expression analysis;gene ontology;gene expression;clusterization},
  doi={10.1109/CSGB56354.2022.9865330},
  ISSN={},
  month={July},}@INPROCEEDINGS{9671788,
  author={Eggleston, Chloe and Abramson, Jeremy},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)}, 
  title={Woolery: Extending Frame Semantics to Structured Documents}, 
  year={2021},
  volume={},
  number={},
  pages={5597-5601},
  abstract={This paper presents Woolery, a system for semantic annotation and mapping of structured documents (such as JSON key-value pairs) to FrameNet. Implemented as a graphical interface, Woolery provides an annotator with a guided means to map keys in a JSON document to FrameNet elements, without the need for extensive knowledge of FrameNet's semantic structures. Candidate frame elements are identified via a search across FrameNet's internal representations, or via mapping keys to their potential WordNet synsets. Final element selection is automated via a pretrained language model. Initial results are promising, with the model giving an overall accuracy of 77.8% when labeling frames across a diverse corpus of JSON document schemas.},
  keywords={Annotations;Conferences;Semantics;Big Data;Labeling;FrameNet;natural language processing;annotation;lexical databases;JSON;ontology alignment;computational semantics},
  doi={10.1109/BigData52589.2021.9671788},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8276845,
  author={Hussain, Ambreen and Wu, Wenyan},
  booktitle={2017 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)}, 
  title={Sustainable Interoperability and Data Integration for the IoT-Based Information Systems}, 
  year={2017},
  volume={},
  number={},
  pages={824-829},
  abstract={Devices used in Internet of Things (IoT) technology are interconnected through internet and continuously send data for storage in disparate databases and files. This poses heterogeneity in data syntax and semantics. In the past few years, researches have been carried out to handle these problems in various information systems such as water and healthcare by applying semantic and syntactic uniformity. This research presents an Information and Communication Technology (ICT) framework to sustain the interoperability between stakeholders within an information system by applying Model Driven Software Development (MDSD) paradigm and ontology. The major contribution of this research is building software model which is annotated with semantic model to maintain the comprehension between the modelers and the programmers allowing sustainability by adapting to the changing environment. Moreover, the framework offers two types of semantic validation i.e. model and data of the documents to integrate sensor data which we claim as a novelty in this study. The validated IoT data is integrated through RESTful web service interface of SensorThings API. The aim of this paper is to apply the methodology of the framework, on IoT based healthcare system, that has been successful in water information system.},
  keywords={Unified modeling language;Semantics;Interoperability;Ontologies;Medical services;Adaptation models;Software;Model Driven Software Development;Ontology;Healthcare systems;Semantic Validation;OGC SensorThings},
  doi={10.1109/iThings-GreenCom-CPSCom-SmartData.2017.126},
  ISSN={},
  month={June},}@INPROCEEDINGS{9218173,
  author={Parvizimosaed, Alireza},
  booktitle={2020 IEEE 28th International Requirements Engineering Conference (RE)}, 
  title={Towards the Specification and Verification of Legal Contracts}, 
  year={2020},
  volume={},
  number={},
  pages={445-450},
  abstract={A contract is a legally binding agreement that expresses high-level requirements of parties in terms of obligations, powers and constraints. Parties' actions influence the status of a contract and shall comply with its clauses. Manual contract monitoring is very laborious in real markets, such as transactive energy, where plenty of complex contracts are running concurrently. Furthermore, liability, right and performance transition through run-time operations such as subcontracting, assignment and substitution complicate contract interpretation. Automation is needed to ensure that contracts respect desirable properties and to support monitoring of compliance and handling of violations. In this thesis research, I propose an innovative ontology that defines fundamental contractual notions (such as the ones mentioned above) and their relationships, on which is built a specification language, called Symboleo, that provides syntax and axiomatic semantics of contracts via first-order logic. Symboleo enables the development of advanced automation tools such as a compliance checker that monitors contracts at runtime, and a model checking verification method that analyzes liveness and safety properties of contracts. This paper reports on the problem domain, research method, current status, expected contributions, and main foreseen challenges.},
  keywords={Contracts;Law;Ontologies;Monitoring;Tools;Legal Contract;Specification Language;Model Checking;Smart Contract;Ontology},
  doi={10.1109/RE48521.2020.00066},
  ISSN={2332-6441},
  month={Aug},}@INPROCEEDINGS{9397038,
  author={Bhoomkar, Yogiraj and Vernekar, Sushant and Kulkarni, Arya and Kulkarni, Pranav and Aniyan, Arun},
  booktitle={2021 International Conference on Emerging Smart Computing and Informatics (ESCI)}, 
  title={Knowledge Graph of Mergers and Acquisitions}, 
  year={2021},
  volume={},
  number={},
  pages={6-12},
  abstract={Context driven decision making is a key factor to make critical decisions in business applications. We present the design and application of a knowledge graph to aid the context driven decision making for studying the patterns in Mergers and Acquisitions (M&A) activities in the industry. Using text data from news articles we make use of a Natural Language Processing pipeline to extract entities and relations to build a knowledge graph. The entity recognition model was 90.97% accurate in detecting the entities. The graph is further enriched with metadata from Wikipedia. We finally demonstrate two use cases to showcase the utility of the knowledge graph for making business decisions.},
  keywords={Industries;Corporate acquisitions;Decision making;Pipelines;Metadata;Natural language processing;Business;Knowledge Graphs;Natural Language Processing},
  doi={10.1109/ESCI50559.2021.9397038},
  ISSN={},
  month={March},}@ARTICLE{9349177,
  author={Amato, Flora and Cozzolino, Giovanni and Moscato, Francesco and Moscato, Vincenzo and Xhafa, Fatos},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={A Model for Verification and Validation of Law Compliance of Smart Contracts in IoT Environment}, 
  year={2021},
  volume={17},
  number={11},
  pages={7752-7759},
  abstract={The interest of Industry 4.0 in smart contracts and blockchain technologies is growing up day by day. Smart contracts have enabled new kinds of interactions whereby contractors can even fully automate processes they agree on. This technology is really appealing in Internet of Things (IoT) domain because smart devices generate events for software agents involved in a smart contract execution, making full automation possible. However, smart contracts have to comply with national and international laws and accountability of participant's actions. Soundness of a smart contract has to be verified in terms of law compliance. Here, we propose a model for verification and validation of law compliance of smart contracts in IoT environments. The main goal of this article is to propose a formal model (based on multiagent logic and ontological description of contracts) for validating law compliance of smart contracts and to determine potential responsibilities of failures.},
  keywords={Unified modeling language;Law;Internet of Things;Insurance;Automobiles;Analytical models;Smart contracts;Blockchain;industry 4.0;Internet of Things (IoT);multiagent systems;smart contracts},
  doi={10.1109/TII.2021.3057595},
  ISSN={1941-0050},
  month={Nov},}@INPROCEEDINGS{9672080,
  author={Parolin, Erick Skorupa and Hu, Yibo and Khan, Latifur and Osorio, Javier and Brandt, Patrick T. and D’Orazio, Vito},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)}, 
  title={CoMe-KE: A New Transformers Based Approach for Knowledge Extraction in Conflict and Mediation Domain}, 
  year={2021},
  volume={},
  number={},
  pages={1449-1459},
  abstract={Knowledge discovery and extraction approaches attract special attention across industries and areas moving toward the 5V Era. In the political and social sciences, scholars and governments dedicate considerable resources to develop intelligent systems for monitoring, analyzing and predicting conflicts and affairs involving political entities across the globe. Such systems rely on background knowledge from external knowledge bases, that conflict experts commonly maintain manually. The high costs and extensive human efforts associated with updating and extending these repositories often compromise their correctness of. Here we introduce CoMe-KE (Conflict and Mediation Knowledge Extractor) to extend automatically knowledge bases about conflict and mediation events. We explore state-of-the-art natural language models to discover new political entities, their roles and status from news. We propose a distant supervised method and propose an innovative zero-shot approach based on a dynamic hypothesis procedure. Our methods leverage pre-trained models through transfer learning techniques to obtain excellent results with no need for a labeled data. Finally, we demonstrate the superiority of our method through a comprehensive set of experiments involving two study cases in the social sciences domain. CoMe-KE significantly outperforms the existing baseline, with (on average) double of the performance retrieving new political entities.},
  keywords={Knowledge based systems;Social sciences;Transfer learning;Natural languages;Big Data;Transformers;Knowledge discovery;knowledge base construction;knowledge extraction;ontologies;link and graph mining;transfer-learning;natural language processing;web search and mining;semantic-based data mining;CAMEO},
  doi={10.1109/BigData52589.2021.9672080},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8369586,
  author={Blackburn, Mark R. and Austin, Mark A. and Coelho, Maria},
  booktitle={2018 Annual IEEE International Systems Conference (SysCon)}, 
  title={Modeling and cross-domain dependability analysis of cyber-physical systems}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper discusses a novel method of modeling and formal verification to support dependability analyses. The method is demonstrated in an example of a fault management capability of robots that interacts with equipment and humans. Hazard analyses produce derived requirements for fault management capabilities. These include safety critical functions for collision avoidance and temporary autonomy. Derived requirements are represented formally in models that are used to produce dependability evidence using theorem proving, model-based test vector generation, test execution with code coverage analysis, and requirement-to-test traceability. To address the challenges of heterogeneity of modeling tools and languages, Semantic Web Technologies are used for model composition and model transformation from modeling tools to formal analysis tools.},
  keywords={Object oriented modeling;Analytical models;Robots;Mathematical model;Tools;Software;Hazards;component;formatting;style;styling;formal methods;modeling;formal verification;dependability;fault management;cyber physical systems},
  doi={10.1109/SYSCON.2018.8369586},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{9995651,
  author={Wang, Hong and Wang, Xiaoqi and Liu, Wenjuan and Xie, Xiaolan and Peng, Shaoliang},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={deepDGA: Biomedical Heterogeneous Network-based Deep Learning Framework for Disease-Gene Association Predictions}, 
  year={2022},
  volume={},
  number={},
  pages={601-606},
  abstract={Accurate prediction of disease-gene associations is a crucial tissue in the treatment of diseases. Currently, deep learning-based methods have been proposed to determine the associations between diseases and genes. However, previous network-based models do not consider the semantic characteristics of various biomedical entities and suffer from the problems of cold-start. To this end, this study proposes a heterogeneous network-based deep learning framework (termed deepDGA) to predict disease-gene associations. First, a heterogeneous network with four kinds of biological nodes and eight kinds of edges is constructed. Second, we develop a meta path-driven deep Transformer encoder to learn node representations which contains semantic characteristics of nodes in the heterogeneous network. Finally, the inductive matrix completion algorithm that can solve problem of cold-start, is used for disease-gene association prediction. The results of 5-flod cross-validation and top-ranked predictions suggest that deepDGA is superior to other methods. In addition, we further observe that deepDGA performs the highest predictive ability for specific diseases via the literature verification, KEGG human pathway analyses, and GO enrichment analyses. In summary, deepDGA is an effective framework for predicting the diseases-gene associations.},
  keywords={Deep learning;Biological system modeling;Semantics;Pipelines;Predictive models;Transformers;Prediction algorithms;disease-gene association;heterogeneous network;deep Transformer encoder;inductive matrix completion},
  doi={10.1109/BIBM55620.2022.9995651},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10350773,
  author={Lange, Arne and Atkinson, Colin},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Modeling in LML with DOCL: A Contribution to the MULTI Warehouse Challenge}, 
  year={2023},
  volume={},
  number={},
  pages={649-658},
  abstract={This paper responds to the “Warehouse” challenge that was posed to the community of multi-level modeling researchers for the MULTI 2023 workshop. Given the many flavors of multi-level modeling approaches, the purpose of this and other similar challenges defined by the MULTI workshop community is to clarify the trade-offs entailed by the design choices underpinning the different approaches. This challenge revolves around product copies, product specifications, and product type specifications and how to guarantee certain properties at the product instance level. After first providing an overview of our modeling approach, and summarising the requirements laid out in the challenge, we present our solution using the LML and DOCL languages. We then discuss how well the solution fulfills the requirements laid out in the challenge.},
  keywords={Conferences;Semantics;Syntactics;Model driven engineering;Complexity theory;Safety;Currencies;Multi-level modeling;LML;DOCL},
  doi={10.1109/MODELS-C59198.2023.00106},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10554074,
  author={Chiş, Andrei and Stoica, Oliviu Ionuţ and Ghiran, Ana-Maria and Buchmann, Robert Andrei},
  booktitle={2024 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)}, 
  title={A Knowledge Graph Approach to Cyber Threat Mitigation Derived from Data Flow Diagrams}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Data Flow Diagrams (DFD) have proven effective in designing and analyzing the flow of data in enterprise systems. They serve as indispensable tools for enterprises that are undergoing transition to cloud services. DFDs aid in understanding the current processes, identifying interfaces and integration points that require security measures. This paper reports a Design Science project to mitigate the cyber security threats at the design phase of a system and to perform auditing of an existing system through knowledge graphs. The proposal leverages knowledge gathered from various sources in a knowledge graph to identify semantic relationships and patterns, enabling automated inference, analysis and detection of vulnerability patterns. Furthermore, LLM-based (large language models) capabilities transform data management details captured as Data Flow Diagrams (DFD) into knowledge graphs for semantic querying and improved decision support.},
  keywords={Current measurement;Semantics;Knowledge graphs;Transforms;Data models;Security;Proposals;knowledge graphs;security;privacy;data flow diagrams;threat modeling;LLMs},
  doi={10.1109/AQTR61889.2024.10554074},
  ISSN={1844-7872},
  month={May},}@INPROCEEDINGS{8054828,
  author={Aydemir, Fatma Başak and Dalpiaz, Fabiano},
  booktitle={2017 IEEE 25th International Requirements Engineering Conference Workshops (REW)}, 
  title={Towards Aligning Multi-concern Models via NLP}, 
  year={2017},
  volume={},
  number={},
  pages={46-50},
  abstract={The design of large-scale complex systems requires their analysis from multiple perspectives, often through the use of requirements models. Diversely located experts with different backgrounds (e.g., safety, security, performance) create such models using different requirements modeling languages. One open challenge is how to align these models such that they cover the same parts of the domain. We propose a technique based on natural language processing (NLP) that analyzes several models included in a project and provides suggestions to modelers based on what is represented in the models that analyze other concerns. Unlike techniques based on meta-model alignment, ours is flexible and language agnostic. We report the results of a focus group session in which experts from the air traffic management domain discussed our approach.},
  keywords={Conferences;Requirements engineering;requirements models;natural language processing;alignment;collaborative modeling;model management},
  doi={10.1109/REW.2017.82},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9816191,
  author={Andreadis, Stelios and Elias, Mirette and Mavropoulos, Thanassis and Papadopoulos, Charis and Pantelidis, Nick and Gialampoukidis, Ilias and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
  booktitle={2022 IEEE 14th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP)}, 
  title={SPARQL querying for validating the usage of automatically georeferenced social media data as human sensors for air quality}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={The problem of air pollution is one of the countless topics discussed on social media on an everyday basis. This rich, crowdsourced information can be exploited to assess the air quality of urban areas, using humans as sensors. Nevertheless, the majority of social media data are falsely geotagged or completely lack geoinformation, which is an essential attribute, while the reliability of the air pollution events reported by online citizens has to be proven. The scope of this work is to present a framework that collects Twitter messages in German that refer to the atmosphere, automatically georeferences them, and finally validates them through semantic representation and SPARQL queries in order to associate them with real measurements of air quality sensors. The georeferencing models are evaluated against state-of-the-art works and the proposed framework is validated in a near-six-month scenario in Germany.},
  keywords={Multidimensional signal processing;Social networking (online);Atmospheric measurements;Atmospheric modeling;Urban areas;Semantics;Sensor phenomena and characterization;air quality;social media;georeferencing;semantic representation;SPARQL querying},
  doi={10.1109/IVMSP54334.2022.9816191},
  ISSN={},
  month={June},}@INPROCEEDINGS{8600201,
  author={Lohar, Nitin K. and Kar, Subrat},
  booktitle={2018 Twenty Fourth National Conference on Communications (NCC)}, 
  title={Control and Management of Optical Networks Using Optical Network Description Language}, 
  year={2018},
  volume={},
  number={},
  pages={1-5},
  abstract={Management and configuration of optical networks, implementing new policies to keep up with ever-changing network etc. have always been tedious tasks. Software-defined networking (SDN) has provided many network solutions in the electrical counterpart. SDN for optical networks can provide new opportunities to make the above mentioned tasks easier and faster. As a first step towards this goal, we develop an optical network description language (ONDL). We use it to describe various network components, and their configuration and run-time states, such as modulation schemes, wavelength and spectral-width of a transponder, switching matrix of an optical switch etc. The language is based on resource description framework (RDF). Furthermore, we develop a controller which understands and sends instructions in this language to different network devices to provide/change their states. We show the applicability of ONDL by simulating a network, controlling and managing its nodes using ONDL and developed controller.},
  keywords={Optical switches;Optical fiber networks;Resource description framework;Modulation},
  doi={10.1109/NCC.2018.8600201},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10275701,
  author={Knorr, Felix and Kastner, Wolfgang},
  booktitle={2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Towards a Uniform Exchange Format for Home and Building Automation using VDI 3814}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Exchanging technical documents in the building automation domain is a complicated process. Files are distributed either as drawings, spreadsheets, or text documents. Each stakeholder has to re-enter the data into their own system, and changes are revised manually, often even without revision control. This paper presents a uniform exchange format based on the ’graphical’ standard VDI 3814. To increase acceptance, the industry standards XSD and XML were chosen. As a result of this work, a model is provided that covers the concepts and exchange files provided in the VDI 3814 standard. Given a supporting tool, data can be entered, revised, and exchanged automatically. Based on this unified representation, it is subsequently possible to transfer the data into one of the already existing ontologies in this domain by using model transformations. Some of these ontologies are also referred to in this paper.},
  keywords={Industries;Buildings;XML;Ontologies;Data models;Stakeholders;Standards;Building Automation;Uniform Format;VDI 3813;VDI 3814},
  doi={10.1109/ETFA54631.2023.10275701},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{8862131,
  author={Vijaya, B. and Gharpure, Prachi},
  booktitle={2019 International Conference on Computational Intelligence in Data Science (ICCIDS)}, 
  title={Candidate Generation for Instance Matching on Semantic Web}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  abstract={The growth of semantic web has given rise to proliferation of data sources wherein the task of recognizing real world entities and identifying multiple references of the same real world entity becomes an essential task in order to facilitate sharing and integration of data. Due to the heterogeneous nature of data on the semantic web, entities belonging to different sources are compared by assessing the similarity of features that are common in order to identify matches. With the increasing size of data sets Candidate generation methods are generally employed to avoid quadratic time complexity that would otherwise be incurred if pairwise similarity of all entities are computed. Here we propose a novel index based approach for candidate generation and reduction. The evaluation shows that the proposed method scales well and improves recall significantly.},
  keywords={Indexes;Semantic Web;Task analysis;Mathematical model;Computational intelligence;Data science;Couplings;Instance matching;Record Linkage;Blocking;Candidate generation;Semantic web},
  doi={10.1109/ICCIDS.2019.8862131},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{9507178,
  author={Vlasenko, Sergey V.},
  booktitle={2021 XXIV International Conference on Soft Computing and Measurements (SCM)}, 
  title={Usage Features of Semantic Query and Rule Languages of Semantic Web in the Intelligent Systems, Based on Conceptual Graphs Technologies}, 
  year={2021},
  volume={},
  number={},
  pages={120-123},
  abstract={The article considers some problems related to applying languages of semantic query and rules, oriented towards their application within Semantic Web, in the information systems, based on conceptual graphs techniques and knowledge models of the appropriate type. At this, special attention is paid to the analysis of given languages interpretation correctness, as well as technological aspects of organizing for processing chosen-class knowledge models.},
  keywords={Semantic Web;ISO Standards;Semantics;W3C;Tools;Software;Regulation;intelligent systems;knowledge models;Semantic Web;conceptual graphs},
  doi={10.1109/SCM52931.2021.9507178},
  ISSN={},
  month={May},}@INPROCEEDINGS{10350821,
  author={Henzgen, Arne and Strey, Lukas},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Model-Driven Approach for Automatic Model Information Aggregation in Structured Documents}, 
  year={2023},
  volume={},
  number={},
  pages={403-413},
  abstract={While models are widely used in software development projects originating from industry and academic research, their documentation can be a time-intensive process. This paper focuses on providing a Proof of Concept for the automatic aggregation of various model data in two different document types conforming to ISO/IEC/IEEE 42010 architecture descriptions or instructional information documents according to ISO/IEC/IEEE 26514. Therefore, this work leverages a model-driven mapping approach of model information to the required document structure, dynamic templating algorithms to transform model data into text and a prototypical implementation that executes the defined mapping and transformation logic in practice. The generation results show that most of the documentation standard requirements can be fulfilled automatically and therefore, reduce the manual processing effort while enhancing consistency.},
  keywords={Industries;ISO Standards;Heuristic algorithms;Documentation;Computer architecture;Transforms;Data models;Model-Driven Engineering;Documentation;BPMN;UML;GSN;Model-to-Document;Architecture Description;Instructional Information},
  doi={10.1109/MODELS-C59198.2023.00072},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10678676,
  author={Gerasimov, Irina and Mehrabian, Armin and KC, Binita and Alfred, Jerome and McGuire, Michael P.},
  booktitle={2024 IEEE 20th International Conference on e-Science (e-Science)}, 
  title={Discovering Research Areas in Dataset Applications through Knowledge Graphs and Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={Scientific datasets are increasingly cited in peer-reviewed journal publications, facilitating easy access to research utilizing those datasets. Datasets undergo a life cycle where older versions of datasets are replaced by newer versions often due to improvements in data resolution, algorithms, and other factors. Unlike peer reviewed documents registered with a single Digital Unique Identifier (DOI), datasets can be updated over time and the newer version of the datasets are registered with a new DOI which is not necessarily linked to the previous version of the dataset. It is challenging when publications citing a dataset need to be traced over the entire life cycle of that dataset. We provide an innovative approach to link the dataset versions and publications using a knowledge graph (KG). KG can help to trace the dataset cited in publications over the entire dataset life cycle and shed light into dataset usage in various applied research areas. We fine-tuned the pretrained NASA IMPACT INDUS Large Language Model (LLM) on a set of labeled publications abstracts. Our results showed that 87% of the publications were classified into one of twenty applied research areas, while the remaining 13% were classified into non-applied research areas. By linking datasets to applied research areas through the KG and employing Global Change Master Directory (GCMD), a well-established controlled vocabulary of scientific keywords describing Earth science datasets, we contribute to a transparent and advanced search and discovery mechanism for datasets across the Earth data ecosystem. The integrated KG and LLM approach is now incorporated and operational in dataset publication management at one of NASA’s Earth science data archival centers.},
  keywords={Vocabulary;Navigation;Large language models;Instruments;NASA;Ecosystems;Knowledge graphs;LLM;Knowledge Graph;data citation},
  doi={10.1109/e-Science62913.2024.10678676},
  ISSN={2325-3703},
  month={Sep.},}@INPROCEEDINGS{10223881,
  author={Ji, Wei and Cao, Qinghong and Shi, Jin and Zhu, Enyao and Xu, Tianyi and He, Hao},
  booktitle={2023 26th ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)}, 
  title={Research on Domain Knowledge Representation Techniques}, 
  year={2023},
  volume={},
  number={},
  pages={150-157},
  abstract={Knowledge is the fruit of human's knowledge of the objective world in practice and the crystallization of wisdom. A knowledge base makes a knowledge-based system (or expert system) intelligent by structuring and sorting out knowledge in a specific field and storing, organizing, managing and using it in a computer using a scientific knowledge representation. Based on the research of knowledge base construction in securities industry, this paper first summarizes and explains several representative knowledge representation models. Then, it summarizes the application scenarios of common knowledge representation techniques in the fields of product design, robot control, and natural language processing. In addition, based on the investigation of the knowledge characteristics of the securities industry, a knowledge representation model of the securities industry based on 5W1lH is proposed to organize and manage the multimodal information resources and provide high-value information for user needs, while the knowledge representation technology of the mechanical industry based on hypergraph embedding is examined and the specific processes and application scenarios are summarized.},
  keywords={Industries;Service robots;Robot control;Knowledge representation;Product design;Natural language processing;Security;knowledge representation;knowledge graph;industry knowledge base},
  doi={10.1109/SNPD-Winter57765.2023.10223881},
  ISSN={},
  month={July},}@INPROCEEDINGS{10205180,
  author={Gu, Xin and Chen, Guang and Wang, Yufei and Zhang, Libo and Luo, Tiejian and Wen, Longyin},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Text with Knowledge Graph Augmented Transformer for Video Captioning}, 
  year={2023},
  volume={},
  number={},
  pages={18941-18951},
  abstract={Video captioning aims to describe the content of videos using natural language. Although significant progress has been made, there is still much room to improve the performance for real-world applications, mainly due to the long-tail words challenge. In this paper, we propose a text with knowledge graph augmented transformer (TextKG)for video captioning. Notably, TextKG is a two-stream transformer, formed by the external stream and internal stream. The external stream is designed to absorb additional knowledge, which models the interactions between the additional knowledge, e.g., pre-built knowledge graph, and the built-in information of videos, e.g., the salient object regions, speech transcripts, and video captions, to mitigate the long-tail words challenge. Meanwhile, the internal stream is designed to exploit the multi-modality information in videos (e.g., the appearance of video frames, speech transcripts, and video captions) to ensure the quality of caption results. In addition, the cross attention mechanism is also used in between the two streams for sharing information. In this way, the two streams can help each other for more accurate results. Extensive experiments conducted on four challenging video captioning datasets, i.e., YouCookII, ActivityNet Captions, MSR-VTT, and MSVD, demonstrate that the proposed method performs favorably against the state-of-the-art methods. Specifically, the proposed TextKG method out-performs the best published results by improving 18.7% absolute CIDEr scores on the YouCookII dataset.},
  keywords={Computer vision;Semantics;Natural languages;Knowledge graphs;Speech recognition;Streaming media;Transformers;Vision;language;and reasoning},
  doi={10.1109/CVPR52729.2023.01816},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10394492,
  author={Ming, Yan and Shang, Yong and Li, Huiting},
  booktitle={2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Relation Extraction with Knowledge-Enhanced Prompt-Tuning on Multimodal Knowledge Graph}, 
  year={2023},
  volume={},
  number={},
  pages={460-465},
  abstract={Recently, Multimodal Knowledge Graphs (MKGs) with visual and textual factual knowledge have been widely used in tasks such as knowledge question answering, recommender systems, and entity disambiguation. Since most of the current MKGs still have defects, a multimodal knowledge graph completion technology is proposed, and multimodal relation extraction (MRE) is one of the basic processes. However, visual objects with high object classification scores are usually selected in previous tasks, which may result in the addition of noise from objects that are either irrelevant or redundant, which can adversely affect multimodal relationship extraction. For this reason, in this paper, we propose a Relation Extraction with Knowledge-enhanced Prompt-tuning modal on multimodal knowledge graph (REKP) to address these issues. Specifically, we inject potential knowledge from relational labels into the prompt construction of answer words and optimize their representation with structured constraints. A Transformer architecture with cross-modal attention is then used to fuse the visual and textual representations. We conduct extensive experiments to verify that our REKP model can achieve SOTA performance on the MNRE dataset with multimodal relational extraction.},
  keywords={Visualization;Fuses;Knowledge graphs;Transformers;Question answering (information retrieval);Task analysis;Recommender systems;Multimodal;Knowledge Graphs;relation extraction;Knowledge-enhanced;Prompt-tuning},
  doi={10.1109/SMC53992.2023.10394492},
  ISSN={2577-1655},
  month={Oct},}@INPROCEEDINGS{10463196,
  author={Kundu, Arghya and Nguyen, Uyen Trang},
  booktitle={2024 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={Automated Fact Checking Using A Knowledge Graph-based Model}, 
  year={2024},
  volume={},
  number={},
  pages={709-716},
  abstract={Misinformation is a growing threat to the economy, social stability, public health, democracy, and national security. One of the most effective methods to combat misinformation is fact checking. Fact checking is the process of verifying the factual accuracy of a statement or claim. Fact checkers employ rigorous methodologies to scrutinize claims, verify sources, and expose falsehoods. However, the huge volume of content circulating online makes it challenging for humans to identify misinformation manually. Automated tools can analyze large datasets to detect patterns in misinformation content, scaling up fact checking efforts. This paper proposes a knowledge graph-based fact checking model that uses two separate knowledge graphs, one containing true claims and the other, false claims. The model uses knowledge graph embeddings which are based on convolutional neural networks. The deep learning model is trained on the above two knowledge graphs to learn distinguishing patterns between true and false claims. Additionally, we employ explainable artificial intelligence (XAI) techniques to provide explanations for the model's classification, reducing cost of errors and increasing transparency and user trust in the system.},
  keywords={Training;Explainable AI;Knowledge graphs;Predictive models;Vectors;Stability analysis;Convolutional neural networks;Misinformation;disinformation;fake news;fact checking;knowledge graphs;machine learning;natural language processing},
  doi={10.1109/ICAIIC60209.2024.10463196},
  ISSN={2831-6983},
  month={Feb},}@INPROCEEDINGS{10260382,
  author={Xia, Liqiao and Zheng, Pai and Liang, Yongshi and Zheng, Ge and Ling, Zhengyang},
  booktitle={2023 IEEE 19th International Conference on Automation Science and Engineering (CASE)}, 
  title={Secure Co-Creation of Industrial Knowledge Graph: Graph Complement Method with Federated Learning and ChatGPT}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Industrial areas have increasingly developed their own Knowledge Graph (KG) for organizing and leveraging vast amounts of data. One major challenge in constructing KG is the heavy reliance on available resources, restricting the scalability and accuracy of the resulting graphs. To address this issue, an end-to-end method is proposed to create a multi-benefit ecosystem by integrating Federated Learning with ChatGPT (a popular language model). Different stakeholders may leverage ChatGPT to search for novel knowledge that complements their existing KGs, however, this approach could potentially introduce ambiguous and wrong triples into the KG. To overcome this, Federated Learning is applied to align and disambiguate the triples using other industrial KGs as super-vision. The proposed method applies a multi-field hyperbolic embedding method to vectorize entities and edges, which are then associatively aggregated to achieve edge replenishment and entity fusion for each KG encrypted. Finally, an incentive win-win mechanism is proposed to motivate diverse stakeholders to contribute to this co-creation actively. A case study is conducted on different industrial KG to evaluate the proposed method. Results demonstrate that this method provides a practical solution for KG co-creation and no compromise to data security.},
  keywords={Federated learning;Scalability;Semantics;Ecosystems;Knowledge graphs;Chatbots;Reliability engineering},
  doi={10.1109/CASE56687.2023.10260382},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{10394554,
  author={Jing, Xiaonan and Rayz, Julia M.},
  booktitle={2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Prompt-assisted Relation Fusion in Knowledge Graph Acquisition}, 
  year={2023},
  volume={},
  number={},
  pages={2960-2965},
  abstract={This paper investigated how prompt-based learning techniques can assist with relation fusion in Knowledge Graph (KG) acquisition. We created a unsupervised framework to generate a KG from a real-world dataset. The framework incorporates prompting with knowledge entity metadata and generating predicate embeddings with the pretrained Masked Language Model (MLM) RoBERTa. Predicate embeddings were clustered to form conceptual groups and feature tokens were used to derive relation labels. In addition, we conducted a comparative study on the effects of different prompting templates. The resulting relation labels were evaluated by human annotators, which indicated that prompt-based learning, if applied appropriately, can help with deducing conceptualized relations. Our framework proposed a way to improve the quality of KGs acquired using traditional Relation Extraction (RE). It can also assist human experts effectively in semi-automated knowledge acquisition.},
  keywords={Knowledge acquisition;Knowledge graphs;Metadata;Cybernetics},
  doi={10.1109/SMC53992.2023.10394554},
  ISSN={2577-1655},
  month={Oct},}@ARTICLE{10839124,
  author={Wang, Xingfei and Zhang, Ke and Niu, Muyuan and Wang, Xiaofen},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={SemSI-GAT: Semantic Similarity-based Interaction Graph Attention Network for Knowledge Graph Completion}, 
  year={2025},
  volume={},
  number={},
  pages={1-13},
  abstract={Graph Neural Networks (GNNs) show great power in Knowledge Graph Completion (KGC) as they can handle nonEuclidean graph structures and do not depend on the specific shape or topology of the graph. However, many current GNNbased KGC models have difficulty in effectively capturing and utilizing the substantial structure and global semantic information in Knowledge Graphs (KGs). For more effective use of GNN for KGC, we innovatively propose the Semantic Similaritybased Interaction Graph Attention Network (SemSI-GAT) for the KGC task. In SemSI-GAT, we utilize BERT, a pre-trained language model, to learn the global semantic information and obtain semantic similarity between entities and their neighbors. Furthermore, we creatively design a novel encoder network called the interaction graph attention network and introduce a semantic similarity sampling mechanism to optimize the aggregation of interaction information between neighbors. By aggregating local features with interaction features, this network can generate more expressive structural embeddings. This network generates more expressive embeddings by fusing global semantic information, local structure features, and interaction features. The experimental evaluations demonstrate that the proposed SemSIGAT outperforms existing state-of-the-art KGC methods on four benchmark datasets},
  keywords={Semantics;Knowledge graphs;Encoding;Bidirectional control;Decoding;Computational modeling;Vectors;Training;Predictive models;Feature extraction;Knowledge graph completion;interaction information;semantic similarity sampling;graph attention network},
  doi={10.1109/TKDE.2025.3528496},
  ISSN={1558-2191},
  month={},}@INPROCEEDINGS{10800417,
  author={Chow, Sabrina and Guo, Lilian and Chow, Jonathan and Chia, Chelsea and Li, Sarah and Huang, Dong-Yan},
  booktitle={2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP)}, 
  title={Semantic Search Using LLM-Aided Topic Generation on Knowledge Graphs for Paper Discovery}, 
  year={2024},
  volume={},
  number={},
  pages={353-357},
  abstract={The exponential growth of academic papers presents a huge challenge for researchers, exacerbating the already tedious literature review process. Current tools like Google Scholar and Connected Papers offer solutions for text-based and citation-based searches but fail to address the need for finding semantically similar yet terminologically different papers efficiently. This paper proposes an innovative approach to paper discovery using semantic search to create a knowledge graph of topics and papers. By generating a tree of topics using ChatGPT 4o and calculating semantic similarity with SciBERT, this method aims to uncover relevant papers overlooked by traditional citation-based searches. The solution, validated through quantitative evaluation, demonstrates the potential to improve the efficiency and comprehensiveness of paper discovery.},
  keywords={Semantic search;Navigation;Bibliographies;Focusing;Knowledge graphs;Chatbots;Rendering (computer graphics);Internet;Semantic Search;Knowledge Graphs;Literature Review;Natural Language Processing (NLP);SciBERT},
  doi={10.1109/ISCSLP63861.2024.10800417},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10814632,
  author={Von der Assen, Jan and Huertas, Alberto and Sharif, Jamo and Feng, Chao and Bovet, Gérôme and Stiller, Burkhard},
  booktitle={2024 20th International Conference on Network and Service Management (CNSM)}, 
  title={ThreatFinderAI: Automated Threat Modeling Applied to LLM System Integration}, 
  year={2024},
  volume={},
  number={},
  pages={1-3},
  abstract={Artificial Intelligence (AI) is a rapidly integrated technology, significantly contributing to advancements like 6G. However, its swift adoption raises considerable security concerns. Large Language Models (LLMs) pose risks such as spear phishing, code injections, and remote code execution. Conventional threat modeling, used in secure software development, faces challenges when applied to AI systems, as existing methodologies are designed for traditional software. Furthermore, AI-specific threat modeling research is sparse and lacks approaches providing practical support or automation. Thus, this demo paper presents ThreatFinderAI, an asset-centric threat modeling and risk assessment framework. ThreatFinderAI fulfills seven steps aligned with AI system design and transforms AI threat and control knowledge bases into a queryable knowledge graph for automated asset identification and threat elicitation. It also proposes business impact analysis and expert estimates for AI threat impact quantification. In the demonstration, ThreatFinderAI is illustrated by securing a customer care application relying on LLMs. Through this, it is demonstrated how the proposed framework can be used to identify relevant threats and practical countermeasures and communicate strategic risk.},
  keywords={Threat modeling;Codes;Transforms;System integration;Software;Security;Risk management;Artificial intelligence;System analysis and design;Software development management;Threat Modeling;AI Systems;Large Language Models;AI Security},
  doi={10.23919/CNSM62983.2024.10814632},
  ISSN={2165-963X},
  month={Oct},}@ARTICLE{9386086,
  author={Liu, Shu-Kan and Xu, Rui-Lin and Geng, Bo-Ying and Sun, Qiao and Duan, Li and Liu, Yi-Ming},
  journal={IEEE Access}, 
  title={Metaknowledge Extraction Based on Multi-Modal Documents}, 
  year={2021},
  volume={9},
  number={},
  pages={50050-50060},
  abstract={The triplet-based knowledge in large-scale knowledge bases is most likely lacking in structural logic and problematic of conducting knowledge hierarchy. In this paper, we introduce the concept of metaknowledge to knowledge engineering research for the purpose of structural knowledge construction. Therefore, the Metaknowledge Extraction Framework and Document Structure Tree model are presented to extract and organize metaknowledge elements (titles, authors, abstracts, sections, paragraphs, etc.), so that it is feasible to extract the structural knowledge from multi-modal documents. Experiment results have proved the effectiveness of metaknowledge elements extraction by our framework. Meanwhile, detailed examples are given to demonstrate what exactly metaknowledge is and how to generate it. At the end of this paper, we propose and analyze the task flow of metaknowledge applications and the associations between knowledge and metaknowledge.},
  keywords={Task analysis;Optical character recognition software;Layout;Object detection;Semantics;Knowledge based systems;Computational modeling;Metaknowledge;multi-modal;document layout analysis;knowledge graph},
  doi={10.1109/ACCESS.2021.3068728},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10062878,
  author={Li, Jianli and Yilahun, Hankiz and Hamdulla, Askar},
  booktitle={2022 International Conference on Virtual Reality, Human-Computer Interaction and Artificial Intelligence (VRHCIAI)}, 
  title={Chinese Named Entity Recognition Based on Improved K-BERT}, 
  year={2022},
  volume={},
  number={},
  pages={248-254},
  abstract={Named entity recognition of Chinese text using pre-trained models is the mainstream approach at present, and the proposed K-BERT model overcomes the problem that BERT models do not possess background knowledge. We use the Chinese pre-trained model BERT-wwm and adversarial training based on full-word masking technology on the basis of K-BERT to improve the disadvantages of poor robustness of traditional K-BERT neural network and the existence of WordPiece sequence mask of traditional BERT. The experimental results on three open source datasets, MEDICAL_NER, MSRA_NER and FINANCIAL_NER, show that the evaluation index F1-score is improved after adding the adversarial training model. And the comparison experiments verify that adding adversarial training can elevate the prediction ability and robustness.},
  keywords={Training;Human computer interaction;Solid modeling;Text recognition;Bit error rate;Neural networks;Virtual reality;K-BERT;Knowledge Graph;Chinese Named Entity Recognition;Pre-trained language models;Adversarial Training},
  doi={10.1109/VRHCIAI57205.2022.00050},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10229400,
  author={Wu, Kuan-Wei and Hsu, Tz-Huan and Huang, Yen-Hao and Chen, Yi-Shin and Wang, Ho-Lung and Hsieh, Bing-Jing and Hsu, Chi-Hung},
  booktitle={2023 IEEE 24th International Conference on Information Reuse and Integration for Data Science (IRI)}, 
  title={LeCAR: Leveraging Context for Enhanced Automotive Specification Retrieval}, 
  year={2023},
  volume={},
  number={},
  pages={185-190},
  abstract={In the domain of automotive manufacturing, specification documents represent intricate descriptions detailing every aspect of a product, design, or service. Conventionally, these specifications demand the deployment of expert teams to manually identify crucial data from the extensive documentation. The need to automate the extraction of candidate information from these documents is increasingly pressing in this industry. This research encounters two central challenges: Firstly, the queries for the specifications input by users are typically concise and ambiguous; secondly, not every word in a query carries the same significance. In response to these challenges, we propose LeCAR, which exploits contextual data to clarify query sentences and concentrate the search scope. Our experiments validate that the proposed method outperforms existing techniques that employ pre-trained language models, all without necessitating additional training data.},
  keywords={Training;Semantics;Training data;Knowledge graphs;Pressing;Information retrieval;Product design;Information Retrieval;Dense Retrieval;Specific Domain;Knowledge Graph},
  doi={10.1109/IRI58017.2023.00038},
  ISSN={2835-5776},
  month={Aug},}@INPROCEEDINGS{9719180,
  author={Chen, Meiling and Tian, Ye and Wang, Zhaorui and Jiang, Bo and Xu, Hong},
  booktitle={2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI)}, 
  title={A Comprehensive Study of Cognitive Graphs: Techniques, Applications, and Challenges}, 
  year={2021},
  volume={},
  number={},
  pages={1108-1122},
  abstract={The realization of third-generation artificial intelligence (AI) requires the evolution from perceptual intelligence to cognitive intelligence; in this context, knowledge graphs may no longer meet practical needs. Based on dual channel theory, cognitive graphs are established and developed by coordinating an implicit extraction module and an explicit reasoning module, as well as integrating knowledge graphs, cognitive reasoning, and logical expressions, which have achieved successes in multi-hop question answering. The widespread use of cognitive graphs in advanced AI applications (e.g., large-scale knowledge representations and intelligent responses) is desirable, which will promote the development of AI dramatically. This review discusses cognitive graphs systematically and elaborately, including their basic concepts, generation, theories, and technologies. Moreover, we try to predict the development of cognitive intelligence in the short-term future and further encourage more research works and studies.},
  keywords={Deep learning;Information science;Knowledge representation;Learning (artificial intelligence);Computer architecture;Cognition;Cognitive science;cognitive graph;knowledge graph;knowledge reasoning;natural language generation},
  doi={10.1109/CISAI54367.2021.00222},
  ISSN={},
  month={Sep.},}@ARTICLE{8047091,
  author={Zhou, Nan and Li, Di and Li, Song and Wang, Shiyong and Liu, Chengliang},
  journal={IEEE Access}, 
  title={Model-Based Development of Knowledge-Driven Self-Reconfigurable Machine Control Systems}, 
  year={2017},
  volume={5},
  number={},
  pages={19909-19919},
  abstract={To accommodate the trend toward mass customization launched by intelligent manufacturing in the era of Industry 4.0, this paper proposes the combination of model-driven engineering and knowledgedriven engineering during the development process of self-reconfigurable machine control systems. The complete tool chain for model development, execution, and reconfiguration is established. For the design phase, a machine-control-domain-specific modeling language and the supporting design environment are developed. With regard to the execution stage, a runtime framework compliant with the IEC 61499 standard is proposed. On the ground of the modeling environment and the reconfigurable run-time framework, a self-adaptive control module is developed to establish the close-loop self-reconfiguration infrastructure. The ontological representation of knowledge base toward this end is described, along with extendable SQWRL rules specified to automatically initiate the reconfiguration process in the cases of external user demands and internal faults. A prototype motion control kernel in the low-level layer of machine control system architecture is developed with the proposed modeling language and is then deployed to the runtime framework. Two case studies on self-reconfiguration of the proof-of-concept motion control kernel are demonstrated, which prove the feasibility of our proposal.},
  keywords={Machine control;Computer architecture;Software;IEC Standards;Control systems;Real-time systems;Microwave integrated circuits;Reconfiguration;machine control system;domain-specific modeling language;ontology;IEC 61499},
  doi={10.1109/ACCESS.2017.2754507},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8088312,
  author={Hoppe, Tobias and Eisenmann, Harald and Viehl, Alexander and Bringmann, Oliver},
  booktitle={2017 IEEE International Systems Engineering Symposium (ISSE)}, 
  title={Shifting from data handling to knowledge engineering in aerospace industry}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  abstract={The development of increasingly complex systems with improved quality levels becomes more and more challenging. Engineering data frameworks with integrated system models have been developed to manage such systems. This paper presents the experiences that have been made in digital systems engineering in the aerospace domain and focuses on the roadmap that has been taken to establish a knowledge engineering framework. While working with first versions of these tools, it became obvious that an engineering framework reflecting all aspects of an engineering data object was required. In addition, data analytics and technologies used to check data consistency became increasingly relevant. As a consequence, semantically rich data models expressed by ontologies come into focus of forming the engineering framework baseline in conjunction with related technologies such as reasoning, error avoidance based on data analytics, and knowledge-driven engineering environments.},
  keywords={Data models;Semantics;Ontologies;OWL;Unified modeling language;Model-based Systems Engineering;Digitalization;Industry 4.0;Ontology;Semantic Engineering Framework},
  doi={10.1109/SysEng.2017.8088312},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9256484,
  author={Helmke, Hartmut and Kleinert, Matthias and Ohneiser, Oliver and Ehr, Heiko and Shetty, Shruthi},
  booktitle={2020 AIAA/IEEE 39th Digital Avionics Systems Conference (DASC)}, 
  title={Machine Learning of Air Traffic Controller Command Extraction Models for Speech Recognition Applications}, 
  year={2020},
  volume={},
  number={},
  pages={1-9},
  abstract={Increasing digitization and automation is a widely accepted method to cope with the challenges of constantly increasing air traffic. The analogue communication of air traffic controllers (ATCo) to pilots has been excluded so far from the digitization process. However, the content of this communication is of decisive importance for various automation systems. Although Assistant Based Speech Recognition (ABSR) has recently significantly improved the recognition performance and, therefore, enables the digitization of ATCo-pilot-communication, its adaptation to other airports is a critical and costly process, This is even more important, if ATCos tend to deviate from the published ICAO phraseology: “start reducing to two fifty” instead of “reduce two five zero knots” is just an example. User acceptance requires that these deviations are also correctly recognized. Therefore, this paper presents an approach, which automatically learns a so-called Command Extraction Model from labelled controller utterances. The initial Command Extraction Model without learning only covers 60% of the commands, whereas the automatically learned Command Extraction Model covers more than 98%. With just six hours of training data we could achieve 94%.},
  keywords={Speech recognition;Atmospheric modeling;Adaptation models;Ontologies;Engines;Annotations;Airports;Automatic Speech Recognition;Machine Learning;Annotation;Ontology;Controller Command Extraction Model},
  doi={10.1109/DASC50938.2020.9256484},
  ISSN={2155-7209},
  month={Oct},}@INPROCEEDINGS{8577518,
  author={Wan, Guangxi and Wang, Peng and Xue, Lingling and Zeng, Peng},
  booktitle={2018 IEEE 3rd Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, 
  title={An Integrated Design Method For Cyber-Physical Production Systems}, 
  year={2018},
  volume={},
  number={},
  pages={791-796},
  abstract={The industrial cyber-physical production system contains a variety of heterogeneous devices and models, which increase the complexity of system design, particularly in software programming. Although the proposed model-driven engineering (MDE) is applied to industrial automation for this issue, the inherent complex dependencies and constraints between models and the availability of suitable tools for code generation limit the application of the model-driven approach in practice, especially as the system becomes more enormous. The component-based design (CBD) approach, which is a bottom-up approach, in contrast to the MDE that is top-down, is proposed to manage the complexity of software aspect based on the idea that building a system from existing components instead of the scratch. However, CBD can't provide the whole system model, so other ways should be adopted to design the system structure in advance. This paper presents an integrated design approach based on the combination of CBD and MDE to realize auto-design for cyber-physical production systems.},
  keywords={Unified modeling language;Automation;Control systems;Software;Ontologies;Testing;Integrated design;Component-based Design;IEC 61499;Industrial automation;Model-based design;Ontology;Semantic technology},
  doi={10.1109/IAEAC.2018.8577518},
  ISSN={2381-0947},
  month={Oct},}@INPROCEEDINGS{10651121,
  author={Yang, Dongyu and Deng, Wenqing and Wang, Zhe and Wang, Kewen and Zhuang, Zhiqiang and Li, Hao},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Learning Choice Nuance for Multiple-Choice Commonsense Question Answering}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Existing models for commonsense question answering (CQA) usually focus on combining pre-trained language models (PLMs) and structured knowledge graphs (KGs) for joint reasoning. However, such approaches encode a QA context (i.e., a pair of the question and a choice) separately from other choices, ineffective for explicitly capturing useful subtle differences among the choices, which results in incorrect answers in some cases. This paper proposes a novel model LNC (Learning Nuance among Choices) for addressing this problem and thus provides an improved approach to multiple-choice question answering. Specifically, LNC explicitly interacts between the text knowledge corresponding to each choice and the external KG knowledge corresponding to each choice, and removes the commonalities among similar choices, allowing the model to focus on different relevant knowledge based on the choices, thereby distinguishing semantically similar choices. Experimental results on major benchmark datasets show that LNC is competitive comparing to the baseline models.},
  keywords={Neural networks;Knowledge based systems;Knowledge graphs;Benchmark testing;Question answering (information retrieval);Cognition;Optimization;Commonsense Question Answering;Multiple-Choice Question Answering;Pre-trained Language Model;Knowledge Graph},
  doi={10.1109/IJCNN60899.2024.10651121},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10394522,
  author={Li, Jinbao and Wang, Guangchen and Tian, Cheng and Liu, Song},
  booktitle={2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={A Dynamic Global Semantic Fusion GNN Model For Commonsense Question Answering}, 
  year={2023},
  volume={},
  number={},
  pages={659-665},
  abstract={Commonsense question answering (CSQA) is a challenging learning task that aims to give correct answers to commonsense questions. CSQA models combining large pretrained language models with knowledge graphs are proposed to perform one-way or two-way information fusion to enhance their commonsense reasoning ability. However, existing CSQA models only fuse local information at the word level, ignoring the global semantic information fusion. Furthermore, current CSQA models often introduce noise nodes when constructing the knowledge subgraph. In addition, existing methods neglect the edge information in message aggregation. To solve these shortcomings, we propose a novel CSQA model named MDEQA. In our model, we design the multi-layer attention fusion module to bidirectionally fuse the word-level local information and global semantic information of question context and knowledge subgraph. Moreover, we design the dynamic graph neural network module with improved GAT and aggregating edge information to form the dynamic subgraphs which alleviate the interference of noise nodes on reasoning and enhance the commonsense reasoning ability of our model. Finally, we evaluated our model on CommonsenseQA and OpenBookQA datasets to compare with other baseline models.},
  keywords={Fuses;Semantics;Question answering (information retrieval);Graph neural networks;Task analysis;Commonsense reasoning;Context modeling;Commonsense Question Answering;Large Pretrained Language Model;Knowledge Graph;Multi-layer Attention Fusion;Dynamic Graph Neural Network},
  doi={10.1109/SMC53992.2023.10394522},
  ISSN={2577-1655},
  month={Oct},}@INPROCEEDINGS{9435777,
  author={Sarsembayeva, Talshyn and Mansurova, Madina and Chikibayeva, Darya and Karymsakova, Dariya},
  booktitle={2020 IEEE 8th Workshop on Advances in Information, Electronic and Electrical Engineering (AIEEE)}, 
  title={The Problem of Named Entities Unification based on Geographical Ontologies}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={The subject of this research is to develop a system for extracting knowledge from both semi-structured and unstructured data and filling with this system a knowledge base that would provide support for decision-making on any problematic issues. The article deals with the problem of unification of named entities based on geographical ontologies.},
  keywords={Knowledge engineering;Decision making;Knowledge based systems;Machine learning;Ontologies;Information retrieval;Data models;semi-structured and unstructured data;decision support system;ontology;thesaurus;information extraction;knowledge extraction;knowledge base;machine learning;named entities},
  doi={10.1109/AIEEE51419.2021.9435777},
  ISSN={2689-7342},
  month={April},}@INPROCEEDINGS{9921520,
  author={Deppe, Sahar and Brandt, Lukas and Brünninghaus, Marc and Papenkordt, Jörg and Heindorf, Stefan and Tschirner-Vinke, Gudrun},
  booktitle={2022 IEEE 27th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={AI-Based Assistance System for Manufacturing}, 
  year={2022},
  volume={},
  number={},
  pages={1-4},
  abstract={Manufacturing companies are challenged to make the increasingly complex work processes equally manageable for all employees to prevent an impending loss of competence. In this contribution, an intelligent assistance system is proposed enabling employees to help themselves in the workplace and provide them with competence-related support. This results in increasing the short- and long-term efficiency of problem solving in companies.},
  keywords={Employment;Companies;Manufacturing;Problem-solving;Manufacturing automation;Assistance system;Knowledge graph;Information retrieval;Neural networks;AR},
  doi={10.1109/ETFA52439.2022.9921520},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10219862,
  author={You, Jiuxiang and Yang, Zhenguo and Li, Qing and Liu, Wenyin},
  booktitle={2023 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={A Retriever-Reader Framework with Visual Entity Linking for Knowledge-Based Visual Question Answering}, 
  year={2023},
  volume={},
  number={},
  pages={13-18},
  abstract={In this paper, we propose a Retriever-Reader framework with Visual Entity Linking (RR-VEL) for knowledge-based visual question answering. Given images and original questions, the visual entity linking (VEL) module extracts key entities in images to replace the question referents for semantic disambiguation, achieving entity-oriented queries with explicit entities. Furthermore, the Retriever encodes the queries and knowledge items by Bert with a feed-forward layer, and obtains a set of knowledge candidates. The Reader encodes the questions with image captions and knowledge candidates in two branches, which avoids their interference during self-attentive encoding. Finally, the decoder of Reader fuses the encoded features to generate answers. Extensive experiments conducted on the two public datasets show that our method significantly outperforms the existing baselines.},
  keywords={Visualization;Image coding;Fuses;Knowledge based systems;Semantics;Interference;Benchmark testing;VQA;Knowledge graph;Entity linking},
  doi={10.1109/ICME55011.2023.00011},
  ISSN={1945-788X},
  month={July},}@INPROCEEDINGS{9664111,
  author={Yang, Jianxi and Yang, Xiaoxia and Li, Ren and Luo, Mengting},
  booktitle={2021 7th International Conference on Systems and Informatics (ICSAI)}, 
  title={A Multi-Task Information Extraction Framework for Bridge Inspection Based on Joint Neural Networks}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Focused on the issue that insufficient information extraction and knowledge services in the bridge management and maintenance domain, a multi-task information extraction framework for bridge inspection based on joint neural networks is proposed. Firstly, a multi-task information extraction training dataset for bridge inspection is constructed and a distributed representation of the text is obtained using BERT as the embedding layer. Secondly, the subtasks of topic word detection and other bridge inspection information extraction are jointly learned by sharing BERT weights and fine-tuning, and the context features are further extracted in depth. Finally, the bridge inspection knowledge service is used as application examples to verify the effectiveness of the bridge inspection information extraction model in actual application scenarios such as bridge domain question answering. In the comparison experiments with mainstream models, the proposed method outperforms the mainstream models with F1-score of 85.27%, 72.73%, and 90.76% for the NER, RE, and topic word detection respectively. The experimental results show that the model can meet the requirements of a variety of practical tasks for information extraction of bridge inspection.},
  keywords={Bridges;Bit error rate;Neural networks;Inspection;Information retrieval;Multitasking;Knowledge discovery;bridge inspection;information extraction;BERT;knowledge graph;knowledge base question answering},
  doi={10.1109/ICSAI53574.2021.9664111},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10098037,
  author={Ma, Haoyang and Li, Zeyu and Guo, Hongyu},
  booktitle={2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Using Noise and External Knowledge to Enhance Chinese Pre-trained Model}, 
  year={2022},
  volume={},
  number={},
  pages={476-480},
  abstract={Pre-trained language models (PLMs) have the risk of overfitting pre-trained tasks and data in fine-tuning, while Chinese PLMs often ignore external knowledge such as word and sentence to learn representations. Therefore, we propose a Chinese PLM enhancement method using noise and external knowledge (NEK). NEK first adds different uniform noises to the PLM according to the standard deviation of different parameter matrices, so as to obtain the perturbed PLM. In the fine-tuning phase, NEK builds a heterogeneous linguistic graph based on external knowledge. This module adopts a graph-based approach to generalize information of different granularities in Chinese linguistics, and enhances Chinese PLM on this basis. Experimental results show that NEK brings performance improvements to a variety of different Chinese PLMs on six natural language processing tasks on eight benchmark datasets.},
  keywords={Linguistics;Benchmark testing;Natural language processing;Data models;Task analysis;Artificial intelligence;Standards;External Knowledge;Graph neural network;Pre-trained language model},
  doi={10.1109/ICTAI56018.2022.00076},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{9914462,
  author={Wu, Jun and Yan, Qingguo and Dong, Qinwei and Zha, Xianguang and Cui, Lin and Zhao, Xindong and Dai, Wei and Luo, Chong},
  booktitle={2022 9th International Conference on Dependable Systems and Their Applications (DSA)}, 
  title={Task-driven Dialogue System based on Dynamic Intention Capture}, 
  year={2022},
  volume={},
  number={},
  pages={859-864},
  abstract={With the continuous development of artificial intelligence technology, dialogue system has attracted more and more attention because of its strong applicability and wide application scenarios. It has gradually entered all aspects of people's lives. The development of science and technology such as speech recognition and synthesis, natural language processing, machine learning, deep neural network and soon has also accelerated the transformation of this process, Make the machine closer to the goal of smooth dialogue with people. This paper mainly studies the task driven human-computer interaction system. Aiming at the sequence track of humancomputer conversation, we use the encoding and decoding model to capture human's dynamic intention. From the final system, this way can significantly improve the accuracy of machine response compared with the previous way of only recommending answers through single state conversation.},
  keywords={Human computer interaction;Knowledge engineering;Deep learning;Neural networks;Oral communication;Speech recognition;Natural language processing;man-computer communication;dynamic intention;knowledge graph;encoder-decoder network;natural language processing},
  doi={10.1109/DSA56465.2022.00121},
  ISSN={2767-6684},
  month={Aug},}@INPROCEEDINGS{10222675,
  author={Dao, Minh-Son and Zettsu, Koji},
  booktitle={2023 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)}, 
  title={Leveraging Knowledge Graphs for CheapFakes Detection: Beyond Dataset Evaluation}, 
  year={2023},
  volume={},
  number={},
  pages={99-104},
  abstract={The proliferation of the internet and the availability of vast amounts of information have given rise to a critical and pressing issue of fake news. Among the various forms of fake news, cheapfakes are particularly prominent in deceiving people. Existing research on cheapfakes detection has primarily focused on analyzing the context and correlation between textual and visual information, but has largely overlooked the significance of external knowledge. As a result, most previous approaches, apart from the baseline of ICME‘23 Grand Challenge on Detecting Cheapfakes, have heavily relied on evaluating the dataset itself to improve performance. However, despite achieving impressive results on public test datasets, these approaches often suffer from poor performance in real-world scenarios due to their overreliance on the given dataset. In this study, we propose a novel approach that utilizes knowledge graphs to address the issue of insufficient information from external knowledge. Unlike previous approaches, our proposal does not directly alter or participate in the public test dataset to enhance performance, which can potentially result in significant overfitting. Our proposed approach achieved an accuracy score of 83.52% on Task 1, surpassing the baseline by 1.7%, and an accuracy score of 84% on Task 2, outperforming the best result from the previous challenge by 8%.},
  keywords={Visualization;Correlation;Conferences;Knowledge graphs;Pressing;Syntactics;Linguistics;Deep Learning;Computer Vision;Natural Language Processing;Knowledge Graph;Cheapfakes;Misinformation;News},
  doi={10.1109/ICMEW59549.2023.00024},
  ISSN={},
  month={July},}@INPROCEEDINGS{10391221,
  author={Zhang, Zilin and YuFan and Li, BaiHong and Zeng, ZhiZhong},
  booktitle={2023 International Conference on Intelligent Education and Intelligent Research (IEIR)}, 
  title={Review of research on synonym equivalence relation mining}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Synonym discovery, also known as synonym relation mining or synonym extraction, aims to identify and establish synonymous relationships between words, phrases, or sentences. The primary objective of this relationship mining is to enhance the performance of natural language processing (NLP) tasks, such as information retrieval, question-answering systems, text summarization, and machine translation. Considering that there are still numerous areas and issues awaiting further research in synonym relation mining, this paper provides a comprehensive review of the research methods employed in this field over the past two decades. The review is organized into four main categories. The first category explores the u e of language models for synonym extraction, including techniques such as word embeddings and the recent BERT model. The second category focuses on computing semantic similarity in synonym semantic spaces. The third category examines neural network-based approaches for synonym relation mining. Lastly, the fourth category delves into constructing synonym relationships based on knowledge graphs. Additionally, this paper provides an outlook and summary of potential future developments in this direction, with the aim of offering valuable guidance for future research in the field.},
  keywords={Computational modeling;Semantics;Education;Knowledge graphs;Information retrieval;Machine translation;Task analysis;Synonym discovery;equivalence relation detection;pre-trained model (BERT);knowledge graph;neural network},
  doi={10.1109/IEIR59294.2023.10391221},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10212829,
  author={Xian, Guangming and Zhang, Wencong and Lan, Fucai and Lin, Yifan and Lin, Yanhang},
  booktitle={2023 5th International Conference on Electronic Engineering and Informatics (EEI)}, 
  title={Multimodal Knowledge Triple Extraction Based on Representation Learning}, 
  year={2023},
  volume={},
  number={},
  pages={684-689},
  abstract={Knowledge based visual question-answering is an emerging technique that combines computer vision and natural language processing to address image-based questions. This approach requires the model to possess internal reasoning ability and incorporate external knowledge to enhance its generalization performance. Knowledge graphs are commonly employed to integrate various types of knowledge, such as image knowledge, text knowledge, and basic common sense, into visual language question answering models, significantly enhancing their interpretability. However, introducing knowledge into these models often involves retrieval and pre-training methods, which can introduce noise due to the local correlation among knowledge sources and the existence of modality gaps, thereby affecting the model's performance. To mitigate this issue, this paper proposes a multimodal knowledge extraction approach based on distributed representation learning. The approach models inexpressible multimodal facts using explicit triples, considering the semantic gap between visual and textual modalities. Cross-modal representations are obtained through an attention mechanism, resulting in knowledge triplets comprising visual features, cross-modal representations, and text features. By incorporating these knowledge triplets into the visual language question answering model, the task is transformed into pattern matching using knowledge triples. The proposed approach comprehensively considers multiple factors, is not restricted to specific forms of knowledge, and can effectively incorporate a substantial amount of knowledge. Experimental results demonstrate its superiority, with a performance improvement of 0.5% on the OKVQA dataset compared to the baseline model, highlighting its strong generalization ability.},
  keywords={Representation learning;Visualization;Knowledge based systems;Semantics;Transforms;Knowledge graphs;Feature extraction;component;Knowledge Graph;Knowledge-based Visual Question Answering;Represention Learning},
  doi={10.1109/EEI59236.2023.10212829},
  ISSN={},
  month={June},}@INPROCEEDINGS{10687974,
  author={Cao, Han and Wei, Lingwei and Zhou, Wei and Hu, Songlin},
  booktitle={2024 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={Multi-source Knowledge Enhanced Graph Attention Networks for Multimodal Fact Verification}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Multimodal fact verification is an under-explored and emerging field that has gained increasing attention in recent years. The goal is to assess the veracity of claims that involve multiple modalities by analyzing the retrieved evidence. The main challenge in this area is to effectively fuse features from different modalities to learn meaningful multimodal representations. To this end, we propose a novel model named Multi-Source Knowledge-enhanced Graph Attention Network (MultiKE-GAT). MultiKE-GAT introduces external multimodal knowledge from different sources and constructs a heterogeneous graph to capture complex cross-modal and cross-source interactions. We exploit a Knowledge-aware Graph Fusion (KGF) module to learn knowledge-enhanced representations for each claim and evidence and eliminate inconsistencies and noises introduced by redundant entities. Experiments on two public benchmark datasets demonstrate that our model outperforms other comparison methods, showing the effectiveness and superiority of the proposed model.},
  keywords={Knowledge engineering;Fuses;Noise;Benchmark testing;Feature extraction;multimodal fact verification;multi-source knowledge;graph attention network},
  doi={10.1109/ICME57554.2024.10687974},
  ISSN={1945-788X},
  month={July},}@INPROCEEDINGS{10435553,
  author={Wang, Hongchang and Ma, Qingxin},
  booktitle={2023 3rd International Conference on Electronic Information Engineering and Computer Science (EIECS)}, 
  title={Domain Knowledge Enhanced BERT for Chinese Named Entity Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={406-409},
  abstract={Digitalization of educational resources and revolutionizing knowledge frameworks are essential in achieving smart education. Knowledge graphs play a pivotal role in addressing knowledge representation, correlation, and sharing within digital education. Named Entity Recognition (NER) is a fundamental task in constructing knowledge graphs. This study introduces a Domain Knowledge-Enhanced BERT Chinese NER model, DK-BERT-CRF (Domain Knowledge BERT CRF), to address the deficiency of lexical information features within the Chinese NER task using the BERT pre-trained model. To construct an automated labeling dataset, we perform an automated labeling dataset construction based on ChatGPT, focusing on the example of computer science's data structures. We conduct experiments and evaluations using this dataset and the general CLUENER2020 dataset. Comparative experiments with BERT+CRF and BiLSTM+CRF are also conducted. The experimental results demonstrate that the DK-BERT-CRF model, enriched with domain knowledge, exhibits an improved F1 score compared to the other two models. Particularly, the DK-BERT-CRF model showcases enhanced F1 scores on the computer science data structure dataset after the incorporation of domain knowledge.},
  keywords={Computer science;Annotations;Computational modeling;Knowledge graphs;Data structures;Labeling;Task analysis;Smart Education;Knowledge Graph;Named Entity Recognition;BERT;ChatGPT},
  doi={10.1109/EIECS59936.2023.10435553},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10551258,
  author={Fang, Yunfei and Chen, Yong and Jiang, Zhonglin and Xiao, Jun and Ge, Yanli},
  booktitle={2023 4th International Conference on Computer, Big Data and Artificial Intelligence (ICCBD+AI)}, 
  title={Automatic Knowledge Structuration of Automotive User Manual for Question Answering}, 
  year={2023},
  volume={},
  number={},
  pages={184-190},
  abstract={Automotive user manuals serve as repositories of valuable information pertaining to a vehicle, leveraging question answering (QA) systems provides users with a convenient means to access this knowledge. In pursuit of developing an efficient QA system for such documents, this paper proposes the organization of the content into a structured knowledge graph-like triplet format.After conducting a comprehensive analysis of the automotive user manual content, we introduce a <subject, function, content> (<s, f, c>) triplet knowledge representation to represent the knowledge. Our approach involves a three-step pipeline for extracting these triplets from semi-structured XML documents. Central to this structure is the "content" node, forming the core of knowledge items. Leveraging the in-context learning abilities of an off-the-shelf Large Language Model (LLM), specifically ChatGPT, the "subject" and "function" components are induced from the "content" node. To ensure compactness and coherence in knowledge representation, a tailored phrase normalization process is designed to select identical phrases.Additionally, a LLM-powered evaluation method is employed to validate the extracted triplets, affirming their accuracy and relevance. This methodology demonstrates the effectiveness of our proposed approach in automating the structuration of knowledge within automotive user manuals for seamless QA.},
  keywords={Pipelines;XML;Manuals;Knowledge representation;Organizations;Question answering (information retrieval);Data models;large language models;question answering;knowledge graph construction;information extraction},
  doi={10.1109/ICCBD-AI62252.2023.00038},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10030076,
  author={Li, Zhiyuan and Jin, Zhou and Wang, Rujing and Ji, Jie and Liu, Haiyan},
  booktitle={2022 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={A Generative Adversarial Net Assisted Method for User Intention Recognition on Imbalanced Dataset}, 
  year={2022},
  volume={},
  number={},
  pages={157-163},
  abstract={Class imbalance is a classic problem in machine learning and deep learning. This problem bothers recognizing user intention in knowledge graph question answering domain. Some intentions would not be recognized well when the training set is unbalanced. Sampling and weighted loss methods are widely used to address it. However, these methods pay little attention to the form of question texts in feature space. We propose a novel method based on generative adversarial net to deal with imbalanced class problem, which firstly learns how a user question text of a certain class is represented in feature space and then generates samples. Our method designs a model that contains three parts: text encoder, generator, and discriminator. The text encoder transforms user question text into words vectors. The generator generates vectors that would converge to expected classes and the discriminator is responsible for recognizing classes. We split the training period into two stages. In the first stage, the discriminator is going to distinguish between samples from encoder and generator. In the second stage, the generator learns data distribution and generates related samples to enhance the ability of encoder. Experimental results show that our method can balance the model performance on each class and significantly outperforms traditional methods on unbalanced datasets for user intention recognition.},
  keywords={Training;Deep learning;Fluctuations;Design methodology;Transforms;Generators;Robustness;user intention recognition;imbalanced dataset;generative adversarial net;generating samples;two-stage training},
  doi={10.1109/ICKG55886.2022.00027},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10412757,
  author={Zhang, Bofeng and Yao, Xiuhong and Li, Haiyan and Aini, Mirensha},
  booktitle={2023 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={Chinese Medical Named Entity Recognition based on Expert Knowledge and Fine-tuning Bert}, 
  year={2023},
  volume={},
  number={},
  pages={84-90},
  abstract={Medical Named Entity Recognition (MNER) plays a pivotal role in Natural Language Processing (NLP), particularly within the medical domain. This research presents an innovative methodology for MNER, adeptly tackling the challenges presented by the diversity of terminologies, privacy issues, and the significant costs associated with annotation in Chinese medical records. In this study, we begin by assembling a meticulously curated medical terminology database, drawing upon the expertise of domain specialists. This comprehensive database enhances the precision in comprehending medical semantic structures, enabling more accurate recognition of named entities in medical texts. To further improve the performance of MNER, we leverage the power of the pre-trained Bidirectional Encoder Representations from Transformers (BERT) language model. By fine-tuning BERT on Chinese medical diagnostic and treatment texts, we achieve proficient recognition and categorization of entities, along with their respective types. Through rigorous experimentation on a large-scale, real-world Chinese medical record datasets, we showcase the superior performance of our proposed framework. Our comprehensive and novel approach is instrumental in accurately extracting named entities from medical texts, thereby enhancing clinical decision support, organizing medical knowledge, facilitating drug discovery, and contributing to a broad spectrum of other medical applications. The results of this study demonstrate the effectiveness and potential impact of our methodology in the field of medical named entity recognition. By addressing the challenges specific to Chinese medical records, we provide valuable insights and advancements that can benefit various healthcare applications and ultimately improve patient care. This research holds significant value and relevance in the field.},
  keywords={Terminology;Annotations;Databases;Text recognition;Semantics;Natural language processing;Medical diagnostic imaging;Medical Named Entity Recognition;Bert;Expert Knowledge;Fine-tuning},
  doi={10.1109/ICKG59574.2023.00016},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9194511,
  author={Peng, Yuwei and Jiang, Huan and Li, Rongrong and Peng, Zhiyong},
  booktitle={2020 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={PZXG: A Genealogy Data Service Platform for Kinship Management and Application}, 
  year={2020},
  volume={},
  number={},
  pages={505-512},
  abstract={Genealogy is one of the three kinds of historical literatures which are valuable to historical research and humanities research. However, traditionally, it is hard to build and revise a genealogy. And it is even impossible to manage and query the genealogy efficiently. In this paper, we proposed PZXG, which is a genealogy data service platform, to solve these issues. Firstly, for structured and unstructured genealogy data, we employ relational database, graph database and distributed file system to store them respectively. Secondly, depending on the hybrid data model of genealogy data, various management features like data acquisition, kin seeking, genealogy exhibition and automatic typesetting are provided. Then in order to extract knowledge from genealogy data, the features of routine statistics and query, root tracing, correlation analysis and data visualization in genealogical data are discussed, and several methods and ideas of text mining for genealogical text are given. Finally, two more urgent research points in the genealogical data management platform are suggested: extracting structural information from unstructured genealogy data and construction of genealogy-specific text language model.},
  keywords={Data models;Biographies;Data acquisition;Optical character recognition software;Data mining;Typesetting;Computer architecture;Genealogy Data;Service Platform;Kinship;Management},
  doi={10.1109/ICBK50248.2020.00077},
  ISSN={},
  month={Aug},}@ARTICLE{10428937,
  author={Wu, Kan and Zhou, Yonglin and Ma, Jun and Guo, Xianhui},
  journal={IEEE Access}, 
  title={Topic-Specific Political Stance Inference in Social Networks With Case Studies}, 
  year={2024},
  volume={12},
  number={},
  pages={21921-21935},
  abstract={Topic-specific political stance inference in social networks (SNs) aims at inferring target users’ attitudes toward different target topics. Traditional methods mainly used a language model to classify sentiments from the postings of the SN users. However, people’s stances are not always equal to their sentiments. Some others tried to build separate models toward different target topics. In many cases, though SN users talked about the target topics, the information given was limited; or they only expressed attitudes toward some other issues except the target topics. When information is incomplete, the methods that treat the topics independently fail to work, let alone for users who didn’t post any of the topics. To solve the above problems, we introduced a political knowledge graph (PKG) to supplement side information for users and topics and proposed a united Knowledge Graph-aware and Social Network-enhanced framework (KGSN) to capture not only the knowledge connections between topics but also the social connections between users. KGSN utilized two levels of graph convolutional networks, the one at the knowledge graph level generating knowledge-aware representations merging knowledge entities for the users and topics respectively, and the one at the social graph level generating social-enhanced representations merging social neighbors for the target users. Beyond that, the respective topic-specific attention mechanisms were leveraged to emphasize special knowledge entities in the knowledge graph and special neighboring users in the social graph. The advantages of KGSN are that: first, it can infer users’ attitudes toward more than one topic in one model; second, it can infer users’ implicit attitudes toward the target topics through users’ explicit attitudes toward the other issues; last but not least, even for users without any postings, KGSN can infer users’ implicit attitudes through their social neighbors. Finally, extensive experiments were conducted to demonstrate the superiority of KGSN over state-of-the-art models and case studies were investigated to testify the effectiveness of the model.},
  keywords={Social networking (online);Knowledge graphs;Blogs;Knowledge engineering;Training;Solid modeling;Knowledge based systems;Graph neural networks;Government;Graph neural networks;knowledge graph;political stance inference;social networks},
  doi={10.1109/ACCESS.2024.3360487},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9736481,
  author={Lim, Chae-Gyun and Lee, Dongkun and Lee, Young-Jun and Choi, Ho-Jin},
  booktitle={2022 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Knowledge Management Approach for Memory Components Based on User-friendly Conversational System}, 
  year={2022},
  volume={},
  number={},
  pages={401-403},
  abstract={Due to the recent technological development and the growth of computing resources, there are various studies that apply large-scale language models in the field of natural language processing such as conversational systems. Also, there are researches that attempt to maintain a conversation flow and naturally lead a dialogue by treating the contextual information exchanged with users from the perspective of knowledge. In this paper, we propose a method for managing various contextual information such as chat history, situations, and preferred topics based on a knowledge base and generating a conversation customized for the specific user. We design a schema of memory components to deal with the user's contextual information so that implement a Web-based conversational system, which is friendly come to those users. It is expected that these systems using the user-specific memory components will be helpful in such domains like education or customer consultation.},
  keywords={Conferences;Computational modeling;Memory management;Knowledge based systems;Education;Lead;Big Data;memory component;ontology-based approach;conversation history;conversational system},
  doi={10.1109/BigComp54360.2022.00091},
  ISSN={2375-9356},
  month={Jan},}@INPROCEEDINGS{9673037,
  author={Amini, M. Mohammad and Aldanondo, M. and Vareilles, E. and Coudert, T.},
  booktitle={2021 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)}, 
  title={Twenty Years of Configuration Knowledge Modeling Research. Main Works, What To Do Next?}, 
  year={2021},
  volume={},
  number={},
  pages={1328-1332},
  abstract={A configuration software (configurator) associates a knowledge base (KB) with a knowledge processing unit (PU). The KB describes all possible combinations of components while the PU overlays this knowledge with the customer requirements. Our work deals with the KB and the approaches, models, or tools for modeling configuration knowledge. Our goal is to present a small quantitative literature survey highlighting two work streams: the first one gathers modeling works dealing with constraint-based approaches while the second deals with ontologies, description logic, or object-oriented modeling approach. We will also consider hybrid approaches. We will present a quantitative analysis of published materials in Web of science over the last twenty years. The keywords occurrence versus time will also be studied in detail to identify tendencies in configuration knowledge modeling.},
  keywords={Knowledge engineering;Statistical analysis;Object oriented modeling;Engineering management;Knowledge based systems;Ontologies;Maintenance engineering;Configuration knowledge modeling;constraints satisfaction problem;ontology;UML;OWL;rules},
  doi={10.1109/IEEM50564.2021.9673037},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10471842,
  author={Do, Nhon V. and Mai, Thanh T.},
  booktitle={2023 RIVF International Conference on Computing and Communication Technologies (RIVF)}, 
  title={A Knowledge Representation Model for Designing the Knowledge Querying System in Programming Language C/C++}, 
  year={2023},
  volume={},
  number={},
  pages={366-371},
  abstract={Knowledge querying support systems need to assist users in querying the knowledge, relationships, or combination of multiple requirements. A proper knowledge representation model and the well-structured query language play important roles in the developing the knowledge querying systems. There are knowledge representation models and systems that support the querying or searching on the knowledge-based, but they have not supported well for various query requirements on the knowledge. Specially, the structured query sentences combine the multiple requirements. The paper will propose a knowledge representation model for the programming language C/C++ knowledge domain. Moreover, the paper will present structured query sentences that meet various requirements from users. Especially, the combination of multiple requirements based on the operators AND, OR, and NOT. Results of the research will be applied to design the knowledge querying system in the programming language C/C++ knowledge domain. The system is useful for first and second-year students in the field of technology information.},
  keywords={Computational modeling;Knowledge based systems;Knowledge representation;Communications technology;Database languages;ontology;structured query sentences;knowledge representation;knowledge querying system;programming language},
  doi={10.1109/RIVF60135.2023.10471842},
  ISSN={2473-0130},
  month={Dec},}@INPROCEEDINGS{10784366,
  author={Fang, Yunfei and Chen, Yong and Jiang, Zhonglin and Xiao, Jun and Ge, Yanli},
  booktitle={2024 IEEE International Conference on e-Business Engineering (ICEBE)}, 
  title={Effective and Reliable Domain-Specific Knowledge Question Answering}, 
  year={2024},
  volume={},
  number={},
  pages={238-243},
  abstract={Large language models (LLMs) serve as powerful knowledge bases, capable of embedding knowledge from text into model parameters. However, the widespread issue of hallucination and opaque black-box processes restrict their broader application in professional sectors that place high demands on safety and predictability. Conversely, knowledge graphs provide inherent superiority in the authenticity and interpretability in question answering sessions. However, building a knowledge graph in specialized fields and conduct question answering over the knowledge graph are both nontrivial problems. This paper introduces an efficient knowledge graph question answering (KGQA) approach tailored to customized domain-specific knowledge graphs. Our “extract-then-bind” KGQA method leverages the in-context learning capabilities of the LLM to extract mention and relation proposals from the query, which are then matched with nodes and edges in the knowledge graph. Experimental results demonstrate the effectiveness of our approach in a question answering task using automotive user manuals. Notably, the knowledge graph was created through an automated process. By combining this innovative KGQA technique with its corresponding knowledge graph construction method, this paper proposes an effective and reliable system for addressing domain-specific knowledge question answering. This integrated solution guarantees authenticity and traceability in question answering while significantly reducing the need for manual labour.},
  keywords={Training;Large language models;Knowledge based systems;Knowledge graphs;Manuals;Question answering (information retrieval);Safety;Reliability;Proposals;Automotive engineering;Large language models;knowledge graph;question answering;domain-specific},
  doi={10.1109/ICEBE62490.2024.00044},
  ISSN={2472-8527},
  month={Oct},}@INPROCEEDINGS{10391425,
  author={Sharma, Hemendra Shanker and Sharma, Ashish},
  booktitle={2023 Second International Conference On Smart Technologies For Smart Nation (SmartTechCon)}, 
  title={Query Expansion Using Word Embedding, Ontology and Natural Language Processing}, 
  year={2023},
  volume={},
  number={},
  pages={410-414},
  abstract={Query Expansion (QE) is the art of reconstructing specific queries to expand validation presentation, especially in the data mining process in a requirement understanding environment. Expanding requirements is one of the techniques involved in finding information. In the search engine environment, the query extension includes the evaluation of the value of the construction and the extension of search queries to match new documents. In natural language processing (NLP), word embedding is a term used in textbook parsing, usually as a real-valued vector that encodes the meaning of adjacent words in the vector. It is assumed that the space will be analogous in meaning. Word embedding can be achieved using a set of language models and point literacy methods where vocabulary words or expressions are mapped to vectors of real numbers. For query expansion, one method used is natural language processing through word embedding. Other approaches are ontology, machine learning, and deep learning for automatic query expansion. This paper proposes a hybrid approach for query expansion by combining NLP and ontology through word embedding.},
  keywords={Deep learning;Vocabulary;Art;Ontologies;Search engines;Natural language processing;Data mining;Query expansion;word embedding;natural language processing;Data mining;information retrieval},
  doi={10.1109/SmartTechCon57526.2023.10391425},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10802273,
  author={Nakajima, Haru and Miura, Jun},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Combining Ontological Knowledge and Large Language Model for User-Friendly Service Robots}, 
  year={2024},
  volume={},
  number={},
  pages={4755-4762},
  abstract={Lifestyle support through robotics is an increasingly promising field, with expectations for robots to take over or assist with chores like floor cleaning, table setting and clearing, and fetching items. The growth of AI, particularly foundation models, such as large language models (LLMs) and visual language models (VLMs), is significantly shaping this sector. LLMs, by facilitating natural interactions and providing vast general knowledge, are proving invaluable for robotic tasks. This paper focuses on the benefits of LLMs for "bring-me" tasks, where robots fetch specific items for users, often based on ambiguous instructions. Our previous efforts utilized an ontology extended to handle environmental data to resolve such ambiguities, but faced limitations when unresolvable ambiguities required user intervention for clarity. Here, we enhance our approach by integrating LLMs for providing additional commonsense knowledge, pairing it with ontological data to mitigate the issue of hallucinations and reduce the need for user queries, thus improving system usability. We present a system that merges these knowledge bases and assess its efficacy on "bring-me" tasks, aiming to provide a more seamless and efficient robotic assistance experience.},
  keywords={Visualization;Service robots;Foundation models;Large language models;Knowledge based systems;Ontologies;Usability;Intelligent robots;Floors;Commonsense reasoning},
  doi={10.1109/IROS58592.2024.10802273},
  ISSN={2153-0866},
  month={Oct},}@INPROCEEDINGS{9412877,
  author={Tang, Xunzhu and Sun, Tiezhu and Zhu, Rujie and Wang, Shi},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={CKG: Dynamic Representation Based on Context and Knowledge Graph}, 
  year={2021},
  volume={},
  number={},
  pages={2889-2895},
  abstract={Recently, neural language representation models pre-trained on large corpus can capture rich co-occurrence information and be fine-tuned in downstream tasks to improve the performance. As a result, they have achieved state-of-the-art results in a large range of language tasks. However, there exists other valuable semantic information such as similar, opposite, or other possible meanings in external knowledge graphs (KGs). We argue that entities in KGs could be used to enhance the correct semantic meaning of language sentences. In this paper, we propose a new method CKG: Dynamic Representation Based on Context and Knowledge Graph. On the one side, CKG can extract rich semantic information of large corpus. On the other side, it can make full use of inside information such as co-occurrence in large corpus and outside information such as similar entities in KGs. We conduct extensive experiments on a wide range of tasks, including QQP, MRPC, SST-5, SQuAD, CoNLL 2003, and SNLI. The experiment results show that CKG achieves SOTA 89.2 on SQuAD compared with SAN (84.4), ELMo (85.8), and BERTBase (88.5).},
  keywords={Semantics;Data mining;Task analysis;Context modeling},
  doi={10.1109/ICPR48806.2021.9412877},
  ISSN={1051-4651},
  month={Jan},}@INPROCEEDINGS{8473447,
  author={Zhu, Minchen and Ye, Xinshu and Xiang, Tao and Ma, Yun and Chen, Xing},
  booktitle={2018 IEEE International Congress on Internet of Things (ICIOT)}, 
  title={Runtime Knowledge Graph Based Approach to Smart Home Application Development}, 
  year={2018},
  volume={},
  number={},
  pages={110-117},
  abstract={Smart home is an important application area of the Internet of things (IoT). However, the diversification of smart home application scenarios increases the difficulty of understanding the scenarios for developers. And the heterogeneity of the programming interfaces of smart devices as well as the close coupling of the code to the underlying systems is still an important work for the developers. Furthermore, the complexity and variability of business requirements poses a great challenge to the development of applications logic. In this paper, we present a runtime knowledge graph based approach to smart home application development. First, a conceptual model describing the smart home scenarios is defined. Second, the manageability of smart devices is abstracted as runtime knowledge graphs that are automatically connected with the corresponding systems. Last, a method of automatically generating smart home applications is proposed. Our approach can reduce code by about 85 percent at least, and an experiment on a real-world application scenario demonstrates the feasibility, effectiveness, and benefits of the new approach to smart home application development.},
  keywords={Smart homes;Runtime;Smart devices;Programming;Internet of Things;Data models;Software;Internet of things;models at runtime;software architecture},
  doi={10.1109/ICIOT.2018.00022},
  ISSN={},
  month={July},}@INPROCEEDINGS{10004260,
  author={Lin, Gongqi and Zhong, Xiuqin and Fu, Hongguang},
  booktitle={2022 17th International Conference on Control, Automation, Robotics and Vision (ICARCV)}, 
  title={MathCLM: Mathematical Cognitive Learning Model Based on the Evolution of Knowledge Graph}, 
  year={2022},
  volume={},
  number={},
  pages={616-622},
  abstract={In real-world applications, the effective integration of learning and reasoning in a cognitive agent model is a challenging mission. However, such integration may lead to a better understanding, practice, and construction of more realistic models, especially for mathematical learning. Unfortunately, existing models are either oversimplified or require much processing time, which is unsuitable for online learning and education. Therefore, we propose a novel cognitive learning model, called Mathematical Cognitive Learning Model (MathCLM) based on the evolution of knowledge graph, for online mathematical learning that seeks to effectively represent, learn, and reason in online learning environments. The model's architecture combines cognitive learning with symbolic knowledge representation based on natural language processing (NLP). We introduce the mathematical instance concept to build the strategies by mathematical knowledge, such as theorems, axioms, etc., and infer new custom instances based on the learning knowledge. Furthermore, it can deal with uncertainty and errors from instances recommendation using a graph matching model and displays the inference progressing with different combinations of instances. We build a platform to promote and validate our model. The validation of the model on the real-world platform and the results presented here indicate the promise of the approach when performing online learning and reasoning in real-world scenarios, with possible applications in various areas.},
  keywords={Visualization;Uncertainty;Automation;Education;Knowledge representation;Mathematical models;Natural language processing},
  doi={10.1109/ICARCV57592.2022.10004260},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10350156,
  author={Zheng, Yao and Cen, Jianhe and Sun, Shiqi and Yin, Dahu and Li, Jingyuan and Wang, Yuanzhuo},
  booktitle={2023 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)}, 
  title={MKGS: Maintains the Original Structure of Knowledge Graph}, 
  year={2023},
  volume={},
  number={},
  pages={449-456},
  abstract={In the field of graph-generated text, one of the core issues commonly explored is how to maintain the structural information of the graph to the maximum extent and reduce the problem of knowledge loss during training. Current research has mainly focused on exploring the ability of models to learn graph structures by increasing model size and refining the graph representation. In contrast, our work emphasizes the importance of perceiving and exploring edges in the graph itself. Edges provide a wide variety of structures to the graph structure, offering it freedom and diversity. Therefore, improving the model's ability to perceive edges could potentially enhance the task metrics of graph generation for text. To address this, we propose a graph-generated text model MKGS that maintains the original structure of the knowledge graph, effectively reducing knowledge loss during the learning process. Our approach achieves this at three levels: reorganizing the knowledge sequence as input to the model, enhancing edge perception during processing, and incorporating a graph rational activation function at the output. We validate our method using the Kg-to-text benchmark dataset WebNLG, where MKGS achieves a score of 66.22%. Additionally, the model exhibits fewer syntactic errors and produces smoother expressions in the generated text.},
  keywords={Training;Measurement;Computational modeling;Refining;Knowledge graphs;Syntactics;Benchmark testing;Graph-generated Text;Knowledge Representation;Knowledge Graphs;Graph Theory;Edge-aware Attention},
  doi={10.1109/WI-IAT59888.2023.00074},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10386182,
  author={Kosten, Catherine and Cudré-Mauroux, Philippe and Stockinger, Kurt},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph Question Answering Systems}, 
  year={2023},
  volume={},
  number={},
  pages={5272-5281},
  abstract={With the recent spike in the number and availability of Large Language Models (LLMs), it has become increasingly important to provide large and realistic benchmarks for evaluating Knowledge Graph Question Answering (KGQA) systems. So far the majority of benchmarks rely on pattern-based SPARQL query generation approaches. The subsequent natural language (NL) question generation is conducted through crowdsourcing or other automated methods, such as rule-based paraphrasing or NL question templates. Although some of these datasets are of considerable size, their pitfall lies in their pattern-based generation approaches, which do not always generalize well to the vague and linguistically diverse questions asked by humans in real-world contexts. In this paper, we introduce Spider4SPARQL -a new SPARQL benchmark dataset featuring 9,693 previously existing manually generated NL questions and 4,721 unique, novel, and complex SPARQL queries of varying complexity. In addition to the NL/SPARQL pairs, we also provide their corresponding 166 knowledge graphs and ontologies, which cover 138 different domains. Our complex benchmark enables novel ways of evaluating the strengths and weaknesses of modern KGQA systems. We evaluate the system with state-of-the-art KGQA systems as well as LLMs, which achieve only up to 45% execution accuracy, demonstrating that Spider4SPARQL is a challenging benchmark for future research.},
  keywords={Measurement;Crowdsourcing;Natural languages;Knowledge graphs;Benchmark testing;Ontologies;Question answering (information retrieval);Benchmark for Question Answering over Knowledge Graphs;Language Models;Performance Evaluation},
  doi={10.1109/BigData59044.2023.10386182},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10822556,
  author={Qi, Jiewei and Luo, Ling and Yang, Zhihao and Wang, Jian and Zhou, Huiwei and Lin, Hongfei},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={An Improved Method for Phenotype Concept Recognition Using Rich HPO Information}, 
  year={2024},
  volume={},
  number={},
  pages={1135-1140},
  abstract={Automatically identifying human phenotype ontology (HPO) concepts from text is important for disease analysis. Existing ontology-driven methods for phenotype concept recognition mainly rely on concept names and synonym information from the ontology, without fully exploiting the rich ontology information. In this paper, we present an improved phenotype concept recognition method by incorporating rich HPO information. We first design prompts with HPO information and use a cutting-edge large language model GPT-4 to generate synonym augmentation for expanding distant supervised training data. We then propose an ontology vector-enhanced phenotype concept classification model to efficiently integrate the taxonomic hierarchical structure of HPO. Additionally, we employ noisy data augmentation to improve the model’s recognition ability in noisy texts and implement a negation detection function. Experimental results on three standard corpora and two typo corpora show our method compares favorably to previous methods and achieves a significant improvement in noisy texts. The source code and data are freely available at https://github.com/DUTIR-BioNLP/PhenoTagger-Updates.},
  keywords={Phenotypes;Source coding;Large language models;Training data;Ontologies;Data augmentation;Noise measurement;Reliability;Standards;Diseases;Phenotype Concept Recognition;Human Phenotype Ontology;Ontology Information Enhancement},
  doi={10.1109/BIBM62325.2024.10822556},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10521188,
  author={Timperley, Louis and Berthoud, Lucy and Snider, Chris and Tryfonas, Theo},
  booktitle={2024 IEEE Aerospace Conference}, 
  title={Mapping the MBSE Environment and Complementary Design Space Exploration Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={1-20},
  abstract={Today’s MBSE tools and environments are highly varied and therefore present a challenge for organizations looking to implement MBSE. Furthermore, while MBSE environments are highly capable of supporting the description of design baselines, the current capabilities within these environments could be further refined for exploring alternative designs. As a result it is important to gain an understanding of the limitations of current MBSE tooling in performing the valuable activity of design space exploration, and identify a set of candidate techniques to combat these. This paper reviews the various options available to MBSE practitioners by comparing some of the most common MBSE languages, tools and methods. The possible issues that can be encountered when exploring different designs have been identified and assigned a severity rating. A set of design space exploration techniques are presented, and where possible these have been sourced from existing literature. A knowledge graph has been constructed to collect all this data into a structured format, containing all the MBSE languages, tools, methods, design space exploration-related issues and techniques, as well as the relationships between each of these. This knowledge graph, implemented as a Neo4j graph database, allowed deeper insights to be drawn from the collected information. By defining a selected MBSE environment, including language, tool and method, the knowledge graph could be used to identify the least troublesome sequence (with minimum number of related issues) to arrive at a desired design artifact, for example a set of optimized system parameters. Beside this, the knowledge graph could be used to display the relationships and clusters of MBSE languages, tools and methods, to assist organizations with selecting suitable MBSE environment elements. Future work will bring greater depth to the analysis available with the knowledge graph, for instance, differentiation between different types of design space exploration issues and techniques.},
  keywords={Codes;Reviews;Databases;Design methodology;Knowledge based systems;Knowledge graphs;Organizations},
  doi={10.1109/AERO58975.2024.10521188},
  ISSN={1095-323X},
  month={March},}@INPROCEEDINGS{9194518,
  author={Zhang, Jingzhuo and Liu, Weijie and Wang, Ping},
  booktitle={2020 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={Drug Drug Interaction Extraction from Chinese Biomedical Literature Using Distant Supervision}, 
  year={2020},
  volume={},
  number={},
  pages={593-598},
  abstract={The field of pharmacovigilance has attracted widespread attention due to the increasing impact of drug safety incidents. In this paper, we try to extract Drug Drug Interactions (DDIs) from Chinese biomedical literature. In addition, we used a variety of biomedical resources to develop the first Chinese DDIs database with the help of expert annotations. Based on this database, we applied distant supervision method to extract DDIs from 11,319 biomedical sentences. In order to classify the relationship instances, we extract feature based on the Bidirectional Encoder Representation from Transformers (BERT) model, combine the attention mechanism to select effective instances, and provide drug descriptions to supplement background knowledge. At last. Our method achieves an F-score of 0.732, which is better than the traditional method. Furthermore, we analyze the false negatives in our results.},
  keywords={5G mobile communication;Conferences;distant supervision;drug drug interaction;relation extraction;attention mechanism},
  doi={10.1109/ICBK50248.2020.00089},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10412786,
  author={Guo, Dongsheng and Yue, Aizhen and Ning, Fanggang and Huang, Dengrong and Chang, Bingxin and Duan, Qiang and Zhang, Lianchao and Chen, Zhaoliang and Zhang, Zheng and Zhan, Enhao and Zhang, Qilai and Jiang, Kai and Li, Rui and Zhao, Shaoxiang and Wei, Zizhong},
  booktitle={2023 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={A Study Case of Automatic Archival Research and Compilation using Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={52-59},
  abstract={Archival research and compilation is a specialized task that focuses on exploration, selection and processing of vast quantities of archival documents pertaining to specific subjects. Traditionally, this task has been characterized by its labor-intensive and time-consuming requirements. In recent years, the advancement of artificial intelligence has made automatic archival research and compilation tasks feasible. However, the limited availability of relevant samples imposes significant constraints on the application of deep learning models, given their high demand for sufficient data and knowledge. In this paper, we present a study case and propose an innovative method for automatic archival research and compilation, leveraging the robust knowledge base and text generation ability offered by large language models. Specifically, our method comprises three essential components: document retrieval, document summarization, and rule-based compilation. In the document summarization component, we leverage fine-tuned large language models to enhance the performance by simulation data generation and summary generation. Experimental results substantiate the effectiveness of our method. Furthermore, our method provides a general idea in using large language models, as well as a solution for addressing similar challenges in different domains.},
  keywords={Deep learning;Knowledge based systems;Manuals;Knowledge graphs;Data models;Task analysis;Archival research and compilation;Automatic method;Large language models;Fine-tuning},
  doi={10.1109/ICKG59574.2023.00012},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10412842,
  author={Iqbal, Mohammad and Laili Udhiah, Rosita and Rana Nugraha, Tsamarah and Pao, Hsing-Kuo},
  booktitle={2023 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={ASAGeR: Automated Short Answer Grading Regressor via Sentence Simplification}, 
  year={2023},
  volume={},
  number={},
  pages={60-68},
  abstract={We propose an automated short answer grading system (ASAG) to estimate the student answer scores via text summarization from LLMs. The step of text summarization provides enough question answer normalization so that the summarized answers have the answer keys well organized and the grading based on that should be more accurate and easier than before, no matter the answers are graded by human or automatic graders. On the other hand, we also discuss the scenario when more than one grader are involved in the grading but providing inconsistent scores. We adopt a majority voting mechanism to overcome such difficulty and produce superior result in average. Overall the proposed methodology has its evaluation done to show the superiority to other state-of-the-art methods. The pre-trained transformer version 3.5 (GPT 3.5) is used to serve the text summarization tool given a well-designed prompt.},
  keywords={Performance evaluation;Computational modeling;Natural languages;Transformers;Question answering (information retrieval);Magnetic heads;Task analysis;Natural Language Processing;Answer Grading;Regression;Text Simplification},
  doi={10.1109/ICKG59574.2023.00013},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10480162,
  author={Hang, Ching Nam and Yu, Pei-Duo and Tan, Chee Wei},
  booktitle={2024 58th Annual Conference on Information Sciences and Systems (CISS)}, 
  title={TrumorGPT: Query Optimization and Semantic Reasoning over Networks for Automated Fact-Checking}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT, a novel generative artificial intelligence solution designed for automated fact-checking. TrumorGPT aims to distinguish "trumors", which are rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework merges machine learning with natural language processing techniques, leveraging a large language model (LLM) with few-shot learning for knowledge graph construction and semantic reasoning. TrumorGPT addresses the "hallucination" issue common in LLMs and the limitations of static training data by incorporating retrieval-augmented generation. This approach involves accessing and utilizing information from regularly updated knowledge graphs that consist of the latest news and information, ensuring that fact-checking of TrumorGPT is based on the most recent data. Accessing updated knowledge graphs greatly enhances the proficiency of TrumorGPT in delivering accurate and reliable information promptly. Evaluating with extensive datasets, TrumorGPT demonstrates superior performance in automated fact-checking. Its ability to effectively conduct automated fact-checking across various platforms marks a critical step forward in the fight against misinformation, enhancing trust and accuracy in the digital information age.},
  keywords={Social networking (online);Voting;Query processing;Semantics;Training data;Knowledge graphs;Cognition;Fact-checking;large language models;retrieval-augmented generation;semantic reasoning;knowledge graph},
  doi={10.1109/CISS59072.2024.10480162},
  ISSN={2837-178X},
  month={March},}@INPROCEEDINGS{10386615,
  author={Samel, Karan and Ma, Jun and Wang, Zhengyang and Zhao, Tong and Essa, Irfan},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={Integrating Noisy Knowledge into Language Representations for E-Commerce Applications}, 
  year={2023},
  volume={},
  number={},
  pages={548-553},
  abstract={Integrating structured knowledge into language model representations increases recall of domain-specific information useful for downstream tasks. Matching between knowledge graph entities and text entity mentions can be easily performed when entity names are unique or there exists entity linking data. When extending this setting to new domains, newly mined knowledge contains ambiguous and incorrect information, with no explicit linking information. In such settings, we design a framework to robustly link relevant knowledge to input texts as an intermediate modeling step while performing end-to-end domain fine-tuning tasks. This is done by first computing the similarity of the existing task labels with candidate knowledge triplets to generate relevance labels. We use these labels to train a relevance model, which predicts the relevance of the inserted triplets to the original text. This relevance model is integrated within a language model, leading to our Knowledge Relevance BERT (KR-BERT) framework. We test KR-BERT for entity linking tasks on a real-world e-commerce dataset as well as a public linking task, where we show performance improvements over strong baselines.},
  keywords={Computational modeling;Design methodology;Knowledge graphs;Predictive models;Logic gates;Transformers;Noise measurement;noisy data;knowledge graphs;search;entity linking},
  doi={10.1109/BigData59044.2023.10386615},
  ISSN={},
  month={Dec},}@ARTICLE{9815253,
  author={Rony, Md Rashad Al Hasan and Kumar, Uttam and Teucher, Roman and Kovriguina, Liubov and Lehmann, Jens},
  journal={IEEE Access}, 
  title={SGPT: A Generative Approach for SPARQL Query Generation From Natural Language Questions}, 
  year={2022},
  volume={10},
  number={},
  pages={70712-70723},
  abstract={SPARQL query generation from natural language questions is complex because it requires an understanding of both the question and underlying knowledge graph (KG) patterns. Most SPARQL query generation approaches are template-based, tailored to a specific knowledge graph and require pipelines with multiple steps, including entity and relation linking. Template-based approaches are also difficult to adapt for new KGs and require manual efforts from domain experts to construct query templates. To overcome this hurdle, we propose a new approach, dubbed SGPT, that combines the benefits of end-to-end and modular systems and leverages recent advances in large-scale language models. Specifically, we devise a novel embedding technique that can encode linguistic features from the question which enables the system to learn complex question patterns. In addition, we propose training techniques that allow the system to implicitly employ the graph-specific information (i.e., entities and relations) into the language model’s parameters and generate SPARQL queries accurately. Finally, we introduce a strategy to adapt standard automatic metrics for evaluating SPARQL query generation. A comprehensive evaluation demonstrates the effectiveness of SGPT over state-of-the-art methods across several benchmark datasets.},
  keywords={Measurement;Linguistics;Syntactics;Resource description framework;Adaptation models;Standards;Training;Knowledge based systems;knowledge graph;information retrieval;query generation;language models},
  doi={10.1109/ACCESS.2022.3188714},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10191495,
  author={Zhang, Jingyi and Wei, Yuting and Zhu, Yangfu and Wu, Bin},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Self-adaptive Prompt-tuning for Event Extraction in Ancient Chinese Literature}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Extracting different types of war events from ancient Chinese literature is significant, as war is an important factor in driving the development of Chinese history. The existing trend of event extraction models utilizes template-based generative approaches, which do not take into account the brevity and obscurity of ancient Chinese, as well as the diversity of templates for similar event types. In this paper, we propose a novel Knowledge Graph-based generative event extraction framework with a self-Adaptive Prompt (KGAP) for ancient Chinese war. Specifically, we construct a self-adaptive prompt, which considers its unique trigger words for different types of wars and is designed to solve the problem of the similarity in events. Moreover, we construct a semantic knowledge graph of ancient literature, assisting the pre-trained language model to better understand the ancient Chinese text. Since there is no public dataset for the ancient Chinese event extraction task, we provide an event extraction dataset and conduct experiments on it. Experimental results show that our model is more state-of-the-art than both the classification-based and generative-based methods for event extraction in ancient Chinese literature.},
  keywords={Semantics;Neural networks;Knowledge graphs;Market research;History;Task analysis;Ancient Chinese;Event Extraction;Self-adaptive Prompt;Knowledge Graph},
  doi={10.1109/IJCNN54540.2023.10191495},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10224086,
  author={Jousselme, A-L. and de Villiers, J.P. and de Freitas, A. and Blasch, E. and Dragos, V. and Pavlin, G. and Costa, P. C. and Laskey, K. B. and Laudy, C.},
  booktitle={2023 26th International Conference on Information Fusion (FUSION)}, 
  title={Uncertain about ChatGPT: enabling the uncertainty evaluation of large language models}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={ChatGPT, OpenAI’s chatbot, has gained consider-able attention since its launch in November 2022, owing to its ability to formulate articulated responses to text queries and comments relating to seemingly any conceivable subject. As impressive as the majority of interactions with ChatGPT are, this large language model has a number of acknowledged shortcomings, which in several cases, may be directly related to how ChatGPT handles uncertainty. The objective of this paper is to pave the way to formal analysis of ChatGPT uncertainty handling. To this end, the ability of the Uncertainty Representation and Reasoning Framework (URREF) ontology is assessed, to support such analysis. Elements of structured experiments for reproducible results are identified. The dataset built varies Information Criteria of Correctness, Non-specificity, Self-confidence, Relevance and Inconsistency, and the Source Criteria of Reliability, Competency and Type. ChatGPT’s answers are analyzed along Information Criteria of Correctness, Non-specificity and Self-confidence. Both generic and singular information are sequentially provided. The outcome of this preliminary study is twofold: Firstly, we validate that the experimental setup is efficient in capturing aspects of ChatGPT uncertainty handling. Secondly, we identify possible modifications to the URREF ontology that will be discussed and eventually implemented in URREF ontology Version 4.0 under development.},
  keywords={Uncertainty;Ontologies;Media;Chatbots;Cognition;Reliability;Disruptive technologies;Uncertainty evaluation;Ontology;Information quality;Source quality;Large Language Models;NLP},
  doi={10.23919/FUSION52260.2023.10224086},
  ISSN={},
  month={June},}@INPROCEEDINGS{10385346,
  author={Xie, Jing and Li, Xin and Yuan, Ye and Guan, Yi and Guo, Xitong and Lin, Yi},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Disease Diagnosis based on Multiple Semantic Relationship Prompt Subgraph}, 
  year={2023},
  volume={},
  number={},
  pages={3398-3405},
  abstract={Existing disease diagnosis models are often trained on clinical data such as electronic medical records (EMRs), which are lack of knowledge guidance. In this paper, we propose a new disease diagnosis model based on Multiple Semantic Relationship Prompt Subgraph (MSRPS), which can efficiently incorporate medical knowledge and utilize pretrained language models (PLMs) to achieve reasonable and accurate diagnosis results. Firstly, the MSRPS is extracted from the medical knowledge graph based on the patient’s personalized condition. Then graph encoder is used to generate a continuous prompt learning template based on graph neural network (GNN). Finally, a disease diagnosis model with prompt learning template under pre-trained and prompt paradigm is constructed to predict diagnosis results. Compared with the fine-tuning approach, this method needs fewer trainable parameters and less training data but achieve better performance. Experiments are conduct on two multi-label disease diagnosis datasets: ChineseEMR-50 and MIMIC-III-50. Results demonstrate that our model can be used in various pre-trained models and achieve the state-of-the-art results.},
  keywords={Semantics;MIMICs;Training data;Predictive models;Graph neural networks;Medical diagnosis;Task analysis;disease diagnosis;medical knowledge;pre-trained language model;promote learning;electronic medical record},
  doi={10.1109/BIBM58861.2023.10385346},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10261615,
  author={Zeng, Ling and Liang, Zaoqing and Liang, Yun and Huang, Peijie},
  booktitle={2023 5th International Conference on Computer Science and Technologies in Education (CSTE)}, 
  title={Research on Key Technologies of Automated Instructional Design for Engineering Education Courses}, 
  year={2023},
  volume={},
  number={},
  pages={86-92},
  abstract={An Automated Instructional Design (AID) solution for engineering education courses instructional design is proposed in this research. By limiting the Domain of AID to engineering education courses, and limiting the course objectives to the graduate attributes and professional competence of engineering education programme, as well as limiting the instructional methods to task-and-activity-based methods, and applying the evaluation vocabulary regularly used in the field of engineering education, a complete set of instructional design vocabulary related to the instructional design domain can be abstracted. Using the Unified Modeling Language (UML) Profile mechanism, the complete set of instructional design vocabulary can be represented by a designed visual model language, which provide a complete instructional design description language, named Instructional Design Visual Model Language (IDVML), dedicated to the field of engineering education for AID tool implementation. Then the existed Model-Driven Architecture (MDA) tools can be applied to design IDVML-related Platform Independent Model (PIM) and various Platform Specific Model (PSM) meta-models, and the conversion templates from PIM to various PSM, as well as various conversion templates from PSM to executable code, database table, Web page, course Ontology, etc., so as to realize the conversion from IDVML to program code directly. Finally, using the Meta-model Object Facility (MOF), an independent AID can be implemented. This AID can help the teacher to design the teaching objectives, content, tasks and activities, evaluation of engineering education related courses which meet the requirements of engineering education accreditation. At the same time, the course Ontology can be generated to provide Ontology basis for the subsequent construction of individualized learning system.},
  keywords={Vocabulary;Visualization;Limiting;Unified modeling language;Computer architecture;Ontologies;Accreditation;automated instructional design;engineering education;instructional design visulized modleing language;instructional design meta-model},
  doi={10.1109/CSTE59648.2023.00022},
  ISSN={},
  month={April},}@INPROCEEDINGS{9905090,
  author={Pradeepani, M. K. T. and Jayawardena, C. and Rajapaksha, U. U. S.},
  booktitle={2022 International Research Conference on Smart Computing and Systems Engineering (SCSE)}, 
  title={Adding Commonsense to Robotic Application Using Ontology-Based Model Retraining}, 
  year={2022},
  volume={5},
  number={},
  pages={157-164},
  abstract={In terms of the level of technological capability in the world today, the use of automated robotics is common in various fields. There are large projects going on in many industries that collaborate between robots and other robots, as well as humans and robots. In hospital environments, care for people with medical needs and their needs and used to make appropriate suggestions to their problems. Robots can also be found in certain areas that can respond quickly as an emergency rescue agent. Furthermore, robots, which can be seen in the hotel industry as waiters and as farm assistants in agriculture, have a great tendency to be used as multi-tasking agents in many fields. In each of these areas, robots must co-operate with humans. In that situation, the importance of the exchange of mutual knowledge between robots-robots and between humans-robots comes into the picture. What matters here is not only the quantitative vastness of knowledge but also the ability to understand each other in the same medium. Although the common sense that people need in their day-to-day work is completely obvious to humans, the commonsense knowledge domain needs to be implanted in robots. Whatever concept is defined for adding commonsense to robotics, it should be a consistent concept that can be logically constructed so that it can be understood by a machine. As will be discussed later in the paper, different methods have been used in various related works to add a different kind of domain knowledge to robotics. The objective of this paper is to provide an improved retrained model for robotics in order to give them the ability to act more human-like when performing tasks. By using the proposed model robots are able to answer the incomplete command or inquiries related to a given context. One of the objectives of this work is to use the ontology-based, commonsense-support existing knowledge base as a mechanism to retrain and build a new model.},
  keywords={Training;Adaptation models;Service robots;Knowledge based systems;Robot sensing systems;Hardware;Sensors;BERT;commonsense;robotics;transfer learning},
  doi={10.1109/SCSE56529.2022.9905090},
  ISSN={2613-8662},
  month={Sep.},}@INPROCEEDINGS{8226470,
  author={Kidanu, Solomon Asres and Chbeir, Richard and Cardinale, Yudith},
  booktitle={2017 XLIII Latin American Computer Conference (CLEI)}, 
  title={MAS2DES-onto: Ontology for MAS-based digital ecosystems}, 
  year={2017},
  volume={},
  number={},
  pages={1-8},
  abstract={Multi-Agent Systems (MASs) have received much attention in recent years because of their advantages on modeling complex distributed systems, such Digital Ecosystems (DESs). Many existing modeling languages that support the design of such systems are based on ontologies to assist the representation of agents knowledge. However, in the context of DESs, there is still a need for more general conceptual models to represent the specific characteristics of DESs in terms of win-win interaction, engagement, equilibrium, and self-organization. Then, concepts such behavior, roles, rules, and environment are needed. This paper describes an ontology-based approach by proposing MAS2DES-Onto, as the conceptual model, which considers the essential static and dynamic aspects of MASs by a clear representation of their concepts and relationships to support the design and development of DESs. To validate and conduct experimental tests, we integrate MAS2DES-Onto into a framework to automatically generate MAS-based DESs. Results show the efficiency and effectiveness of our approach.},
  keywords={Ontologies;Cognition;Ecosystems;Biological system modeling;Organizations;Electronic mail},
  doi={10.1109/CLEI.2017.8226470},
  ISSN={},
  month={Sep.},}@ARTICLE{10553231,
  author={Li, Xiaodong and Tian, Guohui and Cui, Yongcheng},
  journal={IEEE Robotics and Automation Letters}, 
  title={Fine-Grained Task Planning for Service Robots Based on Object Ontology Knowledge via Large Language Models}, 
  year={2024},
  volume={9},
  number={8},
  pages={6872-6879},
  abstract={In domestic environment, the successful execution of service tasks heavily relies on the robot's capability to identify and understand objects within its surrounding. This crucial process predominantly takes place during task planning, prior to the actual performance of service tasks. Therefore, it is vital that the robot is capable of formulating object-specific action sequences through task planning. In this letter, we propose the Fine-Grained Task Planning (FGTP) framework, an innovative method that combines object ontology knowledge with Large Language Models (LLMs) to create detailed action sequences. The FGTP framework is uniquely designed to process both text descriptions of service tasks and images of relevant objects, enabling a thorough comprehension of object attributes essential for task execution. Moreover, we have developed a set of rules based on these attributes to assist in the robot's decision-making process. In scenarios where service tasks fail because the object is in an unsuitable state, our framework deploys a logic-based reasoning method, concentrating on object attributes to identify suitable substitutes. This process leverages a pre-established semantic map to locate these alternatives, thus enabling a transition back to standard task planning. Our evaluations, conducted in both the VirtualHome simulation environment and with the TIAGo real robot, demonstrate the efficacy of our approach. This confirms our framework's capability to generate practical and implementable plans for various service tasks.},
  keywords={Task analysis;Planning;Ontologies;Service robots;Robot kinematics;Object recognition;Task planning;service robotics},
  doi={10.1109/LRA.2024.3412593},
  ISSN={2377-3766},
  month={Aug},}@INPROCEEDINGS{10706469,
  author={Incitti, Francesca and Salfinger, Andrea and Snidaro, Lauro and Challapalli, Sri},
  booktitle={2024 27th International Conference on Information Fusion (FUSION)}, 
  title={Leveraging LLMs for Knowledge Engineering from Technical Manuals: A Case Study in the Medical Prosthesis Manufacturing Domain}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Ontologies are nowadays widely used to organize information across specific domains, being effective due to their hierarchical structure and the ability to explicitly represent relationships between concepts. Knowledge engineering, like compiling companies’ vast bodies of knowledge into these structures, however, still represents a time-consuming, largely manually performed process, esp. with significant amounts of knowledge often only recorded within unstructured text documents. Since the recently introduced Large Language Models (LLMs) excel on text summarization, this raises the question whether these could be exploited within dedicated knowledge fusion architectures to assist human knowledge engineers by automatically suggesting relevant classes, instances and relations extracted from textual corpora. We therefore propose a novel approach that leverages the taxonomic structure of a partially defined ontology to prompt LLMs for hierarchical knowledge organization. Unlike conventional methods that rely solely on static ontologies, our methodology dynamically generates prompts based on the ontology’s existing class taxonomy, prompting the LLM to generate responses that extract supplementary information from unstructured documents. It thus introduces the concept of using ontologies as scaffolds for guiding LLMs, in order to realize a mutual interplay between structured ontological knowledge and the soft fusion capabilities of LLMs. We evaluate our proposed algorithm on a real-world case study, performing a knowledge fusion task on heterogeneous technical documentation from a medical prosthesis manufacturer.},
  keywords={Knowledge engineering;Large language models;Taxonomy;Text summarization;Organizations;Manuals;Documentation;Ontologies;Manufacturing;Prosthetics;Large Language Models;Knowledge Engineering;Ontology Population;Soft Fusion;Natural Language Processing},
  doi={10.23919/FUSION59988.2024.10706469},
  ISSN={},
  month={July},}@INPROCEEDINGS{10507374,
  author={Yang, Yi and Du, Mengjun and Li, Ang and Qian, Jin and Wang, Junyi},
  booktitle={2023 9th International Conference on Computer and Communications (ICCC)}, 
  title={A Fine Granular Relational Reasoning KGQA model Based on Weak Supervised Learning}, 
  year={2023},
  volume={},
  number={},
  pages={2198-2206},
  abstract={Knowledge graph question-answering (KGQA) task aims to provide answers in the form of entities from the knowledge graph for natural language questions. Conventional information retrieval modeling methods focus on controlling the size of the candidate reasoning path set through effective relationship filtering. However, these methods often perform iterative relational reasoning solely based on the overall semantic information of the question, neglecting the local relevance of relationships within the question statement and the value of fine granular information for filtering. KGQA task datasets typically lack annotations for inference paths, and existing weak supervised learning strategies may not effectively eliminate the misleading influence of erroneous reference reasoning paths on model reasoning capabilities. Based on the fundamental ideas of “reducing the size of candidate path sets” and “eliminating erroneous reference reasoning paths,” this paper proposes a knowledge graph question-answering model for fine granular relational reasoning based on weak supervised learning. First, it enhances relationship and path filtering capabilities from a model structure perspective and then, on this basis, improves the model’s ability to differentiate erroneous reference reasoning paths from a supervised strategy perspective. Compared to state-of-the-art KGQA models, the weak supervised fine granular relational reasoning KGQA model proposed in this paper achieves significant performance improvement.},
  keywords={Filtering;Computational modeling;Supervised learning;Semantics;Natural languages;Knowledge graphs;Cognition;Deep Learning;Natural Language Processing;Knowledge Graph Question Answering;Weak Supervised Learning},
  doi={10.1109/ICCC59590.2023.10507374},
  ISSN={2837-7109},
  month={Dec},}@INPROCEEDINGS{10386013,
  author={Xu, Dexuan and Chen, Yanyuan and Zhang, Jiayu and Lou, Yiwei and Wang, Hanpin and He, Jing and Huang, Yu},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Radiology Report Generation via Structured Knowledge-Enhanced Multi-modal Attention and Contrastive Learning}, 
  year={2023},
  volume={},
  number={},
  pages={2320-2325},
  abstract={The automated generation of radiology reports has attracted significant attention in the field of bioinformatics. Currently, the main limitations of this task include insufficient utilization of prior medical knowledge, lack of efficient knowledge fusion algorithms, and less distinctiveness between different generated reports. To address these issues, we propose a novel algorithm for radiology report generation, which includes Structured Knowledge-Enhanced Multi-modal Attention (SKEMA) and Dual-Branch Contrastive Learning (DBCL) for the first time. SKEMA aims to effectively bridge the gap between visual and prior knowledge by leveraging the high-order adjacency matrix of the knowledge graph to weightedly fuse image features and knowledge features. We enhance both features through masking, and use the original features and augmented features as positive and negative samples in the dual-branch contrastive learning (DBCL). DBCL increases the differences between positive and negative samples to avoid generating templated results, and enhances the robustness of the model. Finally, we conducted experiments to demonstrate the effectiveness of our model on two public radiology datasets, IU-Xray and MIMIC-CXR. Our model outperformed previous baseline methods on both datasets and achieved excellent evaluation scores.},
  keywords={Visualization;Fuses;MIMICs;Self-supervised learning;Knowledge graphs;Radiology;Robustness;Radiology Report Generation;Knowledge Graph;Contrastive Learning;Multi-modal Fusion},
  doi={10.1109/BIBM58861.2023.10386013},
  ISSN={2156-1133},
  month={Dec},}@ARTICLE{10858700,
  author={Yhdego, Tsegai O. and Wang, Hui},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Automated Ontology Generation for Zero-shot Defect Identification in Manufacturing}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={A lack of labeled data presents a significant challenge to automatic defect identification in manufacturing, which is a crucial step in process control and certification during process development. State-of-the-art transfer learning is incapable of handling such zero-shot learning (ZSL) when defect labels are absent in training datasets. The latest research on ZSL leverages natural language processing (NLP) based on large language models (LLM) and shows promise by supplementing information to generate labels. However, its performance is hampered by the supporting LLMs pre-trained on generic vocabulary that failed to characterize manufacturing defects accurately. This paper establishes a methodology to automatically extract multi-level attributes from literature to improve defect representation, thereby facilitating ZSL. The extracted attributes contribute to a hierarchical knowledge graph, called defect ontology, to characterize multiple aspects of manufacturing defects. The proposed algorithm takes the defect images and associated text from the literature as input and develops an unsupervised method to identify the hierarchical relationships among the tokenized information extracted from the input text-feature corpora. The hierarchical graph is refined to retain the most relevant information by a pruning algorithm based on a minimum path search. A walk algorithm, along with NLP, parsed the generated ontology to create embedding of defects to enable zero-shot attribute learning to identify defects. The proposed method advances the ZSL methodology by automatically creating a hierarchical knowledge representation from literature and images to replace generic vocabulary in LLM adopted by ZSL algorithms, thus improving defect representation. The case studies are among the earlier attempts to demonstrate the feasibility of using literature data from public sources to extract attributes automatically to identify defects in a real additive manufacturing process based on direct-ink-writing.},
  keywords={Manufacturing;Zero shot learning;Ontologies;Automation;Data mining;Vocabulary;Accuracy;Transfer learning;Certification;Process control;Zero-shot learning;defect identification;manufacturing automation;self-supervised learning},
  doi={10.1109/TASE.2025.3537463},
  ISSN={1558-3783},
  month={},}@ARTICLE{10179162,
  author={Sui, Yicheng and Zhang, Yuzhe and Sun, Jianjun and Xu, Ting and Zhang, Shenglin and Li, Zhengdan and Sun, Yongqian and Guo, Fangrui and Shen, Junyu and Zhang, Yuzhi and Pei, Dan and Yang, Xiao and Yu, Li},
  journal={IEEE Transactions on Services Computing}, 
  title={LogKG: Log Failure Diagnosis Through Knowledge Graph}, 
  year={2023},
  volume={16},
  number={5},
  pages={3493-3507},
  abstract={Logs are one of the most valuable data to describe the running state of services. Failure diagnosis through logs is crucial for service reliability and security. The current automatic log failure diagnosis methods cannot fully use the multiple fields of logs, which fail to capture the relation between them. In this article, we propose LogKG, a new framework for diagnosing failures based on knowledge graphs (KG) of logs. LogKG fully extracts entities and relations from logs to mine multi-field information and their relations through the KG. To fully use the information represented by KG, we propose a failure-oriented log representation (FOLR) method to extract the failure-related patterns. Utilizing the OPTICS clustering method, LogKG aggregates historical failure cases, labels typical failure cases, and trains a failure diagnosis model to identify the root cause. We evaluate the effectiveness of LogKG on a real-world log dataset and a public log dataset, respectively, showing that it outperforms existing methods. With the deployment in a top-tier global Internet Service Provider (ISP), we demonstrate the performance and practicability of LogKG.},
  keywords={Semantics;Task analysis;Sun;Manuals;Security;Natural language processing;Knowledge graphs;Cluster;diagnosis;embedding;LogKG},
  doi={10.1109/TSC.2023.3293890},
  ISSN={1939-1374},
  month={Sep.},}@ARTICLE{10582529,
  author={Wu, Feng and Zhao, Guoshuai and Li, Tengjiao and Shen, Jialie and Qian, Xueming},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Improving Conversational Recommendation System Through Personalized Preference Modeling and Knowledge Graph}, 
  year={2024},
  volume={36},
  number={12},
  pages={8529-8540},
  abstract={Conversational recommendation systems (CRS) can actively discover users’ preferences and perform recommendations during conversations. The majority of works on CRS tend to focus on a single conversation and dig it using knowledge graphs, language models, etc. However, they often overlook the abundant and rich preference information that exists in the user's historical conversations. Meanwhile, end-to-end generation of recommendation results may lead to a decrease in recommendation quality. In this work, we propose a personalized conversational recommendation system infused with historical interaction information. This framework leverages users’ preferences extracted from their historical conversations and integrates them with the users’ preferences in current conversations. We find that this contributes to higher accuracy in recommendations and fewer recommendation turns. Moreover, we improve the interactive pattern between the recommendation module and the dialogue generation module by utilizing the slot filling method. This enables the results inferred by the recommendation module to be integrated into the conversation naturally and accurately. Our experiments on the benchmark dataset demonstrate that our model significantly outperforms the state-of-the-art methods in the evaluation of recommendations and dialogue generation.},
  keywords={Oral communication;Recommender systems;Knowledge graphs;Task analysis;Accuracy;Motion pictures;History;Conversational recommendation system;dialogue generation;personalized recommendations},
  doi={10.1109/TKDE.2024.3421580},
  ISSN={1558-2191},
  month={Dec},}@INPROCEEDINGS{10698993,
  author={Kumar, Amala Rashmi and Kumari, S. Meena and Rao, Tanvi and Shetty, Tavishi S},
  booktitle={2024 Second International Conference on Networks, Multimedia and Information Technology (NMITCON)}, 
  title={ReidLM: Fine-Tuning LLaMA3 using Evol-Instruct for Enhanced Contextual Accuracy in Rare Disease Research}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This study introduces ReidLM, a fine-tuned large language model (LLM) optimized for the rare disease domain. By generating a diverse and complex question-and-answer dataset using the EvolInstruct methodology, Meta’s LLaMA-3-8B-Instruct model was enhanced to deliver better performance across multiple evaluation metrics. ReidLM demonstrates significant improvements in generating contextually accurate responses for rare diseases, highlighting the potential of Evol-Instruct in specialized medical applications. Specifically, ReidLM achieved the highest ROUGE-1 (0.3281) and GEval (0.87) scores among the evaluated models, along with strong performances in METEOR (0.3662) and BERTScore (0.8782), indicating its effectiveness in producing semantically sound and relevant responses. These results offer promising advancements in medical research and patient care, with future work aimed at expanding datasets and validating clinical utility.},
  keywords={Measurement;Accuracy;Terminology;Multimedia systems;Large language models;Medical services;Meteors;Information technology;Medical diagnostic imaging;Diseases;fine-tuning;LLaMA 3;evol-instruct;rare diseases},
  doi={10.1109/NMITCON62075.2024.10698993},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9973695,
  author={Filgueira, Rosa},
  booktitle={2022 IEEE 18th International Conference on e-Science (e-Science)}, 
  title={frances: A Deep Learning NLP and Text Mining Web Tool to Unlock Historical Digital Collections: A Case Study on the Encyclopaedia Britannica}, 
  year={2022},
  volume={},
  number={},
  pages={246-255},
  abstract={This work presents frances, an integrated text mining tool that combines information extraction, knowledge graphs, NLP, deep learning, parallel processing and Semantic Web techniques to unlock the full value of historical digital textual collections, offering new capabilities for researchers to use powerful analysis methods without being distracted by the technology and middleware details. To demonstrate these capabilities, we use the first eight editions of the Encyclopaedia Britannica offered by the National Library of Scotland (NLS) as an example digital collection to mine and analyse. We have developed novel parallel heuristics to extract terms from the original collection (alongside metadata), which provides a mix of unstructured and semi-structured input data, and populated a new knowledge graph with this information. Our Natural Language Processing models enable frances to perform advanced analyses that go significantly beyond simple search using the information stored in the knowledge graph. Furthermore, frances also allows for creating and running complex text mining analyses at scale. Our results show that the novel computational techniques developed within frances provide a vehicle for researchers to formalize and connect findings and insights derived from the analysis of large-scale digital corpora such as the Encyclopaedia Britannica.},
  keywords={Text mining;Deep learning;Semantic Web;Knowledge engineering;Parallel processing;Metadata;Information retrieval;information extraction;knowledge graphs;deep transfer learning;natural language processing;text mining;web tools;semantic web;parallel computing;digital tools;historical digital textual collections},
  doi={10.1109/eScience55777.2022.00038},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10005324,
  author={Zindel, Andreas and Feo-Arenis, Sergio and Helle, Philipp and Schramm, Gerrit and Elaasar, Maged},
  booktitle={2022 IEEE International Symposium on Systems Engineering (ISSE)}, 
  title={Building a Semantic Layer for Early Design Trade Studies in the Development of Commercial Aircraft}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={To improve the adoption of Model-based Systems Engineering (MBSE), data that is distributed across engineering disciplines needs to be made available in an open and descriptive way. This paper describes a new approach to implementing a semantic layer that allows integrating and publishing MBSE data stored in heterogeneous models in a uniform way by means of Semantic Web technologies. The tool-independent views on engineering data provided by the semantic layer enable the implementation of services for accessing, classifying, checking and reuse of federated information. We report on the creation of a common vocabulary in the Ontology Modeling Language (OML) that can be automatically instantiated from distributed models into a knowledge graph. We demonstrate the benefits of our approach using a Systems Modeling Language (SysML) based early design trade study in the aeronautics domain.},
  keywords={Semantic Web;Vocabulary;Atmospheric modeling;Unified modeling language;Semantics;Ontologies;Data models},
  doi={10.1109/ISSE54508.2022.10005324},
  ISSN={2687-8828},
  month={Oct},}@INPROCEEDINGS{10834324,
  author={Chen, Xin and Yin, Chuantao and Chen, Hui and Rong, Wenge and Ouyang, Yuanxin and Chai, Yanmei},
  booktitle={2024 IEEE International Conference on Teaching, Assessment and Learning for Engineering (TALE)}, 
  title={Course Recommendation System Based on Course Knowledge Graph Generated by Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={With the advent of the big data era, knowledge graphs, as important tools for organizing, managing, and understanding massive amounts of information, are gradually becoming a research hotspot in the field of artificial intelligence. This article focuses on the research and practice of automated construction and application of knowledge graphs in the field of university courses, aiming to improve the efficiency and accuracy of knowledge graph construction and provide strong support for the application in related fields.This study integrated publicly available datasets, mainstream online education platforms, and course explanation texts. Using rule-based and deep learning information extraction methods, combined with a large language model, the automatic extraction of entities, attributes, and relationships was successfully achieved, and an initial course knowledge graph was constructed based on this. Furthermore, by calculating the similarity between course description texts and combining the extracted course prerequisite and peer relationships from the texts, the study not only enriches the structure and content of the course knowledge graph, but also enhances its accuracy and practicality. In order to provide more personalized course recommendation services, this article combines sequence based recommendation algorithms and graph embedding algorithms, fully utilizing the information of the course itself and the dependency information of the course sequence, designing a unique personalized recommendation algorithm, and verifying its effectiveness and accuracy through experiments. This study not only provides strong knowledge graph support for online education platforms, but also provides strong technical support for personalized learning recommendations.},
  keywords={Deep learning;Accuracy;Large language models;Soft sensors;Education;Knowledge graphs;Big Data;Information retrieval;Data mining;Recommender systems;component;formatting;style;styling;insert},
  doi={10.1109/TALE62452.2024.10834324},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8406642,
  author={Figueiredo, Guylerme and Duchardt, Amelie and Hedblom, Maria M. and Guizzardi, Giancarlo},
  booktitle={2018 12th International Conference on Research Challenges in Information Science (RCIS)}, 
  title={Breaking into pieces: An ontological approach to conceptual model complexity management}, 
  year={2018},
  volume={},
  number={},
  pages={1-10},
  abstract={In recent years, there has been a growth in the use of reference conceptual models, in general, and domain ontologies, in particular, to capture information about complex and critical domains. These models play a fundamental role in different types of critical semantic interoperability tasks. Therefore, it is essential that domain experts are able to understand and reason using the models' content. In other words, it is important that conceptual models are cognitively tractable. However, it is unavoidable that when the information of the represented domain grows, so does the size and complexity of the artifacts and models that represent them. For this reason, more sophisticated techniques for complexity management in ontology-driven conceptual models, need to be developed. Some approaches are based on the notion of model modularization. In this paper, we follow the work on model modularization to present an approach for view extraction for the ontology-driven conceptual modeling language OntoUML. We provide a formal definition for ontological views over OntoUML conceptual models that completely leverages on the ontologically well-grounded real-world semantics of that language. Moreover, we present a plug-in tool, particularly developed for an OntoUML model-based editor that implements this formal view structure in terms of queries defined over the OntoUML metamodel embedded in that tool.},
  keywords={Unified modeling language;Ontologies;Semantics;Complexity theory;Marine vehicles;Tools;Computational modeling;Conceptual Model Modularization;Ontological Views;Complexity Management in Conceptual Modeling;On-toUML},
  doi={10.1109/RCIS.2018.8406642},
  ISSN={2151-1357},
  month={May},}@ARTICLE{10328733,
  author={Wang, Songsong and Xu, Ouguan},
  journal={IEEE Access}, 
  title={Semantic Information Modeling and Implementation Method for Water Conservancy Equipment}, 
  year={2023},
  volume={11},
  number={},
  pages={133879-133890},
  abstract={Water conservancy equipment (WCE) has a large amount of information, structural heterogeneity and complex relationship leads to the difficulty of semantic interoperability in smart water conservancy. To overcome this issue, we propose the WCE information interaction dimension theory, modeling process and instancing method. First, we analyze the smart water conservancy ontology and information factor, and propose semantic information interaction dimension structure of water conservancy Ontology. Second, we construct the network information model structure of water conservancy, through the relationship degree, a tree model which can realize semantic expression and interoperability is formed through the dimensionality reduction of the model. Third, the component attribute set hierarchical relationship architecture water conservancy information model is established, which use XML language to describe this model. Moreover, the three types of instancing methods are proposed. Through OPC unified architecture (OPC UA) technology, water conservancy information model can implement semantic interoperability. The experimental show that the proposed method of semantic information modeling and semantic interoperability of WCE is feasible, and obvious advantages of complete semantic interoperability than in the model architecture, semantic structure and technical implementation.},
  keywords={Water conservation;Ontologies;Optical wavelength conversion;Semantics;Data models;Interoperability;Water resources;Information model;semantics;smart water conservancy;water conservancy equipment;OPC UA},
  doi={10.1109/ACCESS.2023.3336817},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10708094,
  author={Wang, Dan and Li, Xiaofeng and Gu, Bin and Cao, Yue and Liu, Yusheng},
  booktitle={2023 6th International Conference on Mechatronics, Robotics and Automation (ICMRA)(}, 
  title={An Architecture Modeling Framework for Distributed Automation Systems Using SysML and Semantic Web Technologies}, 
  year={2023},
  volume={},
  number={},
  pages={191-200},
  abstract={The rising interdisciplinarity and complexity of the Distributed Automation Systems (DASs) require the systems to be modeled in an unambiguous and high-level abstract way for cross-discipline/stage communication and interoperability in the Model-Driven Development (MDD) lifecycle. The concept of the System Architecture Model in systems engineering has been adopted for this challenge. To support the creation and analysis of this model, a modeling framework with a modeling methodology, modeling language, knowledge base, and related toolkit is established based on SysML and Semantic Web Technologies. A modeling methodology which is the core of the framework for modeling the architecture of DASs is formally defined with domain-specificity, comprehensiveness, discipline-neutrality, and platform-independency. Based on it, the SysML-DAS modeling language is extended from SysML, and the System Architecture Ontology is built with the help of the Knowledge Extraction Tool. This ontology works as the knowledge base not only to provide a unified and unambiguous view of the system but also to support the automated accomplishment of tasks in the MDD process. As a typical task, the semantic correctness and integrity of the system architecture model can be assessed by the Knowledge Analysis Tool in this framework.},
  keywords={Semantic Web;Analytical models;Automation;Mechatronics;Knowledge based systems;Semantics;Systems architecture;Ontologies;Service-oriented architecture;Model-driven development;distributed automation system;SysML;Semantic Web technologies;System Architecture Model},
  doi={10.1109/ICMRA59796.2023.10708094},
  ISSN={2996-380X},
  month={Nov},}@INPROCEEDINGS{10692968,
  author={Bhargava, Urvashi and Teresha, Y. and Koul, Nishank and Chavan, Chandrashekhar Pomu},
  booktitle={2024 IEEE 7th International Conference on Big Data and Artificial Intelligence (BDAI)}, 
  title={Overcoming the Challenges of Large Language Models: Introducing a Novel Proposition for Synthetic Data Validation}, 
  year={2024},
  volume={},
  number={},
  pages={290-295},
  abstract={The market debut of ChatGPT gave rise to the development and deployment of various other Large Language Models (LLMs) that achieve state-of-the-art performance across various tasks. The growing popularity of these models has captivated some to attempt to construct or enhance their own LLM. We must be aware of the significant problems that already exist and that we might face along the way. This paper aims to identify and investigate the main challenges in this field, provide existing solutions, and propose novel approaches to mitigate them. A unique Truth-Table proposition for validating synthetic data is presented examining two models, along with a bidirectional knowledge graph-based solution for curing the reverse curse problem, data generation strategies, domain adaptation methods, and the use of a custom dataset to address model hallucinations. The methodology and findings of this study provide valuable insights for users, researchers, and industry experts who are interested in LLMs. It serves as a reference for future research on current models, refining models or developing domain-specific ones.},
  keywords={Industries;Adaptation models;Analytical models;Large language models;Refining;Knowledge graphs;Data models;Reliability;Faces;Synthetic data;Large Language Models;Challenges;Strategies;Synthetic Data Validation;Reversal Curse;Model Hallucinations;Knowledge Graph;Data Scarcity},
  doi={10.1109/BDAI62182.2024.10692968},
  ISSN={},
  month={July},}@ARTICLE{9119385,
  author={Hou, Jiaqi and Li, Xin and Yao, Haipeng and Sun, Haichun and Mai, Tianle and Zhu, Rongchen},
  journal={IEEE Access}, 
  title={BERT-Based Chinese Relation Extraction for Public Security}, 
  year={2020},
  volume={8},
  number={},
  pages={132367-132375},
  abstract={The past few years have witnessed some public safety incidents occurring around the world. With the advent of the big data era, effectively extracting public security information from the internet has become of great significance. Up to hundreds of TBs of data are injected into the network every second, and thus it is impossible to process them manually. Natural Language Processing (NLP) is dedicated to the development of an intelligent system for effective text information mining. By analysing the text and quickly extracting the relationships between the relevant entities, NLP can establish the knowledge graph (KG) of public security, which lays the foundation for safety case analysis, information monitoring, and activity tracking and locating. One of the current pre-training relation extraction models is the Word2Vec model. The Word2vec model is single mapped, and it produces a static, single representation of the words in sentences. Then, the BERT model considers contextual information and provides more dynamic, richer vector representations of generated words. Therefore, in this paper, we propose a Bidirectional Encoder Representation from Transformers (BERT) based on the Chinese relation extraction algorithm for public security, which can effectively mine security information. The BERT model is obtained by training the Masked Language Model and predicting the next sentence task, which is based on the Transformer Encoder and the main model structure is the stacked Transformers. Extensive simulations are conducted to evaluate our proposed algorithm in comparison to some state-of-the-art schemes.},
  keywords={Bit error rate;Feature extraction;Security;Task analysis;Training;Semantics;Data mining;BERT;relationship extraction;public security;MaxPooler;ReLU},
  doi={10.1109/ACCESS.2020.3002863},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9265932,
  author={Duan, Jiajia and Zhao, Hui and Zhou, Qian and Qiu, Meikang and Liu, Meiqin},
  booktitle={2020 IEEE International Conference on Smart Cloud (SmartCloud)}, 
  title={A Study of Pre-trained Language Models in Natural Language Processing}, 
  year={2020},
  volume={},
  number={},
  pages={116-121},
  abstract={Pre-trained Language Model (PLM) is a very popular topic in natural language processing (NLP). It is the rapid development of pre-trained language models (PLMs) that has led to the achievements of natural language today. In this article, we give a review of important PLMs. First, we generally introduce the development history and achievements of PLMs. Second, we present several extraordinary PLMs, including BERT, the variants of BERT, Multimodal PLMs, PLMs combined with Knowledge Graph and PLMs applied to natural language generation. In the end, we summarize and look into the future of PLMs. We expect this article will provide a practical guide for learners to understanding, using and developing PLMs with the abundant literature existing for various NLP tasks.},
  keywords={Bit error rate;Task analysis;Training;Computational modeling;Predictive models;Natural language processing;Context modeling;Pre-trained;Embedding;BERT;Cross-modal;KG;Natural Language Generation},
  doi={10.1109/SmartCloud49737.2020.00030},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9892028,
  author={Li, Chen and Bai, Jun and Wang, Chuanarui and Hu, Yuanhao and Rong, Wenge and Xiong, Zhang},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Graph and Question Interaction Aware Graph2Seq Model for Knowledge Base Question Generation}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={The Knowledge Base Question Generation (KBQG) is an essential natural language processing task. Taking knowledge graph and answer entities as input, KBQG aims to generate corresponding natural language question. Recently Graph2Seq has been proposed to encode the knowledge graph and achieved remarkable results, while one important challenge still remains, i.e., the graph encoding lacks the interaction with the target question. To deal with the above challenge, we propose a graph and question interaction enhanced Graph2Seq model, in which we design an encoder-decoder parallel enhancement mechanism and apply the knowledge distillation for both inter-mediate representation and prediction distribution to employ the knowledge of the target question into the graph representation. Experiments have been conducted on KBQG benchmark dataset and experimental results have shown the promising potential of proposed method.},
  keywords={Knowledge based systems;Neural networks;Predictive models;Benchmark testing;Natural language processing;Encoding;Task analysis;Question Generation;Knowledge Graph;Graph and Question Interaction;Knowledge Distillation},
  doi={10.1109/IJCNN55064.2022.9892028},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{10511095,
  author={C, Naveen and B, Amutha},
  booktitle={2024 5th International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV)}, 
  title={Comparative Methods of Implementation for Different Question Answering Systems}, 
  year={2024},
  volume={},
  number={},
  pages={567-575},
  abstract={This research introduces an innovative Question Answering (QA) system tailored explicitly for government department inquiries regarding individuals. Harnessing the prowess of cutting-edge language models such as BERT and T5 (Text-to-Text Transfer Transformer), the system excels in understanding complex queries within diverse governmental domains. Moreover, it incorporates a specialized Knowledge Graph meticulously curated with interconnected information about people across various departments. By integrating BERT and T5 for versatile query comprehension and answer generation alongside a comprehensive People-centric Knowledge Graph, this system aims to revolutionize information retrieval within government entities. The seamless fusion of these technologies promises accurate, contextually rich responses, optimizing operational efficiency across government departments and fostering streamlined access to crucial information.},
  keywords={Government;Knowledge graphs;Transformers;Information retrieval;Question answering (information retrieval);Communications technology;BERT(Bidirectional Encoder Representations from Transformers);T5(Text-to-Text Transfer Transformer);Natural Language Processing;Knowledge Graph;Question Answering},
  doi={10.1109/ICICV62344.2024.00096},
  ISSN={},
  month={March},}
@INPROCEEDINGS{8731457,
  author={Wang, Shuo},
  booktitle={2019 IEEE 35th International Conference on Data Engineering (ICDE)}, 
  title={Knowledge Representation for Emotion Intelligence}, 
  year={2019},
  volume={},
  number={},
  pages={2096-2100},
  abstract={Emotion intelligence (EI) is a traditional topic for psychology, sociology, biology and medical science. Because emotion is related with the personality, interpersonal effect, social function, disease treatment, etc. Analyzing the emotion from the Web data by computer technology becomes more and more popular, and the scientists from the non-computer domains need more helpful computing models to deal with professional problems that are not traditional for computer science. Knowledge representation is a basic and possible solution as a bridge between emotion intelligence and artificial intelligence. For the sentiment words, word embedding can map the words to vectors that represent the semantic context of the words. Sentiment embedding based on the word embedding can capture both semantics and the emotion information. We have introduced two kinds of improving embedding methods (MEC and Emo2Vec) for the sentiment words embedding. For emotion structure based on the psychology of emotion, knowledge graph can represent the cognitive relations between different emotion types. The same emotional expressions can affect the reaction and behaviors of the recipient in different ways due to factors such as social relations, information processing, time pressure, etc. Knowledge graph can represent these complicated situations as the relations between the entities and attributes. Based on this graph, we make the inference or prediction of the emotion influence on decision making.},
  keywords={Appraisal;Semantics;Psychology;Sentiment analysis;Neural networks;Context modeling;Data mining;emotion intelligence, sentiment embedding, cognitive structure of emotion, knowledge graph},
  doi={10.1109/ICDE.2019.00247},
  ISSN={2375-026X},
  month={April},}@INPROCEEDINGS{10456739,
  author={Zhang, Heyan and Xing, Chenlin and Luo, Tao},
  booktitle={2023 3rd International Conference on Electronic Information Engineering and Computer Communication (EIECC)}, 
  title={Semantic Propagation Method Enhanced by Structure for Link Prediction}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In link prediction, text is widely used due to its rich semantic information. However, existing researches have the problems of high information redundancy, large complexity of algorithms, and lack of integration with structure knowledge of knowledge graph. This paper proposes a component-focused Siamese textual encoder CFSTE, which reduces the negative impact of redundant text description by adding a component focusing module and introducing a Siamese structure to reuse the same entities and relationships to reduce the complexity of the encoder. In addition, the structure knowledge of knowledge graph is also jointed through the integration strategy of interpolation prediction. Our proposed model is tested on multiple datasets and achieves state-of-the-art performance on the overall indicators. On the WN18rr, Hits@1, Hits@3 and MRR are increased by 0.22%, 1.63% and 5.28%, respectively. For FB15k-237, Hits@1, Hits@3, Hits@10 and MRR are increased by 8.76%, 4.99%, 3.04% and 4.68%, respectively.},
  keywords={Interpolation;Computational modeling;Semantics;Redundancy;Focusing;Knowledge graphs;Predictive models;Link prediction;Siamese textual encoder;knowledge graph;text extraction},
  doi={10.1109/EIECC60864.2023.10456739},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10702264,
  author={Li, Qiang and Ouyang, Hong and Zheng, Jianning and Xiang, Hui and Zhao, Feng and Zhao, Linlin and Wang, Guaiqiang and Li, Wenpu},
  booktitle={2024 20th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)}, 
  title={An Auxiliary Decision Model for Distribution Network Dispatching Based on Scientific Computing}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={As distributed generation resources are interconnected on a large scale, the management and dispatching of distribution networks have become increasingly complex, and there is an urgent need for the auxiliary decision-making of distribution network dispatching to transition towards intelligence. Currently, operations such as load transfer still rely on manual handling, encountering issues like low digitalization levels and poor plan optimization. Decisions on load transfer strategies fail to utilize human-machine intelligent collaboration for adaptive decision-making in actual operational contexts. Therefore, an auxiliary decision model for distribution network dispatching based on scientific computing is proposed. Firstly, an optimized model for load transfer in distribution networks based on the collaborative optimization of primary and secondary distribution was developed. The objective is to minimize the operation costs of the transmission network and the costs of load shedding in the distribution network, thereby refining load supply transfer strategies and improving the computational efficiency and the optimality of these strategies. Secondly, an intent understanding and information recognition model is built upon LLaMA -CRF and knowledge graph methodologies. This model utilizes the LLaMA large model in conjunction with a parallel application architecture of the knowledge graph to facilitate the comprehension of intentions and the extraction of load supply transfer information. Finally, by establishing evaluation indicators and a test dataset for the load transfer operation auxiliary decision-making model, the effectiveness of intent understanding, information extraction, and optimization strategies for load transfer are assessed, verifying the validity of the proposed scientific computing-based auxiliary decision-making model for distribution network dispatching and operation.},
  keywords={Adaptation models;Scientific computing;Computational modeling;Decision making;Distribution networks;Knowledge graphs;Dispatching;Computational efficiency;Optimization;Load modeling;component;formatting;style;styling;intelligent decision support;load transfer;scientific computing;LLaMA;knowledge graph},
  doi={10.1109/ICNC-FSKD64080.2024.10702264},
  ISSN={},
  month={July},}@INPROCEEDINGS{10035573,
  author={Min, Chanwook and Ahn, Jinhyun and Lee, Taewhi and Im, Dong-Hyuk},
  booktitle={2023 17th International Conference on Ubiquitous Information Management and Communication (IMCOM)}, 
  title={TK-BERT: Effective Model of Language Representation using Topic-based Knowledge Graphs}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Recently, the K-BERT model was proposed to add knowledge for language representation in specialized fields. The K-BERT model uses a knowledge graph to perform transfer learning on the pre-trained BERT model. However, the K-BERT model adds the knowledge that exists in the knowledge graph rather than the data relevant to the topic of the input data when using the knowledge graph of the corresponding field. Hence, the K-BERT model can cause confusion in the training. To solve this problem, this study proposes a topic-based knowledge graph BERT (TK-BERT) model, which uses the topic modeling technique. The TK-BERT model divides the knowledge graph by topic using the knowledge graph&#x0027;s topic model and infers the topic for the input sentence, adding only knowledge relevant to the topic. Therefore, the TK-BERT model does not add unnecessary knowledge to the knowledge graph. Moreover, the proposed TK-BERT model outperforms the K-BERT model.},
  keywords={Training;Bit error rate;Transfer learning;Data models;Information management;Knowledge Graphs;Language Representation;BERT;K-BERT;Topic Modeling},
  doi={10.1109/IMCOM56909.2023.10035573},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10826046,
  author={Ojima, Yuta and Sakaji, Hiroki and Nakamura, Tadashi and Sakata, Hiroaki and Seki, Kazuya and Teshigawara, Yuu and Yamashita, Masami and Aoyama, Kazuhiro},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Knowledge Management for Automobile Failure Analysis Using Graph RAG}, 
  year={2024},
  volume={},
  number={},
  pages={6624-6631},
  abstract={This paper presents a knowledge management system for automobile failure analysis using retrieval-augmented generation (RAG) with large language models (LLMs) and knowledge graphs (KGs). In the automotive industry, there is a growing demand for knowledge transfer of failure analysis from experienced engineers to young engineers. However, failure events are phenomena that occur in a chain reaction, making them difficult for beginners to analyze them. While knowledge graphs, which can describe semantic relationships and structure information is effective in representing failure events, due to their capability of representing the relationships between components, there is much information in KGs, so it is challenging for young engineers to extract and understand sub-graphs from the KG. On the other hand, there is increasing interest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for knowledge management. However, when using the current Graph RAG framework with an existing knowledge graph for automobile failures, several issues arise because it is difficult to generate executable queries for a knowledge graph database which is not constructed by LLMs. To address this, we focused on optimizing the Graph RAG pipeline for existing knowledge graphs. Using an original Q&A dataset, the ROUGE F1 score of the sentences generated by the proposed method showed an average improvement of 157.6% compared to the current method. This highlights the effectiveness of the proposed method for automobile failure analysis.},
  keywords={Large language models;Semantics;Retrieval augmented generation;Pipelines;Failure analysis;Knowledge graphs;Automobiles;Prompt engineering;Knowledge transfer;Automotive engineering;Graph RAG;Large Language Model;Knowledge Graph;Knowledge Management;Automobile Failure},
  doi={10.1109/BigData62323.2024.10826046},
  ISSN={2573-2978},
  month={Dec},}@ARTICLE{10530439,
  author={Qiu, Zhangchi and Tao, Ye and Pan, Shirui and Liew, Alan Wee-Chung},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Knowledge Graphs and Pretrained Language Models Enhanced Representation Learning for Conversational Recommender Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-15},
  abstract={Conversational recommender systems (CRSs) utilize natural language interactions and dialog history to infer user preferences and provide accurate recommendations. Due to the limited conversation context and background knowledge, existing CRSs rely on external sources such as knowledge graphs (KGs) to enrich the context and model entities based on their interrelations. However, these methods ignore the rich intrinsic information within entities. To address this, we introduce the knowledge-enhanced entity representation learning (KERL) framework, which leverages both the KG and a pretrained language model (PLM) to improve the semantic understanding of entities for CRS. In our KERL framework, entity textual descriptions are encoded via a PLM, while a KG helps reinforce the representation of these entities. We also employ positional encoding to effectively capture the temporal information of entities in a conversation. The enhanced entity representation is then used to develop a recommender component that fuses both entity and contextual representations for more informed recommendations, as well as a dialog component that generates informative entity-related information in the response text. A high-quality KG with aligned entity descriptions is constructed to facilitate this study, namely, the Wiki Movie Knowledge Graph (WikiMKG). The experimental results show that KERL achieves state-of-the-art results in both recommendation and response generation tasks. Our code is publicly available at the link: https://github.com/icedpanda/KERL.},
  keywords={Oral communication;Motion pictures;Encoding;Task analysis;Knowledge graphs;History;Semantics;Conversational recommender system (CRS);knowledge graph (KG);pretrained language model (PLM);representation learning},
  doi={10.1109/TNNLS.2024.3395334},
  ISSN={2162-2388},
  month={},}@INPROCEEDINGS{10184605,
  author={Chen, Zhuo and Zhang, Wen and Huang, Yufeng and Chen, Mingyang and Geng, Yuxia and Yu, Hongtao and Bi, Zhen and Zhang, Yichi and Yao, Zhen and Song, Wenting and Wu, Xinliang and Yang, Yi and Chen, Mingyi and Lian, Zhaoyang and Li, Yingying and Cheng, Lei and Chen, Huajun},
  booktitle={2023 IEEE 39th International Conference on Data Engineering (ICDE)}, 
  title={Tele-Knowledge Pre-training for Fault Analysis}, 
  year={2023},
  volume={},
  number={},
  pages={3453-3466},
  abstract={In this work, we share our experience on tele-knowledge pre-training for fault analysis, a crucial task in telecommunication applications that requires a wide range of knowledge normally found in both machine log data and product documents. To organize this knowledge from experts uniformly, we propose to create a Tele-KG (tele-knowledge graph). Using this valuable data, we further propose a tele-domain language pre-training model TeleBERT and its knowledge-enhanced version, a tele-knowledge re-training model KTeleBERT. which includes effective prompt hints, adaptive numerical data encoding, and two knowledge injection paradigms. Concretely, our proposal includes two stages: first, pre-training TeleBERT on 20 million tele-related corpora, and then re-training it on 1 million causal and machine-related corpora to obtain KTeleBERT. Our evaluation on multiple tasks related to fault analysis in tele-applications, including root-cause analysis, event association prediction, and fault chain tracing, shows that pretraining a language model with tele-domain data is beneficial for downstream tasks. Moreover, the KTeleBERT re-training further improves the performance of task models, highlighting the effectiveness of incorporating diverse tele-knowledge into the model.},
  keywords={Adaptation models;Analytical models;Semantics;Predictive models;Data models;Encoding;Communications technology;telecommunication;model pre-training;knowledge graph;numeric encoding;fault analysis},
  doi={10.1109/ICDE55515.2023.00265},
  ISSN={2375-026X},
  month={April},}@ARTICLE{9961919,
  author={Sun, Guangzhi and Zhang, Chao and Woodland, Philip C.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Minimising Biasing Word Errors for Contextual ASR With the Tree-Constrained Pointer Generator}, 
  year={2023},
  volume={31},
  number={},
  pages={345-354},
  abstract={Contextual knowledge is essential for reducing speech recognition errors on high-valued long-tail words. This paper proposes a novel tree-constrained pointer generator (TCPGen) component that enables end-to-end ASR models to bias towards a list of long-tail words obtained using external contextual information. With only a small overhead in memory use and computation cost, TCPGen can structure thousands of biasing words efficiently into a symbolic prefix-tree, and creates a neural shortcut between the tree and the final ASR output to facilitate the recognition of the biasing words. To enhance TCPGen, we further propose a novel minimum biasing word error (MBWE) loss that directly optimises biasing word errors during training, along with a biasing-word-driven language model discounting (BLMD) method during the test. All contextual ASR systems were evaluated on the public Librispeech audiobook corpus and the data from the dialogue state tracking challenges (DSTC) with the biasing lists extracted from the dialogue-system ontology. Consistent word error rate (WER) reductions were achieved with TCPGen, which were particularly significant on the biasing words with around 40% relative reductions in the recognition error rates. MBWE and BLMD further improved the effectiveness of TCPGen, and achieved more significant WER reductions on the biasing words. TCPGen also achieved zero-shot learning of words not in the audio training set with large WER reductions on the out-of-vocabulary words in the biasing list.},
  keywords={Training;Generators;Decoding;Hidden Markov models;Context modeling;Speech recognition;Error analysis;Contextual speech recognition;end-to-end;language model discounting;minimum Bayes' risk;Pointer generator},
  doi={10.1109/TASLP.2022.3224286},
  ISSN={2329-9304},
  month={},}@INPROCEEDINGS{9529829,
  author={Neji, Sameh and Chenaina, Tarek and Shoeb, Abdullah M. and Ben Ayed, Leila},
  booktitle={2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={HIR: A Hybrid IR Ranking Model}, 
  year={2021},
  volume={},
  number={},
  pages={1717-1722},
  abstract={The aim of information retrieval (IR) process is to respond effectively to user queries by retrieving information that are better meets their expectations. A gap could exist between user information needs and his/her defined context as the level of the user's expertise in the search domain is directly influencing the query richness. The information retrieval system (IRS) must be intelligent enough to identify information needs and respond effectively to meet the expected needs, regardless the level of user's expertise level. This process is difficult and it remains a major and open challenge in the domain of IR. IR models integrate many sources to achieve an effective retrieval system. Semantic IR is an environment in which semantic techniques were applied to sort the documents according to their degree of relevance to the query. The present work proposes a hybrid model to rank documents. The proposed model is based on a query likelihood language model and the semantic similarity between concepts to assess the relevance between query-document pairs. Concepts were extracted by projection on WordNet ontology then word sense disambiguation was conducted. A semantic index was built to validate the proposed model. The conducted empirical experiments show that the proposed model is outperformed the compared benchmarks in the measured IR metrics.},
  keywords={Measurement;Semantic search;Computational modeling;Semantics;Ontologies;Benchmark testing;Information retrieval;semantic information retrieval;query-document relevance;language model;conceptual model;semantic similarity},
  doi={10.1109/COMPSAC51774.2021.00256},
  ISSN={0730-3157},
  month={July},}@INPROCEEDINGS{9999076,
  author={Japa, Sai Sharath and Green, Sarah},
  booktitle={2022 IEEE Eighth International Conference on Multimedia Big Data (BigMM)}, 
  title={Question Answering over Knowledge Base with Variational Auto-Encoder}, 
  year={2022},
  volume={},
  number={},
  pages={29-36},
  abstract={Knowledge Base is a semantic data repos-itory made available over the web. Knowledge bases are multi-relational graphs consisting of entities and relations representing facts about the world. These facts follow a controlled vocabulary that drives the knowledge base, often called ontology. Entities are the graph nodes, while relations are the edges connecting the nodes. While knowledge base attention prolifer-ates, extracting information from these resources is challenging. One of the promising approaches is Question Answering systems. The Question Answering over Knowledge Base(KB-QA) system aims to provide a natural language interface to pose a query to the KB. QA systems use embeddings to map the posed Question to the facts stored in the knowledge bases. QA systems can be effectively improved using large transformer models. These transformer models are computationally expensive, which prevents their use in real-world applications. To solve this, we started our study with the use-case of Knowledge Base Question Answering systems. In KB-QA systems, representing the entities and relationships play a pivotal role in extracting the answer to the posed query. Traditional KB embedding techniques represent entities/relations as vectors, mapping them in different semantic spaces and ignoring the subtle cor-relation. A pre-training language model for encoding the KB facts will preserve the semantic and contextual correlation. While learning linguistic knowledge, these models also store relational knowledge in the training data. These Language Models are often trained on a massive corpus addressing the Out-Of- Vocabulary problem. In this paper, we incorporate a co-embedding model for knowledge base embedding, which learns low-dimensional representations of entities and relations in the same semantic space. We propose a Variational Auto-Encoder, an efficient transformer architecture that represents knowledge base representations as Gaussian distributions to address the issue of neglecting uncertainty. In addition, our method has high quality and interpretability advantages compared with previous methods. Our experimental results show the effectiveness and the superiority of the VAE approach for question-answering systems on knowledge bases over other well-known pre-trained embedding methods.},
  keywords={Vocabulary;Uncertainty;Computational modeling;Knowledge based systems;Semantics;Training data;Transformers;knowledge base question answering;Bert;Language Model;KBQA;Multi-Head Attention;VAE;Encoder;Transformers},
  doi={10.1109/BigMM55396.2022.00012},
  ISSN={},
  month={Dec},}@ARTICLE{10091124,
  author={Tu, Yamei and Wang, Xiaoqi and Qiu, Rui and Shen, Han-Wei and Miller, Michelle and Rao, Jinmeng and Gao, Song and Huber, Patrick R. and Hollander, Allan D. and Lange, Matthew and Garcia, Christian R. and Stubbs, Joe},
  journal={IEEE Computer Graphics and Applications}, 
  title={An Interactive Knowledge and Learning Environment in Smart Foodsheds}, 
  year={2023},
  volume={43},
  number={3},
  pages={36-47},
  abstract={The Internet of Food (IoF) is an emerging field in smart foodsheds, involving the creation of a knowledge graph (KG) about the environment, agriculture, food, diet, and health. However, the heterogeneity and size of the KG present challenges for downstream tasks, such as information retrieval and interactive exploration. To address those challenges, we propose an interactive knowledge and learning environment (IKLE) that integrates three programming and modeling languages to support multiple downstream tasks in the analysis pipeline. To make IKLE easier to use, we have developed algorithms to automate the generation of each language. In addition, we collaborated with domain experts to design and develop a dataflow visualization system, which embeds the automatic language generations into components and allows users to build their analysis pipeline by dragging and connecting components of interest. We have demonstrated the effectiveness of IKLE through three real-world case studies in smart foodsheds.},
  keywords={Knowledge graphs;Ontologies;Data visualization;Food products;Smart agriculture;Internet of Things;Data models},
  doi={10.1109/MCG.2023.3263960},
  ISSN={1558-1756},
  month={May},}@INPROCEEDINGS{9533494,
  author={Ji, Xin and Zhao, Wen},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={SKGSUM: Abstractive Document Summarization with Semantic Knowledge Graphs}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={In abstractive single-document summarization task, generated summaries always suffer from fabricated and less informative content. An intuitive way to alleviate this problem is to merge external semantic knowledge into the model framework. In this paper, we incorporate explicit graphs based on semantic knowledge, including term frequency, discourse information, and entities with their relations, into neural abstractive summarization for the problem. We propose a novel model for abstractive single-document Summarization based on Semantic Knowledge Graphs (SKGSUM), which regards sentences and entities as nodes, captures the relations between units in different textual levels, and focuses on salient content in the source documents to guide the summary generation process. To the best of our knowledge, we are the first to exploit different textual-unit levels explicit graph representations in a unified framework for the neural abstractive summarization task. Results show that our model achieves significant improvements on both XSum and CNN/Daily Mail datasets over some strong baselines. Human evaluations further indicate that our model can generate informative and coherent summaries.},
  keywords={Semantics;Neural networks;Decoding;Task analysis;Postal services},
  doi={10.1109/IJCNN52387.2021.9533494},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9874185,
  author={Huang, Yen-Hao and Harryyanto, Kevin and Tsai, Che-Wei and Pornvattanavichai, Ratana and Chen, Yi-Shin},
  booktitle={2022 IEEE 23rd International Conference on Information Reuse and Integration for Data Science (IRI)}, 
  title={Graph Knowledge Transfer for Offensive Language Identification with Graph Neural Networks}, 
  year={2022},
  volume={},
  number={},
  pages={216-221},
  abstract={Identifying offensive language (OL) has become ever more important with the rise of online social media (OSM). Most works on OL identification have applied sequential models to learn offensive semantics. In a different light, recent popular graph neural networks (GNNs) model text in a word graph and learn local word-level usages for each document. This work aims to explore text GNNs on learning OL usages in a non-sequential view yet there are two barriers: (1) Similar to sequential models, GNNs also suffer from the out-of-vocabulary (OOV) issue due to the informal usages of OSM texts. (2) Lacking of appropriate edge weights derived from short content to the word graph. Motivated by the rich resources of OSM, this work leverages existing OL corpus to build a global reference graph and proposes a graph knowledge transfer mechanism (GKtran) to tackle the limited extent of learning from short texts. To embed OL knowledge, GKtran infers edge weights from a reference graph and transfers weights to each document's word graph. Edges that connect OOV words are assigned weights through weight propagation. Thus, each document graph is fully weighted for graph classification. Experimental results show that the additional information from training or external data transferred by GKtran effectively improves OL identification with graphs.},
  keywords={Training;Social networking (online);Semantics;Data science;Graph neural networks;Rough surfaces;Knowledge transfer;Natural language processing;Graph neural network;Knowledge transfer;offensive identification;graph},
  doi={10.1109/IRI54793.2022.00056},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10385584,
  author={He, Yichen and Liu, Xiaofeng and Hu, Jinlong and Dong, Shoubin},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Entity Relation Aware Graph Neural Ranking for Biomedical Information Retrieval}, 
  year={2023},
  volume={},
  number={},
  pages={1118-1124},
  abstract={The performance of biomedical information retrieval greatly depends on biomedical knowledge; however the knowledge of available medical knowledge base is often incomplete and out-of-dated. To solve the problem that incomplete knowledge bases cannot provide the medical knowledge required for biomedical information retrieval, the paper proposes an Entity Relation Aware Graph Neural Ranking model (ERAGNR), aiming to fully leverage the internal knowledge of the document to alleviate the problem caused by incomplete external knowledge bases. ERAGNR mines the relationships between biomedical entities in the document through entity relation extraction and combines them with external knowledge. It increases the semantic association and reduces the semantic gap between the query and the document. The method first constructs a knowledge-query graph and a document-entity graph, and then fuses the two graphs to obtain a knowledge-query-document-entity graph. In a multi-task learning framework that combines text retrieval and relation extraction tasks, ERAGNR employs a shared text encoder and a graph neural network. This enables ERAGNR to learn semantic matching patterns between queries and documents and recognize relationships between entities in the documents. As a result, the model can capture semantic matching signals between entity relationships in the context and queries. The experimental results show that ERAGNR outperforms the state-of-the-art models. Through biomedical relation extraction task, the model can learn the ability to capture the context of the entity relations in the document, so that the model can more accurately match the semantics between the query and the document.},
  keywords={Fuses;Biological system modeling;Semantics;Knowledge based systems;Predictive models;Information retrieval;Multitasking;Biomedical Text Retrieval;Multi-task Learning;Graph Neural Ranking;Document-level Biomedical Relation Extraction},
  doi={10.1109/BIBM58861.2023.10385584},
  ISSN={2156-1133},
  month={Dec},}@ARTICLE{10864471,
  author={Wickramarachchi, Ruwan and Henson, Cory and Sheth, Amit},
  journal={IEEE Internet Computing}, 
  title={Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities of Neurosymbolic AI}, 
  year={2024},
  volume={28},
  number={6},
  pages={62-67},
  abstract={In the era of generative artificial intelligence (AI), neurosymbolic AI is emerging as a powerful approach for tasks spanning from perception to cognition. The use of neurosymbolic AI has been shown to achieve enhanced capabilities, including improved grounding, alignment, explainability, and reliability. However, due to its nascent stage, there is a lack of widely available real-world benchmark datasets that are tailored to neurosymbolic AI tasks. To address this gap and support the evaluation of current and future methods, we introduce DSceneKG (Driving Scenes Knowledge Graph), a suite of KGs of driving scenes that is built from real-world, high-quality scenes from multiple open autonomous driving datasets. In this article, we detail the construction process of DSceneKG and highlight its application in seven different tasks.},
  keywords={Generative AI;Knowledge graphs;Benchmark testing;Cognition;Internet;Reliability;Artificial intelligence;Autonomous vehicles;Neural engineering;Autonomous driving;Task analysis;Autonomous vehicles;Advanced driver assistance systems},
  doi={10.1109/MIC.2024.3494972},
  ISSN={1941-0131},
  month={Nov},}@INPROCEEDINGS{10831494,
  author={Geng, Haibin and Shi, Chenglong and Jiang, Xuesong and Kong, Zan and Liu, Song},
  booktitle={2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={An Entity Relation Extraction Framework Based on Large Language Model and Multi-Tasks Iterative Prompt Engineering}, 
  year={2024},
  volume={},
  number={},
  pages={4763-4769},
  abstract={Document-level entity relation extraction is an important task in the field of natural language processing, which plays an important role in semantic understanding and knowledge graph construction. However, existing deep neural networks and graph neural networks models are limited by their performance and parameters number, which can not capture global semantics and have poor generalization ability. Furthermore, existing methods employing large language model for entity relation extraction do not establish good relationships among multi-tasks of entity relation extraction task, resulting in more information can not be effectively shared and transmitted between tasks. In addition, previous approaches can not effectively eliminate false entities and relationships. To solve these problems, we propose an entity relation extraction framework based on large language model and multi-tasks iterative prompt engineering. In our model, we design an iterative prompt engineering, which can better establish the relationship among multi-tasks, and ensure every task to obtain the optimal results. Moreover, we design semantic merging, group disambiguation and self-verification modules to eliminate the false entity relations and noise nodes. Additionally, we design summary prompts to provide sufficient global semantics for better text segmentation. Finally, we evaluated our model on wikiann, wikineural, ACE2005, CoNLL2003, CoNLL2004, and SciERC datasets and compared it with other baseline models.},
  keywords={Large language models;Semantics;Noise;Merging;Knowledge graphs;Multitasking;Natural language processing;Iterative methods;Prompt engineering;Data mining},
  doi={10.1109/SMC54092.2024.10831494},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10796004,
  author={Bouquet, Paolo and Molinari, Andrea and Sandri, Simone},
  booktitle={2024 4th International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)}, 
  title={Challenges in Working with Unstructured Data in the LLM Era: NER Processes Using Graph Neural Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The application of neural networks and large language models (LLMs) in the financial technology (fintech) sector has significantly enhanced capabilities of providing better services based on unstructured documents, mainly thanks to the astonishing interactive capabilities of large language models (LLMs). However, in these contexts, precisely recognizing named entities (for example, the beneficiary of an insurance policy) and creating reliable, domain-dependent, private knowledge graphs is more crucial than conversational capabilities. Moreover, optimizing these models for dynamic data environments and ensuring Explainability presents notable challenges. This paper aims to present our approach for the creation of a knowledge graph that uses graph neural nets (GNN) to provide a better named-entity recognition (NER) process, to identify entities in the text precisely and to solve other issues in knowledge graph management, like the co-ference resolution problem or focusing on dynamic data environments and the critical need for Explainability of modern AI-based solutions. The paper presents our preliminary results on one of the various pipeline steps that creates the navigable knowledge graph.},
  keywords={Mechatronics;Large language models;Pipelines;Insurance;Focusing;Knowledge graphs;Graph neural networks;Data models;Reliability;Artificial Intelligence;Graph Neural Nets;Knolwedge Graph;Named entity resolution},
  doi={10.1109/ICECCME62383.2024.10796004},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9164163,
  author={Li, FangGuo and Zhang, BeiKe and Gao, Dong},
  booktitle={2020 Chinese Control And Decision Conference (CCDC)}, 
  title={Chinese Named Entity Recognition for Hazard And Operability Analysis Text}, 
  year={2020},
  volume={},
  number={},
  pages={374-378},
  abstract={To solve the problem that it is difficult to identify the key Chinese entity information in the hazard and operability analysis text, a deep neural network model based on bidirectional long short-term memory and conditional random field (BiLSTM-CRF) is proposed to identify key named entities in the text. In the word vector pre-training process, bidirectional encoder representation from transformers (BERT) model is used to pre-train word vectors instead of the static word vectors in the traditional word2vec model, and then obtain context-related dynamic word vectors, to improve the representational ability of word vectors, and solve the problem of word boundary division when word vectors are used in Chinese corpus training. This model has the ability of complete the task of Chinese named entity recognition. The F1 value on the test corpus reaches 93.31%, which is 21.82% higher than conditional random field (CRF) Baseline, and 4.49% higher than the traditional BiLSTM-CRF model. The experimental results show that the BERT-BiLSTM-CRF model is effective for the named entity recognition (NER) task of the hazard and operability analysis text, and it is helpful to automatically extract the relationship between the entities in the hazard and operability analysis text and build safety analysis knowledge graph.},
  keywords={Hazards;Context modeling;Bit error rate;Task analysis;Analytical models;Computer architecture;Logic gates;Chinese named entity recognition;Hazard and operability analysis;BERT;BiLSTM-CRF model},
  doi={10.1109/CCDC49329.2020.9164163},
  ISSN={1948-9447},
  month={Aug},}@INPROCEEDINGS{10020770,
  author={Zhang, Yu and Zhang, Yunyi and Jiang, Yucheng and Michalski, Martin and Deng, Yu and Popa, Lucian and Zhai, ChengXiang and Han, Jiawei},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Entity Set Co-Expansion in StackOverflow}, 
  year={2022},
  volume={},
  number={},
  pages={4792-4795},
  abstract={Given a few seed entities of a certain type (e.g., Software or Programming Language), entity set expansion aims to discover an extensive set of entities that share the same type as the seeds. Entity set expansion in software-related domains such as StackOverflow can benefit many downstream tasks (e.g., software knowledge graph construction) and facilitate better IT operations and service management. Meanwhile, existing approaches are less concerned with two problems: (1) How to deal with multiple types of seed entities simultaneously? (2) How to leverage the power of pre-trained language models (PLMs)? Being aware of these two problems, in this paper, we study the entity set co-expansion task in StackOverflow, which extracts Library, OS, Application, and Language entities from StackOverflow question-answer threads. During the co-expansion process, we use PLMs to derive embeddings of candidate entities for calculating similarities between entities. Experimental results show that our proposed SECoExpan framework outperforms previous approaches significantly.},
  keywords={Computer languages;Big Data;Software;Libraries;Data mining;Task analysis;set expansion;entity extraction;StackOverflow},
  doi={10.1109/BigData55660.2022.10020770},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10021030,
  author={Khabiri, Elham and Agrawal, Bhavna and Lindquist, Joseph and Li, Yingjie and Bhamidipaty, Anuradha},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Big data techniques for industrial problems with little data}, 
  year={2022},
  volume={},
  number={},
  pages={2285-2289},
  abstract={Technicians and maintenance managers in industrial environments would benefit from automatically extracting entities and relationships from different text data sources such as logs, event reports, and manuals. Extracting components from pieces of text and classifying them to the right failure type is not trivial in the domain specific setting where the vocabulary has specific meaning to the industry or domain, and labeled data set is very small. In this paper we address how to overcome these challenges in named entity recognition and classification of text, and present a way to improve the model iteratively and quickly. This interaction between components and related failures in the system can be represented in a knowledge graph, which enables further investigations such as Root Cause Analysis and Problem Diagnosis.},
  keywords={Root cause analysis;Vocabulary;Text recognition;Soft sensors;Pipelines;Training data;Manuals},
  doi={10.1109/BigData55660.2022.10021030},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9735670,
  author={Yiyi, Zha and Chong, Wang and Mingming, Zhang and Kai, Liu},
  booktitle={2021 IEEE Sustainable Power and Energy Conference (iSPEC)}, 
  title={Research on Power Information Knowledge Extraction Model Based on BERT}, 
  year={2021},
  volume={},
  number={},
  pages={3900-3904},
  abstract={Knowledge extraction is the basic step of constructing power information knowledge graph. Aiming at the problems of polysemy, poor context awareness and limited sentence types in the previous text processing methods in the power field, we propose a knowledge extraction method based on BERT from the characteristics of power information text. In the first part, BERT-BiGRU-CRF model is used to complete the named entity recognition of power information text. In the second part, BERT-BiGRU-attention model is used to extract the relation between entities. The model can improve the effect of named entity recognition and relation extraction in complex context, and can process diversified power information texts.},
  keywords={Text recognition;Conferences;Bit error rate;Context awareness;Data mining;Text processing;Context modeling;power information;natural language processing;named entity recognition;relation extraction},
  doi={10.1109/iSPEC53008.2021.9735670},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10628558,
  author={Fieblinger, Romy and Alam, Md Tanvirul and Rastogi, Nidhi},
  booktitle={2024 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)}, 
  title={Actionable Cyber Threat Intelligence Using Knowledge Graphs and Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={100-111},
  abstract={Cyber threats are constantly evolving. Extracting actionable insights from unstructured Cyber Threat Intelligence (CTI) data is essential to guide cybersecurity decisions. Increasingly, organizations like Microsoft, Trend Micro, and CrowdS trike are using generative AI to facilitate CTI extraction. This paper addresses the challenge of automating the extraction of actionable CTI using advancements in Large Language Models (LLMs) and Knowledge Graphs (KGs). We explore the application of state-of-the-art open-source LLMs, including the Llama 2 series, Mistral 7B Instruct, and Zephyr for extracting meaningful triples from CTI texts. Our methodology evaluates techniques such as prompt engineering, the guidance framework, and fine-tuning to optimize information extraction and structuring. The extracted data is then utilized to construct a KG, offering a structured and queryable representation of threat intelligence. Experimental results demonstrate the effectiveness of our approach in extracting relevant information, with guidance and fine-tuning showing superior performance over prompt engineering. However, while our methods prove effective in small-scale tests, applying LLMs to large-scale data for KG construction and Link Prediction presents ongoing challenges.},
  keywords={Large language models;Refining;Knowledge graphs;Organizations;Predictive models;Ontologies;Cyber threat intelligence;Cyber Threat Intelligence;Large Language Models;Knowledge Graphs;Threat Prediction},
  doi={10.1109/EuroSPW61312.2024.00018},
  ISSN={2768-0657},
  month={July},}@INPROCEEDINGS{10410334,
  author={Hoogendoorn, Tjalling and Arachchige, Jeewanie Jayasinghe and Bukhsh, Faiza A.},
  booktitle={2023 International Conference on Frontiers of Information Technology (FIT)}, 
  title={Survey of Explainability within Process Mining: A case study of BPI challenge 2020}, 
  year={2023},
  volume={},
  number={},
  pages={43-48},
  abstract={The need for explainability in Business process management is tremendously increasing, especially in the age of generative AI. The number of published articles on explainable AI (XAI) has skyrocketed for five years. AI impacts the decision-making process in business analytics. Process mining as a sub-discipline of data science can play a role in explainable business decision-making. Process mining exhibits its intention in process discovery, performance measures of processes, and process improvements based on the event logs. Although the accuracy of the outcome of process mining models has been investigated at a certain level, the explainability of those is possible through the discretization of the analytic steps. As an initial step in exploring the explainability of process mining, this research conducts a technical analysis of 37 research papers submitted to the Business Process Intelligence (BPI) Challenge 2020. The main focus of this analysis aims to answer the question, "How and why a process model is produced? " To make a foundation for the research question, the notion of explainability is explored based on an Explainable AI ontology. Due to the small sample size, the study cannot identify clear trends of explainability in process mining. However, the results conclude that explainability depends on the process model’s transparency and reproducibility. Moreover, further research with a large sample size is required to understand the discrete factors impacting decision-making in business process management.},
  keywords={Analytical models;Explainable AI;Decision making;Market research;Business process management;Data mining;Business;Data Science;Process Mining;Explainability;Transparency},
  doi={10.1109/FIT60620.2023.00018},
  ISSN={2473-7569},
  month={Dec},}@INPROCEEDINGS{10718742,
  author={Datta, Arkajit and Verma, Tushar and Chawla, Rajat and S, Mukunda N. and Bhola, Ishaan},
  booktitle={2024 29th International Conference on Automation and Computing (ICAC)}, 
  title={AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={In recent advancements within the domain of Large Language Models (LLMs), there has been a notable emergence of agents capable of addressing Robotic Process Automation (RPA) challenges through enhanced cognitive capabilities and sophisticated reasoning. This development heralds a new era of scalability and human-like adaptability in goal attainment. In this context, we introduce AUTONODE (Autonomous User-interface Transformation through Online Neuro-graphic Operations and Deep Exploration). AUTONODE employs advanced neuro-graphical techniques to facilitate autonomous navigation and task execution on web interfaces, thereby obviating the necessity for predefined scripts or manual intervention. Our engine empowers agents to comprehend and implement complex workflows, adapting to dynamic web environments with unparalleled efficiency. Our methodology synergizes cognitive functionalities with robotic automation, endowing AUTONODE with the ability to learn from experience. We have integrated an exploratory module, DoRA (Discovery and mapping Operation for graph Retrieval Agent), which is instrumental in constructing a knowledge graph that the engine utilizes to optimize its actions and achieve objectives with minimal supervision. The versatility and efficacy of AUTONODE are demonstrated through a series of experiments, highlighting its proficiency in managing a diverse array of web-based tasks, ranging from data extraction to transaction processing. The implementation of our paper can be accessed at :https://github.com/TransformerOptimus/AutoNode},
  keywords={YOLO;Intelligent automation;Scalability;Large language models;Refining;Manuals;Knowledge graphs;Robustness;Engines;Graphical user interfaces;Self-Operating Computer;Generative AI;LLMs;Transformers;Vision-Transformers;Graphs;Reinforcement Learning},
  doi={10.1109/ICAC61394.2024.10718742},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10825154,
  author={Kumar, Neha Mohan and Lisa, Fahmida Tasnim and Islam, Sheikh Rabiul},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Prompt Chaining-Assisted Malware Detection: A Hybrid Approach Utilizing Fine-Tuned LLMs and Domain Knowledge-Enriched Cybersecurity Knowledge Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={1672-1677},
  abstract={As malware threats continue to evolve in complexity, developing accurate and explainable detection systems is crucial for robust cybersecurity. This paper introduces a hybrid malware classification system that leverages fine-tuned large language models (LLMs) and an enriched cybersecurity knowledge graph (KG) to enhance both detection accuracy and interpretability. The system processes two types of input data—network packets and memory dumps, and classifies applications as benign or malign. Fine-tuned Llama models provide initial classifications, along with reasoning, which are further refined through prompt chaining using a knowledge graph populated with MITRE ATT&CK data and SecureBERT embeddings. The KG facilitates contextual reasoning, resulting in more accurate detection and informed decision-making. Experimental results demonstrate classification accuracies of 91.2% for network packet data and 94.35% for memory dump data, despite challenges related to LLM hallucinations and output parsing.},
  keywords={Adaptation models;Accuracy;Large language models;Refining;Knowledge graphs;Malware;Cognition;Data models;Reliability;Computer security;malware detection;explainability;knowledge graph;chain-prompting;large language models},
  doi={10.1109/BigData62323.2024.10825154},
  ISSN={2573-2978},
  month={Dec},}@ARTICLE{9091129,
  author={Jia, Yongnan and Min, Gaochen and Xu, Cong and Li, Xisheng and Zhang, Dezheng},
  journal={IEEE Access}, 
  title={A Knowledge Driven Dialogue Model With Reinforcement Learning}, 
  year={2020},
  volume={8},
  number={},
  pages={131741-131749},
  abstract={In recent decades, many researchers pay a lot of attention on generating informative responses in end-to-end neural dialogue systems. In order to output the responses with knowledge and fact, many works leverage external knowledge to guide the process of response generation. However, human dialogue is not a simple sequence to sequence task but a process heavily relying on their background knowledge about the topic. Thus, the key of generating informative responses is leveraging the appropriate knowledge associated with current topic. This paper focus on addressing incorporating the appropriate knowledge in response generation. We adopt the reinforcement learning to select the most proper knowledge as the input information of the response generation part. Then we design an end-to-end dialogue model consisting of the knowledge decision part and the response generation part. The proposed model is able to effectively complete the knowledge driven dialogue task with specific topic. Our experiments clearly demonstrate the superior performance of our model over other baselines.},
  keywords={Learning (artificial intelligence);Task analysis;Knowledge engineering;Decision making;Information retrieval;Computational modeling;Cognition;Dialogue model;policy gradient;knowledge graph;transformer network},
  doi={10.1109/ACCESS.2020.2993924},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10837958,
  author={Okuhara, Fumika and Egami, Shusaku and Sei, Yuichi and Tahara, Yasuyuki and Ohsuga, Akihiko},
  booktitle={2024 IEEE 7th International Conference on Computer and Communication Engineering Technology (CCET)}, 
  title={Enhancing Panoramic Competency Through Link Prediction in Question Knowledge Graphs using a Language Representation Model}, 
  year={2024},
  volume={},
  number={},
  pages={267-272},
  abstract={Recently, panoramic knowledge has been required. On the other hand, multiple choice questions are suitable for efficient self-learning. Therefore, the purpose of this study is to create multiple choice questions that can reinforce learners' panoramic knowledge. Specifically, we proposed a method for automatically generating multiple choice questions that use Linked Data to present relevant information to give respondents an overall picture of relevant knowledge. There is some research on the methods that generated questions by extracting small subgraphs from the knowledge graphs consisting of entities(words) and relations(links) between the entities and hiding target words (correct answer words). In this study, our goal is to enhance the panoramic of the subgraphs of a specified size by using the link prediction method to complement edges and represent relationships not present in the knowledge graph when generating questions targeted at specific fields. The method of complementing edges involves first inputting two words as subject and object in Knowledge Graph to calculate the cosine similarity using a pretrained language model based on Wikipedia and Wikidata, then predicting the links as a predicate that should be complemented, and finally generating subgraphs by using the Graph Database added the complemented edges. For this study, we generated questions in the field of history, and since history requires temporal and spatial panoramic knowledge, words related to these aspects were focused on and complemented the relationships between them. As a result, 2,746 relationships were complemented by the proposed method in the subgraphs, and the subgraphs contained more words to learn (words found in textbooks that need to be learned) in a specific field compared to those generated using existing methods.},
  keywords={Hands;Databases;Linked data;Computational modeling;Knowledge graphs;Encyclopedias;Predictive models;History;Online services;Panoramic Knowledge;Linked Data;Knowledge Graph;Automatic Generate Question;Educational Application},
  doi={10.1109/CCET62233.2024.10837958},
  ISSN={2836-5992},
  month={Aug},}@INPROCEEDINGS{10597753,
  author={Helali, Mossad and Monjazeb, Niki and Vashisth, Shubham and Carrier, Philippe and Helal, Ahmed and Cavalcante, Antonio and Ammar, Khaled and Hose, Katja and Mansour, Essam},
  booktitle={2024 IEEE 40th International Conference on Data Engineering (ICDE)}, 
  title={KGLiDS: A Platform for Semantic Abstraction, Linking, and Automation of Data Science}, 
  year={2024},
  volume={},
  number={},
  pages={179-192},
  abstract={In recent years, we have witnessed the growing interest from academia and industry in applying data science technologies to analyze large amounts of data. In this process, a myriad of artifacts (datasets, pipeline scripts, etc.) are created. However, there has been no systematic attempt to holistically collect and exploit all the knowledge and experiences that are implicitly contained in those artifacts. Instead, data scientists recover information and expertise from colleagues or learn via trial and error. Hence, this paper presents a scalable platform, KGLiDS, that employs machine learning and knowledge graph technologies to abstract and capture the semantics of data science artifacts and their connections. Based on this information, KGLiDS enables various downstream applications, such as data discovery and pipeline automation. Our comprehensive evaluation covers use cases in data discovery, data cleaning, transformation, and AutoML. It shows that KGLiDS is significantly faster with a lower memory footprint than the state-of-the-art systems while achieving comparable or better accuracy.},
  keywords={Automation;Accuracy;Systematics;Semantics;Pipelines;Machine learning;Knowledge graphs;Linked Data Science;Knowledge Graphs;Graph Neural Networks;Data Integration;Data Discovery},
  doi={10.1109/ICDE60146.2024.00021},
  ISSN={2375-026X},
  month={May},}@INPROCEEDINGS{10800474,
  author={Li, Zehao and Zhong, Ling and Han, Xinyi},
  booktitle={2024 4th International Conference on Electronic Information Engineering and Computer Science (EIECS)}, 
  title={Research on the Entity Relationship Extraction Method of Large Model Chinese Electronic Medical Record With Low-Moment Features}, 
  year={2024},
  volume={},
  number={},
  pages={1043-1046},
  abstract={The entity relationship extraction of Chinese electronic medical records is used to construct the knowledge graph in the medical field, and it plays an important role in serving downstream tasks. However, due to the fuzzy boundary of Chinese electronic medical records, the complex relationship between medical texts, and the high density of entities, the task of extracting the physical relationships of medical texts is not accurate enough. To solve this problem, this paper adopts a large language model ChatGLM2-6B based on (Generative Language Model) architecture and Quantized Low-rank Adaptation of Large Language Models (QLoRA) for Chinese electronic medical record entity relationship extraction. By leveraging the powerful language understanding capabilities of ChatGLM2-6B and the efficient fine-tuning characteristics of QLoRA, the low-moment features are integrated to achieve accurate and efficient Chinese electronic medical record entity relationship extraction, which provides strong support for medical data analysis and clinical decision-making.},
  keywords={Knowledge engineering;Analytical models;Accuracy;Large language models;Computational modeling;Decision making;Knowledge graphs;Feature extraction;Data mining;Electronic medical records;knowledge graph;relationship extraction;large language models;low-moment features},
  doi={10.1109/EIECS63941.2024.10800474},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10266269,
  author={Sierra-Múnera, Alejandro and Westphal, Jan and Krestel, Ralf},
  booktitle={2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL)}, 
  title={Efficient Ultrafine Typing of Named Entities}, 
  year={2023},
  volume={},
  number={},
  pages={205-214},
  abstract={Ultrafine named entity typing (UFET) refers to the assignment of predefined labels to entity mentions in a given context. In contrast to traditional named entity typing, the number of potential labels is in the thousands and one mention can have more than one assigned type. Previous approaches either depend on large training datasets, or require inefficient encoding of all input-type combinations. Therefore, there is a need for investigating the efficiency during training and prediction of entity typing models in the ultrafine-grained setting, considering its distinctively bigger search space, compared to the coarse- and fine-grained tasks. To efficiently solve UFET, we propose Decent, a lightweight model that encodes, using a pretrained language model, the input sentences separately from the type labels. Additionally, we make use of negative oversampling to speed up the training while improving the generalization of unseen types. Using an openly available UFET dataset, we evaluated the classification and runtime performance of Decent and observed that training and prediction runtime is orders of magnitude faster than the current state-of-the-art approaches, while maintaining a competitive classification performance.},
  keywords={Training;Vocabulary;Runtime;Limiting;Computational modeling;Semantics;Predictive models;ultrafine enity typing;named entity recognition},
  doi={10.1109/JCDL57899.2023.00038},
  ISSN={2575-8152},
  month={June},}@ARTICLE{8540089,
  author={Zhou, Bo and Maines, Curtis and Tang, Stephen and Shi, Qi and Yang, Po and Yang, Qiang and Qi, Jun},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={A 3-D Security Modeling Platform for Social IoT Environments}, 
  year={2018},
  volume={5},
  number={4},
  pages={1174-1188},
  abstract={Social Internet-of-Things (SIoT) environment comprises not only smart devices but also the humans who interact with these IoT devices. The benefits of such system are overshadowed due to the cyber security issues. A novel approach is required to understand the security implication under such a dynamic environment while taking both the social and technical aspects into consideration. This paper addressed such challenges and proposed a 3-D security modeling platform that can capture and model the security requirements in the SIoT environment. The modeling process is graphical notation based and works as a security extension to the Business Process Model and Notation. Still, it utilizes the latest 3-D game technology; thus, the security extensions are generated through the third dimension. Consequently, the introduction of security extensions will not increase the complexity of the original SIoT scenario, while keeping all the key information on the same platform. Together with the proposed security ontology, these comprehensive security notations created a unique platform that aims at addressing the ever complicated security issues in the SIoT environment.},
  keywords={Ontologies;Security;Complexity theory;Internet of Things;Process modeling;Business process;game technology;notation;security modeling;social Internet of Things (SIoT)},
  doi={10.1109/TCSS.2018.2878921},
  ISSN={2329-924X},
  month={Dec},}@INPROCEEDINGS{9087557,
  author={Matveev, Anton and Makhnytkina, Olesia and Lizunova, Inna and Vinogradova, Taisiia and Chirkovskii, Artem and Svischev, Aleksei and Mamaev, Nikita},
  booktitle={2020 26th Conference of Open Innovations Association (FRUCT)}, 
  title={A Virtual Dialogue Assistant for Conducting Remote Exams}, 
  year={2020},
  volume={},
  number={},
  pages={284-290},
  abstract={In this paper, we demonstrate issues and possible solutions to building an Artificial Intelligence Dialogue Assistant for human-machine communication. We specialize it for conducting written exams at online education platforms, talk about the main logical components of the system: knowledge base, question encoder, question generation module, question analysis module. As a knowledge base we consider text fragments representing parts of the course of text fragments representing parts of a course and is a source for a format ontology; and is also sourced for neural network generation of fact-based questions in question generation module and building dependency trees for answer evaluation in question analysis module.},
  keywords={Technological innovation;Knowledge based systems;Buildings;Neural networks;Ontologies;Solids;Complexity theory},
  doi={10.23919/FRUCT48808.2020.9087557},
  ISSN={2305-7254},
  month={April},}@INPROCEEDINGS{8282005,
  author={D'Haro, Luis Fernando and Niculescu, Andreea I. and Cai, Caixia and Nair, Suraj and Banchs, Rafael E. and Knoll, Alois and Li, Haizhou},
  booktitle={2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)}, 
  title={An integrated framework for multimodal human-robot interaction}, 
  year={2017},
  volume={},
  number={},
  pages={076-082},
  abstract={Recent research progresses in speech recognition, text-to-speech, natural language understanding, or dialog management components are improving the way humans interact with advanced robot machines. However, far from being solved, we are just starting the process of creating meaningful multimodal platforms that can allow operators to use and control industrial robots through spoken dialogue. This paper describes our ongoing efforts on creating a modular platform that combines different technologies to cover typical requirements in an industrial setting, i.e. robust speech recognition, low level skill functions to operate the robot, recommendations and validation procedures to setup parameters, combination of audio-visual information for challenging environments, integration of domain-knowledge by means of an ontology, a flexible definition of the dialog model and natural language rules, as well as a test and control interface to quickly check the functionality of each module during development and operation. All platform modules are intercommunicated by the ROS operative system which allows the integration of external plugins and modules easily. Finally, a preliminary user study with IT experts simulating a welding task has been doing giving us clues on what should be the focus of our next developments.},
  keywords={Task analysis;Robots;Ontologies;Welding;Speech recognition;Speech;Natural languages},
  doi={10.1109/APSIPA.2017.8282005},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10148526,
  author={Zhang, Xiang and Yu, Bruce X.B. and Liu, Yan and Chen, Gong and Ng, George Wing-Yiu and Chia, Nam-Hung and So, Eric Hang-Kwong and So, Sze-Sze and Cheung, Victor Kai-Lam},
  booktitle={2022 IEEE International Conference on Teaching, Assessment and Learning for Engineering (TALE)}, 
  title={Conversational System for Clinical Communication Training Supporting User-defined Tasks}, 
  year={2022},
  volume={},
  number={},
  pages={396-403},
  abstract={Effective clinical communication is essential for delivering safe and high-quality patient care, especially in emergent cases. Standard communication protocols have been developed to improve communication accuracy and efficiency. However, traditional training and evaluation require substantial manpower and time, which can be infeasible during public crises when training is most needed. This research aims to facilitate autonomous, low-cost, adaptive clinical communication training via artificial intelligence (AI)-powered techniques. We propose a conversational system for clinical communication training supporting user-defined tasks. Two data augmentation (DA) methods, term replacement and context expansion, are proposed to allow non-professional users to create Al models with a small number of samples. Equipped with biomedical ontology and pre-trained language models, our system is able to simulate clinical communication scenarios, provide timely evaluation, and adapt to new tasks with minimal editing. Various experiments demonstrate that our proposed algorithms can achieve satisfactory performance using a small amount of training data. Real-world practice in local hospitals shows that our system can provide expert-level evaluation and deliver effective clinical communication training.},
  keywords={Training;Adaptation models;Protocols;Biological system modeling;Training data;Ontologies;Data models;clinical communication;human-computer interaction;autonomous communication training;conversational system},
  doi={10.1109/TALE54877.2022.00071},
  ISSN={2470-6698},
  month={Dec},}@INPROCEEDINGS{10196264,
  author={Okazaki, Sho and Shirafuji, Shouhei and Yasui, Toshinori and Ota, Jun},
  booktitle={2023 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)}, 
  title={A Framework to Support Failure Cause Identification in Manufacturing Systems through Generalization of Past FMEAs}, 
  year={2023},
  volume={},
  number={},
  pages={858-865},
  abstract={This study proposes a framework for inferring the causes of failures occurring in manufacturing systems from past Failure Mode and Effect Analyses (FMEAs) conducted on other systems to assist in inspecting and maintaining the systems. Among various manufacturing systems, a framework to search past FMEAs and the corresponding causes of the failure requires solving the following problems. First, the difference in products, equipment, and wording to represent them make it difficult to search the similar failure phenomenon from FMEAs. Secondly, the causes of failure highly depend on the process flow of the system until the failure occurs. Therefore, it is also hard to find appropriate failure causes from FMEAs without reflecting on the process. The framework solves the first issue by generalizing descriptions in past FMEAs based on structured concepts of manufacturing systems in an ontology before inference of causes to address. Furthermore, the framework analyzes the correspondence of the process flows between the target manufacturing system and past FMEAs using a process order model generated by SysML diagrams to solve the second issue. The comparison between the causes inferred by the proposed framework and by skilled experts for three typical failures in the manufacturing system and the interview with them about the plausibility of the inference results showed that more than 73 % of them were valid.},
  keywords={Analytical models;Mechatronics;Ontologies;Maintenance engineering;Search problems;Cognition;Data models},
  doi={10.1109/AIM46323.2023.10196264},
  ISSN={2159-6255},
  month={June},}@INPROCEEDINGS{10655250,
  author={Quan, Ruijie and Wang, Wenguan and Ma, Fan and Fan, Hehe and Yang, Yi},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Clustering for Protein Representation Learning}, 
  year={2024},
  volume={},
  number={},
  pages={319-329},
  abstract={Protein representation learning is a challenging task that aims to capture the structure and function of proteins from their amino acid sequences. Previous methods largely ignored the fact that not all amino acids are equally important for protein folding and activity. In this article, we propose a neural clustering framework that can automatically discover the critical components of a protein by considering both its primary and tertiary structure information. Our framework treats a protein as a graph, where each node represents an amino acid and each edge represents a spatial or sequential connection between amino acids. We then apply an iterative clustering strategy to group the nodes into clusters based on their 1D and 3D positions and assign scores to each cluster. We select the highest-scoring clusters and use their medoid nodes for the next iteration of clustering, until we obtain a hierarchical and informative representation of the protein. We evaluate on four protein-related tasks: protein fold classification, enzyme reaction classification, gene ontology term prediction, and enzyme commission number prediction. Experimental results demonstrate that our method achieves state-of-the-art performance.},
  keywords={Proteins;Representation learning;Enzymes;Computer vision;Three-dimensional displays;Ontologies;Amino acids;Protein Representation Learning;Clustering},
  doi={10.1109/CVPR52733.2024.00038},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10508456,
  author={Yang, Puhai and Huang, Heyan and Shi, Shumin and Mao, Xian-Ling},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={STN4DST: A Scalable Dialogue State Tracking Based on Slot Tagging Navigation}, 
  year={2024},
  volume={32},
  number={},
  pages={2494-2507},
  abstract={Dialogue state tracking plays a key role in tracking user intentions in task-oriented dialogue systems. Traditional dialogue state tracking methods usually rely on selecting slot values from a fixed ontology to represent the dialogue state. In recent years, more flexible open vocabulary based approaches have become the mainstream focus which are mainly divided into two categories: generative methods and span extraction methods. Among them, the span extraction method is favored for its outstanding ability to predict unknown slot values. However, the span extraction method only focuses on the predicted slot values, but ignores other potential slot values in the utterance, which leads to insufficient semantic understanding of the utterance and difficulty in dealing with complex utterance scenarios, such as more or longer unknown slot values. To tackle the above drawbacks, in this paper, we propose a novel scalable dialogue state tracking method, which employs slot tagging to locate all potential slot values in the utterances and jointly learns slot pointers to select the predicted slot value from them. Specifically, our STN4DST (Slot Tagging Navigation for Dialogue State Tracking) model not only adopts the above joint learning strategy, which we call slot tagging navigation, to extract slot values from utterances, but also uses previous dialogue states as dialogue contexts to track the change of slot values, and introduces appendix slot values to predict special slot values that cannot be extracted. Extensive experiments show that in the open vocabulary setting, STN4DST achieves the state-of-the-art joint goal accuracy of 85.4% and 96.5% on Sim-M and Sim-R datasets with a large number of unknown slot values, and is also comparable to other state-of-the-art models in the absence of token-level slot annotations for all potential slot values.},
  keywords={Tagging;Navigation;Speech processing;Vocabulary;Semantics;Predictive models;Ontologies;Task-oriented dialogue system;dialogue state tracking;scalable DST;unknown slot value},
  doi={10.1109/TASLP.2024.3393733},
  ISSN={2329-9304},
  month={},}@ARTICLE{10288434,
  author={Tara, Andrei and Turesson, Hjalmar K. and Natea, Nicolae and Kim, Henry M.},
  journal={IEEE Access}, 
  title={An Evaluation of Storage Alternatives for Service Interfaces Supporting a Decentralized AI Marketplace}, 
  year={2023},
  volume={11},
  number={},
  pages={116919-116931},
  abstract={Given the exploding interest in generative AI and the concern that a few companies like Microsoft will monopolize access to such models, we address this centralization risk in the context of a DApp that matches buyers and sellers of various AI services. A key question for a decentralized marketplace is where and how to store the metadata that specifies the services’ properties in human and machine-readable formats. Having one or a few actors controlling access to that data constitutes undesirable centralization. We explore data storage alternatives to ensure decentralization, equitable match-making, and efficiency. Classifying decentralized storage alternatives as simple peer-to-peer replication, replication governed by a permissionless consensus, and replication governed by a private consensus, we select an exemplar for each category: IPFS, Tendermint Cosmos and Hyperledger Fabric. We conduct experiments on performance and find that read and write speeds are fastest for IPFS, about two times slower for Tendermint and slowest for Hyperledger. Writing using IPFS and Tendermint takes significantly longer than reading, and finally, specifically with IPFS, write speeds strongly depend on configuration. Given these results and the properties of the storage technologies, we conclude that simple peer-to-peer storage is the best option for the proposed AI marketplace.},
  keywords={Artificial intelligence;Ontologies;Peer-to-peer computing;Blockchains;Distributed ledger;Resource description framework;Fabrics;Decentralized applications;Storage management;Blockchain;decentralized AI decentralized storage;distributed ontology;semantic models},
  doi={10.1109/ACCESS.2023.3326418},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10191094,
  author={Liu, Tengfei and Hu, Yongli and Chen, Puman and Sun, Yanfeng and Yin, Baocai},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Zero-Shot Text Classification with Semantically Extended Textual Entailment}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Zero-shot text classification (0SHOT-TC) aims to detect classes that the model never seen in the training set, and has attracted much attention in the research community of Natural Language Processing (NLP). The emergence of pre-trained language models has fostered the progress of 0SHOT-TC, which turns the task into a textual entailment problem of binary classification. It learns an entailment relatedness (yes/no) between the given sentence (premise) and each category (hypothesis) separately. However, the hypothesis generation paradigms need to be further studied, since the label itself or the label descriptions have limited ability to fully express the category space. Conversely, humans can easily extend a set of words describing the categories to be classified. In this paper, we propose a novel zero-shot text classification method called Semantically Extended Textual Entailment (SETE), which imitates the human's ability in knowledge extension. In the proposed method, three semantic extension methods are used to enrich the categories through a combination of static knowledge (e.g. expert knowledge, knowledge graph) and dynamic knowledge (e.g. language models), and the textual entailment model is finally used for 0SHOT-TC. The experimental results on the benchmarks show that our approach significantly outperforms the current methods in both generalized and non-generalized 0SHOT-TC.},
  keywords={Knowledge engineering;Training;Vocabulary;Text categorization;Semantics;Neural networks;Benchmark testing;Zero-shot text classification;semantic extension;text entailment},
  doi={10.1109/IJCNN54540.2023.10191094},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10236713,
  author={Liang, Meng and Shi, Yao},
  booktitle={2023 5th International Conference on Natural Language Processing (ICNLP)}, 
  title={Named Entity Recognition Method Based on BERT-whitening and Dynamic Fusion Model}, 
  year={2023},
  volume={},
  number={},
  pages={191-197},
  abstract={In the context of Natural Language Processing (NLP), Named Entity Recognition (NER) plays a crucial role in tasks like entity relationship extraction and knowledge graph construction. The accuracy of Chinese NER heavily relies on the representation of word embeddings. However, traditional word representation methods like word2vec suffer from word ambiguity and singular word vectors. Similarly, BERT-based word embeddings also exhibit anisotropy. To tackle these challenges, we propose a novel NER method that leverages BERT-whitening and dynamic fusion of BERT’s output from different layers. The dynamic fusion module calculates a weighted sum of BERT’s output across multiple layers, while the whitening module applies a whitening operation to eliminate the anisotropy of word embeddings. By integrating these modules, our model effectively captures the characteristics of input words, providing robust support for subsequent decoding. We evaluate our approach on the CLUENER2020 Chinese fine-grained named entity recognition dataset. Experimental results demonstrate that our method outperforms the traditional BERT-BiLSTM-CRF model without external resources and data expansion, leading to significant improvements in performance.},
  keywords={Training;Anisotropic magnetoresistance;Heuristic algorithms;Knowledge graphs;Natural language processing;Data models;Decoding;NER;word embedding;BERT;anisotropy;whitening},
  doi={10.1109/ICNLP58431.2023.00041},
  ISSN={},
  month={March},}@INPROCEEDINGS{10423971,
  author={Jiang, Yan and Zhang, Zhihou and He, Lingfeng and Gong, Tianyi and Du, Jiawen and Yin, Xinyu},
  booktitle={2023 6th International Conference on Data Science and Information Technology (DSIT)}, 
  title={Application of DA-Bi-SRU and Improved RoBERTa Model in Entity Relationship Extraction for High-Speed Train Bogie}, 
  year={2023},
  volume={},
  number={},
  pages={89-96},
  abstract={Due to the large number of professional terms and complex entity relationships in the field of high-speed train (HST) bogie, the accuracy of entity relationship extraction is low. In order to improve the efficiency and accuracy of entity relationship extraction in high-speed train bogie domain, we propose a novel entity relationship extraction model for the domain of high-speed train (HST) bogie with the aim of improving the efficiency and accuracy of entity relationship extraction. The proposed model is based on RoBERTa-wwm (A Robustly Optimized BERT Pretraining Approach with Whole Word Masking) and DA-Bi-SRU (Double-Attention-Based Bidirectional Simple Recurrent Unit). To facilitate this, we construct a new bogie relation extraction dataset comprising of 25,000 statements collected from literature and professional annotations. The RoBERTa-wwm is employed to obtain dynamic word vectors from the input statements and optimized using the bogie dataset. Subsequently, a Bi-SRU model based on dual attention mechanism is developed to capture bidirectional semantic information and contextual semantic linkage in a rapid manner. Our experiments show that the RoBERTa-wwm-DA-Bi-SRU model outperforms Bi-LSTM and RNN methods with a prediction accuracy of 88.53% and an F1 value of 86.60%. Our proposed model thus demonstrates the potential to accurately extract entity relationships in the bogie knowledge graph of high-speed trains, simplifying the construction process.},
  keywords={Semantics;Knowledge graphs;Predictive models;Data science;Data models;Data mining;Information technology;Improved BERT;Whole Word Masking;Bi-SRU;Attention mechanism;Deep learning;Entity relationship extraction;High-speed train bogie},
  doi={10.1109/DSIT60026.2023.00023},
  ISSN={},
  month={July},}@INPROCEEDINGS{10142258,
  author={Wang, Juan and Peng, Bitao and Tang, Jing},
  booktitle={2023 5th International Conference on Communications, Information System and Computer Engineering (CISCE)}, 
  title={Research on Named Entity Recognition in Judicial Field Based on ERNIE-Gram}, 
  year={2023},
  volume={},
  number={},
  pages={228-233},
  abstract={Named Entity Recognition is a key and fundamental task in natural language processing and can benefit many downstream tasks such as knowledge graph construction, question answering system, machine reading, etc. In view of the contradiction between the rapidly increasing of judgement documents and the low efficiency of manual analysis in the judicial field, we propose a NER model by using a combination of ERNIE-Gram, BiGRU, and CRF. ERNIE-Gram adopts multi-granularity n-gram language learning mechanism to learn the semantic of n-grams more adequately. Thus, we first employ the ERNIE-Gram to capture rich language representation, then we feed them into the BiGRU to obtain more important text features, finally, we use the CRF to decode and output optimal labeling sequence. We conduct experiments on an open dataset of the 2021 “Challenge of AI in Law” information extraction subtask and compare our model with currently familiar models for NER task. Experimental results demonstrate that our proposed model achieves a F1-score of 87.01%, and outperforms all the baseline models.},
  keywords={Learning systems;Text recognition;Computational modeling;Semantics;Knowledge graphs;Manuals;Information retrieval;judicial field;name entity recognition;pretrained language models;explicitly n-gram masked language modeling},
  doi={10.1109/CISCE58541.2023.10142258},
  ISSN={2833-2423},
  month={April},}@ARTICLE{9483677,
  author={Cao, Yixin and Kuang, Jun and Gao, Ming and Zhou, Aoying and Wen, Yonggang and Chua, Tat-Seng},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Learning Relation Prototype From Unlabeled Texts for Long-Tail Relation Extraction}, 
  year={2023},
  volume={35},
  number={2},
  pages={1761-1774},
  abstract={Relation Extraction (RE) is a vital step to complete Knowledge Graph (KG) by extracting entity relations from texts. However, it usually suffers from the long-tail issue. The training data mainly concentrates on a few types of relations, leading to the lack of sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypes from unlabeled texts, to facilitate the long-tail relation extraction by transferring knowledge from the relation types with sufficient training data. We learn relation prototypes as an implicit factor between entities, which reflects the meanings of relations as well as their proximities for transfer learning. Specifically, we construct a co-occurrence graph from texts, and capture both first-order and second-order entity proximities for embedding learning. Based on this, we further optimize the distance from entity pairs to corresponding prototypes, which can be easily adapted to almost arbitrary RE frameworks. Thus, the learning of infrequent or even unseen relation types will benefit from semantically proximate relations through pairs of entities and large-scale textual information. We have conducted extensive experiments on two publicly available datasets: New York Times and Google Distant Supervision. Compared with eight state-of-the-art baselines, our proposed model achieves significant improvements (4.1 percent F1 on average). Further results on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study to investigate the impacts of varying components, and apply it to four basic relation extraction models to verify the generalization ability. Finally, we analyze several example cases to give intuitive impressions as qualitative analysis. Our codes and data can be found in https://github.com/CrisJk/PA-TRP.},
  keywords={Prototypes;Training;Training data;Data mining;Annotations;Urban areas;Transfer learning;Relation extraction;long-tail;knowledge graph;prototype learning},
  doi={10.1109/TKDE.2021.3096200},
  ISSN={1558-2191},
  month={Feb},}@INPROCEEDINGS{10385748,
  author={Zhou, Zongzhen and Yang, Tao and Hu, Kongfa},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Traditional Chinese Medicine Epidemic Prevention and Treatment Question-Answering Model Based on LLMs}, 
  year={2023},
  volume={},
  number={},
  pages={4755-4760},
  abstract={Background: Epidemic diseases in Traditional Chinese Medicine (TCM) constitute an essential part of Chinese medical science. TCM has accumulated rich theoretical and practical experiences in the prevention and treatment of epidemic diseases, forming the academic system of epidemic febrile disease, providing robust support for epidemic prevention and resistance in TCM. However, the numerous and complex literature on TCM epidemic diseases brings challenges to the organization and discovery of epidemic disease knowledges. Objective: To leverage the powerful knowledge learning ability of state-of-the-art LLMs (LLMs) to address the efficient acquisition and utilization of TCM epidemic disease knowledges. Methods: By collecting content related to epidemic diseases from 194 ancient TCM books, as well as the knowledge graph of TCM epidemic disease prevention and treatment, we built the large TCM epidemic disease model EpidemicCHAT based on the ChatGLM model. To assess the performances of the model, several open-source LLMs were compared in the study. Results: Compared to traditional LLMs, which may fail to answer or produce hallucinations in the field of TCM epidemic diseases, EpidemicCHAT demonstrates superior answering and reasoning abilities. In the evaluation of TCM epidemic disease prescription generation, the model achieved scores of 44.02, 61.10, and 59.40 on the BLEU-4, ROUGE-L, and METEOR metrics, respectively. Conclusion: The EpidemicCHAT model proposed in this study performs excellently in the field of TCM epidemic diseases, which might provide a reference for the construction of TCM LLMs and applications such as TCM auxiliary diagnosis and Chinese herbal prescription generation.},
  keywords={Measurement;Epidemics;Knowledge acquisition;Knowledge graphs;Organizations;Question answering (information retrieval);Meteors;LLMs;traditional Chinese medicine;epidemic;knowledge graph},
  doi={10.1109/BIBM58861.2023.10385748},
  ISSN={2156-1133},
  month={Dec},}@ARTICLE{10246785,
  author={Gupta, Amit and Bhatia, Rajesh},
  journal={Journal of Web Engineering}, 
  title={Knowledge Based Deep Inception Model for Web Page Classification}, 
  year={2021},
  volume={20},
  number={7},
  pages={2131-2168},
  abstract={Web Page Classification is decisive for information retrieval and management task and plays an imperative role for natural language processing (NLP) problems in web engineering. Traditional machine learning algorithms excerpt covet features from web pages whereas deep leaning algorithms crave features as the network goes deeper. Pre-trained models such as BERT attains remarkable achievement for text classification and continue to show state-of-the-art results. Knowledge Graphs can provide rich structured factual information for better language modelling and representation. In this study, we proposed an ensemble Knowledge Based Deep Inception (KBDI) approach for web page classification by learning bidirectional contextual representation using pre-trained BERT incorporating Knowledge Graph embeddings and fine-tune the target task by applying Deep Inception network utilizing parallel multi-scale semantics. Proposed ensemble evaluates the efficacy of fusing domain specific knowledge embeddings with the pre-trained BERT model. Experimental interpretation exhibit that the proposed BERT fused KBDI model outperforms benchmark baselines and achieve better performance in contrast to other conventional approaches evaluated on web page classification datasets.},
  keywords={Machine learning algorithms;Knowledge based systems;Text categorization;Semantics;Web pages;Knowledge graphs;Information retrieval;Web page classification;transfer learning;knowledge graph embedding;pre-trained model},
  doi={10.13052/jwe1540-9589.2075},
  ISSN={1544-5976},
  month={October},}@INPROCEEDINGS{10298416,
  author={Chakraborty, Sarthak and Agarwal, Shubham and Garg, Shaddy and Sethia, Abhimanyu and Pandey, Udit Narayan and Aggarwal, Videh and Saini, Shiv},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={ESRO: Experience Assisted Service Reliability against Outages}, 
  year={2023},
  volume={},
  number={},
  pages={255-267},
  abstract={Modern cloud services are prone to failures due to their complex architecture, making diagnosis a critical process. Site Reliability Engineers (SREs) spend hours leveraging multiple sources of data, including the alerts, error logs, and domain expertise through past experiences to locate the root cause(s). These experiences are documented as natural language text in outage reports for previous outages. However, utilizing the raw yet rich semi-structured information in the reports systematically is time-consuming. Structured information, on the other hand, such as alerts that are often used during fault diagnosis, is voluminous and requires expert knowledge to discern. Several strategies have been proposed to use each source of data separately for root cause analysis. In this work, we build a diagnostic service called ESRO that recommends root causes and remediation for failures by utilizing structured as well as semi-structured sources of data systematically. ESRO constructs a causal graph using alerts and a knowledge graph using outage reports, and merges them in a novel way to form a unified graph during training. A retrieval based mechanism is then used to search the unified graph and rank the likely root causes and remediation techniques based on the alerts fired during an outage at inference time. Not only the individual alerts, but their respective importance in predicting an outage group is taken into account during recommendation. We evaluated our model on several cloud service outages of a large SaaS enterprise over the course of ~2 years, and obtained an average improvement of 27% in rouge scores after comparing the likely root causes against the ground truth over state-of-the-art baselines. We further establish the effectiveness of ESRO through qualitative analysis on multiple real outage examples.},
  keywords={Training;Fault diagnosis;Root cause analysis;Natural languages;Software as a service;Knowledge graphs;Reliability engineering;System Monitoring;Cloud Services;Causal Graph;Knowledge Graph},
  doi={10.1109/ASE56229.2023.00131},
  ISSN={2643-1572},
  month={Sep.},}@ARTICLE{10736645,
  author={Mi, Li and Dai, Xianjie and Castillo-Navarro, Javiera and Tuia, Devis},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Knowledge-Aware Text–Image Retrieval for Remote Sensing Images}, 
  year={2024},
  volume={62},
  number={},
  pages={1-13},
  abstract={Image-based retrieval in large Earth observation archives is challenging because one needs to navigate across thousands of candidate matches only with the query image as a guide. By using text as information supporting the visual query, the retrieval system gains in usability, but at the same time faces difficulties due to the diversity of visual signals that cannot be summarized by a short caption only. For this reason, as a matching-based task, cross-modal text–image retrieval often suffers from information asymmetry between text and images. To address this challenge, we propose a Knowledge-aware Text–Image Retrieval (KTIR) method for remote sensing images. By mining relevant information from an external knowledge graph, KTIR enriches the text scope available in the search query and alleviates the information gaps between text and images for better matching. Moreover, by integrating domain-specific knowledge, KTIR also enhances the adaptation of pretrained vision–language models to remote sensing applications. Experimental results on three commonly used remote sensing text–image retrieval benchmarks show that the proposed knowledge-aware method leads to varied and consistent retrievals, outperforming state-of-the-art retrieval methods.},
  keywords={Remote sensing;Feature extraction;Lakes;Commonsense reasoning;Adaptation models;Visualization;Image retrieval;Boats;Sensors;Knowledge graphs;Knowledge graph;remote sensing;text–image retrieval},
  doi={10.1109/TGRS.2024.3486977},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{9313210,
  author={Yang, Songchun and Zheng, Xiangwen and Tong, Fan and Mao, Huajian and Zhao, Dongsheng},
  booktitle={2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={A Novel Algorithm of Expansion Term Selection and Weight Assignment for Query Expansion of Chinese EMR Retrieval}, 
  year={2020},
  volume={},
  number={},
  pages={2139-2146},
  abstract={The techniques of information retrieval eliminate the difficulty of finding specific information in electronic medical records (EMR), and the methods of query expansion (QE) improve the recall of EMR retrieval. However, most existing QE methods of EMR retrieval can't get high-quality expansion terms and corresponding weights for Chinese EMR retrieval because of low-quality sources and improper calculation methods. In this paper, we propose a novel algorithm of expansion term selection and weight assignment for QE of Chinese EMR retrieval based on the clinical needs and unique characteristics of Chinese medical terms. The algorithm first selects expansion terms from a high quality Chinese medical knowledge graph and standard medical term sets, which can ensure the quality of expansion terms. Then it assigns weights to the selected expansion terms based on semantic similarities and manually designed expansion categories that reflect the opinions of medical experts. Experiment results show that our algorithm gets higher-quality expansion terms and more rational weights compared with four benchmark algorithms, using Precision at 10, Recall, Mean Average Precision and Binary Preference-based Measure as evaluation metrics, which implies that our algorithm can significantly improve the effectiveness of QE.},
  keywords={Semantics;Training;Standards;Medical diagnostic imaging;Compounds;Internet;Diseases;Information Retrieval;Query Expansion;Knowledge Graph;Word2Vec;Chinese Electronic Medical Record},
  doi={10.1109/BIBM49941.2020.9313210},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8995179,
  author={Liu, Chunfeng and Zhang, Yan and Yu, Mei and Li, Xuewei and Zhao, Mankun and Xu, Tianyi and Yu, Jian and Yu, Ruiguo},
  booktitle={2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Text-Enhanced Knowledge Representation Learning Based on Gated Convolutional Networks}, 
  year={2019},
  volume={},
  number={},
  pages={308-315},
  abstract={Knowledge representation learning (KRL), which transforms both the entities and relations into continuous low dimensional continuous vector space, has attracted considerable research. Most of existing knowledge graph (KG) completion models only considers the structural representation of triples, but do not consider the important text information about entity descriptions in the knowledge base. We propose a text-enhanced KG model based on gated convolution network (GConvTE), which can learn entity descriptions and symbol triples jointly by feature fusion. Specifically, each triple (head entity, relation, tail entity) is represented as a 3-column structural embedding matrix, a 3-column textual embedding matrix and a 3-column joint embedding matrix where each column vector represents a triple element. Textual embeddings are obtained by bidirectional gated recurrent unit with attention (A-BGRU) encoding entity descriptions and joint embeddings are obtained by the combination of textual embeddings and structural embeddings. Extending feature dimension in embedding layer, these three matrixs are concatenated into 3-channel feature block to be fed into convolution layer, where the gated unit is added to selectively output the joint features maps. These feature maps are concatenated and then multiplied with a weight vector via a dot product to return a score. The experimental results show that our model GConvTE achieves better link performance than previous state-of-art embedding models on two benchmark datasets.},
  keywords={representation learning;knowledge graph completion;joint representation;multidimensional feature;gated convolution network},
  doi={10.1109/ICTAI.2019.00051},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10738062,
  author={Fortuna, Carolina and Hanžel, Vid and Bertalanič, Blaz},
  booktitle={2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)}, 
  title={Natural Language Interaction with a Household Electricity Knowledge-based Digital Twin}, 
  year={2024},
  volume={},
  number={},
  pages={8-14},
  abstract={Domain specific digital twins, representing a digital replica of various segments of the smart grid, are foreseen as able to model, simulate, and control the respective segments. At the same time, knowledge-based digital twins, coupled with AI, may also empower humans to understand aspects of the system through natural language interaction in view of planning and policy making. This paper is the first to assess and report on the potential of Retrieval Augmented Generation (RAG) question answers related to household electrical energy measurement aspects leveraging a knowledge-based energy digital twin. Relying on the recently published electricity consumption knowledge graph that actually represents a knowledge-based digital twin, we study the capabilities of ChatGPT, Gemini and Llama in answering electricity related questions. Furthermore, we compare the answers with the ones generated through a RAG techniques that leverages an existing electricity knowledge-based digital twin. Our findings illustrate that the RAG approach not only reduces the incidence of incorrect information typically generated by LLMs but also significantly improves the quality of the output by grounding responses in verifiable data. This paper details our methodology, presents a comparative analysis of responses with and without RAG, and discusses the implications of our findings for future applications of AI in specialized sectors like energy data analysis.},
  keywords={Data analysis;Electricity;Knowledge based systems;Natural languages;Chatbots;Robustness;Question answering (information retrieval);Digital twins;Smart grids;Planning;retrieval augmented generation;large language models;knowledge-based digital twin;knowledge graph;house-holds},
  doi={10.1109/SmartGridComm60555.2024.10738062},
  ISSN={2474-2902},
  month={Sep.},}@INPROCEEDINGS{10740424,
  author={Colliani, Felice Paolo and Futia, Giuseppe and Garifo, Giovanni and Vetrò, Antonio and De Martin, Juan Carlos},
  booktitle={2024 IEEE 18th International Conference on Application of Information and Communication Technologies (AICT)}, 
  title={Towards Named Entity Disambiguation with Graph Embeddings}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Extracting structured knowledge from scientific literature is crucial for helping professionals make well-informed decisions. This paper presents an approach to distilling knowledge from biomedical documents within the context of Named Entity Disambiguation (NED). The proposed method leverages a joint representation of biomedical entities, combining pre-trained language models with graph machine learning techniques. A Siamese Neural Network (SNN) is trained to optimize this joint representation by integrating the contextual text embeddings of entity mentions with the graph embeddings of corresponding canonical entities in a biomedical Knowledge Graph (KG). During the inference phase, the SNN model assigns a score to this joint representation to disambiguate the target entity among a set of candidates. To the best of our knowledge, this is the first NED method in the biomedical domain that incorporates graph embeddings using a neural model. We empirically evaluated the effectiveness of our approach against well-known biomedical datasets, such as MedMentions and BC5CDR. The results demonstrate a promising direction in utilizing the relational knowledge captured by graph embeddings for the NED task.},
  keywords={Biological system modeling;Neural networks;Machine learning;Knowledge graphs;Information and communication technology;Named Entity Disambiguation;Graph Embeddings;Knowledge graph},
  doi={10.1109/AICT61888.2024.10740424},
  ISSN={2472-8586},
  month={Sep.},}@INPROCEEDINGS{9825235,
  author={Ji, Pengfei and Song, Dandan},
  booktitle={2022 3rd International Conference on Computer Vision, Image and Deep Learning & International Conference on Computer Engineering and Applications (CVIDL & ICCEA)}, 
  title={A Dual Knowledge Aggregation Network for Cross-Domain Sentiment Analysis}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={Cross-domain sentiment analysis (CDSA) is an essential subtask of sentiment analysis. It aims to utilize rich source domain data to conquer the data-hungry problem on target domain. Most existing approaches depending on deep learning mainly concentrate on common features or pivots. However, few of them consider the effect of external Knowledge Graph (KG). In this paper, we propose a Dual Knowledge Aggregation Network for Cross-Domain Sentiment Analysis (DKAN), which leverages prior knowledge from two external KGs. Specifically, DKAN comprises two main parts. One is extracting sentence representation features. The other aims to introduce external knowledge better. Also, we use SenticNet to avoid noise from KG by selecting top-n words and inserting special tokens in sentences. We also conduct empirical analyses on the effectiveness of our model on the Amazon reviews dataset. DKAN achieves promising performance compared with other methods.},
  keywords={Knowledge engineering;Deep learning;Sentiment analysis;Analytical models;Feature extraction;corss-domain;sentiment analysis;knowledge graph;SenticNet},
  doi={10.1109/CVIDLICCEA56201.2022.9825235},
  ISSN={},
  month={May},}@INPROCEEDINGS{10691283,
  author={Sumanathilaka, Deshan and Micallef, Nicholas and Hough, Julian},
  booktitle={2024 IEEE 15th Control and System Graduate Research Colloquium (ICSGRC)}, 
  title={Assessing GPT's Potential for Word Sense Disambiguation: A Quantitative Evaluation on Prompt Engineering Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={204-209},
  abstract={Modern digital communications (including social media content) often contain ambiguous words due to their potential for multiple related interpretations (polysemy). This ambiguity poses challenges for traditional Word Sense Disambiguation (WSD) methods, which struggle with limited data and lack of contextual understanding. These limitations hinder efficient translation, information retrieval, and question-answering systems, thereby restricting the benefits of computational linguistics techniques when applied to digital communication technologies. Our research investigates the use of Large Language Models (LLMs) to improve WSD using various prompt engineering techniques. We propose and evaluate a novel method that combines a knowledge graph, together with Part-of-Speech (POS) tagging and few-shot prompting to guide LLMs. By utilizing prompt augmentation with human-in-loop on few-shot prompt approaches, this work demonstrates a substantial improvement in WSD. This research advances accurate word interpretation in digital communications, leading to important implications for improved translation systems, better search results, and more intelligent question-answering technology.},
  keywords={Accuracy;Social networking (online);Large language models;Knowledge graphs;Tagging;Digital communication;Control systems;Computational linguistics;Prompt engineering;Large Language Models;Word Sense Disambiguation;FEWS sense tags;Few Shot Prompting;Knowledge Graph},
  doi={10.1109/ICSGRC62081.2024.10691283},
  ISSN={2833-1028},
  month={Aug},}@INPROCEEDINGS{9616677,
  author={P, Pavithra C and Mandal, Supriya},
  booktitle={2021 Fourth International Conference on Electrical, Computer and Communication Technologies (ICECCT)}, 
  title={An Overview of Relevant Literature on Different Approaches to Word Sense Disambiguation}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={WSD (Word Sense Disambiguation) is a common issue in Natural Language Processing (NLP) and Machine Learning technology. In NLP, word sense disambiguation is described as the capacity to detect which meaning of a word is activated by its use in a specific context. WSD is a solution to the uncertainty that occurs when words have different meanings in different contexts. Contextual word meaning plays an important role in various applications such as sentiment analysis, search engine, information extraction, machine translation etc. It is a challenge for these systems to detect and overcome the uncertainty that emerges from the lexical ambiguity. Many studies have been conducted over the decades to propose various approaches to the WSD problem. In this manuscript, a comparative study of three approaches namely LESK algorithm, embedding techniques, and Neural Network techniques based on the text collected from children's story books is performed. We explored an approach that combines Bi-LSTM neural network with Knowledge Graph to predict contextual word meaning. Our study shows that the combined approach accuracy is 80.34approaches},
  keywords={Deep learning;Knowledge engineering;Sentiment analysis;Uncertainty;Neural networks;Knowledge based systems;Search engines;Word Sense Disambiguation;Natural Language Processing;Lesk Algorithms;Embedding Techniques;Neural Network;Bi-LSTM;Knowledge Graph},
  doi={10.1109/ICECCT52121.2021.9616677},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10417286,
  author={Jayasooriya, Asel and Ahamed, Akeel and Bandara, Yomal and Gavindya, Chamod and Kasthurirathna, Dharshana and Abeywardhana, Lakmini},
  booktitle={2023 5th International Conference on Advancements in Computing (ICAC)}, 
  title={An Integrated Approach to Enhance Legal Information Retrieval of Sri Lankan Supreme Court Verdicts}, 
  year={2023},
  volume={},
  number={},
  pages={316-321},
  abstract={Legal professionals and law students often grapple with the arduous task of locating prior Supreme Court rulings for argument preparation and academic study, leading to a laborious and resource-intensive process. This study introduces an innovative approach to streamline the retrieval of legal information from Sri Lankan Supreme Court verdicts. The method centers on developing a Custom Named Entity Recognition (NER) model, boasting a remarkable accuracy exceeding 90%. This model efficiently extracts crucial legal entities from Supreme Court rulings, readily available on the Ministry of Justice's Supreme Court website. The high accuracy ensures precise entity extraction, followed by systematic organization within a dedicated database. Subsequently, a knowledge graph is formed by linking recorded legal entities, reducing information retrieval time to a mere 179 milliseconds, significantly outperforming existing methods. Moreover, a BART summarization model is crafted to generate concise, accurate, and insightful summaries of Supreme Court decisions, boasting an impressive ROGUE1 score of 85%. This approach revolutionizes legal information retrieval, delivering a user-friendly platform that enhances the identification of cases and fosters a deeper understanding, ultimately elevating the quality of legal research and practice.},
  keywords={Adaptation models;Law;Databases;Information retrieval;Feature extraction;Data mining;Task analysis;BART Model;Knowledge Graph;Legal Information Retrieval;Named Entity Recognition;Supreme Court Judgments},
  doi={10.1109/ICAC60630.2023.10417286},
  ISSN={2837-5424},
  month={Dec},}@INPROCEEDINGS{10650447,
  author={Li, Sirui and Wong, Kok Wai and Zhu, Dengya and Fung, Chun Che},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Enhancing Question Answering through Effective Candidate Answer Selection and Mitigation of Incomplete Knowledge Graphs and over-smoothing in Graph Convolutional Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Question answering over knowledge graphs (KGQA) seeks to automatically answer natural language questions by retrieving triples within the knowledge graph (KG). In the context of multi-hop KGQA, reasoning across multiple edges of the KG becomes crucial for obtaining answers. Existing methods align with either the path-searching-based mainstream, emphasizing structural KG analysis, or the subgraph-based mainstream, focusing on semantic KG embeddings. Both streams have two primary challenges: (1) KG incompleteness, where path searching or subgraph construction faces limitations in the absence of links between entities; (2) candidate answer selection, wherein most approaches employ pre-defined searching sizes or heuristics. Many recent studies incorporate Graph Convolutional Network (GCN) to encode KGs, yet they overlook the potential over-smoothing issue inherent in GCNs. The over-smoothing problem arises from the tendency of closely connected nodes to exhibit similar embeddings within the deep convolutional architecture of GCNs. To address these challenges, this paper proposes a two-stage framework named ComPath, leveraging insights from both mainstreams. ComPath utilizes GCN to tackle KG incompleteness and introduces a path analyser to mitigate the over-smoothing issue associated with GCN. Candidate answers are selected using semantic similarity. The ablation studies and comparative experiments on the three KGQA benchmark datasets shown that the proposed ComPath performed better than the other KGQAs.},
  keywords={Graph convolutional networks;Prevention and mitigation;Semantics;Neural networks;Natural languages;Knowledge graphs;Spread spectrum communication;Question Answering;Graph Convolutional Network;Incomplete Knowledge Graph;Over-smoothing;Candidate Selection},
  doi={10.1109/IJCNN60899.2024.10650447},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10500264,
  author={Thapa, Astha and Patil, Rajvardhan},
  booktitle={SoutheastCon 2024}, 
  title={ChatGPT based ChatBot Application}, 
  year={2024},
  volume={},
  number={},
  pages={157-164},
  abstract={Since the invention of ChatGPT, there have been several applications demonstrating how ChatGPT can be used to solve real world problems. This paper focuses on implementing one such application, where we design ChatBot using ChatGPT API. To carry out the experiments, we choose dataset from a specific domain, and store the data in graph oriented database. Sample training dataset consisting of English and its equivalent Cypher queries were designed for prompt tuning ChatGPT on the local Knowledge-graph. This training dataset helps ChatGPT understand the mapping between the English and their equivalent Cypher queries. Once the mapping is learnt, it becomes easier for ChatGPT to encode or represent a real time (previously unseen) English query fed by the user into its equivalent Cypher query format. In this paper, we demonstrate our results on sample Rope-related dataset scrapped from internet, and conclude with limitations and future work.},
  keywords={Training;Ciphers;Technological innovation;Databases;Knowledge graphs;Chatbots;Real-time systems;Chatbot;Chat Assistant;ChatGPT API;Knowledge Graph;Cypher Query Language;Graph Database},
  doi={10.1109/SoutheastCon52093.2024.10500264},
  ISSN={1558-058X},
  month={March},}@INPROCEEDINGS{10825070,
  author={Li, Tangrui and Zhou, Jun and Wang, Hongzheng},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Aligning Knowledge Graphs Provided by Humans and Generated by Neural Networks}, 
  year={2024},
  volume={},
  number={},
  pages={3441-3447},
  abstract={In this paper, an approach that extracts knowledge graphs (KGs) from neural networks (NNs) and aligns the generated KGs with human-provided ones is proposed for network optimization or transparency enhancement, which is achieved by leveraging Vector Symbolic Architectures (VSAs). The approach identifies entities and relations of NN’s knowledge along with the training process, which makes it a plug-and-play solution. Experiments on synthetic data showed that the matching method works on middle and small-size KGs, and tests on MNIST demonstrated that the aligned NN-generated KG could be very close to the human-provided ones. Further tests on Text2KGBench showed that the method could produce KGs from embedding generated by backbone large language models (LLM) that aligned well with human-provided labels as well.},
  keywords={Training;Codes;Large language models;Artificial neural networks;Knowledge graphs;Ontologies;Vectors;Optimization;Synthetic data;Software development management;Vector Symbolic Architecture;Knowledge Graph;Knowledge Graph Alignment},
  doi={10.1109/BigData62323.2024.10825070},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10392374,
  author={Gou, Yunhe and Jie, Cao},
  booktitle={2023 IEEE 3rd International Conference on Data Science and Computer Application (ICDSCA)}, 
  title={A lightweight biomedical named entity recognition with pre-trained model}, 
  year={2023},
  volume={},
  number={},
  pages={117-121},
  abstract={Biomedical Named Entity Recognition (BioNER) is a specialized subfield of Named Entity Recognition (NER) that focuses on identifying and classifying named entities in biomedical and clinical texts. The goal of BioNER is to extract essential information, such as genes, proteins, diseases, drugs, et al., from scientific literature, electronic health records (EHRs), biomedical databases, and other biomedical text sources. The recognition and classification of these entities are crucial for various biomedical and healthcare-related tasks, including information retrieval, data integration, knowledge extraction, and drug discovery. Traditional BioNER methods typically involve rule-based approaches or machine learning algorithms, et al. These methods have been widely used before the advent of deep learning and transformer-based models. Bidirectional Encoder Representations from Transformers (BERT) is a groundbreaking transformer-based language model. It has revolutionized various natural language processing (NLP) tasks by capturing contextual information and obtain optimal results in multiple benchmarks. A lightweight BioNER optimized model from traditional BERT (LWNER) is proposed in this study, which can capture contextual information and its knowledge transfer from pre-training on large-scale text corpora without relying heavily on feature engineering and handcrafted rules. Fine-tuning BERT on biomedical-specific data helps adapt the model to the nuances and terminology of the biomedical domain. We conduct the method LWNER on BioCreative dataset, BC2GM, BC4CHEMD, BC5CDR, especially the chemical entity in BC5CDR achieve F1- score 91.3%. We construct an online web tool based on LWNER to identify the arbitrary text from scientific literatures for building knowledge graph.},
  keywords={Adaptation models;Text recognition;Terminology;Biological system modeling;Bidirectional control;Transformers;Encoding;Transformer;BioNER;BERT;BiLSTM},
  doi={10.1109/ICDSCA59871.2023.10392374},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9339166,
  author={Feng, Haojun and Duan, Li and Liu, Shukan and Liu, Sijie},
  booktitle={2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={Entity Hierarchical Clustering Method Based on Multi-channel and T-SNE Dimension Reduction}, 
  year={2020},
  volume={9},
  number={},
  pages={2155-2159},
  abstract={Named entity clustering is a basic work in the field of natural language processing, which is helpful to excavate the implicit relationship between entities. Most of the existing clustering algorithms are unable to combine various features of entities and have some problems such as poor hierarchical clustering analysis. Based on this, this paper proposes a multi-channel dimensionless entity clustering method and carries out experimental verification. A multi-channel framework is constructed, and channels based on knowledge graph, language model and statistics are respectively set up to express the features of entities in objective knowledge, co-existing relationship and different texts. Network embedding method, BERT model and automatic coding machine are respectively used to convert the entities into vector forms. The t-SNE algorithm is used to reduce the dimension and map it to the two-dimensional space, so that it can be expressed visually in the low-dimensional space. An improved hierarchical clustering method is proposed to cluster entities in two-dimensional space and construct hierarchical clustering trees. Experiments show that the F1 value of this algorithm can reach 78.72% at most under the test set. At the same time, through analysis, the algorithm has a strong ability of expansion and generalization.},
  keywords={Dimensionality reduction;Clustering methods;Scalability;Bit error rate;Clustering algorithms;Feature extraction;Natural language processing;multi-channel;network embedding;BERT;t-SNE;improved hierarchical clustering},
  doi={10.1109/ITAIC49862.2020.9339166},
  ISSN={2693-2865},
  month={Dec},}@INPROCEEDINGS{10577330,
  author={Zhao, Ava and Su, Zhanqi and Fei, Bill and Zhuo, Na and Wang, Hao and Yu, Tianzhou and Li, Zuotian and Qian, Cheryl and Chen, Yingjie Victor},
  booktitle={2023 VAST Challenge}, 
  title={Facilitating Visual Analytics with ChatGPT: 2023 VAST Challenge Award - Application of LLMs to Support VA Process}, 
  year={2023},
  volume={},
  number={},
  pages={5-6},
  abstract={To solve the VAST Challenge 2023 MC3, our team employed a large language model, ChatGPT, to explore the potential of AI -guided visual analytics for the detection of anomalies within a knowledge graph in the context of illegal fishing and marine trade. We employed a systematic and iterative approach, guided by GPT augmentation, that enabled problem understanding, data processing, solution exploration, code writing, and results analysis. By generating and analyzing various graphs, we identified anomalies related to revenue and product services. Further analyses unveiled potential illegal fishing activities and identified instances warranting additional investigation. Overall, our work highlights both the strengths and limitations of ChatGPT in aiding the visual analytics process and emphasizes the importance of human judgment in refining AI-generated outputs.},
  keywords={Systematics;Visual analytics;Scalability;Refining;Collaboration;Manuals;Writing;Visual Analytics;ChatGPT;Assisted Learning;AI Language Model;Knowledge GraphMarine Trade},
  doi={10.1109/VASTChallenge60523.2023.00008},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10587417,
  author={Fu, Hanyu and Liu, Chunfang and Li, Xiaoli},
  booktitle={2024 36th Chinese Control and Decision Conference (CCDC)}, 
  title={Dynamic Task Planning: An Integrated Approach with Scene Relation Perception and Knowledge Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={1539-1543},
  abstract={Confronted with the limitations of conventional Planning Domain Definition Language (PDDL) in dynamic and unpredictable kitchen environments, this paper introduces a novel task planning methodology that leverages the synergy between advanced scene graphs. This interdisciplinary approach begins with the construction of a detailed task directive-target state dataset, which serves as the foundation for refining the capabilities of VisualBERT, a vision-language model. Through fine-tuning, VisualBERT becomes adept at accurately interpreting complex scene dynamics and anticipating the target states to achieve the task query. This is followed by the creation of an extensive knowledge graph containing important parameters of actions and objects. This knowledge base is instrumental in generating PDDL domain and problem files, taking into account both initial and target states. It plays a crucial role in the flexible subtask sequence generation for dynamic environments and tasks. Our proposed method significantly enhances the adaptability to real-world variability, thereby enabling dynamic task planning within domestic kitchen environments efficiently.},
  keywords={Trajectory planning;Instruments;Refining;Knowledge based systems;Knowledge graphs;Predictive models;Manipulators;task planning;PDDL;vision-language model;target state prediction;sub task sequences},
  doi={10.1109/CCDC62350.2024.10587417},
  ISSN={1948-9447},
  month={May},}@INPROCEEDINGS{9721762,
  author={Sajid, Hira and Kanwal, Javeria and Bhatti, Saeed Ur Rehman and Qureshi, Saad Ali and Basharat, Amna and Hussain, Shujaat and Khan, Kifayat Ullah},
  booktitle={2022 16th International Conference on Ubiquitous Information Management and Communication (IMCOM)}, 
  title={Resume Parsing Framework for E-recruitment}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Modern approaches to improve networking and communication have given ways to the advancement of recruitment process through the development of e-recruitment recommender systems. The increasing expansion of internet- based recruiting has resulted in a large number of resumes being stored in recruitment systems. Most resumes are prepared in a variety of styles to attract the attention of recruiters, including different font sizes, font colors, and table formats. However, data mining operations such as resume information extraction, automatic profile matching, and applicant ranking are immensely affected by the variety of formats. Rule-based methods, supervised methods and semantics-based methods have been introduced to extract facts from resume accurately, however, these methods heavily depend on large amounts of annotated data that is usually difficult to collect Furthermore, these techniques are time-intensive and bear knowledge incompleteness that strongly affect the accuracy of resume parser. In this paper, we present a resume parsing framework that handles the limitations faced in the previous techniques. At first, the raw text is extracted from resumes and blocks are separated using text block classification. Furthermore, the entities are extracted using named entity recognition and enriched using ontology. The proposed resume parser accurately extracts information from resumes that directly contributes towards the selection of best candidate.},
  keywords={Training;Resumes;Layout;Ontologies;Information retrieval;Feature extraction;Information management;Resume parsing;Data Enrichment;Text Extraction;Ontology;Boolean Naive Bayes},
  doi={10.1109/IMCOM53663.2022.9721762},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10187511,
  author={Weigand, Hans and Johannesson, Paul},
  booktitle={2023 IEEE 25th Conference on Business Informatics (CBI)}, 
  title={How to Identify your Design Science Research Artifact}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  abstract={Design Science Research (DSR) is about the development and investigation of artifacts in context. However, in many articles that subscribe to a DSR approach, the artifact is not clearly classified and identified. Very little attention has been given in the DSR literature on this topic, so guidelines are lacking. Based on artifact ontology, this paper proposes guidelines for design researchers in the IS domain on how to specify both the artifact and the research objective. For validation, the guidelines have been applied to a range of DSR papers. Our results show that the artifact definition guidelines can add to the precision of the research object specification.},
  keywords={Philosophical considerations;Design methodology;Bibliographies;Ontologies;Data science;Data models;Informatics;Design Science;artifact ontology;research methodology},
  doi={10.1109/CBI58679.2023.10187511},
  ISSN={2378-1971},
  month={June},}@INPROCEEDINGS{8104864,
  author={Yang, Chen-Wei and Vyatkin, Valeriy},
  booktitle={2017 IEEE 15th International Conference on Industrial Informatics (INDIN)}, 
  title={On requirements-driven design of distributed smart grid automation control}, 
  year={2017},
  volume={},
  number={},
  pages={738-745},
  abstract={This paper proposes a novel method of modelling requirements for smart grid automation systems that fits and complements the leading standards used for software development in smart grid: IEC 61850 and IEC 61499. Ontological representation is used to formally model the systems requirements and the integration of the requirement modelling to be used in conjunction with IEC 61850 specifications and IEC 61499 implementation. The proposed model is illustrated on a case study presented by the CIGRE working group where the system requirement in natural language was firstly modelled in ontology, then implemented in IEC 61499. Lastly, the resulting IEC 61499 control system was tested and validated against the system requirement by signal testing of the end control system.},
  keywords={IEC Standards;Ontologies;Unified modeling language;Smart grids;Natural languages;Automation;Timing;IEC 61850;IEC 61499;Smart Grid;Ontology;Requirements Modelling},
  doi={10.1109/INDIN.2017.8104864},
  ISSN={2378-363X},
  month={July},}@ARTICLE{9205800,
  author={Larhrib, Mohamed and Escribano, Miguel and Cerrada, Carlos and Escribano, Juan Jose},
  journal={IEEE Access}, 
  title={Converting OCL and CGMES Rules to SHACL in Smart Grids}, 
  year={2020},
  volume={8},
  number={},
  pages={177255-177266},
  abstract={Models are first-class elements in Model-Driven Engineering (MDE). In this paradigm, the most widespread approaches adopted by the development community are Object-Oriented and ontological, formalized using Unified Modeling Language (UML) and Resource Description Framework (RDF), respectively. However, Object Management Group (OMG) does not provide a specific standard language for validating UML models against Object Constraints Language (OCL) constraints; meanwhile, World Wide Web Consortium (W3C) has defined Shapes Constraint Language (SHACL) as a standard validation language. Although the transformation between UML and RDF can be performed at the structural level, no effort has been made to transform OCL to SHACL. This paper addresses the transformation of OCL and text-based constraints to SHACL shapes in the context of Common Grid Model Exchange Standard (CGMES), a UML-based standard for electric utilities in Europe. This paper presents several contributions to the software engineering community. First, solving the validation problem in a standardized way. Second, facilitating European Network of Transmission System Operators for Electricity (ENTSO-E) the construction of an ontology associated with the CGMES standard. Third, allowing developers to integrate the two complementary approaches. Finally, Promoting the adoption and integration of the ontological approach in the software community.},
  keywords={Unified modeling language;Object oriented modeling;Resource description framework;IEC Standards;Ontologies;Common Information Model (electricity);CIM for ENTSO-E (CGMES);OCL rules;ontology;RDF/RDFS;SHACL standard},
  doi={10.1109/ACCESS.2020.3026941},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10711329,
  author={Höfgen, Josua and Vogel-Heuser, Birgit and Vicaria, Alejandra and Pouzolz, François and Kurzhals, Christian},
  booktitle={2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)}, 
  title={Enhancing Model-Based System Architecting Through Formalized Decision Management}, 
  year={2024},
  volume={},
  number={},
  pages={1053-1060},
  abstract={System architecture decisions are typically informally captured in design documents. This practice leads to a loss of knowledge that impedes later activities like design changes, impact analysis, and reuse. Model-Based Systems Engineering (MBSE) frameworks support the development of increasingly complex systems but must address the problem of capitalization on architectural knowledge. To this end the "Decision Ontology for System Architectures (DOSA)" is developed to provide a formalized data model to capture system architecture decisions. DOSA is developed through a synthesis of decisions observed while developing an architecture model for a preliminary study of a novel satellite navigation system at Airbus Defence and Space. The approach is integrated into an MBSE framework enabling engineers to capture decisions that influence the architecture’s characteristics while developing the system model and imminently trace decision to artifacts of the system architecture. Subsequent visual inspection and formal querying of the decision graph facilitates the analysis of made decisions, and their interrelations.},
  keywords={Knowledge engineering;Visualization;Computer aided software engineering;Atmospheric modeling;Systems architecture;Ontologies;Inspection;Satellite navigation systems;Data models;Complex systems;MBSE;System Architecture;Decision Management;Ontology},
  doi={10.1109/CASE59546.2024.10711329},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{10205809,
  author={Nadhila, Fadiah and Alamsyah, Andry},
  booktitle={2023 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)}, 
  title={Mapping Personality Traits to Customer Complaints: Framework for Personalized Customer Service}, 
  year={2023},
  volume={},
  number={},
  pages={96-101},
  abstract={The study establishes utilizing the Big Five Personality framework and a Personality Measurement Platform (PMP) for personality analysis. Moreover, Customer Complaint Ontology (CCOntology) framework implements a Naive Bayes machine learning methodology to evaluate and scrutinize customer complaints. The algorithm works by calculating the probability of each complaint category. This association is measured in percentages, enabling the identification of specific personality traits related to customer complaints through identifying complaint characteristics and areas of concern. The study has found that individuals with neurotic personality traits who encounter customer complaints are often associated with problem categories such as Non-Contract, Privacy, and Contract and are more likely to express strong emotional dissatisfaction with a product or service. Linking customer complaints with their corresponding personalities can be an incredibly effective and innovative strategy for personalized customer service businesses in anticipating their needs and providing tailored recommendations that can improve the likelihood of customers making purchases. This approach involves educating employees on the importance of actively listening to customers, asking relevant questions, and anticipating their needs, ensuring that businesses can enhance customer satisfaction while building a loyal customer base.},
  keywords={Technological innovation;Privacy;Social networking (online);Customer services;Oral communication;Ontologies;Big Data;Big Five Personality;Customer Complaint Ontology (CCOntology);Naive Bayes;Personality Measurement Platform;Personalized Customer Service},
  doi={10.1109/IAICT59002.2023.10205809},
  ISSN={2834-8249},
  month={July},}@INPROCEEDINGS{9849830,
  author={Yang, Danyang and Wan, Fangjie and Zhang, Yonggan},
  booktitle={2022 4th International Conference on Advances in Computer Technology, Information Science and Communications (CTISC)}, 
  title={Named Entity Recognition in XLNet Cyberspace Security Domain Based on Dictionary Embedding}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={With the increase of network security incidents, network security analysts need to analyze massive log information. The introduction of knowledge graph into the field of network security can facilitate analysis by security analysts. NER (Named Entity Recognition) is the upstream task of knowledge graph construction, and the quality of the NER model determines the quality of the knowledge graph to a certain extent. However, the general domain named entity recognition model cannot extract the entities in the network security domain very well. For this phenomenon, this paper proposes an XLNet-Feature-Att model, which uses XLNet in the embedding layer to embed words into the vector space, the encoding layer uses the improved BILSTM FB structure and the Attention layer, and the decoding layer uses the CRF model to achieve sequence labeling and binding. Finally, the experimental comparison is carried out on the data set in the field of network security, and the F1-score reaches the highest 92.28%.},
  keywords={Knowledge engineering;Dictionaries;Computational modeling;Semantics;Cyberspace;Network security;Feature extraction;network security;named entity recognition;XLNet;BILSTM;CRF},
  doi={10.1109/CTISC54888.2022.9849830},
  ISSN={},
  month={April},}@ARTICLE{10843672,
  author={Feng, Wenyan and Li, Yuhang and Ma, Chunhao and Yu, Lisai},
  journal={IEEE Access}, 
  title={From ChatGPT to Sora: Analyzing Public Opinions and Attitudes on Generative Artificial Intelligence in Social Media}, 
  year={2025},
  volume={13},
  number={},
  pages={14485-14498},
  abstract={This study examines public opinions, emotional tendencies, and psychological linguistic characteristics associated with the launch of OpenAI’s ChatGPT and the advanced video generation model, Sora, by analyzing discussions on the Chinese social media platform Weibo. A total of 24,727 valid user-generated texts (1,762,296 words) were collected and analyzed using Python and its associated APIs. Word co-occurrence network analysis, topic modeling based on Latent Dirichlet Allocation (LDA), and emotional characteristics based on the DLUT Emotion Ontology and psycholinguistic analyses based on the Linguistic Inquiry and Word Count (LIWC) dictionary were employed to explore public views on these generative AI technologies. The findings reveal a shift in public focus over time, from initial excitement about technological advancements to growing interest in commercialization, labor, education, ethics, and global competition. The public’s emotional responses to AI were a mix of excitement and apprehension. The study identifies seven distinct emotional types, providing a nuanced understanding of public psychological reactions, which contrasts with previous binary classifications. This research contributes valuable insights for policymakers, businesses, and researchers, highlighting the public’s evolving acceptance of generative AI technologies.},
  keywords={Chatbots;Artificial intelligence;Generative AI;Social networking (online);Blogs;Psychology;Sentiment analysis;Analytical models;Text mining;Ethics;Generative artificial intelligence;ChatGPT;sora;topic modeling;sentiment analysis;psycholinguistics},
  doi={10.1109/ACCESS.2025.3530683},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10066652,
  author={Hattori, Shotaro and Fujii, Akihiro},
  booktitle={2023 IEEE 17th International Conference on Semantic Computing (ICSC)}, 
  title={Criminal Deduction Using Similarity Analysis Between Mystery Stories}, 
  year={2023},
  volume={},
  number={},
  pages={290-291},
  abstract={The " Knowledge Graph Reasoning Challenge" an annual event in which participants try to infer a mystery novel using Knowledge Graph and other tools, has been attracting a lot of attention. The purpose of this research is to propose and implement a new method using BERT for this challenge. To prove this, we focused on the degree of similarity between the two novels and estimated the culprit. As a result of the analysis focusing on the words and actions of the culprit, we succeeded in extracting the culprit's name. However, the successful extraction was based on a specific condition, and further discussion is needed to pursue more precise accuracy in the future.},
  keywords={Bit error rate;Semantics;Focusing;Knowledge graphs;Cognition;BERT;BERTscore;Natural Language Processing;Similarity},
  doi={10.1109/ICSC56153.2023.00057},
  ISSN={2325-6516},
  month={Feb},}@ARTICLE{7496983,
  author={Alvarez, María Luz and Sarachaga, Isabel and Burgos, Arantzazu and Estévez, Elisabet and Marcos, Marga},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={A Methodological Approach to Model-Driven Design and Development of Automation Systems}, 
  year={2018},
  volume={15},
  number={1},
  pages={67-79},
  abstract={The growing complexity of industrial automation demands the adoption of software engineering principles for improving the development process of control systems. This paper presents a methodological approach to the design and development of complex automation systems relying on model-driven engineering (MDE). A benefit of this approach is the integration of methods and techniques widespread within the automation discipline with modern MDE techniques guiding the designer through the development phases. A second advantage is to add flexibility enough to adapt the steps to the needs of the system under design. Finally, the architecture presented is prepared to be adapted to methodology extensions to cover other aspects of automation systems. The framework is based on domain models that are defined through the development phases using the terminology of the automation field. Using model transformations both documentation about system analysis and design and the skeleton of software units are automatically generated. A proof-of-concept tool has been developed that has been tested on the design of medium-complexity projects to assess the impact of its use with respect to project documentation and maintenance.Note to Practitioners—Control software development can be considered one of the challenges in automation field for achieving leadership in the future economic market. This work presents a model-driven engineering-based approach making use of both automation and software engineering methods and techniques for developing automation control systems. The framework implements the methodology for industrial automation systems ( ${\rm MeiA}_{\bullet }$ ) for guiding developers through the development phases and generates the analysis and design documentation using domain terminology, the design documentation that involves the minimal units of design, and the program organization units in one-to-one correspondence with the minimal units of design. From a practical point of view, it should be highly emphasized that developers of automation projects benefit from more structured designs, reduced number of errors, and improved project documentation.},
  keywords={Automation;Unified modeling language;Software engineering;Documentation;Software;Production;Control systems;Engineering frameworks;IEC 61131-3;industrial automation;methodology for industrial automation systems (  ${\mathrm{MeiA}}_{\bullet}$   );model-driven engineering (MDE);PLCopen XML},
  doi={10.1109/TASE.2016.2574644},
  ISSN={1558-3783},
  month={Jan},}@INPROCEEDINGS{9081794,
  author={Annighoefer, Bjoern and Halle, Martin and Schweiger, Andreas and Reich, Marina and Watkins, Christopher and VanderLeest, Steven H. and Harwarth, Stefan and Deiber, Patrick},
  booktitle={2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC)}, 
  title={Challenges and Ways Forward for Avionics Platforms and their Development in 2019}, 
  year={2019},
  volume={},
  number={},
  pages={1-10},
  abstract={Today's air vehicles depend on digital technology. It accounts for more than 30% of their development costs. The number of functions, the lines of code, the degree of autonomy, and the number of vehicles rise. This is why there is a need for cutting-edge technology and development methods. There is a gap between academia's methods and industrial applications due to multi-disciplinary challenges. We summarize the state-of-the-art in avionics, namely avionics platforms, requirements engineering, model-based development, automated verification, emerging technologies, and emerging demands. Experts review the most demanding challenges, research gaps, and promising solutions. They provide recommendations for the enhancement of the cooperation between industry and academia and suggest necessary research topics. This article is an introduction for those who are new to avionics. It is an up-to-date summary, for insiders looking for most promising solutions to their current problems; and it is a guide for those advancing avionics research.},
  keywords={avionics platforms;requirements engineering;model-based development;automated verification;multi-core},
  doi={10.1109/DASC43569.2019.9081794},
  ISSN={2155-7209},
  month={Sep.},}@INPROCEEDINGS{9140236,
  author={Torres, Victoria and Serral, Estefanía and Valderas, Pedro and Pelechano, Vicente and Grefen, Paul},
  booktitle={2020 IEEE 22nd Conference on Business Informatics (CBI)}, 
  title={Modeling of IoT devices in Business Processes: A Systematic Mapping Study}, 
  year={2020},
  volume={1},
  number={},
  pages={221-230},
  abstract={The Internet of Things (IoT) enables to connect the physical world to digital business processes (BP). By using the IoT, a BP can, e.g.: 1) take into account real-world data to take more informed business decisions, and 2) automate and/or improve BP tasks. To achieve these benefits, the integration of IoT and BPs needs to be successful. The first step to this end is to support the modeling of IoT-enhanced BPs. Although numerous researchers have studied this subject, it is unclear what is the current state of the art in terms of current modeling solutions and gaps. In this work, we carry out a Systematic Mapping Study (SMS) to find out how current solutions are modelling IoT into business processes. After studying 600 papers, we identified and analyzed in depth a total of 36 different solutions. In addition, we report on some important issues that should be addressed in the near future, such as, for instance the lack of standardization.},
  keywords={Conferences;Unified modeling language;Internet of Things;Systematics;Task analysis;Computers;Business process modeling;Internet of Things;IoT devices;IoT-enhanced BP;Systematic mapping study},
  doi={10.1109/CBI49978.2020.00031},
  ISSN={2378-1971},
  month={June},}@ARTICLE{9050669,
  author={Bandyszak, Torsten and Daun, Marian and Tenbergen, Bastian and Kuhs, Patrick and Wolf, Stefanie and Weyer, Thorsten},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Orthogonal Uncertainty Modeling in the Engineering of Cyber-Physical Systems}, 
  year={2020},
  volume={17},
  number={3},
  pages={1250-1265},
  abstract={Software-intensive cyber-physical systems (CPS) perform essential tasks such as controlling automated production processes in industrial production plants. The required levels of autonomy, openness, and self-adaptation, as well as the dynamic nature of the context of such CPS, result in challenging tasks for their engineering. During operation, unexpected situations in which the system has insufficient knowledge about the current state of the system itself as well as its context may occur. Engineering CPS, e.g., for industrial production sites, must account for such uncertainties the system will have to cope with during its lifetime in a structured and systematic way. Since the development of CPS requires consideration of different system perspectives, current uncertainty modeling approaches cannot be applied right away, as they do not explicitly consider uncertainty aspects that affect different artifacts. To aid the engineering of CPS, this article presents a model-based approach to document uncertainty. We propose “Orthogonal Uncertainty Models,” which closely integrate with other engineering artifacts from different perspectives, as a means for capturing a dedicated uncertainty viewpoint. Our approach has been evaluated in the industry automation domain. The application shows that the idea of regarding uncertainty within a dedicated perspective is highly beneficial. Particularly, our approach helps to uncover and document uncertainties related to behavioral, functional, and structural properties of a system, as well as uncertainties related to business models that would otherwise possibly remain covert. Note to Practitioners-Identifying and documenting uncertainties, which may occur during operation of a system, is a common problem in engineering processes. Such uncertainties may lead to severe damage, and thus need to be mitigated appropriately. It is crucial to account for these uncertainties during engineering, especially in the early phases. Depending on the specific project characteristics, a multitude of different diagram types are used to model a system. Uncertainties thus reflect in many artifacts, which leads to: 1) redundancies in the specified uncertainty attached to diagram elements and 2) uncertainty information (e.g., about the cause or effect of uncertainty) that is spread across different diagrams. The latter makes it difficult to structure uncertainty information and trace it throughout the engineering process so that uncertainty can be systematically considered. Our approach provides a graphical modeling language that employs a dedicated perspective on uncertainty in separate diagrams that can be linked to any engineering artifact.},
  keywords={Uncertainty;Robot sensing systems;Context modeling;Collaboration;Production;Runtime;Cyber-physical systems;industry automation case study;model-based engineering;orthogonal modeling;uncertainty;uncertainty modeling},
  doi={10.1109/TASE.2020.2980726},
  ISSN={1558-3783},
  month={July},}@INPROCEEDINGS{8501304,
  author={Shakeri Hossein Abad, Zahra and Gervasi, Vincenzo and Zowghi, Didar and Barker, Ken},
  booktitle={2018 5th International Workshop on Artificial Intelligence for Requirements Engineering (AIRE)}, 
  title={ELICA: An Automated Tool for Dynamic Extraction of Requirements Relevant Information}, 
  year={2018},
  volume={},
  number={},
  pages={8-14},
  abstract={Requirements elicitation requires extensive knowledge and deep understanding of the problem domain where the final system will be situated. However, in many software development projects, analysts are required to elicit the requirements from an unfamiliar domain, which often causes communication barriers between analysts and stakeholders. In this paper, we propose a requirements ELICitation Aid tool (ELICA) to help analysts better understand the target application domain by dynamic extraction and labeling of requirements-relevant knowledge. To extract the relevant terms, we leverage the flexibility and power of Weighted Finite State Transducers (WFSTs) in dynamic modeling of natural language processing tasks. In addition to the information conveyed through text, ELICA captures and processes non-linguistic information about the intention of speakers such as their confidence level, analytical tone, and emotions. The extracted information is made available to the analysts as a set of labeled snippets with highlighted relevant terms which can also be exported as an artifact of the Requirements Engineering (RE) process. The application and usefulness of ELICA are demonstrated through a case study. This study shows how pre-existing relevant information about the application domain and the information captured during an elicitation meeting, such as the conversation and stakeholders' intentions, can be captured and used to support analysts achieving their tasks.},
  keywords={Tools;Stakeholders;Task analysis;Interviews;Ontologies;Feature extraction;Data mining;Requirements elicitation, Natural language processing, Tool support, Dynamic information extraction},
  doi={10.1109/AIRE.2018.00007},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9338531,
  author={Fattouch, Najla and Ben Lahmar, Imen and Boukadi, Khouloud},
  booktitle={2020 IEEE 29th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE)}, 
  title={IoT-aware Business Process: comprehensive survey, discussion and challenges}, 
  year={2020},
  volume={},
  number={},
  pages={100-105},
  abstract={In the last years, the Internet of Things (IoT) know a huge widespread thanks to the increase of the connected objects number. The IoT technology has several benefits that make it among the proliferation technology. The major advantage of this technology is the communication between devices known as Machine-to-Machine (M2M) communication allowing them to be connected without human intervention. Thanks to this advantage, the technology become able to facilitate the people's lives that it become smoother through a seamless cooperation between virtual objects and physical ones. As well as, the IoT sweep various fields (e.g., industry, health) thanks to its capacity to automate tasks.In this setting, a tremendous number of business managers are interesting to integrate the IoT devices into their Business Processes (BPs), known in literature as IoT-aware BP. This integration gives the opportunity to the business managers to avail from the IoT technology in their process through an enhancement of the business performance and an achievement of the business competitiveness. Thus, several researchers competed to identify approaches and methods to integrate the IoT technology within the BP paradigm. In this paper, we present a review of the different proposed approaches that deal with the integration of the IoT technology within the BP. Furthermore, we give in this paper, a rich comparative analysis based on a set of criteria. Finally, we identify some initiatives and challenges in the IoT-aware BP paradigm.},
  keywords={Performance evaluation;Industries;Machine-to-machine communications;Internet of Things;Time factors;Task analysis;Business;IoT-aware Business Process;Internet of Things;Business Process;Industry 4.0},
  doi={10.1109/WETICE49692.2020.00027},
  ISSN={2641-8169},
  month={Sep.},}@ARTICLE{8540835,
  author={Gómez, Francisco J. and Aguilera Chaves, Miguel and Vanfretti, Luigi and Olsen, Svein Harald},
  journal={IEEE Access}, 
  title={Multi-Domain Semantic Information and Physical Behavior Modeling of Power Systems and Gas Turbines Expanding the Common Information Model}, 
  year={2018},
  volume={6},
  number={},
  pages={72663-72674},
  abstract={Due to the rapid increase of intermittent energy resources (IERs), there is a need to have dispatchable production available to ensure secure operation and increase opportunity for energy system flexibility. Gas turbine-based power plants offer flexible operation that is being improved with new technology advancements. Those plants provide, in general, quick start together with significant ramping capability, which can be exploited to balance IERs. Consequently, to understand the potential source of flexibility, better models for gas turbines are required for power system studies and analysis. In this paper, both the required semantic information and physical behavior models of such multi-domain systems are considered. First, UML class diagrams and RDF schemas based on the common information model standards are used to describe the semantic information of the electrical power grid. An extension that exploits the ISO 15926 standard is proposed herein to derive the multi-domain semantics required by integrated electrical power grid with detailed gas turbine dynamic models. Second, the Modelica language is employed to create the equation-based models, which represent the behavior of a multi-domain physical system. A comparative simulation analysis between the power system domain model and the multi-domain model has been performed. Some differences between the turbine dynamics representation of the commonly used GGOV1 standard model and a more detailed gas turbine model are shown.},
  keywords={Unified modeling language;Mathematical model;Power system dynamics;Semantics;Turbines;Standards;Common Information Model (electricity);CIM;cyber-physical systems;dynamic simulation;equation-based modeling;IEC 61970;information modeling;ISO 15926;Modelica;power systems simulation;power systems modeling},
  doi={10.1109/ACCESS.2018.2882311},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8015307,
  author={Walch, Michael},
  booktitle={2017 IEEE International Conference on Agents (ICA)}, 
  title={Knowledge-driven enrichment of cyber-physical systems for industrial applications using the KbR modelling approach}, 
  year={2017},
  volume={},
  number={},
  pages={84-89},
  abstract={This paper addresses design and engineering of models that connect cyber-physical systems and conceptualizations of industrial applications (IA). For this purpose, the hybrid modelling method SeRoIn is constructed using metamodel-based implementation (MMbI). Details are presented on specific building blocks of the modelling method, following the Knowledge-based Robotics (KbR) approach. Focusing on Knowledge-driven Enrichment (KdE) to integrate models, two experiments are conducted in the robotics section (OMiRob) of the Open Models Laboratory (OMiLAB), which involves concepts for hybrid automata, processes and ontologies. Thereby, intelligent behaviour is formalized, coining the notion of “smart models”. The experiments prove the capability of KdE and suggest to establish focused research in the area of KbR.},
  keywords={Tools;Computational modeling;Ontologies;Business;Service robots;Automata},
  doi={10.1109/AGENTS.2017.8015307},
  ISSN={},
  month={July},}@ARTICLE{9141288,
  author={Huang, Zhao and Zhao, Wei},
  journal={IEEE Access}, 
  title={Combination of ELMo Representation and CNN Approaches to Enhance Service Discovery}, 
  year={2020},
  volume={8},
  number={},
  pages={130782-130796},
  abstract={With the rapid growth of Web services, the demand for discovering the optimal services to satisfy the users' requirements is no longer an easy task. The critical issue in the process of service discovery is to conduct a similarity calculation. To solve such an issue, this study proposes an effective approach that combines the Embeddings from Language Models (ELMo) representation and Convolutional Neural Network (CNN) to obtain a more accurate similarity score for retrieving target Web services. More specifically, first, the study adopts the ELMo model to generate effective word representations for capturing the sufficient information from services and queries. Then, the word representations are used to compose a similarity matrix, which will be taken as the input for the CNN to learn the matching relationships. Finally, the combination of the ELMo representation and CNN is used to address the representation and interaction processes within the matching task to improve the service discovery performance. The results demonstrate the effectiveness of our proposed approach for retrieving better targeted Web services.},
  keywords={Semantics;Syntactics;Task analysis;Ontologies;Linguistics;Service-oriented architecture;Service discovery;ELMo;CNN;service similarity;web service},
  doi={10.1109/ACCESS.2020.3009393},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8285628,
  author={Bowles, J. and Caminati, M. B. and Cha, S.},
  booktitle={2017 International Symposium on Theoretical Aspects of Software Engineering (TASE)}, 
  title={An integrated framework for verifying multiple care pathways}, 
  year={2017},
  volume={},
  number={},
  pages={1-8},
  abstract={Common chronic conditions are routinely treated following standardised procedures known as clinical pathways. For patients suffering from two or more chronic conditions, referred to as multimorbidities, several pathways have to be applied simultaneously. However, since pathways rarely consider the presence of comorbidities, applying several pathways may lead to potentially harmful (medication) conflicts. This paper proposes an automated framework to detect, highlight and resolve conflicts in the treatments used for patients with multimorbidites. We use BPMN as a modelling language for capturing care guidelines. A BPMN model is transformed into an intermediate formal model capturing the possible unfoldings of the pathway. Putting together the constraint solver Z3 and the theorem prover Isabelle, we combine treatment plans and check the correctness of the approach. We illustrate the approach with an example from the medical domain and discuss future work.},
  keywords={Logic gates;Task analysis;Ontologies;Unified modeling language;Semantics;Guidelines;Computational modeling},
  doi={10.1109/TASE.2017.8285628},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9688274,
  author={Kim, Seokhwan and Liu, Yang and Jin, Di and Papangelis, Alexandros and Gopalakrishnan, Karthik and Hedayatnia, Behnam and Hakkani-Tür, Dilek},
  booktitle={2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)}, 
  title={“How Robust R U?”: Evaluating Task-Oriented Dialogue Systems on Spoken Conversations}, 
  year={2021},
  volume={},
  number={},
  pages={1147-1154},
  abstract={Most prior work in dialogue modeling has been on written conversations mostly because of existing data sets. However, written dialogues are not sufficient to fully capture the nature of spoken conversations as well as the potential speech recognition errors in practical spoken dialogue systems. This work presents a new benchmark on spoken task-oriented conversations, which is intended to study multi-domain dialogue state tracking and knowledge-grounded dialogue modeling. We report that the existing state-of-the-art models trained on written conversations are not performing well on our spoken data, as expected. Furthermore, we observe improvements in task performances when leveraging $n$-best speech recognition hypotheses such as by combining predictions based on individual hypotheses. Our data set enables speech-based benchmarking of task-oriented dialogue systems.},
  keywords={Conferences;Benchmark testing;Data models;Robustness;Task analysis;Automatic speech recognition;spoken dialogue systems;dialogue state tracking;knowledge-grounded dialogue generation},
  doi={10.1109/ASRU51503.2021.9688274},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8588250,
  author={Papazoglou, Michael P.},
  booktitle={2018 Sixth International Conference on Enterprise Systems (ES)}, 
  title={Metaprogramming Environment for Industry 4.0}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={Industry 4.0 is blurring the lines between the physical, and digital spheres of global production systems. Industry 4.0 sets the foundations for a completely connected factories that are characterized by the digitization and interconnection of supply chains, production equipment and production lines, and the application of the latest advanced digital information technologies to manufacturing activities. To fully realize the promise of Industry 4.0, disparate manufacturing systems, devices, data and processes need to connect, communicate, and interoperate. This paper has dual purpose. It first introduces a model-based engineering that enables a concurrent, collaborative design process where users examine and define requirements, propose solution architectures, demonstrate and exchange ideas with stakeholders, and consider product feature tradeoffs. Subsequently, it proposes a novel programming paradigm and a flexible environment that helps developers develop design-to-production industrial automation solutions by employing structured higher-level modular software techniques.},
  keywords={Manufacturing;Industries;Solid modeling;Supply chains;Three-dimensional displays;Real-time systems;Industry 4.0;Enterprise Knowledge Engineering;Information integration and interoperability;Model-based development;Digital twins;Meta-programming},
  doi={10.1109/ES.2018.00008},
  ISSN={2572-6609},
  month={Oct},}@ARTICLE{9866769,
  author={Silega, Nemury and Noguera, Manuel and Rogozov, Yuri I. and Lapshin, Vyacheslav S. and González, Tahumara},
  journal={IEEE Access}, 
  title={Transformation From CIM to PIM: a Systematic Mapping}, 
  year={2022},
  volume={10},
  number={},
  pages={90857-90872},
  abstract={Model Driven Architecture (MDA) is the most prominent and accepted methodology based on the Model Driven Development (MDD) principles. MDA includes three abstraction levels: Computer Independent Models (CIM), Platform Independent models (PIM) and Platform specific models (PSM). MDA encourages the automatic transformation of models as a means to increase the speed of the software development process and to prevent human errors. There are plenty of solutions to transform PIMs to PSMs, however the CIM to PIM transformation does not receive a similar attention. In that sense, this paper aims to describe a systematic mapping to analyze the main characteristics of the approaches that deal with the CIM to PIM transformation as well as to discuss research directions stemming out from our analysis. The results of this mapping study could be a valuable information source for the scientific community in order to know the real advances in this topic and to avoid unnecessary effort dealing with problems that have already been addressed. For example, this study yielded the models at the CIM level that have already been transformed into models at the PIM level. Hence, with this information, the researchers could focus their attention on finding solutions to transform those models at CIM level that have not been transformed into models at PIM level. Likewise, this mapping study provides information regarding the technological support of this type of transformation. This information could be useful for those software projects interested to adopt MDA.},
  keywords={Computational modeling;Software;Systematics;Business;Software engineering;Mathematical models;Internet;Model driven architecture (MDA);computer independent models (CIM);platform independent models (PIM);systematic mapping},
  doi={10.1109/ACCESS.2022.3201556},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9447088,
  author={Giachetti, Ronald E. and Vaneman, Warren},
  booktitle={2021 IEEE International Systems Conference (SysCon)}, 
  title={Requirements for a System Model in the Context of Digital Engineering}, 
  year={2021},
  volume={},
  number={},
  pages={1-7},
  abstract={The vision of achieving digital engineering in the US Department of Defense has instigated work on defining the information content and structure of the system model. However, few seem to have asked what are the requirements for the system model? In this paper, we use a requirements process to elicit and define the requirements for the system model. The system model is a digital artifact containing descriptions of all the essential objects, their properties, and the relationships between them for the system-of-interest (SoI). The paper describes the context of the system model in relationships to the other components of model-based systems engineering (MBSE) consisting of a modeling language, schema, model-based process, presentation framework, MBSE tools, and knowledgeable workforce. The paper describes how these components interact to provide effective MBSE. Requirements are stated for each component. The paper additionally derives information requirements for the system model according to the systems engineering process’s information needs by examining the inputs and outputs of each activity in the systems engineering process. Lastly, the paper derives the quality characteristics for the system model from the literature on ontologies, modeling languages, and semiotics. The result is a set of requirements for the system model to support MBSE and the digital thread.},
  keywords={Context;Conferences;Project management;Tools;Ontologies;IEEE Standards;Data models},
  doi={10.1109/SysCon48628.2021.9447088},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{10275523,
  author={Dhouib, Saadia and Huang, Yining and Smaoui, Asma and Bhanja, Tapanta and Gezer, Volkan},
  booktitle={2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Papyrus4Manufacturing: A Model-Based Systems Engineering approach to AAS Digital Twins}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={As digital twins gain momentum in their usage in diverse domains, the concept of Asset Administration Shells (AAS) has become very relevant for achieving the digital twin approach, where Administration Shells are the digital representation of physical assets. Being a relatively new concept in the Industrial Internet of Things (IIoT) domain, the tools and approaches for creating and deploying AASs are likewise in infancy. This paper introduces an open-source tool, Papyrus4Manufacturing, which provides a model-based systems engineering approach to the AAS. This toolset supports the creation of AAS digital twins from modeling to automatic deployment and connection to assets using the OPC UA protocol. This paper also includes an evaluation of its usability, as it is put to test with an academic use case.},
  keywords={Protocols;Databases;Memory;Software;Digital twins;Servers;Modeling;Digital Twins;Asset Administration Shell;Model-Based System Engineering;Unified Modelling Language;UML Profiles;Generative Software Engineering;OPC UA;BaSyx;Eclipse Papyrus},
  doi={10.1109/ETFA54631.2023.10275523},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{8612870,
  author={Bouzidi, Aljia and Haddar, Nahla and Ben Abdallah, Mounira and Haddar, Kais},
  booktitle={2018 IEEE/ACS 15th International Conference on Computer Systems and Applications (AICCSA)}, 
  title={Alignment of Business Processes and Requirements Through Model Integration}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={Business process and requirement specification are crucial phases in the software development process. Yet, business and requirement modeling are often carried out separately by different languages and design teams, leading to misaligned models. The degree of misalignment grows continuously with their independent evolution. Thus, the potential of model-driven software development cannot be fully exploited. There is a considerable agreement among researchers about the integration technique roles to bridge the gap between heterogeneous models. Therefore, this approach is based on this technique to align the business world represented by BPMN and the software requirement world represented by the UML use case. We define an integrated meta-model that incorporates all BPMN and use case elements as well as new others to map traceable elements. Further, we define a new diagram that provides a means to visualize and combines their use within an integrated model. Our approach supplements CASE tools with additional information and relationships to maintain the global system consistency. To illustrate it, we implement an editor for designing the proposed diagram, and we apply it in a topical case study.},
  keywords={Unified modeling language;Business;Ontologies;Computational modeling;Software;Semantics;Bridges;meta-model integration;Alignment;business process models;requirements models;BPMN;UML use case},
  doi={10.1109/AICCSA.2018.8612870},
  ISSN={2161-5330},
  month={Oct},}@INPROCEEDINGS{8482078,
  author={Ali, Abbas Raza},
  booktitle={2018 IEEE 17th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)}, 
  title={Cognitive Computing to Optimize IT Services}, 
  year={2018},
  volume={},
  number={},
  pages={54-60},
  abstract={In this paper, the challenges of maintaining a healthy IT operational environment have been addressed by proactively analyzing IT Service Desk tickets, customer satisfaction surveys and social media data. A Cognitive solution goes beyond the traditional structured data analysis solutions by deep analyses of both structured and unstructured text. The salient features of the proposed platform include language identification, translation, hierarchical extraction of the most frequently occurring topics, entities and their relationships, text summarization, sentiments and knowledge extraction from the unstructured text using Natural Language Processing techniques. Moreover, the insights from unstructured text combined with structured data allows the development of various classification, segmentation and time-series forecasting use-cases on incident, problem and change datasets. The text and predictive insights together with raw data are used for visualization and exploration of actionable insights on a rich and interactive dashboard. However, it is hard not only to find these insights using traditional Analytics solutions but it might also take very long time to discover them, especially while dealing with massive amount of unstructured data. By taking actions on these insights, organizations can benefit from significant reduction of ticket volume, reduced operational costs and increased customer satisfaction. In various experiments, on average, up to 18-25 % of yearly ticket volume has been reduced using the proposed approach.},
  keywords={Feature extraction;Cognitive systems;Customer satisfaction;Data visualization;Semantics;Servers;Social network services;Knowledge Extraction;Optimizing IT Services;Cognitive Computing;Topic Clustering;Semantic Text Analytics;Service Desk},
  doi={10.1109/ICCI-CC.2018.8482078},
  ISSN={},
  month={July},}@ARTICLE{10418085,
  author={Sewunetie, Walelign Tewabe and Kovács, László},
  journal={IEEE Access}, 
  title={Exploring Sentence Parsing: OpenAI API-Based and Hybrid Parser-Based Approaches}, 
  year={2024},
  volume={12},
  number={},
  pages={38801-38815},
  abstract={This study focuses on the fundamental process of parsing sentences to create semantic graphs from textual documents. It introduces novel techniques for parsing phrases within semantic graph-based induction, employing both ChatGPT-based and Hybrid parser-based approaches. Through a thorough analysis, the study evaluates the performance of these methods in generating semantic networks from text, particularly in capturing detailed event descriptions and relationships. Results indicate a slight advantage in accuracy for the Hybrid parser-based approach (87%) compared to ChatGPT (85%) in sentence parsing tasks. Furthermore, efficiency analysis reveals that ChatGPT’s response quality varies with prompt sizes, while the Hybrid parser-based method consistently maintains excellent response quality.},
  keywords={Semantics;Chatbots;Adaptation models;Knowledge graphs;Task analysis;Context modeling;Training;Natural language processing;Predictive models;Application of sentence parsing;adverb prediction;ChatGPT;hybrid parser;natural language processing;sentence parsing;semantic graph},
  doi={10.1109/ACCESS.2024.3360480},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9497473,
  author={Wu, Shouxuan and Lu, Jinzhi and Hu, Zhenchao and Yang, Pengfei and Wang, Guoxin and Kiritsis, Dimitris},
  booktitle={2021 16th International Conference of System of Systems Engineering (SoSE)}, 
  title={Cognitive Thread Supports System of Systems for Complex System Development}, 
  year={2021},
  volume={},
  number={},
  pages={82-87},
  abstract={Model-based Systems Engineering (MBSE) has been widely used in the development of complex systems. The system architectures, organizations, research and development processes using MBSE to design complex systems can be seen as a System of Systems (SoS), which has high complexity and hard to manage. The concept of digital thread is proposed to integrate all the models and data in the SoS. However, lack of cognition ability makes it hard to connect the models and data with human, processes and things in the SoS, which reduces the efficiency of complex system development. In this paper, a new concept named Cognitive Thread is first proposed as digital thread with augmented semantic capabilities for identifying the information of the SoS. Then a cognitive thread construction approach based on Open Services for Lifecycle Collaboration (OSLC) specification and knowledge graphs is proposed to support decision-making and management in the SoS. Finally, the feasibility of the proposed approach is verified through a case study of the advanced driver-assistance system development.},
  keywords={Instruction sets;Semantics;Systems architecture;Organizations;Cognition;Data models;Natural language processing;Digital Thread;Cognitive Thread;OSLC specification;System of Systems;Model-based Systems Engineering},
  doi={10.1109/SOSE52739.2021.9497473},
  ISSN={},
  month={June},}@INPROCEEDINGS{10350365,
  author={Ali, Syed Juned and Gavric, Aleksandar and Proper, Henderik and Bork, Dominik},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Encoding Conceptual Models for Machine Learning: A Systematic Review}, 
  year={2023},
  volume={},
  number={},
  pages={562-570},
  abstract={Conceptual models are essential in Software and Information Systems Engineering to meet many purposes since they explicitly represent the subject domains. Machine Learning (ML) approaches have recently been used in conceptual modeling to realize, among others, intelligent modeling assistance, model transformation, and metamodel classification. These works en-code models in various ways, making the encoded models suitable for applying ML algorithms. The encodings capture the models' structure and/or semantics, making this information available to the ML model during training. Therefore, the choice of the encoding for any ML-driven task is crucial for the ML model to learn the relevant contextual information. In this paper, we report findings from a systematic literature review which yields insights into the current research in machine learning for conceptual modeling (ML4CM). The review focuses on the various encodings used in existing ML4CM solutions and provides insights into i) which are the information sources, ii) how is the conceptual model's structure and/or semantics encoded, iii) why is the model encoded, i.e., for which conceptual modeling task and, iv) which ML algorithms are applied. The results aim to structure the state of the art in encoding conceptual models for ML.},
  keywords={Training;Analytical models;Systematics;Machine learning algorithms;Bibliographies;Semantics;Machine learning;Machine learning;Model-driven engineering;Model Encoding;Systematic Literature Review},
  doi={10.1109/MODELS-C59198.2023.00094},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9256709,
  author={Batista, Leandro and Monsuez, Bruno},
  booktitle={2020 AIAA/IEEE 39th Digital Avionics Systems Conference (DASC)}, 
  title={The conception of a large-scale Systems Engineering environment}, 
  year={2020},
  volume={},
  number={},
  pages={1-7},
  abstract={With the rise of artificial intelligence, it is time to shape the systems engineering tooling environment for the future. In the last decade, we have seen several emerging technologies that will potentially have a great impact in complex systems. These new technologies are expected to cause a disruptive impact not only in the products but also in to the tools used across the whole product life cycle. For this reason, is imperative to perform a critical review of the current systems engineering tooling ecosystem. This assessment should also map the open research problems that could prevent the complete integration of the new technologies into the systems engineering framework. This paper proposes a new architecture for a system engineering environment to operate in large scale projects. The objective of this research is twofold: it will first identify the capabilities for the next generation platform, and secondly, it will evaluate how artificial intelligence applications can be integrated in compliance with DO-330. The concept developed by this research will drive tool design recommendations enabling the use of artificial intelligence driven applications in a systems engineering tooling ecosystem.},
  keywords={Tools;Systems engineering and theory;Modeling;Standards;Ecosystems;Complex systems;Vocabulary;Systems Engineering;MBSE;Artificial Intelligence;DO-330},
  doi={10.1109/DASC50938.2020.9256709},
  ISSN={2155-7209},
  month={Oct},}@INPROCEEDINGS{10402219,
  author={Liem, Truc Nguyen and Cao Hoai, Sinh Nguyen and Quoc, Hung Nguyen and Van, Tien Nguyen and Pham Trung, Hieu and Quoc, Trung Nguyen and Hoang, Vinh Truong},
  booktitle={2023 IEEE 15th International Conference on Computational Intelligence and Communication Networks (CICN)}, 
  title={GradTOD - A Unified Dialogue State Tracking Model for Task-Oriented and Open Domain Dialogues}, 
  year={2023},
  volume={},
  number={},
  pages={711-719},
  abstract={The task-oriented dialogue domain system requires classifying intent and replying to a specific goal domain. In the sub-module of Task-oriented, the Dialogue State Tracker (DST) is well-known as a variety processing tracker. However, existing DST models often specialize in only task-oriented domains (ToD), leading to limited performance when applied to scenarios. In this paper, we propose GradTOD, a unified DST model that predicts both two task types, task-oriented dialogue (TOD) and open-domain dialogue (ODD). Our model leverages the recent advances in prompt engineering and conditional generation to perform zero-shot learning. After experiments, GradTOD has achieved an 88.6% and 82.5% score on Joint Goal Accuracy metrics when evaluating the Scheme-Guided Dialogue (SGD) and FusedChat test sets correspondingly, demonstrating the adaption ability for multi-domains.},
  keywords={Measurement;Adaptation models;Zero-shot learning;Computational modeling;Predictive models;Task analysis;Computational intelligence;component;formatting;style;styling;insert},
  doi={10.1109/CICN59264.2023.10402219},
  ISSN={2472-7555},
  month={Dec},}@ARTICLE{10320368,
  author={Quevedo, Ernesto and Cerny, Tomas and Rodriguez, Alejandro and Rivas, Pablo and Yero, Jorge and Sooksatra, Korn and Zhakubayev, Alibek and Taibi, Davide},
  journal={IEEE Access}, 
  title={Legal Natural Language Processing From 2015 to 2022: A Comprehensive Systematic Mapping Study of Advances and Applications}, 
  year={2024},
  volume={12},
  number={},
  pages={145286-145317},
  abstract={The surge in legal text production has amplified the workload for legal professionals, making many tasks repetitive and time-consuming. Furthermore, the complexity and specialized language of legal documents pose challenges not just for those in the legal domain but also for the general public. This emphasizes the potential role and impact of Legal Natural Language Processing (Legal NLP). Although advancements have been made in this domain, particularly after 2015 with the advent of Deep Learning and Large Language Models (LLMs), a systematic exploration of this progress until 2022 is nonexistent. In this research, we perform a Systematic Mapping Study (SMS) to bridge this gap. We aim to provide a descriptive statistical analysis of the Legal NLP research between 2015 and 2022. Categorize and sub-categorize primary publications based on their research problems. Identify limitations and areas of improvement in current research. Using a robust search methodology across four reputable indexers, we filtered 536 papers down to 75 pivotal articles. Our findings reveal the diverse methods employed for tasks such as Multiclass Classification, Summarization, and Question Answering in the Legal NLP field. We also highlight resources, challenges, and gaps in current methodologies and emphasize the need for curated datasets, ontologies, and a focus on inherent difficulties like data accessibility. As the legal sector gradually embraces Natural Language Processing (NLP), understanding the capabilities and limitations of Legal NLP becomes vital for ensuring efficient and ethical application. The research offers insights for both Legal NLP researchers and the broader legal community, advocating for continued advancements in automation while also addressing ethical concerns.},
  keywords={Law;Natural language processing;Task analysis;Systematics;Information retrieval;Surveys;Search problems;Deep learning;Systematic-mapping-study;legal-NLP;deep learning},
  doi={10.1109/ACCESS.2023.3333946},
  ISSN={2169-3536},
  month={},}@ARTICLE{10123130,
  author={Zelina, Petr and Halámková, Jana and Nováček, Vít},
  journal={IEEE Transactions on NanoBioscience}, 
  title={Extraction, Labeling, Clustering, and Semantic Mapping of Segments From Clinical Notes}, 
  year={2023},
  volume={22},
  number={4},
  pages={781-788},
  abstract={This work is motivated by the scarcity of tools for accurate, unsupervised information extraction from unstructured clinical notes in computationally underrepresented languages, such as Czech. We introduce a stepping stone to a broad array of downstream tasks such as summarisation or integration of individual patient records, extraction of structured information for national cancer registry reporting or building of semi-structured semantic patient representations that can be used for computing patient embeddings. More specifically, we present a method for unsupervised extraction of semantically-labeled textual segments from clinical notes and test it out on a dataset of Czech breast cancer patients, provided by Masaryk Memorial Cancer Institute (the largest Czech hospital specialising exclusively in oncology). Our goal was to extract, classify (i.e. label) and cluster segments of the free-text notes that correspond to specific clinical features (e.g., family background, comorbidities or toxicities). Finally, we propose a tool for computer-assisted semantic mapping of segment types to pre-defined ontologies and validate it on a downstream task of category-specific patient similarity. The presented results demonstrate the practical relevance of the proposed approach for building more sophisticated extraction and analytical pipelines deployed on Czech clinical notes.},
  keywords={Task analysis;Semantics;Feature extraction;Ontologies;Nanobioscience;Measurement;Clinical diagnosis;Text categorization;Information retrieval;NLP;EHR;clinical notes;information extraction;text classification},
  doi={10.1109/TNB.2023.3275195},
  ISSN={1558-2639},
  month={Oct},}@INPROCEEDINGS{10595652,
  author={Arrotta, Luca and Bettini, Claudio and Civitarese, Gabriele and Fiori, Michele},
  booktitle={2024 IEEE International Conference on Smart Computing (SMARTCOMP)}, 
  title={ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models}, 
  year={2024},
  volume={},
  number={},
  pages={55-62},
  abstract={Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human activities. In this work, we propose ContextGPT: a novel prompt engineering approach to retrieve from LLMs common-sense knowledge about the relationship between human activities and the context in which they are performed. Unlike ontologies, ContextGPT requires limited human effort and expertise, while sharing similar privacy concerns if the reasoning is performed in the cloud. An extensive evaluation using two public datasets shows how a NeSy model obtained by infusing common-sense knowledge from ContextGPT is effective in data scarcity scenarios, leading to similar (and sometimes better) recognition rates than logic-based approaches with a fraction of the effort.},
  keywords={Knowledge engineering;Deep learning;Training;Privacy;Computational modeling;Ontologies;Human activity recognition;human activity recognition;context-awareness;large language models},
  doi={10.1109/SMARTCOMP61445.2024.00029},
  ISSN={2693-8340},
  month={June},}@INPROCEEDINGS{10350790,
  author={Majumder, Mainak},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={A Domain-Driven Model Generation Framework for Cyber-Physical Production Systems}, 
  year={2023},
  volume={},
  number={},
  pages={172-178},
  abstract={The growing influence of Information Technologies in the manufacturing domain has led to the fourth industrial revolution (Industry 4.0). Cyber-Physical Production System (CPPS) is one of the fundamental concepts of Industry 4.0 that aims to develop an intelligent manufacturing environment by leveraging concepts like the Internet of Things (IoT), cloud computing, virtualization, and Artificial Intelligence (AI). However, the challenges originating from the technological heterogeneity in the manufacturing domain remain primary obstacles towards realising a fully automated CPPS. Among them, semantic heterogeneity in manufacturing information is the most crucial which can be attributed to technology and vendor-specific information modelling mechanisms. A CPPS requires seamless machine-to-machine communication which could be hindered due to the non-interoperability among machine data on a semantic level. Therefore, the primary focus of this thesis work is to understand the semantic interoperability challenges of CPPS and propose solutions to address those challenges. The proposed solution revolves around the development of semantic domain models using the modelling philosophies of Domain-Driven Design (DDD).},
  keywords={Production systems;Machine-to-machine communications;Philosophical considerations;Semantics;Model driven engineering;Fourth Industrial Revolution;Internet of Things;CPPS;Domain-Driven Design (DDD);Information Model;Industry 4.0},
  doi={10.1109/MODELS-C59198.2023.00044},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8010730,
  author={Kaczmarek-Heß, Monika},
  booktitle={2017 IEEE 19th Conference on Business Informatics (CBI)}, 
  title={A Multilevel Model of Events in Support of Enterprise Agility in the Realm of Enterprise Modeling}, 
  year={2017},
  volume={01},
  number={},
  pages={267-276},
  abstract={One of the challenges faced by enterprises is a need to be agile, i.e., having the ability to detect changes and respond to them efficiently and effectively. These changes are caused by `change drivers', i.e., events occurring within an enterprise itself or within its environment. Many scholars argue that the use of conceptual modeling in general, and enterprise modeling in particular, may help achieve and sustain enterprise agility. This, however, requires enterprise modeling approaches not only to provide an integrated view on the enterprise action system and information system, but also to account for events indicating business-impacting situations and information on their impact on the enterprise. In addition, being agile requires the ability to acquire information from event sources (sensing), to process obtained information, and to support a decision-making process (response). In this paper, we argue that the fulfillment of those postulates demands the application of an alternative language paradigm, namely multilevel modeling. Therefore, we contribute a multilevel model of events developed using an integrated modeling and programming approach.},
  keywords={Ontologies;Information systems;Organizations;Analytical models;Sensors;Programming;enterprise modeling;multilevel modeling;FMMLx;modeling of events},
  doi={10.1109/CBI.2017.58},
  ISSN={2378-1971},
  month={July},}@ARTICLE{9556591,
  author={Marschall, Benedikt and Ochsenkuehn, Daniel and Voigt, Tobias},
  journal={IEEE Journal of Emerging and Selected Topics in Industrial Electronics}, 
  title={Design and Implementation of a Smart, Product-Led Production Control Using Industrial Agents}, 
  year={2022},
  volume={3},
  number={1},
  pages={48-56},
  abstract={In theory, the design of modern production systems in the form of a cyber–physical production system (CPPS) allows more flexibility, simple expandability, quick adaptability, and intelligent production control by the product. Multiagent systems (MASs) are thereby recommended as control solution because of their autonomy and dynamic decentralized architecture. Although their potential use and technical excellence have been proven, the costs of implementation and maintenance still outweigh their supposed advantages. This results in low acceptance and usage of operational MAS in the industry. This article describes topics that need to be considered when designing and implementing an MAS as production control for a CPPS in the context of customized mass production. Generally developed approaches are presented, which were implemented in a commercially developed agent framework and validated on the basis of an industrial use case for a product-led filling process in lot size one. All implemented concepts aim to be reusable in comparable applications across industries. In combination with the MAS-internal testing approach also presented, this should contribute to faster, more cost-effective implementation of reliable MAS solutions and ultimately increase their technical maturity.},
  keywords={Production systems;Production control;XML;Radiofrequency identification;Costs;Companies;Testing;Industrial agents (IAs);cyber–physical product ion system (CPPS);multiagent system (MAS)},
  doi={10.1109/JESTIE.2021.3117121},
  ISSN={2687-9743},
  month={Jan},}@INPROCEEDINGS{10386763,
  author={Jesus, Vitor and Patel, Asma and Kumar, Deepak},
  booktitle={2023 10th International Conference on Behavioural and Social Computing (BESC)}, 
  title={Feasibility of Structured, Machine-Readable Privacy Notices}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper offers a novel approach to the long standing problem of the interface of humans and online privacy notices. As literature and practice, and even art, for more than a decade have identified, privacy notices are nearly always ignored and "accepted" with little thought, mostly because it is not practical nor user-friendly to depend on reading a long text simply to access, e.g., a news website. Nevertheless, privacy notices are a central element, often mandated by law.We approach the problem by (partially) relieving the human from the task of inspecting such documents. Because they are documents written in natural language, often legal language, we assess the feasibility of representing privacy notices in a machine-readable format. Should this be feasible, automated processing of notices that still respect individual choices could be enabled. To this end, we manually inspected privacy notices under EU/UK's GDPR from common websites, and designed a JSON schema that captures their structure.},
  keywords={Privacy;Social computing;Art;Law;Natural languages;Task analysis},
  doi={10.1109/BESC59560.2023.10386763},
  ISSN={},
  month={Oct},}
@INPROCEEDINGS{9970920,
  author={Cahyaningsih, Elin and Silalahi, Natascha Lestari Eunike and Rohajawati, Siti and Avianti, Yuliza Maulina},
  booktitle={2022 International Conference on Information Technology Systems and Innovation (ICITSI)}, 
  title={COMMONKADS for Knowledge Based System Development: A Literature Study}, 
  year={2022},
  volume={},
  number={},
  pages={213-218},
  abstract={COMMONKADS is a method for developing knowledge-based system. This method describes foundation, technique, modeling language and document structure for develop the knowledge-based system. COMMONKADS is people-oriented system development methodology, and this methodology is often used for developing organizational knowledge management system. COMMONKADS approach is divided based on context (organizational model, task model, agent model), concept (knowledge model) and artifact (design model). COMMONKADS have been used widely for knowledge-based system in several fields, such as COMMONKADS that integrated in tourism knowledge-based system, COMMONKADS for irrigation expert system, expertise model using COMMONKADS in manufactured company, COMMONKADS in energy management system and many more. Generally, there are eight strengths of COMMONKADS methodology for develop knowledge-based system. Its strength is flexible to use in any scope, represent knowledge (organizational, domain, task and inference knowledge), complete (representation, model, and form), powerful, accurate, comprehensive, represent KM process, systematic and effective. While the weakness of COMMONKADS methodology only three, there are don't have validation process and difficult to acquisition knowledge and use semi formal language, large data storage. Nevertheless, COMMONKADS is recommended methodology for develop knowledge-based system.},
  keywords={Technological innovation;Systematics;Knowledge based systems;Government;Memory;Software;Regulation;COMMONKADS;strength of COMMONKADS;knowledge-based system;weakness of COMMONKADS;methodology;knowledge-based engineering},
  doi={10.1109/ICITSI56531.2022.9970920},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{8453933,
  author={Gand, Kai},
  booktitle={2018 IEEE 20th Conference on Business Informatics (CBI)}, 
  title={Towards Conceptual Enhancements of the Business Model Canvas: The Case of Health Information Technology}, 
  year={2018},
  volume={02},
  number={},
  pages={62-71},
  abstract={Business models (BM) describe mechanisms of value creation, delivery and capture. Business Model Representation (BMRs) in terms of conceptual models systematically and formally visualize BMs. The field of BMRs is characterized by conceptual inconsistencies such as heterogeneous notations and insufficiently described semantics. Describing and constructing structured and comparable BMs capturing concrete business cases is hampered. Even the success of the most prominent BMR approach - the business model canvas (BMC) - could only lead to slight advances. So, we aim to mature research on BMRs by enhancing the BMC with further conceptual modeling's concepts. We will focus on Health Information Technology (HIT) initiatives that are prone to fail. Analyses showed that the domain's networked nature of value creation requires to link and interrelate the BM's building blocks. Current BMR approaches, including the BMC, can only hardly provide such. For addressing BMC's conceptual weaknesses, we propose to substantiate the BMC by means of a layered concept making use of conceptual modeling's principles and considering the domain's specifics to a greater extent. Systematic and HITspecific BM(R) approaches are worthwhile as these identify and map all relevant stakeholders as well as their interrelations for value creation. That is a primary prerequisite for sustainable HIT solutions. So, we aim at bringing necessary complex information into a structured concept that allows to make profound managerial decisions. In turn, this provides more profound guidance for the implementation of HIT solutions that may lead to a higher success rates of such.},
  keywords={Organizations;Visualization;Stakeholders;Semantics;Information systems;Information technology;Business Models;Business Model Representations;Conceptual Modeling;Health Information Technology},
  doi={10.1109/CBI.2018.10047},
  ISSN={2378-1971},
  month={July},}@INPROCEEDINGS{10131078,
  author={Chen, Rui and Wang, Guoxin and Wu, Shouxuan and Lu, Jinzhi and Yan, Yan},
  booktitle={2023 IEEE International Systems Conference (SysCon)}, 
  title={A Service-oriented Approach Supporting Model Integration in Model-based Systems Engineering}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={When using Model-Based Systems Engineering (MBSE) to develop complex systems, models using different syntax and semantics are typically implemented in a heterogeneous environment which leads to difficulties to realize data integrations across the entire lifecycle. Specifically, seamless exchanges between models of different modeling tools are needed to support system lifecycle activities such as requirement analysis, function analysis, verification and validation. This article illustrates a service-oriented approach to support model integration for model-based systems engineering, especially between architecture design and system verification. First, a set of semantic mapping rules between architecture models and simulation models based on Open Service of Lifecycle Collaboration (OSLC) are proposed to support the formalization of technical resources (models, data, APIs). Then OSLC adapters are developed to transform models, data and APIs into web-based services. The services are deployed by a service discovering plug-in within a specific modeling tool for model information exchange. The approach is illustrated by a case study on KARMA architecture model and Modelica simulation model for a six-degree-of-freedom robot (RobotR3) system. We evaluate the availability and efficiency of this method from both qualitative and quantitative perspectives. The results show that our approach is effective in model and data integration.},
  keywords={Adaptation models;Analytical models;System verification;Semantics;Data integration;Transforms;Syntactics;Model integration;Model-Based Systems Engineering;Open Service for Lifecycle Collaboration;Modelica},
  doi={10.1109/SysCon53073.2023.10131078},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{10708354,
  author={Kawther, Dridi and Wahiba, Ben Abdessalem Karaa},
  booktitle={2024 10th International Conference on Control, Decision and Information Technologies (CoDIT)}, 
  title={Comparative Analysis of Multilingual Text Classification Techniques: A Review of Current Approaches and Emerging}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The widespread availability of electronic documents and the exponential growth of the World Wide Web have made the automatic categorization of documents a critical method for organizing information and facilitating knowledge discovery, and information retrieval. This paper aims to examine the key techniques and methodologies utilized in multilingual document classification, while also bringing attention to some of the complex challenges that still need to be addressed. In particular, the paper presents a thorough review of the literature concerning the theory and methods of multilingual document representation and classification.},
  keywords={Dimensionality reduction;Reviews;Text categorization;Process control;Ontologies;Knowledge discovery;Vectors;Web sites;Information technology;Indexing},
  doi={10.1109/CoDIT62066.2024.10708354},
  ISSN={2576-3555},
  month={July},}@INPROCEEDINGS{8595070,
  author={Tueno Fotso, Steve Jeffrey and Frappier, Marc and Laleau, Régine and Mammar, Amel},
  booktitle={2018 23rd International Conference on Engineering of Complex Computer Systems (ICECCS)}, 
  title={Back Propagating B System Updates on SysML/KAOS Domain Models}, 
  year={2018},
  volume={},
  number={},
  pages={160-169},
  abstract={Nowadays, the usefulness of the formal verification and validation of system specifications is well established, at least for critical systems. However, one of the main obstacles to their adoption lies in obtaining the formal specification of the system, and, in the case of refinement-based formal methods such as B System or Event-B, in obtaining the most abstract specification that heads the development of the system. The SysML/KAOS requirements engineering method is proposed to overcome this difficulty. It includes a goal modeling language to model requirements from stakeholders needs. Translation rules from a goal model to a B System specification have already been defined. They allow to obtain a skeleton of the system specification. To complete it, a language has been defined to express the domain model associated to the goal model. Its translation gives the structural part of the B System specification. However, it very often appears that new elements must be added in the B System specification obtained from SysML/KAOS models, discovered for instance when specifying the body of events and/or by using formal validation and/or verification tools. We have therefore defined a set of rules allowing the back propagation, within domain models, of every newly added element. This paper describes these rules and how they are specified in Event-B. Their consistency is proved using the Rodin tool. We show that they are structure preserving: two related elements within the B System specification remain related within the domain model. This is done by proving various isomorphisms between the B System specification and the domain models.},
  keywords={Optimized production technology;Backpropagation;Actuators;Boilers;Sensor phenomena and characterization;Requirements engineering;Requirements Engineering;Domain Modeling;SysML/KAOS;Event-B;B System},
  doi={10.1109/ICECCS2018.2018.00025},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10621570,
  author={Motevallian, Mahsa and Esfar-E-Alam, AM and Taherkordi, Amir and Abbasi, Golnoush},
  booktitle={2024 20th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT)}, 
  title={Semantic Modeling of Waste Dataflow for Automating Circular Economy Systems}, 
  year={2024},
  volume={},
  number={},
  pages={677-684},
  abstract={Circular Economy (CE) is a model with a concrete action plan covering the whole life cycle of a product, from production and consumption to waste management (WM). Information technologies considerably contribute to the transition towards CE, e.g., waste tracking using Internet of Things (IoT). This will cause the businesses and organizations to confront a large diversity of data (i.e. waste amount, types, locations, etc.). The generated data is often stored and processed through manual or semi-manual methods by each business or organization. However, an automated method which can also interpret and integrate the diverse data in WM fields across different organizations is still in its infancy. Often, such data is not organized and falls short of reaching its full potential in facilitating coordinated management and enabling Circular Economy initiatives. In this paper, we aim to address this need through automated interpretation and integration of municipal waste data by applying semantic data modeling. Our approach proposes to capture the semantical description of entities in the WM process and their relations, which can appear between waste producers, authorities and consumers. Then, the obtained semantic model will facilitate and automate the required interpretation and integration of waste data, both for intra- and inter-organization scenarios. We realize intelligent semantic-based searching using natural language processing and large language models.},
  keywords={Waste management;Computational modeling;Large language models;Semantics;Organizations;Production;Manuals;Circular Economy;Waste Data Modeling;Semantic Data;Neural Search;NLP;Large Language Models},
  doi={10.1109/DCOSS-IoT61029.2024.00105},
  ISSN={2325-2944},
  month={April},}@INPROCEEDINGS{10298592,
  author={Li, Linyu and Xu, Sihan and Liu, Yang and Gao, Ya and Cai, Xiangrui and Wu, Jiarun and Song, Wenli and Liu, Zheli},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={LiSum: Open Source Software License Summarization with Multi-Task Learning}, 
  year={2023},
  volume={},
  number={},
  pages={787-799},
  abstract={Open source software (OSS) licenses regulate the conditions under which users can reuse, modify, and distribute the software legally. However, there exist various OSS licenses in the community, written in a formal language, which are typically long and complicated to understand. In this paper, we conducted a 661-participants online survey to investigate the perspectives and practices of developers towards OSS licenses. The user study revealed an indeed need for an automated tool to facilitate license understanding. Motivated by the user study and the fast growth of licenses in the community, we propose the first study towards automated license summarization. Specifically, we released the first high quality text summarization dataset and designed two tasks, i.e., license text summarization (LTS), aiming at generating a relatively short summary for an arbitrary license, and license term classification (LTC), focusing on the attitude inference towards a predefined set of key license terms (e.g., Distribute). Aiming at the two tasks, we present LiSum, a multi-task learning method to help developers overcome the obstacles of understanding OSS licenses. Comprehensive experiments demonstrated that the proposed jointly training objective boosted the performance on both tasks, surpassing state-of-the-art baselines with gains of at least 5 points w.r.t. F1 scores of four summarization metrics and achieving 95.13% micro average F1 score for classification simultaneously. We released all the datasets, the replication package, and the questionnaires for the community.},
  keywords={Training;Measurement;Learning systems;Surveys;Formal languages;Licenses;Multitasking;Open Source Software Licenses;Multi-Task Learning;License comprehension},
  doi={10.1109/ASE56229.2023.00150},
  ISSN={2643-1572},
  month={Sep.},}@INPROCEEDINGS{10427395,
  author={Hendrik, Hendrik and Permanasari, Adhistya Erna and Fauziati, Silmi and Kusumawardani, Sri Suning},
  booktitle={2023 8th International Conference on Information Technology and Digital Applications (ICITDA)}, 
  title={Judging Knowledge by its Cover: Leveraging Large Language Models in Establishing Criteria for Knowledge Graph Sources Selection}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={The Knowledge Graph (KG) is a widely used paradigm for knowledge representation that leverages graph structures. It facilitates the discovery of relationships and provides context across various domains. Despite its popularity, there's a notable lack of emphasis on quality control during the KG construction process, particularly concerning the selection of knowledge sources. Our paper addresses this research gap by introducing criteria for selecting high-quality websites during the data acquisition phase of KG construction. We propose a set of criteria—credibility, relevance, content quality, coverage, comprehensiveness, and accessibility—that serves as a benchmark for evaluating potential online knowledge sources for KGs. We determine the weight of each criterion through expert judgment and validation using Large Language Models (LLMs) representing AI technologies. Our panel, consisting of academicians and practitioners, ranks the criteria by importance, with the Rank Ordered Centroid and Rank Reciprocal weighting methods generating weights from these rankings. We employ four LLMs: Google Bard, OpenAI ChatGPT 4, Anthropic Claude-2, and Meta Llama-2. Our findings suggest that LLMs can effectively validate human weighting results, as evidenced by a high correlation (Pearson's $r > 0.9$) between human and LLM criterion weights. Furthermore, hypothesis testing reveals no significant differences ($p > 0.8$), with the models demonstrating high internal consistency. Our study offers valuable insights for academics and industry professionals seeking to improve KG construction processes and establish standards for data quality and verification.},
  keywords={Correlation;Process control;Knowledge graphs;Quality control;Chatbots;Reliability;Standards;knowledge graphs construction;knowledge source selection;large language models;website evaluation criteria;criteria development approach},
  doi={10.1109/ICITDA60835.2023.10427395},
  ISSN={},
  month={Nov},}@ARTICLE{10227516,
  author={Xie, Qianqian and Tiwari, Prayag and Ananiadou, Sophia},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Knowledge-Enhanced Graph Topic Transformer for Explainable Biomedical Text Summarization}, 
  year={2024},
  volume={28},
  number={4},
  pages={1836-1847},
  abstract={Given the overwhelming and rapidly increasing volumes of the published biomedical literature, automatic biomedical text summarization has long been a highly important task. Recently, great advances in the performance of biomedical text summarization have been facilitated by pre-trained language models (PLMs) based on fine-tuning. However, existing summarization methods based on PLMs do not capture domain-specific knowledge. This can result in generated summaries with low coherence, including redundant sentences, or excluding important domain knowledge conveyed in the full-text document. Furthermore, the black-box nature of the transformers means that they lack explainability, i.e. it is not clear to users how and why the summary was generated. The domain-specific knowledge and explainability are crucial for the accuracy and transparency of biomedical text summarization methods. In this article, we aim to address these issues by proposing a novel domain knowledge-enhanced graph topic transformer (DORIS) for explainable biomedical text summarization. The model integrates the graph neural topic model and the domain-specific knowledge from the Unified Medical Language System (UMLS) into the transformer-based PLM, to improve the explainability and accuracy. Experimental results on four biomedical literature datasets show that our model outperforms existing state-of-the-art (SOTA) PLM-based summarization methods on biomedical extractive summarization. Furthermore, our use of graph neural topic modeling means that our model possesses the desirable property of being explainable, i.e. it is straightforward for users to understand how and why the model selects particular sentences for inclusion in the summary. The domain-specific knowledge helps our model to learn more coherent topics, to better explain the performance.},
  keywords={Biological system modeling;Semantics;Unified modeling language;Correlation;Task analysis;Transformers;Knowledge based systems;Biomedical text summarization;domain knowledge;graph neural topic model;explainability;pre-trained language models},
  doi={10.1109/JBHI.2023.3308064},
  ISSN={2168-2208},
  month={April},}@ARTICLE{10375098,
  author={Yang, Xu and Deng, Cheng and Wei, Kun and Tao, Dacheng},
  journal={IEEE Transactions on Cybernetics}, 
  title={Robust Commonsense Reasoning Against Noisy Labels Using Adaptive Correction}, 
  year={2024},
  volume={54},
  number={7},
  pages={4138-4149},
  abstract={Commonsense reasoning based on knowledge graphs (KGs) is a challenging task that requires predicting complex questions over the described textual contexts and relevant knowledge about the world. However, current methods typically assume clean training scenarios with accurately labeled samples, which are often unrealistic. The training set can include mislabeled samples, and the robustness to label noises is essential for commonsense reasoning methods to be practical, but this problem remains largely unexplored. This work focuses on commonsense reasoning with mislabeled training samples and makes several technical contributions: 1) we first construct diverse augmentations from knowledge and model, and offer a simple yet effective multiple-choice alignment method to divide the training samples into clean, semi-clean, and unclean parts; 2) we design adaptive label correction methods for the semi-clean and unclean samples to exploit the supervised potential of noisy information; and 3) finally, we extensively test these methods on noisy versions of commonsense reasoning benchmarks (CommonsenseQA and OpenbookQA). Experimental results show that the proposed method can significantly enhance robustness and improve overall performance. Furthermore, the proposed method is generally applicable to multiple existing commonsense reasoning frameworks to boost their robustness. The code is available at https://github.com/xdxuyang/CR_Noisy_Labels.},
  keywords={Noise measurement;Commonsense reasoning;Training;Robustness;Adaptation models;Context modeling;Knowledge graphs;Commonsense reasoning;knowledge graph (KG);noisy labels},
  doi={10.1109/TCYB.2023.3339629},
  ISSN={2168-2275},
  month={July},}@ARTICLE{10179264,
  author={Wang, Guan and Li, Weihua and Bai, Quan and Lai, Edmund M-K},
  journal={IEEE Transactions on Emerging Topics in Computing}, 
  title={Maximizing Social Influence With Minimum Information Alteration}, 
  year={2024},
  volume={12},
  number={2},
  pages={419-431},
  abstract={With the rapid advancement of the Internet and social platforms, how to maximize the influence across popular online social networks has attracted great attention from both researchers and practitioners. Almost all the existing influence diffusion models assume that influence remains constant in the process of information spreading. However, in the real world, people tend to alternate information by attaching opinions or modifying the contents before spreading it. Namely, the meaning and idea of a message normally mutate in the process of influence diffusion. In this article, we investigate how to maximize the influence in online social platforms with a key consideration of suppressing the information alteration in the diffusion cascading process. We leverage deep learning models and knowledge graphs to present users’ personalised behaviours, i.e., actions after receiving a message. Furthermore, we investigate the information alteration in the process of influence diffusion. A novel seed selection algorithm is proposed to maximize the social influence without causing significant information alteration. Experimental results explicitly show the rationale of the proposed user behaviours deep learning model architecture and demonstrate the novel seeding algorithm's outstanding performance in both maximizing influence and retaining the influence originality.},
  keywords={Integrated circuit modeling;Social networking (online);Deep learning;Biological system modeling;Mathematical models;Diffusion processes;Knowledge graphs;Information alteration;information diffusion;influence maximization;knowledge graph;natural language processing},
  doi={10.1109/TETC.2023.3292384},
  ISSN={2168-6750},
  month={April},}@ARTICLE{9822405,
  author={Harrando, Ismail and Reboud, Alison and Schleider, Thomas and Ehrhart, Thibault and Troncy, Raphael},
  journal={IEEE Internet Computing}, 
  title={ProZe: Explainable and Prompt-Guided Zero-Shot Text Classification}, 
  year={2022},
  volume={26},
  number={6},
  pages={69-77},
  abstract={As technology accelerates the generation and communication of textual data, the need to automatically understand this content becomes a necessity. In order to classify text, being it for tagging, indexing, or curating documents, one often relies on large, opaque models that are trained on preannotated datasets, making the process unexplainable, difficult to scale, and ill-adapted for niche domains with scarce data. To tackle these challenges, we propose ProZe, a text classification approach that leverages knowledge from two sources: prompting pretrained language models, as well as querying ConceptNet, a common-sense knowledge base which can be used to add a layer of explainability to the results. We evaluate our approach empirically and we show how this combination not only performs on par with state-of-the-art zero shot classification on several domains, but also offers explainable predictions that can be visualized.},
  keywords={Task analysis;Predictive models;Internet;Computational modeling;Semantics;Adaptation models;Transformers;Text classification;Knowledge graphs;Text classification;zero-shot;explainability;common sense knowledge graph;prompting language models},
  doi={10.1109/MIC.2022.3187080},
  ISSN={1941-0131},
  month={Nov},}@INPROCEEDINGS{10650108,
  author={Wang, Chaoguo and Zhang, Liang and Yan, Wei},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={A Combination of LERT and CNN-BILSTM Models for Chinese Music Named Entity Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Named Entity Recognition (NER) is an important task in the field of natural language processing(NLP), serving as the foundation for knowledge graphs, recommendation systems, machine translation, and other applications. However, progress in NER for the music domain has been relatively slow. This is mainly due to the diverse and irregular nature of music entities, as well as the labor-intensive process of entity annotation, resulting in a lack of publicly available datasets, therefore, this paper organizes music texts on the internet through techniques such as data mining and NLP, constructing a NER dataset tailored to the field of music. Pre-trained models have been widely adopted in various fields, including NLP and computer vision, as they can learn rich language knowledge and semantic representations. To address the limitations of existing NER methods in capturing contextual information and handling word sense disambiguation, this paper proposes a deep learning model called LERT-CNN-BILSTM-CRF for entity recognition in the Chinese music domain. Experimental results demonstrate that our proposed method achieves good performance, with precision reaching 93.87%, recall reaching 95.06%, and an F1 score of 94.46%. Compared to previous machine learning-based methods for music NER, our proposed approach better captures the semantic information of complex music named entities, thereby improving the performance and generalization ability of the model.},
  keywords={Deep learning;Training;Recurrent neural networks;Computational modeling;Semantics;Named entity recognition;Transformers;Named entity recognition;Deep learning;Linguistically-motivated bidirectional Encoder Representation from Transformer;Long Short-Term Memory;Knowledge Graph},
  doi={10.1109/IJCNN60899.2024.10650108},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10488152,
  author={Ren, Yuan and Li, Xutong and Liu, Xudong and Zhang, Richong},
  booktitle={2024 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Few-Shot KBQA Method Based on Multi-Task Learning}, 
  year={2024},
  volume={},
  number={},
  pages={226-233},
  abstract={Question-answering systems have become a prominent topic in the field of artificial intelligence. A crucial aspect is knowledge-based question answering (KBQA), used in search engines and intelligent customer service to enhance user experiences. However, existing methods often struggle to model complex relationships and operations in few-shot learning environments. To solve this problem, a multi-task KBQA method has been proposed. This method includes various auxiliary tasks such as relational sequence prediction, knowledge completion prediction, and query program reconstruction. A multi-task fusion training approach was adopted for model generation. Experimental results show that accuracy can be significantly improved by more than 6% in few-shot learning environments, achieving better performance with an accuracy rate of 92.45%.},
  keywords={Training;Customer services;Knowledge based systems;Search engines;Big Data;Multitasking;Question answering (information retrieval);knowledge graph;question-answering system;semantic parsing;few-shot learning;multi-task learning},
  doi={10.1109/BigComp60711.2024.00043},
  ISSN={2375-9356},
  month={Feb},}@INPROCEEDINGS{10650892,
  author={Xiao, Mengxi and Liu, Ben and Peng, Miao and Xu, Wenjie and Peng, Min},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Clarified Aggregation and Predictive Modeling (CAPM): High-Interpretability Framework for Inductive Link Prediction}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={In inductive link prediction for evolving knowledge graphs (KGs), interpretability is crucial yet often overlooked in relational message aggregation methods. Previous approaches typically neglect deeper relational insights, limiting their explanatory power. In this paper, we propose CAPM (Constrained Aggregation and Predictive Modeling) to address this critical gap by uniquely incorporating semantic descriptions of entities. This integration allows for a clearer understanding of how and why certain links are predicted and strengthens its ability to contextualize and clarify the relationships within the KG. In particular, CAPM combines entity type and an attention mechanism during aggregation, ensuring a sophisticated blend of structured and semantic information. This method also improves relevance assessment in relational paths, leveraging prior knowledge in adjacent relations. Tested on sparse KGs, CAPM demonstrates exceptional performance in inductive link prediction scenarios. Ablation studies confirm its superiority, particularly when combined with embedding-based methods for entity-type representation, highlighting its effectiveness in evolving KGs. Through this innovative approach, CAPM offers a comprehensive solution that balances the dynamic nature of KGs with the essential need for interpretability in link prediction.},
  keywords={Attention mechanisms;Limiting;Semantics;Neural networks;Knowledge graphs;Predictive models;knowledge graph completion;inductive link prediction;message aggregation;model interpretability},
  doi={10.1109/IJCNN60899.2024.10650892},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{9750529,
  author={Zhou, Ling and Luan, Tianjiao and Yao, Na and Li, Jianming and Ding, Jie},
  booktitle={2021 IEEE Sixth International Conference on Data Science in Cyberspace (DSC)}, 
  title={BDCP: An Improved Nested Named Entity Recognition Model Based on LSTM}, 
  year={2021},
  volume={},
  number={},
  pages={346-352},
  abstract={In construction of knowledge graphs (KGs), named entity recognition (NER) is a sub-task to identify the boundaries of entities with special meaning and predict their categories in texts. The instance that an entity contains one or more entities is called nested entity. The current work on NER usually ignores the nested NER. We propose an improved model named Boundary Detection and Category Prediction (BDCP) for Nested NER. Our model uses LSTM to extract the contextual features of the sequence, and uses boundary detection unit to mark the boundaries of entities in the text. By introducing boundary detection unit, our model extracts the boundaries of entities and restrict the number of candidate entities. We also design a layer-by-layer decoding module by boundary detection unit for Nested NER. Experiments on Nested NER datasets named GENIA [1] demonstrate the effectiveness of our model on nested NER.},
  keywords={Knowledge engineering;Text recognition;Conferences;Cyberspace;Predictive models;Data science;Feature extraction;knowledge graph;neural network;nested entity;nested named entity recognition},
  doi={10.1109/DSC53577.2021.00055},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9657857,
  author={He, Bingjun and Chen, Jianfeng},
  booktitle={2021 IEEE 21st International Conference on Communication Technology (ICCT)}, 
  title={Named Entity Recognition Method in Network Security Domain Based on BERT-BiLSTM-CRF}, 
  year={2021},
  volume={},
  number={},
  pages={508-512},
  abstract={With the increase of the number of network threats, the knowledge graph is an effective method to quickly analyze the network threats from the mass of network security texts. Named entity recognition in network security domain is an important task to construct knowledge graph. Aiming at the problem that key Chinese entity information in network security related text is difficult to identify, a named entity recognition model in network security domain based on BERT-BiLSTM-CRF is proposed to identify key named entities in network security related text. This model adopts the BERT pre-training model to obtain the word vectors of the preceding and subsequent text information, and the obtained word vectors will be input to the subsequent BiLSTM module and CRF module for encoding and sorting. The test results show that this model has a good effect on the data set of network security domain. The recognition effect of this model is better than that of LSTM-CRF, BERT-LSTM-CRF, BERT-CRF and other models, and the F1=93.81%.},
  keywords={Knowledge engineering;Text recognition;Conferences;Network security;Encoding;Data models;Communications technology;cybersecurity;named entity recognition;BERT;BiLSTM;CRF},
  doi={10.1109/ICCT52962.2021.9657857},
  ISSN={2576-7828},
  month={Oct},}@INPROCEEDINGS{10763589,
  author={Zhu, Zhui and Qi, Guangpeng and Shang, Guangyong and He, Qingfeng and Zhang, Weichen and Li, Ningbo and Chen, Yunzhi and Hu, Lijun and Zhang, Wenqiang and Dang, Fan},
  booktitle={2024 IEEE 30th International Conference on Parallel and Distributed Systems (ICPADS)}, 
  title={Enhancing Large Language Models with Knowledge Graphs for Robust Question Answering}, 
  year={2024},
  volume={},
  number={},
  pages={262-269},
  abstract={In recent years, large language models (LLMs) have shown rapid development, becoming one of the most popular topics in the field of artificial intelligence. LLMs have demonstrated powerful generalization and learning capabilities, and their performance on various language tasks has been remarkable. Despite their successes, LLMs face significant challenges, particularly in domain-specific tasks that require structured knowledge, often leading to issues such as hallucinations. To mitigate these challenges, we propose a novel system, SynaptiQA, which integrates LLMs with Knowledge Graphs (KGs) to answer more questions about knowledge. Our approach leverages the generative capabilities of LLMs to create and optimize KG queries, thereby improving the accuracy and contextual relevance of responses. Experimental results in an industrial data set demonstrate that SynaptiQA outperforms baseline models and naive retrieval-augmented generation (RAG) systems, demonstrating improved accuracy and reduced hallucinations. This integration of KGs with LLMs paves the way for more reliable and interpretable domain-specific question answering systems.},
  keywords={Accuracy;Large language models;Distributed databases;Knowledge graphs;Question answering (information retrieval);Vectors;Cognition;Data models;Reliability;Faces;Artificial Intelligence;Knowledge Graph;Large Language Model},
  doi={10.1109/ICPADS63350.2024.00042},
  ISSN={2690-5965},
  month={Oct},}@INPROCEEDINGS{10575550,
  author={Li, Yiyuan},
  booktitle={2024 IEEE 6th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)}, 
  title={Knowledge Base Question Answering based on Machine Reading Comprehension}, 
  year={2024},
  volume={6},
  number={},
  pages={1616-1620},
  abstract={Currently, society has entered the era of "Internet +," where technology brings more convenience to people. Knowledge Graph (KG) is a structured information system of world knowledge and serves as the cornerstone for artificial intelligence reasoning and memory. In recent years, with the improvement of deep models and computing power, Knowledge Graph technology has rapidly developed, providing foundational support for applications such as question answering, recommendation systems, and search engines. Knowledge Based Question Answering (KBQA) utilizes knowledge bases (KB) to answer questions and finds wide application in fields like search, finance, and healthcare. Due to the gap in representation between unstructured natural language questions and structured Knowledge Graphs (KGs), answering questions on knowledge graphs presents certain challenges. Existing methods based on semantic parsing struggle to construct structured queries that can be executed on knowledge bases, thereby failing to cover various complex questions. Conversely, methods based on information retrieval exhibit poor interpretability. This paper proposes a novel approach driven by machine reading comprehension. To transform KG subgraphs centered around subject entities into text, this paper employs schema trees to depict the subgraphs, which facilitates retrieving multiple semantically equivalent answer entities. The KG2Text method is utilized to generate corresponding text from the constructed schema trees. In contrast to seeking answers from all automatically generated paragraphs, this paper employs a contrastive learning approach to filter out paragraphs that may contain answers. Finally, answers are obtained based on answer fragments detected by the machine reading comprehension module. Experimental results demonstrate that this approach significantly outperforms existing methods.},
  keywords={Knowledge based systems;Semantics;Knowledge graphs;Transforms;Medical services;Search engines;Cognition;KG2Text model;Knowledge Based Question Answering;Machine Reading Comprehension;contrastive learning},
  doi={10.1109/IMCEC59810.2024.10575550},
  ISSN={2693-2776},
  month={May},}@INPROCEEDINGS{10711734,
  author={Wen, Sijie and Chen, Yongming and Pan, Xinyu and Zhuang, Weibin and Li, Xinyu},
  booktitle={2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)}, 
  title={Enhancing Fault Troubleshooting through Human-Machine Collaboration: A Multi-Stage Reasoning Approach}, 
  year={2024},
  volume={},
  number={},
  pages={460-467},
  abstract={Ensuring the stable operation of critical industrial equipment is pivotal for maintaining production efficiency and economic gains. The complexity of modern industrial machinery, however, places a substantial cognitive load on maintenance personnel. To alleviate this, a Diagnostic Semantic-Enhanced Fault Causality Knowledge Graph (DSFCKG) is proposed to formalize fault information for computational analysis. Additionally, a Large Language Model (LLM)-based Knowledge Graph Construction (KGC) method is introduced for the automated assembly of DSFCKG. Building upon this, a multi-stage reasoning approach is designed for human-machine collaborative fault Troubleshooting. Experiments on real-world fault tickets demonstrate that our proposed method significantly enhances fault diagnosis and troubleshooting accuracy, especially in complex scenarios with long fault causal chains, which bring insights into futuristic smart maintenance.},
  keywords={Fault diagnosis;Accuracy;Human-machine systems;Semantics;Collaboration;Cause effect analysis;Knowledge graphs;Production facilities;Maintenance;Standards},
  doi={10.1109/CASE59546.2024.10711734},
  ISSN={2161-8089},
  month={Aug},}@ARTICLE{10848141,
  author={Zhu, Lilu and Su, Xiaolu and Tang, Jiaxuan and Hu, Yanfeng and Wang, Yang},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={View-Based Knowledge-Augmented Multimodal Semantic Understanding for Optical Remote Sensing Images}, 
  year={2025},
  volume={63},
  number={},
  pages={1-33},
  abstract={Optical remote sensing (RS) images serve as a pivotal source of geographic information. Due to the continuous development of deep learning technology, the evolving demands for multisource optical RS of the public shifted from recognition and acquisition of explicit features to comprehension and application of the fine-grained semantics and relationships implied in images. To address this challenge, we propose a semantic-augmented approach integrated multiview knowledge graph for a comprehensive understanding of optical RS images (RSMVKF). The RSMVKF delves into the structured representations of external knowledge from different human-like cognitive views and further explores the discovery ability of high-level features on the basis of multiple modalities and granularities. Specifically, the RSMVKF consists of two stages. First, we guide a large language model (LLM) to condense relevant knowledge from lengthy external knowledge passages and generate a view-level knowledge graph (RS-VKG). Then, an asymmetric multimodal contrastive network model (RS-M2CL) is designed to investigate efficient semantic augmentation. In this way, two types of contrastive loss functions, cross-modal and cross-granularity, are adopted to improve the understanding of implicit knowledge. The experimental results demonstrate that the RSMVKF greatly improves several perception tasks and reasoning tasks with rich features in optical RS imagery. In particular, in perception tasks such as fine-grained object detection and k-nearest neighbor (KNN) retrieval, the RSMVKF yields enhancements of 6.7% and 8.1%, respectively. In addition, in knowledge-driven reasoning tasks such as RS image captioning (RSCP), RS visual grounding (RSVG), and RS visual question answering (RSVQA), the RSMVKF demonstrates superior performance with margins of 8.9%, 5.3%, and 11.4%, respectively.},
  keywords={Urban areas;Optical sensors;Optical imaging;Lakes;Semantics;Bridges;Roads;Rivers;Satellites;Buildings;Fine-grained semantics and relationships;multimodal contrastive network;multiview knowledge graph;optical remote sensing (RS) images;semantic-augmented approach},
  doi={10.1109/TGRS.2025.3532349},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{8003819,
  author={Suri, Kunal and Gaaloul, Walid and Cuccuru, Arnaud and Gerard, Sebastien},
  booktitle={2017 IEEE 26th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE)}, 
  title={Semantic Framework for Internet of Things-Aware Business Process Development}, 
  year={2017},
  volume={},
  number={},
  pages={214-219},
  abstract={The proliferation of connected devices, wherein Sensors, Actuators and Tags (such as Radio-Frequency Identification (RFID)) are able to seamlessly communicate to their environment and to each other for sharing information or to perform some actions has created the Internet of Things (IoT) ecosystem. These devices expose their functionality via standard services and application programming interfaces (APIs). They are considered to be one of the key technology enablers to foster the vision of a smart world, comprising of smart objects, smart supply chain management, smart manufacturing (Industry 4.0), smart buildings, to name a few. In fact, today these IoT devices continuously take part in various business processes that are being executing within the boundaries of the same enterprise or in different enterprises. Thus, there is an evident need to model these processes that are associated with IoT resources in a formal and unambiguous manner. However, in context of business processes, these is a lack of formalized and explicit description for IoT resources, thus hampering their efficient modeling and management. To bridge this gap, we propose a semantic framework for developing IoT-aware business processes as follows, (i) formalizing IoT resource description w.r.t Internet of Things Architecture (IoT-A) reference model in context of business processes, (ii) formalizing IoT properties and allocation rules for optimal resource management and (iii) resolving resource conflicts based on strategies. To illustrate the feasibility of our framework, we evaluated our semantic model for coverage of concepts in IoT-A reference model along with development of a proof of concept tool for integrating the IoT resources and our semantic model during the process modeling phase.},
  keywords={Semantics;Temperature measurement;Temperature sensors;Radiofrequency identification;Resource management;Internet of Things (IoT);Business Process Management;Resource Management;Semantics;Ontology;Process Modeling},
  doi={10.1109/WETICE.2017.54},
  ISSN={},
  month={June},}@INPROCEEDINGS{8374390,
  author={Bendib, Issam and Laouar, Mohammed Ridda},
  booktitle={2018 2nd International Conference on Natural Language and Speech Processing (ICNLSP)}, 
  title={A semantic indexing approach of multimedia documents content based partial transcription}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={The complexity of searching and indexing spoken document retrieval depends closely on content structure and access strategies. However, Spoken Document Retrieval (SDR) approaches based on the transcription of content by LVCSR systems must treat the impact of recognition errors on the performance of the information retrieval system. Currently, several multimedia resources (spoken document) are unexploited, because the errors generated in the transcription process, decrease the performance of the systems of searches for spoken documents. Our contribution in this paper is the proposition a novel approach semantic based indexing content of multimedia documents by using the results of a partial automatic transcription to ride out research and indexing problems on these resources. In this context, the main hypothesis is developed around the following question: is it possible to identify and retrieve spoken document by using a semantic content indexing system based on his partial transcription? Also, does the partial content transcription of a spoken document is sufficient for his indexing? First, we are interested to look for a linguistic syntactic representation of the whole of the content spoken document. Then, it will be used to define the most discriminating indexing terms for his content. Nevertheless, in our approach, we have an assumption that the use of segments of a spoken document with an efficient semantic enrichment process instead of the whole spoken document content is sufficient. Also, with this strategy, we can resolve OOV problems, recognition errors and technical terms. Finally, in validation and experimentation phase, we tested the different modules proposed on the TED-LIUM corpora. The results obtained are interesting and encourage us to lead off our future perspectives.},
  keywords={Semantics;Indexing;Multimedia communication;Speech recognition;Syntactics;Lattices;Semantic content Indexing;Spoken Term detection;L VCSR;Keyword Spotting;Spoken Document;WordNet Ontology;similarity measures},
  doi={10.1109/ICNLSP.2018.8374390},
  ISSN={},
  month={April},}@INPROCEEDINGS{10020513,
  author={Chen, Xianlai and Lin, Jiamiao and An, Ying},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={DL-BERT: a time-aware double-level BERT-style model with pre-training for disease prediction}, 
  year={2022},
  volume={},
  number={},
  pages={1801-1808},
  abstract={Disease prediction based on the Electronic Health Record (EHR) is an important task in healthcare. EHR records patients’ every visit by time, and there are many kinds of medical codes within a visit, therefore, EHR has characteristics of temporal irregularity and hierarchical structure. Some recent works employ BERT-style models to process EHR data for disease prediction. However, few of these models can give consideration to capture both the interaction between medical codes and the impact of temporal irregularity. To solve this problem, we propose the Double-Level BERT-style model (DL-BERT). Considering EHR’s hierarchical structure, the model contains a code-level and a visit-level representation layer which can learn the relationship between medical codes and temporal influence respectively. In the code-level representation layer, the model achieves the representation power by employing external medical ontologies to provide multi-resolution information of medical codes and the Transformer to embed medical codes. Besides, the model adopts two pre-training tasks to enhance the ability to capture the link between different kinds of codes. In the visit-level representation layer, DL-BERT utilizes a special time-aware Transformer to model temporal information. And the model adopts a visit-level pre-training task for better learning context information. Experiments are conducted on two real-world healthcare datasets and show that our model outperforms all baselines demonstrating the effectiveness of our model.},
  keywords={Codes;Medical services;Ontologies;Predictive models;Big Data;Transformers;Data models;Electronic Health Record;BERT;Transformer;medical ontology;pre-train},
  doi={10.1109/BigData55660.2022.10020513},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8306008,
  author={Wouters, Laurent and Creff, Stephen and Bella, Emma Effa and Koudri, Ali},
  booktitle={2017 24th Asia-Pacific Software Engineering Conference (APSEC)}, 
  title={Towards Semantic-Aware Collaborations in Systems Engineering}, 
  year={2017},
  volume={},
  number={},
  pages={719-724},
  abstract={The design of nowadays complex systems requires a collaboration between a diversity of stakeholders: from domain experts to customers. For a collaboration to be efficient, the relevant information have to reach the right stakeholder at the right time in a format that is understandable to him/her. We propose a formal framework to integrate the meaning projected by stakeholders onto their data (the denotation), so that it can be unambiguously used by others. An implementation of this framework, relying on the existing language xOWL (extension of OWL2 with behavioral constructs), is then provided to perform the semantic integration of the captured denotations in an MBSE approach.},
  keywords={Semantics;Syntactics;Stakeholders;Collaboration;Unified modeling language;Assistive technology;Silicon;Collaborative Engineering;Semantic Integration;Modeling;Domain Intention;Ontology Mapping;Behavioral Semantics},
  doi={10.1109/APSEC.2017.92},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10350799,
  author={Elaasar, Maged and Rouquette, Nicolas and Wagner, David and Oakes, Bentley James and Hamou-Lhadj, Abdelwahab and Hamdaqa, Mohammad},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={openCAESAR: Balancing Agility and Rigor in Model-Based Systems Engineering}, 
  year={2023},
  volume={},
  number={},
  pages={221-230},
  abstract={Model-Based System Engineering (MBSE) employs models and formal languages to support development of complex (systems-of-) systems. NASA Jet Propulsion Laboratory (JPL) sees MBSE as a key approach to managing the complexity of system development. However, balancing agility and rigor in MBSE has been reported as a challenging task not yet addressed by modeling tools and frameworks. This is because existing MBSE approaches may enable agility but compromise rigor, or enhance rigor but impede agility. We discuss the challenges of balancing agility and rigor in MBSE across seven systems engineering architectural functions defined by the JPL Integrated Model-Centric Engineering (IMCE) initiative. We demonstrate how openCAESAR, an open-source MBSE methodology and framework created at JPL, can strike a balance between agility and rigor through a case study of the Kepler16b project and discussion of lessons learned from past projects.},
  keywords={NASA;Formal languages;Propulsion;Model driven engineering;Complexity theory;Modeling;Task analysis;Systems Engineering;Model-Based Systems Engineering;Ontology-based Modeling;OML;openCAESAR},
  doi={10.1109/MODELS-C59198.2023.00051},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9525789,
  author={Weerakoon, Charmy and Ranathunga, Surangika},
  booktitle={2021 Moratuwa Engineering Research Conference (MERCon)}, 
  title={Question Classification for the Travel Domain using Deep Contextualized Word Embedding Models}, 
  year={2021},
  volume={},
  number={},
  pages={573-578},
  abstract={Question answering can be considered as a key area in Natural Language Processing and Information Retrieval, where users construct queries in natural language and receive suitable answers in return. In the travel domain, most questions are “content questions”, where the expected answer is not the equivalent of “yes” or “no”, but rather factual information. Replying to a free-form factual question based on a large collection of text is challenging. Previous research has shown that the accuracy of question answering systems can be improved by adding a classification phase based on the expected answer type. This paper focuses on implementing a multi-level, multi-class question classification system focusing on the travel domain. Existing research for the travel domain is conducted using language-specific features and traditional Machine Learning models. In contrast, this research employs transformer-based state-of-the-art deep contextualized word embedding models for question classification. The proposed method improves the coarse class Micro F1-Score by 5.43% compared to the baseline. Fine-grain Micro F1-Score has also improved by 3.8%. We also present an empirical analysis of the effectiveness of different transformer-based deep contextualized word embedding models for multi-level multi-class classification.},
  keywords={Analytical models;Focusing;Machine learning;Transformers;Knowledge discovery;Information retrieval;Natural language processing;question classification;expected answer type;ontology learning;transformers;RoBERTa},
  doi={10.1109/MERCon52712.2021.9525789},
  ISSN={2691-364X},
  month={July},}@INPROCEEDINGS{9610664,
  author={Kiv, Soreangsey and Heng, Samedi and Wautelet, Yves and Kolp, Manuel},
  booktitle={2021 IEEE 23rd Conference on Business Informatics (CBI)}, 
  title={Towards a Systematic Socio-Intentional Framework for Agile Methods Tailoring}, 
  year={2021},
  volume={02},
  number={},
  pages={143-152},
  abstract={Agile has become one of the most popular software development approaches thanks to its flexible and evolutive features. To find further suitable practices, teams start to follow the tailoring approach by choosing only the fragments of different methods that fit their needs and context. Many tailoring approaches have been proposed by orienting different aspects such as process, resource and goal. While the interaction between team members is very important in agile methods, none of these approaches focuses on the socio-intentional aspect. In the literature, we can find many case studies that link socio-intentional aspects to the tailoring of agile practices. Even though it is helpful to know it, locating relevant information can be effort and time-consuming. This research proposes a socio-intentional framework that can analyze agile practices and indicate how to tailor them with the help of an evidence-based tool and a modeling language. This framework will allow practitioners to identify the right practices to achieve their goals and analyze their suitability and vulnerability. It will also indicate how to successfully implement them in the software development process.},
  keywords={Analytical models;Visualization;Systematics;Focusing;Tools;Software;Planning;Evidence-based System;Ontology;Tailoring Agile;Socio-intentional Modeling},
  doi={10.1109/CBI52690.2021.10065},
  ISSN={2378-1971},
  month={Sep.},}@INPROCEEDINGS{10450023,
  author={Pal, Suman and Gaur, Monica and Chaudhuri, Rupanjali and Benny Anto, Oshin and R, Kalaivanan and KV, Chetan and Pradhan, Pragnya},
  booktitle={2023 International Conference on Computational Intelligence, Networks and Security (ICCINS)}, 
  title={Recommendation System for Clinical Concept Mapping}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In the past decade, the healthcare industry has shifted from paper-based document storage to Electronic Health Records (EHR), enabling quick, safe access to patient data. A key role is played by Semantic Interoperability (SI) which enables seamless data exchange between diverse care settings and clinical software. SI necessitates the linking of bio-medical data (aka. clinical events) with shared, standardized, and controlled vocabularies like SNOMED, LOINC, etc. However, the healthcare data across various client domains are filled with ambiguous textual representations of clinical events that may be present in the form of synonyms, acronyms, and abbreviations. To make interoperability work, Healthcare IT service providers must map related clinical events with the appropriate standard concepts, which requires additional time and resources. Natural Language Processing (NLP) plays a vital role in addressing the challenges of SI by learning effective representations of text words in the bio-medical domain thereby capturing their semantic meaning. Our method utilizes various pre-trained word embeddings trained on the bio-medical corpus like BioWordVec fastText and SapBERT that captures fine-grained semantic relationships. In this study, we have developed a recommendation system that provides recommendations of Top ‘N’ clinical events for mapping to a standard concept. The recommendation system showed good performance with a sensitivity of above 99 % using both the pre-trained word embedding. Further, this product can be integrated into the mapping workflow to help make accurate automated suggestions that minimize manual effort.},
  keywords={Sensitivity;Semantics;Medical services;Manuals;Natural language processing;Recommender systems;Interoperability;semantic interoperability;ontology;mapping;natural language processing;word embeddings;SapBERT},
  doi={10.1109/ICCINS58907.2023.10450023},
  ISSN={},
  month={Dec},}@ARTICLE{10804145,
  author={Villegas-Ch, William and Govea, Jaime and Gutierrez, Rommel},
  journal={IEEE Access}, 
  title={Optimizing Language Model-Based Educational Assistants Using Knowledge Graphs: Integration With Moodle LMS}, 
  year={2024},
  volume={12},
  number={},
  pages={191994-192012},
  abstract={Chatbots in educational settings have grown significantly, facilitating interaction between students and learning platforms. However, current systems, such as Rasa, Moodle Integrated Chatbots, and ChatterBot, present significant limitations in precision, adaptability, and response time, affecting their effectiveness in resolving academic queries and personalizing learning. To address these shortcomings, this work proposes the development of an advanced educational chatbot that combines large language models (LLMs) with knowledge graphs, allowing for more accurate and contextualized responses and offering valuable suggestions to enrich the learning process. The system is evaluated based on its ability to adjust to different student profiles and offer fast and accurate responses. The results show that the proposed chatbot achieves a precision of 85%, outperforming Rasa and ChatterBot, which achieved accuracies of 83% and 81%, respectively. Furthermore, the chatbot reduces response times to 0.41 seconds, improving efficiency compared to other solutions. The system also demonstrates adaptability, effectively adjusting to students’ learning styles and academic levels. This work indicates that knowledge graph integration and hyperparameter optimization are crucial to improving educational chatbots’ precision, speed, and adaptability, presenting an innovative solution that overcomes the limitations of current systems.},
  keywords={Chatbots;Knowledge graphs;Time factors;Accuracy;Servers;Real-time systems;Data models;Adaptation models;Context modeling;Scalability;Educational chatbots;large language models;knowledge graphs;learning personalization},
  doi={10.1109/ACCESS.2024.3518952},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9006589,
  author={Lieder, Itay and Segal, Meirav and Avidan, Eran and Cohen, Asaf and Hope, Tom},
  booktitle={2019 IEEE International Conference on Big Data (Big Data)}, 
  title={Learning a Faceted Customer Segmentation for Discovering new Business Opportunities at Intel}, 
  year={2019},
  volume={},
  number={},
  pages={6136-6138},
  abstract={For sales and marketing organizations within large enterprises, identifying and understanding new markets, customers and partners is a key challenge. Intel's Sales and Marketing Group (SMG) faces similar challenges while growing in new markets and domains and evolving its existing business. In today's complex technological and commercial landscape, there is need for intelligent automation supporting a fine-grained understanding of businesses in order to help SMG sift through millions of companies across many geographies and languages and identify relevant directions. We present a system developed in our company that mines millions of public business web pages, and extracts a faceted customer representation. We focus on two key customer aspects that are essential for finding relevant opportunities: industry segments (ranging from broad verticals such as healthcare, to more specific fields such as “video analytics”) and functional roles (e.g., “manufacturer” or “retail”). To address the challenge of labeled data collection, we enrich our data with external information gleaned from Wikipedia, and develop a semi-supervised multi-label, multi-lingual deep learning model that parses customer website texts and classifies them into their respective facets. Our system scans and indexes companies as part of a large-scale knowledge graph that currently holds tens of millions of connected entities with thousands being fetched, enriched and connected to the graph by the hour in real time, and also supports knowledge and insight discovery. In experiments conducted in our company, we are able to significantly boost the performance of sales personnel in the task of discovering new customers and commercial partnership opportunities.},
  keywords={Companies;Industries;Encyclopedias;Electronic publishing;Internet;Bit error rate;AI for Enterprise;NLP;Web Mining},
  doi={10.1109/BigData47090.2019.9006589},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9325805,
  author={Wang, Yu and Sun, Yining and Ma, Zuchang and Gao, Lisheng and Xu, Yang and Wu, Yichen},
  booktitle={2020 13th International Symposium on Computational Intelligence and Design (ISCID)}, 
  title={A Method of Relation Extraction Using Pre-training Models}, 
  year={2020},
  volume={},
  number={},
  pages={176-179},
  abstract={Relation Extraction (RE), as an essential task of Natural Language Processing (NLP), aims to extract potential relations between two entities in a sentence. It is a crucial step in information extraction from unstructured data and building a Knowledge Graph (KG). The performance of deep learning methods for RE, like Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN), heavily depends on the quality and scale of the training set. Recently, pre-training models like BERT and ERNIE, have achieved State-Of-The-Art (SOTA) results in many NLP tasks, because they can obtain the prior semantic knowledge during the procedure of pre-training. Therefore, it is interesting to know whether the performance of RE can be improved utilizing the pre-training models. In this paper, we propose a method of RE using two kinds of pre-training models: BERT and ERNIE. First, in the input sequence, unique symbols are appended around the entities. RE is then regarded as a text classification task, and the prior semantic knowledge obtained by pre-training models is used to improve the performance. Experiments are carried on the SemEval 2010 Task 8 dataset. Results demonstrate that the method we proposed improves the performance of RE compared with previous approaches.},
  keywords={Task analysis;Semantics;Bit error rate;Training;Natural language processing;Feature extraction;Predictive models;relation extraction;pre-training models;BERT;ERNIE},
  doi={10.1109/ISCID51228.2020.00046},
  ISSN={2473-3547},
  month={Dec},}@INPROCEEDINGS{9245454,
  author={Sirirattanajakarin, Sorratat and Jitkongchuen, Duangjai and Intarapaiboon, Peerasak},
  booktitle={2020 1st International Conference on Big Data Analytics and Practices (IBDAP)}, 
  title={BoydCut: Bidirectional LSTM-CNN Model for Thai Sentence Segmenter}, 
  year={2020},
  volume={},
  number={},
  pages={1-4},
  abstract={Sentence is imperative in order to build the top level of NLP (Natural Language Processing) applications such as Information retrieval, News Summarization, Knowledge graph. In Thai language, neither each word token is not separated by using just only space like English language, nor the sentence is verified its boundary by using full stop symbols. This paper proposes BoydCut, an NLP framework for identifying sentence boundaries based on Bidirectional LSTM-CNN Model. We develop this framework by utilizing the combination of character, word, and part of speech features. With Bidirectional LSTM, it can learn sequent of word-level in sentences and learn extracted features from character level. With the benefit of the combination Bidirectional LSTM-CNN Model, we do not need feature engineering for feature extraction that can be saved a lot of cost and time in order to build a sentence segmentation model. We also simply design the experiments in a different bucket of features extracted from a deep sequential model. The result empirically shows well perform in internal and external data, as well as help a lot in order to build several useful on the top level of NLP applications.},
  keywords={Analytical models;Big Data;Feature extraction;Information retrieval;Natural language processing;Data models;natural language processing;sequent labeling;sentence segmentation;Thai language},
  doi={10.1109/IBDAP50342.2020.9245454},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10393214,
  author={Jha, Aashish and Purohit, Madhavi and Samanta, Gaurav and Jadhav, Dipti},
  booktitle={2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)}, 
  title={Questify: Your Intelligent Web Page Assistant for Quick Answers}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Retaining and recalling information from web pages can often feel like a daunting task, especially when faced with the vast amount of information contained within an article. However, researchers have come up with an innovative solution to tackle this. In this study, we propose a hybrid question-answering system that uses Natural Language Processing (NLP) to logically describe the data on a web page and then react to user inquiries. The system consists of a web scraper, named entity recognition module, and classifiers to learn more about the information.The process begins with a web scraper, which gets the text from a certain online page and makes sure that the required data is gathered. The system learns more about the information by identifying and classifying items like persons, locations, organizations, or other pertinent phrases. The question-answering module uses a knowledge graph to provide accurate and comprehensive answers to user queries. Using benchmark datasets, we evaluated the system’s performance to confirm the efficacy of their strategy. Across several datasets, the F1 score, which is a metric for precision and recall, varied from 0.7 to 0.95. The system can serve as a useful resource for information retrieval from web sites, according to these results, which show that it is able to provide users trustworthy responses to their questions.},
  keywords={Training;Surveys;Measurement;Text recognition;Web pages;Organizations;Natural language processing;natural language understating;web scraping;information;neural networks;text preprocessing;transformer models},
  doi={10.1109/ICACTA58201.2023.10393214},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10431540,
  author={Sadiq, Mohamed Abdul Karim and Shanmugam, Thirumurugan and Al-Fannah, Nasser Mohammed and Kausar, Firdous},
  booktitle={2023 IEEE 5th International Conference on Cognitive Machine Intelligence (CogMI)}, 
  title={Skills Extraction from Entities in Arabic CVs}, 
  year={2023},
  volume={},
  number={},
  pages={204-208},
  abstract={Named Entity Recognition in low resource languages such as Arabic is relatively less accurate. Despite this, many applications attempt query-document matching, which necessitates entity resolution. We explore certain possible solutions in this regard to enhance the matching process. A system is proposed to build a sub graph of terms in the Knowledge Graph. These terms specifically deal with skills in the Arabic documents related to the Human Resources domain. The accuracy measures of skills extraction using two different Language Models viz., SparkNLP and Hatmi are presented. The results are promising and have scope for improvement in entity resolution.},
  keywords={Knowledge graphs;Machine intelligence;automated recruitment;human resources;language models;named entity recognition;natural language processing},
  doi={10.1109/CogMI58952.2023.00037},
  ISSN={},
  month={Nov},}@ARTICLE{9745473,
  author={},
  journal={International Journal of Crowd Science}, 
  title={Front cover}, 
  year={2022},
  volume={6},
  number={1},
  pages={c1-c1},
  abstract={In recent years, neural networks have been widely used in natural language processing, especially in sentence similarity modeling. Most of the previous studies focused on the current sentence, ignoring the commonsense knowledge related to the current sentence in the task of sentence similarity modeling. Commonsense knowledge can be remarkably useful for understanding the semantics of sentences. CK-Encoder, which can effectively acquire commonsense knowledge to improve the performance of sentence similarity modeling, is proposed in this paper. Specifically, the model first generates a commonsense knowledge graph of the input sentence and calculates this graph by using the graph convolution network. In addition, CKER, a framework combining CK-Encoder and sentence encoder, is introduced. Experiments on two sentence similarity tasks have demonstrated that CK-Encoder can effectively acquire commonsense knowledge to improve the capability of a model to understand sentences.},
  keywords={Convolution;Semantics;Neural networks;Natural language processing;Task analysis;Commonsense reasoning;CK-Encoder;sentence similarity;commonsense knowledge},
  doi={},
  ISSN={2398-7294},
  month={April},}@INPROCEEDINGS{10331969,
  author={Wan, Lu and Wan, Xue},
  booktitle={2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={Entity Relationship Extraction for Chinese Electronic Medical Records Based on Tightly Cascaded Binary Pointers}, 
  year={2023},
  volume={},
  number={},
  pages={1212-1217},
  abstract={Relationship extraction, as a subtask of information extraction, converts unstructured medical data in electronic medical records into structured data, which lays the foundation for subsequent knowledge graph construction. The existing relationship extraction ignores the potential relationships of similar entities in a single sentence and fails to make full use of the relationships among the closer entities in the text. Thus, we propose a novel tightness mechanism that increases the correlation between head and tail entities to better determine the relationship between similar entities. The experiments of the proposed model on the public dataset demonstrate that the model can extract the triples more accurately and the F1 value is improved by 2.84% over the baseline model.},
  keywords={Electric potential;Head;Costs;Knowledge graphs;Transforms;Tail;Tagging;Chinese electronic medical records;Entity relationship extraction;cascade binary pointer tagging},
  doi={10.1109/PRAI59366.2023.10331969},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10066681,
  author={Lee, JinKyu and Kim, Jihie},
  booktitle={2023 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Improving Generation of Sentiment Commonsense by Bias Mitigation}, 
  year={2023},
  volume={},
  number={},
  pages={308-311},
  abstract={Commonsense knowledge graphs (CSKG) are crucial for artificial intelligence systems to understand natural language. Recently, with the construction of COMET (Commonsense Transformer) and ATOMIC2020, a comprehensive coverage commonsense reasoning knowledge graph, CSKG research is increasingly vital in natural language understanding and reasoning. Since sentiment commonsense knowledge is understudied yet, our work focuses on improving the generation of sentiment commonsense in ATOMIC2020. We first show a problem in natural language generation that degrades the accuracy due to the unbalanced sentiment distribution in the dataset. Next, we propose the EDA (Easy Data Augmentation) and UDA(Unsupervised Data Augmentation) based methods that improve the accuracy through biased mitigation of the unbalanced dataset. Our experimental results show that EDA method has little effect on the accuracy, while UDA-based method has some accuracy improvements in ROUGE-I, ROUGE-2, and ROUGE-L.},
  keywords={Comets;Tail;Knowledge graphs;Big Data;Transformers;Natural language processing;Task analysis;Commonsense;Sentiment;Bias;EDA;UDA},
  doi={10.1109/BigComp57234.2023.00061},
  ISSN={2375-9356},
  month={Feb},}@INPROCEEDINGS{10597884,
  author={Le, Duy and Zhao, Kris and Wang, Mengying and Wu, Yinghui},
  booktitle={2024 IEEE 40th International Conference on Data Engineering (ICDE)}, 
  title={GraphLingo: Domain Knowledge Exploration by Synchronizing Knowledge Graphs and Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={5477-5480},
  abstract={Knowledge graphs (KGs) are routinely curated to provide factual data for various domain-specific analyses. Nevertheless, it remains nontrivial to explore domain knowledge with standard query languages. We demonstrate GraphLingo, a natural language (NL)-based knowledge exploration system designed for exploring domain-specific knowledge graphs. It differs from conventional knowledge graph search tools in that it enables an interactive exploratory NL query over domain-specific knowledge graphs. GraphLingo seamlessly integrates graph query processing and large language models with a graph pattern-based prompt generation approach to guide users in exploring relevant factual knowledge. It streamlines NL-based question & answer, graph query optimization & refining, and automatic prompt generation. A unique feature of GraphLingo is its capability to enable users to explore by seamlessly switching between a more ‘open’ approach and a more relevant yet ‘conservative’ one, facilitated by diversified query suggestions. We show cases of GraphLingo in curriculum suggestion, and materials scientific data search.},
  keywords={Knowledge engineering;Query processing;Large language models;Refining;Natural languages;Knowledge graphs;Switches;Knowledge graphs;Large Language Models;Graph query;Exploratory search},
  doi={10.1109/ICDE60146.2024.00432},
  ISSN={2375-026X},
  month={May},}@INPROCEEDINGS{10350261,
  author={Liu, Xinyu},
  booktitle={2023 International Conference on Computers, Information Processing and Advanced Education (CIPAE)}, 
  title={EU-BERT: Efficient Unified Named Entity Recognition via Multi-exit BERT}, 
  year={2023},
  volume={},
  number={},
  pages={643-649},
  abstract={Named entity recognition (NER) is an important Natural Language Processing (NLP) task with wide application, such as document analysis, knowledge graph and query understanding. Although pretrain-based language models such as BERT has achieved great results on NER task, the large amount of parameters of BERT makes it slow during inference, which limits its usage in industry. Experiments show that the metric of traditional early exiting has great defects. In this work, we propose EU-BERT, an Efficient Unified named entity recognition framework via multi-exit BERT, to accelerate BERT inference on NER task. To handle nested and discontinuous NER tasks, we adopt W2NER [1], a unified NER framework using table filling technique. EU-BERT proposes a better metric for early exiting and uses contrastive learning to enhance its ability. Experiments on 9 benchmark NER datasets demonstrate that our method can improve the performance of multi-exit BERT on NER task while maintaining its inference speed.},
  keywords={Measurement;Industries;Text analysis;Knowledge graphs;Information processing;Benchmark testing;Natural language processing;BERT;Named Entity Recognition;Early Exit;Contrastive Learning},
  doi={10.1109/CIPAE60493.2023.00125},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10824634,
  author={Liu, Xiangfeng and Sun, Jianting and Lei, Aidi and Zhu, Junzhi},
  booktitle={2024 3rd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE)}, 
  title={Research and Applications of Large Language Models for Converting Unstructured Data into Structured Data}, 
  year={2024},
  volume={},
  number={},
  pages={305-308},
  abstract={The rapid growth of data has led to a significant increase in unstructured data, such as text, audio, and images, which dominate modern information processing. However, the complexity of unstructured data presents challenges for automated analysis and processing. Converting unstructured data into structured formats is crucial for tasks like data mining, information extraction, and knowledge graph construction. Traditional methods that rely on manual rules or statistical models struggle with complex, context-dependent data. Recently, large language models (LLMs), such as GPT-4 and BERT, have demonstrated great potential in unstructured data processing due to their powerful natural language understanding and generation capabilities. This study explores the application of large language models in transforming unstructured data into structured formats. It begins by reviewing the limitations of traditional approaches and then presents a framework for unstructured data processing using LLMs, covering data preprocessing, information extraction, and structured representation. The study's experiments demonstrate the superior performance of LLMs in handling various types of unstructured data, particularly in tasks like named entity recognition, relation extraction, and contextual understanding. The results show that LLMs achieve higher accuracy and generalization while reducing reliance on manual rules. The paper discusses the strengths and limitations of this method, proposing future improvements, such as combining domain-specific knowledge for model optimization and expanding applications to multimodal data processing. The research highlights the promising role of large language models in converting unstructured data into structured formats.},
  keywords={Adaptation models;Accuracy;Data conversion;Large language models;Computational modeling;Manuals;Information retrieval;Data models;Data mining;Software engineering;Large language model;unstructured data;information extraction;data management system;database query processing},
  doi={10.1109/CBASE64041.2024.10824634},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10825160,
  author={Kim, Edward and Shrestha, Manil and Foty, Richard and DeLay, Tom and Seyfert-Margolis, Vicki},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Structured Extraction of Real World Medical Knowledge using LLMs for Summarization and Search}, 
  year={2024},
  volume={},
  number={},
  pages={3421-3430},
  abstract={Creation and curation of knowledge graphs at scale can be used to exponentially accelerate the discovery, matching, and analysis of diseases in real-world data. While disease ontologies are useful for annotation, integration, and analysis of biological data, codified disease and procedure categories e.g. SNOMED-CT, ICD10, CPT, etc. rarely capture all of the nuances in a patient condition or, in the case of rare disease, may not even exist. Furthermore, there are multiple disease definitions used in data sources and publications, each having its own structure and hierarchy. Mapping between ontologies, finding disease clusters, and building a representation of the chosen disease area are resource-intensive, often requiring significant human capital. We propose the creation and curation of a patient knowledge graph utilizing large language model extraction techniques. In order to expand in volume and scale, knowledge graphs with generalized language capability allow for data to be extracted using natural language rather than being constrained by the exact terminology or hierarchy of existing ontologies. We develop a method of mapping back to existing ontologies such as MeSH, SNOMED-CT, RxNORM, HPO, etc. to ground the extracted entities to known entities in the medical community.We have access to one of the largest ambulatory care EHR databases in the country. To demonstrate the effectiveness of our method, we benchmark our extraction in a test set with over 33.6M unique patients, in the area of patient search. In this case study, we perform a patient search for a rare disease: Dravet syndrome. Dravet syndrome was codified as an ICD10 recognizable disease in October 2020. In the following research, we describe our method of the construction of patient-specific knowledge graphs and subsequent searches for patients who exhibit symptoms of a particular disease. Using patients with confirmed ICD10 codes for Dravet syndrome as our ground truth, we utilize our LLM-based entity extraction techniques and formalize an algorithmic way of characterizing patients in a grounded ontology to assist in mapping patients to specific diseases. Finally, we present the results of a real-world discovery method on Beta-propeller protein-associated neurodegeneration (BPAN), identifying patients with a rare disease, where no ground truth currently exists.},
  keywords={Proteins;Translation;Annotations;Terminology;Large language models;Knowledge graphs;Ontologies;Benchmark testing;Data mining;Diseases;Large Language Models;Knowledge Graphs;Ontology Mapping;Structured Extraction;Dravet Syndrome;Beta-propeller protein-associated neurodegeneration (BPAN)},
  doi={10.1109/BigData62323.2024.10825160},
  ISSN={2573-2978},
  month={Dec},}@ARTICLE{9576612,
  author={Zhu, Biru and Zhang, Xingyao and Gu, Ming and Deng, Yangdong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Knowledge Enhanced Fact Checking and Verification}, 
  year={2021},
  volume={29},
  number={},
  pages={3132-3143},
  abstract={As the Internet and social media offer increasing opportunities for organizations and individuals to publicize online contents, it has become essential to develop effective means to identify misinformation like fake news. Recently, fact checking systems have been regarded as a promising tool to automatically deal with large amounts of information. How to effectively take advantage of existing unstructured document knowledge bases and structured knowledge graphs to build robust fact checking systems, however, remains to be a challenge. In this paper, we propose a knowledge enhanced fact checking system, which leverages the Wikidata5M knowledge graph and Wikipedia documents to incorporate external knowledge into the claim to be checked for more robust and accurate fact checking. First, we devise a contextualized knowledge graph selection method to identify the most relevant sub-graph with the checked claim from the large knowledge graph. We then construct a novel claim-evidence-knowledge graph and use a graph attention network to integrate natural language evidence with structured knowledge triplets by allowing them to propagate information among each other. By integrating the claim, retrieved evidence and selected knowledge triplets in a unified claim-evidence-knowledge graph, our method improves the label accuracy of predicted claims by more than 4% on the FEVER dataset over state-of-the-art fact checking models.},
  keywords={Internet;Knowledge based systems;Encyclopedias;Online services;Semantics;Feature extraction;Tools;Automated fact checking;knowledge selection;knowledge enhanced fact checking},
  doi={10.1109/TASLP.2021.3120636},
  ISSN={2329-9304},
  month={},}@INPROCEEDINGS{10388202,
  author={Bringsjord, Selmer and Slowik, John and Govindarajulu, Naveen Sundar and Giancola, Michael and Oswald, James and Ghosh, Rikhiya},
  booktitle={2023 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)}, 
  title={Affect-based Planning for a Meta-Cognitive Robot Sculptor: First Steps}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Today’s so-called “generative AI” is in the minds of some capable of specifically generating genuine visual art; DALL-E is an example. While in fact intensely skeptical, we grant for the sake of argument that the likes of DALL-E can in fact generate genuine visual art. However, we observe that fine visual artistry is by no means monolithic; in particular, some forms of fine visual artistry are seemingly harder than others for artificial agents to achieve. In the traditional ontology, perhaps the very hardest type of fine visual artistry for an AI to achieve is literary sculpture, an activity carried out in the human sphere at the very highest level by Rodin. Artificial agents that operate in this sphere must for obvious reasons be cognitive robots. We explain such literary sculpture in broad terms, making clear that it’s undertaken by the sculptor in question with the aim of bringing about certain affective states in the mind of the viewer of the sculpture. In short, literary sculpting is affect-driven. We provide a case study of robot sculpting, with help from pre-existing logic-based formalisms and automated-reasoning technology, and the cognitive robot PERI.2, operating as a sculptor. To our knowledge, ours is the very first foray into literary sculpture in AI and cognitive robotics.},
  keywords={Visualization;Affective computing;Art;Generative AI;Conferences;Ontologies;Planning;creativity;AI;cognitive robotics;sculpture;logic-based AI},
  doi={10.1109/ACIIW59127.2023.10388202},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{7970506,
  author={Pisarev, I. A.},
  booktitle={2017 XX IEEE International Conference on Soft Computing and Measurements (SCM)}, 
  title={Probabilistic methods of automated dynamic thesauri creation from heterogeneous knowledge sources}, 
  year={2017},
  volume={},
  number={},
  pages={96-98},
  abstract={Probabilistic methods of the automated formation of dynamic thesauri are considered on the basis of algorithms for processing, analyzing and classifying linguistic resources of a very large volume from heterogeneous data sources in application to the support systems of learning processes. Probabilistic methods are implemented in the automated software tool “Ontomaster-Ontology”. An essential feature of the implementation is the provision of automated processing of very large volumes of linguistic resources from heterogeneous sources. Probabilistic methods have been applied to the formation of thematic text corpora in Russian and English languages, extraction of terms of subject domains, classification of documents. Reprezented the results of the study of improving the efficiency of document classification based on the Bayesian approach are presented using the n-gram model and verbose terms. Probabilistic methods of computer linguistics are applied in the development of a system for supporting the process of teaching students in the direction “Information Systems and Technologies”.},
  keywords={Thesauri;Classification algorithms;Pragmatics;Probabilistic logic;Databases;Information systems;Bayes methods;algorithms of computational linguistics;classification;learning process Introduction},
  doi={10.1109/SCM.2017.7970506},
  ISSN={},
  month={May},}@INPROCEEDINGS{9887476,
  author={Dudija, Nidya and Natalia, Lezia and Alamsyah, Andry and Romadhony, Ade},
  booktitle={2022 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)}, 
  title={Identification of Extraversion and Neuroticism Personality Dimensions Using IndoBERT’s Deep Learning Model}, 
  year={2022},
  volume={},
  number={},
  pages={155-159},
  abstract={Human resources are essential for the business organization to adapt to change. Identifying the personality dimensions of new talent could help recruiters conduct the selection process of matching skilled talent to the organization’s needs. The objective of this study is to identify the personality dimensions corresponding to the job need, which correlates with extraversion and neuroticism. The legacy methodology to determine personality dimensions is through interviews or questionnaire surveys, but this process is costly and takes longer time to complete. This paper proposes a work on a person personality identification based on social media text as a complementary methodology. We utilize the textual data to support identifying new talent personality dimensions. In this study, we use IndoBERT model to capture person personality dimension based on their post on Twitter social media. As a result, our model achieves 96% accuracy in identifying extraversion and neuroticism personality dimensions. We also compare our result with the previous work based on the ontology model.},
  keywords={Deep learning;Costs;Social networking (online);Blogs;Ontologies;Communications technology;Fourth Industrial Revolution;Human Resource;Talent Selection;Personality Identification Dimension;Deep Learning;IndoBERT},
  doi={10.1109/IAICT55358.2022.9887476},
  ISSN={},
  month={July},}@INPROCEEDINGS{9896311,
  author={Rogachev, Aleksey and Melikhova, Elena},
  booktitle={2022 International Russian Automation Conference (RusAutoCon)}, 
  title={Automation of Architecture Justification and Parameters Selection of Artificial Neural Networks for Intelligent Detection of Cyber-Physical Threats}, 
  year={2022},
  volume={},
  number={},
  pages={908-912},
  abstract={The problems of improving the quality of training of deep artificial neural networks (ANN) for various applied tasks require automatization of the selection of hyperparameters of neural networks. The KerasTuner software toolkit can be used to automate the search for optimal values of ANN hyperparameters. It includes random search methods, Bayesian optimization, etc. The formation of training text samples for neural network identification of cyber-physical threats is a separate scientific and methodological task. The complexity of the problem is due to the diversity of the ontology of the key terms of the cyberphysical thesaurus, the variety of styles of lexicological content, as well as the partial intersection of the content of previously identified ontological categories. In the process of experimental study of hyperparameters of deep ANNs being developed, models of “embedding”, “bag of words” and dense vector representation in Python were compared. On the basis of a systematic approach, an information-morphological matrix of thematic blocks is constructed. In the conducted experiments, the values of parameters such as the number of convolutional blocks, the number of their filters, the type of activation functions, the parameters of the “dropout” layers, etc. were changed. The studied tools provided optimization of hyperparameters of the convolutional network, while the calculation time on the Colaboratory platform for the studied ANN architectures using GPU graphics accelerators was 5…9 o’clock. The developed modified algorithm for computer detection of cyberphysical threats in electronic resources allowed to substantiate alternative architectures and optimize the main hyperparameters of ANN.},
  keywords={Training;Automation;Graphics processing units;Artificial neural networks;Computer architecture;Ontologies;Cyber-physical systems;cyber-physical threat;artificial neural network;hyperparameter;intelligent detection;Automation},
  doi={10.1109/RusAutoCon54946.2022.9896311},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10507138,
  author={Anass, Bayaga},
  booktitle={2024 Conference on Information Communications Technology and Society (ICTAS)}, 
  title={Advancing STEM cognition with current AI landscape and systems}, 
  year={2024},
  volume={},
  number={},
  pages={20-25},
  abstract={Application of AI explores the potential of algorithms to ensure fairness, accuracy, and efficiency in grading students' performance, offering valuable insights into their strengths and areas for improvement. While the current AI landscape showcases remarkable progress, there are several areas ripe for exploration. One such avenue is AI steps to consider in STEM, wherein researchers aim to develop specialised steps/models to understand and generate domain-specific STEM content. The systematic literature review highlighted the importance of domain adaptation techniques for enhancing STEM comprehension by fine-tuning transformer-based language models like BERT. Integrating domain knowledge through ontology-based and context of STEM disciplines. Future research should focus on building domain-specific annotated datasets to improve the performance models in STEM comprehension. Additionally, exploring unsupervised domain adaptation techniques and leveraging domain-specific knowledge graphs can further enhance the NLP models’ adaptability to diverse STEM domains.},
  keywords={Adaptation models;Vocabulary;Systematics;Bibliographies;Laboratories;Knowledge graphs;Transformers;Data models;Artificial intelligence;STEM;Artificial Intelligence;STEM Education;Explainable AI;Natural Language Processing;AI-Augmented Laboratories},
  doi={10.1109/ICTAS59620.2024.10507138},
  ISSN={},
  month={March},}@INPROCEEDINGS{9206790,
  author={Shen, Tianhao and Wang, Xiaojie},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Multi-Domain Dialogue State Tracking with Hierarchical Task Graph}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={Multi-domain dialogue state tracking (DST), which tracks user goals and intentions across multiple domains, is a core task for multi-domain task-oriented dialogue system. Previous works in multi-domain DST focus on the open-vocabulary setting to alleviate the over-dependence on pre-defined ontology. However, they come up short of modeling the relationships among domains and slots in an explicit and efficient way. In this paper, we propose a multi-domain dialogue state tracker with hierarchical task graph (DST-HTG) to address the above issues. DST-HTG uses a copy mechanism to perform DST under the open-vocabulary setting, which makes our model eliminate the dependence on pre-defined full ontology. Moreover, we extend our DST model with a hierarchical task graph which has simple structure and rich semantic information to incorporate the relationships among domains and slots into DST process explicitly and efficiently. Empirical results show that DST-HTG achieves the state-of-the-art joint goal accuracy and slot accuracy in MultiWOZ 2.0, a recently proposed multi-domain task-oriented dialogue dataset, which indicates the effectiveness of our proposed model.},
  keywords={Task analysis;History;Ontologies;Context modeling;Generators;Fuses;Computational modeling;natural language processing;task-oriented dialogue systems;multi-domain dialogue state tracking;task graph},
  doi={10.1109/IJCNN48605.2020.9206790},
  ISSN={2161-4407},
  month={July},}@ARTICLE{9861204,
  author={Huang, Weiming and Xu, Shiting and Yuhan, Wang and Fan, Jin and Chang, Qingling},
  journal={IEEE Access}, 
  title={AttractionDetailsQA: An Attraction Details Focused on Chinese Question Answering Dataset}, 
  year={2022},
  volume={10},
  number={},
  pages={86215-86221},
  abstract={With the increase in the number of domestic tourists and the popularity of digital upgrades in attractions, it is crucial to develop a question-answering(QA) system about the details of the attractions. However, there is little work on attractions QA, and the main bottleneck is the lack of available datasets. While previous QA datasets usually focus on news domain like CNN/DAILYMAIL and NewsQA, we present the first large-scale dataset for QA over attraction details. To ensure that the data we collected are useful, we only gather the data from public travel information website. Unlike other QA datasets like SQuAD, which is labeled manually, we formed the dataset by manual and question-answer pair generation(QAG) annotated model. Finally, we obtained a dataset covering 2,808 attractions with a total of 18,245 QA pairs, including seven types of attraction details: location, time, component, area, layout, rating, and character. The dataset is available at https://github.com/wyman130/AttractionDetailsQA. Considering that QAG has not been much studied in attraction details, we experimented some QAG models on this dataset and obtained the benchmark. This provides a basis for subsequent improvements to the dataset and research on QAG in attraction details.},
  keywords={Annotations;Data models;Question answering (information retrieval);Manuals;Layout;Benchmark testing;Tourism industry;Attraction detail dataset;question-answering pair generation},
  doi={10.1109/ACCESS.2022.3181188},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9172685,
  author={Mordecai, Yaniv and James, Nicholas K. and Crawley, Edward F.},
  booktitle={2020 IEEE Aerospace Conference}, 
  title={Object-Process Model-Based Operational Viewpoint Specification for Aerospace Architectures}, 
  year={2020},
  volume={},
  number={},
  pages={1-15},
  abstract={Remote-controlled or autonomous multi-rotor air vehicles, or drones, have become common and commercially available even to individual consumers, mostly for imaging purposes. Drones appeal to mission architects looking to extend the toolbox provided to operators performing challenging missions such as public safety operations. However, careful analysis of the operational context and concept of operations must take place before major acquisitions. The purpose of this paper is to propose a model-based operational architecture definition framework, which is based on the Department of Defense Architecture Framework (DoDAF) ontology and uses Object Process Methodology (OPM) as its underlying modeling language. Through careful mapping of DoDAF Operational Viewpoint (OV) ontology to OPM ontology, we were able to show that the entire OV ontology can be covered by a small set of objects, processes, relations among them, and constructs comprising them. We then show how to instantiate the ontology to create a model of an actual architecture of interest (AoI) while maintaining strong typing of the model elements to ensure validity, integrity, consistency, and continuous compliance with the OV. We demonstrate our approach on the case of using drones in public safety enterprises for the purpose of crowd management in massively attended events and locations. The proposed framework allows for capturing ConOps and OpsCon in a lightweight, yet robust and consistent manner, and improve communication and concept validation between operational stakeholders and enterprise architects.},
  keywords={Conferences;Imaging;Ontologies;Safety;Stakeholders;Drones},
  doi={10.1109/AERO47225.2020.9172685},
  ISSN={1095-323X},
  month={March},}@ARTICLE{10348568,
  author={Johnston, Penny and Nogueira, Keiller and Swingler, Kevin},
  journal={IEEE Access}, 
  title={NS-IL: Neuro-Symbolic Visual Question Answering Using Incrementally Learnt, Independent Probabilistic Models for Small Sample Sizes}, 
  year={2023},
  volume={11},
  number={},
  pages={141406-141420},
  abstract={This paper is motivated by the challenge of providing accurate and contextually relevant answers to natural language questions about visual scenes, particularly in support of individuals with visual impairments. We present a system that is capable of incrementally learning both visual concepts and symbolic facts to answer natural language questions about visual scenes via rich concepts. Deep neural networks are used to learn a feature space from which visual classes are learned as independent probability distributions, allowing new classes to be added arbitrarily with small sample sizes and without the risk of catastrophic forgetting associated with traditional neural networks. Visual classes are not limited to object labels, but also include visual attributes. A knowledge graph is used to represent facts about objects, such as their actions, locations and the relationships between different objects. This allows facts to be stored explicitly and added incrementally. A large language model is used to translate between natural language questions and knowledge graph traversal queries, providing a natural visual question answering process.},
  keywords={Visualization;Natural languages;Knowledge graphs;Cognition;Question answering (information retrieval);Adaptation models;Training;Neuro-symbolic system;visual question answering;classification system;Gaussian mixture model;incremental learning},
  doi={10.1109/ACCESS.2023.3341007},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10729340,
  author={Li, Daifeng and Xu, Fan},
  booktitle={2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)}, 
  title={The Deep Integration of Knowledge Graphs and Large Language Models: Advancements, Challenges, and Future Directions}, 
  year={2024},
  volume={},
  number={},
  pages={157-162},
  abstract={In recent years, large language models have emerged with amazing capabilities, but they also have limitations such as hallucinations and black boxes, while knowledge graphs have accurate knowledge and symbolic reasoning capabilities. Therefore, the integration of knowledge graph and large language model becomes necessary. The paper systematically conducts the recent research pertaining to the integration of knowledge graph with large language mode. By analyzing the methods and locations of their confluence, we propose a unifying framework that aims to facilitate comprehension and inspiration for fellow researchers in related fields. This framework not only consolidates existing knowledge but also enhances the translational potential of research by delineating innovative pathways for practical implementation.},
  keywords={Knowledge engineering;Accuracy;Large language models;Government;Knowledge graphs;Reliability engineering;Robustness;Telecommunications;Sensors;Optimization;knowledge graphs;large language models;deep integration;system review},
  doi={10.1109/ICSECE61636.2024.10729340},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10692530,
  author={Rong, Shiqiang and Li, Rili and Sun, Xiaocui and Yi, Faling},
  booktitle={2024 6th International Conference on Internet of Things, Automation and Artificial Intelligence (IoTAAI)}, 
  title={Research on Named Entity Recognition Algorithm for Diabetes Intelligent Question and Answer System}, 
  year={2024},
  volume={},
  number={},
  pages={631-637},
  abstract={Named Entity Recognition (NER) is an important basis for constructing knowledge graph. Combining knowledge graph with pre-trained large language model is an important research direction of current intelligent question and answer system. Based on the knowledge system of daily prevention and management of diabetes and the corresponding answers, the corresponding named entity recognition method is studied, and the recognition method of dictionary rules and Bert + BiLSTM + CRF model fusion is proposed. Firstly, the dictionary rules using AC automaton character matching and context analysis method to effectively identify the disease, symptoms, indicators and other fixed types of entities and classification are built; then, the Bert + BiLSTM + CRF model is constructed to achieve good performances in the identification of extensible entities such as inspections, drugs, treatment methods, and food. In the process of training Bert + BiLSTM + CRF model, various strategies such as entity replacement, entity masking and entity splicing are adopted, which effectively improves the accuracy of recognition. The experimental results show that the recognition accuracy of the fusion algorithm reaches 93 %.},
  keywords={Training;Dictionaries;Accuracy;Splicing;Prevention and mitigation;Large language models;Knowledge based systems;Named entity recognition;Knowledge graphs;Diabetes;Knowledge Graph;Entity Recognition;Dictionary Rules;Bert + BiLSTM + CRF;Diabetes},
  doi={10.1109/IoTAAI62601.2024.10692530},
  ISSN={},
  month={July},}@ARTICLE{10879360,
  author={Zhao, Wenqing and Ding, Yingxue and Zhang, Le and Liu, Bin and Yang, Cen and Zhao, Zhenhuan and Zhao, Zhenbing and Zhai, Yongjie and Xu, Minfu},
  journal={IEEE Transactions on Power Delivery}, 
  title={CMKR-PBDM: A Transmission Line Pin-missing Bolts Detection Method Based on Cross-media and Knowledge Reasoning}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={To address the limited single-image information-processing method, insufficient global knowledge-processing capability of the traditional model, and the lack of explainability in detection results for the task of transmission line pin-missing bolts detection, the researchers propose a pin-missing bolts detection method based on cross-media and knowledge reasoning (CMKR-PBDM). First, we construct a fitting-bolt image-text pair dataset and a bolt knowledge graph (BoltKG). Subsequently, a cross-media bolt knowledge fusion model (CBKFM) is proposed, thus generating the image's overall text description by fusing the global knowledge extracted by the FB-GPT with the local knowledge captured by YOLOv8. Finally, the study proposes a large language reasoning model based on the knowledge graph (LLRM-KG), which utilizes BoltKG to guide the big language model in performing knowledge reasoning on the CBKFM output information; thus, explainable pin-missing bolts detection results are obtained. In the experimental stage, the researchers select bolts on four types of background fittings as experimental objects. The experimental results indicate that the method not only improves the accuracy of pin-missing bolts detection, but also makes the pin-missing bolts detection results explainable.},
  keywords={Fasteners;Cognition;Fitting;Pins;Feature extraction;Power transmission lines;Shock absorbers;Vibrations;Knowledge graphs;Electronic mail;Bolt;Knowledge graph;Knowledge reasoning;Cross-media;Pin-missing detection;Transmission line},
  doi={10.1109/TPWRD.2025.3540475},
  ISSN={1937-4208},
  month={},}@INPROCEEDINGS{10849427,
  author={Armary, Pauline and El-Vaigh, Cheikh-Brahim and Spicher, Antoine and Narsis, Ouassila Labbani and Nicolle, Christophe},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Identifying Logical Patterns in Text for Reasoning}, 
  year={2024},
  volume={},
  number={},
  pages={837-844},
  abstract={Translating unstructured text into logical format is a key challenge for building ontologies automatically and addressing deductive inference. Most of the approaches have tackled the identification of concepts and relations in text, but few of them have addressed the most complex axioms like class expression subsumption. This work proposes DeLIR, a neuro-symbolic approach to identify complex logical patterns in text by combining a grammatical translation of dependency parsing trees and a fine-tuned Large language Model (LLM). DeLIR combines the strength of the parsing accuracy provided by a grammatical approach and pattern flexibility provided by a finetuned LLM. We evaluated our approach on FOLIO dataset for both translation capacity and inference capability. Our grammatical approach has a perfect parsing accuracy and combining the grammatical approach with LLMs improves the LLMS translation capacity: tinyLlama, T5-small-text2logic, Llama-7B and Mistral-7B. We also evaluate the inference capacity of the different LLMs. Mistral-7B, while being smaller than the state-of-the-art approach using GPT-4, presents similar results to predict the correct inference labels.},
  keywords={Hands;Translation;Accuracy;Large language models;Zero shot learning;Buildings;Ontologies;Syntactics;Cognition;Ontology Learning;Translation to Logic;Natural Language Inference},
  doi={10.1109/ICTAI62512.2024.00122},
  ISSN={2375-0197},
  month={Oct},}@ARTICLE{10592819,
  author={Sun, Zhigang and Wang, Zixu and Halilaj, Lavdim and Luettin, Juergen},
  journal={IEEE Robotics and Automation Letters}, 
  title={SemanticFormer: Holistic and Semantic Traffic Scene Representation for Trajectory Prediction Using Knowledge Graphs}, 
  year={2024},
  volume={9},
  number={9},
  pages={7381-7388},
  abstract={Trajectory prediction in autonomous driving relies on accurate representation of all relevant contexts of the driving scene, including traffic participants, road topology, traffic signs, as well as their semantic relations to each other. Despite increased attention to this issue, most approaches in trajectory prediction do not consider all of these factors sufficiently. We present SemanticFormer, an approach for predicting multimodal trajectories by reasoning over a semantic traffic scene graph using a hybrid approach. It utilizes high-level information in the form of meta-paths, i.e. trajectories on which an agent is allowed to drive from a knowledge graph which is then processed by a novel pipeline based on multiple attention mechanisms to predict accurate trajectories. SemanticFormer comprises a hierarchical heterogeneous graph encoder to capture spatio-temporal and relational information across agents as well as between agents and road elements. Further, it includes a predictor to fuse different encodings and decode trajectories with probabilities. Finally, a refinement module assesses permitted meta-paths of trajectories and speed profiles to obtain final predicted trajectories. Evaluation of the nuScenes benchmark demonstrates improved performance compared to several SOTA methods. In addition, we demonstrate that our knowledge graph can be easily added to two graph-based existing SOTA methods, namely VectorNet and LaFormer, replacing their original homogeneous graphs. The evaluation results suggest that by adding our knowledge graph the performance of the original methods is enhanced by 5% and 4%, respectively.},
  keywords={Trajectory;Knowledge graphs;Semantics;Ontologies;Encoding;Predictive models;Transformers;Semantic scene understanding;autonomous agents;intelligent transportation systems},
  doi={10.1109/LRA.2024.3426386},
  ISSN={2377-3766},
  month={Sep.},}@INPROCEEDINGS{9534360,
  author={Li, Jipeng and Sun, Yujing and Li, Chenhui and Hu, Yanpeng and Wang, Changbo},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Industry Chain Graph Building Based on Text Semantic Association Mining}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={The current volume of data in the field of securities investment is increasing dramatically. Simultaneously, the linkage of data from multiple parties makes investment reasoning decisions more challenging than ever. In response to this problem, the financial field's knowledge graph can improve the efficiency, depth, and breadth of financial practitioners' information analysis. Some existing financial knowledge graphs analyze the shareholding relationship between companies. Still, because they are limited to observing data from the company's perspective, users without professional industry background cannot quickly find the industry factors of stock market changes. This paper proposes a financial knowledge graph from the industry chain's perspective. This paper builds upstream and downstream relationships between industries through Transformer-based bidirectional encoder to mine potential industry chain associations from text data and completes the long industry chain of the stock market. This paper also builds a visualization system to display and explore the connection between listed companies and industries. Users can inspect the industry chain's composition and each company's revenue status and stock market conditions in the industry chain. The experiment shows that when the market price fluctuation is detected, the stock price fluctuation can be traced back to its origin in the knowledge graph.},
  keywords={Industries;Visualization;Fluctuations;Semantics;Neural networks;Companies;Transformers;Industry Chain;Relation Extraction},
  doi={10.1109/IJCNN52387.2021.9534360},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9879230,
  author={Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={TableFormer: Table Structure Understanding with Transformers}, 
  year={2022},
  volume={},
  number={},
  pages={4604-4613},
  abstract={Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph's, etc, since they enhance their predictive capabilities. Unfortu-nately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct iden-tification of the table-structure from an image is a nontrivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from program-matic PDF's directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple tables and from 88.7% to 95% on complex tables.},
  keywords={Deep learning;Training;Shape;Optical character recognition;Object detection;Search engines;Transformers;retrieval; Vision applications and systems;categorization;Document analysis and understanding; Deep learning architectures and techniques; Recognition: detection},
  doi={10.1109/CVPR52688.2022.00457},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10192008,
  author={Liu, Shu and Ye, Zhiqiang and Liao, Jian and Wu, Jinxin and Li, Zhao},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Unsupervised Product Title Optimization Based on Search Behavior Knowledge in E-commerce}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Nowadays, E-commercial platforms such as Taobao and Amazon, provide massive products which greatly improve the convenience of online shopping. However, there have been many redundant and lengthy titles, which are misleading and hurt the experience of consumers. Under this background, the product title compression task has been proposed to generate readable short titles. The existing supervised models rely on paired corpora, which is often expensive to annotate. Whereas unsupervised methods are not adopted in the E-commercial scenario due to lack of domain knowledge. To overcome the above limitations, we propose an unsupervised title compression method based on search behavior graph. Specifically, we build the knowledge graph extracted from search queries to detect redundancy. Moreover, click behaviors are utilized to give a clue to recognize inaccurate information. Then we employ a novel graph-to-text model consisting of an edge-importance encoder and a length-countdown decoder. The encoder combines the title with both query knowledge and click behaviors, and the decoder generates the compressed title with the desired length. Experiments on datasets show that our model outperforms the state-of-the-art models on both automatic and human evaluations. Moreover, further analysis demonstrates that our method has the ability to detect redundancy and keep the appropriate information.},
  keywords={Image edge detection;Neural networks;Termination of employment;Knowledge graphs;Search problems;Behavioral sciences;Decoding;Unsupervised Title Compression Framework;Query Knowledge;Click Behavior},
  doi={10.1109/IJCNN54540.2023.10192008},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10711793,
  author={Pan, Xinyu and Gong, Jie and Wen, Sijie and Zhuang, Weibin and Li, Xinyu},
  booktitle={2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)}, 
  title={Mining User Requirement Scenarios and Generating Design Solutions for Rehabilitation Aids Based on Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={3155-3161},
  abstract={The development of the rehabilitation aids industry for the disabled has been pivotal in recent years, particularly in the personalized design of lower limb rehabilitation aids. Facing challenges in meeting individualized demands in design practice and the information gap between medical professionals and users, we propose a design Knowledge Graph (KG) method based on the Function-Behavior-Structure (FBS) model. This approach utilizes open-source large language models (LLMs) and fine-tunes them with instruction data generated by self-instructions to improve the accuracy of user requirements mining. The method aims to enhance the personalization and innovation of rehabilitation aids design through the integration of KG and LLM, effectively narrowing the cognitive gap between service providers and users. The anticipated results of the study are expected to promote efficient innovation in rehabilitation aids design, better meeting the needs of the disabled community.},
  keywords={Industries;Technological innovation;Computer aided software engineering;Automation;Accuracy;Large language models;Conferences;Knowledge graphs;Data mining},
  doi={10.1109/CASE59546.2024.10711793},
  ISSN={2161-8089},
  month={Aug},}@ARTICLE{9525274,
  author={Nourani, Esmaeil and Asgari, Ehsaneddin and McHardy, Alice C. and Mofrad, Mohammad R.K.},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
  title={TripletProt: Deep Representation Learning of Proteins Based On Siamese Networks}, 
  year={2022},
  volume={19},
  number={6},
  pages={3744-3753},
  abstract={Pretrained representations have recently gained attention in various machine learning applications. Nonetheless, the high computational costs associated with training these models have motivated alternative approaches for representation learning. Herein we introduce TripletProt, a new approach for protein representation learning based on the Siamese neural networks. Representation learning of biological entities which capture essential features can alleviate many of the challenges associated with supervised learning in bioinformatics. The most important distinction of our proposed method is relying on the protein-protein interaction (PPI) network. The computational cost of the generated representations for any potential application is significantly lower than comparable methods since the length of the representations is significantly smaller than that in other approaches. TripletProt offers great potentials for the protein informatics tasks and can be widely applied to similar tasks. We evaluate TripletProt comprehensively in protein functional annotation tasks including sub-cellular localization (14 categories) and gene ontology prediction (more than 2000 classes), which are both challenging multi-class, multi-label classification machine learning problems. We compare the performance of TripletProt with the state-of-the-art approaches including a recurrent language model-based approach (i.e., UniRep), as well as a protein-protein interaction (PPI) network and sequence-based method (i.e., DeepGO). Our TripletProt showed an overall improvement of F1 score in the above mentioned comprehensive functional annotation tasks, solely relying on the PPI network. Availability: The source code and datasets are available at https://github.com/EsmaeilNourani/TripletProt.},
  keywords={Proteins;Task analysis;Computational modeling;Training;Protein engineering;Feature extraction;Computational efficiency;Protein representation learning;triplet loss;siamese networks},
  doi={10.1109/TCBB.2021.3108718},
  ISSN={1557-9964},
  month={Nov},}@ARTICLE{10504595,
  author={Wu, Xiang and Wang, Huanhuan and Zhang, Yongting and Zou, Baowen and Hong, Huaqing},
  journal={IEEE Transactions on Learning Technologies}, 
  title={A Tutorial-Generating Method for Autonomous Online Learning}, 
  year={2024},
  volume={17},
  number={},
  pages={1532-1541},
  abstract={Generative artificial intelligence has become the focus of the intelligent education field, especially in the generation of personalized learning resources. Current learning resource generation methods recommend customized courses based on learning styles and interests, improving learning efficiency. However, these methods cannot generate personalized tutorials according to learners’ preferences, nor can they adjust tutorial content as moods or levels of knowledge change. Therefore, this study develops an intelligent tutorial-generating system (Self-GT) for self-aid learning, integrating cognitive computing and generative learning to capture learners’ dynamic preferences. The critical components of Self-GT are the tutorial-generating model based on cyclic deep reinforcement learning (RL) and the multimodal knowledge graph containing complex relationships. Specifically, the proposed RL model dynamically explores learners’ preferences from the temporal dimension, enabling RL agents to express learning behavior characteristics accurately and generate personalized tutorials. Then, relying on the internal self-developed education base and external Internet sources, a multimodal knowledge graph with multiple self-defined relationships is designed to enhance the precision of tutorial generation. Finally, the experimental results indicate that the Self-GT performs well in generating tutorials and has been successfully applied in the generating tutorial for “Hospital Network Architecture Planning and Design.”},
  keywords={Tutorials;Knowledge graphs;Hospitals;Visualization;Task analysis;Sociology;Scalability;Cognitive computing;course tutorial generating;deep reinforcement learning (RL);generative learning;multimodal knowledge graph},
  doi={10.1109/TLT.2024.3390593},
  ISSN={1939-1382},
  month={},}@INPROCEEDINGS{9577534,
  author={Marino, Kenneth and Chen, Xinlei and Parikh, Devi and Gupta, Abhinav and Rohrbach, Marcus},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA}, 
  year={2021},
  volume={},
  number={},
  pages={14106-14116},
  abstract={One of the most challenging question types in VQA is when answering the question requires outside knowledge not present in the image. In this work we study open-domain knowledge, the setting when the knowledge required to answer a question is not given/annotated, neither at training nor test time. We tap into two types of knowledge representations and reasoning. First, implicit knowledge which can be learned effectively from unsupervised language pretraining and supervised training data with transformer-based models. Second, explicit, symbolic knowledge encoded in knowledge bases. Our approach combines both—exploiting the powerful implicit reasoning of transformer models for answer prediction, and integrating symbolic representations from a knowledge graph, while never losing their explicit semantics to an implicit embedding. We combine diverse sources of knowledge to cover the wide variety of knowledge needed to solve knowledge-based questions. We show our approach, KRISP (Knowledge Reasoning with Implicit and Symbolic rePresentations), significantly out-performs state-of-the-art on OK-VQA, the largest available dataset for open-domain knowledge-based VQA. We show with extensive ablations that while our model successfully exploits implicit knowledge reasoning, the symbolic answer module which explicitly connects the knowledge graph to the answer vocabulary is critical to the performance of our method and generalizes to rare answers.1},
  keywords={Training;Vocabulary;Knowledge based systems;Semantics;Training data;Knowledge representation;Predictive models},
  doi={10.1109/CVPR46437.2021.01389},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10753166,
  author={Lotfy, Abdelrahman and Ashraf, Ahmed and Barakat, Mariam and Saleh, Kirollos and Ahmed, Eslam and Sherif, Bassel and Badawy, Ali Nasser and Khoriba, Ghada},
  booktitle={2024 6th Novel Intelligent and Leading Emerging Sciences Conference (NILES)}, 
  title={A Comparative Analysis of Large Language Models for Automated Course Content Generation from Books}, 
  year={2024},
  volume={},
  number={},
  pages={437-442},
  abstract={Large Language Models (LLMs) have emerged as powerful tools for extracting course topics from textbooks in today's fast-paced educational landscape. Additionally, harnessing the potential of Knowledge Graphs to visualize the mutuality among topics enhances the informativeness of the extracted content. This paper presents a comprehensive comparative study that explores and assesses the effectiveness of different LLMs in extracting, identifying, and summarizing course topics within textbooks and generating knowledge graphs to visualize topic interdependencies. Moreover, we present a comprehensive methodology for knowledge graph development, incorporating specialized models, GPT2, Falcon 7B, and Llama-2-7b-chat-hf, fine-tuned with eight book tables of contents. Also, we have used Llama3, Llama3.1, Gemma, and Mistral-Nemo as a zero-shot model. Our findings show that llama3 has achieved the best performance among the zero-shot models in the following constraints: quality of content, correctness, clarity, and overall rating. Also, GPT2- Large excels in generating meaningful content, while GPT2-Base performs efficiently. In addition, challenges in knowledge graph integration were addressed by representing table of content data as knowledge graphs, providing more meaningful insights. This research enhances knowledge representation, demonstrating LLMs' value in knowledge graphs and data balance optimization.},
  keywords={Analytical models;Large language models;Education;Knowledge graphs;Tutorials;Organizations;Data mining;Tuning;Optimization;Context modeling;Large Language Models (LLMs);Knowledge Graphs & Finetuning},
  doi={10.1109/NILES63360.2024.10753166},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10698122,
  author={Prudhvith, Tavva and Swattik, Chakrabarty and Prakash, Selvakumar},
  booktitle={2024 International Conference on Electrical, Computer and Energy Technologies (ICECET}, 
  title={Enhancing Retrieval Augmented Generation Systems with Knowledge Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Knowledge graphs play a pivotal role in information retrieval, yet their effectiveness can be further optimized. This paper introduces a comprehensive approach to enriching knowledge graphs, addressing contemporary challenges in answering user queries. Our methodology integrates key phrase extraction indulged with a custom prompt to create a Knowledge Graph (KG) with every node connected with each other, node embedding generation w.r.t properties, and an autonomous updating agent during the creation of KG and inference. Furthermore, we explore the incorporation of traditional vector search to enhance contextual understanding. Our experiments demonstrate a substantial improvement in accuracy, reaching approximately 96% compared to traditional KG approaches and our creation of KG process. Notably, the hybrid model sometimes outperforms the Retrieval-Augmented Generation (RAG) system, showcasing the efficacy of the integrated approach. This enhancement is attributed to the methodology proposed coupled with the additional context provided by traditional vector search. The results underscore the significance of our approach in delivering more accurate and contextually relevant information, showcasing the potential of this integrated method in advancing knowledge graph systems.},
  keywords={Electric potential;Accuracy;Knowledge graphs;Information retrieval;Vectors;Hybrid power systems;Knowledge Graphs;Key Phrase Extraction;Embeddings;Vector Search;Information Retrieval;GPT4},
  doi={10.1109/ICECET61485.2024.10698122},
  ISSN={},
  month={July},}@INPROCEEDINGS{9534192,
  author={Ranade, Priyanka and Piplai, Aritran and Mittal, Sudip and Joshi, Anupam and Finin, Tim},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Generating Fake Cyber Threat Intelligence Using Transformer-Based Models}, 
  year={2021},
  volume={},
  number={},
  pages={1-9},
  abstract={Cyber-defense systems are being developed to automatically ingest Cyber Threat Intelligence (CTI) that contains semi-structured data and/or text to populate knowledge graphs. A potential risk is that fake CTI can be generated and spread through Open-Source Intelligence (OSINT) communities or on the Web to effect a data poisoning attack on these systems. Adversaries can use fake CTI examples as training input to subvert cyber defense systems, forcing their models to learn incorrect inputs to serve the attackers' malicious needs. In this paper, we show how to automatically generate fake CTI text descriptions using transformers. Given an initial prompt sentence, a public language model like GPT-2 with fine-tuning can generate plausible CTI text that can mislead cyber-defense systems. We use the generated fake CTI text to perform a data poisoning attack on a Cybersecurity Knowledge Graph (CKG) and a cybersecurity corpus. The attack introduced adverse impacts such as returning incorrect reasoning outputs, representation poisoning, and corruption of other dependent AI-based cyber defense systems. We evaluate with traditional approaches and conduct a human evaluation study with cyber-security professionals and threat hunters. Based on the study, professional threat hunters were equally likely to consider our fake generated CTI and authentic CTI as true.},
  keywords={Training;Neural networks;Transformers;Cognition;Computer crime;Open source software;Cybersecurity;Cyber Threat Intelligence;Artificial Intelligence;Data Poisoning Attack},
  doi={10.1109/IJCNN52387.2021.9534192},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9835701,
  author={Zhong, Ziyue and Zhang, Meihui and Fan, Ju and Dou, Chenxiao},
  booktitle={2022 IEEE 38th International Conference on Data Engineering (ICDE)}, 
  title={Semantics Driven Embedding Learning for Effective Entity Alignment}, 
  year={2022},
  volume={},
  number={},
  pages={2127-2140},
  abstract={Knowledge-based data service has become an emerging form of service in the world wide web (WWW). To ensure the service quality, a comprehensive knowledge base has to be constructed. Knowledge base integration is often a primary way to improve the completeness. In this paper, we focus on the fundamental problem in knowledge base integration, i.e., entity alignment (EA). EA has been studied for years. Traditional approaches focus on the symbolic features of entities and propose various similarity measures to identify equivalent entities. With recent development in knowledge graph representation learning, embedding-based entity alignment has emerged, which encodes the entities into vectors according to the semantic or structural information and computes the relatedness of entities based on the vector representation. While embedding-based approaches achieve promising results, we identify some important information that are not well exploited in existing works: 1) The neighboring entities contribute differently in the EA process, and should be carefully assigned the importance in learning the relatedness of entities; 2) The attribute values (especially the long texts) contain rich semantics that can build supplementary associations between entities. To this end, we propose SDEA - a Semantics Driven entity embedding method for Entity Alignment. SDEA consists of two modules, namely attribute embedding and relation embedding. The attribute embedding captures the semantic information from attribute values with a pre-trained transformer-based language model. The relation embedding selectively aggregates the semantic information from neighbors using a GRU model equipped with an attention mechanism. Both attribute embedding and relation embedding are driven by semantics, building bridges between entities. Experimental results show that our method significantly outperforms the state-of-the-art approaches on three benchmarks.},
  keywords={Representation learning;Conferences;Semantics;Knowledge based systems;Buildings;Benchmark testing;World Wide Web;Entity Alignment;Semantics Driven;Transformer;Knowledge Base Integration},
  doi={10.1109/ICDE53745.2022.00205},
  ISSN={2375-026X},
  month={May},}@INPROCEEDINGS{10203925,
  author={Hu, Ziniu and Iscen, Ahmet and Sun, Chen and Wang, Zirui and Chang, Kai-Wei and Sun, Yizhou and Schmid, Cordelia and Ross, David A. and Fathi, Alireza},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Reveal: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory}, 
  year={2023},
  volume={},
  number={},
  pages={23369-23379},
  abstract={In this paper, we propose an end-to-end Retrieval-Augmented Visual Language Model (REVEAL) that learns to encode world knowledge into a large-scale memory, and to retrieve from it to answer knowledge-intensive queries. Reveal consists of four key components: the memory, the encoder, the retriever and the generator. The large-scale memory encodes various sources of multimodal world knowledge (e.g. image-text pairs, question answering pairs, knowledge graph triplets, etc.) via a unified encoder. The retriever finds the most relevant knowledge entries in the memory, and the generator fuses the retrieved knowledge with the input query to produce the output. A key novelty in our approach is that the memory, encoder, retriever and generator are all pre-trained end-to-end on a massive amount of data. Furthermore, our approach can use a diverse set of multimodal knowledge sources, which is shown to result in significant gains. We show that Reveal achieves state-of-the-art results on visual question answering and image captioning. The project page of this work is reveal. github. io.},
  keywords={Visualization;Computer vision;Fuses;Memory management;Knowledge graphs;Generators;Question answering (information retrieval);Vision;language;and reasoning},
  doi={10.1109/CVPR52729.2023.02238},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{9623313,
  author={Latif, Shahid and Agarwal, Shivam and Gottschalk, Simon and Chrosch, Carina and Feit, Felix and Jahn, Johannes and Braun, Tobias and Tchenko, Yanick Christian and Demidova, Elena and Beck, Fabian},
  booktitle={2021 IEEE Visualization Conference (VIS)}, 
  title={Visually Connecting Historical Figures Through Event Knowledge Graphs}, 
  year={2021},
  volume={},
  number={},
  pages={156-160},
  abstract={Knowledge graphs store information about historical figures and their relationships indirectly through shared events. We developed a visualization system, VisKonnect, for analyzing the intertwined lives of historical figures based on the events they participated in. A user’s query is parsed for identifying named entities, and related data is retrieved from an event knowledge graph. While a short textual answer to the query is generated using the GPT-3 language model, various linked visualizations provide context, display additional information related to the query, and allow exploration.},
  keywords={Conferences;Natural languages;Data visualization;Context modeling;Knowledge graphs;general public;question answering;visualization;natural language generation},
  doi={10.1109/VIS49827.2021.9623313},
  ISSN={},
  month={Oct},}@ARTICLE{9770789,
  author={Rony, Md Rashad Al Hasan and Chaudhuri, Debanjan and Usbeck, Ricardo and Lehmann, Jens},
  journal={IEEE Access}, 
  title={Tree-KGQA: An Unsupervised Approach for Question Answering Over Knowledge Graphs}, 
  year={2022},
  volume={10},
  number={},
  pages={50467-50478},
  abstract={Most Knowledge Graph-based Question Answering (KGQA) systems rely on training data to reach their optimal performance. However, acquiring training data for supervised systems is both time-consuming and resource-intensive. To address this, in this paper, we propose Tree-KGQA, an unsupervised KGQA system leveraging pre-trained language models and tree-based algorithms. Entity and relation linking are essential components of any KGQA system. We employ several pre-trained language models in the entity linking task to recognize the entities mentioned in the question and obtain the contextual representation for indexing. Furthermore, for relation linking we incorporate a pre-trained language model previously trained for language inference task. Finally, we introduce a novel algorithm for extracting the answer entities from a KG, where we construct a forest of interpretations and introduce tree-walking and tree disambiguation techniques. Our algorithm uses the linked relation and predicts the tree branches that eventually lead to the potential answer entities. The proposed method achieves 4.5% and 7.1% gains in F1 score in entity linking tasks on LC-QuAD 2.0 and LC-QuAD 2.0 (KBpearl) datasets, respectively, and a 5.4% increase in the relation linking task on LC-QuAD 2.0 (KBpearl). The comprehensive evaluations demonstrate that our unsupervised KGQA approach outperforms other supervised state-of-the-art methods on the WebQSP-WD test set (1.4% increase in F1 score) - without training on the target dataset.},
  keywords={Prediction algorithms;Context modeling;Question answering (information retrieval);Training data;Information retrieval;Knowledge engineering;Knowledge based systems;information retrieval;question answering;entity linking;relation linking;indexing;pre-trained language models},
  doi={10.1109/ACCESS.2022.3173355},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10449869,
  author={Omar, Mussa A.},
  booktitle={2023 IEEE 11th International Conference on Systems and Control (ICSC)}, 
  title={Measurement of ChatGPT Performance in Mapping Natural Language Speficaction into an Entity Relationship Diagram}, 
  year={2023},
  volume={},
  number={},
  pages={530-535},
  abstract={This paper explores the entity relationship diagram, a popular conceptual model used to depict entities, attributes, and relationships graphically. To help with this, we use ChatGPT, a sophisticated language model based on the GPT architecture, which can translate natural language text into an entity relationship diagram. The paper details the process of evaluating how well ChatGPT can perform compared to other state-of-the-art approaches for entity and relationship extraction. Our experimental findings demonstrate the strong ability of ChatGPT to translate natural language text into entity relationship diagrams, which has potential applications for knowledge graph building, data integration, and database schema design. Moreover, it can aid in automating the extraction and organization of information from unstructured text data, thereby simplifying the study of complex systems.},
  keywords={Adaptation models;Natural languages;Machine learning;Companies;Chatbots;Task analysis;Software engineering;entity relationship diagram;ChatGPT;natural language processing},
  doi={10.1109/ICSC58660.2023.10449869},
  ISSN={2379-0067},
  month={Dec},}@INPROCEEDINGS{10650292,
  author={Liu, Ching-Hsuan and Chen, Chih-Ming and Lou, Jing-Kai and Tsai, Ming-Feng and Huang, Jiun-Lang and Wang, Chuan-Ju},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={SARA: Semantic-assisted Reinforced Active Learning for Entity Alignment}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={This paper introduces SARA, a semantic-assisted reinforced active learning framework for enhancing entity alignment (EA) under limited supervision scenarios. SARA addresses the challenges of EA in real-world scenarios, including knowledge graph heterogeneity and limited training ground truth. SARA effectively selects valuable entity pairs with limited labeled data by combining reinforced active learning and semantic information. It utilizes a pair-wise language model based on Sentence-BERT to learn informative name embeddings that capture entity name semantics. These embeddings are combined with structural embeddings and trained using a novel semantic-assisted alignment loss. Extensive experiments on benchmark datasets and a real-world dataset demonstrate the superiority of SARA over existing approaches, particularly in limited labeled data scenarios. The paper also provides insights into fine-tuning strategies, presents ablation studies, and conducts sensitivity analyses to validate the effectiveness of SARA.},
  keywords={Training;Sensitivity analysis;Semantics;Neural networks;Knowledge graphs;Benchmark testing;Data models},
  doi={10.1109/IJCNN60899.2024.10650292},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{9582475,
  author={Mandel, Constantin and Böning, Jannis and Behrendt, Matthias and Albers, Albert},
  booktitle={2021 IEEE International Symposium on Systems Engineering (ISSE)}, 
  title={A Model-Based Systems Engineering Approach to Support Continuous Validation in PGE - Product Generation Engineering}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Increasing customer demands, especially regarding functionality, safety and environmental sustainability, are major drivers of nowadays product development processes. Those increasing demands as well as today’s product development context of distributed interdisciplinary development teams lead to an increasing complexity of systems as well as their respective development processes. MBSE - Model-Based Systems Engineering is regarded as a promising approach to cope with this complexity. MBSE aims at supporting during the whole product lifecycle in system analysis, requirements management, design as well as verification and validation. Especially validation plays a central role in product development as it is the only activity that can ensure customer satisfaction and thus a successful product on the market. However, comprehensive MBSE-approaches to support validation in product development seem to be missing. This paper describes such a MBSE approach to support validation in product development. The approach includes an ontology of terms and their interrelations in the context of validation. The ontology is used to construct viewpoints, views and a modeling framework to structure a system model in the understanding of MBSE. In addition, a modeling method interacting with the constructed views is developed and presented. The presented approach aims at enabling a continuous validation concept, starting in the early phase of PGE - Product Generation Engineering and continuing throughout the entire lifecycle. Furthermore, the approach should support in integrating the development of products and appropriate validation systems, creating a consistent traceability of information throughout the created models. Finally, a specific focus of the approach lies on usability in order to guarantee individual and organizational acceptance. This acceptance is of particular importance to realize a human centered development as it is envisioned in approaches such as ASE - Advanced Systems Engineering.},
  keywords={Requirements management;Green products;Customer satisfaction;Ontologies;Product development;Complexity theory;Safety;MBSE;Validation;Continuous Validation Concept;Modeling Framework;PGE;Product Generation Engineering},
  doi={10.1109/ISSE51541.2021.9582475},
  ISSN={2687-8828},
  month={Sep.},}@ARTICLE{10711243,
  author={Malusare, Aditya and Aggarwal, Vaneet},
  journal={IEEE Transactions on Computational Biology and Bioinformatics}, 
  title={Improving Molecule Generation and Drug Discovery With a Knowledge-Enhanced Generative Model}, 
  year={2025},
  volume={22},
  number={1},
  pages={375-381},
  abstract={Recent advancements in generative models have established state-of-the-art benchmarks in the generation of molecules and novel drug candidates. Despite these successes, a significant gap persists between generative models and the utilization of extensive biomedical knowledge, often systematized within knowledge graphs, whose potential to inform and enhance generative processes has not been realized. In this paper, we present a novel approach that bridges this divide by developing a framework for knowledge-enhanced generative models called KARL. We develop a scalable methodology to extend the functionality of knowledge graphs while preserving semantic integrity, and incorporate this contextual information into a generative framework to guide a diffusion-based model. The integration of knowledge graph embeddings with our generative model furnishes a robust mechanism for producing novel drug candidates possessing specific characteristics while ensuring validity and synthesizability. KARL outperforms state-of-the-art generative models on both unconditional and targeted generation tasks.},
  keywords={Biological system modeling;Drugs;Knowledge graphs;Computational modeling;Biological information theory;Drug discovery;Diseases;Context modeling;Training;Data models;Diffusion models;drug discovery;generative AI;knowledge representation},
  doi={10.1109/TCBB.2024.3477313},
  ISSN={2998-4165},
  month={Jan},}@INPROCEEDINGS{10749735,
  author={Casalicchio, Emiliano and Cotumaccio, Alberto},
  booktitle={2024 IEEE International Conference on Cloud Engineering (IC2E)}, 
  title={AI-CRAS: AI-driven Cloud Service Requirement Analysis and Specification}, 
  year={2024},
  volume={},
  number={},
  pages={11-21},
  abstract={Automated analysis and specification of software requirements expressed in natural language is a challenge addressed by the research community and is becoming a reality thanks to the advances in Artificial Intelligence (AI) and Natural Language Processing (NLP) techniques. While the research community focuses mainly on generic software requirements or specialized solutions for security requirements, we find a gap in the automation of analysis and specification for requirements in the cloud computing domain and the automatic mapping of requirements on actual products offered in the cloud service market. In this research work, we propose AI-CRAS an AI-driven cloud service requirement analysis and specification methodology. The proposed method, which leverages state-of-the-art transformer-based large language model, has been implemented and validated in a real case. Experimental results demonstrate that the model performed well in binary and multi-label classification of requirements (achieving recall/F1-score of $0.96 / 0.92$ and $0.86 / 0.76$, respectively) and mapping requirements into actual cloud services.},
  keywords={Training;Cloud computing;Accuracy;Feature extraction;Transformers;Natural language processing;Software;Vectors;Stakeholders;Testing;cloud computing;cloud migration;cloud services;requirement engineering;artificial intelligence;natural language processing;cloud service broker},
  doi={10.1109/IC2E61754.2024.00009},
  ISSN={2694-0825},
  month={Sep.},}@ARTICLE{9357868,
  author={Gaur, Manas and Faldu, Keyur and Sheth, Amit},
  journal={IEEE Internet Computing}, 
  title={Semantics of the Black-Box: Can Knowledge Graphs Help Make Deep Learning Systems More Interpretable and Explainable?}, 
  year={2021},
  volume={25},
  number={1},
  pages={51-59},
  abstract={The recent series of innovations in deep learning (DL) have shown enormous potential to impact individuals and society, both positively and negatively. DL models utilizing massive computing power and enormous datasets have significantly outperformed prior historical benchmarks on increasingly difficult, well-defined research tasks across technology domains such as computer vision, natural language processing, and human-computer interactions. However, DL's black-box nature and over-reliance on massive amounts of data condensed into labels and dense representations pose challenges for interpretability and explainability. Furthermore, DLs have not proven their ability to effectively utilize relevant domain knowledge critical to human understanding. This aspect was missing in early data-focused approaches and necessitated knowledge-infused learning (K-iL) to incorporate computational knowledge. This article demonstrates how knowledge, provided as a knowledge graph, is incorporated into DL using K-iL. Through examples from natural language processing applications in healthcare and education, we discuss the utility of K-iL towards interpretability and explainability.},
  keywords={Deep learning;Computational modeling;Semantics;Medical services;Natural language processing;Artificial intelligence;Computer vision;Human computer interaction},
  doi={10.1109/MIC.2020.3031769},
  ISSN={1941-0131},
  month={Jan},}@ARTICLE{10234662,
  author={Hu, Linmei and Liu, Zeyi and Zhao, Ziwang and Hou, Lei and Nie, Liqiang and Li, Juanzi},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey of Knowledge Enhanced Pre-Trained Language Models}, 
  year={2024},
  volume={36},
  number={4},
  pages={1413-1430},
  abstract={Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are categorized into KG-based and retrieval-based methods. Finally, we point out some promising future directions of KE-PLMs.},
  keywords={Task analysis;Natural language processing;Training;Taxonomy;Linguistics;Computational modeling;Surveys;Knowledge enhanced pre-trained language models;natural language generation;natural language processing;natural language understanding;pre-trained language models},
  doi={10.1109/TKDE.2023.3310002},
  ISSN={1558-2191},
  month={April},}@ARTICLE{9153759,
  author={Jing, Yun and Zhiwei, Xu and Guanglai, Gao},
  journal={IEEE Access}, 
  title={Context-Driven Image Caption With Global Semantic Relations of the Named Entities}, 
  year={2020},
  volume={8},
  number={},
  pages={143584-143594},
  abstract={Automatic image captioning has achieved a great progress. However, the existing captioning frameworks basically enumerate the objects in the image. The generated captions lack the real-world knowledge about named entities and their relations, such as the relations among famous persons, organizations and buildings. On the contrary, humans interpret images in a specific way by providing real-world knowledge with relations of the aforementioned named entities. To generate human-like captions, we focus on captioning the images of news, which could provide real-world knowledge of the whole story behind the images. Then we propose a novel model that makes captions closer to the human-like description of the image, by leveraging the semantic relevance of the named entities. The named entities are not only extracted from news under the guidance of the image content, but also extended with external knowledge based on the semantic relations. In detail, we propose a sentence correlation analysis algorithm to selectively draw the contextual information from news, and use entity-linking algorithm based on knowledge graph to discover the relations of entities with a global sight. The results of extensive experiments based on real-world dataset which is collected from the news show that our model generates image captions closer to the corresponding real-world captions.},
  keywords={Semantics;Visualization;Task analysis;Computational modeling;Knowledge based systems;Correlation;Organizations;Image caption;named entity;semantic relation},
  doi={10.1109/ACCESS.2020.3013321},
  ISSN={2169-3536},
  month={},}@ARTICLE{9352490,
  author={Zhang, Zhuosheng and Li, Junlong and Zhao, Hai},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Multi-Turn Dialogue Reading Comprehension With Pivot Turns and Knowledge}, 
  year={2021},
  volume={29},
  number={},
  pages={1161-1173},
  abstract={Multi-turn dialogue reading comprehension aims to teach machines to read dialogue contexts and solve tasks such as response selection and answering questions. The major challenges involve noisy history contexts and especial prerequisites of commonsense knowledge that is unseen in the given material. Existing works mainly focus on context and response matching approaches. This work thus makes the first attempt to tackle the above two challenges by extracting substantially important turns as pivot utterances and utilizing external knowledge to enhance the representation of context. We propose a pivot-oriented deep selection model (PoDS) on top of the Transformer-based language models for dialogue comprehension. In detail, our model first picks out the pivot utterances from the conversation history according to the semantic matching with the candidate response or question, if any. Besides, knowledge items related to the dialogue context are extracted from a knowledge graph as external knowledge. Then, the pivot utterances and the external knowledge are combined together with a well-designed mechanism for refining predictions. Experimental results on four dialogue comprehension benchmark tasks show that our proposed model achieves great improvements on baselines. A series of empirical comparisons are conducted to show how our selection strategies and the extra knowledge injection influence the results.},
  keywords={Context modeling;Task analysis;Speech processing;History;Bit error rate;Computational modeling;Benchmark testing;Multi-turn dialogue comprehension;response selection;utterance selection;commonsense modeling},
  doi={10.1109/TASLP.2021.3058616},
  ISSN={2329-9304},
  month={},}@ARTICLE{10328469,
  author={Zhu, Yi and Wang, Ye and Qiang, Jipeng and Wu, Xindong},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Prompt-Learning for Short Text Classification}, 
  year={2024},
  volume={36},
  number={10},
  pages={5328-5339},
  abstract={In the short text, the extremely short length, feature sparsity, and high ambiguity pose huge challenges to classification tasks. Recently, as an effective method for tuning Pre-trained Language Models for specific downstream tasks, prompt-learning has attracted a vast amount of attention and research. The main intuition behind the prompt-learning is to insert the template into the input and convert the tasks into equivalent cloze-style tasks. However, most prompt-learning methods only consider the class name and monotonous strategy for knowledge incorporating in cloze-style prediction, which will inevitably incur omissions and bias in short text classification tasks. In this paper, we propose a short text classification method with prompt-learning. Specifically, the top $M$M concepts related to the entity in the short text are retrieved from the open Knowledge Graph like Probase, these concepts are first selected by the distance with class labels, which takes both the short text itself and the class name into consideration during expanding label word space. Then, we conducted four additional strategies for the integration of the expanded concepts, and the union of these concepts are adopted finally in the verbalizer of prompt-learning. Experimental results show that the obvious improvement is obtained compared with other state-of-the-art methods on five well-known datasets.},
  keywords={Task analysis;Business;Sports;Semantics;Blogs;Companies;Tuning;Prompt-Learning;short text classification},
  doi={10.1109/TKDE.2023.3332787},
  ISSN={1558-2191},
  month={Oct},}@INPROCEEDINGS{9882574,
  author={Sovrano, Francesco and Vitali, Fabio},
  booktitle={2022 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
  title={How to Quantify the Degree of Explainability: Experiments and Practical Implications}, 
  year={2022},
  volume={},
  number={},
  pages={1-9},
  abstract={Explainable AI was born as a pathway to allow humans to explore and understand the inner working of complex systems. Though, establishing what is an explanation and objectively evaluating explainability, are not trivial tasks. With this paper, we present a new model-agnostic metric to measure the Degree of Explainability of (correct) information in an objective way, exploiting a specific theoretical model from Ordinary Language Philosophy called the Achinstein’s Theory of Explanations, implemented with an algorithm relying on deep language models for knowledge graph extraction and information retrieval. In order to understand whether this metric is actually behaving as explainability is expected to, we have devised an experiment on two realistic Explainable AI-based systems for healthcare and finance, using famous AI technology including Artificial Neural Networks and TreeSHAP. The results we obtained suggest that our proposed metric for measuring the Degree of Explainability is robust on several scenarios.},
  keywords={Measurement;Philosophical considerations;Finance;Medical services;Information retrieval;Data mining;Artificial intelligence;Objective Explainability Metric;XAI;Degree of Explainability},
  doi={10.1109/FUZZ-IEEE55066.2022.9882574},
  ISSN={1558-4739},
  month={July},}@ARTICLE{9852225,
  author={Chatterjee, Joyjit and Dethlefs, Nina},
  journal={IEEE Access}, 
  title={Automated Question-Answering for Interactive Decision Support in Operations & Maintenance of Wind Turbines}, 
  year={2022},
  volume={10},
  number={},
  pages={84710-84737},
  abstract={Intelligent question-answering (QA) systems have witnessed increased interest in recent years, particularly in their ability to facilitate information access, data interpretation or decision support. The wind energy sector is one of the most promising sources of renewable energy, yet turbines regularly suffer from failures and operational inconsistencies, leading to downtimes and significant maintenance costs. Addressing these issues requires rapid interpretation of complex and dynamic data patterns under time-critical conditions. In this article, we present a novel approach that leverages interactive, natural language-based decision support for operations & maintenance (O&M) of wind turbines. The proposed interactive QA system allows engineers to pose domain-specific questions in natural language, and provides answers (in natural language) based on the automated retrieval of information on turbine sub-components, their properties and interactions, from a bespoke domain-specific knowledge graph. As data for specific faults is often sparse, we propose the use of paraphrase generation as a way to augment the existing dataset. Our QA system leverages encoder-decoder models to generate Cypher queries to obtain domain-specific facts from the KG database in response to user-posed natural language questions. Experiments with an attention-based sequence-to-sequence (Seq2Seq) model and a transformer show that the transformer accurately predicts up to 89.75% of responses to input questions, outperforming the Seq2Seq model marginally by 0.76%, though being 9.46 times more computationally efficient. The proposed QA system can help support engineers and technicians during O&M to reduce turbine downtime and operational costs, thus improving the reliability of wind energy as a source of renewable energy.},
  keywords={Wind turbines;Natural languages;Question answering (information retrieval);Predictive models;Maintenance engineering;Data models;Transformers;Decision support systems;Knowledge engineering;Wind energy;Deep learning;Decision support;artificial intelligence;interactive systems;wind energy;questionanswering;knowledge graphs;formal language generation;deep learning},
  doi={10.1109/ACCESS.2022.3197167},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9658757,
  author={Upadhyay, Chirayu and Abu-Rasheed, Hasan and Weber, Christian and Fathi, Madjid},
  booktitle={2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Explainable Job-Posting Recommendations Using Knowledge Graphs and Named Entity Recognition}, 
  year={2021},
  volume={},
  number={},
  pages={3291-3296},
  abstract={The growth of online job-posting repositories provided job-seekers with access to a large number of potential jobs. User assessment of recommended jobs becomes especially a tedious and time-consuming task with the overwhelming number of job recommendations. To enhance the job-seeker’s ability to evaluate the suitability of a recommended job, we propose an explainable job recommendation system, which matches the user to the most relevant jobs based on their profile. Then, the system explains to the user why each job-posting has been recommended to them. The proposed system uses a knowledge graph (KG) structure to model job-postings and user profiles in one homogeneous structure. Graph relations between the job-seekers and job-postings are mined through natural language processing (NLP) of the textual content from job-postings and user-profiles. Based on the graph structure itself and a customized named entity classifier, a human-readable explanation is generated for each recommendation and provided to the job-seeker. The explanation includes information about the matching factors that led the system to recommend a certain job-posting to the user. The proposed system is implemented and tested on a sample data-set of user profiles and job-postings from open online repositories. We use BELU and Rouge-L scores to show that the proposed systems generated relevant explanations for recommended jobs.},
  keywords={Conferences;Natural language processing;Task analysis;Cybernetics},
  doi={10.1109/SMC52423.2021.9658757},
  ISSN={2577-1655},
  month={Oct},}@ARTICLE{10177927,
  author={Naseem, Usman and Khushi, Matloob and Dunn, Adam G. and Kim, Jinman},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={K-PathVQA: Knowledge-Aware Multimodal Representation for Pathology Visual Question Answering}, 
  year={2024},
  volume={28},
  number={4},
  pages={1886-1895},
  abstract={Pathology imaging is routinely used to detect the underlying effects and causes of diseases or injuries. Pathology visual question answering (PathVQA) aims to enable computers to answer questions about clinical visual findings from pathology images. Prior work on PathVQA has focused on directly analyzing the image content using conventional pretrained encoders without utilizing relevant external information when the image content is inadequate. In this article, we present a knowledge-driven PathVQA (K-PathVQA), which uses a medical knowledge graph (KG) from a complementary external structured knowledge base to infer answers for the PathVQA task. K-PathVQA improves the question representation with external medical knowledge and then aggregates vision, language, and knowledge embeddings to learn a joint knowledge-image-question representation. Our experiments using a publicly available PathVQA dataset showed that our K-PathVQA outperformed the best baseline method with an increase of 4.15% in accuracy for the overall task, an increase of 4.40% in open-ended question type and an absolute increase of 1.03% in closed-ended question types. Ablation testing shows the impact of each of the contributions. Generalizability of the method is demonstrated with a separate medical VQA dataset.},
  keywords={Visualization;Medical diagnostic imaging;Task analysis;Pathology;Transformers;Bioinformatics;Training;Pathology images;medical visual question answering (MedVQA);multimodal representation},
  doi={10.1109/JBHI.2023.3294249},
  ISSN={2168-2208},
  month={April},}@INPROCEEDINGS{9035170,
  author={Albared, Mohammed and Gallofré Ocaña, Marc and Ghareb, Abdullah and Al-Moslmi, Tareq},
  booktitle={2019 First International Conference of Intelligent Computing and Engineering (ICOICE)}, 
  title={Recent Progress of Named Entity Recognition over the Most Popular Datasets}, 
  year={2019},
  volume={},
  number={},
  pages={1-9},
  abstract={Named entity recognition (NER) has been considered as an initial step for many applications and tasks such as information retrieval and extraction, question answering, topic modelling, open information extraction, knowledge graph construction, and so forth. Therefore, NER has been receiving increasing attention in the research community. Despite the abundant availability of previous studies on NER, few of them have been applied for more than one dataset. Hence, one system might outperform other systems in one dataset and fail to do in another one. The previous NER surveys have mostly focused on reporting the NER systems without providing a clear comparison for all systems proposed for each dataset. In this paper, we will track the NER performance progress for the most commonly used datasets in NER and report the most recent best systems that have been proposed for each dataset during the last few years.},
  keywords={Task analysis;Labeling;Neural networks;Hidden Markov models;Bit error rate;Context modeling;Training;Named entity recognition;Natural language processing;Machine learning;Deep learning},
  doi={10.1109/ICOICE48418.2019.9035170},
  ISSN={},
  month={Dec},}@ARTICLE{9115025,
  author={Quiroz-Mercado, Job Isaias and Barrón-Fernández, Ricardo and Ramírez-Salinas, Marco Antonio},
  journal={IEEE Access}, 
  title={Semantic Similarity Estimation Using Vector Symbolic Architectures}, 
  year={2020},
  volume={8},
  number={},
  pages={109120-109132},
  abstract={For many natural language processing applications, estimating similarity and relatedness between words are key tasks that serve as the basis for classification and generalization. Currently, vector semantic models (VSM) have become a fundamental language modeling tool. VSMs represent words as points in a high-dimensional space and follow the distributional hypothesis of meaning, which assumes that semantic similarity is related to the context. In this paper, we propose a model whose representations are based on the semantic features associated with a concept within the ConceptNet knowledge graph. The proposed model is based on a vector symbolic architecture framework, which defines a set of arithmetic operations to encode the semantic features within a single high-dimensional vector. In addition to word distribution, these vector representations consider several types of information. Moreover, owing to the properties of high-dimensional spaces, they have the additional advantage of being interpretable. We analyze the model's performance on the SimLex-999 dataset, a dataset where commonly used distributional models (e.g., word2vec or GloVe) perform poorly. Our results are similar to those of other hybrid models, and they surpass several state-of-the-art distributional and knowledge-based models.},
  keywords={Semantics;Benchmark testing;Computational modeling;Correlation;Computer architecture;Natural language processing;Estimation;Concept representation;semantic similarity;vector symbolic architectures;word embeddings},
  doi={10.1109/ACCESS.2020.3001765},
  ISSN={2169-3536},
  month={},}@ARTICLE{9758661,
  author={Jiang, Tao and Kang, Fengjian and Guo, Wei and He, Wei and Liu, Lei and Lu, Xudong and Xu, Yonghui and Cui, Lizhen},
  journal={International Journal of Crowd Science}, 
  title={CK-Encoder: Enhanced Language Representation for Sentence Similarity}, 
  year={2022},
  volume={6},
  number={1},
  pages={17-22},
  abstract={In recent years, neural networks have been widely used in natural language processing, especially in sentence similarity modeling. Most of the previous studies focused on the current sentence, ignoring the commonsense knowledge related to the current sentence in the task of sentence similarity modeling. Commonsense knowledge can be remarkably useful for understanding the semantics of sentences. CK-Encoder, which can effectively acquire commonsense knowledge to improve the performance of sentence similarity modeling, is proposed in this paper. Specifically, the model first generates a commonsense knowledge graph of the input sentence and calculates this graph by using the graph convolution network. In addition, CKER, a framework combining CK-Encoder and sentence encoder, is introduced. Experiments on two sentence similarity tasks have demonstrated that CK-Encoder can effectively acquire commonsense knowledge to improve the capability of a model to understand sentences.},
  keywords={Convolution;Semantics;Neural networks;Natural language processing;Task analysis;Commonsense reasoning;CK-Encoder;sentence similarity;commonsense knowledge},
  doi={10.26599/IJCS.2022.9100001},
  ISSN={2398-7294},
  month={April},}@INPROCEEDINGS{10020207,
  author={Ostapuk, Natalia and Difallah, Djellel and Cudré-Mauroux, Philippe},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={ParaGraph: Mapping Wikidata Tail Entities to Wikipedia Paragraphs}, 
  year={2022},
  volume={},
  number={},
  pages={6008-6017},
  abstract={Bridging unstructured data with knowledge bases is an essential task in many problems related to natural language understanding. Traditionally, this task is considered in one direction only: linking entity mentions in a text to their counterpart in a knowledge base (also known as entity linking). In this paper, we propose to tackle this problem from a different angle: linking entities from a knowledge base to paragraphs describing those entities. We argue that such a new perspective can be beneficial to several applications, including information retrieval, knowledge base population, and joint entity and word embedding. We present a transformer-based model, ParaGraph, which, given a Wikidata entity as input, retrieves its corresponding Wikipedia section. To perform this task, ParaGraph first generates an entity summary and compares it to sections to select an initial set of candidates. The candidates are then ranked using additional information from the entity’s textual description and contextual information. Our experimental results show that ParaGraph achieves 87% Hits@10 when ranking Wikipedia sections given a Wikidata entity as input. The obtained results show that ParaGraph can reduce the information gap between Wikipedia-based entities and tail entities and demonstrate the effectiveness of our proposed approach towards linking knowledge graph entities to their text counterparts.},
  keywords={Knowledge based systems;Encyclopedias;Tail;Big Data;Transformers;Internet;Online services;Linked Data;Knowledge Graphs;Entity Linking},
  doi={10.1109/BigData55660.2022.10020207},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9995551,
  author={He, Kai and Mao, Bing and Zhou, Xiangyu and Li, Yufei and Gong, Tieliang and Li, Chen and Wu, Jialun},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Knowledge Enhanced Coreference Resolution via Gated Attention}, 
  year={2022},
  volume={},
  number={},
  pages={2287-2293},
  abstract={Coreference resolution aims at linking all mentions that refer to the same entity, which are widely adopted in many biomedical and bioinformatics tasks, such as biomedical knowledge graph construction and metabolic pathway integration. Many recent studies focus on improving neural model structures. However, we argue that a practical method that integrates commonsense knowledge can further improve coreference resolution performance, because commonsense delivers extra prior knowledge for reasoning and can enhance related representations, rather than naive mention-context occurrence modeling. In this work, we propose an effective method to integrate external commonsense knowledge into a neural coreference resolution model. Specially, a gated attention mechanism is employed in our method to leverage commonsense according to different contexts. By using ConceptNet as the knowledge base in three span-ranking backbone models, the models can yield significant performance gains on used datasets. We also achieve improvements in tasks of long-term mention detection and cross-sentence coreferences after incorporating knowledge.},
  keywords={Biological system modeling;Knowledge based systems;Logic gates;Performance gain;Task analysis;Bioinformatics;Commonsense reasoning;Coreference Resolution;Attention;Knowledge Enhanced},
  doi={10.1109/BIBM55620.2022.9995551},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9669713,
  author={Yang, Songchun and Zheng, Xiangwen and Xiao, Yu and Yang, Yu and Zhao, Dongsheng},
  booktitle={2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Combining Query Reformulation and Re-ranking to Improve Query Expansion in Chinese EMR Retrieval}, 
  year={2021},
  volume={},
  number={},
  pages={2912-2919},
  abstract={The methods of query expansion (QE) have achieved significant performance in information retrieval of electronic medical records (EMR). It is a pity that the direct addition of expansion terms may cause query drift, which decreases the precision of EMR retrieval. To solve the issue, we combine the methods of query reformulation and re-ranking to improve the performance of QE in Chinese EMR retrieval. First, the synonyms and hyponyms are extracted from a Chinese medical knowledge graph as expansion terms, and the weights of expansion terms are calculated with the combination of semantic similarities, category weights, and co-occurrence frequencies. Then the query is reformulated by these expansion terms, and the EMR documents are retrieved by the expanded query. Second, four categories of medical terms in top retrieval results are selected, and the performances of all combinations in re-ranking are tested to select the most proper terms for re-ranking. Then the original retrieval results are re-ranked by these terms. Experiments show that the algorithm can promote the effectiveness of EMR retrieval compared with baselines, which shows that the combination of query reformulation and re-ranking can differentiate expansion terms of different categories and maximize their effects.},
  keywords={Conferences;Semantics;Information retrieval;Electronic medical records;Bioinformatics;information retrieval;query expansion;query reformulation;re-ranking;electronic medical record},
  doi={10.1109/BIBM52615.2021.9669713},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10020399,
  author={Xu, Zihang and Fu, Luoyi and Wang, Xinbing},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={PromptRGD: Prompt Learning with Relation-aware Gradient Denoising for Low-resource Relation Extraction}, 
  year={2022},
  volume={},
  number={},
  pages={1162-1171},
  abstract={Relation extraction is a fundamental task to construct a knowledge graph, which aims to find the relation categories between two entities from a string of text input sequences. Considering the fact that high-quality labeled data is rare, a recent trend is low-resource setting. Existing works either utilize prompt-based learning method to achieve encouraging results for few-shot tasks by determining an appropriate prompt template, or leverage unlabeled data by generating pseudo labels. However, the manual design of the prompt template requires not only labor-intensive but also expert knowledge. Roughly utilizing a semi-supervised method to assign pseudo-label to unlabeled data will suffer from noise accumulation. To tackle these problems, we propose a novel semi-supervised prompt learning framework with relation-aware gradient denoising for low-resource relation extraction(PromptRGD). Firstly, using learnable template words and virtual labels for prompt learning, we introduce entity type and relation prior knowledge into prompt template construction. Secondly, given the relation-aware gradient similarity between labeled and unlabeled data, PromptRGD generates a pseudo label and then improves the quality of pseudo labels on unlabeled samples in a self-training fashion. The main experimental results and a series of analyses prove the effectiveness of PromptRGD.},
  keywords={Learning systems;Computer vision;Noise reduction;Training data;Manuals;Big Data;Market research;low-resource relation extraction;semi-supervised prompt learning;self-training;gradient denoising},
  doi={10.1109/BigData55660.2022.10020399},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10593437,
  author={Samuel, Prithi and Berna, Eugene and Kumar, Arun and Reshmy, A K and Sriraj, Swaroop Kadaba and Saketh, Yanda},
  booktitle={2024 International Conference on Communication, Computer Sciences and Engineering (IC3SE)}, 
  title={A Hybrid Solution for Summarizing Diverse Medical Texts in the Health Domain}, 
  year={2024},
  volume={},
  number={},
  pages={131-136},
  abstract={Since medical text summarization has become essential in facilitating rapid access to vital information for healthcare professionals, this research paper introduces a novel hybrid medical text summarizer that combines both extractive and abstractive summarization techniques while incorporating domain knowledge for the extractive process. Our approach first employs a domain-specific knowledge graph to guide the identification of salient and clinically relevant content from medical literature. This is followed by the application of advanced natural language processing techniques for abstractive summarization, ensuring the generated summaries maintain coherence and readability. We detail the use of the domain-specific knowledge base containing embedder, which captures medical concepts and relationships, enabling the system to discern important information from the input text. The cosine similarity-based ranking algorithm is adapted to prioritize sentences based on their relevance to the domain and their connectivity. The hybrid medical text summarizer is evaluated on a diverse set of medical articles at the same time to value summarizing multiple documents at the same time and compare its performance with existing approaches. Results demonstrate significant improvements in summary quality, as measured by ROUGE scores and human evaluations. Furthermore, we observe that incorporating domain knowledge in the extractive process enhances the overall effectiveness of the summarization system. We achieve a range-topping ROUGE score of a bit more than 0.6 for most of the texts summarized.},
  keywords={Accuracy;Knowledge based systems;Text summarization;Medical services;Knowledge graphs;Information retrieval;Natural language processing;abstractive summarization;extractive summarization;embedder;Text rank},
  doi={10.1109/IC3SE62002.2024.10593437},
  ISSN={},
  month={May},}@INPROCEEDINGS{10455613,
  author={Xu, Zhitong and Duan, Jianyong and He, Li and Wang, Hao and Zhang, Qing},
  booktitle={2023 3rd International Conference on Digital Society and Intelligent Systems (DSInS)}, 
  title={Evidence-driven Entity Relation Matching Enhances Document-level Relation Extraction}, 
  year={2023},
  volume={},
  number={},
  pages={464-472},
  abstract={To address the issues of limited evidence and a single extraction method in Document-level Relation Extraction (DocRE), we propose an evidence self-training approach and a novel DocRE processing method. We introduce two methods to enhance the performance of existing models: (1) utilizing an evidence self-training approach to generate evidence sentences on distantly supervised data and assisting the model’s performance in DocRE through data augmentation. (2) Inspired by Triplet completion in the knowledge graph, we present an entity and predefined relation-matching method, guiding the model to focus on evidence sentences in the document via evidence distribution to enhance the model’s inference and prediction of entity relations. The EdEM model proposed in this paper achieves 63.43 in F1 and 60.31 in Ign F1 on the Dev set of the DocRED dataset, and 62.27 in F1 and 60.26 in Ign F1 on the Test set. Experimental results demonstrate that our improved model outperforms current baseline models. Evidence self-training effectively addresses the issue of limited evidence and guides the model’s focus on the location of evidence sentences through evidence distribution, enhancing the performance of DocRE tasks.},
  keywords={Training;Knowledge graphs;Predictive models;Data augmentation;Data models;Task analysis;Intelligent systems;Relation Extraction;Evidence Sentences;Evidence Self-Training},
  doi={10.1109/DSInS60115.2023.10455613},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10651447,
  author={Gao, Shang and Li, Yanling and Ge, Fengpei and Lin, Min and Yu, Haiqing and Wang, Sukun and Miao, Zhongyi},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={LeGalFormer: A Graph Representation Learning and Transformer-based Approach for Legal Similar Case Retrieval}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={Legal Similar Case Retrieval (LSCR) is a critical application in legal Artificial Intelligence (AI). It involves retrieving the most relevant cases from legal case databases through query cases. Legal cases are semi-structured documents characterized by long text sequences and high specialization. Existing approaches rely on pre-trained language models for retrieval. However, these methods are constrained by the length of text input, preventing them from fully comprehending cases, which results in poor retrieval performance. Knowledge Graph (KG) are a type of graph structure data with dense knowledge and clear logic, which can represent the criminal processes and relationships among characters within legal cases. Currently, the mainstream approach for handling KG remains Graph Neural Networks (GNNs). However, these methods are limited by message passing and are prone to over-smoothing problems in the process of aggregating node features. To address these issues, we propose a Legal similar case retrieval model that combines Graph representation learning with the Transformer, called LeGalFormer. Three encoding methods are introduced to incorporate the structural information of the graph into the Transformer architecture. We evaluate the model on a real legal dataset, and the experimental results show that LeGalFormer significantly enhances the model's understanding capacity and achieves state-of-the-art performance.},
  keywords={Representation learning;Training;Law;Message passing;Transformers;Encoding;Graph neural networks;information retrieval;legal similar case retrieval;graph transformer;deep learning},
  doi={10.1109/IJCNN60899.2024.10651447},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10105766,
  author={Peng, Da and Pei, Zhongmin and Mo, Delin},
  booktitle={2023 3rd International Conference on Neural Networks, Information and Communication Engineering (NNICE)}, 
  title={MTL-JER: Meta-Transfer Learning for Low-Resource Joint Entity and Relation Extraction}, 
  year={2023},
  volume={},
  number={},
  pages={78-83},
  abstract={Joint entity and relation extraction has achieved impressive advances in NLP, such as document understanding and knowledge graph construction. The typical methods for entity and relation extraction typically break down the joint task into several smaller components or stages for ease of implementation, but this leads to a loss of the interconnected knowledge in the triple. Hence, we propose to model the triple in one module jointly. Furthermore, the labeling of a joint entity and relation extraction tasks is costly and domain-specific; therefore, it is important to improve its performance on low-resource data and domain adaption. To address this issue, we suggest using two sources that are rich in information, namely pre-trained models on large data and multi-domain text corpora. Pretraining allows us to provide the model with the fundamental ability to perform joint entity and relationship extraction. Second, through meta-learning on multi-domain text, we can improve the model's generalization capabilities, enabling it to perform well even with limited data. We present MTL-JER, a Meta-Transfer Learning method for Joint Entity and Relation Extraction in low-resource settings in this paper. Using exhaustive experiments on five datasets, we prove that our model obtains optimal results.},
  keywords={Adaptation models;Training data;Knowledge graphs;Artificial neural networks;Data models;Data mining;Labeling;Joint entity and relation extraction;Meta-Learning;Low-resource},
  doi={10.1109/NNICE58320.2023.10105766},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{8992818,
  author={Lindén, Johanne and Wang, Xutao and Forsström, Stefan and Zhang, Tingting},
  booktitle={2019 8th International Congress on Advanced Applied Informatics (IIAI-AAI)}, 
  title={Bilingual Auto-Categorization Comparison of Two LSTM Text Classifiers}, 
  year={2019},
  volume={},
  number={},
  pages={599-604},
  abstract={Multi linguistic problems such as auto-categorization is not an easy task. It is possible to train different models for each language, another way to do auto-categorization is to build the model in one base language and use automatic translation from other languages to that base language. Different languages have a bias to a language specific grammar and syntax and will therefore pose problems to be expressed in other languages. Translating from one language into a non-verbal language could potentially have a positive impact of the categorization results. A non-verbal language could for example be pure information in form of a knowledge graph relation extraction from the text. In this article a comparison is conducted between Chinese and Swedish languages. Two categorization models are developed and validated on each dataset. The purpose is to make an auto-categorization model that works for n'importe quel langage. One model is built upon LSTM and optimized for Swedish and the other is an improved Bidirectional-LSTM Convolution model optimized for Chinese. The improved algorithm is trained on both languages and compared with the LSTM algorithm. The Bidirectional-LSTM algorithm performs approximately 20% units better than the LSTM algorithm, which is significant.},
  keywords={neural network;chinese dataset;memory cell;convolutional neural network;feature map;chinese language;swedish dataset;auto categorization;information system;natural language processing;news article},
  doi={10.1109/IIAI-AAI.2019.00127},
  ISSN={},
  month={July},}@INPROCEEDINGS{10484052,
  author={Sahu, Pragya Paramita and Raut, Abhishek and Samant, Jagdish Singh and Gorijala, Mahesh and Lakshminarayanan, Vignesh and Bhaskar, Pinaki},
  booktitle={2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={POP-VQA – Privacy preserving, On-device, Personalized Visual Question Answering}, 
  year={2024},
  volume={},
  number={},
  pages={8455-8464},
  abstract={The next generation of device smartness needs to go beyond being able to understand basic user commands. As our systems become more efficient, they need to be taught to understand user interactions and intents from all possible input modalities. This is where the recent advent of large scale multi-modal models can form the foundation for next-gen technologies. However, the true power of such interactive systems can only be realized with privacy conserving personalization. In this paper, we propose an on-device visual question answering system that generates personalized answers using on-device user knowledge graph. These systems have the potential to serve as a fundamental ground-work for the development of genuinely intelligent and tailored assistants, targeted specifically to the needs and preferences of each individual. We validate our model performance on both in-realm, public datasets and personal user data. Our results show consistent performance increase across both tasks, with an absolute improvement of ≈36% with KVQA data-set on 1-hop inferences and ≈6% improvement on user personal data. We also conduct and showcase user-study results to validate our hypothesis of the need and relevance of proposed system.},
  keywords={Training;Visualization;Privacy;Biological system modeling;Computational modeling;System performance;Optical character recognition;Applications;Smartphones / end user devices;Algorithms;Generative models for image;video;3D;etc.;Algorithms;Vision + language and/or other modalities},
  doi={10.1109/WACV57701.2024.00828},
  ISSN={2642-9381},
  month={Jan},}@INPROCEEDINGS{10488237,
  author={Lim, Chae-Gyun and Choi, Ho-Jin and Mswahili, Medard Edmund and Ndomba, Goodwill Erasmo and Jeong, Young-Seob},
  booktitle={2024 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Machine Feeling by Knowledge Acquisition with Emotion Map}, 
  year={2024},
  volume={},
  number={},
  pages={90-96},
  abstract={Artificial intelligent systems have been developed by many industries and academic institutes, and there are also many recent studies about emotion or sentiment. The previous studies about the emotion commonly treated the emotion as knowledge trainable from emotion-annotated data, and focused on emotion perception or expression but not emotion generation. In this paper, we design a graph attention network (GAT) for emotion generation based on the assumption that emotion is intrinsic and propagates as more knowledge is acquired. We represent the knowledge as a graph, and formulate that the knowledge acquisition as an expansion of the knowledge graph. The graph gets larger as time-step goes by, and the GAT learns from the time-series graph data. We simulate this knowledge acquisition based on the assumption that emotion propagates to the newly acquired knowledge. Interestingly, the simulation results exhibited behaviors that are consistent with previous findings of psychiatry. We believe our study will contribute to development of human-like emotional agents that have its own unique emotion about what it experienced or learned.},
  keywords={Industries;Knowledge acquisition;Simulation;Scalability;Psychology;Medical services;Knowledge graphs;graph attention networks;machine feeling;time-series graph data;emotion map},
  doi={10.1109/BigComp60711.2024.00023},
  ISSN={2375-9356},
  month={Feb},}@INPROCEEDINGS{10479333,
  author={Piat, Guilhem and Semmar, Nasredine and Tourille, Julien and Allauzen, Alexandre and Essafi, Hassane},
  booktitle={2023 20th ACS/IEEE International Conference on Computer Systems and Applications (AICCSA)}, 
  title={What does KnowBert-UMLS forget?}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Integrating a source of structured prior knowledge, such as a knowledge graph, into transformer-based language models is an increasingly popular method for increasing data efficiency and adapting them to a target domain. However, most methods for integrating structured knowledge into language models require additional training in order to adapt the model to the non-textual modality. This process typically leads to some amount of catastrophic forgetting on the general domain. KnowBert is one such knowledge integration method which can incorporate information from a variety of knowledge graphs to enhance the capabilities of transformer-based language models such as BERT. We conduct a qualitative analysis of the results of KnowBert-UMLS, a biomedically specialized KnowBert model, on a variety of linguistic tasks. Our results reveal that its increased understanding of biomedical concepts comes at the cost, specifically, of general common-sense knowledge and understanding of casual speech.},
  keywords={Training;Adaptation models;Costs;Biological system modeling;Knowledge graphs;Linguistics;Transformers;Domain Adaptation;Knowledge based systems;Catastrophic Forgetting;Machine learning;Biomedical informatics},
  doi={10.1109/AICCSA59173.2023.10479333},
  ISSN={2161-5330},
  month={Dec},}
@INPROCEEDINGS{10476299,
  author={Zhai, Guohao and Pei, Songwen},
  booktitle={2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS)}, 
  title={Knowledge Enhancement and Feature Purification for Single-stage Joint Entity and Relation Extraction}, 
  year={2023},
  volume={},
  number={},
  pages={1733-1740},
  abstract={Joint entity and relation extraction aim to achieve named entity recognition and relation extraction in unstructured text. We use the form of triples (subject, relation, object) to describe entity and relation. Joint entity and relation extraction play an important role in knowledge graph construction, question and answering, data analysis, and other natural language processing domains. Most of the existing works suffered from insufficient interaction between entity features and relation features due to the extraction order. Moreover, there is a prevalence of heterogeneous representations of entities and relations. They not only increase the amount of model design and the complexity of training, but also lead to the problem of exposure bias due to the extraction order. Therefore, in this paper, we propose KFRel, where we enhance the entity and relation representations by encoding text and relation. Then, the features are fused to enhance the entity and relation representations, and adopt a feature purification module, where the features are enhanced by a feature purification module that removes feature information irrelevant to the joint entity and relation extraction while retaining feature information of relevance. In addition, we adopt a single-stage joint entity and relation extraction module to address the issue of overlapping triples. This module aims to help improve the effectiveness of joint entity and relation extraction. Comprehensive experiments are conducted on two widely-used datasets, and the experimental results demonstrate that the proposed method is effective and outperforms the state-of-the-art baselines.},
  keywords={Training;Data analysis;Purification;Semantics;Focusing;Knowledge graphs;Feature extraction;Joint Entity and Relation Extraction;Knowledge Enhancement;Feature Purification},
  doi={10.1109/ICPADS60453.2023.00241},
  ISSN={2690-5965},
  month={Dec},}@INPROCEEDINGS{10381546,
  author={Yao, Yutong and Potikas, Petros and Potika, Katerina},
  booktitle={2023 IEEE Sixth International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)}, 
  title={Knowledge Graphs for Textbooks: Extraction and Completion Techniques}, 
  year={2023},
  volume={},
  number={},
  pages={38-45},
  abstract={This paper aims to apply knowledge graph construction techniques to textbooks, explicitly focusing on the challenge of the absence of domain-specific schema for each textbook. Various entity and relation extraction models are utilized to capture logical and semantic information related to the textbook’s topic. These models include a Text-Encoding-Initiative (TEI) model to extract hierarchical concepts, spaCy Natural Language Processing (NLP), and Google Cloud Natural Language to extract semantic information from the main textual content. The study includes a case study on a cloud computing textbook, where each approach is evaluated and analyzed. Ultimately, the goal is to create knowledge graphs of textbooks, enabling the completion task of predicting missing entities or relations in a low-dimensional space.},
  keywords={Knowledge engineering;Cloud computing;Face recognition;Computational modeling;Semantics;Pipelines;Knowledge graphs;Knowledge Graphs;Natural Language Processing;Textbook analysis;Entity recognition;Relation extraction;Domain-enrichment},
  doi={10.1109/AIKE59827.2023.00014},
  ISSN={2831-7203},
  month={Sep.},}@INPROCEEDINGS{10447566,
  author={Yang, Caihua and Bao, Jianzhu and Liang, Bin and Xu, RuiFeng},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Enhancing Argumentative Relation Classification by Multi-Granularity Retrieval and Heterogeneous Graph Reasoning}, 
  year={2024},
  volume={},
  number={},
  pages={12772-12776},
  abstract={Argumentative relation classification (ARC) aims to identify the relation between arguments. Previous methods that employ structured knowledge graphs to tackle the ARC task have achieved promising results. However, the prerequisite for structured knowledge to function is that the knowledge includes the topics of arguments. In practice, the topics of arguments are constantly emerging, making it impractical to construct a structured knowledge graph that contains all potential topics in advance. To address this issue, we investigate ARC from a novel perspective by utilizing unstructured knowledge to enhance the learning of ARC, where useful information for topics and arguments could be flexibly obtained from unstructured knowledge. Specifically, to retrieve diverse and comprehensive knowledge for topics and arguments, we first propose a multi-granularity retrieval method tailored for ARC, which acquires unstructured knowledge by dense retrieval at three levels of granularity: the concept level, the concept relation level, and the argument level. Further, we introduce a Knowledge-aware Heterogeneous Graph Reasoner (KHGR), which enables better utilization of retrieved knowledge to facilitate ARC. Extensive experiments on three publicly available datasets verify the superiority of our model compared with several state-of-the-art baselines. Further analysis shows that our method yields more significant benefits in low-resource scenarios.},
  keywords={Knowledge graphs;Signal processing;Benchmark testing;Cognition;Acoustics;Task analysis;Speech processing;argumentative relation classification;unstructured knowledge retrieval;heterogeneous graph reasoning},
  doi={10.1109/ICASSP48485.2024.10447566},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10761764,
  author={Gan, Lu and Wang, Peng and Zhang, Di and Cao, Yiqun and Zhou, Liwan and Zhang, Yongjun},
  booktitle={2024 IEEE/IAS Industrial and Commercial Power System Asia (I&CPS Asia)}, 
  title={Application Analysis and Prospect of GPT-enabled Digital Transformation of Power Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The rapid development of Generative Pre-trained Transformer (GPT) models brings both opportunities and challenges to power system digital and intelligence transformation. Starting with the summarization of the concept and technical architecture of GPT models, four kinds of potential application scenarios in new power systems related to GPT models are presented. Then, the integration of GPT models with several typical digital technologies in new power systems are reviewed, such as computer vision, human-computer interaction, knowledge graph, Internet of Things, and Blockchain. Finally, the challenge and development direction of GPT models for new power systems are prospected. The comprehensive analysis could provide theoretical and application guidance for the development of GPT-enabled models in new power systems.},
  keywords={Human computer interaction;Analytical models;Computer vision;Computational modeling;Knowledge graphs;Predictive models;Data models;Power systems;Blockchains;Internet of Things;new power system;digital transformation;large language model;GPT model;application scenarios},
  doi={10.1109/ICPSAsia61913.2024.10761764},
  ISSN={},
  month={July},}@ARTICLE{9237126,
  author={Eckhart, Matthias and Ekelhart, Andreas and Weippl, Edgar},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={Automated Security Risk Identification Using AutomationML-Based Engineering Data}, 
  year={2022},
  volume={19},
  number={3},
  pages={1655-1672},
  abstract={Systems integrators and vendors of industrial components need to establish a security-by-design approach, which includes the assessment and subsequent treatment of security risks. However, conducting security risk assessments along the engineering process is a costly and labor-intensive endeavor due to the complexity of the system(s) under consideration and the lack of automated methods. This, in turn, hampers the ability of security analysts to assess risks pertaining to cyber-physical systems (CPSs) in an efficient manner. In this work, we propose a method that automatically identifies security risks based on the CPS's data representation, which exists within engineering artifacts. To lay the foundation for our method, we present security-focused semantics for the engineering data exchange format AutomationML (AML). These semantics enable the reuse of security-relevant know-how in AML artifacts by means of a formal knowledge representation, modeled with a security-enriched ontology. Our method is capable of automating the identification of security risk sources and potential consequences in order to construct cyber-physical attack graphs that capture the paths adversaries may take. We demonstrate the benefits of the proposed method through a case study and an open-source prototypical implementation. Finally, we prove that our solution is scalable by conducting a rigorous performance evaluation.},
  keywords={Security;IEC Standards;Risk management;Topology;Semantics;Data models;Knowledge engineering;Cyber-physical systems;information security;AutomationML;security modeling;security risk assessment;industrial control systems;IEC 62443},
  doi={10.1109/TDSC.2020.3033150},
  ISSN={1941-0018},
  month={May},}@INPROCEEDINGS{8088273,
  author={Mordecai, Yaniv and Dori, Dov},
  booktitle={2017 IEEE International Systems Engineering Symposium (ISSE)}, 
  title={Model-based requirements engineering: Architecting for system requirements with stakeholders in mind}, 
  year={2017},
  volume={},
  number={},
  pages={1-8},
  abstract={Specifying system requirements (SysReqs) is a critical activity in complex systems development. The SysReqs and emerging architecture are constructed through gradual and iterative transition from the problem domain and operational stakeholder requirements to the conceptual solution domain. They later constitute the basis for functional requirements elaborating, concept formation, technology selection, function-to-form allocation, and asset utilization. Only rarely can stakeholder requirements (SHRs) readily translate to SysReqs. Systems engineers must therefore elicit, analyze, and evolve the SysReqs, as these will radically affect the system's performance, robustness, endurance, and appeal. Model-Based Systems Engineering (MBSE) provides a framework for effective and consistent systems engineering and architecting. MBSE relies on modeling languages, such as Object-Process Methodology (OPM). OPM is a holistic MBSE paradigm and language for complex systems and processes, standardized as ISO 19450, which relies on the principle of minimal universal ontology. In this paper, we propose a model-based requirement engineering (MBRE) approach to facilitate the transition from SHRs to SysReqs, and from SysReqs to system architecture specification. We demonstrate the applicability of this framework in architecting a robotic baggage loading system for a leading international airport.},
  keywords={Modeling;Stakeholders;Unified modeling language;Requirements engineering;Standards;Complex systems;Model-Based Systems Engineering;Object-Process Methodology;Requirements Engineering;Systems Architecting},
  doi={10.1109/SysEng.2017.8088273},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8241499,
  author={Wally, Bernhard and Huemer, Christian and Mazak, Alexandra},
  booktitle={2017 IEEE 10th Conference on Service-Oriented Computing and Applications (SOCA)}, 
  title={Aligning Business Services with Production Services: The Case of REA and ISA-95}, 
  year={2017},
  volume={},
  number={},
  pages={9-17},
  abstract={Industrie 4.0 aims at flexible production networks that require horizontal integration across companies. Evidently, any production related information exchanged in the network must be vertically forwarded to the corresponding service endpoints of the local production system. Accordingly, there is a need to align information that flows between companies and within each company. The Resource-Event-Agent (REA) business ontology describes a metamodel for internal business activities (e.g., production) and for inter-organizational exchange constellations on the enterprise resource planning (ERP) level. ISA-95 is a series of standards targeting the integration of enterprise control systems on the interface between ERP systems and manufacturing execution systems. Consequently, we align elements of REA and ISA-95 and define conversion rules for the transformation of elements from one system to the other. By interleaving the semantics of both standards, we formally strengthen the links between the services of the business level and the production level, and support multi-system adaptation in flexible production environments.},
  keywords={Production;Manufacturing;Unified modeling language;IEC Standards;Companies;REA;ISA-95;Metamodel Alignment},
  doi={10.1109/SOCA.2017.10},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9534250,
  author={Choudhary, Rishabh and Doboli, Simona and Minai, Ali A.},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={A Comparative Study of Methods for Visualizable Semantic Embedding of Small Text Corpora}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Text embedding has recently emerged as a very useful and successful method for semantic representation. Following initial word-level embedding methods such as Latent Semantic Analysis (LSA) and topic-based bag-of-words approaches like Latent Dirichlet Allocation (LDA), the focus has turned to language models and text encoders implemented as neural networks - ranging from word-level models to those embedding whole documents. The distinctive feature of these models is their ability to infer semantic spaces at all levels based purely on data, with no need for complexities such as syntactic analysis or ontology building. Many of these models are available pre-trained on enormous amounts of data, providing downstream applications with general-purpose semantic spaces. In particular, embedding models at the sentence level or higher are most useful in applications because the meaning of text only becomes clear at that level. Most text embedding methods produce text embeddings in high-dimensional spaces, with a dimensionality ranging from a few hundred to thousands. However, it is often useful to visualize semantic spaces in very low dimension, which requires the use of dimensionality reduction methods. It is not clear what language models and what method of dimensionality reduction would work well in these cases. In this paper, we compare four text embedding methods in combination with three methods of dimensionality reduction to map three related real-world datasets comprising textual descriptions of items in a particular domain (sports) to a 2-dimensional semantic visualization space. The results provide several insights into the utility of these methods for data of this type.},
  keywords={Dimensionality reduction;Training;Analytical models;Visualization;Semantics;Neural networks;Bit error rate;semantic spaces;text embedding;language models;semantic visualization},
  doi={10.1109/IJCNN52387.2021.9534250},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{8501495,
  author={Rabinia, Amin and Ghanavati, Sepideh},
  booktitle={2018 IEEE 8th International Model-Driven Requirements Engineering Workshop (MoDRE)}, 
  title={The FOL-Based Legal-GRL (FLG) Framework: Towards an Automated Goal Modeling Approach for Regulations}, 
  year={2018},
  volume={},
  number={},
  pages={58-67},
  abstract={In recent years, several goal modeling approaches have been used and extended to capture the complexity of legal requirements and help modeling them in notations familiar to the requirements engineers and analysts. Legal-GRL, which is an extension of the Goal-oriented Requirements Language (GRL), is used for modeling and analyzing legal requirements. However, creating Legal-GRL models is still a manual process, which limits its effectiveness and scalability. In this paper, we propose a new goal modeling framework based on GRL to facilitate the automation of the legal requirements modeling process. Our FOL-based Legal-GRL (FLG) framework uses a legal ontology, which entails a modal theory and First-order Logic (FOL) approach, for the purpose of extraction, refinement, and representation of legal requirements. Our FLG framework consists of a database design and a set of methods for automating the modeling process. We evaluate our work by modeling several statements from HIPAA, PHIPA, the EU GDPR and EU-US Privacy Shield.},
  keywords={Law;Ontologies;Analytical models;Complexity theory;Computational modeling;Databases;Goal Modeling, Legal Requirements, First Order Logic, GRL},
  doi={10.1109/MoDRE.2018.00014},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10309534,
  author={Nanwani, Laksh and Agarwal, Anmol and Jain, Kanishk and Prabhakar, Raghav and Monis, Aaron and Mathur, Aditya and Jatavallabhula, Krishna Murthy and Abdul Hafez, A. H. and Gandhi, Vineet and Krishna, K. Madhava},
  booktitle={2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)}, 
  title={Instance-Level Semantic Maps for Vision Language Navigation}, 
  year={2023},
  volume={},
  number={},
  pages={507-512},
  abstract={Humans have a natural ability to perform semantic associations with the surrounding objects in the environment. This allows them to create a mental map of the environment, allowing them to navigate on-demand when given linguistic instructions. A natural goal in Vision Language Navigation (VLN) research is to impart autonomous agents with similar capabilities. Recent works take a step towards this goal by creating a semantic spatial map representation of the environment without any labeled data. However, their representations are limited for practical applicability as they do not distinguish between different instances of the same object. In this work, we address this limitation by integrating instance-level information into spatial map representation using a community detection algorithm and utilizing word ontology learned by large language models (LLMs) to perform open-set semantic associations in the mapping representation. The resulting map representation improves the navigation performance by two-fold (233%) on realistic language commands with instance-specific descriptions compared to the baseline. We validate the practicality and effectiveness of our approach through extensive qualitative and quantitative experiments.},
  keywords={Measurement;Visualization;Three-dimensional displays;Navigation;Semantics;Linguistics;Ontologies},
  doi={10.1109/RO-MAN57019.2023.10309534},
  ISSN={1944-9437},
  month={Aug},}@INPROCEEDINGS{8945629,
  author={Daneth, Horn and Ali, Nazakat and Hong, Jang-Eui},
  booktitle={2019 26th Asia-Pacific Software Engineering Conference (APSEC)}, 
  title={Automatic Identifying Interaction Components in Collaborative Cyber-Physical Systems}, 
  year={2019},
  volume={},
  number={},
  pages={197-203},
  abstract={Due to diverse set of heterogeneous computing devices communicating with one another and fusing with physical components in Cyber-Physical Systems, software engineers may use different tools and/or modeling languages to formally describe or verify the system properties. As a result, the integration of these diverse constituents poses key challenges such as task for identifying interactions of components to be synthesized for a function in the systems. Although existing studies such as ontology and integration semantic languages have been used for specifying interactions of components in a Cyber-Physical System, these are still not applicable to discover the component interactions in collaborative Cyber-Physical Systems. It is due to the fact that functionalities of Cyber-Physical Systems are generally realized through interactions among multiple systems in a collaborative environment. This paper proposes a model interaction language, CyPhyML+ which can identify component interactions of realized functions in collaborative Cyber-Physical Systems. We show the proposed approach validity and applicability via an Automatic Incident Detection System.},
  keywords={Safety;Finite element analysis;Collaboration;Cyber-physical systems;Computational modeling;Software;Logic gates;Model Interaction Language, Component Interactions , Cyber-Physical Systems},
  doi={10.1109/APSEC48747.2019.00035},
  ISSN={2640-0715},
  month={Dec},}@INPROCEEDINGS{9679873,
  author={Sakhrani, Harsh and Parekh, Saloni and Ratadiya, Pratik},
  booktitle={2021 International Conference on Data Mining Workshops (ICDMW)}, 
  title={Transformer-based Hierarchical Encoder for Document Classification}, 
  year={2021},
  volume={},
  number={},
  pages={852-858},
  abstract={Document Classification has a wide range of applications in various domains like Ontology Mapping, Sentiment Analysis, Topic Categorization and Document Clustering, to mention a few. Unlike Text Classification, Document Classification works with longer sequences that typically contain multiple paragraphs. Previous approaches for this task have achieved promising results, but have often relied on complex recurrence mechanisms that are expensive and time-consuming in nature. Recently, self-attention based models like Transformers and BERT have achieved state-of-the-art performance on several Natural Language Understanding (NLU) tasks, but owing to the quadratic computational complexity of the self-attention mechanism with respect to the input sequence length, these approaches are generally applied to shorter text sequences. In this paper, we address this issue, by proposing a new Transformer-based Hierarchical Encoder approach for the Document Classification task. The hierarchical framework we adopt helps us extend the self-attention mechanism to long-form text modelling thereby reducing the complexity considerably. We use the Bidirectional Transformer Encoder (BTE) at the sentence-level to generate a fixed-size sentence embedding for each sentence in the document. A document-level Transformer Encoder is then used to model the global document context and learn the inter-sentence dependencies. We also carry out experiments with the BTE in a feature-extraction and a fine-tuning setup, allowing us to evaluate the trade-off between computation power and accuracy. Furthermore, we also conduct ablation experiments, and evaluate the impact of different pre-training strategies on the overall performance. Experimental results demonstrate that our proposed model achieves state-of-the-art performance on two standard benchmark datasets.},
  keywords={Training;Sentiment analysis;Computational modeling;Transfer learning;Text categorization;Natural languages;Ontologies;Transformer;Self-attention;Document Classification},
  doi={10.1109/ICDMW53433.2021.00109},
  ISSN={2375-9259},
  month={Dec},}@INPROCEEDINGS{9582510,
  author={Odukoya, Kofoworola Adebowale and Whitfield, Robert Ian and Hay, Laura and Harrison, Neil and Robb, Malcolm},
  booktitle={2021 IEEE International Symposium on Systems Engineering (ISSE)}, 
  title={An Architectural Description For The Application Of Mbse In Complex Systems}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={The design of a complex warship is a multidisciplinary effort which often encounters major challenges, particularly with respect to integration across interfaces in the System of Systems (SoS). In principle, the goal of Model Based Systems Engineering (MBSE) with respect to system design is to provide a means of capturing and communicating the system design in a structured, consistent, and coherent fashion; that can be easily assessed by engineering teams and quickly analysed using queries and toolsets. The focus of this paper is to investigate the potential to achieve a consistent description, identify a viable methodology that minimises mismatch in requirements and to avoid an extended design lifecycle. This study highlights the need to develop a generic Architectural Description (AD) that is based on a common ontology which would clearly define the fundamental tenets of applying state-of-the-art Architectural Frameworks (AFs) in naval ship design. An investigation on the effectiveness and accuracy of a graph-based approach is needed to assess whether it is possible to create a ‘Rosetta stone’ for AFs, which links any two or more different model viewpoints in different AF’s using the approach.},
  keywords={Analytical models;Ontologies;Stakeholders;Marine vehicles;Complex systems;System analysis and design;System of systems;Systems architecture;System of systems;Complex systems1},
  doi={10.1109/ISSE51541.2021.9582510},
  ISSN={2687-8828},
  month={Sep.},}@INPROCEEDINGS{10020339,
  author={Wullschleger, Pascal and Lionetti, Simone and Daly, Donnacha and Volpe, Francesca and Caro, Grégoire},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Auto-Regressive Self-Attention Models for Diagnosis Prediction on Electronic Health Records}, 
  year={2022},
  volume={},
  number={},
  pages={1950-1956},
  abstract={Insufficient data and data lacking the diversity to represent the general public is a common challenge when modelling diagnosis prediction. We consider a much larger and more diverse database of commercial Electronic Health Records than what is prevalent in the literature. We formulate a simplified version of diagnosis prediction that focuses on major developments in medical histories of patients. To this end, we leverage Auto-Regressive Self-Attention models that have seen promising applications in language modelling and extend them to incorporate ontological representations of medical codes. Additionally, we include time-intervals between diagnoses into the attention calculation. We evaluate models and baselines at different levels of diagnostic granularity and our results suggest that using very detailed clinical classifications does not significantly degrade performance, possibly allowing their use in practice. Our model outperforms all baselines and we suggest that leveraging the ontology for generating diagnosis representations is mostly helpful for rare diagnoses.},
  keywords={Codes;Databases;Predictive models;Big Data;Ontologies;Data models;History;Electronic Health Record;Transformer;Ontological Representation;Diagnosis Prediction},
  doi={10.1109/BigData55660.2022.10020339},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10651359,
  author={Liu, Xu and Chen, Xinming and Zhu, Yangfu and Wu, Bin},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Prompt-Enhanced Prototype Framework for Few-shot Event Detection}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Few-shot event detection (ED) aims at identifying and typing event mentions from text with limited annotations. Most existing methods for few-shot ED use event ontology and related knowledge to construct prototypes and fail to fully leverage the rich knowledge of pre-trained language models (PLMs) which could help improve the representation of prototypes. Motivated by this, we propose an prompt-enhanced prototype framework which combines prototype and prompt for few-shot ED. Considering the scarcity of labeled data, we also introduce contrastive learning to enrich prototypes. Specifically, we use heuristic rules to align FrameNet with annotated data to get corresponding prompts for each event and convert them into prompt prototype. We then leverage contrastive learning to aggregate event mentions into prototypes and maintain these prototypes for few-shot ED. Furthermore, We explore diverse prompt formats for representing prompt prototypes and introduce a more comprehensive lexical prompt which improves the performance of few-shot ED. We conduct extensive experiments on the MAVEN corpus to reveal the effectiveness of the proposed framework compared to state-of-the-art methods.},
  keywords={Event detection;Annotations;Aggregates;Semantics;Neural networks;Prototypes;Contrastive learning},
  doi={10.1109/IJCNN60899.2024.10651359},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10655304,
  author={Ma, Jiawei and Huang, Po-Yao and Xie, Saining and Li, Shang-Wen and Zettlemoyer, Luke and Chang, Shih-Fu and Yih, Wen-Tau and Xu, Hu},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={MoDE: CLIP Data Experts via Clustering}, 
  year={2024},
  volume={},
  number={},
  pages={26344-26353},
  abstract={The success of contrastive language-image pretraining (CLIP) relies on the supervision from the pairing between images and captions, which tends to be noisy in web- crawled data. We present Mixture of Data Experts (MoDE) and learn a system of CLIP data experts via clustering. Each data expert is trained on one data cluster, being less sensitive to false negative noises in other clusters. At inference time, we ensemble their outputs by applying weights determined through the correlation between task metadata and cluster conditions. To estimate the correlation pre-cisely, the samples in one cluster should be semantically similar, but the number of data experts should still be rea-sonable for training and inference. As such, we consider the ontology in human language and propose to use fine- grained cluster centers to represent each data expert at a coarse-grained level. Experimental studies show that four CLIP data experts on ViT-B/16 outperform the ViT-L/14 by OpenAI CLIP and OpenCLIP on zero-shot image classification but with less (<35%) training cost. Meanwhile, MoDE can train all data expert asynchronously and can flexibly include new data experts. The code is available here.},
  keywords={Training;Adaptation models;Correlation;Costs;Computational modeling;Noise;Semantics;Data Expert;Multi-Modal;Data Clustering},
  doi={10.1109/CVPR52733.2024.02489},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10357717,
  author={Sevastjanova, Rita and Vogelbacher, Simon and Spitz, Andreas and Keim, Daniel and El-Assady, Mennatallah},
  booktitle={2023 IEEE Visualization in Data Science (VDS)}, 
  title={Visual Comparison of Text Sequences Generated by Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={11-20},
  abstract={Causal language models have emerged as the leading technology for automating text generation tasks. Although these models tend to produce outputs that resemble human writing, they still suffer from quality issues (e.g., social biases). Researchers typically use automatic analysis methods to evaluate the model limitations, such as statistics on stereotypical words. Since different types of issues are embedded in the model parameters, the development of automated methods that capture all relevant aspects remains a challenge. To tackle this challenge, we propose a visual analytics approach that supports the exploratory analysis of text sequences generated by causal language models. Our approach enables users to specify starting prompts and effectively groups the resulting text sequences. To this end, we leverage a unified, ontology-driven embedding space, serving as a shared foundation for the thematic concepts present in the generated text sequences. Visual summaries provide insights into various levels of granularity within the generated data. Among others, we propose a novel comparison visualization that slices the embedding space and represents the differences between two prompt outputs in a radial layout. We demonstrate the effectiveness of our approach through case studies, showcasing its potential to reveal model biases and other quality issues.},
  keywords={Analytical models;Visual analytics;Semantics;Layout;Data visualization;Writing;Linguistics;Causal Language Models;Text Generation;Prompt Output Comparison},
  doi={10.1109/VDS60365.2023.00007},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10020509,
  author={Parolin, Erick Skorupa and Hu, Yibo and Khan, Latifur and Brandt, Patrick T. and Osorio, Javier and D’Orazio, Vito},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Confli-T5: An AutoPrompt Pipeline for Conflict Related Text Augmentation}, 
  year={2022},
  volume={},
  number={},
  pages={1906-1913},
  abstract={Recent advances in natural language processing (NLP) and Big Data technologies have been crucial for scientists to analyze political unrest and violence, prevent harm, and promote global conflict management. Government agencies and public security organizations have invested heavily in deep learning-based applications to study global conflicts and political violence. However, such applications involving text classification, information extraction, and other NLP-related tasks require extensive human efforts in annotating/labeling texts. While limited labeled data may drastically hurt the models’ performance (over-fitting), large demands on annotation tasks may turn real-world applications impracticable. To address this problem, we propose Confli-T5, a prompt-based method that leverages the domain knowledge from existing political science ontology to generate synthetic but realistic labeled text samples in the conflict and mediation domain. Our model allows generating textual data from the ground up and employs our novel Double Random Sampling mechanism to improve the quality (coherency and consistency) of the generated samples. We conduct experiments over six standard datasets relevant to political science studies to show the superiority of Confli-T5. Our codes are publicly available 1.},
  keywords={Text categorization;Standards organizations;Pipelines;Big Data;Natural language processing;Data models;Safety;text augmentation;generation;classification;natural language processing;conflict;coding event data;CAMEO},
  doi={10.1109/BigData55660.2022.10020509},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8808078,
  author={Kreines, Mikhail G. and Kreines, Elena M.},
  booktitle={2019 IEEE 21st Conference on Business Informatics (CBI)}, 
  title={Artificial Intelligence Tools for Business Applications: Objective Map of Science and Analysis of Texts}, 
  year={2019},
  volume={01},
  number={},
  pages={445-451},
  abstract={Business is looking for technological and investment possibilities in research and development (R&D). Here the basic problems are to find R&D's results and/or teams for solving the professional tasks and for making investment. But business has no personal view on scientific problems. So business is seeking the objective tools for forecasting and evaluation of R&D prospects and results. Experts have own interests and require a lot of funding. R&D reflections are the texts. The modern methods of computer analysis of texts can do a lot of the experts' work for making it more objective and cheaper. The tools for search, systematization and ranking R&D's results and teams are computer analysis of texts and the map of science. The map of science is the distribution of the collection of texts of a scientific nature by the topics. The map of science is a way to navigate through the world of scientific publications and R&D's teams, a tool for identifying trends and assessing R&D directions. The usual ways for the map of science formation use bibliometric/scientometric data, general probability models of the texts, expert's opinion or artificial intelligence (AI) models and methods based on the thesaurus or on the ontology of the subject domains. The interests of business are not in line with the orientation on a priori established ideas about possible topics or there number for rapidly changing scientific fields. Precisely these fields are of the greatest interest to business. On the basis of mathematical modeling of texts and large-scale text collections, an approach is proposed for the computational formation of the adaptive dynamic map of science that does not use a priori classification schemes and data of the scientific publications' citation. Topics (thematic groups), their number and the distribution of texts over the topics are determined computationally without experts' involvement. Examples of the maps of science for various collections of scientific publications are given. The original method is proposed for checking the adequacy of the text models and the map of science. The method uses the categorization of articles and their abstracts as the separate objects on the basis of computationally generated map (its topics). The results of the large-scale experiment confirmed the high efficiency of the proposed mathematical modeling of texts and text collections. The possibilities of practical use of the map of science for business applications are considered.},
  keywords={Research and development;Computational modeling;Analytical models;Business;Semantics;Tools;Adaptation models;text;collection;semantics;semiotics;modeling;interpretation;research and development;information retrieval;analytical support},
  doi={10.1109/CBI.2019.00058},
  ISSN={2378-1971},
  month={July},}@ARTICLE{10713368,
  author={Lee, Jinkyu and Kim, Jihie},
  journal={IEEE Access}, 
  title={Improving Commonsense Bias Classification by Mitigating the Influence of Demographic Terms}, 
  year={2024},
  volume={12},
  number={},
  pages={161480-161489},
  abstract={Understanding commonsense knowledge is crucial in the field of Natural Language Processing (NLP). However, the presence of demographic terms in commonsense knowledge poses a potential risk of compromising the performance of NLP models. This study aims to investigate and propose methods for enhancing the performance and effectiveness of a commonsense polarization classifier by mitigating the influence of demographic terms. Three methods are introduced in this paper: (1) hierarchical generalization of demographic terms (2) threshold-based augmentation and (3) integration of hierarchical generalization and threshold-based augmentation methods(IHTA). The first method involves replacing demographic terms with more general ones based on a term hierarchy ontology, aiming to mitigate the influence of specific terms. To address the limited bias-related information, the second method measures the polarization of demographic terms by comparing the changes in the model’s predictions when these terms are masked versus unmasked. This method augments commonsense sentences containing terms with high polarization values by replacing their predicates with synonyms generated by ChatGPT. The third method combines the two approaches, starting with threshold-based augmentation followed by hierarchical generalization. The experiments show that the first method increases the accuracy over the baseline by 2.33%, and the second one by 0.96% over standard augmentation methods. The IHTA techniques yielded an 8.82% and 9.96% higher accuracy than threshold-based and standard augmentation methods, respectively.},
  keywords={Accuracy;Predictive models;Commonsense reasoning;Standards;Ontologies;Chatbots;Training data;Systematics;Semantics;Prevention and mitigation;Demography;Natural language processing;Classification algorithms;Commonsense bias;demographic term;bias mitigation;hierarchical generalization;threshold-based augmentation},
  doi={10.1109/ACCESS.2024.3477599},
  ISSN={2169-3536},
  month={},}@ARTICLE{9032420,
  author={},
  journal={IEEE Std 2413-2019}, 
  title={IEEE Standard for an Architectural Framework for the Internet of Things (IoT)}, 
  year={2020},
  volume={},
  number={},
  pages={1-269},
  abstract={An architecture framework description for the Internet of Things (IoT) which conforms to the international standard ISO/IEC/IEEE 42010:2011 is defined. The architecture framework description is motivated by concerns commonly shared by IoT system stakeholders across multiple domains (transportation, healthcare, Smart Grid, etc.). A conceptual basis for the notion of things in the IoT is provided and the shared concerns as a collection of architecture viewpoints is elaborated to form the body of the framework description.},
  keywords={IEEE Standards;Computer architecture;Internet of Things;architectural framework;IEEE 2413;Internet of Things (IoT)},
  doi={10.1109/IEEESTD.2020.9032420},
  ISSN={},
  month={March},}@INPROCEEDINGS{10805070,
  author={Derin, Mehmet Oguz and Uçar, Erdem and Yergesh, Banu and Shimada, Yuki and Hong, Youjin and Lin, Xin-Yu},
  booktitle={2024 IEEE 3rd International Conference on Problems of Informatics, Electronics and Radio Engineering (PIERE)}, 
  title={Evaluating the Impact of Model Size on Multilingual JSON Structuring for Knowledge Graphs with Recent LLMs}, 
  year={2024},
  volume={},
  number={},
  pages={1890-1895},
  abstract={This study investigates the impact of model size on the multilingual JSON structuring capabilities of commercial Large Language Models (LLMs) for Knowledge Graph creation, emphasizing the integration of expert feedback and in-context learning. Focusing on Old Uyghur and Old Turkic as the subject language and various study or work languages, including Japanese and Kazakh, we evaluated the performance of the latest generation LLMs of two sizes within the same family in structuring complex philological plain text. Our methodology involved a comparative analysis of LLM performance across different model sizes and languages, incorporating expert feedback and in-context learning techniques through a custom-built annotation tool to anonymize the specific LLM size for fair evaluation and integrate structurization and structure translation. Our findings indicate that smaller models can perform comparably to larger, more costly models in JSON structuring tasks when leveraging expert feedback and in-context learning despite struggling with the quality of initial structurization. This trend was consistent across the evaluated work languages, albeit with some performance variations. The study underscores the significant potential of in-context learning and expert feedback in enhancing LLMs' structuring capabilities, particularly for under-resourced languages with unstructured yet comprehensive publications. These results have important implications for efficient and cost-effective Knowledge Graph creation in multilingual contexts, offering new avenues for processing and integrating complex philological data into structured, machine-readable formats.},
  keywords={Knowledge engineering;Analytical models;Translation;Annotations;Large language models;Focusing;Knowledge graphs;Market research;Multilingual;Informatics;Knowledge Graphs;Multilingual Structuring;Large Language Models (LLMs);In-Context Learning;GPT-4o},
  doi={10.1109/PIERE62470.2024.10805070},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10830995,
  author={Li, Jinghong and Phan, Huy and Gu, Wen and Ota, Koichi and Hasegawa, Shinobu},
  booktitle={2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Fish-Bone Diagram of Research Issue: Gain a Bird's-Eye View on a Specific Research Topic}, 
  year={2024},
  volume={},
  number={},
  pages={4936-4941},
  abstract={Novice researchers often face difficulties in understanding a multitude of academic papers and grasping the fundamentals of a new research field. To solve such problems, the knowledge graph supporting research survey is gradually being developed. Existing keyword-based knowledge graphs make it difficult for researchers to deeply understand abstract concepts. Meanwhile, novice researchers may find it difficult to use ChatGPT effectively for research surveys due to their limited understanding of the research field. Without the ability to ask proficient questions that align with key concepts, obtaining desired and accurate answers from this large language model (LLM) could be inefficient. This study aims to help novice researchers by providing a fish-bone diagram that includes causal relationships, offering an overview of the research topic. The diagram is constructed using the issue ontology from academic papers, and it offers a broad, highly generalized perspective of the research field, based on relevance and logical factors. Furthermore, we evaluate the strengths and improvable points of the fish-bone diagram derived from this study's development pattern, emphasizing its potential as a viable tool for supporting research survey.},
  keywords={Surveys;Training;Reviews;Knowledge graphs;Machine learning;Ontologies;User interfaces;Information retrieval;Prompt engineering;Sustainable development},
  doi={10.1109/SMC54092.2024.10830995},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10810521,
  author={Sophaken, Chotanansub and Vongpanich, Kantapong and Intaphan, Wachirawit and Utasri, Tharathon and Deepho, Chutamas and Takhom, Akkharawoot},
  booktitle={2024 8th International Conference on Information Technology (InCIT)}, 
  title={Leveraging Graph-RAG for Enhanced Diagnostic and Treatment Strategies in Dentistry}, 
  year={2024},
  volume={},
  number={},
  pages={606-611},
  abstract={This paper presents a method for extracting and interpreting information from diverse, unstructured dental literature using advanced AI techniques. By integrating information extraction, ontologies, and knowledge graphs, the approach enhances the efficiency and accuracy of dental data analysis. Named Entity Recognition (NER) and a Large Language Model (LLM) are employed to extract relevant entities and relationships, which are then structured into triples and integrated with a dental ontology to ensure contextual relevance. This enriched ontology supports Retrieval-Augmented Generation (RAG) applications, enabling advanced querying and analysis. The methodology improves the identification and categorization of dental conditions, treatments, and anatomical terms, providing a structured representation of dental knowledge. Knowledge graphs facilitate the representation and analysis of relationships between entities, fostering insightful interpretations and supporting hypothesis generation, thereby enhancing the accessibility and usability of dental knowledge. Experimental results demonstrate the effectiveness of this approach in managing complex dental information, showcasing the benefits of combining Knowledge Representation (KR) with Machine Learning (ML). This research contributes to dental studies by offering a robust framework for extracting and utilizing knowledge from diverse and extensive datasets.},
  keywords={Large language models;Retrieval augmented generation;Knowledge graphs;Named entity recognition;Machine learning;Ontologies;Dentistry;Data mining;Usability;Information technology;Information Extraction;Dental Literature;Large Language Models;Ontologies;Knowledge Graphs;Oral Health;Dentistry},
  doi={10.1109/InCIT63192.2024.10810521},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10727135,
  author={Maninger, Daniel and Narasimhan, Krishna and Mezini, Mira},
  booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER)}, 
  title={Towards Trustworthy AI Software Development Assistance}, 
  year={2024},
  volume={},
  number={},
  pages={112-116},
  abstract={It is expected that in the near future, AI software development assistants will play an important role in the software industry. However, current software development assistants tend to be unreliable, often producing incorrect, unsafe, or low-quality code. We seek to resolve these issues by introducing a holistic architecture for constructing, training, and using trustworthy AI software development assistants. In the center of the architecture, there is a foundational LLM trained on datasets representative of real-world coding scenarios and complex software architectures, and fine-tuned on code quality criteria beyond correctness. The LLM will make use of graph-based code representations for advanced semantic comprehension. We envision a knowledge graph integrated into the system to provide up-to-date background knowledge and to enable the assistant to provide appropriate explanations. Finally, a modular framework for constrained decoding will ensure that certain guarantees (e.g., for correctness and security) hold for the generated code.},
  keywords={Training;Codes;Software architecture;Semantics;Computer architecture;Knowledge graphs;Software;Security;Artificial intelligence;Software development management},
  doi={10.1145/3639476.3639770},
  ISSN={2832-7632},
  month={April},}@INPROCEEDINGS{10822514,
  author={Yang, Wenjun and Shen, Yiqing and Wang, Zehong and Zhao, Rui and Lu, Qitong and Liu, Xinsheng and Liu, Yungeng and Debbah, Merouane and Wang, Yu Guang and Li Wang, Shir},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={FalconProtein: Finetuning Falcon Foundation Model for Protein Engineering}, 
  year={2024},
  volume={},
  number={},
  pages={5409-5417},
  abstract={Large Language Models (LLMs) have demonstrated zero-shot generalization capabilities in analyzing and predicting protein properties through natural language interactions. However, existing protein-focused datasets for LLM fine-tuning, such as ProteinLMDataset with its 17.46 billion tokens for pre-training and 893,000 instructions for fine-tuning, face limitations. Specifically, they include insufficient coverage of protein functional properties, inadequate protein-protein interaction data, and limited integration of contextual information from biomedical literature. To overcome these challenges, we present ProteinPFAIDataset, which integrates data from UniProt and PubMed, comprising 72.8 million tokens for Supervised Fine-Tuning (SFT). ProteinPFAIDataset encompasses important protein characteristics including enzyme activities, molecular functions, pH dependence, tissue specificity, temperature sensitivity, subunit structure, and disease associations. Additionally, we propose a novel knowledge graph-based approach that incorporates over 300,000 biomedical literature entries, providing rich contextual information about protein functions and interactions. To validate the effectiveness of our dataset, we fine-tuned Falcon2-11B LLM, resulting in a model we call Falcon2-11B-PFAI. The fine-tuned model achieved state-of-the-art performance on ProteinLMBench, improving accuracy from 47.10% to 58.37%. The dataset is available at https://huggingface.co/datasets/xiaorui1/PFAI. The fine-tuned model is available at https://huggingface.co/xiaorui1/PFAI/tree/main.},
  keywords={Proteins;Temperature sensors;Protein engineering;Temperature dependence;Accuracy;Temperature;Sensitivity;Large language models;Biological system modeling;Protein sequence},
  doi={10.1109/BIBM62325.2024.10822514},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10722791,
  author={Haque, Mohd Ariful and Kamal, Marufa and George, Roy and Gupta, Kishor Datta},
  booktitle={2024 IEEE 11th International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={Utilizing Structural Metrics from Knowledge Graphs to Enhance the Robustness Quantification of Large Language Models (Extended Abstract)}, 
  year={2024},
  volume={},
  number={},
  pages={1-2},
  abstract={The goal of this study is to determine whether large language models (LLMs) like CodeLlama, Mistral, and Vicuna can be used to build knowledge graphs (KGs) from textual data. We create class descriptions for well-known KGs such as DBpedia, YAGO, and Google Knowledge Graph, from which we extract RDF triples and enhance these graphs using different preprocessing methods. Six structural quality measures are used in the study to compare the constructed and existing KGs. Our results demonstrate how important LLMs are to improving KG construction and provide insightful information for KG construction researchers. Moreover, an in-depth analysis of popular open-source LLM models enables researchers to identify the most efficient model for various tasks, ensuring optimal performance in specific applications.},
  keywords={Measurement;Analytical models;Large language models;Knowledge graphs;Ontologies;Data science;Resource description framework;Robustness;Internet},
  doi={10.1109/DSAA61799.2024.10722791},
  ISSN={2766-4112},
  month={Oct},}@INPROCEEDINGS{10411620,
  author={Shi, Yucheng and Ma, Hehuan and Zhong, Wenliang and Tan, Qiaoyu and Mai, Gengchen and Li, Xiang and Liu, Tianming and Huang, Junzhou},
  booktitle={2023 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
  title={ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs}, 
  year={2023},
  volume={},
  number={},
  pages={515-520},
  abstract={ChatGPT, as a recently launched large language model (LLM), has shown superior performance in various natural language processing (NLP) tasks. However, two major limitations hinder its potential applications: 1) the inflexibility of finetuning on downstream tasks, and 2) the lack of interpretability in the decision-making process. To tackle these limitations, we propose a novel framework that leverages the power of ChatGPT for specific tasks, such as text classification, while improving its interpretability. The proposed framework conducts a knowledge graph extraction task to extract refined and structural knowledge from the raw data using ChatGPT. The rich knowledge is then converted into a graph, which is further used to train an interpretable linear classifier to make predictions. To evaluate the effectiveness of our proposed method, we conduct experiments on four benchmark datasets. The results demonstrate that our method can significantly improve the prediction performance compared to directly utilizing ChatGPT for text classification tasks. Furthermore, our method provides a more transparent decision-making process compared with previous text classification methods. The code is available at https://github.com/sycny/ChatGraph.},
  keywords={Text categorization;Decision making;Knowledge graphs;Chatbots;Data models;Data mining;Task analysis;Text Classification;Large Language Models;Interpretability},
  doi={10.1109/ICDMW60847.2023.00073},
  ISSN={2375-9259},
  month={Dec},}@INPROCEEDINGS{10801853,
  author={Cornelio, Cristina and Diab, Mohammed},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery}, 
  year={2024},
  volume={},
  number={},
  pages={12435-12442},
  abstract={Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor’s logical rules accurately detect all failures in the analyzed tasks, and that Recover considerably outperforms, for both failure detection and recovery, a baseline method reliant solely on LLMs. Supplementary material, including the OntoThor ontology, is available at: https://recover-ontothor.github.io.},
  keywords={Costs;Large language models;Ontologies;Intelligent robots},
  doi={10.1109/IROS58592.2024.10801853},
  ISSN={2153-0866},
  month={Oct},}@INPROCEEDINGS{9216770,
  author={Jiang, Chen and Jagersand, Martin},
  booktitle={2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)}, 
  title={Bridging Visual Perception with Contextual Semantics for Understanding Robot Manipulation Tasks}, 
  year={2020},
  volume={},
  number={},
  pages={1447-1452},
  abstract={Understanding manipulation scenarios allows intelligent robots to plan for appropriate actions to complete a manipulation task successfully. It is essential for intelligent robots to semantically interpret manipulation knowledge by describing entities, relations and attributes in a structural manner. In this paper, we propose an implementing framework to generate high-level conceptual dynamic knowledge graphs from video clips. A combination of a Vision-Language model and an ontology system, in correspondence with visual perception and contextual semantics, is used to represent robot manipulation knowledge with Entity-Relation-Entity (E-R-E) and Entity-Attribute-Value (E-A-V) tuples. The proposed method is flexible and well-versed. Using the framework, we present a case study where robot performs manipulation actions in a kitchen environment, bridging visual perception with contextual semantics using the generated dynamic knowledge graphs.},
  keywords={Semantics;Ontologies;Manipulator dynamics;Visual perception;Robot kinematics;Task analysis},
  doi={10.1109/CASE48305.2020.9216770},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{10782119,
  author={Munzir, Syed I. and Hier, Daniel B. and Carrithers, Michael D.},
  booktitle={2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
  title={High Throughput Phenotyping of Physician Notes with Large Language and Hybrid NLP Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Deep phenotyping is the detailed description of patient signs and symptoms using concepts from an ontology. The deep phenotyping of the numerous physician notes in electronic health records requires high throughput methods. Over the past 30 years, progress toward making high-throughput phenotyping feasible. In this study, we demonstrate that a large language model and a hybrid NLP model (combining word vectors with a machine learning classifier) can perform high throughput phenotyping on physician notes with high accuracy. Large language models will likely emerge as the preferred method for high throughput deep phenotyping physician notes.Clinical relevance: Large language models will likely emerge as the dominant method for the high throughput phenotyping of signs and symptoms in physician notes},
  keywords={Accuracy;Large language models;Biological system modeling;Medical services;Machine learning;Ontologies;Assistive technologies;Throughput;Vectors;Electronic medical records},
  doi={10.1109/EMBC53108.2024.10782119},
  ISSN={2694-0604},
  month={July},}@INPROCEEDINGS{10679445,
  author={von der Assen, Jan and Sharif, Jamo and Feng, Chao and Killer, Christian and Bovet, Gérôme and Stiller, Burkhard},
  booktitle={2024 IEEE International Conference on Cyber Security and Resilience (CSR)}, 
  title={Asset-Centric Threat Modeling for AI-Based Systems}, 
  year={2024},
  volume={},
  number={},
  pages={437-444},
  abstract={Threat modeling for systems relying on Artificial In-telligence is not well explored. While conventional threat modeling methods and tools do not address AI-related threats, research on this amalgamation still lacks solutions capable of guiding and automating the process, as well as providing evidence that the methods hold up in practice. Consequently, this paper presents ThreatFinderAI, an approach and tool providing guidance and automation to model AI-related assets, threats, countermeasures, and quantify residual risks. To do so, ThreatFinderAI presents a novel AI-based stencil library for automated asset extraction, a threat knowledge graph spanning several community initiatives, and a novel method to identify business impacts of AI threats and an approach to quantify them. To evaluate the practicality of the approach, participants were tasked to recreate a threat model developed by cybersecurity experts of an AI-based healthcare platform. Secondly, the approach was used to identify and discuss strategic risks in an LLM-based application through a case study. Overall, the solution's usability was well-perceived and effectively supports threat identification and risk discussion.},
  keywords={Threat modeling;Automation;Medical services;Knowledge graphs;Libraries;Artificial intelligence;Usability;AI Security;Threat Modeling;Risk Analysis},
  doi={10.1109/CSR61664.2024.10679445},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10825705,
  author={Purohit, Sumit and Chin, George and Mackey, Patrick S and Cottam, Joseph A},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={GraphAide: Advanced Graph-Assisted Query and Reasoning System}, 
  year={2024},
  volume={},
  number={},
  pages={3485-3493},
  abstract={Curating knowledge from multiple siloed sources that contain both structured and unstructured data is a major challenge in many real-world applications. Pattern matching and querying represent fundamental tasks in modern data analytics that leverage this curated knowledge. The development of such applications necessitates overcoming several research challenges, including data extraction, named entity recognition, data modeling, and designing query interfaces. Moreover, the explainability of these functionalities is critical for their broader adoption.The emergence of Large Language Models (LLMs) has accelerated the development lifecycle of new capabilities. Nonetheless, there is an ongoing need for domain-specific tools tailored to user activities. The creation of such digital assistants has gained considerable traction in recent years, with LLMs offering a promising avenue to develop such assistants utilizing domain-specific knowledge and assumptions.In this context, we introduce an advanced query and reasoning system, GraphAide, which constructs a knowledge graph (KG) from diverse sources and allows to query and reason over the resulting KG. GraphAide harnesses both the KG and LLMs to rapidly develop domain-specific digital assistants. It integrates design patterns from retrieval augmented generation (RAG) and the semantic web to create an agentic LLM application. GraphAide underscores the potential for streamlined and efficient development of specialized digital assistants, thereby enhancing their applicability across various domains.},
  keywords={Semantic Web;Accuracy;Large language models;Scalability;Retrieval augmented generation;Semantics;Knowledge graphs;Cognition;Usability;Pattern matching},
  doi={10.1109/BigData62323.2024.10825705},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{9949792,
  author={Lu, Yao and Feng, Ding and Sun, Xiaojun and Lin, Sheng},
  booktitle={2022 IEEE/IAS Industrial and Commercial Power System Asia (I&CPS Asia)}, 
  title={Research on Efficient Information Mining Method for Defect Record of Traction Power Supply Equipment}, 
  year={2022},
  volume={},
  number={},
  pages={109-114},
  abstract={The defect record of traction power supply equipment not only describes the defect situation of the equipment, but also provides historical experience for the defect treatment of similar equipment. The recognition of named entity information of the defect record is the premise of constructing the knowledge graph. In order to mine the entity information of defect records more efficiently and accurately, this paper proposes a named entity recognition algorithm to solve the current situation of lack of utilization and analysis of defect records of traction power supply equipment and low efficiency of manual information processing modes. The bidirectional encoder representation from transformers (BERT) pre-training language model is used as the encoding layer of the word vector, the bidirectional long short term memory (BiLSTM) is used as the character label prediction layer, and the conditional random field (CRF) is used to output the global optimal label. The defect records of the traction power supply equipment of a maintenance department from 2016 to 2019 are used as the data set for case study, and, the comprehensive evaluation index of entity recognition of the model has reached 94.66%, saving 98.50% time compared with manual recognition. The results show that the entity information recognition algorithm proposed in this paper can effectively and accurately mine the defect record information of traction power supply equipment.},
  keywords={Decision making;Manuals;Information processing;Predictive models;Maintenance engineering;Transformers;Traction power supplies;named entity recognition;traction power supply equipment;defect record;deep learning;natural language processing},
  doi={10.1109/ICPSAsia55496.2022.9949792},
  ISSN={},
  month={July},}@INPROCEEDINGS{10822688,
  author={Chen, Dehua and Shen, Zijian and Wang, Mei and Dong, Na and Pan, Qiao and Su, Jianwen},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Extracting Structure Information from Narrative Medical Reports based on LLMs}, 
  year={2024},
  volume={},
  number={},
  pages={5616-5623},
  abstract={Extracting structured information and key details from medical report narratives is crucial to support healthcare data management, analysis and decision-making. However, the specialized nature of the reports, the complexity of the contents, and the high accuracy requirements of the results pose significant challenges to the structuring task. In this paper, we develop an LLM-based method to extract structure information from medical report narratives. Defining the structuring problem as mapping the narrative reports to the domain ontology, we design a framework to develop specialized LLMs that automatically learn and establish the mappings. At the core of this framework are report partitioning and interactive training data generation modules are. By separating complete reports into logically independent segments and training the LLMs on these segments independently, the trained LLMs can accurately capture the semantic relationships within each segment. Additionally, we explore different LLMs and formulate a simplistic scoring method to compare their accuracy, enabling us to select the best-performing model. Experimental evaluation on a real-world breast ultrasound report dataset demonstrates that our method achieves high accuracy with a small training dataset (400 samples). Specifically, the accuracy of structural information extraction and the attribute-value matching accuracy both exceed 96%.},
  keywords={Training;Accuracy;Ultrasonic imaging;Semantics;Training data;Medical services;Ontologies;Information retrieval;Data mining;Standards;Medical examination reports;Large language models;Report structuring},
  doi={10.1109/BIBM62325.2024.10822688},
  ISSN={2156-1133},
  month={Dec},}
